Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 465?472
Manchester, August 2008
Stopping Criteria for Active Learning of Named Entity Recognition
Florian Laws
Institute for NLP
Universit?at Stuttgart
fl@ifnlp.org
Hinrich Sch?utze
Institute for NLP
Universit?at Stuttgart
hs999@ifnlp.org
Abstract
Active learning is a proven method for re-
ducing the cost of creating the training sets
that are necessary for statistical NLP. How-
ever, there has been little work on stopping
criteria for active learning. An operational
stopping criterion is necessary to be able
to use active learning in NLP applications.
We investigate three different stopping cri-
teria for active learning of named entity
recognition (NER) and show that one of
them, gradient-based stopping, (i) reliably
stops active learning, (ii) achieves near-
optimal NER performance, (iii) and needs
only about 20% as much training data as
exhaustive labeling.
1 Introduction
Supervised statistical learning methods are impor-
tant and widely successful tools for natural lan-
guage processing. These methods learn by esti-
mating a statistical model on labeled training data.
Often, these models require a large amount of
training data that needs to be hand-annotated by
human experts. This is time-consuming and ex-
pensive. Active learning (AL) reduces this annota-
tion effort by selecting unlabeled examples that are
maximally informative for the statistical learning
method and handing them to a human annotator
for labeling. The statistical model is then updated
with the newly gathered information. In this pa-
per, we adopt the uncertainty sampling approach
to AL (Lewis and Gale, 1994). Uncertainty sam-
pling selects those examples in the pool as most in-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
formative for which the statistical classifier is least
certain in its classification decision.
While AL is an active area of research in NLP,
the issue of determining when to stop the AL
process has only recently come into focus (Zhu
and Hovy, 2007; Vlachos, 2008). This is some-
what surprising because the main purpose of ac-
tive learning is to save on annotation effort; decid-
ing on the point when enough data is annotated is
crucial to fulfilling this goal.
We investigate three different stopping criteria
in this paper. First, a user of a classification system
may want to set a minimum absolute performance
for the system to be deployed. The standard way of
assessing classifier performance uses a held-out la-
beled test set. However, labeling a test set of suffi-
cient size is contrary to the goal of minimizing an-
notation effort and impractical in most real-world
settings. We will show that the classifier can esti-
mate its own performance using only an unlabeled
reference set and propose to stop active learning
if estimated performance reaches the threshold set
by the user. The estimation is somewhat inaccu-
rate, however, and we investigate possible reasons
for estimation error.
An alternative criterion is based on maximum
possible performance. We will show that our per-
formance estimation method supports stopping AL
at a point where performance is almost optimal.
The third and last criterion is convergence. The
basic idea here is to stop active learning when more
examples from the pool do not contribute more
information, indicated either by the fact that the
classifier has reached maximum performance or by
the fact that the ?uncertainty? of the classifier can-
not be decreased further. We determine the point
where the pool has become uninformative by com-
puting the gradient of either performance or uncer-
465
tainty.
This paper is organized as follows. Section 2
shows that three uncertainty measures achieve
near-optimal performance for NER at a fraction
of the labeling cost of exhaustive labeling of the
training set. In Section 3, we introduce a new
method for estimating the performance of an ac-
tively learned classifier in support of stopping ac-
tive learning when a certain level of performance
has been reached. Section 4 shows that the stop-
ping criterion of reaching peak confidence is not
applicable to NER with multiclass logistic regres-
sion. Section 5 presents stopping criteria based
on convergence. Sections 6 and 7 discuss related
work and present our conclusions.
2 Selection Functions
For measuring the uncertainty of a classification
decision in uncertainty sampling there exist diverse
measures appropriate for different basic classifiers
(e.g. margin-based measures for SVMs, and mea-
sures based on class probability for classification).
Choosing such an uncertainty measure is relatively
straightforward for a binary classification problem,
but for multiclass problems we need different mea-
sures, and it is not obvious which will perform
best.
Following Schein (2005), but in the context of
NER, we compare several measures of uncertainty
for multiclass logistic regression. For a given mea-
sureM
i,X
, we select in each iteration the unlabeled
example(s) in the pool that have the smallest value
for M
i,X
(corresponding to the maximum uncer-
tainty).
1-Entropy.
M
i,1-Entropy
= 1 ?H(p?(.|x
i
))
= 1 +
?
j
p?(c
j
|x
i
) log p?(c
j
|x
i
)
where p?(c
j
|x
i
) is the current estimate of the proba-
bility of class c
j
given the example x
i
.
1
1-Entropy
favors examples where the classifier assigns simi-
lar probabilities to all classes.
Margin. If c and c
?
are the two most likely
classes, the margin is defined as follows:
M
i,Margin
= |p?(c|x
i
) ? p?(c
?
|x
i
)|
Margin picks examples where the distinction be-
tween two likely classes is hard.
1
We use 1-Entropy instead of entropy, so all three mea-
sures will have lower values for less certain instances.
MinMax.
M
i,MinMax
= max
j
(p?(c
j
|x
i
))
The rationale here is that a low probability of the
selected class indicates uncertainty. We propose
MinMax as a measure that is more directly based
on the classifier?s decision for a particular exam-
ple. The other two measures also take into account
the classifier?s assessment of classes that were not
chosen for the unlabeled example.
2.1 Experiments
We used the newswire section of the ACE 2005
Multilingual Training Corpus (128 documents,
66,015 tokens) for our experiments. A subset of
the documents was randomly sampled into an eval-
uation set that consists of 6301 tokens. We used
30.000 of the remaining tokens as the uncertainty
sampling pool. The rest was left aside for future
experiments. We use the BBR package (Genkin et
al., 2007) for binary logistic regression as our base
classifier, with default values for all of BBR?s pa-
rameters. As our main focus is on AL, we only
use basic features like capitalization, puctuation as
well as word identity, prefixes and suffixes, each
for the classified word itself and for left and right
contexts.
We train separate classifiers for each named en-
tity (NE) class and another one for the class ?not an
NE? (0). For each token we normalize the output
probability of the individual classifiers so they sum
to 1 and then select for each token the class with
the highest probability. Evaluation is performed by
comparing individual tokens to the gold standard.
2
Using all labeled training data as our fully super-
vised baseline results in a performance of 78.7%
F
1
(henceforth: F ) and 96.6% accuracy. This is
comparable to the accuracy of 96.29% reported
by (Daume III, 2007) on the newswire domain.
Daum?e?s work is the only study known to us that
uses the ACE dataset, but not the proprietary ACE
value score. In the rest of this paper, we report F
scores, because we believe that F is a more infor-
mative measure for NER than accuracy.
We use AL based on uncertainty sampling. We
start with a seed set of ten consecutive tokens
randomly selected from the training pool and la-
bel it. In each round of AL we select the ten
tokens with the smallest value of M
i,X
(where
2
Chunk-based NER results are not directly comparable
with this token-based evaluation.
466
Selection Baseline Peak perf.
1-Entropy 78.7 2139 (7.1%) 80.8 3460 (11.5%)
MinMax 78.7 2108 (7.0%) 80.8 3650 (12.1%)
Margin 78.7 2019 (6.7%) 81.2 3694 (12.3%)
Table 1: Percentage of data needed by AL to reach
baseline or peak performance.
X ? {1-Entropy,Margin,MinMax}) from the re-
maining pool, including tokens with the label 0.
We then label these tokens and add them to the la-
beled training set. The classifiers are retrained with
the new training set and the AL loop repeats. We
performed 20 runs of the experiments, each with
the same sampling pool, but a different seed set,
randomly selected as described above.
Table 1 shows that AL is quite successful for
NER. Only 7% of the training data is needed to
achieve the same performance as the supervised
baseline.
Furthermore we find that after the baseline per-
formance is reached the increase in performance
quickly levels off to a point where using more
training data does not yield performance improve-
ments anymore. In fact, our experiments show
that there is a peak in performance reached at
about 12% of the training data and performance
decreases again after this point (see Figure 1).
The peak is more prominent if the pool is large.
On a pool of 30,000 tokens, peak performance is
about 2.5% F -Score better than the baseline; on a
6000 token pool, the difference is only about 1.7%.
Therefore, once the peak is reached, the AL pro-
cess should stop, even if the annotation budget is
not yet used up.
0 2000 4000 6000 8000 10000
0.
60
0.
65
0.
70
0.
75
0.
80
Training examples
F?
Sc
or
e
Margin
1?Entropy
MinMax
Figure 1: Performance as a function of number of
labeled training examples used
Comparing the different selection functions, we
found little difference between their performance.
Margin performs significantly better (Student?s t-
test, ? = 0.05), but the difference is small (< 1%
F -Score). If we compare two AL processes (say
Margin and 1-Entropy) that were started with the
same pool and seed set and stop both processes
when they each reach their respective peak per-
formances, Margin has a better peak performance
of 0.3% F -Score on average (significant at ? =
0.05).
The differences between 1-Entropy andMinMax
are not statistically significant, except for a short
start-up phase (see Figure 1).
3 Performance Estimation
In practical applications, classifiers can only be re-
liably deployed when they attain a predefined min-
imum absolute performance level. Thus, we would
like to determine if this level has been reached and
then stop the annotation process. However, this is
not a simple task, because in these settings there
is no labeled test set available to evaluate perfor-
mance. Creating this test set would mean a sub-
stantial annotation effort, which is what we want
to avoid by using AL in the first place. Therefore,
we will try to estimate the classifier?s performance
on unlabeled data.
Following Lewis (1995), we estimate the F -
Score based on the current estimates of the class
probabilities. Based on the F measure?s definition
as the harmonic mean of precision (P) and recall
(R), we can write F as a function of true positives
(TP), false positives (TP) and false negatives (FN):
F =
2 ? P ?R
P +R
=
2TP
2TP + FP + FN
Similar to Lewis, we estimate
?
TP,
?
FP,
?
FN, but we
need to extend their work from binary classifica-
tion to 1-vs-all multiclass classification:
?
TP =
n
?
i
E
?
j
p?(c
j
|x
i
)d
i,j
(1)
?
FP =
n
?
i
E
?
j
(1 ? p?(c
j
|x
i
))d
i,j
(2)
?
FN =
n
?
i
E
?
j
p?(c
j
|x
i
)(1 ? d
i,j
) (3)
where n is the number of examples, E is the num-
ber of named entity classes, excluding the ?not
467
an NE? class. p?(c
j
|x
i
) is the estimated probabil-
ity that example x
i
has class c
j
. The flag d
i,j
indicates ?is winning class?: d
i,j
= 1 if j =
argmax
j
p?(c
j
|x
i
) and d
i,j
= 0 else.
Like standard NER evaluation schemes, e.g.
(Tjong Kim Sang and De Meulder, 2003), we con-
sider only those decisions to be TPs where (i) the
reference class matches the selected class and (ii)
this class is not ?not an NE?. When estimating
TP, we assume that the probability of a match
equals the probability of the selected class (which
is p?(c
j
|x
i
) ? d
i,j
). The probability of making an
FP error is just the remaining probability mass.
For FN, we can calculate the estimated probabil-
ity by summing up the class probabilities of the
non-selected named entity classes.
3.1 Evaluation of Performance Estimation
To evaluate the performance estimation method,
we ran it on an unlabeled reference set. The ref-
erence set is a set of unlabeled data distinct from
the sampling pool. In our experiments, we use the
tokens in the test set from 2.1, but with the labels
stripped off.
We compare the true performance on the test set
(reported as ?True? in Table 2) with the estimate
(reported as ?Lewis?). The ? columns report the
difference of the named method to ?True?. We also
tested leave-one-out (LOO) estimation of F , P and
R using the data of the selected training set.
True Lewis ? Lewis LOO ? LOO
F 79 92 +13 85 +6
P 81 92 +11 86 +5
R 77 92 +15 84 +7
Table 2: Performance estimation. LOO and Lewis
overestimate true F by 6% and 13%, respectively.
We find that both methods overestimate preci-
sion and recall by a large margin. We also note
that the peak in performance at about 4200 train-
ing examples that we found when evaluating on
held-out data (see Figure 1) does not occur when
evaluating performance using the Lewis method.
Instead, the estimate of F grows monotonically.
This means that we cannot use a peak of estimated
F as a criterion for stopping. When setting an ab-
solute threshold of F = 80% for stopping, active
learning stops at about 1000 iterations, yielding a
true performance of only F = 73% (selection by
Margin, 20 trials). This indicates that we cannot
directly use Lewis estimates for stopping.
3.2 Error Analysis
The reason for the overestimation is that the logis-
tic regression classifier is too confident in its own
decision. For positive decisions, the class proba-
bility very often is close to 1, for negative deci-
sions, it is close to 0. As a result, the estimator
gives very little score for FN (Equation 3) or FP
(Equation 2) in most instances, which leads to the
high overestimation of performance.
To verify this, we grouped the empirical prob-
ability of a selected class being the correct class
in bins according to the estimated probability of
the logistic classifier. Table 3 shows this empiri-
cal probability given a class and its estimate. The
table is split into two halves, such that the empir-
ical probabilities for positive decisions (the class
got chosen as the best class) and negative deci-
sions are shown separately. The top value in each
cell (?emp?) shows the empirical probability as op-
posed to the estimated probability, which is the
value below (?est?). The product of the differ-
ence of these two probabilities and the number of
instances that were counted into this bin (?cnt?),
gives an estimate of how much the probability es-
timates in the bin contribute to the error (absolute
value) of the performance estimation.
The table shows that class probabilities are in
fact estimated too optimistically. For many of the
entries in the positives table, the estimated prob-
abilities are greater than the empirical probabili-
ties. In the negatives table, the estimated proba-
bilities are smaller. In both cases, the estimates
are closer to the respective extreme values 1 or 0,
which means they are overconfident. Note that for
positive decisions, the estimation error of the val-
ues in a single bin contributes to the overall estima-
tion error in two ways: overestimating TPs and un-
derestimating FPs. For example, the estimation er-
ror for the cell in bold is 29.2, contributing ?29.2
for FP (underestimation) and +29.2 for TP (over-
estimation). Also note that due to the high num-
ber of non-NE tokens in the text, there is a large
number of negative decisions for each entity-class
classifier; thus, small differences in the probabili-
ties make large contributions to error.
We ran a separate experiment in which we
trained a classifier on the entire labeled pool. The
Lewis estimator overestimated F by 12% in this
case. This indicates that the estimation error does
not primarily come from the biased selection of
training examples inherent in the selective sam-
468
negative decisions positive decisions
0-.2 .2-.4 .4-.6 .2-.4 .4-.6 .6-.8 .8-1
O emp 0.0643 0.269 0.25 0.0 0.25 0.233 0.991
est 0.00825 0.295 0.438 0.394 0.537 0.714 0.999
cnt 607 26 12 1 16 30 5609
err 34 -0.67 -2.25 0.394(tn) 4.6 (tn) 14.4 (tn) 45.4 (tn)
GPE emp 0.00384 0.391 0.5 0.0 0.333 0.571 0.875
est 0.000812 0.296 0.435 0.357 0.535 0.687 0.989
cnt 5985 23 6 1 9 21 256
err 18.1 (fn) 2.19 (fn) 0.388 (fn) 0.357 (fp) 1.82 (fp) 2.42 (fp) 29.2 (fp)
ORG emp 0.00853 0.393 0.667 0.5 0.615 0.828
est 0.000847 0.283 0.441 0.545 0.71 0.968
cnt 6093 28 12 14 26 128
err 46.8 (fn) 3.06 (fn) 2.7 (fn) 0.631 (fp) 2.46 (fp) 17.9 (fp)
PER emp 0.0041 0.455 0.5 0.273 0.5 0.93
est 0.000748 0.283 0.48 0.563 0.718 0.98
cnt 6102 22 6 11 18 142
err 20.4 (fn) 3.78 (fn) 0.121 (fn) 3.2 (fp) 3.93 (fp) 7.19 (fp)
Table 3: Empirical probabilities and contribution to estimation errors. (We omit small classes and empty
columns.) Example (cell in bold): 256 tokens were estimated to be a GPE by the classifier with estimated
probabilities between 0.8 and 1.0. The average estimate was 0.989. In reality, only 224 of these tokens
(87.5%) were GPEs. The contribution of this cell to the overall FP count is (0.989?0.875) ?256 ? 29.2.
pling method, but from bias inherent in either the
whole pool of training data or the base classifier.
3.3 Towards a Better Estimate
Over-optimistic estimates for precision and recall
stem from the classifier?s over-optimistic probabil-
ity estimates. We try to correct the estimates by
replacing the predicted class probabilities with the
appropriate value in an empirical probability table
like the one shown in Table 3. However, since
in practice we do not have labels for the test set,
we cannot compute the empirical probabilities di-
rectly. Instead, we use leave-one-out estimation to
bootstrap the adjustment table from the selected
training data. The adjusted estimation shows a
marked increase in the estimates for FP and FN,
leading to a quite accurate estimate for precision
(+5 absolute error), but the now pessimistic esti-
mate for recall (?16) leads to underestimation of
F -Score overall (?8) (see Table 4).
True Lewis adj. Lewis ? adj. Lewis
F 78 91 70 -8
P 81 93 86 +5
R 76 89 60 -16
TP 520 596 555 +35
FP 125 48 90 -35
FN 163 70 379 +216
Table 4: Lewis estimation with adjusted probabili-
ties
As we see, the adjustment overshoots for recall,
indicating that the new estimated probabilities are
still off. There could be several reasons for this.
The first reason is that the bin width is quite coarse,
as there are only five bins for the entire probability
interval, each bin covering a range of 0.2. How-
ever, using finer bin widths can lead to data spar-
sity problems.
Another reason might be the estimation errors
within individual bins that compound to a quite
large overall error especially in the negative case.
Finally, differences in the distributions of training
set and reference set could cause unreliable esti-
mates. The empirical probabilities for the adjust-
ment table are estimated with leave-one-out on the
training set. However, since the training set is cre-
ated by selective sampling, it will be biased.
4 Confidence-based Stopping
We have found that performance estimation is not
yet reliable enough to stop when a desired perfor-
mance level is reached. However, since there is
a maximum performance that can be reached on
any given sampling pool, the annotation process
still should stop at this point regardless of whether
a target performance level has been reached or
not. We therefore seek a stopping criterion that
finds the maximum possible performance when the
classifier is iteratively trained on a given sampling
pool. Again, in practice we do not have a labeled
test set to evaluate against, so we have to try to find
the stopping point from either the remaining pool,
or the separate unlabeled reference set.
Vlachos (2008) proposes to calculate the confi-
dence of the classifier by using the average uncer-
469
tainty on the unlabeled reference set. For multi-
class problems, he uses SVM classifiers with the
SVM margin size as the uncertainty measure. Us-
ing this measure, Vlachos reports finding, albeit
distorted by fluctuations, a peak pattern in this con-
fidence measure that coincides with reaching max-
imal performance in his experiments. He then sug-
gests to use this peak confidence as the stopping
criterion.
However, in our experiments with multiclass
logistic regression, we could not find this peak
pattern when calculating the confidence using the
three uncertainty measures introduced above: 1-
Entropy, Margin and MinMax.
0 2000 4000 6000 8000 10000
0.
80
0.
85
0.
90
0.
95
1.
00
Iterations
Co
nf
id
en
ce
 o
n 
un
la
be
le
d 
re
fe
re
nc
e 
da
ta
1?Entropy
Minmax
Margin
Figure 2: Confidence on unlabeled reference set
(selection: 1-Entropy). The vertical lines indi-
cate when baseline and optimal performance are
reached. There is no peak pattern in the curves,
so reaching peak confidence cannot be used as a
stopping criterion.
In Figure 2, we show the three measures, av-
eraged over 20 trials as described in section 2.1.
Due to instability of AL during start-up, there are
some fluctuations in the first 100 iterations. Af-
ter 500 iterations the confidence curves stabilize
and at about 4000 iterations approach asymptotes,
without exhibiting peak patterns. Thus, the pro-
posed criterion of peak confidence based on aver-
age reference uncertainty does not seem applicable
for controlling AL with multiclass logistic regres-
sion.
5 Gradient-based Stopping
Since we cannot use peaks for stopping, we pro-
pose to stop when a base measurement character-
0 2000 4000 6000 8000 10000
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Iterations
M
ar
gi
n
Figure 3: Margin uncertainty of selected instance
(single run). The graph demonstrates that without
smoothing this criterion is too noisy.
izing the progress of active learning has converged.
We identify the point of convergence by computing
gradients. We find that the rise of the performance
estimation slows to an almost horizontal slope at
about the time when the true performance reaches
its peak. We therefore propose the following new
stopping criterion: Estimate the gradient of the
curve and stop when it approaches 0. Since we
do not need an accurate estimation of absolute per-
formance here, we can use the unadjusted Lewis
estimate for this method. We call this stopping cri-
terion (estimated) performance convergence.
In a similar way, we can use the gradient of the
uncertainty of the last selected instance. The in-
stance that was selected last is always the one with
maximum uncertainty, and thus the most informa-
tive for training. When the uncertainty measure
comes close to the extreme value of 1, we decide
that there are no informative examples left in the
pool and we stop the AL process. (Unfortunately,
1 is minimum uncertainty and 0 is maximum un-
certainty according to our definitions of the three
measures.) The gradient of the uncertainty mea-
sure approaches 0 at this point (see Figure 3), so
we can again use a gradient criterion for imple-
menting this idea. We call this stopping criterion
uncertainty convergence.
In Figure 3, which shows a graph of the Mar-
gin uncertainty of the selected instance, we can
also see that it is quite noisy. The value drops
sharply when some examples are encountered but
quickly returns to the previous level after a few it-
erations. The performance estimation measure is
470
slightly noisy as well, so we need a robust way of
computing the gradient. We achieve this with a
moving median approach. At each step, we com-
pute the median of w
2
= {a
n?k
, . . . , a
n
} (the last
n values) and of w
1
= {a
n?k?1
, . . . , a
n?1
} (the
previous last n values). Each value a
i
is the per-
formance at iteration i (for the performance gradi-
ent) or the uncertainty of the instance selected in
iteration i (for the uncertainty gradient).
We then estimate the gradient using the medians
of the two windows:
g = (median(w
2
) ? median(w
1
))/1 (4)
For the performance estimate, which is less noisy,
we can also use the arithmetic mean instead of the
median. In this case, we simply replace ?median?
with ?mean? in Equation 4.
We found that a window of size k = 100 yields
good results in mitigating the noise while still re-
acting fast enough to the changes in the gradient.
We combine this criterion with a maximum crite-
rion and only stop if the last value a
n
is a newmax-
imum. We stop the AL process when (i) the current
certainty or estimated performance is a new max-
imum and (ii) the newly calculated gradient g is
positive and (iii) g falls below a predefined level .
5.1 Evaluation
We show the results of gradient stopping applied
to each of the three uncertainty measures and the
Lewis estimate. For comparison, we also include
results with a threshold-based criterion, where AL
stops when the uncertainty measure of the selected
instance reaches a threshold of 1?. This is similar
to (Zhu and Hovy, 2007), but extended by us to all
three uncertainty measures.
Table 5 shows results for each criterion. The
?Stop? value indicates number of tokens at which
the stopping criterion stopped AL. ??Bl? indicates
the difference between baseline performance and
performance at the stopping point, ??Pk? the dif-
ference to peak performance. The ?sd? columns
show the respective standard deviations.
We find that all stopping criteria stop before
20% of the pool is used, providing a large reduc-
tion in annotation effort. While the point of peak
performance can not be precisely found by the cri-
teria, all criteria reliably stop at a performance
level that surpasses the fully supervised baseline.
The threshold criteria seem to be a bit better in
finding a stopping point closer to optimal perfor-
mance. Not unsurprisingly, the stopping function
that matches the selection function performs best.
The gradient methods, however, seem to be provid-
ing better-than-baseline performance more consis-
tently (less variation) and might require less tuning
of the threshold parameter when other factors (e.g.,
the batch size) change. If lower noise allows it, as
for the Lewis estimate, moving averages should be
used in place of moving medians.
6 Related Work
Sch?utze et al (2006) studied a Lewis-based per-
formance estimation method in a binary text clas-
sification setting. They attribute difficulties in esti-
mating recall to a ?missed cluster effect?, meaning
that the active sampling procedure is failing to se-
lect some clusters of relevant training examples in
the pool that are too dissimilar to the relevant ex-
amples already known. Diversity measures as pro-
posed by (Shen et al, 2004) might help in mitigat-
ing this effect, but our experiments show that there
are fundamental differences between text classifi-
cation and NER. Since missed clusters of relevant
examples in the training data would eventually be
used as we exhaustively label the entire pool, we
should see improvements in recall when the missed
clusters get used. Instead, we observed in section
2.1, that there are no further performance gains af-
ter a certain portion of the pool is labeled. Thus, all
examples that the classifier can make use of must
have been taken into account, and there appear to
be no missed clusters.
Tomanek et al (2007) present a stopping cri-
terion for query-by-committee-based AL that is
based on the rate of disagreement of the classifiers
in the committee. While our uncertainty conver-
gence criterion can only be applied to uncertainty
sampling, the performance convergence criterion
can be used in a committee-based setting.
Li and Sethi (2006) estimate the conditional er-
ror as a measure of uncertainty in selection (instead
of using it for stopping as we do), using a variable-
bin histogram for improving the error estimates.
They do not evaluate the quality of the probabil-
ity estimates. As with our stopping criterion, we
expect this selection criterion to be the more ef-
fective the more accurate the probability estimates
are. We therefore believe that our method of im-
proving probability estimates based on LOO bins
could improve their selection criterion.
471
Stop crit.  Peak Stop ? Bl sd ? Pk sd
1-Entropy threshold 0.01 80.8 3645 12.0% 1.44 0.7 ?0.68 0.4
MinMax threshold 0.01 80.8 3133 10.3% 0.11 1.0 ?2.0 0.8
Margin threshold 0.01 80.8 3158 10.4% 1.1 0.8 ?1.0 0.8
1-Entropy gradient 0.00005 80.8 4572 15.0% 0.97 0.4 ?1.1 0.5
MinMax gradient 0.00005 80.8 4397 14.5% 1.02 0.4 ?1.1 0.5
Margin gradient 0.00005 80.8 5292 17.5% 0.81 0.3 ?1.32 0.4
Lewis grd. (Median) 0.00005 80.8 2791 9.2% 0.8 1.4 ?1.3 1.4
Lewis grd. (Mean) 0.00005 80.8 3999 13.1% 1.1 0.8 ?0.95 0.6
Table 5: Performance at stopping points (baseline perf. 78.7, Selection: 1-Entropy)
7 Conclusion and Future Work
In this paper, we presented several criteria to stop
the AL process. For stopping the training at a
user-defined performance level, we proposed a
method for estimating classifier performance in a
multiclass classification setting. While we could
achieve acceptable accuracy in estimation of pre-
cision, we find that recall estimation is hard. Esti-
mation is not accurate enough to assist in making
a reliable decision if the performance of the classi-
fier is acceptable for practical use. In the future, we
plan to improve on performance estimation quality,
e.g., by using the variable-bin approach suggested
by Li and Sethi (2006).
Nevertheless, we showed that the gradient of the
performance estimate can successfully be used as
a stopping criterion relative to the optimal perfor-
mance that is attainable on a given pool. We also
describe stopping criteria based on the gradient of
the uncertainty measure of the instances selected
for training. The criteria reliably determine stop-
ping points that result in a performance that is bet-
ter than the supervised baseline and close to the
optimal performance. We believe that these crite-
ria can be applied to any AL setting based on un-
certainty sampling, not just NER.
If it turns out that the maximum possible per-
formance does not meet a user?s expectations, the
user needs to acquire fresh data and refill the pool.
This might lead to an approach to reduce the com-
putational cost of AL we want to evaluate in fu-
ture work: Subdivide a large sampling pool into
smaller sub-pools, run AL sequentially on the sub-
pools. When the stopping criterion is reached,
switch to the next sub-pool.
We also found that uncertainty curves of the se-
lected examples are quite noisy. We would like to
investigate which properties of the training exam-
ples cause these drops in the uncertainty curve.
References
Daume III, Hal. 2007. Frustratingly easy domain adap-
tation. In ACL-07, pages 256?263.
Genkin, A., D.D. Lewis, and D. Madigan. 2007.
Large-scale bayesian logistic regression for text cat-
egorization. Technometrics, 49(3):291?304.
Lewis, D.D. and W.A. Gale. 1994. A sequential algo-
rithm for training text classifiers. ACM SIGIR.
Lewis, D.D. 1995. Evaluating and optimizing au-
tonomous text classification systems. ACM SIGIR.
Li, M. and I.K. Sethi. 2006. Confidence-Based Ac-
tive Learning. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 28(8):1251?1261.
Schein, Andrew I. 2005. Active Learning for Logistic
Regression. Ph.D. thesis, University of Pennsylva-
nia.
Sch?utze, H., E. Velipasaoglu, and J.O. Pedersen. 2006.
Performance thresholding in practical text classifica-
tion. In CIKM, pages 662?671.
Shen, Dan, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In ACL ?04.
Tjong Kim Sang, Erik F. and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003, pages 142?147.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains reusabil-
ity of annotated data. In EMNLP-CoNLL.
Vlachos, Andreas. 2008. A stopping criterion for
active learning. Computer Speech and Language,
22(3):295?312.
Zhu, J. and E. Hovy. 2007. Active learning for word
sense disambiguation with methods for addressing
the class imbalance problem. In EMNLP-CoNLL.
472
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777?784
Manchester, August 2008
Estimation of Conditional Probabilities With Decision Trees and an
Application to Fine-Grained POS Tagging
Helmut Schmid and Florian Laws
IMS, University of Stuttgart
{schmid,lawsfn}@ims.uni-stuttgart.de
Abstract
We present a HMM part-of-speech tag-
ging method which is particularly suited
for POS tagsets with a large number of
fine-grained tags. It is based on three ideas:
(1) splitting of the POS tags into attribute
vectors and decomposition of the contex-
tual POS probabilities of the HMM into a
product of attribute probabilities, (2) esti-
mation of the contextual probabilities with
decision trees, and (3) use of high-order
HMMs. In experiments on German and
Czech data, our tagger outperformed state-
of-the-art POS taggers.
1 Introduction
A Hidden-Markov-Model part-of-speech tagger
(Brants, 2000, e.g.) computes the most probable
POS tag sequence
?
t
N
1
=
?
t
1
, ...,
?
t
N
for a given word
sequence w
N
1
.
?
t
N
1
= argmax
t
N
1
p(t
N
1
, w
N
1
)
The joint probability of the two sequences is de-
fined as the product of context probabilities and
lexical probabilities over all POS tags:
p(t
N
1
, w
N
1
) =
N
?
i=1
p(t
i
|t
i?1
i?k
)
? ?? ?
context prob.
p(w
i
|t
i
)
? ?? ?
lexical prob.
(1)
HMM taggers are fast and were successfully ap-
plied to a wide range of languages and training cor-
pora.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
POS taggers are usually trained on corpora with
between 50 and 150 different POS tags. Tagsets
of this size contain little or no information about
number, gender, case and similar morphosyntac-
tic features. For languages with a rich morphol-
ogy such as German or Czech, more fine-grained
tagsets are often considered more appropriate. The
additional information may also help to disam-
biguate the (base) part of speech. Without gender
information, for instance, it is difficult for a tagger
to correctly disambiguate the German sentence Ist
das Realit?at? (Is that reality?). The word das is
ambiguous between an article and a demonstrative.
Because of the lack of gender agreement between
das (neuter) and the noun Realit?at (feminine), the
article reading must be wrong.
The German Tiger treebank (Brants et al, 2002)
is an example of a corpus with a more fine-grained
tagset (over 700 tags overall). Large tagsets aggra-
vate sparse data problems. As an example, take the
German sentence Das zu versteuernde Einkommen
sinkt (?The to be taxed income decreases?; The
taxable income decreases). This sentence should
be tagged as shown in table 1.
Das ART.Def.Nom.Sg.Neut
zu PART.Zu
versteuernde ADJA.Pos.Nom.Sg.Neut
Einkommen N.Reg.Nom.Sg.Neut
sinkt VFIN.Full.3.Sg.Pres.Ind
. SYM.Pun.Sent
Table 1: Correct POS tags for the German sentence
Das zu versteuernde Einkommen sinkt.
Unfortunately, the POS trigram consisting of
the tags of the first three words does not occur
in the Tiger corpus. (Neither does the pair con-
sisting of the first two tags.) The unsmoothed
777
context probability of the third POS tag is there-
fore 0. If the probability is smoothed with the
backoff distribution p(?|PART.Zu), the most
probable tag is ADJA.Pos.Acc.Sg.Fem rather than
ADJA.Pos.Nom.Sg.Neut. Thus, the agreement be-
tween the article and the adjective is not checked
anymore.
A closer inspection of the Tiger corpus reveals
that it actually contains all the information needed
to completely disambiguate each component of the
POS tag ADJA.Pos.Nom.Sg.Neut:
? All words appearing after an article (ART)
and the infinitive particle zu (PART.zu) are at-
tributive adjectives (ADJA) (10 of 10 cases).
? All adjectives appearing after an article and
a particle (PART) have the degree positive
(Pos) (39 of 39 cases).
? All adjectives appearing after a nominative
article and a particle have nominative case (11
of 11 cases).
? All adjectives appearing after a singular arti-
cle and a particle are singular (32 of 32 cases).
? All adjectives appearing after a neuter article
and a particle are neuter (4 of 4 cases).
By (1) decomposing the context probability of
ADJA.Pos.Nom.Sg.Neut into a product of attribute
probabilities
p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu)
? p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA)
? p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA, 0:ADJA.Pos)
? p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom)
? p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg)
and (2) selecting the relevant context attributes
for the prediction of each attribute, we obtain the
following expression for the context probability:
p(ADJA | ART, PART.Zu)
? p(Pos | 2:ART, 1:PART, 0:ADJA)
? p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA)
? p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA)
? p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA)
The conditional probability of each attribute is
1. Hence the context probability of the whole tag is
also 1. Without having observed the given context,
it is possible to deduce that the observed POS tag
is the only possible tag in this context.
These considerations motivate an HMM tagging
approach which decomposes the POS tags into a
set of simple attributes, and uses decision trees to
estimate the probability of each attribute. Deci-
sion trees are ideal for this task because the iden-
tification of relevant attribute combinations is at
the heart of this method. The backoff smoothing
methods of traditional n-gram POS taggers require
an ordering of the reduced contexts which is not
available, here. Discriminatively trained taggers,
on the other hand, have difficulties to handle the
huge number of features which are active at the
same time if any possible combination of context
attributes defines a separate feature.
2 Decision Trees
Decision trees (Breiman et al, 1984; Quinlan,
1993) are normally used as classifiers, i.e. they as-
sign classes to objects which are represented as at-
tribute vectors. The non-terminal nodes are labeled
with attribute tests, the edges with the possible out-
comes of a test, and the terminal nodes are labeled
with classes. An object is classified by evaluating
the test of the top node on the object, following the
respective edge to a daughter node, evaluating the
test of the daughter node, and so on until a termi-
nal node is reached whose class is assigned to the
object.
Decision Trees are turned into probability esti-
mation trees by storing a probability for each pos-
sible class at the terminal nodes instead of a single
result class. Figure 1 shows a probability estima-
tion tree for the prediction of the probability of the
nominative attribute of nouns.
2.1 Induction of Decision Trees
Decision trees are incrementally built by first se-
lecting the test which splits the manually anno-
tated training sample into the most homogeneous
subsets with respect to the class. This test, which
maximizes the information gain
1
wrt. the class, is
1
The information gain measures how much the test de-
creases the uncertainty about the class. It is the difference
between the entropy of the empirical distribution of the class
variable in the training set and the weighted average entropy
778
2:N.Reg
p=0.571 p=0.938
p=0.999
0:N.Name
1:ART.Nom
0:N.Name 0:N.Name
p=0.948 p=0.998 ....
1:ADJA.Nom
yes
yes no
noyes no
yes no
no
yes
Figure 1: Probability estimation tree for the nomi-
native case of nouns. The test 1:ART.Nom checks
if the preceding word is a nominative article.
assigned to the top node. The tree is recursively
expanded by selecting the best test for each sub-
set and so on, until all objects of the current subset
belong to the same class. In a second step, the de-
cision tree may be pruned in order to avoid overfit-
ting to the training data.
Our tagger generates a predictor for each feature
(such as base POS, number, gender etc.) Instead of
using a single tree for the prediction of all possible
values of a feature (such as noun, article, etc. for
base POS), the tagger builds a separate decision
tree for each value. The motivation was that a tree
which predicts a single value (say verb) does not
fragment the data with tests which are only rele-
vant for the distinction of two other values (e.g. ar-
ticle and possessive pronoun).
2
Furthermore, we
observed that such two-class decision trees require
no optimization of the pruning threshold (see also
section 2.2.)
The tree induction algorithm only considers bi-
nary tests, which check whether some particular
attribute is present or not. The best test for each
node is selected with the standard information gain
criterion. The recursive tree building process ter-
minates if the information gain is 0. The decision
tree is pruned with the pruning criterion described
below.
Since the tagger creates a separate tree for each
attribute, the probabilities of a set of competing at-
tributes such as masculine, feminine, and neuter
will not exactly sum up to 1. To understand why,
assume that there are three trees for the gender at-
tributes. Two of them (say the trees for mascu-
line and feminine) consist of a single terminal node
in the two subsets. The weight of each subset is proportional
to its size.
2
We did not directly compare the two alternatives (two-
valued vs. multi-valued tests), because the implementational
effort required would have been too large.
which returns a probability of 0.3. The third tree
for neuter has one non-terminal and two terminal
nodes returning a probability of 0.3 and 0.5, re-
spectively. The sum of probabilities is therefore
either 0.9 or 1.1, but never exactly 1. This problem
is solved by renormalizing the probabilities.
The probability of an attribute (such as ?Nom?)
is always conditioned on the respective base POS
(such as ?N?) (unless the predicted attribute is the
base POS) in order to make sure that the probabil-
ity of an attribute is 0 if it never appeared with the
respective base POS. All context attributes other
than the base POS are always used in combination
with the base POS. A typical context attribute is
?1:ART.Nom? which states that the preceding tag
is an article with the attribute ?Nom?. ?1:ART? is
also a valid attribute specification, but ?1:Nom? is
not.
The tagger further restricts the set of possible
test attributes by requiring that some attribute of
the POS tag at position i-k (i=position of the pre-
dicted POS tag, k ? 1) must have been used be-
fore an attribute of the POS tag at position i-(k+1)
may be examined. This restriction improved the
tagging accuracy for large contexts.
2.2 Pruning Criterion
The tagger applies
3
the critical-value pruning strat-
egy proposed by (Mingers, 1989). A node is
pruned if the information gain of the best test mul-
tiplied by the size of the data subsample is below a
given threshold.
To illustrate the pruning, assume that D is the
data of the current node with 50 positive and 25
negative elements, and that D
1
(with 20 positive
and 20 negative elements) and D
2
(with 30 posi-
tive and 5 negative elements) are the two subsets
induced by the best test. The entropy of D is
?2/3 log
2
2/3 ? 1/3 log
2
1/3 = 0.92, the entropy
ofD
1
is?1/2 log
2
1/2?1/2 log
2
1/2 = 1, and the
entropy of D
2
is ?6/7 log
2
6/7 ? 1/7 log
2
1/7 =
0.59. The information gain is therefore 0.92 ?
(8/15 ? 1 ? 7/15 ? 0.59) = 0.11. The resulting
score is 75 ? 0.11 = 8.25. Given a threshold of 6,
the node is therefore not pruned.
We experimented with pre-pruning (where a
node is always pruned if the gain is below the
3
We also experimented with a pruning criterion based on
binomial tests, which returned smaller trees with a slightly
lower accuracy, although the difference in accuracy was never
larger than 0.1% for any context size. Thus, the simpler prun-
ing strategy presented here was chosen.
779
threshold) as well as post-pruning (where a node
is only pruned if its sub-nodes are terminal nodes
or pruned nodes). The performance of pre-pruning
was slightly better and it was less dependent on
the choice of the pruning threshold. A threshold
of 6 consistently produced optimal or near optimal
results for pre-pruning. Thus, pre-pruning with a
threshold of 6 was used in the experiments.
3 Splitting of the POS Tags
The tagger treats dots in POS tag labels as attribute
separators. The first attribute of a POS tag is the
main category. The number of additional attributes
is fixed for each main category. The additional
attributes are category-specific. The singular at-
tribute of a noun and an adjective POS tag are
therefore two different attributes.
4
Each position in the POS tags of a given cate-
gory corresponds to a feature. The attributes oc-
curring at a certain position constitute the value set
of the feature.
4 Our Tagger
Our tagger is a HMM tagger which decomposes
the context probabilities into a product of attribute
probabilities. The probability of an attribute given
the attributes of the preceding POS tags as well as
the preceding attributes of the predicted POS tag
is estimated with a decision tree as described be-
fore. The probabilities at the terminal nodes of the
decision trees are smoothed with the parent node
probabilities (which themselves were smoothed in
the same way). The smoothing is implemented by
adding the weighted class probabilities p
p
(c) of the
parent node to the frequencies f(c) before normal-
izing them to probabilities:
p(c) =
f(c) + ?p
p
(c)
? +
?
c
f(c)
The weight ? was fixed to 1 after a few experi-
ments on development data. This smoothing strat-
egy is closely related to Witten-Bell smoothing.
The probabilities are normalized by dividing them
by the total probability of all attribute values of the
respective feature (see section 2.1).
The best tag sequence is computed with the
Viterbi algorithm. The main differences of our tag-
ger to a standard trigram tagger are that the order of
the Markov model (the k in equation 1) is not fixed
4
This is the reason why the attribute tests in figure 1 used
complex attributes such as ART.Nom rather than Nom.
and that the context probability p(t
i
|t
i?1
i?k
) is inter-
nally computed as a product of attribute probabili-
ties. In order to increase the speed, the tagger also
applies a beam-search strategy which prunes all
search paths whose probability is below the prob-
ability of the best path times a threshold. With a
threshold of 10
?3
or lower, the influence of prun-
ing on the tagging accuracy was negligible.
4.1 Supplementary Lexicon
The tagger may use an external lexicon which sup-
plies entries for additional words which are not
found in the training corpus, and additional tags for
words which did occur in the training data. If an
external lexicon is provided, the lexical probabili-
ties are smoothed as follows: The tagger computes
the average tag probabilities of all words with the
same set of possible POS tags. The Witten-Bell
method is then applied to smooth the lexical prob-
abilities with the average probabilities.
If the word w was observed with N different
tags, and f(w, t) is the joint frequency of w and
POS tag t, and p(t|[w]) is the average probability
of t among words with the same set of possible
tags as w, then the smoothed probability of t given
w is defined as follows:
p(t|w) =
f(w, t) + Np(t|[w])
f(w) + N
The smoothed estimates of p(tag|word) are di-
vided by the prior probability p(tag) of the tag and
used instead of p(word|tag).
5
4.2 Unknown Words
The lexical probabilities of unknown words are
obtained as follows: The unknown words are di-
vided into four disjoint classes
6
with numeric ex-
pressions, words starting with an upper-case letter,
words starting with a lower-case letter, and a fourth
class for the other words. The tagger builds a suf-
fix trie for each class of unknown words using the
known word types from that class. The maximal
length of the suffixes is 7.
The suffix tries are pruned until (i) all suffixes
have a frequency of at least 5 and (ii) the informa-
tion gain multiplied by the suffix frequency and di-
5
p(word|tag) is equal to p(tag|word)p(word)/p(tag)
and p(word) is a constant if the tokenization is unambiguous.
Therefore dropping the factor p(word) has no influence on
the ranking of the different tag sequences.
6
In earlier experiments, we had used a much larger num-
ber of word classes. Decreasing their number to 4 turned out
to be better.
780
vided by the number of different POS tags is above
a threshold of 1. More precisely, if T
?
is the set of
POS tags that occurred with suffix ?, |T | is the
size of the set T , f
?
is the frequency of suffix ?,
and p
?
(t) is the probability of POS tag t among the
words with suffix ?, then the following condition
must hold:
f
a?
|T
a?
|
?
t?T
a?
p
a?
(t) log
p
a?
(t)
p
?
(t)
< 1
The POS probabilities are recursively smoothed
with the POS probabilities of shorter suffixes us-
ing Witten-Bell smoothing.
5 Evaluation
Our tagger was first evaluated on data from the
German Tiger treebank. The results were com-
pared to those obtained with the TnT tagger
(Brants, 2000) and the SVMTool (Gim?enez and
M`arquez, 2004), which is based on support vec-
tor machines.
7
The training of the SVMTool took
more than a day. Therefore it was not possible to
optimize the parameters systematically. We took
standard features from a 5 word window and M4-
LRL training without optimization of the regular-
ization parameter C.
In a second experiment, our tagger was also
evaluated on the Czech Academic corpus 1.0
(Hladk?a et al, 2007) and compared to the TnT tag-
ger.
5.1 Tiger Corpus
The German Tiger treebank (Brants et al, 2002)
contains over 888,000 tokens. It is annotated with
POS tags from the coarse-grained STTS tagset
and with additional features encoding informa-
tion about number, gender, case, person, degree,
tense, and mood. After deleting problematic sen-
tences (e.g. with an incomplete annotation) and au-
tomatically correcting some easily detectable er-
rors, 885,707 tokens were left. The first 80% were
used as training data, the first half of the rest as
development data, and the last 10% as test data.
Some of the 54 STTS labels were mapped to
new labels with dots, which reduced the number
of main categories to 23. Examples are the nom-
inal POS tags NN and NE which were mapped to
N.Reg and N.Name. Some lexically decidable dis-
tinctions missing in the Tiger corpus have been
7
It was planned to include also the Stanford tagger
(Toutanova et al, 2003) in this comparison, but it was not
possible to train it on the Tiger data.
automatically added. Examples are the distinc-
tion between definite and indefinite articles, and
the distinction between hyphens, slashes, left and
right parentheses, quotation marks, and other sym-
bols which the Tiger treebank annotates with ?$(?.
A supplementary lexicon was created by analyz-
ing a word list which included all words from the
training, development, and test data with a German
computational morphology. The analyses gener-
ated by the morphology were mapped to the Tiger
tagset. Note that only the words, but not the POS
tags from the test and development data were used,
here. Therefore, it is always possible to create a
supplementary lexicon for the corpus to be pro-
cessed.
In case of the TnT tagger, the entries of the sup-
plementary lexicon were added to the regular lex-
icon with a default frequency of 1 if the word/tag-
pair was unknown, and with a frequency propor-
tional to the prior probability of the tag if the word
was unknown. This strategy returned the best re-
sults on the development data. In case of the SVM-
Tool, we were not able to successfully integrate the
supplementary lexicon.
5.1.1 Refined Tagset
Prepositions are not annotated with case in the
Tiger treebank, although this information is impor-
tant for the disambiguation of the case of the next
noun phrase. In order to provide the tagger with
some information about the case of prepositions,
a second training corpus was created in which
prepositions which always select the same case,
such as durch (through), were annotated with this
case (APPR.Acc). Prepositions which select gen-
itive case, but also occur with dative case
8
, were
tagged with APPR.Gen. The more frequent ones
of the remaining prepositions, such as in (in), were
lexicalized (APPR.in). The refined tagset alo dis-
tinguished between the auxiliaries sein, haben, and
werden, and used lexicalized tags for the coor-
dinating conjunctions aber, doch, denn, wie, bis,
noch, and als whose distribution differs from the
distribution of prototypical coordinating conjunc-
tions such as und (and) or oder (or).
For evaluation purposes, the refined tags are
mapped back to the original tags. This mapping
is unambiguous.
8
In German, the genitive case of arguments is more and
more replaced by the dative.
781
tagger default refined ref.+lexicon
baseline 67.3 67.3 69.4
TnT 86.3 86.9 90.4
SVMTool 86.6 86.6 ?
2 tags 87.0 87.9 91.5
10 tags 87.6 88.5 92.2
Table 2: Tagging accuracies on development data
in percent. Results for 2 and for 10 preceding POS
tags as context are reported for our tagger.
5.1.2 Results
Table 2 summarizes the results obtained with
different taggers and tagsets on the development
data. The accuracy of a baseline tagger which
chooses the most probable tag
9
ignoring the con-
text is 67.3% without and 69.4% with the supple-
mentary lexicon.
The TnT tagger achieves 86.3% accuracy on the
default tagset. A tag is considered correct if all
attributes are correct. The tagset refinement in-
creases the accuracy by about 0.6%, and the ex-
ternal lexicon by another 3.5%.
The SVMTool is slightly better than the TnT
tagger on the default tagset, but shows little im-
provement from the tagset refinement. Apparently,
the lexical features used by the SVMTool encode
most of the information of the tagset refinement.
With a context of two preceding POS tags (sim-
ilar to the trigram tagger TnT), our tagger outper-
forms TnT by 0.7% on the default tagset, by 1%
on the refined tagset, and by 1.1% on the refined
tagset plus the additional lexicon. A larger context
of up to 10 preceding POS tags further increased
the accuracy by 0.6, 0.6, and 0.7%, respectively.
default refined ref.+lexicon
TnT STTS 97.28
TnT Tiger 97.17 97.26 97.51
10 tags 97.39 97.57 97.97
Table 3: STTS accuracies of the TnT tagger trained
on the STTS tagset, the TnT tagger trained on the
Tiger tagset, and our tagger trained on the Tiger
tagset.
These figures are considerably lower than
e.g. the 96.7% accuracy reported in Brants (2000)
for the Negra treebank which is annotated with
STTS tags without agreement features. This is to
9
Unknown words are tagged by choosing the most fre-
quent tag of words with the same capitalization.
be expected, however, because the STTS tagset is
much smaller. Table 3 shows the results of an eval-
uation based on the plain STTS tagset. The first
result was obtained with TnT trained on Tiger data
which was mapped to STTS before. The second
row contains the results for the TnT tagger when
it is trained on the Tiger data and the output is
mapped to STTS. The third row gives the corre-
sponding figures for our tagger.
91.491.5
91.691.7
91.891.9
9292.1
92.292.3
2 3 4 5 6 7 8 9 10
Figure 2: Tagging accuracy on development data
depending on context size
Figure 2 shows that the tagging accuracy tends
to increase with the context size. The best results
are obtained with a context size of 10. What type
of information is relevant across a distance of ten
words? A good example is the decision tree for the
attribute first person of finite verbs, which looks
for a first person pronoun at positions -1 through
-10 (relative to the position of the current word) in
this order. Since German is a verb-final language,
these tests clearly make sense.
Table 4 shows the performance on the test data.
Our tagger was used with a context size of 10. The
suffix length parameter of the TnT tagger was set
to 6 without lexicon and to 3 with lexicon. These
values were optimal on the development data. The
accuracy of our tagger is lower than on the devel-
opment data. This could be due to the higher rate
of unknown words (10.0% vs. 7.7%). Relative to
the TnT tagger, however, the accuracy is quite sim-
ilar for test and development data. The differences
between the two taggers are significant.
10
tagger default refined ref.+lexicon
TnT 83.45 84.11 89.14
our tagger 85.00 85.92 91.07
Table 4: Tagging accuracies on test data.
By far the most frequent tagging error was the
confusion of nominative and accusative case. If
10
726 sentences were better tagged by TnT (i.e. with few
errors), 1450 sentences were better tagged by our tagger. The
resulting score of a binomial test is below 0.001.
782
this error is not counted, the tagging accuracy
on the development data rises from 92.17% to
94.27%.
Our tagger is quite fast, although not as fast as
the TnT tagger. With a context size of 3 (10), it an-
notates 7000 (2000) tokens per second on a com-
puter with an Athlon X2 4600 CPU. The training
with a context size of 10 took about 4 minutes.
5.2 Czech Academic Corpus
We also evaluated our tagger on the Czech Aca-
demic corpus (Hladk?a et al, 2007) which contains
652.131 tokens and about 1200 different POS tags.
The data was divided into 80% training data, 10%
development data and 10% test data.
88.5
88.6
88.7
88.8
88.9
89
2 3 4 5 6 7 8 9 10
?context-data2?
Figure 3: Accuracy on development data depend-
ing on context size
The best accuracy of our tagger on the develop-
ment set was 88.9% obtained with a context of 4
preceding POS tags. The best accuracy of the TnT
tagger was 88.2% with a maximal suffix length of
5. The corresponding figures for the test data are
89.53% for our tagger and 88.88% for the TnT tag-
ger. The difference is significant.
6 Discussion
Our tagger combines two ideas, the decomposition
of the probability of complex POS tags into a prod-
uct of feature probabilities, and the estimation of
the conditional probabilities with decision trees. A
similar idea was previously presented in Kempe
(1994), but apparently never applied again. The
tagging accuracy reported by Kempe was below
that of a traditional trigram tagger. Unlike him,
we found that our tagging method out-performed
state-of-the-art POS taggers on fine-grained POS
tagging even if only a trigram context was used.
Schmid (1994) and M`arquez (1999) used deci-
sion trees for the estimation of contextual tag prob-
abilities, but without a decomposition of the tag
probability. Magerman (1994) applied probabilis-
tic decision trees to parsing, but not with a genera-
tive model.
Provost & Domingos (2003) noted that well-
known decision tree induction algorithms such as
C4.5 (Quinlan, 1993) or CART (Breiman et al,
1984) fail to produce accurate probability esti-
mates. They proposed to grow the decision trees to
their maximal size without pruning, and to smooth
the probability estimates with add-1 smoothing
(also known as the Laplace correction). Ferri
et al (2003) describe a more complex backoff
smoothing method. Contrary to them, we ap-
plied pruning and found that some pruning (thresh-
old=6) gives better results than no pruning (thresh-
old=0). Another difference is that we used N two-
class trees with normalization to predict the prob-
abilities of N classes. These two-class trees can be
pruned with a fixed pruning threshold. Hence there
is no need to put aside training data for parameter
tuning.
An open question is whether the SVMTool (or
other discriminatively trained taggers) could out-
perform the presented tagger if the same decompo-
sition of POS tags and the same context size was
used. We think that this might be the case if the
SVM features are restricted to the set of relevant
attribute combinations discovered by the decision
tree, but we doubt that it is possible to train the
SVMTool (or other discriminatively trained tag-
gers) without such a restriction given the difficul-
ties to train it with the standard context size.
Czech POS tagging has been extensively stud-
ied in the past (Haji?c and Vidov?a-Hladk?a, 1998;
Haji?c et al, 2001; Votrubec, 2006). Spoustov et
al. (2007) compared several POS taggers includ-
ing an n-gram tagger and a discriminatively trained
tagger (Mor?ce), and evaluated them on the Prague
Dependency Treebank (PDT 2.0). Mor?ce?s tag-
ging accuracy was 95.12%, 0.3% better than the
n-gram tagger. A hybrid system based on four
different tagging methods reached an accuracy of
95.68%. Because of the different corpora used and
the different amounts of lexical information avail-
able, a direct comparison to our results is difficult.
Furthermore, our tagger uses no corpus-specific
heuristics, whereas Mor?ce e.g. is optimized for
Czech POS tagging.
The German tagging results are, to the best of
our knowledge, the first published results for fine-
grained POS tagging with the Tiger tagset.
783
7 Summary
We presented a HMM POS tagger for fine-grained
tagsets which splits the POS tags into attribute
vectors and estimates the conditional probabili-
ties of the attributes with decision trees. In ex-
periments with German and Czech corpora, this
method achieved a higher tagging accuracy than
two state-of-the-art general-purpose POS taggers
(TnT and SVMTool).
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Brants, Thorsten. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference ANLP-
2000, Seattle, WA.
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J.
Stone. 1984. Classification and Regression Trees.
Wadsworth and Brooks, Pacific Grove CA.
Ferri, C., P. Flach, and J. Hern?andez-Orallo. 2003. Im-
proving the AUC of probabilistic estimators trees. In
Proceedings of 14th European Conference on Ma-
chine Learning (ECML?03), pages 121?132.
Gim?enez, Jes?us and Llu??s M`arquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of the IV Interna-
tional Conference on Language Resources and Eval-
uation (LREC?04), pages 43?46, Lisbon, Portugal.
Haji?c, Jan and Barbora Vidov?a-Hladk?a. 1998. Tag-
ging inflective languages: Prediction of morpholog-
ical categories for a rich, structured tagset. In Pro-
ceedings of ACL-COLING?98, Montreal, Canada.
Haji?c, Jan, Pavel Krbec, Karel Oliva, Pavel Kv?eto?n,
and Vladim??r Petkevi?c. 2001. Serial combination of
rules and statistics: A case study in czech tagging. In
Proceedings of the 39th Annual Meeting of the ACL,
Toulouse, France.
Hladk?a, Barbora Vidov?a, Jan Hajic, Jir?? Hana, Jaroslava
Hlav?acov?a, Jir?? M??rovsk?y, and Jan Votrubec. 2007.
Czech Academic Corpus 1.0 Guide. Karolinum
Press, Prag, Czechia.
Kempe, Andr?e. 1994. Probabilistic tagging with fea-
ture structures. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING 1994), pages 161?165, Kyoto, Japan.
Magerman, David M. 1994. Natural Language Pro-
cessing as Statistical Pattern Recognition. Ph.D.
thesis, Stanford University.
M`arquez, Llu??s. 1999. POS Tagging : A Ma-
chine Learning Approach based on Decision Trees.
Ph.D. thesis, Dep. LSI, Universitat Politecnica de
Catalunya (UPC), Barcelona, Spain, July.
Mingers, John. 1989. An empirical comparison of
pruning methods for decision tree induction. Ma-
chine Learning, 4:227?243.
Provost, Foster and Pedro Domingos. 2003. Tree
induction for probability-based ranking. Machine
Learning, 52(3):199?215.
Quinlan, J. Ross. 1993. C4.5 : Programs for Machine
Learning. Morgan Kaufmann, San Mateo , CA.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Spoustov?a, Drahom??ra, Jan Haji?c, Jan Votrubec, Pavel
Krbec, and Pavel Kv?eto?n. 2007. The best of two
worlds: Cooperation of statistical and rule-based tag-
gers for czech. In Proceedings of the Workshop on
Balto-Slavonic Natural Language Processing, pages
67?74, Prague, Czech Republic, June.
Toutanova, Kristina, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network. In
Proceedings of HLT-NAACL 2003, pages 252?259,
Edmonton, Canada.
Votrubec, Jan. 2006. Morphological tagging based on
averaged perceptron. In Proceedings of the 15th An-
nual Conference of Doctoral Students (WDS), pages
191?195.
784
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 91?95,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A Graph-Theoretic Algorithm for Automatic Extension of Translation
Lexicons
Beate Dorow Florian Laws Lukas Michelbacher Christian Scheible Jason Utt
Institute for Natural Language Processing
Universita?t Stuttgart
{dorowbe,lawsfn,michells,scheibcn,uttjn}@ims.uni-stuttgart.de
Abstract
This paper presents a graph-theoretic
approach to the identification of yet-
unknown word translations. The proposed
algorithm is based on the recursive Sim-
Rank algorithm and relies on the intuition
that two words are similar if they estab-
lish similar grammatical relationships with
similar other words. We also present a for-
mulation of SimRank in matrix form and
extensions for edge weights, edge labels
and multiple graphs.
1 Introduction
This paper describes a cross-linguistic experiment
which attempts to extend a given translation dic-
tionary with translations of novel words.
In our experiment, we use an English and
a German text corpus and represent each cor-
pus as a graph whose nodes are words and
whose edges represent grammatical relationships
between words. The corpora need not be parallel.
Our intuition is that a node in the English and a
node in the German graph are similar (that is, are
likely to be translations of one another), if their
neighboring nodes are. Figure 1 shows part of the
English and the German word graph.
Many of the (first and higher order) neighbors
of food and Lebensmittel translate to one another
(marked by dotted lines), indicating that food and
Lebensmittel, too, are likely mutual translations.
Our hypothesis yields a recursive algorithm for
computing node similarities based on the simi-
larities of the nodes they are connected to. We
initialize the node similarities using an English-
German dictionary whose entries correspond to
known pairs of equivalent nodes (words). These
node equivalences constitute the ?seeds? from
which novel English-German node (word) corre-
spondences are bootstrapped.
We are not aware of any previous work using a
measure of similarity between nodes in graphs for
cross-lingual lexicon acquisition.
Our approach is appealing in that it is language
independent, easily implemented and visualized,
and readily generalized to other types of data.
Section 2 is dedicated to related research on
the automatic extension of translation lexicons. In
Section 3 we review SimRank (Jeh and Widom,
2002), an algorithm for computing similarities of
nodes in a graph, which forms the basis of our
work. We provide a formulation of SimRank in
terms of simple matrix operations which allows
an efficient implementation using optimized ma-
trix packages. We further present a generalization
of SimRank to edge-weighted and edge-labeled
graphs and to inter-graph node comparison.
Section 4 describes the process used for build-
ing the word graphs. Section 5 presents an experi-
ment for evaluating our approach to bilingual lex-
icon acquisition. Section 6 reports the results. We
present our conclusions and directions for future
research in Section 7.
2 Related Work on cross-lingual lexical
acquisition
The work by Rapp (1999) is driven by the idea
that a word and its translation to another lan-
guage are likely to co-occur with similar words.
Given a German and an English corpus, he com-
putes two word-by-word co-occurrence matrices,
one for each language, whose columns span a vec-
tor space representing the corresponding corpus.
In order to find the English translation of a Ger-
man word, he uses a base dictionary to translate
all known column labels to English. This yields
a new vector representation of the German word
in the English vector space. This mapped vector
is then compared to all English word vectors, the
most similar ones being candidate translations.
91
food Lebensmittel
receive erhalten
award Preis
provide liefern
evidence Beweis
buy kaufen
book Buch
publish verlegen
boat Haus
waste ablehnen
Figure 1: Likely translations based on neighboring nodes
Rapp reports an accuracy of 72% for a small
number of test words with well-defined meaning.
Diab and Finch (2000) first compute word sim-
ilarities within each language corpus separately
by comparing their co-occurrence vectors. Their
challenge then is to derive a mapping from one
language to the other (i.e. a translation lexicon)
which best preserves the intra-language word sim-
ilarities. The mapping is initialized with a few seed
?translations? (punctuation marks) which are as-
sumed to be common to both corpora.
They test their method on two corpora written
in the same language and report accuracy rates of
over 90% on this pseudo-translation task. The ap-
proach is attractive in that it does not require a
seed lexicon. A drawback is its high computational
cost.
Koehn and Knight (2002) use a (linear) com-
bination of clues for bootstrapping an English-
German noun translation dictionary. In addition to
similar assumptions as above, they consider words
to be likely translations of one another if they have
the same or similar spelling and/or occur with sim-
ilar frequencies. Koehn and Knight reach an accu-
racy of 39% on a test set consisting of the 1,000
most frequent English and German nouns. The
experiment excludes verbs whose semantics are
more complex than those of nouns.
Otero and Campos (2005) extract English-
Spanish pairs of lexico-syntactic patterns from a
small parallel corpus. They then construct con-
text vectors for all English and Spanish words by
recording their frequency of occurrence in each of
these patterns. English and Spanish vectors thus
reside in the same vector space and are readily
compared.
The approach reaches an accuracy of 89% on a
test set consisting of 100 randomly chosen words
from among those with a frequency of 100 or
higher. The authors do not report results for low-
frequency words.
3 The SimRank algorithm
An algorithm for computing similarities of nodes
in graphs is the SimRank algorithm (Jeh and
Widom, 2002). It was originally proposed for di-
rected unweighted graphs of web pages (nodes)
and hyperlinks (links).
The idea of SimRank is to recursively com-
pute node similarity scores based on the scores
of neighboring nodes. The similarity Sij of two
different nodes i and j in a graph is defined as
the normalized sum of the pairwise similarities of
their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl. (1)
N(i) and N(j) are the set of i?s and j?s neigh-
bors respectively, and c is a multiplicative factor
smaller than but close to 1 which demotes the con-
tribution of higher order neighbors. Sij is set to 1
if i and j are identical, which provides a basis for
the recursion.
3.1 Matrix formulation of SimRank
We derive a formulation of the SimRank similarity
updates which merely consists of matrix multipli-
cations as follows. In terms of the graph?s (binary)
adjacency matrix A, the SimRank recursion reads:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Aik Ajl Skl
(2)
noting that AikAjl = 1, iff k is a neighbor of i
and l is a neighbor of j at the same time. This is
92
equivalent to
Sij = c
?
k,l
Aik
|N(i)|
Ajl
|N(j)| Skl (3)
= c
?
k,l
Aik
?
? Ai?
Ajl
?
? Aj?
Skl.
The Sij can be assembled in a square node sim-
ilarity matrix S, and it is easy to see that the indi-
vidual similarity updates can be summarized as:
Sk = c A? Sk?1A?T (4)
where A? is the row-normalized adjacency matrix
and k denotes the current level of recursion. A? is
obtained by dividing each entry of A by the sum of
the entries in its row. The SimRank iteration is ini-
tialized with S = I , and the diagonal of S, which
contains the node self-similarities, is reset to ones
after each iteration.
This representation of SimRank in closed ma-
trix form allows the use of optimized off-the-shelf
sparse matrix packages for the implementation of
the algorithm. This rendered the pruning strate-
gies proposed in the original paper unnecessary.
We also note that the Bipartite SimRank algorithm
introduced in (Jeh and Widom, 2002) is just a spe-
cial case of Equation 4.
3.2 Extension with weights and link types
The SimRank algorithm assumes an unweighted
graph, i.e. a binary adjacency matrix A. Equa-
tion 4 can equally be used to compute similarities
in a weighted graph by letting A? be the graph?s
row-normalized weighted adjacency matrix. The
entries of A? then represent transition probabili-
ties between nodes rather than hard (binary) adja-
cency. The proof of the existence and uniqueness
of a solution to this more general recursion pro-
ceeds in analogy to the proof given in the original
paper.
Furthermore, we allow the links in the graph to
be of different types and define the following gen-
eralized SimRank recursion, where T is the set of
link types and Nt(i) denotes the set of nodes con-
nected to node i via a link of type t.
Sij =
c
|T |
?
t?T
1
|Nt(i)| |Nt(j)|
?
k?Nt(i),l?Nt(j)
Skl.
(5)
In matrix formulation:
Sk =
c
|T |
?
t?T
A?t Sk?1A?t
T (6)
where At is the adjacency matrix associated with
link type t and, again, may be weighted.
3.3 SimRank across graphs
SimRank was originally designed for the com-
parison of nodes within a single graph. However,
SimRank is readily and accordingly applied to
the comparison of nodes of two different graphs.
The original SimRank algorithm starts off with the
nodes? self-similarities which propagate to other
non-identical pairs of nodes. In the case of two dif-
ferent graphs A and B, we can instead initialize the
algorithm with a set of initially known node-node
correspondences.
The original SimRank equation (2) then be-
comes
Sij =
c
|N(i)| |N(j)|
?
k,l
Aik Bjl Skl, (7)
which is equivalent to
Sk = c A? Sk?1 B?T , (8)
or, if links are typed,
Sk =
c
|T |
?
t?T
A?t Sk?1 B?t
T . (9)
The similarity matrix S is now a rectangular
matrix containing the similarities between nodes
in A and nodes in B. Those entries of S which
correspond to known node-node correspondences
are reset to 1 after each iteration.
4 The graph model
The grammatical relationships were extracted
from the British National Corpus (BNC) (100 mil-
lion words), and the Huge German Corpus (HGC)
(180 million words of newspaper text). We com-
piled a list of English verb-object (V-O) pairs
based on the verb-argument information extracted
by (Schulte im Walde, 1998) from the BNC. The
German V-O pairs were extracted from a syntactic
analysis of the HGC carried out using the BitPar
parser (Schmid, 2004).
We used only V-O pairs because they consti-
tute far more sense-discriminative contexts than,
for example, verb-subject pairs, but we plan to ex-
amine these and other grammatical relationships
in future work.
We reduced English compound nouns to their
heads and lemmatized all data. In English phrasal
93
English German
Low Mid High Low Mid High
N V N V N V N V N V N V
0.313 0.228 0.253 0.288 0.253 0.255 0.232 0.247 0.205 0.237 0.211 0.205
Table 1: The 12 categories of test words, with mean relative ranks of test words
verbs, we attach the particles to the verbs to dis-
tinguish them from the original verb (e.g put off
vs. put). Both the English and German V-O pairs
were filtered using stop lists consisting of modal
and auxiliary verbs as well as pronouns. To reduce
noise, we decided to keep only those relationships
which occurred at least three times in the respec-
tive corpus.
The English and German data alike are then rep-
resented as a bipartite graph whose nodes divide
into two sets, verbs and nouns, and whose edges
are the V-O relationships which connect verbs to
nouns (cf. Figure 1). The edges of the graph are
weighted by frequency of occurrence.
We ?prune? both the English and German graph
by recursively removing all leaf nodes (nodes with
a single neighbor). As these correspond to words
which appear only in a single relationship, there is
only limited evidence of their meaning.
After pruning, there are 4,926 nodes (3,365
nouns, 1,561 verbs) and 43,762 links in the En-
glish, and 3,074 nodes (2,207 nouns, 867 verbs)
and 15,386 links in the German word graph.
5 Evaluation experiment
The aim of our evaluation experiment is to test
the extended SimRank algorithm for its ability to
identify novel word translations given the English
and German word graph of the previous section
and an English-German seed lexicon. We use the
dict.cc English-German dictionary 1.
Our evaluation strategy is as follows. We se-
lect a set of test words at random from among the
words listed in the dictionary, and remove their en-
tries from the dictionary. We run six iterations of
SimRank using the remaining dictionary entries
as the seed translations (the known node equiv-
alences), and record the similarities of each test
word to its known translations. As in the original
SimRank paper, c is set to 0.8.
We include both English and German test words
and let them vary in frequency: high- (> 100),
1http://www.dict.cc/ (May 5th 2008)
mid- (> 20 and ? 100), and low- (? 20) fre-
quent as well as word class (noun, verb). Thus, we
obtain 12 categories of test words (summarized in
Table 1), each of which is filled with 50 randomly
selected words, giving a total of 600 test words.
SimRank returns a matrix of English-German
node-node similarities. Given a test word, we ex-
tract its row from the similarity matrix and sort the
corresponding words by their similarities to the
test word. We then scan this sorted list of words
and their similarities for the test word?s reference
translations (those listed in the original dictionary)
and record their positions (i.e. ranks) in this list.
We then replace absolute ranks with relative ranks
by dividing by the total number of candidate trans-
lations.
6 Results
Table 1 lists the mean relative rank of the reference
translations for each of the test categories. The
values of around 0.2-0.3 clearly indicate that our
approach ranks the reference translations much
higher than a random process would.
Relative rank
Fr
eq
ue
nc
y
0.0 0.2 0.4 0.6 0.8 1.0
0
5
15
25
Figure 2: Distribution of the relative ranks of the
reference translations in the English-High-N test
set.
Exemplary of all test sets, Figure 2 shows the
distribution of the relative ranks of the reference
translations for the test words in English-High-N.
The bulk of the distribution lies below 0.3, i.e. in
the top 30% of the candidate list.
In order to give the reader an idea of the results,
we present some examples of test words and their
94
Test word Top 10 predicted translations Ranks
sanction Ausgangssperre Wirtschaftssanktion
Ausnahmezustand Embargo Moratorium
Sanktion Todesurteil Geldstrafe Bu?geld
Anmeldung
Sanktion(6)
Ma?nahme(1407)
delay anfechten revidieren zuru?ckstellen
fu?llen verku?nden quittieren vertagen
verschieben aufheben respektieren
verzo?gern(78)
aufhalten(712)
Kosten hallmark trouser blouse makup uniform
armour robe testimony witness jumper
cost(285)
o?ffnen unlock lock usher step peer shut guard
hurry slam close
open(12)
undo(481)
Table 2: Some examples of test words, their pre-
dicted translations, and the ranks of their true
translations.
predicted translations in Table 2.
Most of the 10 top-ranked candidate transla-
tions of sanction are hyponyms of the correct
translations. This is mainly due to insufficient
noun compound analysis. Both the English and
German nouns in our graph model are single
words. Whereas the English nouns consist only of
head nouns, the German nouns include many com-
pounds (as they are written without spaces), and
thus tend to be more specific.
Some of the top candidate translations of de-
lay are correct (verschieben) or at least acceptable
(vertagen), but do not count as such as they are
missing in the gold standard dictionary.
The mistranslation of the German noun Kosten
is due to semantic ambiguity. Kosten co-occurs of-
ten with the verb tragen as in to bear costs. The
verb tragen however is ambiguous and may as
well be translated as to wear which is strongly as-
sociated with clothes.
We find several antonyms of o?ffnen among its
top predicted translations. Verb-object relation-
ships alone do not suffice to distinguish synonyms
from antonyms. Similarly, it is extremely difficult
to differentiate between the members of closed
categories (e.g. the days of the week, months of
the year, mass and time units) using only syntactic
relationships.
7 Conclusions and Future Research
The matrix formulation of the SimRank algorithm
given in this paper allows an implementation using
efficient off-the-shelf software libraries for matrix
computation.
We presented an extension of the SimRank
algorithm to edge-weighted and edge-labeled
graphs. We further generalized the SimRank equa-
tions to permit the comparison of nodes from two
different graphs, and proposed an application to
bilingual lexicon induction.
Our system is not yet accurate enough to be
used for actual compilation of translation dictio-
naries. We further need to address the problem of
data sparsity. In particular, we need to remove the
bias towards low-degree words whose similarities
to other words are unduly high.
In order to solve the problem of ambiguity, we
intend to apply SimRank to the incidence repre-
sentation of the word graphs, which is constructed
by putting a node on each link. The proposed al-
gorithm will then naturally return similarities be-
tween the more sense-discriminative links (syn-
tactic relationships) in addition to similarities be-
tween the often ambiguous nodes (isolated words).
References
M. Diab and S. Finch. 2000. A statistical word-
level translation model for comparable corpora. In
In Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
G. Jeh and J. Widom. 2002. Simrank: A measure of
structural-context similarity. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 538?543.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of the ACL-02 Workshop on Unsupervised Lexical
Acquisition, pages 9?16.
P. Gamallo Otero and J. Ramon Pichel Campos. 2005.
An approach to acquire word translations from non-
parallel texts. In EPIA, pages 600?610.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 519?526.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING ?04: Proceedings of the 20th International
Conference on Computational Linguistics, page 162.
Sabine Schulte im Walde. 1998. Automatic Se-
mantic Classification of Verbs According to Their
Alternation Behaviour. Master?s thesis, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
95
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 9?17,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On Proper Unit Selection in Active Learning:
Co-Selection Effects for Named Entity Recognition
Katrin Tomanek1? Florian Laws2? Udo Hahn1 Hinrich Schu?tze2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
2Institute for Natural Language Processing, Universita?t Stuttgart, Germany
{fl|hs999}@ifnlp.org
Abstract
Active learning is an effective method for cre-
ating training sets cheaply, but it is a biased
sampling process and fails to explore large
regions of the instance space in many appli-
cations. This can result in a missed cluster
effect, which signficantly lowers recall and
slows down learning for infrequent classes.
We show that missed clusters can be avoided
in sequence classification tasks by using sen-
tences as natural multi-instance units for label-
ing. Co-selection of other tokens within sen-
tences provides an implicit exploratory com-
ponent since we found for the task of named
entity recognition on two corpora that en-
tity classes co-occur with sufficient frequency
within sentences.
1 Introduction
Active learning (AL) has been shown to be an effec-
tive approach to reduce the amount of data needed
to train an accurate statistical classifier. AL selects
highly informative examples from a pool of unla-
beled data and prompts a human annotator for the
labels of these examples. The newly labeled exam-
ples are added to a training set used to build a statis-
tical classifier. This classifier is in turn used to assess
the informativeness of further examples. Thus, a
select-label-retrain loop is formed that quickly se-
lects hard to classify examples, honing in on the de-
cision boundary (Cohn et al, 1996).
A fundamental characteristic of AL is the fact that
it constitutes a biased sampling process. This is so
? Both authors contributed equally to this work.
by design, but the bias can have an undesirable con-
sequence: partial coverage of the instance space. As
a result, classes or clusters within classes may be
completely missed, resulting in low recall or slow
learning progress. This has been called the missed
cluster effect (Schu?tze et al, 2006). While AL has
been studied for a range of NLP tasks, the missed
cluster problem has hardly been addressed.
This paper studies the missed class effect, a spe-
cial case of the missed cluster effect where complete
classes are overlooked by an active learner. The
missed class effect is the result of insufficient ex-
ploration before or during a mainly exploitative AL
process. In AL approaches where exploration is only
addressed by an initial seed set, poor seed set con-
struction gives rise to the missed class effect.
We focus on the missed class effect in the con-
text of a common NLP task: named entity recogni-
tion (NER). We show that for this task the missed
class effect is avoided by increasing the sampling
granularity from single-instance units (i.e., tokens)
to multi-instance units (i.e., sentences). For AL ap-
proaches to NER, sentence selection recovers better
from unfavorable seed sets than token selection due
to what we call the co-selection effect. Under this
effect, a non-targeted entity class co-occurs in sen-
tences that were originally selected because of un-
certainty on tokens of a different entity class.
The rest of the paper is structured as follows: Sec-
tion 2 introduces the missed class effect in detail.
Experiments which demonstrate the co-selection ef-
fect achieved by sentence selection for NER are de-
scribed in Section 3 and their results presented in
Section 4. We draw conclusions in Section 5.
9
2 The Missed Class Effect
This section first describes the missed class ef-
fect. Then, we discuss several factors influencing
this effect, focusing on co-selection, a natural phe-
nomenon in common NLP applications of AL.
2.1 Sampling bias and misguided AL
The distribution of the labeled data points obtained
with an active learner deviates from the true data
distribution. While this sampling bias is intended
and accounts for the effectiveness of AL, it also
poses challenges as it leads to classifiers that per-
form poorly in some regions, or clusters, of the ex-
ample space. In the literature, this phenomenon has
been described as the missed cluster effect (Schu?tze
et al, 2006; Dasgupta and Hsu, 2008)
In this context, we must distinguish between ex-
ploration and exploitation. By design, AL is a
highly exploitative strategy: regions around decision
boundaries are inspected thoroughly so that decision
boundaries are learned well, but regions far from any
of the initial decision boundaries remain unexplored.
An exploitative sampling approach thus has to be
combined with some kind of exploratory strategy to
make sure the example space is adequately covered.
A common approach is to start an AL process with
an initial seed set that accounts for the exploration
step. However, a seed set which is not represen-
tative of the example space may completely mis-
guide AL ? at least when no other explorative tech-
niques are applied as a remedy. While approaches
to balancing exploration and exploitation (Baram et
al., 2003; Dasgupta and Hsu, 2008; Cebron and
Berthold, 2009) have been discussed, we here fo-
cus on a ?pure? AL scenario where exploration takes
only place in the beginning by a seed set. In sum-
mary, the missed clusters are the result of a sce-
nario where poor exploration is combined with ex-
clusively exploitative sampling.
Why is AL an exploitative sampling strategy? AL
selects data points based on the confidence of the ac-
tive learner. Assume an initial seed set that does not
contain examples of a specific cluster. This leads to
an initial active learner that is mistakenly overconfi-
dent about the class membership of instances in this
missed cluster. Far away from the decision bound-
ary, the active learner assumes a high confidence for
A
B
C
(a)
A
B
C
(b)
Figure 1: Illustration of the missed cluster effect in a 1-
d scenario. Shaded points are contained in the seed set,
vertical lines are final decision boundaries, and dashed
rectangles mark the explored regions
all instances in that cluster, even if they are in fact
misclassified. Consequently, the active learner will
fail to select these instances for long until some re-
direction impulse is received (if at all).
To give an example, let us consider a simple 1-
d toy scenario with examples from three clusters A,
B, and C as shown in Figure 1. In scenario (a), AL
is started from a seed set including one example of
clusters A and B only. In subsequent rounds, AL
will select examples in these clusters only (shown as
the dashed box in the figure). Examples in cluster
C are ignored as they are far from the initial deci-
sion boundary. Eventually, a decision boundary is
fixed as shown by the vertical line which indicates
that this AL process has completely overlooked ex-
amples from cluster C.
Assuming that the examples fall in two classes
X1 = {A ? C} and X2 = {B} the learned clas-
sifier has low recall for class X1 and relatively low
precision for class X2 as it erroneously assigns ex-
amples of cluster C to class X2. In a related sce-
nario with three classes X1 = {A}, X2 = {B}, and
X3 = {C} this would even mean that the classifier
is not at all aware about the third class resulting in
the missed class problem.
A more representative seed set circumvents this
problem. Given a seed set including one example
of each cluster, AL might find a second decision
boundary1 between clusters B and C because it is
now aware of examples from C. Figure 1(b) shows
a possible result of AL on this seed set.
The missed cluster effect can be understood as
the generalized problem. A special case of it is the
1Assuming a classifier that can learn several boundaries.
10
missed class effect as shown in the previous exam-
ple. In general, it has the same causes (insufficient
exploration and misguided exploitation), but is eas-
ier to test. Often we know (at least the number of) all
classes under scrutiny, while we usually cannot as-
sume all clusters in the feature space to be known. In
this paper, we focus on the missed class effect, i.e.,
scenarios where classes are overlooked by a mis-
guided AL process resulting in a slow (active) learn-
ing progress.
2.2 Factors influcencing the missed class effect
AL in a practical scenario is subject to several fac-
tors which mitigate or intensify the missed class ef-
fect described before. In the following, we describe
three such factors, with a special focus on the co-
selection effect, which we claim to significantly mit-
igate the missed class effect in a specific type of NLP
tasks, sequence learning problems such as NER or
POS tagging.
Class imbalance Many studies on AL for NLP
tasks assume that AL is started from a randomly
drawn seed set. Such a seed set can be problem-
atic when the class distribution in the data is highly
skewed. In this case, ?rare? classes might not be
represented in the seed set, increasing the chance to
completely miss out such a class using AL. When
classes are relatively frequent, an active learner ?
even when started from an unfavorable seed set ?
might still mistake an example of one class for an
uncertain example of a different class and conse-
quently select it. Thereby, it can acquire information
about the former class ?by accident? leading to sud-
den and rapid discovery of the newly-found class.
However, in the case of extreme class imbalance this
is very unlikely. Severe class imbalance intensifies
the missed cluster effect.
Similarity of considered classes If, e.g., two of
the classes to be learned, say Xi and Xj , are harder
to discriminate than others, or if the data contains
lots of noise, an active learner is more likely to select
some instances of Xi if at least its ?similar? coun-
terpart Xj was represented in the seed set. Hence,
it may mistake the instances of Xi and Xj before it
has acquired enough information to discriminate be-
tween them. So, under certain situations similarity
of classes can mitigate the missed class effect.
The co-selection effect Many NLP tasks are se-
quence learning problems including, e.g., POS tag-
ging, and named entity recognition. Sequences are
consecutive text tokens constituting linguistically
plausible chunks, e.g., sentences. Algorithms for se-
quence learning obviously work on sequence data,
so respective AL approaches need to select complete
sequences instead of single text tokens (Settles and
Craven, 2008). Furthermore, sentence selection has
been preferred over token selection in other works
with the argument that the manual annotation of sin-
gle, possibly isolated tokens is almost impossible or
at least extremely time-consuming (Ringger et al,
2007; Tomanek et al, 2007).
Within such sequences, instances of different
classes often co-occur. Thus, an active learner that
selects uncertain examples of one class gets exam-
ples of a second class as an unintended, yet pos-
itive side effect. We call this the co-selection ef-
fect. As a result, AL for sequence labeling is not
?pure? exploitative AL, but implicitly comprises an
exploratory aspect which can substantially reduce
the missed class problem. In scenarios where we
cannot hope for such a co-selection, we are much
more likely to have decreased AL performance due
to missed clusters or classes.
3 Experiments
We ran several experiments to investigate how the
sampling granularity, i.e. the size of the selection
unit, influences the missed class effect. AL based
on token selection (T-AL) is compared to AL based
on sentence selection (S-AL). Although our experi-
ments are certainly also subject to the other factors
mitigating the missed class effect (e.g. similarity of
classes), the main focus of the experiments is on the
co-selection effect that we expected to observe in
S-AL. Several scenarios of initial exploration were
simulated by seed sets of different characteristics.
The experiments were run on synthetic and real data
in the context of named entity recognition (NER).
3.1 Classifiers and active learning setup
The active learning approach used for both S-AL
and T-AL is based on uncertainty sampling (Lewis
and Gale, 1994) with the margin metric (Schein and
Ungar, 2007) as uncertainty measure. Let c and c?
11
be the two most likely classes predicted for token
xj with p?c,xj and p?c?,xj being the associated class
probabilities. The per-token margin is calculated as
M = |p?c,xj ? p?c?,xj |.
For T-AL, the sampling granularity is the token,
while in S-AL, complete sentences are selected. For
S-AL, the margins of all tokens in a sentence are
averaged and the aggregate margin is used to select
sentences. We chose this uncertainty measure for S-
AL for better comparison with T-AL. In either case,
examples (tokens or sentences) with a small margin
are preferred for selection. In every iteration, a batch
of examples is selected: 20 sentences for S-AL, 200
tokens for T-AL.
Bayesian logistic regression as implemented in
the BBR classification package (Genkin et al, 2007)
with out-of-the-box parameter settings was used as
base learner for T-AL. For S-AL, a linear-chain
Conditional Random Field (Lafferty et al, 2001) is
employed as implemented in MALLET (McCallum,
2002). Both base learners employ standard features
for NER including the lexical token itself, various
orthographic features such as capitalization, the oc-
currence of special characters like hyphens, and con-
text information in terms of features of neighboring
tokens to the left and right of the current token.
3.2 Data sets
We used three data sets in our experiments. Two of
them (ACE and PBIO) are standard data sets. The
third (SYN) is a synthetic set constructed to have
specific characteristics. For simplicity, we consider
only scenarios with two entity classes, a majority
class (MAJ) and a minority class (MIN). We dis-
carded all other entity annotations originally con-
tained in the corpus assigning the OUTSIDE class.2
The first data set (PBIO) is based on the annota-
tions of the PENNBIOIE corpus for biomedical en-
tity extraction (Kulick et al, 2004). As PENNBIOIE
makes fine-grained and subtle distinctions between
various subtypes of classes irrelevant for this study,
we combined several of the original classes into two
entity classes: The majority class consists of the
three original classes ?gene-protein?, ?gene-generic?,
and ?gene-rna?. The minority class consists of
the original and similar classes ?variation-type? and
2The OUTSIDE class marks that a token is not part of an
named entity.
?variation-event?. All other entity labels were re-
placed by the OUTSIDE class.
The second data set (ACE) is based on the
newswire section of the ACE 2005 Multilingual
Training Corpus (Walker et al, 2006). We chose
the ?person? class as majority class and the ?organi-
zation? class as the minority class. Again, all other
classes are mapped to OUTSIDE.
The synthetic data set (SYN) was constructed by
combining the sentences from the original ACE and
PENNBIOIE corpora. The ?person? class consti-
tutes the minority class, the very similar classes
?malignancy? and ?malignancy-type? were merged
to form the majority class. All other class la-
bels were set to OUTSIDE. SYN?s construction
was motivated by the following characteristics of
the new data set which would make the appear-
ance of the missed class effect very likely for
insufficient exploration scenarios:
(i) absence of inner-sentence entity class correlation
to ensure that sentences contain either mentions of
only a single entity class or no mentions at all.
(ii) marked entity class imbalance between the ma-
jority and minority classes
(iii) dissimilar surface patterns of entity mentions of
the two entity classes with the rationale that class
similarity will be low.
Table 1 summarizes characteristics of the data
sets. While SYN exhibits high imbalance (e.g., 1:9.4
on the token level), PBIO and ACE are moderately
skewed. In PBIO, the number of sentences contain-
ing any entity mention is relatively high compared
to ACE or SYN. For our experiments, the corpora
were randomly split in a pool for AL and a test set
for performance evaluation.
Inner-sentence entity class co-occurrence We
have described co-selection as a potential mitigat-
ing factor for the missed class effect in Section 2.
For this effect to occur, there must be some corre-
lation between the occurrence of entity mentions of
the MAJ class with those from MIN.
Table 2 shows correlation statistics based on the
?2 measure. We found strong correlation in all three
corpora3: For ACE and PBIO, the correlation is pos-
itive; for SYN it is negative so when a sentence in
SYN contains a majority class entity mention, it is
3All correlations are statistically significant (p < 0.01).
12
PBIO ACE SYN
sentences (all) 11,164 2,642 13,804
sentences (MAJ) 7,075 767 5,667
sentences (MIN) 2,156 974 974
MIN-MAJ ratio 1 : 3.3 1 : 1.3 1 : 5.8
tokens (all) 277,053 66,752 343,773
tokens (MAJ) 17,928 2,008 18,959
tokens (MIN) 4,079 1,822 2,008
MIN-MAJ ratio 1 : 4.4 1 : 1.1 1 : 9.4
Table 1: Characteristics of the data sets; ?sentences
(MAJ)?, e.g., specifies the number of sentences contain-
ing mentions of the majority class.
PBIO ACE SYN
?2 132.34 6.07 727
P (MIN |MAJ) 0.26 0.31 0.0
Table 2: Co-occurrence of entity classes in sentences
highly unlikely that it also contains a minority entity.
In fact, it is impossible by construction of the data
set. Further, this table shows the probability that a
sentence containing the majority class also contains
the minority class. As expected, this is exactly 0 for
SYN, but significantly above 0 for PBIO and ACE.
3.3 Seed sets
Selection of an appropriate seed set for the start of an
AL process is important to the success of AL. This is
especially relevant in the case of imbalanced classes
because a typically small random sample will pos-
sibly not contain any example of the rare class. We
constructed different types of seed sets (whose nam-
ing intentionally reflects the use of the entity classes
from Section 3.2) to simulate different scenarios of
ill-managed initial exploration. All seed sets have
a size of 20 sentences. The RANDOM set was ran-
domly sampled, the MAJ set is made of sentences
containing at least one majority class entity, but no
minority class entity. Accordingly, MIN is densely
populated with minority entities. Finally, OUTSIDE
contains only sentences without entity mentions.
One could think of the OUTSIDE and MAJ seed
sets of cases where a random seed set selection has
unluckily produced an especially bad seed set. MIN
serves to demonstrate the opposite case. For each
type of seed set, we sampled ten independent ver-
sions to calculate averages over several AL runs.
3.4 Cost measure
The success of AL is usually measured as reduc-
tion of annotation effort according to some cost mea-
sure. Traditionally, the most common cost measure
considers a unit cost per annotated token, which fa-
vors AL systems that select individual tokens. In
a real annotation setting, however, it is unnatural,
and therefore hard for humans to annotate single,
possibly isolated tokens, leading to bad annotation
quality (Hachey et al, 2005; Ringger et al, 2007).
When providing context, the question arises whether
the annotator can label several tokens present in the
context (e.g., an entire multi-token entity or even
the whole sentence) at little more cost than anno-
tating a single token. Thus, assigning a linear cost
of n to a sentence where n is the sentence?s length
in tokens seems to unfairly disadvantage sentence-
selection AL setups.
However, more work is needed to find a more re-
alistic cost measure. At present there is no other
generally accepted cost measure than unit cost per
token, so we report costs using the token measure.
4 Results
This section presents the results of our experiments
on the missed class effect in two different AL
scenarios, i.e., sentence selection (S-AL) and to-
ken selection (T-AL). The AL runs were stopped
when convergence on the minority class F-score was
achieved. This was done because early AL iterations
before the convergence point are most important and
representative for a real-life scenario where the pool
is extremely large, so that absolute convergence of
the classifier?s performance will never be reached.
The learning curves in Figures 2, 3, and 4 reveal
general characteristics of S-AL compared to T-AL.
For S-AL, the number of tokens on the x-axis is the
total number of tokens in the sentences labeled so
far. While S-AL generally yields higher F-scores, T-
AL converges much earlier when counted in terms
of tokens. The reason for this is that T-AL can se-
lect uncertain data more specifically. In contrast, S-
AL also selects tokens that the classifier can already
classify reliably ? these tokens are selected because
they co-occur in a sentence that also contains an un-
certain token. Whether T-AL is really more efficient
clearly depends on the cost-metric applied (cf. Sec-
13
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with OUTSIDE seed
0 5000 10000 150000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with OUTSIDE seed
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 2: Results on SYN corpus for token selection (a,b,c) and sentence selection (d,e,f)
tion 3.4). Since the focus of this paper is on compar-
ing the missed class effect in a sentence and a token
selection AL setting (T-AL and S-AL) we apply the
straight-forward token measure.
4.1 The pathological case
Figure 2 shows results on the SYN corpus for T-AL
(upper row) and S-AL (lower row). Figures 2(a)
and 2(d) show the minority and majority class learn-
ing curves for a single run starting from the OUT-
SIDE seed set, which was particularly problematic
on SYN. (We show single runs to give a better pic-
ture of what happens during the selection process.)
The figures show that for both AL scenarios, the
OUTSIDE seed set caused the active learner to focus
exclusively on the majority class and to completely
ignore the minority class for many AL iterations (al-
most 30,000 tokens for S-AL and over 4,000 tokens
for T-AL). Had we stopped the AL process before
this turning point, the classifier?s performance on
the majority entity class would have been reason-
ably high while the minority class would not have
been learned at all ? which is precisely the defini-
tion of an (initially) missed class.
Figures 2(b) and 2(e) show the corresponding
mean margin plots of these AL runs, indicating the
confidence of the classifier on each class. The mean
margin is calculated as the average margin over to-
kens in the remaining pool, separately for each true
class label.4 As expected, the active learner is over-
confident but wrong on instances of the minority
class (assigning them to the OUTSIDE class, we
assume). Only after some time, margin scores on
minority class tokens start decreasing. This hap-
pens because from time to time minority class ex-
amples are mistakenly considered as majority class
examples with low confidence and thus selected by
accident. Lowered minority class confidence then
causes the selection of further minority class exam-
ples, resulting in a turning point with a steep slope
of the minority class learning curve.
Consequences of seed set selection We compare
the minority class learning curves for all types of
4Note that in a real, non-simulation active learning task, the
true class labels would be unknown.
14
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with MAJ seed
0 5000 10000 15000 200000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with MAJ seed
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 3: Results on PBIO corpus for token selection (a,b,c) and sentence selection (d,e,f)
seed sets and for random selection (cf. Figures 2(c)
and 2(f)), now averaged over 10 runs. On S-AL all
but the MIN seed set were inferior to random selec-
tion. Even the commonly used random seed set se-
lection is problematic because the minority class is
so rare that there are random seed sets without any
example of the minority class.
On T-AL, all seed sets are better than random se-
lection. This, however, is because random selec-
tion is an extremely weak baseline for T-AL due to
the token distribution (cf. Table 1). Still, the RAN-
DOM, MAJ, and OUTSIDE seed sets are signifi-
cantly worse than a seed set which covers the minor-
ity class well. Note that the majority class learning
curves are relatively invariant against different seed
sets. The minority class seed set does have some
negative impact on initial learning progress on the
majority class (not shown here), but the impact is
rather small. Because of the higher frequency of
the majority class, the classifier soon finds major-
ity class examples to compensate for the seed set by
chance or class similarity.
4.2 Missed class effect mitigated by co-selection
Results on PBIO corpus On the PBIO corpus,
where minority and majority class entity mentions
naturally co-occur on the sentence level, we get
a different picture. Figure 3 shows the learning
(3(a), 3(d)) and mean margin (3(b), 3(e)) curves for
the MAJ seed set. T-AL still exhibits the missed
class effect on this seed set. The minority class
learning curve again has a delayed slope and high
mean margin scores of minority tokens at the be-
ginning, resulting in insufficient selection and slow
learning. S-AL, on the other hand, does not re-
ally suffer from the missed class effect: minor-
ity class entity mentions are co-selected in sen-
tences which were chosen due to uncertainty on
majority class tokens. Minority class mean mar-
gin scores quickly fall, reinforcing selection for mi-
nority class entities. Learning curves for minority
and majority classes run approximately in parallel.
Figure 3(f) shows that all seed sets perform quite
similar for S-AL. MIN unsurprisingly is a bit better.
With the other seed sets, S-AL performance is com-
15
0 2000 4000 6000 80000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(a) T-AL
0 5000 15000 250000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(b) S-AL
Figure 4: Minority class learning curves for all seeds on
ACE averaged over 10 runs
parable to random selection. On the PBIO corpus,
random selection is a strong baseline as almost every
sentence contains an entity mention ? which is not
the case for SYN and ACE (cf. Table 1). As there is
no co-selection effect for T-AL, the MAJ and OUT-
SIDE seed sets also here are subject to the missed
class problem (Figure 3(c)), although not as severely
as on the SYN corpus.
Results on ACE corpus Figure 4 shows learning
curves averaged over 10 runs on ACE. Overall, the
missed class effect is less pronounced on ACE com-
pared to PBIO. Still, co-selection avoids a good por-
tion of the missed class effect on S-AL ? all seed
sets yield results much better than random selection
right from the beginning.
On T-AL, the OUTSIDE seed set has a marked
negative effect. However, while different seed
sets still have visible differences in learning perfor-
mance, the magnitude of the effect is smaller than
on PBIO. It is difficult to find the exact reasons
in a non-synthetic, natural language corpus where a
lot of different effects are intermingled. One might
assume higher class similarity between the major-
ity (?persons?) and the minority (?organizations?)
classes on the ACE corpus than, e.g., on the PBIO
corpus. Moreover, there is hardly any imbalance
in frequency between the two entity classes on the
ACE corpus. We briefly discussed such influencing
factors possibly mitigating the missed class effect in
Section 2.2.
4.3 Discussion
To summarize, on a synthetic corpus (SYN) the
missed class effect can be well studied in both
AL scenarios, i.e., S-AL and T-AL. Moving from
a relatively controlled, synthetic corpus (extreme
class imbalance, no inner-sentence co-occurrence
between entity classes, quite different entity classes)
to more realistic corpora, effects generally mix a bit
due to different degrees of class imbalance and prob-
ably higher similarity between entity classes.
Our experiments unveil that co-selection in S-AL
effectively helps avoid dysfunctional classifiers that
insufficiently explore the instance space due to a
disadvantageous seed set. In contrast, AL based
on token-selection (T-AL) cannot recover from in-
sufficient exploration as easy as AL with sentence-
selection and is thus more sensitive to the missed
class effect.
5 Conclusion
We have shown that insufficient exploration in the
initial stages of active learning gives rise to regions
of the sample space that contain missed classes that
are incorrectly classified. This results in low clas-
sification performance and slow learning progress.
Comparing two sampling granularities, tokens vs.
sentences, we found that the missed class effect is
more severe when isolated tokens instead of sen-
tences are selected for labeling.
The missed class problem in sequence classifica-
tion tasks can be avoided using sentences as natural
multi-instance units for selection and labeling. Us-
ing multi-instance units, co-selection of other tokens
within sentences provides an implicit exploratory
component. This solution is effective if classes co-
occur sufficiently within sentences which is the case
for many real-life entity recognition tasks.
While other work has proposed sentence selection
in AL for sequence labeling as a means to ease and
speed up annotation, we have gathered here addi-
tional motivation from the perspective of robustness
of learning. Future work will compare the beneficial
effect introduced by co-selection with other forms of
exploration-enabled active learning.
Acknowledgements
The first and the third author were funded by the
German Ministry of Education and Research within
the StemNet project (01DS001A-C) and by the EC
within the BOOTStrep project (FP6-028099).
16
References
Yoram Baram, Ran El-Yaniv, and Kobi Luz. 2003. On-
line choice of active learning algorithms. In ICML
?03: Proceedings of the 20th International Conference
on Machine Learning, pages 19?26.
Nicolas Cebron and Michael R. Berthold. 2009. Active
learning for object classification: From exploration to
exploitation. Data Mining and Knowledge Discovery,
18(2):283?299.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
sampling for active learning. In ICML ?08: Proceed-
ings of the 25th International Conference on Machine
Learning, pages 208?215.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49(3):291?304.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ?05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan T. McDonald, Martha S. Palmer, and Andrew Ian
Schein. 2004. Integrated annotation for biomedical
information extraction. In Proceedings of the HLT-
NAACL 2004 Workshop ?Linking Biological Litera-
ture, Ontologies and Databases: Tools for Users?,
pages 61?68.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ?01: Proceedings of the 18th International
Conference on Machine Learning, pages 282?289.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101?108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235?265.
Hinrich Schu?tze, Emre Velipasaoglu, and Jan Pedersen.
2006. Performance thresholding in practical text clas-
sification. In CIKM ?06: Proceedings of the 15th ACM
International Conference on Information and Knowl-
edge Management, pages 662?671.
Burr Settles and Mark Craven. 2008. An analysis of ac-
tive learning strategies for sequence labeling tasks. In
EMNLP ?08: Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 1070?1079.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486?495.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
17
Coling 2010: Poster Volume, pages 614?622,
Beijing, August 2010
A Linguistically Grounded Graph Model for Bilingual Lexicon
Extraction
Florian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,
Ulrich Heid, Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn,michells,dorowbe}@ims.uni-stuttgart.de
Abstract
We present a new method, based on
graph theory, for bilingual lexicon ex-
traction without relying on resources with
limited availability like parallel corpora.
The graphs we use represent linguis-
tic relations between words such as ad-
jectival modification. We experiment
with a number of ways of combining
different linguistic relations and present
a novel method, multi-edge extraction
(MEE), that is both modular and scalable.
We evaluate MEE on adjectives, verbs
and nouns and show that it is superior
to cooccurrence-based extraction (which
does not use linguistic analysis). Finally,
we publish a reproducible baseline to es-
tablish an evaluation benchmark for bilin-
gual lexicon extraction.
1 Introduction
Machine-readable translation dictionaries are an
important resource for bilingual tasks like ma-
chine translation and cross-language information
retrieval. A common approach to obtaining bilin-
gual translation dictionaries is bilingual lexicon
extraction from corpora. Most work has used
parallel text for this task. However, parallel cor-
pora are only available for few language pairs and
for a small selection of domains (e.g., politics).
For other language pairs and domains, monolin-
gual comparable corpora and monolingual lan-
guage processing tools may be more easily avail-
able. This has prompted researchers to investigate
bilingual lexicon extraction based on monolingual
corpora (see Section 2) .
In this paper, we present a new graph-theoretic
method for bilingual lexicon extraction. Two
monolingual graphs are constructed based on syn-
tactic analysis, with words as nodes and relations
(such as adjectival modification) as edges. Each
relation acts as a similarity source for the node
types involved. All available similarity sources
interact to produce one final similarity value for
each pair of nodes. Using a seed lexicon, nodes
from the two graphs can be compared to find a
translation.
Our main contributions in this paper are: (i) we
present a new method, based on graph theory,
for bilingual lexicon extraction without relying
on resources with limited availability like paral-
lel corpora; (ii) we show that with this graph-
theoretic framework, information obtained by lin-
guistic analysis is superior to cooccurrence data
obtained without linguistic analysis; (iii) we ex-
periment with a number of ways of combining dif-
ferent linguistic relations in extraction and present
a novel method, multi-edge extraction, which is
both modular and scalable; (iv) progress in bilin-
gual lexicon extraction has been hampered by the
lack of a common benchmark; we therefore pub-
lish a benchmark and the performance of MEE as
a baseline for future research.
The paper discusses related work in Section 2.
We then describe our translation model (Sec-
tion 3) and multi-edge extraction (Section 4). The
benchmark we publish as part of this paper is de-
scribed in Section 5. Section 6 presents our ex-
perimental results and Section 7 analyzes and dis-
cusses them. Section 8 summarizes.
2 Related Work
Rapp (1999) uses word cooccurrence in a vector
space model for bilingual lexicon extraction. De-
tails are given in Section 5.
Fung and Yee (1998) also use a vector space
approach, but use TF/IDF values in the vector
components and experiment with different vec-
tor similarity measures for ranking the translation
candidates. Koehn and Knight (2002) combine
614
a vector-space approach with other clues such as
orthographic similarity and frequency. They re-
port an accuracy of .39 on the 1000 most frequent
English-German noun translation pairs.
Garera et al (2009) use a vector space model
with dependency links as dimensions instead of
cooccurring words. They report outperforming
a cooccurrence vector model by 16 percentage
points accuracy on English-Spanish.
Haghighi et al (2008) use a probabilistic model
over word feature vectors containing cooccur-
rence and orthographic features. They then use
canonical correlation analysis to find matchings
between words in a common latent space. They
evaluate on multiple languages and report high
precision even without a seed lexicon.
Most previous work has used vector spaces and
(except for Garera et al (2009)) cooccurrence
data. Our approach uses linguistic relations like
subcategorization, modification and coordination
in a graph-based model. Further, we evaluate our
approach on different parts of speech, whereas
some previous work only evaluates on nouns.
3 Translation Model
Our model has two components: (i) a graph repre-
senting words and the relationships between them
and (ii) a measure of similarity between words
based on these relationships. Translation is re-
garded as cross-lingual word similarity. We rank
words according to their similarity and choose the
top word as the translation.
We employ undirected graphs with typed nodes
and edges. Node types represent parts of speech
(POS); edge types represent different kinds of re-
lations. We use a modified version of SimRank
(Jeh and Widom, 2002) as a similarity measure
for our experiments (see Section 4 for details).
SimRank is based on the idea that two nodes
are similar if their neighbors are similar. We ap-
ply this notion of similarity across two graphs. We
think of two words as translations if they appear
in the same relations with other words that are
translations of each other. Figure 1 illustrates this
idea with verbs and nouns in the direct object rela-
tion. Double lines indicate seed translations, i.e.,
known translations from a dictionary (see Sec-
tion 5). The nodes buy and kaufen have the same
house
magazine
book
thought
buy
read
Haus
Zeitschrift
Buch
Gedanke
kaufen
lesen
Figure 1: Similarity through seed translations
objects in the two languages; one of these (maga-
zine ? Zeitschrift) is a seed translation. This re-
lationship contributes to the similarity of buy ?
kaufen. Furthermore, book and Buch are similar
(because of read ? lesen) and this similarity will
be added to buy ? kaufen in a later iteration. By
repeatedly applying the algorithm, the initial sim-
ilarity introduced by seeds spreads to all nodes.
To incorporate more detailed linguistic infor-
mation, we introduce typed edges in addition to
typed nodes. Each edge type represents a linguis-
tic relation such as verb subcategorization or ad-
jectival modification. By designing a model that
combines multiple edge types, we can compute
the similarity between two words based on mul-
tiple sources of similarity. We superimpose dif-
ferent sets of edges on a fixed set of nodes; a node
is not necessarily part of every relation.
The graph model can accommodate any kind of
nodes and relations. In this paper we use nodes
to represent content words (i.e., non-function
words): adjectives (a), nouns (n) and verbs (v).
We extracted three types of syntactic relations
from a corpus: see Table 1.
Nouns participate in two bipartite relations
(amod, dobj) and one unipartite relation (ncrd).
This means that the computation of noun similar-
ities will benefit from three different sources.
Figure 2 depicts a sample graph with all node
and edge types. For the sake of simplicity, a
monolingual example is shown. There are four
nouns in the sample graph all of which are (i)
modified by the adjectives interesting and polit-
ical and (ii) direct objects of the verbs like and
615
relation entities description example
used in this paper
amod a, n adjectival modification a fast car
dobj v, n object subcategorization drive a car
ncrd n, n noun coordination cars and busses
other possible relations
vsub v, n subject subcategorization a man sleeps
poss n, n possessive the child?s toy
acrd a, a adjective coordination red or blue car
Table 1: Relations used in this paper (top) and
possible extensions (bottom).
dobj
amod
ncrd
verb
adjective
noun
like promote
idea
article book
magazine
interesting political
Figure 2: Graph snippet with typed edges
promote. Based on amod and dobj, the four nouns
are equally similar to each other. However, the
greater similarity of article, book, and magazine
to each other can be deduced from the fact that
these three nouns also occur in the relation ncrd.
We exploit this information in the MEE method.
Data and Preprocessing. Our corpus in this
paper is the Wikipedia. We parse all German
and English articles with BitPar (Schmid, 2004)
to extract verb-argument relations. We extract
adjective-noun modification and noun coordina-
tions with part-of-speech patterns based on a
version of the corpus tagged with TreeTagger
(Schmid, 1994). We use lemmas instead of sur-
face forms. Because we perform the SimRank
matrix multiplications in memory, we need to fil-
ter out rare words and relations; otherwise, run-
ning SimRank to convergence would not be feasi-
ble. For adjective-noun pairs, we apply a filter on
pair frequency (? 3). We process noun pairs by
applying a frequency threshold on words (? 100)
and pairs (? 3). Verb-object pairs (the smallest
data set) were not frequency-filtered. Based on
the resulting frequency counts, we calculate asso-
ciation scores for all relationships using the log-
likelihood measure (Dunning, 1993). For noun
pairs, we discard all pairs with an association
score < 3.84 (significance at ? = .05). For all
three relations, we discard pairs whose observed
frequency was smaller than their expected fre-
quency (Evert, 2004, p. 76). As a last step,
we further reduce noise by removing nodes of de-
gree 1. Key statistics for the resulting graphs are
given in Table 2.
We have found that accuracy of extraction is
poor if unweighted edges are used. Using the
log-likelihood score directly as edge weight gives
too much weight to ?semantically weak? high-
frequency words like put and take. We there-
fore use the logarithms of the log-likelihood score
as edge weights in all SimRank computations re-
ported in this paper.
nodes n a v
de 34,545 10,067 2,828
en 22,257 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,906
en 288,889 686,073 510,351
Table 2: Node and edge statistics
4 SimRank
Our work is based on the SimRank graph similar-
ity algorithm (Jeh and Widom, 2002). In (Dorow
et al, 2009), we proposed a formulation of Sim-
Rank in terms of matrix operations, which can be
applied to (i) weighted graphs and (ii) bilingual
problems. We now briefly review SimRank and
its bilingual extension. For more details we refer
to (Dorow et al, 2009).
The basic idea of SimRank is to consider two
nodes as similar if they have similar neighbor-
hoods. Node similarity scores are recursively
computed from the scores of neighboring nodes:
the similarity Sij of two nodes i and j is computed
616
as the normalized sum of the pairwise similarities
of their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl.
where N(i) and N(j) are the sets of i?s and j?s
neighbors. As the basis of the recursion, Sij is set
to 1 if i and j are identical (self-similarity). The
constant c (0 < c < 1) dampens the contribution
of nodes further away. Following Jeh and Widom
(2002), we use c = 0.8. This calculation is re-
peated until, after a few iterations, the similarity
values converge.
For bilingual problems, we adapt SimRank for
comparison of nodes across two graphs A and B.
In this case, i is a node in A and j is a node in B,
and the recursion basis is changed to S(i, j) = 1 if
i and j are a pair in a predefined set of node-node
equivalences (seed translation pairs).
Sij =
c
|NA(i)| |NB(j)|
?
k?NA(i),l?NB(j)
Skl.
Multi-edge Extraction (MEE) Algorithm To
combine different information sources, corre-
sponding to edges of different types, in one Sim-
Rank computation, we use multi-edge extrac-
tion (MEE), a variant of SimRank (Dorow et al,
2009). It computes an aggregate similarity matrix
after each iteration by taking the average similar-
ity value over all edge types T :
Sij =
c
|T |
?
t?T
1
f(|NA,t(i)|)f(|NB,t(j)|)
?
k?NA,t(i),
l?NB,t(j)
Skl.
f is a normalization function (either f = g,
g(n) = n as before or the normalization discussed
in the next section).
While we have only reviewed the case of un-
weighted graphs, the extended SimRank can also
be applied to weighted graphs. (See (Dorow et
al., 2009) for details.) In what follows, all graph
computations are weighted.
Square Root Normalization Preliminary ex-
periments showed that SimRank gave too much
influence to words with few neighbors. We there-
fore modified the normalization function g(n) =
n. To favor words with more neighbors, we want
f to grow sublinearly with the number of neigh-
bors. On the other hand, it is important that,
even for nodes with a large number of neigh-
bors, the normalization term is not much smaller
than |N(i)|, otherwise the similarity computation
does not converge. We use the function h(n) =?n?
?
maxk(|N(k)|). h grows quickly for small
node degrees, while returning values close to the
linear term for large node degrees. This guaran-
tees that nodes with small degrees have less influ-
ence on final similarity scores. In all experiments
reported in this paper, the matrices A?, B? are nor-
malized with f = h (rather than using the stan-
dard normalization f = g). In one experiment,
accuracy of the top-ranked candidate (acc@1) was
.52 for h and .03 for g, demonstrating that the
standard normalization does not work in our ap-
plication.
Threshold Sieving For larger experiments,
there is a limit to scalability, as the similarity ma-
trix fills up with many small entries, which take up
a large amount of memory. Since these small en-
tries contribute little to the final result, Lizorkin et
al. (2008) proposed threshold sieving: an approxi-
mation of SimRank using less space by deleting
all similarity values that are below a threshold.
The quality of the approximation is set by a pa-
rameter ? that specifies maximum acceptable dif-
ference of threshold-sieved similarity and the ex-
act solution. We adapted this to the matrix formu-
lation by integrating the thresholding step into a
standard sparse matrix multiplication algorithm.
We verified that this approximation yields use-
ful results by comparing the ranks of exact and ap-
proximate solutions. We found that for the high-
ranked words that are of interest in our task, siev-
ing with a suitable threshold does not negatively
affect results.
5 Benchmark Data Set
Rapp?s (1999) original experiment was carried out
on newswire corpora and a proprietary Collins
dictionary. We use the free German (280M to-
kens) and English (850M tokens) Wikipedias as
source and target corpora. Reinhard Rapp has
generously provided us with his 100 word test set
617
n a v
training set .61 .31 .08
TS100 .65 .28 .07
TS1000 .66 .14 .20
Table 3: Percentages of POS in test and training
(TS100) and given us permission to redistribute
it. Additionally, we constructed a larger test set
(TS1000) consisting of the 1000 most frequent
words from the English Wikipedia. Unlike the
noun-only test sets used in other studies, (e.g.,
Koehn and Knight (2002), Haghighi et al (2008)),
TS1000 also contains adjectives and verbs. As
seed translations, we use a subset of the dict.cc
online dictionary. For the creation of the sub-
set we took raw word frequencies from Wikipedia
as a basis. We extracted all verb, noun and ad-
jective translation pairs from the original dictio-
nary and kept the pairs whose components were
among the 5,000 most frequent nouns, the 3,500
most frequent adjectives and the 500 most fre-
quent verbs for each language. These numbers are
based on percentages of the different node types
in the graphs. The resulting dictionary contains
12,630 pairs: 7,767 noun, 3,913 adjective and 950
verb pairs. Table 3 shows the POS composition of
the training set and the two test sets. For experi-
ments evaluated on TS100 (resp. TS1000), the set
of 100 (resp. 1000) English words it contains and
all their German translations are removed from the
seed dictionary.
Baseline. Our baseline is a reimplementation
of the vector-space method of Rapp (1999). Each
word in the source corpus is represented as a word
vector, the dimensions of which are words of seed
translation pairs. The same is done for corpus
words in the target language, using the translated
seed words as dimensions. The value of each di-
mension is determined by association statistics of
word cooccurrence. For a test word, a vector is
constructed in the same way. The labels on the
dimensions are then translated, yielding an input
vector in the target language vector space. We
then find the closest corpus word vector in the tar-
get language vector space using the city block dis-
tance measure. This word is taken as the transla-
tion of the test word.
We went to great lengths to implement Rapp?s
method, but omit the details for reasons of space.
Using the Wikipedia/dict.cc-based data set, we
achieve 50% acc@1 when translating words from
English to German. While this is somewhat lower
than the performance reported by Rapp, we be-
lieve this is due to Wikipedia being more hetero-
geneous and less comparable than news corpora
from identical time periods used by Rapp.
Publication. In conjunction with this paper we
publish the benchmark for bilingual lexicon ex-
traction described. It consists of (i) two Wikipedia
dumps from October 2008 and the linguistic re-
lations extracted from them, (ii) scripts to recre-
ate the training and test sets from the dict.cc
data base, (iii) the TS100 and TS1000 test sets,
and (iv) performance numbers of Rapp?s system
and MEE. These can serve as baselines for fu-
ture work. Note that (ii)?(iv) can be used in-
dependently of (i) ? but in that case the effect
of the corpus on performance would not be con-
trolled. The data and scripts are available at
http://ifnlp.org/wiki/extern/WordGraph
6 Results
In addition to the vector space baseline experi-
ment described above, we conducted experiments
with the SimRank model. Because TS100 only
contains one translation per word, but words can
have more than one valid translation, we manu-
ally extended the test set with other translations,
which we verified using dict.cc and leo.org. We
report the results separately for the original test set
(?strict?) and the extended test set in Table 4. We
also experimented with single-edge models con-
sisting of three separate runs on each relation.
The accuracy columns report the percentage of
test cases where the correct translation was found
among the top 1 (acc@1) or top 10 (acc@10)
candidate words found by the translation mod-
els. Some test words are not present in the data at
all; we count these as 0s when computing acc@1
and acc@10. The acc@10 measure is more use-
ful for indicating topical similarity while acc@1
measures translation accuracy.
MRR is Mean Reciprocal Rank of correct trans-
lations: 1n
?n
i
1
ranki (Voorhees and Tice, 1999).
MRR is a more fine-grained measure than acc@n,
618
TS100, strict TS100, extended TS1000
acc@1 acc@10 MRR acc@1 acc@10 MRR acc@1 acc@10 MRR
baseline .50 .67 .56 .54 .70 .60 .33 .56 .41
single .44 .67 .52 .49 .68 .56 .40? .70? .50
MEE .52 .79? .62 .58 .82? .68 .48? .76? .58
Table 4: Results compared to baseline?
e.g., it will distinguish ranks 2 and 10. All MRR
numbers reported in this paper are consistent with
acc@1/acc@10 and support our conclusions.
The results for acc@1, the measure that most
directly corresponds to utility in lexicon extrac-
tion, show that the SimRank-based models out-
perform the vector space baseline ? only slightly
on TS100, but significantly on TS1000. Using the
various relations separately (single) already yields
a significant improvement compared to the base-
line. Using all relations in the integrated MEE
model further improves accuracy. With an acc@1
score of 0.48, MEE outperforms the baseline by
.15 compared to TS1000. This shows that a com-
bination of several sources of information is very
valuable for finding the correct translation.
MEE outperforms the baseline on TS1000 for
all parts of speech, but performs especially well
compared to the baseline for adjectives and verbs
(see Table 5). It has been suggested that vector
space models perform best for nouns and poorly
for other parts of speech. Our experiments seem to
confirm this. In contrast, MEE exhibits good per-
formance for nouns and adjectives and a marked
improvement for verbs.
On acc@10, MEE is consistently better than the
baseline, on both TS100 and TS1000. All three
differences are statistically significant.
6.1 Relation Comparison
Table 5 compares baseline, single-edge and MEE
accuracy for the three parts of speech covered.
Each single-edge experiment can compute noun
similarity; for adjectives and verbs, only amod,
dobj and MEE can be used.
Performance for nouns varies greatly depend-
ing on the relation used in the model. ncrd per-
?We indicate statistical significance at the ? = 0.05 (?)
and 0.01 level (?) when compared to the baseline. We did
not calculate significance for MRR.
forms best, while dobj shows the worst perfor-
mance. We hypothesize that dobj performs badly
because (i) many verbs are semantically non-
restrictive with respect to their arguments, (e.g.,
use, contain or include) and as a result seman-
tically unrelated nouns become similar because
they share the same verb as a neighbor; (ii) light
verb constructions (e.g., take a walk or give an ac-
count) dilute the extracted relations; and (iii) dobj
is the only relation we extracted with a syntac-
tic parser. The parser was trained on newswire
text, a genre that is very different from Wikipedia.
Hence, parsing is less robust than the relatively
straightforward POS patterns used for the other
relations.
Similarly, many semantically non-restrictive
adjectives such as first and new can modify vir-
tually any noun, diluting the quality of the amod
source. We conjecture that ncrd exhibits the best
performance because there are fewer semantically
non-restrictive nouns than non-restrictive adjec-
tives and verbs.
MEE performance for nouns (.45) is signifi-
cantly better than that of the single-edge models.
The information about nouns that is contained in
the verb-object and adjective-noun data is inte-
grated in the model and helps select better trans-
lations. This, however, is only true for the noun
noun adj verb all
TS100 baseline .55 .43 .29 .50
amod .15 .71 - .30
ncrd .34 - - .22
dobj .02 - .43 .04
MEE .45 .71 .43 .52
TS1000 baseline .42 .26 .18 .33
MEE .53 .55 .27 .48
Table 5: Relation comparison, acc@1
619
source acc@1 acc@10
dobj .02 .10
amod .15 .37
amod+dobj .22 .43
ncrd+dobj .32 .65
ncrd .34 .60
ncrd+amod .49 .74
MEE .45 .77
Table 6: Accuracy of sources for nouns
node type, the ?pivot? node type that takes part in
edges of all three types. For adjectives and verbs,
the performance of MEE is the same as that of the
corresponding single-edge model.
We ran three additional experiments each of
which combines only two of the three possible
sources for noun similarity, namely ncrd+amod,
ncrd+dobj and amod+dobj and performed strict
evaluation (see Table 6). We found that in gen-
eral combination increases performance except
for ncrd+dobj vs. ncrd. We attribute this to the
lack of robustness of dobj mentioned above.
6.2 Comparison MEE vs. All-in-one
An alternative to MEE is to use untyped edges in
one large graph. In this all-in-one model (AIO),
we connect two nodes with an edge if they are
linked by any of the different linguistic relations.
While MEE consists of small adjacency matrices
for each type, the two adjacency matrices for AIO
are much larger. This leads to a much denser sim-
ilarity matrix taking up considerably more mem-
ory. One reason for this is that AIO contains simi-
larity entries between words of different parts of
speech that are 0 (and require no memory in a
sparse matrix representation) in MEE.
Since AIO requires more memory, we had to
filter the data much more strictly than before to be
able to run an experiment. We applied the follow-
ing stricter thresholds on relationships to obtain
a small graph: 5 instead of 3 for adjective-noun
MEEsmall AIOsmall
acc@1 .51 .52
acc@10 .72 .75
MRR .62 .59
Table 7: MEE vs. AIO
pairs, and 3 instead of 0 for verb-object pairs,
thereby reducing the total number of edges from
2.1M to 1.4M. We also applied threshold sieving
(see Section 4) with ? = 10?10 for AIO. The re-
sults on TS100 (strict evaluation) are reported in
Table 7. For comparison, MEE was also run on
the smaller graph. Performance of the two models
is very similar, with AIO being slightly better (not
significant). The slight improvement does not jus-
tify the increased memory requirements. MEE is
able to scale to more nodes and edge types, which
allows for better coverage and performance.
7 Analysis and Discussion
Error analysis. We examined the cases where a
reference translation was not at the top of the sug-
gested list of translation candidates. There are a
number of elements in the translation process that
can cause or contribute to this behavior.
Our method sometimes picks a cohyponym of
the correct translation. In many of these cases, the
correct translation is in the top 10 (together with
other words from the same semantic field). For
example, the correct translation of moon, Mond, is
second in a list of words belonging to the semantic
field of celestial phenomena: Komet (comet), Mond
(moon), Planet (planet), Asteroid (asteroid), Stern (star),
Galaxis (galaxy), Sonne (sun), . . . While this behavior
is undesirable for strict lexicon extraction, it can
be exploited for other tasks, e.g. cross-lingual se-
mantic relatedness (Michelbacher et al, 2010).
Similarly, the method sometimes puts the
antonym of the correct translation in first place.
For example, the translation for swift (schnell) is
in second place behind langsam (slow). Based
on the syntactic relations we use, it is difficult to
discriminate between antonyms and semantically
similar words if their syntactic distributions are
similar.
Ambiguous source words also pose a problem
for the system. The correct translation of square
(the geometric shape) is Quadrat. However, 8 out
of its top 10 translation candidates are related to
the location sense of square. The other two are ge-
ometric shapes, Quadrat being listed second. This
is only a concern for strict evaluation, since cor-
rect translations of a different sense were included
in the extended test set.
620
bed is also ambiguous (piece of furniture vs.
river bed). This introduces translation candidates
from the geographical domain. As an additional
source of errors, a number of bed?s neighbors
from the furniture sense have the German transla-
tion Bank which is ambiguous between the furni-
ture sense and the financial sense. This ambiguity
in the target language German introduces spurious
translation candidates from the financial domain.
Discussion. The error analysis demonstrates
that most of the erroneous translations are words
that are incorrect, but that are related, in some ob-
vious way, to the correct translation, e.g. by co-
hyponymy or antonymy. This suggests another
application for bilingual lexicon extraction. One
of the main challenges facing statistical machine
translation (SMT) today is that it is difficult to
distinguish between minor errors (e.g., incorrect
word order) and major errors that are completely
implausible and undermine the users? confidence
in the machine translation system. For example,
at some point Google translated ?sarkozy sarkozy
sarkozy? into ?Blair defends Bush?. Since bilin-
gual lexicon extraction, when it makes mistakes,
extracts closely related words that a human user
can understand, automatically extracted lexicons
could be used to discriminate smaller errors from
grave errors in SMT.
As we discussed earlier, parallel text is not
available in sufficient quantity or for all impor-
tant genres for many language pairs. The method
we have described here can be used in such cases,
provided that large monolingual corpora and ba-
sic linguistic processing tools (e.g. POS tagging)
are available. The availability of parsers is a more
stringent constraint, but our results suggest that
more basic NLP methods may be sufficient for
bilingual lexicon extraction.
In this work, we have used a set of seed trans-
lations (unlike e.g., Haghighi et al (2008)). We
believe that in most real-world scenarios, when
accuracy and reliability are important, seed lexica
will be available. In fact, seed translations can be
easily found for many language pairs on the web.
Although a purely unsupervised approach is per-
haps more interesting from an algorithmic point
of view, the semisupervised approach taken in this
paper may be more realistic for applications.
In this paper, we have attempted to reimplement
Rapp?s system as a baseline, but have otherwise
refrained from detailed comparison with previous
work as far as the accuracy of results is concerned.
The reason is that none of the results published so
far are easily reproducible. While previous publi-
cations have tried to infer from differences in per-
formance numbers that one system is better than
another, these comparisons have to be viewed with
caution since neither the corpora nor the gold stan-
dard translations are the same. For example, the
paper by Haghighi et al (2008) (which demon-
strates how orthography and contextual informa-
tion can be successfully used) reports 61.7% ac-
curacy on the 186 most confident predictions of
nouns. But since the evaluation data sets are not
publicly available it is difficult to compare other
work (including our own) with this baseline. We
simply do not know how methods published so far
stack up against each other.
For this reason, we believe that a benchmark
is necessary to make progress in the area of bilin-
gual lexicon extraction; and that our publication of
such a benchmark as part of the research reported
here is an important contribution, in addition to
the linguistically grounded extraction and the new
graph-theoretical method we present.
8 Summary
We have presented a new method, based on graph
theory, for bilingual lexicon extraction without re-
lying on resources with limited availability like
parallel corpora. We have shown that with this
graph-theoretic framework, information obtained
by linguistic analysis is superior to cooccurrence
data obtained without linguistic analysis. We have
presented multi-edge extraction (MEE), a scalable
graph algorithm that combines different linguis-
tic relations in a modular way. Finally, progress
in bilingual lexicon extraction has been hampered
by the lack of a common benchmark. We publish
such a benchmark with this paper and the perfor-
mance of MEE as a baseline for future research.
9 Acknowledgement
This research was funded by the German Re-
search Foundation (DFG) within the project A
graph-theoretic approach to lexicon acquisition.
621
References
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In EACL 2009 Workshop on Geo-
metrical Models of Natural Language Semantics.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Evert, Stefan. 2004. The Statistics of Word Cooccur-
rences - Word Pairs and Collocations. Ph.D. thesis,
Institut fu?r maschinelle Sprachverarbeitung (IMS),
Universita?t Stuttgart.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In COLING-ACL, pages 414?
420.
Garera, Nikesh, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon
induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL ?09: Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 129?137, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In KDD
?02, pages 538?543.
Koehn, Philipp and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,
and Denis Turdakov. 2008. Accuracy estimate and
optimization techniques for simrank computation.
PVLDB, 1(1):422?433.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, may.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In COLING 1999.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Schmid, Helmut. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In COLING ?04, page 162.
Voorhees, Ellen M. and Dawn M. Tice. 1999. The
TREC-8 question answering track evaluation. In
Proceedings of the 8th Text Retrieval Conference.
622
Coling 2010: Poster Volume, pages 1104?1112,
Beijing, August 2010
Sentiment Translation through Multi-Edge Graphs
Christian Scheible, Florian Laws, Lukas Michelbacher, and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart
{scheibcn, lawsfn, michells}@ims.uni-stuttgart.de
Abstract
Sentiment analysis systems can benefit
from the translation of sentiment informa-
tion. We present a novel, graph-based ap-
proach using SimRank, a well-established
graph-theoretic algorithm, to transfer sen-
timent information from a source lan-
guage to a target language. We evaluate
this method in comparison with semantic
orientation using pointwise mutual infor-
mation (SO-PMI), an established unsuper-
vised method for learning the sentiment of
phrases.
1 Introduction
Sentiment analysis is an important topic in com-
putational linguistics that is of theoretical interest
but is also useful in many practical applications.
Usually, two aspects are of importance in senti-
ment analysis. The first is the detection of sub-
jectivity, i.e., whether a text or an expression is
meant to express sentiment at all; the second is the
determination of sentiment orientation, i.e., what
sentiment is to be expressed in a structure that is
considered subjective.
Work on sentiment analysis most often cov-
ers resources or analysis methods in a single lan-
guage, usually English. However, the transfer
of sentiment analysis between languages can be
advantageous by making use of resources for a
source language to improve the analysis of the tar-
get language.
This paper presents an approach to the transfer
of sentiment information between two languages
that does not rely on resources with limited avail-
ability like parallel corpora. It is built around Sim-
Rank, a graph similarity algorithm that has suc-
cessfully been applied to the acquisition of bilin-
gual lexicons (Laws et al, 2010) and semantic
similarity (Michelbacher et al, 2010). It uses
linguistic relations extracted from two monolin-
gual corpora to determine the similarity of words
in different languages. One of the main benefits
of our method is its ability to handle sparse data
about the relations between the languages well
(i.e., a small seed lexicon). Further, we experi-
ment with combining multiple types of linguistic
relations for graph-based translation. Our exper-
iments are carried out using English as a source
language and German as a target language. We
evaluate our method using a hand-annotated set of
German adjectives which we intend to publish.
In the following section, related work is dis-
cussed. Section 3.1 gives an introduction to Sim-
Rank and its application to lexicon induction,
while section 3.2 reviews SO-PMI (Turney, 2002),
an unsupervised baseline method for the genera-
tion of sentiment lexicons. In section 4, we define
our sentiment transfer method which we apply in
experiments in section 5.
2 Related Work
Mihalcea et al (2007) propose two methods for
translating sentiment lexicons. The first method
simply uses bilingual dictionaries to translate an
English sentiment lexicon. A sentence-based clas-
sifier built with this list achieved high precision,
but low recall on a small Romanian test set. The
second method is based on parallel corpora. The
source language in the corpus is annotated with
sentiment information, and the information is then
projected to the target language. Problems arise
due to mistranslations.
Banea et al (2008) use machine translation for
multilingual sentiment analysis. Given a corpus
annotated with sentiment information in one lan-
guage, machine translation is used to produce an
annotated corpus in the target language, by pre-
serving the annotations. The original annotations
1104
can be produced either manually or automatically.
Wan (2009) constructs a multilingual classi-
fier using co-training. In co-training, one classi-
fier produces additional training data for a second
classifier. In this case, an English classifier assists
in training a Chinese classifier.
The induction of a sentiment lexicon is the sub-
ject of early work by Hatzivassiloglou and McK-
eown (1997). They construct graphs from coordi-
nation data from large corpora based on the intu-
ition that adjectives with the same sentiment ori-
entation are likely to be coordinated. For example,
fresh and delicious is more likely than rotten and
delicious. They then apply a graph clustering al-
gorithm to find groups of adjectives with the same
orientation. Finally, they assign the same label to
all adjectives that belong to the same cluster.
Corpus work and bilingual dictionaries are
promising resources for translating sentiment. In
contrast to previous approaches, the work pre-
sented in this paper uses corpora that are not an-
notated with sentiment.
Turney (2002) suggests a corpus-based extrac-
tion method based on his pointwise mutual infor-
mation (PMI) synonymy measure. He assumes
that the sentiment orientation of a phrase can be
determined by comparing its pointwise mutual in-
formation with a positive (excellent) and a nega-
tive phrase (poor). An introduction to this method
is given in Section 3.2.
3 Background
3.1 Lexicon Induction via SimRank
We use the extension of the SimRank (Jeh and
Widom, 2002) node similarity algorithm proposed
by Dorow et al (2009). Given two graphs A and
B, the similarity between two nodes a in A and b
in B is computed in each iteration as:
S(a, b) = c
|NA(a)||NB(b)|
?
k?NA(a),l?NB(b)
S(k, l).
NX(x) is the neighborhood of node x in graph
X . To compute similarities between two graphs,
some initial links between these graphs have to be
given, called seed links. These form the recursion
basis which sets S(a, b) = 1 if there is a seed
link between a and b. At the beginning of each
iteration, all known equivalences between nodes
are reset to 1.
Multi-Edge Extraction (MEE). MEE is an ex-
tension of SimRank that, in each iteration, com-
putes the average node-node similarity of several
different SimRank matrices. In our case, we use
two different SimRank matrices, one for coordi-
nations and one for adjective modification. See
(Dorow et al, 2009) for details. We also used
the node degree normalization function h(n) =?n ??maxk(|N(k)|) (where n is the node de-
gree, and N(k) the degree of node k) to decrease
the harmful effect of high-degree nodes on final
similarity values. See (Laws et al, 2010) for de-
tails.
3.2 SO-PMI
Semantic orientation using pointwise mutual in-
formation (SO-PMI) (Turney, 2002) is an algo-
rithm for the unsupervised learning of semantic
orientation of words or phrases. A word has pos-
itive (resp. negative) orientation if it is associ-
ated with positive (resp. negative) terms more
frequently than with negative (resp. positive)
terms. Association of terms is measured using
their pointwise mutual information (PMI) which
is defined for two words w1 and w2 as follows:
PMI(w1, w2) = log
( p(w1, w2)
p(w1)p(w2)
)
Using PMI, Turney defines SO-PMI for a word
w as
SO-PMI(w) =
log
?
p?P hits(word NEAR p)?
?
n?N hits(n)?
n?N hits(word NEAR n)?
?
p?P hits(p)
hits is a function that returns the number of hits
in a search engine given the query. P is a set of
known positive words, N a set of known negative
words, and NEAR an operator of a search engine
that returns documents in which the operands oc-
cur within a close range of each other.
1105
4 Sentiment Translation
Unsupervised methods like SO-PMI are suitable
to acquire basic sentiment information in a lan-
guage. However, since hand-annotated resources
for sentiment analysis exist in other languages,
it seems plausible to use automatic translation of
sentiment information to leverage these resources.
In order to translate sentiment, we will use multi-
ple sources of information that we represent in a
MEE graph as given in Section 3.1.
In our first experiments (Scheible, 2010), coor-
dinated adjectives were used as the sole training
source. Two adjectives are coordinated if they are
linked with a conjunction like and or but. The
intuition behind using coordinations ? based on
work by Hatzivassiloglou and McKeown (1997)
and Widdows and Dorow (2002) ? was that words
which are coordinated share properties. In partic-
ular, coordinated adjectives usually express sim-
ilar sentiments even though there are exceptions
(e.g., ?The movie was both good and bad?).
In this paper, we focus on using multiple edge
types for sentiment translation. In particular, the
graph we will use contains two types of relations,
coordinations and adjective-noun modification. In
the sentence ?The movie was enjoyable and fun?,
enjoyable and fun are coordinated. In This is an
enjoyable movie, the adjective enjoyable modifies
the noun movie.
We selected these two relation types for two
reasons. First, the two types provide clues for
sentiment analysis. Coordination information is
an established source for sentiment similarity (e.g.
Hatzivassiloglou and McKeown (1997)) while
adjective-noun relations provide a different type
of information on sentiment. For example, nouns
with positive associations (vacation) tend to occur
with positive adjectives and nouns with negative
associations (pain) tend to occur with negative ad-
jectives. Second, we have successfully used these
two types for a similar acquisition task, the acqui-
sition of word-to-word translation pairs (Laws et
al., 2010).
In the resulting graph, adjectives and nouns are
represented as nodes, each containing a word and
its part of speech, and relations are represented as
links which are distinguished by their edge types.
Two graphs, one in the source language and one in
the target language, are needed to translate words
between those languages. Figure 1 shows an ex-
ample for such a setup. Black links in this graph
are coordinations, grey links are seed relations.
In order to calculate sentiment for all nodes in
the target language, we apply the SimRank algo-
rithm to the graphs which gives us similarities be-
tween all nodes in the source graph and all nodes
in the target graph. Using the similarity S(ns, nt)
between a node ns in the source language graph
S and a node nt in the target language graph T ,
the sentiment score (sent(nt)) is the similarity-
weighted average of all sentiment scores in the
target language:
sent(nt) =
?
ns?S
simnorm(ns, nt) sent(ns)
We assume that sentiment scores in the source
language are expressed on a numeric scale. The
normalized similarity simnorm is defined as
simnorm(ns, nt) = S(ns, nt)?
ns?S S(ns, nt)
.
The normalization assures that all resulting sen-
timent values are within [?1, 1], with ?1 being
the most negative sentiment and 1 the most posi-
tive.
5 Experiments
5.1 Data Acquisition
For our experiments, we needed coordination data
to build weighted graphs and a bilingual lexi-
con to define seed relations between those graphs.
Coordinations were extracted from the English
and German versions of Wikipedia1 by applying
pattern-based search using the Corpus Query Pro-
cessor (CQP) (Christ et al, 1999). We annotated
both corpora with parts of speech using the Tree
Tagger (Schmid, 1994). A total of 477,291 En-
glish coordinations and 112,738 German coordi-
nations were collected. A sample of this data is
given in Figure 2. We restrict these experiments
to the use of and/und since other coordinations
1http://www.wikipedia.org/ (01/19/2009)
1106
affordable
delicious
nutritiousjuicy
tasty
healthylovely
schmackhaft
gesundstrange
frisch
wertvoll
nahrhaft angesehen
ertragreich
Figure 1: A German and an English graph with coordinated adjectives including seed links
affordable
delicious
diverse
popularnutritious
inexpensive
original
varied
melodious
rare
strange
juicy
tasty
exotic healthy
tempting
lovely
hearty fragrant
dangerous
beautiful
charming authentic
Figure 2: English sample coordinations (adjectives)
1107
behave differently and might even express dissim-
ilarity (e.g. Was the weather good or bad?).
The seed lexicon was constructed from the
dict.cc dictionary2. While the complete dictionary
contains 30,551 adjective pairs, we reduced the
number of pairs used in the experiments to 1,576.
To produce a smaller seed lexicon which still
makes sense from a semantic point of view, we
used the General Service List (GSL) (West, 1953)
which contains about 2000 words the author con-
sidered central to the English language. More
specifically, a revised list was used3.
SO-PMI needs a larger amount of training data.
Since Wikipedia does not satisfy this need, we
collected additional coordination data from the
web using search result counts from Google. In
Turney?s original paper, he uses the NEAR oper-
ator, which returns documents that contain two
search terms that are within a certain distance of
each other, to collect collocations. Unfortunately,
Google does not support this operator, so instead,
we searched for coordinations using the queries
+ "w and s" and
+ "w und s"
for English and German, respectively. We added
the quotes and the + operator to make sure that
both spelling correction and synonym replace-
ments were disabled.
The original experiments were made for En-
glish, so we had to construct our own set of
seed words. For German, we chose gut (good),
nett (nice), richtig (right), scho?n (beautiful), or-
dentlich (neat), angenehm (pleasant), aufrichtig
(honest), gewissenhaft (faithful), and hervorra-
gend (excellent) as positive seed words, and
schlecht (bad), teuer (expensive), falsch (wrong),
bo?se (evil), feindlich (hostile), verhasst (invidi-
ous), widerlich (disgusting), fehlerhaft (faulty),
and mangelhaft (flawed) as negative ones.
5.2 Sentiment Lexicon
For our experiments, we used two different polar-
ity lexicons. The lexicon of Wilson et al (2005)
contains sentiment annotations for 8,221 words
2http://www.dict.cc
3http://jbauman.com/aboutgsl.html
annotation value
positive 1.0
weakpos 0.5
neutral 0.0
weakneg ?0.5
negative ?1.0
Table 1: Assigned values for Wilson et al set
which are tagged as positive, neutral, or nega-
tive. A few words are tagged as weakneg, imply-
ing weak negativity. These categorial annotations
are mapped to the range [-1,1] using the assign-
ment scheme given in Table 1.
5.3 Human Ratings
In order to manually annotate a test set, we
chose 200 German adjectives that occurred in the
Wikipedia corpus and that were part of a coor-
dination. From these words, we removed those
which we deemed uncommon, too complicated,
or which were mislabeled as adjectives by the tag-
ger. The test set contained 150 adjectives of which
seven were excluded after annotators discarded
them.
We asked 9 native speakers of German to anno-
tate the adjectives. Possible annotations were very
positive, slightly positive, neutral, slightly nega-
tive, or very negative. These categories are the
same as the ones used in the training data.
In order to capture the general sentiment, i.e.,
sentiment that is not related to a specific context,
the judges were asked to stay objective and not
let their personal opinions influence the annota-
tion. However, some words with strong political
implications were annotated by some judges as
non-neutral which led to disagreement beyond the
usual level. Nuklear (nuclear) is an example for
such a word. We measured the agreement of the
judges with Kendall?s coefficient of concordance
(W ) with tie correction (Legendre, 2005), yield-
ing W = 0.674 with a high level of significance
(p < .001); thus, inter-annotator agreement was
high (Landis and Koch, 1977).
5.4 Experimental Setup
Given the relations extracted from Wikipedia, we
built a German and an English graph by setting
1108
Method r
MEE 0.63
MEE-GSL 0.47
SR 0.63
SR-GSL 0.48
SO-PMI 0.58
Table 2: Correlation with human ratings
the weight of each link to the log-likelihood ra-
tio of the two words it connects according to the
corpus frequencies. There are two properties of
the graph transfer algorithm that we intend to in-
vestigate. First, we are interested in the merits of
applying multi edge extraction (MEE) for senti-
ment transfer. Second, we are interested in how
the transfer quality changes when the seed lexi-
con is reduced in size. This way, a sparse data
situation is simulated where large dictionaries are
unavailable. Having these two properties in mind,
four possible setups are evaluated: (i) using the
full seed lexicon with all 30,551 entries, but using
only coordination data (SR), (ii) reducing the seed
lexicon to 1,576 entries from the General Service
List (SR-GSL), (iii) applying MEE by adding ad-
jective modification data (MEE), and (iv) using
MEE with a reduced seed lexicon (MEE-GSL).
SimRank was run for 6 iterations in all experi-
ments. All experiments use the weight function
h as described above. We show that this function
improves similarities and thus lexicon induction
in Laws et al (2010).
Correlation. First, we will examine the correla-
tion between the automatic methods (SO-PMI and
the aforementioned SimRank variations) and the
gold standard as done by Turney in his evaluation.
For this purpose, the human ratings are mapped
to float values following Table 1 and the aver-
age rating over all judges for each word is used.
The correlation coefficients r are given in Table 2.
Judging from these results, the ordering of SR and
MEE matches the human ratings better than SO-
PMI, however it decreases when using any of the
GSL variations instead which can be attributed to
using less data.
Classification. The correct identification of the
classes positive, neutral, and negative is more im-
portant than the correct assignment of values on
a scale since the rank ordering is debatable ? this
becomes apparent when measuring the agreement
of human annotators. Since the assignments made
by the human judges are not unanimous in most
cases, the averages are distributed across the in-
terval [-1,1]; this means that the borders between
the three distinct categories are not clear. Since
there is no standard evaluation for this particu-
lar problem, we need to devise a way to make
the range of the neutral category dynamic. In or-
der to find possible borders, we first assume that
sentiment is distributed symmetrically around 0.
We then define a threshold x which assumes the
values x ? { i20 |0 ? i ? 20}, covering the in-terval [0,0.5]. Since 0.5 is slightly positive, we
do not believe that values above it are plausible.
Then, each word w is positive if its human rating
scoreh(w) ? x, negative if scoreh(w) ? ?x, and
neutral if ?x < scoreh(w) < x. The result of
this process is a gold standard for the three cate-
gories for each of the values for x. The percentiles
of the sizes of those categories are mapped to the
values produced by the automatic methods. For
example, if x = 0.35 means that the top 21% of
all adjectives are in the positive class, the top 21%
of all adjectives as assigned by SO-PMI and the
SimRank varieties are positive as well.
The size of the neutral category increases the
larger x becomes. Thus, high values for x are
unlikely to produce a correct partitioning of the
data. Since slightly positive was defined as 0.5,
we expect the highest plausible value for x to be
below that. The size of the neutral category for
each value of x is given in Table 3. (Recall that
the total size of the set is 143.)
We can then compute the assignment accu-
racy on the positive, neutral, and negative classes,
as well macro- and micro-averages over these
classes.
5.5 Results and Discussion
Figures 3 and 4 show the macro- and micro-
averaged accuracies over the positive, negative,
and neutral class for each automatic method, re-
spectively. Overall, the SimRank variations per-
form better for x in the interval [0, 0.3]. In partic-
ular, MEE has a slightly higher accuracy than SR,
1109
x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
# neutral 0 13 35 46 56 64 74 82 92 99 99
Table 3: Size of neutral category given x
word (translation) humans SO MEE MEE-GSL SR SR-GSL
chemisch (chemical) 0.00 -20.20 0.185 0.185 0.186 0.184
auferstanden (resurrected) 0.39 -10.96 -0.075 -0.577 -0.057 -0.493
intelligent (intelligent) 0.94 46.59 0.915 0.939 0.834 0.876
versiert (skilled) 0.67 -5.26 0.953 0.447 0.902 0.404
mean -0.04 -9.58 0.003 0.146 0.010 0.142
median 0.00 -15.60 0.110 0.157 0.114 0.157
Table 4: Example adjectives including translation, and their scores
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (macro)MEE (macro)MEE-GSL (macro)SR (macro)SR-GSL (macro)
Figure 3: Macro-averaged Accuracy
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (micro)MEE (micro)MEE-GSL (micro)SR (micro)SR-GSL (micro)
Figure 4: Micro-averaged Accuracy
1110
however, not significantly.
Table 4 shows selected example words with
their scores. These values can be understood bet-
ter together with the means and medians of the
respective methods which are given in the table as
well. These values give us an idea of where we
might expect the neutral point of a particular dis-
tribution of polarities.
Chemisch (chemical) is misclassified by SO-
PMI since it occurs in negative contexts on the
web. SimRank in turn was able to recognize
that most words similar to chemisch are neutral,
the most similar one being its literal translation,
chemical. Auferstanden (resurrected) is an exam-
ple for misclassification by SimRank which hap-
pens because the word is usually coordinated with
words that have negative sentiment, e.g. gestor-
ben (deceased) and gekreuzigt (crucified). This
problem could not be fixed by including adjective-
noun modification data since the coordinations
produced high log-likelihood values which lead to
dead being the most similar word to auferstanden.
Intelligent receives a score close to neutral with
the original (coordination-only) training method,
which could be corrected by applying MEE sim-
ply because the ordering of similar words changes
through the new weighting method. Nouns modi-
fied by intelligent include Leben (life) and Wesen
(being) whose translations are modified by pos-
itive adjectives. Many words, such as versiert
(skilled) are classified more accurately due to the
new weighting method when compared to our pre-
vious experiments (Scheible, 2010) where it re-
ceived a SimRank polarity of only 0.224.
The inclusion of adjective modifications does
not improve the classification results as often as
we had hoped. For some cases (cf. intelligent
mentioned above), the scores do improve, but the
overall impact is limited.
6 Conclusion and Outlook
We were able to show that sentiment translation
with SimRank is able to classify adjectives more
accurately than SO-PMI, an unsupervised base-
line method. We demonstrated that SO-PMI is
outperformed by SimRank when choosing a rea-
sonable region of neutral adjectives. In addition,
we showed that the improvements of SimRank
lead to better accuracy in sentiment translation in
some cases. In future work, we will apply a senti-
ment lexicon generated with SimRank in a senti-
ment classification task for reviews.
The algorithms we compared are different in
their purpose of application. While SO-PMI is
applicable when large corpora are available for a
language, it fails when used in a sparse-data situ-
ation, as noted by Turney (2002). We showed that
despite reducing the seed lexicon for SimRank to
a small fraction of its original size, it still performs
better than SO-PMI.
Currently, our experiments are limited by the
choice of using adjectives for our test set. While
the examination of adjectives is highly important
for sentiment analysis (as shown by Pang et al
(2002) who were able to achieve high accuracy
even when using only adjectives), the application
of our algorithms to a broader set of linguistic
units is an important goal for future work.
Acknowledgments. We are grateful to
Deutsche Forschungsgemeinschaft for fund-
ing this research as part of the WordGraph
project.
References
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Empirical
Methods in Natural Language Processing, pages
127?135.
Christ, O., B.M. Schulze, A. Hofmann, and E. Koenig.
1999. The IMS Corpus Workbench: Corpus Query
Processor (CQP): User?s Manual. University of
Stuttgart, March, 8:1999.
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Workshop on Geometrical Mod-
els of Natural Language Semantics, pages 91?95.
Hatzivassiloglou, Vasileios and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation of
adjectives. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 174?181.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In Pro-
ceedings of the Eighth ACM SIGKDD Interna-
1111
tional Conference on Knowledge Discovery and
Data Mining, pages 538?543.
Landis, J.R. and G.G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Laws, Florian, Lukas Michelbacher, Beate Dorow,
Christian Scheible, Ulrich Heid, and Hinrich
Schu?tze. 2010. A linguistically grounded graph
model for bilingual lexicon extraction. In Proceed-
ings of the 23nd International Conference on Com-
putational Linguistics.
Legendre, P. 2005. Species associations: the Kendall
coefficient of concordance revisited. Journal of
Agricultural Biological and Environment Statistics,
10(2):226?245.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
Conference on International Language Resources
and Evaluation.
Mihalcea, Rada, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 976?983.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Scheible, Christian. 2010. Sentiment translation
through lexicon induction. In Proceedings of the
ACL 2010 Student Research Workshop, Uppsala,
Sweden. Association for Computational Linguis-
tics.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Turney, Peter. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424.
Wan, Xiaojun. 2009. Co-training for cross-lingual
sentiment classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235?
243, Suntec, Singapore, August. Association for
Computational Linguistics.
West, Michael. 1953. A general service list of english
words.
Widdows, Dominic and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. InCOL-
ING.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 347?354, October.
1112
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1546?1556,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Active Learning with Amazon Mechanical Turk
Florian Laws Christian Scheible Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn, scheibcn}@ims.uni-stuttgart.de
Abstract
Supervised classification needs large amounts
of annotated training data that is expensive to
create. Two approaches that reduce the cost
of annotation are active learning and crowd-
sourcing. However, these two approaches
have not been combined successfully to date.
We evaluate the utility of active learning in
crowdsourcing on two tasks, named entity
recognition and sentiment detection, and show
that active learning outperforms random selec-
tion of annotation examples in a noisy crowd-
sourcing scenario.
1 Introduction
Supervised classification is the predominant tech-
nique for a large number of natural language pro-
cessing (NLP) tasks. The large amount of labeled
training data that supervised classification relies on
is time-consuming and expensive to create, espe-
cially when experts perform the data annotation.
Recently, crowdsourcing services like Amazon Me-
chanical Turk (MTurk) have become available as an
alternative that offers acquisition of non-expert an-
notations at low cost. MTurk is a software service
that outsources small annotation tasks ? called HITs
? to a large group of freelance workers. The cost of
MTurk annotation is low, but a consequence of us-
ing non-expert annotators is much lower annotation
quality. This requires strategies for quality control
of the annotations.
Another promising approach to the data acqui-
sition bottleneck for supervised learning is active
learning (AL). AL reduces annotation effort by set-
ting up an annotation loop where, starting from a
small seed set, only the maximally informative ex-
amples are chosen for annotation. With these an-
notated examples, the classifier is then retrained to
again select more informative examples for further
annotation. In general, AL needs a lot fewer anno-
tations to achieve a desired performance level than
random sampling.
AL has been successfully applied to a number of
NLP tasks such as part-of-speech tagging (Ringger
et al, 2007), parsing (Osborne and Baldridge, 2004),
text classification (Tong and Koller, 2002), senti-
ment detection (Brew et al, 2010), and named entity
recognition (NER) (Tomanek et al, 2007). Until
recently, most AL studies focused on simulating the
annotation process by using already available gold
standard data. In reality, however, human annota-
tors make mistakes, leading to noise in the annota-
tions. For this reason, some authors have questioned
the applicability of AL to noisy annotation scenarios
such as MTurk (Baldridge and Palmer, 2009; Re-
hbein et al, 2010).
AL and crowdsourcing are complementary ap-
proaches: AL reduces the number of annotations
used while crowdsourcing reduces the cost per an-
notation. Combined, the two approaches could sub-
stantially lower the cost of creating training sets.
Our main contribution in this paper is that we
show for the first time that AL is significantly bet-
ter than randomly selected annotation examples in
a real crowdsourcing annotation scenario. Our
experiments directly address two tasks, named en-
tity recognition and sentiment detection, but our
1546
evidence suggests that AL is of general benefit in
crowdsourcing. We also show that the effectiveness
of MTurk annotation with AL can be further en-
hanced by using two techniques that increase label
quality: adaptive voting and fragment recovery.
2 Related Work
2.1 Crowdsourcing
Pioneered by Snow et al (2008), Crowdsourcing,
especially using MTurk, has become a widely used
service in the NLP community. A number of stud-
ies have looked at crowdsourcing for NER. Voyer et
al. (2010) use a combination of expert and crowd-
sourced annotations. Finin et al (2010) annotate
Twitter messages ? short sequences of words ? and
this is reflected in their vertically oriented user in-
terface. Lawson et al (2010) choose an annotation
interface where annotators have to drag the mouse
to select entities. Carpenter and Poesio (2010) ar-
gue that dragging is less convenient for workers than
marking tokens.
These papers do not address AL in crowdsourc-
ing. Another important difference is that previous
studies on NER have used data sets for which no
?linguistic? gold annotation is available. In con-
trast, we reannotate the CoNLL-2003 English NER
dataset. This allows us to conduct a detailed com-
parison of MTurk AL to conventional expert anno-
tation.
2.2 Active Learning with Noisy Labels
Hachey et al (2005) were among the first to in-
vestigate the effect of actively sampled instances
on agreement of labels and annotation time. They
demonstrate applicability of AL when annotators are
trained experts. This is an important result. How-
ever, AL depends on accurate assessments of uncer-
tainty and informativeness and such an accurate as-
sessment is made more difficult if labels are noisy
as is the case in crowdsourcing. For this reason, the
problem of AL performance with noisy labels has
become a topic of interest in the AL community. Re-
hbein et al (2010) investigate AL with human expert
annotators for word sense disambiguation, but do
not find convincing evidence that AL reduces anno-
tation cost in a realistic (non-simulated) annotation
scenario. Brew et al (2010) carried out experiments
on sentiment active learning through crowdsourcing.
However, they use a small set of volunteer labelers
instead of anonymous paid workers.
Donmez and Carbonell (2008) propose a method
to choose annotators from a set of noisy annotators.
However, in a crowdsourcing scenario, it is not pos-
sible to ask specific annotators for a label, as crowd-
sourcing workers join and leave the site. Further-
more, they only evaluate their approach in simula-
tions. We use the actual labels of human annotators
to avoid the risk of unrealistic assumptions when
modeling annotators.
We are not aware of any study that shows that AL
is significantly better than a simple baseline of hav-
ing annotators annotate randomly selected examples
in a highly noisy annotation setting like crowdsourc-
ing. While AL generally is superior to this base-
line in simulated experiments, it is not clear that
this result carries over to crowdsourcing annotation.
Crowdsourcing differs in a number of ways from
simulated experiments: the difficulty and annotation
consistency of examples drawn by AL differs from
that drawn by random sampling; crowdsourcing la-
bels are noisy; and because of the noisiness of labels
statistical classifiers behave differently in simulated
and real annotation experiments.
3 Annotation System
One fundamental design criterion for our annotation
system was the ability to select examples in real time
to support, e.g., the interactive annotation experi-
ments presented in this paper. Thus, we could not
use the standard MTurk workflow or services like
CrowdFlower.1
We therefore designed our own system for anno-
tation experiments. It consists of a two-tiered ap-
plication architecture. The frontend tier is a web
application that serves two purposes. First, the ad-
ministrator can manage annotation experiments us-
ing a web interface and publish annotation tasks as-
sociated with an experiment on MTurk. The front-
end also provides tools for efficient review of the
received answers. Second, the frontend web appli-
cation presents annotation tasks to MTurk workers.
Because we wanted to implement interactive anno-
tation experiments, we used the ?external question?
1http://crowdflower.com/
1547
feature of MTurk. An external question contains
an URL to our frontend web application, which is
queried when a worker views an annotation task.
Our frontend then in turn queries our backend com-
ponent for an example to be annotated and renders it
in HTML.
The backend component is responsible for selec-
tion of an example to be annotated in response to a
worker?s request for an annotation task. The back-
end implements a diverse choice of random and ac-
tive selection strategies as well as the multilabel-
ing strategies described in section 3.2. The backend
component runs as a standalone server and is queried
by the frontend via REST-like HTTP calls.
For the NER task, we present one sentence per
HIT, segmented into tokens, with a select box under-
neath each token containing the classes. The defini-
tion of the classes is based on the CoNLL-2003 an-
notation guidelines (Tjong Kim Sang and De Meul-
der, 2003). Examples were given for every class.
Annotators are forced to make a selection for upper-
case tokens. Lowercase tokens are prelabeled with
?O? (no named entity), but annotators are encour-
aged to change this label if the token is in fact part
of an entity phrase.
For sentiment annotation, we found in prelim-
inary experiments that using simple radio button
selection for the choice of the document label
(positive or negative) leads to a very high
amount of spam submissions, taking the overall clas-
sification accuracy down to around 55%. We then
designed a template that forced annotators to type
the label as well as a randomly chosen word from
the text. Individual label accuracy was around 75%
in this scheme.
3.1 Concurrent example selection
AL works by setting up an interactive annotation
loop where at each iteration, the most informative
example is selected for annotation. We use a pool-
based AL setup where the most informative exam-
ple is selected from a pool of unlabeled examples.
Informativeness is calculated as uncertainty (Lewis
and Gale, 1994) using the margin metric (Schein
and Ungar, 2007). This metric chooses examples for
which the margin of probabilities from the classifier
between the two most probable classes is the small-
est:
Mn = |P? (c1|xn) ? P? (c2|xn)|
Here, xn is the instance to be classified, c1 and c2
are the two most likely classes, and P? the classifier?s
estimate of probability.
For NER, the margins of the tokens are averaged
to get an uncertainty assessment of the sentence. For
sentiment, whole documents are classified, thus un-
certainties can be used directly.
After annotation, the selected example is removed
from the unlabeled pool and, together with its la-
bel(s), added to the set of labeled examples. The
classifier is then retrained on the labeled examples
and the informativeness of the remaining examples
in the pool is re-evaluated.
Depending on the classifier and the sizes of pool
and labeled set, retraining and reevaluation can take
some time. To minimize wait times, traditional AL
implementations select examples in batches of the
n most informative examples. However, batch se-
lection might not give the optimum selection (exam-
ples in a batch are likely to be redundant, see Brinker
(2003)) and wait times can still occur between one
batch and the next.
When performing annotation with MTurk, wait
times are unacceptable. Thus, we perform the re-
training and uncertainty rescoring concurrently with
the annotation user interface. The unlabeled pool is
stored in a priority queue that is ordered according to
the examples? informativeness. The annotation user
interface takes the most informative example from
the pool and presents it to the annotator. The la-
beled example is then inserted into a second queue
that feeds and updates retraining and rescoring pro-
cesses. The pool queue then is resorted according to
the new informativeness. In this way, annotation and
example selection can run in parallel. This is similar
to Haertel et al (2010).
3.2 Adaptive voting and fragment recovery
MTurk labels often have a high error rate. A com-
mon strategy for improving label quality is to ac-
quire multiple labels by different workers for each
example and then consolidate the annotations into
a single label of higher quality. To trade off num-
ber of annotated examples against quality of anno-
tations, we adopt adaptive voting. It uses majority
1548
NER Sentiment
Budget 5820 6931 1130 1756
#train F1 cost/sent w.-accuracy #train F1 #train Acc cost/doc w.-accuracy #train Acc
RS 1 S 5820 59.6 1.00 51.6 ? ? 1130 70.4 1 74.8 ? ?
2 3-v 1624 61.4? 3.58 70.1 ? ? ? ? ? ? ? ?
3 5/4-v 1488 63.0? 3.91 71.6 1774 63.5 450 71.2 2.51 89.6 735 79.2
4 5-v+f 1996 63.6? 2.91 71.8 2385 64.9? ? ? ? ? ? ?
AL 5 S 5820 67.0 1.00 66.5 ? ? 1130 74.8 1 76.0 ? ?
6 3-v 1808 70.0? 3.21 78.8 ? ? ? ? ? ? ? ?
7 5/4-v 1679 70.4? 3.46 79.6 1966 70.6 455 77.4 2.48 89.0 715 81.8
8 5-v+f 2165 70.5 2.68 79.3 2691 71.2 ? ? ? ? ? ?
Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F1 evaluated on
CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5-
and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment
budget 1756 averaged over 2 runs.
voting and is adaptive in the number of repeated an-
notations. For NER, a sentence is first annotated by
two workers. Then majority voting is performed for
each token individually. If there is a majority for ev-
ery token that is greater than an agreement threshold
?, the sentence is accepted with each token labeled
with the majority label. Otherwise additional anno-
tations are requested. A sentence is discarded if the
number of repeated annotations exceeds a discard
threshold d (d-voting).2 We use the same scheme
for sentiment; note that there is just one decision per
HIT in this case, not several as in NER.
For NER, we also use fragment recovery: we sal-
vage tokens with agreeing labels from discarded sen-
tences. We cut the token sequence of a discarded
sentence into several fragments that have agreeing
tokens and discard only those parts that disagree. We
then include these recovered fragments in the train-
ing data just like complete sentences.
Software release. Our active learning framework
used can be downloaded at http://www.ims.
uni-stuttgart.de/?lawsfn/active/.
4 Experiments, Results and Analysis
4.1 Experiments
In our NER experiments, we have workers reanno-
tate the English corpus of the CoNLL-2003 NER
shared task. We chose this corpus to be able to com-
pare crowdsourced annotations with gold standard
2It can take a while in this scheme for annotators to agree
on a final annotation for a sentence. We make tentative labels
of a sentence available to the classifier immediately and replace
them with the final labels once voting is completed.
annotations. A HIT is one sentence and is offered
for a base payment of $0.01. We filtered out answers
that contained unannotated tokens or were obvious
spam (e.g., all tokens labeled as MISC). For test-
ing NER performance, we used a system based on
conditional random fields with standard named en-
tity features including the token itself, orthographic
features like the occurrence of capitalization or spe-
cial characters and context information about the to-
kens to the left/right of the current token.
The sentiment detection task was modeled after a
well-known document analysis setup for sentiment
classification, introduced by Pang et al (2002). We
use their corpus of 1000 positive and 1000 negative
movie reviews and the Stanford maximum entropy
classifier (Manning and Klein, 2003) to predict the
sentiment label of each document d from a unigram
representation of d. We randomly split this corpus
into a test set of 500 reviews and an active learn-
ing pool of 1500 reviews. Each HIT consists of one
document, valued at $0.01.
We compare random sampling (RS) and AL in
combination with the proposed voting and fragment
strategies with different parameters. We want to
avoid rerunning experiments on MTurk over and
over again, but on the other hand, we believe that us-
ing synthetic data for simulations is problematic be-
cause it is difficult to generate synthetic data with a
realistic model of annotator errors. Thus, we logged
a play-by-play record of the annotator interactions
and labels. With this recording, we can then rerun
strategies with different parameters.
We chose voting with at most d = 5 repetitions as
1549
our main reannotation strategy for both random and
active sampling for NER annotation. We use simple
majority voting (? = .5) for NER.
For sentiment, we set d = 4 and minimum agree-
ment ? = .75 because the number of labels is
smaller (2 vs. 5) and so random agreement is more
likely for sentiment.
To get results for 3-voting NER, we take the
recording and discard 5-voting votes not needed in
3-voting. This will result in roughly the same num-
ber of annotated sentences, but at a lower cost. This
simulation of 3-voting is not exactly what would
have happened on MTurk (e.g., the final vote on a
sentence might be different, which then influences
AL example selection), but we will assume that dif-
ferences are rare and simulated and actual results
are similar. The same considerations apply to sin-
gle votes and to the sentiment experiments.
We always compare two strategies for the same
annotation budget. For example, the number of
training sentences in Table 1 differ in the two rel-
evant columns, but all strategies compared use ex-
actly the same annotation budget (5820, 6931, 1130,
and 1756, respectively).
For the single annotation strategy, each interac-
tion record contained only about 40% usable anno-
tations, the rest were repeats. A comparison with
the single annotation strategy over approx. 2000 sen-
tences or 450 documents would not have been mean-
ingful; therefore we chose to run an extra experiment
with the single annotation strategy to match this up
with the budgets of the voting strategies. The re-
sults are presented in two separate columns of Ta-
ble 1 (budgets 6931 and 1756).
4.2 Results
For sentiment detection, worker accuracy or label
quality ? the percentage of correctly annotated doc-
uments ? is 74.8. In contrast, for NER, worker accu-
racy ? the percentage of non-O tokens annotated cor-
rectly ? is only 51.6 (Table 1, line 1). This demon-
strates the challenge of using MTurk for NLP an-
notation tasks. When we use single annotations of
each sentence, NER performance is 59.6 F1 for ran-
dom sampling (line 1). When training with gold la-
bels on the same sentences, the performance is 80.0
(not shown). This means we lose more than 20%
due to poor worker accuracy. Adaptive voting and
fragment recovery manage to recover a small part of
the lost performance (lines 2?4); each of the three
F1 scores is significantly better than the one above
it as indicated by ? (Approximate Randomization
Test (Noreen, 1989; Chinchor et al, 1993) as im-
plemented by Pado? (2006)).
Using AL turns out to be quite successful for NER
performance. For single annotations, NER perfor-
mance is 67.0 (line 5), an improvement of 7.4%
compared to random sampling. Adaptive voting
and fragment recovery again increase worker accu-
racy (lines 6?8) although total improvement of 3.5%
(lines 8 vs. 5) is smaller than 4% for random (lines
4 vs. 1). The learning curves of AL vs. random in
Figure 1 (top left) confirm this good result for AL.
These learning curves are for tokens ? not for sen-
tences ? to show that the reason for AL?s better per-
formance is not that it selects slightly longer sen-
tences than random. In addition, the relative advan-
tage of AL vs random decreases over time, which is
typical of pool-based AL experiments.
We carried out two runs of the same experiment
for sentiment to validate our first positive result since
the difference between the two conditions is not as
large as in NER (Figure 1, top right). After about
300 documents, active learning consistently outper-
forms random sampling. The first AL run performs
better because of higher label quality in the begin-
ning. The overall advantage of AL over random
is lower than for NER because the set of labels is
smaller in sentiment, making the classification task
easier. Second, there is a large amount of simple lex-
ical clues for detecting sentiment (cf. Wilson et al
(2005)). It is likely that some of them can be learned
well through random sampling at first; however, ac-
tive learning can gain accuracy over time because it
selects examples with more difficult clues.
In Figure 1 (bottom), we compare single annota-
tion with adaptive voting. The graphs show F1 as
a function of cost. Adaptive voting trades quantity
of sampled sentences for quality of labels and thus
incurs higher net costs per sentence. This results in
a smaller dataset for a given budget, but this dataset
is still more useful for classifier training. For NER
(Figure 1, bottom left), the single annotation strat-
egy has a faster start; so for small budgets, cover-
ing a somewhat larger portion of the sample space
is beneficial. For larger budgets, however, quality of
1550
0 5000 10000 15000 20000
0.
4
0.
5
0.
6
0.
7
0.
8
Tokens
F?
Sc
or
e
active, 5?voting
random, 5?voting
0 200 400 600
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Documents
Ac
cu
ra
cy
active 1
active 2
random 1
random 2
0 1000 2000 3000 4000 5000 6000
0.
4
0.
5
0.
6
0.
7
0.
8
Cost
F?
Sc
or
e
single ann.
3?voting
5?voting
5?voting +frags
0 500 1000 1500 2000
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Cost
Ac
cu
ra
cy
single ann.
4?voting
Figure 1: Top: Active learning vs. Random sampling for NER (left) and sentiment (right). Bottom: Active
learning: adaptive voting vs. single annotation for NER (left) and sentiment (right).
the voted labels trumps quantity.
For sentiment (Figure 1, bottom right), results are
similar: voting has no benefit initially, but as find-
ing maximally informative examples to annotate be-
comes harder in later stages of learning, adaptive
voting gains an advantage over single annotations.
The main result of the experiment is that active
learning is better by about 7% F1 than random sam-
pling for NER and by 2.6% accuracy for sentiment
(averaged over two runs at budget 1756). Adaptive
voting further improves AL performance for both
NER and sentiment.
4.3 Annotation time per token
Most AL work assumes constant cost per annotation
unit. This assumption has been questioned because
AL often selects hard examples that take longer to
annotate (Hachey et al, 2005; Settles et al, 2008).
In annotation with MTurk, cost is not a function
of annotation time because workers are paid a fixed
amount per HIT. Nevertheless, annotation time plays
a part in whether workers are willing to work on a
given task for the offered reward. This is particularly
problematic for NER since workers have to examine
each token individually. We therefore investigate
for NER whether the time MTurk workers spend on
annotating sentences differs for random vs. AL.
We first compute median and mean annotation
times and number of tokens per sentence:
sec/sentence tokens/sentence
strategy median mean all required
random 17.2 33.1 15.0 3.4
AL 17.8 33.0 17.7 4.0
We see that most sentences are annotated in a very
short time; but the mean is much larger than the me-
dian because there are outliers of up to eight min-
utes. AL tends to select slightly longer sentences as
1551
0 500 1000 1500 2000
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Sentences
F?
Sc
or
e
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
0 200 400 600
0.
50
0.
60
0.
70
0.
80
Documents
Ac
cu
ra
cy
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
Figure 3: Performance on gold labels. Left: NER. Right: sentiment (run 1).
0 2 4 6 8 10 13 16 19 23 29
0
10
0
20
0
30
0
40
0
Number of uppercase tokens
An
no
ta
tio
n 
tim
e 
(se
co
nd
s)
random
active
Figure 2: Annotation time vs. # uppercase tokens
well as sentences with slightly more uppercase to-
kens that require annotation.
In a more detailed analysis, we attempt to distin-
guish between (i) the effect of more uppercase (?an-
notation required?) tokens vs. (ii) the effect of ex-
ample difficulty. We fit a linear regression model
to annotation time vs. the number of uppercase to-
kens. For the regression fit, we removed all annota-
tion times > 60 seconds. Such long times indicate
distraction of the worker and are not a reliable mea-
sure of difficulty.
Figure 2 shows the distribution of annotation
times for both cases combined and the fitted models
for each. The model estimated an annotation time of
2.3 secs for each required token for random vs. 2.7
secs for AL. We conclude that the difference in dif-
ficulty between sentences selected by random sam-
pling vs. AL is small, but noticeable.
4.4 Influence of noise on the selection process
While NER performance for AL is much higher than
for random sampling, it is still quite a bit lower than
what is possible on gold labels. In the case of AL,
there are two reasons why this happens: (i) The
noisy labels negatively affect the classifier?s ability
to learn a good model that is used for classifying the
test set. (ii) The noisy labels result in bad interme-
diate models that then select suboptimal examples
to be annotated next. The AL selection process is
?misled? by the noisy examples.
We conduct an experiment to determine the con-
tribution of factors (i) and (ii) to the performance
loss. First, we preserve the sequence of sentences
chosen by our AL experiments on MTurk, with 5-
voting for NER and 4-voting for sentiment but re-
place the noisy worker-provided labels by gold la-
bels. The performance of classifiers trained on this
sequence is the dashed line ?MTurk selection, gold
labels? in Figure 3 for NER (left) and sentiment
(right).
Second, we compare with a traditional simulated
AL experiment with gold labels. Here, the selection
too is controlled by gold labels, so the selection has
a noiseless classifier available for scoring and can
perform optimal uncertainty selection. These are the
1552
1 5 10 50 100 500
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Number of sentences
Qu
ali
ty 
(%
 co
rre
ct 
en
tity
 to
ke
n
s)
1 2 5 10 20 50 100 200
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Number of documents
Qu
ali
ty 
(%
 co
rre
ct 
do
cu
me
nt 
lab
els
)
Figure 4: Worker accuracy vs. number of HITs. Each point corresponds to one worker (? = active, +
=random sampling; black and grey for different runs). Left: NER. Right: Sentiment.
dotted lines ?gold selection, gold labels? in Figure 3.
We used a batch-mode AL setup for this compari-
son experiment. For a fair comparison, we adjust the
batchsize to be equal to the average staleness of a se-
lected example in concurrent MTurk active learning.
The staleness of an example is defined as the num-
ber of annotations the system has received, but not
yet incorporated in the computation of an example?s
uncertainty score (Haertel et al, 2010).
For our concurrent NER system, the average stal-
eness of an example was about 12 (min: 1, max: 40),
for sentiment it was about 2. The figure for NER is
higher than the number cited by Haertel et al (2010)
because there are more annotators accessing our sys-
tem at the same time via MTurk but not as high for
sentiment since documents are longer and retraining
the sentiment classifier is faster. The average stale-
ness of an example in a batch-mode system is half
the batch size. Thus, we set the batch size of our
comparison system to 25 for NER and to 4 for sen-
timent.
Returning to the two factors introduced above ?
(i) final effect of noise on test set performance vs.
(ii) intermediate effect of noise on example selec-
tion ? we see in Figure 3 that (i) has a large effect
on NER whereas (ii) has a noticeable, but small ef-
fect.3 For example, at 1966 sentences, F1 scores are
3Our comparison unit for NER is the sentence. We can-
not compare on cost here since we do not know what the per-
sentence cost of a ?gold? expert annotation is.
70.6 (MTurk-MTurk), 81.4 (MTurk-gold) and 84.9
(gold-gold). This means that a performance differ-
ence of 10 points F1 has to be attributed to noisy
labels resulting in a worse final classifier (effect i),
and another 3.5 points are lost due to sub-optimal
example selection (effect ii).
For sentiment, the results are different. There is
no clear difference between the three runs. We at-
tribute this to the fact that the quality of the labels
is higher in sentiment than in NER. Our initial ex-
periments on sentiment were all negative (showing
no improvement of AL compared to random) be-
cause label quality was too low. Only after we intro-
duced the template described in Section 3 and used
4-voting with ? = .75 did we get positive results for
AL. This leads to an overall label quality of about
90% (over all runs) which is so high that the differ-
ence to using gold labels is small if present at all.
5 Worker Quality
So far we have assumed that all workers provide
annotations of the same quality. However, this is
not the case. Figure 4 shows plots of worker accu-
racy as a function of worker productivity (number
of annotated examples). Some workers submit only
one or two HITs just to try out the task. For NER,
the majority of workers submit between 5 and 10
sentences, with label qualities between 0.5 and 0.8.
The chance level for correctness is around 0.25 (four
1553
different named entity categories for uppercase to-
kens). For sentiment, most workers submit 1 to 5
documents, with label qualities between 0.5 and 1.
Chance level lies at around 0.5 (for two equally dis-
tributed labels).
While quality for highly productive workers is
mediocre in our experiments, other researchers have
found extremely bad quality for their most prolific
workers (Callison-Burch, 2009). Some of these
workers might be spammers who try to submit an-
swers with automatic scripts. We encountered some
spammers that our heuristics did not detect (shown
in the bottom-right areas of Figure 4, left), but the
voting mechanism was able to mitigate their nega-
tive influence.
Given the large variation in Figure 4, using worker
quality in crowdsourcing for improved training set
creation seems promising. We now test two such
strategies for NER in an oracle setup.
5.1 Blocking low-quality workers
A simple approach is to refuse annotations from
workers that have been determined to provide low
quality answers. We simulated this strategy on NER
data using oracle quality ratings. We chose NER be-
cause of its lower overall label quality. The re-
sults are presented in Figure 5 for random (a) and
AL (b). For random, quality filtering with low cut-
offs helps by removing bad annotations that likely
come from spammers. While the voting strategy
prevented a performance decrease with bad anno-
tations, it needed to expend many extra annotations
for correction. With filtering, these extra annotations
become unnecessary and the system can learn faster.
When low-quality workers are less active, as in the
AL dataset, we find no meaningful performance in-
crease for low cutoffs up to 0.4. For very high cut-
offs (0.7), the beginning of the performance curve
shows that further cost reductions can be achieved.
However, we did not have enough recorded human
annotations available to perform a simulation for the
full budget.
5.2 Trusting high-quality workers
The complementary approach is to take annotations
from highly rated workers at face value and imme-
diately accept them as the correct label, bypassing
the voting procedure. Bypassing saves the cost of
repeated annotation of the same sentence. Figure 5
shows learning curves for two bypass thresholds on
worker quality (measured as proportion of correct
non-O tokens) for random (c) and AL (d). Bypass-
ing performs surprisingly well. We find a steeper
rise of the learning curve, meaning less cost for the
same performance. Not only do we find substantial
cost reductions, but also higher overall performance.
We believe this is because high-quality annotations
can sometimes be voted down by other annotations.
If we can identify high-quality workers and directly
use their annotations, this can be avoided.
These experiments are oracle experiments using
gold data that is normally not available. In future
work, we would like to repeat the experiments using
methods for worker quality estimation (Ipeirotis et
al., 2010; Donmez et al, 2009). For AL, the choice
as to which labels are used (as a result of voting, by-
passing or other) also has an influence on the selec-
tion. However, we had to keep the sequence of the
selected sentences fixed in the simulations reported
above. While our method of sample selection for
AL proved to be quite robust even in the presence
of noise, higher quality labels do have an influence
on the sample selection (see section 4.4), so the im-
provement could be even better than indicated here.
5.3 Differences in quality between AL and
random
The essence of AL is to select examples that are dif-
ficult to classify. As observed in our experiments
on annotation time, this difficulty is reflected in the
amount of time a human needs to work on examples
selected through AL. Another effect to expect from
difficulty could be lower annotation accuracy. We
therefore examined the accuracies for each worker
who contributed to both the AL and the random ex-
periment. We found that in the NER task, the 20
workers in this group had a slightly higher (0.07) av-
erage quality for randomly selected examples. This
difference is low and does not suggest a significant
drop in accuracy for examples selected in AL.
6 Conclusion
We have investigated the use of AL in a real-life
annotation experiment with human annotators in-
stead of traditional simulations with gold labels for
1554
(a) (b) (c) (d)
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
bypass 0.9
bypass 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
bypass 0.9
bypass 0.7
Figure 5: Blocking low-quality workers: (a) random, (b) AL. Bypass voting: (c) random, (d) AL.
named entity recognition and sentiment classifica-
tion. The annotation was performed using MTurk in
an AL framework that features concurrent example
selection without wait times. We also evaluated two
strategies, adaptive voting and fragment recovery, to
improve label quality at low additional cost. We find
that even for the relatively high noise levels of anno-
tations gathered with MTurk, AL is successful, im-
proving performance by +6.9 points F1 compared to
random sampling for NER and by +2.6% accuracy
for sentiment. Furthermore, this performance level
is reached at a smaller MTurk cost compared to ran-
dom sampling. Thus AL not only reduces annotation
costs, but also offers an improvement in absolute
performance for these tasks. This is clear evidence
that active learning and crowdsourcing are comple-
mentary methods for lowering annotation cost and
should be used together in training set creation for
natural language processing tasks.
We have also conducted oracle experiments that
show that further performance gains and cost sav-
ings can be achieved by using information about
worker quality. We plan to confirm these results by
using estimates of quality in the future.
7 Acknowledgments
Florian Laws is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by his fellowship.
Christian Scheible is supported by the Deutsche
Forschungsgemeinschaft project Sonderforschungs-
bereich 732.
References
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based eval-
uation of cost-reduction strategies for language docu-
mentation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 296?305.
Anthony Brew, Derek Greene, and Pa?draig Cunningham.
2010. Using crowdsourcing and active learning to
track sentiment in online media. In Proceeding of the
2010 conference on ECAI 2010: 19th European Con-
ference on Artificial Intelligence, pages 145?150.
Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proceed-
ings of the Twentieth International Conference on Ma-
chine Learning (ICML 2003), pages 59?66.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 286?295.
Bob Carpenter and Massimo Poesio. 2010. Models
of data annotation. Tutorial at the seventh interna-
tional conference on Language Resources and Eval-
uation (LREC 2010).
Nancy Chinchor, David D. Lewis, and Lynette
Hirschman. 1993. Evaluating message understanding
systems: an analysis of the third message understand-
ing conference (muc-3). Computational Linguistics,
19(3):409?449.
Pinar Donmez and Jaime G. Carbonell. 2008. Proactive
learning: cost-sensitive active learning with multiple
imperfect oracles. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 619?628.
Pinar Donmez, Jaime G. Carbonell, and Jeff Schnei-
der. 2009. Efficiently learning the accuracy of la-
1555
beling sources for selective sampling. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 259?
268.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating named entities in twitter data with
crowdsourcing. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 80?88.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ?05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151.
Robbie Haertel, Paul Felt, Eric K. Ringger, and Kevin
Seppi. 2010. Parallel active learning: Eliminating
wait time with minimal staleness. In Proceedings of
the NAACL HLT 2010 Workshop on Active Learning
for Natural Language Processing, pages 33?41.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on amazon mechanical
turk. In Proceedings of the ACM SIGKDD Workshop
on Human Computation (HCOMP ?10).
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large email
datasets for named entity recognition with mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 71?79.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, pages 8?8.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: an introduction. Wiley.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 89?
96.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 949?957.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101?108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235?265.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069?1078.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 254?263.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL (CoNLL 2003), pages 142?147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486?495.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. The Journal of Machine Learning Re-
search, 2:45?66.
Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Han-
nah Copperman. 2010. A hybrid model for anno-
tating named entity training corpora. In Proceedings
of the Fourth Linguistic Annotation Workshop, pages
243?246.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
1556
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 508?512,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Active Learning for Coreference Resolution
Florian Laws1 Florian Heimerl2 Hinrich Schu?tze1
1 Institute for Natural Language Processing (IMS)
Universita?t Stuttgart
florian.laws@ims.uni-stuttgart.de
2 Institute for Visualization and Interactive Systems
Universita?t Stuttgart
florian.heimerl@vis.uni-stuttgart.de
Abstract
We present an active learning method for
coreference resolution that is novel in three re-
spects. (i) It uses bootstrapped neighborhood
pooling, which ensures a class-balanced pool
even though gold labels are not available. (ii)
It employs neighborhood selection, a selection
strategy that ensures coverage of both posi-
tive and negative links for selected markables.
(iii) It is based on a query-by-committee selec-
tion strategy in contrast to earlier uncertainty
sampling work. Experiments show that this
new method outperforms random sampling in
terms of both annotation effort and peak per-
formance.
1 Introduction
Coreference resolution (CR) ? the task of determin-
ing if two expressions in natural language text re-
fer to the same real-world entity ? is an important
NLP task. One popular approach to CR is super-
vised classification. This approach needs manually
labeled training data that is expensive to create. Ac-
tive learning (AL) is a technique that can reduce this
cost by setting up an interactive training/annotation
loop that selects and annotates training examples
that are maximally useful for the classifier that is
being trained. However, while AL has been proven
successful for many other NLP tasks, such as part-
of-speech tagging (Ringger et al, 2007), parsing
(Osborne and Baldridge, 2004), text classification
(Tong and Koller, 2002) and named entity recogni-
tion (Tomanek et al, 2007), AL has not been suc-
cessfully applied to coreference resolution so far.
In this paper, we present a novel approach to AL
for CR based on query-by-committee sampling and
bootstrapping and show that it performs better than
a number of baselines.
2 Related work
Coreference resolution. The perhaps most widely
used supervised learning approach to CR is the
mention-pair model (Soon et al, 2001). This model
classifies links (pairs of two mentions) as corefer-
ent or disreferent, followed by a clustering stage that
partitions entities based on the link decisions. Our
AL method is partially based on the class balancing
strategy proposed by Soon et al (2001).
While models other than mention-pair have been
proposed (Culotta et al, 2007), none performs
clearly better as evidenced by recent shared evalu-
ations such as SemEval 2010 (Recasens et al, 2010)
and CoNLL 2011 (Pradhan et al, 2011).
Active learning. The only existing publication
on AL for CR that we are aware of is (Gasperin,
2009). She uses a mention-pair model on a biomed-
ical corpus. The classifier is Naive Bayes and the
AL method uncertainty sampling (Lewis and Gale,
1994). The results are negative: AL is not bet-
ter than random sampling. In preliminary experi-
ments, we replicated this result for our corpus and
our system: Uncertainty sampling is not better than
random sampling for CR. Uncertainty sampling can
fail if uncertainty assessments are too unstable for
successful example selection (cf. Dwyer and Holte
(2007)). This seems to be the case for the decision
trees we use. Naive Bayes is also known to give bad
uncertainty assessments (Domingos and Pazzani,
508
1997). We therefore adopted a query-by-committee
approach combined with a class-balancing strategy.
3 Active learning for CR
The classifier in the mention-pair model is faced
with a severe class imbalance: there are many more
disreferent than coreferent links. To address this im-
balance, we use a neighborhood pool or N-pool as
proposed by Soon et al (2001).
Generation of the N-pool. The neighborhood
of markable x used in N-pooling is defined as the
set consisting of the link between x and its closest
coreferent markable y(x) to the left and all disref-
erent links in between. For a particular markable x,
let y(x) be the closest coreferent markable for x to
the left of x. Between y(x) and x, there are disref-
erent markables zi, so we have a constellation like
y(x), z1, . . . , zn, x. The neighborhood of x is then
the set of links
{(y, x), (z1, x) . . . , (zn, x)}
This set is empty if x does not have a coreferent
markable to the left.
We call the set of all such neighborhoods the N-
pool. The N-pool is a subset of the entire pool of
links.
Bootstrapping the neighborhood. Soon et al
(2001) introduce N-pooling for labeled data. In AL,
no labeled data (or very little of it) is available. In-
stead, we employ the committee of classifiers that
we use for AL example selection for bootstrapping
the N-pool. We query the committee of classifiers
from the last AL iteration and treat a link as coref-
erent if and only if the majority of the classifiers
classifies it as coreferent. We then construct the N-
pool using these bootstrapped labels to determine
the coreferent markables y(x) and then construct the
neighborhoods as described above.
If this procedure yields no coreferent links in an
iteration, we sample links left of randomly selected
markables instead of N-pooling.
Example selection granularity. We use a query-
by-committee approach to AL. The committee con-
sists of 10 instances of the link classifier of the CR
system, each trained on a randomly chosen subset of
the links that have been manually labeled so far.
In each iteration, the N-pool is recomputed and
a small subset of the N-pool is selected for label-
ing. We experiment with two selection granularities.
In neighborhood selection, entire neighborhoods are
selected and labeled in each iteration. We define the
utility of a neighborhood as the average of the vote
entropies (Argamon-Engelson and Dagan, 1999) of
its links.
In link selection, individual links with the highest
utility are selected ? in most cases these will be from
different neighborhoods. Utility is again defined as
vote entropy.
Our hypothesis is that, compared to selection of
individual links, neighborhood selection yields a
more balanced sample that covers both positive and
negative links for a markable. At the same time,
neighborhood selection retains the benefits of AL
sampling: difficult (or highly informative) links are
selected.
4 Experiments
We use the mention-pair CR system SUCRE (Kob-
dani et al, 2011). The link classifier is a deci-
sion tree and the clustering algorithm a variant of
best-first clustering (Ng and Cardie, 2002). SUCRE
results were competitive in SEMEVAL 2010 (Re-
casens et al, 2010). We implemented N-pool boot-
strapping and selection methods on top of the AL
framework of Tomanek et al (2007).
We use the English part of the SemEval-2010 CR
task data set, a subset of OntoNotes 2.0 (Hovy et al,
2006). Training and test set sizes are about 96,000
and 24,000 words. Since we focus on the coref-
erence resolution subtask, we use the true mention
boundaries for the markables.
The pool for example selection is created by pair-
ing every markable with every preceding markable
within a window of 100 markables. This yields a
pool of 1.7 million links, of which only 1.5% are
labeled as coreferent. This drastic class imbalance
necessitates our bootstrapped class-balancing.
We run two baseline experiments for compari-
son: (i) random selection on the entire pool, with-
out any class balancing, and (ii) random selection
from a gold-label-based N-pool. We chose to use
gold neighborhood information for the baseline to
remove the influence of badly predicted neighbor-
509
20,000 links 50,000 links
MUC B3 CEAF mean MUC B3 CEAF mean
(1) random entire pool 49.68 86.07 82.34 72.70 48.81 86.00 82.24 72.34
(2) N-pooling 61.60 85.00 82.85 76.48 62.60 85.99 83.44 77.33
(3) AL link selection 55.65 86.91? 83.67? 75.41 55.84 86.94? 83.70 75.49
(4) neighborhood sel. 63.07? 86.94? 84.42? 78.14? 63.81? 87.11? 84.33? 78.42?
Table 1: Performance of different methods. All measures are F1 measures.
hoods and focus on the performance of random sam-
pling. Hence, this is a very strong random baseline.
The performance with bootstrapped neighborhoods
would likely be lower.
We run 10 runs of each experiment, starting from
10 different seed sets. These seed sets contained 200
links, drawn randomly from the entire pool, for ran-
dom sampling; and 20 neighborhoods for neighbor-
hood selection, with a comparable number of links.
We verified that each seed set contained instances of
both classes.
5 Results
We determine the performance of CR depending on
the number of links used for training. The results
of the experiments are shown in Table 1 and Fig-
ures 1a to 1d. We show results for four coreference
measures: MUC, B3, entity-based CEAF (hence-
forth: CEAF), and the arithmetic mean of MUC, B3
and CEAF (as suggested by the CoNLL-2011 shared
evaluation).
In all four figures, the AL curves have reached a
plateau at 20,000 links. At this point, neighborhood
selection AL (line 4 in Table 1) outperforms random
sampling from the N-pool (line 2) for all coreference
measures, with gains from 1.47 points for MUC to
1.94 points for B3.
At 20,000 links, the N-pooling random baseline
(line 2) has not yet reached maximum performance,
but even at 50,000 links, neighborhood selection AL
still outperforms the baselines. (AL and baseline
performance will eventually converge when most
links from the pool are sampled, but this will hap-
pen much later, since the pool has 1.7 million links
in total).
?Statistically significant at p < .05 compared to baseline 2
using the sign test (N = 10, k ? 9 successes).
Link selection AL (line 3) outperforms the base-
lines for B3 and CEAF, but is performing markedly
worse than the N-pooling random baseline (line 2)
for MUC (due to low recall for MUC) and mean F1.
Link selection yields a CR system that proposes a
lot of singleton entities that are not coreferent with
any other entity. The MUC scoring scheme does not
give credit to singletons at all, thus the lower recall.
Neighborhood selection AL initially has low
MUC, but starts to outperform the baseline at 15,000
links (Figure 1a). For B3 and CEAF, neighborhood
selection AL outperforms the baselines much ear-
lier, at a few 1000 links (Figures 1b and 1c). It thus
shows more robust performance for all evaluation
metrics.
Neighborhood selection AL also performs at least
as well as (for B3) or better than (MUC and CEAF)
link selection AL. Learning curves of neighborhood
selection AL are consistently above the link selec-
tion curves. We therefore consider neighborhood se-
lection AL to be the preferred AL setup for CR.
6 Conclusion
We have presented a new AL method for corefer-
ence resolution. The proposed method is novel in
three respects. (i) It uses bootstrapped N-pooling,
which ensures a class-balanced pool even though
gold labels are not available. (ii) It further improves
class balancing by neighborhood selection, a selec-
tion strategy that ensures coverage of positive and
negative links per markable while still focusing on
selecting difficult links. (iii) It is based on a query-
by-committee selection strategy in contrast to ear-
lier uncertainty sampling work. Experiments show
that this new method outperforms random sampling
in terms of both annotation effort and peak perfor-
mance.
510
Acknowledgments
Florian Laws is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by his fellowship.
Florian Heimerl was supported by the Deutsche
Forschungsgemeinschaft as part of the priority pro-
gram 1335 ?Scalable Visual Analytics?.
0 10000 30000 50000
0.3
0.4
0.5
0.6
Links sampled
MU
C.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(a) Learning curve for MUC
0 10000 30000 50000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
Links sampled
B3
.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(b) Learning curve for B3
0 10000 30000 50000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
Links sampled
CE
AF
.
E.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(c) Learning curve for CEAF
0 10000 30000 50000
0.6
0
0.6
5
0.7
0
0.7
5
0.8
0
Links sampled
CO
NL
L.M
EA
N.F
Random, entire pool
Random, N?pooling
AL, link selection
AL, neighborhood sel.
(d) Learning curve for the mean of the CR measures.
Figure 1: Learning curves for AL and baseline experiments. All measures are F1 measures.
511
References
S. Argamon-Engelson and I. Dagan. 1999. Committee-
based sample selection for probabilistic classifiers.
JAIR, 11:335?360.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT-NAACL 2007.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple bayesian classifier under zero-
one loss. Mach. Learn., 29(2-3):103?130.
K. Dwyer and R. Holte. 2007. Decision tree instability
and active learning. In ECML.
C. Gasperin. 2009. Active learning for anaphora resolu-
tion. In Proceedings of the NAACL HLT 2009 Work-
shop on Active Learning for Natural Language Pro-
cessing.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In HLT-NAACL.
H. Kobdani, H. Schu?tze, M. Schiehlen, and H. Kamp.
2011. Bootstrapping coreference resolution using
word associations. In ACL.
D. Lewis and W. Gale. 1994. A sequential algorithm for
training text classifiers. In SIGIR.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In HLT-NAACL.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In CoNLL.
M. Recasens, L. Ma`rquez, E. Sapena, M. A. Mart??,
M. Taule?, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In Linguistic Annotation
Workshop at ACL-2007.
W. M. Soon, D. Chung, D. Chung Yong Lim, Y. Lim,
and H. T. Ng. 2001. A machine learning approach
to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4).
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts annota-
tion costs and maintains reusability of annotated data.
In EMNLP-CoNLL.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
JMLR, 2:45?66.
512
