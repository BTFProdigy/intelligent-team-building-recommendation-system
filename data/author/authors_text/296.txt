Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 534?542,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
The role of named entities in Web People Search
Javier Artiles
UNED NLP & IR group
Madrid, Spain
javart@bec.uned.es
Enrique Amig
?
o
UNED NLP & IR group
Madrid, Spain
enrique@lsi.uned.es
Julio Gonzalo
UNED NLP & IR group
Madrid, Spain
julio@lsi.uned.es
Abstract
The ambiguity of person names in the Web
has become a new area of interest for NLP
researchers. This challenging problem has
been formulated as the task of clustering
Web search results (returned in response
to a person name query) according to the
individual they mention. In this paper we
compare the coverage, reliability and in-
dependence of a number of features that
are potential information sources for this
clustering task, paying special attention to
the role of named entities in the texts to
be clustered. Although named entities are
used in most approaches, our results show
that, independently of the Machine Learn-
ing or Clustering algorithm used, named
entity recognition and classification per se
only make a small contribution to solve the
problem.
1 Introduction
Searching the Web for names of people is a highly
ambiguous task, because a single name tends to
be shared by many people. This ambiguity has
recently become an active research topic and, si-
multaneously, in a relevant application domain for
web search services: Zoominfo.com, Spock.com,
123people.com are examples of sites which per-
form web people search, although with limited
disambiguation capabilities.
A study of the query log of the AllTheWeb and
Altavista search sites gives an idea of the relevance
of the people search task: 11-17% of the queries
were composed of a person name with additional
terms and 4% were identified as person names
(Spink et al, 2004). According to the data avail-
able from 1990 U.S. Census Bureau, only 90,000
different names are shared by 100 million people
(Artiles et al, 2005). As the amount of informa-
tion in the WWW grows, more of these people are
mentioned in different web pages. Therefore, a
query for a common name in the Web will usually
produce a list of results where different people are
mentioned.
This situation leaves to the user the task of find-
ing the pages relevant to the particular person he
is interested in. The user might refine the original
query with additional terms, but this risks exclud-
ing relevant documents in the process. In some
cases, the existence of a predominant person (such
as a celebrity or a historical figure) makes it likely
to dominate the ranking of search results, compli-
cating the task of finding information about other
people sharing her name. The Web People Search
task, as defined in the first WePS evaluation cam-
paign (Artiles et al, 2007), consists of grouping
search results for a given name according to the
different people that share it.
Our goal in this paper is to study which doc-
ument features can contribute to this task, and in
particular to find out which is the role that can be
played by named entities (NEs): (i) How reliable
is NEs overlap between documents as a source of
evidence to cluster pages? (ii) How much recall
does it provide? (iii) How unique is this signal?
(i.e. is it redundant with other sources of informa-
tion such as n-gram overlap?); and (iv) How sen-
sitive is this signal to the peculiarities of a given
NE recognition system, such as the granularity of
its NE classification and the quality of its results?
Our aim is to reach conclusions which are are
not tied to a particular choice of Clustering or Ma-
chine Learning algorithms. We have taken two de-
cisions in this direction: first, we have focused on
the problem of deciding whether two web pages
refer to the same individual or not (page corefer-
ence task). This is the kind of relatedness measure
that most clustering algorithms use, but in this way
we can factor out the algorithm and its parameter
settings. Second, we have developed a measure,
Maximal Pairwise Accuracy (PWA) which, given
534
an information source for the problem, estimates
an upper bound for the performance of any Ma-
chine Learning algorithm using this information.
We have used PWA as the basic metric to study the
role of different document features in solving the
coreference problem, and then we have checked
the predictive power of PWA with a Decision Tree
algorithm.
The remainder of the paper is organised as fol-
lows. First, we examine the previous work in Sec-
tion 2. Then we describe the our experimental set-
tings (datasets and features we have used) in Sec-
tion 3 and our empirical study in Section 4. The
paper ends with some conclusions in Section 5.
2 Previous work
In this section we will discuss (i) the state of the
art in Web People Search in general, focusing on
which features are used to solve the problem; and
(ii) lessons learnt from the WePS evaluation cam-
paign where most approaches to the problem have
been tested and compared.
The disambiguation of person names in Web
results is usually compared to two other Natu-
ral Language Processing tasks: Word Sense Dis-
ambiguation (WSD) (Agirre and Edmonds, 2006)
and Cross-document Coreference (CDC) (Bagga
and Baldwin, 1998). Most of early research work
on person name ambiguity focuses on the CDC
problem or uses methods found in the WSD litera-
ture. It is only recently that the web name ambigu-
ity has been approached as a separate problem and
defined as an NLP task - Web People Search - on
its own (Artiles et al, 2005; Artiles et al, 2007).
Therefore, it is useful to point out some crucial
differences between WSD, CRC and WePS:
? WSD typically concentrates in the disam-
biguation of common words (nouns, verbs,
adjectives) for which a relatively small num-
ber of senses exist, compared to the hun-
dreds or thousands of people that can share
the same name.
? WSD can rely on dictionaries to define the
number of possible senses for a word. In the
case of name ambiguity no such dictionary
is available, even though in theory there is an
exact number of people that can be accounted
as sharing the same name.
? The objective of CDC is to reconstruct the
coreference chain for every mention of a per-
son. In Web person name disambiguation it
suffices to group the documents that contain
at least one mention to the same person.
Before the first WePS evaluation campaign in
2007 (Artiles et al, 2007), research on the topic
was not based on a consistent task definition, and
it lacked a standard manually annotated testbed.
In the WePS task, systems were given the top web
search results produced by a person name query.
The expected output was a clustering of these re-
sults, where each cluster should contain all and
only those documents referring to the same indi-
vidual.
2.1 Features for Web People Search
Many different features have been used to repre-
sent documents where an ambiguous name is men-
tioned. The most basic is a Bag of Words (BoW)
representation of the document text. Within-
document coreference resolution has been applied
to produce summaries of text surrounding occur-
rences of the name (Bagga and Baldwin, 1998;
Gooi and Allan, 2004). Nevertheless, the full
document text is present in most systems, some-
times as the only feature (Sugiyama and Okumura,
2007) and sometimes in combination with others -
see for instance (Chen and Martin, 2007; Popescu
and Magnini, 2007)-. Other representations use
the link structure (Malin, 2005) or generate graph
representations of the extracted features (Kalash-
nikov et al, 2007).
Some researchers (Cucerzan, 2007; Nguyen and
Cao, 2008) have explored the use of Wikipedia
information to improve the disambiguation pro-
cess. Wikipedia provides candidate entities that
are linked to specific mentions in a text. The obvi-
ous limitation of this approach is that only celebri-
ties and historical figures can be identified in this
way. These approaches are yet to be applied to the
specific task of grouping search results.
Biographical features are strongly related to
NEs and have also been proposed for this task
due to its high precision. Mann (2003) extracted
these features using lexical patterns to group pages
about the same person. Al-Kamha (2004) used a
simpler approach, based on hand coded features
(e.g. email, zip codes, addresses, etc). In Wan
(2005), biographical information (person name, ti-
tle, organisation, email address and phone num-
ber) improves the clustering results when com-
bined with lexical features (words from the doc-
535
ument) and NE (person, location, organisation).
The most used feature for the Web People
Search task, however, are NEs. Ravin (1999) in-
troduced a rule-based approach that tackles both
variation and ambiguity analysing the structure of
names. In most recent research, NEs (person, lo-
cation and organisations) are extracted from the
text and used as a source of evidence to calculate
the similarity between documents -see for instance
(Blume, 2005; Chen and Martin, 2007; Popescu
and Magnini, 2007; Kalashnikov et al, 2007)-
. For instance, Blume (2005) uses NEs coocur-
ring with the ambiguous mentions of a name as a
key feature for the disambiguation process. Sag-
gion (2008) compared the performace of NEs ver-
sus BoW features. In his experiments a only a
representation based on Organisation NEs outper-
formed the word based approach. Furthermore,
this result is highly dependent on the choice of
metric weighting (NEs achieve high precision at
the cost of a low recall and viceversa for BoW).
In summary, the most common document repre-
sentations for the problem include BoW and NEs,
and in some cases biographical features extracted
from the text.
2.2 Named entities in the WePS campaign
Among the 16 teams that submitted results for the
first WePS campaign, 10 of them
1
used NEs in
their document representation. This makes NEs
the second most common type of feature; only
the BoW feature was more popular. Other fea-
tures used by the systems include noun phrases
(Chen and Martin, 2007), word n-grams (Popescu
and Magnini, 2007), emails and URLs (del Valle-
Agudo et al, 2007), etc. In 2009, the second
WePS campaign showed similar trends regarding
the use of NE features (Artiles et al, 2009).
Due to the complexity of systems, the results
of the WePS evaluation do not provide a direct
answer regarding the advantages of using NEs
over other computationally lighter features such as
BoW or word n-grams. But the WePS campaigns
did provide a useful, standardised resource to per-
form the type of studies that were not possible be-
fore. In the next Section we describe this dataset
and how it has been adapted for our purposes.
1
By team ID: CU-COMSEM, IRST-BP, PSNUS, SHEF,
FICO, UNN, AUG, JHU1, DFKI2, UC3M13
3 Experimental settings
3.1 Data
We have used the testbeds from WePS-1 (Artiles et
al., 2007)
2
and WePS-2 (Artiles et al, 2009) eval-
uation campaigns
3
.
Each WePS dataset consists of 30 test cases: a
random sample of 10 names from the US Cen-
sus, 10 names from Wikipedia, and 10 names from
Programme Committees in the Computer Science
domain (ACL and ECDL). Each test case consists
of, at most, 100 web pages from the top search
results of a web search engine, using a (quoted)
person name as query.
For each test case, annotators were asked to or-
ganise the web pages in groups where all docu-
ments refer to the same person. In cases where
a web page refers to more than one person us-
ing the same ambiguous name (e.g. a web page
with search results from Amazon), the document
is assigned to as many groups as necessary. Doc-
uments were discarded when they did not contain
any useful information about the person being re-
ferred.
Both the WePS-1 and WePS-2 testbeds have
been used to evaluate clustering systems by WePS
task participants, and are now the standard testbed
to test Web People Search systems.
3.2 Features
The evaluated features can be grouped in four
main groups: token-based, n-grams, phrases and
NEs. Wherever possible, we have generated lo-
cal versions of these features that only consider
the sentences of the text that mention the ambigu-
ous person name
4
. Token-based features consid-
ered include document full text tokens, lemmas
(using the OAK analyser, see below), title, snip-
pet (returned in the list of search results) and URL
(tokenised using non alphanumeric characters as
boundaries) tokens. English stopwords were re-
moved, including Web specific stopwords, as file
and domain extensions, etc.
We generated word n-grams of length 2 to 5,
2
The WePS-1 corpus includes data from the Web03
testbed (Mann, 2006) which follows similar annotation
guidelines, although the number of document per ambiguous
name is more variable.
3
Both corpora are available from the WePS website
http://nlp.uned.es/weps
4
A very sparse feature might never occur in a sentence
with the person name. In that cases there is no local version
of the feature.
536
using the sentences found in the document text.
Punctuation tokens (commas, dots, etc) were gen-
eralised as the same token. N-grams were dis-
carded when they were composed only of stop-
words or when they did not contain at least one
token formed by alphanumeric characters (e.g. n-
grams like ?at the? or ?# @?). Noun phrases (us-
ing OAK analyser) were detected in the document
and filtered in a similar way.
Named entities were extracted using two dif-
ferent tools: the Stanford NE Recogniser and the
OAK System
5
.
Stanford NE Recogniser
6
is a high-performance
Named Entity Recognition (NER) system based
on Machine Learning. It provides a general im-
plementation of linear chain Conditional Ran-
dom Field sequence models and includes a model
trained on data from CoNLL, MUC6, MUC7, and
ACE newswire. Three types of entities were ex-
tracted: person, location and organisation.
OAK
7
is a rule based English analyser that in-
cludes many functionalities (POS tagger, stemmer,
chunker, Named Entity (NE) tagger, dependency
analyser, parser, etc). It provides a fine grained
NE recognition covering 100 different NE types
(Sekine, 2008). Given the sparseness of most of
these fine-grained NE types, we have merged them
in coarser groups: event, facility, location, person,
organisation, product, periodx, timex and numex.
We have also used the results of a baseline
NE recognition for comparison purposes. This
method detects sequences of two or more upper-
cased tokens in the text, and discards those that are
found lowercased in the same document or that are
composed solely of stopwords.
Other features are: emails, outgoing links found
in the web pages and two boolean flags that in-
dicate whether a pair of documents is linked or
belongs to the same domain. Because of their
low impact in the results these features haven?t re-
ceived an individual analysis, but they are included
in the ?all features? combination in Figure 7.
5
From the output of both systems we have discarded per-
son NEs made of only one token (these are often first names
that significantly deteriorate the quality of the comparison be-
tween documents).
6
http://nlp.stanford.edu/software/CRF-NER.shtml
7
http://nlp.cs.nyu.edu/oak . OAK was also used to detect
noun phrases and extract lemmas from the text.
4 Experiments and results
4.1 Reformulating WePS as a classification
task
As our goal is to study the impact of different fea-
tures (information sources) in the task, a direct
evaluation in terms of clustering has serious disad-
vantages. Given the output of a clustering system
it is not straightforward to assess why a document
has been assigned to a particular cluster. There are
at least three different factors: the document sim-
ilarity function, the clustering algorithm and its
parameter settings. Features are part of the doc-
ument similarity function, but its performance in
the clustering task depends on the other factors as
well. This makes it difficult to perform error anal-
ysis in terms of the features used to represent the
documents.
Therefore we have decided to transform the
clustering problem into a classification problem:
deciding whether two documents refer to the same
person. Each pair of documents in a name dataset
is considered a classification instance. Instances
are labelled as coreferent (if they share the same
cluster in the gold standard) or non coreferent (if
they do not share the same cluster). Then we
can evaluate the performance of each feature sep-
arately by measuring its ability to rank coreferent
pairs higher and non coreferent pairs lower. In the
case of feature combinations we can study them by
training a classifier or using the maximal pairwise
accuracy methods (explained in Section 4.3).
Each instance (pair of documents) is repre-
sented by the similarity scores obtained using dif-
ferent features and similarity metrics. We have
calculated for each feature three similarity met-
rics: Dice?s coefficient, cosine (using standard
tf.idf weighting) and a measure that simply counts
the size of the intersection set for a given feature
between both documents. After testing these met-
rics we found that Dice provides the best results
across different feature types. Differences be-
tween Dice and cosine were consistent, although
they were not especially large. A possible expla-
nation is that Dice does not take into account the
redundancy of an n-gram or NE in the document,
and the cosine distance does. This can be a cru-
cial factor, for instance, in the document retrieval
by topic; but it doesn?t seem to be the case when
dealing with name ambiguity.
The resulting classification testbed consists of
293,914 instances with the distribution shown in
537
Table 1, where each instance is represented by 69
features.
true false total
WePS1 61,290 122,437 183,727
WePS2 54,641 55,546 110,187
WePS1+WePS2 115,931 177,983 293,914
Table 1: Distribution of classification instances
4.2 Analysis of individual features
There are two main aspects related with the use-
fulness of a feature for WePS task. The first one is
its performance. That is, to what extent the simi-
larity between two documents according to a fea-
ture implies that both mention the same person.
The second aspect is to what extent a feature is or-
thogonal or redundant with respect to the standard
token based similarity.
4.2.1 Feature performance
According to the transformation of WePS cluster-
ing problem into a classification task (described
in Section 4.1), we follow the next steps to study
the performance of individual features. First, we
compute the Dice coefficient similarity over each
feature for all document pairs. Then we rank the
document pair instances according to these simi-
larities. A good feature should rank positive in-
stances on top. If the number of coreferent pairs
in the top n pairs is t
n
and the total number of
coreferent pairs is t, then P =
t
n
n
and R =
t
n
t
. We
plot the obtained precision/recall curves in Figures
1, 2, 3 and 4.
From the figures we can draw the following
conclusions:
First, considering subsets of tokens or lemma-
tised tokens does not outperform the basic token
distance (figure 1 compares token-based features).
We see that only local and snippet tokens perform
slightly better at low recall values, but do not go
beyond recall 0.3.
Second, shallow parsing or n-grams longer than
2 do not seem to be effective, but using bi-grams
improves the results in comparison with tokens.
Figure 2 compares n-grams of different sizes with
noun phrases and tokens. Overall, noun phrases
have a poor performance, and bi-grams give the
best results up to recall 0.7. Four-grams give
slightly better precision but only reach 0.3 recall,
and three-grams do not give better precision than
bi-grams.
Figure 1: Precision/Recall curve of token-based
features
Figure 2: Precision/Recall curve of word n-grams
Third, individual types of NEs do not improve
over tokens. Figure 3 and Figure 4 display the
results obtained by the Stanford and OAK NER
tools respectively. In the best case, Stanford per-
son and organisation named entities obtain results
that match the tokens feature, but only at lower
levels of recall.
Finally, using different NER systems clearly
leads to different results. Surprisingly, the base-
line NE system yields better results in a one to
one comparison, although it must be noted that
this baseline agglomerates different types of en-
538
Figure 3: Precision/Recall curve of NEs obtained
with the Stanford NER tool
Figure 4: Precision/Recall curve of NEs obtained
with the OAK NER tool
tities that are separated in the case of Stanford and
OAK, and this has a direct impact on its recall.
The OAK results are below the tokens and NE
baseline, possibly due to the sparseness of its very
fine grained features. In NE types, cases such as
person and organisation results are still lower than
obtained with Stanford.
4.2.2 Redundancy
In addition to performance, named entities (as well
as other features) are potentially useful for the task
only if they provide information that complements
(i.e. that does not substantially overlap) the basic
token based metric. To estimate this redundancy,
let us consider all document tuples of size four <
a, b, c, d >. In 99% of the cases, token similarity is
different for < a, b > than for < c, d >. We take
combinations such that < a, b > are more similar
to each other than < c, d > according to tokens.
That is:
sim
token
(a, b) > sim
token
(c, d)
Then for any other feature similarity
sim
x
(a, b), we will talk about redundant samples
when sim
x
(a, b) > sim
x
(c, d), non redundant
samples when sim
x
(a, b) < sim
x
(c, d), and
non informative samples when sim
x
(a, b) =
sim
x
(c, d). If all samples are redundant or
non informative, then sim
x
does not provide
additional information for the classification task.
Figure 5 shows the proportion of redundant, non
redundant and non informative samples for sev-
eral similarity criteria, as compared to token-based
similarity. In most cases NE based similarities
give little additional information: the baseline NE
recogniser, which has the largest independent con-
tribution, gives additional information in less than
20% of cases.
In summary, analysing individual features, the
NEs do not outperform BoW in terms of the clas-
sification task. In addition, NEs tend to be re-
dundant regarding BoW. However, if we are able
to combine optimally the contributions of the dif-
ferent features, the BoW approach could be im-
proved. We address this issue in the next section.
Figure 5: Independence of similarity criteria with
respect to the token based feature
539
4.3 Analysis of feature combinations
Up to now we have analysed the usefulness of in-
dividual features for the WePS Task. However,
this begs to ask to what extent the NE features can
contribute to the task when they are combined to-
gether and with token and n-gram based features.
First, we use each feature combinations as the in-
put for a Machine Learning algorithm. In particu-
lar, we use a Decision Tree algorithm and WePS-1
data for training and WePS-2 data for testing. The
Decision Tree algorithm was chosen because we
have a small set of features to train (similarity met-
rics) and some of these features output Boolean
values.
Results obtained with this setup, however, can
be dependent on the choice of the ML approach.
To overcome this problem, in addition to the re-
sults of a Decision Tree Machine Learning algo-
rithm, we introduce a Maximal Pairwise Accuracy
(MPA) measure that provides an upper bound for
any machine learning algorithm using a feature
combination.
We can estimate the performance of an individ-
ual similarity feature x in terms of accuracy. It
is considered a correct answer when the similarity
x(a, a
?
) between two pages referring to the same
person is higher than the similarity x(b, c) between
two pages referring to different people. Let us
call this estimation Pairwise Accuracy. In terms
of probability it can be defined as:
PWA = Prob(x(a, a
?
) > x(c, d))
PWA is defined over a single feature (similar-
ity metric). When considering more than one sim-
ilarity measure, the results depend on how mea-
sures are weighted. In that case we assume that
the best possible weighting is applied. When com-
bining a set of features X = {x
1
. . . x
n
}, a per-
fect Machine Learning algorithm would learn to
always ?listen? to the features giving correct in-
formation and ignore the features giving erroneous
information. In other words, if at least one feature
gives correct information, then the perfect algo-
rithm would produce a correct output. This is what
we call the Maximal Pairwise Accuracy estimation
of an upper bound for any ML system using the set
of features X:
MaxPWA(X) =
Prob(?x ? X.x(a, a
?
) > x(c, d))
Figure 6: Estimated PWA upper bound versus the
real PWA of decision trees trained with feature
combinations
Figure 7: Maximal Pairwise Accuracy vs. results
of a Decision Tree
The upper bound (MaxPWA) of feature combi-
nations happens to be highly correlated with the
PWA obtained by the Decision Tree algorithm (us-
ing its confidence values as a similarity metric).
Figure 6 shows this correlation for several features
combinations. This is an indication that the Deci-
sion Tree is effectively using the information in the
feature set.
Figure 7 shows the PWA upper bound estima-
tion and the actual PWA performance of a Deci-
sion Tree ML algorithm for three combinations:
(i) all features; (ii) non linguistic features, i.e.,
features which can be extracted without natural
language processing machinery: tokens, url, title,
snippet, local tokens, n-grams and local n-grams;
and (iii) just tokens. The results show that accord-
ing to both the Decision Tree results and the upper-
bound (MaxPWA), adding new features to tokens
improves the classification. However, taking non-
linguistic features obtains similar results than tak-
ing all features. Our conclusion is that NE features
are useful for the task, but do not seem to offer a
540
competitive advantage when compared with non-
linguistic features, and are more computationally
expensive. Note that we are using NE features in a
direct way: our results do not exclude the possibil-
ity of effectively exploiting NEs in more sophisti-
cated ways, such as, for instance, exploiting the
underlying social network relationships between
NEs in the texts.
4.3.1 Results on the clustering task
In order to validate our results, we have tested
whether the classifiers learned with our feature
sets lead to competitive systems for the full clus-
tering task. In order to do so, we use the output of
the classifiers as similarity metrics for a particu-
lar clustering algorithm, using WePS-1 to train the
classifiers and WePS-2 for testing.
We have used a Hierarchical Agglomerative
Clustering algorithm (HAC) with single linkage,
using the classifier?s confidence value in the nega-
tive answer for each instance as a distance metric
8
between document pairs. HAC is the algorithm
used by some of the best performing systems in the
WePS-2 evaluation. The distance threshold was
trained using the WePS-1 data. We report results
with the official WePS-2 evaluation metrics: ex-
tended B-Cubed Precision and Recall (Amig?o et
al., 2008).
Two Decision Tree models were evaluated: (i)
ML-ALL is a model trained using all the available
features (which obtains 0.76 accuracy in the clas-
sification task) (ii) ML-NON LING was trained
with all the features except for OAK and Stanford
NEs, noun phrases, lemmas and gazetteer features
(which obtains 0.75 accuracy in the classification
task). These are the same classifiers considered in
Figure 7.
Table 2 shows the results obtained in the clus-
tering task by the two DT models, together with
the four top scoring WePS-2 systems and the av-
erage values for all WePS-2 systems. We found
that a ML based clustering using only non linguis-
tic information slightly outperforms the best par-
ticipant in WePS-2. Surprisingly, adding linguis-
tic information (NEs, noun phrases, etc.) has a
small negative impact on the results (0.81 versus
0.83), although the classifier with linguistic infor-
mation was a bit better than the non-linguistic one.
This seems to be another indication that the use of
8
The DT classifier output consists of two confidence val-
ues, one for the positive and one for the negative answer, that
add up to 1.0 .
noun phrases and other linguistic features to im-
prove the task is non-obvious to say the least.
B-Cubed
run F-? =
0.5
Pre. Rec.
ML-NON LING .83 .91 .77
S-1 .82 .87 .79
ML- ALL .81 .89 .76
S-2 .81 .85 .80
S-3 .81 .93 .73
S-4 .72 .82 .66
WePS-2 systems aver. .61 .74 .63
Table 2: Evaluation on the WePS-2 clustering task
5 Conclusions
We have presented an empirical study that tries to
determine the potential role of several sources of
information to solve the Web People Search clus-
tering problem, with a particular focus on studying
the role of named entities in the task.
To abstract the study from the particular choice
of a clustering algorithm and a parameter set-
ting, we have reformulated the problem as a co-
reference classification task: deciding whether
two pages refer to the same person or not. We
have also proposed the Maximal Pairwise Accu-
racy estimation that establish an upper bound for
the results obtained by any Machine Learning al-
gorithm using a particular set of features.
Our results indicate that (i) NEs do not provide a
substantial competitive advantage in the clustering
process when compared to a rich combination of
simpler features that do not require linguistic pro-
cessing (local, global and snippet tokens, n-grams,
etc.); (ii) results are sensitive to the NER system
used: when using all NE features for training, the
richer number of features provided by OAK seems
to have an advantage over the simpler types in
Stanford NER and the baseline NER system.
This is not exactly a prescription against the use
of NEs for Web People Search, because linguistic
knowledge can be useful for other aspects of the
problem, such as visualisation of results and de-
scription of the persons/clusters obtained: for ex-
ample, from a user point of view a network of the
connections of a person with other persons and or-
ganisations (which can only be done with NER)
can be part of a person?s profile and may help as
a summary of the cluster contents. But from the
perspective of the clustering problem per se, a di-
rect use of NEs and other linguistic features does
not seem to pay off.
541
Acknowledgments
This work has been partially supported by the
Regional Government of Madrid, project MAVIR
S0505-TIC0267.
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Reema Al-Kamha and David W. Embley. 2004.
Grouping search-engine returned citations for
person-name queries. In WIDM ?04: Proceedings of
the 6th annual ACM international workshop on Web
information and data management. ACM Press.
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2008. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information Retrieval.
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2005.
A testbed for people searching strategies in the
www. In SIGIR.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The semeval-2007 weps evaluation: Estab-
lishing a benchmark for the web people search task.
In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007).
ACL.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009. Weps 2 evaluation campaign: overview of
the web people search clustering task. In WePS 2
Evaluation Workshop. WWW Conference 2009.
Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 17th inter-
national conference on Computational linguistics.
ACL.
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to ner, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis.
Ying Chen and James H. Martin. 2007. Cu-comsem:
Exploring rich features for unsupervised web per-
sonal name disambiguation. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations. ACL.
Silviu Cucerzan. 2007. Large scale named entity
disambiguation based on wikipedia data. In The
EMNLP-CoNLL-2007.
David del Valle-Agudo, C?esar de Pablo-S?anchez, and
Mar??a Teresa Vicente-D??ez. 2007. Uc3m-13: Dis-
ambiguation of person names based on the compo-
sition of simple bags of typed terms. In Proceedings
of the Fourth International Workshop on Semantic
Evaluations. ACL.
Chung Heong Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
HLT-NAACL.
Dmitri V. Kalashnikov, Stella Chen, Rabia Nuray,
Sharad Mehrotra, and Naveen Ashish. 2007. Dis-
ambiguation algorithm for people search on the web.
In Proc. of IEEE International Conference on Data
Engineering (IEEE ICDE).
Bradley Malin. 2005. Unsupervised name disam-
biguation via social network similarity. In Workshop
on Link Analysis, Counterterrorism, and Security.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceed-
ings of the seventh conference on Natural Language
Learning (CoNLL) at HLT-NAACL 2003. ACL.
Gideon S. Mann. 2006. Multi-Document Statistical
Fact Extraction and Fusion. Ph.D. thesis, Johns
Hopkins University.
Hien T. Nguyen and Tru H. Cao, 2008. Named En-
tity Disambiguation: A Hybrid Statistical and Rule-
Based Incremental Approach. Springer.
Octavian Popescu and Bernardo Magnini. 2007. Irst-
bp: Web people search using name entities. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations. ACL.
Y. Ravin and Z. Kazi. 1999. Is hillary rodham clinton
the president? disambiguating names across docu-
ments. In Proceedings of the ACL ?99 Workshop
on Coreference and its Applications Association for
Computational Linguistics.
Horacio Saggion. 2008. Experiments on semantic-
based clustering for cross-document coreference. In
International Joint Conference on Natural language
Processing.
Satoshi Sekine. 2008. Extended named entity on-
tology with attribute information. In Proceedings
of the Sixth International Language Resources and
Evaluation (LREC?08).
Amanda Spink, Bernard Jansen, and Jan Pedersen.
2004. Searching for people on web search engines.
Journal of Documentation, 60:266 ? 278.
Kazunari Sugiyama and Manabu Okumura. 2007.
Titpi: Web people search task using semi-supervised
clustering approach. In Proceedings of the Fourth
International Workshop on Semantic Evaluations.
ACL.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong
Ding. 2005. Person resolution in person search re-
sults: Webhawk. In CIKM ?05: Proceedings of the
14th ACM international conference on Information
and knowledge management. ACM Press.
542
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 361?364,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
The Impact of Query Refinement in the Web People Search Task
Javier Artiles
UNED NLP & IR group
Madrid, Spain
javart@bec.uned.es
Julio Gonzalo
UNED NLP & IR group
Madrid, Spain
julio@lsi.uned.es
Enrique Amig
?
o
UNED NLP & IR group
Madrid, Spain
enrique@lsi.uned.es
Abstract
Searching for a person name in a Web
Search Engine usually leads to a number
of web pages that refer to several people
sharing the same name. In this paper we
study whether it is reasonable to assume
that pages about the desired person can be
filtered by the user by adding query terms.
Our results indicate that, although in most
occasions there is a query refinement that
gives all and only those pages related to
an individual, it is unlikely that the user is
able to find this expression a priori.
1 Introduction
The Web has now become an essential resource
to obtain information about individuals but, at the
same time, its growth has made web people search
(WePS) a challenging task, because every single
name is usually shared by many different peo-
ple. One of the mainstream approaches to solve
this problem is designing meta-search engines that
cluster search results, producing one cluster per
person which contains all documents referring to
this person.
Up to now, two evaluation campaigns ? WePS 1
in 2007 (Artiles et al, 2007) and WePS 2 in 2009
(Artiles et al, 2009) ? have produced datasets for
this clustering task, with over 15 research groups
submitting results in each campaign. Since the re-
lease of the first datasets, this task is becoming an
increasingly popular research topic among Infor-
mation Retrieval and Natural Language Process-
ing researchers.
For precision oriented queries (for instance,
finding the homepage, the email or the phone num-
ber of a given person), clustered results might help
locating the desired data faster while avoiding con-
fusion with other people sharing the same name.
But the utility of clustering is more obvious for re-
call oriented queries, where the goal is to mine the
web for information about a person. In a typical
hiring process, for instance, candidates are eval-
uated not only according to their cv, but also ac-
cording to their web profile, i.e. information about
them available in the Web.
One question that naturally arises is whether
search results clustering can effectively help users
for this task. Eventually, a query refinement made
by the user ? for instance, adding an affiliation or
a location ? might have the desired disambigua-
tion effect without compromising recall. The hy-
pothesis underlying most research on Web People
Search is that query refinement is risky, because it
can enhance precision but it will usually harm re-
call. Adding the current affiliation of a person, for
instance, might make information about previous
jobs disappear from search results.
This hypothesis has not, up to now, been em-
pirically confirmed, and it is the goal of this pa-
per. We want to evaluate the actual impact of us-
ing query refinements in the Web People Search
(WePS) clustering task (as defined in the frame-
work of the WePS evaluation). For this, we have
studied to what extent a query refinement can suc-
cessfully filter relevant results and which type of
refinements are the most successful. In our ex-
periments we have considered the search results
associated to one individual as a set of relevant
documents, and we have tested the ability of dif-
ferent query refinement strategies to retrieve those
documents. Our results are conclusive: in most
occasions there is a ?near-perfect? refinement that
filters out most relevant information about a given
person, but this refinement is very hard to predict
from a user?s perspective.
In Section 2 we describe the datasets that where
used for our experiments. The experimental
methodology and results are presented in Section
3. Finally we present our conclusions in 4.
361
2 Dataset
2.1 The WePS-2 corpus
For our experiments we have used the WePS-2
testbed (Artiles et al, 2009)
1
. It consists of 30
datasets, each one related to one ambiguous name:
10 names were sampled from the US Census, 10
from Wikipedia, and 10 from the Computer Sci-
ence domain (Programme Committee members of
the ACL 2008 Conference). Each dataset consists
of, at most, 100 web pages written in English and
retrieved as the top search results of a web search
engine, using the (quoted) person name as query
2
.
Annotators were asked to organize the web
pages from each dataset in groups where all docu-
ments refer to the same person. For instance, the
?James Patterson? web results were gruped in four
clusters according to the four individuals men-
tioned with that name in the documents. In cases
where a web page refers to more than one person
using the same ambiguous name (e.g. a web page
with search results from Amazon), the document
is assigned to as many groups as necessary. Doc-
uments were discarded when there wasn?t enough
information to cluster them correctly.
2.2 Query refinement candidates
In order to generate query refinement candidates,
we extracted several types of features from each
document. First, we applied a simple preprocess-
ing to the HTML documents in the corpus, con-
verting them to plain text and tokenizing. Then,
we extracted tokens and word n-grams for each
document (up to four words lenght). A list of En-
glish stopwords was used to remove tokens and n-
grams beginning or ending with a stopword. Using
the Stanford Named Entity Recognition Tool
3
we
obtained the lists of persons, locations and organi-
zations mentioned in each document.
Additionally, we used attributes manually an-
notated for the WePS-2 Attribute Extraction Task
(Sekine and Artiles, 2009). These are person
attributes (affiliation, occupation, variations of
name, date of birth, etc.) for each individual shar-
ing the name searched. These attributes emulate
the kind of query refinements that a user might try
in a typical people search scenario.
1
http://nlp.uned.es/weps
2
We used the Yahoo! search service API.
3
http://nlp.stanford.edu/software/CRF-NER.shtml
field F prec. recall cover.
ae affiliation 0.99 0.98 1.00 0.46
ae award 1.00 1.00 1.00 0.04
ae birthplace 1.00 1.00 1.00 0.09
ae degree 0.85 0.80 1.00 0.10
ae email 1.00 1.00 1.00 0.11
ae fax 1.00 1.00 1.00 0.06
ae location 0.99 0.99 1.00 0.27
ae major 1.00 1.00 1.00 0.07
ae mentor 1.00 1.00 1.00 0.03
ae nationality 1.00 1.00 1.00 0.01
ae occupation 0.95 0.93 1.00 0.48
ae phone 0.99 0.99 1.00 0.13
ae relatives 0.99 0.98 1.00 0.15
ae school 0.99 0.99 1.00 0.15
ae work 0.96 0.95 1.00 0.07
stf location 0.96 0.95 1.00 0.93
stf organization 1.00 1.00 1.00 0.98
stf person 0.98 0.97 1.00 0.82
tokens 1.00 1.00 1.00 1.00
bigrams 1.00 1.00 1.00 0.98
trigrams 1.00 1.00 1.00 1.00
fourgrams 1.00 1.00 1.00 0.98
fivegrams 1.00 1.00 1.00 0.98
Table 1: Results for clusters of size 1
field F prec. recall cover.
ae affiliation 0.76 0.99 0.65 0.40
ae award 0.67 1.00 0.50 0.02
ae birthplace 0.67 1.00 0.50 0.10
ae degree 0.63 0.87 0.54 0.15
ae email 0.74 1.00 0.60 0.16
ae fax 0.67 1.00 0.50 0.09
ae location 0.77 1.00 0.66 0.32
ae major 0.71 1.00 0.56 0.09
ae mentor 0.75 1.00 0.63 0.04
ae nationality 0.67 1.00 0.50 0.01
ae occupation 0.76 0.98 0.65 0.52
ae phone 0.75 1.00 0.63 0.13
ae relatives 0.78 0.96 0.68 0.15
ae school 0.68 0.96 0.56 0.17
ae work 0.81 1.00 0.72 0.17
stf location 0.83 0.97 0.77 0.98
stf organization 0.89 1.00 0.83 1.00
stf person 0.83 0.99 0.74 0.98
tokens 0.96 0.99 0.94 1.00
bigrams 0.95 1.00 0.92 1.00
trigrams 0.94 1.00 0.92 1.00
fourgrams 0.91 1.00 0.86 0.99
fivegrams 0.89 1.00 0.84 0.99
Table 2: Results for clusters of size 2
field F prec. recall cover.
ae affiliation 0.51 0.96 0.39 0.81
ae award 0.26 1.00 0.16 0.20
ae birthplace 0.33 0.99 0.24 0.28
ae degree 0.37 0.90 0.26 0.36
ae email 0.35 0.96 0.23 0.33
ae fax 0.30 1.00 0.19 0.15
ae location 0.34 0.96 0.23 0.64
ae major 0.30 0.97 0.20 0.22
ae mentor 0.23 0.95 0.15 0.22
ae nationality 0.36 0.88 0.26 0.16
ae occupation 0.52 0.93 0.40 0.80
ae phone 0.34 0.96 0.23 0.33
ae relatives 0.32 0.95 0.22 0.16
ae school 0.40 0.95 0.29 0.43
ae work 0.45 0.94 0.34 0.38
stf location 0.62 0.87 0.53 1.00
stf organization 0.67 0.96 0.56 1.00
stf person 0.59 0.95 0.47 1.00
tokens 0.87 0.90 0.86 1.00
bigrams 0.79 0.95 0.70 1.00
trigrams 0.75 0.96 0.65 1.00
fourgrams 0.67 0.97 0.55 1.00
fivegrams 0.62 0.96 0.50 1.00
Table 3: Results for clusters of size >=3
3 Experiments
In our experiments we consider each set of doc-
uments (cluster) related to one individual in the
WePS corpus as a set of relevant documents for
a person search. For instance the James Patter-
362
field F prec. recall cover.
best-ae 1.00 0.99 1.00 0.74
best-all 1.00 1.00 1.00 1.00
best-ner 1.00 1.00 1.00 0.99
best-nl 1.00 1.00 1.00 1.00
Table 4: Results for clusters of size 1
field F prec. recall cover.
best-ae 0.77 1.00 0.65 0.79
best-all 0.95 1.00 0.93 1.00
best-ner 0.92 0.99 0.88 1.00
best-nl 0.96 1.00 0.94 1.00
Table 5: Results for clusters of size 2
field F prec. recall cover.
best-ae 0.60 0.97 0.47 0.92
best-all 0.89 0.96 0.85 1.00
best-ner 0.74 0.95 0.63 1.00
best-nl 0.89 0.95 0.85 1.00
Table 6: Results for clusters of size >=3
son dataset in the WePS corpus contains a total of
100 documents, and 10 of them belong to a British
politician named James Patterson. The WePS-2
corpus contains a total of 552 clusters that were
used to evaluate the different types of QRs.
For each person cluster, our goal is to find the
best query refinements; in an ideal case, an expres-
sion that is present in all documents in the clus-
ter, and not present in documents outside the clus-
ter. For each QR type (affiliation, e-mail, n-grams
of various sizes, etc.) we consider all candidates
found in at least one document from the cluster,
and pick up the one that leads to the best harmonic
mean (F
?=.5
) of precision and recall on the cluster
documents (there might be more than one).
For instance, when we evaluate a set of token
QR candidates for the politician in the James Pat-
terson dataset we find that among all the tokens
that appear in the documents of its cluster, ?repub-
lican? gives us a perfect score, while ?politician?
obtains a low precision (we retrieve documents of
other politicians named James Patterson).
In some cases a cluster might not have any can-
didate for a particular type of QR. For instance,
manual person attributes like phone number are
sparse and won?t be available for every individual,
whereas tokens and ngrams are always present.
We exclude those cases when computing F, and
instead we report a coverage measure which rep-
resents the number of clusters which have at least
one candidate of this type of QR. This way we
know how often we can use an attribute (coverage)
field 1 2 >=3
ae affiliation 20.96 17.88 29.41
ae occupation 20.25 21.79 24.60
ae work 3.23 8.38 8.56
ae location 12.66 12.29 8.02
ae school 7.03 6.70 6.42
ae degree 3.23 3.91 5.35
ae email 5.34 6.15 4.28
ae phone 6.19 5.03 3.21
ae nationality 0.28 0.00 3.21
ae relatives 7.03 5.03 2.67
ae birthplace 4.22 5.03 1.60
ae fax 2.95 1.68 1.60
ae major 3.52 3.91 1.07
ae mentor 1.41 2.23 0.00
ae award 1.69 0.00 0.00
Table 7: Distribution of the person attributes used
for the ?best-ae? strategy
and how useful it is when available (F measure).
These figures represent a ceiling for each type
of query refinement: they represent the efficiency
of the query when the user selects the best possible
refinement for a given QR type.
We have split the results in three groups depend-
ing on the size of the target cluster: (i) rare people,
mentioned in only one document (335 clusters of
size 1); (ii)people that appear in two documents
(92 clusters of size 2), often these documents be-
long to the same domain, or are very similar; and
(iii) all other cases (125 clusters of size >=3).
We also report on the aggregated results for cer-
tain subsets of QR types. For instance, if we want
to know what results will get a user that picks the
best person attribute, we consider all types of at-
tributes (e-mail, affiliation, etc.) for every cluster,
and pick up the ones that lead to the best results.
We consider four groups: (i) best-all selects the
best QR among all the available QR types (ii) best-
ae considers all manually annotated attributes (iii)
best-ner considers automatically annotated NEs;
and (iv) best-ng uses only tokens and ngrams.
3.1 Results
The results of the evaluation for each cluster size
(one, two, more than two) are presented in Ta-
bles 1, 2 and 3. These tables display results for
each QR type. Then Tables 4, 5 and 6 show the
results for aggregated QR types.
Two main results can be highlighted: (i) The
best overall refinement is, in average, very good
(F = .89 for clusters of size ? 3). In other words,
there is usually at least one QR that leads to (ap-
proximately) the desired set of results; (ii) this best
363
refinement, however, is not necessarily an intu-
itive choice for the user. One would expect users
to refine the query with a person?s attribute, such
as his affiliation or location. But the results for
the best (manually extracted) attribute are signifi-
cantly worse (F = .60 for clusters of size ? 3),
and they cannot always be used (coverage is .74,
.79 and .92 for clusters of size 1, 2 and ? 3).
The manually tagged attributes from WePS-2
are very precise, although their individual cover-
age over the different person clusters is generally
low. Affiliation and occupation, which are the
most frequent, obtain the largest coverage (0.81
and 0.80 for sizes ? 3). Also the recall of this
type of QRs is low in clusters of two, three or more
documents. When evaluating the ?best-ae? strat-
egy we found that in many clusters there is at least
one manual attribute that can be used as QR with
high precision. This is the case mostly for clusters
of three or more documents (0.92 coverage) and it
decreases with smaller clusters, probably because
there is less information about the person and thus
less biographical attributes are to be found.
In Table 7 we show the distribution of the actual
QR types selected by the ?best-ae? strategy. The
best type is affiliation, which is selected in 29%
of the cases. Affiliation and occupation together
cover around half of the cases (54%), and the rest
is a long tail where each attribute makes a small
contribution to the total. Again, this is a strong
indication that the best refinement is probably very
difficult to predict a priori for the user.
Automatically recognized named entities in the
documents obtain better results, in general, than
manually tagged attributes. This is probably due
to the fact that they can capture all kinds of related
entities, or simply entities that happen to coocur
with the person name. For instance, the pages of a
university professor that is usually mentioned to-
gether with his PhD students could be refined with
any of their names. This goes to show that a good
QR can be any information related to the person,
and that we might need to know the person very
well in advance in order to choose this QR.
Tokens and ngrams give us a kind of ?upper
boundary? of what is possible to achieve using
QRs. They include almost anything that is found
in the manual attributes and the named entities.
They also frequently include QRs that are not re-
alistic for a human refinement. For instance, in
clusters of only two documents it is not uncom-
mon that both pages belong to the same domain
or that they are near duplicates. In those cases to-
kens and ngram QR will probably include non in-
formative strings. In some cases the QRs found
are neither directly biographical or related NEs,
but topical information (e.g. the term ?soccer? in
the pages of a football player or the ngram ?align-
ment via structured multilabel? that is the title of a
paper written by a Computer Science researcher).
These cases widen even more the range of effec-
tive QRs. The overall results of using tokens and
ngrams are almost perfect for all clusters, but at
the cost of considering every possible bit of infor-
mation about the person or even unrelated text.
4 Conclusions
In this paper we have studied the potential effects
of using query refinements to perform the Web
People Search task. We have shown that although
in theory there are query refinements that perform
well to retrieve the documents of most individuals,
the nature of these ideal refinements varies widely
in the studied dataset, and there is no single in-
tuitive strategy leading to robust results. Even if
the attributes of the person are well known before-
hand (which is hardly realistic, given that in most
cases this is precisely the information needed by
the user), there is no way of anticipating which
expression will lead to good results for a particu-
lar person. These results confirm that search re-
sults clustering might indeed be of practical help
for users in Web people search.
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The semeval-2007 weps evaluation: Estab-
lishing a benchmark for the web people search task.
In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007).
ACL.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009. Weps 2 evaluation campaign: overview of
the web people search clustering task. In WePS 2
Evaluation Workshop. WWW Conference 2009.
Satoshi Sekine and Javier Artiles. 2009. Weps2 at-
tribute extraction task. In 2nd Web People Search
Evaluation Workshop (WePS 2009), 18th WWW
Conference.
364
Word Sense Disambiguation based on
Term to Term Similarity in a Context Space
Javier Artiles
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
javart@bec.uned.es
Anselmo Pen?as
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
anselmo@lsi.uned.es
Felisa Verdejo
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
felisa@lsi.uned.es
Abstract
This paper describes the exemplar based ap-
proach presented by UNED at Senseval-3. In-
stead of representing contexts as bags of terms
and defining a similarity measure between con-
texts, we propose to represent terms as bags
of contexts and define a similarity measure be-
tween terms. Thus, words, lemmas and senses
are represented in the same space (the context
space), and similarity measures can be defined
between them. New contexts are transformed
into this representation in order to calculate
their similarity to the candidate senses. We
show how standard similarity measures obtain
better results in this framework. A new similar-
ity measure in the context space is proposed for
selecting the senses and performing disambigua-
tion. Results of this approach at Senseval-3 are
here reported.
1 Introduction
Word Sense Disambiguation (WSD) is the task
of deciding the appropriate sense for a partic-
ular use of a polysemous word, given its tex-
tual or discursive context. A previous non triv-
ial step is to determine the inventory of mean-
ings potentially attributable to that word. For
this reason, WSD in Senseval is reformulated as
a classification problem where a dictionary be-
comes the class inventory. The disambiguation
process, then, consists in assigning one or more
of these classes to the ambiguous word in the
given context. The Senseval evaluation forum
provides a controlled framework where different
WSD systems can be tested and compared.
Corpus-based methods have offered encour-
aging results in the last years. This kind of
methods profits from statistics on a training
corpus, and Machine Learning (ML) algorithms
to produce a classifier. Learning algorithms
can be divided in two main categories: Super-
vised (where the correct answer for each piece of
training is provided) and Unsupervised (where
the training data is given without any answer
indication). Tests at Senseval-3 are made in
various languages for which two main tasks are
proposed: an all-words task and a lexical sam-
ple task. Participants have available a training
corpus, a set of test examples and a sense inven-
tory in each language. The training corpora are
available in a labelled and a unlabelled format;
the former is mainly for supervised systems and
the latter mainly for the unsupervised ones.
Several supervised ML algorithms have been
applied to WSD (Ide and Ve?ronis, 1998), (Es-
cudero et al, 2000): Decision Lists, Neural Net-
works, Bayesian classifiers, Boosting, Exemplar-
based learning, etc. We report here the
exemplar-based approach developed by UNED
and tested at the Senseval-3 competition in the
lexical sample tasks for English, Spanish, Cata-
lan and Italian.
After this brief introduction, Sections 2 and
3 are devoted, respectively, to the training data
and the processing performed over these data.
Section 4 characterizes the UNED WSD system.
First, we describe the general approach based on
the representation of words, lemmas and senses
in a Context Space. Then, we show how results
are improved by applying standard similarity
measures as cosine in this Context Space. Once
the representation framework is established, we
define the criteria underlying the final similar-
ity measure used at Senseval-3, and we com-
pare it with the previous similarity measures.
Section 5 reports the official results obtained at
the Senseval-3 Lexical Sample tasks for English,
Spanish, Italian and Catalan. Finally, we con-
clude and point out some future work.
2 Data
Each Lexical Sample Task has a relatively large
training set with disambiguated examples. The
test examples set has approximately a half of
the number of the examples in the training data.
Each example offers an ambiguous word and its
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
surrounding context, where the average context
window varies from language to language. Each
training example gives one or more semantic la-
bels for the ambiguous word corresponding to
the correct sense in that context.
Senseval-3 provided the training data and the
test data in XML format. The XML tagging
conventions provides an excellent ground for the
corpora processing, allowing a simple way for
the data browsing and transformation. How-
ever, some of the XML well-formedness con-
straints are not completely satisfied. For exam-
ple, there is no XML declaration and no root
element in the English Lexical Sample docu-
ments. Once these shortcomings are fixed any
XML parser can normally read and process the
data.
Despite the similarity in the structure of the
different corpora at the lexical sample task
in different languages, we had found a het-
erogeneous vocabulary both in the XML tags
and the attributes, forcing to develop ?ad hoc?
parsers for each language. We missed a common
and public document type definition for all the
tasks.
Sense codification is another field where dif-
ferent solutions had been taken. In the English
corpus nouns and adjectives are annotated using
the WordNet 1.7.1. classification1 (Fellbaum,
1998), while the verbs are based on Wordsmyth2
(Scott, 1997). In the Catalan and Spanish tasks
the sense inventory gives a more coarse-grained
classification than WordNet. Both tasks have
provided a dictionary with additional informa-
tion as examples, typical collocations and the
equivalent synsets at WordNet 1.5. Finally, the
Italian sense inventory is based on the Multi-
Wordnet dictionary3 (Pianta et al, 2002). Un-
like the other mentioned languages , the Italian
task doesn?t provide a separate file with the dic-
tionary.
Besides the training data provided by Sen-
seval, we have used the SemCor (Miller et al,
1993) collection in which every word is already
tagged in its part of speech, sense and synset of
WordNet.
3 Preprocessing
A tokenized version of the Catalan, Spanish and
Italian corpora has been provided. In this ver-
sion every word is tagged with its lemma and
1http://www.cogsci.princeton.edu/ wn/
2http://www.wordsmyth.net
3http://multiwordnet.itc.it/
part of speech tag. This information has been
manually annotated by human assessors both in
the Catalan and the Spanish corpora. The Ital-
ian corpus has been processed automatically by
the TnT POStagger4 (Brants, 2000) including
similar tags.
The English data lacked of this information,
leading us to apply the TreeTagger5 (Schmid,
1994) tool to the training and test data as a
previous step to the disambiguation process.
Since the SemCor collection is already tagged,
the preprocessing consisted in the segmentation
of texts by the paragraph tag, obtaining 5382
different fragments. Each paragraph of Semcor
has been used as a separate training example
for the English lexical sample task. We applied
the mapping provided by Senseval to represent
verbs according to the verb inventory used in
Senseval-3.
4 Approach
The supervised UNED WSD system is an ex-
emplar based classifier that performs the disam-
biguation task measuring the similarity between
a new instance and the representation of some
labelled examples. However, instead of repre-
senting contexts as bags of terms and defining
a similarity measure between the new context
and the training contexts, we propose a rep-
resentation of terms as bags of contexts and
the definition of a similarity measure between
terms. Thus, words, lemmas and senses are
represented in the same space, where similar-
ity measures can be defined between them. We
call this space the Context Space. A new disam-
biguation context (bag of words) is transformed
into the Context Space by the inner product,
becoming a kind of abstract term suitable to be
compared with singular senses that are repre-
sented in the same Context Space.
4.1 Representation
The training corpus is represented in the usual
two-dimension matrix A as shown in Figure 1,
where
? c1, ..., cN is the set of examples or con-
texts in the training corpus. Contexts are
treated as bags of words or lemmas.
? lem1, ..., lemT is the set of different words
or lemmas in all the training contexts.
4http://www.coli.uni-sb.de/ thorsten/tnt/
5http://www.ims.uni-stuttgart.de/projekte/corplex/
TreeTagger/
? wi,j is the weight for lemi in the training
context cj .
A new instance q, represented with the vec-
tor of weights (w1q, ..., wiq, ..., wTq), is trans-
formed into a vector in the context space ~q =
(q1, ..., qj , ..., qN ), where ~q is given by the usual
inner product ~q = q ? A (Figure 1):
qj =
T?
i=1
wiqwij
Figure 1: Representation of terms in the Con-
text Space, and transformation of new in-
stances.
If vectors cj (columns of matrix A) and vector
q (original test context) are normalized to have
a length equal to 1, then qj become the cosine
between vectors q and cj . More formally,
~q = q.A = (cos(q, c1), ..., cos(q, cj), ..., cos(q, cN ))
where
cos(q, cj) =
T?
i=1
wiq
?q?
wij
?cj?
and
?x? =
?
?
i
x2i
At this point, both senses and the representa-
tion of the new instance ~q are represented in the
same context space (Figure 2) and a similarity
measure can be defined between them:
sim( ~senik, ~q)
where senik is the k candidate sense for the
ambiguous lemma lemi. Each component j of
~senik is set to 1 if lemma lemi is used with sense
senik in the training context j, and 0 otherwise.
Figure 2: Similarity in the Context Space.
For a new context of the ambiguous lemma
lemi, the candidate sense with higher similarity
is selected:
argmaxk sim( ~senik, ~q)
4.2 Bag of words versus bag of contexts
Table 1 shows experimental results over the En-
glish Lexical Sample test of Senseval-3. Sys-
tem has been trained with the Senseval-3 data
and the SemCor collection. The Senseval train-
ing data has been lemmatized and tagged with
TreeTagger. Only nouns and adjectives have
been considered in their canonical form.
Three different weights wij have been tested:
? Co-occurrence: wij and wiq are set to {0,1}
depending on whether lemi is present or
not in context cj and in the new instance q
respectively. After the inner product q ? A,
the components qj of ~q get the number of
co-occurrences of different lemmas in both
q and the training context cj .
? Term Frequency: wij is set to tfij , the num-
ber of occurrences of lemi in the context cj .
? tf.idf : wij = (1 + log(tfij)) ? (log( Ndfi )),
a standard tf.idf weight where dfi is the
number of contexts that contain lemi.
These weights have been normalized ( wij||cj ||)
and so, the inner product q?A generates a vector
~q of cosines as described above, where qj is the
cosine between q and context cj .
Two similarity measures have been compared.
The first one (maximum) is a similarity of q as
bag of words with the training contexts of sense
sen. The second one (cosine) is the similarity
of sense sen with ~q in the context space:
? Maximum: sim( ~sen, ~q) =
= MaxNj=1 (senj ? qj) =
= Max{j/sen?cj}qj =
= Max{j/sen?cj}cos(q, cj)
Weight Similarity Nouns Adjectives Verbs Total
Co-occurrences Maximum 60.76% 35.85% 60.75% 59.75%
(normalized) Cosine 59.99% 55.97% 63.88% 61.78%
Term frequency Maximum 56.83% 50.31% 56.85% 56.58%
(normalized) Cosine 60.76% 53.46% 63.83% 62.01%
tf.idf Maximum 59.82% 48.43% 59.94% 59.42%
(normalized) Cosine 60.27% 53.46% 64.29% 62.01%
Most frequent
(baseline) 54.01% 54.08% 56.45% 55.23%
Table 1: Bag of words versus bag of contexts, precision-recall
Similarity with sense sen is the high-
est similarity (cosine) between q (as bag of
words) and each of the training contexts
(as bag of words) for sense sen.
? Cosine: sim( ~sen, ~q) = cos( ~sen, ~q) =
=
?
{j/sen?cj}
senj
|| ~sen|| ?
cos(q,cj)
||~q||
Similarity with sense sen is the co-
sine in the Context Space between ~q and
~sen
Table 1 shows that almost all the results are
improved when the similarity measure (cosine)
is applied in the Context Space. The exception
is the consideration of co-ocurrences to disam-
biguate nouns. This exception led us to explore
an alternative similarity measure aimed to im-
prove results over nouns. The following sections
describe this new similarity measure and the cri-
teria underlying it.
4.3 Criteria for the similarity measure
Co-occurrences behave quite good to disam-
biguate nouns as it has been shown in the exper-
iment above. However, the consideration of co-
occurrences in the Context Space permits acu-
mulative measures: Instead of selecting the can-
didate sense associated to the training context
with the maximum number of co-occurrences,
we can consider the co-occurences of q with all
the contexts. The weights and the similarity
function has been set out satisfying the follow-
ing criteria:
1. Select the sense senk assigned to more
training contexts ci that have the maxi-
mum number of co-occurrences with the
test context q. For example, if sense sen1
has two training contexts with the highest
number of co-occurrences and sense sen2
has only one with the same number of co-
occurrences, sen1 must receive a higher
value than sen2.
2. Try to avoid label inconsistencies in the
training corpus. There are some training
examples where the same ambiguous word
is used with the same meaning but tagged
with different sense by human assessors.
Table 2 shows an example of this kind of
inconsistencies.
4.4 Similarity measure
We assign the weights wij and wiq to have ~q a
vector of co-occurrences, where qj is the number
of different nouns and adjectives that co-occurr
in q and the training context cj . In this way, wij
is set to 1 if lemi is present in the context cj .
Otherwise wij is set to 0. Analogously for the
new instance q, wiq is set to 1 if lemi is present
in q and it is set to 0 otherwise.
According to the second criterium, if there
is only one context c1 with the higher num-
ber of co-occurrences with q, then we reduce
the value of this context by reducing artifi-
cially its number of co-occurrences: Being c2
a context with the second higher number of co-
occurrences with q, then we assign to the first
context c1 the number of co-occurrences of con-
text c2.
After this slight modification of ~q we imple-
ment the similarity measure between ~q and a
sense senk according to the first criterium:
sim( ~sen, ~q) =
N?
j=1
senj ? N
qj
Finally, for a new context of lemi we select
the candidate sense that gives more value to the
similarity measure:
argmaxk sim( ~senk, ~q)
<answer instance=?grano.n.1? senseid=?grano.4?/>
<previous> La Federacin Nacional de Cafeteros de Colombia explic que el nuevo valor fue estable-
cido con base en el menor de los precios de reintegro mnimo de grano del pas de los ltimos tres das,
y que fue de 1,3220 dlares la libra, que fue el que alcanz hoy en Nueva York, y tambin en la tasa rep-
resentativa del mercado para esta misma fecha (1.873,77 pesos por dlar). </previous> <target>
El precio interno del caf colombiano permaneci sin modificacin hasta el 10 de noviembre de 1999,
cuando las autoridades cafetaleras retomaron el denominado ?sistema de ajuste automtico?, que
tiene como referencia la cotizacin del <head>grano</head> nacional en los mercados interna-
cionales. </target>
<answer instance=?grano.n.9? senseid=?grano.3?/>
<previous> La carga qued para maana en 376.875 pesos (193,41 dlares) frente a los 375.000 pesos
(192,44 dlares) que rigi hasta hoy. </previous> <target> El reajuste al alza fue adoptado por
el Comit de Precios de la Federacin que fijar el precio interno diariamente a partir de este lunes
tomando en cuenta la cotizacin del <head>grano</head> en el mercado de Nueva York y la tasa
de cambio del da, que para hoy fueron de 1,2613 dlares libra y1.948,60 pesos por dlar </target>
Table 2: Example of inconsistencies in human annotation
Weight Similarity Nouns Adjectives Verbs Total
Co-occurrences Without criterium 2 65.6% 45.9% 62.5% 63.3%
(not normalized) With criterium 2 66.5% 45.9% 63.4% 64.1%
Table 3: Precision-recall for the new similarity measure
Table 3 shows experimental results over the
English Lexical Sample test under the same con-
ditions than experiments in Table 1.
Comparing results in both tables we observe
that the new similarity measure only behaves
better for the disambiguation of nouns. How-
ever, the difference is big enough to improve
overall results. The application of the second
criterium (try to avoid label inconsistencies)
also improves the results as shown in Tables 3
and 4. Table 4 shows the effect of applying this
second criterium to all the languages we have
participated in. With the exception of Cata-
lan, all results are improved slightly (about 1%)
after the filtering of singular labelled contexts.
Although it is a regular behavior, this improve-
ment is not statistically significative.
With Without
Criterium 2 Criterium 2
Spanish 81.8% 80.9%
Catalan 81.8% 82.0%
English 64.1% 63.3%
Italian 49.8% 49.3%
Table 4: Incidence of Criterium 2, precision-
recall
5 Results at Senseval-3
The results submited to Senseval-3 were gener-
ated with the system described in Section 4.4.
Since one sense is assigned to every test con-
text, precison and recall have equal values. Ta-
ble 4 shows official results for the Lexical Sam-
ple Task at Senseval-3 in the four languages we
have participated in: Spanish, Catalan, English
and Italian.
Fine Coarse Baseline
grained grained (most frequent)
Spanish 81.8% - 67%
Catalan 81.8% - 66%
English 64.1% 72% 55%
Italian 49.8% - -
Table 5: Official results at Senseval-3, precision-
recall
Differences between languages are quite re-
markable and show the system dependence on
the training corpora and the sense inventory.
In the English task, 16 test instances have
a correct sense not present in the training cor-
pus. Since we don?t use the dictionary informa-
tion our system was unable to deal with none of
them. In the same way, 68 test instances have
been tagged as ?Unasignable? sense and again
the system was unable to detect none of them.
6 Conclusion and work in progress
We have shown the exemplar-based WSD sys-
tem developed by UNED for the Senseval-3 lexi-
cal sample tasks. The general approach is based
on the definition of a context space that be-
comes a flexible tool to prove quite different
similarity measures between training contexts
and new instances. We have shown that stan-
dard similarity measures improve their results
applied inside this context space. We have es-
tablished some criteria to instantiate this gen-
eral approach and the resulting system has been
evaluated at Senseval-3. The new similarity
measure improves the disambiguation of nouns
and obtains better overall results. The work in
progress includes:
? the study of new criteria to lead us to al-
ternative measures,
? the development of particular disambigua-
tion strategies for verbs, nouns and adjec-
tives,
? the inclusion of the dictionary information,
and
? the consideration of WordNet semantic re-
lationships to extend the training corpus.
Acknowledgements
Special thanks to Julio Gonzalo for the lending
of linguistic resources, and to V??ctor Peinado for
his demonstrated sensibility.
This work has been supported by the Spanish
Ministry of Science and Technology through the
following projects:
? Hermes (TIC-2000-0335-C03-01)
? Syembra (TIC-2003-07158-C04-02)
? R2D2 (TIC 2003-07158-104-01)
References
Thorsten Brants. 2000. Tnt - a statistical part-
of-speech tagger. In In Proceedings of the
Sixth Applied Natural Language Processing
Conference ANLP-2000.
G. Escudero, L. Ma`rquez, and G. Rigau. 2000.
A comparison between supervised learning al-
gorithms for word sense disambiguation. In
In Proceedings of the 4th Computational Nat-
ural Language Learning Workshop, CoNLL.
Christiane Fellbaum, editor. 1998. WordNet
An Electronic Lexical Database. The MIT
Press.
N. Ide and J. Ve?ronis. 1998. Introduction to the
special issue on word sense disambiguation:
The state of the art. Computational Linguis-
tics.
G. Miller, C. Leacock, T. Randee, and
R. Bunker. 1993. A semantic concordance.
In In Procedings of the 3rd DARPA Work-
shop on Human Language Technology.
E. Pianta, L. Bentivogli, and C. Girardi. 2002.
Multiwordnet: developing an aligned mul-
tilingual database. In In Proceedings of
the First International Conference on Global
WordNet.
Helmut Schmid. 1994. Probabilistic part-of-
speech tagging using decision trees. In Inter-
national Conference on New Methods in Lan-
guage Processing.
M. Scott. 1997. Wordsmith tools lexical analy-
sis software for data driven learning and re-
search. Technical report, The University of
Liverpool.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 64?69,
Prague, June 2007. c?2007 Association for Computational Linguistics
The SemEval-2007 WePS Evaluation: Establishing a benchmark for the
Web People Search Task
Javier Artiles
UNED NLP & IR group
Madrid, Spain
javart@bec.uned.es
nlp.uned.es/?javier
Julio Gonzalo
UNED NLP & IR group
Madrid, Spain
julio@lsi.uned.es
nlp.uned.es/?julio
Satoshi Sekine
Computer Science Department
New York University, USA
sekine@cs.nyu.edu
nlp.cs.nyu.edu/sekine
Abstract
This paper presents the task definition, re-
sources, participation, and comparative re-
sults for the Web People Search task, which
was organized as part of the SemEval-2007
evaluation exercise. This task consists of
clustering a set of documents that mention
an ambiguous person name according to the
actual entities referred to using that name.
1 Introduction
Finding information about people in the World Wide
Web is one of the most common activities of Internet
users. Person names, however, are highly ambigu-
ous. In most cases, the results for a person name
search are a mix of pages about different people
sharing the same name. The user is then forced ei-
ther to add terms to the query (probably losing recall
and focusing on one single aspect of the person), or
to browse every document in order to filter the infor-
mation about the person he is actually looking for.
In an ideal system the user would simply type a
person name, and receive search results clustered ac-
cording to the different people sharing that name.
And this is, in essence, the WePS (Web People
Search) task we have proposed to SemEval-2007
participants: systems receive a set of web pages
(which are the result of a web search for a per-
son name), and they have to cluster them in as
many sets as entities sharing the name. This task
has close links with Word Sense Disambiguation
(WSD), which is generally formulated as the task
of deciding which sense a word has in a given con-
text. In both cases, the problem addressed is the res-
olution of the ambiguity in a natural language ex-
pression. A couple of differences make our prob-
lem different. WSD is usually focused on open-
class words (common nouns, adjectives, verbs and
adverbs). The first difference is that boundaries be-
tween word senses in a dictionary are often subtle
or even conflicting, making binary decisions harder
and sometimes even useless depending on the ap-
plication. In contrast, distinctions between people
should be easier to establish. The second difference
is that WSD usually operates with a dictionary con-
taining a relatively small number of senses that can
be assigned to each word. Our task is rather a case
of Word Sense Discrimination, because the number
of ?senses? (actual people) is unknown a priori, and
it is in average much higher than in the WSD task
(there are 90,000 different names shared by 100 mil-
lion people according to the U.S. Census Bureau).
There is also a strong relation of our proposed
task with the Co-reference Resolution problem, fo-
cused on linking mentions (including pronouns) in
a text. Our task can be seen as a co-reference reso-
lution problem where the focus is on solving inter-
document co-reference, disregarding the linking of
all the mentions of an entity inside each document.
An early work in name disambiguation (Bagga
and Baldwin, 1998) uses the similarity between doc-
uments in a Vector Space using a ?bag of words?
representation. An alternative approach by Mann
and Yarowsky (2003) is based on a rich feature space
of automatically extracted biographic information.
Fleischman and Hovy (2004) propose a Maximum
Entropy model trained to give the probability that
64
two names refer to the same individual 1.
The paper is organized as follows. Section 2 pro-
vides a description of the experimental methodol-
ogy, the training and test data provided to the par-
ticipants, the evaluation measures, baseline systems
and the campaign design. Section 3 gives a descrip-
tion of the participant systems and provides the eval-
uation results. Finally, Section 4 presents some con-
clusions.
2 Experimental Methodology
2.1 Data
Following the general SemEval guidelines, we have
prepared trial, training and test data sets for the task,
which are described below.
2.1.1 Trial data
For this evaluation campaign we initially deliv-
ered a trial corpus for the potential participants. The
trial data consisted of an adapted version of the
WePS corpus described in (Artiles et al, 2006). The
predominant feature of this corpus is a high number
of entities in each document set, due to the fact that
the ambiguous names were extracted from the most
common names in the US Census. This corpus did
not completely match task specifications because it
did not consider documents with internal ambiguity,
nor it did consider non-person entities; but it was,
however, a cost-effective way of releasing data to
play around with. During the first weeks after releas-
ing this trial data to potential participants, some an-
notation mistakes were noticed. We preferred, how-
ever, to leave the corpus ?as is? and concentrate our
efforts in producing clean training and test datasets,
rather than investing time in improving trial data.
2.1.2 Training data
In order to provide different ambiguity scenarios,
we selected person names from different sources:
US Census. We reused the Web03 corpus (Mann,
2006), which contains 32 names randomly picked
from the US Census, and was well suited for the
task.
Wikipedia. Another seven names were sampled
from a list of ambiguous person names in the En-
glish Wikipedia. These were expected to have a
1For a comprehensive bibliography on person name disam-
biguation refer to http://nlp.uned.es/weps
few predominant entities (popular or historical), and
therefore a lower ambiguity than the previous set.
ECDL. Finally, ten additional names were ran-
domly selected from the Program Committee listing
of a Computer Science conference (ECDL 2006).
This set offers a scenario of potentially low am-
biguity (computer science scholars usually have a
stronger Internet presence than other professional
fields) with the added value of the a priori knowl-
edge of a domain specific type of entity (scholar)
present in the data.
All datasets consist of collections of web pages
obtained from the 100 top results for a person name
query to an Internet search engine 2. Note that 100
is an upper bound, because in some occasions the
URL returned by the search engine no longer exists.
The second and third datasets (developed explic-
itly for our task) consist of 17 person names and
1685 associated documents in total (99 documents
per name in average). Each web page was down-
loaded and stored for off-line processing. We also
stored the basic metadata associated to each search
result, including the original URL, title, position in
the results ranking and the corresponding snippet
generated by the search engine.
In the process of generating the corpus, the se-
lection of the names plays an important role, poten-
tially conditioning the degree of ambiguity that will
be found later in the Web search results. The reasons
for this variability in the ambiguity of names are di-
verse and do not always correlate with the straight-
forward census frequency. A much more decisive
feature is, for instance, the presence of famous en-
tities sharing the ambiguous name with less popular
people. As we are considering top search results,
these can easily be monopolized by a single entity
that is popular in the Internet.
After the annotation of this data (see section
2.1.4.) we found our predictions about the average
ambiguity of each dataset not to be completely ac-
curate. In Table 1 we see that the ECDL-06 average
ambiguity is indeed relatively low (except for the
documents for ?Thomas Baker? standing as the most
ambiguous name in the whole training). Wikipedia
names have an average ambiguity of 23,14 entities
2We used the Yahoo! API from Yahoo! Search Web Ser-
vices (http://developer.yahoo.com/search/web/).
65
Name entities documents discarded
Wikipedia names
John Kennedy 27 99 6
George Clinton 27 99 6
Michael Howard 32 99 8
Paul Collins 37 98 6
Tony Abbott 7 98 9
Alexander Macomb 21 100 14
David Lodge 11 100 9
Average 23,14 99,00 8,29
ECDL-06 Names
Edward Fox 16 100 36
Allan Hanbury 2 100 32
Donna Harman 7 98 6
Andrew Powell 19 98 48
Gregory Crane 4 99 17
Jane Hunter 15 99 59
Paul Clough 14 100 35
Thomas Baker 60 100 31
Christine Borgman 7 99 11
Anita Coleman 9 99 28
Average 15,30 99,20 30,30
WEB03 Corpus
Tim Whisler 10 33 8
Roy Tamashiro 5 23 6
Cynthia Voigt 1 405 314
Miranda Bollinger 2 2 0
Guy Dunbar 4 51 34
Todd Platts 2 239 144
Stacey Doughty 1 2 0
Young Dawkins 4 61 35
Luke Choi 13 20 6
Gregory Brennan 32 96 38
Ione Westover 1 4 0
Patrick Karlsson 10 24 8
Celeste Paquette 2 17 2
Elmo Hardy 3 55 15
Louis Sidoti 2 6 3
Alexander Markham 9 32 16
Helen Cawthorne 3 46 13
Dan Rhone 2 4 2
Maile Doyle 1 13 1
Alice Gilbreath 8 74 30
Sidney Shorter 3 4 0
Alfred Schroeder 35 112 58
Cathie Ely 1 2 0
Martin Nagel 14 55 31
Abby Watkins 13 124 35
Mary Lemanski 2 152 78
Gillian Symons 3 30 6
Pam Tetu 1 4 2
Guy Crider 2 2 0
Armando Valencia 16 79 20
Hannah Bassham 2 3 0
Charlotte Bergeron 5 21 8
Average 5,90 47,20 18,00
Global average 10,76 71,02 26,00
Table 1: Training Data
per name, which is higher than for the ECDL set.
The WEB03 Corpus has the lowest ambiguity (5,9
entities per name), for two reasons: first, randomly
picked names belong predominantly to the long tail
of unfrequent person names which, per se, have low
ambiguity. Being rare names implies that in average
there are fewer documents returned by the search en-
gine (47,20 per name), which also reduces the pos-
sibilities to find ambiguity.
2.1.3 Test data
For the test data we followed the same process
described for the training. In the name selection we
tried to maintain a similar distribution of ambigu-
ity degrees and scenario. For that reason we ran-
domly extracted 10 person names from the English
Wikipedia and another 10 names from participants
in the ACL-06 conference. In the case of the US cen-
sus names, we decided to focus on relatively com-
mon names, to avoid the problems explained above.
Unfortunately, after the annotation was finished
(once the submission deadline had expired), we
found a major increase in the ambiguity degrees (Ta-
ble 2) of all data sets. While we expected a raise in
the case of the US census names, the other two cases
just show that there is a high (and unpredictable)
variability, which would require much larger data
sets to have reliable population samples.
This has made the task particularly challenging
for participants, because naive learning strategies
(such as empirical adjustment of distance thresholds
to optimize standard clustering algorithms) might be
misleaded by the training set.
2.1.4 Annotation
The annotation of the data was performed sepa-
rately in each set of documents related to an ambigu-
ous name. Given this set of approximately 100 doc-
uments that mention the ambiguous name, the an-
notation consisted in the manual clustering of each
document according to the actual entity that is re-
ferred on it.
When non person entities were found (for in-
stance, organization or places named after a person)
the annotation was performed without any special
rule. Generally, the annotator browses documents
following the original ranking in the search results;
after reading a document he will decide whether the
mentions of the ambiguous name refer to a new en-
tity or to a entity previously identified. We asked
the annotators to concentrate first on mentions that
strictly contained the search string, and then to pay
attention to the co-referent variations of the name.
For instance ?John Edward Fox? or ?Edward Fox
Smith? would be valid mentions. ?Edward J. Fox?,
however, breaks the original search string, and we
do not get into name variation detection, so it will
be considered valid only if it is co-referent to a valid
66
Name entities documents discarded
Wikipedia names
Arthur Morgan 19 100 52
James Morehead 48 100 11
James Davidson 59 98 16
Patrick Killen 25 96 4
William Dickson 91 100 8
George Foster 42 99 11
James Hamilton 81 100 15
John Nelson 55 100 25
Thomas Fraser 73 100 13
Thomas Kirk 72 100 20
Average 56,50 99,30 17,50
ACL06 Names
Dekang Lin 1 99 0
Chris Brockett 19 98 5
James Curran 63 99 9
Mark Johnson 70 99 7
Jerry Hobbs 15 99 7
Frank Keller 28 100 20
Leon Barrett 33 98 9
Robert Moore 38 98 28
Sharon Goldwater 2 97 4
Stephen Clark 41 97 39
Average 31,00 98,40 12,80
US Census Names
Alvin Cooper 43 99 9
Harry Hughes 39 98 9
Jonathan Brooks 83 97 8
Jude Brown 32 100 39
Karen Peterson 64 100 16
Marcy Jackson 51 100 5
Martha Edwards 82 100 9
Neil Clark 21 99 7
Stephan Johnson 36 100 20
Violet Howard 52 98 27
Average 50,30 99,10 14,90
Global average 45,93 98,93 15,07
Table 2: Test Data
mention.
In order to perform the clustering, the annotator
was asked to pay attention to objective facts (bi-
ographical dates, related names, occupations, etc.)
and to be conservative when making decisions. The
final result is a complete clustering of the docu-
ments, where each cluster contains the documents
that refer to a particular entity. Following the pre-
vious example, in documents for the name ?Edward
Fox? the annotator found 16 different entities with
that name. Note that there is no a priori knowledge
about the number of entities that will be discovered
in a document set. This makes the task specially
difficult when there are many different entities and
a high volume of scattered biographical information
to take into account.
In cases where the document does not offer
enough information to decide whether it belongs to
a cluster or is a new entity, it is discarded from the
evaluation process (not from the dataset). Another
common reason for discarding documents was the
absence of the person name in the document, usu-
ally due to a mismatch between the search engine
cache and the downloaded URL.
We found that, in many cases, different entities
were mentioned using the ambiguous name within a
single document. This was the case when a doc-
ument mentions relatives with names that contain
the ambiguous string (for instance ?Edward Fox?
and ?Edward Fox Jr.?). Another common case of
intra-document ambiguity is that of pages contain-
ing database search results, such as book lists from
Amazon, actors from IMDB, etc. A similar case is
that of pages that explicitly analyze the ambiguity of
a person name (Wikipedia ?disambiguation? pages).
The way this situation was handled, in terms of the
annotation, was to assign each document to as many
clusters as entities were referred to on it with the
ambiguous name.
2.2 Evaluation measures
Evaluation was performed in each document set
(web pages mentioning an ambiguous person name)
of the data distributed as test. The human annotation
was used as the gold standard for the evaluation.
Each system was evaluated using the standard pu-
rity and inverse purity clustering measures Purity is
related to the precision measure, well known in In-
formation Retrieval. This measure focuses on the
frequency of the most common category in each
cluster, and rewards the clustering solutions that in-
troduce less noise in each cluster. Being C the set
of clusters to be evaluated, L the set of categories
(manually annotated) and n the number of clustered
elements, purity is computed by taking the weighted
average of maximal precision values:
Purity =
?
i
|Ci|
n
max Precision(Ci, Lj)
where the precision of a cluster Ci for a given cat-
egory Lj is defined as:
Precision(Ci, Lj) =
|Ci
?
Lj |
|Ci|
Inverse Purity focuses on the cluster with maxi-
mum recall for each category, rewarding the clus-
tering solutions that gathers more elements of each
category in a corresponding single cluster. Inverse
Purity is defined as:
67
Inverse Purity =
?
i
|Li|
n
max Precision(Li, Cj)
For the final ranking of systems we used the har-
monic mean of purity and inverse purity F?=0,5 . The
F measure is defined as follows:
F =
1
? 1Purity + (1? ?)
1
Inverse Purity
F?=0,2 is included as an additional measure giv-
ing more importance to the inverse purity aspect.
The rationale is that, for a search engine user, it
should be easier to discard a few incorrect web
pages in a cluster containing all the information
needed, than having to collect the relevant infor-
mation across many different clusters. Therefore,
achieving a high inverse purity should be rewarded
more than having high purity.
2.3 Baselines
Two simple baseline approaches were applied to the
test data. The ALL-IN-ONE baseline provides a
clustering solution where all the documents are as-
signed to a single cluster. This has the effect of al-
ways achieving the highest score in the inverse pu-
rity measure, because all classes have their docu-
ments in a single cluster. On the other hand, the
purity measure will be equal to the precision of the
predominant class in that single cluster. The ONE-
IN-ONE baseline gives another extreme clustering
solution, where every document is assigned to a dif-
ferent cluster. In this case purity always gives its
maximum value, while inverse purity will decrease
with larger classes.
2.4 Campaign design
The schedule for the evaluation campaign was set by
the SemEval organisation as follows: (i) release task
description and trial data set; (ii) release of training
and test; (iii) participants send their answers to the
task organizers; (iv) the task organizers evaluate the
answers and send the results.
The task description and the initial trial data set
were publicly released before the start of the official
evaluation.
The official evaluation period started with the si-
multaneous release of both training and test data, to-
gether with a scoring script with the main evaluation
measures to be used. This period spanned five weeks
in which teams were allowed to register and down-
load the data. During that period, results for a given
task had to be submitted no later than 21 days af-
ter downloading the training data and no later than 7
days after downloading the test data. Only one sub-
mission per team was allowed.
Training data included the downloaded web
pages, their associated metadata and the human clus-
tering of each document set, providing a develop-
ment test-bed for the participant?s systems. We also
specified the source of each ambiguous name in the
training data (Wikipedia, ECDL conference and US
Census). Test data only included the downloaded
web pages and their metadata. This section of the
corpus was used for the systems evaluation. Partici-
pants were required to send a clustering for each test
document set.
Finally, after the evaluation period was finished
and all the participants sent their data, the task orga-
nizers sent the evaluation for the test data.
3 Results of the evaluation campaign
29 teams expressed their interest in the task; this
number exceeded our expectations for this pilot ex-
perience, and confirms the potential interest of the
research community in this highly practical prob-
lem. Out of them, 16 teams submitted results within
the deadline; their results are reported below.
3.1 Results and discussion
Table 3 presents the macro-averaged results ob-
tained by the sixteen systems plus the two baselines
on the test data. We found macro-average 3 prefer-
able to micro-average 4 because it has a clear inter-
pretation: if the evaluation measure is F, then we
should calculate F for every test case (person name)
and then average over all trials. The interpretation
of micro-average F is less clear.
The systems are ranked according to the scores
obtained with the harmonic mean measure F?=0,5 of
3Macro-average F consists of computing F for every test set
(person name) and then averaging over all test sets.
4Micro-average F consists of computing the average P and
IP (over all test sets) and then calculating F with these figures.
68
Macro-averaged Scores
F-measures
rank team-id ? =,5 ? =,2 Pur Inv Pur
1 CU COMSEM ,78 ,83 ,72 ,88
2 IRST-BP ,75 ,77 ,75 ,80
3 PSNUS ,75 ,78 ,73 ,82
4 UVA ,67 ,62 ,81 ,60
5 SHEF ,66 ,73 ,60 ,82
6 FICO ,64 ,76 ,53 ,90
7 UNN ,62 ,67 ,60 ,73
8 ONE-IN-ONE ,61 ,52 1,00 ,47
9 AUG ,60 ,73 ,50 ,88
10 SWAT-IV ,58 ,64 ,55 ,71
11 UA-ZSA ,58 ,60 ,58 ,64
12 TITPI ,57 ,71 ,45 ,89
13 JHU1-13 ,53 ,65 ,45 ,82
14 DFKI2 ,50 ,63 ,39 ,83
15 WIT ,49 ,66 ,36 ,93
16 UC3M 13 ,48 ,66 ,35 ,95
17 UBC-AS ,40 ,55 ,30 ,91
18 ALL-IN-ONE ,40 ,58 ,29 1,00
Table 3: Team ranking
purity and inverse purity. Considering only the par-
ticipant systems, the average value for the ranking
measure was 0, 60 and its standard deviation 0, 11.
Results with F?=0,2 are not substantially different
(except for the two baselines, which roughly swap
positions). There are some ranking swaps, but gen-
erally only within close pairs.
The good performance of the ONE-IN-ONE base-
line system is indicative of the abundance of single-
ton entities (entities represented by only one doc-
ument). This situation increases the inverse purity
score for this system giving a harmonic measure
higher than the expected.
4 Conclusions
The WEPS task ended with considerable success in
terms of participation, and we believe that a careful
analysis of the contributions made by participants
(which is not possible at the time of writing this re-
port) will be an interesting reference for future re-
search. In addition, all the collected and annotated
dataset will be publicly available 5 as a benchmark
for Web People Search systems.
At the same time, it is clear that building a re-
liable test-bed for the task is not simple. First of
all, the variability across test cases is large and un-
predictable, and a system that works well with the
5http://nlp.uned.es/weps
names in our test bed may not be reliable in practi-
cal, open search situations. Partly because of that,
our test-bed happened to be unintentionally chal-
lenging for systems, with a large difference be-
tween the average ambiguity in the training and test
datasets. Secondly, it is probably necessary to think
about specific evaluation measures beyond standard
clustering metrics such as purity and inverse purity,
which are not tailored to the task and do not be-
have well when multiple classification is allowed.
We hope to address these problems in a forthcom-
ing edition of the WEPS task.
5 Acknowledgements
This research was supported in part by the National
Science Foundation of United States under Grant
IIS-00325657 and by a grant from the Spanish gov-
ernment under project Text-Mess (TIN2006-15265-
C06). This paper does not necessarily reflect the po-
sition of the U.S. Government.
References
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2005.
A Testbed for People Searching Strategies in the
WWW In Proceedings of the 28th annual Interna-
tional ACM SIGIR conference on Research and De-
velopment in Information Retrieval (SIGIR?05), pages
569-570.
Amit Bagga and Breck Baldwin. 1998. Entity-
Based Cross-Document Coreferencing Using the Vec-
tor Space Model In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and the 17th International Conference on Compu-
tational Linguistics (COLING-ACL?98), pages 79-85.
Michael B. Fleischman and Eduard Hovy 2004. Multi-
document person name resolution. In Proceedings of
ACL-42, Reference Resolution Workshop.
Gideon S. Mann. 2006. Multi-Document Statistical Fact
Extraction and Fusion Ph.D. Thesis.
Gideon S. Mann and David Yarowsky 2003. Unsuper-
vised Personal Name Disambiguation In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL, pages 33-40.
69
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1357?1366,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Wikipedia as Sense Inventory to Improve Diversity in Web Search Results
Celina Santamar??a, Julio Gonzalo and Javier Artiles
nlp.uned.es
UNED, c/Juan del Rosal, 16, 28040 Madrid, Spain
celina.santamaria@gmail.com julio@lsi.uned.es javart@bec.uned.es
Abstract
Is it possible to use sense inventories to
improve Web search results diversity for
one word queries? To answer this ques-
tion, we focus on two broad-coverage lex-
ical resources of a different nature: Word-
Net, as a de-facto standard used in Word
Sense Disambiguation experiments; and
Wikipedia, as a large coverage, updated
encyclopaedic resource which may have a
better coverage of relevant senses in Web
pages.
Our results indicate that (i) Wikipedia has
a much better coverage of search results,
(ii) the distribution of senses in search re-
sults can be estimated using the internal
graph structure of the Wikipedia and the
relative number of visits received by each
sense in Wikipedia, and (iii) associating
Web pages to Wikipedia senses with sim-
ple and efficient algorithms, we can pro-
duce modified rankings that cover 70%
more Wikipedia senses than the original
search engine rankings.
1 Motivation
The application of Word Sense Disambiguation
(WSD) to Information Retrieval (IR) has been sub-
ject of a significant research effort in the recent
past. The essential idea is that, by indexing and
matching word senses (or even meanings) , the re-
trieval process could better handle polysemy and
synonymy problems (Sanderson, 2000). In prac-
tice, however, there are two main difficulties: (i)
for long queries, IR models implicitly perform
disambiguation, and thus there is little room for
improvement. This is the case with most stan-
dard IR benchmarks, such as TREC (trec.nist.gov)
or CLEF (www.clef-campaign.org) ad-hoc collec-
tions; (ii) for very short queries, disambiguation
may not be possible or even desirable. This is
often the case with one word and even two word
queries in Web search engines.
In Web search, there are at least three ways of
coping with ambiguity:
? Promoting diversity in the search results
(Clarke et al, 2008): given the query ?oa-
sis?, the search engine may try to include rep-
resentatives for different senses of the word
(such as the Oasis band, the Organization
for the Advancement of Structured Informa-
tion Standards, the online fashion store, etc.)
among the top results. Search engines are
supposed to handle diversity as one of the
multiple factors that influence the ranking.
? Presenting the results as a set of (labelled)
clusters rather than as a ranked list (Carpineto
et al, 2009).
? Complementing search results with search
suggestions (e.g. ?oasis band?, ?oasis fash-
ion store?) that serve to refine the query in the
intended way (Anick, 2003).
All of them rely on the ability of the search en-
gine to cluster search results, detecting topic simi-
larities. In all of them, disambiguation is implicit,
a side effect of the process but not its explicit tar-
get. Clustering may detect that documents about
the Oasis band and the Oasis fashion store deal
with unrelated topics, but it may as well detect
a group of documents discussing why one of the
Oasis band members is leaving the band, and an-
other group of documents about Oasis band lyrics;
both are different aspects of the broad topic Oa-
sis band. A perfect hierarchical clustering should
distinguish between the different Oasis senses at a
first level, and then discover different topics within
each of the senses.
Is it possible to use sense inventories to improve
search results for one word queries? To answer
1357
this question, we will focus on two broad-coverage
lexical resources of a different nature: WordNet
(Miller et al, 1990), as a de-facto standard used
in Word Sense Disambiguation experiments and
many other Natural Language Processing research
fields; and Wikipedia (www.wikipedia.org), as a
large coverage and updated encyclopedic resource
which may have a better coverage of relevant
senses in Web pages.
Our hypothesis is that, under appropriate con-
ditions, any of the above mechanisms (clustering,
search suggestions, diversity) might benefit from
an explicit disambiguation (classification of pages
in the top search results) using a wide-coverage
sense inventory. Our research is focused on four
relevant aspects of the problem:
1. Coverage: Are Wikipedia/Wordnet senses
representative of search results? Otherwise,
trying to make a disambiguation in terms of a
fixed sense inventory would be meaningless.
2. If the answer to (1) is positive, the reverse
question is also interesting: can we estimate
search results diversity using our sense inven-
tories?
3. Sense frequencies: knowing sense frequen-
cies in (search results) Web pages is crucial
to have a usable sense inventory. Is it possi-
ble to estimate Web sense frequencies from
currently available information?
4. Classification: The association of Web pages
to word senses must be done with some unsu-
pervised algorithm, because it is not possible
to hand-tag training material for every pos-
sible query word. Can this classification be
done accurately? Can it be effective to pro-
mote diversity in search results?
In order to provide an initial answer to these
questions, we have built a corpus consisting of 40
nouns and 100 Google search results per noun,
manually annotated with the most appropriate
Wordnet and Wikipedia senses. Section 2 de-
scribes how this corpus has been created, and in
Section 3 we discuss WordNet and Wikipedia cov-
erage of search results according to our testbed.
As this initial results clearly discard Wordnet as
a sense inventory for the task, the rest of the pa-
per mainly focuses on Wikipedia. In Section 4 we
estimate search results diversity from our testbed,
finding that the use of Wikipedia could substan-
tially improve diversity in the top results. In Sec-
tion 5 we use the Wikipedia internal link structure
and the number of visits per page to estimate rel-
ative frequencies for Wikipedia senses, obtaining
an estimation which is highly correlated with ac-
tual data in our testbed. Finally, in Section 6 we
discuss a few strategies to classify Web pages into
word senses, and apply the best classifier to en-
hance diversity in search results. The paper con-
cludes with a discussion of related work (Section
7) and an overall discussion of our results in Sec-
tion 8.
2 Test Set
2.1 Set of Words
The most crucial step in building our test set is
choosing the set of words to be considered. We
are looking for words which are susceptible to
form a one-word query for a Web search engine,
and therefore we should focus on nouns which
are used to denote one or more named entities.
At the same time we want to have some degree
of comparability with previous research on Word
Sense Disambiguation, which points to noun sets
used in Senseval/SemEval evaluation campaigns1.
Our budget for corpus annotation was enough for
two persons-month, which limited us to handle
40 nouns (usually enough to establish statistically
significant differences between WSD algorithms,
although obviously limited to reach solid figures
about the general behaviour of words in the Web).
With these arguments in mind, we decided to
choose: (i) 15 nouns from the Senseval-3 lexi-
cal sample dataset, which have been previously
employed by (Mihalcea, 2007) in a related ex-
periment (see Section 7); (ii) 25 additional words
which satisfy two conditions: they are all am-
biguous, and they are all names for music bands
in one of their senses (not necessarily the most
salient). The Senseval set is: {argument, arm,
atmosphere, bank, degree, difference, disc, im-
age, paper, party, performance, plan, shelter,
sort, source}. The bands set is {amazon, apple,
camel, cell, columbia, cream, foreigner, fox, gen-
esis, jaguar, oasis, pioneer, police, puma, rain-
bow, shell, skin, sun, tesla, thunder, total, traffic,
trapeze, triumph, yes}.
For each noun, we looked up all its possible
senses in WordNet 3.0 and in Wikipedia (using
1http://senseval.org
1358
Table 1: Coverage of Search Results: Wikipedia vs. WordNet
Wikipedia WordNet
# senses # documents # senses # documents
available/used assigned to some sense available/used assigned to some sense
Senseval set 242/100 877 (59%) 92/52 696 (46%)
Bands set 640/174 1358 (54%) 78/39 599 (24%)
Total 882/274 2235 (56%) 170/91 1295 (32%)
Wikipedia disambiguation pages). Wikipedia has
an average of 22 senses per noun (25.2 in the
Bands set and 16.1 in the Senseval set), and Word-
net a much smaller figure, 4.5 (3.12 for the Bands
set and 6.13 for the Senseval set). For a conven-
tional dictionary, a higher ambiguity might indi-
cate an excess of granularity; for an encyclopaedic
resource such as Wikipedia, however, it is just
an indication of larger coverage. Wikipedia en-
tries for camel which are not in WordNet, for in-
stance, include the Apache Camel routing and me-
diation engine, the British rock band, the brand
of cigarettes, the river in Cornwall, and the World
World War I fighter biplane.
2.2 Set of Documents
We retrieved the 150 first ranked documents for
each noun, by submitting the nouns as queries to a
Web search engine (Google). Then, for each doc-
ument, we stored both the snippet (small descrip-
tion of the contents of retrieved document) and the
whole HTML document. This collection of docu-
ments contain an implicit new inventory of senses,
based on Web search, as documents retrieved by
a noun query are associated with some sense of
the noun. Given that every document in the top
Web search results is supposed to be highly rele-
vant for the query word, we assume a ?one sense
per document? scenario, although we allow an-
notators to assign more than one sense per doc-
ument. In general this assumption turned out to be
correct except in a few exceptional cases (such as
Wikipedia disambiguation pages): only nine docu-
ments received more than one WordNet sense, and
44 (1.1% of all annotated pages) received more
than one Wikipedia sense.
2.3 Manual Annotation
We implemented an annotation interface which
stored all documents and a short description for
every Wordnet and Wikipedia sense. The annota-
tors had to decide, for every document, whether
there was one or more appropriate senses in each
of the dictionaries. They were instructed to pro-
vide annotations for 100 documents per name; if
an URL in the list was corrupt or not available,
it had to be discarded. We provided 150 docu-
ments per name to ensure that the figure of 100 us-
able documents per name could be reached with-
out problems.
Each judge provided annotations for the 4,000
documents in the final data set. In a second round,
they met and discussed their independent annota-
tions together, reaching a consensus judgement for
every document.
3 Coverage of Web Search Results:
Wikipedia vs Wordnet
Table 1 shows how Wikipedia and Wordnet cover
the senses in search results. We report each noun
subset separately (Senseval and bands subsets) as
well as aggregated figures.
The most relevant fact is that, unsurprisingly,
Wikipedia senses cover much more search results
(56%) than Wordnet (32%). If we focus on the
top ten results, in the bands subset (which should
be more representative of plausible web queries)
Wikipedia covers 68% of the top ten documents.
This is an indication that it can indeed be useful
for promoting diversity or help clustering search
results: even if 32% of the top ten documents are
not covered by Wikipedia, it is still a representa-
tive source of senses in the top search results.
We have manually examined all documents
in the top ten results that are not covered by
Wikipedia: a majority of the missing senses con-
sists of names of (generally not well-known) com-
panies (45%) and products or services (26%); the
other frequent type (12%) of non annotated doc-
ument is disambiguation pages (from Wikipedia
and also from other dictionaries).
It is also interesting to examine the degree of
overlap between Wikipedia and Wordnet senses.
Being two different types of lexical resource,
they might have some degree of complementar-
ity. Table 2 shows, however, that this is not the
case: most of the (annotated) documents either fit
Wikipedia senses (26%) or both Wikipedia and
Wordnet (29%), and just 3% fit Wordnet only.
1359
Table 2: Overlap between Wikipedia and Wordnet in Search Results
# documents annotated with
Wikipedia & Wordnet Wikipedia only Wordnet only none
Senseval set 607 (40%) 270 (18%) 89 (6%) 534 (36%)
Bands set 572 (23%) 786 (31%) 27 (1%) 1115 (45%)
Total 1179 (29%) 1056 (26%) 116 (3%) 1649 (41%)
Therefore, Wikipedia seems to extend the cover-
age of Wordnet rather than providing complemen-
tary sense information. If we wanted to extend the
coverage of Wikipedia, the best strategy seems to
be to consider lists of companies, products and ser-
vices, rather than complementing Wikipedia with
additional sense inventories.
4 Diversity in Google Search Results
Once we know that Wikipedia senses are a rep-
resentative subset of actual Web senses (covering
more than half of the documents retrieved by the
search engine), we can test how well search results
respect diversity in terms of this subset of senses.
Table 3 displays the number of different senses
found at different depths in the search results rank,
and the average proportion of total senses that they
represent. These results suggest that diversity is
not a major priority for ranking results: the top
ten results only cover, in average, 3 Wikipedia
senses (while the average number of senses listed
in Wikipedia is 22). When considering the first
100 documents, this number grows up to 6.85
senses per noun.
Another relevant figure is the frequency of the
most frequent sense for each word: in average,
63% of the pages in search results belong to the
most frequent sense of the query word. This is
roughly comparable with most frequent sense fig-
ures in standard annotated corpora such as Sem-
cor (Miller et al, 1993) and the Senseval/Semeval
data sets, which suggests that diversity may not
play a major role in the current Google ranking al-
gorithm.
Of course this result must be taken with care,
because variability between words is high and un-
predictable, and we are using only 40 nouns for
our experiment. But what we have is a positive
indication that Wikipedia could be used to im-
prove diversity or cluster search results: poten-
tially the first top ten results could cover 6.15 dif-
ferent senses in average (see Section 6.5), which
would be a substantial growth.
5 Sense Frequency Estimators for
Wikipedia
Wikipedia disambiguation pages contain no sys-
tematic information about the relative importance
of senses for a given word. Such information,
however, is crucial in a lexicon, because sense dis-
tributions tend to be skewed, and knowing them
can help disambiguation algorithms.
We have attempted to use two estimators of ex-
pected sense distribution:
? Internal relevance of a word sense, measured
as incoming links for the URL of a given
sense in Wikipedia.
? External relevance of a word sense, measured
as the number of visits for the URL of a given
sense (as reported in http://stats.grok.se).
The number of internal incoming links is ex-
pected to be relatively stable for Wikipedia arti-
cles. As for the number of visits, we performed
a comparison of the number of visits received by
the bands noun subset in May, June and July 2009,
finding a stable-enough scenario with one notori-
ous exception: the number of visits to the noun
Tesla raised dramatically in July, because July 10
was the anniversary of the birth of Nicola Tesla,
and a special Google logo directed users to the
Wikipedia page for the scientist.
We have measured correlation between the rela-
tive frequencies derived from these two indicators
and the actual relative frequencies in our testbed.
Therefore, for each noun w and for each sense wi,
we consider three values: (i) proportion of doc-
uments retrieved for w which are manually as-
signed to each sense wi; (ii) inlinks(wi): rela-
tive amount of incoming links to each sense wi;
and (iii) visits(wi): relative number of visits to the
URL for each sense wi.
We have measured the correlation between
these three values using a linear regression corre-
lation coefficient, which gives a correlation value
of .54 for the number of visits and of .71 for the
number of incoming links. Both estimators seem
1360
Table 3: Diversity in Search Results according to Wikipedia
average # senses in search results average coverage of Wikipedia senses
Bands set Senseval set Total Bands set Senseval set Total
First 10 docs 2.88 3.2 3.00 .21 .21 .21
First 25 4.44 4.8 4.58 .28 .33 .30
First 50 5.56 5.47 5.53 .33 .36 .34
First 75 6.56 6.33 6.48 .37 .43 .39
First 100 6.96 6.67 6.85 .38 .45 .41
to be positively correlated with real relative fre-
quencies in our testbed, with a strong preference
for the number of links.
We have experimented with weighted combina-
tions of both indicators, using weights of the form
(k, 1? k), k ? {0, 0.1, 0.2 . . . 1}, reaching a max-
imal correlation of .73 for the following weights:
freq(wi) = 0.9?inlinks(wi)+0.1?visits(wi) (1)
This weighted estimator provides a slight ad-
vantage over the use of incoming links only (.73
vs .71). Overall, we have an estimator which has
a strong correlation with the distribution of senses
in our testbed. In the next section we will test its
utility for disambiguation purposes.
6 Association of Wikipedia Senses to
Web Pages
We want to test whether the information provided
by Wikipedia can be used to classify search results
accurately. Note that we do not want to consider
approaches that involve a manual creation of train-
ing material, because they can?t be used in prac-
tice.
Given a Web page p returned by the search
engine for the query w, and the set of senses
w1 . . . wn listed in Wikipedia, the task is to assign
the best candidate sense to p. We consider two
different techniques:
? A basic Information Retrieval approach,
where the documents and the Wikipedia
pages are represented using a Vector Space
Model (VSM) and compared with a standard
cosine measure. This is a basic approach
which, if successful, can be used efficiently
to classify search results.
? An approach based on a state-of-the-art su-
pervised WSD system, extracting training ex-
amples automatically from Wikipedia con-
tent.
We also compute two baselines:
? A random assignment of senses (precision is
computed as the inverse of the number of
senses, for every test case).
? A most frequent sense heuristic which uses
our estimation of sense frequencies and as-
signs the same sense (the most frequent) to
all documents.
Both are naive baselines, but it must be noted
that the most frequent sense heuristic is usually
hard to beat for unsupervised WSD algorithms in
most standard data sets.
We now describe each of the two main ap-
proaches in detail.
6.1 VSM Approach
For each word sense, we represent its Wikipedia
page in a (unigram) vector space model, assigning
standard tf*idf weights to the words in the docu-
ment. idf weights are computed in two different
ways:
1. Experiment VSM computes inverse docu-
ment frequencies in the collection of re-
trieved documents (for the word being con-
sidered).
2. Experiment VSM-GT uses the statistics pro-
vided by the Google Terabyte collection
(Brants and Franz, 2006), i.e. it replaces the
collection of documents with statistics from a
representative snapshot of the Web.
3. Experiment VSM-mixed combines statistics
from the collection and from the Google
Terabyte collection, following (Chen et al,
2009).
The document p is represented in the same vec-
tor space as the Wikipedia senses, and it is com-
pared with each of the candidate senses wi via the
cosine similarity metric (we have experimented
1361
with other similarity metrics such as ?2, but dif-
ferences are irrelevant). The sense with the high-
est similarity to p is assigned to the document. In
case of ties (which are rare), we pick the first sense
in the Wikipedia disambiguation page (which in
practice is like a random decision, because senses
in disambiguation pages do not seem to be ordered
according to any clear criteria).
We have also tested a variant of this approach
which uses the estimation of sense frequencies
presented above: once the similarities are com-
puted, we consider those cases where two or more
senses have a similar score (in particular, all senses
with a score greater or equal than 80% of the high-
est score). In that cases, instead of using the small
similarity differences to select a sense, we pick up
the one which has the largest frequency according
to our estimator. We have applied this strategy to
the best performing system, VSM-GT, resulting in
experiment VSM-GT+freq.
6.2 WSD Approach
We have used TiMBL (Daelemans et al, 2001),
a state-of-the-art supervised WSD system which
uses Memory-Based Learning. The key, in this
case, is how to extract learning examples from the
Wikipedia automatically. For each word sense, we
basically have three sources of examples: (i) oc-
currences of the word in the Wikipedia page for
the word sense; (ii) occurrences of the word in
Wikipedia pages pointing to the page for the word
sense; (iii) occurrences of the word in external
pages linked in the Wikipedia page for the word
sense.
After an initial manual inspection, we decided
to discard external pages for being too noisy, and
we focused on the first two options. We tried three
alternatives:
? TiMBL-core uses only the examples found
in the page for the sense being trained.
? TiMBL-inlinks uses the examples found in
Wikipedia pages pointing to the sense being
trained.
? TiMBL-all uses both sources of examples.
In order to classify a page p with respect to the
senses for a word w, we first disambiguate all oc-
currences of w in the page p. Then we choose the
sense which appears most frequently in the page
according to TiMBL results. In case of ties we
pick up the first sense listed in the Wikipedia dis-
ambiguation page.
We have also experimented with a variant of
the approach that uses our estimation of sense fre-
quencies, similarly to what we did with the VSM
approach. In this case, (i) when there is a tie be-
tween two or more senses (which is much more
likely than in the VSM approach), we pick up the
sense with the highest frequency according to our
estimator; and (ii) when no sense reaches 30% of
the cases in the page to be disambiguated, we also
resort to the most frequent sense heuristic (among
the candidates for the page). This experiment is
called TiMBL-core+freq (we discarded ?inlinks?
and ?all? versions because they were clearly worse
than ?core?).
6.3 Classification Results
Table 4 shows classification results. The accuracy
of systems is reported as precision, i.e. the number
of pages correctly classified divided by the total
number of predictions. This is approximately the
same as recall (correctly classified pages divided
by total number of pages) for our systems, because
the algorithms provide an answer for every page
containing text (actual coverage is 94% because
some pages only contain text as part of an image
file such as photographs and logotypes).
Table 4: Classification Results
Experiment Precision
random .19
most frequent sense (estimation) .46
TiMBL-core .60
TiMBL-inlinks .50
TiMBL-all .58
TiMBL-core+freq .67
VSM .67
VSM-GT .68
VSM-mixed .67
VSM-GT+freq .69
All systems are significantly better than the
random and most frequent sense baselines (using
p < 0.05 for a standard t-test). Overall, both ap-
proaches (using TiMBL WSD machinery and us-
ing VSM) lead to similar results (.67 vs. .69),
which would make VSM preferable because it is
a simpler and more efficient approach. Taking a
1362
Figure 1: Precision/Coverage curves for VSM-GT+freq classification algorithm
closer look at the results with TiMBL, there are a
couple of interesting facts:
? There is a substantial difference between us-
ing only examples taken from the Wikipedia
Web page for the sense being trained
(TiMBL-core, .60) and using examples from
the Wikipedia pages pointing to that page
(TiMBL-inlinks, .50). Examples taken from
related pages (even if the relationship is close
as in this case) seem to be too noisy for the
task. This result is compatible with findings
in (Santamar??a et al, 2003) using the Open
Directory Project to extract examples auto-
matically.
? Our estimation of sense frequencies turns
out to be very helpful for cases where our
TiMBL-based algorithm cannot provide an
answer: precision rises from .60 (TiMBL-
core) to .67 (TiMBL-core+freq). The differ-
ence is statistically significant (p < 0.05) ac-
cording to the t-test.
As for the experiments with VSM, the varia-
tions tested do not provide substantial improve-
ments to the baseline (which is .67). Using idf fre-
quencies obtained from the Google Terabyte cor-
pus (instead of frequencies obtained from the set
of retrieved documents) provides only a small im-
provement (VSM-GT, .68), and adding the esti-
mation of sense frequencies gives another small
improvement (.69). Comparing the baseline VSM
with the optimal setting (VSM-GT+freq), the dif-
ference is small (.67 vs .69) but relatively robust
(p = 0.066 according to the t-test).
Remarkably, the use of frequency estimations
is very helpful for the WSD approach but not for
the SVM one, and they both end up with similar
performance figures; this might indicate that using
frequency estimations is only helpful up to certain
precision ceiling.
6.4 Precision/Coverage Trade-off
All the above experiments are done at maximal
coverage, i.e., all systems assign a sense for every
document in the test collection (at least for every
document with textual content). But it is possible
to enhance search results diversity without anno-
tating every document (in fact, not every document
can be assigned to a Wikipedia sense, as we have
discussed in Section 3). Thus, it is useful to inves-
tigate which is the precision/coverage trade-off in
our dataset. We have experimented with the best
performing system (VSM-GT+freq), introducing
a similarity threshold: assignment of a document
to a sense is only done if the similarity of the doc-
ument to the Wikipedia page for the sense exceeds
the similarity threshold.
We have computed precision and coverage for
every threshold in the range [0.00? 0.90] (beyond
0.90 coverage was null) and represented the results
in Figure 1 (solid line). The graph shows that we
1363
can classify around 20% of the documents with a
precision above .90, and around 60% of the docu-
ments with a precision of .80.
Note that we are reporting disambiguation re-
sults using a conventional WSD test set, i.e., one
in which every test case (every document) has
been manually assigned to some Wikipedia sense.
But in our Web Search scenario, 44% of the
documents were not assigned to any Wikipedia
sense: in practice, our classification algorithm
would have to cope with all this noise as well.
Figure 1 (dotted line) shows how the preci-
sion/coverage curve is affected when the algo-
rithm attempts to disambiguate all documents re-
trieved by Google, whether they can in fact be as-
signed to a Wikipedia sense or not. At a coverage
of 20%, precision drops approximately from .90 to
.70, and at a coverage of 60% it drops from .80 to
.50. We now address the question of whether this
performance is good enough to improve search re-
sults diversity in practice.
6.5 Using Classification to Promote Diversity
We now want to estimate how the reported clas-
sification accuracy may perform in practice to en-
hance diversity in search results. In order to pro-
vide an initial answer to this question, we have
re-ranked the documents for the 40 nouns in our
testbed, using our best classifier (VSM-GT+freq)
and making a list of the top-ten documents with
the primary criterion of maximising the number
of senses represented in the set, and the secondary
criterion of maximising the similarity scores of the
documents to their assigned senses. The algorithm
proceeds as follows: we fill each position in the
rank (starting at rank 1), with the document which
has the highest similarity to some of the senses
which are not yet represented in the rank; once all
senses are represented, we start choosing a second
representative for each sense, following the same
criterion. The process goes on until the first ten
documents are selected.
We have also produced a number of alternative
rankings for comparison purposes:
? clustering (centroids): this method ap-
plies Hierarchical Agglomerative Clustering
? which proved to be the most competitive
clustering algorithm in a similar task (Artiles
et al, 2009) ? to the set of search results,
forcing the algorithm to create ten clusters.
The centroid of each cluster is then selected
Table 5: Enhancement of Search Results Diversity
rank@10 # senses coverage
Original rank 2.80 49%
Wikipedia 4.75 77%
clustering (centroids) 2.50 42%
clustering (top ranked) 2.80 46%
random 2.45 43%
upper bound 6.15 97%
as one of the top ten documents in the new
rank.
? clustering (top ranked): Applies the same
clustering algorithm, but this time the top
ranked document (in the original Google
rank) of each cluster is selected.
? random: Randomly selects ten documents
from the set of retrieved results.
? upper bound: This is the maximal diversity
that can be obtained in our testbed. Note that
coverage is not 100%, because some words
have more than ten meanings in Wikipedia
and we are only considering the top ten doc-
uments.
All experiments have been applied on the full
set of documents in the testbed, including those
which could not be annotated with any Wikipedia
sense. Coverage is computed as the ratio of senses
that appear in the top ten results compared to the
number of senses that appear in all search results.
Results are presented in Table 5. Note that di-
versity in the top ten documents increases from
an average of 2.80 Wikipedia senses represented
in the original search engine rank, to 4.75 in the
modified rank (being 6.15 the upper bound), with
the coverage of senses going from 49% to 77%.
With a simple VSM algorithm, the coverage of
Wikipedia senses in the top ten results is 70%
larger than in the original ranking.
Using Wikipedia to enhance diversity seems to
work much better than clustering: both strategies
to select a representative from each cluster are un-
able to improve the diversity of the original rank-
ing. Note, however, that our evaluation has a bias
towards using Wikipedia, because only Wikipedia
senses are considered to estimate diversity.
Of course our results do not imply that the
Wikipedia modified rank is better than the original
1364
Google rank: there are many other factors that in-
fluence the final ranking provided by a search en-
gine. What our results indicate is that, with simple
and efficient algorithms, Wikipedia can be used as
a reference to improve search results diversity for
one-word queries.
7 Related Work
Web search results clustering and diversity in
search results are topics that receive an increas-
ing attention from the research community. Diver-
sity is used both to represent sub-themes in a broad
topic, or to consider alternative interpretations for
ambiguous queries (Agrawal et al, 2009), which
is our interest here. Standard IR test collections do
not usually consider ambiguous queries, and are
thus inappropriate to test systems that promote di-
versity (Sanderson, 2008); it is only recently that
appropriate test collections are being built, such as
(Paramita et al, 2009) for image search and (Ar-
tiles et al, 2009) for person name search. We see
our testbed as complementary to these ones, and
expect that it can contribute to foster research on
search results diversity.
To our knowledge, Wikipedia has not explicitly
been used before to promote diversity in search
results; but in (Gollapudi and Sharma, 2009), it
is used as a gold standard to evaluate diversifica-
tion algorithms: given a query with a Wikipedia
disambiguation page, an algorithm is evaluated as
promoting diversity when different documents in
the search results are semantically similar to dif-
ferent Wikipedia pages (describing the alternative
senses of the query). Although semantic similarity
is measured automatically in this work, our results
confirm that this evaluation strategy is sound, be-
cause Wikipedia senses are indeed representative
of search results.
(Clough et al, 2009) analyses query diversity in
a Microsoft Live Search, using click entropy and
query reformulation as diversity indicators. It was
found that at least 9.5% - 16.2% of queries could
benefit from diversification, although no correla-
tion was found between the number of senses of a
word in Wikipedia and the indicators used to dis-
cover diverse queries. This result does not discard,
however, that queries where applying diversity is
useful cannot benefit from Wikipedia as a sense
inventory.
In the context of clustering, (Carmel et al,
2009) successfully employ Wikipedia to enhance
automatic cluster labeling, finding that Wikipedia
labels agree with manual labels associated by hu-
mans to a cluster, much more than with signif-
icant terms that are extracted directly from the
text. In a similar line, both (Gabrilovich and
Markovitch, 2007) and (Syed et al, 2008) provide
evidence suggesting that categories of Wikipedia
articles can successfully describe common con-
cepts in documents.
In the field of Natural Language Processing,
there has been successful attempts to connect
Wikipedia entries to Wordnet senses: (Ruiz-
Casado et al, 2005) reports an algorithm that
provides an accuracy of 84%. (Mihalcea, 2007)
uses internal Wikipedia hyperlinks to derive sense-
tagged examples. But instead of using Wikipedia
directly as sense inventory, Mihalcea then manu-
ally maps Wikipedia senses into Wordnet senses
(claiming that, at the time of writing the paper,
Wikipedia did not consistently report ambiguity
in disambiguation pages) and shows that a WSD
system based on acquired sense-tagged examples
reaches an accuracy well beyond an (informed)
most frequent sense heuristic.
8 Conclusions
We have investigated whether generic lexical re-
sources can be used to promote diversity in Web
search results for one-word, ambiguous queries.
We have compared WordNet and Wikipedia and
arrived to a number of conclusions: (i) unsurpris-
ingly, Wikipedia has a much better coverage of
senses in search results, and is therefore more ap-
propriate for the task; (ii) the distribution of senses
in search results can be estimated using the in-
ternal graph structure of the Wikipedia and the
relative number of visits received by each sense
in Wikipedia, and (iii) associating Web pages to
Wikipedia senses with simple and efficient algo-
rithms, we can produce modified rankings that
cover 70% more Wikipedia senses than the orig-
inal search engine rankings.
We expect that the testbed created for this re-
search will complement the - currently short - set
of benchmarking test sets to explore search re-
sults diversity and query ambiguity. Our testbed
is publicly available for research purposes at
http://nlp.uned.es.
Our results endorse further investigation on the
use of Wikipedia to organize search results. Some
limitations of our research, however, must be
1365
noted: (i) the nature of our testbed (with every
search result manually annotated in terms of two
sense inventories) makes it too small to extract
solid conclusions on Web searches (ii) our work
does not involve any study of diversity from the
point of view of Web users (i.e. when a Web
query addresses many different use needs in prac-
tice); research in (Clough et al, 2009) suggests
that word ambiguity in Wikipedia might not be re-
lated with diversity of search needs; (iii) we have
tested our classifiers with a simple re-ordering of
search results to test how much diversity can be
improved, but a search results ranking depends on
many other factors, some of them more crucial
than diversity; it remains to be tested how can we
use document/Wikipedia associations to improve
search results clustering (for instance, providing
seeds for the clustering process) and to provide
search suggestions.
Acknowledgments
This work has been partially funded by the Span-
ish Government (project INES/Text-Mess) and the
Xunta de Galicia.
References
R. Agrawal, S. Gollapudi, A. Halverson, and S. Leong.
2009. Diversifying Search Results. In Proc. of
WSDM?09. ACM.
P. Anick. 2003. Using Terminological Feedback for
Web Search Refinement : a Log-based Study. In
Proc. ACM SIGIR 2003, pages 88?95. ACM New
York, NY, USA.
J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS
2 Evaluation Campaign: overview of the Web Peo-
ple Search Clustering Task. In 2nd Web People
Search Evaluation Workshop (WePS 2009), 18th
WWW Conference. 2009.
T. Brants and A. Franz. 2006. Web 1T 5-gram, version
1. Philadelphia: Linguistic Data Consortium.
D. Carmel, H. Roitman, and N. Zwerdling. 2009. En-
hancing Cluster Labeling using Wikipedia. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 139?146. ACM.
C. Carpineto, S. Osinski, G. Romano, and Dawid
Weiss. 2009. A Survey of Web Clustering Engines.
ACM Computing Surveys, 41(3).
Y. Chen, S. Yat Mei Lee, and C. Huang. 2009.
PolyUHK: A Robust Information Extraction System
for Web Personal Names. In Proc. WWW?09 (WePS-
2 Workshop). ACM.
C. Clarke, M. Kolla, G. Cormack, O. Vechtomova,
A. Ashkan, S. Bu?ttcher, and I. MacKinnon. 2008.
Novelty and Diversity in Information Retrieval Eval-
uation. In Proc. SIGIR?08, pages 659?666. ACM.
P. Clough, M. Sanderson, M. Abouammoh, S. Navarro,
and M. Paramita. 2009. Multiple Approaches to
Analysing Query Diversity. In Proc. of SIGIR 2009.
ACM.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2001. TiMBL: Tilburg Memory
Based Learner, version 4.0, Reference Guide. Tech-
nical report, University of Antwerp.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Ex-
plicit Semantic Analysis. In Proceedings of The
20th International Joint Conference on Artificial In-
telligence (IJCAI), Hyderabad, India.
S. Gollapudi and A. Sharma. 2009. An Axiomatic Ap-
proach for Result Diversification. In Proc. WWW
2009, pages 381?390. ACM New York, NY, USA.
R. Mihalcea. 2007. Using Wikipedia for Automatic
Word Sense Disambiguation. In Proceedings of
NAACL HLT, volume 2007.
G. Miller, C. R. Beckwith, D. Fellbaum, Gross, and
K. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicograph,
3(4).
G.A Miller, C. Leacock, R. Tengi, and Bunker R. T.
1993. A Semantic Concordance. In Proceedings of
the ARPA WorkShop on Human Language Technol-
ogy. San Francisco, Morgan Kaufman.
M. Paramita, M. Sanderson, and P. Clough. 2009. Di-
versity in Photo Retrieval: Overview of the Image-
CLEFPhoto task 2009. CLEF working notes, 2009.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
Automatic Assignment of Wikipedia Encyclopaedic
Entries to Wordnet Synsets. Advances in Web Intel-
ligence, 3528:380?386.
M. Sanderson. 2000. Retrieving with Good Sense. In-
formation Retrieval, 2(1):49?69.
M. Sanderson. 2008. Ambiguous Queries: Test Col-
lections Need More Sense. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 499?506. ACM New York, NY, USA.
C. Santamar??a, J. Gonzalo, and F. Verdejo. 2003.
Automatic Association of Web Directories to Word
Senses. Computational Linguistics, 29(3):485?502.
Z. S. Syed, T. Finin, and Joshi. A. 2008. Wikipedia
as an Ontology for Describing Documents. In Proc.
ICWSM?08.
1366
