Limited-Domain Speech-to-Speech Translation
between English and Pashto
Kristin Precoda, Horacio Franco, Ascander Dost, Michael Frandsen, John Fry,
Andreas Kathol, Colleen Richey, Susanne Riehemann, Dimitra Vergyri, Jing Zheng
SRI International
333 Ravenswood Ave.
Menlo Park, CA 94025
Christopher Culy
FX Palo Alto Laboratory, Inc.
3400 Hillview Ave., Building 4
Palo Alto, CA 94304
Abstract
This paper describes a prototype system for
near-real-time spontaneous, bidirectional
translation between spoken English and
Pashto, a language presenting many
technological challenges because of its lack of
resources, including both data and expert
knowledge. Development of the prototype is
ongoing, and we propose to demonstrate a
fully functional version which shows the basic
capabilities, though not yet their final depth
and breadth.
1 Introduction
This demonstration will present a prototype system for
bidirectional speech-to-speech translation within a
limited semantic domain, that of first encounters
between a patient and a medical professional. A major
goal of the work is to explore techniques that are
appropriate for languages that are not of great
commercial interest and that consequently are lacking in
data and resources of many kinds. The system has been
developed for American English and Pashto, one of the
major languages of Afghanistan, which presents a
variety of challenges for both data-intensive and
knowledge-based approaches.
It should be noted that the system must be viewed as
one component in a real-time transaction between two
cooperating humans. The ultimate goal of those humans
is to exchange information by whatever means is
effective: not, necessarily, to rely exclusively on the
system?s output, but to use it in combination with other,
non-speech modalities of conveying meaning and with
ordinary world knowledge.
The final system is intended to run on a handheld
device, such as a PDA, with its attendant memory and
speed limitations and restriction to integer-only
computation. Most components of the demonstration
system run in a PocketPC emulation environment on a
Windows laptop, with a few in the full Windows
environment. All aspects of the prototype system are
undergoing active development.
2 Overall Architecture
A simple description of the architecture is as follows.
The system is controlled by the English speaker, who is
expected to have greater technological familiarity and
who has the benefit of visual feedback on the system
performance. Spoken input, in either English or Pashto,
is recognized by SRI?s small-footprint DynaSpeak?
recognizer, and an ordered list of hypotheses is
produced. The most likely hypothesis is input to SRI's
Gemini natural language parser/generator (Dowding et
al. 1993), which attempts to parse the speech
recognition output. Handling of possible errors or
failures will be discussed in Section 3.
When a successful parse is obtained, Gemini creates
a quasi-logical form representing the meaning of the
sentence. In general, multiple quasi-logical forms may
be created, reflecting differing interpretations of the
input sentence. These forms, which are domain
independent and serve here as an interlingua, can be
ordered by heuristically assigning preferences or
dispreferences to the parsing rules applied to create
them. Gemini uses a grammar for the target language to
generate a translation from the most-preferred
interpretation possible, and outputs a textual
representation of the translation.
Theta, a small-footprint concatenative synthesizer
from Cepstral LLC (Cepstral LLC 2004), then produces
synthetic spoken output in the target language from the
textual representation generated by Gemini. The Pashto
voice was created specifically for this project.
An English and a Pashto version of each component
are called by a single application which includes a
graphical user interface. A screen shot of the interface
is shown in Figure 1.
Figure 1. Screen shot of prototype system
interface.
3 Redundancy and Handling Infelicities
As in any complex system, performance can differ from
the ideal in a number of ways, and it is important for the
system to provide alternative ways to support successful
communication. Some kinds of subideal performance
and recovery approaches are described here.
The most likely hypothesis output by the speech
recognizer may not be exactly what was spoken. When
the input speech is English, the English speaker can see
whether the most likely hypothesis (shown in the
"Input" box in Figure 1) is correct or approximately
correct. If the English speaker judges the hypothesis to
not be close enough to the intended text, s/he may either
repeat the utterance or click on the "Guesses?" button
to see an ordered list of the best hypotheses. A sample
list is shown in Figure 2. If the correct text is on this list,
the user may select it to submit for translation. When
the input speech is in Pashto, this functionality is less
useful, as the Pashto speaker is not assumed to be able
to read, even if the hypotheses were displayed in Pashto
script.
Figure 2. Sample ordered list of recognizer
hypotheses.
Once a recognition hypothesis has been submitted
for translation, several possible problems can arise.
Pashto is a moderately inflected, split-ergative Indo-
European language, and for Pashto in particular,
recognition errors may lead to apparent lack of syntactic
agreement between elements of the sentence which
should (and did in fact) agree. As Gemini tries to
generate a full parse of the input, it has the option of
using parse rules that relax agreement requirements.
These rules are dispreferred and a parse built upon them
may be kept only if a full, grammatically correct parse
cannot be completed. Another possible problem is that
unknown words, some grammatical constructions, and
input errors may render any full parse unachievable. In
this case, fallback strategies can be applied to translate
partial parses, fragments, or any known words. Other
strategies are currently in development.
Another class of approaches for assisting
communication allows the English speaker to quickly
perform certain actions or play high-frequency phrases
to the Pashto speaker. If there is background noise or
distractions or the TTS quality is not high enough for
easy comprehension, pressing the "Replay" button will
immediately play back the last translation result. "Ask
for Rep" plays a prerecorded sentence asking the Pashto
speaker to repeat what s/he said. Several other useful
phrases are available by clicking on the "Phrases?"
button and then selecting from the screen shown in
Figure 3.
Figure 3. Prerecorded Pashto phrases which can
be played back with a single click.
4 Sample Interaction
The table below shows an excerpt of a dialog between
an English and a Pashto speaker, both new to this
system, whose interaction was part of an informal trial
run by the MITRE Corporation in February 2004 for the
DARPA CAST program. The English speaker, a doctor,
had just received eighteen minutes of training in how to
use the system and had had no other exposure to it. The
Pashto speaker, playing the role of an injured patient,
had received training in complaints consistent with a
particular injury scenario and had seen others use the
system, but had not interacted with the system himself.
Except as noted, the translations are understandable.
Spoken input System result
I am a doctor, can I help
you?
[failure to translate
because two sentences]
I am a doctor z@ ddAkttar y@m
Can I help you? AyA z@ d@ tA sara
komak kaw@lay S@m
ho, mehrabAni w@krra yes make [partial
translation; full translation
should have been "yes
please do"]
Where are you hurt? [rerecorded after one
misrecognition] t@ cherta
khugx ye
ghwagx aw ugxa [misrecognized
repeatedly; unable to give
meaningful translation;
should have been "ear and
shoulder"]
Can you breathe? AyA t@ sA akhIst@lay
Se
na, mUSkel lar@m no I have the problem
Do you have pain? AyA t@ dard lare
zAyt of much [minor
misrecognition; translation
should have been "much"]
Do you take medications? AyA t@ dawAuna akhle
[incorrect plural form of
dawA, but understood by
Pashto speaker anyway]
na no
Do you want pain
medication?
AyA t@ d@ dard dawA
ghwArre
ho yes
Do you have allergies? AyA t@ hasAsiyatuna lare
ho yes
What are your allergies? stA hasAsiyat ts@ day
antibyutik [misrecognized but
correctly understood by
doctor as "antibiotics"]
5 Challenges
Three main challenges face this project. First,
commercially nonviable languages, such as Pashto,
often offer very few linguistic resources (such as
linguistic descriptions, acoustic data, texts, language
processing tools). The lack of resources makes
development more difficult, and severely constrains the
approaches that are viable: approaches that rely on large
corpora cannot be used. In addition, there is a shortage
of literate speakers who are available to act as
consultants, and a scarcity of basic knowledge about the
language. This impedes progress and renders difficult
approaches that rely on a large body of hand-crafted
translation rules. The challenge of having no single
person who has a deep understanding of both the
language and the technology and who can serve as a
bridge between them is substantial, and causes more
iterative development than is ordinarily the case when
bilingual technologists are available, as newly
discovered phenomena or new understanding cause
revisions of previous work.
A second major challenge is due to the nature of the
domain and the underlying concept of operations. Real
speech occurs in noisy environments, has disfluencies,
and is highly variable (e.g., phrasings, dialect
differences). In addition, the output of a speech
recognizer contains more and qualitatively different
errors than typical text input to automatic translation
systems. While the problems of real speech are not
unique to this project, they are magnified by the fact
that the non-English speakers will largely be
unsophisticated users of technology, who will often be
using the system for the first and only time. The system
must work well from the very first utterance ? there
cannot be much of a learning curve. This applies to the
translation quality and to other aspects of the system,
such as the synthetic speech, as speakers are not familiar
with synthesized Pashto speech. These speakers are also
not expected to be literate, and their understanding will
not be bolstered by the extra redundancy and
capabilities that the display offers to the English
speaker.
A third major challenge is posed by the handheld
device platform with its computational and storage
limitations, and the near-real-time requirement of the
envisioned usage. The restriction to integer-only
computation is most serious for the speech recognition,
as nearly all medium- or large-vocabulary speech
recognizers perform extensive floating-point
computations, and the conversion of a speech recognizer
to use only integer computation required considerable
effort. The severe memory limitations perhaps impact
most the parsing/generation components of the system.
6 Summary
We have described a prototype of a spoken language
translation system for use by English-speaking medical
personnel treating Pashto-speaking patients.  The
system, which is targeted to a handheld device, is being
developed with extremely little Pashto-language data of
any kind.  It builds on medium-vocabulary speaker-
independent HMM-based speech recognition, rule-
based translation and supporting methods, and
concatenative synthesis. While the system is certainly
still under development, it has provided a reasonable
proof of concept in informal trials.
Acknowledgments
This work was supported under SPAWAR contract N66001-
99-D-8504 with funding from DARPA. Many thanks are due
to Mohammad Shahabuddin Khan, David Kale, Robert J.
Podesva, Valerie Wagner, and many Pashto speakers who
worked with us in various capacities.
References
Cepstral, LLC. 2004. Theta: Small footprint text-to-
speech synthesizer, Pittsburgh, PA, http://
www.cepstral.com
J. Dowding, J. M. Gawron, D. E. Appelt, J. Bear, L.
Cherny, R. Moore, and D. B. Moran. 1993. Gemini:
A natural language system for spoken language
understanding.  Proceedings of the Thirty-First
Annual Meeting of the Association for Computational
Linguistics, 54-61.
Proceedings of NAACL HLT 2009: Short Papers, pages 265?268,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Anchored Speech Recognition for Question Answering
Sibel Yaman1, Gokhan Tur2, Dimitra Vergyri2, Dilek Hakkani-Tur1,
Mary Harper3 and Wen Wang2
1 International Computer Science Institute
2 SRI International
3 Hopkins HLT Center of Excellence, University of Maryland
{sibel,dilek}@icsi.berkeley.edu,{gokhan,dverg,wwang}@speech.sri.com,mharper@casl.umd.edu
Abstract
In this paper, we propose a novel question
answering system that searches for responses
from spoken documents such as broadcast
news stories and conversations. We propose a
novel two-step approach, which we refer to as
anchored speech recognition, to improve the
speech recognition of the sentence that sup-
ports the answer. In the first step, the sen-
tence that is highly likely to contain the an-
swer is retrieved among the spoken data that
has been transcribed using a generic automatic
speech recognition (ASR) system. This candi-
date sentence is then re-recognized in the sec-
ond step by constraining the ASR search space
using the lexical information in the question.
Our analysis showed that ASR errors caused
a 35% degradation in the performance of the
question answering system. Experiments with
the proposed anchored recognition approach
indicated a significant improvement in the per-
formance of the question answering module,
recovering 30% of the answers erroneous due
to ASR.
1 Introduction
In this paper, we focus on finding answers to user
questions from spoken documents, such as broad-
cast news stories and conversations. In a typical
question answering system, the user query is first
processed by an information retrieval (IR) system,
which finds out the most relevant documents among
massive document collections. Each sentence in
these relevant documents is processed to determine
whether or not it answers user questions. Once a
candidate sentence is determined, it is further pro-
cessed to extract the exact answer.
Answering factoid questions (i.e., questions like
?What is the capital of France??) using web makes
use of the redundancy of information (Whittaker et
al., 2006). However, when the document collection
is not large and when the queries are complex, as
in the task we focus on in this paper, more sophis-
ticated syntactic, semantic, and contextual process-
ing of documents and queries is performed to ex-
tract or construct the answer. Although much of the
work on question answering has been focused on
written texts, many emerging systems also enable
either spoken queries or spoken document collec-
tions (Lamel et al, 2008). The work we describe
in this paper also uses spoken data collections to
answer user questions but our focus is on improv-
ing speech recognition quality of the documents by
making use of the wording in the queries. Consider
the following example:
Manual transcription: We understand from Greek of-
ficials here that it was a Russian-made rocket which is
available in many countries but certainly not a weapon
used by the Greek military
ASR transcription: to stand firm greek officials here that
he was a a russian made rocket uh which is available in
many countries but certainly not a weapon used by he
great moments
Question: What is certainly not a weapon used by the
Greek military?
Answer: a Russian-made rocket
Answering such questions requires as good ASR
transcriptions as possible. In many cases, though,
there is one generic ASR system and a generic lan-
guage model to use. The approach proposed in this
paper attempts to improve the ASR performance by
re-recognizing the candidate sentence using lexical
information from the given question. The motiva-
265
tion is that the question and the candidate sentence
should share some common words, and therefore
the words of the answer sentence can be estimated
from the given question. For example, given a fac-
toid question such as: ?What is the tallest build-
ing in the world??, the sentence containing its an-
swer is highly likely to include word sequences such
as: ?The tallest building in the world is NAME?
or ?NAME, the highest building in the world, ...?,
where NAME is the exact answer.
Once the sentence supporting the answer is lo-
cated, it is re-recognized such that the candidate an-
swer is constrained to include parts of the question
word sequence. To achieve this, a word network is
formed to match the answer sentence to the given
question. Since the question words are taken as a ba-
sis to re-recognize the best-candidate sentence, the
question acts as an anchor, and therefore, we call
this approach anchored recognition.
In this work, we restrict out attention to questions
about the subject, the object and the locative, tempo-
ral, and causative arguments. For instance, the fol-
lowings are the questions of interest for the sentence
Obama invited Clinton to the White House to discuss
the recent developments:
Who invited Clinton to the White House?
Who did Obama invite to the White House?
Why did Obama invite Clinton to the White House?
2 Sentence Extraction
The goal in sentence extraction is determining the
sentence that is most likely to contain the answer
to the given question. Our sentence extractor relies
on non-stop word n-gram match between the ques-
tion and the candidate sentence, and returns the sen-
tence with the largest weighted average. Since not
all word n-grams have the same importance (e.g.
function vs. content words), we perform a weighted
sum as typically done in the IR literature, i.e., the
matching n-grams are weighted with respect to their
inverse document frequency (IDF) and length.
A major concern for accurate sentence extraction
is the robustness to speech recognition errors. An-
other concern is dealing with alternative word se-
quences for expressing the same meaning. To tackle
the second challenge, one can also include syn-
onyms, and compare paraphrases of the question and
the candidate answer. Since our main focus is on ro-
Predicate
Sentence ExtractionSemantic Roles
Answering Sentence
Answer Extraction Anchored Recognition
Document
Speech Recognition
Question
Searched Argument
Baseline
Proposed
Figure 1: Conceptual scheme of the baseline and pro-
posed information distillation system.
bustness to speech recognition errors, our data set
is limited to those questions that are worded very
similarly to the candidate answers. However, the
approach is more general, and can be extended to
tackle both challenges.
3 Answer Extraction
When the answer is to be extracted from ASR out-
put, the exact answers can be erroneous because (1)
the exact answer phrase might be misrecognized, (2)
other parts of the sentence might be misrecognized,
so the exact answer cannot be extracted either be-
cause parser fails or because the sentence cannot
match the query.
The question in the example in the Introduction
section is concerned with the object of the predicate
?is? rather than of the other predicates ?understand?
or ?was?. Therefore, a pre-processing step is needed
to correctly identify the object (in this example) that
is being asked, which is described next.
Once the best candidate sentence is estimated, a
syntactic parser (Harper and Huang, ) that also out-
puts function tags is used to parse both the ques-
tion and candidate answering sentence. The parser
is trained on Fisher, Switchboard, and speechified
Broadcast News, Brown, and Wall Street Journal
treebanks without punctuation and case to match in-
put the evaluation conditions.
An example of such a syntactic parse is given in
Figure 2. As shown there, the ?SBJ? marks the sur-
face subject of a given predicate, and the ?TMP? tag
marks the temporal argument. There are also the
?DIR? and ?LOC? tags indicating the locative ar-
gument and the ?PRP? tag indicating the causal ar-
gument. Such parses not only provide a mechanism
to extract information relating to the subject of the
predicate of interest, but also to extract the part of
266
Figure 2: The function tags assist in finding the subject,
object, and arguments of a given predicate.
the sentence that the question is about, in this ex-
ample ?a Russian-made rocket [which] is certainly
not a weapon used by the Greek military?. The ex-
traction of the relevant part is achieved by matching
the predicate of the question to the predicates of the
subsentences in the best candidate sentence. Once
such syntactic parses are obtained for the part of the
best-candidate sentence that matches the question, a
set of rules are used to extract the argument that can
answer the question.
4 Anchored Speech Recognition
In this study we employed a state-of-the-art broad-
cast news and conversations speech recognition
system (Stolcke et al, 2006). The recognizer
performs a total of seven decoding passes with
alternating acoustic front-ends: one based on
Mel frequency cepstral coefficients (MFCCs) aug-
mented with discriminatively estimated multilayer-
perceptron (MLP) features, and one based on per-
ceptual linear prediction (PLP) features. Acoustic
models are cross-adapted during recognition to out-
put from previous recognition stages, and the output
of the three final decoding steps are combined via
confusion networks.
Given a question whose answer we expect to find
in a given sentence, we construct a re-decoding net-
work to match that question. We call this process an-
chored speech recognition, where the anchor is the
question text. Note that this is different than forced
alignment, which enforces the recognition of an au-
dio stream to align with some given sentence. It is
used for detecting the start times of individual words
or for language learning applications to exploit the
acoustic model scores, since there is no need for a
language model.
Our approach is also different than the so-called
flexible alignment (Finke and Waibel, 1997), which
is basically forced alignment that allows skipping
any part of the given sentence, replacing it with a re-
ject token, or inserting hesitations in between words.
In our task, we require all the words in the ques-
tion to be in the best-candidate sentence without any
skips or insertions. If we allow flexible alignment,
then any part of the question could be deleted. In the
proposed anchored speech recognition scheme, we
allow only pauses and rejects between words, but do
not allow any deletions or skips.
The algorithm for extracting anchored recognition
hypotheses is as follows: (i) Construct new recogni-
tion and rescoring language models (LMs) by inter-
polating the baseline LMs with those trained from
only the question sentences and use the new LM
to generate lattices - this aims to bias the recogni-
tion towards word phrases that are included in the
questions. (ii) Construct for each question an ?an-
chored? word network that matches the word se-
quence of the question, allowing any other word se-
quence around it. For example if the question is
WHAT did Bruce Gordon say?, we construct a word
network to match Bruce Gordon said ANYTHING
where ?ANYTHING? is a filler that allows any word
(a word loop). (iii) Intersect the recognition lat-
tices from step (i) with the anchored network for
each question in (ii), thus extracting from the lattice
only the paths that match as answers to the ques-
tion. Then rescore that new lattice with higher order
LM and cross-word adapted acoustic models to get
the best path. (iv) If the intersection part in (iii) fails
then we use a more constrained recognition network:
Starting with the anchored network in (ii) we first
limit the vocabulary in the ANYTHING word-loop
sub-network to only the words that were included in
the recognition lattice from step (i). Then we com-
pose this network with the bigram LM (from step (i))
to add bigram probabilities to the network. Vocab-
ulary limitation is done for efficiency reasons. We
also allow optional filler words and pauses to this
network to allow for hesitations, non-speech events
and pauses within the utterance we are trying to
match. This may limit somewhat the potential im-
provement from this approach and we are working
267
Question Type ASR Output Manual Trans.
Subject 85% 98%
Object 75% 90%
Locative Arg. 81% 93%
Temporal Arg. 94% 98%
Reason 86% 100%
Total 83% 95%
Table 1: Performance figures for the sentence extraction
system using automatic and manual transcriptions.
Question ASR Manual Anchored
Type Output Trans. Output
Subject 51% 77% 61%
Object 41% 73% 51%
Locative Arg. 18% 22% 22%
Temporal Arg. 55% 73% 63%
Reason 26% 47% 26%
Total 44% 68% 52%
Table 2: Performance figures for the answer extraction
system using automatic and manual transcriptions com-
pared with anchored recognition outputs.
towards enhancing the vocabulary with more candi-
date words that could contain the spoken words in
the region. (v) Then we perform recognition with
the new anchored network and extract the best path
through it. Thus we enforce partial alignment of
the audio with the question given, while the regu-
lar recognition LM is still used for the parts outside
the question.
5 Experiments and Results
We performed experiments using a set of questions
and broadcast audio documents released by LDC for
the DARPA-funded GALE project Phase 3. In this
dataset we have 482 questions (177 subject, 160 ob-
ject, 73 temporal argument, 49 locative argument,
and 23 reason) from 90 documents. The ASR word
error rate (WER) for the sentences from which the
questions are constructed is 37% with respect to
noisy closed captions. To factor out IR noise we as-
sumed that the target document is given.
Table 1 presents the performance of the sentence
extraction system using manual and automatic tran-
scriptions. As seen, the system is almost perfect
when there is no noise, however performance de-
grades about 12% with the ASR output.
The next set of experiments demonstrate the per-
formance of the answer extraction system when the
correct sentence is given using both automatic and
manual transcriptions. As seen from Table 2, the
answer extraction performance degrades by about
35% relative using the ASR output. However, using
the anchored recognition approach, this improves to
23%, reducing the effect of the ASR noise signifi-
cantly1 by more than 30% relative. This is shown
in the last column of this table, demonstrating the
use of the proposed approach. We observe that the
WER of the sentences for which we now get cor-
rected answers is reduced from 45% to 28% with
this approach, a reduction of 37% relative.
6 Conclusions
We have presented a question answering system
for querying spoken documents with a novel an-
chored speech recognition approach, which aims to
re-decode an utterance given the question. The pro-
posed approach significantly lowers the error rate for
answer extraction. Our future work involves han-
dling audio in foreign languages, that is robust to
both ASR and machine translation noise.
Acknowledgments: This work was funded by DARPA un-
der contract No. HR0011-06-C-0023. Any conclusions or rec-
ommendations are those of the authors and do not necessarily
reflect the views of DARPA.
References
M. Finke and A. Waibel. 1997. Flexible transcription
alignment. In Proceedings of the IEEE ASRU Work-
shop, Santa Barbara, CA.
M. Harper and Z. Huang. Chinese Statistical Parsing,
chapter To appear.
L. Lamel, S. Rosset, C. Ayache, D. Mostefa, J. Turmo,
and P. Comas. 2008. Question answering on speech
transcriptions: the qast evaluation in clef. In Proceed-
ings of the LREC, Marrakech, Morocco.
A. Stolcke, B. Chen, H. Franco, V. R. R. Gadde,
M. Graciarena, M.-Y. Hwang, K. Kirchhoff, N. Mor-
gan, X. Lin, T. Ng, M. Ostendorf, K. Sonmez,
A. Venkataraman, D. Vergyri, W. Wang, J. Zheng,
and Q. Zhu. 2006. Recent innovations in speech-
to-text transcription at SRI-ICSI-UW. IEEE Trans-
actions on Audio, Speech, and Language Processing,
14(5):1729?1744, September.
E. W. D. Whittaker, J. Mrozinski, and S. Furui. 2006.
Factoid question answering with web, mobile and
speech interfaces. In Proceedings of the NAACL/HLT,
Morristown, NJ.
1according to the Z-test with 0.95 confidence interval
268
Automatic diacritization of Arabic for Acoustic Modeling in
Speech Recognition
Dimitra Vergyri
Speech Technology and Research Lab.,
SRI International,
Menlo Park, CA 94025, USA
dverg@speech.sri.com
Katrin Kirchhoff
Department of Electrical Engineering,
University of Washington,
Seattle, WA 98195, USA
katrin@ee.washington.edu
Abstract
Automatic recognition of Arabic dialectal speech is
a challenging task because Arabic dialects are es-
sentially spoken varieties. Only few dialectal re-
sources are available to date; moreover, most avail-
able acoustic data collections are transcribed with-
out diacritics. Such a transcription omits essen-
tial pronunciation information about a word, such
as short vowels. In this paper we investigate var-
ious procedures that enable us to use such train-
ing data by automatically inserting the missing dia-
critics into the transcription. These procedures use
acoustic information in combination with different
levels of morphological and contextual constraints.
We evaluate their performance against manually dia-
critized transcriptions. In addition, we demonstrate
the effect of their accuracy on the recognition perfor-
mance of acoustic models trained on automatically
diacritized training data.
1 Introduction
Large-vocabulary automatic speech recognition
(ASR) for conversational Arabic poses several
challenges for the speech research community.
The most difficult problems in developing highly
accurate speech recognition systems for Arabic
are the predominance of non-diacritized text
material, the enormous dialectal variety, and
the morphological complexity.
Most available acoustic training material for
Arabic ASR is transcribed in the Arabic script
form, which does not include short vowels and
other diacritics that reflect differences in pro-
nunciation, such as the shadda, tanween, etc. In
particular, almost all additional text data that
can easily be obtained (e.g. broadcast news cor-
pora) is represented in standard script form. To
our knowledge, the only available corpus that
does include detailed phonetic information is
the CallHome (CH) Egyptian Colloquial Ara-
bic (ECA) corpus distributed by the Linguis-
tic Data Consortium (LDC). This corpus has
been transcribed in both the script form and
a so-called romanized form, which is an ASCII
representation that includes short vowels and
other diacritic information and thus has com-
plete pronunciation information. It is quite
challenging to create such a transcription: na-
tive speakers of Arabic are not used to writing
their language in a ?romanized? form, or even in
fully diacritized script form. Consequently, this
task is considered almost as difficult as phonetic
transcription. Transcribing a sufficiently large
amount of training data in this way is there-
fore labor-intensive and costly since it involves
(re)-training native speakers for this purpose.
The constraint of having mostly non-
diacritized texts as recognizer training material
leads to problems for both acoustic and lan-
guage modeling. First, it is difficult to train
accurate acoustic models for short vowels if
their identity and location in the signal is not
known. Second, the absence of diacritics leads
to a larger set of linguistic contexts for a given
word form; language models trained on non-
diacritized material may therefore be less pre-
dictive than those trained on diacritized texts.
Both of these factors may lead to a loss in
recognition accuracy. Previous work (Kirchhoff
et al, 2002; Lamel, 2003) has shown that ig-
noring available vowel information does indeed
lead to a significant increase in both language
model perplexity and word error rate. There-
fore, we are interested in automatically deriv-
ing a diacritized transcription from the Arabic
script representation when a manual diacritiza-
tion is not available. Some software companies
(Sakhr, Apptek, RDI) have developed commer-
cial products for the automatic diacritization of
Arabic. However, these products use only text-
based information, such as the syntactic context
and possible morphological analyses of words, to
predict diacritics. In the context of diacritiza-
tion for speech recognition, by contrast, acous-
tic data is available that can be used as an ad-
ditional knowledge source. Moreover, commer-
cial products concentrate exclusively on Modern
Standard Arabic (MSA), whereas a common ob-
jective of Arabic ASR is conversational speech
recognition, which is usually dialectal. For this
reason, a more flexible set of tools is required
in order to diacritize dialectal Arabic prior to
speech recognizer training.
In this work we investigate the relative ben-
efits of a variety of knowledge sources (acous-
tic, morphological, and contextual) to automat-
ically diacritize MSA transcriptions. We eval-
uate the different approaches in two different
ways: (a) by comparing the automatic output
against a manual reference diacritization and
computing the diacritization error rate, and (b)
by using automatically diacritized training data
in a cross-dialectal speech recognition applica-
tion.
The remainder of this paper is structured as
follows: Section 2 gives a detailed description of
the motivation as well as prior work. Section 3
describes the corpora used for the experiments
reported in this paper. The automatic diacriti-
zation procedures and results are explained in
Section 4. The speech recognition experiments
and results are reported in Section 5. Section 6
presents our conclusions.
2 Motivation and Prior Work
We first describe the Arabic writing system
and its inherent problems for speech recognizer
training, and then discuss previous attempts at
automatic diacritization.
2.1 The Arabic Writing System
The Arabic alphabet consists of twenty-eight
letters, twenty-five of which represent conso-
nants and three of which represent the long
vowels (/i:/,/a:/,/u:/). A distinguishing fea-
ture of Arabic-script based writing systems is
that short vowels are not represented by the
letters of the alphabet. Instead, they are
marked by so-called diacritics, short strokes
placed either above or below the preceding con-
sonant. Several other pronunciation phenom-
ena are marked by diacritics, such as consonant
doubling (phonemic in Arabic), which is indi-
cated by the ?shadda? sign, and the ?tanween?,
i.e. word-final adverbial markers that add /n/ to
the pronunciation of the word. These diacritics
are listed in Table 1. Arabic texts are almost
never fully diacritized; normally, diacritics are
used sparingly and only to prevent misunder-
standings. Exceptions are important religious
and/or political texts or beginners? texts for
MSA Symbol Name Meaning

@ fatHa /a/
@

kasra /i/

@ Damma /u/

P shadda consonant doubling
?

PX sukuun vowel absence

@ tanween al-fatHa /an/
@

tanween al-kasr /in/

@ tanween aD-Damm /un/
Table 1: Arabic diacritics
students of Arabic. The lack of diacritics may
lead to considerable lexical ambiguity that must
be resolved by contextual information, which
in turn presupposes knowledge of the language.
It was observed in (Debili et al, 2002) that
a non-diacritized dictionary word form has 2.9
possible diacritized forms on average and that
an Arabic text containing 23,000 word forms
showed an average ratio of 1:11.6. The form
I
.

J?, for instance, has 21 possible diacritiza-
tions. The correspondence between graphemes
and phonemes is relatively transparent com-
pared to other languages like English or French:
apart from certain special graphemes (e.g. laam
alif), the relationship is one to one. Finally,
it is worth noting that the writing system de-
scribed above is that of MSA. Arabic dialects
are primarily oral varieties in that they do not
have generally agreed-upon writing standards.
Whenever there is the need to write down di-
alectal speech, speakers will try to approximate
the standard system as far as possible and use a
phonetic spelling for non-MSA or foreign words.
The lack of diacritics in standard Arabic texts
makes it difficult to use non-diacritized text for
training since the location and identity of short
vowels and other phonetic segments are un-
known. One possible approach is to use acous-
tic models for long vowels and consonants only,
where the acoustic signal portions correspond-
ing to unwritten segments are implicitly incor-
porated into the acoustic models for consonants
(Billa et al 2002). However, this leads to less
discriminative acoustic and language models.
Previous work (Kirchhoff et al, 2002; Lamel,
2003) has compared the word error rates of
two CH ECA recognizers: one trained on script
transcriptions and another trained on roman-
ized transcriptions. It was shown that the loss
in information due to training on script forms
results in significantly worse performance: a rel-
ative increase in word error rate of almost 10%
was observed.
It seems clear that diacritized data should be
used for training Arabic ASR systems whenever
possible. As explained above, however, it is very
expensive to obtain manually transcribed data
in a diacritized form. Therefore, the corpora
that do include detailed transcriptions are fairly
small and any dialectal data that might become
available in the future will also very likely be
of limited size. By contrast, it is much easier
to collect publicly available data (e.g. broadcast
news data) and to transcribe it in script form.
In order to be able to take advantage of such
resources, we need to restore short vowels and
other missing diacritics in the transcription.
2.2 Prior Work
Various software companies have developed
automatic diacritization products for Arabic.
However, all of these are targeted towards MSA;
to our knowledge, there are no products for di-
alectal Arabic. In a previous study (Kirchhoff
et al, 2002) one of these products was tested
on three different texts, two MSA texts and one
ECA text. It was found that the diacritization
error rate (percentage of missing and wrongly
identified or inserted diacritics) on MSA ranged
between 9% and 28%, depending on whether or
not case vowel endings were counted. However,
on the ECA text, the diacritization software ob-
tained an error rate of 48%.
A fully automatic approach to diacritization
was presented in (Gal, 2002), where an HMM-
based bigram model was used for decoding
diacritized sentences from non-diacritized sen-
tences. The technique was applied to the Quran
and achieved 14% word error (incorrectly dia-
critized words).
A first attempt at developing an automatic
diacritizer for dialectal speech was reported in
(Kirchhoff et al, 2002). The basic approach
was to use a small set of parallel script and dia-
critized data (obtained from the ECA CallHome
corpus) and to derive diacritization rules in an
example-based way. This entirely knowledge-
free approach achieved a 16.6% word error rate.
Other studies (El-Imam, 2003) have ad-
dressed problems of grapheme-to-phoneme con-
version in Arabic, e.g. for the purpose of speech
synthesis, but have assumed that a fully dia-
critized version of the text is already available.
Several knowledge sources are available for
determining the most appropriate diacritization
of a script form: analysis of the morphological
structure of the word (including segmentation
into stems, prefixes, roots and patterns), con-
sideration of the syntactic context in which the
word form appears, and, in the context of speech
recognition, the acoustic data that accompanies
the transcription. Specific dictionary informa-
tion could in principle be added (such as infor-
mation about proper names), but this knowl-
edge source is ignored for the purpose of this
study. All of the approaches described above
make use of text-based information only and do
not attempt to use acoustic information.
3 Data
For the present study we used two different cor-
pora, the FBIS corpus of MSA speech and the
LDC CallHome ECA corpus.
The FBIS corpus is a collection of radio news-
casts from various radio stations in the Ara-
bic speaking world (Cairo, Damascus, Bagh-
dad) totaling approximately 40 hours of speech
(roughly 240K words). The transcription of the
FBIS corpus was done in Arabic script only
and does not contain any diacritic information.
There were a total of 54K different script forms,
with an average of 2.5 different diacritizations
per word.
The CallHome corpus, made available by
LDC, consists of informal telephone conversa-
tions between native speakers (friends and fam-
ily members) of Egyptian Arabic, mostly from
the Cairene dialect region. The corpus con-
sists of about 20 hours of training data (roughly
160K words) and 6 hours of test data. It is tran-
scribed in two different ways: (a) using stan-
dard Arabic script, and (b) using a romaniza-
tion scheme developed at LDC and distributed
with the corpus. The romanized transcription
contains short vowels and phonetic segments
corresponding to other diacritics. It is not en-
tirely equivalent to a diacritized Arabic script
representation since it includes additional in-
formation. For instance, symbols particular to
Egyptian Arabic were used (e.g. ?g? for /g/,
the ECA pronunciation of the MSA letter `),
whereas the script transcriptions contain MSA
letters only. In general, the romanized tran-
scription provides more information about ac-
tual pronunciation and is thus closer to a broad
phonetic transcription.
4 Automatic Diacritization
We describe three techniques for the automatic
diacritization of Arabic text data. The first
combines acoustic, morphological and contex-
tual information to predict the correct form, the
second ignores contextual information, and the
third is fully acoustics based. The latter tech-
nique uses no morphological or syntactic con-
straints, and allows for all possible items to be
inserted at every possible position.
4.1 Combination of Acoustic,
Morphological and Contextual
Information
Most Arabic script forms can have a number
of possible morphological interpretations, which
often correspond to different diacritized forms.
Our goal is to combine morphological knowledge
with contextual information in order to identify
possible diacritizations and assign probabilities
to them. Our procedure is as follows:
1. Generate all possible diacritized variants
for each word, along with their morphological
analyses (tags).
2. Train an unsupervised tagger to assign
probabilities to sequences of these morpholog-
ical tags.
3. Use the trained tagger to assign proba-
bilities to all possible diacritizations for a given
utterance.
For the first step we used the Buckwalter
stemmer, which is an Arabic morphological
analysis tool available from the LDC. The stem-
mer produces all possible morphological anal-
yses of a given Arabic script form; as a by-
product it also outputs the concomitant dia-
critized word forms. An example of the output
is shown in Figure 1. The next step was to train
an unsupervised tagger on the output to obtain
tag n-gram probabilities. The number of differ-
ent morphological tags generated by applying
the stemmer to the FBIS text was 763. In or-
der to obtain a smaller tag set and to be able
to estimate probabilities for tag sequences more
robustly, this initial tag needed to be conflated
to a smaller set. We adopted the set used in
the LDC Arabic TreeBank project, which was
also developed based on the Buckwalter mor-
phological analysis scheme. The FBIS tags were
mapped to TreeBank tags using longest com-
mon substring matching; this resulted in 392
tags. Further possible reductions of the tag
set were investigated but it was found that too
much clustering (e.g. of verb subclasses into a
LOOK-UP WORD: ?J
.

? (qbl)
SOLUTION 1: (qabola) qabola/PREP
(GLOSS): + before +
SOLUTION 2: (qaboli) qaboli/PREP
(GLOSS): + before +
SOLUTION 3: (qabolu) qabolu/ADV
(GLOSS): + before/prior +
SOLUTION 4:(qibal) qibal/NOUN
(GLOSS): + (on the) part of +
SOLUTION 5:(qabila)
qabil/VERB PERFECT+a/PVSUFF SUBJ:3MS
(GLOSS): + accept/receive/approve + he/it <verb>
SOLUTION 6: (qab?ala)
qab al/VERB PERFECT+a/PVSUFF SUBJ:3MS
(GLOSS): + kiss + he/it <verb>
Figure 1: Sample output of Buckwalter stem-
mer showing the possible diacritizations and
morphological analyses of the script form ?J
.

?
(qbl). Lower-case o stands for sukuun (lack of
vowel).
single verb class) could result in the loss of im-
portant information. For instance, the tense
and voice features of verbs are strong predictors
of the short vowel patterns and should therefore
be preserved in the tagset.
We adopted a standard statistical trigram
tagging model:
P (t0, . . . , tn|w0, . . . , wn) =
n?
i=0
P (wi|ti)P (ti|ti?1, ti?2) (1)
where t is a tag, w is a word, and n is the to-
tal number of words in the sentence. In this
model, words (i.e. non-diacritized script forms)
and morphological tags are treated as observed
random variables during training. Training is
done in an unsupervised way, i.e. the correct
morphological tag assignment for each word is
not known. Instead, all possible assignments
are initially considered and the Expectation-
Maximization (EM) training procedure itera-
tively trains the probability distributions in the
above model (the probability of word given
tag, P (wi|ti), and the tag sequence probabil-
ity, P (ti|ti?1, ti?2)) until convergence. During
testing, only the word sequence is known and
the best tag assignment is found by maximiz-
ing the probability in Equation 1. We used the
graphical modeling toolkit GMTK (Bilmes and
Zweig, 2002) to train the tagger. The trained
tagger was then used to assign probabilities to
all possible sequences of three successive mor-
phological tags and their associated diacritiza-
tions to all utterances in the FBIS corpus.
Using the resulting possible diacritizations
for each utterance we constructed a word-
pronunciation network with the probability
scores assigned by the tagger acting as transi-
tion weights. These word networks were used
as constraining recognition networks with the
acoustic models trained on the CallHome cor-
pus to find the most likely word sequence (a
process called alignment). We performed this
procedure with different weights on the tagger
probabilities to see how much this information
should be weighted compared to the acoustic
scores. Results for weights 1 and 5 are reported
below.
Since the Buckwalter stemmer does not pro-
duce case endings, the word forms obtained
by adding case endings were included as vari-
ants in the pronunciation dictionary used by the
aligner. Additional variants listed in the dictio-
nary are the taa marbuta alternations /a/ and
/at/. In some cases (approximately 1.5% of all
words) the Buckwalter stemmer was not able to
produce an analysis of the word form due to mis-
spellings or novel words. These were mapped to
a generic reject model.
4.2 Combination of Acoustic and
Morphological Constraints
We were interested in separately evaluating the
usefulness of the probabilistic contextual knowl-
edge provided by the tagger, and the morpho-
logical knowledge contributed by the Buckwal-
ter tool. To that end we used the word networks
produced by the method described above but
stripped the tagger probabilities, thus assigning
uniform probability to all diacritized forms pro-
duced by the morphological analyzer. We used
the same acoustic models to find the most likely
alignment from the word networks.
4.3 Using only Acoustic Information
Similarly, we wanted to evaluate the importance
of using morphological information versus only
acoustic information to constrain the possible
diacritizations. This is particularly interesting
since, as new dialectal speech data become avail-
able, the acoustics may be the only informa-
tion source. As explained above, existing mor-
phological analysis tools such as the Buckwalter
stemmer have been developed for MSA only.
For that purpose, we generated word net-
works that include all possible short vowels at
each allowed position in the word and allowed
all possible case endings. This means that af-
ter every consonant there are at least 5 dif-
ferent choices: no vowel (corresponding to the
sukuun diacritic), /i/, /a/, /u/, or consonant
doubling caused by a shadda sign. Combina-
tions of shadda and a short vowel are also pos-
sible. Since we do not use acoustic models for
doubled consonants in our speech recognizer, we
ignore the variants involving shadda and allow
only four possibilities after every word-medial
consonant: the three short vowels or absence of
a vowel. Finally, we include the three tanween
endings in addition to these four possibilities in
word-final position. As before, the taa marbuta
variants are also included.
In this way, many more possible ?pronuncia-
tions? are generated for a script form than could
ever occur. The number of possible variants in-
creases exponentially with the number of pos-
sible vowel slots in the word. For instance, for
a longer word with 7 possible positions, more
than 16K diacritized forms are possible, not
even counting the possible word endings. As be-
fore, we use these large pronunciation networks
to constrain our alignment with acoustic models
trained on CallHome data and choose the most
likely path as the output diacritization.
In principle it would also be possible to deter-
mine diacritization performance in the absence
of acoustic information, using only morphologi-
cal and contextual knowledge. This can be done
by selecting the best path from the weighted
word transition networks without rescoring the
network with acoustic models. However, this
would not lead to a valid comparison in our case
because case endings are only represented in the
pronunciation dictionary used by the acoustic
aligner; they are not present in the weighted
transition network and thus cannot be hypoth-
esized unless the acoustic aligner is used.
4.4 Autodiacritization Error Rates
We measured the performance of all three meth-
ods by comparing the output against hand tran-
scribed references on a 500 word subset of the
FBIS corpus. These references were fully dia-
critized script transcriptions created by a na-
tive speaker of Arabic who was trained in or-
thographic transcription but not in phonetic
transcription. The diacritization error rate was
measured as the percentage of wrong diacritiza-
tion decisions out of all possible decisions. In
particular, an error occurs when:
? a vowel is inserted although the reference
transcription shows either sukuun or no dia-
critic mark at the corresponding position (in-
sertion).
? no vowel is produced by the automatic pro-
cedure but the reference contains a vowel mark
at the corresponding position (deletion).
? the short vowel inserted does not match the
vowel at the corresponding position (substitu-
tion).
? in the case of tanween and taa marbuta end-
ings, either the required consonants or vowels
are missing or wrongly inserted. Thus, in the
case of a taa marbuwta ending with a following
case vowel /i/, for instance, both the /t/ and
the /i/ need to be present. If either is missing,
one error is assigned; if both are missing, two
errors are assigned.
Results are listed in Table 2. The first column
reports the error rate at the word level, i.e. the
percentage of words that contained at least one
diacritization mistake. The second column lists
the diacritization error computed as explained
above. The first three methods have a very sim-
ilar performance with respect to diacritization
error rate. The use of contextual information
(the tagger probabilities) gives a slight advan-
tage, although the difference is not statistically
significant. Despite these small differences, the
word error rate is the same for all three meth-
ods; this is because a word that contains at least
one mistake is counted as a word error, regard-
less of the total number of mistakes in the word,
which may vary from system to system. Using
only acoustic information doubles the diacriti-
zation error rate and increases the word error
rate to 50%. Errors result mostly from incorrect
insertions of vowels (e.g. X@

Y
	
?

K
.
? X@

Y

	
?

K
.
). Many
of these insertions may stem from acoustic ef-
fects created by neighbouring consonants, that
give a vowel-like quality to transitions between
consonants. The main benefit of using morpho-
logical knowledge lies in the prevention of such
spurious vowel insertions, since only those inser-
tions are permitted which result in valid words.
Even without the use of morphological infor-
mation, the vast majority of the missing vowels
are still identified correctly. Thus, this method
might be of use when diacritizing a variety of
Arabic for which morphological analysis tools
are not available. Note that the results obtained
here are not directly comparable to any of the
works described in Section 2.2 since we used a
data set with a much larger vocabulary size.
Word Character
Information used level level
acoustic + morphological
+ contextual 27.3 13.24
(tagger prob. weight=5)
acoustic + morphological
+ contextual 27.3 11.54
(tagger prob. weight=1)
acoustic + morphological
(tagger prob. weight=0) 27.3 11.94
acoustic only 50.0 23.08
Table 2: Automatic diacritization error rates
(%).
5 ASR Experiments
Our overall goal is to use large amounts of MSA
acoustic data to enrich training material for a
speech recognizer for conversational Egyptian
Arabic. The ECA recognizer was trained on the
romanized transcription of the CallHome cor-
pus described above and uses short vowel mod-
els. In order to be able to use the phonetically
deficient MSA transcriptions, we first need to
convert them to a diacritized form. In addition
to measuring autodiacritization error rates, as
above, we would like to evaluate the different
diacritization procedures by investigating how
acoustic models trained on the different outputs
affect ASR performance.
One motivation for using cross-dialectal data
is the assumption that infrequent triphones in
the CallHome corpus might have more training
samples in the larger MSA corpus. In (Kirch-
hoff and Vergyri, 2004) we demonstrated that
it is possible to get a small improvement in this
task by combining the scores of models trained
strictly on CallHome (CH) with models trained
on the combined FBIS+CH data, where the
FBIS data was diacritized using the method de-
scribed in Section 4.1. Here we compare that ex-
periment with the experiments where the meth-
ods described in Sections 4.2 and 4.3 were used
for diacritizing the FBIS corpus.
5.1 Baseline System
The baseline system was trained with only
CallHome data (CH-only). For these exper-
iments we used a single front-end (13 mel-
frequency cepstral coefficients with first and
second differences). Mean and variance as
well as Vocal Tract Length (VTL) normaliza-
tion were performed per conversation side for
CH and per speaker cluster (obtained auto-
matically) for FBIS. We trained non-crossword,
System dev96 eval03
simple CH-only 56.1 42.7
RT-2003 CH-only 52.6 39.7
Table 3: CH-only baseline WER (%)
continuous-density, genonic hidden Markov
models (HMMs) (Digalakis and Murveit, 1994),
with 128 gaussians per genone and 250 genones.
Recognition was done by SRI?s DECIPHERTM
engine in a multipass approach: in the first
pass, phone-loop adaptation with two Max-
imum Likelihood Linear Regression (MLLR)
transforms was applied. A recognition lexicon
with 18K words and a bigram language model
were used to generate the first pass recogni-
tion hypothesis. In the second pass the acoustic
models were adapted using constrained MLLR
(with 6 transformations) based on the previ-
ous hypothesis. Bigram lattices were generated
and then expanded using a trigram language
model. Finally, N-best lists were generated us-
ing the adapted models and the trigram lattices.
The final best hypothesis was obtained using N-
best ROVER (?). This system is simpler than
our best current recognition system (submitted
for the NIST RT-2003 benchmark evaluations)
(Stolcke et al, 2003) since we used a single front
end (instead of a combination of systems based
on different front ends) and did not include
HLDA, cross-word triphones, MMIE training
or a more complex language model. The lack
of these features resulted in a higher error rate
but our goal here was to explore exclusively the
effect of the additional MSA training data us-
ing different diacritization approaches. Table 3
shows the word error rates of the system used
for these experiments and the full system used
for the NIST RT-03 evaluations. Our full sys-
tem was about 2% absolute worse than the best
system submitted for that task. This shows that
even though the system is simpler we are not
operating far from the state-of-the-art perfor-
mance for this task.
5.2 ASR Systems Using FBIS Data
In order to investigate the effect of additional
MSA training data, we trained a system similar
to the baseline but used training data pooled
from both corpora (CH+FBIS). After perform-
ing alignment of the FBIS data with the net-
works described in Section 4.1, 10% of the data
was discarded since no alignments could be
found. This could be due to segmentation prob-
lems or noise in the acoustic files. The remain-
ing 90% were used for our experiments. In or-
der to account for the fact that we had much
more data, and also more dissimilar data, we
increased the model size to 300 genones.
For training the CH+FBIS acoustic models,
we first used the whole data set with weight
2 for CH utterances and 1 for FBIS utterances.
Models were then MAP adapted on the CH-only
data (Digalakis et al, 1995). Since training in-
volves several EM iterations, we did not want
to keep the diacritization fixed from the first
pass, which used CH-only models. At every it-
eration, we obtain better acoustic models which
can be used to re-align the data. Thus, for the
first two approaches, where the size of the pro-
nunciation networks is limited due to the use
of morphological information, the EM forward-
backward counts were collected using the whole
diacritization network and the best diacritiza-
tion path was allowed to change at every iter-
ation. In the last case, where only acoustic in-
formation was used, the pronunciation networks
were too large to be run efficiently. For this rea-
son, we updated the diacritized references once
during training by realigning the networks with
the newer models after the first training iter-
ation. As reported in (Kirchhoff and Vergyri,
2004) the CH+FBIS trained system by itself did
not improve much over the baseline (we only
found a small improvement on the eval03 test-
set) but it provided sufficiently different infor-
mation, so that ROVER combination (Fiscus,
1997) with the baseline yielded an improvement.
As we can see in Table 4, all diacritization pro-
cedures performed practically the same: there
was no significant difference in the word error
rates obtained after the combination with the
CH-only baseline. This suggests that we may
be able to obtain improvements with automat-
ically diacritized data even when using inaccu-
rate diacritization, produced without the use of
morphological constraints.
6 Conclusions
In this study we have investigated different op-
tions for automatically diacritizing Arabic text
for use in acoustic model training for ASR. A
comparison of the different approaches showed
that more linguistic information (morphology
and syntactic context) in combination with
the acoustics provides lower diacritization er-
ror rates. However, there is no significant dif-
ference among the word error rates of ASR sys-
dev96 eval03
System alone Rover with CH-only alone Rover with CH-only
CH-only 56.1 42.7
CH+FBIS1(weight 1) 56.3 55.3 42.2 41.6
CH+FBIS1(weight 5) 56.1 55.2 42.2 41.8
CH+FBIS2 56.2 55.3 42.4 41.6
CH+FBIS3 56.6 55.7 42.1 41.6
Table 4: Word error rates (%) obtained after the final recognition pass and with ROVER combina-
tion with the baseline system. FBIS1, FBIS2 and FBIS3 correspond to the diacritization procedures
described in Sections 4.1, 4.2 and 4.3 respectively. For the first approach we report results using
the tagger probabilities with weights 1 and 5.
tems trained on data resulting from the different
methods. This result suggests that it is pos-
sible to use automatically diacritized training
data for acoustic modeling, even if the data has
a comparatively high diacritization error rate
(23% in our case). Note, however, that one
reason for this may be that the acoustic mod-
els are finally adapted to the accurately tran-
scribed CH-only data. In the future, we plan to
apply knowledge-poor diacritization procedures
to other dialects of Arabic, for which morpho-
logical analyzers do not exist.
7 Acknowledgments
This work was funded by DARPA under con-
tract No. MDA972-02-C-0038. We are grateful
to Kathleen Egan for making the FBIS corpus
available to us, and to Andreas Stolcke and Jing
Zheng for valuable advice on several aspects of
this work.
References
J. Billa et al 2002. Audio indexing of Broad-
cast News. In Proceedings of ICASSP.
J. Bilmes and G. Zweig. 2002. The Graphical
Models Toolkit: An open source software sys-
tem for speech and time-series processing. In
Proceedings of ICASSP.
F. Debili, H. Achour, and E Souissi. 2002. De
l?e?tiquetage grammatical a` la voyellation au-
tomatique de l?arabe. Technical report, Cor-
respondances de l?Institut de Recherche sur
le Maghreb Contemporain.
V. Digalakis and H. Murveit. 1994.
GENONES: Optimizing the degree of
mixture tying in a large vocabulary hidden
markov model based speech recognizer. In
Proceeding of ICASSP, pages I?537?540.
V.V. Digalakis, D. Rtischev, and L. G.
Neumeyer. 1995. Speaker adaptation using
constrained estimation of gaussian mixtures.
IEEE Transactions SAP, 3:357?366.
Yousif A. El-Imam. 2003. Phonetization of
Arabic: rules and algorithms. Computer,
Speech and Language, in press, preprint avail-
able online at www.sciencedirect.com.
J. G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer
output voting error reduction (ROVER). In
Proceedings IEEE Automatic Speech Recog-
nition and Understanding Workshop, pages
347?352, Santa Barbara, CA.
Ya?akov Gal. 2002. An HMM approach to vowel
restoration in Arabic and Hebrew. In Pro-
ceedings of the Workshop on Computational
Approaches to Semitic Languages, pages 27?
33, Philadelphia, July. Association for Com-
putational Linguistics.
K. Kirchhoff and D. Vergyri. 2004. Cross-
dialectal acoustic data sharing for Ara-
bic speech recognition. In Proceedings of
ICASSP.
K. Kirchhoff, J. Bilmes, J. Henderson,
R. Schwartz, M. Noamany, P. Schone, G. Ji,
S. Das, M. Egan, F. He, D. Vergyri, D. Liu,
and N. Duta. 2002. Novel approaches to Ara-
bic speech recognition - final report from the
JHU summer workshop 2002. Technical re-
port, Johns Hopkins University.
L. Lamel. 2003. Personal communication.
A. Stolcke, Y. Konig, and M. Weintraub. 1997.
Explicit word error minimization in N-best
list rescoring. In Proceedings of Eurospeech,
volume 1, pages 163?166.
A. Stolcke et al 2003. Speech-to-text re-
search at sri-icsi-uw. Technical report, NIST
RT-03 Spring Workshop. availble online
http://www.nist.gov/speech/tests/rt/rt2003/
spring/presentations/sri+-rt03-stt.pdf.
