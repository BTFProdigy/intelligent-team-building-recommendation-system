Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 248?258,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Metaphor Detection with Cross-Lingual Model Transfer
Yulia Tsvetkov Leonid Boytsov Anatole Gershman Eric Nyberg Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ytsvetko, srchvrs, anatoleg, ehn, cdyer}@cs.cmu.edu
Abstract
We show that it is possible to reliably dis-
criminate whether a syntactic construction
is meant literally or metaphorically using
lexical semantic features of the words that
participate in the construction. Our model
is constructed using English resources,
and we obtain state-of-the-art performance
relative to previous work in this language.
Using a model transfer approach by piv-
oting through a bilingual dictionary, we
show our model can identify metaphoric
expressions in other languages. We pro-
vide results on three new test sets in Span-
ish, Farsi, and Russian. The results sup-
port the hypothesis that metaphors are
conceptual, rather than lexical, in nature.
1 Introduction
Lakoff and Johnson (1980) characterize metaphor
as reasoning about one thing in terms of another,
i.e., a metaphor is a type of conceptual mapping,
where words or phrases are applied to objects and
actions in ways that do not permit a literal inter-
pretation. They argue that metaphors play a fun-
damental communicative role in verbal and writ-
ten interactions, claiming that much of our every-
day language is delivered in metaphorical terms.
There is empirical evidence supporting the claim:
recent corpus studies have estimated that the pro-
portion of words used metaphorically ranges from
5% to 20% (Steen et al, 2010), and Thibodeau and
Boroditsky (2011) provide evidence that a choice
of metaphors affects decision making.
Given the prevalence and importance of
metaphoric language, effective automatic detec-
tion of metaphors would have a number of ben-
efits, both practical and scientific. Language pro-
cessing applications that need to understand lan-
guage or preserve meaning (information extrac-
tion, machine translation, dialog systems, senti-
ment analysis, and text analytics, etc.) would have
access to a potentially useful high-level bit of in-
formation about whether something is to be under-
stood literally or not. Second, scientific hypothe-
ses about metaphoric language could be tested
more easily at a larger scale with automation.
However, metaphor detection is a hard problem.
On one hand, there is a subjective component: hu-
mans may disagree whether a particular expres-
sion is used metaphorically or not, as there is no
clear-cut semantic distinction between figurative
and metaphorical language (Shutova, 2010). On
the other, metaphors can be domain- and context-
dependent.
1
Previous work has focused on metaphor identi-
fication in English, using both extensive manually-
created linguistic resources (Mason, 2004; Gedi-
gian et al, 2006; Krishnakumaran and Zhu, 2007;
Turney et al, 2011; Broadwell et al, 2013) and
corpus-based approaches (Birke and Sarkar, 2007;
Shutova et al, 2013; Neuman et al, 2013; Shutova
and Sun, 2013; Hovy et al, 2013). We build on
this foundation and also extend metaphor detec-
tion into other languages in which few resources
may exist. Our work makes the following con-
tributions: (1) we develop a new state-of-the-art
English metaphor detection system that uses con-
ceptual semantic features, such as a degree of ab-
stractness and semantic supersenses;
2
(2) we cre-
ate new metaphor-annotated corpora for Russian
and English;
3
(3) using a paradigm of model trans-
fer (McDonald et al, 2011; T?ackstr?om et al, 2013;
Kozhenikov and Titov, 2013), we provide sup-
port for the hypothesis that metaphors are concep-
1
For example, drowning students could be used metaphor-
ically to describe the situation where students are over-
whelmed with work, but in the sentence a lifeguard saved
drowning students, this phrase is used literally.
2
https://github.com/ytsvetko/metaphor
3
http://www.cs.cmu.edu/
?
ytsvetko/
metaphor/datasets.zip
248
tual (rather than lexical) in nature by showing that
our English-trained model can detect metaphors in
Spanish, Farsi, and Russian.
2 Methodology
Our task in this work is to define features that dis-
tinguish between metaphoric and literal uses of
two syntactic constructions: subject-verb-object
(SVO) and adjective-noun (AN) tuples.
4
We give
examples of a prototypical metaphoric usage of
each type:
? SVO metaphors. A sentence containing a
metaphoric SVO relation is my car drinks
gasoline. According to Wilks (1978), this
metaphor represents a violation of selectional
preferences for the verb drink, which is nor-
mally associated with animate subjects (the
car is inanimate and, hence, cannot drink in
the literal sense of the verb).
? AN metaphors. The phrase broken promise
is an AN metaphor, where attributes from
a concrete domain (associated with the con-
crete word broken) are transferred to a more
abstract domain, which is represented by the
relatively abstract word promise. That is, we
map an abstract concept promise to a concrete
domain of physical things, where things can
be literally broken to pieces.
Motivated by Lakoff?s (1980) argument that
metaphors are systematic conceptual mappings,
we will use coarse-grained conceptual, rather than
fine-grained lexical features, in our classifier. Con-
ceptual features pertain to concepts and ideas as
opposed to individual words or phrases expressed
in a particular language. In this sense, as long as
two words in two different languages refer to the
same concepts, their conceptual features should
be the same. Furthermore, we hypothesize that
our coarse semantic features give us a language-
invariant representation suitable for metaphor de-
tection. To test this hypothesis, we use a cross-
lingual model transfer approach: we use bilingual
dictionaries to project words from other syntactic
constructions found in other languages into En-
glish and then apply the English model on the de-
rived conceptual representations.
4
Our decision to focus on SVO and AN metaphors is jus-
tified by corpus studies that estimate that verb- and adjective-
based metaphors account for a substantial proportion of all
metaphoric expressions, approximately 60% and 24%, re-
spectively (Shutova and Teufel, 2010; Gandy et al, 2013).
Each SVO (or AN) instance will be represented
by a triple (duple) from which a feature vector
will be extracted.
5
The vector will consist of the
concatenation of the conceptual features (which
we discuss below) for all participating words, and
conjunction features for word pairs.
6
For example,
to generate the feature vector for the SVO triple
(car, drink, gasoline), we compute all the features
for the individual words car, drink, gasoline and
combine them with the conjunction features for
the pairs car drink and drink gasoline.
We define three main feature categories (1) ab-
stractness and imageability, (2) supersenses, (3)
unsupervised vector-space word representations;
each category corresponds to a group of features
with a common theme and representation.
? Abstractness and imageability. Abstract-
ness and imageability were shown to be use-
ful in detection of metaphors (it is easier to
invoke mental pictures of concrete and im-
ageable words) (Turney et al, 2011; Broad-
well et al, 2013). We expect that abstract-
ness, used in conjunction features (e.g., a
feature denoting that the subject is abstract
and the verb is concrete), is especially use-
ful: semantically, an abstract agent perform-
ing a concrete action is a strong signal of
metaphorical usage.
Although often correlated with abstractness,
imageability is not a redundant property.
While most abstract things are hard to visu-
alize, some call up images, e.g., vengeance
calls up an emotional image, torture calls up
emotions and even visual images. There are
concrete things that are hard to visualize too,
for example, abbey is harder to visualize than
banana (B. MacWhinney, personal commu-
nication).
? Supersenses. Supersenses
7
are coarse se-
mantic categories originating in WordNet.
For nouns and verbs there are 45 classes:
26 for nouns and 15 for verbs, for example,
5
Looking at components of the syntactic constructions in-
dependent of their context has its limitations, as discussed
above with the drowning students example; however, it sim-
plifies the representation challenges considerably.
6
If word one is represented by features u ? R
n
and word
two by features v ? R
m
then the conjunction feature vector
is the vectorization of the outer product uv
>
.
7
Supersenses are called ?lexicographer classes? in Word-
Net documentation (Fellbaum, 1998), http://wordnet.
princeton.edu/man/lexnames.5WN.html
249
noun.body, noun.animal, verb.consumption,
or verb.motion (Ciaramita and Altun, 2006).
English adjectives do not, as yet, have a sim-
ilar high-level semantic partitioning in Word-
Net, thus we use a 13-class taxonomy of ad-
jective supersenses constructed by Tsvetkov
et al (2014) (discussed in ?3.2).
Supersenses are particularly attractive fea-
tures for metaphor detection: coarse sense
taxonomies can be viewed as semantic con-
cepts, and since concept mapping is a pro-
cess in which metaphors are born, we
expect different supersense co-occurrences
in metaphoric and literal combinations.
In ?drinks gasoline?, for example, map-
ping to supersenses would yield a pair
<verb.consumption, noun.substance>, con-
trasted with <verb.consumption, noun.food>
for ?drinks juice?. In addition, this coarse
semantic categorization is preserved in trans-
lation (Schneider et al, 2013), which makes
supersense features suitable for cross-lingual
approaches such as ours.
? Vector space word representations. Vec-
tor space word representations learned us-
ing unsupervised algorithms are often effec-
tive features in supervised learning methods
(Turian et al, 2010). In particular, many such
representations are designed to capture lex-
ical semantic properties and are quite effec-
tive features in semantic processing, includ-
ing named entity recognition (Turian et al,
2009), word sense disambiguation (Huang et
al., 2012), and lexical entailment (Baroni et
al., 2012). In a recent study, Mikolov et
al. (2013) reveal an interesting cross-lingual
property of distributed word representations:
there is a strong similarity between the vec-
tor spaces across languages that can be eas-
ily captured by linear mapping. Thus, vector
space models can also be seen as vectors of
(latent) semantic concepts, that preserve their
?meaning? across languages.
3 Model and Feature Extraction
In this section we describe a classification model,
and provide details on mono- and cross-lingual
implementation of features.
3.1 Classification using Random Forests
To make classification decisions, we use a random
forest classifier (Breiman, 2001), an ensemble of
decision tree classifiers learned from many inde-
pendent subsamples of the training data. Given
an input, each tree classifier assigns a probabil-
ity to each label; those probabilities are averaged
to compute the probability distribution across the
ensemble. Random forest ensembles are partic-
ularly suitable for our resource-scarce scenario:
rather than overfitting, they produce a limiting
value of the generalization error as the number
of trees increases,
8
and no hyperparameter tuning
is required. In addition, decision-tree classifiers
learn non-linear responses to inputs and often out-
perform logistic regression (Perlich et al, 2003).
9
Our random forest classifier models the probabil-
ity that the input syntactic relation is metaphorical.
If this probability is above a threshold, the relation
is classified as metaphoric, otherwise it is literal.
We used the scikit-learn toolkit to train our
classifiers (Pedregosa et al, 2011).
3.2 Feature extraction
Abstractness and imageability. The MRC psy-
cholinguistic database is a large dictionary listing
linguistic and psycholinguistic attributes obtained
experimentally (Wilson, 1988).
10
It includes,
among other data, 4,295 words rated by the de-
grees of abstractness and 1,156 words rated by the
imageability. Similarly to Tsvetkov et al (2013),
we use a logistic regression classifier to propagate
abstractness and imageability scores from MRC
ratings to all words for which we have vector space
representations. More specifically, we calculate
the degree of abstractness and imageability of all
English items that have a vector space representa-
tion, using vector elements as features. We train
two separate classifiers for abstractness and im-
ageability on a seed set of words from the MRC
database. Degrees of abstractness and imageabil-
ity are posterior probabilities of classifier predic-
tions. We binarize these posteriors into abstract-
concrete (or imageable-unimageable) boolean in-
dicators using pre-defined thresholds.
11
Perfor-
8
See Theorem 1.2 in (Breiman, 2001) for details.
9
In our experiments, random forests model slightly out-
performed logistic regression and SVM classifiers.
10
http://ota.oucs.ox.ac.uk/headers/
1054.xml
11
Thresholds are equal to 0.8 for abstractness and to 0.9
for imageability. They were chosen empirically based on ac-
250
mance of these classifiers, tested on a sampled
held-out data, is 0.94 and 0.85 for the abstractness
and imageability classifiers, respectively.
Supersenses. In the case of SVO relations, we
incorporate supersense features for nouns and
verbs; noun and adjective supersenses are used in
the case of AN relations.
Supersenses of nouns and verbs. A lexical item
can belong to several synsets, which are associ-
ated with different supersenses. Degrees of mem-
bership in different supersenses are represented
by feature vectors, where each element corre-
sponds to one supersense. For example, the word
head (when used as a noun) participates in 33
synsets, three of which are related to the super-
sense noun.body. The value of the feature corre-
sponding to this supersense is 3/33 ? 0.09.
Supersenses of adjectives. WordNet lacks
coarse-grained semantic categories for adjectives.
To divide adjectives into groups, Tsvetkov et al
(2014) use 13 top-level classes from the adapted
taxonomy of Hundsnurscher and Splett (1982),
which is incorporated in GermaNet (Hamp and
Feldweg, 1997). For example, the top-level
classes in GermaNet include: adj.feeling (e.g.,
willing, pleasant, cheerful); adj.substance (e.g.,
dry, ripe, creamy); adj.spatial (e.g., adjacent, gi-
gantic).
12
For each adjective type in WordNet,
they produce a vector with a classifier posterior
probabilities corresponding to degrees of mem-
bership of this word in one of the 13 semantic
classes,
13
similar to the feature vectors we build
for nouns and verbs. For example, for a word
calm the top-2 categories (with the first and second
highest degrees of membership) are adj.behavior
and adj.feeling.
Vector space word representations. We em-
ploy 64-dimensional vector-space word represen-
tations constructed by Faruqui and Dyer (2014).
14
Vector construction algorithm is a variation on
traditional latent semantic analysis (Deerwester
et al, 1990) that uses multilingual information
to produce representations in which synonymous
words have similar vectors. The vectors were
curacy during cross-validation.
12
For the full taxonomy see http://www.sfs.
uni-tuebingen.de/lsd/adjectives.shtml
13
http://www.cs.cmu.edu/
?
ytsvetko/
adj-supersenses.tar.gz
14
http://www.cs.cmu.edu/
?
mfaruqui/soft.
html
trained on the news commentary corpus released
by WMT-2011,
15
comprising 180,834 types.
3.3 Cross-lingual feature projection
For languages other than English, feature vectors
are projected to English features using translation
dictionaries. We used the Babylon dictionary,
16
which is a proprietary resource, but any bilingual
dictionary can in principle be used. For a non-
English word in a source language, we first ob-
tain all translations into English. Then, we av-
erage all feature vectors related to these transla-
tions. Consider an example related to projection
of WordNet supersenses. A Russian word ??????
is translated as head and brain. Hence, we select
all the synsets of the nouns head and brain. There
are 38 such synsets (33 for head and 5 for brain).
Four of these synsets are associated with the su-
persense noun.body. Therefore, the value of the
feature noun.body is 4/38 ? 0.11.
4 Datasets
In this section we describe a training and testing
dataset as well a data collection procedure.
4.1 English training sets
To train an SVO metaphor classifier, we employ
the TroFi (Trope Finder) dataset.
17
TroFi includes
3,737 manually annotated English sentences from
the Wall Street Journal (Birke and Sarkar, 2007).
Each sentence contains either literal or metaphori-
cal use for one of 50 English verbs. First, we use a
dependency parser (Martins et al, 2010) to extract
subject-verb-object (SVO) relations. Then, we fil-
ter extracted relations to eliminate parsing-related
errors, and relations with verbs which are not in
the TroFi verb list. After filtering, there are 953
metaphorical and 656 literal SVO relations which
we use as a training set.
In the case of AN relations, we construct and
make publicly available a training set contain-
ing 884 metaphorical AN pairs and 884 pairs
with literal meaning. It was collected by two
annotators using public resources (collections of
metaphors on the web). At least one additional
person carefully examined and culled the col-
lected metaphors, by removing duplicates, weak
metaphors, and metaphorical phrases (such as
15
http://www.statmt.org/wmt11/
16
http://www.babylon.com
17
http://www.cs.sfu.ca/
?
anoop/students/
jbirke/
251
drowning students) whose interpretation depends
on the context.
4.2 Multilingual test sets
We collect and annotate metaphoric and literal test
sentences in four languages. Thus, we compile
eight test datasets, four for SVO relations, and
four for AN relations. Each dataset has an equal
number of metaphors and non-metaphors, i.e., the
datasets are balanced. English (EN) and Russian
(RU) datasets have been compiled by our team
and are publicly available. Spanish (ES) and Farsi
(FA) datasets are published elsewhere (Levin et al,
2014). Table 1 lists test set sizes.
SVO AN
EN 222 200
RU 240 200
ES 220 120
FA 44 320
Table 1: Sizes of the eight test sets. Each dataset is
balanced, i.e., it has an equal number of metaphors
and non-metaphors. For example, English SVO
dataset has 222 relations: 111 metaphoric and 111
literal.
We used the following procedure to compile the
EN and RU test sets. A moderator started with seed
lists of 1000 most common verbs and adjectives.
18
Then she used the SketchEngine, which pro-
vides searching capability for the TenTen Web cor-
pus,
19
to extract sentences with words that fre-
quently co-occurred with words from the seed
lists. From these sentences, she removed sen-
tences that contained more than one metaphor, and
sentences with non-SVO and non-AN metaphors.
Remaining sentences were annotated by several
native speakers (five for English and six for Rus-
sian), who judged AN and SVO phrases in con-
text. The annotation instructions were general:
?Please, mark in bold all words that, in your opin-
ion, are used non-literally in the following sen-
tences. In many sentences, all the words may be
used literally.? The Fleiss? Kappas for 5 English
and 6 Russian annotators are: EN-AN = .76, RU-
18
Selection of 1000 most common verbs and adjectives
achieves much broader lexical and domain coverage than
what can be realistically obtained from continuous text. Our
test sentence domains are, therefore, diverse: economic, po-
litical, sports, etc.
19
http://trac.sketchengine.co.uk/wiki/
Corpora/enTenTen
AN = .85, EN-SVO = .75, RU-SVO = .78. For the fi-
nal selection, we filtered out low-agreement (<.8)
sentences.
The test candidate sentences were selected by
a person who did not participate in the selection
of the training samples. No English annotators of
the test set, and only one Russian annotator out
of 6 participated in the selection of the training
samples. Thus, we trust that annotator judgments
were not biased towards the cases that the system
is trained to process.
5 Experiments
5.1 English experiments
Our task, as defined in Section 2, is to classify
SVO and AN relations as either metaphoric or lit-
eral. We first conduct a 10-fold cross-validation
experiment on the training set defined in Section
4.1. We represent each candidate relation using
the features described in Section 3.2, and evalu-
ate performance of the three feature categories and
their combinations. This is done by computing an
accuracy in the 10-fold cross validation. Experi-
mental results are given in Table 2, where we also
provide the number of features in each feature set.
SVO AN
# FEAT ACC # FEAT ACC
AbsImg 20 0.73
?
16 0.76
?
Supersense 67 0.77
?
116 0.79
?
AbsImg+Sup. 87 0.78
?
132 0.80
?
VSM 192 0.81 228 0.84
?
All 279 0.82 360 0.86
Table 2: 10-fold cross validation results for three
feature categories and their combination, for clas-
sifiers trained on English SVO and AN training
sets. # FEAT column shows a number of features.
ACC column reports an accuracy score in the 10-
fold cross validation. Statistically significant dif-
ferences (p < 0.01) from the all-feature combina-
tion are marked with a star.
These results show superior performance over
previous state-of-the-art results, confirming our
hypothesis that conceptual features are effective
in metaphor classification. For the SVO task, the
cross-validation accuracy is about 10% better than
that of Tsvetkov et al (2013). For the AN task,
the cross validation accuracy is better by 8% than
the result of Turney et al (2011) (two baseline
252
methods are described in Section 5.2). We can
see that all types of features have good perfor-
mance on their own (VSM is the strongest feature
type). Noun supersense features alone allows us to
achieve an accuracy of 75%, i.e., adjective super-
sense features contribute 4% to adjective-noun su-
persense feature combination. Experiments with
the pairs of features yield better results than in-
dividual features, implying that the feature cate-
gories are not redundant. Yet, combining all fea-
tures leads to even higher accuracy during cross-
validation. In the case of the AN task, a difference
between the All feature combination and any other
combination of features listed in Table 2 is statis-
tically significant (p < 0.01 for both the sign and
the permutation test).
Although the first experiment shows very high
scores, the 10-fold cross-validation cannot fully
reflect the generality of the model, because all
folds are parts of the same corpus. They are col-
lected by the same human judges and belong to the
same domain. Therefore, experiments on out-of-
domain data are crucial. We carry out such exper-
iments using held-out SVO and AN EN test sets,
described in Section 4.2 and Table 1. In this ex-
periment, we measure the f -score. We classify
SVO and AN relations using a classifier trained on
the All feature combination and balanced thresh-
olds. The values of the f -score are 0.76, both for
SVO and AN tasks. This out-of-domain experi-
ment suggests that our classifier is portable across
domains and genres.
However, (1) different application may have
different requirements for recall/precision, and (2)
classification results may be skewed towards hav-
ing high precision and low recall (or vice versa). It
is possible to trade precision for recall by choos-
ing a different threshold. Thus, in addition to
giving a single f -score value for balanced thresh-
olds, we present a Receiver Operator Characteris-
tic (ROC) curve, where we plot a fraction of true
positives against the fraction of false positives for
100 threshold values in the range from zero to one.
The area under the ROC curve (AUC) can be in-
terpreted as the probability that a classifier will as-
sign a higher score to a randomly chosen positive
example than to a randomly chosen negative ex-
ample.
20
For a randomly guessing classifier, the
ROC curve is a dashed diagonal line. A bad classi-
20
Assuming that positive examples are labeled by ones,
and negative examples are labeled by zeros.
fier has an ROC curve that goes close to the dashed
diagonal or even below it.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
Supersenses (area = 0.77)
AbsImg (area = 0.73)
VSM (area = 0.8)
All (area = 0.79)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
AbsImg (area = 0.9)
Supersenses (area = 0.86)
VSM (area = 0.89)
All (area = 0.92)
(b) AN
Figure 1: ROC curves for classifiers trained using
different feature sets (English SVO and AN test
sets).
According to ROC plots in Figure 1, all three
feature sets are effective, both for SVO and for
AN tasks. Abstractness and Imageability features
work better for adjectives and nouns, which is in
line with previous findings (Turney et al, 2011;
Broadwell et al, 2013). It can be also seen that
VSM features are very effective. This is in line
with results of Hovy et al (2013), who found that
it is hard to improve over the classifier that uses
only VSM features.
5.2 Comparison to baselines
In this section, we compare our method to state-of-
the-art methods of Tsvetkov et al (2013) and of
Turney et al (2011), who focused on classifying
SVO and AN relations, respectively.
In the case of SVO relations, we use software
253
and datasets from Tsvetkov et al (2013). These
datasets, denoted as an SVO-baseline, consist of
98 English and 149 Russian sentences. We train
SVO metaphor detection tools on SVO relations
extracted from TroFi sentences and evaluate them
on the SVO-baseline dataset. We also use the same
thresholds for classifier posterior probabilities as
Tsvetkov et al (2013). Our approach is different
from that of Tsvetkov et al (2013) in that it uses
additional features (vector space word representa-
tions) and a different classification method (we use
random forests while Tsvetkov et al (2013) use
logistic regression). According to Table 3, we ob-
tain higher performance scores for both Russian
and English.
EN RU
SVO-baseline 0.78 0.76
This work 0.86 0.85
Table 3: Comparing f -scores of our SVO
metaphor detection method to the baselines.
In the case of AN relations, we use the dataset
(denoted as an AN-baseline) created by Turney
et al (2011) (see Section 4.1 in the referred pa-
per for details). Turney et al (2011) manu-
ally annotated 100 pairs where an adjective was
one of the following: dark, deep, hard, sweet,
and worm. The pairs were presented to five
human judges who rated each pair on a scale
from 1 (very literal/denotative) to 4 (very non-
literal/connotative). Turney et al (2011) train
logistic-regression employing only abstractness
ratings as features. Performance of the method
was evaluated using the 10-fold cross-validation
separately for each judge.
We replicate the above described evaluation
procedure of Turney et al (2011) using their
model and features. In our classifier, we use the
All feature combination and the balanced thresh-
old as described in Section 5.1.
According to results in Table 4, almost all of the
judge-specific f -scores are slightly higher for our
system, as well as the overall average f -score.
In both baseline comparisons, we obtain perfor-
mance at least as good as in previously published
studies.
5.3 Cross-lingual experiments
In the next experiment we corroborate the main
hypothesis of this paper: a model trained on En-
AN-baseline This work
Judge 1 0.73 0.75
Judge 2 0.81 0.84
Judge 3 0.84 0.88
Judge 4 0.79 0.81
Judge 5 0.78 0.77
average 0.79 0.81
Table 4: Comparing AN metaphor detection
method to the baselines: accuracy of the 10-
fold cross validation on annotations of five human
judges.
glish data can be successfully applied to other
languages. Namely, we use a trained English
model discussed in Section 5.1 to classify literal
and metaphoric SVO and AN relations in English,
Spanish, Farsi and Russian test sets, listed in Sec-
tion 4.2. This time we used all available features.
Experimental results for all four languages, are
given in Figure 2. The ROC curves for SVO and
AN tasks are plotted in Figure 2a and Figure 2b,
respectively. Each curve corresponds to a test set
described in Table 1. In addition, we perform an
oracle experiment, to obtain actual f -score values
for best thresholds. Detailed results are shown in
Table 5.
Consistent results with high f -scores are ob-
tained across all four languages. Note that higher
scores are obtained for the Russian test set. We hy-
pothesize that this happens due to a higher-quality
translation dictionary (which allows a more accu-
rate model transfer). Relatively lower (yet rea-
sonable) results for Farsi can be explained by a
smaller size of the bilingual dictionary (thus, fewer
feature projections can be obtained). Also note
that, in our experience, most of Farsi metaphors
are adjective-noun constructions. This is why the
AN FA dataset in Table 1 is significantly larger
than SVO FA. In that, for the AN Farsi task we
observe high performance scores.
Figure 2 and Table 5 confirm, that we ob-
tain similar, robust results on four very differ-
ent languages, using the same English classi-
fiers. We view this result as a strong evidence of
language-independent nature of our metaphor de-
tection method. In particular, this shows that pro-
posed conceptual features can be used to detect se-
lectional preferences violation across languages.
To summarize the experimental section, our
metaphor detection approach obtains state-of-the-
254
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.79)
ES (area = 0.71)
FA (area = 0.69)
RU (area = 0.89)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.92)
ES (area = 0.73)
FA (area = 0.83)
RU (area = 0.8)
(b) AN
Figure 2: Cross-lingual experiment: ROC curves
for classifiers trained on the English data using a
combination of all features, and applied to SVO
and AN metaphoric and literal relations in four test
languages: English, Russian, Spanish, and Farsi.
art performance in English, is effective when ap-
plied to out-of-domain English data, and works
cross-lingually.
5.4 Examples
Manual data analysis on adjective-noun pairs sup-
ports an abstractness-concreteness hypothesis for-
mulated by several independent research studies.
For example, in English we classify as metaphoric
dirty word and cloudy future. Word pairs dirty
diaper and cloudy weather have same adjectives.
Yet they are classified as literal. Indeed, diaper
is a more concrete term than word and weather
is more concrete than future. Same pattern is ob-
served in non-English datasets. In Russian, ????-
??? ???????? ?sick society? and ?????? ????
?empty sound? are classified as metaphoric, while
SVO AN
EN 0.79 0.85
RU 0.84 0.77
ES 0.76 0.72
FA 0.75 0.74
Table 5: Cross-lingual experiment: f -scores for
classifiers trained on the English data using a com-
bination of all features, and applied, with optimal
thresholds, to SVO and AN metaphoric and literal
relations in four test languages: English, Russian,
Spanish, and Farsi.
??????? ??????? ?sick grandmother? and ??-
???? ????? ?empty cup? are classified as literal.
Spanish example of an adjective-noun metaphor
is a well-known m?usculo econ?omico ?economic
muscle?. We also observe that non-metaphoric ad-
jective noun pairs tend to have more imageable ad-
jectives, such as literal derecho humano ?human
right?. In Spanish, human is more imageable than
economic.
Verb-based examples that are correctly clas-
sified by our model are: blunder escaped no-
tice (metaphoric) and prisoner escaped jail (lit-
eral). We hypothesize that supersense features are
instrumental in the correct classification of these
examples: <noun.person,verb.motion> is usually
used literally, while <noun.act,verb.motion> is
used metaphorically.
6 Related Work
For a historic overview and a survey of
common approaches to metaphor detection,
we refer the reader to recent reviews by
Shutova et al (Shutova, 2010; Shutova et al,
2013). Here we focus only on recent approaches.
Shutova et al (2010) proposed a bottom-up
method: one starts from a set of seed metaphors
and seeks phrases where verbs and/or nouns be-
long to the same cluster as verbs or nouns in seed
examples.
Turney et al (2011) show how abstractness
scores could be used to detect metaphorical AN
phrases. Neuman et al (2013) describe a Concrete
Category Overlap algorithm, where co-occurrence
statistics and Turney?s abstractness scores are used
to determine WordNet supersenses that corre-
spond to literal usage of a given adjective or verb.
For example, given an adjective, we can learn that
it modifies concrete nouns that usually have the
255
supersense noun.body. If this adjective modifies
a noun with the supersense noun.feeling, we con-
clude that a metaphor is found.
Broadwell et al (2013) argue that metaphors
are highly imageable words that do not belong
to a discussion topic. To implement this idea,
they extend MRC imageability scores to all dic-
tionary words using links among WordNet super-
senses (mostly hypernym and hyponym relations).
Strzalkowski et al (2013) carry out experiments
in a specific (government-related) domain for four
languages: English, Spanish, Farsi, and Russian.
Strzalkowski et al (2013) explain the algorithm
only for English and say that is the same for Span-
ish, Farsi, and Russian. Because they heavily
rely on WordNet and availability of imageability
scores, their approach may not be applicable to
low-resource languages.
Hovy et al (2013) applied tree kernels to
metaphor detection. Their method also employs
WordNet supersenses, but it is not clear from the
description whether WordNet is essential or can
be replaced with some other lexical resource. We
cannot compare directly our model with this work
because our classifier is restricted to detection of
only SVO and AN metaphors.
Tsvetkov et al (2013) propose a cross-lingual
detection method that uses only English lexical re-
sources and a dependency parser. Their study fo-
cuses only on the verb-based metaphors. Tsvetkov
et al (2013) employ only English and Russian
data. Current work builds on this study, and incor-
porates new syntactic relations as metaphor candi-
dates, adds several new feature sets and different,
more reliable datasets for evaluating results. We
demonstrate results on two new languages, Span-
ish and Farsi, to emphasize the generality of the
method.
A words sense disambiguation (WSD) is a re-
lated problem, where one identifies meanings of
polysemous words. The difference is that in the
WSD task, we need to select an already existing
sense, while for the metaphor detection, the goal
is to identify cases of sense borrowing. Studies
showed that cross-lingual evidence allows one to
achieve a state-of-the-art performance in the WSD
task, yet, most cross-lingual WSD methods em-
ploy parallel corpora (Navigli, 2009).
7 Conclusion
The key contribution of our work is that we show
how to identify metaphors across languages by
building a model in English and applying it?
without adaptation?to other languages: Spanish,
Farsi, and Russian. This model uses language-
independent (rather than lexical or language spe-
cific) conceptual features. Not only do we estab-
lish benchmarks for Spanish, Farsi, and Russian,
but we also achieve state-of-the-art performance
in English. In addition, we present a comparison
of relative contributions of several types of fea-
tures. We concentrate on metaphors in the con-
text of two kinds of syntactic relations: subject-
verb-object (SVO) relations and adjective-noun
(AN) relations, which account for a majority of all
metaphorical phrases.
Future work will expand the scope of metaphor
identification by including nominal metaphoric re-
lations as well as explore techniques for incor-
porating contextual features, which can play a
key role in identifying certain kinds of metaphors.
Second, cross-lingual model transfer can be im-
proved with more careful cross-lingual feature
projection.
Acknowledgments
We are extremely grateful to Shuly Wintner for a
thorough review that helped us improve this draft;
we also thank people who helped in creating the
datasets and/or provided valuable feedback on this
work: Ed Hovy, Vlad Niculae, Davida Fromm,
Brian MacWhinney, Carlos Ram??rez, and other
members of the CMU METAL team. This work
was supported by the U.S. Army Research Labo-
ratory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533.
References
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proc. of
EACL, pages 23?32.
Julia Birke and Anoop Sarkar. 2007. Active learning
for the identification of nonliteral language. In Proc.
of the Workshop on Computational Approaches to
Figurative Language, FigLanguages ?07, pages 21?
28.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
256
George Aaron Broadwell, Umit Boz, Ignacio Cases,
Tomek Strzalkowski, Laurie Feldman, Sarah Taylor,
Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb.
2013. Using imageability and topic chaining to lo-
cate metaphors in linguistic corpora. In Social Com-
puting, Behavioral-Cultural Modeling and Predic-
tion, pages 102?110. Springer.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proc. of EACL. Association for Com-
putational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder,
Newton Howard, Sergey Kanareykin, Moshe Kop-
pel, Mark Last, Yair Neuman, and Shlomo Arga-
mon. 2013. Automatic identification of conceptual
metaphors with limited knowledge. In Proc. of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 328?334.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48.
Birgit Hamp and Helmut Feldweg. 1997. Germanet-
a lexical-semantic net for German. In Proc. of
ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proc. of
the First Workshop on Metaphor in NLP, page 52.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of ACL, pages 873?882.
Franz Hundsnurscher and Jochen Splett. 1982. Se-
mantik der Adjektive des Deutschen. Number 3137.
Westdeutscher Verlag.
Mikhail Kozhenikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models. In
Proc. of ACL, pages 1190?1200.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proc. of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20.
George Lakoff and Mark Johnson. 1980. Conceptual
metaphor in everyday language. The Journal of Phi-
losophy, pages 453?486.
Lori Levin, Teruko Mitamura, Davida Fromm, Brian
MacWhinney, Jaime Carbonell, Weston Feely,
Robert Frederking, Anatole Gershman, and Carlos
Ramirez. 2014. Resources for the detection of con-
ventionalized metaphors in four languages. In Proc.
of LREC.
Andr?e F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M?ario A. T. Figueiredo. 2010.
Turbo parsers: dependency parsing by approximate
variational inference. In Proc. of ENMLP, pages 34?
44.
Zachary J Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proc. of EMNLP.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for Ma-
chine Translation. CoRR, abs/1309.4168.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1?10:69,
February.
Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last,
Shlomo Argamon, Newton Howard, and Ophir
Frieder. 2013. Metaphor identification in large texts
corpora. PloS one, 8(4):e62343.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Si-
monoff. 2003. Tree induction vs. logistic regres-
sion: a learning-curve analysis. Journal of Machine
Learning Research, 4:211?255.
Nathan Schneider, Behrang Mohit, Chris Dyer, Kemal
Oflazer, and Noah A Smith. 2013. Supersense tag-
ging for Arabic: the MT-in-the-middle attack. In
Proc. of NAACL-HLT, pages 661?667.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised
metaphor identification using hierarchical graph fac-
torization clustering. In Proc. of NAACL-HLT,
pages 978?988.
257
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source-target domain
mappings. In Proc. of LREC, pages 3255?3261.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proc. of COLING, pages 1002?1010.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301?353.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proc. of ACL, pages 688?697.
Gerard J Steen, Aletta G Dorst, J Berenike Her-
rmann, Anna A Kaal, and Tina Krennmayr.
2010. Metaphor in usage. Cognitive Linguistics,
21(4):765?796.
Tomek Strzalkowski, George Aaron Broadwell, Sarah
Taylor, Laurie Feldman, Boris Yamrom, Samira
Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,
et al 2013. Robust extraction of metaphors from
novel data. In Proc. of the First Workshop on
Metaphor in NLP, page 67.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1?12.
Paul H Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in
reasoning. PLoS One, 6(2):e16782.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection using
common semantic features. In The 1st Workshop on
Metaphor in NLP 2013, page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting English adjective senses with super-
senses. In Proc. of LREC.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning, pages 1?8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL, pages
384?394.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proc. of EMNL, pages 680?690.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
Michael Wilson. 1988. MRC Psycholinguistic
Database: Machine-usable dictionary, version 2.00.
Behavior Research Methods, Instruments, & Com-
puters, 20(1):6?10.
258
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136?144,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
CONE: Metrics for Automatic Evaluation of Named Entity              
Co-reference Resolution  
 
 
Bo Lin, Rushin Shah, Robert Frederking, Anatole Gershman 
Language Technologies Institute, School of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave., PA 15213, USA 
 {bolin,rnshah,ref,anatoleg}@cs.cmu.edu 
 
 
Abstract 
Human annotation for Co-reference Resolu-
tion (CRR) is labor intensive and costly, and 
only a handful of annotated corpora are cur-
rently available. However, corpora with 
Named Entity (NE) annotations are widely 
available. Also, unlike current CRR systems, 
state-of-the-art NER systems have very high 
accuracy and can generate NE labels that are 
very close to the gold standard for unlabeled 
corpora.  We propose a new set of metrics col-
lectively called CONE for Named Entity Co-
reference Resolution (NE-CRR) that use a 
subset of gold standard annotations, with the 
advantage that this subset can be easily ap-
proximated using NE labels when gold stan-
dard CRR annotations are absent. We define 
CONE B3 and CONE CEAF metrics based on 
the traditional B3 and CEAF metrics and show 
that CONE B3 and CONE CEAF scores of any 
CRR system on any dataset are highly corre-
lated with its B3 and CEAF scores respectively. 
We obtain correlation factors greater than 0.6 
for all CRR systems across all datasets, and a 
best-case correlation factor of 0.8. We also 
present a baseline method to estimate the gold 
standard required by CONE metrics, and show 
that CONE B3 and CONE CEAF scores using 
this estimated gold standard are also correlated 
with B3 and CEAF scores respectively. We 
thus demonstrate the suitability of CONE 
B3and CONE CEAF for automatic evaluation 
of NE-CRR. 
1 Introduction 
Co-reference resolution (CRR) is the problem of 
determining whether two entity mentions in a 
text refer to the same entity in real world or not. 
Noun Phrase CRR (NP-CRR) considers all noun 
phrases as entities, while Named Entity CRR 
restricts itself to noun phrases that describe a 
Named Entity. In this paper, we consider the task 
of Named Entity CRR (NE-CRR) only. Most, if 
not all, recent efforts in the field of CRR have 
concentrated on machine-learning based ap-
proaches. Many of them formulate the problem 
as a pair-wise binary classification task, in which 
possible co-reference between every pair of men-
tions is considered, and produce chains of co-
referring mentions for each entity as their output. 
One of the most important problems in CRR is 
the evaluation of CRR results. Different evalua-
tion metrics have been proposed for this task. B-
cubed (Bagga and Baldwin, 1998) and CEAF 
(Luo, 2005) are the two most popular metrics; 
they compute Precision, Recall and F1 measure 
between matched equivalent classes and use 
weighted sums of Precision, Recall and F1 to 
produce a global score. Like all metrics, B3 and 
CEAF require gold standard annotations; howev-
er, gold standard CRR annotations are scarce, 
because producing such annotations involves a 
substantial amount of human effort since it re-
quires an in-depth knowledge of linguistics and a 
high level of understanding of the particular text. 
Consequently, very few corpora with gold stan-
dard CRR annotations are available (NIST, 2003; 
MUC-6, 1995; Agirre, 2007). By contrast, gold 
standard Named Entity (NE) annotations are easy 
to produce; indeed, there are many NE annotated 
corpora of different sizes and genres. Similarly, 
there are few CRR systems and even the best 
scores obtained by them are only in the region of 
F1 = 0.5 - 0.6. There are only four such CRR 
systems freely available, to the best of our know-
ledge (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). In comparison, there are nu-
merous Named Entity recognition (NER) sys-
tems, both general-purpose and specialized, and 
many of them achieve scores better than F1 = 
0.95 (Ratinov and Roth, 2009; Finkel et al, 
136
2005). Although these facts can be partly attri-
buted to the ?hardness? of CRR compared to 
NER, they also reflect the substantial gap be-
tween NER and CRR research. In this paper, we 
present a set of metrics, collectively called 
CONE, that leverage widely available NER sys-
tems and resources and tools for the task of eva-
luating co-reference resolution systems. The ba-
sic idea behind CONE is to predict a CRR sys-
tem?s performance for the task of full NE-CRR 
on some dataset using its performance for the 
subtask of named mentions extraction and group-
ing (NMEG) on that dataset. The advantage of 
doing so is that measuring NE-CRR performance 
requires the co-reference information of all men-
tions of a Named Entity, including named men-
tions, nominal and pronominal references, while 
measuring the NMEG performance only requires 
co-reference information of named mentions of a 
NE, and this information is relatively easy to ob-
tain automatically even in the absence of gold 
standard annotations. We compute correlation 
between CONE B3, B3, CONE CEAF and CEAF 
scores for various CRR systems on various gold-
standard annotated datasets and show that the 
CONE B3 and B3 scores are highly correlated for 
all such combinations of CRR systems and data-
sets, as are CONE CEAF and CEAF scores, with 
a best-case correlation of 0.8. We produce esti-
mated gold standard annotations for the Enron 
email corpus, since no actual gold standard CRR 
annotations exist for it, and then use CONE B3 
and CONE CEAF with these estimated gold 
standard annotations to compare the performance 
of various NE-CRR systems on this corpus. No 
such comparison has been previously performed 
for the Enron corpus. 
We adopt the same terminology as in (Luo, 
2005): a mention refers to each individual phrase 
and an entity refers to the equivalence class or 
co-reference chain with several mentions. This 
allows us to note some differences between NE-
CRR and NP-CRR. NE-CRR involves indentify-
ing named entities and extracting their co-
referring mentions; equivalences classes without 
any NEs are not considered. NE-CRR is thus 
clearly a subset of NP-CRR, where all co-
referring mentions and equivalence classes are 
considered. However, we focus on NE-CRR be-
cause it is currently a more active research area 
than NP-CRR and a better fit for target applica-
tions such as text forensics and web mining, and 
also because it is more amenable to the automatic 
evaluation approach that we propose. 
The research questions that motivate our work 
are:  
(1) Is it possible to use only NER resources to 
evaluate NE-CRR systems? If so, how is this 
problem formulated?  
(2) How does one perform evaluation in a way 
that is accurate and automatic with least hu-
man intervention?  
(3) How does one perform evaluation on large 
unlabeled datasets?  
We show that our CONE metrics achieve good 
results and represent a promising first step to-
ward answering these questions.  
 
The rest of the paper is organized as follows. We 
present related work in the field of automatic 
evaluation methods for natural language 
processing tasks in Section 2. In Section 3, we 
give an overview of the standard metrics current-
ly used for evaluating co-reference resolution. 
We define our new metrics CONE B3 and CONE 
CEAF in Section 4. In section 5, we provide ex-
perimental results that illustrate the performance 
of CONE B3 and CONE CEAF compared to B3 
and CEAF respectively. In Section 6, we give an 
example of the application of CONE metrics by 
evaluating NE-CRR systems on an unlabeled 
dataset, and discuss possible drawbacks and ex-
tensions of these metrics. Finally, in section 7 we 
present our conclusions and ideas for future 
work.  
2 Related Work 
There has been a substantial amount of research 
devoted to automatic evaluation for natural lan-
guage processing, especially tasks involving lan-
guage generation. The BLEU score (Papineni et 
al., 2002) proposed for evaluating machine trans-
lation results is the best known example of this. 
It uses n-gram statistics between machine gener-
ated results and references. It inspired the 
ROUGE metric (Lin and Hovy, 2003) and other 
methods (Louis and Nenkova, 2009) to perform 
automatic evaluation of text summarization. Both 
these metrics have show strong correlation be-
tween automatic evaluation results and human 
judgments. The two metrics successfully reduce 
the need for human judgment and help speed up 
research by allowing large-scale evaluation. 
Another example is the alignment entropy (Per-
vouchine et al, 2009) for evaluating translitera-
tion alignment. It reduces the need for alignment 
gold standard and highly correlates with transli-
teration system performance. Thus it is able to 
137
serve as a good metric for transliteration align-
ment. We contrast our work with (Stoyanov et al, 
2009), who show that the co-reference resolution 
problem can be separated into different parts ac-
cording to the type of the mention. Some parts 
are relatively easy to solve. The resolver per-
forms equally well in each part across datasets. 
They use the statistics of mentions in different 
parts with test results on other datasets as a pre-
dictor for unseen datasets, and obtain promising 
results with good correlations. We approach the 
problem from a different perspective. In our 
work, we show the correlation between the 
scores on traditional metrics and scores on our 
CONE metrics, and show how to automatically 
estimate the gold standard required by CONE 
metrics. Thus our method is able to predict the 
co-reference resolution performance without 
gold standard at all. We base our new metrics on 
the standard B3 and CEAF metrics used for com-
puting CRR scores. (Vilian et al, 1995; Bagga 
and Baldwin, 1998; Luo, 2005). B3 and CEAF 
are believed to be more discriminative and inter-
pretable than earlier metrics and are widely 
adopted especially for machine-learning based 
approaches.  
 
3 Standard Metrics: B3 and CEAF 
We now provide an overview of the standard B3 
and CEAF metrics used to evaluate CRR sys-
tems. Both metrics assume that a CRR system 
produces a set of equivalence classes {O} and 
assigns each mention to only one class. Let Oi be 
the class to which the ith mention was assigned 
by the system. We also assume that we have a set 
of correct equivalence classes {G} (the gold 
standard). Let Gi be the gold standard class to 
which the ith mention should belong. Let Ni de-
note the number of mentions in Oi which are also 
in Gi ? the correct mentions. B
3 computes the 
presence rate of correct mentions in the same 
equivalent classes. The individual precision and 
recall score is defined as follows: 
|| i
i
i O
NP ?
 
|| i
i
i G
NR ?
 
Here |Oi| and |Gi| are the cardinalities of sets Oi 
and Gi.   
The final precision and recall scores are: 
?
?
?
n
i
ii PwP
1
 ?
?
?
n
i
ii RwR
1
 
Here, in the simplest case the weight wi is set to 
1/n, equal for all mentions. 
CEAF (Luo, 2005) produces the optimal 
matching between output classes and true classes 
first, with the constraint that one true class, Gi, 
can be mapped to at most one output class, say 
Of(i) and vice versa. This can be solved by the 
KM algorithm (Kuhn, 1955; Munkres, 1957) for 
maximum matching in a bipartite graph. CEAF 
then computes the precision and recall score as 
follows: 
?
?
?
i
i
i
ifi
O
M
P
)(,      
?
?
?
i
i
i
ifi
G
M
R
)(,  
jiji GOM ??,
 
We use the terms Mi,j from CEAF to re-write B
3, 
its formulas then reduce to: 
???? i j i
ji
i
i O
M
O
P
2
,1  
???? i j i
ji
i
i G
M
G
R
2
,1  
We can see that B3 simply iterates through all 
pairs of matchings instead of considering the one 
to one mappings as CEAF does. Thus, B3 com-
putes the weighted sum of the F-measures for 
each individual mention which helps alleviate the 
bias in the pure link-based F-measure, while 
CEAF computes the same as B3 but enforces at 
most one matched equivalence class for every 
class in the system output and gold standard out-
put. 
4 CONE B3 and CONE CEAF Metrics:  
We now formally define the new CONE B3 and 
CONE CEAF metrics that we propose for 
automatic evaluation of NE-CRR systems. 
      Let G denote the set of gold standard 
annotations and O denote the output of an NE-
CRR system. Let Gi denote the equivalent class 
of entity i in the gold standard and Oj denote the 
equivalence class for entity j in the system output.  
Also let Gij denote the j
th mention in the 
equivalence class of entity i in the gold standard 
and Oij denote the j
th mention in the system 
output. 
As described earlier, the standard B3 and CEAF 
metrics evaluate scores using G and O and can 
be thought of as functions of the form B3(G, O). 
and CEAF(G, O) respectively. Let us use 
Score(G, O) to collectively refer to both these 
138
functions. An equivalence class Gi in G may 
contain three types of mentions: named mentions 
gNMij, nominal mentions g
NO
ij, and pronominal 
mentions gPRij. Similarly, we can define o
NM
ij, 
oNOij and o
PR
ij for a class Oi in O. Now for each 
gold standard equivalence class Gi and system 
output equivalence class Oi, we define the 
following sets GNMi  and  O
NM
i: 
iijNMijNMiNM GggGi ??? },{,  
iijNMijNMiNM OooOi ??? },{,  
In other words, GNMi and O
NM
i are the subsets of 
Gi and Oi containing all named mentions and no 
mentions of any other type.  
Let GNM denote the set of all such equivalance 
classes GNMi and O
NM denote the set of all 
equivalence classes ONMi. It is clear that G
NM and 
ONM are pruned versions of the gold standard 
annotations and system output respectively. 
We now define CONE B3 and CONE CEAF as 
follows: 
CONE B3 = B3(GNM, ONM) 
CONE CEAF = CEAF(GNM, ONM) 
 
Following our previous notation, we denote 
CONE B3 and CONE CEAF collectively as 
Score(GNM, ONM). We observe that Score(GNM, 
ONM) measures a NE-CRR system?s  
performance for the NE-CRR subtask of named 
mentions extraction and grouping (NMEG). We 
find that Score(GNM, ONM) is highly correlated 
with Score(G, O) for all the freely available NE-
CRR systems over various datasets. This 
provides the neccessary  justification for the use 
of Score(GNM, ONM).  
We use SYNERGY (Shah et al, 2010), an 
ensemble NER system that combines the UIUC 
NER (Ritanov and Roth, 2009) and Stanford 
NER (Finkel et al, 2005) systems, to produce 
GNM and ONM from G and O by  selecting named 
mentions. However, any other good NER system 
would serve the same purpose. 
We see that while standard evaluation metrics 
require the use of G, i.e. the full set of NE-CRR 
gold standard annotations including named, 
nominal and pronimal mentions, CONE metrics 
require only GNM, i.e. gold standard annotations 
consisting of named mentions only. The key 
advantage of using CONE metrics is that GNM 
can be automatically approximated using an 
NER system with a good degree of accuracy. 
This is because state-of-the-art NER systems 
achieve near-optimal performance, exceeding F1 
= 0.95 in many cases, and after obtaining their 
output, the task of estimating GNM reduces to 
simply clustering it to seperate mentions of 
diffrerent real-world entities. This clustering can 
be thought of as a form of named entity matching, 
which is not a very hard problem. There exist 
systems that perform such matching in a 
sophisticated manner with a high degree of 
accuracy. We use simple heuristics such as exact 
matching, word matches, matches between in-
itials, etc. to design such a matching system 
ourselves and use it to obtain estimates of GNM, 
say GNM-approx. We then calculate CONE B3 and 
CONE CEAF scores using GNM-approx instead of 
GNM; in other words, we perform fully automatic 
evaluation of NE-CRR systems by using 
Score(GNM-approx, ONM) instead of Score(GNM, 
ONM). In order to show the validity of this 
evaluation, we calculate the correlation between 
the Score(GNM-approx, ONM) and Score(G, O) for  
different NE-CRR systems across different 
datasets and find that they are indeed correlated. 
CONE thus makes automatic evaluation of NE-
CRR systems possible. By leveraging the widely 
available named entity resources, it reduces the 
need for gold standard annotations in the 
evaluation process. 
4.1 Analysis 
There are two major kinds of errors that affect 
the performance of NE-CRR systems for the full 
NE-CRR task: 
? Missing Named Entity (MNE): If a named 
mention is missing from the system output, 
it is very likely that its nearby nominal and 
anaphoric mentions will be lost, too 
? Incorrectly grouped Named Entity (IGNE): 
Even if the named mention is correctly iden-
tified with its nearby nominal and anaphoric 
mentions to form a chain, it is still possible 
to misclassify the named mentions and its 
co-reference chain 
Consider the following example of these two 
types of errors. Here, the alphabets represent the 
named mentions and numbers represent other 
type of mentions: 
 
Gold standard, G: (A, B, C, 1, 2, 3, 4) 
Output from System 1, O1: (A, B, 1, 2, 3) 
Output from System 2, O2: (A, C, 1, 2, 4), (B, 3) 
O1 shows an example of an MNE error, while 
O2 shows an example of an IGNE error.  
 
Both these types of errors are in fact rooted in 
named mention extraction and grouping 
(NMEG). Therefore, we hypothesize that they 
must be preserved in a NE-CRR system?s output 
139
for the subtask of named mentions extraction and 
grouping (NMEG) and will be reflected in the 
CONE B3 and CONE CEAF metrics that eva-
luate scores for this subtask. Consider the follow-
ing extension of the previous example:  
 
GNM: (A, B, C) 
O1NM: (A, B) 
O2NM: (A, C), (B) 
 
We observe that the MNE error in O1 is pre-
served in O1NM, and the IGNE error in O2 is pre-
served in O2NM. Empirically we sample several 
output files in our experiments and observe the 
same phenomena. Therefore, we argue that it is 
possible to capture the two major kinds of errors 
described by considering only GNM and ONM in-
stead of G and O.  
 
We now provide a more detailed theoretical 
analysis of the CONE metrics. For a given NE-
CRR system and dataset, consider the system 
output O and gold standard annotation G. Let P 
and R indicate precision and recall scores ob-
tained by evaluating O against G, using CEAF. If 
we replace both G and O with their subsets GNM 
and ONM respectively, such that GNM and ONM 
contain only named mentions, we can modify the 
equations for precision and recall for CEAF to 
derive the following equations for precision PNM 
and recall RNM for CONE CEAF: 
??
i
iNMOOSum NM }{
     
??
i
iNMNM GGSum }{
 
??
i
NM
ifi
NM
NM
OSum
M
P }{
)(,     
??
i
NM
ifi
NM
NM
GSum
M
R }{
)(, 
 
The corresponding equations for CONE B3 Pre-
cision are: 
?
?
?
?
i
NM
i
NM
j
ji
NM
NM
OSumO
M
P
}{
2
,
?
?
?
?
i
NM
i
NM
j
ji
NM
RSumR
M
R
}{
'
2
,
 
 
In order to support the hypothesis that CONE 
metrics evaluated using (GNM, ONM) represent an 
effective substitute for standard metrics that use 
(G, O), we compute entity level correlation be-
tween the corresponding CONE and standard 
metrics. For example, in the case of CEAF / 
CONE CEAF Precision, we calculate correlation 
between the following quantities: 
??? }{
)(,
NM
ifi
NM
NM
SSum
M
P?
 and 
??? }{
)(,
SSum
M
P ifi?  
We perform this experiment with the LBJ and 
BART CRR systems on the ACE Phase 2 corpus. 
We illustrate the correlation results in Figure 1.  
 
Figure 1. Correlation between NMP? andP?  - 
Entity Level CEAF Precision 
From Figure 1, we can see that the two 
measures are highly correlated. In fact, we find 
that the Pearson?s correlation coefficient (Soper 
et al, 1917; Cohen, 1988) is 0.73. The points 
lining up on the x-axis and y=1.0 represent very 
small equivalence classes and are a form of noise; 
their removal doesn?t affect this coefficient. To 
show that this strong correlation is not a 
statistical anomaly, we also compute entity-level 
correlation using (Gi - G
NM
i, Oj - O
NM
j) and (Gi, 
Oj) instead of (G
NM
i, O
NM
j) and (Gi, Oj) and find 
that the coefficient drops to 0.03, which is 
obviously not correlated at all.  
We now know NMP? andP?  are highly correlated. 
Assume the correlation is linear, with the 
following equation: 
?? ?? iNMi PP  
where ? and ? are the linear regression 
parameters. 
Thus 
? ? ???? nPnPPP NM
i
iNM
i i
????? ??
   
Here, n is the number of equivalence classes.    
We conclude that the overall CEAF Precision 
and CONE CEAF Precision should be highly 
140
correlated too. We repeat this experiment with 
CEAF / CONE CEAF Recall, B3 / CONE B3 
Precision and B3 / CONE B3 Recall and obtain 
similar results, allowing us to conclude that these 
sets of measures should also be highly correlated. 
We note here some generally accepted 
terminology regarding correlation: If two 
quantities have a Pearson?s correlation 
coefficient greater than 0.7, they are considered  
"strongly correlated", if their correlation is 
between 0.5 and 0.7, they are considered "highly 
correlated", if it is between 0.3 and 0.5, they are 
considered "correlated", and otherwise they are 
considered "not correlated".  
It is important to note that like all automatic 
evaluation metrics, CONE B3 and CONE CEAF 
too can be easily ?cheated?, e.g. a NE-CRR sys-
tem that performs NER and named entity match-
ing well but does not even detect and classify 
anaphora or nominal mentions would nonethe-
less score highly on these metrics. A possible 
solution to this problem would be to create gold 
standard annotations for a small subset of the 
data, call these annotations G?, and report two 
scores: B3 / CEAF (G?), and CONE B3 / CONE 
CEAF (GNM-approx). Discrepancies between these 
two scores would enable the detection of such 
?cheating?. A related point is that designers of 
NE-CRR systems should not optimize for CONE 
metrics alone, since by using GNM-approx (or GNM 
where gold standard annotations are available), 
these metrics are obviously biased towards 
named mentions. This issue can also be ad-
dressed by having gold standard annotations G? 
for a small subset. One could then train a system 
by optimizing both B3 / CEAF (G?) and CONE 
B3 / CONE CEAF (GNM-approx). This can be 
thought of as a form of semi-supervised learning, 
and may be useful in areas such as domain adap-
tation, where we could use some annotated test-
set in a standard domain, e.g. newswire as the 
smaller set and an unlabeled large testset from 
some other domain, such as e-mail or biomedical 
documents. An interesting future direction is to 
monitor the effectiveness of our metrics over 
time. As co-reference resolution systems evolve 
in strength, our metrics might be less effective, 
however this could be a good indicator to discri-
minate on different subtasks the improvements 
gained by the co-reference resolution systems. 
5 Experimental Results 
We present experimental results in support of the 
validity and effectiveness of CONE metrics. As 
mentioned earlier, we used the following four 
publicly available CRR systems: UIUC?s LBJ 
system (L), BART from JHU Summer Workshop 
(B), LingPipe from Alias-i (LP), and OpenNLP 
(OP) (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). All these CRR systems per-
form Noun Phrase co-reference resolution (NP-
CRR), not NE-CRR. So, we must first eliminate 
all equivalences classes that do not contain any 
named mentions. We do so using the SYNERGY 
NER system to separate named mentions from 
unnamed ones. Note that this must not be con-
fused with the use of SYNERGY to produce GNM 
and ONM from G and O respectively. For that task, 
all equivalence classes in G and O already con-
tain at least one named mention and we remove 
all unnamed mentions from each class. This 
process effectively converts the NP-CRR results 
of these systems into NE-CRR ones. We use the 
ACE Phase 2 NWIRE and ACE 2005 English 
datasets. We avoid using the ACE 2004 and 
MUC6 datasets because the UIUC LBJ system 
was trained on ACE 2004 (Bengston and Roth, 
2008), while BART and LingPipe were trained 
on MUC6. There are 29 files in the test set of 
ACE Phrase 2 and 81 files in ACE 2005, sum-
ming up to 120 files with around 50,000 tokens 
with 5000 valid co-reference mentions. Tables 1 
and 2 show the Pearson?s correlation coefficients  
between CONE metric scores of the type 
Score(GNM, ONM) and standard metric scores of 
the type Score(G, O) for combinations of various 
CRR systems and datasets.  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.82 0.71 0.7 0.81 0.71 0.77 
B 0.85 0.5 0.66 0.71 0.61 0.68 
LP 0.84 0.66 0.67 0.74 0.71 0.73 
OP 0.31 0.57 0.61 0.79 0.72 0.79 
Table 1. GNM: Correlation on ACE Phase 2 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.6 0.62 0.62 0.75 0.61 0.68 
B 0.74 0.82 0.84 0.72 0.68 0.67 
LP 0.91 0.65 0.73 0.44 0.57 0.53 
OP 0.48 0.77 0.8 0.54 0.67 0.65 
Table 2. GNM: Correlation on ACE 2005 
 
We observe from Tables 1 and 2 that CONE B3 
and CONE CEAF scores are highly correlated 
141
with B3 and CEAF scores respectively, and this 
holds true for Precision, Recall and F1 scores, for 
all combinations of CRR systems and datasets. 
This justifies our assumption that a system?s per-
formance for the subtask of NMEG is a good 
predictor of its performance for the full task of 
NE-CRR. These correlation coefficients are 
graphically illustrated in Figures 2 and 3. 
We now use our baseline named entity matching 
method to automatically generate estimated gold 
standard annotations GNM-approx and recalculate 
CONE CEAF and CONE B3 scores using GNM-
approx instead of GNM. Tables 3 and 4 show the 
correlation coefficients between the new CONE 
scores and the standard metric scores. 
 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.31 0.23 0.22 0.33 0.55 0.56 
B 0.71 0.44 0.43 0.61 0.63 0.71 
LP 0.57 0.43 0.49 0.36 0.25 0.31 
OP 0.1 0.6 0.64 0.35 0.53 0.53 
Table 3. GNM-approx: Correlation on ACE Phase 2  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.33 0.32 0.42 0.22 0.34 0.36 
B 0.25 0.66 0.65 0.2 0.45 0.37 
LP 0.19 0.33 0.34 0.77 0.68 0.72 
OP 0.26 0.66 0.67 0.28 0.42 0.38 
Table 4. GNM-approx: Correlation on ACE Phase 2 
We observe from Tables 3 and 4 that these corre-
lation factors are encouraging, but not as good as 
those in Tables 1 and 2. All the corresponding 
CONE B3 and CONE CEAF scores are corre-
lated, but very few are highly correlated. We 
should note however that our baseline system to 
create GNM-approx uses relatively simple clustering 
methods and heuristics. It is easy to observe that 
a sophisticated named entity matching system 
would produce a GNM-approx that better approx-
imates GNM than our baseline method, and CONE 
B3 and CONE CEAF scores calculated using this 
GNM-approx would be more correlated with stan-
dard B3 and CEAF scores.  
We note from the above results that correlations 
scores are very similar across different systems 
and datasets. In order to formalize this assertion, 
we calculate correlation scores in a system-
independent and data-independent manner. We 
combine all the data points across all four differ-
ent systems and plot them in Figure 2 and 3 for 
ACE Phase 2 NWIRE corpus and in Figure 4 and 
5 for ACE 2005 corpus respectively. We illu-
strate only F1 scores; the results for precision 
and recall are similar. 
 
Figure 2. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2 
 
Figure 3. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2 
 
Figure 2 reflects a Pearson?s correlation coeffi-
cient of 0.70, suggesting that all the B3 F1 and 
CONE B3 F1 scores for different systems are 
highly correlated and that CONE B3 F1 does not 
bias towards any particular system. Figure 3 re-
flects a Pearson?s correlation coefficient of 0.83, 
providing similar evidence for the system-
independence of correlation between CEAF F1 
and CONE CEAF F1 scores. Figures 4 and 5 
corresponding to ACE 2005 reflect similar corre-
lation coefficients of 0.89 and 0.82, and thus 
support the idea that the correlations between B3 
F1 and CONE B3 F1, as well as between CEAF 
F1and CONE CEAF F1, are dataset-independent 
in addition to being system-independent.  
 
142
 
Figure 4. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2005 
 
 
Figure 5. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2005 
6 Application and Discussion  
To illustrate the applicability of CONE metrics, 
we consider the Enron e-mail corpus. It is of a 
different genre than the newswire corpora that 
CRR systems are usually trained on, and no CRR 
gold standard annotations exist for it. Conse-
quently, no CRR systems have been evaluated on 
it so far. We used CONE B3 and CONE CEAF to 
evaluate and compare the NE-CRR performance 
of various CRR systems on a subset of the Enron 
e-mail corpus (Klimt and Yang, 2004) that was 
cleaned and stripped of spam messages. We re-
port the results in Table 5. 
 
  CONE B3  CONE CEAF 
  P R F1 P R F1 
L 0.43 0.21 0.23 0.31 0.17 0.21 
B 0.26 0.18 0.2 0.26 0.16 0.2 
LP 0.61 0.51 0.53 0.58 0.53 0.54 
OP 0.19 0.03 0.05 0.11 0.02 0.04 
Table 5. GNM-approx Scores on Enron corpus 
 
We find that LingPipe is the best of all the sys-
tems we considered, and LBJ is slightly ahead of 
BART in all measures. We suspect that since 
LingPipe is a commercial system, it may have 
extra training resources in the form of non-
traditional corpora. Nevertheless, we believe our 
method is robust and scalable for large corpora 
without NE-CRR gold standard annotations. 
 
7 Conclusion and Future Work 
We propose the CONE B3 and CONE CEAF me-
trics for automatic evaluation of Named Entity 
Co-reference Resolution (NE-CRR). These me-
trics measures a NE-CRR system?s performance 
on the subtask of named mentions extraction and 
grouping (NMEG) and use it to estimate the sys-
tem?s performance on the full task of NE-CRR. 
We show that CONE B3 and CONE CEAF 
scores of various systems across different data-
sets are strongly correlated with their standard B3 
and CEAF scores respectively. The advantage of 
CONE metrics compared to standard ones is that 
instead of the full gold standard data G, they only 
require a subset GNM of named mentions which 
even if not available can be closely approximated 
by using a state-of-the-art NER system and clus-
tering its results. Although we use a simple base-
line algorithm for producing the approximate 
gold standard GNM-approx, CONE B3 and CONE 
CEAF scores of various systems obtained using 
this GNM-approx still prove to be correlated with 
their standard B3 and CEAF scores obtained us-
ing the full gold standard G. CONE metrics thus 
reduce the need of expensive labeled corpora. 
We use CONE B3 and CONE CEAF to evaluate 
the NE-CRR performance of various CRR sys-
tems on a subset of the Enron email corpus, for 
which no gold standard annotations exist and no 
such evaluations have been performed so far. In 
the future, we intend to use more sophisticated 
named entity matching schemes to produce better 
approximate gold standards GNM-approx. We also 
intend to use the CONE metrics to evaluate NE-
CRR systems on new datasets in domains such as 
chat, email, biomedical literature, etc. where very 
few corpora with gold standard annotations exist. 
 
Acknowledgments 
We would like to thank Prof. Ani Nenkova from 
the University of Pennsylvania for her talk about 
automatic evaluation for text summarization at 
the spring 2010 CMU LTI Colloquium and ano-
nymous reviewers for insightful comments.  
143
References  
E. Agirre, L. M?rquez and R. Wicentowski, Eds. 
2007. Proceedings of the Fourth International 
Workshop on Semantic Evaluations (SemEval).   
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. Proceedings of LREC 
Workshop on Linguistic Coreference. 
J. Baldridge and T. Morton. 2004. OpenNLP. 
http://opennlp.sourceforge.net/. 
B. Baldwin and B. Carpenter. 2003. LingPipe. Alias-i. 
E. Bengtson and D. Roth. 2008. Understanding the 
Value of Features for Coreference Resolution. Pro-
ceedings of EMNLP. 
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences. (2nd ed.) 
A.K. Elmagarmid, P.G. Ipeirotis and V.S. Verykios. 
2007. Duplicate Record Detection: A Survey. IEEE 
Transactions on Knowledge and Data Engineering, 
v.19 n.1, 2007.   
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Informa-
tion Extraction Systems by Gibbs Sampling. Pro-
ceedings of ACL. 
B. Klimt and Y. Yang. 2004. The Enron corpus: A 
new dataset for email classification research. Pro-
ceedings of ECML. 
H.W. Kuhn. 1955. The Hungarian method for the 
assignment problem. Naval Research Logistics 
Quarterly, 2(83). 
C. Lin and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. 
Proceedings of HLT-NAACL.    
C. Lin and F.J. Och. 2004. Automatic evaluation of 
machine translation quality using longest common 
subsequence and skip-bigram statistics. Proceed-
ings of ACL.  
A. Louis and A. Nenkova. 2009. Automatically Eva-
luating Content Selection in Summarization with-
out Human Models. Proceedings of EMNLP, pages 
306?314, Singapore, 6-7 August 2009. 
X. Luo. 2005. On coreference resolution performance 
metrics. Proceedings of EMNLP. 
MUC-6. 1995. Proceedings of the Sixth Understand-
ing Conference (MUC-6). 
J. Munkres. 1957. Algorithms for the assignment and 
transportation problems. Journal of SIAM, 5:32-38. 
NIST. 2003. The ACE evaluation plan. 
www.nist.gov/speech/tests/ace/index.htm. 
K Papineni, S Roukos, T Ward and W.J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. Proceedings of ACL. 
V. Pervouchine, H. Li and B. Lin. 2009. Translitera-
tion alignment. Proceedings of ACL. 
L. Ratinov and D. Roth. 2009. Design Challenges and 
Misconceptions in Named Entity Recognition. 
Proceedings of CoNLL. 
R. Shah, B. Lin, A. Gershman and R. Frederking. 
2010. SYNERGY: a named entity recognition sys-
tem for resource-scarce languages such as Swahili 
using online machine translation. Proceedings of 
LREC Workshop on African Language Technology. 
H.E. Soper, A.W. Young, B.M. Cave, A. Lee and K. 
Pearson. 1917. On the distribution of the correla-
tion coefficient in small samples. Appendix II to 
the papers of "Student" and R. A. Fisher. A co-
operative study. Biometrika, 11, 328-413. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the-Art. 
Proceedings of ACL. 
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman, A. 
Jern, J. Smith, X. Yang and A. Moschitti. 2008. 
BART: A Modular Toolkit for Coreference Reso-
lution. Proceedings of EMNLP. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. 
Hirschman. 1995. A model-theoretic coreference 
scoring scheme. Proceedings of MUC 6. 
 
 
144
Proceedings of the First Workshop on Metaphor in NLP, pages 45?51,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Cross-Lingual Metaphor Detection Using Common Semantic Features
Yulia Tsvetkov Elena Mukomel Anatole Gershman
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{ytsvetko,helenm,anatoleg}@cs.cmu.edu
Abstract
We present the CSF - Common Semantic Fea-
tures method for metaphor detection. This
method has two distinguishing characteristics:
it is cross-lingual and it does not rely on the
availability of extensive manually-compiled
lexical resources in target languages other than
English. A metaphor detecting classifier is
trained on English samples and then applied to
the target language. The method includes pro-
cedures for obtaining semantic features from
sentences in the target language. Our exper-
iments with Russian and English sentences
show comparable results, supporting our hy-
pothesis that a CSF-based classifier can be ap-
plied across languages. We obtain state-of-
the-art performance in both languages.
1 Introduction
Metaphors are very powerful pervasive communica-
tion tools that help deliver complex concepts and
ideas simply and effectively (Lakoff and Johnson,
1980). Automatic detection and interpretation of
metaphors is critical for many practical language
processing tasks such as information extraction,
summarization, opinion mining, and translation. In
this paper, we focus on the automatic metaphor de-
tection task. This problem gained much attention
in natural language processing research mostly us-
ing the detection principles articulated by the Prag-
glejaz Group (2007). According to these princi-
ples, a lexical unit (a word or expression) is used
metaphorically if its contextual meaning is different
from its ?basic contemporary? meaning. To apply
this method, we need to be able to determine the ba-
sic meaning of a lexical unit and then test if this in-
terpretation makes sense in the current context.
Several approaches to automatic detection of
metaphors have been proposed (Gedigian et al,
2006; Krishnakumaran and Zhu, 2007; Shutova et
al., 2010), all of which rely on the availability of
extensive manually crafted lexical resources such
as WordNet, VerbNet, FrameNet, TreeBank, etc.
Unfortunately, such resources exist only for a few
resource-rich languages such as English. For most
other languages, such resources either do not exist
or are of a low quality.
To our knowledge this work is the first empiri-
cal study of cross-lingual metaphor detection. We
present the Common Semantic Features (CSF) ap-
proach to metaphor detection in languages without
extensive lexical resources. In a target language
it requires only a dependency parser and a target-
English dictionary. We classify sentences into lit-
eral and metaphoric using automatically extracted
coarse-grained semantic properties of words such as
their propensity to refer to abstract versus concrete
concepts, animate entities, artifacts, body parts, etc.
These properties serve as features for the key re-
lations in a sentence, which include Subject-Verb-
Object (SVO) and Adjective-Noun (AN). A clas-
sifier trained on English sentences obtains a 0.78
F -score. The same classifier, trained solely on
English sentences, achieves a similar level of per-
formance on sentences from other languages such
as Russian; this is the central contribution of this
work. An additional important contribution is that in
Russian we obtain the necessary semantic features
45
without recourse to sophisticated non-English lexi-
cal resources. In this paper, we focus on the sen-
tences where verbs are used metaphorically, leaving
Adjective-Noun relations for future work. Based on
our examination of over 500 metaphorical sentences
in English and Russian collected from general news
articles, we estimate that verb-based metaphors con-
stitute about 40-50% of all metaphors.
We present and discuss our experiments with
three sets of features: (1) features corresponding to
the lexicographer file names defined in WordNet
3.0 (Fellbaum, 1998), (2) features based on abstract-
ness vs. concreteness computed using Vector Space
Models (VSM), and (3) features based on the types
of named entities, if present. Our main target lan-
guage in these experiments has been Russian, but we
also present preliminary experiments with Spanish.
The paper is organized as follows: Section 2 con-
tains an overview of the resources we use; Sec-
tion 3 discusses the methodology; Section 4 presents
the experiments; in Section 5, we discuss related
work, and we conclude with suggestions for future
research in Section 6.
2 Datasets
We use the following English lexical resources to
train our model:
TroFi Example Base1 (Birke and Sarkar, 2007) of
3,737 English sentences from the Wall Street Jour-
nal. Each sentence contains one of the seed verbs
and is marked L by human annotators if the verb
is used in a literal sense. Otherwise, the sentence
is marked N (non-literal). The model was evalu-
ated on 25 target verbs with manually annotated 1
to 115 sentences per verb. TroFi does not define the
basic meanings of these verbs, but provides exam-
ples of literal and metaphoric sentences which we
use to train and evaluate our metaphor identification
method.
WordNet (Fellbaum, 1998) is an English lexical
database where each entry contains a set of syn-
onyms (a synset) all representing the same con-
cept. This database is compiled from a set of
1http://www.cs.sfu.ca/ anoop/students/jbirke/
45 lexicographer files2 such as ?noun.body? or
?verb.cognition? identified by a number from 0 to
44, called lexicographer file number (henceforth
lexFN ). The lexFN of each synset is contained in
the database. We use lexFNs as coarse-grain se-
mantic features of nouns and verbs.
MRC Psycholinguistic Database3 (Wilson, 1988)
is a dictionary containing 150,837 words with up to
26 linguistic and psycholinguistic attributes rated by
human subjects in psycholinguistic experiments. It
includes 4,295 words rated with degrees of abstract-
ness; the ratings range from 158 (highly abstract)
to 670 (highly concrete). We use these words as a
seed when we calculate the values of abstractness
and concreteness features for nouns and verbs in our
training and test sets.
Word Representations via Global Context is a
collection of 100,232 words and their vector rep-
resentations.4 These representations were extracted
from a statistical model embedding both local and
global contexts of words (Huang et al, 2012), in-
tended to capture better the semantics of words. We
use these vectors to calculate the values of abstract-
ness and concreteness features of a word.
3 Methodology
We treat the metaphor detection problem as a task
of binary classification of sentences. A sentence
is represented by one or more key relations such
as Subject-Verb-Object triples and Adjective-Noun
pairs. In this paper, we focus only on the SVO rela-
tions and we allow either the S part or the O part to
be empty. If all relations representing a sentence are
classified literal by our model then the whole sen-
tence is tagged literal. Otherwise, the sentence is
tagged metaphoric.
2See http://wordnet.princeton.edu/man/lexnames.5WN.html
for a full list of lexicographer file names.
3http://ota.oucs.ox.ac.uk/headers/1054.xml
4http://www.socher.org/index.php/Main/Improving-
WordRepresentationsViaGlobalContextAndMultipleWordPrototypes
46
3.1 Model
We classify an SVO relation x as literal vs.
metaphorical using a logistic regression classifier:
p(y | x) ? exp
?
j
?jhj(y, x),
where hj(?) are feature values computed for each
word in x, ?j are the corresponding weights, and
y ? {L,M} refer to our classes: L for literal and
M for metaphoric. The parameters ?j are learned
during training.
3.2 Features
An SVO relation is a concatenation of features for
the S, V, and O parts. The S and O parts contain
three types of features: (1) semantic categories of a
word, (2) degree of abstractness of a word, and (3)
types of named entities. The V part contains only
the first two types of features.
Semantic categories are features corresponding
to the WordNet lexFNs, introduced in Section 2.
Since S and O are assumed to be nouns,5 each has
26 semantic category features corresponding to the
lexFNs for nouns (3 through 28). These categories
include noun.animal, noun.artefact, noun.body,
noun.cognition, noun.food, noun.location, etc. The
V part has 15 semantic category features corre-
sponding to lexical ids for verbs (29 through 43),
for example, verb.motion and verb.cognition. A lex-
ical item can belong to several synsets with different
lexFNs. For example, the word ?head? when used
as a noun participates in 33 synsets, 3 of which have
lexFN 08 (noun.body). The value of the feature
corresponding to this lexFN is 3/33 = 0.09.
For a non-English word, we first obtain its most
common translations to English and then select all
corresponding English WordNet synsets. For exam-
ple, when Russian word `??????' is translated as
?head? and ?brain?, we select all the synsets for the
nouns head and brain. There are 38 such synsets (33
for head and 5 for brain). Four of these synsets have
lexFN 08 (noun.body). Therefore, the value of
the feature corresponding to this lexFN is 4/38 =
0.10. This dictionary-based mapping of non-English
5We currently exclude pronouns from the relations that we
learn.
words into WN synsets is rather coarse. A more dis-
criminating approach may improve the overall per-
formance. In addition, WN synsets may not always
capture all the meanings of non-English words. For
example, Russian word `????' refers to both the
?foot? and the ?leg?. WN has synsets for foot, leg
and extremity, but not for lower extremity.
Degree of abstractness According to Turney et al
(2011), ?Abstract words refer to ideas and concepts
that are distant from immediate perception, such as
economics, calculating and disputable.? Concrete
words refer to physical objects and actions. Words
with multiple senses can refer to both concrete and
abstract concepts. Evidence from several languages
suggests that concrete verbs tend to have concrete
subjects and objects. If either the subject or an object
of a concrete verb is abstract, then the verb is typi-
cally used in a figurative sense, indicating the pres-
ence of a metaphor. For example, when we hear that
?an idea was born?, we know that the word ?born?
is used figuratively. This observation motivates our
decision to include the degree of abstractness in our
feature set.
To calculate the degree of abstractness of English
lexical items we use the vector space representations
of words computed by Huang et al (2012) and a sep-
arate supervised logistic regression classifier trained
on a set of abstract and concrete words from the
MRC dataset. Each value in a word?s vector is a fea-
ture, thus, semantically similar words have similar
feature values. Degrees of abstractness are posterior
probabilities of the classifier predictions.
For non-English words, we use the following pro-
cedure. Suppose word w has n English transla-
tions whose degrees of abstractness are a1, a2, . . . an
in decreasing order. If the majority is deemed
abstract then ABSTRACT (w) = a1, otherwise
ABSTRACT (w) = an. This heuristic prefers the
extreme interpretations, and is based on an observa-
tion that translations tend to be skewed to one side or
the other of ?abstractness?. Our results may improve
if we map non-English words more precisely into the
most contextually-appropriate English senses.
Named entities (NE) is an additional category
of features instrumental in metaphor identification.
Specifically, we would like to distinguish whether
an action (a verb in SVO) is performed by a human,
47
an organization or a geographical entity. These dis-
tinctions are often needed to detect metonymy, as in
?the White House said?. Often, these entities are
mentioned by their names which are not found in
common dictionaries. Fortunately, there are many
named entity recognizers (NER) for all major lan-
guages. In addition, Shah et al (2010) showed
that named entities tend to survive popular machine
translation engines and can be relatively reliably de-
tected even without a native NER. Based on these
observations, we decided to include three boolean
features corresponding to these NE categories: per-
son, organization, and location.
4 Experiments
We train two classifiers: the first to calculate the de-
gree of abstractness of a given word and the second
to classify an SVO relation as metaphoric or literal.
Both are logistic regression classifiers trained with
the creg regression modeling framework.6 To min-
imize the number of free parameters in our model we
use `1 regularization.
4.1 Measuring abstractness
To train the abstractness classifier, we normalize ab-
stractness scores of nouns from the MRC dataset
to probabilities, and select 1,225 most abstract and
1,225 most concrete words. From these words, we
set aside 25 randomly selected samples from each
category for testing. We obtain the vector space rep-
resentations of the remaining 1,400 samples and use
the dimensions of these representations as features.
We train the abstractness classifier on the 1,400 la-
beled samples and test it on the 50 samples that were
set aside, obtaining 76% accuracy. The degree of ab-
stractness of a word is the posterior probability pro-
duced by the abstractness classifier.
4.2 Metaphor detection
We train the metaphor classifier using labeled En-
glish SVO relations. To obtain these relations,
we use the Turbo parser (Martins et al, 2010) to
parse 1,592 literal and 1,609 metaphorical man-
ually annotated sentences from the TroFi Exam-
ple Base and extract 1,660 sentences that have
SVO relations that contain annotated verbs: 696
6https://github.com/redpony/creg
literal and 964 metaphorical training instances.
For example, the verb flourish is used literally in
?Methane-making bacteria flourish in the stom-
ach? and metaphorically in ?Economies flourish in
free markets?. From the first sentence we extract
SVO relation <bacteria, flourish, NIL>,
and <economies, flourish, NIL> from the
second. We then build feature vectors, using feature
categories described in Section 3.
We train several versions of the metaphor classi-
fier for each feature category and for their combina-
tions. The feature categories are designated as fol-
lows:
? WN - Semantic categories based on WordNet lexFNs
? VSM - Degree of abstractness based on word vectors
? NE - Named Entity categories
We evaluate the metaphor classifiers using 10-fold
cross validation. The results are listed in Table 1.
Feature categories Accuracy
WN 63.7%
VSM 64.1%
WN+VSM 67.7%
WN+NE 64.5%
WN+VSM+NE 69.0%
Table 1: 10-fold cross validation results of the
metaphor classifier.
Our results are comparable to the accuracy of
64.9% reported by Birke and Sarkar (2007) on the
TroFi dataset. The combination of all feature cate-
gories significantly improves over this baseline.
4.2.1 English metaphor detection
We compute precision, recall and F -score on a
test set of 98 English sentences. This test set consists
of 50 literal and 48 metaphorical sentences, where
each metaphoric sentence contains a verb used in a
figurative sense. The test sentences were selected
from general news articles by independent collec-
tors. Table 2 shows the results.
In this experiment, the WN group of features con-
tributes the most. The addition of NE, while not im-
proving the overall F -score, helps to reduce false
positives and better balance precision and recall.
The VSM features are considerably weaker perhaps
48
Feature categories Precision Recall F -score
WN 0.75 0.81 0.78
VSM 0.57 0.71 0.63
WN+VSM 0.66 0.90 0.76
WN+NE 0.78 0.79 0.78
WN+VSM+NE 0.68 0.71 0.69
Table 2: Evaluation of the metaphor classifier on
the test set of 50 literal and 48 metaphoric English
sentences from news articles.
because we used single model vector space repre-
sentations where each word uses only one vector that
combines all its senses.
4.2.2 Russian metaphor detection
In a cross-lingual experiment, we evaluate our al-
gorithm on a set of 140 Russian sentences: 62 literal
and 78 metaphoric, selected from general news arti-
cles by two independent collectors. As in English,
each metaphoric sentence contains a verb used in a
figurative sense. We used the AOT parser7 to ob-
tain the SVO relations and the Babylon dictionary8
to obtain English translations of individual words.
The example sentence in Figure 1 contains one SVO
relation with missing O part. We show the set of fea-
tures and their values that were extracted from words
in this relation.
The results of the Russian test set, listed in Ta-
ble 3, are similar to the English results, supporting
our hypothesis that a semantic classifier can work
across languages. As in the previous experiment, the
WN features are the most effective and the NE fea-
tures contribute to improved precision.
Feature categories Precision Recall F -score
WN 0.74 0.76 0.75
VSM 0.66 0.73 0.69
WN+VSM 0.70 0.73 0.71
WN+NE 0.82 0.71 0.76
WN+VSM+NE 0.74 0.72 0.73
Table 3: Evaluation of the metaphor classifier on
the test set of 62 literal and 78 metaphoric Russian
sentences from news articles.
While we did not conduct a full-scale experiment
7www.aot.ru
8www.babylon.com
with Spanish, we ran a pilot using 51 sentences: 24
literal and 27 metaphoric. We obtained the F -score
of 0.66 for the WN+VSM combination. We take it as
a positive sign and will conduct more experiments.
5 Related work
Our work builds on the research of Birke and Sarkar
(2007) who used an active learning approach to cre-
ate an annotated corpus of sentences with literal
and figurative senses of 50 common English verbs.
The result was the TroFi Example Base set of 3,737
labeled sentences, which was used by the authors
to train several classifiers. These algorithms were
tested on sentences containing 25 English verbs not
included in the original set. The authors report F -
scores around 64.9%. We used this dataset for train-
ing and evaluation, and Birke and Sarkar?s (2007)
results as a baseline.
In a more recent work, Turney et al (2011) sug-
gested that the degree of abstractness of a word?s
context is correlated with the likelihood that the
word is used metaphorically. To compute the ab-
stractness of a word, the authors use a variation
of Turney and Littman?s (2003) algorithm compar-
ing the word to twenty typically abstract words and
twenty typically concrete words. Latent Semantic
Analysis (Deerwester et al, 1990) is used to mea-
sure semantic similarity between each pair of words.
A feature vector is generated for each word and a
logistic regression classifier is used. The result is
an average F -score of 63.9% on the TroFi dataset,9
compared to Birke and Sarkar?s (2007) 64.9%. In
another experiment on 100 adjective-noun phrases
labeled as literal or non-literal, according to the
sense of the adjective, this algorithm obtains an av-
erage accuracy of 79%. While we obtain compara-
ble results, our work extends this method in several
important directions. First, we show how to apply
a metaphor classifier across languages. Second, we
extend our feature set beyond abstractness criteria.
Finally, we propose an alternative technique to mea-
sure degrees of abstractness.
9Turney et al (2011) report on two experimental setups with
TroFi, our setup is closer to their first experiment.
49
???????? ????? ????????????? .
?Society ripens over decades?
SVO = <????????, ?????, NIL>
Subject Verb
WN
noun.group 0.54
noun.state 0.23
noun.possession 0.15
noun.location 0.08
verb.change 0.75
verb.body 0.125
verb.communication 0.125
VSM Abstractness 0.87 Abstractness 0.93
Figure 1: Features extracted for a Russian test sentence classified as metaphoric by our model.
6 Conclusions and future work
We presented CSF ? an approach to metaphor de-
tection based on semantic rather than lexical fea-
tures. We described our experiments with an ini-
tial set of fairly coarse-grained features and showed
how these features can be obtained in languages that
lack extensive lexical resources. Semantic, as op-
posed to lexical features, are common to all lan-
guages which allows a classifier trained to detect
metaphors in one language to be successfully ap-
plied to sentences in another language. Our results
suggest that metaphors can be detected on a con-
ceptual level, independently of whether they are ex-
pressed in Russian or English, supporting Lakoff
and Johnson?s (1980) claim that metaphors are parts
of a pervasive conceptual system.
Our current work has been limited to the detection
of figurative SVO relations, which account for about
half of all metaphors in English and Russian. Other
languages such as Farsi have a greater proportion of
metaphors based on figurative use of adjectives and
nouns. We plan to include more relations and ex-
pand our set of semantic features as part of the future
research.
Acknowledgments
We are grateful to Chris Dyer for his invaluable advice.
We are also grateful to the three anonymous reviewers for
their constructive suggestions. Supported by the Intelli-
gence Advanced Research Projects Activity (IARPA) via
Department of Defense US Army Research Laboratory
contract number W911NF-12-C-0020. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and conclu-
sions contained herein are those of the authors and should
not be interpreted as necessarily representing the official
policies or endorsements, either expressed or implied, of
IARPA, DoD/ARL, or the U.S. Government.
References
Julia Birke and Anoop Sarkar. 2007. Active learning for
the identification of nonliteral language. In Proceed-
ings of the Workshop on Computational Approaches to
Figurative Language, FigLanguages ?07, pages 21?28.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural Language
Understanding, pages 41?48.
Pragglejaz Group. 2007. MIP: A method for identify-
ing metaphorically used words in discourse. Metaphor
and Symbol, 22(1):1?39.
Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2012.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20.
George Lakoff and Mark Johnson. 1980. Conceptual
metaphor in everyday language. The Journal of Phi-
losophy, pages 453?486.
50
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: dependency parsing by approximate
variational inference. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 34?44.
Rushin Shah, Bo Lin, Anatole Gershman, and Robert
Frederking. 2010. SYNERGY: a named entity recog-
nition system for resource-scarce languages such as
Swahili using online machine translation. In Proceed-
ings of the Second Workshop on African Language
Technology, AfLaT 2010, pages 21?26.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics, pages 1002?1010.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation and System Security, 21(4):315?346.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?11, pages
680?690.
Michael Wilson. 1988. MRC Psycholinguistic Database:
Machine-usable dictionary, version 2.00. Behav-
ior Research Methods, Instruments, & Computers,
20(1):6?10.
51
