Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41?48,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Language models for contextual error detection and correction
Herman Stehouwer
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
j.h.stehouwer@uvt.nl
Menno van Zaanen
Tilburg Centre for Creative Computing
Tilburg University
Tilburg, The Netherlands
mvzaanen@uvt.nl
Abstract
The problem of identifying and correcting
confusibles, i.e. context-sensitive spelling
errors, in text is typically tackled using
specifically trained machine learning clas-
sifiers. For each different set of con-
fusibles, a specific classifier is trained and
tuned.
In this research, we investigate a more
generic approach to context-sensitive con-
fusible correction. Instead of using spe-
cific classifiers, we use one generic clas-
sifier based on a language model. This
measures the likelihood of sentences with
different possible solutions of a confusible
in place. The advantage of this approach
is that all confusible sets are handled by
a single model. Preliminary results show
that the performance of the generic clas-
sifier approach is only slightly worse that
that of the specific classifier approach.
1 Introduction
When writing texts, people often use spelling
checkers to reduce the number of spelling mis-
takes in their texts. Many spelling checkers con-
centrate on non-word errors. These errors can be
easily identified in texts because they consist of
character sequences that are not part of the lan-
guage. For example, in English woord is is not
part of the language, hence a non-word error. A
possible correction would be word .
Even when a text does not contain any non-
word errors, there is no guarantee that the text is
error-free. There are several types of spelling er-
rors where the words themselves are part of the
language, but are used incorrectly in their context.
Note that these kinds of errors are much harder
to recognize, as information from the context in
which they occur is required to recognize and cor-
rect these errors. In contrast, non-word errors can
be recognized without context.
One class of such errors, called confusibles,
consists of words that belong to the language, but
are used incorrectly with respect to their local,
sentential context. For example, She owns to cars
contains the confusible to. Note that this word is
a valid token and part of the language, but used
incorrectly in the context. Considering the con-
text, a correct and very likely alternative would be
the word two. Confusibles are grouped together in
confusible sets. Confusible sets are sets of words
that are similar and often used incorrectly in con-
text. Too is the third alternative in this particular
confusible set.
The research presented here is part of a
larger project, which focusses on context-sensitive
spelling mistakes in general. Within this project
all classes of context-sensitive spelling errors are
tackled. For example, in addition to confusibles,
a class of pragmatically incorrect words (where
words are incorrectly used within the document-
wide context) is considered as well. In this arti-
cle we concentrate on the problem of confusibles,
where the context is only as large as a sentence.
2 Approach
A typical approach to the problem of confusibles
is to train a machine learning classifier to a specific
confusible set. Most of the work in this area has
concentrated on confusibles due to homophony
(to , too , two) or similar spelling (desert , dessert ).
However, some research has also touched upon in-
flectional or derivational confusibles such as I ver-
sus me (Golding and Roth, 1999). For instance,
when word forms are homophonic, they tend to
get confused often in writing (cf. the situation with
to, too , and two, affect and effect , or there , their ,
and they?re in English) (Sandra et al, 2001; Van
den Bosch and Daelemans, 2007).
41
Most work on confusible disambiguation using
machine learning concentrates on hand-selected
sets of notorious confusibles. The confusible sets
are typically very small (two or three elements)
and the machine learner will only see training
examples of the members of the confusible set.
This approach is similar to approaches used in ac-
cent restoration (Yarowsky, 1994; Golding, 1995;
Mangu and Brill, 1997; Wu et al, 1999; Even-
Zohar and Roth, 2000; Banko and Brill, 2001;
Huang and Powers, 2001; Van den Bosch, 2006).
The task of the machine learner is to decide, us-
ing features describing information from the con-
text, which word taken from the confusible set re-
ally belongs in the position of the confusible. Us-
ing the example above, the classifier has to decide
which word belongs on the position of the X in
She owns X cars , where the possible answers for
X are to , too , or two. We call X, the confusible
that is under consideration, the focus word.
Another way of looking at the problem of con-
fusible disambiguation is to see it as a very spe-
cialized case of word prediction. The problem is
then to predict which word belongs at a specific
position. Using similarities between these cases,
we can use techniques from the field of language
modeling to solve the problem of selecting the best
alternative from confusible sets. We will investi-
gate this approach in this article.
Language models assign probabilities to se-
quences of words. Using this information, it
is possible to predict the most likely word in
a certain context. If a language model gives
us the probability for a sequence of n words
PLM (w1, . . . , wn), we can use this to predict the
most likely word w following a sequence of n? 1
words arg maxw PLM (w1, . . . , wn?1, w). Obvi-
ously, a similar approach can be taken with w in
the middle of the sequence.
Here, we will use a language model as a classi-
fier to predict the correct word in a context. Since
a language model models the entire language, it is
different from a regular machine learning classifier
trained on a specific set of confusibles. The advan-
tage of this approach to confusible disambiguation
is that the language model can handle all potential
confusibles without any further training and tun-
ing. With the language model it is possible to take
the words from any confusible set and compute the
probabilities of those words in the context. The
element from the confusible set that has the high-
est probability according to the language model is
then selected. Since the language model assigns
probabilities to all sequences of words, it is pos-
sible to define new confusible sets on the fly and
let the language model disambiguate them with-
out any further training. Obviously, this is not
possible for a specialized machine learning clas-
sifier approach, where a classifier is fine-tuned to
the features and classes of a specific confusible set.
The expected disadvantage of the generic (lan-
guage model) classifier approach is that the accu-
racy is expected to be less than that of the specific
(specialized machine learning classifier) approach.
Since the specific classifiers are tuned to each spe-
cific confusible set, the weights for each of the
features may be different for each set. For in-
stance, there may be confusibles for which the cor-
rect word is easily identified by words in a specific
position. If a determiner, like the , occurs in the po-
sition directly before the confusible, to or too are
very probably not the correct answers. The spe-
cific approach can take this into account by assign-
ing specific weights to part-of-speech and position
combinations, whereas the generic approach can-
not do this explicitly for specific cases; the weights
follow automatically from the training corpus.
In this article, we will investigate whether it is
possible to build a confusible disambiguation sys-
tem that is generic for all sets of confusibles using
language models as generic classifiers and investi-
gate in how far this approach is useful for solving
the confusible problem. We will compare these
generic classifiers against specific classifiers that
are trained for each confusible set independently.
3 Results
To measure the effectiveness of the generic clas-
sifier approach to confusible disambiguation, and
to compare it against a specific classifier approach
we have implemented several classification sys-
tems. First of these is a majority class baseline sys-
tem, which selects the word from the confusible
set that occurs most often in the training data.1
We have also implemented several generic classi-
fiers based on different language models. We com-
pare these against two machine learning classi-
fiers. The machine learning classifiers are trained
separately for each different experiment, whereas
1This baseline system corresponds to the simplest lan-
guage model classifier. In this case, it only uses n-grams with
n = 1.
42
the parameters and the training material of the lan-
guage model are kept fixed throughout all the ex-
periments.
3.1 System description
There are many different approaches that can be
taken to develop language models. A well-known
approach is to use n-grams, or Markov models.
These models take into account the probability
that a word occurs in the context of the previous
n ? 1 words. The probabilities can be extracted
from the occurrences of words in a corpus. Proba-
bilities are computed by taking the relative occur-
rence count of the n words in sequence.
In the experiments described below, we will use
a tri-gram-based language model and where re-
quired this model will be extended with bi-gram
and uni-gram language models. The probability
of a sequence is computed as the combination of
the probabilities of the tri-grams that are found in
the sequence.
Especially when n-grams with large n are used,
data sparseness becomes an issue. The training
data may not contain any occurrences of the par-
ticular sequence of n symbols, even though the
sequence is correct. In that case, the probability
extracted from the training data will be zero, even
though the correct probability should be non-zero
(albeit small). To reduce this problem we can ei-
ther use back-off or smoothing when the probabil-
ity of an n-gram is zero. In the case of back-off,
the probabilities of lower order n-grams are taken
into account when needed. Alternatively, smooth-
ing techniques (Chen and Goodman, 1996) redis-
tribute the probabilities, taking into account previ-
ously unseen word sequences.
Even though the language models provide us
with probabilities of entire sequences, we are
only interested in the n-grams directly around the
confusible when using the language models in
the context of confusible disambiguation. The
probabilities of the rest of the sequence will re-
main the same whichever alternative confusible
is inserted in the focus word position. Fig-
ure 1 illustrates that the probability of for example
P (analysts had expected ) is irrelevant for the de-
cision between then and than because it occurs in
both sequences.
The different language models we will consider
here are essentially the same. The differences lie
in how they handle sequences that have zero prob-
ability. Since the probabilities of the n-grams are
multiplied, having a n-gram probability of zero re-
sults in a zero probability for the entire sequence.
There may be two reasons for an n-gram to have
probability zero: there is not enough training data,
so this sequence has not been seen yet, or this se-
quence is not valid in the language.
When it is known that a sequence is not valid
in the language, this information can be used to
decide which word from the confusible set should
be selected. However, when the sequence simply
has not been seen in the training data yet, we can-
not rely on this information. To resolve the se-
quences with zero probability, we can use smooth-
ing. However, this assumes that the sequence is
valid, but has not been seen during training. The
other solution, back-off, tries not to make this as-
sumption. It checks whether subsequences of the
sequence are valid, i.e. have non-zero probabili-
ties. Because of this, we will not use smoothing to
reach non-zero probabilities in the current exper-
iments, although this may be investigated further
in the future.
The first language model that we will investi-
gate here is a linear combination of the differ-
ent n-grams. The probability of a sequence is
computed by a linear combination of weighted n-
gram probabilities. We will report on two different
weight settings, one system using uniform weight-
ing, called uniform linear, and one where uni-
grams receive weight 1, bi-grams weight 138, and
tri-grams weight 437.2 These weights are normal-
ized to yield a final probability for the sequence,
resulting in the second system called weighted lin-
ear.
The third system uses the probabilities of the
different n-grams separately, instead of using the
probabilities of all n-grams at the same time as is
done in the linear systems. The continuous back-
off method uses only one of the probabilities at
each position, preferring the higher-level probabil-
ities. This model provides a step-wise back-off.
The probability of a sequence is that of the tri-
grams contained in that sequence. However, if the
probability of a trigram is zero, a back-off to the
probabilities of the two bi-grams of the sequence
is used. If that is still zero, the uni-gram probabil-
ity at that position is used. Note that this uni-gram
probability is exactly what the baseline system
2These weights are selected by computing the accuracy of
all combinations of weights on a held out set.
43
. . . much stronger most analysts had expected .
than then
P (much stronger than) P (much stronger then)
?P (stronger than most) ?P (stronger then most)
?P (than most analysts) ?P (then most analysts)
Figure 1: Computation of probabilities using the language model.
uses. With this approach it may be the case that
the probability for one word in the confusible set
is computed based on tri-grams, whereas the prob-
ability of another word in the set of confusibles is
based on bi-grams or even the uni-gram probabil-
ity. Effectively, this means that different kinds of
probabilities are compared. The same weights as
in the weighted linear systems are used.
To resolve the problem of unbalanced probabil-
ities, a fourth language model, called synchronous
back-off, is proposed. Whereas in the case of the
continuous back-off model, two words from the
confusible set may be computed using probabil-
ities of different level n-grams, the synchronous
back-off model uses probabilities of the same level
of n-grams for all words in the confusible set, with
n being the highest value for which at least one of
the words has a non-zero probability. For instance,
when word a has a tri-gram probability of zero and
word b has a non-zero tri-gram probability, b is se-
lected. When both have a zero tri-gram probabil-
ity, a back-off to bi-grams is performed for both
words. This is in line with the idea that if a proba-
bility is zero, the training data is sufficient, hence
the sequence is not in the language.
To implement the specific classifiers, we used
the TiMBL implementation of a k-NN classifier
(Daelemans et al, 2007). This implementation of
the k-NN algorithm is called IB1. We have tuned
the different parameter settings for the k-NN clas-
sifier using Paramsearch (Van den Bosch, 2004),
which resulted in a k of 35.3 To describe the in-
stances, we try to model the data as similar as pos-
sible to the data used by the generic classifier ap-
proach. Since the language model approaches use
n-grams with n = 3 as the largest n, the features
for the specific classifier approach use words one
and two positions left and right of the focus word.
3We note that k is handled slightly differently in TiMBL
than usual, k denotes the number of closest distances consid-
ered. So if there are multiple instances that have the same
(closest) distance they are all considered.
The focus word becomes the class that needs to
be predicted. We show an example of both train-
ing and testing in figure 2. Note that the features
for the machine learning classifiers could be ex-
panded with, for instance, part-of-speech tags, but
in the current experiments only the word forms are
used as features.
In addition to the k-NN classifier, we also run
the experiments using the IGTree classifier, which
is denoted IGTree in the rest of the article, which is
also contained in the TiMBL distribution. IGTree
is a fast, trie based, approximation of k-nearest
neighbor classification (Knuth, 1973; Daelemans
et al, 1997). IGTree allows for fast training and
testing even with millions of examples. IGTree
compresses a set of labeled examples into a deci-
sion tree structure similar to the classic C4.5 algo-
rithm (Quinlan, 1993), except that throughout one
level in the IGTree decision tree, the same feature
is tested. Classification in IGTree is a simple pro-
cedure in which the decision tree is traversed from
the root node down, and one path is followed that
matches the actual values of the new example to
be classified. If a leaf is found, the outcome stored
at the leaf of the IGTree is returned as the clas-
sification. If the last node is not a leaf node, but
there are no outgoing arcs that match a feature-
value combination of the instance, the most likely
outcome stored at that node is produced as the re-
sulting classification. This outcome is computed
by collating the outcomes of all leaf nodes that can
be reached from the node.
IGTree is typically able to compress a large
example set into a lean decision tree with high
compression factors. This is done in reasonably
short time, comparable to other compression al-
gorithms. More importantly, IGTree?s classifica-
tion time depends only on the number of features
(O(f)). Indeed, in our experiments we observe
high compression rates. One of the unique char-
acteristics of IGTree compared to basic k-NN is
its resemblance to smoothing of a basic language
44
Training . . . much stronger than most analysts had expected .
?much, stronger, most, analysts? ?than
Testing . . . much stronger most analysts had expected .
?much, stronger, most, analysts? ??
Figure 2: During training, a classified instance (in this case for the confusible pair {then , than}) are
generated from a sentence. During testing, a similar instance is generated. The classifier decides what
the corresponding class, and hence, which word should be the focus word.
model (Zavrel and Daelemans, 1997), while still
being a generic classifier that supports any number
and type of features. For these reasons, IGTree is
also included in the experiments.
3.2 Experimental settings
The probabilities used in the language models of
the generic classifiers are computed by looking at
occurrences of n-grams. These occurrences are
extracted from a corpus. The training instances
used in the specific machine learning classifiers
are also extracted from the same data set. For
training purposes, we used the Reuters news cor-
pus RCV1 (Lewis et al, 2004). The Reuters cor-
pus contains about 810,000 categorized newswire
stories as published by Reuters in 1996 and 1997.
This corpus contains around 130 million tokens.
For testing purposes, we used the Wall Street
Journal part of the Penn Treebank corpus (Marcus
et al, 1993). This well-known corpus contains ar-
ticles from the Wall Street Journal in 1987 to 1989.
We extract our test-instances from this corpus in
the same way as we extract our training data from
the Reuters corpus. There are minor tokenization
differences between the corpora. The data is cor-
rected for these differences.
Both corpora are in the domain of English lan-
guage news texts, so we expect them to have simi-
lar properties. However, they are different corpora
and hence are slightly different. This means that
there are also differences between the training and
testing set. We have selected this division to cre-
ate a more realistic setting. This should allow for a
more to real-world use comparison than when both
training and testing instances are extracted from
the same corpus.
For the specific experiments, we selected a
number of well-known confusible sets to test
the different approaches. In particular, we
look at {then, than}, {its, it?s}, {your, you?re},
{their, there, they?re}. To compare the difficulty
of these problems, we also selected two words at
random and used them as a confusible set.
The random category consists of two words that
where randomly selected from all words in the
Reuters corpus that occurred more than a thousand
times. The words that where chosen, and used for
all experiments here are refugees and effect . They
occur around 27 thousand times in the Reuters cor-
pus.
3.3 Empirical results
Table 1 sums up the results we obtained with the
different systems. The baseline scores are gen-
erally very high, which tells us that the distribu-
tion of classes in a single confusible set is severely
skewed, up to a ten to one ratio. This also makes
the task hard. There are many examples for one
word in the set, but only very few training in-
stances for the other(s). However, it is especially
important to recognize the important aspects of the
minority class.
The results clearly show that the specific clas-
sifier approaches outperform the other systems.
For instance, on the first task ({then, than}) the
classifier achieves an accuracy slightly over 98%,
whereas the language model systems only yield
around 96%. This is as expected. The classifier
is trained on just one confusible task and is there-
fore able to specialize on that task.
Comparing the two specific classifiers, we see
that the accuracy achieved by IB1 and IGTree is
quite similar. In general, IGTree performs a bit
worse than IB1 on all confusible sets, which is
as expected. However, in general it is possible
for IGTree to outperform IB1 on certain tasks. In
our experience this mainly happens on tasks where
the usage of IGTree, allowing for more compact
internal representations, allows one to use much
more training data. IGTree also leads to improved
45
{then, than} {its, it?s} {your, you?re} {their, there, they?re} random
Baseline 82.63 92.42 78.55 68.36 93.16
IB1 98.01 98.67 96.36 97.12 97.89
IGTree 97.07 96.75 96.00 93.02 95.79
Uniform linear 68.27 50.70 31.64 32.72 38.95
Weighted linear 94.43 92.88 93.09 93.25 88.42
Continuous back-off 81.49 83.22 74.18 86.01 63.68
Synchronous back-off 96.42 94.10 92.36 93.06 87.37
Number of cases 2,458 4,830 275 3,053 190
Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%).
The Number of cases denotes the number of instances in the testset.
performance in cases where the features have a
strong, absolute ordering of importance with re-
spect to the classification problem at hand.
The generic language model approaches per-
form reasonably well. However, there are clear
differences between the approaches. For instance
the weighted linear and synchronous back-off ap-
proaches work well, but uniform linear and con-
tinuous back-off perform much worse. Especially
the synchronous back-off approach achieves de-
cent results, regardless of the confusible problem.
It is not very surprising to see that the contin-
uous back-off method performs worse than the
synchronous back-off method. Remember that
the continuous back-off method always uses lower
level n-grams when zero probabilities are found.
This is done independently of the probabilities of
the other words in the confusible set. The contin-
uous back-off method prefers n-grams with larger
n, however it does not penalize backing off to an
n-gram with smaller n. Combine this with the fact
that n-gram probabilities with large n are compar-
atively lower than those for n-grams with smaller
n and it becomes likely that a bi-gram contributes
more to the erroneous option than the correct tri-
gram does to the correct option. Tri-grams are
more sparse than bi-grams, given the same data.
The weighted linear approach outperforms the
uniform linear approach by a large margin on all
confusible sets. It is likely that the contribution
from the n-grams with large n overrules the prob-
abilities of the n-grams with smaller n in the uni-
form linear method. This causes a bias towards the
more frequent words, compounded by the fact that
bi-grams, and uni-grams even more so, are less
sparse and therefore contribute more to the total
probability.
We see that the both generic and specific clas-
sifier approaches perform consistently across the
different confusible sets. The synchronous back-
off approach is the best performing generic clas-
sifier approach we tested. It consistently outper-
forms the baseline, and overall performs better
than the weighted linear approach.
The experiments show that generic classifiers
based on language model can be used in the con-
text of confusible disambiguation. However, the
n in the different n-grams is of major importance.
Exactly which n grams should be used to com-
pute the probability of a sequence requires more
research. The experiments also show that ap-
proaches that concentrate on n-grams with larger
n yield more encouraging results.
4 Conclusion and future work
Confusibles are spelling errors that can only be de-
tected within their sentential context. This kind
of errors requires a completely different approach
compared to non-word errors (errors that can be
identified out of context, i.e. sequences of char-
acters that do not belong to the language). In
practice, most confusible disambiguation systems
are based on machine learning classification tech-
niques, where for each type of confusible, a new
classifier is trained and tuned.
In this article, we investigate the use of language
models in the context of confusible disambigua-
tion. This approach works by selecting the word
in the set of confusibles that has the highest prob-
ability in the sentential context according to the
language model. Any kind of language model can
be used in this approach.
The main advantage of using language models
as generic classifiers is that it is easy to add new
sets of confusibles without retraining or adding ad-
ditional classifiers. The entire language is mod-
46
eled, which means that all the information on
words in their context is inherently present.
The experiments show that using generic clas-
sifiers based on simple n-gram language models
yield slightly worse results compared to the spe-
cific classifier approach, where each classifier is
specifically trained on one confusible set. How-
ever, the advantage of the generic classifier ap-
proach is that only one system has to be trained,
compared to different systems for each confusible
in the specific classifier case. Also, the exact com-
putation of the probabilities using the n-grams, in
particular the means of backing-off, has a large
impact on the results.
As future work, we would like to investigate the
accuracy of more complex language models used
as classifiers. The n-gram language models de-
scribed here are relatively simple, but more com-
plex language models could improve performance.
In particular, instead of back-off, smoothing tech-
niques could be investigated to reduce the impact
of zero probability problems (Chen and Goodman,
1996). This assumes that the training data we are
currently working with is not enough to properly
describe the language.
Additionally, language models that concentrate
on more structural descriptions of the language,
for instance, using grammatical inference tech-
niques (de la Higuera, 2005), or models that ex-
plicitly take long distance dependencies into ac-
count (Griffiths et al, 2005) can be investigated.
This leads to much richer language models that
could, for example, check whether there is already
a verb in the sentence (which helps in cases such
as {its, it?s}).
A different route which we would also like to in-
vestigate is the usage of a specific classifier, such
as TiMBL?s IGTree, as a language model. If a
classifier is trained to predict the next word in the
sentence or to predict the word at a given position
with both left and right context as features, it can
be used to estimate the probability of the words in
a confusible set, just like the language models we
have looked at so far. Another type of classifier
might estimate the perplexity at a position, or pro-
vide some other measure of ?surprisedness?. Ef-
fectively, these approaches all take a model of the
entire language (as described in the training data)
into account.
References
Banko, M. and Brill, E. (2001). Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of the 39th Annual Meeting of the As-
sociation for Computational Linguistics, pages 26?
33. Association for Computational Linguistics.
Chen, S. and Goodman, J. (1996). An empirical study
of smoothing techniques for language modelling. In
Proceedings of the 34th Annual Meeting of the ACL,
pages 310?318. ACL.
Daelemans, W., Van den Bosch, A., and Weijters, A.
(1997). IGTree: using trees for compression and
classification in lazy learning algorithms. Artificial
Intelligence Review, 11:407?423.
Daelemans, W., Zavrel, J., Van der Sloot, K., and Van
den Bosch, A. (2007). TiMBL: Tilburg Memory
Based Learner, version 6.1, reference guide. Techni-
cal Report ILK 07-07, ILK Research Group, Tilburg
University.
de la Higuera, C. (2005). A bibliographical study
of grammatical inference. Pattern Recognition,
38(9):1332 ? 1348. Grammatical Inference.
Even-Zohar, Y. and Roth, D. (2000). A classification
approach to word prediction. In Proceedings of the
First North-American Conference on Computational
Linguistics, pages 124?131, New Brunswick, NJ.
ACL.
Golding, A. and Roth, D. (1999). A Winnow-Based
Approach to Context-Sensitive Spelling Correction.
Machine Learning, 34(1?3):107?130.
Golding, A. R. (1995). A Bayesian hybrid method for
context-sensitive spelling correction. In Proceed-
ings of the 3rd workshop on very large corpora,
ACL-95.
Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-
baum, J. B. (2005). Integrating topics and syntax. In
In Advances in Neural Information Processing Sys-
tems 17, pages 537?544. MIT Press.
Huang, J. H. and Powers, D. W. (2001). Large scale ex-
periments on correction of confused words. In Aus-
tralasian Computer Science Conference Proceed-
ings, pages 77?82, Queensland AU. Bond Univer-
sity.
Knuth, D. E. (1973). The art of computer program-
ming, volume 3: Sorting and searching. Addison-
Wesley, Reading, MA.
Lewis, D. D., Yang, Y., Rose, T. G., Dietterich, G., Li,
F., and Li, F. (2004). Rcv1: A new benchmark col-
lection for text categorization research. Journal of
Machine Learning Research, 5:361?397.
Mangu, L. and Brill, E. (1997). Automatic rule ac-
quisition for spelling correction. In Proceedings of
the International Conference on Machine Learning,
pages 187?194.
47
Marcus, M., Santorini, S., and Marcinkiewicz, M.
(1993). Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Quinlan, J. (1993). C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Sandra, D., Daems, F., and Frisson, S. (2001). Zo
helder en toch zoveel fouten! wat leren we uit psy-
cholingu??stisch onderzoek naar werkwoordfouten
bij ervaren spellers? Tijdschrift van de Vereniging
voor het Onderwijs in het Nederlands, 30(3):3?20.
Van den Bosch, A. (2004). Wrapped progressive
sampling search for optimizing learning algorithm
parameters. In Verbrugge, R., Taatgen, N., and
Schomaker, L., editors, Proceedings of the Sixteenth
Belgian-Dutch Conference on Artificial Intelligence,
pages 219?226, Groningen, The Netherlands.
Van den Bosch, A. (2006). Scalable classification-
based word prediction and confusible correction.
Traitement Automatique des Langues, 46(2):39?63.
Van den Bosch, A. and Daelemans, W. (2007). Tussen
Taal, Spelling en Onderwijs, chapter Dat gebeurd
mei niet: Computationele modellen voor verwarbare
homofonen, pages 199?210. Academia Press.
Wu, D., Sui, Z., and Zhao, J. (1999). An information-
based method for selecting feature types for word
prediction. In Proceedings of the Sixth European
Conference on Speech Communication and Technol-
ogy, EUROSPEECH?99, Budapest.
Yarowsky, D. (1994). Decision lists for lexical ambi-
guity resolution: application to accent restoration in
Spanish and French. In Proceedings of the Annual
Meeting of the ACL, pages 88?95.
Zavrel, J. and Daelemans, W. (1997). Memory-based
learning: Using similarity for smoothing. In Pro-
ceedings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 436?443.
48
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 7?12,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
A high speed transcription interface for annotating primary linguistic
data
Mark Dingemanse, Jeremy Hammond, Herman Stehouwer,
Aarthy Somasundaram, Sebastian Drude
Max Planck Institute for Psycholinguistics
Nijmegen
{mark.dingemanse, jeremy.hammond, herman.stehouwer,
aarthy.somasundaram, sebastian.drude}@mpi.nl
Abstract
We present a new transcription mode for
the annotation tool ELAN. This mode is
designed to speed up the process of creat-
ing transcriptions of primary linguistic data
(video and/or audio recordings of linguistic
behaviour). We survey the basic transcrip-
tion workflow of some commonly used
tools (Transcriber, BlitzScribe, and ELAN)
and describe how the new transcription in-
terface improves on these existing imple-
mentations. We describe the design of
the transcription interface and explore some
further possibilities for improvement in the
areas of segmentation and computational
enrichment of annotations.
1 Introduction
Recent years have seen an increasing interest in
language documentation: the creation and preser-
vation of multipurpose records of linguistic pri-
mary data (Gippert et al, 2006; Himmelmann,
2008). The increasing availability of portable
recording devices enables the collection of pri-
mary data even in the remotest field sites, and the
exponential growth in storage makes it possible
to store more of this data than ever before. How-
ever, without content annotation for searching and
analysis, such corpora are of limited use. Ad-
vances in machine learning can bring some mea-
sure of automation to the process (Tscho?pel et
al., 2011), but the need for human annotation re-
mains, especially in the case of primary data from
undocumented languages. This paper describes
the development and use of a new rapid transcrip-
tion interface, its integration in an open source
software framework for multimodality research,
and the possibilities it opens up for computational
uses of the annotated data.
Transcription, the production of a written rep-
resentation of audio and video recordings of
communicative behaviour, is one of the most
time-intensive tasks faced by researchers work-
ing with language data. The resulting data is use-
ful in many different scientific fields. Estimates
for the ratio of transcription time to data time
length range from 10:1 or 20:1 for English data
(Tomasello and Stahl, 2004, p. 104), but may
go up to 35:1 for data from lesser known and en-
dangered languages (Auer et al, 2010). As in all
fields of research, time is a most important limit-
ing factor, so any significant improvement in this
area will make available more data and resources
for analysis and model building. The new tran-
scription interface described here is designed for
carrying out high-speed transcription of linguis-
tic audiovisual material, with built-in support for
multiple annotation tiers and for both audio and
video streams.
Basic transcription is only the first step; fur-
ther analysis often necessitates more fine-grained
annotations, for instance part of speech tagging
or morpheme glossing. Such operations are even
more time intensive. Time spent on further an-
notations generally goes well over a 100:1 anno-
tation time to media duration ratio1 (Auer et al,
2010).The post-transcription work is also an area
with numerous possibilities for further reduction
of annotation time by applying semi-automated
annotation suggestions, and some ongoing work
1Cf. a blog post by P.K.Austin http://blogs.usyd.edu.au
/elac/2010/04/how long is a piece of string.html.
7
to integrate such techniques in our annotation sys-
tem is discussed below.
2 Semi-automatic transcription:
terminology and existing tools
Transcription of linguistic primary data has long
been a concern of researchers in linguistics and
neighbouring fields, and accordingly several tools
are available today for time-aligned annotation
and transcription. To describe the different user
interfaces these tools provide, we adopt a model
of the transcription process by (Roy and Roy,
2009), adjusting its terminology to also cover the
use case of transcribing sign language. According
to this model, the transcription of primary linguis-
tic data can be divided into four basic subtasks:
1) find linguistic utterances in the audio or video
stream, 2) segment the stream into short chunks
of utterances, 3) play the segment, and 4) type the
transcription for the segment.
Existing transcription tools implement these
four steps in different ways. To exemplify this we
discuss three such tools below. All three can be
used to create time-aligned annotations of audio
and/or video recordings, but since they have dif-
ferent origins and were created for different goals,
they present the user with interfaces that differ
quite radically.
Transcriber (Barras et al, 2001) was ?designed
for the manual segmentation and transcription of
long duration broadcast news recordings, includ-
ing annotation of speech turns, topics and acoustic
condition? (Barras et al, 2001, p. 5). It provides
a graphical interface with a text editor at the top
and a waveform viewer at the bottom. All four
subtasks from the model above, FSPT, are done
in this same interface. The text editor, where Seg-
menting and Typing are done, is a vertically ori-
ented list of annotations. Strengths of the Tran-
scriber implementation are the top-to-bottom ori-
entation of the text editor, which is in line with
the default layout of transcripts in the discipline,
and the fact that it is possible to rely on only one
input device (the keyboard) for all four subtasks.
Weaknesses are the fact that it does not mark an-
notation ends, only beginnings,and that it treats
the data as a single stream and insists on a strict
partitioning, making it difficult to handle overlap-
ping speech, common in conversational data (Bar-
ras et al, 2001, p. 18).
BlitzScribe (Roy and Roy, 2009) was devel-
oped in the context of the Human Speechome
project at the MIT Media Lab as a custom solu-
tion for the transcription of massive amounts of
unstructured English speech data collected over a
period of three years (Roy et al, 2006). It is not
available to the academic community, but we de-
scribe it here because its user interface presents
significant improvements over previous models.
BlitzScribe uses automatic speech detection for
segmentation, and thus eliminates the first two
steps of the FSPT model, Find and Segment, from
the user interface. The result is a minimalist de-
sign which focuses only on Playing and Typing.
The main strength of BlitzScribe is this stream-
lined interface, which measurably improves tran-
scription speed ? it is about four times as fast as
Transcriber (Roy and Roy, 2009, p. 1649). Weak-
nesses include its monolingual, speech-centric fo-
cus, its lack of a mechanism for speaker identi-
fication, and its single-purpose design which ties
it to the Human Speechome project and makes it
unavailable to the wider academic community.
ELAN (Wittenburg et al, 2006) was developed
as a multimedia linguistic annotation framework.
Unlike most other tools it was built with multi-
modal linguistic data in mind, supporting the si-
multaneous display and annotation of multiple au-
dio and video streams. Its data model is tier-
based, with multiple tiers available for annota-
tions of different speakers or different modalities
(e.g. speech and gesture). Its strengths are its
support for multimodal data, its handling of over-
lapping speech, its flexible tier structure, and its
open source nature. Its noted weaknesses include
a steep learning curve and a user interface that
was, as of 2007, ?not the best place to work on a
?first pass? of a transcript? (Berez, 2007, p. 288).
The new user interface we describe in this pa-
per is integrated in ELAN as a separate ?Tran-
scription Mode?, and was developed to combine
the strengths of existing implementations while at
the same time addressing their weaknesses. Fig-
ure 1 shows a screenshot of the new transcription
mode.
3 Description of the interface
From the default Annotation Mode in ELAN, the
user can switch to several other modes, one of
which is Transcription Mode. Transcription Mode
displays annotations in one or more columns. A
column collects annotations of a single type. For
8
Figure 1: The interface of the transcription mode, showing two columns: transcriptions and the corresponding
translations.
instance, the first column in Figure 1 displays all
annotations of the type ?practical orthography?
in chronological order, colour-coding for differ-
ent speakers. The second column displays cor-
responding, i.e., time aligned, annotations of the
type ?literal translation?. Beside the annotation
columns there is a pane showing the data (video
and/or audio stream) for the selected utterance.
Below it are basic playback parameters like vol-
ume and rate, some essential interface settings,
and a button ?Configure? which brings up the col-
umn selection dialog window. We provide an ex-
ample of this preference pane in Figure 2.
The basic organisation of the Transcription
Mode interface reflects its task-oriented design:
the annotation columns occupy pride of place and
only the most frequently accessed settings are
directly visible. Throughout, the user interface
is keyboard-driven and designed to minimise the
number of actions the user needs to carry out. For
instance, selecting a segment (by mouse or key-
board) will automatically trigger playback of that
segment (the user can play and pause using the
Tab key). Selecting a grey (non-existent) field in
a dependent column will automatically create an
annotation. Selection always opens up the field
for immediate editing. Arrow keys as well as user-
configurable shortcuts move to adjacent fields.
ELAN Transcription Mode improves the tran-
scription workflow by taking apart the FSPT
model and focusing only on the last two steps:
Play and Type. In this respect it is like
BlitzScribe; but it is more advanced than that and
other tools in at least two important ways. First,
it is agnostic to the type of data transcribed. Sec-
ond, it does not presuppose monolingualism and
is ready for multilingual work. It allows the dis-
play of multiple annotation layers and makes for
easy navigation between them.Further, when tran-
scription is done with the help of a native speaker
it allows for them to provide other relevant infor-
mation at the same time (such as cultural back-
ground explanations) keeping primary data and
meta-data time aligned and linked.
Some less prominently visible features of the
user interface design include: the ability to re-
order annotation columns by drag and drop; a tog-
gle for the position of the data streams (to the left
or to the right of the annotation columns); the abil-
ity to detach the video stream (for instance for dis-
play on a secondary monitor); the option to show
names (i.e. participant ID?s) in the flow of anno-
9
Figure 2: The interface of the transcription mode; the configuration dialog.
tations or to indicate them by colour-coding only;
the option to keep the active annotation centered;
and settings for font size and number of columns
(in the ?Configure? pane). These features enable
the user to customise the transcription experience
to their own needs.
The overall design of Transcription Mode
makes the process of transcription as smooth as
possible by removing unnecessary clutter, fore-
grounding the interface elements that matter, and
enabling a limited degree of customisation. Over-
all, the new interface has realised significant
speedups for many people2. User feedback in re-
sponse to the new transcription mode has been
overwhelmingly positive, e.g., the members of
mailing lists such as the Resource Network for
Linguistic Diversity3.
4 A prerequisite: semi-automatic
segmentation
As we noted in the introduction, the most im-
portant step before transcription is that of seg-
mentation (steps Find and Segment in the FSPT
model). Segmentation is a large task that involves
subdividing the audio or video stream in, possi-
bly overlapping, segments. The segments each
denote a distinct period of speech or any other
communicative act and each segment is com-
2Including ourselves, Jeremy Hammond claims that:
?Based on my last two field work trips, I am getting my tran-
scription time down below that of transcriber (but perhaps
not by much) but still keeping the higher level of data that
ELANs tiers provide - probably around 18-20 hours for an
hour of somewhat detailed trilingual annotation.?
3www.rnld.org
monly assigned to a specific speaker. This step
can potentially be sped up significantly by doing
it semi-automatically using pattern recognition
techniques, as pursued in the AVATecH project
(Auer et al, 2010).
In the AVATecH project, audio and video
streams can be sent to detection components
called ?recognisers?. Some detection compo-
nents accept the output of other recognisers as
additional input, next to the audio and/or video
streams, thus facilitating cascaded processing of
these streams. Amongst the tasks that can be per-
formed by these recognisers is the segmentation
of audio and video, including speaker assignment.
A special challenge for the recognisers in this
project is the requirement of language indepen-
dence (in contrast to the English-only situation
in the Human Speechome project that produced
Blitzscribe(Roy et al, 2006)). The recognisers
should ideally accommodate the work of field
linguists and other alike researchers and there-
fore cannot simply apply existing language and
acoustic models. Furthermore, the conditions that
are encountered in the field are often not ideal,
e.g., loud and irregular background noises such as
those from animals are common. Nevertheless,
automatic segmentation has the potential to speed
up the segmentation step greatly.
5 Future possibilities: computational
approaches to data enrichment
While a basic transcription and translation is es-
sential as a first way into the data, it is not suf-
ficient for many research questions, linguistic or
10
otherwise. Typically a morphological segmenta-
tion of the words and a labelling of each individ-
ual morph is required. This level of annotation is
also known as basic glossing (Bow et al, 2003b;
Bow et al, 2003a).
Automatically segmenting the words into their
morphological parts, without resorting to the use
of pre-existing knowledge has seen a wide vari-
ety of research (Hammarstro?m and Borin, 2011).
Based on the knowledge-free induction of mor-
phological boundaries the linguist will usually
perform corrections. Above all, a system must
learn from the input of the linguist, and must in-
corporate it in the results, improving the segmen-
tation of words going forward. However, it is well
known from typological research that languages
differ tremendously in their morphosyntactic or-
ganisation and the specific morphological means
that are employed to construct complex meanings
(Evans and Levinson, 2009; Hocket, 1954).
As far as we know, there is no current morpho-
logical segmentation or glossing system that deals
well with all language types, in particular inflec-
tional and polysynthetic languages or languages
that heavily employ tonal patterns to mark differ-
ent forms of the same word. Therefore, there is
a need for an interactive, modular glossing sys-
tem. For each step of the glossing task, one would
use one, or a set of complementary modules. We
call such modules ?annotyzers?. They generate
content on the basis of the source tiers and addi-
tional data, e.g. lexical data (or learnt states from
earlier passes). Using such modules will result
in a speedup for the researcher. We remark that
there are existing modular NLP systems, such as
GATE(Cunningham et al, 2011), however these
are tied to different workflows, i.e., they are not as
suitable for the multimodal multi-participant an-
notation process.
Currently a limited set of such functionality is
available in Toolbox and FLEX. In the case of
both Toolbox and FLEX the functionality is lim-
ited to a set of rules written by the linguist (i.e.
in a database-lookup approach). Even though
the ELAN modules will offer support for such
rules, our focus is on the automation of machine-
learning systems in order to scale the annotation
process.
Our main aim for the future is to incorporate
learning systems that support the linguists by im-
proving the suggested new annotations on the
bases of choices the linguist made earlier. The
goal there is, again, to reduce annotation time, so
that the linguist can work more on linguistic anal-
ysis and less on annotating. At the same time,
a working set of annotyzers will promote more
standardised glossing, which can then be used for
further automated research, cf. automatic tree-
bank production or similar (Bender et al, 2011).
6 Conclusions
The diversity of the world?s languages is in dan-
ger. Perhaps user interface design is not the first
thing that comes to mind in response to this sober-
ing fact. Yet in a field that increasingly works with
digital annotations of primary linguistic data, it is
imperative that the basic tools for annotation and
transcription are optimally designed to get the job
done.
We have described Transcription Mode, a new
user interface in ELAN that accelerates the tran-
scription process. This interface offers several ad-
vantages compared to similar tools in the software
landscape. It automates actions wherever pos-
sible, displays multiple parallel information and
annotation streams, is controllable with just the
keyboard, and can handle sign language as well
as spoken language data. Transcription Mode re-
duces the required transcription time by providing
an optimised workflow.
The next step is to optimise the preceding and
following stages in the annotation process. Pre-
ceding the transcription stage is segmentation and
speaker labelling, which we address using auto-
matic audio/video recogniser techniques that are
independent of the language that is transcribed.
Following transcription, we aim to support basic
glossing (and similar additional annotations based
on transcriptions) with a modular software archi-
tecture. These semi-automated steps lead to fur-
ther time savings, allowing researchers to focus
on the analysis of language data rather than on the
production of annotations.
The overall goal of the developments described
here is to help researchers working with primary
language data to use their time more optimally.
Ultimately, these improvements will lead to an in-
crease in both quality and quantity of primary data
available for analysis. Better data and better anal-
yses for a stronger digital humanities.
11
References
Eric Auer, Peter Wittenburg, Han Sloetjes, Oliver
Schreer, Stefano Masneri, Daniel Schneider, and
Sebastian Tscho?pel. 2010. Automatic annotation
of media field recordings. In Proceedings of the
ECAI 2010 Workshop on Language Technology for
Cultural Heritage, Social Sciences, and Humanities
(LaTeCH 2010), pages 31?34.
Claude Barras, Edouard Geoffrois, Zhibiao Wu, and
Mark Liberman. 2001. Transcriber: Develop-
ment and use of a tool for assisting speech corpora
production. Speech Communication, 33(1-2):5?22,
January.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and
non-local deep dependencies in a large corpus. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
397?408, Edinburgh, Scotland, UK., July. Associ-
ation for Computational Linguistics.
Andrea L. Berez. 2007. Review of EUDICO linguis-
tic annotator (ELAN). Language Documentation &
Conservation, 1(2):283?289, December.
Catherine Bow, Baden Hughes, and Steven Bird.
2003a. A four-level model for interlinear text.
Cathy Bow, Baden Hughes, and Steven Bird. 2003b.
Towards a general model of interlinear text. In
Proceedings of EMELD Workshop 2003: Digitizing
and Annotating Texts and Field Recordings. Lans-
ing MI, USA.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, Angus
Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
Nicholas Evans and Stephen C. Levinson. 2009. The
myth of language universals: Language diversity
and its importance for cognitive science. Behav-
ioral and Brain Sciences, 32(05):429?448.
Jost Gippert, Nikolaus P. Himmelmann, and Ulrike
Mosel, editors. 2006. Essentials of language docu-
mentation. Mouton de Gruyter, Berlin / New York.
Harald Hammarstro?m and Lars Borin. 2011. Un-
supervised learning of morphology. To Appear in
Computational Linguistics.
Nikolaus P. Himmelmann. 2008. Reproduction and
preservation of linguistic knowledge: Linguistics?
response to language endangerment. In Annual Re-
view of Anthropology, volume 37 (1), pages 337?
350.
Charles F. Hocket. 1954. Two models of grammatical
description. Word 10, pages 210?234.
Chris Rogers. 2010. Review of fieldworks language
explorer (flex) 3.0. In Language Documentation
& Conservation 4, pages 1934?5275. University of
Hawai?i Press.
Brandon C. Roy and Deb Roy. 2009. Fast transcrip-
tion of unstructured audio recordings. In Proceed-
ings of Interspeech 2009, Brighton, England.
Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat,
Michael Fleischman, Brandon C. Roy, Nikolaos
Mavridis, Stefanie Tellex, Alexia Salata, Jethran
Guinness, Micheal Levit, and Peter Gorniak. 2006.
The human speechome project. In Paul Vogt, Yu-
uga Sugita, Elio Tuci, and Chrystopher Nehaniv, ed-
itors, Symbol Grounding and Beyond, volume 4211
of Lecture Notes in Computer Science, pages 192?
196. Springer, Berlin / Heidelberg.
Michael Tomasello and Daniel Stahl. 2004. Sam-
pling children?s spontaneous speech: How much is
enough? Journal of Child Language, 31(01):101?
121.
Sebastian Tscho?pel, Daniel Schneider, Rolf Bardeli,
Peter Wittenburg, Han Sloetjes, Oliver Schreer, Ste-
fano Masneri, Przemek Lenkiewicz, and Eric Auer.
2011. AVATecH: Audio/Video technology for hu-
manities research. Language Technologies for Dig-
ital Humanities and Cultural Heritage, page 86.
Peter Wittenburg, Hennie Brugman, Albert Russel,
and Han Sloetjes. 2006. ELAN: a professional
framework for multimodality research. In Proceed-
ings of LREC 2006.
12
