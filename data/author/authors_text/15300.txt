A cheap and fast way to build useful translation lexicons  
 
Dan TUFIS 
Romanian Academy Centre for Artificial Intelligence 
13, ?13 Septembrie?  
Bucharest, ROMANIA, RO-74311 
tufis@racai.ro 
 
Abstract  
The paper presents a statistical approach to 
automatic building of translation lexicons 
from parallel corpora. We briefly describe 
the pre-processing steps, a baseline iterative 
method, and the actual algorithm. The 
evaluation for the two algorithms is 
presented in some detail in terms of 
precision, recall and processing time.  We 
conclude by briefly presenting some of our 
applications of the multilingual lexicons 
extracted by the method described herein. 
Introduction 
The scientific and technological advancement in 
many domains is a constant source of new term 
coinage and therefore keeping up with 
multilingual lexicography in such areas is very 
difficult unless computational means are used. 
Translation lexicons, based on translation 
equivalence relation are lexical knowledge 
sources, which can be extracted from parallel 
texts (even from comparable texts), with very 
limited human resources. The translation 
lexicons appear to be quite different from the 
corresponding printed lexicons, meant for the 
human users. There are well known reasons for 
these differences and we will not discuss the 
issue here, but exactly these differences make 
them very useful (in spite of inherent noise 
content) in many computer-based applications. 
We will discuss some of our experiments based 
on automatically extracted multilingual lexicons. 
Most modern approaches to automatic extraction 
of translation equivalents rely on statistical 
techniques and roughly fall into two categories. 
The hypotheses-testing methods such as Gale 
and Church (1991), Smadja et al (1996), 
Tiedmann (1998), Ahrenberg (2000), Melamed 
(2001) etc. use a generative device that produces 
a list of translation equivalence candidates 
(TECs), extracted from corresponding segments 
of the parallel texts (translation units-TU), each 
of them being subject to an independence 
statistical test. The TECs that show an 
association measure higher than expected under 
the independence assumption are assumed to be 
translation-equivalence pairs (TEPs). The TEPs 
are extracted independently one of another and 
therefore the process might be characterized as a 
local maximization (greedy) one. The estimating 
approach such as Brown et al (1993), Kay and 
R?scheisen (1993), Kupiec (1993), Hiemstra 
(1997) etc. is based on building from data a 
statistical bitext model, the parameters of which 
are to be estimated according to a given set of 
assumptions. The bitext model allows for global 
maximization of the translation equivalence 
relation, considering not individual translation 
equivalents but sets of translation equivalents 
(sometimes called assignments). There are pros 
and cons for each type of approach, some of 
them discussed in Hiemstra (1997).  
Our translation equivalents extraction process 
may be characterized as a ?hypotheses testing? 
approach and does not need a pre-existing 
bilingual lexicon for the considered languages. If 
such a lexicon exists it can be used to eliminate 
spurious candidate translation equivalence pairs 
and thus to speed up the process and increase its 
accuracy. 
1 Assumptions, preprocessing and a baseline  
There are several underlying assumptions one 
may consider in keeping the computational 
complexity of a translation lexicon extraction 
algorithm as low as possible. None of these 
hopotheses is true in general, but the situations 
where they are not observed are rare enough so 
that ignoring the exceptions would not produce a 
significant number of errors and would not lose 
too many useful translations. The assumptions 
we made were the following: 
? a lexical token in one half of the translation 
unit (TU) corresponds to at most one non-empty 
lexical unit in the other half of the TU; this is the 
1:1 mapping assumption which underlines the 
work of many other researchers (Ahrenberg et al
(2000), Brew and McKelvie (1996), Hiemstra 
(1996), Kay and R?scheisen (1993), Tiedmann 
(1998), Melamed (2001) etc);  
? a polysemous lexical token, if used several 
times in the same TU, is used with the same 
meaning; this assumption is explicitly used by 
Gale and  Church (1991), Melamed (2001) and 
implicitly by all the previously mentioned 
authors; 
? a lexical token in one part of a TU can be 
aligned to a lexical token in the other part of the 
TU only if the two tokens have compatible types 
(part-of-speech); in most cases, compatibility 
reduces to the same POS, but it is also possible 
to define other compatibility mappings (e.g. 
participles or gerunds in English are quite often 
translated as adjectives or nouns in Romanian 
and vice-versa); 
? although the word order is not an invariant of 
translation, it is not random either (Ahrenberg et 
al (2000)); when two or more candidate 
translation pairs are equally scored, the one 
containing tokens which are closer in relative 
position are preferred.  
 The proper extraction of translation 
equivalents requires special pre-processing:  
? sentence alignment; we used a slightly 
modified version of CharAlign described by 
Gale and Church (1993) . 
? tokenization; the segmenter we used (MtSeg, 
developed by P. di Cristo for the MULTEXT 
project: http://www.lpl.univ-aix.fr/projects/multext/ 
MtSeg/), may process multiword expressions as 
single lexical tokens. The segmenter comes with 
tokenization resources for several Western 
European languages, further enhanced in the 
MULTEXT-EAST project (Dimitrova et al
(1998), Erjavec et al(1998), Tufis et al(1998)) 
with corresponding resources for Bulgarian, 
Czech, Estonian, Hungarian, Romanian and 
Slovene.  
? tagging and lemmatization; we used a tiered 
tagging with combined language models 
approach (Tufis (1999, 2000)) based on a 
Brants?s TnT tagger. 
After the sentence alignment, tagging and 
lemmatization, the first step is to compute a list 
of translation equivalence candidates (TECL). 
This list contains several sub-lists, one for each 
POS considered in the extraction procedure.  
Each POS-specific sub-list contains several pairs 
of tokens <tokenS tokenT> of the corresponding 
POS that appeared in the same TUs. TECL 
contains a lot of noise and many TECs are very 
improbable. In order to eliminate much of this 
noise, the most unlikely candidates are  filtered 
out of  TECL. The filtering is based on scoring 
the association between the tokens in a TEC.  
For the ranking of the TECs and their filtering 
we experimented with 4 scoring functions: MI 
(pointwise mutual information), DICE, LL 
(loglikelihood), and ?2 (chi-square). After 
empirical tests we decided to use LL test with 
the threshold value set to 9. 
Our baseline algorithm, BASE, is a very simple 
iterative algorithm, very fast and can be 
enhanced in many ways. It has some similarities 
to the iterative algorithm presented in Ahrenberg 
et al(2000) but unlike it, our algorithm avoids 
computing various probabilities (or better said 
probability estimates) and scores (t-score). At 
each iteration step, the pairs that pass the 
selection  (see below) will be removed from 
TECL so that this list is shortened after each step 
and eventually may be emptied. Based on 
TECL, for each POS a Sm*Tn contingency table 
is constructed, with Sm the number of token 
types in the first part of the bitext (call it source) 
and Tn the number of token types in the other 
part of the bitext (call it target). Source token 
types index the rows of the table and the target 
token types (of the same POS) index the 
columns. Each cell (i,j) contains the number of 
occurrences in TECL of the <TSi, TTj> TEC. 
Equations below express the selection condition:  
{ }
       3n               )2(
&)n(n)n(n qp, | TTTP (1)
 ij
pj ijiq ijTj Si
k
?
????><=
This is the key idea of the iterative extraction 
algorithm: it expresses the requirement that in 
order to select a TEC <TSi, TTj> as a translation 
equivalence pair, the number of associations of 
TSi with TTj must be higher than (or at least equal 
to) any other TTp (p?j). The same holds for the 
other way around. All the pairs selected in TPk 
are removed (the respective counts are zeroed). 
If TSi is translated in more than one way the rest 
of translations will be found in subsequent steps 
(if frequent enough). The most used translation 
of a token TSi will be found first. The equation 
(2) represents a frequency relevance threshold, 
necessary in order to diminish the influence of 
data sparseness. 
 
2 An improved algorithm (BETA) 
One of the main deficiencies of the BASE 
algorithm is that it is sensitive to what Melamed 
(2001) calls indirect associations. If <TSi, TTj> 
has a high association score and TTj collocates 
with TTk, it might very well happen that <TSi, 
TTk> gets also a high association score. 
Although, as observed by Melamed (2001), in 
general, the indirect associations have lower 
scores than the direct (correct) associations, they 
could receive higher scores than many correct 
pairs and this will not only generate wrong 
translation equivalents, but will eliminate from 
further considerations several correct pairs, 
deteriorating the procedure?s recall. To weaken 
this sensitivity, the BASE algorithm had to 
impose that the number of occurrences of a TEC 
be at least 3, thus filtering out more than 50% of 
all the possible TECs. Still, because of the 
indirect association effect, in spite of a very 
good precision (more than 98%) out of the 
considered pairs another approximately 50% 
correct pairs were missed. The BASE algorithm 
has this deficiency because it looks on the 
association scores globally, and does not check 
within the TUs if the tokens making the indirect 
association are still there.  
To diminish the influence of the indirect 
associations and consequently removing the 
frequency threshold, we modified the BASE 
algorithm so that the maximum score is not 
considered globally but within each of the TUs. 
This brings BETA closer to the competitive 
linking algorithm described in Melamed (2001). 
The competing pairs are only the TECs 
generated from the current TU and the one with 
the best score is the first selected. Based on the 
1:1 mapping hypothesis, any TEC containing the 
tokens in the winning pair are discarded. Then, 
the next best scored TEC in the current TU is 
selected and again the remaining pairs that 
include one of the two tokens in the selected pair 
are discarded. The multiple-step control in 
BASE, where each TU was scanned several 
times (equal to the number of iteration steps) is 
not necessary anymore. The BETA algorithm 
will see each TU unit only once but the TU is 
processed until no further TEPs can be reliably 
extracted or TU is emptied. This modification 
improves both the precision and recall in 
comparison with the BASE algorithm. In 
accordance with the 1:1 mapping hypothesis, 
when two or more TEC pairs of the same TU 
share the same token and they are equally 
scored, the algorithm has to make a decision and 
choose only one of them. If there exists a seed 
lexicon and one of the competitors is in this 
lexicon it will be the winner. Otherwise, 
decision is made based on two heuristics: string 
similarity scoring and relative distance. 
The similarity measure we used, COGN(TS, TT), 
is very similar to the XXDICE score described in 
Brew and McKevie (1996).  
The threshold for the COGN(TS, TT) test was 
empirically set to 0.42. This value depends on 
the pair of languages in the considered bitext. 
The actual implementation of the COGN test 
considers a language dependent normalization 
step, which strips some suffixes, discards the 
diacritics and reduces some consonant doubling 
etc. This normalization step was hand written, 
but, based on available lists of cognates, it could 
be automatically induced.  
The second filtering condition, DIST(TS, TT) is 
based on the difference between the relative 
positions in the TU of the TS and TT 
respectively. The threshold for the DIST(TS, TT) 
was set to 2.   
The COGN(TS, TT) filter is stronger than 
DIST(TS, TT), so that the TEC with the highest 
similarity score is the preferred one. If the 
similarity score is irrelevant, the weaker filter 
DIST(TS, TT) gives priority to the pairs with the 
smallest relative distance between the 
constituent tokens. 
3 BASE and BETA Evaluations  
We conducted experiments on the "1984" 
multilingual corpus (Dimitrova et al(1998)) 
containing 6 translations of the English original. 
This corpus was developed within the Multext-
East project, published on a CD-ROM (Erjavec 
et al(1998)) and recently improved within the 
CONCEDE project. The newer version is 
distributed by TRACTOR (www.tractor.de). 
Each monolingual part of the corpus (Bulgarian-
Bg, Czech-Cz, Estonian-Et, Hungarian-Hu, 
Romanian-Ro and Slovene-Si) was tokenized, 
lemmatized, tagged and sentence aligned to the 
English hub.  
The evaluation protocol specified that all the 
translation pairs be judged in context, so that if 
one pair is found to be correct in at least one 
context, then it should be judged as correct. The 
evaluation was done for both BASE and BETA 
algorithms but on different scales. The BASE 
algorithm was run on all the 6 bitexts with the 
English hub and native speakers of the second 
language in the bitexts (with good command of 
English) validated 4 of the 6 bilingual lexicons. 
The lexicons contained all parts of speech 
defined in the MULTEXT-EAST lexicon 
specifications (Erjavec et al(1998)) except for 
interjections, particles and residuals.  
The BETA algorithm was ran on the Romanian-
English bitext, but at the time of this writing the 
evaluation was finalized only for the nominal 
translation pairs. 
3.1 BASE Evaluation 
For validation purposes we limited the number of 
iteration steps to 4. The extracted dictionaries 
contain adjectives (A), conjunctions (C), 
determiners (D), numerals (M), nouns (N), 
pronouns (P), adverbs (R), prepositions (S) and 
verbs (V). The precision (Prec) was computed as 
the number of correct TEPs divided by the total 
number of extracted TEPs. The recall (considered 
for the non-English language in the bitext) was 
computed two ways: the first one, Rec*, which 
took into account only the tokens processed by 
the algorithm (those that appeared at least three 
times). The second one, Rec, took into account 
all the tokens irrespective of their frequency 
counts. Rec* is defined as the number of source 
lemma types in the correct TEPs divided by the 
number of lemma types in the source language 
with at least 3 occurrences. Rec is defined as the 
number of source lemma types in the correct 
TEPs divided by the number of lemma types in 
the source language.  
The rationale for showing Rec* is to estimate the 
proportion of the missed considered tokens. This 
might be of interest when precision is of utmost 
importance. When the threshold of minimal 3 
occurrences is considered, the algorithm 
provides a high precision and a good recall 
(Rec*). The evaluation was fully done for Et, Hu 
and Ro and partially for Si (the first step was 
fully evaluated while the rest were evaluated 
from randomly selected pairs).  
The results after 4 iteration steps are shown in 
the table below for the Et-En, Hu-En, Ro-En and 
Si-En lexicons. From the 6 bilingual lexicons we 
also derived a 7-language lexicon (2862 entries), 
with English as a search hub (see 
http://www.racai.ro/~tufis/BilingualLexicons/ 
AutomaticallyExtractedBilingualLexicons.html). 
 Et-En Hu-En Ro-En Si-En 
Entries 
Prec/Rec* 
Rec 
1911 
96.2/57.9 
18.8 
1935 
96.9/56.9 
19.3 
2227 
98.4/58.8 
25.2 
1646 
98.7/57.9 
22.7 
Table 1: BASE evaluation for all POS and 4 
iteration steps 
To facilitate the comparison with the evaluation 
of the BETA algorithm we ran the BASE 
algorithm for extracting the noun translation pairs 
from the Romanian-English bitext. The noun 
extraction had the second worst accuracy (the 
worst was the adverb), and therefore we 
considered that an in-depth evaluation of this 
case would be more informative than a global 
evaluation. We set no limit for the number of 
steps and lowered the occurrence threshold to 2. 
The program stopped after 10 steps with a 
number of 1900 extracted translation pairs, out of 
which 126 were wrong. Compared with the 4 
steps run the precision decreased to 93.4%, but 
both Rec* (70.1%) and Rec  (39.8%) improved.  
In the 10-step run of the BASE algorithm, the 
extracted noun pairs covered 85.83% of the 
nouns in the Romanian part of the bitext. 
We should mention that in spite of the general 
practice in computing recall for bilingual lexicon 
extraction task (be it Rec* or Rec) this is only an 
approximation of the real recall. The reason for 
this approximation is that in order to compute 
the real recall one should have a gold standard 
with all the words aligned by human evaluators. 
In general such a gold standard bitext is not 
available and the recall is either approximated as 
above, or is evaluated on a small sample and the 
result is taken to be more or less true for all the 
bitext. 
3.2   BETA Evaluation 
The BETA algorithm preserves the simplicity of 
the BASE algorithm but it significantly 
improves its recall (Rec) at the expense of some 
loss in precision (Prec). Its evaluation was done 
for the Romanian-English bitext, without a seed 
lexicon and only with respect to the lexicon of 
nouns. The filtering condition in case of ties was 
the following:  
max(COGN(TjS,TjT)?0.4)?min(DIST(TjS,TjT)?2) 
The results show that the Rec (72.7%) almost 
doubled compared with the best Rec obtained by 
the BASE algorithm for nouns (39.9%). 
However, the price for these significant 
improvements was a serious deterioration of the 
Prec (78.3% versus 93.4%).  
Noun types in text 3435 
No. entries 4023 
Correct entries 3149 
Types in correct entries 2496 
Prec/Rec 78.3/72.7 
Table2: BETA evaluation for the Ro-EN lexicon 
of nouns; both COGN and DIST filters used 
The analysis of the wrong translation pairs 
revealed that most of them were hapax pairs 
(pairs appearing only once) and they were 
selected because the DIST measure enabled 
them, so we considered that this filter is not 
discriminative enough for hapaxes. On the other 
hand for the non-hapax pairs the DIST condition 
was successful in more than 85% of the cases. 
Therefore, we decided that the additional DIST 
filtering condition be preserved for non-hapax 
competitors only. 
Although 166 erroneous TEPs were removed, 
144 good TEP were lost. Prec improved  (81.0% 
versus 78.3%) but Rec depreciated (69.0% 
versus 72.7%).   
The BASE algorithm allows for trading off 
between Prec and Rec by means of the number 
of iteration steps. 
Noun types in text 3435 
No. entries 3713 
Correct entries 3007 
Types in correct entries 2371 
Prec/Rec 81.0/69.0 
Table3: BETA evaluation for the Ro-EN 
lexicon of nouns; only COGN filter used 
The BETA algorithm allows for similar trading 
off between Prec and Rec by means of the 
COGN and DIST thresholds and obviously by 
means of an occurrence threshold. For instance 
when BETA was set to ignore the hapax pairs, 
its Prec was 96.1% (better then the BASE 
precision 93.4%) Rec* was 96.4% (BASE with 
10 iterations had a Rec* of 70.1%) and Rec was 
60.0% (BASE with 10 iterations had a Rec of 
39.8%). 
4 Partial translations 
As the alignment model used by the translation 
equivalence extraction is based on the 1:1 
mapping hypothesis, inherently it will find 
partial translations for those cases where one or 
more words in one language must be translated 
by two or more words in the other language. 
Although we used a tokenizer aware of 
compounds in the two languages, its resources 
were obviously partial. In the extracted noun 
lexicon, the evaluators found 116 partial 
translations (3.86%). In this section we will 
discuss one way to recover the correct 
translations for the partial ones, discovered by 
our 1:1 mapping-based extraction program. 
First, from each part of the bitext a set of 
possible collocations was extracted by a simple 
method called ?repeated segments? analysis. 
Any sequence of two or more tokens that 
appears more than once is retained. 
Additionally, the tags attached to the words 
occurring in a repeated segment must observe 
the syntactic patterns characterizing most of the 
real collocations. For the noun lexicon we 
considered only forms of <head-noun 
(functional_word) modifier> as Romanian 
patterns and <modifier (functional_word) head-
noun> as English patterns. If all the content 
words contained in a repeated segment have 
translation equivalents, then the repeated 
segment is discarded as not being relevant for a 
partial translation. Otherwise, the repeated 
segment is stored in the lexicon as a translation 
for the translation of its head-noun. This simple 
procedure managed to recover 62 partial 
translations and improve other 12 (still partial, 
but better). 
5 Implementation 
The extraction programs, both BASE and 
BETA, are written in Perl and run under 
practically any platform (Perl implementations 
exist not only for UNIX/LINUX but also for 
Windows, and MACOS). Although, as one 
reviewer rightfully noticed, the speed is not 
really relevant for such an algorithm, evaluation 
of the current speed shows that, the approach 
being computationally very cheap,  there is room 
for adding more sophisticated ?association 
functions? without too much concern for the 
overall response time. Table 4 shows the BASE 
running time for each bitext in the "1984" 
parallel corpus (4 steps, all POS considered).  
Bitext Bg-
En 
Cz- 
En 
Et- 
En 
Hu- 
En 
Ro-En 
4         28
steps 
Si- 
En 
Extraction 
time (sec) 
181 148 139 220 183 415 157
Table 4:BASE extraction time for each of the 
bilingual lexicons (all POS) 
The running time for extraction of the noun 
Romanian-English lexicon (Cygwin UNIX 
emulator for Windows on a PII/233Mhz with 96 
MB RAM) for BASE was 103 seconds while for 
BETA was 234 seconds.  
A quite similar approach to our BASE algorithm 
(also implemented in Perl) is presented in 
Ahrenberg et al(2000) and for a novel of about 
half the length of Orwell's "1984" their 
algorithm needed 55 minutes on a Ultrasparc1 
Workstation with 320 MB RAM. They used a 
frequency threshold of 3 and the best results 
reported are 92.5% precision and 54.6% recall 
(our Rec*). For a computer manual containing 
about 45% more tokens than our corpus, their 
algorithm needed 4.5 hours with the best results 
being 74.94% precision and 67,3% recall (Rec*).  
The BETA algorithm is closer to Melamed?s 
extractor, although our program is greedier and 
never returns to a visited translation unit. 
6 Applications and further work 
We used the multilingual lexicon, mentioned 
before, for a sense discrimination exercise 
described in Erjavec et al(2001) where the 
criterion for sense clustering was the way the 
different occurrences of an English word in the 
?1984? parallel corpus were translated in the 
other 6 languages. The experiment carried on 
involved 91 highly ambiguous English nouns 
and was extremely encouraging; new results are 
described in Erjavec&el all (2002).  
Another application of the translation lexicons 
was in the BALKANET project aimed at 
developing wordnets for Balkan languages, 
Romanian included. The translation lexicons 
were used both in building from scratch, but in a 
harmonized way, the synsets for the base 
concepts and also for cross-lingual validation on 
running text (this was again the ?1984? novel) of 
the interlingual index (ILI) mapping of these 
basic concepts. Considering that 4 languages in 
the BALKANET are represented in the ?1984? 
parallel corpus we plan to take advantage of the 
ILI mapping for further refinement of the word-
sense discrimination method mentioned above 
and add cluster labeling. The obvious language 
independent labeling is based on ILI-record 
numbers. 
The experiments reported here were evaluated 
on European language. A new experiment has 
been preliminarily evaluated for an extract of 
500 sentences Chinese-English form a parallel 
corpus of juridical texts. The experiment was 
focused on noun translations extraction, used an 
LL-score threshold set to 9 and no conflict 
resolution method for the competitive 
translations. We had two result sets: 
RS1: contains translations which haven?t 
competitors (that is whenever there were 
competing translations for the same word 
none of them was selected) 
RS2:  differs from DS1 by the inclusion in 
the output lexicon of all the competing 
translations. 
It is obvious that if 1:1 mapping hypothesis is 
true, for any competing translations included in 
RS2 only 1 is correct and all the others are 
errors. Therefore the precision for RS2 is much 
less than for RS1.  
The results of this experiment are shown in 
Table 5 and they show that without making a 
decision on the competing translations we either 
loose many good translations (RS1) or include a 
lot of noise (RS2). 
Result set # extr. pairs precision recall 
RS1 187 93.04% 33.6% 
RS2 545 49.9% 98.1% 
Table 5: BETA results for CN-EN experiment 
Further work will address the issue of defining 
adequate heuristics for filtering out competing 
candidates. 
Acknowledgements 
Our thanks go to Chang Baobao who evaluated 
the Chinese-English results as well as to 
Wolfgang Teubert who launched the CN-EN 
experiment in the framework of the European 
concerted action TELRI. 
References  
Ahrenberg, L., M. Anderson, M. Merke (2000) A 
knowledge-lite approach to word alignment, In 
?Parallel Text Processing?. V?ronis, J. (ed). Text, 
Speech and Language Technology Series, Kluwer 
Academic Publishers, pp. 97-116 
Brew, C., McKelvie, D. (1996) Word-pair extraction 
for lexicography http:///www.ltg.ed.ac.uk/~chrisbr/ 
papers/nemplap96 
Brown, P., Pietra, S. A. Della Pietra, V. J. Della 
Pietra, and R. L. Mercer (1993). The mathematics 
of statistical machine translation: parameter 
estimation. In Computational Linguistics 19/2, pp. 
263-311.  
Dimitrova, L, T. Erjavec, N. Ide, H. Kaalep, V. 
Petkevic, D. Tufis (1998) Multext-East: Parallel 
and Comparable Corpora and Lexicons for Six 
Central and East European Languages. In 
Proceedings of COLING, Montreal, Canada, pp. 
315-319. 
Gale, W., K.W. Church (1991) Identifying word 
correspondences in parallel texts. In Proceedings 
of the 4th DARPA Workshop on Speech and 
Natural Language,  pp. 152-157.  
Gale, W.A., K.W. Church (1993) A Program for 
Aligning Sentences in Bilingual Corpora in 
Computational Linguistics, 19/1, pp. 75-102 
Erjavec, T., Lawson A., Romary, L. (1998) East Meet 
West: A Compendium of Multilingual Resources. 
TELRI-MULTEXT EAST CD-ROM, ISBN: 3-
922641-46-6. 
Erjavec T., Ide N., Tufis, D.(2001) Automatic Sense 
Tagging Using Parallel Corpora. In Proceedings 
of the 6th Natural Language Processing Pacific Rim 
Symposium, Tokyo, Japan,  pp. 212-219 
Hiemstra, D. (1997) Deriving a bilingual lexicon for 
cross language information retrieval. In 
Proceedings of Gronics, pp. 21-26  
Kay, M., R?scheisen M. (1993) Text-Translation 
Alignment. In Computational Linguistics, 19/1, pp. 
121-142 
Kupiec, J.(1993) An algorithm for finding noun 
phrase correspondences in bilingual corpora. In 
Proceedings of the 31st Annual Meeting of the 
ACL, pp. 17-22 
Melamed, D (2001) Empitical ethods for Exploiting 
Parallel Texts. The MIT Press. Cambridge, 
Massachusetts, London, England, 195 p. 
Smadja,F., K. R. McKeown, V. Hatzivassiloglou 
(1996) Translating collocations for bilingual 
lexicons: A statistical approach. In Computational 
Linguistics, 22/1, pp. 1-38 
Tiedemann, J. (1998) Extraction of Translation 
Equivalents from Parallel Corpora 
http://stp.ling.uu.se /~joerg 
Tufis, D.(1999) Tiered Tagging and Combined 
Classifiers. In Text, Speech and Dialogue, F. 
Jelinek, E. N?th (eds), Lecture Notes in Artificial 
Intelligence 1692, Springer,  pp. 29-33 
Tufis, D., Ide, N. Erjavec, T (1998) Standardized 
Specifications, Development and Assessment of 
Large Morpho-Lexical Resources for Six Central 
and Eastern European Languages. In Proceedings 
of LREC, Granada, Spain,   pp. 233-240 
Tufis, D.(2000) Using a Large Set of Eagles-
compliant Morpho-Syntactic Descriptors as a 
Tagset for Probabilistic Tagging. In Proceedings of 
the LREC, Athens, Greece, pp. 1105 -1112 
Tufis, D., Barbu, A. (2001) Extracting multilingual 
lexicons from parallel corpora. In Proceedings of 
the ACH/ALLC, New York University, pp.122-
124 
Tufis D., Barbu A.(2001) Computational Bilingual 
Lexicography: Automatic Extraction of Translation 
Dictionaries. In International Journal on Science 
and Technology of Information, Romanian 
Academy, ISSN 1453-8245, 4/3-4,  pp.325-352.  
Erjavec T., Ide N., Tufis D. (2002) Sense 
discrimination with Parallel Corpora. Proceedings 
of the SIGLEX Workshop on Word Sense 
Disambiguation: Recent Successes and Future 
Directions.  ACL2002, July, Philadelphia 
Fine-Grained Word Sense Disambiguation Based on Parallel Corpora,  
Word Alignment, Word Clustering and Aligned Wordnets 
Dan TUFI? 
Institute for Artificial 
Intelligence 
13, ?13 Septembrie? 
Bucharest, 050711 
Romania 
tufis@racai.ro 
Radu ION 
Institute for Artificial 
Intelligence  
13, ?13 Septembrie? 
Bucharest, 050711 
Romania 
radu@racai.ro 
Nancy IDE  
Department of Computer Science
Vassar College  
Poughkeepsie,  
NY 12604-0520  
USA 
ide@cs.vassar.edu 
Abstract 
The paper presents a method for word sense 
disambiguation based on parallel corpora. The 
method exploits recent advances in word 
alignment and word clustering based on 
automatic extraction of translation equivalents 
and being supported by available aligned 
wordnets for the languages in the corpus. The 
wordnets are aligned to the Princeton 
Wordnet, according to the principles 
established by EuroWordNet. The evaluation 
of the WSD system, implementing the 
method described herein showed very 
encouraging results. The same system used in 
a validation mode, can be used to check and 
spot alignment errors in multilingually 
aligned wordnets as BalkaNet and 
EuroWordNet.  
1 Introduction 
Word Sense Disambiguation (WSD) is well-
known as one of the more difficult problems in 
the field of natural language processing, as noted 
in  (Gale et al 1992; Kilgarriff, 1997; Ide and 
V?ronis, 1998), and others. The difficulties stem 
from several sources, including the lack of means 
to formalize the properties of context that 
characterize the use of an ambiguous word in a 
given sense, lack of a standard (and possibly 
exhaustive) sense inventory, and the subjectivity 
of the human evaluation of such algorithms. To 
address the last problem, (Gale et al 1992) argue 
for upper and lower bounds of precision when 
comparing automatically assigned sense labels 
with those assigned by human judges. The lower 
bound should not drop below the baseline usage 
of the algorithm (in which every word that is 
disambiguated is assigned the most frequent 
sense) whereas the upper bound should not be too 
restrictive? when the word in question is hard to 
disambiguate even for human judges (a measure 
of this difficulty is the computation of the 
agreement rates between human annotators). 
Identification and formalization of the 
determining contextual parameters for a word 
used in a given sense is the focus of WSD work 
that treats texts in a monolingual setting?that is, 
a setting where translations of the texts in other 
languages either do not exist or are not 
considered. This focus is based on the 
assumption that for a given word w and two of its 
contexts C1 and C2, if C1 ? C2 (are perfectly 
equivalent), then w is used with the same sense in 
C1 and C2. A formalized definition of context for 
a given sense would then enable a WSD system 
to accurately assign sense labels to occurrences 
of w in unseen texts. Attempts to characterize 
context for a given sense of a word have 
addressed a variety of factors: 
? Context length: what is the size of the window 
of text that should be considered to determine 
context?  Should it consist of only a few words, 
or include much larger portions of text? 
? Context content: should all context words be 
considered, or only selected words (e.g., only 
words in a certain part of speech or a certain 
grammatical relations to the target word)? Should 
they be weighted based on distance from the 
target or treated as a ?bag of words?? 
? Context formalization: how can context 
information be represented to enable definitions 
of an inter-context equivalence function? Is there 
a single representation appropriate for all words, 
or does it vary according to, for example, the 
word?s part of speech? 
The use of multi-lingual parallel texts 
provides a very different approach to the problem 
of context identification and characterization. 
?Context? now becomes the word(s) by which 
the target word (i.e., the word to be 
disambiguated) is translated in one or more other 
languages. The assumption here is that different 
senses of a word are likely to be lexicalized 
differently in different languages; therefore, the 
translation can be used to identify the correct 
sense of a word. Effectively, the translation 
captures the context as the translator conceived it. 
The use of parallel translations for sense 
disambiguation brings up a different set of issues, 
primarily because the assumption that different 
senses are lexicalized differently in different 
languages is true only to an extent. For instance, 
it is well known that many ambiguities are 
preserved across languages (e.g. the French 
int?r?t and the English interest), especially 
languages that are relatively closely related. This 
raises new questions: how many languages, and 
of which types (e.g., closely related languages, 
languages from different language families), 
provide adequate information for this purpose? 
How do we measure the degree to which 
different lexicalizations provide evidence for a 
distinct sense? 
We have addressed these questions in 
experiments involving sense clustering based on 
translation equivalents extracted from parallel 
corpora (Ide, 199; Ide et al, 2002). Tufi? and Ion 
(2003) build on this work and further describe a 
method to accomplish a ?neutral? labelling for 
the sense clusters in Romanian and English that 
is not bound to any particular sense inventory. 
Our experiments confirm that the accuracy of 
word sense clustering based on translation 
equivalents is heavily dependent on the number 
and diversity of the languages in the parallel 
corpus and the language register of the parallel 
text. For example, using six source languages 
from three language families (Romance, Slavic 
and Finno-Ugric), sense clustering of English 
words was approximately 74% accurate; when 
fewer languages and/or languages from less 
diverse families are used accuracy drops 
dramatically. This drop is obviously a result of 
the decreased chances that two or more senses of 
an ambiguous word in one language will be 
lexicalized differently in another when fewer 
languages, and languages that are more closely 
related, are considered. 
To enhance our results, we have explored the 
use of additional resources, in particular, the 
aligned wordnets in BalkaNet (Tufi? et al 
2004a). BalkaNet  is a European project that is 
developing monolingual wordnets for five Balkan 
languages (Bulgarian, Greek, Romanian Serbian, 
and Turkish) and improving the Czech wordnet 
developed in the EuroWordNet project. The 
wordnets are aligned to the Princeton Wordnet 
(PWN2.0), taken as an interlingual index, 
following the principles established by the 
EuroWordNet consortium. The underlying 
hypothesis in this experiment exploits the 
common intuition that reciprocal translations in 
parallel texts should have the same (or closely 
related) interlingual meanings (in terms of 
BalkaNet, interlingual index (ILI) codes). 
However, this hypothesis is reasonable if the 
monolingual wordnets are reliable and correctly 
linked to the interlingual index (ILI). Quality 
assurance of the wordnets is a primary concern in 
the BalkaNet project, and to this end, the 
consortium developed several methods and tools 
for validation, described in various papers 
authored by BalkaNet consortium members (see 
Proceedings of the Global WordNet Conference, 
Brno, 2004).  
We previously implemented a language-
independent disambiguation program, called 
WSDtool, which has been extended to serve as a 
multilingual wordnet checker and specialized 
editor for error-correction. In (Tufi?, et al, 2004) 
it was demonstrated that the tool detected several 
interlingual alignment errors that had escaped 
human analysis. In this paper, we describe a 
disambiguation experiment that exploits the ILI 
information in the corrected wordnets 
2 Methodology and the algorithm 
Our methodology consists of the following steps: 
1. given a bitext TL1L2 in languages L1 and L2 for 
which there are aligned wordnets, extract all pairs 
of lexical items that are reciprocal 
translations:{<WiL1 WjL2>+} 
2. for each lexical alignment <WiL1 WjL2>, extract 
the ILI codes for the synsets that contain WiL1 and 
WjL2 respectively to yield two lists of ILI codes, 
L1ILI(WiL1) and L2ILI(WjL2) 
3. identify one ILI code common to the 
intersection L1ILI(WiL1) ? L2ILI(WjL2) or a pair of 
ILI codes ILI1? L1ILI(WiL1)  and ILI2? L2ILI(WjL2), 
so that ILI1 and ILI2 are the most similar ILI 
codes (defined below) among the candidate pairs 
(L1ILI(WiL1)?L2ILI(WjL2) [? = Cartesian product]. 
The accuracy of step 1 is essential for the 
success of the validation method. A recent shared 
task evaluation) of different word aligners 
(www.cs.unt.edu/~rada/wpt, organized on the 
occasion of the Conference of the NAACL 
showed that step 1 may be solved quite reliably. 
Our system (Tufi? et al 2003) produced lexicons 
relevant for wordnets evaluation, with an 
aggregated F-measure as high as 84.26%. 
Meanwhile, the word-aligner was further 
improved so that current performance on the 
same data is about 1% better on all scores in 
word alignment and about 2% better in wordnet-
relevant dictionaries. The word alignment 
problem includes cases of null alignment, where 
words in one part of the bitext are not translated 
in the other part; and cases of expression 
alignment, where multiple words in one part of 
the bitext are translated as one or more words in 
the other part. Word alignment algorithms 
typically do not take into account the part of 
speech (POS) of the words comprising a 
translation equivalence pair, since cross-POS 
translations are rather frequent. However, for the 
aligned wordnet-based word sense 
disambiguation we discard both translation pairs 
which do not preserve the POS and null 
alignments. Multiword expressions included in a 
wordnet are dealt with by the underlying 
tokenizer. Therefore, we consider only one-to-
one, POS-preserving alignments. 
Once the translation equivalents were 
extracted, then, for any translation equivalence 
pair <WL1 WL2> and two aligned wordnets, the 
steps 2 and 3 above should ideally identify one 
ILI concept lexicalized by WL1 in language L1 
and by WL2 in language L2. However, due to 
various reasons, the wordnets alignment might 
reveal not the same ILI concept, but two concepts 
which are semantically close enough to license 
the translation equivalence of WL1 and WL2. This 
can be easily generalized to more than two 
languages. Our measure of interlingual concepts 
semantic similarity is based on PWN2.0 
structure. We compute semantic-similarity score 
by formula: 
ss(ILI1, ILI2) = 1/1+k 
where k is the number of links from ILI1 to ILI2 
or from both ILI1 and ILI2 to the nearest common 
ancestor. The semantic similarity score is 1 when 
the two concepts are identical, 0.33 for two sister 
concepts, and 0.5 for mother/daughter, 
whole/part, or concepts related by a single link. 
Based on empirical studies, we decided to set the 
significance threshold of the semantic similarity 
score to 0.33.  Other approaches to similarity 
measures are described in (Budanitsky and Hirst 
2001). 
In order to describe the algorithm for WSD 
based on aligned wordnets let us assume we have 
a parallel corpus containing texts in k+1 
languages (T, L1, L2?Lk), where T is the target 
language and L1, L2?Lk are the source languages 
and monolingual wordnets for each of the k+1 
languages interlinked via an ILI-like structure. 
For each source language and for all occurrences 
of a specific word in the target language T, we 
build a matrix of translation equivalents as shown 
in Table 1 (eqij represents the translation 
equivalent in the ith source language of the jth 
occurrence of the word in the target language).  
 Occ #1 Occ #2 ? Occ #n 
L1 eq11 eq12 ? eq1n 
L2 eq21 eq22 ? eq2n 
? ? ? ? ? 
Lk eqk1 eqk2 ? eqkn 
Table 1. The translation equivalents matrix 
(EQ matrix) 
If the target word is not translated in language Li, 
eqij is represented by the null string.  
The second step transforms the matrix in 
Table 1 to a VSA (Validation and Sense 
Assignment) matrix with the same dimensions 
(Table 2).  
 Occ #1 Occ #2 ? Occ #n 
L1 VSA11  VSA12 ? VSA 1n  
L2 VSA21 VSA22  VSA22 
? ? ? ? ? 
Lk VSAk1 VSAk2 ? VSAkn 
Table 2. The VSA matrix 
Here,  VSAij = LENILI(WEN) ? LiILI(WjLi),, where 
LENILI(WEN) represent the ILI codes of all synsets 
in which the target word WEN occurs, and 
LiILI(WjLi) is the list of ILI-codes for all synsets in 
which the translation equivalent for the jth 
occurrence of WEN occurs. 
If no translation equivalent is found in 
language Li for the jth occurrence of WEN, 
VSA(i,j) is undefined; otherwise, it is a set 
containing 0, 1, or more ILI codes. For undefined 
VSAs, the algorithm cannot determine the sense 
number for the corresponding occurrence of the 
target word. However, it is very unlikely that an 
entire column in Table 2 is undefined, i.e., that 
there is no translation equivalent for an 
occurrence of the target word in any of the source 
languages.  
When VSA(i,j) contains a single ILI code, the 
target occurrence and its translation equivalent 
are assigned the same sense. 
When VSA(i,j) is empty?i.e., when none of 
the senses of the target word corresponds to an 
ILI code to which a sense of the translation 
equivalent was linked--the algorithm selects the 
pair in LENILI(WEN) ? LiILI(WjLi) with the highest 
similarity score. If no pair in LENILI(WEN) ? 
LiILI(WjLi) has a  the semantic similarity score 
above the significance threshold, neither the 
occurrence of the target word nor its translation 
equivalent can be semantically disambiguated; 
but once again, it is extremely rare that there is 
no translation equivalent for an occurrence of the 
target word in any of the source languages. 
In case of ties, the pair corresponding to the 
most frequent sense of the target word in the 
current bitext pair is selected. If this heuristic in 
turn fails, the choice is made in favor of the pair 
corresponding to the lowest PWN2.0 sense 
number for the target word, since PWN senses 
are ordered by frequency.  
When the VSA cell contains two or more ILI-
codes, we have the case of cross-lingual 
ambiguity, i.e., two or more senses are common 
to the target word and the corresponding 
translation equivalent in the ith language.  
2.1 Agglomerative clustering   
As noted before, when VSA(i,j) is undefined, we 
may get the information from a VSA 
corresponding to the same occurrence of the 
target word in a different language. However, this 
demands that aligned wordnets are available for 
all languages in the parallel corpus, and that the 
quality of the inter-lingual linking is high for all 
languages concerned. In cases where we cannot 
fulfill these requirements, we rely on a ?back-
off? method involving sense clustering based on 
translation equivalents, as discussed in (Ide, et 
al., 2002). We apply the clustering method after 
the wordnet-based method has been applied, and 
therefore each cluster containing an 
undisambiguated occurrence of the target word 
will also typically contain several occurrences 
that have already been assigned a sense. We can 
therefore assign the most frequent sense 
assignment in the cluster to previously unlabeled 
occurrences within the same cluster. The 
combined approach has two main advantages: 
? it eliminates reliance only on high-quality, k-1 
aligned wordnets. Indeed, having k+1 languages 
in our corpus, we need only apply the WSD 
method to the aligned wordnets for the target 
language (English in our case) and one source 
language, say Li, and alignment lexicons from the 
target language to every other language in the 
corpus. The WSD procedure in the bilingual 
setting would ensure the sense assignment for 
most of the non-null translation equivalence pairs 
and the clustering algorithm would classify the 
target words which were not translated (or for 
which the word alignment algorithm didn?t find a 
correct translation) in Li based on their 
equivalents in the other k-1 source languages. 
? it can reinforce or modify the sense 
assignment decided by the tie heuristics in case 
of cross-lingual ambiguity. 
To perform the clustering, we derive a set of 
m binary vectors VECT(Lp, TWi) for each source 
language Lp and each target word i occurring m 
times in the corpus. To compute the vectors, we 
first construct a Dictionary Entry List 
DEL(Lp,TWi)={Wj | <TWi, Wj> is a translation 
equivalence pair}, comprising the ordered list of 
all the translation equivalents in the source 
language pL of the target word TW
i. In this part 
of the experiment, the translation equivalents are 
automatically extracted from the parallel corpus 
using a hypothesis testing algorithm described in 
(Tufi? 2002). VECT(Lp,TWik)  specifies which of 
the possible translations of TWi was actually 
used as an equivalent for the kth occurrence of 
TWi. All positions in VECT(Lp,TWik)  are set to 
0 except the bit at position h, which is 1 if the 
translation equivalent (Lp,TWik)=DELh(Lp,TWi). 
The vector for each target word occurrence is 
obtained by concatenating the VECT(Lp,TWik) 
for all k souce languages  and its length is 
?
=
k
1p
i
p  |)TW,DEL(L| . 
We use a Hierarchical Clustering Algorithm 
based on Stolcke?s Cluster2.9 to classify similar 
vectors into sense classes. Stolcke?s algorithm 
generates a clustering tree, the root of which 
corresponds to a baseline clustering (all the 
occurrences are clustered in one sense class) and 
the leaves are single element classes, 
corresponding to each occurrence vector of the 
target word. An interior cut in the clustering tree 
will produce a specific number (say X) of sub-
trees, the roots of which stand for X classes each 
containing the vectors of their leaves. We call an 
interior cut a pertinent cut if X is equal to the 
number of senses TWi has been used throughout 
the entire corpus. One should note that in a 
clustering tree many pertinent cuts could be 
possible. The pertinent cut which corresponds to 
the correct sense clustering of the m occurrences 
of TWi is called a perfect cut.  However, if TWi 
has Y possible senses, it is possible that only a 
subset of the Y senses will be used in an arbitrary 
text. Therefore, a perfect cut in a clustering tree 
cannot be deterministically computed. Instead of 
deriving the clustering tree and guessing at a 
perfect cut, we stop the clustering algorithm 
when Z clusters have been created, where Z is the 
number of senses in which the occurrences of 
TWi have been used in the text in question. 
However, the value of Z is specific to each word 
and depends on the type and size of the text; it 
cannot therefore be computed a priori. In our 
previous work (Tufi? and Ion, 2003), to 
approximate Z we imposed an exit condition for 
the clustering algorithm based on distance 
heuristics. In particular, the algorithm stops when 
the minimal distance between the existing classes 
increases beyond a given threshold level:  
?>+
?+
)1(
)()1(
kdist
kdistkdist                                   (1) 
where dist(k) is the minimal distance between 
two clusters at the k-th  iteration  step and ? is  an 
empirical numerical threshold. Experimentation 
revealed that reasonable results are achieved with 
a value for ? is 0.12. However, although the 
threshold is a parameter for the clustering 
algorithm irrespective of the target words, the 
number of classes the clustering algorithm 
generates (Z) is still dependent on the particular 
target word and the corpus in which it appears. 
By using sense information produced by the 
ILI-similarity approach, the algorithm and its exit 
condition have been modified as described 
below:  
- the sense label of a cluster is given by the 
majority sense of its members as assigned by the 
wordnet-based sense labelling; a cluster 
containing only non-disambiguated occurrences 
has an wild-card sense label;    
- two joinable clusters (that is the clusters with 
the minimal distance and the exit condition (1) 
not satisfied) are joint only when their sense 
labels is the same or one of them has an wild-
card sense label; in this case the wild-card sense 
label is turned into the sense label of the sense-
assigned cluster. Otherwise the next distant 
clusters are tried. 
- the algorithm stops when no clusters can be 
further joined. 
3 The Experiment 
The parallel corpus we used for our experiments 
is based on Orwell?s novel ?Ninety Eighty Four? 
(1984) which has been initially developed by the 
Multext-East consortium. Besides Orwell?s 
original text, the corpus contained professional 
translations in six languages (Bulgarian, Czech, 
Estonian, Hungarian, Romanian and Slovene). 
The Multext-East corpus (and other language 
resources) is maintained by Toma? Erjavec and a 
new release of it may be found at 
http://nl.ijs.si/ME/V3. Later, the parallel corpus 
has been extended with many other new language 
translations. The BalkaNet consortium added 
three new translations to the ?1984? corpus: 
Greek, Serbian and Turkish. Each language text 
is tokenized, tagged and sentence aligned to the 
English original. We extracted from the entire 
parallel corpus only the languages of concern in 
the BalkaNet project (English, Bulgarian, Czech, 
Greek, Romaniann, Serbian and Turkish) and 
further retained only the 1-1 sentence alignments 
between English and all the other languages. This 
way, we built a unique alignment for all the 
languages and, by exploiting the transitivity of 
sentence alignment, we are able to make 
experiments with any combination of languages. 
The BalkaNet version of the ?1984? corpus is 
encoded as a sequence of translation units (TU), 
each containing one sentences per language, so 
that they are reciprocal translations.  In order to 
evaluate both the performance of the WSDtool 
and to assess the accuracy of the interlingual 
linking of the BalkaNet wordnets we selected a 
bag of English target words (nouns and verbs) 
occurring in the corpus. The selection considered 
only polysemous words (at least two senses per 
part of speech) implemented (and ILI linked) in 
all BalkaNet wordnets. There resulted 211 words 
with 1644 occurrences in the English part of the 
parallel corpus. 
Three experts independently sense-tagged all 
the occurrences of the target words and the 
disagreements were negotiated until consensus 
was obtained. The commonly agreed annotation 
represented the Gold Standard (GS) against 
which the WSD algorithm was evaluated. 
Additionally, a number of 13 students, enrolled in 
a Computational Linguistics Master program, 
were asked to manually sense-tag overlapping 
subsets of the same word occurrences.  The 
overlapping ensured that each target word 
occurrence was seen by at least three students. 
Based on the students? annotations, using a 
majority voting, we computed another set of 
comparison data which below is referred to as 
SMAJ (Students MAJority). 
Finally, the same targeted words were 
automatically disambiguated by the WSDtool 
algorithm (ALG) which was run both with and 
without the back-off clustering algorithm.  For 
the basic wordnet-based WSD we used the 
Princeton Wordnet, the Romanian wordnet and 
the English-Romanian translation equivalence 
dictionary. For the back-off clustering we 
extracted a four1 language translation dictionary 
(EN-RO-CZ-BG) based on which we computed 
the initial clustering vectors for all occurrences of 
the target words. 
                                                     
1 Although we used only RO, CZ and BG 
translation texts, nothing prevents us from using any 
other translations, irrespective of whether their 
languages belong or not to the BalkaNet consortium.  
Out of the 211 set of targeted words, with 
1644 occurrences the system could not make a 
decision for 38 (18 %) words with 63 
occurrences (3.83%). Most of these words were 
happax legomena (21) for which neither the 
wordnet-based step not the clustering back-off 
could do anything. Others, were not translated by 
the same part of speech, were wrongly translated 
by the human translator or not translated at all 
(28). Finally, four occurrences remained 
untagged due to the incompleteness of the 
Romanian synsets linked to the relevant concepts 
(that is the four translation equivalents had their 
relevant sense missing from the Romanian 
wordnet). Applying the simple heuristics (SH) 
that says that any unlabelled target occurrence 
receives its most frequent sense, 42 out of 63 of 
them got a correct sense-tag. The table below 
summarizes the results.   
WSD annotation Precision Recall F 
AWN  74.88% 72.01% 73.41%
AWN + C 75.26% 72.38% 73.79%
AWN + C + SH 74.93% 74.93% 74.93%
SMAJ 72.99% 72.99% 72.99%
Table 4. WSD precision recall and F-measure for 
the algorithm based on aligned wordnets (AWN), 
for AWN with clustering (AWN+C) and for 
AWN+C and the simple heuristics 
(AWN+C+SH) and for the students? majority 
voting (SMAJ) 
It is interesting to note that in this experiment 
the students? majority annotation is less accurate 
than the one achieved by the automatic WSD 
annotation in all three variants. This is a very 
encouraging result since it shows that the tedious 
hand-made WSD in building word-sense 
disambiguated corpora for supervised training 
can be avoided. 
4 Conclusion 
Considering the fine granularity of the PWN2.0 
sense inventory, our disambiguation results using 
parallel resources are superior to the state of the 
art in monolingual WSD (with the same sense 
inventory). This is not surprising since the 
parallel texts contain implicit knowledge about 
the sense of an ambiguous word, which has been 
provided by human translators.  The drawback of 
our approach is that it relies on the existence of 
parallel data, which in the vast majority of cases 
is not available. On the other hand, supervised 
monolingual WSD relies on the existence of large 
samples of training data, and our method can be 
applied to produce such data to bootstrap 
monolingual applications. Given that parallel 
resources are becoming increasingly available, in 
particular on the World Wide Web (see for 
instance http://www.balkantimes.com where the 
same news is published in 10 languages), and 
aligned wordnets are being produced for more 
and more languages, it should be possible to 
apply our and similar methods to large amounts 
of parallel data in the not-too-distant future.  
One of the greatest advantages of our 
approach is that it can be used to automatically 
sense-tag corpora in several languages at once. 
That is, if we have a parallel corpus in multiple 
languages (such as the Orwell corpus), 
disambiguation performed on any one of them 
propagates to the rest via the ILI linkage. Also, 
given that the vast majority of words in any given 
language are monosemous (e.g., approximately 
82% of the words in PWN have only one sense), 
the use of parallel corpora in multiple languages 
for WSD offers the potential to significantly 
improve results and provide substantial sense-
annotated corpora for training in a range of 
languages.  
Acknowledgements 
The work reported here was carried within the 
European project BalkaNet, no. IST-2000 29388 
and support from the Romanian Ministry of 
Education and Research. 
References  
Alex. Budanitsky and Graeme Hirst 2001. 
Semantic distance in WordNet: An 
experimental, application-oriented evaluation 
of five measures. Proceedings of the Workshop 
on WordNet and Other Lexical Resources, 
Second meeting of the NAACL, Pittsburgh, 
June. 
William Gale, Ken Church and Dan Yarowsky 
1992. Estimating upper and lower bounds on 
the performance of wordsense disambiguation 
programs. Proceedings of the 30th Annual 
Meeting of ACL, 249-256. 
Adam Kilgarriff 1997. I don't believe in word 
senses. In Computers and the Humanities, 31 
(2): 91-113. 
Nancy Ide and Jean V?ronis 1998. Word Sense 
Disambiguation: The State of the Art. 
Computational Linguistics,24(1): 1-40. 
Nancy Ide, N. 1999. Parallel translations as sense 
discriminators. SIGLEX99: Standardizing 
Lexical Resources, ACL99 Workshop, College 
Park, Maryland, 52-61. 
Nancy Ide, Toma? Erjavec and Dan Tufi? 2002. 
Sense Discrimination with Parallel Corpora. In 
Proceedings of the SIGLEX Workshop on Word 
Sense Disambiguation: Recent Successes and 
Future Directions, 56-60, Philadelphia. 
Andreas Stolcke 1996. ftp.icsi.berkeley.edu/ 
pub/ai/stolcke/software/cluster-2.9.tar.Z/ 
Dan Tufi?. 2002. A cheap and fast way to build 
useful translation lexicons. In Proceedings of 
the 19th International Conference on 
Computational Linguistics, 1030-1036, Taipei.  
Dan Tufi? and Radu Ion. 2003. Word sense 
clustering based on translation equivalence in 
parallel texts; a case study in Romanian. In 
Proceedings of the International Conference on 
Speech and Dialog ? SPED, 13-26, Bucharest.  
Dan Tufi?,  Ana-Maria Barbu and Radu Ion 
2003. A word-alignment system with limited 
language resources. In Proceedings of the 
NAACL 2003 Workshop on Building and Using 
Parallel Texts; Romanian-English Shared 
Task, 36-39, Edmonton.  
Dan Tufi?, Radu Ion and Nancy Ide 2004. Word 
sense disambiguation as a wordnets validation 
method in Balkanet. In Proceedings of the 
LREC?2004, 741-744, Lisbon 
Dan Tufi?, Dan Cristea and Sofia Stamou 2004a. 
BalkaNet: Aims, Methods, Results and 
Perspectives. A General Overview. In D. Tufi? 
(ed): Special Issue on BalkaNet. Romanian 
Journal on Science and Technology of 
Information, 7(3-4):9-44 
Improved Lexical Alignment by Combining Multiple Reified
Alignments
Dan Tufi?
 Institute for Artificial 
Intelligence
 13, ?13 Septembrie?, 
 050711, Bucharest 5, 
Romania 
tufis@racai.ro
Radu Ion 
Institute for Artificial
Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania 
radu@racai.ro
Alexandru Ceau?u
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5, 
Romania 
alceausu@racai.ro
Dan ?tef?nescu
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5,
 Romania 
danstef@racai.ro
Abstract
We describe a word alignment platform 
which ensures text pre-processing (to-
kenization, POS-tagging, lemmatization, 
chunking, sentence alignment) as re-
quired by an accurate word alignment. 
The platform combines two different 
methods, producing distinct alignments. 
The basic word aligners are described in 
some details and are individually evalu-
ated. The union of the individual align-
ments is subject to a filtering post-
processing phase. Two different filtering 
methods are also presented. The evalua-
tion shows that the combined word 
alignment contains 10.75% less errors 
than the best individual aligner. 
1 Introduction 
It is almost a truism that more decision makers, 
working together, are likely to find a better solu-
tion than when working alone. Dieterich (1998) 
discusses conditions under which different deci-
sions (in his case classifications) may be com-
bined for obtaining a better result. Essentially, a 
successful automatic combination method would 
require comparable performance for the decision 
makers and, additionally, that they should not 
make similar errors. This idea has been exploited 
by various NLP researchers in language model-
ling, statistical POS tagging, parsing, etc.
We developed two quite different word align-
ers, driven by two distinct objectives: the first 
one was motivated by a project aiming at the de-
velopment of an interlingually aligned set of 
wordnets while the other one was developed 
within an SMT ongoing project. The first one 
was used for validating, against a multilingual 
corpus, the interlingual synset equivalences and 
also for WSD experiments. Although, initially, it 
was concerned only with open class words re-
corded in a wordnet, turning it into an ?all 
words? aligner was not a difficult task. This 
word aligner, called YAWA is described in sec-
tion 3.
A quite different approach from the one used 
by YAWA, is implemented in our second word 
aligner, called MEBA, described in section 4. It 
is a multiple parameter and multiple step algo-
rithm using relevance thresholds specific to each 
parameter, but different from each step to the 
other. The implementation of MEBA was 
strongly influenced by the notorious five IBM 
models described in (Brown et al 1993). We 
used GIZA++ (Och and Ney 2000; Och and Ney, 
2003) to estimate different parameters of the 
MEBA aligner. 
The alignments produced by MEBA were 
compared to the ones produced by YAWA and 
evaluated against the Gold Standard (GS)1 anno-
tations used in the Word Alignment Shared 
Tasks (Romanian-English track) organized at 
HLT-NAACL2003 (Mihalcea and Pedersen 
2003).
Given that the two aligners are based on quite 
different models and that their F-measures are 
comparable, it was quite a natural idea to com-
bine their results and hope for an improved align-
ment. Moreover, by analyzing the alignment er-
rors done by each word aligner, we found that 
the number of common mistakes was small, so 
1 We noticed in the GS Alignment various errors (both sen-
tence and word alignment errors) that were corrected. The 
tokenization of the bitexts used in the GS Alignment was 
also modified, with the appropriate modification of the ref-
erence alignment. These reference data are available at 
http://www.racai.ro/res/WA-GS
153
the premises for a successful combination were 
very good (Dieterich, 1998). The Combined 
Word Aligner, COWAL-described in section 5, 
is a wrapper of the two aligners (YAWA and 
MEBA) merging the individual alignments and 
filtering the result. At the Shared Task on Word 
Alignment organized by the ACL2005 Work-
shop on ?Building and Using Parallel Corpora: 
Data-driven Machine Translation and Beyond? 
(Martin, et al 2005), we participated (on the 
Romanian-English track) with the two aligners 
and the combined one (COWAL). Out of 37 
competing systems, COWAL was rated the first, 
MEBA the 20th and TREQ-AL (Tufi? et al 
2003), the former version of YAWA, was rated 
the 21st. The usefulness of the aligner combina-
tion was convincingly demonstrated. 
Meanwhile, both the individual aligners and 
their combination were significantly improved. 
COWAL is now embedded into a larger platform 
that incorporates several tools for bitexts pre-
processing (briefly reviewed in section 2), a 
graphical interface that allows for comparing and 
editing different alignments, as well as a word 
sense disambiguation module.  
2 The bitext processing  
The two base aligners and their combination use 
the same format for the input data and provide 
the alignments in the same format. The input 
format is obtained from two raw texts that repre-
sent reciprocal translations. If not already sen-
tence aligned, the two texts are aligned by our 
sentence aligner that builds on Moore?s aligner 
(Moore, 2002) but which unlike it, is able to re-
cover the non-one-to-one sentence alignments. 
The texts in each language are then tokenized, 
tagged and lemmatized by the TTL module (Ion, 
2006). More often than not, the translation 
equivalents have the same part-of speech, but 
relying on such a restriction would seriously af-
fect the alignment recall. However, when the 
translation equivalents have different parts of 
speech, this difference is not arbitrary.  During 
the training phase, we estimated POS affinities: 
{p(POSmRO|POSnEN)} and p(POSnEN|POSmRO)}
and used them to filter out improbable translation 
equivalents candidates.
The next pre-processing step is represented by 
sentence chunking in both languages. The 
chunks are recognized by a set of regular expres-
sions defined over the tagsets and they corre-
spond to (non-recursive) noun phrases, adjectival 
phrases, prepositional phrases and verb com-
plexes (analytical realization of tense, aspect 
mood and diathesis and phrasal verbs). Finally, 
the bitext is assembled as an XML document 
(Tufi? and Ion, 2005), which is the standard input 
for most of our tools, including COWAL align-
ment platform. 
3 YAWA 
YAWA is a three stage lexical aligner that uses 
bilingual translation lexicons and phrase bounda-
ries detection to align words of a given bitext. 
The translation lexicons are generated by a dif-
ferent module, TREQ (Tufi?, 2002), which gen-
erates translation equivalence hypotheses for the 
pairs of words (one for each language in the par-
allel corpus) which have been observed occur-
ring in aligned sentences more than expected by 
chance. The hypotheses are filtered by a log-
likelihood score threshold. Several heuristics 
(string similarity-cognates, POS affinities and 
alignments locality2) are used in a competitive 
linking manner (Melamed, 2001) to extract the 
most likely translation equivalents. 
YAWA generates a bitext alignment by in-
crementally adding new links to those created at 
the end of the previous stage. The existing links 
act as contextual restrictors for the new added 
links. From one phase to the other new links are 
added without deleting anything. This monotonic 
process requires a very high precision (at the 
price of a modest recall) for the first step. The 
next two steps are responsible for significantly 
improving the recall and ensuring an increased 
F-measure.  
In the rest of this section we present the three 
stages of YAWA and evaluate the contribution 
of each of them to the final result. 
3.1 Phase 1: Content Words Alignment 
YAWA begins by taking into account only very 
probable links that represent the skeleton align-
ment used by the second phase. This alignment is 
done using outside resources such as translation 
lexicons and involves only the alignment of con-
tent words (nouns, verbs, adjective and adverbs). 
The translation equivalence pairs are ranked 
according to an association score (i.e. log-
likelihood, DICE, point-wise mutual informa-
2 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
requires that all alignment links starting from a chunk in the 
one language end in a chunk in the other language.
154
tion, etc.). We found that the best filtering of the 
translation equivalents was the one based on the
log-likelihood (LL) score with a threshold of 9.
Each translation unit (pair of aligned sen-
tences) of the target bitext is scanned for estab-
lishing the most likely links based on a competi-
tive linking strategy that takes into account the
LL association scores given by the TREQ trans-
lation lexicon. If a candidate pair of words is not 
found in the translation lexicon, we compute
their orthographic similarity (cognate score 
(Tufi?, 2002)). If this score is above a predeter-
mined threshold (for Romanian-English task we 
used the empirically found value of 0.43), the 
two words are treated as if they existed in the
translation lexicon with a high association score 
(in practice we have multiplied the cognate score
by 100 to yield association scores in the range 0
.. 100). The Figure 1 exemplifies the links cre-
ated between two tokens of a parallel sentence by
the end of the first phase.
Figure 1: Alignment after the first step 
3.2 Phase 2: Chunks Alignment 
The second phase requires that each part of the
bitext is chunked. In our Romanian-English ex-
periments, this requirement was fulfilled by us-
ing a set of regular expressions defined over the
tagsets used in the target bitext. These simple
chunkers recognize noun phrases, prepositional
phrases, verbal and adjectival or adverbial group-
ings of both languages.
In this second phase YAWA produces first
chunk-to-chunk matching and then aligns the 
words within the aligned chunks. Chunk align-
ment is done on the basis of the skeleton align-
ment produced in the first phase. The algorithm
is simple: align two chunks c(i) in source lan-
guage and c(j) in the target language if c(i) and 
c(j) have the same type (noun phrase, preposi-
tional phrase, verb phrase, adjectival/adverbial 
phrase) and if there exist a link ?w(s), w(t)? so 
that w(s) ? c(i) then w(t) ? c(j).
After alignment of the chunks, a language pair 
dependent module takes over to align the un-
aligned words belonging to the chunks. Our 
module for the Romanian-English pair of lan-
guages contains some very simple empirical
rules such as: if b is aligned to c and b is pre-
ceded by a, link a to c, unless there exist d in the 
same chunk with c and the POS category of d has 
a significant affinity with the category of a. The 
simplicity of these rules derives from the shallow
structures of the chunks. In the above example b
and c are content words while a is very likely a 
determiner or a modifier for b. The result of the 
second alignment phase, considering the same
sentence in Figure 1, is shown in Figure 2. The
new links are represented by the double lines. 
 Figure 2: Alignment after the second step 
3.3 Phase 3: Dealing with sequences of un-
aligned words
This phase identifies contiguous sequences of
words (blocks) in each part of the bitext which
remain unaligned and attempts to heuristically
match them. The main criteria used to this end
are the POS-affinities of the remaining unaligned 
words and their relative positions. Let us illus-
trate, using the same example and the result 
shown in Figure 2, how new links are added in
this last phase of the alignment. At the end of 
phase 2 the blocks of consecutive words that re-
main to be aligned are: English {en0 = (you), en1
= (that), en2 = (is, not), en3 = (and), en4 = (.)} and 
155
Romanian {ro0 = (), ro1 = (c?), ro2 = (nu, e), ro3 = 
(?i), ro4 = (.)}. The mapping of source and target
unaligned blocks depends on two conditions: that 
surrounding chunks are already aligned and that
pairs in candidate unaligned blocks have signifi-
cant POS-affinity. For instance in the figure
above, blocks en1 = (that) and ro1 = (c?) satisfy
the above conditions because they appear among
already aligned chunks (<?ll notice> ? <ve?i
observa> and <D?ncu ?s generosity> ? <gene- 
rozitatea lui D?ncu>) and they contain words 
with the same POS.
After block alignment3, given a pair of aligned
blocks, the algorithm links words with the same
POS and then the phase 2 is called again with 
these new links as the skeleton alignment. In 
Figure 3 is shown the result of phase 3 alignment
of the sentence we used as an example through-
out this section. The new links are shown (as
before) by double lines. 
Figure 3: Alignment after the third step 
The third phase is responsible for significant
improvement of the alignment recall, but it also 
generates several wrong links. The detection of
some of them is quite straightforward, and we
added an additional correction phase 3.f. By ana-
lysing the bilingual training data we noticed the trans-
lators? tendency to preserve the order of the 
phrasal groups. We used this finding (which 
might not be valid for any language pair) as a 
removal heuristics for the links that cross two or
more aligned phrase groups.  One should notice 
that the first word in the English side of the ex-
ample in Figure 3 (?you?) remained unaligned
(interpreted as not translated in the Romanian
side). According to the Gold Standard used for 
3 Only 1:1 links are generated between blocks. 
evaluation in the ACL2005 shared task, this in-
terpretation was correct, and therefore, for the
example in Figure 3, the F-measure for the 
YAWA alignment was 100%.
However, Romanian is a pro-drop language 
and although the translation of the English pro-
noun is not lexicalized in Romanian, one could 
argue that the auxiliary ?ve?i? should be aligned 
also to the pronoun ?you? as it incorporates the
grammatical information carried by the pronoun. 
Actually, MEBA (as exemplified in Figure 4)
produced this multiple token alignment (and was 
penalized for it!). 
3.4 Performance analysis
The table that follows presents the results of the
YAWA aligner at the end of each alignment
phase. Although the Precision decreases from
one phase to the next one, the Recall gains are 
significantly higher, so the F-measure is mono-
tonically increasing.
Precision Recall F-Measure
Phase 1 94.08% 34.99% 51.00%
Phase 1+2 89.90% 53.90% 67.40%
Phase 1+2+3 88.82% 73.44% 80.40%
Phase 1+2+3+3.f 88.80% 74.83% 81.22%
Table 1: YAWA evaluation 
4 MEBA 
MEBA uses an iterative algorithm that takes ad-
vantage of all pre-processing phases mentioned
in section 2. Similar to YAWA aligner, MEBA 
generates the links step by step, beginning with 
the most probable (anchor links). The links to be 
added at any later step are supported or restricted 
by the links created in the previous iterations. 
The aligner has different weights and different
significance thresholds on each feature and itera-
tion. Each of the iterations can be configured to
align different categories of tokens (named enti-
ties, dates and numbers, content words, func-
tional words, punctuation) in decreasing order of 
statistical evidence. 
The first iteration builds anchor links with a 
high level of certainty (that is cognates, numbers,
dates, pairs with high translation probability).
The next iteration tries to align content words
(open class categories) in the immediate vicinity
of the anchor links. In all steps, the candidates
are considered if and only if they meet the mini-
mal threshold restrictions.
A link between two tokens is characterized by
a set of features (with values in the [0,1] inter-
val). We differentiate between context independ-
156
ent features that refer only to the tokens of the
current link (translation equivalency, part-of-
speech affinity, cognates, etc.) and context de-
pendent features that refer to the properties of the 
current link with respect to the rest of links in a 
bi-text (locality, number of traversed links, to-
kens indexes displacement, collocation). Also, 
we distinguish between bi-directional features
(translation equivalence, part-of-speech affinity)
and non-directional features (cognates, locality,
number of traversed links, collocation, indexes 
displacement).
Precision Recall F-measure
?Anchor? links 98.50% 26.82% 42.16%
Words around 
?anchors? 96.78% 42.41% 58.97%
Funct. words 
and punctuation 94.74% 59.48% 73.08%
Probable links 92.05% 71.00% 80.17%
Table 2: MEBA evaluation 
The score of a candidate link (LS) between a 
source token i and a target token j is computed
by a linear function of several features scores
(Tiedemann, 2003).
?
 
 
n
i
ii ScoreFeatjiLS
1
*),( O ; 1
1
 ?
 
n
i
iO
Each feature has defined a specific signifi-
cance threshold, and if the feature?s value is be-
low this threshold, the contribution to the LS of 
the current link of the feature in case is nil. 
The thresholds of the features and lambdas are 
different from one iteration to the others and they
are set by the user during the training and system
fine-tuning phases. There is also a general 
threshold for the link scores and only the links 
that have the LS above this threshold are retained
in the bitext alignment. Given that this condition 
is not imposing unique source or target indexes,
the resulting alignment is inherently many-to-
many.
In the following subsections we briefly discuss
the main features we use in characterising a link.
4.1 Translation equivalence
This feature may be used for two types of pre-
processed data: lemmatized or non-lemmatized
input. Depending on the input format, MEBA
invokes GIZA++ to build translation probability
lists for either lemmas or the occurrence forms of 
the bitext4. Irrespective of the lemmatisation op-
tion, the considered token for the translation 
model build by GIZA++ is the respective lexical 
item (lemma or wordform) trailed by its POS tag 
(eg. plane_N, plane_V, plane_A). In this way we 
avoid data sparseness and filter noisy data. For 
instance, in case of highly inflectional languages 
(as Romanian is) the use of lemmas significantly
reduces the data sparseness. For languages with
weak inflectional character (as English is) the 
POS trailing contributes especially to the filter-
ing the search space. A further way of removing
the noise created by GIZA++ is to filter out all 
the translation pairs below a LL-threshold. We 
made various experiments and, based on the es-
timated ratio between the number of false nega-
tives and false positive, empirically set the value
of this threshold to 6. All the probability losses 
by this filtering were redistributed proportionally
to their initial probabilities to the surviving trans-
lation equivalence candidates. 
4.2 Translation equivalence entropy score 
The translation equivalence relation is a se-
mantic one and it directly addresses the notion of 
word sense. One of the Zipffian laws prescribes a 
skewed distribution of the senses of a word oc-
curring several times in a coherent text. We used
this conjecture as a highly informative informa-
tion source for the validity of a candidate link.
The translation equivalence entropy score is a 
favouring parameter for the words that have few 
high probability translations. Since this feature is
definitely sensitive to the order of the lexical 
items, we compute an average value for the link: 
DES(A)+EES(B). Currently we use D=E=0.5, but 
it might be interesting to see, depending on dif-
ferent language pairs, how the performance of 
the aligner would be affected by a different set-
tings of these parameters.
N
TRWpTRWp
N
i
ii
WES log
),(log*),(
11)(
?
 

 
4.3 Part-of-speech affinity
In faithful translations the translated words tend
to be translated by words of the same part-of-
speech. When this is not the case, the different 
POSes, are not arbitrary. The part of speech af-
finity, P(cat(A)|cat(B), can be easily computed
from a gold standard alignment. Obviously, this
4 Actually, this is a user-set parameter of the MEBA aligner;
if the input bitext contain lemmatization information, both 
translation probability tables may be requested. 
157
is a directional feature, so an averaging operation 
is necessary in order to ascribe this feature to a 
link: PA=DP(cat(A)|cat(B)) + EP(cat(B)|cat(A)).
Again, we used D=E=0.5 but different values of 
these weights might be worthwhile investigating. 
4.4 Cognates 
The similarity measure, COGN(TS, TT), is im-
plemented as a Levenstein metric. Using the
COGN test as a filtering device is a heuristic 
based on the cognate conjecture, which says that 
when the two tokens of a translation pair are 
orthographically similar, they are very likely to
have similar meanings (i.e. they are cognates). 
The threshold for the COGN(TS, TT) test was 
empirically set to 0.42. This value depends on 
the pair of languages in the bitext. The actual 
implementation of the COGN test includes a lan-
guage-dependent normalisation step, which strips 
some suffixes, discards the diacritics, reduces 
some consonant doubling, etc. This normalisa-
tion step was hand written, but, based on avail-
able lists of cognates, it could be automatically
induced.
4.5 Obliqueness 
Each token in both sides of a bi-text is character-
ized by a position index, computed as the ratio 
between the relative position in the sentence and 
the length of the sentence. The absolute value of 
the difference between tokens? position indexes,
subtracted from 15, gives the link?s ?oblique-
ness?.
)()(
1),(
TS
ji Sentlength
j
Sentlength
iTWSWOBL  
This feature is ?context free? as opposed to the 
locality feature described below.
4.6 Locality 
Locality is a feature that estimates the degree to 
which the links are sticking together. 
MEBA has three features to account for local-
ity: (i) weak locality, (ii) chunk-based locality
and (iii) dependency-based locality.
The value of the weak locality feature is de-
rived from the already existing alignments in a 
window of N tokens centred on the focused to-
ken. The window size is variable, proportional to 
the sentence length. If in the window there exist
k linked tokens and the relative positions of the 
5 This is to ensure that values close to 1 are ?good? ones and 
those near 0 are ?bad?. This definition takes into account the
relatively similar word order in English and Romanian.
tokens in these links are <i1 j1>, ?<ik jk> then 
the locality feature of the new link <ik+1, jk+1> is 
defined by the equation below: 
)
||
||1,1min(
1 1
1?
 


 
k
m mk
mk
jj
ii
k
LOC
If the new link starts from or ends in a token 
already linked, the index difference that would
be null in the formula above is set to 1. This way,
such candidate links would be given support by
the LOC feature (and avoid overflow error). In 
the case of chunk-based locality the window 
span is given by the indexes of the first and last 
tokens of the chunk. 
Dependency-based locality uses the set of the 
dependency links of the tokens in a candidate
link for the computation of the feature value. In
this case, the LOC feature of a candidate link
<ik+1, jk+1> is set to 1 or 0 according to the fol-
lowing rule: 
if between ik+1 and iD there is a (source lan-
guage) dependency and if between jk+1 and jE
there is also a (target language) dependency then 
LOC is 1 if iD and jE are aligned, and 0 otherwise. 
Please note that in case jk+1{ jE a trivial depend-
ency (identity) is considered and the LOC attrib-
ute of the link <ik+1, jk+1> is set to always to 1.
Figure 4: Chunk and dependency-based locality
4.7 Collocation 
Monolingual collocation is an important clue for 
word alignment. If a source collocation is trans-
lated by a multiword sequence, very often the
lexical cohesion of source words can also be
found in the corresponding translated words. In 
this case the aligner has strong evidence for 
158
many to many linking. When a source colloca-
tion is translated as a single word, this feature is
a strong indication for a many to 1 linking.
Bi-gram lists (only content words) were built 
from each monolingual part of the training cor-
pus, using the log-likelihood score (threshold of 
10) and minimal occurrence frequency (3) for
candidates filtering.
We used the bi-grams list to annotate the 
chains of lexical dependencies among the con-
tents words. Then, the value of the collocation 
feature is computed similar to the dependency-
based locality feature. The algorithm searches for
the links of the lexical dependencies around the 
candidate link. 
5 Combining the reified alignments 
From a given alignment one can compute a se-
ries of properties for each of its links (such as the 
parameters used by the MEBA aligner). A link
becomes this way a structured object that can be 
manipulated in various ways, independent of the
bitext (or even of the lexical tokens of the link)
from which it was extracted. We call this proce-
dure alignment reification. The properties of the 
links of two or more alignments are used for our 
methods of combining the alignments.
One simple, but very effective method of
alignment combination is a heuristic procedure, 
which merges the alignments produced by two or
more word aligners and filters out the links that 
are likely to be wrong. For the purpose of filter-
ing, a link is characterized by its type defined by
the pair of indexes (i,j) and the POS of the tokens
of the respective link. The likelihood of a link is 
proportional to the POS affinities of the tokens of
the link and inverse proportional to the bounded
relative positions (BRP) of the respective tokens:
  where avg is the average
displacement in a Gold Standard of the aligned 
tokens with the same POSes as the tokens of the 
current link. From the same gold standard we 
estimated a threshold below which a link is re-
moved from the final alignment.
||||1 avgjiBRP  
A more elaborated alignment combination
(with better results than the previous one) is 
modelled as a binary statistical classification 
problem (good / bad) and, as in the case of the 
previous method, the net result is the removal of 
the links which are likely to be wrong. We used
an ?off-the-shelf? solution for SVM training and 
classification - LIBSVM6 (Fan et al, 2005) with 
6 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
the default parameters (C-SVC classification and
radial basis kernel function). Both context inde-
pendent and context dependent features charac-
terizing the links were used for training. The
classifier was trained with both positive and 
negative examples of links. A set of links ex-
tracted from the Gold Standard alignment was
used as positive examples set. The same number
of negative examples was extracted from the
alignments produced by COWAL and MEBA 
where they differ from the Gold Standard.
It is interesting to notice that for the example
discussed in Figures 1-4, the first combiner
didn?t eliminate the link <you ve?i> producing 
the result shown in Figure 4. This is because the 
relative positions of the two words are the same
and the POS-affinity of the English personal
pronouns and the Romanian auxiliaries is signifi-
cant. On the other hand, the SVM-based com-
biner deleted this link, producing the result
shown in Figure 3. The explanation is that, ac-
cording to the Gold Standard we used, the links 
between English pronouns and Romanian auxil-
iaries or main verbs in pro-drop constructions
were systematically dismissed (although we 
claim that they shouldn?t and that the alignment
in Figure 4 is better than the one in Figure 3).
The evaluation (according to the Gold Standard)
of the SVM-based combination (COWAL),
compared with the individual aligners, is shown
in Table 3. 
Aligner Precision Recall F-measure
YAWA 88.80% 74.83% 81.22%
MEBA 92.05% 71.00% 80.17%
COWAL 86.99% 79.91% 83.30%
Table 3: Combined alignment
6 Conclusions and further work
Neither YAWA nor MEBA needs an a priori bi-
lingual dictionary, as this will be automatically
extracted by TREQ or GIZA++. We made
evaluation of the individual alignments in both
experimental settings: without a start-up bilin-
gual lexicon and with an initial mid-sized bilin-
gual lexicon. Surprisingly enough, we found that
while the performance of YAWA increases a
little bit (approx. 1% increase of the F-measure)
MEBA is doing better without an additional lexi-
con. Therefore, in the evaluation presented in the 
previous section MEBA uses only the training
data vocabulary.
YAWA is very sensitive to the quality of the
bilingual lexicons it uses. We used automatically
translation lexicons (with or without a seed lexi-
159
con), and the noise inherently present might have 
had a bad influence on YAWA?s precision. Re-
placing the TREQ-generated bilingual lexicons 
with validated (reference bilingual lexicons) 
would further improve the overall performance 
of this aligner.  Yet, this might be a harder to 
meet condition for some pairs of languages than 
using parallel corpora. 
MEBA is more versatile as it does not require 
a-priori bilingual lexicons but, on the other hand, 
it is very sensitive to the values of the parameters 
that control its behaviour. Currently they are set 
according to the developers? intuition and after 
the analysis of the results from several trials. 
Since this activity is pretty time consuming (hu-
man analysis plus re-training might take a couple 
of hours) we plan to extend MEBA with a super-
vised learning module, which would automati-
cally determine the ?optimal? parameters 
(thresholds and weights) values. 
It is worth noticing that with the current ver-
sions of our basic aligners, significantly im-
proved since the ACL shared word alignment 
task in June 2005, YAWA is now doing better 
than MEBA, and the COWAL F-measure in-
creased with 9.4%. However, as mentioned be-
fore, these performances were measured on a 
different tokenization of the evaluation texts and 
on the partially corrected gold standard align-
ment (see footnote 1).  
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert J. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter 
estimation. Computational Linguistics, 19(2): 263?
311.
Thomas G. Dietterich. 1998. Approximate Statistical 
Tests for Comparing Supervised Classification 
Learning Algorithms. Neural Computation, 10 (7) 
1895-1924.
Rong-en Fan, Pai-Hsuen Chen, Chij-Jen Lin. 2005. 
Working set selection using the second order 
information for training SVM. Technical report, 
Department of Computer Science, National Taiwan 
University (www.csie.ntu.edu.tw/~cjlin/papers/
quadworkset.pdf).
William A. Gale, Kenneth W. Church. 1991. Identify-
ing word correspondences in parallel texts. In Pro-
ceedings of the Fourth DARPA Workshop on 
Speech and Natural Language. Asilomar, CA:152?
157.
Radu Ion. 2006. TTL: A portable framework for to-
kenization, tagging and lemmatization of large cor-
pora. PhD thesis progress report. Research Institute 
for Artificial Intelligence, Romanian Academy, 
Bucharest (in Romanian), 22p. 
Dan Melamed. 2001. Empirical Methods for Exploit-
ing Parallel Texts. Cambridge, MA, MIT Press. 
Rada Mihalcea, Ted Pedersen. 2003. An Evaluation 
Exercise for Word Alignment. Proceedings of the 
HLT-NAACL 2003 Workshop: Building and Using 
Parallel Texts Data Driven Machine Translation 
and Beyond. Edmonton, Canada: 1?10. 
Joel Martin, Rada Mihalcea, Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proceeding of the ACL2005 Workshop 
on ?Building and Using Parallel Corpora: Data-
driven Machine Translation and Beyond?. June,
2005, Ann Arbor, Michigan, June, Association for 
Computational Linguistics, 65?74 
Robert Moore. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora in Machine Trans-
lation: From Research to Real Users. In Proceed-
ings of the 5th Conference of the Association for 
Machine Translation in the Americas, Tiburon, 
California), Springer-Verlag, Heidelberg, Ger-
many: 135-244. 
Franz J. Och, Herman Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Models, 
Computational Linguistics, 29(1):19-51. 
Franz J. Och, Herman Ney. 2000. Improved Statistical 
Alignment Models. In Proceedings of the 38th Con-
ference of ACL, Hong Kong: 440-447. 
Joerg Tiedemann. 2003. Combining clues for word 
alignment. In Proceedings of the 10th EACL, Bu-
dapest, Hungary: 339?346. 
Dan Tufi?. 2002. A cheap and fast way to build useful 
translation lexicons. In Proceedings of COL-
ING2002, Taipei, China: 1030-1036. 
Dan Tufi?, Ana-Maria Barbu, Radu Ion. 2003. TREQ-
AL: A word-alignment system with limited lan-
guage resources. In Proceedings of the NAACL 
2003 Workshop on Building and Using Parallel 
Texts; Romanian-English Shared Task, Edmonton, 
Canada: 36-39. 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan Ste-
f?nescu. 2005. Combined Aligners. In Proceeding
of the ACL2005 Workshop on ?Building and Using 
Parallel Corpora: Data-driven Machine Transla-
tion and Beyond?. June, 2005, Ann Arbor, Michi-
gan, June, Association for Computational Linguis-
tics, pp. 107-110. 
Dan Tufi?, Radu Ion. 2005. Multiple Sense Invento-
ries and Test-Bed Corpora. In C. Burileanu (ed.) 
Trends in Speech Technology, Publishing House of 
the Romanian Academy, Bucharest: 49-58.
160
Sense Discrimination with Parallel Corpora
Nancy Ide
Dept. of Computer Science
Vassar College
Poughkeepsie,
New York 12604-0520
 USA
ide@cs.vassar.edu
Tomaz Erjavec
Dept. of Intelligent Systems
Institute "Jozef Stefan"
Jamova 39,
SI-1000 Ljubljana
SLOVENIA
tomaz.erjavec@ijs.si
Dan Tufis
RACAI
Romanian Academy
Casa Academiei,
Calea 13 Septembrie 13,
Bucharest 74311, ROMANIA
tufis@racai.ro
Abstract
This paper describes an experiment that
uses translation equivalents derived from
parallel corpora to determine sense
distinctions that can be used for automatic
sense-tagging and other disambiguation
tasks. Our results show that sense
distinctions derived from cross-lingual
information are at least as reliable as those
made by human annotators. Because our
approach is fully automated through all its
steps, it could provide means to obtain
large samples of ?sense-tagged? data
without the high cost of human
annotation.
1 Introduction
It is well known that the most nagging issue for
word sense disambiguation (WSD) is the definition
of just what a word sense is. At its base, the
problem is a philosophical and linguistic one that is
far from being resolved. However, work in
automated language processing has led to efforts to
find practical means to distinguish word senses, at
least to the degree that they are useful for natural
language processing tasks such as summarization,
document retrieval, and machine translation.
Resnik and Yarowsky (1997) suggest that for the
purposes of WSD, the different senses of a word
could be determined by considering only sense
distinctions that are lexicalized cross-linguistically.
In particular, they propose that some set of target
languages be identified, and that the sense
distinctions to be considered for language
processing applications and evaluation be restricted
to those that are realized lexically in some
minimum subset of those languages. This idea
would seem to provide an answer, at least in part,
to the problem of determining different senses of a
word: intuitively, one assumes that if another
language lexicalizes a word in two or more ways,
there must be a conceptual motivation. If we look
at enough languages, we would be likely to find the
significant lexical differences that delimit different
senses of a word.
Several studies have used parallel texts for WSD
(e.g., Gale et al, 1993; Dagan et al, 1991; Dagan
and Itai, 1994) as well as to define semantic
properties of and relations among lexemes (Dyvik,
1998). More recently, two studies have examined
the use of cross-lingual lexicalization as a criterion
for validating sense distinctions: Ide (1999) used
translation equivalents derived from aligned
versions of Orwell?s Nineteen Eighty-Four among
five languages from four different languages
families, while Resnik and Yarowsky (2000) used
translations generated by native speakers presented
with isolated sentences in English. In both of these
studies, translation information was used to
validate sense distinctions provided in lexicons
such as WordNet (Miller et al, 1990). Although
the results are promising, especially for coarse-
grained sense distinctions, they rest on the
acceptance of a previously established set of
senses. Given the substantial divergences among
sense distinctions in dictionaries and lexicons,
together with the ongoing debate within the WSD
community concerning which sense distinctions, if
any, are appropriate for language processing
applications, fitting cross-linguistic information to
pre-established sense inventories may not be the
optimal approach.
                     July 2002, pp. 54-60.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
This paper builds on previously reported work (Ide
et al, 2001) that uses translation equivalents
derived from a parallel corpus to determine sense
distinctions that can be used to automatically
sense-tag the data. Our results show that sense
distinctions derived from cross-lingual information
are at least as reliable as those made by human
annotators. Our approach therefore provides a
promising means to automatically identify sense
distinctions.
2 Methodology
We conducted a study using parallel, aligned
versions of George Orwell's Nineteen Eighty-Four
(Erjavec and Ide, 1998) in seven languages:
English, Romanian, Slovene, Czech, Bulgarian,
Estonian, and Hungarian. The study involves
languages from four language families (Germanic,
Romance, Slavic, and Finno-Ugric),  three
languages from the same family (Czech, Slovene
and Bulgarian), as well as two  non-Indo-European
languages (Estonian and Hungarian). Although
Nineteen Eighty-Four, (ca. 100,000 words),  is a
work of fiction, Orwell's prose is not highly
stylized and, as such, it provides a reasonable
sample of modern, ordinary language that is not
tied to a given topic or sub-domain (which is the
case for newspapers, technical reports, etc.).
Furthermore, the translations of the text seem to be
relatively faithful to the original: over 95% of the
sentence alignments in the full parallel corpus of
seven languages are one-to-one (Priest-Dorman, et
al., 1997).
2.1 Preliminary Experiment
We constructed a multilingual lexicon based on the
Orwell corpus, using a method outlined in Tufis
and Barbu (2001, 2002). The complete English
Orwell contains 7,069 different lemmas, while the
computed lexicon comprises 1,233 entries, out of
which 845 have (possibly multiple) translation
equivalents in all languages. We then conducted a
preliminary study using a subset of 33 nouns
covering a range of frequencies and degrees of
ambiguity (Ide, et al, 2001).
For each noun in the sample, we extracted all
sentences from the English Nineteen Eighty-Four
containing the lemma in question, together with the
parallel sentences from each of the six translations.
The aligned sentences were automatically scanned
to extract translation equivalents.
1
 A vector was
then created for each occurrence, representing all
possible lexical translations in the six parallel
versions: if a given word is used to translate that
occurrence, the vector contains a 1 in the
corresponding position, 0 otherwise. The vectors
for each ambiguous word were fed to an
agglomerative clustering algorithm (Stolcke,
1996), where the resulting clusters are taken to
represent different senses and sub-senses of the
word in question.
The clusters produced by the algorithm were
compared with sense assignments made by two
human annotators on the basis of WordNet 1.6.
2
 In
order to compare the algorithm results with the
annotators? sense assignments, we normalized the
data as follows: for each annotator and the
algorithm, each of the 33 words was represented as
a vector of length n(n-1)/2, where n is the number
of occurrences of the word in the corpus. The
positions in the vector represent a ?yes-no?
assignment for each pair of occurrences, indicating
whether or not they were judged to have the same
sense (the same WordNet sense for the annotators,
and the same cluster for the algorithm).
Representing the clustering algorithm results in this
form required some means to ?flatten? the cluster
hierarchies, which typically extend to 5 or 6 levels,
to conform more closely to the completely flat
WordNet-based data. Therefore, clusters with a
minimum distance value (as assigned by the
clustering algorithm) at or below 1.7 were
combined, and each leaf of the resulting collapsed
tree was treated as a different sense. This yielded a
set of sense distinctions for each word roughly
similar in number to those assigned by the
annotators.
3
The cluster output for glass  in Figure 1 is an
example of the results obtained from the clustering
algorithm. For clarity, the occurrences have been
manually labeled with WordNet 1.6 senses (Figure
2). The tree shows that the algorithm correctly
                                                           
1
 Sentences in which more than one translation equivalent
appears were eliminated (cca. 5% of the translations).
2
 Originally, the annotators attempted to group occurrences
without reference to an externally defined sense set, but this
proved to be inordinately difficult and produced highly
variable results and was eventually abandoned.
3
 We used the number of senses annotators assigned rather
than the number of WordNet senses as a guide to determine
the minimum distance cutoff, because many WordNet senses
are not represented in the corpus.
grouped occurrences corresponding to WordNet
sense 1 (a solid material) in one of the two main
branches, and those corresponding to sense 2
(drinking vessel) in the other.  The top group is
further divided into two sub-clusters, the lower of
which refer to a looking glass and a magnifying
glass, respectively. While this is a particularly clear
example of good results from the clustering
algorithm, results for other words are, for the most
part, similarly reasonable.
Figure 1 : Output of the clustering algorithm
1. a brittle transparent solid with
irregular atomic structure
2. a glass container for holding liquids
while drinking
3. the quantity a glass will hold
4. a small refracting telescope
5. a mirror; usually a ladies' dressing
mirror
6. glassware collectively; "She collected
old glass"
Figure 2 : WordNet 1.6 senses for glass (noun)
The results of the first experiment are summarized
in Table 1, which shows the percentage of
agreement between the cluster algorithm and each
annotator, between the two annotators, and for the
algorithm and both annotators taken together.
4
 The
percentages are similar to those reported in earlier
work; for example, Ng et al (1999) achieved a raw
percentage score of 58% agreement among
annotators tagging nouns with WordNet 1.6 senses.
Cluster/Annotator 1 66.7%
Cluster/Annotator 2 63.6%
Annotator 1/Annotator 2 76.3%
Cluster/Annotator 1/ Annotator 2 53.4%
Table 1 : Levels of agreement
2.2 Second experiment
Comparison of sense differentiation achieved using
translation equivalents, as determined by the
clustering algorithm, with those assigned by human
annotators suggests that use of translation
equivalents for word sense tagging and
disambiguation is worth pursuing. Agreement
levels are comparable to (and in some cases higher
than) those obtained in earlier studies tagging with
WordNet senses. Furthermore, the pairwise
difference in agreement between the human
annotators and the annotators and the clustering
algorithm is only 10-13%, which is also similar to
scores obtained in other studies.
In the second phase, the experiment was broadened
to include 76 nouns from the multi-lingual lexicon,
including words with varying ambiguity (the range
in number of WordNet senses is 2 to 29, average
7.09) and semantic characteristics (e.g., abstract vs.
concrete: ?thought?, ?stuff?, ?meaning?, ?feeling?
vs. ?hand?, ?boot?, ?glass?, ?girl?, etc.). We chose
nouns that occur a minimum of 10 times in the
corpus, have no undetermined translations and at
least five different translations in the six non-
English languages, and have the log likelihood
score of at least 18; that is:
LL(T
T
, T
S
)  =
? ?
= =
2
1
ij
2
1i
n*2
j
*
j**i
**ij
n*n
n*n
log  
? 18
where n
ij
 stands for the number of times T
T
 and T
S
have been seen together in aligned sentences, n
i*
and n
*j 
stand for the number occurrences of T
T
 and
T
S,
 respectively, and n
**
 represents the total
                                                           
4
 We computed raw percentages only; common measures of
annotator agreement such as the Kappa statistic (Carletta,
1996) proved to be inappropriate for our two-category (?yes-
no?) classification scheme.
                _____|-> (1)
         |-----|     |-> (1)
         |     |_____|---> (1)
         |           |___|-> (1)
         |               |-> (1)
         |         |---> (1)
    |----|         |            _|-> (1)
    |    |         |         |-| |-> (1)
    |    |     |---|       |-| |-> (1)
    |    |     |   |     |-| |-> (1)
    |    |-----|   |   |-| |-> (1)
 |--|          |   |---| |-> (1)
 |  |          |       |-> (1)
 |  |          |___|---> (6)
 |  |              |___|-----> (1)
 |  |                  |-----> (1)
 |  |     _____|-----> (1)
-|  |----|     |-----> (5)
 |       |-----> (4)
 |      |---> (2)
 |  |---|      _|-> (2)
 |  |   |   |-| |-> (2)
 |  |   |---| |-> (2)
 |--|       |-> (2)
    |   |-----> (2)
    |   |           ___|-----> (2)
    |---|     |----|   |-----> (2)
        |     |    |    _|-> (2)
        |     |    |---| |-> (2)
        |-----|        |-> (2)
              |     ____|-> (3)
              |----|    |-> (2)
                   |     _|-> (2)
                   |----| |-> (2)
                               |-> (2)
number of potential translation equivalents in the
parallel corpus. The LL score is set at a maximum
value to ensure high precision for the extracted
translation equivalents, which minimizes sense
clustering errors due to incorrect word alignment.
Table 2 summarizes the data.
No. of words 76
No. of example sentences 2399
Average examples/word 32
No. of senses (annotator 1) 241
No. of senses (annotator 2) 280
No. of senses (annotator 3) 213
No. of senses (annotator 4) 232
No. of senses (all annotators) 345
Average senses per word 4.53
Percentage of annotator agreement:
Full agreement (4/4) 54.27
75% agreement (3/4) 28.13
50% agreement (2/4) 16.92
No agreement 0.66
Table 2 : Summary of the data
In this second experiment, we increased the
number of annotators to four. The results of the
clustering algorithm and the sense assignments
made by the human annotators were normalized
differently than in the earlier experiment, by
ignoring sense numbers and interpreting the
annotators? sense assignments as clusters only. To
see why this was necessary, consider the following
set of sense assignments for the seven occurrences
of ?youth? in Nineteen Eighty-Four:
OCC 1 2 3 4 5 6 7
Ann1
3 1 6 3 6 3 1
Ann2
2 1 4 2 6 2 1
Agreement is 43%; however, both annotators
classify occurrences 1, 4, and 6  as having the same
sense, although each assigned a different sense
number to the group. If we ignore sense numbers
and consider only the annotators? ?clusters?, the
agreement rate is much higher,
5
 and the data is
more comparable to that obtained from the cluster
algorithm.
We also addressed the issue of the appropriate
point at which to cut off the clustering by the
algorithm. Our use of a pre-defined minimum
                                                           
5
 In fact, the only remaining disagreement is that Annotator 1
assigns occurrences 3 and 5 together, whereas Annotator 2
assigns a different sense to occurrence 3?in effect, Annotator
2 makes a finer distinction than Annotator 1 between
occurrences 3 and 5.
distance value to determine the number of clusters
(senses) in the earlier experiment  yielded varying
results for different words (especially words with
significantly different numbers of translation
equivalents) and we sought a more principled
means to determine the cut-off value. The
clustering algorithm was therefore modified to
compute the correct number of clusters
automatically by halting the clustering process
when the number of clusters reached a value
similar to the average number obtained by the
annotators.
6
 As criteria, we used the minimum
distance between existing clusters at each iteration,
which determines the two clusters to be joined,
where minimum distance is computed between two
vectors v
1
, v
2
 length n as:
? 
(v
1
(i) - v
2
(i))
2
i=1
n
?
Best results were obtained when the clustering was
stopped at the point where:
(dist(k)-dist(k+1))/dist(k+1) < 0.12
where dist(k) is the minimal distance between two
clusters at the kth iteration step.
We defined a ?gold standard? annotation by taking
the majority vote of the four annotators (in case of
ties, the annotator closest to the majority vote in
the greatest number of cases was considered to be
right). Using this heuristic, the clustering algorithm
assigned the same number of senses as the gold
standard for 41 words. However, overall agreement
was much worse (67.9%) than when the number of
clusters was pre-specified. The vast majority of
clustering errors occurred when sense distributions
are skewed; we therefore added a post-processing
phase in which the smallest clusters are eliminated
and their members included in the largest cluster
when the number of occurrences in the largest
cluster is at least ten times that of any other
cluster.
7
With this new heuristic, the algorithm produced the
same number of clusters as the gold standard for
only 15 words, but overall agreement reached
74.6%. Mismatching clusters typically included
                                                           
6
 In principle, the upper limit for the number of senses for a
word is the number of senses in WordNet 1.6; however, there
was no case in which all WordNet senses appeared in the text.
7
 The factor of 10 is a conservative threshold; additional
experiments might yield evidence for a lower value.
only one element. There were only five words for
which a difference in the  number of clusters
assigned by the gold standard vs. the algorithm
significantly contributed to the 2.7% depreciation
in agreement.
We also experimented with eliminating the data for
?non-contributing? languages  (i.e., languages for
which there is only one translation for the target
word); this was ultimately abandoned because it
worsened results by amplifying the effect of
synonymous translations in other languages.
Finally, we compared the use of weighted vs.
unweighted clustering algorithms (see, e.g.,
Yarowsky and Florian, 1999) and determined that
results were improved using weighted clustering.
The clusters produced by each pair of classifiers
(human or machine) were mapped for maximum
overlap; differences were considered as
divergences. The agreement between two different
classifications was computed as the number of
common occurrences in the corresponding clusters
of the two classifications divided by the total
number of the occurrences of the target word. For
example, the word movement occurs 40 times in
the corpus; both the ?gold standard? and the
algorithm identified four clusters, but the
distribution of the 40 occurrences was substantially
different, as summarized in Table 3.  Thirty-four of
the 40 occurrences appear in the clusters common
to the two classifications; therefore, the agreement
rate is 85%.
CLUSTER 1 2 3 4
Gold standard 28 6 3 3
Algorithm 25 7 6 2
Intersection 24 6 3 1
Table 3 : Gold standard vs. algorithm clustering for
movement
2.3 Results
The results of our second experiment are
summarized in Table 4, which gives the agreement
rate between baseline clustering (B), in which it is
assumed all occurrences are labeled with the same
sense; each pair of human annotators (1-4); the
gold standard (G); and the clustering algorithm
(A). The table shows that agreement rates among
the human annotators, as compared to those
between the algorithm and all but one annotator,
are not significantly different, and that the
algorithm?s highest level of agreement is with the
baseline. This is not surprising because of the
second heuristic used. However, the second best
agreement rate for the algorithm is with the gold
standard, which suggests that sense distinctions
determined using the algorithm are almost as
reliable as sense distinctions determined manually.
The agreement of the algorithm with the gold
standard falls slightly below that of the human
annotators, but is still well within the range of
acceptability. Also, given that the gold standard
was computed on the basis of the human
annotations, it is understandable that these
annotations do better than the algorithm.
1 2 3 4 G A
B
71.1 65.1 76.3 74.1 75.5 81.5
1
78.1 75.6 83.1 88.6 74.4
2
71.3 75.9 82.5 66.9
3
77.3 82.1 77.1
4
90.4 75.9
G
77.3
Table 4 Agreement rates among baseline, the four
annotators, gold standard, and the algorithm
3 Discussion and Further Work
Our results show that sense distinctions based on
translation variants from parallel corpora are
similar to those obtained from human annotators,
which suggests several potential applications.
Because our approach is fully automated through
all its steps, it could be used to automatically
obtain large samples of ?sense-differentiated? data
without the high cost of human annotation.
Although our method does not choose sense
assignments from a pre-defined list, most language
processing applications (e.g. information retrieval)
do not require this knowledge; they need only the
information that different occurrences of a given
word are used in the same or a different sense.
A by-product of applying our method is that once
words in a text in one language are tagged using
this method, different senses of the corresponding
translations in the parallel texts are also identified,
potentially providing a source of information for
use in other language processing tasks and for
building resources in the parallel languages (e.g.,
WordNets for the Eastern European languages in
our study).  In addition, if different senses of target
words are identified in parallel texts, contextual
information for different senses of a word can be
gathered for use in disambiguating other, unrelated
texts. The greatest obstacle to application of this
approach is, obviously, the lack of parallel corpora:
existing freely available parallel corpora including
several languages are typically small (e.g., the
Orwell), domain dependent (e.g. the MULTEXT
Journal of the Commission (JOC) corpus; Ide and
V?ronis, 1994), and/or represent highly stylized
language (e.g. the Bible; Resnik et al, 1999).
Appropriate parallel data including Asian
languages  is virtually non-existent. Given that our
method applies only to words for which different
senses are lexicalized differently in at least one
other language, its broad application depends on
the future availability of large-scale parallel
corpora including a variety of language types.
Many studies have pointed out that coarser-grained
sense distinctions can be assigned more reliably by
human annotators than finer distinctions such as
those in WordNet. In our study, the granularity of
the sense distinctions was largely ignored, except
insofar as we attempted to cut off the number of
clusters produced by the algorithm at a value
similar to the number identified by the annotators.
The sense distinctions derived from the clustering
algorithm are hierarchical, often identifying four or
five levels of refinement, whereas the WordNet
sense distinctions are organized as a flat list with
no indication of their degree of relatedness. Our
attempt to flatten the cluster data in fact loses much
information about the relatedness of senses.
8
 As a
result, both annotators and the clustering algorithm
are penalized as much for failing to distinguish
fine-grained as coarse-grained distinctions. We are
currently exploring two possible sources of
information about sense relatedness: the output of
the clustering algorithm itself, and WordNet
hypernyms, which may not only improve but also
broaden the applicability of our method.
                                                           
8
 Interestingly, the clustering for ?glass? in Figure 1 reveals
additional sub-groupings that are not distinguished in
WordNet:  the top sub-group of the top cluster includes
occurrences that deal with some physical aspect of the material
(?texture of?, ?surface of?, ?rainwatery?, ?soft?, etc.). In the
lower cluster, the two main sub-groups distinguish a (drinking)
glass as a manipulatable object (by washing, holding, on a
shelf, etc.) from its sense as a vessel (mainly used as the object
of ?pour into?, ?fill?, ?take/pick up?, etc. or modified by
?empty?, ?of gin?, etc.).
We note in our data that although it is not
statistically significant, there is some correlation (-
.51) between the number of WordNet senses for a
word and overall agreement levels. The lowest
overall agreement levels were for ?line? (29
senses), ?step? (10), position (15), ?place? (17),
and ?corner? (11). Perfect agreement was achieved
for several words with under 5 senses, e.g., ?hair?
(5), ?morning? (4), ?sister? (4), ?tree? (2), and
?waist? (2)?all of which were judged by both the
annotators and the algorithm to occur in only one
sense in the text. On the other hand, agreement
levels for some words with under five WordNet
senses had low agreement: e.g., ?rubbish? (2),
?rhyme? (2), ?destruction? (3), and ?belief? (3).
Because both the algorithm (which based
distinctions on translations) and the human
annotators (who used WordNet senses) had low
agreement in these cases, the WordNet sense
distinctions may be overly fine-grained and,
possibly, irrelevant to many language processing
tasks.
We continue to explore the viability of our method
to automatically determine sense distinctions
comparable to those achieved by human
annotators. We are currently exploring methods to
refine the clustering results as well as their
comparison to results obtained from human
annotators (e.g., the Gini Index  [Boley, et al,
1999]).
4 Conclusion
The results reported here represent a first step in
determining the degree to which automated
clustering based on translation equivalents can be
used to differentiate word senses.  Our work so far
indicates that the method is promising and could
provide a significant means to automatically
acquire sense-differentiated data in multiple
languages. Our current results suggest that coarse-
grained agreement is the best that can be expected
from humans, and that our method is capable of
duplicating sense differentiation at this level.
5 Acknowledgements
Our thanks go to Arianna Schlegel, Christine
Perpetua, and Lindsay Schulz who annotated the
data, and to Ion Radu who modified the clustering
algorithm. We would also like to thank the
anonymous reviewers for their comments and
suggestions. All errors, of course, remain our own.
6 References
Boley D., Gini, M, Gross, R., Han, S.,.
Hastings, K and Karypis, G., Kumar, V.,
Mobasher, B, Moore, J. (1999) Partitioning-Based
Clustering for Web Document Categorization.
Decision Support Systems, 27:3, 329-341.
Carletta, J. (1996). Assessing Agreement on
Classification Tasks: The Kappa Statistic.
Computational Linguistics, 22:2, 249-254.
Dagan, I. and Itai, A. (1994). Word sense
disambiguation using a second language
monolingual corpus. Computational Linguistics,
20:4, 563-596.
Dagan, I., Itai, A., and Schwall, U. (1991). Two
languages are more informative than one.
Proceedings of the 29th Annual Meeting of the
ACL, 18-21 Berkeley, California, 130-137.
Dyvik, H. (1998). Translations as Semantic
Mirrors. Proceedings of Workshop Multilinguality
in the Lexicon II, ECAI 98, Brighton, UK, 24-44.
Erjavec, T. and Ide, N. (1998). The
MULTEXT-EAST Corpus. Proceedings of the
First International Conference on Language
Resources and Evaluation, Granada, 971-74.
Gale, W. A., Church, K. W. and Yarowsky, D.
(1993). A method for disambiguating word senses
in a large corpus. Computers and the Humanities,
26, 415-439.
Ide, N. (1999). Cross-lingual sense
determination: Can it work? Computers and the
Humanities, 34:1-2,  223-34.
Ide, N., Erjavec, T., and Tufis, D. (2001).
Automatic sense tagging using parallel corpora.
Proceedings of the Sixth Natural Language
Processing Pacific Rim Symposium, Tokyo,  83-89.
Ide, N., V?ronis, J. (1994). Multext
(Multilingual Tools and Corpora). Proceedings of
the 14th International Conference on
Computational Linguistics, COLING?94, Kyoto,
90-96.
Miller, G. A., Beckwith, R. T. Fellbaum, C. D.,
Gross, D. and Miller, K. J. (1990). WordNet: An
on-line lexical database. International Journal of
Lexicography, 3:4, 235-244.
Ng, H. T., Lim, C. Y., Foo, S. K. (1999). A
Case Study on Inter-Annotator Agreement for
Word Sense Disambiguation. Proceedings of the
ACL SIGLEX Workshop: Standardizing Lexical
Resources, College Park, MD, USA, 9-13.
Priest-Dorman, G.; Erjavec, T.; Ide, N. and
Petkevic, V. (1997). Corpus Markup. COP Project
106 MULTEXT-East D2.3 F.
Resnik, P. and Yarowsky, D. (2000).
Distinguishing systems and distinguishing senses:
New evaluation methods for word sense
disambiguation. Journal of Natural Language
Engineering, 5(2): 113-133.
Resnik, P., Broman Olsen, M., Diab, M. (1999).
Creating a Parallel Corpus from the Book of 2000
Tongues. Computers and the Humanities, 33:1-2.
129-153.
Resnik, Philip and Yarowsky, David (1997). A
perspective on word sense disambiguation methods
and their evaluation. ACL-SIGLEX Workshop
Tagging Text with Lexical Semantics: Why, What,
and How? Washington, D.C., 79-86.
Stolcke, Andreas (1996) Cluster 2.9.
http://www.icsi.berkeley.edu/ftp/global/pub/ai/
stolcke/software/cluster-2.9.tar.Z.
Tufis, D., Barbu, A.-M. (2001) Automatic
Construction of Translation Lexicons. In V.Kluew,
C. D'Attellis N. Mastorakis (eds.) Advances in
Automation, Multimedia and Modern Computer
Science, WSES Press, 156-172
Tufis, D., Barbu, A.-M. (2002), Revealing
translators knowledge: statistical methods in
constructing practical multilingual lexicons for
language and speech processing. International
Journal of Speech Technology (to appear).
Yarowsky, D., Florian. R. (1999). Taking the
load off the conference chairs: towards a digital
paper-routing assistant. Proceedings of the Joint
SIGDAT Conference on Empirical Methods in NLP
and Very Large Corpora, 220-230.
TREQ-AL: A word alignment system with limited language resources 
 
Dan Tufi?, Ana-Maria Barbu, Radu Ion 
Romanian Academy Institute for Artificial Intelligence 
13, ?13 Septembrie?, 74311, Bucharest 5, Romania 
{tufis,abarbu,radu}@racai.ro 
 
 
 
Abstract
 
We provide a rather informal presentation of a 
prototype system for word alignment based on 
our previous translation equivalence approach, 
discuss the problems encountered in the 
shared-task on word-aligning of a parallel 
Romanian-English text, present the preliminary 
evaluation results and suggest further ways of 
improving the alignment accuracy. 
 
1 Introduction 
In (Tufi? and Barbu, 2002; Tufi?, 2002) we largely 
described our extractor of translation equivalents, called 
TREQ. It was aimed at building translation dictionaries 
from parallel corpora. We described in (Ide et al 2002) 
how this program is used in word clustering and in 
checking out the validity of the cross-lingual links 
between the monolingual wordnets of the multilingual 
Balkanet lexical ontology (Stamatou et al 2002). In this 
paper we describe the TREQ-AL system, which builds 
on TREQ and aims at generating a word-alignment map 
for a parallel text (a bitext). TREQ-AL was built in less 
than two weeks for the Shared Task proposed by the 
organizers of the workshop on ?Building and Using 
Parallel Texts:Data Driven Machine Translation and 
Beyond? at the HLT-NAACL 20031 conference. It can 
be improved in several ways that became conspicuous 
when we analyzed the evaluation results. TREQ-AL has 
no need for an a priori bilingual dictionary, as this will 
be automatically extracted by TREQ. However, if such 
a dictionary is available, both TREQ and TREQ-AL 
know to make best use of it. This ability allows both 
systems to work in a bootstrapping mode and to produce 
larger dictionaries and better alignments as they are 
used. 
The word alignment, as it was defined in the shared 
task is different and harder than the problem of 
translation equivalence as previously addressed. In a 
dictionary extraction task one translation pair is 
considered correct, if there is at least one context in 
which it has been rightly observed. A multiply 
occurring pair would count only once for the final 
                                                 
1 http://www.cs.unt.edu/~rada/wpt/index.html#shared 
dictionary. This is in sharp contrast with the alignment 
task where each occurrence of the same pair equally 
counts. 
Another differentiating feature between the two 
tasks is the status of functional word links. In extracting 
translation equivalents one is usually interested only in 
the major categories (open classes). In our case (because 
of the WordNet centered approach of our current 
projects) we were especially interested in POS-
preserving translation equivalents. However, since in 
EuroWordNet and Balkanet one can define cross-POS 
links, the different POS translation equivalents became 
of interest (provided these categories are major ones).  
The word alignment task requires each word 
(irrespective of its POS) or punctuation mark in both 
parts of the bitext be assigned a translation in the other 
part (or the null translation if the case).  
Finally, the evaluations of the two tasks, even if 
both use the same measures as precision or recall, have 
to be differently judged. The null alignments in a 
dictionary extraction task have no significance, while in 
a word alignment task they play an important role (in 
the Romanian-English gold standard data the null 
alignments represent 13,35% of the total number of 
links).  
 
2 The preliminary data processing  
The TREQ system requires sentence aligned parallel 
text, tokenized, tagged and lemmatized. The first 
problem we had with the training and test data was 
related to the tokenization. In the training data there 
were several occurrences of glued words (probably due 
to a problem in text export of the initial data files) plus 
an unprintable character (hexadecimal code A0) that 
generated several tagging errors due to guesser 
imperfect performance (about 70% accurate). 
To remedy these inconveniences we wrote a script 
that automatically split the glued words and eliminated 
the unprintable characters occurring in the training data. 
The set of splitting rules, learnt from the training 
data was posted on the site of the shared task. The set of 
rules is likely to be incomplete (some glued words 
might have survived in the training data) and also might 
produce wrong splitting in some cases (e.g. turnover 
being split always in turn over).  
The text tokenization, as considered by the 
evaluation protocol, was the simplest possible one, with 
white spaces and punctuation marks taken as separators. 
The hyphen (?-?) was always considered a separator and 
consequently taken to be always a token by itself. 
However, in Romanian, the hyphen is more frequently 
used as an elision marker (as in ?intr-o?= ?intru o?/in a), 
a clitics separator (as in ?da-mi-l?=?da ?mi ?l?=?da mie 
el?/give to me it/him) or as a compound marker (as in 
?terchea-berchea? /(approx.) loafer) than as a separator.  
In such cases the hyphen cannot be considered a token. 
A similar problem appeared in English with respect to 
the special quote character, which was dealt with in 
three different ways: it was sometimes split as a distinct 
token (we?ll = we + ? + ll), sometimes was adjoined to 
the string (a contracted positive form or a genitival) 
immediately following it (I?m = I + ?m, you?ve =  
you+?ve,  man?s = man + ?s etc.) and systematically left 
untouched in the negative contracted forms (couldn?t, 
wasn?t, etc).   
Since our processing tools (especially the tokeniser) 
were built with a different segmentation strategy in 
mind, we generated the alignments based on our own 
tokenization and, at the end, we ?re-tokenised? the text 
according to the test data model (and consequently re-
index) all the linking pairs.  
For tagging the Romanian side of the training bitext 
we used the tiered-tagging approach (Tufi?, 1999) but 
we had to construct a new language model since our 
standard model was created from texts containing 
diacritics. As the Romanian training data did not contain 
diacritical characters, this was by no means a trivial task 
in the short period of time at our disposal (actually it 
took most of the training time). The lack of diacritics in 
the training data and the test data induced spurious 
ambiguities that degraded the tagging accuracy with at 
least 1%. This is to say that we estimate that on a 
normal Romanian text (containing the diacritical 
characters) the performance of our system would have 
been better. The English training data was tagged by 
Eric Gaussier, warmly acknowledged here. As the 
tagsets used for the two languages in the parallel 
training corpus were quite different, we defined a tagset 
mapping and translated the tagging of the English part 
into a tagging closer to the Romanian one. This 
mapping introduced some ambiguities that were solved 
by hand. Based on the training data (both Romanian and 
English texts), tagged with similar tagsets, we built the 
language models used for the test data alignment. 
POS-preserving translation equivalence is a too 
restrictive condition for the present task and we defined 
a meta-tagset, common for both languages that 
considered frequent POS alternations. For instance, the 
verb, noun and adjective tags, in both languages were 
prefixed with a common symbol, given that verb-
adjective, noun-verb, noun-adjective and the other 
combinations are typical for Romanian-English 
translation equivalents that do not preserve the POS. 
With these prefixes, the initial algorithm for extracting 
POS-preserving translation equivalents could be used 
without any further modifications. Using the tag-
prefixes seems to be a good idea not only for legitimate 
POS-alternating translations, but also for overcoming 
some typical tagging errors, such as participles versus 
adjectives. In both languages, this is by far the most 
frequent tagging error made by our tagger. 
The last preprocessing phase is encoding the corpus 
in a XCES-Align-ana format as used in the MULTEXT-
EAST corpus (see http://nl.ijs.si/ME/V2/) which is the 
standard input for the TREQ translation equivalents 
extraction program. Since the description of TREQ is 
extensively given elsewhere, we will not go into further 
details, except of saying that the resulted translation 
dictionary extracted from the training data contains 
49283 entries (lemma-form). The filtering of the 
translation equivalents candidates (Tufi? and Barbu, 
2002) was based on the log-likelihood and the cognate 
scores with a threshold value set to 15 and 0,43 
respectively.  We roughly estimated the accuracy of this 
dictionary based on the aligned gold standard: precision 
is about 85% and recall is about 78% (remember, the 
dictionary is evaluated in terms of lemma entries, and 
the non-matching meta-category links are excluded).  
 
3 The TREQ-AL linking program  
This program takes as input the dictionary created by 
TREQ and the parallel text to be word-aligned. The 
alignment procedure is a greedy one and considers the 
aligned translation units independent of the other 
translation units in the parallel corpus.  It has 4 steps: 
1. left-to-right pre-alignment 
2. right-to-left adjustment of the pre-alignment 
3. determining alignment zones and filtering them out 
4. the word-alignment inside the  alignment zones 
 
3.1 The left-to-right pre-alignment 
For each sentence-alignment unit, this step scans the 
words from the first to the last in the source-language 
part (Romanian). The considered word is initially linked 
to all the words in the target-language part (English) of 
the current sentence-alignment unit, which are found in 
the translation dictionary as potential translations. If for 
the source word no translations are identified in the 
target part of the translation unit, the control advances to 
the next source word. The cognate score and the relative 
distance are decision criteria to choose among the 
possible links. When consecutive words in the source 
part are associated with consecutive or close to each 
other words in the target part, these are taken as forming 
an ?alignment chain? and, out of the possible links, are 
considered those that correspond to the densest 
grouping of words in each language. High cognate 
scores in an alignment chain reinforce the alignment. 
One should note that at the end of this step it is possible 
to have 1-to-many association links if multiple 
translations of one or more source words are found in 
the target part of the current translation unit (and, 
obviously, they satisfy the selection criteria). 
 
3.2 The right-to-left adjustment of the pre-alignment 
This step tries to correct the pre-alignment errors (when 
possible) and makes a 1-1 choice in case of the 1-m 
links generated before. The alignment chains (found in 
the previous step) are given the highest priority in 
alignment disambiguation. That is, if for one word in 
the source language there are several alignment 
possibilities, the one that belongs to an alignment chain 
is always selected. Then, if among the competing 
alignments one has a cognate score higher than the 
others then this is the preferred one (this heuristics is 
particularly useful in case of several proper names 
occurring in the same translation unit). Finally, the 
relative position of words in the competing links is 
taken into account to minimize the distance between the 
surrounding already aligned words. 
The first two phases result in a 1-1 word mapping. 
The next two steps use general linguistic knowledge 
trying to align the words that remain unaligned (either 
due to no translation equivalents or because of failure to 
meet the alignment criteria) after the previous steps. 
This could result in n-m word alignments, but also in 
unlinking two previously linked words since a wrong 
translation pair existing in the extracted dictionary 
might license a wrong link.  
 
3.3 Alignment zones and filtering suspicious links out 
An alignment zone (in our approach) is a piece of text 
that begins with a conjunction, a preposition, or a 
punctuation mark and ends with the token preceding the 
next conjunction, preposition, punctuation or end of 
sentence. A source-language alignment zone is mapped 
to one or more target-language alignment zones via the 
links assigned in the previous steps (based on the 
translation equivalents). One has to note that the 
mapping of the alignment zones is not symmetric. An 
alignment zone that contains no link is called a virgin 
zone. 
In most of the cases the words in the source 
alignment zone (starting zone) are linked to words in the 
target algnment zone/s (ending zone/s). The links with 
either side outside the alignment zones are suspicious 
and they are deleted. This filtering proved to be almost 
100% correct in case the outlier resides in a zone non-
adjacent to the starting or ending zones. The failures of 
this filtering were in the majority of cases due to a 
wrong use of punctuation in one or the other part of the 
translation unit (such as omitted comma, a comma 
between the subject and predicate). 
 
3.4 The word-alignment inside the alignment zones 
For each un-linked word in the starting zone the 
algorithm looks for a word in the ending zone/s of the 
same category (not meta-category). If such a mapping 
was not possible, the algorithm tries to link the source 
word to a target word of the same meta-category, thus 
resulting in a cross-POS alignment. The possible meta-
category mappings are specified by the user in an 
external mapping file. Any word in the source or target 
languages that is not assigned a link after the four 
processing steps described above is automatically 
assigned a null link. 
 
4 Post-processing  
As said in the second section, our tokenization was 
different from the tokenization in the training and test 
data. To comply with the evaluation protocol, we had to 
re-tokenize the aligned text and re-compute the indexes 
of the links. Re-tokenizing the text meant splitting 
compounds and contracted future forms and gluing 
together the previously split negative contracted forms 
(do+n?t=don?t). Although the re-tokenization was a 
post-processing phase, transparent for the task itself, it 
was a source of missing some links for the negative 
contracted forms. In our linking the English ?n?t? was 
always linked to the Romanian negation and the English 
auxiliary/modal plus the main verb were linked to the 
Romanian translation equivalent found for the main 
verb. Some multi-word expressions recognized by the 
tokenizer as one token, such as dates (25 Ianuarie, 
2001), compound prepositions (de la, pina la), 
conjunctions (pentru ca, de cind, pina cind) or adverbs 
(de jur imprejur, in fata) as well as the hyphen 
separated nominal compounds (mass-media, prim-
ministru) were split, their positions were re-indexed and 
the initial one link of a split compound was replaced 
with the set obtained by adding one link for each 
constituent of the compound to the target English word. 
If the English word was also a compound the number of 
links generated for one aligned multiword expression 
was equal to the N*M, where N represented the number 
of words in the source compound and M the number of 
words in the target compound.   
5 Evaluation 
The results of the evaluation of TREQ-AL performance 
are shown in the Table 1. In our submission file the 
sentence no. 221 was left out by (our) mistake. We used 
the official evaluation program to re-evaluate our 
submission with the omitted sentence included and the 
precision improved with 0,09%, recall with 0,45%, F-
measure and AER with 0,33%.). The figures in the first 
and second columns of the Table 1 are those considered 
by the official evaluation. The last column contains the 
evaluation of the result that was our main target. Since 
TREQ-AL produces only ?sure? links, AER (alignment 
error rate - see the Shared Task web-page for further 
details) reduces to 1 - F-measure. 
TREQ-AL uses no external bilingual-resources. A 
machine-readable bilingual dictionary would certainly 
improve the overall performance.  The present version 
of the system (which is far from being finalized) seems 
to work pretty well on the non-null assignments and this 
is not surprising, because these links are supposed to be 
relevant for a translation dictionary extraction system 
and this was the very reason we developed TREQ.  
Moreover if we consider only the content words (main 
categories: noun, verbs, adjectives and general adverbs), 
which are the most relevant with respect to our 
immediate goals (multilingual wordnets interlinking and 
word sense disambiguation), we think TREQ-AL 
performs reasonably well and is worth further 
improving it. 
 
 Non-null 
links only 
Null links 
included 
Dictionary 
entries 
Precision 81,38% 60,43% 84,42% 
Recall 60,71% 62,80% 77,72% 
F-measure 69,54% 61,59% 80,93% 
AER 30,46% 38,41% 
 Table 1. Evaluation results 
6 Conclusions and further work 
TREQ-AL was developed in a short period of time and 
is not completely tested and debugged. At the time of 
writing we already noticed two errors that were 
responsible for several wrong or missed links. There are 
also some conceptual limitations which, when removed, 
are likely to further improve the performance. For 
instance all the words in virgin alignment zones are 
automatically given null links but the algorithm could 
be modified to assign all the links in the Cartesian 
product of the words in the corresponding virgin zones. 
The typical example for such a case is represented by 
the idiomatic expressions (tanda pe manda = the list 
that sum up). A bilingual dictionary of idioms as an 
external resource certainly would significantly improve 
the results. Also, with an additional preprocessing 
phase, for collocation recognition, many missing links 
could be recovered. At present only those collocations 
that represent 1-2 or 2-1 alignments are recovered. 
A major improvement will be to make the 
algorithm symmetric. There are many cases when 
reversing the source and target languages new links can 
be established. This can be explained by different 
polysemy degrees of the translation equivalent words 
and the way we associate alignment zones. 
The word order in Romanian and English to some 
extent is similar, but in the present version of TREQ-AL 
this is not explicitly used. One obvious and easy 
improvement of TREQ-AL performance would be to 
take advantage of the similarity in word order and map 
the virgin zones and afterwards, the words in the virgin 
zones. 
Finally, we noticed in the gold standard some 
wrong alignments. One example is the following:  
?? a XI ? a ?? = ?? eleventh?? 
Our program aligned all the 4 tokens in Romanian (a, 
XI, ?, a) to the English token (eleventh), while the gold 
standard assigned only ?XI? to ?eleventh? and the other 
three Romanian tokens were given a null link.  We also 
noticed some very hard to achieve alignments 
(anaphoric links).   
 
7 References 
Tufi?, D. Barbu, A.M.: ?Revealing translators 
knowledge: statistical methods in constructing 
practical translation lexicons for language and speech 
processing?, in International Journal of Speech 
Technology. Kluwer Academic Publishers, no.5, 
pp.199-209, 2002. 
Tufi?, D. ?A cheap and fast way to build useful 
translation lexicons? in Proceedings of the 19th 
International Conference on Computational 
Linguistics, COLING2002,  Taipei, 25-30 August, 
2002, pp. 1030-1036p. 
Ide, N., Erjavec, T., Tufis, D.: ?Sense Discrimination 
with Parallel Corpora? in Proceedings of the SIGLEX 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions. ACL2002, July 
Philadelphia 2002, pp. 56-60. 
Stamou, S., Oflazer K., Pala  K., Christoudoulakis D., 
 Cristea D., Tufis D., Koeva  S., Totkov G., Dutoit 
 D., Grigoriadou M.. ?BALKANET A Multilingual 
Semantic Network for the Balkan Languages?, in 
Proceedings of the International Wordnet Conference, 
Mysore, India, 21-25 January 2002. 
Tufi?, D. ?Tiered Tagging and Combined 
Classifiers? In F. Jelinek, E. N?th (eds) Text, 
Speech and Dialogue, Lecture Notes in Artificial 
Intelligence 1692, Springer, 1999, pp. 28-33. 
An Evaluation Exercise for Romanian Word Sense Disambiguation
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Vivi Na?stase
School of Computer Science
University of Ottawa
Ottawa, ON, Canada
vnastase@site.uottawa.ca
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Doina Ta?tar
Department of Computer Science
Babes?-Bolyai University
Cluj-Napoca, Romania
dtatar@ubb.ro
Dan Tufis?
Romanian Academy Center
for Artificial Intelligence
Bucharest, Romania
tufis@racai.ro
Florentina Hristea
Department of Computer Science
University of Bucharest
Bucharest, Romania
fhristea@mailbox.ro
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for a
Romanian Word Sense Disambiguation task, which
was organized as part of the SENSEVAL-3 evaluation
exercise. Five teams with a total of seven systems
were drawn to this task.
1 Introduction
SENSEVAL is an evaluation exercise of the lat-
est word-sense disambiguation (WSD) systems. It
serves as a forum that brings together researchers in
WSD and domains that use WSD for various tasks.
It allows researchers to discuss modifications that
improve the performance of their systems, and an-
alyze combinations that are optimal.
Since the first edition of the SENSEVAL competi-
tions, a number of languages were added to the orig-
inal set of tasks. Having the WSD task prepared for
several languages provides the opportunity to test
the generality of WSD systems, and to detect dif-
ferences with respect to word senses in various lan-
guages.
This year we have proposed a Romanian WSD
task. Five teams with a total of seven systems have
tackled this task. We present in this paper the data
used and how it was obtained, and the performance
of the participating systems.
2 Open Mind Word Expert
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted to Roma-
nian1.
To overcome the current lack of sense tagged
data and the limitations imposed by the creation of
such data using trained lexicographers, the Open
Mind Word Expert system enables the collection
of semantically annotated corpora over the Web.
Sense tagged examples are collected using a Web-
based application that allows contributors to anno-
tate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are en-
couraged to select only one meaning per word, the
selection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
3 Sense inventory
For the Romanian WSD task, we have chosen a set
of words from three parts of speech - nouns, verbs
and adjectives. Table 1 presents the number of
words under each part of speech, and the average
number of senses for each class.
The senses were (manually) extracted from a Ro-
manian dictionary (Dict?ionarul EXplicativ al limbii
roma?ne - DEX (Coteanu et al, 1975)). These senses
1Romanian Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/romanian
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Number Avg senses Avg senses
Class words (fine) (coarse)
Nouns 25 8.92 4.92
Verbs 9 8.7 4.6
Adjectives 5 9 4
Total 39 8.875 4.725
Table 1: Sense inventory
and their dictionary definitions were incorporated in
the Open Mind Word Expert. For each annotation
task, the contributors could choose from this list of
39 words. For each chosen word, the system dis-
plays the associated senses, together with their def-
initions, and a short (1-4 words) description of the
sense. After the user gets familiarized with these
senses, the system displays each example sentence,
and the list of senses together with their short de-
scription, to facilitate the tagging process.
For the coarse grained WSD task, we had the op-
tion of using the grouping provided by the dictio-
nary. A manual analysis however showed that some
of the senses in the same group are quite distinguish-
able, while others that were separated were very
similar.
For example, for the word circulatie (roughly, cir-
culation). The following two senses are grouped in
the dictionary:
2a. movement, travel along a communication
line/way
2b. movement of the sap in plants or the cytoplasm
inside cells
Sense 2a fits better with sense 1 of circulation:
1. the event of moving about
while sense 2b fits better with sense 3:
3. movement or flow of a liquid, gas, etc. within a
circuit or pipe.
To obtain a better grouping, a linguist clustered
the similar senses for each word in our list of forty.
The average number of senses for each class is al-
most halved.
Notice that Romanian is a language that uses dia-
critics, and the the presence of diacritics may be cru-
cial for distinguishing between words. For example
peste without diacritics may mean fish or over. In
choosing the list of words for the Romanian WSD
task, we have tried to avoid such situations. Al-
though some of the words in the list do have dia-
critics, omitting them does not introduce new ambi-
guities.
4 Corpus
Examples are extracted from the ROCO corpus, a
400 million words corpus consisting of a collection
of Romanian newspapers collected on the Web over
a three years period (1999-2002).
The corpus was tokenized and part-of-speech
tagged using RACAI?s tools (Tufis, 1999). The to-
kenizer recognizes and adequately segments various
constructs: clitics, dates, abbreviations, multiword
expressions, proper nouns, etc. The tagging fol-
lowed the tiered tagging approach with the hidden
layer of tagging being taken care of by Thorsten
Brants? TNT (Brants, 2000). The upper level of
the tiered tagger removed from the assigned tags all
the attributes irrelevant for this WSD exercise. The
estimated accuracy of the part-of-speech tagging is
around 98%.
5 Sense Tagged Data
While several sense annotation schemes have been
previously proposed, including single or dual anno-
tations, or the ?tag until two agree? scheme used dur-
ing SENSEVAL-2, we decided to use a new scheme
and collect four tags per item, which allowed us
to conduct and compare inter-annotator agreement
evaluations for two-, three-, and four-way agree-
ment. The agreement rates are listed in Table 3.
The two-way agreement is very high ? above 90%
? and these are the items that we used to build the
annotated data set. Not surprisingly, four-way agree-
ment is reached for a significantly smaller number of
cases. While these items with four-way agreement
were not explicitly used in the current evaluation,
we believe that this represents a ?platinum standard?
data set with no precedent in the WSD research com-
munity, which may turn useful for a range of future
experiments (for bootstrapping, in particular).
Agreement type Total (%)
TOTAL ITEMS 11,532 100%
At least two agree 10,890 94.43%
At least three agree 8,192 71.03%
At least four agree 4,812 41.72%
Table 3: Inter-agreement rates for two-, three-, and
four-way agreement
Table 2 lists the target words selected for this task,
together with their most common English transla-
tions. For each word, we also list the number of
senses, as defined in the DEX sense inventory (col-
locations included), and the number of annotated ex-
amples made available to task participants.
Word Main English senses senses Train Test Word Main English senses senses Train Test
translation (fine) (coarse) size size translation (fine) (coarse) size size
NOUNS
ac needle 16 7 127 65 accent accent 5 3 172 87
actiune action 10 7 261 128 canal channel 6 5 134 66
circuit circuit 7 5 200 101 circulatie circulation 9 3 221 114
coroana crown 15 11 252 126 delfin doplhin 5 4 31 15
demonstratie demonstration 6 3 229 115 eruptie eruption 2 2 54 27
geniu genius 5 3 106 54 nucleu nucleus 7 5 64 33
opozitie opposition 12 7 266 134 perie brush 5 3 46 24
pictura painting 5 2 221 111 platforma platform 11 8 226 116
port port 7 3 219 108 problema problem 6 4 262 131
proces process 11 3 166 82 reactie reaction 7 6 261 131
stil style 14 4 199 101 timbru stamp 7 3 231 116
tip type 7 4 263 131 val wave 15 9 242 121
valoare value 23 9 251 125
VERBS
cistiga win 5 4 227 115 citi read 10 4 259 130
cobori descend 11 6 252 128 conduce drive 7 6 265 134
creste grow 14 6 209 103 desena draw 3 3 54 27
desface untie 11 5 115 58 fierbe boil 11 4 83 43
indulci sweeten 7 4 19 10
ADJECTIVES
incet slow 6 3 224 113 natural natural 12 5 242 123
neted smooth 7 3 34 17 oficial official 5 3 185 96
simplu simple 15 6 153 82
Table 2: Target words in the SENSEVAL-3 Romanian Lexical Sample task
Team System name Reference (this volume)
Babes-Bolyai University, Cluj-Napoca (1) ubb nbc ro (Csomai, 2004)
Babes-Bolyai University, Cluj-Napoca (2) UBB (Serban and Tatar, 2004)
Swarthmore College swat-romanian (Wicentowski et al, 2004a)
Swarthmore College / Hong Kong Polytechnic University swat-hk-romanian (Wicentowski et al, 2004b)
Hong Kong University of Science and Technology romanian-swat hk-bo
University of Maryland, College Park UMD SST6 (Cabezas et al, 2004)
University of Minnesota, Duluth Duluth-RomLex (Pedersen, 2004)
Table 4: Teams participating in the SENSEVAL-3 Romanian Word Sense Disambiguation task
In addition to sense annotated examples, partici-
pants have been also provided with a large number
of unlabeled examples. However, among all partici-
pating systems, only one system ? described in (Ser-
ban and Ta?tar 2004) ? attempted to integrate this ad-
ditional unlabeled data set into the learning process.
6 Participating Systems
Five teams participated in this word sense disam-
biguation task. Table 4 lists the names of the par-
ticipating systems, the corresponding institutions,
and references to papers in this volume that provide
detailed descriptions of the systems and additional
analysis of their results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of seven submissions was received for this task.
Table 5 shows all the submissions for each team, and
gives a brief description of their approaches.
7 Results and Discussion
Table 6 lists the results obtained by all participating
systems, and the baseline obtained using the ?most
frequent sense? (MFS) heuristic. The table lists pre-
cision and recall figures for both fine grained and
coarse grained scoring.
The performance of all systems is significantly
higher than the baseline, with the best system per-
forming at 72.7% (77.1%) for fine grained (coarse
grained) scoring, which represents a 35% (38%) er-
ror reduction with respect to the baseline.
The best system (romanian-swat hk-bo) relies on
a Maximum Entropy classifier with boosting, using
local context (neighboring words, lemmas, and their
part of speech), as well as bag-of-words features for
surrounding words.
Not surprisingly, several of the top perform-
ing systems are based on combinations of multi-
ple sclassifiers, which shows once again that voting
System Description
romanian-swat hk-bo Supervised learning using Maximum Entropy with boosting, using bag-of-words
and n-grams around the head word as features
swat-hk-romanian The swat-romanian and romanian-swat hk-bo systems combined with majority voting.
Duluth-RLSS An ensemble approach that takes a vote among three bagged decision trees,
based on unigrams, bigrams and co-occurrence features
swat-romanian Three classifiers: cosine similarity clustering, decision list, and Naive Bayes,
using bag-of-words and n-grams around the head word as features
combined with a majority voting scheme.
UMD SST6 Supervised learning using Support Vector Machines, using contextual features.
ubb nbc ro Supervised learning using a Naive Bayes learning scheme, and features extracted
using a bag-of-words approach.
UBB A k-NN memory-based learning approach, with bag-of-words features.
Table 5: Short description of the systems participating in the SENSEVAL-3 Romanian Word Sense Disam-
biguation task. All systems are supervised.
Fine grained Coarse grained
System P R P R
romanian-swat hk-bo 72.7% 72.7% 77.1% 77.1%
swat-hk-romanian 72.4% 72.4% 76.1% 76.1%
Duluth-RLSS 71.4% 71.4% 75.2% 75.2%
swat-romanian 71.0% 71.0% 74.9% 74.9%
UMD SST6 70.7% 70.7% 74.6% 74.6%
ubb nbc ro 71.0% 68.2% 75.0% 72.0%
UBB 67.1% 67.1% 72.2% 72.2%
Baseline (MFS) 58.4% 58.4% 62.9% 62.9%
Table 6: System results on the Romanian Word Sense Disambiguation task.
schemes that combine several learning algorithms
outperform the accuracy of individual classifiers.
8 Conclusion
A Romanian Word Sense Disambiguation task
was organized as part of the SENSEVAL-3 eval-
uation exercise. In this paper, we presented
the task definition, and resources involved, and
shortly described the participating systems. The
task drew the participation of five teams, and in-
cluded seven different systems. The sense an-
notated data used in this exercise is available
online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Romanian Open Mind Word Expert project, mak-
ing this task possible. Special thanks to Bog-
dan Harhata, from the Institute of Linguistics Cluj-
Napoca, for building a coarse grained sense map.
We are also grateful to all the participants in this
task, for their hard work and involvement in this
evaluation exercise. Without them, all these com-
parative analyses would not be possible.
References
T. Brants. 2000. Tnt - a statistical part-of-speech
tagger. In Proceedings of the 6th Applied NLP
Conference, ANLP-2000, Seattle, WA, May.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Ex-
pert. In Proceedings of the Workshop on ?Word
Sense Disambiguation: Recent Successes and Fu-
ture Directions?, ACL 2002, Philadelphia, July.
I. Coteanu, L. Seche, M. Seche, A. Burnei,
E. Ciobanu, E. Contras?, Z. Cret?a, V. Hristea,
L. Mares?, E. St??ngaciu, Z. S?tefa?nescu, T. T?ugulea,
I. Vulpescu, and T. Hristea. 1975. Dict?ionarul
Explicativ al Limbii Roma?ne. Editura Academiei
Republicii Socialiste Roma?nia.
D. Tufis. 1999. Tiered tagging and combined classi-
fiers. In Text, Speech and Dialogue, Lecture Notes
in Artificial Intelligence.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 107?110,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
 
Combined word alignments  
 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan ?tef?nescu 
Romanian Academy Institute for Artificial Intelligence 
13, ?13 Septembrie?, 74311, Bucharest 5, Romania 
{tufis, radu, alceusu, danstef}@racai.ro 
 
 
Abstract
 
We briefly describe a word alignment system 
that combines two different methods in bitext 
correspondences identification. The first one is 
a hypotheses testing approach (Gale and 
Church, 1991; Melamed, 2001; Tufi? 2002) 
while the second one is closer to a model 
estimating approach (Brown et al, 1993; Och 
and Ney, 2000). We show that combining the 
two aligners the results are significantly 
improved as compared to each individual 
aligner. 
 
Introduction 
In (Tufi?, 2002) we described a translation equivalence 
extraction program called TREQ the development of 
which was twofold motivated: to help enriching the 
synsets of the Romanian wordnet (Tufi? et al 2004a) 
with new literals based on bilingual corpora evidence 
and to check the interlingual alignment of our wordnet 
against the Princeton Wordnet. The translation 
equivalence extractor has been also incorporated into a 
WSD system (Tufi? et al, 2004b) part of a semantic 
web annotation platform. It also constituted the 
backbone of our TREQ-AL word aligner which 
successfully participated in the previous HLT-NAACL 
2003 Shared Task1 on word alignment for Romanian-
English parallel texts. A detailed description of 
TREQ&TREQ-AL is given in (Tufi? et al 2003b) and it 
will be very shortly overviewed. 
A quite different approach from our hypotheses 
testing implemented in the TREQ-AL aligner is taken 
by the model-estimating aligners, most of them relying 
on the IBM models (1 to 5) described in the (Brown et 
al. 1993) seminal paper. The first wide-spread and 
publicly available implementation of the IBM models 
was the GIZA program, which itself was part of the 
SMT toolkit EGYPT (Al-Onaizan et al, 1999). GIZA 
has been superseded by its recent extension GIZA++ 
(Och and Ney, 2000, 2003) publicly available2. We used 
the translation probabilities generated by GIZA++ for 
implementing a second aligner, MEBA, described in a 
                                                 
1 http://www.cs.unt.edu/~rada/wpt/index.html#shared  
2 http://www.fjoch.com/GIZA++.2003-09-30.tar.gz 
little more details in a subsequent section. The 
alignments produced by MEBA were compared to the 
ones produced by TREQ-AL. We used for comparison 
the Gold Standard3 annotation from the HLT-NAACL 
2003 Shared Task. In order to combine the two aligners 
we had to check whether their accuracy was comparable 
and that when they are wrong the set of mistakes made 
by one aligner is not a proper set of the errors made by 
the second one. The first check was performed by using 
McNamer?s test  (Dieterich, 1998) and for the second 
we used Brill &Wu test (Brill, Wu, 1998). Both tests 
confirmed that the conditions for combining were 
ensured so, we built the combiner.  
The Combined Word Aligner, COWAL, is a 
wrapper of the two aligners (TREQ-AL and MEBA) 
ensuring the pre- and post-processing. It is 
complemented by a graphical user interface that allows 
for the visualisation of the alignments (intermediary and 
the final ones) as well as for their editing. We should 
note that the corrections made by the user are stored by 
COWAL as positive and negative examples for word 
dependencies (in the monolingual context) and 
translation equivalencies (in the bilingual context). In 
the current version the editorial logs are used by the 
human developers but we plan to further extend 
COWAL for automatic learning from this extremely 
valuable kind of data.    
 
The bitext processing  
The two base aligners and their combination use the 
same format for the input data and provide the 
alignments in the same format. The input format is 
obtained from two raw texts which represent reciprocal 
translations. If not already sentence aligned, the two 
texts are aligned. In the shared task this step was not 
necessary since both the training data and evaluation 
data were provided in the sentence aligned format.  
The texts in each language are then tokenized with 
the MULTEXT multilingual tokenizer4. The tokenizer is 
a finite state automaton using language specific 
                                                 
3 We noticed in the Gold Standard two sentences where 
alignments were wrongly shifted by one position (due to an 
unprintable character) and we corrected them.  
4 http://aune.lpl.univ-aix.fr:16080/projects/multext/MtSeg/  
107
 resources. It recognizes several compounds (phrasal 
verbs, idioms, dates) and split contrasted or cliticized 
constructions. This tokenization considerably differs 
from the one prescribed by the Shared Task where a 
token is any character string delimited by a blank or a 
punctuation sign (which itself is considered a token).   
Since our processing tools (especially the tokeniser) 
were built with a different segmentation strategy in 
mind, we generated the alignments based on our own 
tokenization and, at the end, we ?re-tokenised? the text 
according to original evaluation data (and consequently 
re-index) all the linking pairs. After tokenization, both 
texts are tagged and lemmatized.  We used in-house 
language models and lemmatizers and the Brants?s TnT 
tagger5. For both English and Romanian we used 
MULTEXT-EAST6 compliant tagsets. With different 
tags, a tagset mapping table becomes an obligatory 
external resource. Although, more often than not, the 
translation equivalents have the same part-of speech, 
relying on such a restriction would seriously affect the 
alignment recall. However, when the translation 
equivalents have different parts of speech, this 
difference is not arbitrary.  During the training phase we 
estimated bilingual POS affinities:{p(POSmRO| POSnEN)} 
and {p(POSnEN|POSmRO)}. POS affinities were used as 
one of the information sources in dealing with 
competitive alignments.  
The next preprocessing step is represented by a 
rather primitive form of sentence chunking in both 
languages. They roughly correspond to (non-recursive) 
noun phrases, adjectival phrases, prepositional phrases 
and verb complexes (analytical realization of tense, 
aspect mood and diathesis and phrasal verbs).  The 
?chunks? are recognized by a set of regular expressions 
defined over the tagsets. Finally, the bitext is assembled 
as an XML document (XCES-Align-ana format), as 
used in the MULTEXT-EAST corpus, which is the 
standard input for most of our tools, including COWAL 
alignment platform. 
 
The three aligners  
TREQ-AL generates translation equivalence hypotheses 
for the pairs of words (one for each language in the 
parallel corpus) which have been observed occurring in 
aligned sentences more than expected by chance. The 
hypotheses are filtered by a loglikelihood score 
threshold. Several heuristics (string similarity-cognates, 
POS affinities and alignments locality7) are used in a 
                                                 
5 http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf  
6 http://nl.ijs.si/ME/V2/  
7 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
competitive linking manner (Melamed, 2001) to make 
the final decision on the most likely translation 
equivalents. Given that, initially, this program was 
designed for extracting translation equivalents for the 
alignment of the Romanian wordnet to the Princeton 
wordnet, it deals only with one to one mappings. To 
cope with the many to many mappings (especially for 
functional words alignment), the earlier version of the 
translation equivalence extractor encoded some general 
rules assumed to be valid over a large set of natural 
languages such as: auxiliaries and verbal particles 
(infinitive, subjunctive, aspectual and temporal) are 
related to the closest main verb, determiners (articles, 
pronominal adjectives, quantifiers) are related to the 
closest nominal category (noun or pronoun). Currently 
this part of the TREQ-AL code became redundant 
because the chunking module mentioned before does 
the same job in a more general and flexible way.  
MEBA is an iterative algorithm which uses the 
translation probabilities, distorsions and POS-affinities 
generated by GIZA++ and takes advantage of all 
preprocessing phases mentioned in the previous section. 
In each step are aligned different categories of tokens 
(content words, named entities, functional words) in 
decreasing order of statistical evidence. The score of a 
link is computed by a linear function of 7 parameters? 
scores: translation probability, POS affinity, string 
similarity, alignments locality (both strict and weaker 
versions) distortions and the entropy of the translation 
equivalents. For all these parameters, in each processing 
step, we empirically set minimal thresholds and various 
weights. The tokens considered for the computing 
translation probabilities are the lemmas trailed by the 
grammatical categories (eg. plane_N, plane_V 
plane_A). This way we aimed at avoiding data 
sparseness and filtering noisy data. For highly 
inflectional languages (as Romanian is) the use of 
lemmas instead of word occurrences contributes 
significantly to the data sparseness reduction. For 
languages with weak inflectional character (as English 
is) the POS trailing contributes especially to the filtering 
the search space. Each processing step is controlled by 
above mentioned parameters, the weights and thresholds 
of which vary from step to step (even the order of the 
processing steps is one of the possible parameters). 
The first alignment step builds only links with a 
high level of certainty (that is cognates, pairs of high 
translation probability and high POS affinity). The 
grammatical categories which are considered in this step 
are user controlled (usually nouns, adjectives or non-
auxiliary verbs and which have the fewest competitive 
translations). The next processing steps try to align 
                                                                             
requires that all alignment links starting from a chunk, in the 
one language end in a chunk in the other language. This 
restricted form of locality is relevant for related languages.  
108
 content words (open class categories) as confidently as 
possible, following the alignments in previous steps as 
anchor points. In all steps the candidates are considered 
if and only if they meet the minimal threshold 
restrictions. If the input bitext is chunked, the strict 
alignment locality heuristics is very effective to 
determine the correct alignment even for unseen pairs of 
words (or for which the translation equivalence 
probability is below the considered threshold). When 
the pre-chunking of the parallel texts is not available, 
MEBA uses the weaker form of the locality heuristics 
by analyzing the alignments already existing in a 
window of N tokens centered on the focused token. The 
window size is variable, proportional to the sentence 
length. For all alignments in the window, an average 
displacement is computed and, among the competing 
alignments, preference will be given to the links with 
displacement values closer to the average one.  
The functional words and punctuation are processed 
in the last step and their alignments are guided by the 
POS-affinities and alignment locality heuristics. If none 
of the alignment clues or their combination (Tiedemann, 
2003) is strong enough, the functional words are 
automatically aligned with the word(s) their governor is 
aligned to. The governor is chunk-based defined: it is 
the content word of a chunk (if there are more content 
words in a chunk, then the governor is the grammatical 
head). If the chunking is not available, the closest 
content word is selected as the governor. Proximity is 
checked to the left or to the right according to the 
frequencies of the POS-ngram containing the current 
functional word.  
We should mention that the probabilities computed 
during the training phase are not re-estimated for each 
run-time processing step. At run-time only the weights 
and thresholds change from step to step.  
COWAL, the combined aligner takes advantage of the 
alignments independently provided by TREQ-AL and 
MEBA. The simplest combination method consists in 
computing either the union (high recall, low precision), 
or the intersection (lower recall, higher precision) of the 
independent alignments. We evaluated both these 
simple methods of combination and found that the best 
F-measure was provided by the union-based 
combination. Although for the shared task we submitted 
the union-based combined alignment (Baseline 
COWAL, see Table 1), there are various ways to 
improve it. We discuss three cases where improvement 
is possible (C1, C2 and C3, see below) and which were 
evaluated after the submission deadline. The results of 
this (unofficial) evaluation are summarized in Table 1 
by the f-COWAL line. These cases refer to competing 
links that appeared after the union of the independent 
alignments. The conflicts resolution is based on the 
(weak) locality and distortion heuristics discussed 
before. The currently identified competing links are 
only those for which the following conditions apply: 
C1) if one aligner found for a word W a non-null 
alignment and the other aligner generated for the 
same word W a null link, then the baseline alignment 
contains an impossible situation: the token W is 
recorded both as translated and not-translated in the 
other language. The translation probabilities, POS 
affinity and the relative displacement of the tokens in 
the non-null candidates were the strongest decision 
criteria. We found that in about 60% of the cases the 
null alignments were mistaken. So, for the time being, 
we simply eliminated the null competing alignments 
(this should be addressed in a more principled way by 
the future version of the combiner).  
C2) long distant competing links; this case appears 
when one aligner found for the word Ws the link to 
the target word Wtm, the other aligner found for Ws 
the target Wtn, and the distance between Wtm and 
Wtn, is more than 3 words (in a future version this 
maximum distance will be a dynamic parameter, 
depending on the sentence length and the POS of 
Ws). 
C3) competing links to the same target(s) of a word 
occurring several times in the same sentence; 
consider, for example, the Romanian fragment:  
     ??la1 Neptun, la2 Orastie si la3 Afumati, ?   
     which in English is translated by the next segment: 
     ??in Neptun, Orastie and Afumati? 
In spite of the gold standard considering that all three 
occurrences of the preposition ?la? in Romanian (la1, 
la2 ,la3) are aligned to the same word in English (?in?), 
the filtering, in this case, licensed only the alignment 
?la1 <-> in?. We consider that this filtered alignment 
is correct, since omitting ?la2? and ?la3? does not alter 
the syntactic correctness of the Romanian text, and 
also because the insertion in the English fragment of 
the preposition ?in? before ?Orastie? and before 
?Afumati? wouldn?t alter the grammaticality of the 
English fragment. Since both repetitions and 
omissions are optional, we consider that only the first 
occurrence of the preposition (?la1?) is translated in 
English, while the others are omitted. 
Another possible improvement (not implemented yet) 
was revealed by observing that the final result contained 
several incomplete n-m (phrasal) alignments. It is likely 
that even an elementary n-gram analysis (both sides of 
the bitext) would bring valuable evidence for improving 
the phrasal alignments.  
 
Post-processing  
As said in the second section, our tokenization was 
different from the tokenization in the training and test 
data. To comply with the evaluation protocol, we had to 
re-tokenize the aligned text and re-compute the indexes 
109
 of the links. Some multi-word expressions recognized 
by the tokenizer as one token, such as dates (25 
ianuarie, 2001), compound prepositions (de la, p?n? 
la), conjunctions (pentru ca, de c?nd, p?n? c?nd) or 
adverbs (de jur ?mprejur, ?n fa?a) as well as the hyphen 
separated nominal compounds (mass-media, prim-
ministru) were split, their positions were re-indexed and 
the initial one link of a split compound was replaced 
with the set obtained by adding one link for each 
constituent of the compound to the target English word. 
The same hold for the other way around. Therefore if 
two multiword expressions were initially found to be 
translation equivalents (one alignment link) after the 
post-processing number of  generated links became 
N*M, where N represented the number of words in the 
first language compound and M the number of words in 
the second language compound.   
Evaluation and conclusions 
Neither TREQ-AL nor MEBA needs an a priori 
bilingual dictionary, as this will be automatically 
extracted by the TREQ or GIZA++. We made 
evaluation of the individual alignments in both 
experimental settings: without a startup bilingual 
lexicon and with an initial mid-sized bilingual lexicon. 
Surprisingly enough, we found that while the 
performance of TREQ-AL increases a little bit (approx. 
1% increase of the F-measure) MEBA is doing better 
without an additional lexicon. So, in the evaluation 
below MEBA uses only the training data vocabulary.  
 
Aligner Precision Recall F-
meas. 
AER 
TREQ-AL 81.71 60.57 69.57 30.43 
MEBA 82.85 60.41 69.87 30.13 
Baseline 
(union)COWAL 
70.84 76.67 73.64 26.36 
f-COWAL 
(H1+H2+H3) 
87.17 70.25 77.80 22.20 
         Table 1. Evaluation results against the official GS 
After the release of the official Gold Standard we 
noticed and corrected some obvious errors and also 
removed the controversial links of the type c) discussed 
in the previous section. The evaluations against this new 
?Gold Standard? showed, on average, 3.5% better 
figures (precision, recall, F-measure and AER) for the 
individual aligners, while for the combined classifiers, 
the performance scores were about 4% better. 
MEBA is very sensitive to the values of the 
parameters which control its behavior. Currently they 
are set according to the developers? intuition and after 
the analysis of the results from several trials. Since this 
activity is pretty time consuming (human analysis plus 
re-training might take a couple of hours) we plan to 
extend MEBA with a supervised learning module, 
which would automatically determine the ?optimal? 
parameters (thresholds and weights) values. 
 
References 
Al-Onaizan, Y., Curin, J., Jahr, M., Knight K., Lafferty, J., 
Melamed, D., Och, F. J., Purdy, D., Smith, N.A., 
Yarowsky, D. (1999) : Statistical Machine 
Translation, Final Report, JHU Workshop, 42 pages 
Brill, E., and Wu, J. (1998). ?Classifier Combination for 
Improved Lexical Disambiguation? In Proceedings of 
COLING-ACL?98  Montreal, Canada, 191-195 
Brown, P. F., Della Pietra, S.A.,  Della Pietra, V. J., 
Mercer, R. L.(1993) ?The mathematics of statistical 
machine translation: Parameter estimation?. 
Computational Linguistics, 19(2) pp. 263?311. 
Dietterich, T. G., (1998). ?Approximate Statistical Tests 
for Comparing Supervised Classification Learning 
Algorithms?. Neural Computation, 10 (7) 1895-1924. 
Gale, W.A. and Church, K.W. (1991). ?Identifying word 
correspondences in parallel texts?. Proceedings of the 
Fourth DARPA Workshop on Speech and Natural 
Language. Asilomar, CA, pp. 152?157. 
Melamed, D. (2001). Empirical Methods for Exploiting 
Parallel Texts. Cambridge, MA: MIT Press. 
Och, F.J., Ney, H. (2003) "A Systematic Comparison of 
Various Statistical Alignment Models", Computa-
tional Linguistics, 29(1), pp. 19-51 
Och, F.J., Ney, H.(2000) "Improved Statistical Alignment 
Models". Proceedings of the 38th ACL, Hongkong,  
pp. 440-447 
Tiedemann, J. (2003) ?Combining clues for word 
alignment?. In Proceedings of the 10th EACL, 
Budapest, pp. 339?346 
Tufi?, D.(2002) ?A cheap and fast way to build useful 
translation lexicons?. Proceedings of COLING2002, 
Taipei, pp. 1030-1036. 
Tufi?, D., Barbu, A.M., Ion R (2003).: ?TREQ-AL: A 
word-alignment system with limited language 
resources?, Proceedings of the NAACL 2003 
Workshop on Building and Using Parallel Texts; 
Romanian-English Shared Task, Edmonton, pp. 36-39 
Tufi?, D., Ion, R., Ide, N.(2004a): Fine-Grained Word 
Sense Disambiguation Based on Parallel Corpora, 
Word Alignment, Word Clustering and Aligned 
Wordnets. Proceedings of COLING2004, Geneva, pp. 
1312-1318 
Tufis, D., Barbu, E., Mititelu, V., Ion, R., Bozianu, 
L.(2004b): ?The Romanian Wordnet?.  In Romanian 
Journal on Information Science and Technology, Dan 
Tufi? (ed.) Special Issue on BalkaNet, Romanian 
Academy, 7(2-3), pp. 105-122.  
110
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 282?287,
Prague, June 2007. c?2007 Association for Computational Linguistics
RACAI: Meaning Affinity Models
Radu Ion
Institute for Artificial Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania
radu@racai.ro
Dan Tufis?
Institute for Artificial Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania
tufis@racai.ro
Abstract
This article introduces an unsupervised word
sense disambiguation algorithm that is in-
spired by the lexical attraction models of
Yuret (1998). It is based on the assump-
tion that the meanings of the words that
form a sentence can be best assigned by con-
structing an interpretation of the whole sen-
tence. This interpretation is facilitated by
a dependency-like context specification of
a content word within the sentence. Thus,
finding the context words of a target word
is a matter of finding a pseudo-syntactic de-
pendency analysis of the sentence, called a
linkage.
1 Introduction
Word Sense Disambiguation (WSD) is a difficult
Natural Language Processing task which requires
that for every content word (noun, adjective, verb
or adverb) the appropriate meaning is automatically
selected from the available sense inventory1. Tradi-
tionally, the WSD algorithms are divided into two
rough classes: supervised and unsupervised. The
supervised paradigm relies on sense annotated cor-
pora, with the assumption that neighbouring disam-
biguate words provide a strongly discriminating and
generalizable context representation for the meaning
of a target word. Obviously, this approach suffers
from the knowledge acquisition bottleneck in that
1In principle, one can select meanings for any part of speech
that is represented into the semantic lexicon (prepositions for
instance) but the content words disambiguation is the de facto
standard.
there will never be enough training data to ensure
a scalable result of such algorithms. The unsuper-
vised alternative to WSD tries to alleviate the burden
of manually sense tagging the corpora, by employ-
ing algorithms that use different knowledge sources
to determine the correct meaning in context. In fact,
the ?knowledge source usage? is another way to dis-
tinguish among the WSD methods. Such methods
call upon further processing of the text to be dis-
ambiguated such as parsing and/or use handcrafted,
semantically rich sense inventories such as Word-
Net (Fellbaum, 1998). WSD methods in this cate-
gory range from the very simple ranking based on
counting the number of words occurring in both the
target word?s context and its sense definitions in a
reference dictionary (Lesk, 1986) to the more elabo-
rated approaches using the semantic lexicon?s tax-
onomies, (shallow) parsing, collocation discovery
etc. (Stevenson and Wilks, 2001).
One of the central issues of any WSD implemen-
tation is given by the context representation. The
standard principle that is applied when trying to dis-
ambiguate the meaning of a word is that the same
word in similar contexts should have the same mean-
ing. By and large, the context of a target word is ma-
terialized by a collection of features among which
are: the collocates of the target word, the part-of-
speech (POS) of the target word, ?k words sur-
rounding the target word and/or their POSes and so
on. More often than not, the contexts similarity is es-
timated by the distance in the feature vector space.
Lin (1997) defines the local context of a target word
by the collection of syntactic dependencies in which
the word takes part. According to this notion of con-
282
text, Lin assumes that two different words are likely
to have similar meanings if they occur in identical
local contexts.
What we will attempt here is to combine the two
views of context similarity/identity versus meaning
similarity/identity by using a dependency-like repre-
sentation of the context as a lexical attraction model.
More specifically, we will not consider any feature
of the context and will try to maximize a meaning at-
traction function over all linked words of a sentence.
In section 2 we will describe SynWSD, an unsuper-
vised, knowledge-based WSD algorithm and in sec-
tions 3 and 4 we will present the application of Syn-
WSD to two of SEMEVAL-2007 ?all words? tasks:
English Coarse-Grained and English Fine-Grained.
Finally, with section 5 we will conclude the article.
2 SynWSD
The syntactic context representation is not new in
the realm of WSD algorithms. For instance, Lin
(1997) used the dependency relations of the target
word to specify its context and Stetina (1998) ex-
tracted head-modifier relations to obtain the context
pairs for each word of interest from a constituents
tree. The syntactic representation of the context of a
target word has one main advantage over the collec-
tion of features method: the target word is related
only with the relevant word(s) in its window and
not with all the words and thus, many noisy cooc-
currences are eliminated. Mel?c?uk (1988) further
strengthens the intuition of a syntactic context rep-
resentation with his Meaning Text Model in which
there is a deterministic translation from the surface
syntactic dependency realization of the sentence to
its deep syntactic one and therefore to the semantic
representation.
To use a syntactic analysis as a context representa-
tion, one needs a parser which will supply the WSD
algorithm with the required analysis. Because we
have intended to develop a language independent
WSD algorithm and because there is no available,
reliable dependency parser for Romanian, we have
backed off to a simpler, easier to obtain dependency-
like representation of a sentence: a slightly modified
version of the lexical attraction models of (Yuret,
1998).
2.1 LexPar
Lexical attraction is viewed as the likelihood of a
syntactic dependency relation between two words of
a sentence and is measured by the pointwise mutual
information between them. Yuret (1998) shows that
the search for the lowest entropy lexical attraction
model leads to the unsupervised discovery of undi-
rected dependency relations or links.
LexPar (Ion and Barbu Mititelu, 2006) is a link
analyzer (a linker) which generates a connected,
undirected, acyclic and planar graph of an input sen-
tence in which the nodes are the words of the sen-
tence and the edges are the highest lexical attracted
dependency-like relations. This program is simi-
lar to the suboptimal one presented in (Yuret, 1998)
with the following main differences:
? the policy of checking pairs of words to be re-
lated is based on the assumption that most of
the syntactic relations2 are formed between ad-
jacent words and then between adjacent groups
of linked words;
? it operates on POS-tagged and lemmatized cor-
pora and attempts to improve parameter estima-
tion by using both lemmas and POS tags. The
score of a link is defined as the weighted sum
of the pointwise mutual information of the lem-
mas and of the POS tags, thus coping even with
the unknown lemmas;
? it uses a rule filter that will deny the formation
of certain links based on the POSes of the can-
didate words. For instance, neither the relation
between a determiner and an adverb nor the re-
lation between a singular determiner and a plu-
ral noun should be permitted;
In Figure 1 we have an example of a XML en-
coded, LexPar processed sentence. The head at-
tribute of the w tag specifies the position of the head
word of the tagged word. Because LexPar considers
non-directed dependency relations, for the purposes
of XML encoding3, the first word of every sentence
2At least for our languages of interest, namely English and
Romanian.
3The encoding of the morpho-syntactic descriptors (MSD) is
MULTEXT-East compliant (http://nl.ijs.si/ME/V3/
msd/00README.txt).
283
Figure 1: The XML representation of a LexPar pro-
cessed sentence.
(position 0) is always the root of the syntactic de-
pendency tree, its dependents are its children nodes,
and so on while we recursively build the tree from
the LexPar result.
We have chosen not to give a detailed presentation
of LexPar here (the reader is directed towards (Yuret,
1998; Ion and Barbu Mititelu, 2006)) and instead,
to briefly explain how the linkage in Figure 1 was
obtained. The processor begins by inspecting a list
G of groups of linked words which initially contains
the positions of each of the words in the sentence:
G0 = {(0), (1), (2), (3), (4), (5)}
The linking policy is trying to link words in the
groups (0) and (1) or (1) and (2). The syntactic
rule filter says that auxiliary verbs (Va) can only
be linked with main verbs (Vm) and so one link is
formed and the list of groups becomes:
G1 = {(0), (?1, 2?), (3), (4), (5)}
Next, the processor must decide linking the groups
(?1, 2?) and (3) or (3) and (4) but the syntactic rule
filter is denying any link from positions 1 or 2 to 3
(no links from any kind of verb V to any kind of a
determiner D) or from 3 to 4 (no link from a nega-
tive determiner Dz3 to a qualificative adjective Af).
Continuing this way, the progress of G list is as fol-
lows:
G1 = {(0), (?1, 2?), (3), (?4, 5?)}
G2 = {(?0, 2?, ?1, 2?), (?3, 5?, ?4, 5?)}
G3 = {(?0, 2?, ?1, 2?, ?2, 5?, ?3, 5?, ?4, 5?)}
So in 3 steps G3 contains a single group of linked
words namely the linkage of the sentence.
2.2 Meaning Affinity Models
If the lexical attraction models are geared towards
the discovery of the most probable syntactic rela-
tions of a sentence, we can naturally generalize this
idea to construct a class of models that will find a
combination of meanings that maximizes a certain
meaning attraction function over a linkage of a sen-
tence. We call this class of models the meaning
affinity models.
Optimizing meaning affinity over a syntactic rep-
resentation of a sentence has been tried in (Stetina et
al., 1998; Horbovanu, 2002). SynWSD (Ion, 2007)
is an implementation with two phases of the mean-
ing affinity concept: training which takes as input
a corpus with LexPar linked sentences (of the type
shown in Figure 1) and outputs a table M of mean-
ing co-occurrence frequencies and disambiguation
of a LexPar linked sentence S, based on the counts
in table M from the previous phase.
Before continuing with the descriptions of these
phases, we will introduce the notations that we will
use throughout this section:
? A n-word sentence is represented by a vec-
tor S of n elements, each of them contain-
ing a triple ?wordform, lemma,POS?. For in-
stance, the first element from S in Figure 1 is
S[0] = ?We,we, Pp1?pn?;
? L is the LexPar linkage of S, and is also a vec-
tor containing pairs of positions ?i, j? in S that
are related, where 0 ? i < j < n;
? lem(S, i) and pos(S, i) are two functions that
give the lemma and the POS of the position i in
S, 0 ? i < n.
The training phase is responsible for collecting
meaning co-occurrence counts. It simply iterates
over each sentence S of the training corpus and for
every link L[k] of the form ?a, b? from its linkage,
does the following (K stores the total number of
recorded meaning pairs):
1. extracts the sets of meanings Ia and Ib corre-
sponding to the lemma lem(S, a) with the POS
pos(S, a) and to the lemma lem(S, b) with the
POS pos(S, b) from the sense inventory4;
4If the lemma does not appear in the sense inventory or its
284
2. increases by 1 the M table frequencies for ev-
ery pair of the cartesian product Ia ? Ib. For
every meaning m ? Ia, the frequency of the
special pair ?m, ?? is increased with |Ib|. Simi-
larly, the pair ??,m? frequency is also increased
with |Ia| for m ? Ib);
3. K ? K + |Ia ? Ib|.
We have used the Princeton WordNet (Fellbaum,
1998), version 2.0 (PWN20) as our sense inventory
and the mappings from its synsets to the SUMO
ontology concepts (Niles and Pease, 2003) and to
the IRST domains (Magnini and Cavaglia, 2000).
Thus we have tree different sense inventories each
with a different granularity. For instance, the noun
homeless has 2 senses in PWN20, its first sense
(?someone with no housing?) being mapped onto the
more general Human SUMO concept and onto the
person IRST domain. The second sense of the
same noun is ?people who are homeless? which cor-
responds to the same SUMO concept and to a differ-
ent IRST domain (factotum).
In order to reduce the number of recorded pairs
in the case of PWN20 meanings (the finest granular-
ity available) and to obtain reliable counts, we have
modified the step 1 of the training phase in the fol-
lowing manner:
? if we are dealing with nouns or verbs, for every
meaning mi of the lemma, extract the upper-
most hypernym meaning which does not sub-
sume any other meaning of the same lemma;
? if we are dealing with adjectives, for every
meaning mi of the lemma, extract the meaning
of the head adjective if mi is part of a cluster;
? if we are dealing with adverbs, for every mean-
ing mi of the lemma, return mi (no generaliza-
tion is made available by the sense inventory in
this case).
This generalization procedure will be reversed at the
time of disambiguation as will be explained shortly.
POS does not give a noun, verb, adjective or adverb, the lemma
itself is returned as the sole meaning because in the disambigua-
tion phase we need a meaning for every word of the sentence,
be it content word or otherwise.
Figure 2: The tree representation of the sentence in
Figure 1.
The disambiguation phase takes care of finding
the best interpretation of a linked sentence based on
the frequency table M . For a test sentence S, with
the linkage L, the procedure goes as follows:
1. produce a proper tree T of positions from L by
taking position 0 as the root of the tree. Then,
for every link that contains 0 make the other po-
sition in the link a child of 0 and then, in a re-
cursive manner, apply the same process for all
children of 0. For instance, the tree for Figure
1 if depicted in Figure 2;
2. construct a vector P of sentence positions vis-
ited during a depth-first traversal of the T tree.
The vector of sentence positions for Figure 2 is
P = (0, 2, 5, 3, 5, 4, 5, 2, 1, 2, 0)
3. construct a meaning vector V of the same
length as P . V [i] contains the list of mean-
ings of the lemma lem(S, P [i]) with the POS
pos(S, P [i]). If the sense inventory is PWN20,
every meaning from the list is generalized as
described above;
4. finally, apply the Viterbi algorithm ((Viterbi,
1967)) on the V vector and extract the path (se-
quence of meanings) which maximizes mean-
ing affinity.
Each state transition is scored according to a
meaning affinity function. In our experiments we
have considered three meaning affinity functions. If
K is the total number of meaning pairs and if m1
285
and m2 are two meanings from adjacent V positions
for which f(m1,m2) is the pair frequency extracted
from M , the functions are:
1. DICE:
dice(m1,m2) =
2f(m1,m2)+2f(m2,m1)
f(m1,?)+f(?,m1)+f(m2,?)+f(?,m2)
2. Pointwise mutual information:
mi(m1,m2) =
log Kf(m1,m2)+Kf(m2,m1)(f(m1,?)+f(?,m1))(f(m2,?)+f(?,m2))
3. Log-Likelihood, ll(m1,m2) which is com-
puted as in (Moore, 2004).
After the Viterbi path (best path) has been calcu-
lated, every state (meaning) from V [i] (0 ? i < |V |)
along this path is added to a finalD vector. When the
PWN20 sense inventory is used, the reverse of the
generalization procedure is applied to each meaning
recorded in D, thus coming back to the meanings
of the words of S. Please note that an entry in D
may contain more than one meaning especially in
the case of PWN20 meanings for which there was
not enough training data.
3 SEMEVAL-2007 Task #7:
Coarse-grained English All-Words
LexPar and SynWSD were trained on an 1 million
words corpus comprising the George Orwell?s 1984
novel and the SemCor corpus (Miller et al, 1993).
Both texts have been POS-tagged (with MULTEXT-
East compliant POS tags) and lemmatized and the
result was carefully checked by human judges to en-
sure a correct annotation.
SynWSD was run with all the meaning attraction
functions (dice, mi and ll) for all the sense in-
ventories (PWN20, SUMO categories and IRST do-
mains) and a combined result was submitted to the
task organizers. The combined result was prepared
in the following way:
1. for each sense inventory and for each token
identifier, get the union of the meanings for
each run (dice, mi and ll);
2. for each token identifier with its three union
sets of PWN20 meanings, SUMO categories
and IRST domains:
(a) for each PWN20 meaningmi in the union,
if there is a SUMO category that maps
onto it, increase mi?s weight by 1;
(b) for each PWN20 meaningmi in the union,
if there is a IRST domain that maps onto
it, increase mi?s weight by 1;
(c) from the set of weighted PWN20 mean-
ings, select the subset C that best over-
laps with a cluster. That is, the intersec-
tion between the subset and the cluster has
a maximal number of meanings for which
the sum of weights is also the greatest;
(d) output the lowest numbered meaning inC.
With this combination, the official F-measure of
SynWSD is 0.65712 which places it into the 11th
position out of 16 competing systems5.
Another possible combination is that of the inter-
section which is obtained with the exact same steps
as above, replacing the union operation with the in-
tersection. When the PWN20 meanings set is void,
we can make use of the most frequent sense (MFS)
backoff strategy thus selecting the MFS of the cur-
rent test word from PWN20. Working with the of-
ficial key file and scoring software, the intersection
combination with MFS backoff gives an F-measure
of 0.78713 corresponding to the 6th best result. The
same combination method but without MFS backoff
achieves a precision of 0.80559 but at the cost of a
very low F-measure (0.41492).
4 SEMEVAL-2007 Task #17: English
All-Words
For this task, LexPar and SynWSD were further
trained on a 12 million POS tagged and lemmatized
balanced corpus6. The run that was submitted was
the intersection combination with the MFS backoff
strategy which obtained an F-measure of 0.527. This
score puts our algorithm on the 8th position out of
14 competing systems. For the union combinator
5Precision = Recall = F-measure. In what follows, mention-
ing only the F-measure means that this equality holds.
6A random subset of the BNC (http://www.natcorp.
ox.ac.uk/).
286
(the MFS backoff strategy is not applicable), the F-
measure decreases to 0.445 (10th place). Finally, if
we train SynWSD only on corpora from task#7, the
union combinator leads to an F-measure of 0.344.
5 Conclusions
SynWSD is a knowledge-based, unsupervised WSD
algorithm that uses a dependency-like analysis of a
sentence as a uniform context representation. It is a
language independent algorithm that doesn?t require
any feature selection.
Our system can be improved in several ways.
First, one can modify the generalization procedure
in the case of PWN20 meanings in the sense of se-
lecting a fixed set of top level hypernyms. The size
of this set will directly affect the quality of meaning
co-occurrence frequencies. Second, one may study
the effect of a proper dependency parsing on the re-
sults of the disambiguation process including here
making use of the syntactic relations names and ori-
entation.
Even if SynWSD rankings are not the best avail-
able, we believe that the unsupervised approach to
the WSD problem combined with different knowl-
edge sources represents the future of these systems
even if, at least during the last semantic evalua-
tion exercise SENSEVAL-3, the supervised systems
achieved top rankings.
References
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. MIT Press, May.
Vladimir Horbovanu. 2002. Word Sense Disambigua-
tion using WordNet. ?Alexandru Ioan Cuza? Univer-
sity, Faculty of Computer Science, Ias?i, Romania. In
Romanian.
Radu Ion and Verginica Barbu Mititelu. 2006. Con-
strained lexical attraction models. In Proceedings of
the Nineteenth International Florida Artificial Intelli-
gence Research Society Conference, pages 297?302,
Menlo Park, Calif., USA. AAAI Press.
Radu Ion. 2007. Word Sense Disambiguation meth-
ods applied to English and Romanian. Ph.D. thesis,
Research Institute for Artificial Intelligence (RACAI),
Romanian Academy, January. In Romanian, to be de-
fended.
Michael Lesk. 1986. Automatic sense disambiguation :
How to tell a pine cone from an ice cream cone. In
Proceedings of the 1986 SIGDOC Conference, Asso-
ciation for Computing Machinery, pages 24?26, New
York.
Dekang Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Association
for Computational Linguistics, pages 64?71, Madrid,
Spain, July.
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating Subject Field Codes into WordNet. In Gavrili-
dou M., Crayannis G., Markantonatu S., Piperidis S.,
and Stainhaouer G., editors, Proceedings of LREC-
2000, Second International Conference on Language
Resources and Evaluation, pages 1413?1418, Athens,
Greece, June.
Igor Mel?c?uk. 1988. Dependency Syntax: theory and
practice. State University of New York Press, Albany,
NY.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance.
In Proceedings of the 3rd DARPA Workshop on Hu-
man Language Technology, pages 303?308, Plains-
boro, New Jersey.
Robert C. Moore. 2004. On Log-Likelihood Ratios and
the Significance of Rare Events. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 333?340, Barcelona,
Spain.
Ian Niles and Adam Pease. 2003. Linking Lexicons
and Ontologies: Mapping WordNet to the Suggested
Upper Merged Ontology. In Proceedings of the 2003
International Conference on Information and Knowl-
edge Engineering (IKE 03), Las Vegas, Nevada, June.
Jiri Stetina, Sadao Kurohashi, and Makoto Nagao. 1998.
General word sense disambiguation method based on
a full sentential context. In Proceedings of the Coling-
ACL?98 Workshop ?Usage of WordNet in Natural Lan-
guage Processing Systems?, pages 1?8, Montreal.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?349.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory,
IT(13):260?269, April.
Deniz Yuret. 1998. Discovery of linguistic relations
using lexical attraction. Ph.D. thesis, Department of
Computer Science and Electrical Engineering, MIT,
May.
287
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 692?700,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Large tagset labeling using Feed Forward Neural Networks. Case 
study on Romanian Language 
 
 
Tiberiu Boro Radu Ion Dan Tufi 
Research Institute for 
$UWLILFLDO,QWHOOLJHQFH?0LKDL
DrJQHVFX? 
Romanian Academy 
Research Institute for 
$UWLILFLDO,QWHOOLJHQFH?0LKDL
DrJQHVFX? 
Romanian Academy 
Research Institute for 
$UWLILFLDO,QWHOOLJHQFH?0LKDL
DrJQHVFX? 
Romanian Academy 
tibi@racai.ro radu@racai.ro tufis@racai.ro 
 
 
 
Abstract 
Standard methods for part-of-speech tagging 
suffer from data sparseness when used on 
highly inflectional languages (which require 
large lexical tagset inventories). For this 
reason, a number of alternative methods have 
been proposed over the years. One of the 
most successful methods used for this task, 
FDOOHG7LHUHG7DJJLQJ7XIL, 1999), exploits 
a reduced set of tags derived by removing 
several recoverable features from the lexicon 
morpho-syntactic descriptions. A second 
phase is aimed at recovering the full set of 
morpho-syntactic features. In this paper we 
present an alternative method to Tiered 
Tagging, based on local optimizations with 
Neural Networks and we show how, by 
properly encoding the input sequence in a 
general Neural Network architecture, we 
achieve results similar to the Tiered Tagging 
methodology, significantly faster and without 
requiring extensive linguistic knowledge as 
implied by the previously mentioned method. 
1 Introduction 
Part-of-speech tagging is a key process for 
various tasks such as `information extraction, 
text-to-speech synthesis, word sense 
disambiguation and machine translation. It is also 
known as lexical ambiguity resolution and it 
represents the process of assigning a uniquely 
interpretable label to every word inside a 
sentence. The labels are called POS tags and the 
entire inventory of POS tags is called a tagset.  
There are several approaches to part-of-speech 
tagging, such as Hidden Markov Models (HMM) 
(Brants, 2000), Maximum Entropy Classifiers 
(Berger et al, 1996; Ratnaparkhi, 1996), 
Bayesian Networks (Samuelsson, 1993), Neural 
Networks (Marques and Lopes, 1996) and 
Conditional Random Fields (CRF) (Lafferty et 
al., 2001). All these methods are primarily 
intended for English, which uses a relatively 
small tagset inventory, compared to highly 
inflectional languages. For the later mentioned 
languages, the lexicon tagsets (called morpho-
syntactic descriptions (Calzolari and Monachini, 
1995) or MSDs) may be 10-20 times or even 
larger than the best known tagsets for English. 
For instance Czech MSD tagset requires more 
than 3000 labels (Collins et al, 1999), Slovene 
more than 2000 labels (Erjavec and Krek, 2008), 
and Romanian more than 1100 labels (Tufi, 
1999). The standard tagging methods, using such 
large tagsets, face serious data sparseness 
problems due to lack of statistical evidence, 
manifested by the non-robustness of the language 
models. When tagging new texts that are not in 
the same domain as the training data, the 
accuracy decreases significantly. Even tagging 
in-domain texts may not be satisfactorily 
accurate. 
One of the most successful methods used for 
this taVN FDOOHG 7LHUHG 7DJJLQJ 7XIL, 1999), 
exploits a reduced set of tags derived by 
removing several recoverable features from the 
lexicon morpho-syntactic descriptions. 
According to the MULTEXT EAST lexical 
specifications (Erjavec and Monachini, 1997), 
the Romanian tagset consists of a number of 614 
MSD tags (by exploiting the case and gender 
regular syncretism) for wordforms and 10 
punctuation tags (Tufi et al, 1997), which is 
still significantly larger than the tagset of 
English. The MULTEX EAST version 4 
(Erjavec, 2010) contains specifications for a total 
of 16 languages: Bulgarian, Croatian, Czech, 
Estonian, English, Hungarian, Romanian, 
692
693
In the case of out-of-vocabulary (OOV) 
words, both approaches use suffix analysis to 
determine the most probable tags that can be 
assigned to the current word.  
To clarify how these two methods work, if we 
want to train the network to label the current 
word, using a context window of 1 (previous tag, 
current possible tags, and possible tags for the 
next word) and if we have, say 100 tags in the 
tagset, the input is a real valued vector of 300 
sub-unit elements and the output is a vector 
which contains 100 elements, also sub-unit real 
numbers. As mentioned earlier, each value in the 
output vector corresponds to a distinct tag from 
tagset and the tag assigned to the current word is 
chosen to correspond to the maximum value 
inside the output vector. 
The previously proposed methods still suffer 
from the same issue of data sparseness when 
applied to MSD tagging. However, in our 
approach, we overcome the problem through a 
different encoding of the input data (see section 
2.1).  
The power of neural networks results mainly 
from their ability to attain activation functions 
over different patterns via their learning 
algorithm. By properly encoding the input 
sequence, the network chooses which input 
features contribute in determining the output 
features for MSDs (e.g. patterns composed of 
part of speech, gender, case, type etc. contribute 
independently in selecting the optimal output 
sequence). This way, we removed the need for 
explicit MSD to CTAG conversion and MSD 
recovery from CTAGs.  
2.1 The MSD binary encoding scheme 
A MSD language independently encodes a part 
of speech (POS) with the associated lexical 
attribute values as a string of positional ordered 
character codes (Erjavec, 2004). The first 
character is an upper case character denoting the 
SDUWRIVSHHFKHJ?1? IRUQRXQV?9?IRUYHUEV
?$? IRU DGMHFWLYHV HWF DQG WKH IROORZLQJ
FKDUDFWHUV ORZHU OHWWHUV RU ?-? specify the 
instantiations of the characteristic lexical 
attributes of the POS. For example, the MSD 
?1FIVUQ? specifies a noun (the first character is 
?1? the type of ZKLFK LV FRPPRQ ?F? WKH
second character), feminine gender ?I?VLQJXODU 
number ?V? LQQRPLQDWLYHDFFXVDWLYHFDVH?U?
and indefinite form ?Q?If a specific attribute is 
not relevant for a language, or for a given 
combination of feature-YDOXHVWKHFKDUDFWHU?-?LV
used in the corresponding position. For a 
language which does not morphologically mark 
the gender and definiteness features, the earlier 
H[HPSOLILHG06'ZLOOEHHQFRGHGDV?1F-sr-? 
 
In order to derive a binary vector for each of 
the 614 MSDs of Romanian we proceeded to: 
1. List and sort all possible POSes of 
Romanian (16 POSes) and form a binary 
vector with 16 positions in which position k 
is equal 1 only if the respective MSD has 
the corresponding POS (i.e. the k-th POS in 
the sorted list of POSes); 
2. List and sort all possible values of all lexical 
attributes GLVUHJDUGLQJWKHZLOGFDUG?-? for 
all POSes (94 values) and form another 
binary vector with 94 positions such that the 
k-th position of this vector is 1 if the 
respective MSD has an attribute with the 
corresponding value; 
3. Concatenate the vectors from steps 1 and 2 
and obtain the binary codification of a MSD 
as a 110-position binary vector. 
2.2 The training and tagging procedure 
The tagger automatically assigns four dummy 
tokens (two at the beginning and two at the end) 
to the target utterance and the neural network is 
trained to automatically assign a MSD given the 
context (two previously assigned tags and the 
possible tags for the current and following two 
words) of the current word (see below for 
details).  
In our framework a training example consists 
of the features extracted for a single word inside 
an utterance as input and it?s MSD within that 
utterance as output. The features are extracted 
from a window of 5 words centered on the 
current word. A single word is characterized by a 
vector that encodes either its assigned MSD or its 
possible MSDs. To encode the possible MSDs 
we use equation 2, where each possible attribute 
a, has a single corresponding position inside the 
encoded vector.  
 
2:=S; L %:S?=;%:S;  (2) 
 
Note that we changed the probability 
estimates to account for attributes not tags.  
 
To be precise, for every word wk, we obtain its 
input features by concatenating a number of 5 
vectors. The first two vectors encode the MSDs 
assigned to the previous two words (wk-1 and wk-
694
2).The next three vectors are used to encode the 
possible MSDs for the current word (wk) and the 
following two words (wk+1 and wk+2).  
During training, we also compute a list of 
suffixes with associated MSDs, which is used at 
run-time to build the possible MSDs vector for 
unknown words. When such words are found 
within the test data, we approximate their 
possible MSDs vector using a variation of the 
method proposed by Brants (2000).  
When the tagger is applied to a new utterance, 
the system iteratively calculates the output MSD 
for each individual word. Once a label has been 
assigned to a word, the ZRUG?VDVVRFLDWHGYHFWRU
is edited so it will have the value of 1 for each 
attribute present in its newly assigned MSD.  
As a consequence of encoding each individual 
attribute separately for MSDs, the tagger can 
assign new tags (that were never associated with 
the current word in the training corpus). 
Although this is a nice behavior for dealing with 
unknown words it is often the case that it assigns 
attribute values that are not valid for the 
wordform. To overcome these types of errors we 
use an additional list of words with their allowed 
MSDs. For an OOV word, the list is computed as 
a union from all MSDs that appeared with the 
suffixes that apply to that word. 
When the tagger has to assign a MSD to a 
given word, it selects one from the possible 
wordform?V MSDs in its wordform/MSDs 
associated list using a simple distance function: 
 
???
???
? K? F A?
?
?@4
 
(3) 
2 - The list of all possible MSDs for the given word 
J - The length of the MSD 
encoding (110 bits) 
K - The output of the Neural Network for the current word 
A - Binary encoding for a MSD in P 
3 Network hyperparameters 
In our experiments, we used a fully connected, 
feed forward neural network with 3 layers (1 
input layer, 1 hidden layer and 1 output layer) 
and a sigmoid activation function (equation 3). 
While other network architectures such as 
recurrent neural networks may prove to be more 
suitable for this task, they are extremely hard to 
train, thus, we traded the advantages of such 
architectures for the robustness and simplicity of 
the feed-forward networks. 
 
B:P; L ssE A?? (3) 
B:P; - Neuron output 
P - 
The weighted sum of all the 
neuron outputs from the 
previous layer 
 
Based on the size of the vectors used for MSD 
encoding, the output layer has 110 neurons and 
the input layer is composed of 550 (5 x 110) 
neurons. 
In order to fully characterize our system, we 
took into account the following parameters: 
accuracy, runtime speed, training speed, hidden 
layer configuration and the number of optimal 
training iterations. These parameters have 
complex dependencies and relations among each 
other. For example, the accuracy, the optimal 
number of training iterations, the training and the 
runtime speed are all highly dependent on the 
hidden layer configuration. Small hidden layer 
give high training and runtime speeds, but often 
under-fit the data. If the hidden layer is too large, 
it can easily over-fit the data and also has a 
negative impact on the training and runtime 
speed. The number of optimal training iterations 
changes with the size of the hidden layer (larger 
layers usually require more training iterations). 
To obtain the trade-offs between the above 
mentioned parameters we devised a series of 
experiments, in all of which we used WKH??
MSD annotated corpus, which is composed of 
118,025 words. We randomly kept out 
approximately 1/10 (11,960 words) of the 
training corpus for building a cross-validation 
set. The baseline accuracy on the cross-validation 
set (i.e. returning the most probable tag) is 
93.29%. We also used an additional inflectional 
wordform/MSD lexicon composed of 
approximately 1 million hand-validated entries.  
695
The first experiment was designed to 
determine the trade-off between the run-time 
speed and the size of the hidden layer. We made 
a series of experiments disregarding the tagging 
accuracy. 
 
Hidden size Time (ms) Words/sec 
50 1530 7816 
70 1888 6334 
90 2345 5100 
110 2781 4300 
130 3518 3399 
150 5052 2367 
170 5466 2188 
190 6734 1776 
210 7096 1685 
230 8332 1435 
250 9576 1248 
270 10350 1155 
290 11080 1079 
310 12364 967 
 
Table 1 - Execution time vs. number of neurons on 
the hidden layer 
 
Because, for a given number of neurons in the 
hidden layer, the tagging speed is independent on 
the tagging accuracy, we partially trained (using 
one iteration and only 1000 training sentences) 
several network configurations. The first network 
only had 50 neurons in the hidden layer and for 
the next networks, we incremented the hidden 
layer size by 20 neurons until we reached 310 
neurons. The total number of tested networks is 
14. After this, we measured the time it took to 
tag the 1984 test corpus (11,960 words) for each 
individual network, as an average of 3 tagging 
runs in order to reduce the impact of the 
operating system load on the tagger (Table 1 
shows the figures). 
Determining the optimal size of the hidden 
layer is a very delicate subject and there are no 
perfect solutions, most of them being based on 
trial and error: small-sized hidden layers lead to 
under-fitting, while large hidden layers usually 
cause over-fitting. Also, because of the trade-off 
between runtime speed and the size of hidden 
layers, and if runtime speed is an important 
factor in a particular NLP application, then 
hidden layers with smaller number of neurons are 
preferable, as they surely do not over-fit the data 
and offer a noticeable speed boost. 
 
hidden 
layer 
Train set 
accuracy 
Cross 
validation 
accuracy 
50 99.18 97.95 
70 99.20 98.02 
90 99.27 98.03 
110 99.29 98.05 
130 99.35 98.12 
150 99.35 98.09 
170 99.41 98.07 
190 99.40 98.10 
210 99.40 98.21 
 
Table 2 - Train and test accuracy rates for different 
hidden layer configurations 
 
As shown in Table 1, the runtime speed of our 
system shows a constant decay when we increase 
the hidden layer size. The same decay can be 
seen in the training speed, only this time by an 
order of magnitude larger. Because training a 
single network takes a lot of time, this 
experiment was designed to estimate the size of 
the hidden layer which offers good performance 
in tagging. To do this, we individually trained a 
number of networks in 30 iterations, using 
various hidden layer configurations (50, 70, 90, 
0.97
0.975
0.98
0.985
0.99
0.995
1
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
Test set
Train set
Number of iterrations 
Accuracy 
 
Figure 2 - 130 hidden layer network test and train set tagging accuracy as a function of the number of iterations 
696
110, 130, 150, 170, 190, and 210 neurons) and 5 
initial random initializations of the weights. For 
each configuration, we stored the accuracy of 
reproducing the learning data (the tagging of the 
training corpus) and the accuracy on the unseen 
data (test sets). The results are shown in Table 2. 
Although a hidden layer of 210 neurons did not 
seem to over-fit the data, we stopped the 
experiment, as the training time got significantly 
longer.  
The next experiment was designed to see how 
the number of training iterations influences the 
tagging performance of networks with different 
hidden layer configurations. Intuitively, the 
training process must be stopped when the 
network begins to over-fit the data (i.e. the train 
set accuracy increases, but the test set accuracy 
drops). Our experiments indicate that this is not 
always the case, as in some situations the 
continuation of the training process leads to 
better results on the cross-validation data (as 
shown in Figure 2). So, the problem comes to 
determining which is the most stable 
configuration of the neural network (i.e. which 
hidden unit size will be most likely to return 
good results on the test set) and establish the 
number of iterations it takes for the system to be 
trained. To do this, we ran the training procedure 
for 100 iterations and for each training iteration, 
we computed the accuracy rate of every 
individual network on the cross-validation set 
(see Table 3 for the averaged values). As shown, 
the network configuration using 130 neurons on 
the hidden layer is most likely to produce better 
results on the cross-validation set regardless of 
the number of iterations.  
Although, some other configurations provided 
better figures for the maximum accuracy, their 
average accuracy is lower than that of the 130 
hidden unit network. Other good candidates are 
the 90 and 110 hidden unit networks, but not the 
larger valued ones, which display a lower 
average accuracy and also significantly slower 
tagging speeds.  
The most suitable network configuration for a 
given task depends on the language, MSD 
encoding size, speed and accuracy requirements. 
In our own daily applications we use the 130 
hidden unit network. After observing the 
behavior of the various networks on the cross-
validation set we determined that a good choice 
is to stop the training procedure after 40 
iterations. 
 
Hidden 
units Avg. acc. Max. acc. St. dev. 
50 97.94 98.31 0.127002 
70 98.03 98.31 0.12197 
50 97.94 98.37 0.139762 
70 98.03 98.43 0.124996 
90 98.07 98.39 0.134487 
110 98.08 98.45 0.127109 
130 98.14 98.44 0.136072 
150 98.01 98.36 0.143324 
170 97.94 98.36 0.122834 
 
Table 3 - Average and maximum accuracy for various 
hidden layer configuration calculated over 100 
training iterations on the test set 
 
To obtain the accuracy of the system, in our 
last experiment we used the 130 hidden unit 
network and we performed the training/testing 
procedure on the 1984 corpus, using 10-fold 
validation and 30 random initializations. The 
final accuracy was computed as an average 
between all the accuracy figures measured at the 
end of the training process (after 40 iterations). 
The first 1/10 of the 1984 corpus on which we 
tuned the hyperparameters was not included in 
the test data, but was used for training. The mean 
accuracy of the system (98.41%) was measured 
as an average of 270 values. 
4 Comparison to other methods 
,Q KLV ZRUN &HDXu (2006) presents a 
different approach to MSD tagging using the 
Maximum Entropy framework. He presents his 
results on the same corpus we used for training 
and testing (the 1984 corpus) and he compares 
his method (98.45% accuracy) with the Tiered 
Tagging methodology (97.50%) (Tufi and 
Dragomirescu, 2004). 
Our Neural Network approach obtained 
similar (slightly lower) results (98.41%), 
although it is arguable that our split/train 
procedure is not identical to the one used in his 
work (no details were given as how the 1/10 of 
the training corpus was selected). Also, our POS 
tagger detected cases where the annotation in the 
Gold Standard was erroneous. One such example 
LV LQ ?lame de ras? (QJOLVK ?UD]RU EODGHV?
ZKHUH?ODPH?English ?EODGHV?LVDQRXQ?GH?
?for?LVDSUHSRVLWLRQDQG?UDV??VKDYLQJ?) is a 
supine verb (with a past participle form) which 
was incorrectly annotated as a noun. 
697
5 Network pattern analysis 
Using feed-forward neural networks gives the 
ability to outline what input features contribute to 
the selection of various MSD attribute values in 
the output layer which might help in reducing the 
tagset and thus, redesigning the network 
topology with beneficial effects both on the 
speed and accuracy.  
To determine what input features contribute to 
the selection of certain MSD attribute values, one 
can analyze the weights inside the neural 
network and extract the input ? output links that 
are formed during training. We used the network 
with 130 units on the hidden layer, which was 
previously trained for 100 iterations. Based on 
the input encoding, we divided the features into 5 
groups (one group for each MSD inside the local 
context ? two previous MSDs, current and 
following two possible MSDs). For a target 
attribute value (noun, gender feminine, gender 
masculine, etc.) and for each input group, we 
selected the top 3 input values which support the 
decision of assigning the target value to the 
attribute (features that increase the output value) 
and the top 3 features which discourage this 
decision (features that decrease the output value). 
For clarity, we will use the following notations 
for the groups: 
x G
-2: group one ? the assigned MSD for 
the word at position i-2 
x G
-1: group two ? the assigned MSD for 
the word at position i-1 
x G0: group three ? the possible MSDs for 
the word at position i 
x G1: group four? the possible MSDs for 
the word at position i+1 
x G2: group five ? the possible MSDs for 
the word at position i+2 
where i corresponds to the position of the word 
which is currently being tagged. Also, we 
classify the attribute values into two categories 
(C): (P) want to see (support the decision) and 
(N) GRQ?WZDQWWRVHH (discourage the decision). 
 
Table 4 shows partial (G
-1 G0 G1) examples of 
two target attribute values (cat=Noun and gender 
=Feminine) and their corresponding input 
features used for discrimination. 
 
Target 
value Group C Attribute values  
Noun G
-1 P 
main (of a verb), article, 
masculine (gender of a 
noun/adjective 
N 
particle, conjunctive particle, 
auxiliary (of a verb), 
demonstrative (of a pronoun) 
G0 
P noun, common/proper (of a 
noun) 
N 
adverb, pronoun, numeral, 
interrogative/relative (of a 
pronoun) 
G1 
P 
genitive/dative (of a 
noun/adjective), particle, 
punctuation 
N 
conjunctive particle, strong (of 
a pronoun), non-definite (of a 
noun/adjective), exclamation 
mark 
Fem. 
G
-1 
P 
main (of a verb), preposition, 
feminine (of a 
noun/adjective) 
N auxiliary (of a verb), particle, demonstrative (of a pronoun) 
G0 
P 
feminine (of a 
noun/adjective), 
nominative/accusative (of a 
noun/adjective), past (of a 
verb) 
N 
masculine (of a 
noun/adjective), auxiliary (of a 
verb), interrogative/relative (of 
a pronoun), adverb 
G1 
P 
dative/genitive (of a 
noun/adjective), indicative (of 
a verb), feminine (of a 
noun/adjective) 
N 
conjunctive particle, future 
particle, nominative/accusative 
(of a noun/adjective) 
 
Table 4 ? P/N features for various attribute 
values. 
 
For instance, when deciding on whether to give a 
noun (N) label to current position (G0), we can 
see that the neural network has learned some 
interesting dependencies: at position G
-1 we find 
an article (which frequently determines a noun) 
and at the current position it is very important for 
the word being tagged to actually be a common 
or proper noun (either by lexicon lookup or by 
suffix guessing) and not be an adverb, pronoun 
or numeral (POSes that cannot be found in the 
typical ambiguity class of a noun). At the next 
position of the target (G1) we also find a noun in 
genitive or dative, corresponding to a frequent 
construction in Romanian, HJ ?PDina 
ELDWXOXL? EHLQJ D VHTXHQFH RI WZR nouns, the 
second at genitive/dative. 
If the neural network outputs the feminine 
gender to its current MSD, one may see that it 
698
has actually learned the agreement rules (at least 
locally): the feminine gender is present both 
before (G
-1) the target word as well as after it 
(G1). 
6 Conclusions and future work 
We presented a new approach for large tagset 
part-of-speech tagging using neural networks. An 
advantage of using this methodology is that it 
does not require extensive knowledge about the 
grammar of the target language. When building a 
new MSD tagger for a new language one is only 
required to provide the training data and create 
an appropriate MSD encoding system and as 
shown, the MSD encoding algorithm is fairly 
simple and our proposed version works for any 
other MSD compatible encoding, regardless of 
the language.  
Observing which features do not participate in 
any decision helps design custom topologies for 
the Neural Network, and provides enhancements 
in both speed and accuracy. The configurable 
nature of our system allows users to provide their 
own MSD encodings, which permits them to 
mask certain features that are not useful for a 
given NLP application.  
If one wants to process a large amount of text 
and is interested only in assigning grammatical 
categories to words, he can use a MSD encoding 
in which he strips off all unnecessary features. 
Thus, the number of necessary neurons would 
decrease, which assures faster training and 
tagging. This is of course possible in any other 
tagging approaches, but our framework supports 
this by masking attributes inside the MSD 
encoding configuration file, without having to 
change anything else in the training corpus. 
During testing the system only verifies if the 
MSD encodings are identical and the displayed 
accuracy directly reflects the performance of the 
system on the simplified tagging schema. 
We also proposed a methodology for selecting 
a network configurations (i.e. number of hidden 
units), which best suites the application 
requirements. In our daily applications we use a 
network with 130 hidden units, as it provides an 
optimal speed/accuracy trade-off (approx. 3400 
words per second with very good average 
accuracy).  
The tagger is implemented as part of a larger 
application that is primarily intended for text-to-
speech (TTS) synthesis. The system is free for 
non-commercial use and we provide both web 
and desktop user-interfaces. It is part of the 
METASHARE platform and available online 2 . 
Our primary goal was to keep the system 
language independent, thus all our design choices 
are based on the necessity to avoid using 
language specific knowledge, when possible. The 
application supports various NLP related tasks 
such as lexical stress prediction, syllabification, 
letter-to-sound conversion, lemmatization, 
diacritic restoration, prosody prediction from text 
and the speech synthesizer uses unit-selection. 
From the tagging perspective, our future plans 
include testing the system on other highly 
inflectional languages such as Czech and 
Slovene and investigating different methods for 
automatically determining a more suitable 
custom network topology, such as genetic 
algorithms. 
Acknowledgments 
The work reported here was funded by the 
project METANET4U by the European 
Commission under the Grant Agreement No 
270893  
                                                          
2
 http://ws.racai.ro:9191  
699
References  
Berger, A. L., Pietra, V. J. D. and Pietra, S. A. D. 
1996. A maximum entropy approach to natural 
language processing. Computational linguistics, 
22(1), 39-71. 
 
Brants, T. 2000. TnT: a statistical part-of-speech 
tagger. In Proceedings of the sixth conference on 
applied natural language processing (pp. 224-231). 
Association for Computational Linguistics. 
 
Calzolari, N. and Monachini M. (eds.). 1995. 
Common Specifications and Notation for Lexicon 
Encoding and Preliminary Proposal for the 
Tagsets. MULTEXT Report, March. 
 
&HDXu, A. 2006. Maximum entropy tiered tagging. In 
Proceedings of the 11th ESSLLI Student Session 
(pp. 173-179). 
 
CollinV05DPVKDZ/+DML?-DQG7LOOPDQQ&
1999. A statistical parser for Czech. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics on Computational 
Linguistics (pp. 505-512). Association for 
Computational Linguistics. 
 
Erjavec, T. and Monachini, M. (Eds.). 1997. 
Specifications and Notation for Lexicon Encoding. 
Deliverable D1.1 F. Multext-East Project COP-
106. 
 
Erjavec, T. 2004. MULTEXT-East version 3: 
Multilingual morphosyntactic specifications, 
lexicons and corpora. In Fourth International 
Conference on Language Resources and 
Evaluation, LREC (Vol. 4, pp. 1535-1538). 
 
Erjavec, T. and Krek, S. 2008. The JOS 
morphosyntactically tagged corpus of Slovene. In 
Proceedings of the Sixth International Conference 
RQ/DQJXDJH5HVRXUFHVDQG(YDOXDWLRQ/5(&? 
 
Erjavec, T. 2010. MULTEXT-East Version 4: 
Multilingual Morphosyntactic Specifications, 
Lexicons and Corpora. In Proceedings of the 
Seventh International Conference on Language 
Resources and Evaluation (LREC'10), Valletta, 
Malta. European Language Resources Association 
(ELRA) ISBN 2-9517408-6-7. 
 
Lafferty, J., McCallum, A. and Pereira, F. C. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. 
 
Marques, N. C. and Lopes, G. P. 1996. A neural 
network approach to part-of-speech tagging. In 
Proceedings of the 2nd Meeting for Computational 
Processing of Spoken and Written Portuguese (pp. 
21-22). 
 
Ratnaparkhi, A. 1996. A maximum entropy model for 
part-of-speech tagging. In Proceedings of the 
conference on empirical methods in natural 
language processing (Vol. 1, pp. 133-142). 
 
Samuelsson, C. 1993. Morphological tagging based 
entirely on Bayesian inference. In 9th Nordic 
Conference on Computational Linguistics. 
 
Schmid, H. 1994. Part-of-speech tagging with neural 
networks. In Proceedings of the 15th conference on 
Computational linguistics-Volume 1 (pp. 172-176). 
Association for Computational Linguistics. 
 
Tufi, D., Barbu A.M.,
 
3WUD?FX 9 Rotariu G. and 
Popescu C. 1997. Corpora and Corpus-Based 
Morpho-Lexical Processing. In Recent Advances 
in Romanian Language Technology, (pp. 35-56). 
Romanian Academy Publishing House, ISBN 973-
27-0626-0. 
 
7XIL, D. 1999. Tiered tagging and combined 
language models classifiers. In Text, Speech and 
Dialogue (pp. 843-843). Springer 
Berlin/Heidelberg. 
 
Tufi, D., and Dragomirescu, L. 2004. Tiered tagging 
revisited. In Proceedings of the 4th LREC 
Conference (pp. 39-42). 
 
700
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 19?27,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Experiments with a Differential Semantics Annotation for WordNet 3.0 
 
 
Dan Tufi? Dan  ?tef?nescu 
Research Institute for Artificial Intelligence  
Romanian Academy 
Research Institute for Artificial Intelligence  
Romanian Academy 
Calea ?13 Septembrie?, no.13 Calea ?13 Septembrie?, no.13 
Bucharest 5, 050711, Romania Bucharest 5, 050711, Romania 
tufis@racai.ro danstef@racai.ro 
  
 
Abstract 
This article reports on the methodology and 
the development of a complementary 
information source for the meaning of the 
synsets of Princeton WordNet 3.0. This 
encoded information was built following 
the principles of the Osgoodian differential 
semantics theory and consists of numerical 
values which represent the scaling of the 
connotative meanings along the multiple 
dimensions defined by pairs of antonyms 
(factors). Depending on the selected 
factors, various facets of connotative 
meanings come under scrutiny and 
different types of textual subjective 
analysis may be conducted (opinion 
mining, sentiment analysis). 
1 Introduction 
According to ?Semantic Differential? theory 
(Osgood et al, 1957), the connotative meaning of 
most adjectives can be, both qualitatively and 
quantitatively, differentiated along a scale, the ends 
of which are antonymic adjectives. Such a pair of 
antonymic adjectives is called a factor. The 
intensive experiments Osgood and his colleagues 
made with their students1 outlined that most of the 
variance in the text judgment was explained by 
only three major factors: the evaluative factor (e.g., 
good-bad), the potency factor (e.g., strong-weak), 
and the activity factor (e.g., active-passive).   
                                                           
1
 The students were asked to rate the meaning of words, 
phrases, or texts on different scales defined in terms of pairs 
of bipolar adjectives such as good-bad, active-passive, 
strong-weak, optimistic-pessimistic, beautiful-ugly, etc.) 
Kamps and Marx (2002) implemented a 
WordNet-based method in the spirit of the theory 
of semantic differentials and proposed a method to 
assess the?attitude? of arbitrary texts. In their 
approach, a text unit is regarded as a bag of words 
and the overall scoring of the sentence is obtained 
by combining the scores for the individual words 
of the text. Depending on the selected factor, 
various facets of subjective meanings come under 
scrutiny.  
The inspiring work of Kamps and Marx still has 
several limitations. The majority of researchers 
working on subjectivity agree that the subjectivity 
load of a given word is dependent on the senses of 
the respective word (Andreevskaia and Bergler, 
2006), (Bentivogli et al, 2004), (Mihalcea et al, 
2007), (Valiutti et al, 2004) and many others.; yet, 
in Kamps and Marx?s model (KMM, henceforth), 
because they work with words and not word-
senses, the sense distinctions are lost, making it 
impossible to assign different scores to different 
senses of the word in case. Going up from the level 
of word to the level of sentence, paragraph or 
entire text, the bag of words approach can easily be 
fooled in the presence of valence shifters (Polanyi 
and Zaenen, 2006). In order to cope with this 
problem, the text under investigation needs a 
minimal level of sentence processing, required for 
the identification of the structures that could get 
under the scope of a valence shifter (Tufi?, 2008). 
For dealing with irony or sarcasm, processing 
requirements go beyond sentence level, and 
discourse structure of the text might be necessary. 
On the other hand, although the adjectives make 
up the obvious class of subjectivity words, the 
other open class categories have significant 
potential for expressing subjective meanings.  
19
In our models, unlike KMM, the building block 
is the word sense, thus making possible to assign 
different connotation values to different senses of a 
word. This was possible by using an additional 
source of information besides the WordNet itself, 
namely the SUMO/MILO ontology. Moreover, we 
considered all the word classes contained in 
WordNet, not only adjectives. 
From this point of view, our work, although 
through a different approach, shares objectives 
with other wordnet-based methods such as 
SentiWordNet (Esuli and Sebastiani, 2006) 
(Baccianella et al, 2010) and WordNet Affect 
(Valiuttti et al 2004). 
2 Base Definitions 
Let us begin with some definitions, slightly 
modified, from KMM. We will progressively 
introduce new definitions to serve our extended 
approach. 
Definition 1: Two words w? and w? are related 
if there exists a sequence of words (w? w1 
w2?wi? w?) so that each pair of adjacent words 
in the sequence belong to the same synset. If the 
length of such a sequence is n+1 one says that w? 
and w? are n-related. 
Two words may not be related at all or may be 
related by many different sequences, of various 
lengths. In the latter case, one would be interested 
in their minimal path-length. 
Definition 2: Let MPL(wi, wj) be the partial 
function: 
     
 otherwise    
related-n  are    wand  n when  wsmallest        the),( ji
?
?
?
=
undefined
n
wwMPL ji
 
Kamps and Marx (2002) showed that MPL is a 
distance measure that can be used as a metric for 
the semantic relatedness of two words. Observing 
the properties of the MPL partial function, one can 
quantify the relatedness of an arbitrary word wi to 
one or the other word of a bipolar pair. To this end, 
KMM introduced another partial function as in 
Definition 3. 
Definition 3: Let TRI (wi, w?, w?), with w? ? w? 
be: 
??
??
?
=
 otherwise                              
  defined   MPLs  if  
,(
,(- ,(
),,(
undefined
)wwMPL
)wwMPL)wwMPL
wwwTRI
ii
i ??
??
??
 
When defined, TRI(wi, w?, w?) is a real number 
in the interval [-1, 1]. The words w? and w? are the 
antonymic words of a factor, while wi is the word 
of interest for which TRI is computed. If one takes 
the negative values returned by the partial function 
TRI (wi, w?, w?) as an indication of wi being more 
similar to w? than to w? and the positive values as 
an indication of wi being more similar to w? than to 
w?, then a zero value could be interpreted as wi 
being neutrally related with respect to w? and w?. 
This is different from being unrelated. 
Definition 4: If ?-? is a factor used for the 
computation of relatedness of wi to ? and ?, the 
proper function TRI*?-? (wi) returns a value outside 
the interval [-1, 1] when wi is unrelated to the ?-? 
factor: 
??
?
=
?
 otherwise                      2
defined  ),,(w TRI  iff   ),,(w TRI)( ii* ?????? iwTRI
 
Given a factor ?-?, for each word wi in 
WordNet that can be reached on a path from ? to 
?, the function TRI*?-? (wi)  computes a score 
number, which is a proportional to the distances 
from wi to ? and to ?. The set of these words 
defines the coverage of the factor ? COV(?, ?).  
Our experiments show that the coverage of the 
vast majority of the factors, corresponding to the 
same POS category, is the same. From now on, we 
will use LUC (Literal Unrestricted2 Coverage) to 
designate this common coverage. The table below 
gives coverage figures for each of the POS 
categories in Princeton WordNet 3.0 (PWN 3.0). 
 
Class Factors LUC 
Adjectives  199  4,402 (20.43%) 
Nouns  106 11,964 (10,05%) 
Verbs  223 6,534 (56,66%) 
Adverbs 199 1,291 (28,81%) 
Table 1: LUC Statistics According to the POS of 
the Literals in PWN 3.0  
The PWN structuring does not allow us to 
compute TRI* scores for adverbs using this 
approach, but, more than half of the total number 
of adverbs (63.11%) are derived from adjectives. 
For those adverbs, we transferred the score values 
from their correspondent adjectives in the LUC set 
and we used the adjectival factors. 
                                                           
2
 In the following we will gradually introduce several 
restrictions, thus justifying the acronym used here. 
20
The results reported for adjectives by Kamps 
and Marx3  are consistent with our findings. The 
difference in numbers might be explained by the 
fact that the two compared experiments used 
different versions of the Princeton WordNet. 
3 Introducing Word-Sense Distinctions  
KMM defines a factor as a pair of words with 
antonymic senses. We generalize the notion of a 
factor to a pair of synsets. In the following, we will 
use the colon notation to specify the sense number 
of a literal that licenses the synonymy relation 
within a synset. Synonymy is a lexical relation that 
holds not between a pair of words but between 
specific senses of those words. That is, the notation 
{literal1:n1 literal2:n2 ? literalk:nk} will mean that 
the meaning given by the sense number n1 of the 
literal1, the meaning given by sense number n2 of 
the literal2 and so on are all pair-wise synonymous. 
The term literal is used to denote the dictionary 
entry form of a word (lemma).  
The antonymy is also a lexical relation that 
holds between specific senses of a pair of words. 
The synonyms of the antonymic senses, taken 
pairwise, definitely express a semantic opposition. 
Take for instance the antonymic pair <rise:1 
fall:2>. These two words belong to the synsets 
{rise:1, lift:4, arise:5, move up:2, go up:1, come 
up:6, uprise:6} and {descend:1, fall:2, go down:1, 
come down:1}. The pair <rise:1 fall:2> is 
explicitly encoded as antonymic. However, there is 
a conceptual opposition between the synsets to 
which the two word senses belong, that is between 
any pair of the Cartesian product: {rise:1, lift:4, 
arise:5, move up:2, go up:1, come up:6, 
uprise:6}?{descend:1, fall:2, go down:1, come 
down:1}. This conceptual opposition is even more 
obvious in this example, as the pairs <go up:1 go 
down:1> and <come up:1 come down:1> are also 
explicitly marked as antonymic. 
Definition 5: An S-factor is a pair of synsets 
(S?, S?) for which there exist ???: ??? ? ?? and 
???: ??? ? ?? so that ???: ???  and ???: ???  are 
antonyms and ??????? , ???? is defined. S? and S? 
                                                           
3
 They found 5410 adjectives that were in the coverage of the 
factors they investigated (WordNet 1.7). For PWN 2.0, the 
total number of covered adjectives is 5307. 
have opposite meanings, and we consider 
that ?????? , ??? = ??? ???? , ????.  
The previous example shows that the semantic 
opposition of two synsets may be reinforced by 
multiple antonymic pairs. Because of how MPL is 
defined, choosing different antonymic pairs might 
produce different values for ?????? , ???. That is 
why, wherever is the case, we need to specify the 
antonymic pair which defines the S-factor. 
Based on the definition of the coverage of a 
factor < ?iw , ?iw >, one may naturally introduce the 
notion of coverage of a S-factor - <S?,S?>: the set 
of synsets containing the words in COV< ?iw , ?iw >. 
The coverage of an S-factor <S?,S?> will be 
onward denoted by SCOV<S?, S?>.  
Since the word-relatedness and MPL definitions 
ignore the word senses, it might happen that the 
meaning of some synsets in the coverage of an S- 
factor have little (if anything) in common with the 
semantic field defined by the respective S-factor. 
More often than not, these outliers must be filtered 
out and, to this end, we further introduce the 
notions of semantic type of a synset, typed S-factor, 
and scoped synset with respect to a typed S-factor, 
which represent major deviations from KMM. 
 
Figure 1. Different levels of coverage (marked 
with cross hatching) for the S-factor <S?-S?> 
Before that, we need to introduce the mapping 
between the WordNet synsets and the SUMO/ 
MILO concepts. The Suggested Upper Merged 
Ontology (SUMO), Mid-Level Ontology (MILO) 
and its domain ontologies form the largest formal 
public 4  ontology in existence today, containing 
roughly 20,000 terms and 70,000 axioms (when 
                                                           
4
 http://www.ontologyportal.org/ 
21
SUMO, MILO, and domain ontologies are 
combined). One of the major attractions of this 
ontology (Niles and Pease, 2003) is that it has been 
mapped to the WordNet lexicon. Using this 
mapping, synsets are labeled with a SUMO/MILO 
concept which we will refer to as the synset?s 
semantic type. The hierarchical structure of 
SUMO/MILO induces a partial ordering of the S-
factors. 
Definition 6: An S-factor <S?, S?> is said to be 
a typed S-factor if the types of the synsets S? and 
S? are identical or they have a common ancestor. If 
this ancestor is the lowest common ancestor, it is 
called the 0-semantic type of the S-factor. The 
direct parent of the n-semantic type of an S-factor 
is the n+1-semantic type of the S-factor (Fig. 1). 
A typed S-factor is represented by indexing the 
S-factor with its type as in the examples below: 
<{unfairness:2?}, { fairness:1?}>NormativeAttribute  
<{discomfort:1?}, {comfort:1?}>StateOfMind  
<{distrust:2?}, {trust:3?}>TraitAttribute  
<{decrease:2? }, {increase:3?}>QuantityChange  
In the following, if not otherwise specified, by 
S-factors we mean typed S-factors. Unless there is 
ambiguity, the type of an S-factor will be omitted. 
Definition 7: A synset Si with the type L is n-
scoped relative to a typed S-factor <S?, S?> if L is 
a node in a sub-tree of the SUMO/MILO hierarchy 
having as root the n-semantic type of the S-factor 
<S?, S?>. We say that n defines the level of the 
scope coverage of the S-factor <S?, S?> and that 
every synset in this coverage is n-scoped. 
We use the notation SCOVn<S?, S?> for the 
scope coverage of level n of an S-factor <S?, S?>. 
If the root of the tree has the semantic type ?, we 
will use also use the notation SCOVn<S?, S?>? or 
simply SCOV<S?, S?>?. In other words, 
SCOV<S?, S?>? is the set of synsets the semantic 
types of which are subsumed by ?. For the example 
in Fig. 1, only the synsets S?1, S?2 and S?1 are in the 
SCOV0<S?, S?>. All depicted synsets are in 
SCOV1<S?, S?>.  
It is easy to see that when the value of the scope 
coverage level is increased so as to reach the top of 
the ontology, SCOVn<S?, S?>? will be equal to the 
set of synsets containing the literals in LUC (see 
Table 1).  Let us call this set SUC (Synset 
Unrestricted Coverage). 
 
 
Class S-Factors SUC 
Adjectives  264 4,240 (23.35%) 
Nouns  118 11,704 (14.25%) 
Verbs  246  8,640 (62.75%) 
Adverbs 264 1,284 (35.45%) 
Table 2: SUC Statistics According to the POS of 
the Synsets in PWN 3.0  
From the differential semantics point of view, 
the S-factor <S?, S?> quantitatively characterizes 
each synset in SCOVn<S?, S?> by a TRI*-like 
score (Definition 4). The synsets in SCOV0<S?, 
S?> are best discriminated, meaning that their 
scores for the <S?, S?> factor are the highest. For 
the synsets in SCOVn<S?, S?> but not in SCOVn-
1<S?, S?>, the scores are smaller and we say that 
the characterization of these synsets in terms of the 
<S?, S?> factor is weaker. Our model captures this 
through a slight modification of the TRI function 
in Definition 3, where w? and w? are the antonyms 
belonging to S? and S? respectively, and wi is a 
literal of a synset Sj in SCOVn<S?, S?> but not in 
SCOVn-1<S?, S?>: 
Definition 8: The differential score for a literal 
wi occurring in a synset Sj in SCOVn<S?, S?> but 
not in SCOVn-1<S?, S?> is computed by the 
function TRI+: 
    
,(
,(- ,(),,(
n)wwMPL
)wwMPL)wwMPL
SSwTRI iii +
=
+
??
??
??
 
Since we imposed the requirement that Sj be in 
SCOVn<S?, S?>, ),,( ?? SSwTRI i+  is defined for 
all literals in Sj, thus for any ji Sw ? the value of 
),,( ?? SSwTRI i+ is in the [-1,1] interval. The 
scores computed for the synsets in SCOVn<S?, S?> 
remained unchanged in SCOVn+k<S?, S?> for any  
k?0. The above modification of the TRI function 
insures that the score of a synset gets closer to zero 
(neutrality) with the increase of n.  
It is worth mentioning that using different 
antonymic literal pairs from the same opposed 
synsets does not have any impact on the sign of 
TRI+ scores, but their absolute values may differ.  
If one associates a semantic field with ?, the 
type of an S-factor <S?, S?>, then all the synsets in 
SCOVn<S?, S?>? are supposed to belong to the 
semantic field associated with ?. This observation 
should clarify why different senses of a given word 
22
may belong to different semantic coverages and 
thus, may have different scores for the S-factor in 
case. 
Definition 9: The differential score of a synset 
Si in SCOVn<S?, S?> with respect to the S-factor 
<S?, S?> is given by the function TRIS (Si, S?, S?), 
defined as the average of the TRI+ values 
associated with the m literals in the synset Si. 
m
SSwTRI
SSSTRIS
m
j
j
i
?
=
+
=
1
),,(
),,(
??
??
 
4 Computing the S-Factors and the 
Differential Scores for Synsets 
In accordance with the equations in the previous 
definitions, we associated each synset Sk of 
WordNet 3.0 with an ordered vector <F1, F2? Fn> 
where Fi is a pair (score; level) with score and 
level representing the value of the ith S-factor and, 
respectively, the minimal S-factor coverage level 
in which Sk was found.  
For instance, let us assume that the first S-factor 
in the description of the adjectival synsets is:  
<{nice:3},{nasty:2 ?}>SubjectiveAssesmentAtttribute 
then for the synset {fussy:1, crabby:1, grumpy:1, 
cross:2, grouchy:1, crabbed:1, bad-tempered:1, 
ill-tempered:1}SubjectiveAssesmentAtttribute the vector 
<F1,?> is <(0,66;0) ...> while for the synset 
{unplayful:1 serious:5 sober:4}SubjectiveAssesmentAtttribute 
the vector <F1,?> is    <(-0,166 ; 0) ...>. 
The values signify that the synset {fussy:1, 
crabby:1, grumpy:1, cross:2?}SubjectiveAssesment 
Atttribute is 0-scoped with respect to the S-factor 
<{nice:3}, {nasty:2 ?}> and its connotative 
meaning is significantly closer to the meaning of 
nasty:2 (0,66). Similarly, the synset {unplayful:1 
serious:5 sober:4} is 0-scoped with respect to the 
considered  S-factor and its connotative meaning is 
closer to the meaning of nice:3 (-0,166) 
Our experiments showed that in order to ensure 
the same sets of synsets for all factors of a given 
part-of-speech we had to set the level of the 
semantic coverages to 7 (corresponding to the 
SUC). For each of the typed S-factors <S?, S?> and 
for each synset Si in their respective semantic 
coverage SCOV<S?, S?>? we computed the 
TRIS??? , ??, ???  score. Each synset from the 
coverage of each POS category was associated 
with a vector of scores, as described above. Since 
the number of S-factors depends on the POS 
category the lengths of each of the four type 
vectors is different. The cell values in a synset 
vector have uneven values, showing that factors 
have different discriminative power for a given 
meaning. Because we considered SUC, all S-
factors are relevant and the cells in any synset 
vector are filled with pairs (score; level).  
For the noun part of the PW3.0 we identified 
118 typed S-factors, all of them covering the same 
set of 11,898 noun literals (9.99%) with their 
senses clustered into 11,704 synsets (14.25%).  
For the verb part of the PWN 3.0, we identified 
246 typed S-factors, all of them covering the same 
set of 6,524 verb literals (56.57%) with their senses 
clustered into 8,640 synsets (62.75%).  
For the adjective part of the PWN 3.0, we 
identified 264 typed S-factors, all of them covering 
the same set of 4,383 literals (20.35%) with their 
senses clustered into 4,240 synsets (23.35%)5. As 
previously mentioned, the same factors were used 
for the adverbs derived from adjectives. In this 
way, a total of 1,287 adverbs (28.72%) clustered 
into 1,284 synsets (35.45%) were successfully 
annotated (see Table 2). 
Apparently, the cardinals of the factor sets in 
Table 2 should be identical with those in Table 1. 
The differences are due to the fact that a pair of 
opposed synsets may contain more than a single 
pair of antonymic senses each of them specifying a 
distinct S-factor. 
In case the user restricted the coverages to lower 
levels, the original maximal semantic coverages 
are split into different subsets for which several S-
factors become irrelevant. The cell values 
corresponding to these factors are filled in with a 
conventional value outside the interval [-1, 1].  
Thus, we have the following annotation cases: 
A synset of a certain POS is not in the 
corresponding SUC. This case signifies that the 
synset cannot be characterized in terms of the 
differential semantics methodology and we 
conventionally say that such a synset is ?objective? 
(insensitive to any S-factor). Since this situation 
would require a factor vector with each cell having 
the same value (outside the [-1, 1] interval) and as 
                                                           
5
 In PWN 2.0 the number of covered literals (and synsets) is 
with almost 20% higher (Tufi? and ?tef?nescu, 2010). This 
difference is explained by the fact that 1081 adjectives (5%), 
mostly participial, from PWN 2.0 are not any more listed as 
adjectives in PWN 3.0.   
23
such a vector would be completely uninformative, 
we decided to leave the ?objective? synsets un-
annotated. As one can deduce from Table 2, the 
majority of the synsets in PWN3.0 are in this 
category (89,556 synsets, i.e. 77.58%). 
 Any synset of a certain POS in the 
corresponding SUC will have an associated factor 
vector. There are 25,868 such synsets. The ith cell 
of such a vector will correspond to the ith S-factor 
<S?, S?>. We may have the following sub-cases: 
(a) All cell scores are in the [-1,1] interval, and 
in this case all S-factors are relevant, that is, from 
any word in the synset one could construct a path 
to both words prompting an S-factor, irrespective 
of the S-factor itself. A negative score in the ith cell 
of the S-factor vector signifies that the current 
synset is more semantically related to S? than to S?, 
while a positive score in the ith cell of the factor 
vector signifies that the synset is more 
semantically related to S? than to S?. A zero score 
in the ith cell of the factor vector signifies that the 
synset is neutral with respect to the <S?, S?> S-
factor. 
(b) Several cell scores are not in the interval [-1, 
1], say FV[i1]=FV[i2] ? =FV[ik]=2. This signifies 
that the S-factors corresponding to those cells 
(<S?1,S?1>,<S?2,S?2>,?,<S?3,S?3>) are irrelevant 
for the respective synset and that the current synset 
is not included in the scope of the above-
mentioned S-factors, owing to the selected scope 
level of the coverage6. We say that, at the given 
scope level, the synset became ?objective? with 
respect to the S-factors FV[i1], FV[i2] ?FV[ik]. 
There are various ways to select, for a given 
POS coverage, those S-factors which are most 
informative or more interesting from a specific 
point of view. The simplest criterion is based on 
the coverage level: for a specified coverage level, 
select only those S-factors the coverage of which 
contains the analyzed synsets. In general, the most 
restrictive condition is choosing the 0-level 
coverage. This condition is equivalent to saying 
that the S-factors and the analyzed synsets should 
be in the same semantic class as defined by the 
SUMO/MILO labeling.  For instance, assume that 
the synset under investigation is {good:1} with the 
                                                           
6
 Remember that for the highest level (7) that corresponds to 
SUC, all factors are relevant. When the user selects coverages 
of lower levels some factors might become irrelevant for 
various synsets. 
definition ?having desirable or positive qualities 
especially those suitable for a thing specified? and 
the semantic type SubjectiveAssessmentAttribute.  
Imposing the restriction that the semantic type of 
the selected factors should be the same with the 
semantic type of good:1, some relevant factors for 
estimating the various connotations of ?good? from 
different perspectives are given below. In the 
shown factors, the words in bold face are those the 
meaning of which is closer to ?good?. 
 
good 01123148-a (SubjectiveAssessmentAttribute) 
-------------------------------------------------------------- 
effective ineffective#00834198-a_00835609-a 
(SubjectiveAssessmentAttribute) -0,78 
reasonable unreasonable#01943406-a_01944660-a 
(SubjectiveAssessmentAttribute) -0,71 
rich lean#02026785-a_02027003-a 
(SubjectiveAssessmentAttribute) -0,63 
ample meager#00105746-a_00106456-a 
(SubjectiveAssessmentAttribute) -0,5 
safe dangerous#02057829-a_02058794-a 
(SubjectiveAssessmentAttribute) -0,33 
brave cowardly#00262792-a_00264776-a 
(SubjectiveAssessmentAttribute) -0,14 
distant close#00450606-a_00451510-a 
(SubjectiveAssessmentAttribute) 0,64 
busy idle#00292937-a_00294175-a 
(SubjectiveAssessmentAttribute) 0,63 
cursed blessed#00669478-a_00670741-a 
(SubjectiveAssessmentAttribute) 0,5 
old new#01638438-a_01640850-a 
(SubjectiveAssessmentAttribute) 0,45 
formal informal#01041916-a_01044240-a 
(SubjectiveAssessmentAttribute) 0,38 
 
These factors? values should be clearer in the 
context of adequate examples: 
A good tool is an effective tool; 
A good excuse is a reasonable excuse; 
A good vein of copper is a reach vein of copper; 
A good resource is an ample resource; 
A good position is a safe position; 
A good attitude is a close attitude; 
A good soldier is a brave soldier 
A good time is an idle time; 
A good life is a blessed life; 
A good car is a new car; 
A good party is an informal party.  
 
From the definitions in the previous sections, 
one can easily see that the sign of a S-factor score 
depends on the order in which the semantically 
opposite pairs are considered. If one wishes to 
have a consistent interpretation of the factor scores 
(e.g. negative scores are ?bad? and positive scores 
are ?good?) the synset ordering in the S-factors is 
24
significant. We used a default ordering of 
antonyms in all factors, yet a text analyst could 
modify this ordering. For each POS, we selected a 
representative factor for which the synset order, 
from a subjective point of view, was very intuitive. 
For instance, for the adjective factors we selected 
the factor <good:1, bad:1>, for noun factors we 
selected the factor <order:5, disorder:2>, and for 
verb factors we selected the factor <succeed:1, 
fail:2>, the first word sense in each of the 
representative factors having a clear positive 
connotation. Then for each POS factor <S?, S?> we 
computed the distance of its constituents to the 
synsets of the representative factor of the same 
POS. The one that was closer to the ?positive? side 
of the reference factor was also considered 
?positive? and the order of the synsets was 
established accordingly. This empirical approach 
proved to be successful for most of the factors, 
except for a couple of them, which were manually 
ordered. 
We developed an application that allows text 
analysts to choose the S-factors they would like to 
work with. The interface allows the user to both 
select/deselect factors and to switch the order of 
the poles in any given factor.  Once the user 
decided on the relevant S-factors, the synsets are 
marked up according to the selected S-factors. This 
version of the WordNet can be saved and used as 
needed in the planned application. 
5 Extending the LUCs and SUCs  
Although the maximum semantic coverage of the 
S-factors for the adjectives contains more than 
28% of the PWN3.0 adjectival synsets, many 
adjectives with connotative potential are not in this 
coverage. This happens because the definition of 
the relatedness (Definition 1) implicitly assumes 
the existence of synonyms for one or more senses 
of a given word. Therefore from mono-semantic 
words in mono-literal synsets a path towards other 
synsets cannot be constructed anymore. Because of 
this, there are isolated ?bubbles? of related synsets 
that are not connected with synsets in maximum 
semantic coverage. In order to assign values to at 
least a part of these synsets, we experimented with 
various strategies out of which the one described 
herein was considered the easiest to implement 
and, to some extent motivated, from a conceptual 
point of view. The approach is similar for all the 
synsets which are not in the SUCs, but the 
algorithms for extending these coverages slightly 
differ depending on the part of speech under 
consideration.  
Class E-LUCs E-SUCs 
Adjectives  7,124 (33.07%) 6,216 (34.23%) 
Nouns  27,614 (23.19%) 22,897 (27.88%) 
Verbs  8,910 (77.26%) 10,798 (78.43%) 
Adverbs 1,838 (41.01%) 1,787 (49.35%) 
Table 3: Extended LUCs and SUCs 
The basic idea is to transfer the vectors of the 
synsets in SUC to those in the complementary set 
SUC , provided they have ?similar meanings?. We 
say that POS
POS
i SUCS ?  and POSPOSj SUCS ?  
have ?similar meanings? if ????/????(?????) =????/????(?????)  and ?????  and ?????  are 
directly linked by a semantic WordNet relation of a 
certain type. For adjectival synsets we consider the 
relations similar_to and also_see, for verbal 
synsets we consider the relations hyponym and 
also_see, and for the nominal synsets we take into 
account only the hyponymy. Consequently, the S-
factors coverage increased as shown in Table 3. 
6 A Preliminary Comparison with 
SentiWordnet 3.0  
SentiWordNet 3.0 (Baccianella, et al 2010) is the 
only public resource we are aware of, which 
considers sense distinctions and covers all synsets 
in Princeton WordNet 3.0. Although in 
SentiWordNet (henceforth SWN3.0) only the 
Subjective-Objective dichotomy is marked-up, 
with a further distinction between Positive-
Subjectivity and Negative-Subjectivity, using it for 
the comparison with our annotations is meaningful 
and relevant for both approaches. First, the 
connotative meanings are subjective meanings.  
Then, while the SWN3.0 mark-up is based on ML 
techniques and various heuristics exploiting the 
structure of PWN3.0 and some other external 
resources, the differential semantics approach, as 
implemented here, is a deterministic one, 
considering only the content and structural 
information in PWN3.0 + SUMO/MILO. 
Identifying contradictions in the two annotations 
might reveal limitations in the ML techniques and 
heuristics used by SWN3.0 on one hand, and, on 
25
the other hand, flaws in our method, possible 
incompleteness or inconsistencies in PWN3.0+ 
SUMO/MILO. It has to be noted that the possible 
incompleteness or inconsistencies in PWN3.0 
would also affect the accuracy of the SWN3.0 
values. 
 
Synset SWN DSA Definition 
dangerous, 
grave 
grievous, 
serious, severe 
? 
-0,63 0,42 
causing fear or 
anxiety by threatening 
great harm 
live 0,5 -0,5 exerting force or 
containing energy 
bastardly, 
mean 
-0,5 0,5 of no value or worth 
dangerous, 
unsafe -0,75 0,5 
involving or causing 
danger or risk; liable 
to hurt or harm 
delirious, 
excited,  
unrestrained,  
mad, 
frantic  
0,5 -0,5 
marked by un-
controlled excitement 
or emotion 
haunted 0,5 -0,43 showing emotional 
affliction or disquiet 
impeccable -0,63 0,8 not capable of sin 
evil, vicious 0,5 -0,75 
having the nature of 
vice 
delectable, 
sexually 
attractive 
0,63 -0,5 
capable of arousing 
desire 
ordinary 
 
-0,5 0,75 
not exceptional in any 
way especially in 
quality or ability or 
size or degree 
serious 
 
-0,75 0,75 
requiring effort or 
concentration; 
complex and not easy 
to answer or solve 
excusable 
 
0,63 -0,4 capable of being 
overlooked 
Table 4: Examples of divergent scores among the 
SWN3.0 and DSA 
For the partial comparison we selected the 
adjectives in SWN3.0 with Positive-Subjectivity or 
Negative-Subjectivity greater than or equal to 0.5. 
From our differential semantic (DSA) annotation 
we extracted all the adjectives which along the 
good-bad differential dimension had an absolute 
value greater than 0.4. Those adjectives closer to 
good were considered to be Subjective-Positive 
while the others were considered to be Subjective-
Negative. The threshold value was empirically 
selected, by observing that beyond the 0.4 and ?0.4 
values the factorial annotation was closer to our 
intuition concerning the connotative load of the 
analyzed words. We computed the intersection of 
the two adjectival synsets extracted this way and 
retained only the synsets contradictorily annotated. 
We found only 150 differences, which by itself is a 
small difference, showing that, at least with respect 
to the good-bad factor, SWN3.0 and DSA 
annotations are to a large extent consistent. 
We manually checked-out the 150 synsets 
marked-up with contradictory scores and the 
authors and 6 MSc students negotiated the scores 
towards reaching the consensus. For 142 of these 
synsets the consensus was easily reached with 76 
considered to be correct in the DSA annotation and 
65 correct in the SWN3.0 annotation. Table 4 
shows some examples of synsets, the scores of 
which were correctly judged (in bold) either by 
SWN3.0 or DSA as well as some examples of non-
consensual annotations (in underlined italics). 
7 Conclusions  
Differential semantics annotation addresses the 
connotative meanings of the lexical stock, the 
denotative meanings of which are recorded in 
WordNet 3.0. We revised and improved our 
previous method (Tufi? and ?tef?nescu, 2010). It 
generalizes the SWN3.0 subjectivity mark-up, 
according to a user-based multi-criteria differential 
semantics model.  
The partial comparison with SWN3.0 revealed 
specific limitations of our approach. The major one 
is due to the definitions of n-relatedness and the 
TRI relation. The problem resides in indiscriminate 
treatment of literals which have senses with 
different polarities with respect to a factor. If one 
of these senses is significantly closer to one of the 
poles of the factor, that sense might impose the 
sign for the rest of the senses. This risk is 
amplified when literals with high degrees of 
polysemy and/or high degrees of synonymy are 
reached on the way from the synset of interest to 
the synsets defining the S-factor (higher the 
polysemy/synonymy, higher the number of paths 
to the constituents of the S-factor). Most of the 
erroneous scores we noticed were explained by this 
drawback. We say that the words affected by this 
limitation of the current algorithm have a 
significant connotation shift potential (Tufi?, 
2009), (?tef?nescu, 2010). As such words could 
generate undesired implicatures, they should be 
26
avoided in formal texts and replaced by synonyms 
with less connotation shift potential.  
We also observed some inconsistencies 
regarding the association of SUMO/MILO (and the 
additional domain ontologies) concepts to PWN 
3.0 synsets. The semantic types of two opposable 
synsets (in the same semantic field) should be 
closely related, if not the same. However, for some 
S-factors, such as <agreement:3, disagreement:1> 
this does not happen. The semantic type of the 
synset {agreement:3?} is ?Cooperation?, while 
the semantic type of {disagreement:1?} is 
?SubjectiveAssessmentAttribute?. ?Cooperation? 
is a ?Process? (subsumed by ?Physical?) but, 
?SubjectiveAssessmentAttribute? is an ?Attribute? 
(subsumed by ?Abstract?). There are 9 such cases 
for nouns, 30 for verbs and 16 for adjectives. 
The current multi-factored annotation vectors 
for nominal, verbal, and adjectival synsets for 
PWN3.0, as well as an application to manage these 
annotations, can be freely downloaded from 
http://www.racai.ro/differentialsemantics/. 
Acknowledgments 
This research has been supported by the grant no. 
ID_1443, awarded by the Romanian Ministry for 
Education, Research, Youth and Sport. We thank 
also to SentiWordNet authors for making it public. 
References  
Andreevskaia Alina and Sabine Bergler. 2006. Mining 
WordNet for a fuzzy sentiment: Sentiment tag 
extraction from WordNet glosses. In Proceedings of 
the 11th Conference of the European Chapter of the 
Association for Computational Linguistics (EACL-
2006), Trento, Italy, pages 209?216.  
Stefano Baccianella, Andrea Esuli, and Fabrizio 
Sebastiani. 2010. SENTIWORDNET 3.0: An 
Enhanced Lexical Resource for Sentiment Analysis 
and Opinion Mining, in Proceedings of LREC2010, 
Malta, pp.2200-2204. 
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, 
and Emanuele Pianta. 2004. Revising WordNet 
domains hierarchy: Semantics, coverage, and 
balancing. In Proceedings of COLING 2004 
Workshop on "Multilingual Linguistic Resources", 
Geneva, Switzerland, pages 101?108. 
Andrea Esuli, and Fabrizio Sebastiani. 2006. 
SENTIWORDNET: A publicly available lexical 
resource for opinion mining. In Proceedings of the 
5th Conference on Language Resources and 
Evaluation LREC-06, Genoa, Italy, pages 417?422. 
See also: http://sentiwordnet.isti.cnr.it/  
Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. Academic Press, Cambridge, MA. 
Jaap Kamps and Maarten Marx. 2002. Words with 
attitude. In Proceedings of the 1st International 
WordNet Conference, Mysore, India, pages 332?341. 
Rada Mihalcea, Carmen Banea, and Janice Wiebe. 
2007. Learning multilingual subjective language via 
cross-lingual projections. In Proceedings of the 45th 
Annual Meeting of the Association of Computational 
Linguistics, Prague, Czech Republic, pages 976?983. 
Ian Niles and Adam Pease. 2003. Linking Lexicons and 
Ontologies: Mapping WordNet to the Suggested 
Upper Merged Ontology. In Proceedings of the 2003 
International Conference on Information and 
Knowledge Engineering (IKE 03), Las Vegas, pages 
23?26. 
Charles E. Osgood, George Suci and Percy 
Tannenbaum. 1957. The measurement of meaning, 
University of Illinois Press, Urbana IL. 
Bo Pang and Lillian Lee, 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1?2): 1?135. 
Livia  Polanyi, and Annie Zaenen. 2006. Contextual 
valence shifters. In J. G. Shanahan, Y. Qu and J. 
Wiebe, editors, Computing Attitude and Affect in 
Text: Theory and Applications, The Information 
Retrieval Series, Vol. 20, Springer Verlag, 
Dordrecht, Netherlands, pages 1-10. 
Dan ?tef?nescu. 2010. Intelligent Information Mining 
from Multilingual Corpora (in Romanian). PhD 
Thesis, Romanian Academy, Bucharest. 
Dan Tufi?. 2008. Mind your words! You might convey 
what you wouldn?t like to. Int. J. of Computers, 
Communications & Control, III, pages 139?143. 
Dan Tufi?. 2009. Playing with word meanings,.In Lotfi 
A. Zadeh, Dan Tufi?, Florin Gh. Filip and Ioan 
Dzi?ac, (editors) From Natural Language to Soft 
Computing: New Paradigms in Artificial 
Intelligence. Publishing House of the Romanian 
Academy, Bucharest, pages 211?223. 
Dan Tufi?, Dan ?tef?nescu. 2010. A Differential 
Semantics Approach to the Annotation of the Synsets 
in WordNet. In Proceedings of LREC 2010, Malta, 
pages 3173-3180 
Alessandro Valitutti, Carlo Strapparava, and Oliviero 
Stock. 2004. Developing affective lexical resources, 
Psychology Journal, 2(1), pages 61?83. 
27
