Automatic Learning of Language Model Structure
Kevin Duh and Katrin Kirchhoff
Department of Electrical Engineering
University of Washington, Seattle, USA
{duh,katrin}@ee.washington.edu
Abstract
Statistical language modeling remains a challeng-
ing task, in particular for morphologically rich lan-
guages. Recently, new approaches based on factored
language models have been developed to address
this problem. These models provide principled ways
of including additional conditioning variables other
than the preceding words, such as morphological or
syntactic features. However, the number of possible
choices for model parameters creates a large space of
models that cannot be searched exhaustively. This
paper presents an entirely data-driven model selec-
tion procedure based on genetic search, which is
shown to outperform both knowledge-based and ran-
dom selection procedures on two different language
modeling tasks (Arabic and Turkish).
1 Introduction
In spite of novel algorithmic developments and
the increased availability of large text corpora,
statistical language modeling remains a diffi-
cult problem, particularly for languages with
rich morphology. Such languages typically ex-
hibit a large number of word types in relation
to word tokens in a given text, which leads
to high perplexity and a large number of un-
seen word contexts. As a result, probability es-
timates are often unreliable, even when using
standard smoothing and parameter reduction
techniques. Recently, a new language model-
ing approach, called factored language models
(FLMs), has been developed. FLMs are a gen-
eralization of standard language models in that
they allow a larger set of conditioning variables
for predicting the current word. In addition to
the preceding words, any number of additional
variables can be included (e.g. morphological,
syntactic, or semantic word features). Since
such features are typically shared across mul-
tiple words, they can be used to obtained bet-
ter smoothed probability estimates when train-
ing data is sparse. However, the space of pos-
sible models is extremely large, due to many
different ways of choosing subsets of condition-
ing word features, backoff procedures, and dis-
counting methods. Usually, this space cannot
be searched exhaustively, and optimizing mod-
els by a knowledge-inspired manual search pro-
cedure often leads to suboptimal results since
only a small portion of the search space can
be explored. In this paper we investigate the
possibility of determining the structure of fac-
tored language models (i.e. the set of condition-
ing variables, the backoff procedure and the dis-
counting parameters) by a data-driven search
procedure, viz. Genetic Algorithms (GAs). We
apply this technique to two different tasks (lan-
guage modeling for Arabic and Turkish) and
show that GAs lead to better models than ei-
ther knowledge-inspired manual search or ran-
dom search. The remainder of this paper is
structured as follows: Section 2 describes the
details of the factored language modeling ap-
proach. The application of GAs to the problem
of determining language model structure is ex-
plained in Section 3. The corpora used in the
present study are described in Section 4 and ex-
periments and results are presented in Section 5.
Section 6 compares the present study to related
work and Section 7 concludes.
2 Factored Language Models
A standard statistical language model com-
putes the probability of a word sequence W =
w1, w2, ..., wT as a product of conditional prob-
abilities of each word wi given its history, which
is typically approximated by just one or two pre-
ceding words (leading to bigrams, and trigrams,
respectively). Thus, a trigram language model
is described by
p(w1, ..., wT ) ?
T
?
i=3
p(wi|wi?1, wi?2) (1)
Even with this limitation, the estimation of
the required probabilities is challenging: many
word contexts may be observed infrequently
or not at all, leading to unreliable probabil-
ity estimates under maximum likelihood estima-
tion. Several techniques have been developed
to address this problem, in particular smooth-
ing techniques (Chen and Goodman, 1998) and
class-based language models (Brown and oth-
ers, 1992). In spite of such parameter reduc-
tion techniques, language modeling remains a
difficult task, in particular for morphologically
rich languages, e.g. Turkish, Russian, or Arabic.
Such languages have a large number of word
types in relation to the number of word tokens
in a given text, as has been demonstrated in
a number of previous studies (Geutner, 1995;
Kiecza et al, 1999; Hakkani-Tu?r et al, 2002;
Kirchhoff et al, 2003). This in turn results
in a high perplexity and in a large number of
out-of-vocabulary (OOV) words when applying
a trained language model to a new unseen text.
2.1 Factored Word Representations
A recently developed approach that addresses
this problem is that of Factored Language Mod-
els (FLMs) (Kirchhoff et al, 2002; Bilmes and
Kirchhoff, 2003), whose basic idea is to decom-
pose words into sets of features (or factors) in-
stead of viewing them as unanalyzable wholes.
Probabilistic language models can then be con-
structed over (sub)sets of word features instead
of, or in addition to, the word variables them-
selves. For instance, words can be decomposed
into stems/lexemes and POS tags indicating
their morphological features, as shown below:
Word: Stock prices are rising
Stem: Stock price be rise
Tag: Nsg N3pl V3pl Vpart
Such a representation serves to express lexical
and syntactic generalizations, which would oth-
erwise remain obscured. It is comparable to
class-based representations employed in stan-
dard class-based language models; however, in
FLMs several simultaneous class assignments
are allowed instead of a single one. In general,
we assume that a word is equivalent to a fixed
number (K) of factors, i.e. W ? f1:K . The task
then is to produce a statistical model over the
resulting representation - using a trigram ap-
proximation, the resulting probability model is
as follows:
p(f1:K1 , f1:K2 , ..., f1:KT ) ?
T
?
t=3
p(f1:Kt |f1:Kt?1 , f1:Kt?2 )
(2)
Thus, each word is dependent not only on a sin-
gle stream of temporally ordered word variables,
but also on additional parallel (i.e. simultane-
ously occurring) features. This factored repre-
sentation can be used in two different ways to
improve over standard LMs: by using a product
model or a backoff model. In a product model,
Equation 2 can be simplified by finding con-
ditional independence assumptions among sub-
sets of conditioning factors and computing the
desired probability as a product of individual
models over those subsets. In this paper we
only consider the second option, viz. using the
factors in a backoff procedure when the word
n-gram is not observed in the training data.
For instance, a word trigram that is found in
an unseen test set may not have any counts in
the training set, but its corresponding factors
(e.g. stems and morphological tags) may have
been observed since they also occur in other
words.
2.2 Generalized parallel backoff
Backoff is a common smoothing technique in
language modeling. It is applied whenever
the count for a given n-gram in the training
data falls below a certain threshold ? . In that
case, the maximum-likelihood estimate of the
n-gram probability is replaced with a probabil-
ity derived from the probability of the lower-
order (n ? 1)-gram and a backoff weight. N-
grams whose counts are above the threshold re-
tain their maximum-likelihood estimates, dis-
counted by a factor that re-distributes proba-
bility mass to the lower-order distribution:
pBO(wt|wt?1, wt?2) (3)
=
{
dcpML(wt|wt?1, wt?2) if c > ?3
?(wt?1, wt?2)pBO(wt|wt?1) otherwise
where c is the count of (wt, wt?1, wt?2), pML
denotes the maximum-likelihood estimate and
dc is a discounting factor that is applied to the
higher-order distribution. The way in which the
discounting factor is estimated determines the
actual smoothing method (e.g. Good-Turing,
Kneser-Ney, etc.) The normalization factor
?(wt?1, wt?2) ensures that the entire distribu-
tion sums to one. During standard backoff, the
most distant conditioning variable (in this case
wt?2) is dropped first, then the second most dis-
tant variable etc. until the unigram is reached.
This can be visualized as a backoff path (Fig-
ure 1(a)). If the only variables in the model are
words, such a backoff procedure is reasonable.
tW 1tW? 2tW? 3tW?
tW 1tW? 2tW?
tW 1tW?
tW
(a)
F 1F 2F 3F
F
F 1F 2F F 1F 3F F 2F 3F
F 1F F 3FF 2F
(b)
Figure 1: Standard backoff path for a 4-gram lan-
guage model over words (left) and backoff graph for
4-gram over factors (right).
However, if variables occur in parallel, i.e. do
not form a temporal sequence, it is not imme-
diately obvious in which order they should be
dropped. In this case, several backoff paths are
possible, which can be summarized in a backoff
graph (Figure 1(b)). In principle, there are sev-
eral different ways of choosing among different
paths in this graph:
1. Choose a fixed, predetermined backoff path
based on linguistic knowledge, e.g. always drop
syntactic before morphological variables.
2. Choose the path at run-time based on statis-
tical criteria.
3. Choose multiple paths and combine their
probability estimates.
The last option, referred to as parallel backoff,
is implemented via a new, generalized backoff
function (here shown for a 4-gram):
pGBO(f |f1, f2, f3) (4)
=
{
dcpML(f |f1, f2, f3) if c > ?4
?(f1, f2, f3)g(f, f1, f2, f3) otherwise
where c is the count of (f, f1, f2, f3),
pML(f |f1, f2, f3) is the maximum likeli-
hood distribution, ?4 is the count threshold,
and ?(f1, f2, f3) is the normalization factor.
The function g(f, f1, f2, f3) determines the
backoff strategy. In a typical backoff proce-
dure g(f, f1, f2, f3) equals pBO(f |f1, f2). In
generalized parallel backoff, however, g can be
any non-negative function of f, f1, f2, f3. In
our implementation of FLMs (Kirchhoff et al,
2003) we consider several different g functions,
including the mean, weighted mean, product,
and maximum of the smoothed probability
distributions over all subsets of the conditioning
factors. In addition to different choices for g,
different discounting parameters can be chosen
at different levels in the backoff graph. For
instance, at the topmost node, Kneser-Ney
discounting might be chosen whereas at a
lower node Good-Turing might be applied.
FLMs have been implemented as an add-on
to the widely-used SRILM toolkit1 and have
been used successfully for the purpose of
morpheme-based language modeling (Bilmes
and Kirchhoff, 2003), multi-speaker language
modeling (Ji and Bilmes, 2004), and speech
recognition (Kirchhoff et al, 2003).
3 Learning FLM Structure
In order to use an FLM, three types of para-
meters need to be specified: the initial con-
ditioning factors, the backoff graph, and the
smoothing options. The goal of structure learn-
ing is to find the parameter combinations that
create FLMs that achieve a low perplexity on
unseen test data. The resulting model space
is extremely large: given a factored word rep-
resentation with a total of k factors, there are
?k
n=1
(k
n
)
possible subsets of initial condition-
ing factors. For a set of m conditioning factors,
there are up to m! backoff paths, each with its
own smoothing options. Unless m is very small,
exhaustive search is infeasible. Moreover, non-
linear interactions between parameters make it
difficult to guide the search into a particular
direction, and parameter sets that work well
for one corpus cannot necessarily be expected
to perform well on another. We therefore need
an automatic way of identifying the best model
structure. In the following section, we describe
the application of genetic-based search to this
problem.
3.1 Genetic Algorithms
Genetic Algorithms (GAs) (Holland, 1975) are a
class of evolution-inspired search/optimization
techniques. They perform particularly well
in problems with complex, poorly understood
search spaces. The fundamental idea of GAs is
to encode problem solutions as (usually binary)
strings (genes), and to evolve and test successive
populations of solutions through the use of ge-
netic operators applied to the encoded strings.
Solutions are evaluated according to a fitness
function which represents the desired optimiza-
tion criterion. The individual steps are as fol-
1We would like to thank Jeff Bilmes for providing and
supporting the software.
lows:
Initialize: Randomly generate a set (popula-
tion) of strings.
While fitness improves by a certain threshold:
Evaluate fitness: calculate each string?s fitness
Apply operators: apply the genetic operators
to create a new population.
The genetic operators include the probabilis-
tic selection of strings for the next genera-
tion, crossover (exchanging subparts of differ-
ent strings to create new strings), and muta-
tion (randomly altering individual elements in
strings). Although GAs provide no guarantee
of finding the optimal solution, they often find
good solutions quickly. By maintaining a pop-
ulation of solutions rather than a single solu-
tion, GA search is robust against premature
convergence to local optima. Furthermore, solu-
tions are optimized based on a task-specific fit-
ness function, and the probabilistic nature of ge-
netic operators helps direct the search towards
promising regions of the search space.
3.2 Structure Search Using GA
In order to use GAs for searching over FLM
structures (i.e. combinations of conditioning
variables, backoff paths, and discounting op-
tions), we need to find an appropriate encoding
of the problem.
Conditioning factors
The initial set of conditioning factors F are
encoded as binary strings. For instance, a
trigram for a word representation with three
factors (A,B,C) has six conditioning variables:
{A?1, B?1, C?1, A?2, B?2, C?2} which can be
represented as a 6-bit binary string, with a bit
set to 1 indicating presence and 0 indicating ab-
sence of a factor in F . The string 10011 would
correspond to F = {A?1, B?2, C?2}.
Backoff graph
The encoding of the backoff graph is more dif-
ficult because of the large number of possible
paths. A direct approach encoding every edge
as a bit would result in overly long strings, ren-
dering the search inefficient. Our solution is to
encode a binary string in terms of graph gram-
mar rules (similar to (Kitano, 1990)), which
can be used to describe common regularities in
backoff graphs. For instance, a node with m
factors can only back off to children nodes with
m ? 1 factors. For m = 3, the choices for pro-
ceeding to the next-lower level in the backoff
1. {X1 X2 X3} ?> {X1 X2}
2. {X1 X2 X3} ?> {X1 X3}
3. {X1 X2 X3} ?> {X2 X3}
4.      {X1 X2}  ?> {X1}
5.      {X1 X2}  ?> {X2}
PRODUCTION RULES:
AB
ABC
AB
ABC
BC AB
ABC
BC
A B
AB
ABC
BC
A B
 
 







0
1
4 4
10110
3
(b) Generation of Backoff Graph by rules 1, 3, and 4
(a) Gene activates production rules
GENE:
Figure 2: Generation of Backoff Graph from pro-
duction rules selected by the gene 10110.
graph can thus be described by the following
grammar rules:
RULE 1: {x1, x2, x3} ? {x1, x2}
RULE 2: {x1, x2, x3} ? {x1, x3}
RULE 3: {x1, x2, x3} ? {x2, x3}
Here xi corresponds to the factor at the ith
position in the parent node. Rule 1 indicates
a backoff that drops the third factor, Rule 2
drops the second factor, etc. The choice of
rules used to generate the backoff graph is en-
coded in a binary string, with 1 indicating the
use and 0 indicating the non-use of a rule, as
shown schematically in Figure 2. The presence
of two different rules at the same level in the
backoff graph corresponds to parallel backoff;
the absence of any rule (strings consisting only
of 0 bits) implies that the corresponding backoff
graph level is skipped and two conditioning vari-
ables are dropped simultaneously. This allows
us to encode a graph using few bits but does not
represent all possible graphs. We cannot selec-
tively apply different rules to different nodes at
the same level ? this would essentially require
a context-sensitive grammar, which would in
turn increase the length of the encoded strings.
This is a fundamental tradeoff between the most
general representation and an encoding that is
tractable. Our experimental results described
below confirm, however, that sufficiently good
results can be obtained in spite of the above
limitation.
Smoothing options
Smoothing options are encoded as tuples of in-
tegers. The first integer specifies the discount-
ing method while second indicates the minimum
count required for the n-gram to be included in
the FLM. The integer string consists of succes-
sive concatenated tuples, each representing the
smoothing option at a node in the graph. The
GA operators are applied to concatenations of
all three substrings describing the set of factors,
backoff graph, and smoothing options, such that
all parameters are optimized jointly.
4 Data
We tested our language modeling algorithms on
two different data sets from two different lan-
guages, Arabic and Turkish.
The Arabic data set was drawn from the
CallHome Egyptian Conversational Arabic
(ECA) corpus (LDC, 1996). The training,
development, and evaluation sets contain
approximately 170K, 32K, and 18K words,
respectively. The corpus was collected for the
purpose of speech recognizer development for
conversational Arabic, which is mostly dialectal
and does not have a written standard. No
additional text material beyond transcriptions
is available in this case; it is therefore im-
portant to use language models that perform
well in sparse data conditions. The factored
representation was constructed using linguistic
information from the corpus lexicon, in combi-
nation with automatic morphological analysis
tools. It includes, in addition to the word, the
stem, a morphological tag, the root, and the
pattern. The latter two are components which
when combined form the stem. An example
of this factored word representation is shown
below:
Word:il+dOr/Morph:noun+masc-sg+article/
Stem:dOr/Root:dwr/Pattern:CCC
For our Turkish experiments we used a mor-
phologically annotated corpus of Turkish
(Hakkani-Tu?r et al, 2000). The annotation
was performed by applying a morphological
analyzer, followed by automatic morphological
disambiguation as described in (Hakkani-Tu?r
et al, 2002). The morphological tags consist
of the initial root, followed by a sequence of
inflectional groups delimited by derivation
boundaries (?DB). A sample annotation (for
the word yararlanmak, consisting of the root
yarar plus three inflectional groups) is shown
below:
yararmanlak:
yarar+Noun+A3sg+Pnon+Nom
?DB+Verb+Acquire+Pos
?DB+Noun+Inf+A3sg+Pnon+Nom
We removed segmentation marks (for titles
and paragraph boundaries) from the corpus
but included punctuation. Words may have
different numbers of inflectional groups, but
the FLM representation requires the same
number of factors for each word; we therefore
had to map the original morphological tags to
a fixed-length factored representation. This
was done using linguistic knowledge: according
to (Oflazer, 1999), the final inflectional group
in each dependent word has a special status
since it determines inflectional markings on
head words following the dependent word.
The final inflectional group was therefore
analyzed into separate factors indicating the
number (N), case (C), part-of-speech (P) and
all other information (O). Additional factors
for the word are the root (R) and all remaining
information in the original tag not subsumed
by the other factors (G). The word itself is
used as another factor (W). Thus, the above
example would be factorized as follows:
W:yararlanmak/R:yarar/P:NounInf-N:A3sg/
C:Nom/O:Pnon/G:NounA3sgPnonNom+Verb
+Acquire+Pos
Other factorizations are certainly possible;
however, our primary goal is not to find the
best possible encoding for our data but to
demonstrate the effectiveness of the FLM
approach, which is largely independent of the
choice of factors. For our experiments we used
subsets of 400K words for training, 102K words
for development and 90K words for evaluation.
5 Experiments and Results
In our application of GAs to language model
structure search, the perplexity of models with
respect to the development data was used as
an optimization criterion. The perplexity of
the best models found by the GA were com-
pared to the best models identified by a lengthy
manual search procedure using linguistic knowl-
edge about dependencies between the word fac-
tors involved, and to a random search procedure
which evaluated the same number of strings as
the GA. The following GA options gave good
results: population size 30-50, crossover proba-
bility 0.9, mutation probability 0.01, Stochastic
Universal Sampling as the selection operator, 2-
point crossover. We also experimented with re-
initializing the GA search with the best model
found in previous runs. This method consis-
tently improved the performance of normal GA
search and we used it as the basis for the results
reported below. Due to the large number of fac-
N Word Hand Rand GA ? (%)
Dev Set
2 593.8 555.0 556.4 539.2 -2.9
3 534.9 533.5 497.1 444.5 -10.6
4 534.8 549.7 566.5 522.2 -5.0
Eval Set
2 609.8 558.7 525.5 487.8 -7.2
3 545.4 583.5 509.8 452.7 -11.2
4 543.9 559.8 574.6 527.6 -5.8
Table 1: Perplexity for Turkish language models. N
= n-gram order, Word = word-based models, Hand
= manual search, Rand = random search, GA =
genetic search.
tors in the Turkish word representation, models
were only optimized for conditioning variables
and backoff paths, but not for smoothing op-
tions. Table 1 compares the best perplexity re-
sults for standard word-based models and for
FLMs obtained using manual search (Hand),
random search (Rand), and GA search (GA).
The last column shows the relative change in
perplexity for the GA compared to the better
of the manual or random search models. For
tests on both the development set and evalu-
ation set, GA search gave the lowest perplex-
ity. In the case of Arabic, the GA search was
N Word Hand Rand GA ? (%)
Dev Set
2 229.9 229.6 229.9 222.9 -2.9
3 229.3 226.1 230.3 212.6 -6.0
Eval Set
2 249.9 230.1 239.2 223.6 -2.8
3 285.4 217.1 224.3 206.2 -5.0
Table 2: Perplexity for Arabic language models
(w/o unknown words).
performed over conditioning factors, the back-
off graph, and smoothing options. The results
in Table 2 were obtained by training and test-
ing without consideration of out-of-vocabulary
(OOV) words. Our ultimate goal is to use these
language models in a speech recognizer with a
fixed vocabulary, which cannot recognize OOV
words but requires a low perplexity for other
N Word Hand Rand GA ? (%)
Dev Set
2 236.0 195.5 198.5 193.3 -1.1
3 237.0 199.0 202.0 188.1 -5.5
Eval Set
2 235.2 234.1 247.7 233.4 -0.7
3 253.9 229.2 219.0 212.2 -3.1
Table 3: Perplexity for Arabic language models
(with unknown words).
word combinations. In a second experiment,
we trained the same FLMs from Table 2 with
OOV words included as the unknown word to-
ken. Table 3 shows the results. Again, we see
that the GA outperforms other search methods.
The best language models all used parallel back-
off and different smoothing options at different
backoff graph nodes. The Arabic models made
use of all conditioning variables (Word, Stem,
Root, Pattern, and Morph) whereas the Turkish
models used only the W, P, C, and R variables
(see above Section 4).
6 Related Work
Various previous studies have investigated the
feasibility of using units other than words for
language modeling (e.g. (Geutner, 1995; C?arki
et al, 2000; Kiecza et al, 1999)). However,
in all of these studies words were decomposed
into linear sequences of morphs or morph-like
units, using either linguistic knowledge or data-
driven techniques. Standard language models
were then trained on the decomposed represen-
tations. The resulting models essentially ex-
press statistical relationships between morphs,
such as stems and affixes. For this reason, a
context larger than that provided by a trigram
is typically required, which quickly leads to
data-sparsity. In contrast to these approaches,
factored language models encode morphological
knowledge not by altering the linear segmenta-
tion of words but by encoding words as parallel
bundles of features.
The general possibility of using multiple con-
ditioning variables (including variables other
than words) has also been investigated by
(Dupont and Rosenfeld, 1997; Gildea, 2001;
Wang, 2003; Zitouni et al, 2003). Mostly, the
additional variables were general word classes
derived by data-driven clustering procedures,
which were then arranged in a backoff lattice
or graph similar to the present procedure. All
of these studies assume a fixed path through
the graph, which is usually obtained by an
ordering from more specific probability distri-
butions to more general distributions. Some
schemes also allow two or more paths to be
combined by weighted interpolation. FLMs, by
contrast, allow different paths to be chosen at
run-time, they support a wider range of combi-
nation methods for probability estimates from
different paths, and they offer a choice of dif-
ferent discounting options at every node in the
backoff graph. Most importantly, however, the
present study is to our knowledge the first to
describe an entirely data-driven procedure for
identifying the best combination of parameter
choices. The success of this method will facili-
tate the rapid development of FLMs for differ-
ent tasks in the future.
7 Conclusions
We have presented a data-driven approach to
the selection of parameters determining the
structure and performance of factored language
models, a class of models which generalizes
standard language models by including addi-
tional conditioning variables in a principled
way. In addition to reductions in perplexity ob-
tained by FLMs vs. standard language models,
the data-driven model section method further
improved perplexity and outperformed both
knowledge-based manual search and random
search.
Acknowledgments
We would like to thank Sonia Parandekar for the ini-
tial version of the GA code. This material is based
upon work supported by the NSF and the CIA un-
der NSF Grant No. IIS-0326276. Any opinions, find-
ings, and conclusions expressed in this material are
those of the authors and do not necessarily reflect
the views of these agencies.
References
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In Proceedings of HLT/NACCL, pages 4?6.
P.F. Brown et al 1992. Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467?479.
K. C?arki, P. Geutner, and T. Schultz. 2000. Turkish
LVCSR: towards better speech recognition for ag-
glutinative languages. In Proceedings of ICASSP.
S. F. Chen and J. Goodman. 1998. An empirical
study of smoothing techniques for language mod-
eling. Technical Report Tr-10-98, Center for Re-
search in Computing Technology, Harvard Uni-
versity.
P. Dupont and R. Rosenfeld. 1997. Lattice based
language models. Technical Report CMU-CS-97-
173, Department of Computer Science, CMU.
P. Geutner. 1995. Using morphology towards better
large-vocabulary speech recognition systems. In
Proceedings of ICASSP, pages 445?448.
D. Gildea. 2001. Statistical Language Understand-
ing Using Frame Semantics. Ph.D. thesis, Uni-
versity of California, Berkeley.
D. Hakkani-Tu?r, K. Oflazer, and Go?khan Tu?r. 2000.
Statistical morphological disambiguation for ag-
glutinative languages. In Proceedings of COL-
ING.
D. Hakkani-Tu?r, K. Oflazer, and Go?khan Tu?r. 2002.
Statistical morphological disambiguation for ag-
glutinative languages. Journal of Computers and
Humanities, 36(4).
J.H. Holland. 1975. Adaptation in Natural and Ar-
tificial Systems. University of Michigan Press.
Gand Ji and Jeff Bilmes. 2004. Multi-speaker lan-
guage modeling. In Proceedings of HLT/NAACL,
pages 137?140.
D. Kiecza, T. Schultz, and A. Waibel. 1999. Data-
driven determination of appropriate dictionary
units for Korean LVCSR. In Proceedings of
ICASSP, pages 323?327.
K. Kirchhoff et al 2002. Novel speech recognition
models for Arabic. Technical report, Johns Hop-
kins University.
K. Kirchhoff et al 2003. Novel approaches to Ara-
bic speech recognition: Report from 2002 Johns-
Hopkins summer workshop. In Proceedings of
ICASSP, pages I?344?I?347.
Hiroaki Kitano. 1990. Designing neural networks
using genetic algorithms with graph generation
system. Complex Systems, pages 461?476.
LDC. 1996. http://www.ldc.upenn.edu/Catalog/-
LDC99L22.html.
K. Oflazer. 1999. Dependency parsing with an ex-
tended finite state approach. In Proceedings of the
37th ACL.
W. Wang. 2003. Factorization of language
models through backing off lattices. Com-
putation and Language E-print Archive,
oai:arXiv.org/cs/0305041.
I. Zitouni, O. Siohan, and C.-H. Lee. 2003. Hierar-
chical class n-gram language models: towards bet-
ter estimation of unseen events in speech recogni-
tion. In Proceedings of Eurospeech - Interspeech,
pages 237?240.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 160?167,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilingual Dependency Parsing using Bayes Point Machines
Simon Corston-Oliver
Microsoft Research
One Microsoft Way
Redmond, WA 98052
simonco@microsoft.com
Anthony Aue
Microsoft Research
One Microsoft Way
Redmond, WA 98052
anthaue@microsoft.com
Kevin Duh
Dept. of Electrical Eng.
Univ. of Washington
Seattle, WA 98195
duh@ee.washington.edu
Eric Ringger
Computer Science Dept.
Brigham Young Univ.
Provo, UT 84602
ringger@cs.byu.edu
Abstract
We develop dependency parsers for Ara-
bic, English, Chinese, and Czech using
Bayes Point Machines, a training algo-
rithm which is as easy to implement as
the perceptron yet competitive with large
margin methods. We achieve results com-
parable to state-of-the-art in English and
Czech, and report the first directed depen-
dency parsing accuracies for Arabic and
Chinese. Given the multilingual nature of
our experiments, we discuss some issues
regarding the comparison of dependency
parsers for different languages.
1 Introduction
Dependency parsing is an alternative to constituency
analysis with a venerable tradition going back at
least two millenia. The last century has seen at-
tempts to formalize dependency parsing, particu-
larly in the Prague School approach to linguistics
(Tesnie`re, 1959; Melc?uk, 1988).
In a dependency analysis of syntax, words directly
modify other words. Unlike constituency analysis,
there are no intervening non-lexical nodes. We use
the terms child and parent to denote the dependent
term and the governing term respectively.
Parsing has many potential applications, rang-
ing from question answering and information re-
trieval to grammar checking. Our intended ap-
plication is machine translation in the Microsoft
Research Treelet Translation System (Quirk et al,
2005; Menezes and Quirk, 2005). This system ex-
pects an analysis of the source language in which
words are related by directed, unlabeled dependen-
cies. For the purposes of developing machine trans-
lation for several language pairs, we are interested in
dependency analyses for multiple languages.
The contributions of this paper are two-fold: First,
we present a training algorithm called Bayes Point
Machines (Herbrich et al, 2001; Harrington et al,
2003), which is as easy to implement as the per-
ceptron, yet competitive with large margin meth-
ods. This algorithm has implications for anyone
interested in implementing discriminative training
methods for any application. Second, we develop
parsers for English, Chinese, Czech, and Arabic and
probe some linguistic questions regarding depen-
dency analyses in different languages. To the best of
our knowledge, the Arabic and Chinese results are
the first reported results to date for directed depen-
dencies. In the following, we first describe the data
(Section 2) and the basic parser architecture (Section
3). Section 4 introduces the Bayes Point Machine
while Section 5 describes the features for each lan-
guage. We conclude with experimental results and
discussions in Sections 6 and 7.
2 Data
We utilize publicly available resources in Arabic,
Chinese, Czech, and English for training our depen-
dency parsers.
For Czech we used the Prague Dependency Tree-
bank version 1.0 (LDC2001T10). This is a corpus
of approximately 1.6 million words. We divided
the data into the standard splits for training, devel-
160
opment test and blind test. The Prague Czech De-
pendency Treebank is provided with human-edited
and automatically-assigned morphological informa-
tion, including part-of-speech labels. Training and
evaluation was performed using the automatically-
assigned labels.
For Arabic we used the Prague Arabic De-
pendency Treebank version 1.0 (LDC2004T23).
Since there is no standard split of the data into
training and test sections, we made an approxi-
mate 70%/15%/15% split for training/development
test/blind test by sampling whole files. The Ara-
bic Dependency Treebank is considerably smaller
than that used for the other languages, with approx-
imately 117,000 tokens annotated for morphologi-
cal and syntactic relations. The relatively small size
of this corpus, combined with the morphological
complexity of Arabic and the heterogeneity of the
corpus (it is drawn from five different newspapers
across a three-year time period) is reflected in the
relatively low dependency accuracy reported below.
As with the Czech data, we trained and evaluated us-
ing the automatically-assigned part-of-speech labels
provided with the data.
Both the Czech and the Arabic corpora are anno-
tated in terms of syntactic dependencies. For En-
glish and Chinese, however, no corpus is available
that is annotated in terms of dependencies. We there-
fore applied head-finding rules to treebanks that
were annotated in terms of constituency.
For English, we used the Penn Treebank version
3.0 (Marcus et al, 1993) and extracted dependency
relations by applying the head-finding rules of (Ya-
mada and Matsumoto, 2003). These rules are a
simplification of the head-finding rules of (Collins,
1999). We trained on sections 02-21, used section
24 for development test and evaluated on section
23. The English Penn Treebank contains approxi-
mately one million tokens. Training and evaluation
against the development test set was performed us-
ing human-annotated part-of-speech labels. Evalu-
ation against the blind test set was performed us-
ing part-of-speech labels assigned by the tagger de-
scribed in (Toutanova et al, 2003).
For Chinese, we used the Chinese Treebank ver-
sion 5.0 (Xue et al, 2005). This corpus contains
approximately 500,000 tokens. We made an approx-
imate 70%/15%/15% split for training/development
test/blind test by sampling whole files. As with the
English Treebank, training and evaluation against
the development test set was performed using
human-annotated part-of-speech labels. For evalu-
ation against the blind test section, we used an im-
plementation of the tagger described in (Toutanova
et al, 2003). Trained on the same training section
as that used for training the parser and evaluated on
the development test set, this tagger achieved a to-
ken accuracy of 92.2% and a sentence accuracy of
63.8%.
The corpora used vary in homogeneity from the
extreme case of the English Penn Treebank (a large
corpus drawn from a single source, the Wall Street
Journal) to the case of Arabic (a relatively small
corpus?approximately 2,000 sentences?drawn from
multiple sources). Furthermore, each language
presents unique problems for computational analy-
sis. Direct comparison of the dependency parsing
results for one language to the results for another
language is therefore difficult, although we do at-
tempt in the discussion below to provide some basis
for a more direct comparison. A common question
when considering the deployment of a new language
for machine translation is whether the natural lan-
guage components available are of sufficient quality
to warrant the effort to integrate them into the ma-
chine translation system. It is not feasible in every
instance to do the integration work first and then to
evaluate the output.
Table 1 summarizes the data used to train the
parsers, giving the number of tokens (excluding
traces and other empty elements) and counts of sen-
tences.1
3 Parser Architecture
We take as our starting point a re-implementation
of McDonald?s state-of-the-art dependency parser
(McDonald et al, 2005a). Given a sentence x, the
goal of the parser is to find the highest-scoring parse
y? among all possible parses y ? Y :
y? = arg max
y?Y
s(x, y) (1)
1The files in each partition of the Chinese and Arabic data
are given at http://research.microsoft.com/?simonco/
HLTNAACL2006.
161
Language Total Training Development Blind
Tokens Sentences Sentences Sentences
Arabic 116,695 2,100 446 449
Chinese 527,242 14,735 1,961 2,080
Czech 1,595,247 73,088 7,319 7,507
English 1,083,159 39,832 1,346 2,416
Table 1: Summary of data used to train parsers.
For a given parse y, its score is the sum of the scores
of all its dependency links (i, j) ? y:
s(x, y) = ?
(i,j)?y
d(i, j) = ?
(i,j)?y
w ? f(i, j) (2)
where the link (i, j) indicates a head-child depen-
dency between the token at position i and the token
at position j. The score d(i, j) of each dependency
link (i, j) is further decomposed as the weighted
sum of its features f(i, j).
This parser architecture naturally consists of three
modules: (1) a decoder that enumerates all possi-
ble parses y and computes the argmax; (2) a train-
ing algorithm for adjusting the weights w given the
training data; and (3) a feature representation f(i, j).
Two decoders will be discussed here; the training al-
gorithm and feature representation are discussed in
the following sections.
A good decoder should satisfy several proper-
ties: ideally, it should be able to search through all
valid parses of a sentence and compute the parse
scores efficiently. Efficiency is a significant issue
since there are usually an exponential number of
parses for any given sentence, and the discrimina-
tive training methods we will describe later require
repeated decoding at each training iteration. We re-
implemented Eisner?s decoder (Eisner, 1996), which
searches among all projective parse trees, and the
Chu-Liu-Edmonds? decoder (Chu and Liu, 1965;
Edmonds, 1967), which searches in the space of
both projective and non-projective parses. (A pro-
jective tree is a parse with no crossing dependency
links.) For the English and Chinese data, the head-
finding rules for converting from Penn Treebank
analyses to dependency analyses creates trees that
are guaranteed to be projective, so Eisner?s algo-
rithm suffices. For the Czech and Arabic corpora,
a non-projective decoder is necessary. Both algo-
rithms are O(N3), where N is the number of words
in a sentence.2 Refer to (McDonald et al, 2005b)
for a detailed treatment of both algorithms.
4 Training: The Bayes Point Machine
In this section, we describe an online learning al-
gorithm for training the weights w. First, we ar-
gue why an online learner is more suitable than a
batch learner like a Support Vector Machine (SVM)
for this task. We then review some standard on-
line learners (e.g. perceptron) before presenting the
Bayes Point Machine (BPM) (Herbrich et al, 2001;
Harrington et al, 2003).
4.1 Online Learning
An online learner differs from a batch learner in that
it adjusts w incrementally as each input sample is
revealed. Although the training data for our pars-
ing problem exists as a batch (i.e. all input sam-
ples are available during training), we can apply
online learning by presenting the input samples in
some sequential order. For large training set sizes,
a batch learner may face computational difficulties
since there already exists an exponential number of
parses per input sentence. Online learning is more
tractable since it works with one input at a time.
A popular online learner is the perceptron. It ad-
justs w by updating it with the feature vector when-
ever a misclassification on the current input sample
occurs. It has been shown that such updates con-
verge in a finite number of iterations if the data is lin-
early separable. The averaged perceptron (Collins,
2002) is a variant which averages the w across all
iterations; it has demonstrated good generalization
especially with data that is not linearly separable,
as in many natural language processing problems.
2The Chu-Liu-Edmonds? decoder, which is based on a maxi-
mal spanning tree algorithm, can run in O(N2), but our simpler
implementation of O(N3) was sufficient.
162
Recently, the good generalization properties of Sup-
port Vector Machines have prompted researchers to
develop large margin methods for the online set-
ting. Examples include the margin perceptron (Duda
et al, 2001), ALMA (Gentile, 2001), and MIRA
(which is used to train the parser in (McDonald et al,
2005a)). Conceptually, all these methods attempt to
achieve a large margin and approximate the maxi-
mum margin solution of SVMs.
4.2 Bayes Point Machines
The Bayes Point Machine (BPM) achieves good
generalization similar to that of large margin meth-
ods, but is motivated by a very different philoso-
phy of Bayesian learning or model averaging. In
the Bayesian learning framework, we assume a prior
distribution over w. Observations of the training
data revise our belief of w and produce a poste-
rior distribution. The posterior distribution is used
to create the final wBPM for classification:
wBPM = Ep(w|D)[w] =
|V (D)|?
i=1
p(wi|D) wi (3)
where p(w|D) is the posterior distribution of the
weights given the data D and Ep(w|D) is the expec-
tation taken with respect to this distribution. The
term |V (D)| is the size of the version space V (D),
which is the set of weights wi that is consistent with
the training data (i.e. the set of wi that classifies the
training data with zero error). This solution achieves
the so-called Bayes Point, which is the best approx-
imation to the Bayes optimal solution given finite
training data.
In practice, the version space may be large, so we
approximate it with a finite sample of size I . Further,
assuming a uniform prior over weights, we get the
following equation:
wBPM = Ep(w|D)[w] ?
I?
i=1
1
I wi (4)
Equation 4 can be computed by a very simple al-
gorithm: (1) Train separate perceptrons on different
random shuffles of the entire training data, obtaining
a set of wi. (2) Take the average (arithmetic mean)
of the weights wi. It is well-known that perceptron
training results in different weight vector solutions
Input: Training set D = ((x1, y1), (x2, y2), . . . , (xT , yT ))
Output: wBPM
Initialize: wBPM = 0
for i = 1 to I; do
Randomly shuffle the sequential order of samples in D
Initialize: wi = 0
for t = 1 to T; do
y?t = wi ? xt
if (y?t != yt) thenwi = wi + ytxt
done
wBPM = wBPM + 1Iwidone
Figure 1: Bayes Point Machine pseudo-code.
if the data samples are presented sequentially in dif-
ferent orders. Therefore, random shuffles of the data
and training a perceptron on each shuffle is effec-
tively equivalent to sampling different models (wi)
in the version space. Note that this averaging op-
eration should not be confused with ensemble tech-
niques such as Bagging or Boosting?ensemble tech-
niques average the output hypotheses, whereas BPM
averages the weights (models).
The BPM pseudocode is given in Figure 1. The
inner loop is simply a perceptron algorithm, so the
BPM is very simple and fast to implement. The
outer loop is easily parallelizable, allowing speed-
ups in training the BPM. In our specific implemen-
tation for dependency parsing, the line of the pseu-
docode corresponding to [y?t = wi ? xt] is replaced
by Eq. 1 and updates are performed for each in-
correct dependency link. Also, we chose to average
each individual perceptron (Collins, 2002) prior to
Bayesian averaging.
Finally, it is important to note that the definition of
the version space can be extended to include weights
with non-zero training error, so the BPM can handle
data that is not linearly separable. Also, although we
only presented an algorithm for linear classifiers (pa-
rameterized by the weights), arbitrary kernels can be
applied to BPM to allow non-linear decision bound-
aries. Refer to (Herbrich et al, 2001) for a compre-
hensive treatment of BPMs.
5 Features
Dependency parsers for all four languages were
trained using the same set of feature types. The
feature types are essentially those described in (Mc-
Donald et al, 2005a). For a given pair of tokens,
163
where one is hypothesized to be the parent and the
other to be the child, we extract the word of the par-
ent token, the part of speech of the parent token, the
word of the child token, the part of speech of the
child token and the part of speech of certain adjacent
and intervening tokens. Some of these atomic fea-
tures are combined in feature conjunctions up to four
long, with the result that the linear classifiers de-
scribed below approximate polynomial kernels. For
example, in addition to the atomic features extracted
from the parent and child tokens, the feature [Par-
entWord, ParentPOS, ChildWord, ChildPOS] is also
added to the feature vector representing the depen-
dency between the two tokens. Additional features
are created by conjoining each of these features with
the direction of the dependency (i.e. is the parent to
the left or right of the child) and a quantized measure
of the distance between the two tokens. Every token
has exactly one parent. The root of the sentence has
a special synthetic token as its parent.
Like McDonald et al we add features that con-
sider the first five characters of words longer than
five characters. This truncated word crudely approx-
imates stemming. For Czech and English the addi-
tion of these features improves accuracy. For Chi-
nese and Arabic, however, it is clear that we need a
different backoff strategy.
For Chinese, we truncate words longer than a sin-
gle character to the first character.3 Experimental
results on the development test set suggested that an
alternative strategy, truncation of words longer than
two characters to the first two characters, yielded
slightly worse results.
The Arabic data is annotated with gold-standard
morphological information, including information
about stems. It is also annotated with the output
of an automatic morphological analyzer, so that re-
searchers can experiment with Arabic without first
needing to build these components. For Arabic, we
truncate words to the stem, using the value of the
lemma attribute.
All tokens are converted to lowercase, and num-
bers are normalized. In the case of English, Czech
and Arabic, all numbers are normalized to a sin-
3There is a near 1:1 correspondence between characters
and morphemes in contemporary Mandarin Chinese. However,
most content words consist of more than one morpheme, typi-
cally two.
gle token. In Chinese, months are normalized to a
MONTH token, dates to a DATE token, years to a
YEAR token. All other numbers are normalized to a
single NUMBER token.
The feature types were instantiated using all or-
acle combinations of child and parent tokens from
the training data. It should be noted that when the
feature types are instantiated, we have considerably
more features than McDonald et al For example,
for English we have 8,684,328 whereas they report
6,998,447 features. We suspect that this is mostly
due to differences in implementation of the features
that backoff to stems.
The averaged perceptrons were trained on the
one-best parse, updating the perceptron for every
edge and averaging the accumulated perceptrons af-
ter every sentence. Experiments in which we up-
dated the perceptron based on k-best parses tended
to produce worse results. The Chu-Liu-Edmonds al-
gorithm was used for Czech. Experiments with the
development test set suggested that the Eisner de-
coder gave better results for Arabic than the Chu-
Liu-Edmonds decoder. We therefore used the Eisner
decoder for Arabic, Chinese and English.
6 Results
Table 2 presents the accuracy of the dependency
parsers. Dependency accuracy indicates for how
many tokens we identified the correct head. Root ac-
curacy, i.e. for how many sentences did we identify
the correct root or roots, is reported as F1 measure,
since sentences in the Czech and Arabic corpora can
have multiple roots and since the parsing algorithms
can identify multiple roots. Complete match indi-
cates how many sentences were a complete match
with the oracle dependency parse.
A convention appears to have arisen when report-
ing dependency accuracy to give results for English
excluding punctuation (i.e., ignoring punctuation to-
kens in the output of the parser) and to report results
for Czech including punctuation. In order to facil-
itate comparison of the present results with previ-
ously published results, we present measures includ-
ing and excluding punctuation for all four languages.
We hope that by presenting both sets of measure-
ments, we also simplify one dimension along which
published results of parse accuracy differ. A direct
164
Including punctuation Excluding punctuation
Language Dependency Root Complete Dependency Root Complete
Accuracy Accuracy Match Accuracy Accuracy Match
Arabic 79.9 90.0 9.80 79.8 87.8 10.2
Chinese 71.2 66.2 17.5 73.3 66.2 18.2
Czech 84.0 88.8 30.9 84.3 76.2 32.2
English 90.0 93.7 35.1 90.8 93.7 37.6
Table 2: Bayes Point Machine accuracy measured on blind test set.
comparison of parse results across languages is still
difficult for reasons to do with the different nature
of the languages, the corpora and the differing stan-
dards of linguistic detail annotated, but a compar-
ison of parsers for two different languages where
both results include punctuation is at least preferable
to a comparison of results including punctuation to
results excluding punctuation.
The results reported here for English and Czech
are comparable to the previous best published num-
bers in (McDonald et al, 2005a), as Table 3 shows.
This table compares McDonald et al?s results for an
averaged perceptron trained for ten iterations with
no check for convergence (Ryan McDonald, pers.
comm.), MIRA, a large margin classifier, and the
current Bayes Point Machine results. To determine
statistical significance we used confidence intervals
for p=0.95. For the comparison of English depen-
dency accuracy excluding punctuation, MIRA and
BPM are both statistically significantly better than
the averaged perceptron result reported in (McDon-
ald et al, 2005a). MIRA is significantly better
than BPM when measuring dependency accuracy
and root accuracy, but BPM is significantly better
when measuring sentences that match completely.
From the fact that neither MIRA nor BPM clearly
outperforms the other, we conclude that we have
successfully replicated the results reported in (Mc-
Donald et al, 2005a) for English.
For Czech we also determined significance using
confidence intervals for p=0.95 and compared re-
sults including punctuation. For both dependency
accuracy and root accuracy, MIRA is statisticallty
significantly better than averaged perceptron, and
BPM is statistically significantly better than MIRA.
Measuring the number of sentences that match com-
pletely, BPM is statistically significantly better than
averaged perceptron, but MIRA is significantly bet-
ter than BPM. Again, since neither MIRA nor BPM
outperforms the other on all measures, we conclude
that the results constitute a valiation of the results
reported in (McDonald et al, 2005a).
For every language, the dependency accuracy of
the Bayes Point Machine was greater than the ac-
curacy of the best individual perceptron that con-
tributed to that Bayes Point Machine, as Table 4
shows. As previously noted, when measuring
against the development test set, we used human-
annotated part-of-speech labels for English and Chi-
nese.
Although the Prague Czech Dependency Tree-
bank is much larger than the English Penn Treebank,
all measurements are lower than the corresponding
measurements for English. This reflects the fact that
Czech has considerably more inflectional morphol-
ogy than English, leading to data sparsity for the lex-
ical features.
The results reported here for Arabic are, to our
knowledge, the first published numbers for depen-
dency parsing of Arabic. Similarly, the results for
Chinese are the first published results for the depen-
dency parsing of the Chinese Treebank 5.0.4 Since
the Arabic and Chinese numbers are well short of
the numbers for Czech and English, we attempted
to determine what impact the smaller corpora used
for training the Arabic and Chinese parsers might
have. We performed data reduction experiments,
training the parsers on five random samples at each
size smaller than the entire training set. Figure 2
shows the dependency accuracy measured on the
complete development test set when training with
samples of the data. The graph shows the average
4(Wang et al, 2005) report numbers for undirected depen-
dencies on the Chinese Treebank 3.0. We cannot meaningfully
compare those numbers to the numbers here.
165
Language Algorithm DA RA CM
English Avg. Perceptron 90.6 94.0 36.5
(exc punc) MIRA 90.9 94.2 37.5
Bayes Point Machine 90.8 93.7 37.6
Czech Avg. Perceptron 82.9 88.0 30.3
(inc punc) MIRA 83.3 88.6 31.3
Bayes Point Machine 84.0 88.8 30.9
Table 3: Comparison to previous best published results reported in (McDonald et al, 2005a).
Arabic Chinese Czech English
Bayes Point Machine 78.4 83.8 84.5 91.2
Best averaged perceptron 77.9 83.1 83.5 90.8
Worst averaged perceptron 77.4 82.6 83.3 90.5
Table 4: Bayes Point Machine accuracy vs. averaged perceptrons, measured on development test set, ex-
cluding punctuation.
dependency accuracy for five runs at each sample
size up to 5,000 sentences. English and Chinese
accuracies in this graph use oracle part-of-speech
tags. At all sample sizes, the dependency accu-
racy for English exceeds the dependency accuracy
of the other languages. This difference is perhaps
partly attributable to the use of oracle part-of-speech
tags. However, we suspect that the major contribu-
tor to this difference is the part-of-speech tag set.
The tags used in the English Penn Treebank encode
traditional lexical categories such as noun, prepo-
sition, and verb. They also encode morphological
information such as person (the VBZ tag for exam-
ple is used for verbs that are third person, present
tense?typically with the suffix -s), tense, number
and degree of comparison. The part-of-speech tag
sets used for the other languages encode lexical cat-
egories, but do not encode morphological informa-
tion.5 With small amounts of data, the perceptrons
do not encounter sufficient instances of each lexical
item to calculate reliable weights. The perceptrons
are therefore forced to rely on the part-of-speech in-
formation.
It is surprising that the results for Arabic and Chi-
nese should be so close as we vary the size of the
5For Czech and Arabic we followed the convention estab-
lished in previous parsing work on the Prague Czech Depen-
dency Treebank of using the major and minor part-of-speech
tags but ignoring other morphological information annotated on
each node.
training data (Figure 2) given that Arabic has rich
morphology and Chinese very little. One possible
explanation for the similarity in accuracy is that the
rather poor root accuracy in Chinese indicates parses
that have gone awry. Anecdotal inspection of parses
suggests that when the root is not correctly identi-
fied, there are usually cascading related errors.
Czech, a morphologically complex language in
which root identification is far from straightfor-
ward, exhibits the worst performance at small sam-
ple sizes. But (not shown) as the sample size in-
creases, the accuracy of Czech and Chinese con-
verge.
7 Conclusions
We have successfully replicated the state-of-the-art
results for dependency parsing (McDonald et al,
2005a) for both Czech and English, using Bayes
Point Machines. Bayes Point Machines have the ap-
pealing property of simplicity, yet are competitive
with online wide margin methods.
We have also presented first results for depen-
dency parsing of Arabic and Chinese, together with
some analysis of the performance on those lan-
guages.
In future work we intend to explore the discrim-
inative reranking of n-best lists produced by these
parsers and the incorporation of morphological fea-
tures.
166
60
65
70
75
80
85
90
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000Sample size
Depen
dency
 Accu
racy
EnglishChineseArabicCzech
Figure 2: Dependency accuracy at various sample
sizes. Graph shows average of five samples at each
size and measures accuracy against the development
test set.
Acknowledgements
We would like to thank Ryan McDonald, Otakar
Smrz? and Hiroyasu Yamada for help in various
stages of the project.
References
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Processing. Ph.D. the-
sis, University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
R. O. Duda, P. E. Hart, and D. G. Stork. 2001. Pattern
Classification. John Wiley & Sons, Inc.: New York.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of COLING 1996, pages 340?345.
Claudio Gentile. 2001. A new approximate maximal
margin classification algorithm. Journal of Machine
Learning Research, 2:213?242.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proc. 7th Pacific-Asia
Conference on Knowledge Discovery and Data Min-
ing, pages 241?252.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, pages 245?278.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Assocation for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005b. Online large-margin training of dependency
parsers. Technical Report MS-CIS-05-11, Dept. of
Computer and Information Science, Univ. of Pennsyl-
vania.
Igor A. Melc?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Arul Menezes and Chris Quirk. 2005. Microsoft re-
search treelet translation system: IWSLT evaluation.
In Proceedings of the International Workshop on Spo-
ken Language Translation.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd annual meet-
ing of the Association for Computational Linguistics.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe structurale.
Librairie C. Klincksieck.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005.
Strictly lexical dependency parsing. In Proceedings
of the Ninth International Workshop on Parsing Tech-
nologies, pages 152?159.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2).
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
167
Proceedings of the ACL Student Research Workshop, pages 19?24,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Jointly Labeling Multiple Sequences: A Factorial HMM Approach
Kevin Duh
Department of Electrical Engineering
University of Washington, USA
duh@ee.washington.edu
Abstract
We present new statistical models for
jointly labeling multiple sequences and
apply them to the combined task of part-
of-speech tagging and noun phrase chunk-
ing. The model is based on the Factorial
Hidden Markov Model (FHMM) with dis-
tributed hidden states representing part-
of-speech and noun phrase sequences. We
demonstrate that this joint labeling ap-
proach, by enabling information sharing
between tagging/chunking subtasks, out-
performs the traditional method of tag-
ging and chunking in succession. Fur-
ther, we extend this into a novel model,
Switching FHMM, to allow for explicit
modeling of cross-sequence dependencies
based on linguistic knowledge. We report
tagging/chunking accuracies for varying
dataset sizes and show that our approach
is relatively robust to data sparsity.
1 Introduction
Traditionally, various sequence labeling problems in
natural language processing are solved by the cas-
cading of well-defined subtasks, each extracting spe-
cific knowledge. For instance, the problem of in-
formation extraction from sentences may be broken
into several stages: First, part-of-speech (POS) tag-
ging is performed on the sequence of word tokens.
This result is then utilized in noun-phrase and verb-
phrase chunking. Finally, a higher-level analyzer
extracts relevant information based on knowledge
gleaned in previous subtasks.
The decomposition of problems into well-defined
subtasks is useful but sometimes leads to unneces-
sary errors. The problem is that errors in earlier
subtasks will propagate to downstream subtasks, ul-
timately deteriorating overall performance. There-
fore, a method that allows the joint labeling of sub-
tasks is desired. Two major advantages arise from
simultaneous labeling: First, there is more robust-
ness against error propagation. This is especially
relevant if we use probabilities in our models. Cas-
cading subtasks inherently ?throws away? the prob-
ability at each stage; joint labeling preserves the un-
certainty. Second, information between simultane-
ous subtasks can be shared to further improve ac-
curacy. For instance, it is possible that knowing a
certain noun phrase chunk may help the model infer
POS tags more accurately, and vice versa.
In this paper, we propose a solution to the
joint labeling problem by representing multiple se-
quences in a single Factorial Hidden Markov Model
(FHMM) (Ghahramani and Jordan, 1997). The
FHMM generalizes hidden Markov models (HMM)
by allowing separate hidden state sequences. In our
case, these hidden state sequences represent the POS
tags and phrase chunk labels. The links between the
two hidden sequences model dependencies between
tags and chunks. Together the hidden sequences
generate an observed word sequence, and the task of
the tagger/chunker is to invert this process and infer
the original tags and chunks.
Previous work on joint tagging/chunking has
shown promising results. For example, Xun et
19
Figure 1: Baseline FHMM. The two hidden se-
quences y1:t and z1:t can represent tags and chunks,
respectively. Together they generate x1:t, the ob-
served word sequence.
al. (2000) uses a POS tagger to output an N-best list
of tags, then a Viterbi search to find the chunk se-
quence that maximizes the joint tag/chunk probabil-
ity. Florian and Ngai (2001) extends transformation-
based learning tagger to a joint tagger/chunker by
modifying the objective function such that a trans-
formation rule is evaluated on the classification
of all simultaneous subtasks. Our work is most
similar in spirit to Dynamic Conditional Random
Fields (DCRF) (Sutton et al, 2004), which also
models tagging and chunking in a factorial frame-
work. Some main differences between our model
and DCRF may be described as 1) directed graphical
model vs. undirected graphical model, and 2) gener-
ative model vs. conditional model. The main advan-
tage of FHMM over DCRF is that FHMM requires
considerably less computation and exact inference is
easily achievable for FHMM and its variants.
The paper is structured as follows: Section 2 de-
scribes in detail the FHMM. Section 3 presents a
new model, the Switching FHMM, which represents
cross-sequence dependencies more effectively than
FHMMs. Section 4 discusses the task and data and
Section 5 presents various experimental results Sec-
tion 6 discusses future work and concludes.
2 Factorial HMM
2.1 Basic Factorial HMM
A Factorial Hidden Markov Model (FHMM) is a
hidden Markov model with a distributed state rep-
resentation. Let x1:T be a length T sequence of ob-
served random variables (e.g. words) and y1:T and
z1:T be the corresponding sequences of hidden state
variables (e.g. tags, chunks). Then we define the
FHMM as the probabilistic model:
p(x1:T , y1:T , z1:T ) (1)
= pi0
T
?
t=2
p(xt|yt, zt)p(yt|yt?1, zt)p(zt|zt?1)
where pi0 = p(x0|y0, z0)p(y0|z0)p(z0). Viewed
as a generative process, we can say that the
chunk model p(zt|zt?1) generates chunks depend-
ing on the previous chunk label, the tag model
p(yt|yt?1, zt) generates tags based on the previ-
ous tag and current chunk, and the word model
p(xt|yt, zt) generates words using the tag and chunk
at the same time-step.
This equation corresponds to the graphical model
of Figure 1. Although the original FHMM de-
veloped by Ghahramani (1997) does not explicitly
model the dependencies between the two hidden
state sequences, here we add the edges between the
y and z nodes to reflect the interaction between tag
and chunk sequences. Note that the FHMM can be
collapsed into a hidden Markov model where the
hidden state is the cross-product of the distributed
states y and z. Despite this equivalence, the FHMM
is advantageous because it requires the estimation of
substantiatially fewer parameters.
FHMM parameters can be calculated via maxi-
mum likelihood (ML) estimation if the values of the
hidden states are available in the training data. Oth-
erwise, parameters must be learned using approx-
imate inference algorithms (e.g. Gibbs sampling,
variational inference), since exact Expectation-
Maximization (EM) algorithm is computationally
intractable (Ghahramani and Jordan, 1997). Given
a test sentence, inference of the corresponding
tag/chunk sequence is found by the Viterbi algo-
rithm, which finds the tag/chunk sequence that max-
imizes the joint probability, i.e.
arg max
y1:T ,z1:T
p(x1:T , y1:T , z1:T ) (2)
2.2 Adding Cross-Sequence Dependencies
Many other structures exist in the FHMM frame-
work. Statistical modeling often involves the it-
erative process of finding the best set of depen-
dencies that characterizes the data effectively. As
shown in Figures 2(a), 2(b), and 2(c), dependen-
20
cies can be added between the yt and zt?1, be-
tween zt and yt?1, or both. The model in Fig. 2(a)
corresponds to changing the tag model in Eq. 1 to
p(yt|yt?1, zt, zt?1); Fig. 2(b) corresponds to chang-
ing the chunk model to p(zt|zt?1, yt?1); Fig. 2(c),
corresponds to changing both tag and chunk models,
leading to the probability model:
T
?
t=1
p(xt|yt, zt)p(yt|yt?1, zt, zt?1)p(zt|zt?1, yt?1)
(3)
We name the models in Figs. 2(a) and 2(b) as
FHMM-T and FHMM-C due to the added depen-
dencies to the tag and chunk models, respectively.
The model of Fig. 2(c) and Eq. 3 will be referred to
as FHMM-CT. Intuitively, the added dependencies
will improve the predictive power across chunk and
tag sequences, provided that enough training data
are available for robust parameter estimation.
(a) (b) (c)
Figure 2: FHMMs with additional cross-sequence
dependencies. The models will be referred to as (a)
FHMM-T, (b) FHMM-C, and (c) FHMM-CT.
3 Switching Factorial HMM
A reasonable question to ask is, ?How exactly does
the chunk sequence interact with the tag sequence??
The approach of adding dependencies in Section 2.2
acknowledges the existence of cross-sequence inter-
actions but does not explicitly specify the type of
interaction. It relies on statistical learning to find
the salient dependencies, but such an approach is
feasable only when sufficient data are available for
parameter estimation.
To answer the question, we consider how the
chunk sequence affects the generative process for
tags: First, we can expect that the unigram distri-
bution of tags changes depending on whether the
chunk is a noun phrase or verb phrase. (In a noun
phrase, nouns and adjective tags are more com-
mon; in a verb phrase, verbs and adverb tags are
more frequent.) Similarly, a bigram distribution
p(yt|yt?1) describing tag transition probabilities dif-
fers depending on the bigram?s location in the chunk
sequence, such as whether it is within a noun phrase,
verb phrase, or at a phrase boundary. In other words,
the chunk sequence interacts with tags by switching
the particular generative process for tags. We model
this interaction explicitly using a Switching FHMM:
p(x1:T , y1:T , z1:T ) (4)
=
T
?
t=1
p(xt|yt, zt)p?(yt|yt?1)p?(zt|zt?1)
In this new model, the chunk and tag are now gen-
erated by bigram distributions parameterized by ?
and ?. For different values of ? (or ?), we have
different distributions for p(yt|yt?1) (or p(zt|zt?1)).
The crucial aspect of the model lies in a function
? = f(z1:t), which summarizes information in z1:t
that is relevant for the generation of y, and a func-
tion ? = g(y1:t), which captures information in y1:t
that is relevant to the generation of z.
In general, the functions f(?) and g(?) partition
the space of all tag or chunk sequences into sev-
eral equivalence classes, such that all instances of
an equivalence class give rise to the same genera-
tive model for the cross sequence. For instance, all
consecutive chunk labels that indicate a noun phrase
can be mapped to one equivalence class, while labels
that indicate verb phrase can be mapped to another.
The mapping can be specified manually or learned
automatically. Section 5 discusses a linguistically-
motivated mapping that is used for the experiments.
Once the mappings are defined, the parameters
p?(yt|yt?1) and p?(zt|zt?1) are obtained via max-
imum likelihood estimation in a fashion similar to
that of the FHMM. The only exception is that now
the training data are partitioned according to the
mappings, and each ?- and ?- specific generative
model is estimated separately. Inference of the tags
and chunks for a test sentence proceeds similarly to
FHMM inference. We call this model a Switching
FHMM since the distribution of a hidden sequence
?switches? dynamically depending on the values of
the other hidden sequence.
An idea related to the Switching FHMM is the
Bayesian Multinet (Geiger and Heckerman, 1996;
21
Bilmes, 2000), which allows the dynamic switching
of conditional variables. It can be used to implement
switching from a higher-order model to a lower-
order model, a form of backoff smoothing for deal-
ing with data sparsity. The Switching FHMM differs
in that it switches among models of the same order,
but these models represent different generative pro-
cesses. The result is that the model no longer re-
quires a time-homogenous assumption for state tran-
sitions; rather, the transition probabilities change
dynamically depending on the influence across se-
quences.
4 POS Tagging and NP Chunking
4.1 The Tasks
POS tagging is the task of assigning words the
correct part-of-speech, and is often the first stage
of various natural language processing tasks. As
a result, POS tagging has been one of the most
active areas of research, and many statistical and
rule-based approach have been tried. The most
notable of these include the trigram HMM tagger
(Brants, 2000), maximum entropy tagger (Ratna-
parkhi, 1996), transformation-based tagger (Brill,
1995), and cyclic dependency networks (Toutanova
et al, 2003).
Accuracy numbers for POS tagging are often re-
ported in the range of 95% to 97%. Although
this may seem high, note that a tagger with 97%
accuracy has only a 63% chance of getting all
tags in a 15-word sentence correct, whereas a 98%
accurate tagger has 74% (Manning and Schu?tze,
1999). Therefore, small improvements can be sig-
nificant, especially if downstream processing re-
quires correctly-tagged sentences. One of the most
difficult problems with POS tagging is the handling
of out-of-vocabulary words.
Noun-phrase (NP) chunking is the task of finding
the non-recursive (base) noun-phrases of sentences.
This segmentation task can be achieved by assign-
ing words in a sentence to one of three tokens: B for
?Begin-NP?, I for ?Inside-NP?, or O for ?Outside-
NP? (Ramshaw and Marcus, 1995). The ?Begin-
NP? token is used in the case when an NP chunk
is immediately followed by another NP chunk. The
state-of-the-art chunkers report F1 scores of 93%-
94% and accuracies of 87%-97%. See, for exam-
ple, NP chunkers utilizing conditional random fields
(Sha and Pereira, 2003) and support vector machines
(Kudo and Matsumoto, 2001).
4.2 Data
The data comes from the CoNLL 2000 shared task
(Sang and Buchholz, 2000), which consists of sen-
tences from the Penn Treebank Wall Street Journal
corpus (Marcus et al, 1993). The training set con-
tains a total of 8936 sentences with 19k unique vo-
cabulary. The test set contains 2012 sentences and
8k vocabulary. The out-of-vocabulary rate is 7%.
There are 45 different POS tags and 3 different
NP labels in the original data. An example sentence
with POS and NP tags is shown in Table 1.
The move could pose a challenge
DT NN MD VB DT NN
I I O O I I
Table 1: Example sentence with POS tags (2nd row) and NP
labels (3rd row). For NP, I = Inside-NP, O=Outside-NP.
5 Experiments
We report two sets of experiments. Experiment 1
compares several FHMMs with cascaded HMMs
and demonstrates the benefit of joint labeling. Ex-
periment 2 evaluates the Switching FHMM for
various training dataset sizes and shows its ro-
bustness against data sparsity. All models are
implemented using the Graphical Models Toolkit
(GMTK) (Bilmes and Zweig, 2002).
5.1 Exp1: FHMM vs Cascaded HMMs
We compare the four FHMMs of Section 2 to the
traditional approach of cascading HMMs in succes-
sion, and compare their POS and NP accuracies in
Table 2. In this table, the first row ?Oracle HMM?
is an oracle experiment which shows what NP accu-
racies can be achieved if perfectly correct POS tags
are available in a cascaded approach. The second
row ?Cascaded HMM? represents the traditional ap-
proach of doing POS tagging and NP chunking in
succession; i.e. an NP chunker is applied to the out-
put of a POS tagger that is 94.17% accurate. The
next four rows show the results of joint labeling us-
ing various FHMMs. The final row ?DCRF? are
22
comparable results from Dynamic Conditional Ran-
dom Fields (Sutton et al, 2004).
There are several observations: First, it is im-
portant to note that FHMM outperforms the cas-
caded HMM in terms of NP accuracy for all but one
model. For instance, FHMM-CT achieves an NP
accuracy of 95.93%, significantly higher than both
the cascaded HMM (93.90%) and the oracle HMM
(94.67%). This confirms our hypothesis that joint la-
beling helps prevent POS errors from propagating to
NP chunking. Second, the fact that several FHMM
models achieve NP accuracies higher than the ora-
cle HMM implies that information sharing between
POS and NP sequences gives even more benefit than
having only perfectly correct POS tags. Thirdly, the
fact that the most complex model (FHMM-CT) per-
forms best suggests that it is important to avoid data
sparsity problems, as it requires more parameters to
be estimated in training.
Finally, it should be noted that although the DCRF
outperforms the FHMM in this experiment, the
DCRF uses significantly more word features (e.g.
capitalization, existence in a list of proper nouns,
etc.) and a larger context (previous and next 3
tags), whereas the FHMM considers the word as its
sole feature, and the previous tag as its only con-
text. Further work is required to see whether the
addition of these features in the FHMM?s genera-
tive framework will achieve accuracies close to that
of DCRF. The take-home message is that, in light
of the computational advantages of generative mod-
els, the FHMM should not be dismissed as a poten-
tial solution for joint labeling. In fact, recent results
in the discriminative training of FHMMs (Bach and
Jordan, 2005) has shown promising results in speech
processing and it is likely that such advanced tech-
niques, among others, may improve the FHMM?s
performance to state-of-the-art results.
5.2 Exp2: Switching FHMM and Data Sparsity
We now compare the Switching FHMM to the best
model of Experiment 1 (FHMM-CT) for varying
amounts of training data. The Switching FHMM
uses the following ? and ? mapping. The mapping
? = f(z1:t) partitions the space of chunk history z1:t
into five equivalence classes based on the two most
recent chunk labels:
Model POS NP
Oracle HMM ? 94.67
Cascaded HMM 94.17 93.90
Baseline FHMM 93.82 93.56
FHMM-T 93.73 94.07
FHMM-C 94.16 95.76
FHMM-CT 94.15 95.93
DCRF 98.92 97.36
Table 2: POS and NP Accuracy for Cascaded HMM
and FHMM Models.
Class1. {z1:t : zt?1 = I, zt = I}
Class2. {z1:t : zt?1 = O, zt = O}
Class3. {z1:t : zt?1 = {I,B}, zt = O}
Class4. {z1:t : zt?1 = O, zt = {I,B}}
Class5. {z1:t : (zt?1, zt) = {(I,B), (B, I)}}
Class1 and Class2 are cases where the tag is located
strictly inside or outside an NP chunk. Class3 and
Class4 are situations where the tag is leaving or en-
tering an NP, and Class5 is when the tag transits be-
tween consecutive NP chunks. Class-specific tag bi-
grams p?(yt|yt?1) are trained by dividing the train-
ing data according to the mapping. On the other
hand, the mapping ? = g(y1:t) is not used to en-
sure a single point of comparison with FHMM-CT;
we use FHMM-CT?s chunk model p(zt|zt?1, yt?1)
in place of p?(zt|zt?1).
The POS and NP accuracies are plotted in Figures
3 and 4. We report accuracies based on the aver-
age of five different random subsets of the training
data for datasets of sizes 1000, 3000, 5000, and 7000
sentences. Note that for the Switching FHMM, POS
and NP accuracy remains relatively constant despite
the reduction in data size. This suggests that a more
explicit model for cross sequence interaction is es-
sential especially in the case of insufficient train-
ing data. Also, for the very small datasize of 1000,
the accuracies for Cascaded HMM are 84% for POS
and 70% for NP, suggesting that the general FHMM
framework is still beneficial.
6 Conclusion and Future Work
We have demonstrated that joint labeling with an
FHMM can outperform the traditional approach of
cascading tagging and chunking in NLP. The new
Switching FHMM generalizes the FHMM by allow-
23
1000 2000 3000 4000 5000 6000 7000 8000 9000
86
87
88
89
90
91
92
93
94
95
PO
S 
Ac
cu
ra
cy
Number of training sentences
FHMM?CT
Switch FHMM
Figure 3: POS Accuracy for varying data sizes
1000 2000 3000 4000 5000 6000 7000 8000 9000
93.5
94
94.5
95
95.5
96
N
P 
Ac
cu
ra
cy
Number of training sentences
FHMM?CT
Switch FHMM
Figure 4: NP Accuracy for varying data sizes
ing dynamically changing generative models and is
a promising approach for modeling the type of inter-
actions between hidden state sequences.
Three directions for future research are planned:
First, we will augment the FHMM such that its ac-
curacies are competitive with state-of-the-art taggers
and chunkers. This includes adding word features to
improve accuracy on OOV words, augmenting the
context from bigram to trigram, and applying ad-
vanced smoothing techniques. Second, we plan to
examine the Switching FHMM further, especially in
terms of automatic construction of the ? and ? func-
tion. A promising approach is to learn the mappings
using decision trees or random forests, which has re-
cently achieved good results in a similar problem in
language modeling (Xu and Jelinek, 2004). Finally,
we plan to integrate the tagger/chunker in an end-
to-end system, such as a Factored Language Model
(Bilmes and Kirchhoff, 2003), to measure the over-
all merit of joint labeling.
Acknowledgments
The author would like to thank Katrin Kirchhoff, Jeff Bilmes,
and Gang Ji for insightful discussions, Chris Bartels for support
on GMTK, and the two anonymous reviewers for their construc-
tive comments. Also, the author gratefully acknowledges sup-
port from NSF and CIA under NSF Grant No. IIS-0326276.
References
Francis Bach and Michael Jordan. 2005. Discriminative train-
ing of hidden Markov models for multiple pitch tracking. In
Proc. Intl. Conf. Acoustics, Speech, Signal Processing.
J. Bilmes and K. Kirchhoff. 2003. Factored language models
and generalized parallel backoff. In Proc. of HLT/NACCL.
J. Bilmes and G. Zweig. 2002. The Graphical Models Toolkit:
An open source software system for speech and time-series
processing. In Intl. Conf. on Acoustics, Speech, Signal Proc.
Jeff Bilmes. 2000. Dynamic bayesian multi-networks. In The
16th Conference on Uncertainty in Artificial Intelligence.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proceedings of the Applied NLP.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part of
speech tagging. Computational Linguistics, 21(4):543?565.
Radu Florian and Grace Ngai. 2001. Multidimensional
transformation-based learning. In Proc. CoNLL.
D. Geiger and D. Heckerman. 1996. Knowledge representation
and inference in similarity netwrosk and Bayesian multinets.
Artificial Intelligence, 82:45?74.
Z. Ghahramani and M. I. Jordan. 1997. Factorial hidden
Markov models. Machine Learning, 29:245?275.
T. Kudo and Y. Matsumoto. 2001. Chunking with support vec-
tor machines. In Proceedings of NAACL-2001.
C. D. Manning and H. Schu?tze, 1999. Foundations of Statistical
Natural Language Processing, chapter 10. MIT Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?330.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using
transformation-based learning. In Proceedings of the Third
Workshop on Very Large Corpora (ACL-95).
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proceedings of EMNLP-1996.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proc. CoNLL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of HLT-NAACL.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dy-
namic conditional random fields. In Intl. Conf. Machine
Learning (ICML 2004).
K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In Proc. of HLT-NAACL.
Peng Xu and Frederick Jelinek. 2004. Random forests in lan-
guage modeling. In Proc. EMNLP.
E. Xun, C. Huang, and M. Zhou. 2000. A unified statistical
model for the identification of English BaseNP. In Proc.
ACL.
24
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 37?40,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Beyond Log-Linear Models:
Boosted Minimum Error Rate Training for N-best Re-ranking
Kevin Duh?
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195
kevinduh@u.washington.edu
Katrin Kirchhoff
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195
katrin@ee.washington.edu
Abstract
Current re-ranking algorithms for machine
translation rely on log-linear models, which
have the potential problem of underfitting the
training data. We present BoostedMERT, a
novel boosting algorithm that uses Minimum
Error Rate Training (MERT) as a weak learner
and builds a re-ranker far more expressive than
log-linear models. BoostedMERT is easy to
implement, inherits the efficient optimization
properties of MERT, and can quickly boost the
BLEU score on N-best re-ranking tasks. In
this paper, we describe the general algorithm
and present preliminary results on the IWSLT
2007 Arabic-English task.
1 Introduction
N-best list re-ranking is an important component in
many complex natural language processing applica-
tions (e.g. machine translation, speech recognition,
parsing). Re-ranking the N-best lists generated from
a 1st-pass decoder can be an effective approach be-
cause (a) additional knowledge (features) can be in-
corporated, and (b) the search space is smaller (i.e.
choose 1 out of N hypotheses).
Despite these theoretical advantages, we have of-
ten observed little gains in re-ranking machine trans-
lation (MT) N-best lists in practice. It has often
been observed that N-best list rescoring only yields
a moderate improvement over the first-pass output
although the potential improvement as measured by
the oracle-best hypothesis for each sentence is much
?Work supported by an NSF Graduate Research Fellowship.
higher. This shows that hypothesis features are ei-
ther not discriminative enough, or that the reranking
model is too weak
This performance gap can be mainly attributed to
two problems: optimization error and modeling er-
ror (see Figure 1).1 Much work has focused on de-
veloping better algorithms to tackle the optimization
problem (e.g. MERT (Och, 2003)), since MT eval-
uation metrics such as BLEU and PER are riddled
with local minima and are difficult to differentiate
with respect to re-ranker parameters. These opti-
mization algorithms are based on the popular log-
linear model, which chooses the English translation
e of a foreign sentence f by the rule:
argmaxe p(e|f) ? argmaxe
?K
k=1 ?k?k(e, f)
where ?k(e, f) and ?k are the K features and
weights, respectively, and the argmax is over all hy-
potheses in the N-best list.
We believe that standard algorithms such as
MERT already achieve low optimization error (this
is based on experience where many random re-starts
of MERT give little gains); instead the score gap is
mainly due to modeling errors. Standard MT sys-
tems use a small set of features (i.e. K ? 10) based
on language/translation models.2 Log-linear mod-
els on such few features are simply not expressive
enough to achieve the oracle score, regardless of
how well the weights {?k} are optimized.
1Note that we are focusing on closing the gap to the oracle
score on the training set (or the development set); if we were
focusing on the test set, there would be an additional term, the
generalization error.
2In this work, we do not consider systems which utilize a
large smorgasbord of features, e.g. (Och and others, 2004).
37
BLEU=.40, achieved by re-ranking with MERT
BLEU=.56, achieved byselecting oracle hypotheses
Modeling problem:Log-linear model insufficient?
Optimization problem:Stuck in local optimum?
Figure 1: Both modeling and optimization problems in-
crease the (training set) BLEU score gap between MERT
re-ranking and oracle hypotheses. We believe that the
modeling problem is more serious for log-linear models
of around 10 features and focus on it in this work.
To truly achieve the benefits of re-ranking in MT,
one must go beyond the log-linear model. The re-
ranker should not be a mere dot product operation,
but a more dynamic and complex decision maker
that exploits the structure of the N-best re-ranking
problem.
We present BoostedMERT, a general framework
for learning such complex re-rankers using standard
MERT as a building block. BoostedMERT is easy to
implement, inherits MERT?s efficient optimization
procedure, and more effectively boosts the training
score. We describe the algorithm in Section 2, report
experiment results in Section 3, and end with related
work and future directions (Sections 4, 5).
2 BoostedMERT
The idea for BoostedMERT follows the boosting
philosophy of combining several weak classifiers
to create a strong overall classifier (Schapire and
Singer, 1999). In the classification case, boosting
maintains a distribution over each training sample:
the distribution is increased for samples that are in-
correctly classified and decreased otherwise. In each
boosting iteration, a weak learner is trained to opti-
mize on the weighted sample distribution, attempt-
ing to correct the mistakes made in the previous iter-
ation. The final classifier is a weighted combination
of weak learners. This simple procedure is very ef-
fective in reducing training and generalization error.
In BoostedMERT, we maintain a sample distribu-
tion di, i = 1 . . .M over the M N-best lists.3 In
3As such, it differs from RankBoost, a boosting-based rank-
ing algorithm in information retrieval (Freund et al, 2003). If
each boosting iteration t, MERT is called as as sub-
procedure to find the best feature weights ?t on di.4
The sample weight for an N-best list is increased if
the currently selected hypothesis is far from the ora-
cle score, and decreased otherwise. Here, the oracle
hypothesis for each N-best list is defined as the hy-
pothesis with the best sentence-level BLEU. The fi-
nal ranker is a combination of (weak) MERT ranker
outputs.
Algorithm 1 presents more detailed pseudocode.
We use the following notation: Let {xi} represent
the set of M training N-best lists, i = 1 . . .M . Each
N-best list xi contains N feature vectors (for N hy-
potheses). Each feature vector is of dimension K,
which is the same dimension as the number of fea-
ture weights ? obtained by MERT. Let {bi} be the
set of BLEU statistics for each hypothesis in {xi},
which is used to train MERT or to compute BLEU
scores for each hypothesis or oracle.
Algorithm 1 BoostedMERT
Input: N-best lists {xi}, BLEU scores {bi}
Input: Initialize sample distribution di uniformly
Input: Initialize y0 = [0], a constant zero vector
Output: Overall Ranker: fT
1: for t = 1 to T do
2: Weak ranker: ?t = MERT({xi},{bi},di)
3:
4: if (t ? 2): {yt?1} = PRED(f t?1, {xi})
5: {yt} = PRED(?t, {xi})
6: ?t = MERT([yt?1; yt],{bi})
7: Overall ranker: f t = yt?1 + ?tyt
8:
9: for i = 1 to M do
10: ai = [BLEU of hypothesis selected by f t]
divided by [BLEU of oracle hypothesis]
11: di = exp(?ai)/normalizer
12: end for
13: end for
applied on MT, RankBoost would maintain a weight for each
pair of hypotheses and would optimize a pairwise ranking met-
ric, which is quite dissimilar to BLEU.
4This is done by scaling each BLEU statistic, e.g. n-gram
precision, reference length, by the appropriate sample weights
before computing corpus-level BLEU. Alternatively, one could
sample (with replacement) the N-best lists using the distribu-
tion and use the resulting stochastic sample as input to an un-
modified MERT procedure.
38
The pseudocode can be divided into 3 sections:
1. Line 2 finds the best log-linear feature weights
on distribution di. MERT is invoked as a weak
learner, so this step is computationally efficient
for optimizing MT-specific metrics.
2. Lines 4-7 create an overall ranker by combin-
ing the outputs of the previous overall ranker
f t?1 and current weak ranker ?t. PRED is a
general function that takes a ranker and a M
N-best lists and generates a set of M N -dim
output vector y representing the predicted re-
ciprocal rank. Specifically, suppose a 3-best list
and a ranker predicts ranks (1,3,2) for the 1st,
2nd, and 3rd hypotheses, respectively. Then
y = (1/1,1/3,1/2) = (1,0.3,0.5).5
Finally, using a 1-dimensional MERT, the
scalar parameter ?t is optimized by maximiz-
ing the BLEU of the hypothesis chosen by
yt?1+?tyt. This is analogous to the line search
step in boosting for classification (Mason et al,
2000).
3. Lines 9-11 update the sample distribution di
such that N-best lists with low accuracies ai
are given higher emphasis in the next iteration.
The per-list accuracy ai is defined as the ratio of
selected vs. oracle BLEU, but other measures
are possible: e.g. ratio of ranks, difference of
BLEU.
The final classifier fT can be seen as a voting pro-
cedure among multiple log-linear models generated
by MERT. The weighted vote for hypotheses in an
N-best list xi is represented by the N-dimensional
vector: y? =
?T
t=1 ?
tyt =
?T
t=1 ?
t PRED(?t,xi).
We choose the hypothesis with the maximum value
in y?
Finally, we stress that the above algorithm
is an novel extension of boosting to re-ranking
problems. There are many open questions and
one can not always find a direct analog between
boosting for classification and boosting for rank-
ing. For instance, the distribution update scheme
5There are other ways to define a ranking output that are
worth exploring. For example, a hard argmax definition would
be (1,0,0); a probabilistic definition derived from the dot prod-
uct values can also be used. It is the definition of PRED that
introduces non-linearities in BoostedMERT.
of Lines 9-11 is recursive in the classification
case (i.e. di = di ? exp(LossOfWeakLearner)),
but due to the non-decompositional properties of
argmax in re-ranking, we have a non-recursive
equation based on the overall learner (di =
exp(LossOfOverallLearner)). This has deep impli-
cations on the dynamics of boosting, e.g. the distri-
bution may stay constant in the non-recursive equa-
tion, if the new weak ranker gets a small ?.
3 Experiments
The experiments are done on the IWSLT 2007
Arabic-to-English task (clean text condition). We
used a standard phrase-based statistical MT system
(Kirchhoff and Yang, 2007) to generated N-best lists
(N=2000) on Development4, Development5,
and Evaluation sub-sets. Development4 is
used as the Train set; N-best lists that have the same
sentence-level BLEU statistics for all hypotheses are
filtered since they are not important in impacting
training. Development5 is used as Dev set (in
particular, for selecting the number of iterations in
boosting), and Evaluation (Eval) is the blind
dataset for final ranker comparison. Nine features
are used in re-ranking.
We compare MERT vs. BoostedMERT. MERT is
randomly re-started 30 times, and BoostedMERT is
run for 30 iterations, which makes for a relatively
fair comparison. MERT usually does not improve
its Train BLEU score, even with many random re-
starts (again, this suggests that optimization error
is low). Table 1 shows the results, with Boosted-
MERT outperforming MERT 42.0 vs. 41.2 BLEU
on Eval. BoostedMERT has the potential to achieve
43.7 BLEU, if a better method for selecting optimal
iterations can be devised.
It should be noted that the Train scores achieved
by both MERT and BoostedMERT is still far from
the oracle (around 56). We found empirically that
BoostedMERT is somewhat sensitive to the size (M )
of the Train set. For small Train sets, BoostedMERT
can improve the training score quite drastically; for
the current Train set as well as other larger ones, the
improvement per iteration is much slower. We plan
to investigate this in future work.
39
MERT BOOST ?
Train, Best BLEU 40.3 41.0 0.7
Dev, Best BLEU 24.0 25.0 1.0
Eval, Best BLEU 41.2 43.7 2.5
Eval, Selected BLEU 41.2 42.0 0.8
Table 1: The first three rows show the BLEU score for
Train, Dev, and Eval from 30 iterations of BoostedMERT
or 30 random re-restarts of MERT. The last row shows
the actual BLEU on Eval when selecting the number
of boosting iterations based on Dev. Last column in-
dicates absolute improvements. BoostedMERT outper-
forms MERT by 0.8 points on Eval.
4 Related Work
Various methods are used to optimize log-linear
models in re-ranking (Shen et al, 2004; Venugopal
et al, 2005; Smith and Eisner, 2006). Although
this line of work is worthwhile, we believe more
gain is possible if we go beyond log-linear models.
For example, Shen?s method (2004) produces large-
margins but observed little gains in performance.
Our BoostedMERT should not be confused with
other boosting algorithms such as (Collins and Koo,
2005; Kudo et al, 2005). These algorithms are
called boosting because they iteratively choose fea-
tures (weak learners) and optimize the weights for
the boost/exponential loss. They do not, however,
maintain a distribution over N-best lists.
The idea of maintaining a distribution over N-
best lists is novel. To the best of our knowledge,
the most similar algorithm is AdaRank (Xu and Li,
2007), developed for document ranking in informa-
tion retrieval. Our main difference lies in Lines 4-7
in Algorithm 1: AdaRank proposes a simple closed
form solution for ? and combines only weak fea-
tures, not full learners (as in MERT). We have also
implemented AdaRank but it gave inferior results.
It should be noted that the theoretical training
bounds derived in the AdaRank paper is relevant
to BoostedMERT. Similar to standard boosting, this
bound shows that the training score can be improved
exponentially in the number of iterations. However,
we found that the conditions for which this bound is
applicable is rarely satisfied in our experiments.6
6The explanation for this is beyond the scope of this paper;
the basic reason is that our weak rankers (MERT) are not weak
in practice, so that successive iterations get diminishing returns.
5 Conclusions
We argue that log-linear models often underfit the
training data in MT re-ranking, and that this is the
reason we observe a large gap between re-ranker and
oracle scores. Our solution, BoostedMERT, creates
a highly-expressive ranker by voting among multiple
MERT rankers.
Although BoostedMERT improves over MERT,
more work at both the theoretical and algorithmic
levels is needed to demonstrate even larger gains.
For example, while standard boosting for classifica-
tion can exponentially reduce training error in the
number of iterations under mild assumptions, these
assumptions are frequently not satisfied in the algo-
rithm we described. We intend to further explore
the idea of boosting on N-best lists, drawing inspi-
rations from the large body of work on boosting for
classification whenever possible.
References
M. Collins and T. Koo. 2005. Discriminative reranking
for natural langauge parsing. Computational Linguis-
tics, 31(1).
Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. 2003.
An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4.
K. Kirchhoff and M. Yang. 2007. The UW machine
translation system for IWSLT 2007. In IWSLT.
T. Kudo, J. Suzuki, and H. Isozaki. 2005. Boosting-
based parse reranking with subtree features. In ACL.
L. Mason, J. Baxter, P. Bartless, and M. Frean. 2000.
Boosting as gradient descent. In NIPS.
F.J. Och et al 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT/NAACL.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL.
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3).
L. Shen, A. Sarkar, and F.J. Och. 2004. Discriminative
reranking for machine translation. In HLT-NAACL.
D. Smith and J. Eisner. 2006. Minimum risk anneal-
ing for training log-linear models. In Proc. of COL-
ING/ACL Companion Volume.
A. Venugopal, A. Zollmann, and A. Waibel. 2005. Train-
ing and evaluating error minimization rules for SMT.
In ACL Workshop on Building/Using Parallel Texts.
J. Xu and H. Li. 2007. AdaRank: A boosting algorithm
for information retrieval. In SIGIR.
40
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 55?62,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
POS Tagging of Dialectal Arabic: A Minimally Supervised Approach
Kevin Duh and Katrin Kirchhoff
Department of Electrical Engineering
University of Washington, Seattle, WA, 98195
{duh,katrin}@ee.washington.edu
Abstract
Natural language processing technology
for the dialects of Arabic is still in its
infancy, due to the problem of obtaining
large amounts of text data for spoken Ara-
bic. In this paper we describe the de-
velopment of a part-of-speech (POS) tag-
ger for Egyptian Colloquial Arabic. We
adopt a minimally supervised approach
that only requires raw text data from sev-
eral varieties of Arabic and a morpholog-
ical analyzer for Modern Standard Ara-
bic. No dialect-specific tools are used. We
present several statistical modeling and
cross-dialectal data sharing techniques to
enhance the performance of the baseline
tagger and compare the results to those
obtained by a supervised tagger trained
on hand-annotated data and, by a state-of-
the-art Modern Standard Arabic tagger ap-
plied to Egyptian Arabic.
1 Introduction
Part-of-speech (POS) tagging is a core natural lan-
guage processing task that can benefit a wide range
of downstream processing applications. Tagging
is often the first step towards parsing or chunking
(Osborne, 2000; Koeling, 2000), and knowledge
of POS tags can benefit statistical language mod-
els for speech recognition or machine translation
(Heeman, 1998; Vergyri et al, 2004). Many ap-
proaches for POS tagging have been developed in
the past, including rule-based tagging (Brill, 1995),
HMM taggers (Brants, 2000; Cutting and oth-
ers, 1992), maximum-entropy models (Rathnaparki,
1996), cyclic dependency networks (Toutanova et
al., 2003), memory-based learning (Daelemans et
al., 1996), etc. All of these approaches require ei-
ther a large amount of annotated training data (for
supervised tagging) or a lexicon listing all possible
tags for each word (for unsupervised tagging). Tag-
gers have been developed for a variety of languages,
including Modern Standard Arabic (MSA) (Khoja,
2001; Diab et al, 2004). Since large amount of text
material as well as standard lexicons can be obtained
in these cases, POS tagging is a straightforward task.
The dialects of Arabic, by contrast, are spoken
rather than written languages. Apart from small
amounts of written dialectal material in e.g. plays,
novels, chat rooms, etc., data can only be obtained
by recording and manually transcribing actual con-
versations. Moreover, there is no universally agreed
upon writing standard for dialects (though several
standardization efforts are underway); any large-
scale data collection and transcription effort there-
fore requires extensive training of annotators to en-
sure consistency. Due to this data acquisition bot-
tleneck, the development of NLP technology for di-
alectal Arabic is still in its infancy. In addition to the
problems of sparse training data and lack of writing
standards, tagging of dialectal Arabic is difficult for
the following reasons:
? Resources such as lexicons, morphological an-
alyzers, tokenizers, etc. are scarce or non-
existent for dialectal Arabic.
? Dialectal Arabic is a spoken language. Tagging
spoken language is typically harder than tag-
55
ging written language, due to the effect of dis-
fluencies, incomplete sentences, varied word
order, etc.
? The rich morphology of Arabic leads to a
large number of possible word forms, which
increases the number of out-of-vocabulary
(OOV) words.
? The lack of short vowel information results in
high lexical ambiguity.
In this paper we present an attempt at developing
a POS tagger for dialectal Arabic in a minimally
supervised way. Our goal is to utilize existing re-
sources and data for several varieties of Arabic in
combination with unsupervised learning algorithms,
rather than developing dialect-specific tools. The
resources available to us are the CallHome Egyp-
tian Colloquial Arabic (ECA) corpus, the LDC Lev-
antine Arabic (LCA) corpus, the LDC MSA Tree-
bank corpus, and a generally available morpholog-
ical analysis tool (the LDC-distributed Buckwalter
stemmer) for MSA. The target dialect is ECA, since
this is the only dialectal corpus for which POS an-
notations are available. Our general approach is
to bootstrap the tagger in an unsupervised way us-
ing POS information from the morphological ana-
lyzer, and to subsequently improve it by integrating
additional data from other dialects and by general
machine learning techniques. We compare the re-
sult against the performance of a tagger trained in a
supervised way and an unsupervised tagger with a
hand-developed ECA lexicon.
2 Data
The ECA corpus consists of telephone conversations
between family members or close friends, with one
speaker being located in the U.S. and the other in
Egypt. We use the combined train, eval96 and hub5
sections of the corpus for training, the dev set for
development and the eval97 set for evaluation. The
LCA data consists of telephone conversations on
pre-defined topics between Levantine speakers pre-
viously unknown to each other; all of the available
data was used. The Treebank corpus is a collection
of MSA newswire text from Agence France Press,
An Nahar News, and Unmah Press. We use parts 1
(v3.0), 2 (v2.0) and 3 (v1.0). The sizes of the vari-
ous corpora are shown in Table 1.
The ECA corpus was originally transcribed in a ?ro-
manized? form; a script representation was then de-
rived automatically from the romanized transcripts.
The script, therefore, does not entirely conform to
the MSA standard: romanized forms may repre-
sent actual pronunciations and contain such MSA
? ECA changes as /?/ ? /s/ or /t/ and /?/ ? /z/
or /d/. The resulting representation cannot be unam-
biguously mapped back to MSA script; the variants
/s/ or /t/, for instance, are represented by   or  ,
rather than  . This introduces additional noise into
the data, but it also mimics the real-world situation
of variable spelling standards that need to be handled
by a robust NLP system. We use the script represen-
tation of this corpus for all our experiments. The
ECA corpus is accompanied by a lexicon contain-
ing the morphological analysis of all words, i.e. an
analysis in terms of stem and morphological charac-
teristics such as person, number, gender, POS, etc.
Since the analysis is based on the romanized form, a
single tag can be assigned to the majority of words
(75% of all tokens) in the corpus. We use this assign-
ment as the reference annotation for our experiments
to evaluate the output of our tagger. The remaining
25% tokens (ambiguous words) have 2 or more tags
in the lexicon and are thus ignored during evaluation
since the correct reference tag cannot be determined.
Both the LCA and the MSA Treebank data sets
were transcribed in standard MSA script. The LCA
corpus only consists of raw orthographic transcrip-
tions; no further annotations exist for this data set.
Each word in the Treebank corpus is associated with
all of its possible POS tags; the correct tag has been
marked manually. We use the undecomposed word
forms rather than the forms resulting from splitting
off conjunctions, prepositions, and other clitics. Al-
though improved tokenization can be expected to
result in better tagging performance, tokenization
tools for dialectal Arabic are currently not available,
and our goal was to create comparable conditions
for tagging across all of our data sets. Preprocessing
of the data thus only included removing punctuation
from the MSA data and removing word fragments
from the spoken language corpora. Other disfluen-
cies (fillers and repetitions) were retained since they
are likely to have predictive value. Finally, single-
ton words (e.g. inconsistent spellings) were removed
56
from the LCA data. The properties of the different
data sets (number of words, n-grams, and ambigu-
ous words) are displayed in Table 1.
ECA LCA MSA
train dev test
sentences 25k 6k 2.7k 114k 20k
# tokens 156k 31k 12k 476k 552k
# types 15k 5k 1.5k 16k 65k
# bigrams 81k 20k 7k 180k 336k
# trigrams 125k 26k 10k 320k 458k
% ambig. 24.4 27.8 28.2 ? ?
Table 1: Corpus statistics for ECA, LCA and MSA.
The only resource we utilize in addition to raw
data is the LDC-distributed Buckwalter stemmer.
The stemmer analyzes MSA script forms and out-
puts all possible morphological analyses (stems and
POS tags, as well as diacritizations) for the word.
The analysis is based on an internal stem lexi-
con combined with rules for affixation. Although
the stemmer was developed primarily for MSA, it
can accommodate a certain percentage of dialectal
words. Table 2 shows the percentages of word types
and tokens in the ECA and LCA corpora that re-
ceived an analysis from the Buckwalter stemmer.
Since both sets contain dialectal as well as standard
MSA forms, it is not possible to determine precisely
how many of the unanalyzable forms are dialectal
forms vs. words that were rejected for other rea-
sons, such as misspellings. The higher percentage
of rejected word types in the ECA corpus is most
likely due to the non-standard script forms described
above.
Type Token
N ECA LCA ECA LCA
0 37.6 23.3 18.2 28.2
1 34.0 52.5 33.6 40.4
2 19.4 17.7 26.4 19.9
3 7.2 5.2 16.2 10.5
4 1.4 1.0 5.0 2.3
5 0.1 0.1 0.4 0.6
Table 2: Percentage of word types/tokens with N possible
tags, as determined by the Buckwalter stemmer. Words with
0 tags are unanalyzable.
The POS tags used in the LDC ECA annota-
tion and in the Buckwalter stemmer are rather fine-
grained; furthermore, they are not identical. We
therefore mapped both sets of tags to a unified, sim-
pler tagset consisting only of the major POS cate-
gories listed in Table 2. The mapping from the orig-
inal Buckwalter tag to the simplified set was based
on the conversion scheme suggested in (Bies, 2003).
The same underlying conversion rules were applica-
ble to most of the LDC tags; those cases that could
not be determined automatically were converted by
hand.
Symbol Gloss (%)
CC coordinating conjunction 7.15
DT determiner 2.23
FOR foreign word 1.18
IN preposition 7.46
JJ adjective 6.02
NN noun 19.95
NNP proper noun 3.55
NNS non-singular noun 3.04
NOTAG non-word 0.05
PRP pronoun 5.85
RB adverb 4.13
RP particle 9.71
UH disfluency, interjection 9.55
VBD perfect verb 6.53
VBN passive verb 1.88
VBP imperfect verb 10.62
WP relative pronoun 1.08
Table 3: Collapsed tagset and percentage of occur-
rence of each tag in the ECA corpus.
Prior to the development of our tagger we com-
puted the cross-corpus coverage of word n-grams
in the ECA development set, in order to verify our
assumption that utilizing data from other dialects
might be helpful. Table 4 demonstrates that the
n-gram coverage of the ECA development set in-
creases slightly by adding LCA and MSA data.
Types Tokens
1gr 2gr 3gr 1gr 2gr 3gr
ECA 64 33 12 94 58 22
LCA 31 9 1.4 69 20 3.7
ECA + LCA 68 35 13 95 60 23
MSA 32 3.7 0.2 66 8.6 0.3
ECA + MSA 71 34 12 95 60 22
Table 4: Percentages of n-gram types and tokens in ECA dev
set that are covered by the ECA training set, the LCA set, com-
bined ECA training + LCA set, and MSA sets. Note that adding
the LCA or MSA improves the coverage slightly.
57
3 Baseline Tagger
We use a statistical trigram tagger in the form of a
hidden Markov model (HMM). Let w0:M be a se-
quence of words (w0, w1, . . . , wM ) and t0:M be the
corresponding sequence of tags. The trigram HMM
computes the conditional probability of the word
and tag sequence p(w0:M , t0:M ) as:
P (t0:M |w0:M ) =
M
?
i=0
p(wi|ti)p(ti|ti?1, ti?2) (1)
The lexical model p(wi|ti) characterizes the dis-
tribution of words for a specific tag; the contex-
tual model p(ti|ti?1, ti?2) is trigram model over
the tag sequence. For notational simplicity, in
subsequent sections we will denote p(ti|ti?1, ti?2)
as p(ti|hi), where hi represents the tag history.
The HMM is trained to maximize the likelihood
of the training data. In supervised training, both
tag and word sequences are observed, so the max-
imum likelihood estimate is obtained by relative fre-
quency counting, and, possibly, smoothing. Dur-
ing unsupervised training, the tag sequences are
hidden, and the Expectation-Maximization Algo-
rithm is used to iteratively update probabilities based
on expected counts. Unsupervised tagging re-
quires a lexicon specifying the set of possible tags
for each word. Given a test sentence w?0:M , the
Viterbi algorithm is used to find the tag sequence
maximizing the probability of tags given words:
t?0:M = argmaxt0:M p(t0:M |w?0:M ). Our taggers
are implemented using the Graphical Models Toolkit
(GMTK) (Bilmes and Zweig, 2002), which allows
training a wide range of probabilistic models with
both hidden and observed variables.
As a first step, we compare the performance of
four different baseline systems on our ECA dev set.
First, we trained a supervised tagger on the MSA
treebank corpus (System I), in order to gauge how a
standard system trained on written Arabic performs
on dialectal speech. The second system (System II)
is a supervised tagger trained on the manual ECA
POS annotations. System III is an unsupervised tag-
ger on the ECA training set. The lexicon for this
system is derived from the reference annotations of
the training set ? thus, the correct tag is not known
during training, but the lexicon is constrained by
expert information. The difference in accuracy be-
tween Systems II and III indicates the loss due to the
unsupervised training method. Finally, we trained a
system using only the unannotated ECA data and a
lexicon generated by applying the MSA analyzer to
the training corpus and collecting all resulting tags
for each word. In this case, the lexicon is much less
constrained; moreover, many words do not receive
an output from the stemmer at all. This is the train-
ing method with the least amount of supervision and
therefore the method we are interested in most.
Table 5 shows the accuracies of the four systems
on the ECA development set. Also included is a
breakdown of accuracy by analyzable (AW), unana-
lyzable (UW), and out-of-vocabulary (OOV) words.
Analyzable words are those for which the stemmer
outputs possible analyses; unanalyzable words can-
not be processed by the stemmer. The percent-
age of unanalyzable word tokens in the dev set is
18.8%. The MSA-trained tagger (System I) achieves
an accuracy of 97% on a held-out set (117k words)
of MSA data, but performs poorly on ECA due to
a high OOV rate (43%). By contrast, the OOV
rate for taggers trained on ECA data is 9.5%. The
minimally-supervised tagger (System IV) achieves a
baseline accuracy of 62.76%. In the following sec-
tions, we present several methods to improve this
system, in order to approximate as closely as possi-
ble the performance of the supervised systems. 1
System Total AW UW OOV
I 39.84 55.98 21.05 19.21
II 92.53 98.64 99.09 32.20
III 84.88 90.17 99.11 32.64
IV 62.76 67.07 20.74 21.84
Table 5: Tagging accuracy (%) for the 4 baseline
systems. AW = analyzable words, UW unanalyzable
words, OOV = out-of-vocabulary words.
4 System Improvements
4.1 Adding Affix Features
The low accuracy of unanalyzable and OOV words
may significantly impact downstream applications.
1The accuracy of a naive tagger which labels all words with
the most likely tag (NN) achieves an accuracy of 20%. A tagger
which labels words with the most likely tag amongst its possible
tags achieves an accuracy of 52%.
58
One common way to improve accuracy is to add
word features. In particular, we are interested in
features that can be derived automatically from the
script form, such as affixes. Affix features are
added in a Naive Bayes fashion to the basic HMM
model defined in Eq.1. In addition to the lexical
model p(wi|ti) we now have prefix and suffix mod-
els p(ai|ti) and p(bi|ti), where a and b are the prefix
and suffix variables, respectively. The affixes used
are:   -,   -,  -,  -, 	
 -,   -,   -,  -, - , - , -  ,
-Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 399?407,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Lexicon Acquisition for Dialectal Arabic Using Transductive Learning
Kevin Duh
Dept. of Electrical Engineering
University of Washington
Seattle, WA, USA
duh@ee.washington.edu
Katrin Kirchhoff
Dept. of Electrical Engineering
University of Washington
Seattle, WA, USA
katrin@ee.washington.edu
Abstract
We investigate the problem of learn-
ing a part-of-speech (POS) lexicon for a
resource-poor language, dialectal Arabic.
Developing a high-quality lexicon is often
the first step towards building a POS tag-
ger, which is in turn the front-end to many
NLP systems. We frame the lexicon ac-
quisition problem as a transductive learn-
ing problem, and perform comparisons
on three transductive algorithms: Trans-
ductive SVMs, Spectral Graph Transduc-
ers, and a novel Transductive Clustering
method. We demonstrate that lexicon
learning is an important task in resource-
poor domains and leads to significant im-
provements in tagging accuracy for dialec-
tal Arabic.
1 Introduction
Due to the rising importance of globalization and
multilingualism, there is a need to build natu-
ral language processing (NLP) systems for an in-
creasingly wider range of languages, including
those languages that have traditionally not been
the focus of NLP research. The development of
NLP technologies for a new language is a chal-
lenging task since one needs to deal not only with
language-specific phenomena but also with a po-
tential lack of available resources (e.g. lexicons,
text, annotations). In this study we investigate the
problem of learning a part-of-speech (POS) lexi-
con for a resource-poor language, dialectal Arabic.
Developing a high-quality POS lexicon is the
first step towards training a POS tagger, which in
turn is typically the front end for other NLP appli-
cations such as parsing and language modeling. In
the case of resource-poor languages (and dialec-
tal Arabic in particular), this step is much more
critical than is typically assumed: a lexicon with
too few constraints on the possible POS tags for
a given word can have disastrous effects on tag-
ging accuracy. Whereas such constraints can be
obtained from large hand-labeled corpora or high-
quality annotation tools in the case of resource-
rich languages, no such resources are available for
dialectal Arabic. Instead, constraints on possible
POS tags must be inferred from a small amount
of tagged words, or imperfect analysis tools. This
can be seen as the problem of learning complex,
structured outputs (multi-class labels, with a dif-
ferent number of classes for different words and
dependencies among the individual labels) from
partially labeled data.
Our focus is on investigating several machine
learning techniques for this problem. In partic-
ular, we argue that lexicon learning in resource-
poor languages can be best viewed as transduc-
tive learning. The main contribution of this work
are: (1) a comprehensive evaluation of three trans-
ductive algorithms (Transductive SVM, Spectral
Graph Transducer, and a new technique called
Transductive Clustering) as well as an inductive
SVM on this task; and (2) a demonstration that
lexicon learning is a worthwhile investment and
leads to significant improvements in the tagging
accuracy for dialectal Arabic.
The outline of the paper is as follows: Section 2
describes the problem in more detail and discusses
the situation in dialectal Arabic. The transductive
framework and algorithms for lexicon learning are
elaborated in Section 3. Sections 4 and 5 describe
the data and system. Experimental results are pre-
sented in Section 6. We discuss some related work
in Section 7 before concluding in Section 8.
399
2 The Importance of Lexicons in
Resource-poor POS Tagging
2.1 Unsupervised Tagging
The lack of annotated training data in resource-
poor languages necessitates the use of unsuper-
vised taggers. One commonly-used unsuper-
vised tagger is the Hidden Markov model (HMM),
which models the joint distribution of a word se-
quence w0:M and tag sequence t0:M as:
P (t0:M , w0:M ) =
M
?
i=0
p(wi|ti)p(ti|ti?1, ti?2)
(1)
This is a trigram HMM. Unsupervised learn-
ing is performed by running the Expectation-
Maximization (EM) algorithm on raw text. In this
procedure, the tag sequences are unknown, and the
probability tables p(wi|ti) and p(ti|ti?1, ti?2) are
iteratively updated to maximize the likelihood of
the observed word sequences.
Although previous research in unsupervised
tagging have achieved high accuracies rivaling su-
pervised methods (Kupiec, 1992; Brill, 1995),
much of the success is due to the use of artifi-
cially constrained lexicons. Specifically, the lex-
icon is a wordlist where each word is annotated
with the set of all its possible tags. (We will call
the set of possible tags of a given word the POS-
set of that word; an example: POS-set of the En-
glish word bank may be {NN,VB}.) Banko and
Moore (2004) showed that unsupervised tagger ac-
curacies on English degrade from 96% to 77% if
the lexicon is not constrained such that only high
frequency tags exist in the POS-set for each word.
Why is the lexicon so critical in unsupervised
tagging? The answer is that it provides addi-
tional knowledge about word-tag distributions that
may otherwise be difficult to glean from raw text
alone. In the case of unsupervised HMM taggers,
the lexicon provides constraints on the probability
tables p(wi|ti) and p(ti|ti?1, ti?2). Specifically,
the lexical probability table is initialized such that
p(wi|ti) = 0 if and only if tag ti is not included in
the POS-set of word wi. The transition probability
table is initialized such that p(ti|ti?1, ti?2) = 0 if
and only if the tag sequence (ti, ti?1, ti?2) never
occurs in the tag lattice induced by the lexicon on
the raw text. The effect of these zero-probability
initialization is that they will always stay zero
throughout the EM procedure (modulo the effects
of smoothing). This therefore acts as hard con-
straints and biases the EM algorithm to avoid cer-
tain solutions when maximizing likelihood. If the
lexicon is accurate, then the EM algorithm can
learn very good predictive distributions from raw
text only; conversely, if the lexicon is poor, EM
will be faced with more confusability during train-
ing and may not produce a good tagger. In general,
the addition of rare tags, even if they are correct,
creates a harder learning problem for EM.
Thus, a critical aspect of resource-poor POS
tagging is the acquisition of a high-quality lexi-
con. This task is challenging because the lexicon
learning algorithm must not be resource-intensive.
In practice, one may be able to find analysis tools
or incomplete annotations such that only a partial
lexicon is available. The focus is therefore on ef-
fective machine learning algorithms for inferring
a full high-quality lexicon from a partial, possibly
noisy initial lexicon. We shall now discuss this sit-
uation in the context of dialectal Arabic.
2.2 Dialectal Arabic
The Arabic language consist of a collection of
spoken dialects and a standard written language
(Modern Standard Arabic, or MSA). The dialects
of Arabic are of considerable importance since
they are used extensively in almost all everyday
conversations. NLP technology for dialectal Ara-
bic is still in its infancy, however, due to the lack
of data and resources. Apart from small amounts
of written dialectal material in e.g. plays, novels,
chat rooms, etc., data can only be obtained by
recording and manually transcribing actual con-
versations. Annotated corpora are scarce because
annotation requires another stage of manual ef-
fort beyond transcription work. In addition, ba-
sic resources such as lexicons, morphological an-
alyzers, tokenizers, etc. have been developed for
MSA, but are virtually non-existent for dialectal
Arabic.
In this study, we address lexicon learning for
Levantine Colloquial Arabic. We assume that only
two resources are available during training: (1)
raw text transcriptions of Levantine speech and (2)
a morphological analyzer developed for MSA.
The lexicon learning task begins with a par-
tial lexicon generated by applying the MSA ana-
lyzer to the Levantine wordlist. Since MSA dif-
fers from Levantine considerably in terms of syn-
tax, morphology, and lexical choice, not all Lev-
antine words receive an analysis. In our data,
23% of the words are un-analyzable. Thus, the
400
goal of lexicon learning is to infer the POS-sets
of the un-analyzable words, given the partially-
annotated lexicon and raw text.
Details on the Levantine data and overall system
are provided in Sections 4 and 5. We discuss the
learning algorithms in the next section.
3 Learning Frameworks and Algorithms
Let us formally define the lexicon learning prob-
lem. We have a wordlist of size m + u. A portion
of these words (m) are annotated with POS-set la-
bels, which may be acquired by manual annotation
or an automatic analysis tool. The set of labeled
words {Xm} is the training set, also referred to as
the partial lexicon. The task is to predict the POS-
sets of the remaining u unlabeled words {Xu}, the
test set. The goal of lexicon learning is to label
{Xu} with low error. The final result is a full lex-
icon that contains POS-sets for all m + u words.
3.1 Transductive Learning with Structured
Outputs
We argue that the above problem formulation
lends itself to a transductive learning framework.
Standard inductive learning uses a training set of
fully labeled samples in order to learn a classi-
fication function. After completion of the train-
ing phase, the learned model is then used to clas-
sify samples from a new, previously unseen test
set. Semi-supervised inductive learning exploits
unlabeled data in addition to labeled data to better
learn a classification function. Transductive learn-
ing, first described by Vapnik (Vapnik, 1998) also
describes a setting where both labeled and unla-
beled data are used jointly to decide on a label as-
signment to the unlabeled data points. However,
the goal here is not to learn a general classifica-
tion function that can be applied to new test sets
multiple times but to achieve a high-quality one-
time labeling of a particular data set. Transduc-
tive learning and inductive semi-supervised learn-
ing are sometimes confused in the literature. Both
approaches use unlabeled data in learning ? the
key difference is that a transductive classifier only
optimizes the performance on the given unlabeled
data while an inductive semi-supervised classifier
is trained to perform well on any new unlabeled
data.
Lexicon learning fits in the transductive learn-
ing framework as follows: The test set {Xu},
i.e. the unlabeled words, is static and known dur-
NN?VB vs. ~NN?VB
NN?JJ vs. ~NN?JJ
VB vs. ~VB
NN vs. ~NN
VB?JJ vs. ~VB?JJ
..., etc.
0.8
0.6
?0.4
?0.4
0.7 argmax NN?JJ
NN vs.~NN
VB vs. ~VB
JJ vs. ~JJ
sample
sample
K independent classifiers + 1 overall classifier
SINGLE?LABEL FRAMWORK
COMPOUND?LABEL FRAMEWORK
1 multi?class classifier
(one?vs?rest implementation using N binary classifiers)
0.9
?0.8 {NN,JJ}
0.1
Classifier
2nd Stage
Figure 1: Learning with Structured Outputs using
single or compound labels
ing learning time; we are not interested in inferring
POS-sets for any words outside the word list.
An additional characterization of the lexicon
learning problem is that it is a problem of learn-
ing with complex, structured outputs. The label
for each word is its POS-set, which may contain
one to K POS tags (where K is the size of the
tagset, K=20 in our case). This differs from tra-
ditional classification tasks where the output is a
single scalar variable.
Structured output problems like lexicon learn-
ing can be characterized by the granularity of the
basic unit of labels. We define two cases: single-
label and compound-label. In the single-label
framework (see Figure 1), each individual POS tag
is the target of classification and we have K binary
classifiers each hypothesizing whether a word has
a POS tag k (k = 1, . . . ,K). A second-stage clas-
sifier takes the results of the K individual classi-
fiers and outputs a POS-set. This classifier can
simply take all POS tags hypothesized positive by
the individual binary classifiers to form the POS-
set, or use a more sophisticated scheme for deter-
mining the number of POS tags (Elisseeff and We-
ston, 2002).
The alternative compound-label framework
treats each POS-set as an atomic label for clas-
sification. A POS-set such as {?NN?, ?VB?} is
?compounded? into one label ?NN-VB?, which re-
sults in a different label than, say, ?NN? or ?NN-
JJ?. Suppose there exist N distinct POS-sets in the
401
training data; then we have N atomic units for la-
beling. Thus a (N -ary) multi-class classifier is em-
ployed to directly predict the POS-set of a word. If
only binary classifiers are available (i.e. in the case
of Support Vector Machines), one can use one-vs-
rest, pairwise, or error correcting code schemes to
implement the multi-class classification.
The single-label framework is potentially ill-
suited for capturing the dependencies between
POS tags. Dependencies between POS tags arise
since some tags, such as ?NN? and ?NNP? can of-
ten be tagged to the same word and therefore co-
occur in the POS-set label. The compound-label
framework implicitly captures tag co-occurrence,
but potentially suffers from training data fragmen-
tation as well as the inability to hypothesize POS-
sets that do not already exist in the training data.
In our initial experiments, the compound-label
framework gave better classification results; thus
we implemented all of our algorithms in the multi-
class framework (using the one-vs-rest scheme
and choosing the argmax as the final decision).
3.2 Transductive Clustering
How does a transductive algorithm effectively uti-
lize unlabeled samples in the learning process?
One popular approach is the application of the so-
called cluster assumption, which intuitively states
that samples close to each other (i.e. samples that
form a cluster) should have similar labels.
Transductive clustering (TC) is a simple algo-
rithm that directly implements the cluster assump-
tion. The algorithm clusters labeled and unlabeled
samples jointly, then uses the labels of labeled
samples to infer the labels of unlabeled words in
the same cluster. This idea is relatively straight-
forward, yet what is needed is a principled way
of deciding the correct number of clusters and the
precise way of label transduction (e.g. based on
majority vote vs. probability thresholds). Typ-
ically, such parameters are decided heuristically
(e.g. (Duh and Kirchhoff, 2005a)) or by tuning on
a labeled development set; for resource-poor lan-
guages, however, no such set may be available.
As suggested by (El-Yaniv and Gerzon, 2005),
the TC algorithm can utilize a theoretical error
bound as a principled way of determining the pa-
rameters. Let R?h(Xm) be the empirical risk of a
given hypothesis (i.e. classifier) on the training set;
let Rh(Xu) be the test risk. (Derbeko et al, 2004)
derive an error bound which states that, with prob-
ability 1??, the risk on the test samples is bounded
by:
Rh(Xu) ? R?h(Xm)
+
?
(m+u
u
)
(
u+1
u
)
(
ln 1p(h)+ln
1
?
2m
)
(2)
i.e. the test risk is bounded by the empirical risk on
the labeled data, R?h(Xm), plus a term that varies
with the prior p(h) of the hypothesis or classifier.
This is a PAC-Bayesian bound (McAllester, 1999).
The prior p(h) indicates ones prior belief on the
hypothesis h over the set of all possible hypothe-
ses. If the prior is low or the empirical risk is high,
then the bound is large, implying that test risk may
be large. A good hypothesis (i.e. classifier) will
ideally have a small value for the bound, thus pre-
dicting a small expected test risk.
The PAC-Bayesian bound is important because
it provides a theoretical guarantee on the quality
of a hypothesis. Moreover, the bound in Eq. 2 is
particularly useful because it is easily computable
on any hypothesis h, assuming that one is given
the value of p(h). Given two hypothesized label-
ings of the test set, h1 and h2, the one with the
lower PAC-Bayesian bound will achieve a lower
expected test risk. Therefore, one can use the
bound as a principled way of choosing the pa-
rameters in the Transductive Clustering algorithm:
First, a large number of different clusterings is cre-
ated; then the one that achieves the lowest PAC-
Bayesian bound is chosen. The pseudo-code is
given in Figure 2.
(El-Yaniv and Gerzon, 2005) has applied the
Transductive Clustering algorithm successfully to
binary classification problems and demonstrated
improvements over the current state-of-the-art
Spectral Graph Transducers (Section 3.4). We use
the algorithm as described in (Duh and Kirchhoff,
2005b), which adapts the algorithm to structured
output problems. In particular, the modification
involves a different estimate of the priors p(h),
which was assumed to be uniform in (El-Yaniv and
Gerzon, 2005). Since there are many possible h,
adopting a uniform prior will lead to small values
of p(h) and thus a loose bound for all h. Proba-
bility mass should only be spent on POS-sets that
are possible, and as such, we calculate p(h) based
on frequencies of compound-labels in the training
data (i.e. an empirical prior).
3.3 Transductive SVM
Transductive SVM (TSVM) (Joachims, 1999) is
an algorithm that implicitly implements the cluster
402
1 For ? = 2 : C (C is set arbitrarily to a large number)
2 Apply a clustering algorithm to generate ? clusters on Xm+u.
3 Generate label hypothesis h? (by labeling each cluster with the most frequent label among its labeled samples)
4 Calculate the bound for h? as defined in Eq. 2.
5 Choose the hypothesis h? with the lowest bound; output the corresponding classification of Xu.
Figure 2: Pseudo-code for transductive clustering.
assumption. In standard inductive SVM (ISVM),
the learning algorithm seeks to maximize the mar-
gin subject to misclassification constraints on the
training samples. In TSVM, this optimization is
generalized to include additional constraints on
the unlabeled samples. The resulting optimiza-
tion algorithm seeks to maximize the margin on
both labeled and unlabeled samples and creates a
hyperplane that avoids high-density regions (e.g.
clusters).
3.4 Spectral Graph Transducer
Spectral Graph Transducer (SGT) (Joachims,
2003) achieves transduction via an extension of
the normalized mincut clustering criterion. First,
a data graph is constructed where the vertices are
labeled or unlabeled samples and the edge weights
represent similarities between samples. The min-
cut criteria seeks to partition the graph such that
the sum of cut edges is minimized. SGT extends
this idea to transductive learning by incorporating
constraints that require samples of the same label
to be in the same cluster. The resulting partitions
decide the label of unlabeled samples.
4 Data
4.1 Corpus
The dialect addressed in this work is Levantine
Colloquial Arabic (LCA), primarily spoken in Jor-
dan, Lebanon, Palestine, and Syria. Our devel-
opment/test data comes from the Levantine Ara-
bic CTS Treebank provided by LDC. The train-
ing data comes from the Levantine CTS Audio
Transcripts. Both are from the Fisher collection
of conversational telephone speech between Lev-
antine speakers previously unknown to each other.
The LCA data was transcribed in standard MSA
script and transliterated into ASCII characters us-
ing the Buckwalter transliteration scheme1. No di-
acritics are used in either the training or develop-
ment/test data. Speech effects such as disfluencies
and noises were removed prior to our experiments.
1http://www.ldc.upenn.edu/myl/morph/buckwalter.html
The training set consists of 476k tokens and
16.6k types. It is not annotated with POS tags ?
this is the raw text we use to train the unsuper-
vised HMM tagger. The test set consists of 15k
tokens and 2.4k types, and is manually annotated
with POS tags. The development set is also POS-
annotated, and contains 16k tokens and 2.4k types.
We used the reduced tagset known as the Bies
tagset (Maamouri et al, 2004), which focuses on
major part-of-speech and excludes detailed mor-
phological information.
Using the compound-label framework, we
observe 220 and 67 distinct compound-labels
(i.e. POS-sets) in the training and test sets, respec-
tively. As mentioned in Section 3.1, a classifier
in the compound-label framework can never hy-
pothesize POS-sets that do not exist in the training
data: 43% of the test vocabulary (and 8.5% by to-
ken frequency) fall under this category.
4.2 Morphological Analyzer
We employ the LDC-distributed Buckwalter ana-
lyzer for morphological analyses of Arabic words.
For a given word, the analyzer outputs all possi-
ble morphological analyses, including stems, POS
tags, and diacritizations. The information regard-
ing possible POS tags for a given word is crucial
for constraining the unsupervised learning process
in HMM taggers.
The Buckwalter analyzer is based on an internal
stem lexicon combined with rules for affixation. It
was originally developed for the MSA, so only a
certain percentage of Levantine words can be cor-
rectly analyzed. Table 1 shows the percentages
of words in the LCA training text that received N
possible POS tags from the Buckwalter analyzer.
Roughly 23% of types and 28% of tokens received
no tags (N=0) and are considered un-analyzable.
5 System
Our overall system looks as follows (see Figure
3): In Step 1, the MSA (Buckwalter) analyzer
is applied to the word list derived from the raw
training text. The result is a partial POS lexicon,
403
word2 JJ?NN
word3 JJ
word4 ?
word5 ?
word1 NN?VB
HMM TaggerFull POS LexiconPartial POS Lexicon
RAW
TEXT
Buckwalter
Analyzer (1)
Transductive
Learning (2) Training (3)
EM
word2 JJ?NN
word3 JJ
word1 NN?VB
word4 NN?VB
word5 JJ
Figure 3: Overall System: (1) Apply Buckwalter Analyzer to dialectal Arabic raw text, obtaining a
partial POS lexicon. (2) Use Transductive Learning to infer missing POS-sets. (3) Unsupervised training
of HMM Tagger using both raw text and inferred lexicon.
N Type Token
0 23.3 28.2
1 52.5 40.4
2 17.7 19.9
3 5.2 10.5
4 1.0 2.3
5 0.1 0.6
Table 1: Percentage of word types/tokens with N
possible tags, as determined by the Buckwalter an-
alyzer. Words with 0 tags are un-analyzable.
which lists the set of possible POS tags for those
words for which the analyzer provided some out-
put. All possibilities suggested by the analyzer are
included.
The focus of Step 2 is to infer the POS-sets of
the remaining, unannotated words using one of the
automatic learning procedures described in Sec-
tion 3. Finally, Step 3 involves training an HMM
tagger using the learned lexicon. This is the stan-
dard unsupervised learning component of the sys-
tem. We use a trigram HMM, although modifica-
tions such as the addition of affixes and variables
modeling speech effects may improve tagging ac-
curacy. Our concern here is the evaluation of the
lexicon learning component in Step 2.
An important problem in this system setup is
the possibility of error propagation. In Step 1, the
MSA analyzer may give incorrect POS-sets to ana-
lyzable words. It may not posit the correct tag (low
recall), or it may give too many tags (low preci-
sion). Both have a negative effect on lexicon learn-
ing and EM training. For lexicon learning, Step
1 errors represent corrupt training data; For EM
training, Step 1 error may cause the HMM tagger
to never hypothesize the correct tag (low recall) or
have too much confusibility during training (low
precision). We attempted to measure the extent of
this error by calculating the tag precision/recall on
words that occur in the test set: Among the 12k
words analyzed by the analyzer, 1483 words oc-
cur in the test data. We used the annotations in
the test data and collected all the ?oracle? POS-
sets for each of these 1483 words.2 The aver-
age precision of the analyzer-generated POS-sets
against the oracle is 56.46%. The average recall
is 81.25%. Note that precision is low?this implies
that the partial lexicon is not very constrained. The
recall of 81.25% means that 18.75% of the words
may never receive the correct tag in tagging. In
the experiments, we will investigate to what ex-
tent this kind of error affects lexicon learning and
EM training.
6 Experiments
6.1 Lexicon learning experiments
We seek to answer the following three questions
in our experiments:
? How useful is the lexicon learning step in an
end-to-end POS tagging system? Do the ma-
chine learning algorithms produce lexicons
that result in higher tagging accuracies, when
compared to a baseline lexicon that simply
hypothesizes all POS tags for un-analyzable
words? The answer is a definitive yes.
? What machine learning algorithms perform
the best on this task? Do transductive learn-
ing outperform inductive learning? The em-
pirical answer is that TSVM performs best,
SGT performs worst, and TC and ISVM are
in the middle.
2Since the test set is small, these ?oracle? POS-sets may
be missing some tags. Thus the true precision may be higher
(and recall may be lower) than measured.
404
Orthographic features:
wi matches /?pre/, pre = {set of data-derived prefixes}
wi matches /suf$/, suf = {set of data-derived suffixes}
Contextual features:
wi?1 = voc, voc = {set of words in lexicon}
ti?1 = tag, tag = {set of POS tags}
ti+1 = tag, tag = {set of POS tags}
wi?1 is an un-analyzable word
wi+1 is an un-analyzable word
Table 2: Binary features used for predicting POS-
sets of un-analyzable words.
? What is the relative impact of errors from the
MSA analyzer on lexicon learning and EM
training? The answer is that Step 1 errors af-
fect EM training more, and lexicon learning
is comparably robust to these errors.
In our problem, we have 12k labeled samples
and 3970 unlabeled samples. We define the feature
of each sample as listed in Table 2. The contextual
features are generated by co-occurrence statistics
gleaned from the training data. For instance, for
a word foo, we collect all bigrams consisting of
foo from the raw text; all features [wt?1 = voc]
that correspond to the bigrams (voc, foo) are set
to 1. The idea is that words with similar ortho-
graphic and/or contextual features should receive
similar POS-sets.
All results, unless otherwise noted, are tagging
accuracies on the test set given by training a HMM
tagger on a specific lexicon. Table 3 gives tagging
accuracies of the four machine learning methods
(TSVM, TC, ISVM, SGT) as well as two base-
line approaches for generating a lexicon: (all tags)
gives all 20 possible tags to the un-analyzable
words, whereas (open class) gives only the sub-
set of open-class POS tags.3 The results are given
in descending order of overall tagging accuracy.4
With the exception of TSVM (63.54%) vs. TC
(62.89%), all differences are statistically signifi-
cant. As seen in the table, applying a machine
learning step for lexicon learning is a worthwhile
effort since it always leads to better tagging accu-
racies than the baseline methods.
3Not all un-analyzable words are open-class. Close-class
words may be un-analyzable due to dialectal spelling varia-
tions.
4Note that the unknown word accuracies do not follow
the same trend and are generally quite low. This might be
due to the fact that POS tags of unknown words are usually
best predicted by the HMM?s transition probabilities, which
may not be as robust due to the noisy lexicon.
Method Accuracy UnkAcc
TSVM 63.54 26.19
TC 62.89 26.71
ISVM 61.53 27.68
SGT 59.68 25.82
open class 57.39 27.08
all tags 55.64 25.00
Table 3: Tagging Accuracies for lexicons derived
by machine learning (TSVM, TC, ISVM, SGT)
and baseline methods. Accuracy=Overall accu-
racy; UnkAcc=Accuracy of unknown words.
The poor performance of SGT is somewhat sur-
prising since it is contrary to results presented in
other papers. We attributed this to the difficulty in
constructing the data graph. For instance, we con-
structed k-nearest-neighbor graphs based on the
cosine distance between feature vectors, but it is
difficult to decide the best distance metric or num-
ber of neighbors. Finally, we note that besides the
performance of SGT, transductive learning meth-
ods (TSVM, TC) outperform the inductive ISVM.
We also compute precision/recall statistics of
the final lexicon on the test set words (similar to
Section 5) and measure the average size of the
POS-sets (?POSset?). As seen in Table 4, POS-
set sizes of machine-learned lexicon is a factor of
2 or 3 smaller than that of the baseline lexicons.
On the other hand, recall is better for the baseline
lexicons. These observations, combined with the
fact that machine-learned lexicons gave better tag-
ging accuracy, suggests that we have a constrained
lexicon effect here: i.e. for EM training, it is better
to constrain the lexicon with small POS-sets than
to achieve high recall.
Method Precision Recall ?POSset?
TSVM 58.15 88.85 1.89
TC 59.19 87.88 1.80
ISVM 58.09 88.44 1.87
SGT 53.98 82.60 1.87
open class 54.03 96.77 3.39
all tags 53.31 98.53 5.17
Table 4: Statistics of the Lexicons in Table 3.
Next, we examined the effects of error propa-
gation from the MSA analyzer in Step 1. We at-
tempted to correct these errors by using POS-sets
of words derived from the development data. In
405
particular, of the 1562 partial lexicon words that
also occur in the development set, we found 1044
words without entirely matching POS-sets. These
POS-sets are replaced with the oracle POS-sets de-
rived from the development data, and the result is
treated as the (corrected) partial lexicon of Step 1.
In this procedure, the average POS-set size of the
partial lexicon decreased from 2.13 to 1.10, recall
increased from 82.44% to 100%, and precision in-
creased from 57.15% to 64.31%. We apply lexi-
con learning to this corrected partial lexicon and
evaluate tagging results, shown in Table 5. The
fact that all numbers in Table 5 represent signifi-
cant improvements over Table 3 implies that error
propagation is not a trivial problem, and automatic
error correction methods may be desired.
Method Accuracy UnkAcc
TSVM 66.54 27.38
ISVM 65.08 26.86
TC 64.05 28.20
SGT 63.78 27.23
all tags 62.96 27.91
open class 61.26 27.83
Table 5: Tag accuracies by correcting mistakes in
the partial lexicon prior to lexicon learning. In-
terestingly, we note ISVM outperforms TC here,
which differs from Table 3.
Finally, we determine whether error propaga-
tion impacts lexicon learning (Step 2) or EM train-
ing (Step 3) more. Table 6 shows the results of
TSVM for four scenarios: correcting analyzer er-
rors in the the lexicon: (A) prior to lexicon learn-
ing, (B) prior to EM training, (C) both, or (D)
none. As seen in Table 6, correcting the lexicon
at Step 3 (EM training) gives the most improve-
ments, indicating that analyzer errors affects EM
training more than lexicon learning. This implies
that lexicon learning is relatively robust to train-
ing data corruption, and that one can mainly focus
on improved estimation techniques for EM train-
ing (Wang and Schuurmans, 2005) if the goal is to
alleviate the impact of analyzer errors. The same
evaluation on the other machine learning methods
(TC, ISVM, SGT) show similar results.
6.2 Comparison experiments: Expert lexicon
and supervised learning
Our approach to building a resource-poor POS
tagger involves (a) lexicon learning, and (b) un-
Scenario Step2 Step3 TSVM
(B) N Y 66.70
(C) Y Y 66.54
(A) Y N 64.93
(D) N N 63.54
Table 6: Effect of correcting the lexicon in differ-
ent steps. Y=yes, lexicon corrected; N=no, POS-
set remains the same as analyzer?s output.
supervised training. In this section we examine
cases where (a) an expert lexicon is available, so
that lexicon learning is not required, and (b) sen-
tences are annotated with POS information, so that
supervised training is possible. The goal of these
experiments is to determine when alternative ap-
proaches involving additional human annotations
become worthwhile in this task.
(a) Expert lexicon: First, we build an expert
lexicon by collecting all tags per word in the de-
velopment set (i.e. ?oracle? POS-sets). Then, the
tagger is trained using EM by treating the develop-
ment set as raw text (i.e. ignoring the POS anno-
tations). This achieves an accuracy of 74.45% on
the test set. Note that this accuracy is significantly
higher than the ones in Table 3, which represent
unsupervised training on more raw text (the train-
ing set), but with non-expert lexicons derived from
the MSA analyzer and a machine learner. This re-
sult further demonstrates the importance of obtain-
ing an accurate lexicon in unsupervised training. If
one were to build this expert lexicon by hand, one
would need an annotator to label the POS-sets of
2450 distinct lexicon items.
(b) Supervised training: We build a super-
vised tagger by training on the POS annotations of
the development set, which achieves 82.93% accu-
racy. This improved accuracy comes at the cost of
annotating 2.2k sentences (16k tokens) with com-
plete POS information.
Finally, we present the same results with re-
duced data, taking first 50, 100, 200, etc. sen-
tences in the development set for lexicon or POS
annotation. The learning curve is shown in Table
7. One may be tempted to draw conclusions re-
garding supervised vs. unsupervised approaches
by directly comparing this table with the results
in Section 6.1; we avoid doing so since taggers in
Sections 6.1 and 6.2 are trained on different data
sets (training vs. development set) and the accu-
racy differences are compounded by issues such
406
Supervised Unsupervised, Expert
#Sentence Acc #Vocab Acc
50 47.82 123 47.13
100 55.32 188 54.65
200 61.17 299 57.37
400 69.17 497 64.36
800 76.92 953 70.36
1600 81.73 1754 72.99
2200 82.93 2450 74.45
Table 7: (1) Supervised training accuracies with
varying numbers of sentences. (2) Accuracies of
unsupervised training using a expert lexicon of
different vocabulary sizes.
as ngram coverage, data-set selection, and the way
annotations are done.
7 Related Work
There is an increasing amount of work in NLP
tools for Arabic. In supervised POS tagging, (Diab
et al, 2004) achieves high accuracy on MSA with
the direct application of SVM classifiers. (Habash
and Rambow, 2005) argue that the rich morphol-
ogy of Arabic necessitates the use of a morpho-
logical analyzer in combination with POS tag-
ging. This can be considered similar in spirit to
the learning of lexicons for unsupervised tagging.
The work done at a recent JHU Workshop
(Rambow and others, 2005) is very relevant in that
it investigates a method for improving LCA tag-
ging that is orthogonal to our approach. They do
not use the raw LCA text as we have done. Instead,
they train a MSA supervised tagger and adapt it to
LCA by a combination of methods, such using a
MSA-LCA translation lexicon and redistributing
the probabibility mass of MSA words to LCA.
8 Conclusion
In this study, we investigated several machine
learning algorithms on the task of lexicon learn-
ing and demonstrated its impact on dialectal Ara-
bic tagging. We achieve a POS tagging accuracy
of 63.54% using a transductively-learned lexicon
(TSVM), outperforming the baseline (57.39%).
This result brings us one step closer to the accu-
racies of unsupervised training with expert lexi-
con (74.45%) and supervised training (82.93%),
both of which require significant annotation effort.
Future work includes a more detailed analysis of
transductive learning in this domain and possible
solutions to alleviating error propagation.
Acknowledgments
We would like to thank Rebecca Hwa for discussions regard-
ing the JHU project. This work is funded in part by NSF
Grant IIS-0326276 and an NSF Graduate Fellowship for the
1st author. Any opinions, findings, and conclusions expressed
in this material are those of the authors and do not necessarily
reflect the views of these agencies.
References
M. Banko and R. Moore. 2004. Part-of-speech tagging in
context. In Proc. of COLING 2004.
E. Brill. 1995. Unsupervised learning of disambiguation
rules for part of speech tagging. In Proc. of the Third
Workshop on Very Large Corpora.
P. Derbeko, R. El-Yaniv, and R. Meir. 2004. Explicit learning
curves for transduction and application to clustering and
compression algorithms. Journal of Artificial Intelligence
Research, 22:117-142.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic tag-
ging of Arabic text: from raw text to base phrase chunks.
In Proceedings of HLT/NAACL.
K. Duh and K. Kirchhoff. 2005a. POS tagging of dialectal
arabic: a minimally-supervised approach. In ACL 2005,
Semitic Languages Workshop.
K. Duh and K. Kirchhoff. 2005b. Structured multi-label
transductive learning. In NIPS Workshop on Advances in
Structured Learning for Text/Speech Processing.
R. El-Yaniv and L. Gerzon. 2005. Effective transductive
learning via objective model selection. Pattern Recogni-
tion Letters, 26(13):2104-2115.
A. Elisseeff and J. Weston. 2002. Kernel methods for multi-
labeled classification. In NIPS.
N. Habash and O. Rambow. 2005. Arabic tokenization, mor-
phological analysis, and part-of-speech tagging in one fell
swoop. In ACL.
T. Joachims. 1999. Transductive inference for text classifi-
cation using support vector machines. In ICML.
T. Joachims. 2003. Transductive learning via spectral graph
partitioning. In ICML.
J. Kupiec. 1992. Robust part-of-speech tagging using a hid-
den Markov model. Computer Speech and Language, 6.
M. Maamouri, A. Bies, and T. Buckwalter. 2004. The Penn
Arabic Treebank: Building a large-scale annotated Arabic
corpus. In NEMLAR Conf. on Arabic Language Resources
and Tools.
D. McAllester. 1999. Some PAC-Bayesian theorems. Ma-
chine Learning, 37(3):255-36.
O. Rambow et al 2005. Parsing Arabic dialects. Technical
report, Final Report, 2005 JHU Summer Workshop.
V. Vapnik. 1998. Statistical Learning Theory. Wiley Inter-
science.
Q. Wang and D. Schuurmans. 2005. Improved estimation for
unsupervised part-of-speech tagging. In IEEE NLP-KE.
407
Proceedings of the Third Workshop on Statistical Machine Translation, pages 123?126,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The University of Washington Machine Translation System for
ACL WMT 2008
Amittai Axelrod, Mei Yang, Kevin Duh, Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA 98195
{amittai,yangmei,kevinduh,katrin} @ee.washington.edu
Abstract
This paper present the University of Washing-
ton?s submission to the 2008 ACL SMT shared ma-
chine translation task. Two systems, for English-to-
Spanish and German-to-Spanish translation are de-
scribed. Our main focus was on testing a novel
boosting framework for N-best list reranking and
on handling German morphology in the German-to-
Spanish system. While boosted N-best list reranking
did not yield any improvements for this task, simpli-
fying German morphology as part of the preprocess-
ing step did result in significant gains.
1 Introduction
The University of Washington submitted systems
to two data tracks in the WMT 2008 shared task
competition, English-to-Spanish and German-to-
Spanish. In both cases, we focused on the in-domain
test set only. Our main interest this year was on in-
vestigating an improved weight training scheme for
N-best list reranking that had previously shown im-
provements on a smaller machine translation task.
For German-to-Spanish translation we additionally
investigated simplifications of German morphology,
which is known to be fairly complex due to a large
number of compounds and inflections. In the fol-
lowing sections we first describe the data, baseline
system and postprocessing steps before describing
boosted N-best list reranking and morphology-based
preprocessing for German.
2 Data and Basic Preprocessing
We used the Europarl data as provided (version 3b,
1.25 million sentence pairs) for training the transla-
tion model for use in the shared task. The data was
lowercased and tokenized with the auxiliary scripts
provided, and filtered according to the ratio of the
sentence lengths in order to eliminate mismatched
sentence pairs. This resulted in about 965k paral-
lel sentences for English-Spanish and 950k sentence
pairs for German-Spanish. Additional preprocess-
ing was applied to the German corpus, as described
in Section 5. For language modeling, we addition-
ally used about 82M words of Spanish newswire text
from the Linguistic Data Consortium (LDC), dating
from 1995 to 1998.
3 System Overview
3.1 Translation model
The system developed for this year?s shared task
is a state-of-the-art, two-pass phrase-based statisti-
cal machine translation system based on a log-linear
translation model (Koehn et al 2003). The trans-
lation models and training method follow the stan-
dard Moses (Koehn et al 2007) setup distributed as
part of the shared task. We used the training method
suggested in the Moses documentation, with lexical-
ized reordering (the msd-bidirectional-fe
option) enabled. The system was tuned via Mini-
mum Error Rate Training (MERT) on the first 500
sentences of the devtest2006 dataset.
123
3.2 Decoding
Our system used the Moses decoder to generate
2000 output hypotheses per input sentence during
the first translation pass. For the second pass, the
N-best lists were rescored with the additional lan-
guage models described below. We re-optimized the
model combination weights with a parallelized im-
plementation of MERT over 16 model scores on the
test2007 dataset. Two of these model scores for
each hypothesis were from the two language models
used in our second-pass system, and the rest corre-
spond to the 14 Moses model weights (for reorder-
ing, language model, translation model, and word
penalty).
3.3 Language models
We built all of our language models using the
SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney discounting and interpolating all n-
gram estimates of order > 1. For first-pass de-
coding we used a 4-gram language model trained
on the Spanish side of the Europarl v3b data. The
optimal n-gram order was determined by testing
language models with varying orders (3 to 5) on
devtest2006; BLEU scores obtained using the
various language models are shown in Table 1. The
4-gram model performed best.
Table 1: LM ngram size vs. output BLEU on the dev sets.
order devtest2006 test2007
3-gram 30.54 30.69
4-gram 31.03 30.94
5-gram 30.85 30.84
Two additional language models were used for
second pass rescoring. First, we trained a large out-
of-domain language model on Spanish newswire
text obtained from the LDC, dating from 1995 to
1998.
We used a perplexity-filtering method to filter out
the least relevant half of the out-of-domain text, in
order to significantly reduce the training time of
the large language model and accelerate the rescor-
ing process. This was done by computing the per-
plexity of an in-domain language model on each
newswire sentence, and then discarding all sen-
tences with greater than average perplexity. This
reduced the size of the training set from 5.8M sen-
tences and 166M tokens to 2.8M sentences and 82M
tokens. We then further restricted the vocabulary to
the union of the vocabulary lists of the Spanish sides
of the de-es and en-es parallel training corpora. The
remaining text was used to train the language model.
The second language model used for rescoring
was a 5-gram model over part-of-speech (POS) tags.
This model was built using the Spanish side of the
English-Spanish parallel training corpus. The POS
tags were obtained from the corpus using Freeling
v2.0 (Atserias et al 2006).
We selected the language models for our transla-
tion system were selected based on performance on
the English-to-Spanish task, and reused them for the
German-to-Spanish task.
4 Boosted Reranking
We submitted an alternative system, based on a
different re-ranking method, called BoostedMERT
(Duh and Kirchhoff, 2008), for each task. Boosted-
MERT is a novel boosting algorithm that uses Mini-
mum Error Rate Training (MERT) as a weak learner
to build a re-ranker that is richer than the standard
log-linear models. This is motivated by the obser-
vation that log-linear models, as trained by MERT,
often do not attain the oracle BLEU scores of the N-
best lists in the development set. While this may be
due to a local optimum in MERT, we hypothesize
that log-linear models based on our K re-ranking
features are also not sufficiently expressive.
BoostedMERT is inspired by the idea of Boosting
(for classification), which has been shown to achieve
low training (and generalization) error due to classi-
fier combination. In BoostedMERT, we maintain a
weight for each N-best list in the development set.
In each iteration, MERT is performed to find the best
ranker on weighted data. Then, the weights are up-
dated based on whether the current ranker achieves
oracle BLEU. For N-best lists that achieve BLEU
scores far lower than the oracle, the weights are in-
creased so that they become the emphasis of next
iteration?s MERT. We currently use the factor e?r
to update the N-best list distribution, where r is the
ratio of the oracle hypothesis? BLEU to the BLEU
of the selected hypothesis. The final ranker is a
124
weighted combination of many such rankers.
More precisely, let wi be the weights trained by
MERT at iteration i. Given any wi, we can gener-
ate a ranking yi over an N-best list where yi is an
N-dimensional vector of predicted ranks. The final
ranking vector is a weighted sum: y =
?T
i=1 ?iyi,
where ?i are parameters estimated during the boost-
ing process. These parameters are optimized for
maximum BLEU score on the development set. The
only user-specified parameter is T , the number of
boosting iterations. Here, we choose T by divid-
ing the dev set in half: dev1 and dev2. First, we
train BoostedMERT on dev1 for 50 iterations, then
pick the T with the best BLEU score on dev2. Sec-
ond, we train BoostedMERT on dev2 and choose the
optimal T from dev1. Following the philosophy of
classifier combination, we sum the final rank vectors
y from each of the dev1- and dev2-trained Boosted-
MERT to obtain our final ranking result.
5 German ? Spanish Preprocessing
German is a morphologically complex language,
characterized by a high number of noun compounds
and rich inflectional paradigms. Simplification of
morphology can produce better word alignment, and
thus better phrasal translations, and can also signifi-
cantly reduce the out-of-vocabulary rate. We there-
fore applied two operations: (a) splitting of com-
pound words and (b) stemming.
After basic preprocessing, the German half of the
training corpus was first tagged by the German ver-
sion of TreeTagger (Schmid, 1994), to identify part-
of-speech tags. All nouns were then collected into
a noun list, which was used by a simple compound
splitter, as described in (Yang and Kirchhoff, 2006).
This splitter scans the compound word, hypothesiz-
ing segmentations, and selects the first segmentation
that produces two nouns that occur individually in
the corpus. After splitting the compound nouns in
the filtered corpus, we used the TreeTagger again,
only this time to lemmatize the (filtered) training
corpus.
The stemmed version of the German text was used
to train the translation system?s word alignments
(through the end of step 3 in the Moses training
script). After training the alignments, they were pro-
jected back onto the unstemmed corpus. The parallel
phrases were then extracted using the standard pro-
cedure. Stemming is only used during the training
stage, in order to simplify word alignment. During
the evaluation phase, only the compound-splitter is
applied to the German input.
6 Results
6.1 English ? Spanish
The unofficial results of our 2nd-pass system for the
2008 test set are shown in Table 2, for recased, unto-
kenized output. We note that the basic second-pass
model was better than the first-pass system on the
2008 task, but not on the 2007 task, whereas Boost-
edMERT provided a minor improvement in the 2007
task but not the 2008 task. This is contrary to previ-
ous results in the Arabic-English IWSLT 2007 task,
where boosted MERT gave an appreciable improve-
ment. This result is perhaps due to the difference in
magnitude between the IWSLT and WMT transla-
tion tasks.
Table 2: En?Es system on the test2007 and test2008
sets.
System test2007 test2008
First-Pass 30.95 31.83
Second-Pass 30.94 32.72
BoostedMERT 31.05 32.62
6.2 German ? Spanish
As previously described, we trained two German-
Spanish translation systems: one via the default
method provided in the Moses scripts, and an-
other using word stems to train the word align-
ments and then projecting these alignments onto
the unstemmed corpus and finishing the training
process in the standard manner. Table 3 demon-
strates that the word alignments generated with
word-stems markedly improved first-pass transla-
tion performance on the dev2006 dataset. How-
ever, during the evaluation period, the worse of the
two systems was accidentally used, resulting in a
larger number of out-of-vocabulary words in the
system output and hence a poorer score. Rerun-
ning our German-Spanish translation system cor-
rectly yielded significantly better system results,
also shown in Table 3.
125
Table 3: De?Es first-pass system on the development
and 2008 test set.
System dev2006 test2008
Baseline 23.9 21.2
Stemmed Alignments 26.3 24.4
6.3 Boosted MERT
BoostedMERT is still in an early stage of experi-
mentation, and we were interested to see whether it
improved over traditional MERT in re-ranking. As it
turns out, the BLEU scores on test2008 and test2007
data for the En-Es track are very similar for both re-
rankers. In our post-evaluation analysis, we attempt
to understand the reasons for similar BLEU scores,
since the weights wi for both re-rankers are quali-
tatively different. We found that out of 2000 En-Es
N-best lists, BoostedMERT and MERT differed on
1478 lists in terms of the final hypothesis that was
chosen. However, although the rankers are choosing
different hypotheses, the chosen strings appear very
similar. The PER of BoostedMERT vs. MERT re-
sults is only 0.077, and manual observation indicates
that the differences between the two are often single
phrase differences in a sentence.
We also computed the sentence-level BLEU for
each ranker with respect to the true reference. This
is meant to check whether BoostedMERT improved
over MERT in some sentences but not others: if the
improvements and degradations occur in the same
proportions, a similar corpus-level BLEU may be
observed. However, this is not the case. For a major-
ity of the 2000 sentences, the sentence-level BLEU
for both systems are the same. Only 10% of sen-
tences have absolute BLEU difference greater than
0.1, and the proportion of improvement/degradation
is similar (each 5%). For BLEU differences greater
than 0.2, the percentage drops to 4%.
Thus we conclude that although BoostedMERT
and MERT choose different hypotheses quite of-
ten, the string differences between their hypotheses
are negligible, leading to similar final BLEU scores.
BoostedMERT has found yet another local optimum
during training, but has not improved upon MERT
in this dataset. We hypothesize that dividing up the
original development set into halves may have hurt
BoostedMERT.
7 Conclusion
We have presented the University of Washing-
ton systems for English-to-Spanish and German-to-
Spanish for the 2008 WMT shared translation task.
A novel method for reranking N-best lists based on
boosted MERT training was tested, as was morpho-
logical simplification in the preprocessing compo-
nent for the German-to-Spanish system. Our con-
clusions are that boosted MERT, though successful
on other translation tasks, did not yield any improve-
ment here. Morphological simplification, however,
did result in significant improvements in translation
quality.
Acknowledgements
This work was funded by NSF grants IIS-0308297
and IIS-0326276.
References
Atserias, J. et al 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP library.
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Genoa, Italy.
Duh, K., and Kirchhoff, K. 2008. Beyond Log-Linear
Models: Boosted Minimum Error Rate Training for
MT Re-ranking. To appear, Proceedings of the Associ-
ation for Computational Linguistics (ACL). Columbus,
Ohio.
Koehn, P. and Och, F.J. and Marcu, D. 2003. Statistical
phrase-based translation. Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, (HLT/NAACL). Edmonton, Canada.
Koehn, P. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation Proceedings of MT Summit.
Koehn, P. et al 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session. Prague, Czech Republic.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. International Conference on New
Methods in Language Processing, Manchester, UK.
Stolcke, A. 2002. SRILM - An extensible language mod-
eling toolkit. Proceedings of ICSLP.
Yang, M. and K. Kirchhoff. 2006. Phrase-based backoff
models for machine translation of highly inflected lan-
guages. Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL 2006). Trento, Italy.
126
Proceedings of the Third Workshop on Statistical Machine Translation, pages 191?194,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Ranking vs. Regression in Machine Translation Evaluation
Kevin Duh?
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195
kevinduh@u.washington.edu
Abstract
Automatic evaluation of machine translation
(MT) systems is an important research topic
for the advancement of MT technology. Most
automatic evaluation methods proposed to
date are score-based: they compute scores that
represent translation quality, and MT systems
are compared on the basis of these scores.
We advocate an alternative perspective of au-
tomatic MT evaluation based on ranking. In-
stead of producing scores, we directly produce
a ranking over the set of MT systems to be
compared. This perspective is often simpler
when the evaluation goal is system compari-
son. We argue that it is easier to elicit human
judgments of ranking and develop a machine
learning approach to train on rank data. We
compare this ranking method to a score-based
regression method on WMT07 data. Results
indicate that ranking achieves higher correla-
tion to human judgments, especially in cases
where ranking-specific features are used.
1 Motivation
Automatic evaluation of machine translation (MT)
systems is an important research topic for the ad-
vancement of MT technology, since automatic eval-
uation methods can be used to quickly determine the
(approximate) quality of MT system outputs. This is
useful for tuning system parameters and for compar-
ing different techniques in cases when human judg-
ments for each MT output are expensivie to obtain.
Many automatic evaluation methods have been
proposed to date. Successful methods such as BLEU
?Work supported by an NSF Graduate Research Fellowship.
(Papineni et al, 2002) work by comparing MT out-
put with one or more human reference translations
and generating a similarity score. Methods differ by
the definition of similarity. For instance, BLEU and
ROUGE (Lin and Och, 2004) are based on n-gram
precisions, METEOR (Banerjee and Lavie, 2005)
and STM (Liu and Gildea, 2005) use word-class
or structural information, Kauchak (2006) leverages
on paraphrases, and TER (Snover et al, 2006) uses
edit-distances. Currently, BLEU is the most popu-
lar metric; it has been shown that it correlates well
with human judgments on the corpus level. How-
ever, finding a metric that correlates well with hu-
man judgments on the sentence-level is still an open
challenge (Blatz and others, 2003).
Machine learning approaches have been proposed
to address the problem of sentence-level evalua-
tion. (Corston-Oliver et al, 2001) and (Kulesza
and Shieber, 2004) train classifiers to discrim-
inate between human-like translations and auto-
matic translations, using features from the afore-
mentioned metrics (e.g. n-gram precisions). In con-
trast, (Albrecht and Hwa, 2007) argues for a re-
gression approach that directly predicts human ad-
equecy/fluency scores.
All the above methods are score-based in the
sense that they generate a score for each MT system
output. When the evaluation goal is to compare mul-
tiple MT systems, scores are first generated inde-
pendently for each system, then systems are ranked
by their respective scores. We think that this two-
step process may be unnecessarily complex. Why
solve a more difficult problem of predicting the qual-
ity of MT system outputs, when the goal is simply
191
to compare systems? In this regard, we propose a
ranking-based approach that directly ranks a set of
MT systems without going through the intermediary
of system-specific scores. Our approach requires (a)
training data in terms of human ranking judgments
of MT outputs, and (b) a machine learning algorithm
for learning and predicting rankings.1
The advantages of a ranking approach are:
? It is often easier for human judges to rank MT
outputs by preference than to assign absolute
scores (Vilar et al, 2007). This is because it is
difficult to quantify the quality of a translation
accurately, but relative easy to tell which one
of several translations is better. Thus human-
annotated data based on ranking may be less
costly to acquire.
? The inter- and intra-annotator agreement for
ranking is much more reasonable than that of
scoring. For instance, Callison-Burch (2007)
found the inter-annotator agreement (Kappa)
for scoring fluency/adequency to be around
.22-.25, whereas the Kappa for ranking is
around .37-.56. Thus human-annotated data
based on ranking may be more reliable to use.
? As mentioned earlier, when the final goal of
the evaluation is comparing systems, ranking
more directly solves the problem. A scoring
approach essentially addresses a more difficult
problem of estimating MT output quality.
Nevertheless, we note that score-based ap-
proaches remain important in cases when the ab-
solute difference between MT quality is desired.
For instance, one might wonder by how much does
the top-ranked MT system outperform the second-
ranked system, in which case a ranking-based ap-
proach provide no guidance.
In the following, Section 2 formulates the
sentence-level MT evaluation problem as a ranking
problem; Section 3 explains a machine learning ap-
proach for training and predicting rankings; this is
our submission to the WMT2008 Shared Evaluation
1Our ranking approach is similar to Ye et. al. (2007), who
was the first to advocate MT evaluation as a ranking problem.
Here we focus on comparing ranking vs. scoring approaches,
which was not done in previous work.
task. Ranking vs. scoring approaches are compared
in Section 4.
2 Formulation of the Ranking Problem
We formulate the sentence-level MT evaluation
problem as follows: Suppose there are T source sen-
tences to be translated. Let rt, t = 1..T be the set of
references2 . Corresponding to each source sentence,
there are N MT system outputs o(n)t , n = 1..N and
Mt (Mt ? N ) human evaluations. The evaluations
are represented as Mt-dimensional label vectors yt.
In a scoring approach, the elements of yt may cor-
respond to, e.g. a fluency score on a scale of 1 to 5.
In a ranking approach, they may correspond to rel-
ative scores that are used to represent ordering (e.g.
yt = [6; 1; 3] means that there are three outputs, and
the first is ranked best, followed by third, then sec-
ond.)
In order to do machine learning, we extract fea-
ture vectors x(n)t from each pair of rt and o
(n)
t .
3
The set {(x(n)t , yt)}t=1..T forms the training set.
In a scoring approach, we train a function f with
f(x(n)t ) ? y(n). In a ranking approach, we train
f such that higher-ranked outputs have higher func-
tion values. In the example above, we would want:
f(x(n=1)t ) > f(x
(n=3)
t ) > f(x
(n=2)
t ). Once f is
trained, it can be applied to rank any new data: this is
done by extracting features from references/outputs
and sorting by function values.
3 Implementation
3.1 Sentence-level scoring and ranking
We now describe the particular scoring and rank-
ing implementations we examined and submitted to
the WMT2008 Shared Evaluation task. In the scor-
ing approach, f is trained using RegressionSVM
(Drucker and others, 1996); in the ranking ap-
proach, we examined RankSVM (Joachims, 2002)
and RankBoost (Freund et al, 2003). We used only
linear kernels for RegressionSVM and RankSVM,
while allowed RankBoost to produce non-linear f
based on a feature thresholds.
2Here we assume single reference for ease of notation; this
can be easily extended for multiple reference
3Only Mt (not N ) features vectors are extracted in practice.
192
ID Description
1-4 log of ngram precision, n=1..4
5 ratio of hypothesis and reference length
6-9 ngram precision, n=1..4
10-11 hypothesis and reference length
12 BLEU
13 Smooth BLEU
14-20 Intra-set features for ID 5-9, 12,13
Table 1: Feature set: Features 1-5 can be combined (with
uniform weights) to form the log(BLEU) score. Features
6-11 are redundant statistics, but scaled differently. Fea-
ture 12 is sentence-level BLEU; Feature 13 is a modified
version with add-1 count to each ngram precision (this
avoids prevalent zeros). Features 14-20 are only available
in the ranking approach; they are derived by comparing
different outputs within the same set to be ranked.
The complete feature set is shown in Table 1. We
restricted our feature set to traditional BLEU statis-
tics since our experimental goal is to directly com-
pare regression, ranking, and BLEU. Features 14-
20 are the only novel features proposed here. We
wanted to examine features that are enabled by a
ranking approach, but not possible for a scoring
approach. We thus introduce ?intra-set features?,
which are statistics computed by observing the en-
tire set of existing features {x(n)t }n=1..Mt .
For instance: We define Feature 14 by looking at
the relative 1-gram precision (Feature 1) in the set of
Mt outputs. Feature 14 is set to value 1 for the out-
put which has the best 1-gram precision, and value 0
otherwise. Similarly, Feature 15 is a binary variable
that is 1 for the output with the best 2-gram preci-
sion, and 0 for all others. The advantage of intra-set
features is calibration. e.g. If the outputs for rt=1
all have relatively high BLEU compared to those
of rt=2, the basic BLEU features will vary widely
across the two sets, making it more difficult to fit a
ranking function. On the other hand, intra-set fea-
tures are of the same scale ([0, 1] in this case) across
the two sets and therefore induce better margins.
While we have only explored one particular in-
stantiation of intra-set features, many other defini-
tions are imaginable. Novel intra-set features is a
promising research direction; experiments indicate
that they are most important in helping ranking out-
perform regression.
3.2 Corpus-level ranking
Sentence-level evaluation generates a ranking for
each source sentence. How does one produce
an overall corpus-level ranking based on a set of
sentence-level rankings? This is known as the
?consensus ranking? or ?rank aggregation? prob-
lem, which can be NP-hard under certain formula-
tions (Meila? et al, 2007). We use the FV heuristic
(Fligner and Verducci, 1988), which estimates the
empirical probability Pij that system i ranks above
system j from sentence-level rankings (i.e. Pij =
number of sentences where i ranks better than j, di-
vided by total number of sentences). The corpus-
level ranking of system i is then defined as
?
j? Pij? .
4 Experiments
For experiments, we split the provided development
data into train, dev, and test sets (see Table 2). The
data split is randomized at the level of different eval-
uation tracks (e.g. en-es.test, de-en.test are differ-
ent tracks) in order to ensure that dev/test are suffi-
ciently novel with respect to the training data. This
is important since machine learning approaches have
the risk of overfitting and spreading data from the
same track to both train and test could lead to over-
optimistic results.
Train Dev Test
# tracks 8 3 3
# sets 1504 (63%) 514 (21%) 390 (16%)
# sent 6528 (58%) 2636 (23%) 2079 (19%)
Table 2: Data characteristics: the training data contains
8 tracks, which contained 6528 sentence evaluations or
1504 sets of human rankings (T = 1504).
In the first experiment, we compared Regression
SVM and Rank SVM (both used Features 1-12) by
training on varying amounts of training data. The
sentence-level rankings produced by each are com-
pared to human judgments using the Spearman rank
correlation coefficient (see Figure 1).
In the second experiment, we compared all rank-
ing and scoring methods discussed thus far. The full
training set is used; the dev set is used to tune the
cost parameter for the SVMs and number of itera-
tions for RankBoost, which is then applied without
modification to the test set. Table 3 shows the aver-
193
0 10 20 30 40 50 60 70 80 90 100
0.22
0.24
0.26
0.28
0.3
0.32
0.34
0.36
Percentage of training data
Sp
ea
rm
an
 c
oe
ff
Data ablation results on Dev Set
 
 
RankSVM
RegressionSVM
Figure 1: Ranking slightly outperforms Regression for
various amounts of training data. Regression results ap-
pear to be less stable, with a rise/fall in average Spear-
man coefficent around 20%, possibly because linear re-
gression functions become harder to fit with more data.
age Spearman coefficient for different methods and
different feature sets. There are several interesting
observations:
1. BLEU performs poorly, but SmoothedBLEU is
almost as good as the machine learning meth-
ods that use same set of basic BLEU features.
2. Rank SVM slightly outperforms RankBoost.
3. Regression SVM and Rank SVM gave simi-
lar results under the same feature set. How-
ever, Rank SVM gave significant improve-
ments when intra-set features are incorporated.
The last observation is particularly important: it
shows that the training criteria differences between
the ranking and regression is actually not critical.
Ranking can outperform regression, but only when
ranking-specific features are considered. Without
intra-set features, ranking methods may be suffering
the same calibration problems as regression.
References
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level MT eval-
uation. In ACL.
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In ACL 2005 Wksp on
Intrinsic/Extrinsic Evaluation for MT/Summarization.
J. Blatz et al 2003. Confidence estimation for machine
translation. Technical report, Johns Hopkins Univer-
sity, Natural Language Engineering Workshop.
C. Callison-Burch et al 2007. (meta-) evaluation of ma-
chine translation. In ACL2007 SMT Workshop.
Feature Dev Test
BLEU 1-5 .14 .05
Smoothed BLEU 1-5 .19 .24
Regression SVM 1-12 .33 .24
RankSVM 1-12 .34 .25
RankBoost 1-12 .29 .22
RankSVM 1-20 .52 .42
RankBoost 1-20 .51 .38
Table 3: Average Spearman coefficients on Dev/Test. The
intra-set features gave the most significant gains (e.g. .42
on test of RankSVM). Refer to Table 1 to see what fea-
tures are used in each row. The SVM/RankBoost results
for features 1-12 and 1-5 are similar; only those of 1-12
are reported.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001. A
machine learning approach to the automatic evaluation
of machine translation. In ACL.
H. Drucker et al 1996. Support vector regression ma-
chines. In NIPS.
M. Fligner and J. Verducci. 1988. Multistage ranking
models. Journal of American Statistical Assoc., 88.
Y. Freund, R. Iyer, R. Schapire, and Y. Singer. 2003. An
efficient boosting method for combining preferences.
JMLR, 4.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In KDD.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In NAACL-HLT.
A. Kulesza and S. Shieber. 2004. A learning approach to
improving sentence-level mt evaluation. In TMI.
C.-Y. Lin and F. Och. 2004. Automatic evaluation of ma-
chine translation quality using longest common subse-
quence and skip-bigram statistics. In ACL.
D. Liu and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In ACL 2005 Wksp on
Intrinsic/Extrinsic Evaluation for MT/Summarization.
M. Meila?, K. Phadnis, A. Patterson, and J. Bilmes. 2007.
Consensus ranking under the exponential model. In
Conf. on Uncertainty in Artificial Intelligence (UAI).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Conf. of Assoc. for
Machine Translation in the Americas (AMTA-2006).
D. Vilar, G. Leusch, H. Ney, and R. Banchs. 2007. Hu-
man evaluation of machine translation through binary
system comparisons. In ACL2007 SMT Workshop.
Y. Ye, M. Zhou, and C.-Y. Lin. 2007. Sentence level
machine translation evaluation as a ranking problem.
In ACL2007 Wksp on Statistical Machine Translation.
194
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 439?446,
Beijing, August 2010
Hierarchical Phrase-based Machine Translation with Word-based
Reordering Model
Katsuhiko Hayashi*, Hajime Tsukada**
Katsuhito Sudoh**, Kevin Duh**, Seiichi Yamamoto*
*Doshisha University
katsuhiko-h@is.naist.jp, seyamamo@mail.doshisha.ac.jp
**NTT Communication Science Laboratories
tsukada, sudoh, kevinduh@cslab.kecl.ntt.co.jp
Abstract
Hierarchical phrase-based machine trans-
lation can capture global reordering with
synchronous context-free grammar, but
has little ability to evaluate the correctness
of word orderings during decoding. We
propose a method to integrate word-based
reordering model into hierarchical phrase-
based machine translation to overcome
this weakness. Our approach extends the
synchronous context-free grammar rules
of hierarchical phrase-based model to in-
clude reordered source strings, allowing
efficient calculation of reordering model
scores during decoding. Our experimen-
tal results on Japanese-to-English basic
travel expression corpus showed that the
BLEU scores obtained by our proposed
system were better than those obtained by
a standard hierarchical phrase-based ma-
chine translation system.
1 Introduction
Hierarchical phrase-based machine translation
(Chiang, 2007; Watanabe et al, 2006) is one of
the promising statistical machine translation ap-
proaches (Brown et al, 1993). Its model is for-
mulated by a synchronous context-free grammar
(SCFG) which captures the syntactic information
between source and target languages. Although
the model captures global reordering by SCFG,
it does not explicitly introduce reordering model
to constrain word order. In contrast, lexicalized
reordering models (Tillman, 2004; Koehn et al,
2005; Nagata et al, 2006) are extensively used
for phrase-based translation. These lexicalized re-
ordering models cannot be directly applied to hi-
erarchical phrased-based translation since the hi-
erarchical phrase representation uses nonterminal
symbols.
To handle global reordering in phrase-based
translation, various preprocessing approaches
have been proposed, where the source sentence
is reordered to target language order beforehand
(Xia and McCord, 2004; Collins et al, 2005; Li et
al., 2007; Tromble and Eisner, 2009). However,
preprocessing approaches cannot utilize other in-
formation in the translation model and target lan-
guage model, which has been proven helpful in
decoding.
This paper proposes a method that incorpo-
rates word-based reordering model into hierarchi-
cal phrase-based translation to constrain word or-
der. In this paper, we adopt the reordering model
originally proposed by Tromble and Eisner (2009)
for the preprocessing approach in phrase-based
translation. To integrate the word-based reorder-
ing model, we added a reordered source string
into the right-hand-side of SCFG?s rules. By this
extension, our system can generate the reordered
source sentence as well as target sentence and is
able to efficiently calculate the score of the re-
ordering model. Our method utilizes the transla-
tion model and target language model as well as
the reordering model during decoding. This is an
advantage of our method over the preprocessing
approach.
The remainder of this paper is organized as
follows. Section 2 describes the concept of our
approach. Section 3 briefly reviews our pro-
posed method on hierarchical phrase-based ma-
439
Standard SCFG X ?< X1 wa jinsei no X2 da , X1 is X2 of life>
SCFG (move-to-front) X ?< X1 wa jinsei no X2 da , wa X1 da X2 no jinsei , X1 is X2 of life>
SCFG (attach) X ?< X1 wa jinsei no X2 da , X1 wa da X2 no jinsei , X1 is X2 of life>
Table 1: A Japanese-to-English example of various SCFG?s rule representations. Japanese words are
romanized. Our proposed representation of rules has reordered source string to generate reordered
source sentence S? as well as target sentence T . The ?move-to-front? means Tromble and Eisner (2009)
?s algorithm and the ?attach? means Al-Onaizan and Papineni (2006) ?s algorithm.
chine translation model. We experimentally com-
pare our proposed system to a standard hierarchi-
cal phrase-based system on Japanese-to-English
translation task in Section 4. Then we discuss on
related work in Section 5 and conclude this paper
in Section 6.
2 The Concept of Our Approach
The preprocessing approach (Xia and McCord,
2004; Collins et al, 2005; Li et al, 2007; Tromble
and Eisner, 2009) splits translation procedure into
two stages:
S ? S? ? T (1)
where S is a source sentence, S? is a reordered
source sentence with respect to the word order of
target sentence T . Preprocessing approach has the
very deterministic and hard decision in reorder-
ing. To overcome the problem, Li et al (2007)
proposed k-best appoach. However, even with a
k-best approach, it is difficult to generate good hy-
potheses S? by using only a reordering model.
In this paper, we directly integrated the reorder-
ing model into the decoder in order to use the
reordering model together with other information
in the hierarchical phrase-based translation model
and target language model. Our approach is ex-
pressed as the following equation.
S ? (S? , T ). (2)
Our proposed method generates the reordered
source sentence S? by SCFG and evaluates the
correctness of the reorderings using a word-based
reordering model of S? which will be introduced
in section 3.4.
Figure 1: A derivation tree for Japanse-to-English
translation.
3 Hierarchical Phrase-based Model
Extension
3.1 Hierarchical Phrase-based Model
Hierarchical phrase-based model (Chiang, 2007)
induces rules of the form
X ?< ?, ?,?, w > (3)
where X is a non-terminal symbol, ? is a se-
quence string of non-terminals and source termi-
nals, ? is a sequence string of non-terminals and
target terminals. ? is a one-to-one correspon-
dence for the non-terminals appeared in ? and ?.
Given a source sentence S, the translation task
under this model can be expressed as
T? = T
(
argmax
D:S(D)=S
w(D)
)
(4)
where D is a derivation and w(D) is a score of
the derivation. Decoder seeks a target sentence
440
Figure 2: Reordered source sentence generated by
our proposed system.
T (D) which has the highest score w(D). S(D)
is a source sentence under a derivation D. Fig-
ure 1 shows the example of Japanese-to-English
translation by hierarchical phrase-based machine
translation model.
3.2 Rule Extension
To generate reordered source sentence S? as well
as target sentence T , we extend hierarchical
phrase rule expressed in Equation 3 to
X ?< ?, ?? , ?,?, w > (5)
where ?? is a sequence string of non-terminals and
source terminals, which is reordered ? with re-
spect to the word order of target string ?. The
reason why we add ?? to rules is to efficiently cal-
culate the reordering model scores. If each rule
does not have ?? , the decoder need to keep word
alignments because we cannot know word order
of S? without them. The calculation of reorder-
ing model scores using word alignments is very
wasteful when decoding.
The translation task under our model extends
Equation 4 to the following equation:
T? = (S?? , T? ) = (S? , T )
(
argmax
D:S(D)=S
w(D)
)
. (6)
Our system generates the reordered source sen-
tence S? as well as target sentence T . Figure 2
shows the generated reordered source sentence S?
Uni-gram Features
sr, s-posr
sr
s-posr
sl, s-posl
sl
s-posl
Bi-gram Features
sr, s-posr, sl, s-posl
s-posr, sl, s-posl
sr, sl, s-posl
sr, s-posr, s-posl
sr, s-posr, sl
sr, sl
s-posr, s-posl
Table 2: Features used by Word-based Reordering
Model. pos means part-of-speech tag.
when translating the example of Figure 1. Note
that the structure of S? is the same as that of target
sentence T . The decoder generates both Figure 2
and the right hand side of Figure 1, allowing us to
score both global and local word reorderings.
To add ?? to rules, we permuted ? into ?? after
rule extraction based on Grow-diag-final (Koehn
et al, 2005) alignment by GIZA++ (Och and Ney,
2003). To do this permutation on rules, we ap-
plied two methods. One is the same algorithm
as Tromble and Eisner (2009), which reorders
aligned source terminals and nonterminals in the
same order as that of target side and moves un-
aligned source terminals to the front of aligned
terminals or nonterminals (move-to-front). The
other is the same algorithm as AI-Onaizan and
Papineni (2006), which differs from Tromble and
Eisner?s approach in attaching unaligned source
terminals to the closest prealigned source termi-
nals or nonterminals (attach). This extension of
adding ?? does not increase the number of rules.
Table 1 shows a Japanese-to-English example
of the representation of rules for our proposed sys-
tem. Japanese words are romanized. Suppose that
source-side string is (X1 wa jinsei no X2 da) and
target-side string is (X1 is X2 of life) and their
word alignments are a=((jinsei , life) , (no , of)
, (da , is)). Source-side aligned words and non-
terminal symbols are sorted into the same order of
target string. Source-side unaligned word (wa) is
moved to the front or right of the prealigned sym-
bol (X1).
441
Surrounding Word Pos Features
s-posr, s-posr + 1, s-posl ? 1, s-posl
s-posr ? 1, s-posr, s-posl ? 1, s-posl
s-posr, s-posr + 1, s-posl, s-posl + 1
s-posr ? 1, s-posr, s-posl, s-posl + 1
Table 3: The Example of Context Features
3.3 Word-based Reordering Model
We utilize the following score(S?) as a feature for
the word-based reordering model. This is incor-
polated into the log-linear model (Och and Ney,
2002) of statistical machine translation.
score(S?) =
?
i,j:1?i<j?n
B[s?i, s
?
j ] (7)
B[s?l, s
?
r] = ? ? ?(s
?
l, s
?
r) (8)
where n is the length of reordered source sen-
tence S? (= (s?1 . . . s
?
n)), ? is a weight vector and
? is a vector of features. This reordering model,
which is originally proposed by Tromble and Eis-
ner (2009), can assign a score to any possible per-
mutation of source sentences. Intuitively B[s?l, s
?
r]
represents the score of ordering s?l before s
?
r; the
higher the value, the more we prefer word s?l oc-
curs before s?r. Whether S
?
l should occur before S
?
r
depends on how often this reordering occurs when
we reorder the source to target sentence order.
To train B, we used binary feature functions
? as used in (Tromble and Eisner, 2009), which
were introduced for dependency parsing by Mc-
Donald et al (2005). Table 2 shows the kind
of features we used in our experiments. We did
not use context features like surrounding word pos
features in Table 3 because they were not useful in
our preliminary experiments and propose an effi-
cient implementation described in the next section
in order to calculate this reordering model when
decoding. To train the parameter ?, we used the
perceptron algorithm following Tromble and Eis-
ner (2009).
3.4 Integration to Cube Pruning
CKY parsing and cube-pruning are used for de-
coding of hierarchical phrase-based model (Chi-
ang, 2007). Figure 3 displays that hierarchical
phrase-based decoder seeks new span [1,7] items
Figure 3: Creating new items from subitems and
rules, that have a span [1,7] in source sentence.
with rules, utilizing subspan [1,3] items and sub-
span [4,7] items. In this example, we use 2-gram
language model and +LM decoding. uni(?) means
1-gram language model cost for heuristics and in-
teraction usually means language model cost that
cannot be calculated offline. Here, we introduce
our two implementations to calculate word-based
reordering model scores in this decoding algo-
rithm.
First, we explain a naive implementation shown
in the left side of Figure 4. This algorithm per-
forms the same calculation of reordering model as
that of language model. Each item keeps a part of
reordered source sentence. The reordering score
of new item can be calculated as interaction cost
when combining subitems with the rule.
The right side of Figure 4 shows our pro-
posed implementation. This implementation can
be adopted to decoding only when we do not use
context features like surrounding word pos fea-
tures in Table 3 (and consider a distance between
words in features). If a span is given, the reorder-
ing scores of new item can be calculated for each
rule, being independent from the word order of
reordered source segment of a subitem. So, the
reordering model scores can be calculated for all
rules with spans by using a part of the input source
sentence before sorting them for cube pruning.
We expect this sorting of rules with reordering
442
Figure 4: The ?naive? and ?proposed? implementation to calculate the reordering cost of new items.
model scores will have good influence on cube
pruning. The right hand side of Figure 4 shows
the diffrence between naive and proposed imple-
mentation (S? is not shown to allow for a clear pre-
sentation). Note the difference is in where/when
the reordering scores are inserted: together with
the N -gram scores in the case of naive implemen-
tation; incorpolated into sorted rules for the pro-
posed implementation.
4 Experiment
4.1 Purpose
To reveal the effectiveness of integrating the re-
ordering model into decoder, we compared the
following setups:
? baseline: a standard hierarchical phrase-
based machine translation (Hiero) system.
? preprocessing: applied Tromble and Eisner?s
approach, then translate by Hiero system.
? Hiero system + reordering model: integrated
reordering model into Hiero system.
We used the Joshua Decoder (Li and Khudanpur,
2008) as the baseline Hiero system. This decoder
uses a log-linear model with seven features, which
consist of N -gram language model PLM (T ), lex-
ical translation model Pw(?|?), Pw(?|?), rule
translation model P (?|?), P (?|?), word penalty
and arity penalty.
The ?Hiero + Reordering model? system has
word-based reordering model as an additional fea-
ture to baseline features. For this approach, we
use two systems. One has ?move-to-front? sys-
tem and the other is ?attach? system explained in
Section 3.2. We implemented our proposed algo-
rithm in Section 3.4 to both ?Hiero + Reordering
model? systems. As for beam width, we use the
same setups for each system.
4.2 Data Set
Data Sent. Word. Avg. leng
Training ja 200.8K 2.4M 12.0
en 200.8K 2.3M 11.5
Development ja 1.0K 10.3K 10.3
en 1.0K 9.8K 9.8
Test ja 1.0K 14.2K 14.2
en 1.0K 13.5K 13.5
Table 4: The Data statistics
For experiments we used a Japanese-English
basic travel expression corpus (BTEC). Japanese
word order is linguistically very different from
English and we think Japanese-English pair is
a very good test bed for evaluating reordering
model.
443
XXXXXXXXXXXSystem
Metrics BLEU PER
Baseline (Hiero) 28.09 39.68
Preprocessing 17.32 45.27
Hiero + move-to-front 28.85 39.89
Hiero + attach 29.25 39.43
Table 5: BLEU and PER scores on the test set.
Our training corpus contains about 200.8k sen-
tences. Using the training corpus, we extracted
hierarchical phrase rules and trained 4-gram lan-
guage model and word-based reordering model.
Parameters were tuned over 1.0k sentences (devel-
opment data) with single reference by minimum
error rate training (MERT) (Och, 2003). Test data
consisted of 1.0k sentences with single reference.
Table 4 shows the condition of corpus in detail.
4.3 Results
Table 5 shows the BLEU (Papineni et al, 2001)
and PER (Niesen et al, 2000) scores obtained by
each system. The results clearly indicated that
our proposed system with word-based reorder-
ing model (move-to-front or attach) outperformed
baseline system on BLEU scores. In contrast,
there is no significant improvement from baseline
on PER. This suggests that the improvement of
BLEU mainly comes from reordering. In our ex-
periment, preprocessing approach resulted in very
poor scores.
4.4 Discussion
Table 6 displays examples showing the cause of
the improvements of our system with reordering
model (attach) comparing to baseline system. We
can see that the outputs of our system are more
fluent than those of baseline system because of re-
ordering model.
As a further analysis, we calculated the BLEU
scores of Japanese S? predicted from reorder-
ing model against true Japanese S? made from
GIZA++ alignments, were only 26.2 points on de-
velopment data. We think the poorness mainly
comes from unaligned words since they are un-
tractable for the word-based reordering model.
Actually, Japanese sentences in our training data
include 34.7% unaligned words. In spite of the
poorness, our proposed method effectively utilize
this reordering model in contrast to preprocessing
approach.
5 Related Work
Our approach is similar to preprocessing approach
(Xia and McCord, 2004; Collins et al, 2005; Li
et al, 2007; Tromble and Eisner, 2009) in that it
reorders source sentence in target order. The dif-
ference is this sentence reordering is done in de-
coding rather than in preprocessing.
A lot of studies on lexicalized reordering (Till-
man, 2004; Koehn et al, 2005; Nagata et al,
2006) focus on the phrase-based model. These
works cannnot be directly applied to hierarchi-
cal phrase-based model because of the difference
between normal phrases and hierarchical phrases
that includes nonterminal symbols.
Shen et al (2008,2009) proposed a way to inte-
grate dependency structure into target and source
side string on hierarchical phrase rules. This ap-
proach is similar to our approach in extending the
formalism of rules on hierarchical phrase-based
model in order to consider the constraint of word
order. But, our approach differs from (Shen et al,
2008; Shen et al, 2009) in that syntax annotation
is not necessary.
6 Conclusion and Future Work
We proposed a method to integrate word-based
reordering model into hierarchical phrase-based
machine translation system. We add ?? into the
hiero rules, but this does not increase the num-
ber of rules. So, this extension itself does not af-
fect the search space of decoding. In this paper
we used Tromble and Eisner?s reordering model
for our method, but various reordering model can
be incorporated to our method, for example S?
N -gram language model. Our experimental re-
sults on Japanese-to-English task showed that our
system outperformed baseline system and prepro-
cessing approach.
In this paper we utilize ?? only for reorder-
ing model. However, it is possible to use ?? for
other modeling, for example we can use it for
rule translation probabilities P (?? |?), P (?|??) for
additional feature functions. Of course, we can
444
S america de seihin no hanbai wo hajimeru keikaku ga ari masu ka . kono tegami wa koukuubin de nihon made ikura kakari masu ka .
TB sales of product in america are you planning to start ? this letter by airmail to japan . how much is it ?
TP are you planning to start products in the u.s. ? how much does it cost to this letter by airmail to japan ?
R do you plan to begin selling your products in the u.s. ? how much will it cost to send this letter by air mail to japan ?
Table 6: Examples of outputs for input sentence S from baseline system TB and our proposed sys-
tem (attach) TP . R is a reference. The underlined portions have equivalent meanings and show the
reordering differences.
also utilize reordered target sentence T ? for vari-
ous modeling as well. Addtionally we plan to use
S? for MERT because we hypothesize the fluent
S? leads to fluent T .
References
AI-Onaizan, Y. and K. Papineni. 2006. Distortion
models for statistical machine translation. In Proc.
the 44th ACL, pages 529?536.
Brown, P. F., S. A. D. Pietra, V. D. J. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguitics, 19:263?312.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proc. NAACL, pages 216?226.
Chiang, D. 2007. Hierachical phrase-based transla-
tion. Computational Linguitics, 33:201?228.
Collins, M., P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proc. the 43th ACL, pages 531?540.
Collins, M. 2002. Discriminative training methods for
hidden markov models. In Proc. of EMNLP.
Freund, Y. and R. E. Schapire. 1996. Experiments
with a new boosting algorithm. In Proc. of the 13th
ICML, pages 148?156.
Koehn, P., A. Axelrod, A-B. Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Ed-
inburgh system description for 2005 iwslt speech
translation evaluation. In Proc. the 2nd IWSLT.
Li, Z. and S. Khudanpur. 2008. A scalable decoder
for parsing-based machine translation with equiv-
alent language model state maintenance. In Proc.
ACL SSST.
Li, C-H., D. Zhang, M. Li, M. Zhou, K. Li, and
Y. Guan. 2007. A probabilistic approach to syntax-
based reordering for statistical machine translation.
In Proc. the 45th ACL, pages 720?727.
McDonald, R., K. Crammer, and F. Pereira. 2005.
Spanning tree methods for discriminative training of
dependency parsers. In Thechnical Report MS-CIS-
05-11, UPenn CIS.
Nagata, M., K. Saito, K. Yamamoto, and K. Ohashi.
2006. A clustered global phrase reordering model
for statistical machine translation. In COLING-
ACL, pages 713?720.
Niesen, S., F.J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast
evaluation for mt research. In Proc. the 2nd In-
ternational Conference on Language Resources and
Evaluation.
Och, F. J. and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. In Proc. the 40th ACL, pages 295?
302.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. the 41th ACL,
pages 160?167.
Papineni, K. A., S. Roukos, T. Ward, and W-J. Zhu.
2001. Bleu: a method for automatic evaluation of
machine translation. In Proc. the 39th ACL, pages
311?318.
Shen, L., J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
ACL, pages 577?585.
Shen, L., J. Xu, B. Zhang, S. Matsoukas, and
R. Weischedel. 2009. Effective use of linguistic and
contextual information for statistical machine trans-
lation. In Proc. EMNLP, pages 72?80.
Tillman, C. 2004. A unigram orientation model
for statistical machine translation. In Proc. HLT-
NAACL, pages 101?104.
Tromble, R. and J. Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
445
Watanabe, T., H. Tsukada, and H. Isozaki. 2006. Left-
to-right target generation for hierarchical phrase-
based translation. In Proc. COLING-ACL, pages
777?784.
Xia, F. and M. McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite
patterns. In Proc. the 18th ICON, pages 508?514.
446
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944?952,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Translation Quality for Distant Language Pairs
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jp
Abstract
Automatic evaluation of Machine Translation
(MT) quality is essential to developing high-
quality MT systems. Various evaluation met-
rics have been proposed, and BLEU is now
used as the de facto standard metric. How-
ever, when we consider translation between
distant language pairs such as Japanese and
English, most popular metrics (e.g., BLEU,
NIST, PER, and TER) do not work well. It
is well known that Japanese and English have
completely different word orders, and special
care must be paid to word order in transla-
tion. Otherwise, translations with wrong word
order often lead to misunderstanding and in-
comprehensibility. For instance, SMT-based
Japanese-to-English translators tend to trans-
late ?A because B? as ?B because A.? Thus,
word order is the most important problem
for distant language translation. However,
conventional evaluation metrics do not sig-
nificantly penalize such word order mistakes.
Therefore, locally optimizing these metrics
leads to inadequate translations. In this pa-
per, we propose an automatic evaluation met-
ric based on rank correlation coefficients mod-
ified with precision. Our meta-evaluation of
the NTCIR-7 PATMT JE task data shows that
this metric outperforms conventional metrics.
1 Introduction
Automatic evaluation of machine translation (MT)
quality is essential to developing high-quality ma-
chine translation systems because human evaluation
is time consuming, expensive, and irreproducible. If
we have a perfect automatic evaluation metric, we
can tune our translation system for the metric.
BLEU (Papineni et al, 2002b; Papineni et al,
2002a) showed high correlation with human judg-
ments and is still used as the de facto standard au-
tomatic evaluation metric. However, Callison-Burch
et al (2006) argued that the MT community is overly
reliant on BLEU by showing examples of poor per-
formance. For Japanese-to-English (JE) translation,
Echizen-ya et al (2009) showed that the popular
BLEU and NIST do not work well by using the sys-
tem outputs of the NTCIR-7 PATMT (patent transla-
tion) JE task (Fujii et al, 2008). On the other hand,
ROUGE-L (Lin and Hovy, 2003), Word Error Rate
(WER), and IMPACT (Echizen-ya and Araki, 2007)
worked better.
In these studies, Pearson?s correlation coefficient
and Spearman?s rank correlation ? with human eval-
uation scores are used to measure how closely an
automatic evaluation method correlates with human
evaluation. This evaluation of automatic evaluation
methods is called meta-evaluation. In human eval-
uation, people judge the adequacy and the fluency of
each translation.
Denoual and Lepage (2005) pointed out that
BLEU assumes word boundaries, which is ambigu-
ous in Japanese and Chinese. Here, we assume
the word boundaries given by ChaSen, one of the
standard morphological analyzers (http://chasen-
legacy.sourceforge.jp/) following Fujii et al
(2008)
In JE translation, most Statistical Machine Trans-
lation (SMT) systems translate the Japanese sen-
tence
(J0) kare wa sono hon wo yonda node
sekaishi ni kyoumi ga atta
which means
944
(R0) he was interested in world
history because he read the book
into an English sentence such as
(H0) he read the book because he was
interested in world history
in which the cause and the effect are swapped. Why
does this happen? The former half of (J0) means ?He
read the book,? and the latter half means ?(he) was
interested in world history.? The middle word
?node? between them corresponds to ?because.?
Therefore, SMT systems output sentences like (H0).
On the other hand, Rule-based Machine Translation
(RBMT) systems correctly give (R0).
In order to find (R0), SMT systems have to search
a very large space because we cannot restrict its
search space with a small distortion limit. Most
SMT systems thus fail to find (R0).
Consequently, the global word order is essential
for translation between distant language pairs, and
wrong word order can easily lead to misunderstand-
ing or incomprehensibility. Perhaps, some readers
do not understand why we emphasize word order
from this example alone. A few more examples
will clarify what happens when SMT is applied to
Japanese-to-English translation. Even the most fa-
mous SMT service available on the web failed to
translate the following very simple sentence at the
time of writing this paper.
Japanese: meari wa jon wo koroshita.
Reference: Mary killed John.
SMT output: John killed Mary.
Since it cannot translate such a simple sentence, it
obviously cannot translate more complex sentences
correctly.
Japanese: bobu ga katta hon wo jon wa yonda.
Reference: John read a book that Bob bought.
SMT output: Bob read the book John bought.
Another example is:
Japanese: bobu wa meari ni yubiwa wo kau
tameni, jon no mise ni itta.
Reference: Bob went to John?s store to buy a
ring for Mary.
SMT output: Bob Mary to buy the ring, John
went to the store.
In this way, this SMT service usually gives incom-
prehensible or misleading translations, and thus peo-
ple prefer RBMT services. Other SMT systems also
tend to make similar word order mistakes, and spe-
cial care should be paid to the translation between
distant language pairs such as Japanese and English.
Even Japanese people cannot solve this word or-
der problem easily: It is well known that Japanese
people are not good at speaking English.
From this point of view, conventional automatic
evaluation metrics of translation quality disregard
word order mistakes too much. Single-reference
BLEU is defined by a geometrical mean of n-gram
precisions pn and is modified by Brevity Penalty
(BP) min(1, exp(1? r/h)), where r is the length of
the reference and h is the length of the hypothesis.
BLEU = BP? (p1p2p3p4)
1/4.
Its range is [0, 1]. The BLEU score of (H0) with ref-
erence (R0) is 1.0?(11/11?9/10?6/9?4/8)1/4 =
0.740. Therefore, BLEU gives a very good score to
this inadequate translation because it checks only n-
grams and does not regard global word order.
Since (R0) and (H0) look similar in terms of flu-
ency, adequacy is more important than fluency in
the translation between distant language pairs.
Similarly, other popular scores such as NIST,
PER, and TER (Snover et al, 2006) also give
relatively good scores to this translation. NIST
also considers only local word orders (n-grams).
PER (Position-Independent Word Error Rate) was
designed to disregard word order completely.
TER (Snover et al, 2006) was designed to allow
phrase movements without large penalties. There-
fore, these standard metrics are not optimal for eval-
uating translation between distant language pairs.
In this paper, we propose an alternative automatic
evaluation metric appropriate for distant language
pairs. Our method is based on rank correlation co-
efficients. We use them to compare the word ranks
in the reference with those in the hypothesis.
There are two popular rank correlation coeffi-
cients: Spearman?s ? and Kendall?s ? (Kendall,
1975). In Isozaki et al (2010), we used Kendall?s ?
to measure the effectiveness of our Head Finaliza-
tion rule as a preprocessor for English-to-Japanese
translation, but we measured the quality of transla-
tion by using conventional metrics.
945
It is not clear how well ? works as an automatic
evaluation metric of translation quality. Moreover,
Spearman?s ? might work better than Kendall?s ? .
As we discuss later, ? considers only the direction
of the rank change, whereas ? considers the distance
of the change.
The first objective of this paper is to examine
which is the better metric for distant language pairs.
The second objective is to find improvements of
these rank correlation-metrics.
Spearman?s ? is based on Pearson?s correlation
coefficients. Suppose we have two lists of numbers
x = [0.1, 0.4, 0.2, 0.6],
y = [0.9, 0.6, 0.2, 0.7].
To obtain Pearson?s coefficients between x and y,
we use the raw values in these lists. If we substitute
their ranks for their raw values, we get
x? = [1, 3, 2, 4] and y? = [4, 2, 1, 3].
Then, Spearman?s ? between x and y is given by
Pearson?s coefficients between x? and y?. This ?
can be rewritten as follows when there is no tie:
? = 1?
?
i d
2
i
n+1C3
.
Here, di indicates the difference in the ranks of the
i-th element. Rank distances are squared in this
formula. Because of this square, we expect that ?
decreases drastically when there is an element that
significantly changes in rank. But we are also afraid
that ? may be too severe for alternative good trans-
lations.
Since Pearson?s correlation metric assumes lin-
earity, nonlinear monotonic functions can change
its score. On the other hand, Spearman?s ? and
Kendall?s ? uses ranks instead of raw evaluation
scores, and simple application of monotonic func-
tions cannot change them (use of other operations
such as averaging sentence scores can change them).
2 Methodology
2.1 Word alignment for rank correlations
We have to determine word ranks to obtain rank cor-
relation coefficients. Suppose we have:
(R1) John hit Bob yesterday
(H1) Bob hit John yesterday
The 1st word ?John? in R1 becomes the 3rd word
in H1. The 2nd word ?hit? in R1 becomes the 2nd
word in H1. The 3rd word ?Bob? in R1 becomes the
1st word in H1. The 4th word ?yesterday? in R1 be-
comes the 4th word in H1. Thus, we get H1?s word
order list [3, 2, 1, 4]. The number of all pairs of in-
tegers in this list is 4C2 = 6. It has three increasing
pairs: (3,4), (2,4), and (1,4). Since Kendall?s ? is
given by:
? = 2?
the number of increasing pairs
the number of all pairs
? 1,
H1?s ? is 2? 3/6? 1 = 0.0.
In this case, we can obtain Spearman?s ? as fol-
lows: ?John? moved by d1 = 2 words, ?hit? moved
by d2 = 0 words, ?Bob? moved by d3 = 2 words,
and ?yesterday? moved by d4 = 0 words. Therefore,
H1?s ? is 1? (22 + 02 + 22 + 02)/5C3 = 0.2.
Thus, ? considers only the direction of the move-
ment, whereas ? considers the distance of the move-
ment. Both ? and ? have the same range [?1, 1]. The
main objective of this paper is to clarify which rank
correlation is closer to human evaluation scores.
We have to consider the limitation of the rank cor-
relation metrics. They are defined only when there
is one-to-one correspondence. However, a refer-
ence sentence and a hypothesis sentence may have
different numbers of words. They may have two or
more occurrences of the same word in one sentence.
Sometimes, a word in the reference does not appear
in the hypothesis, or a word in the hypothesis does
not appear in the reference. Therefore, we cannot
calculate ? and ? following the above definitions in
general.
Here, we determine the correspondence of words
between hypotheses and references as follows. First,
we find one-to-one corresponding words. That is,
we find words that appear in both sentences and only
once in each sentence. Suppose we have:
(R2) the boy read the book
(H2) the book was read by the boy
By removing non-aligned words by one-to-one cor-
respondence, we get:
946
(R3) boy read book
(H3) book read boy
Thus, we lost ?the.? We relax this one-to-one cor-
respondence constraint by using one-to-one corre-
sponding bigrams. (R2) and (H2) share ?the boy?
and ?the book,? and we can align these instances of
?the? correctly.
(R4) the1 boy2 read3 the4 book5
(H4) the4 book5 read3 the1 boy2
Now, we have five aligned words, and H4?s word
order is represented by [4, 5, 3, 1, 2].
In returning to H0 and R0, we find that each of
these sentences has eleven words. Almost all words
are aligned by one-to-one correspondence but ?he?
is not aligned because it appears twice in each sen-
tence. By considering one-to-one corresponding bi-
grams (?he was? and ?he read?), ?he? is aligned as
follows.
(R5) he1 was2 interested3 in4 world5
history6 because7 he8 read9 the10
book11
(H5) he8 read9 the10 book11 because7
he1 was2 interested3 in4 world5
history6
H5?s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6].
The number of increasing pairs is: 4C2 = 6 pairs in
[8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5,
6]. Then we obtain ? = 2 ? (6 + 15)/11C2 ? 1 =
?0.236. On the other hand,
?
i d
2
i = 5
2 ? 6 + 22 +
72 ? 4 = 350, and we obtain ? = 1 ? 350/12C3 =
?0.591.
Therefore, both Spearman?s ? and Kendall?s ?
give very bad scores to the misleading translation
H0. This fact implies they are much better metrics
than BLEU, which gave a good score to it. ? is much
lower than ? as we expected.
In general, we can use higher-order n-grams for
this alignment, but here we use only unigrams and
bigrams for simplicity. This algnment algorithm is
given in Figure 1. Since some hypothesis words do
not have corresponding reference words, the output
integer list worder is sometimes shorter than the
evaluated sentence. Therefore, we should not use
worder[i] ? i as di directly. We have to renumber
the list by rank as we did in Section 1.
Read a hypothesis sentence h = h1h2 . . . hm
and its reference sentence r = r1r2 . . . rn.
Initialize worder with an empty list.
For each word hi in h:
? If hi appears only once each in h and r, append j
s.t. rj = hi to worder.
? Otherwise, if the bigram hihi+1 appears only once
each in h and r, append j s.t. rjrj+1 = hihi+1 to
worder.
? Otherwise, if the bigram hi?1hi appears only once
each in h and r, append j s.t. rj?1rj = hi?1hi to
worder.
Return worder.
Figure 1: Word alignment algorithm for rank correlation
2.2 Word order metrics and meta-evaluation
metrics
These rank correlation metrics sometimes have neg-
ative values. In order to make them just like other
automatic evaluation metrics, we normalize them as
follows.
? Normalized Kendall?s ? : NKT = (? + 1)/2.
? Normalized Spearman?s ?: NSR = (?+ 1)/2.
Accordingly, NKT is 0.382 and NSR is 0.205.
These metrics are defined only when the number
of aligned words is two or more. We define both
NKT and NSR as zero when the number is one or
less. Consequently, these normalized metrics have
the same range [0, 1].
In order to avoid confusion, we use these abbre-
viations (NKT and NSR) when we use rank corre-
lations as word order metrics, because these cor-
relation metrics are also used in the machine trans-
lation community for meta-evaluation. For meta-
evaluation, we use Spearman?s ? and Pearson?s cor-
relation coefficient and call them ?Spearman? and
?Pearson,? respectively.
2.3 Overestimation problem
Since we measure the rank correlation of only cor-
responding words, these metrics will overestimate
the correlation. For instance, a hypothesis sentence
might have only two corresponding words among
947
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
BP (brevity penalty)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
? ?
?
?
? ?
?
?
?
?
?
?
?
??
?
?
?
?? ??
?
?
?
??
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
??
?
?
?
??
?
? ?
?
??
?
?
?
?
? ? ?
? ? ? ??
?
?
?
? ?
?
?
?
?? ??
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
? ?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
P (precision)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?? ?
?
?
?
?
?
?
?
??
?
? ?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
??
?
???
?
??? ?
?
?
? ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
??
?
?
?
?
??
?
?
?
?
?
?
?
? ?
? ?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
?? ?
?
???
?
?
??
?
??
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?? ?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
?
?
?
?
? ??
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
??
?
? ?
??
?
?
?
?
?
Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).
(Each ? corresponds to one sentence generated by one MT system)
dozens of words. In this case, these two words
determine the score of the whole sentence. If the
two words appear in their order in the reference,
the whole sentence obtains the best score, NSR =
NKT = 1.0, in spite of the fact that only two words
matched.
Solving this overestimation problem is the second
objective of this paper. BLEU uses ?Brevity Penalty
(BP)? (Section 1) to reduce the scores of too-short
sentences. We can combine the above word order
metrics with BP, e.g., NKT? BP and NSR? BP.
However, we cannot very much expect from this
solution because BP scores do not correlate with
human judgments well. The left graph of Figure
2 shows a scatter plot of BP and ?normalized av-
erage adequacy.? This graph has 15 (systems) ?
100 (sentences) dots. Each dot (?) corresponds to
one sentence from one translation system.
In the NTCIR-7 data, three human judges gave
five-point scores (1, 2, 3, 4, 5) for ?adequacy? and
?fluency? of each translated sentence. Although
each system translated 1,381 sentences, only 100
sentences were evaluated by the judges.
For each translated sentence, we averaged three
judges? adequacy scores and normalized this aver-
age x by (x?1)/4. This is our ?normalized average
adequacy,? and the dots appears only at multiples of
1/3? 1/4.
This graph shows that BP has very little correla-
tion with adequacy, and we cannot expect BP to im-
prove the meta-evaluation performance very much.
Perhaps, BP?s poor performance was caused by the
fact that most MT systems output almost the same
number of words, and if the number exceeds the
length of the reference, BP=1.0 holds.
Therefore, we have to consider other modifiers
for this overestimation problem. We can use other
common metrics such as precision, recall, and F-
measure to reduce the overestimation of NSR and
NKT.
? Precision: P = c/|h|, where c is the number of
corresponding words and |h| is the number of
words in the hypothesis sentence h.
? Recall: R = c/r, where |r| is the number of
words in the reference sentence r.
? F-measure: F? = (1 + ?2)PR/(?2P + R),
where ? is a parameter.
In (R2)&(H2)?s case, precision is 5/7 = 0.714 and
recall is 5/5 = 1.000.
Which metric should we use? Our preliminary
experiments with NTCIR-7 data showed that preci-
sion correlated best with adequacy among these
three metrics (P , R, and F?=1). In addition, BLEU
is essentially made for precision. Therefore, preci-
sion seems the most promising modifier.
The right graph of Figure 2 shows a scatter plot
of precision and normalized average adequacy. The
graph shows that precision has more correlation with
adequacy than BP. We can observe that sentences
with very small P values usually obtain very low
adequacy scores but those with mediocre P values
often obtain good adequacy scores.
948
If we multiply P directly by NSR or NKT, those
sentences with mediocre P values will lose too
much of their scores. The use of
?
x will miti-
gate this problem. Since
?
P is closer to 1.0 than
P itself, multiplication of
?
P instead of P itself
will save these sentences. If we apply
?
x twice
(
??
P = 4
?
P ), it will further save them. There-
fore, we expect?
?
P and? 4
?
P to work better than
?P . Now, we propose two new metrics:
NSRP? and NKTP?,
where ? is a parameter (0 ? ? ? 1).
3 Experiments
3.1 Meta-evaluation with NTCIR-7 data
In order to compare automatic translation evalua-
tion methods, we use submissions to the NTCIR-7
Patent Translation (PATMT) task (Fujii et al, 2008).
Fourteen MT systems participated in the Japanese-
English intrinsic evaluation. There were two Rule-
Based MT (RMBT) systems and one Example-
based MT (EBMT) system. All other systems were
Statistical MT (SMT) systems. The task organiz-
ers provided a baseline SMT system. These 15 sys-
tems translated 1,381 Japanese sentences into En-
glish. The organizers evaluated these translations by
using BLEU and human judgments. In the human
judgements, three experts independently evaluated
100 selected sentences in terms of ?adequacy? and
?fluency.?
For automatic evaluation, we used a single refer-
ence sentence for each of these 100 manually evalu-
ated sentences. Echizen-ya et al (2009) used multi-
reference data, but their data is not publicly available
yet.
For this meta-evaluation, we measured the
corpus-level correlation between the human evalua-
tion scores and the automatic evaluation scores. We
simply averaged scores of 100 sentences for the pro-
posed metrics. For existing metrics such as BLEU,
we followed their definitions for corpus-level eval-
uation instead of simple averages of sentence-level
scores. We used default settings for conventional
metrics, but we tuned GTM (Melamed et al, 2007)
with -e option. This option controls preferences
on longer word runs. We also used the para-
phrase database TERp (http://www.umiacs.umd.
edu/?snover/terp) for METEOR (Banerjee and
Lavie, 2005).
3.2 Meta-evaluation with WMT-07 data
We developed our metric mainly for automatic eval-
uation of translation quality for distant language
pairs such as Japanese-English, but we also want
to know how well the metric works for similar lan-
guage pairs. Therefore, we also use the WMT-
07 data (Callison-Burch et al, 2007) that covers
only European language pairs. Callison-Burch et al
(2007) tried different human evaluation methods and
showed detailed evaluation scores. The Europarl test
set has 2,000 sentences, and The News Commentary
test set has 2,007 sentences.
This data has different language pairs: Spanish,
French, German ? English. We exclude Czech-
English because there were so few systems (See the
footnote of p. 146 in their paper).
4 Results
4.1 Meta-evaluation with NTCIR-7 data
Table 1 shows the main results of this paper. The
left part has corpus-level meta-evaluation with ade-
quacy. Error metrics, WER, PER, and TER, have
negative correlation coefficients, but we did not
show their minus signs here.
Both NSR-based metrics and NKT-based metrics
perform better than conventional metrics for this NT-
CIR PATMT JE translation data. As we expected,
?BP and ?P (1/1) performed badly. Spearman of
BP itself is zero.
NKT performed slightly better than NSR. Per-
haps, NSR penalized alternative good translations
too much. However, one of the NSR-based metrics,
NSRP 1/4, gave the best Spearman score of 0.947,
and the difference between NSRP? and NKTP?
was small. Modification with P led to this improve-
ment.
NKT gave the best Pearson score of 0.922. How-
ever, Pearson measures linearity and we can change
its score through a nonlinear monotonic function
without changing Spearman very much. For in-
stance, (NSRP 1/4)1.5 also has Spearman of 0.947
but its Pearson is 0.931, which is better than NKT?s
0.922. Thus, we think Spearman is a better meta-
evaluation metric than Pearson.
949
Table 1: NTCIR-7 Meta-evaluation: correlation with hu-
man judgments (Spm = Spearman, Prs = Pearson)
human judge Adequacy Fluency
eval\ meta-eval Spm Prs Spm Prs
P 0.615 0.704 0.672 0.876
R 0.436 0.669 0.461 0.854
F?=1 0.525 0.692 0.543 0.871
BP 0.000 0.515 -0.007 0.742
NSR 0.904 0.906 0.869 0.910
NSRP 1/8 0.937 0.905 0.890 0.934
NSRP 1/4 0.947 0.900 0.901 0.944
NSRP 1/2 0.937 0.890 0.926 0.949
NSRP 1/1 0.883 0.872 0.883 0.939
NSR ? BP 0.851 0.874 0.769 0.910
NKT 0.940 0.922 0.887 0.931
NKTP 1/8 0.940 0.913 0.908 0.944
NKTP 1/4 0.940 0.904 0.908 0.949
NKTP 1/2 0.929 0.890 0.897 0.949
NKTP 1/1 0.897 0.869 0.879 0.936
NKT ? BP 0.829 0.878 0.726 0.918
ROUGE-L 0.903 0.874 0.889 0.932
ROUGE-S(4) 0.593 0.757 0.640 0.869
IMPACT 0.797 0.813 0.751 0.932
WER 0.894 0.822 0.836 0.926
TER 0.854 0.806 0.372 0.856
PER 0.375 0.642 0.393 0.842
METEOR(TERp) 0.490 0.708 0.508 0.878
GTM(-e 12) 0.618 0.723 0.601 0.850
NIST 0.343 0.661 0.372 0.856
BLEU 0.515 0.653 0.500 0.795
The right part of Table 1 shows correlation with
fluency, but adequacy is more important, because
our motivation is to provide a metric that is useful to
reduce incomprehensible or misunderstanding out-
puts of MT systems. Again, the correlation-based
metrics gave better scores than conventional metrics,
and BP performed badly. NSR-based metrics proved
to be as good as NKT-based metrics.
Meta-evaluation scores of the de facto standard
BLEU is much lower than those of other metrics.
Echizen-ya et al (2009) reported that IMPACT per-
formed very well for sentence-level evaluation of
NTCIR-7 PATMT JE data. This corpus-level result
also shows that IMPACT works better than BLEU,
but ROUGE-L, WER, and our methods give better
scores than IMPACT.
Table 2: WMT-07 meta-evaluation: Each source lan-
guage has two columns: the left one is News Corpus and
the right one is Europarl.
Spearman?s ? with human ?rank?
source French Spanish German
NSR 0.775 0.837 0.523 0.766 0.700 0.593
NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685
NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714
NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714
NKT 0.845 0.857 0.607 0.838 0.700 0.630
NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714
NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714
NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714
BLEU 0.786 0.679 0.750 0.595 0.400 0.821
WER 0.607 0.857 0.750 0.429 0.000 0.500
ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857
ROUGES 0.883 0.679 0.786 0.690 0.400 0.929
4.2 Meta-evaluation with WMT-07 data
Callison-Burch et al (2007) have performed differ-
ent human evaluation methods for different language
pairs and different corpora. Their Table 5 shows
inter-annotator agreements for the human evaluation
methods. According to their table, the ?sentence
ranking? (or ?rank?) method obtained better agree-
ment than ?adequacy.? Therefore, we show Spear-
man?s ? for ?rank.? We used the scores given in
their Tables 9, 10, and 11. (The ?constituent? meth-
ods obtained the best inter-annotator agreement, but
these methods focus on local translation quality and
have nothing to do with global word order, which we
are discussing here.)
Table 2 shows that our metrics designed for
distant language pairs are comparable to conven-
tional methods even for similar language pairs, but
ROUGE-L and ROUGE-S performed better than
ours for French News Corpus and German Europarl.
BLEU scores in this table agree with those in Table
17 of Callison-Burch et al (2007) within rounding
errors.
After some experiments, we noticed that the use
ofR instead of P often gives better scores for WMT-
07, but it degrades NTCIR-7 scores. We can extend
our metric by F? , weighted harmonic mean of P and
R, or any other interpolation, but the introduction
of new parameters into our metric makes it difficult
950
to control. Improvement without new parameters is
beyond the scope of this paper.
5 Discussion
It has come to our attention that Birch et al (2010)
has independently proposed an automatic evaluation
method based on Kendall?s ? . First, they started
with Kendall?s ? distance, which can be written as
?1?NKT? in our terminology, and then subtracted
it from one. Thus, their metric is nothing but NKT.
Then, they proposed application of the square root
to get better Pearson by improving ?the sensitivity
to small reorderings.? Since they used ?Kendall?s ??
and ?Kendall?s ? distance? interchangeably, it is not
clear what they mean by ?
?
Kendall?s ? ,? but per-
haps they mean 1 ?
?
1?NKT because
?
NKT is
more insensitive to small reorderings. Table 3 shows
the performance of these metrics for NTCIR-7 data.
Pearson?s correlation coefficient with adequacy was
improved by 1 ?
?
1? NKT, but other scores were
degraded in this experiment.
The difference between our method and Birch et
al. (2010)?s method comes from the fact that we
used Japanese-English translation data and Spear-
man?s correlation for meta-evaluation, whereas they
used Chinese-English translation data and only Pear-
son?s correlation for meta-evaluation. Chinese word
order is different from English, but Chinese is a
Subject-Verb-Object (SVO) language and thus is
much closer to English word order than Japanese,
a typical SOV language.
We preferred NSR because it penalizes global
word order mistakes much more than does NKT, and
as discussed above, global word order mistakes of-
ten lead to incomprehensibility and misunderstand-
ing.
On the other hand, they also tried Hamming dis-
tance, and summarized their experiments as follows:
However, the Hamming distance seems to
be more informative than Kendall?s tau for
small amounts of reordering.
This sentence and the introduction of the square root
to NKT imply that Chinese word order is close to
that of English, and they have to measure subtle
word order mistakes.
Table 3: NTCIR-7 meta-evaluation: Effects of square
root (b(x) = 1?
?
1? x)
NKT
?
NKT b(NKT)
Spearman w/ adequacy 0.940 0.940 0.922
Pearson w/ adequacy 0.922 0.817 0.941
Spearman w/ fluency 0.887 0.865 0.858
Pearson w/ fluency 0.931 0.917 0.833
In spite of these differences, the two groups inde-
pendently recognized the usefulness of rank correla-
tions for automatic evaluation of translation quality
for distant language pairs.
In their WMT-2010 paper (Birch and Osborne,
2010), they multiplied NKT with the brevity penalty
and interpolated it with BLEU for the WMT-2010
shared task. This fact implies that incomprehensible
or misleading word order mistakes are rare in trans-
lation among European languages.
6 Conclusions
When Statistical Machine Translation is applied to
distant language pairs such as Japanese and English,
word order becomes an important problem. SMT
systems often fail to find an appropriate translation
because of a large search space. Therefore, they
often output misleading or incomprehensible sen-
tences such as ?A because B? vs. ?B because A.? To
penalize such inadequate translations, we presented
an automatic evaluation method based on rank corre-
lation. There were two questions for this approach.
First, which correlation coefficient should we use:
Spearman?s ? or Kendall?s ?? Second, how should
we solve the overestimation problem caused by the
nature of one-to-one correspondence?
We answered these questions through our exper-
iments using the NTCIR-7 PATMT JE translation
data. For the first question, ? was slightly better
than ?, but ? was improved by precision. For the
second question, it turned out that BLEU?s Brevity
Penalty was counter-productive. A precision-based
penalty gave a better solution. With this precision-
based penalty, both ? and ? worked well and they
outperformed conventional methods for NTCIR-7
data. For similar language pairs, our method was
comparable to conventional evaluation methods. Fu-
951
ture work includes extension of the method so that it
can outperform conventional methods even for sim-
ilar language pairs.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgements. In Proc. of ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and Summarization, pages 65?72.
Alexandra Birch and Miles Osborne. 2010. LRscore for
evaluating lexical and reordering quality in MT. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 327?
332.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluatiing the role of Bleu in ma-
chine translation research. In Proc. of the Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Chrstof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of machine translation. In Proc. of
the Workshop on Machine Translation (WMT), pages
136?158.
Etienne Denoual and Yves Lepage. 2005. BLEU in char-
acters: towards automatic MT evaluation in languages
without word delimiters. In Companion Volume to the
Proceedings of the Second International Joint Confer-
ence on Natural Language Processing, pages 81?86.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In Proceedings of MT Summit XII Workshop on Patent
Translation, pages 151?158.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata,
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, and Noriko Kando. 2009. Meta-
evaluation of automatic evaluation methods for ma-
chine translation using patent translation data in ntcir-
7. In Proceedings of the 3rd Workshop on Patent
Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 250?257.
Maurice G. Kendall. 1975. Rank Correlation Methods.
Charles Griffin.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 71?78.
Dan Melamed, Ryan Green, and Joseph P. Turian. 2007.
Precision and recall of machine translation. In Proc.
of NAACL-HLT, pages 61?63.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002a. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish Results. In
Proc. of the International Conference on Human Lan-
guage Technology Research (HLT), pages 132?136.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002b. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
952
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130?140,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Modeling and Learning Semantic Co-Compositionality
through Prototype Projections and Neural Networks
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{masashi-t,kevinduh,shimbo,matsu}@is.naist.jp
Abstract
We present a novel vector space model for se-
mantic co-compositionality. Inspired by Gen-
erative Lexicon Theory (Pustejovsky, 1995),
our goal is a compositional model where
both predicate and argument are allowed to
modify each others? meaning representations
while generating the overall semantics. This
readily addresses some major challenges with
current vector space models, notably the pol-
ysemy issue and the use of one represen-
tation per word type. We implement co-
compositionality using prototype projections
on predicates/arguments and show that this
is effective in adapting their word represen-
tations. We further cast the model as a
neural network and propose an unsupervised
algorithm to jointly train word representations
with co-compositionality. The model achieves
the best result to date (? = 0.47) on the
semantic similarity task of transitive verbs
(Grefenstette and Sadrzadeh, 2011).
1 Introduction
Vector space models of words have been very
successful in capturing the semantic and syntactic
characteristics of individual lexical items (Turney
and Pantel, 2010). Much research has addressed
the question of how to construct individual word
representations, for example distributional models
(Mitchell and Lapata, 2010) and neural models
(Collobert and Weston, 2008). These word repre-
sentations are used in various natural language pro-
cessing (NLP) tasks such as part-of-speech tagging,
chunking, named entity recognition, and semantic
Vk	
Pcompany=VkTVk	
Co-Compositionality with Prototype Projections	Co-compositional vector	of verb and object	
start	build	
buy	
operate =	
company	run	
VERB	
runcompany	
Prototype	 Projection	
companyrun	
OBJ	
???	 ???	 Ok	
firm	bank	
hotel	
???	 ???	
Prun=OkTOk	? ?m	?	 ?	
Figure 1: Here, we capture the semantics of run in run
company by projecting the original word representation
of run to the prototype space of company (and vice versa).
role labeling (Turian et al, 2010; Collobert et al,
2011).
Recently, modeling of semantic compositionality
(Frege, 1892) in vector space has emerged as another
important line of research (Mitchell and Lapata,
2008; Mitchell and Lapata, 2010; Baroni and Zam-
parelli, 2010; Socher et al, 2012; Grefenstette and
Sadrzadeh, 2011; Van de Cruys et al, 2013). The
goal is to formulate how individual word represen-
tations ought to be combined to achieve phrasal or
sentential semantics.
The main questions for semantic compositionality
that we are concerned with are: (1) how can poly-
semy be handled by a single vector representation
per word type, learned by either a distributional or
neural model, and (2) how does composition resolve
130
these ambiguities. To this end, we are inspired by
the idea of type coercion and co-compositionality
in Generative Lexicon Theory (Pustejovsky, 1995).
Co-compositionality advocates that instead of a
predicate-argument view of composition, both pred-
icate and argument influence/coerce each other to
generate the overall meaning. For example, consider
a polysemous word like run:
? (a) He runs the company.
? (b) He runs the marathon.
Run may have several senses, but the prototypical
verbs that select for company differ from those
that select for marathon, and thus the ambiguity
at the word level is resolved at the sentence level.
The same is true for the other direction, where the
predicate also coerces meaning to the argument to
fit expectation.
We believe that models for semantic com-
position ought to incorporate elements of co-
compositionality. We propose such a model here,
using what we call prototype projections. For each
predicate, we transform its vector representation by
projecting it into a latent space that is prototypical
of its argument. This projection is performed anal-
ogously for each argument as well, and the final
meaning is computed by composition of these trans-
formed vectors (Figure 1). In addition, the model is
cast as a neural network where word representations
could be re-trained or fine-tuned.1
Our contributions are two-fold:
1. We propose a novel model for semantic co-
compositionality. This model, based on
prototype projections, is easy to implement
and achieves state-of-the-art performance in
the sentence similarity dataset developed by
Grefenstette and Sadrzadeh (2011).
2. Our results empirically confirm that existing
word representations (eg., SDS and NLM in
Section 2) are sufficiently effective at capturing
1While we are inspired by co-compositionality, it is impor-
tant to note that our model does not implement qualia structure
and other important components of Generative Lexicon Theory.
We operate within the vector space model of distributional
semantics, so these ideas are implemented with matrix algebra,
which is a natural fit with neural networks.
polysemy, as long as we have the proper mech-
anism to tease out the proper sense during com-
position. We further propose an unsupervised
neural network training algorithm that jointly
fine-tunes the word representations within the
co-composition model, resulting in even better
performance on the sentence similarity task.
We would like to emphasize the second contribu-
tion especially. Semantics research is divided in two
strands, one focusing on learning word represen-
tations without consideration for compositionality,
and the other focusing on compositional semantics
using the representations only as an input. But issues
are actually related from the linguistics perspective,
and even more so if we adopt a Generative Lexicon
perspective. Our neural network model bridges
these two strands of research by modeling co-
compositionality and learning word representations
simultaneously. We note that methods using context
effects have been explored by Erk and Pado? (2008;
2009) and Thater et al (2010; 2011), but to the
best of our knowledge, ours is the first model to
perform co-compositionality and learning of word
representations jointly.
In the following, we first provide background to
the word representations employed here (Section 2).
We describe the model for co-compositionality in
Section 3 and the corresponding neural network in
Section 4. Evaluation and experiments are presented
in Sections 5 and 6. Finally, we end with related
work (Section 7) and conclusions (Section 8).
2 Word Vector Representations
2.1 Simple Distributional Semantic space
(SDS) word vectors
Word meaning is often represented in a high di-
mensional space, where each element corresponds to
some contextual element in which the word is found.
Mitchell and Lapata (2010) present a co-occurrence-
based semantic space called Simple Distributional
Semantic space (SDS). Their SDS model uses a con-
text window of five words on either side of the target
word and 2,000 vector components, representing the
most frequent context words (excluding a list of stop
words). These components vi(t) were set to the
ratio of the probability of the context word given the
131
target word to the probability of the context word
overall:
vi(t) =
p(ci|t)
p(ci)
= freqci,t ? freqtotalfreqt ? freqci
(1)
where freqci,t, freqtotal, freqt and freqci are the
frequencies of the context word ci with the target
word t, the total count of all word tokens, the
frequency of the target word t, and the frequency
of the context word ci, respectively.
2.2 Neural Language Model (NLM) word
embeddings
Another popular way to learn word representations
is based on the Neural Language Model (NLM)
(Bengio et al, 2003). In comparison with SDS,
NLM tend to be low-dimensional (e.g. 50 dimen-
sions) but employ dense features. These dense
feature vectors are usually called word embeddings,
and it has been shown that such vectors can cap-
ture interesting linear relationships, such as king ?
man + woman ? queen (Mikolov et al, 2013).
In this work, we adopt the model by Collobert
and Weston (2008). The idea is to construct a
neural network based on word sequences, where
one outputs high scores for n-grams that occur in a
large unlabeled corpus and low scores for nonsense
n-grams where one word is replaced by a random
word. This word representation with NLM has been
used to good effect, for example in (Turian et al,
2010; Collobert et al, 2011; Huang et al, 2012)
where induced word representations are used with
sophisticated features to improve performance in
various NLP tasks.
Specifically, we first represent the word sequence
as a vector x = [d(w1);d(w2); . . . ;d(wm)], where
wi is ith word in the sequence, m is the win-
dow size, d(w) is the vector representation of
word w (an n-dimensional column vector) and
[d(w1);d(w2); . . . ;d(wm)] is the concatenation of
word vectors as an input of neural network. Second,
we compute the score of the sequence,
score(x) = sT(tanh(Wx+ b)) (2)
where W ? Rh?(mn) and s ? Rh are the first
and second layer weights of the neural network,
and b ? Rh is the bias unit of hidden layer. The
superscript T represents transposition, and tanh is
applied element-wise. We also create a corrupted
sequence xc = [d(w1);d(w2); . . . ;d(wm?)] where
wm? is chosen randomly from the vocabulary. We
compute the score of this implicit negative sequence
xc with the same neural network, score(xc) =
sT(tanh(Wxc + b)). Finally, we get the cost
function of this training algorithm as follow.
J = max(0, 1 ? score(x) + score(xc)) (3)
In order to minimize this cost function, we optimize
the parameters ? = (s,W,b,x) via backpropagation
with stochastic gradient descent (SGD).
3 The Model
3.1 Prototype Projection
Generative Lexicon Theory (Pustejovsky, 1995)
makes a distinction between accidental polysemy
(homonyms, e.g. bank as financial institution vs.
as river side) and logical polysemy (e.g. figure and
ground meanings of door). Our model handles both
cases using the concept of projection to latent proto-
type space. The fundamental idea is that for each
word w and a syntactic/semantic (binary) relation
R (such as verb-object relation), w has a set of
prototype words with which it frequently occurs in
relation R. For example, if w is a word company,
and R is the object-verb relation, prototype words
should include start, build, and buy (Figure 1).
For each word-relation pair, we pre-compute the
latent semantic subspace spanned by these prototype
words.
Later, when we encounter a phrase expressing a
relationR between two wordsw1 andw2, each word
is first projected onto a latent subspace determined
by the other word and relation R. The projection
operation shifts the meaning of individual words in
accordance with context, and through this operation
we realize coercion/co-composition. And finally, the
meaning of the phrase is computed from the two
projected points in the semantic space.
Let us describe how to compute the latent sub-
space associated with a word w0 and a relation R.
First, we collect from a corpus a set of prototype
words that occur frequently in relation R with target
word w0. So for example in Figure 1, if w0 =
132
verb object landmark similarity(verb, landmark) similarity(projected verb, landmark)
run company operate 0.40 0.70
meet criterion satisfy 0.49 0.71
spell name write 0.04 0.50
Table 1: Examples of verb-object pairs. Original verb and landmark verb similarity, prototype projected verb and
landmark verb similarity, as measure by cosine using Collobert and Weston?s word embeddings. Meet has a abstract
meaning itself, but after prototype projection with matrix constructed by word vectors of W (VerbOf, criterion), meet
is more close to meaning of satisfy.
company, and R = VerbOf is the object-verb
relation,
W (VerbOf, company) = {start, build, . . . , buy}.
Now let W (R,w0) = {w1, w2, ? ? ? , wm} be
the m prototype words we collected, and let d(w)
denote the n-dimensional (column) vector represen-
tation of word w (either by SDS or NLM representa-
tion). We make anm?nmatrixC(R,w0) by stacking
the prototype word vectors, i.e.,
C(R,w0) = [d(w1),d(w2), ? ? ? ,d(wm)]
T (4)
and then apply Singular Value Decomposition
(SVD) to extract the latent space from this matrix:
C(R,w0) ? Uk?kV
T
k . (5)
word1	
word vector dimension n	
word2	
wordm	
m	
? ? ?	
?????	
k	
k	
n	
? ? ?	
? ? ?	
?????	
? ? ?	
? ? ?	
k	
k	?k VkTUkC ?
Figure 2: Graphical representation of SVD in our model.
Figure 2 shows the graphical representation of
this matrix factorization. In NLP tasks, SVD is
often applied to a term-document matrix, but in our
model, we apply SVD to the matrix consisting of
word vectors.
Intuitively, ?kVTk represents the latent sub-
space formed by prototypical words W (R,w0) =
{w1, w2, ? ? ? , wm}. We call this matrix the proto-
type space of word w0 with respect to relation R.
Note that the matrix of orthogonal projection
onto this prototype space is given by P(R,w0) =
(?kVTk)T(?kVTk). Hence, when we observe a rela-
tion R(w0, w), the projected representation of word
w in this context is computed by prpj(R,w0)(w)
defined as follows:
prpj(R,w0)(w) = P(R,w0)d(w). (6)
Table 1 shows several examples of how meanings
change after prototype projection using word em-
beddings of Collobert and Weston (2008).2
3.2 Co-Compositionality
In order to model co-compositionality, we apply
prototype projection to both the verb and the object.
In particular, suppose verb is wv and object is wo,
C(VerbOf,wo) is used to project wv and C(ObjOf,wv)
is used to project wo. The vector that represents
the overall meaning of verb-object with prototype
projection is computed by:
cocomp(wv, wo) =
f(prpj(VerbOf,wo)(wv),prpj(ObjOf,wv)(wo)) (7)
Function f can be a compositional computation like
simple addition or element-wise multiplication of
two vectors. This is graphically shown in Figure 1.
4 Unsupervised Learning of
Co-Compositionality
In this section, we propose a new neural language
model that learns word representations while jointly
accounting for compositional semantics. One cen-
tral assumption of our work (and many other works
in compositional semantics) is that a single vector
2ronan.collobert.com/senna/
133
v	 o	
z = f(v, o)	s	
score = sTz	
Compositional	Neural Language Model (C-NLM)	
verb	 obj	
Figure 3: Compositional Neural Language Model (C-
NLM).
per word type sufficiently represents the multiple
meanings and usage patterns of a word.3 That
means that for a polysemous word, its word vector
actually represents an aggregation of the distinctly
different contexts it occurs in. We will show that
such an assumption is quite reasonable under our
model, since the prototype projections successfully
tease out the proper semantics from these aggregate
representations.
However, it is natural to wonder whether one
can do better if one incorporates the compositional
model into the training of the word representations
in the first place. To do so, we formulate a nov-
el model called Compositional Neural Language
Model (Section 4.1). This model is a combination
of an unsupervised training algorithm with basic
compositionality (addition/multiplications). Then,
we extend this model with the projection idea in
section 3.2 to formulate a Co-Compositional Neural
Language Model (Section 4.2).
4.1 Compositional Neural Language Model
(C-NLM)
Compositional Neural Language Model (C-NLM)
is a combination of a word representation learning
method and compositional rule. In contrast to other
compositional models based on machine learning,
our model has no complex parameters for model-
ing composition. Composition is modeled using
straightforward vector addition/multiplications; in-
stead, what is learned is the word representation.
Figure 3 shows the C-NLM. The learning al-
gorithm is unsupervised, and works by artificially
3There are works on multiple representations, e.g.,
(Reisinger and Mooney, 2010); we focus on single represen-
tation here.
z = f(v, o)	s	
score = sTz	
Pobj	 Pverb	
Co-Compositional	Neural Language Model (CoC-NLM)	
v	 o	
y	x	
verb	 obj	
Figure 4: Co-Compositional Neural Language Model
(CoC-NLM) is C-NLM with prototype projection.
generating negative examples in a fashion analogous
to the NLM learning algorithm of (Collobert and
Weston, 2008) and contrastive estimation (Smith
and Eisner, 2005). First, given some initial word
representations and raw sentences, we compute the
compositional vector with function f (in this sec-
tion, we will assume that we will be using the
addition operator). Second, in order to obtain the
score of compositional vector, we compute the dot
product with vector s ? Rn (n is the dimension of
the word vector space): verb vector v = d(wv) and
object vector o = d(wo).
score(v,o) = sTf(v,o) = sT(v + o) (8)
We also create a corrupted pair by substituting a ran-
dom verb wverb?. The cost function J = max(0, 1?
score(v,o) + score(vc,o)), where vc is the word
vector of wverb?, encourages that the score of correct
pair is higher than the score of the corrupt pair. Let
z = v + o, our model parameters are ? = (s, z,v).
The optimization is divided into two steps:
1. Optimize s and z via SGD.
2. Let znew be the updated z via step 1. The new
verb vector vnew trained within additive composi-
tionality is just vnew = znew ? o. Note that if we
also want to optimize o, we may want to also corrupt
the object and run SGD in step 2 as well.
4.2 Co-Compositional Neural Language Model
(CoC-NLM)
We now add prototype projection into C-NLM,
making our final model: Co-Compositional Neural
134
Language Model (CoC-NLM). We define the score
function as dot product of s and additional vector of
prototype projected vectors (Figure 4). Let Pobj =
P(VerbOf,wo) and Pverb = P(ObjOf,wv),
score(v,o) = sT(Pobjv +Pverbo). (9)
Let x = Pobjv, y = Pverbo and z = x + y.
Our model parameters are ? = (s, z,v). The
optimization algorithm of CoC-NLM is divided into
three steps like C-NLM. First, we optimize s and
z. Second, the projected verb vector is updated
as xnew = znew ? y. Finally we optimize v to
minimize the Euclidean distance between xnew and
Pobjv, where ? is a regularization hyper-parameter:
J(v) = 12 ||xnew ?Pobjv||
2 + ?2v
Tv (10)
5 Evaluation
5.1 Dataset
In order to evaluate the performance of our new
co-compositional model with prototype projection
and word representation learning algorithm, we
make use of the disambiguation task of transitive
sentences developed by Grefenstette and Sadrzadeh
(2011). This is an extension of the two words
phrase similarity task defined in Mitchell and Lapata
(2008), and constructed according to similar guide-
lines. The dataset consists of similarity judgments
between a landmark verb and a triple consisting of
a transitive target verb, subject and object extracted
from the BNC corpus. Human judges give scores
between 1 to 7, with higher scores implying higher
semantic similarity. For example, Table 2 shows
some examples from the data: we see that the verb
meet with subject system and object criterion is
judged similar to the landmark verb satisfy but not
visit. The dataset contains a total of 2500 similarity
judgements, provided by 25 participants.4 The
task is to have the model produce a score for each
pair of landmark verb and verb-subject-object triple.
Models are evaluated by computing the Spearman?s
? correlation between its similarity scores and that
of the human judgments.
4http://www.cs.ox.ac.uk/
people/edward.grefenstette/
verb subj obj landmark sim
meet system criterion satisfy 6
meet system criterion visit 1
write student name spell 7
write student paper spell 2
Table 2: Examples from the disambiguation task de-
veloped by Grefenstette and Sadrzadeh (2011). Human
judges give scores between 1 to 7, with higher scores
implying higher semantic similarity. Verb meet with
subject system and object criterion is judged similar to
the landmark verb satisfy but not visit.
5.2 Baselines
We compare our model against multiple baselines
for semantic compositionality:
1. Mitchell and Lapata?s (2008) additive and
element-wise multiplicative model as simplest
baselines.
2. Grefenstette and Sadrzadeh?s (2011) model
based on the abstract categorical framework
(Coecke et al, 2010). This model computes
the outer product of the subject and object
vector, the outer product of the verb vector
with itself, and then the element-wise product
of both results.
3. Erk and Pado??s (2008) model, which adapts the
word vectors based on context and is the most
similar in terms of motivation to ours.
4. Van de Cruy et al (2013) multi-way interaction
model based on matrix factorization. This
achieves the best result for this task to date.
A detailed explanation of these models will be
provided in Section 7. For the underlying word rep-
resentations, we experiment with sparse 2000-dim
SDS and dense 50-dim NLM. These are provided
by Blacoe and Lapata (2012)5 and trained on the
British National Corpus (BNC). We are interested
in knowing how sensitive each model is to the
underlying word representation. In general, this is
a challenging task: the upper-bound of ? = 0.62 is
the inter-annotator agreement.
5http://homepages.inf.ed.ac.uk/
s1066731/index.php?page=resources
135
5.3 Implementation details
In terms of implementation detail, our model and our
re-implementation of Erk and Pado?s model make
use of the ukWaC corpus (Baroni et al, 2009).6 This
corpus is a two billion word corpus automatically
harvested from the web and parsed by the Malt-
Parser (Nivre et al, 2006). We use ukWaC corpus
to collect W (VerbOf, wo) and W (ObjOf, wv) for
prototype projections. We also extract about 5000
verb-object pairs that relevant for testdata from this
corpus to train our neural network learning algorith-
m. In our co-compositional model, the contribution
ratio of SVD is set to 80% (i.e. automatically
fixing k in SVD to include 80% of the top singular
values). We set the number of prototype vectors
to be m = 20, where W (VerbOf, wo) is filtered
with high frequency words and W (ObjOf, wv) is
filtered with both high frequency and high similarity
words. In our model, we output the scores for SVO
triple sentence dataset as (subject=ws, verb=wv,
object=wo, f = Addition/Multiplication):
cocomp(ws, wv, wo) =
f(d(ws), cocomp(wv, wo)) (11)
6 Results and Discussion
6.1 Main Results: The Correlation
Table 3 shows the correlation scores of various
models. Our observations are as follows:
1. The best reported result for this task (Van de
Cruys et al, 2013) is ? = 0.37. Our
model (with NLM as word representation and
f=Addition as operator) achieves ? = 0.44,
outperforming it by a large margin. To the best
of our knowledge, this is now state-of-the-art
result for this task.
2. Our model is not very sensitive to the underly-
ing word representation. With f=Addition, we
have ? = 0.41 for SDS vs ? = 0.44 for NLM.
With f=Multiply, we have ? = 0.37 for SDS
vs. ? = 0.35 for NLM. This implies that the
prototype projection is robust to the underlying
word representation, which is a desired charac-
teristic of compositional models.
6http://wacky.sslmit.unibo.it/
doku.php?id=corpora
Model ?
Grefenstette and Sadrzadeh (2011) ? 0.21
Add (SDS) ? 0.31
Add (NLM) ? 0.31
Multiply (SDS) ? 0.35
Multiply (NLM) ? 0.30
Van de Cruys et al (2013) 0.37
Erk and Pado? (SDS) 0.39
Erk and Pado? (NLM) 0.03
Co-Comp with f=Add (SDS) 0.41
Co-Comp with f=Add (NLM) ? 0.44
Co-Comp with f=Multiply (SDS) 0.37
Co-Comp with f=Multiply (NLM) 0.35
Upper bound ? 0.62
Table 3: Results of the different compositionality models
on the similarity task. The number of prototype words
m = 20 in all our models. Our model (f=Addition and
NLM) achieves the new state-of-the-art performance for
this task (? = 0.44).
3. The contextual model of Erk and Pado? (SDS)
also performed relatively well (? = 0.39),
in fact outperforming the Van de Cruy et al
(2013) result as well. This means that the
general idea of adapting word representations
based on context is a very powerful one. How-
ever, Erk and Pado??s model using the NLM rep-
resentation is extremely poor (? = 0.03). The
reason is that it uses a product operation under-
the-hood to adapt the vectors, which inherently
assumes a sparse representation. In this sense,
our projection approach is more robust.
The state-of-the-art result for our model in Table
3 does not yet make use of the training algorithm
described in Section 4. It is simply implementing
the co-compositionality idea using prototype projec-
tions (Section 3.2). Next in Section 6.2 we will show
additional gains using unsupervised learning.
6.2 Improvements from unsupervised learning
In this experiment, we examine how much gain is
possible by re-training the word representation of
verbs using the unsupervised algorithm described
in Section 4. We focus on the additive model
of Compositional NLM, both basic and prototype
projection. The initial word representation is from
136
model original representation re-trained
C-NLM 0.31 0.38
CoC-NLM 0.44 ? 0.47
Table 4: Results of re-training the word representation
for C-NLM and CoC-NLM. Learning rate ? = 0.01,
regularization ? = 10?4 and iteration = 20. One iteration
is one run through the dataset of 5000 verb-object pairs
which we made from the ukWaC corpus.
NLM. Table 4 shows the gains in correlation score.
This result shows that our learning model suc-
cessfully captures good representation within co-
compositionality of additive model. In contrast to
other previous compositional models, our model
does not require estimating a large number of pa-
rameters for computation of compositional vectors
and word representation itself is more suitable for
it. Furthermore, learning is very fast, taking about
10 minutes for C-NLM on a standard machine with
Intel Core i7 2.93Ghz CPU and 8GB of RAM.
6.3 The number of prototype words
The number of prototype words (m in Figure 1) we
use to generate the prototype space is one hyper-
parameter that our model has. Here, we analyze the
effect of the choice of m. Figure 5 shows the rela-
tion of m and the performance of co-compositional
model with prototype projections using either SDS
or NLM representations. In general, both NLM
and SDS show relatively smooth and flat curves
across m, indicating the relative robustness of the
approach. Nevertheless, results do degrade for large
m, due to increase in noise from non-prototype
words. Further, it does appear that NLM has a slow-
er drop in correlation with increasing m compared
with SDS. This suggests that NLM is more robust,
which is possibly attributable to the dense and low-
dimensional distributed features.
6.4 Variations in model configuration
We have presented a compositional model of the
form d(ws) + cocomp(wv, wo), where prototype
projections are performed on both wv and wo and
ws is composed as is without projection. In general,
we have the freedom to choose what to project
and what not to project under this co-compositional
framework. Here in Table 5 we show the results of
Figure 5: The relation between the number of prototype
words and correlation of SDS or NLM. In general, NLM
has higher correlation than SDS and is more robust across
the m.
Subj Verb Obj NLM ? SDS ?
prpj prpj prpj 0.39 0.37
+ prpj prpj 0.44 0.41
prpj prpj 0.45 0.41
+ prpj + 0.43 0.38
prpj + 0.43 0.38
+ + + 0.31 0.31
Table 5: Variants of the full co-compositional model,
based on how subject, verb, and object vector repre-
sentations are included. prpj indicates that prototype
projection is used. + indicates that the vector is added
without projection first. Blank indicates that the vector is
not used in the final compositional score.
these variants, using f =Addition and SDS/NLM
representations without re-training. We note that
our positive results mainly come from the verb
projections. Subject information actually does not
help. We believe this best configuration is task-
dependent; in this test collection, the subjects appear
to have little contribution to the landmark verb.
7 Related work
In recent years, several sophisticated vector space
models have been proposed for computing compo-
sitional semantics. Mitchell and Lapata (2010), Erk
(2012) and Baroni et al (2013) are recommended
survey papers.
137
One of the first approaches is the vector ad-
dition/multiplication idea of Mitchell and Lapata
(2008). The appeal of this kind of simple approach
is its intuitive geometric interpretation and its ro-
bustness to various datasets. However, it may not
be sufficiently expressive to represent the various
factors involved in compositional semantics, such
as syntax and context. To this end, Baroni and
Zamparelli (2010) present a compositional model
for adjectives and nouns. In their model, an adjective
is a matrix operator that modifies the noun vector
into an adjective-noun vector. Zanzotto et al (2010)
and Guevara (2010) also proposed linear transfor-
mation models for composition and address the issue
of estimating large matrices with least squares or
regression techniques. Socher et al (2012) extend
this linear transformation approach with the more
powerful model of Matrix-Vector Recursive Neural
Networks (MV-RNN). Each node in a parse tree is
assigned both a vector and a matrix. The vector
captures the actual meaning of the word itself, while
the matrix is modeled as a operator that modify the
meaning of neighboring words and phrases. This
model captures semantic change phenomenon like
not bad is similar to good due to a composition
of the bad vector with a meaning-flipping not ma-
trix. But this MV-RNN also need to optimize all
matrices of words from initial value (identity plus
a small amount of Gaussian noise) with supervised
dataset like movie reviews. Our prototype projection
model is similar to these models as a matrix-vector
operation, except that the matrix is not learned and
computed from prototype words. In future work,
we can imagine integrating the two models, using
these prototype projection matrices as initial values
for MV-RNN training (Socher et al, 2012).
Another approach is exemplified by Coecke et
al. (2010). In their mathematical framework u-
nifying categorical logic and vector space models,
the sentence vector is modeled as a function of the
Kronecker product of its word vectors. Grefenstette
and Sadrzadeh (2011) implement this based on un-
supervised learning of matrices for relational words
and apply them to the vectors of their arguments.
Their idea is that words with relational types, such as
verbs, adjectives, and adverbs are matrices that act
as a filter on their arguments. They also developed
a new semantic similarity task based on transitive
Composition Operator Parameter
Add: w1u + w2v w1, w2 ? R
Multiply: uw1 ? vw2 w1, w2 ? R
FullAdd: W1u + W2v W1,W2 ? Rn?n
LexFunc: Auv Au ? Rn?n
FullLex: ?([W1Auv,W2Avu])
?
Au, Av ? Rn?n
W1,W2 ? Rn?n
Ours (Add): P(R,v)u + P(R,u)v SVD?s (m, k)
Ours (Mult): P(R,v)u? P(R,u)v SVD?s (m, k)
Table 6: Comparison of composition operators that com-
bine two word vector representations, u, v ? Rn and
their learning parameters. Our model only needs two
hyper-parameters: the number of prototype words m and
dimensional reduction k in SVD
verbs, which is the dataset we used here. The pre-
vious state-of-the-art result for this task comes from
the model of Van de Cruys et al (2013). They model
compositionality as a multi-way interaction between
latent factors, which are automatically constructed
from corpus data via matrix factorization.
Comprehensive evaluation of various existing
models are reported in (Blacoe and Lapata, 2012; D-
inu et al, 2013). Blacoe and Lapata (2012) highlight
the importance of jointly examining word represen-
tations and compositionality operators. However,
two out of three composition methods they evaluate
are parameter-free, so that they can side-step the
issue of parameter estimation. Dinu et al (2013) de-
scribe the relation between word vector and compo-
sitionality in more detail with free parameters. Table
6 summarizes some ways to compose the meaning
of two word vectors (u, v), following (Dinu et al,
2013). These range from simple operators (e.g. Add
and Multiply) to expressive models with many free
parameters (e.g. LexFunc, FullLex). Many of these
models need to optimize n ? n parameters, which
may be large. On the other hand, our model only
needs two hyper-parameters: the number of proto-
type words m and dimensional reduction k in SVD
(Table 6). Furthermore, our model performance with
neural language model word embeddings is robust to
variations in m.
Most closely related to our work is the work by
Erk and Pado? (2008; 2009) and Thater et al (2010;
2011), which falls under the research theme of
computing word meaning in context. Both methods
are characterized by the use of selectional prefer-
138
ence information for subjects, verbs, and objects in
context; our prototype word vectors are essentially
equivalent to this idea. The main difference is in
how we modify the target word representation v
using this information: whereas we project v onto
a latent subspace formed by collection of prototype
vectors, Erk and Pado? (2008; 2009) and Thater
et al (2010; 2011) use the prototype vectors to
directly modify the elements of v, i.e. by element-
wise product with the centroid prototype vector.
Intuitively, both our method and theirs essentially
delete part of a word vector representation to adapt
the meaning in context. We believe the projection
is more robust to the underlying word representation
(and this is shown in the results for SDS vs. NLM
representations), but we note that we may be able
to borrow some of more sophisticated ways to find
prototype vectors from Erk and Pado? (2008; 2009)
and Thater et al (2010; 2011).
8 Conclusion and Future Work
We began this work by asking how it is possible to
handle polysemy issues in compositional semantics,
especially when adopting distributional semantics
methods that construct only one representation per
word type. After all, the different senses of the
same word are all conflated into a single vector
representation. We found our inspiration in Gen-
erative Lexicon Theory (Pustejovsky, 1995), where
ambiguity is resolved due to co-compositionality of
the words in the sentence, i.e., the meaning of an
ambiguous verb is generated by the properties the
object it takes, and vice versa. We implement this
idea in a novel neural network model using proto-
type projections. The advantages of this model is
that it is robust to the underlying word representation
used and that it enables an effective joint learning
of word representations. The model achieves the
current state-of-the-art performance (? = 0.47)
on the semantic similarity task of transitive verbs
(Grefenstette and Sadrzadeh, 2011).
Directions for future research include:
? Experiments on other semantics tasks, such
as paraphrase detection, word sense induction,
and word meaning in context.
? Extension to more holistic sentence-level com-
position using a matrix-vector recursive frame-
work like (Socher et al, 2012).
? Explore further the potential synergy between
Distributional Semantics and the Generative
Lexicon.
Acknowledgments
This work was partially supported by JSPS KAK-
ENHI Grant Number 24800041, JSPS KAKENHI
2430057 and Microsoft Research CORE Project.
We would like to thank Hiroyuki Shindo and anony-
mous reviewers for their helpful comments.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language resources and evaluation,
43(3):209?226.
Marco Baroni, Raffaella Bernardi, and Roberto Zampar-
elli. 2013. Frege in space: A program for compo-
sitional distributional semantics. Linguistic Issues in
Language Technologies.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
William Blacoe andMirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a composi-
tional distributional model of meaning. CoRR, ab-
s/1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
139
Journal of Machine Learning Research, 12:2493?
2537.
Georgiana Dinu, Nghia The Pham, and Marco Barori.
2013. General estimation and evaluation of composi-
tional distributional semantic models. In Proceedings
of the Workshop on Continuous Vector Space Models
and their Compositionality.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language Semantics.
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635?653.
G Frege. 1892. U?ber sinn und bedeutung. In Zeitschfrift
fu?r Philosophie und philosophische Kritik, 100.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the Workshop on GEomet-
rical Models of Natural Language Semantics.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In Human Language Technologies:
The Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT).
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1439.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the International
Conference on Language Resources and Evaluation
(LREC).
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, MA.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Human Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT).
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and
effective vector model. In Asian Federation of Natural
Language Processing (IJCNLP).
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of artificial intelligence research, 37(1):141?
188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.
2013. A tensor-based factorization model of semantic
compositionality. In Human Language Technologies:
The Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT).
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar.
2010. Estimating linear models for compositional
distributional semantics. In Proceedings of
the International Conference on Computational
Linguistics (COLING).
140
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 154?158,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Dependency Parsers with Supertags
Hiroki Ouchi Kevin Duh Yuji Matsumoto
Computational Linguistics Laboratory
Nara Institute of Science and Technology
{ouchi.hiroki.nt6, kevinduh, matsu}@is.naist.jp
Abstract
Transition-based dependency parsing sys-
tems can utilize rich feature representa-
tions. However, in practice, features are
generally limited to combinations of lexi-
cal tokens and part-of-speech tags. In this
paper, we investigate richer features based
on supertags, which represent lexical tem-
plates extracted from dependency struc-
ture annotated corpus. First, we develop
two types of supertags that encode infor-
mation about head position and depen-
dency relations in different levels of granu-
larity. Then, we propose a transition-based
dependency parser that incorporates the
predictions from a CRF-based supertagger
as new features. On standard English Penn
Treebank corpus, we show that our su-
pertag features achieve parsing improve-
ments of 1.3% in unlabeled attachment,
2.07% root attachment, and 3.94% in com-
plete tree accuracy.
1 Introduction
One significant advantage of transition-based de-
pendency parsing (Yamada and Matsumoto, 2003;
Nivre et al, 2007, Goldberg and Elhadad, 2010;
Huang and Sagae, 2010) is that they can utilize
rich feature representations. However, in prac-
tice, current state-of-the-art parsers generally uti-
lize only features that are based on lexical tokens
and part-of-speech (POS) tags. In this paper, we
argue that more complex features that capture fine-
grained syntactic phenomenon and long-distance
dependencies represent a simple and effective way
to improve transition-based dependency parsers.
We focus on defining supertags for English de-
pendency parsing. Supertags, which are lexical
templates extracted from dependency structure an-
notated corpus, encode linguistically rich infor-
mation that imposes complex constraints in a lo-
cal context (Bangalore and Joshi, 1999). While
supertags have been used in frameworks based
on lexicalized grammars, e.g. Lexicalized Tree-
Adjoining Grammar (LTAG), Head-driven Phrase
Structure Grammar (HPSG) and Combinatory
Categorial Grammar (CCG), they have scarcely
been utilized for dependency parsing so far.
Previous work by Foth et al (2006) demon-
strate that supertags improve German dependency
parsing under a Weighted Constraint Dependency
Grammar (WCDG). Recent work by Ambati et al
(2013) show that supertags based on CCG lexi-
con improves transition-based dependency parsing
for Hindi. In particular, they argue that supertags
can improve long distance dependencies (e.g. co-
ordination, relative clause) in a morphologically-
rich free-word-order language. Zhang et. al.
(2010) define supertags that incorporate that long-
distance dependency information for the purpose
of HPSG parsing. All these works suggest
the promising synergy between dependency pars-
ing and supertagging. Our main contributions
are: (1) an investigation of supertags that work
well for English dependency parsing, and (2) a
novel transition-based parser that effectively uti-
lizes such supertag features.
In the following, we first describe our supertag
design (Section 2) and parser (Section 3). Su-
pertagging and parsing experiments on the Penn
Treebank (Marcus et al., 1993) are shown in Sec-
tion 4. We show that using automatically predicted
supertags, our parser can achieve improvements of
1.3% in unlabeled attachment, 2.07% root attach-
ment, and 3.94% in complete tree accuracy.
2 Supertag Design
The main challenge with designing supertags is
finding the right balance between granularity and
predictability. Ideally, we would like to increase
the granularity of the supertags in order capture
154
Figure 1: Example sentence
Word Model 1 Model 2
No VMOD/R VMOD/R
, P/R P/R
it SUB/R SUB/R
was ROOT+L R ROOT+SUB/L PRD/R
n?t VMOD/L VMOD/L
Black NMOD/R NMOD/R
Monday PRD/L+L PRD/L+L
. P/L P/L
Table 1: Model 1 & 2 supertags for Fig. 1.
more fine-grained syntactic information, but large
tagsets tend to be more difficult to predict auto-
matically. We describe two supertag designs with
different levels of granularity in the following, fo-
cusing on incorporating syntactic features that we
believe are important for dependency parsing.
For easy exposition, consider the example sen-
tence in Figure 1. Our first supertag design, Model
1, represents syntactic information that shows the
relative position (direction) of the head of a word,
such as left (L) or right (R). If a word has root as its
head, we consider it as no direction. In addition,
dependency relation labels of heads are added. For
instance, ?No? in the example in Figure 1 has its
head in the right direction with a label ?VMOD?,
so its supertag can be represented as ?VMOD/R?.
This kind of information essentially provides clues
about the role of the word in sentence.
On top of this, we also add information about
whether a word has any left or right dependents.
For instance, the word ?Monday? has a left de-
pendent ?Black?, so we encode it as ?PRD/L+L?,
where the part before ?+? specifies the head in-
formation (?PRD/L?) and the part afterwards (?L?)
specifies the position of the dependent (?L? for left,
?R? for right). When a word has its dependents
in both left and right directions, such as the word
?was? in Figure 1, we combine them using ? ?,
as in: ?ROOT+L R?. On our Penn Treebank data,
Model 1 has 79 supertags.
unigrams of supertags
for p in p
i?2
, p
i?1
, p
i
, p
i+1
,
p
i+2
, p
i+3
w
p
s
p
, t
p
s
p
bigrams of supertags
for p, q in (p
i
, p
i+1
),
(p
i
, p
i+2
), (p
i?1
, p
i
), (p
i?1
,
p
i+2
), (p
i+1
, p
i+2
)
s
p
s
q
, t
p
s
q
, s
p
t
q
,
w
p
s
q
, s
p
w
q
head-dependent of supertags
for p, q in (p
i
, p
i+1
),
(p
i
, p
i+2
), (p
i?1
, p
i
), (p
i?1
,
p
i+2
), (p
i+1
, p
i+2
)
w
p
sh
p
w
q
sld
q
,
t
p
sh
p
t
q
sld
q
,
w
p
srd
p
w
q
sh
q
,
t
p
srd
p
t
q
sh
q
Table 2: Proposed supertag feature templates.
w = word; t = POS-tag; s = supertag; sh = head part
of supertag; sld = left dependent part of supertag;
srd = right dependent part of supertag
In Model 2, we further add dependency relation
labels of obligatory dependents of verbs. Here we
define obligatory dependents of verbs as depen-
dents which have the following dependency rela-
tion labels, ?SUB?, ?OBJ?, ?PRD? and ?VC?. If a
label of a dependent is not any of the obligatory
dependent labels, the supertag encodes only the
information of direction of the dependents (same
as Model 1). For instance, ?was? in the exam-
ple sentence has an obligatory dependent with a
label ?SUB? in the left direction and ?PRD? in
the right direction, so its supertag is represented
as ?ROOT+SUB/L PRD/R?. If a verb has multi-
ple obligatory dependents in the same direction,
its supertag encodes them in sequence; if a verb
takes a subject and two objects, we may have
?X/X+SUB/L OBJ/R OBJ/R?. The number of su-
pertags of Model 2 is 312.
Our Model 2 is similar to Model F of Foth et
al. (2006) except that they define objects of prepo-
sitions and conjunctions as obligatory as well as
verbs. However, we define only dependents of
verbs because verbs play the most important role
for constructing syntactic trees and we would like
to decrease the number of supertags.
3 Supertags as Features in a
Transition-based Dependency Parser
In this work, we adopt the Easy-First parser of
(Goldberg and Elhadad, 2010), a highly-accurate
transition-based dependency parser. We describe
how we incorporate supertag features in the Easy-
First framework, though it can be done similarly
155
for other transition-based frameworks like left-to-
right arc-eager and arc-standard models (Nivre et
al., 2006; Yamada and Matsumoto, 2003).
In the Easy-First algorithm, a dependency tree
is constructed by two kinds of actions: ATTACH-
LEFT(i) and ATTACHRIGHT(i) to a list of par-
tial tree structures p
1
,...,p
k
initialized with the n
words of the sentence w
1
,...,w
n
. ATTACHLEFT(i)
attaches (p
i
, p
+1
) and removes p
i+1
from the par-
tial tree list. ATTACHRIGHT(i) attaches (p
i+1
, p
i
)
and removes p
i
from the partial tree list. Features
are extracted from the attachment point as well as
two neighboring structures: p
i?2
, p
i?1
, p
i
, p
i+1
,
p
i+2
, p
i+3
. Table 2 summarizes the supertag fea-
tures we extract from this neighborhood; these are
appended to the original baseline features based
on POS/word in Goldberg and Elhadad (2010).
For a partial tree structure p, features are de-
fined based on information in its head: we use
w
p
to refer to the surface word form of the head
word of p, t
p
to refer to the head word?s POS
tag, and s
p
to refer to the head word?s supertag.
Further, we not only use a supertag as is, but
split each supertag into subparts. For instance,
the supertag ?ROOT+SUB/L PRD/R? is split into
?ROOT?, ?SUB/L? and ?PRD/R?, a supertag rep-
resenting the supertag head information sh
p
, su-
pertag left dependent information sld
p
, and su-
pertag right dependent information srd
p
.
For the unigram features, we use information
within a single partial structure, such as conjunc-
tion of head word and its supertag (w
p
s
p
), con-
junction of head word?s POS tag and its supertag
(t
p
s
p
). To consider more context, bigram features
look at pairs of partial structures. For each (p, q)
pair of structures in p
i?2
, p
i?1
, p
i
, p
i+1
, p
i+2
, we
look at e.g. conjunctions of supertags (s
p
s
q
).
Finally, head information of a partial struc-
ture and dependent information of another partial
structure are combined as ?head-dependent fea-
tures? in order to check for consistency in head-
dependent relations. For instance, in Table 1
the supertag for the word ?Black? has head part
?NMOD/R? wanting to attach right and the su-
pertag for the word ?Monday? has dependent part
?L? wanting something to the left; they are likely
to be attached by our parser because of the consis-
tency in head-dependent direction. These features
are used in conjunction with word and POS-tag.
Model # tags Dev Test
Model1 79 87.81 88.12
Model2 312 87.22 87.13
Table 3: Supertag accuracy evaluated on develop-
ment and test set. Dev = development set, PTB 22;
Test = test set, PTB 23
4 Experiments
To evaluate the effectiveness of supertags as fea-
tures, we perform experiments on the Penn Tree-
bank (PTB), converted into dependency format
with Penn2Malt
1
. Adopting standard approach,
we split PTB sections 2-21 for training, section 22
for development and 23 for testing. We assigned
POS-tags to the training data by ten-fold jackknif-
ing following Huang and Sagae (2010). Develop-
ment and test sets are automatically tagged by the
tagger trained on the training set.
4.1 Supertagging Experiments
We use the training data set to train a supertagger
of each model using Conditional Random Fields
(CRF) and the test data set to evaluate the accu-
racies. We use version 0.12 of CRFsuite
2
for our
CRF implementation. First-order transitions, and
word/POS of uni, bi and trigrams in a 7-word win-
dow surrounding the target word are used as fea-
tures. Table 3 shows the result of the supertagging
accuracies. The supertag accuracies are around
87-88% for both models, suggesting that most of
the supertags can be effectively learned by stan-
dard CRFs. The tagger takes 0.001 and 0.005 sec-
ond per sentence for Model 1 and 2 respectively.
In our error analysis, we find it is challeng-
ing to assign correct supertags for obligatory
dependents of Model 2. In the test set, the
number of the supertags encoding obligatory de-
pendents is 5432 and its accuracy is 74.61%
(The accuracy of the corresponding supertags in
Model 1 is 82.18%). Among them, it is es-
pecially difficult to predict the supertags encod-
ing obligatory dependents with a head informa-
tion of subordination conjunction ?SBAR?, such as
?SBAR/L+SUB/L PRD/R?. The accuracy of such
supertags is around 60% (e.g., the accuracy of
a supertag ?SBAR/L+SUB/L PRD/R? is 57.78%),
while the supertags encoding dependents with a la-
1
http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.jar
2
http://www.chokkan.org/software/crfsuite/
156
feature Model1 Model2
baseline 90.25 90.25
+unigram of supertag 90.59 90.76
+bigram of supertag 91.37 91.08
+head-dependent 91.22 91.28
Table 4: Unlabeled attachment scores (UAS) on
the development set for each feature template.
Model UAS Root Complete
baseline 90.05 91.10 37.41
Model 1 91.35 93.17 41.35
Model 2 91.23 92.72 41.35
Table 5: Accuracies for English dependency pars-
ing on the test set. UAS = unlabeled attachment
score; Root = root attachment score; Complete =
the percentage of sentences in which all tokens
were assigned their correct heads.
bel ?VC? are assigned almost correctly (e.g., the
accuracy of ?VC/L+VC/R? is 97.41%). A verb
within a subordinating clause usually has the sub-
ordinating conjunction as its head and it tends
to be long-range dependency, which is harder to
predict. ?VC? represents verb complements. A
gerund and a past participle is often a dependent
of the immediate front verb, so it is not so difficult
to identify the dependency relation.
4.2 Dependency Parsing Experiments
First, we evaluate the effectiveness of the feature
templates proposed in Section 3. Following the
same procedure as our POS tagger, we first assign
supertags to the training data by ten-fold jackknif-
ing, then train our Easy-First dependency parser
on these predicted supertags. For development and
test sets, we assign supertags based on a supertag-
ger trained on the whole training data.
Table 4 shows the effect of new supertag fea-
tures on the development data. We start with the
baseline features, and incrementally add the uni-
grams, bigrams, and head-dependent feature tem-
plates. For Model 1 we observe that adding uni-
gram features improve the baseline UAS slightly
by 0.34% while additionally adding bigram fea-
tures give larger improvements of 0.78%. On the
other hand, for Model 2 unigram features make
bigger contribution on improvements by 0.51%
than bigram ones 0.32%. One possible expla-
nation is that because each supertag of Model 2
encodes richer syntactic information, an individ-
ual tag can make bigger contribution on improve-
ments than Model 1 as a unigram feature. How-
ever, since supertags of Model 2 can be erroneous
and noisy combination of multiple supertags, such
as bigram features, can propagate errors.
Using all features, the accuracy of the accu-
racy of Model 2 improved further by 0.20%, while
Model 1 dropped by 0.15%. It is unclear why
Model 1 accuracy dropped, but one hypothesis is
that coarse-grained supertags may conflate some
head-dependent. The development set UAS for
combinations of all features are 91.22% (Model 1)
and 91.28% (Model 2), corresponding to 0.97%
and 1.03% improvement over the baseline.
Next, we show the parsing accuracies on the
test set, using all unigram, bigram, and head-
dependents supertag features. The UAS
3
, Root
attachment scores, and Complete accuracy are
shown in Table 5. Both Model 1 and 2 outperform
the baseline in all metrics. UAS improvements
for both models are statistically significant under
the McNemar test, p < 0.05 (difference between
Model 1 and 2 is not significant). Notably, Model
1 achieves parsing improvements of 1.3% in un-
labeled attachment, 2.07% root attachment, and
3.94% in complete accuracy. Comparing Model
1 to baseline, attachment improvements binned by
distance to head are as follows: +0.54 F1 for dis-
tance 1, +0.81 for distance 2, +2.02 for distance
3 to 6, +2.95 for distance 7 or more, implying su-
pertags are helpful for long distance dependencies.
5 Conclusions
We have demonstrated the effectiveness of su-
pertags as features for English transition-based de-
pendency parsing. In previous work, syntactic in-
formation, such as a head and dependents of a
word, cannot be used as features before partial tree
structures are constructed (Zhang and Nivre, 2011;
Goldberg and Elhadad, 2010). By using supertags
as features, we can utilize fine-grained syntactic
information without waiting for partial trees to be
built, and they contribute to improvement of ac-
curacies of English dependency parsing. In future
work, we would like to develop parsers that di-
rectly integrate supertag ambiguity in the parsing
decision, and to investigate automatic pattern min-
ing approaches to supertag design.
3
For comparison, MaltParser and MSTParser with base-
line features is 88.68% and 91.37% UAS respectively
157
References
Bharat R Ambati, Tejaswini Deoskar and Mark Steed-
man. 2013. Using CCG categories to improve Hindi
dependency parsing. In Proceedings of ACL, pages
604-609, Sofia, Bulgaria, August.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
puational Linguistics, 25(2):237-265.
Kilian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a Constraint Dependency Parser with Su-
pertags. In Proceedings of COLING/ACL 2006,
pages 289-296, Sydney, Australia, July.
Yoav Goldberg and Michael Elhadad. 2010. An Effi-
cient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Proceedings of HLT/NAACL,
pages 742-750, Los Angeles, California, June.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL, pages 1077-1086, Uppsala,
Sweden, July.
Mitchell. P. Marcus, Beatrice Santorini and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313-330
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryi?git, Sandra K?ubler, Svetoslav,
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95-135
Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsen Eryi?git
and Svetoslav, Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of CoNLL, pages 221-
225, New York, USA.
N Okazaki. 2007. CRFsuite: a fast imple-
mentation of Conditional Random Fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proceedings of IWPT, Nancy, France.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Feaures.
In Proceedings of ACL, pages 188-193, Porland,
Oregon, June.
Yao-zhong Zhang, Takuya Matsuzaki and Jun?ichi Tsu-
jii. 2010. A Simple Approach for HPSG Supertag-
ging Using Dependency Information. In Proceed-
ings of HLT/NAACL, pages 645-648, Los Angeles,
California, June.
158
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 190?194,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Analysis and Prediction of Unalignable Words in Parallel Text
Frances Yung Kevin Duh
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192 Japan
pikyufrances-y|kevinduh|matsu@is.naist.jp
Yuji Matsumoto
Abstract
Professional human translators usually do
not employ the concept of word align-
ments, producing translations ?sense-for-
sense? instead of ?word-for-word?. This
suggests that unalignable words may be
prevalent in the parallel text used for ma-
chine translation (MT). We analyze this
phenomenon in-depth for Chinese-English
translation. We further propose a sim-
ple and effective method to improve au-
tomatic word alignment by pre-removing
unalignable words, and show improve-
ments on hierarchical MT systems in both
translation directions.
1 Motivation
It is generally acknowledged that absolute equiva-
lence between two languages is impossible, since
concept lexicalization varies across languages.
Major translation theories thus argue that texts
should be translated ?sense-for-sense? instead of
?word-for-word? (Nida, 1964). This suggests that
unalignable words may be an issue for the parallel
text used to train current statistical machine trans-
lation (SMT) systems. Although existing auto-
matic word alignment methods have some mech-
anism to handle the lack of exact word-for-word
alignment (e.g. null probabilities, fertility in the
IBM models (Brown et al., 1993)), they may be
too coarse-grained to model the ?sense-for-sense?
translations created by professional human trans-
lators.
For example, the Chinese term ?tai-yang? liter-
ally means ?sun?, yet the concept it represents is
equivalent to the English term ?the sun?. Since the
concept of a definite article is not incorporated in
the morphology of ?tai yang?, the added ?the? is
not aligned to any Chinese word. Yet in another
context like ?the man?, ?the? can be the translation
of the Chinese demonstrative pronoun ?na?, liter-
ally means ?that?. A potential misunderstanding is
that unalignable words are simply function words;
but from the above example, we see that whether a
word is alignable depends very much on the con-
cept and the linguistic context.
As the quantity and quality of professionally-
created parallel text increase, we believe there is a
need to examine the question of unalignable words
in-depth. Our goal is to gain a better understand-
ing of what makes a fluent human translation and
use this insight to build better word aligners and
MT systems. Our contributions are two-fold:
1) We analyze 13000 sentences of manually word-
aligned Chinese-English parallel text, quantifying
the characteristics of unalignable words.
2) We propose a simple and effective way to im-
prove automatic word alignment, based on pre-
dicting unalignable words and temporarily remov-
ing them during the alignment training procedure.
2 Analysis of Unalignable Words
Our manually-aligned data, which we call OR-
ACLE data, is a Chinese-to-English corpus re-
leased by the LDC (Li et al., 2010)
1
. It con-
sists of ?13000 Chinese sentences from news and
blog domains and their English translation . En-
glish words are manually aligned with the Chinese
characters. Characters without an exact counter-
part are annotated with categories that state the
functions of the words. These characters are ei-
ther aligned to ?NULL?, or attached to their depen-
dency heads, if any, and aligned together to form
a multi-word alignment. For example, ?the? is an-
notated as [DET], for ?determiner?, and aligned to
?tai-yang? together with ?sun?.
In this work, any English word or Chinese char-
acter without an exact counterpart are called un-
alignable words, since they are not core to the
1
LDC2012T16, LDC2012T20 and LDC2012T24
190
word unalignable core
types tokens tokens
core or 3581 146,693 562,801
unalignable (12%) (17%) (66%)
always 25320 / 147,373
core (88%) (17%)
Table 1: Number of core and unalignable words in
hand aligned ORACLE corpus
multi-word alignment. All other English words or
Chinese characters are referred to as core words.
2.1 What kind of words are unalignable?
Analyzing the hand aligned corpus, we find that
words annotated as unalignable do not come from
a distinct list. Table 1 reveals that 88% of the
word types are unambiguously core words. Yet
these word types, including singletons, account
for only 17% of the word tokens. On the other
hand, another 17% of the total word tokens are
annotated as unalignable. So, most word types are
possibly unalignable but only in a small portion of
their occurrence, such as the following examples:
(1a) Chi: yi ge di fang
one (measure word) place
Eng: one place
(1b) Chi: ge ren
personal
Eng: personal
(2a) Chi: ming tian zhong wu
(tomorrow) (midday)
Eng: tomorrow at midday
(2b) Chi: zai jia
at/in/on home
Eng: at home
In example (1a), ?ge? is a measure word that is
exclusive in Chinese, but in (1b), it is part of the
multiword unit ?ge-ren? for ?personal?. Similarly,
prepositions, such as ?at?, can either be omitted or
translated depending on context.
Nonetheless, unalignable words are by no
means evenly distributed among word types. Ta-
ble 2 shows that the top 100 most frequent un-
alignable word types already covers 78% and 94%
of all Chinese and English unalignable instances,
respectively. Word type is thus an important clue.
Intuitively, words with POS defined only in one
of the languages are likely to be unalignable. To
examine this, we automatically tagged the ORA-
CLE data using the Standford Tagger (Toutanova
Most frequent Token count
unalignable word types Chinese English
Top 50 34,987 83,905
(68%) (88%)
Top 100 40,121 89,609
(78%) (94%)
Table 2: Count of unalignable words by types
et al., 2003). We find that the unalignable words
include all POS categories of either language,
though indeed some POS are more frequent. Ta-
ble 3 lists the top 5 POS categories that most un-
alignable words belong to and the percentage they
are annotated as unalignable. Some POS cate-
gories like DEG are mostly unalignable regardless
of context, but other POS tags such as DT and IN
depend on context.
Chi. No. and % of Eng. No. and % of
POS unalign. POS unalign.
DEG 7411(97%) DT 27715 (75%)
NN 6138 (4%) IN 19303 (47%)
AD 6068 (17%) PRP 5780 (56%)
DEC 5572 (97%) TO 5407 (62%)
VV 4950 (6%) CC 4145 (36%)
Table 3: Top 5 POS categories of Chinese and En-
glish unalignable words
Note also that many Chinese unalignable words
are nouns (NN) and verbs (VV). Clearly we cannot
indiscriminately consider all nouns as unalignable.
Some examples of unalignable content words in
Chinese are:
(3) Chi: can jia hui jian huo dong
participate meeting activity
Eng: participate in the meeting
(4) Chi: hui yi de yuan man ju xing
meeting ?s successful take place
Eng: success of the meeting
English verbs and adjectives are often nomi-
nalized to abstract nouns (such as ?meeting? from
?meet?, or ?success? from ?succeed?), but such
derivation is rare in Chinese morphology. Since
POS is not morphologically marked in Chinese,
?meeting? and?meet? are the same word. To reduce
the processing ambiguity and produce more nat-
ural translation, extra content words are added to
mark the nominalization of abstract concepts. For
example, ?hui jian? is originally ?to meet?. Adding
?huo dong?(activity) transforms it to a noun phrase
191
(example 3), similar to the the addition of ?ju
sing?(take place) to the adjective ?yuan man? (ex-
ample 4). These unalignable words are not lexi-
cally dependent but are inferred from the context,
and thus do not align to any source words.
To summarize, a small number of word types
cover 17% of word tokens that are unalignable,
but whether these words are unalignable depends
significantly on context. Although there is no list
of ?always unalignable? words types or POS cat-
egories, our analysis shows there are regularities
that may be exploited by an automatic classifier.
3 Improved Automatic Word Alignment
We first propose a classifier for predicting whether
a word is unalignable. Let (e
J
1
, f
K
1
) be a pair of
sentence with length J and K. For each word in
(e
J
1
, f
K
1
) that belongs to a predefined list
2
of po-
tentially unalignable words, we run a binary clas-
sifier. A separate classifier is built for each word
type in the list, and an additional classifier for all
the remaining words in each language.
We train an SVM classifier based on the fol-
lowing features: Local context: Unigrams and
POS in window sizes of 1, 3, 5, 7 around the
word in question. Top token-POS pairs: This
feature is defined by whether the token in ques-
tion and its POS tag is within the top n frequent
token-POS pairs annotated as unalignable like in
Tables 2 and 3. Four features are defined with n =
10, 30, 50, 100. Since the top frequent unalignable
words cover most of the counts as shown in the
previous analysis, being in the top n list is a strong
positive features. Number of likely unalignable
words per sentence: We hypothesize that the
translator will not add too many tokens to the
translation and delete too many from the source
sentence. In the ORACLE data, 68% sentences
have more than 2 unalignable words. We approx-
imate the number of likely unalignable words in
the sentence by counting the number of words
within the top 100 token-POS pairs annotated as
unalignable. Sentence length and ratio: Longer
sentences are more likely to contain unalignable
words than shorter sentences. Also sentence ra-
tios that deviate significantly from the mean are
likely to contain unalignable words. Presence of
alignment candidate: This is a negative feature
defined by whether there is an alignment candi-
2
We define the list as the top 100 word types with the
highest count of unalignable words per language according
to the hand annotated data.
date in the target sentence for the source word in
question, or vice versa. The candidates are ex-
tracted from the top n frequent words aligned to
a particular word according to the manual align-
ments of the ORACLE data. Five features are de-
fined with n = 5, 10, 20, 50, 100 and one ?without
limit?, such that a more possible candidate will be
detected by more features.
Next, we propose a simple yet effective mod-
ification to the word alignment training pipeline:
1. Predict unalignable words by the classifier
2. Remove these words from the training corpus
3. Train word alignment model (e.g. GIZA++)
3
4. Combine the word alignments in both direc-
tions with heuristics (grow-diag-final-and)
5. Restore unaligned words to original position
6. Continue with rule extraction and the rest of
the MT pipeline.
The idea is to reduce the difficulty for the word
alignment model by removing unaligned words.
4 End-to-End Translation Experiments
In our experiments, we first show that removing
manually-annotated unaligned words in ORACLE
data leads to improvements in MT of both trans-
lation directions. Next, we show how a classifier
trained on ORACLE data can be used to improve
MT in another large-scale un-annotated dataset.
4
4.1 Experiments on ORACLE data
We first performed an ORACLE experiment us-
ing gold standard unaligned word labels. Follow-
ing the training pipeline in Section 3, we removed
gold unalignable words before running GIZA++
and restore them afterwards. 90% of the data is
used for alignment and MT training, while 10% of
the data is reserved for testing.
The upper half of Table 4 list the alignment
precision, recall and F1 of the resulting align-
ments, and quality of the final MT outputs. Base-
line is the standard MT training pipeline with-
out removal of unaligned words. Our Proposed
approach performs better in alignment, phrase-
based (PBMT) and hierarchical (Hiero) systems.
The results, evaluated by BLEU, METEOR and
TER, support our hypothesis that removing gold
unalignable words helps improve word alignment
and the resulting SMT.
3
We can suppress the NULL probabilities of the model.
4
All experiments are done using standard settings for
Moses PBMT and Hiero with 4-gram LM and mslr-
bidirectional-fe reordering (Koehn et al., 2007). The clas-
sifier is trained using LIBSVM (Chang and Lin, 2011).
192
Align PBMT Hiero
acc. C-E E-C C-E E-C
ORACLE P .711 B 11.4 17.4 10.3 15.8
Baseline R .488 T 70.9 69.0 75.9 72.3
F1.579 M 21.8 23.9 21.08 23.7
ORACLE P .802 B 11.8
+
18.3
+
11.0
+
17.2
+
Proposed R .509 T 71.4
?
65.7
+
74.7
+
68.7
+
(gold) F1.623 M 22.1
+
24.1
+
22.0
+
24.0
+
REAL B 18.2 18.5 17.0 17.2
Baseline T 63.4 67.2 68.0 71.4
M 22.9 24.6 22.9 24.8
REAL B 18.6 18.5 17.6
+
18.1
+
Proposed T 63.8
?
66.5
+
67.6 69.7
+
(predict) M 23.2
+
24.5 23.4
+
24.7
Table 4: MT results of ORACLE and REAL ex-
periments. Highest score per metric is bolded.
{
+
/
?
} indicates statistically significant improve-
ment/degradation, p < 0.05. (P: precision; R: re-
call; B: BLEU; M: METEOR; T:TER)
For comparison, a naive classifier that labels
all top-30 token-POS combinations as unalignable
performs poorly as expected (PBMT BLEU: 9.87
in C-E direction). We also evaluated our proposed
classifier on this task: the accuracy is 92% and it
achieves BLEU of 11.55 for PBMT and 10.84 for
Hiero in C-E direction, which is between the re-
sults of gold-unalign and baseline.
4.2 Experiments on large-scale REAL data
We next performed a more realistic experiment:
the classifier trained on ORACLE data is used to
automatically label a large data, which is then used
to train a MT system. This REAL data consists of
parallel text from the NIST OpenMT2008.
5
MT
experiments are performed in both directions.
The lower half of Table 4 shows the perfor-
mance of the resulting MT systems. We observe
that our proposed approach is still able to improve
over the baseline. In particular, Hiero achieved
statistical significant improvements in BLEU and
METEOR.
6
Comparing to the results of PBMT,
this suggests our method may be most effective in
improving systems where rule extraction is sen-
5
We use the standard MT08 test sets; the training
data includes LDC2004T08, 2005E47, 2005T06, 2007T23,
2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15,
and 2010T03 (34M English words and 1.1M sentences).
Since we do not have access to all OpenMT data, e.g. FBIS,
our results may not be directly comparable to other systems
in the evaluation.
6
Interestingly, PBMT did better than Hiero in this setup.
Chinese English lexical translation
word Baseline only Propose only
xie (bring) him bringing
xing (form) and model
dan (but) it, the, they yet, nevertheless
pa (scare) that, are, be fears, worried
Table 5: Examples of translations exclusively
found in the top 15 lexical translation.
Figure 1: Classifier accuracy and MT results V.S.
proportion of ORACLE data
sitive to the underlying alignments, such as Hi-
ero and Syntax-based MT. Table 5 shows the lex-
ical translations for some rare Chinese words: the
baseline tends to incorrectly align these to func-
tion words (garbage collection), while the pro-
posed method?s translations are more reasonable.
To evaluate how much annotation is needed for
the classifier, we repeat experiments using differ-
ent proportions of the ORACLE data. Figure 1
shows training by 20% of the data (2600 sents.)
already leads to significant improvements (p <
0.05), which is a reasonable annotation effort.
5 Conclusion
We analyzed in-depth the phenomenon of un-
alignable words in parallel text, and show that
what is unalignable depends on the word?s concept
and context. We argue that this is not a trivial prob-
lem, but with an unalignable word classifier and
a simple modified MT training pipeline, we can
achieve small but significant gains in end-to-end
translation. In related work, the issue of dropped
pronouns (Chung and Gildea, 2010) and function
words (Setiawan et al., 2010; Nakazawa and Kuro-
hashi, 2012) have been found important in word
alignment, and (Fossum et al., 2008) showed that
syntax features are helpful for fixing alignments.
An interesting avenue of future work is to integrate
these ideas with ours, in particular by exploiting
syntax and viewing unalignable words as aligned
at a structure above the lexical level.
193
References
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm : a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(27).
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. Pro-
ceedings of the Conference on Empirical Methods
on Natural Language Processing.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. Pro-
ceedings of the Workshop on Statistical Machine
Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie M.
Strassel, and Kazuaki Maeda. 2010. Enriching
word alignment with linguistic tags. Proceedings
of International Conference on Language Resources
and Evaluation.
Toshiaki Nakazawa and Sado Kurohashi. 2012.
Alignment by bilingual generation and monolingual
derivation. Proceedings of the International Confer-
ence on Computational Linguistics.
Eugene A Nida. 1964. Toward a Science of Translat-
ing: with Special Reference to Principles and Pro-
cedures Involved in Bible Translating. BRILL.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.
194
Proceedings of NAACL-HLT 2013, pages 947?957,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Multi-Metric Optimization Using Ensemble Tuning
Baskaran Sankaran, Anoop Sarkar
Simon Fraser University
Burnaby BC. CANADA
{baskaran,anoop}@cs.sfu.ca
Kevin Duh
Nara Institute of Science & Technology
Ikoma, Nara. JAPAN
kevinduh@is.naist.jp
Abstract
This paper examines tuning for statistical ma-
chine translation (SMT) with respect to mul-
tiple evaluation metrics. We propose several
novel methods for tuning towards multiple ob-
jectives, including some based on ensemble
decoding methods. Pareto-optimality is a nat-
ural way to think about multi-metric optimiza-
tion (MMO) and our methods can effectively
combine several Pareto-optimal solutions, ob-
viating the need to choose one. Our best
performing ensemble tuning method is a new
algorithm for multi-metric optimization that
searches for Pareto-optimal ensemble models.
We study the effectiveness of our methods
through experiments on multiple as well as
single reference(s) datasets. Our experiments
show simultaneous gains across several met-
rics (BLEU, RIBES), without any significant
reduction in other metrics. This contrasts the
traditional tuning where gains are usually lim-
ited to a single metric. Our human evaluation
results confirm that in order to produce better
MT output, optimizing multiple metrics is bet-
ter than optimizing only one.
1 Introduction
Tuning algorithms are used to find the weights for a
statistical machine translation (MT) model by min-
imizing error with respect to a single MT evalua-
tion metric. The tuning process improves the per-
formance of an SMT system as measured by this
metric; with BLEU (Papineni et al, 2002) being
the most popular choice. Minimum error-rate train-
ing (MERT) (Och, 2003) was the first approach in
MT to directly optimize an evaluation metric. Sev-
eral alternatives now exist: MIRA (Watanabe et al,
2007; Chiang et al, 2008), PRO (Hopkins and May,
2011), linear regression (Bazrafshan et al, 2012)
and ORO (Watanabe, 2012) among others.
However these approaches optimize towards the
best score as reported by a single evaluation met-
ric. MT system developers typically use BLEU and
ignore all the other metrics. This is done despite
the fact that other metrics model wide-ranging as-
pects of translation: from measuring the translation
edit rate (TER) in matching a translation output to a
human reference (Snover et al, 2006), to capturing
lexical choices in translation as in METEOR (Lavie
and Denkowski, 2009) to modelling semantic simi-
larity through textual entailment (Pado? et al, 2009)
to RIBES, an evaluation metric that pays attention
to long-distance reordering (Isozaki et al, 2010).
While some of these metrics such as TER, ME-
TEOR are gaining prominence, BLEU enjoys the
status of being the de facto standard tuning metric
as it is often claimed and sometimes observed that
optimizing with BLEU produces better translations
than other metrics (Callison-Burch et al, 2011).
The gains obtained by the MT system tuned on
a particular metric do not improve performance as
measured under other metrics (Cer et al, 2010), sug-
gesting that over-fitting to a specific metric might
happen without improvements in translation quality.
In this paper we propose a new tuning framework
for jointly optimizing multiple evaluation metrics.
Pareto-optimality is a natural way to think about
multi-metric optimization and multi-metric opti-
mization (MMO) was recently explored using the
notion of Pareto optimality in the Pareto-based
Multi-objective Optimization (PMO) approach (Duh
et al, 2012). PMO provides several equivalent solu-
tions (parameter weights) having different trade-offs
between the different MT metrics. In (Duh et al,
2012) the choice of which option to use rests with
the MT system developer and in that sense their ap-
proach is an a posteriori method to specify the pref-
erence (Marler and Arora, 2004).
In contrast to this, our tuning framework pro-
vides a principled way of using the Pareto optimal
options using ensemble decoding (Razmara et al,
2012). We also introduce a novel method of ensem-
ble tuning for jointly tuning multiple MT evaluation
metrics and further combine this with the PMO ap-
947
proach (Duh et al, 2012). We also introduce three
other approaches for multi-metric tuning and com-
pare their performance to the ensemble tuning. Our
experiments yield the highest metric scores across
many different metrics (that are being optimized),
something that has not been possible until now.
Our ensemble tuning method over multiple met-
rics produced superior translations than single met-
ric tuning as measured by a post-editing task.
HTER (Snover et al, 2006) scores in our human
evaluation confirm that multi-metric optimization
can lead to better MT output.
2 Related Work
In grammar induction and parsing (Spitkovsky et al,
2011; Hall et al, 2011; Auli and Lopez, 2011) have
proposed multi-objective methods based on round-
robin iteration of single objective optimizations.
Research in SMT parameter tuning has seen a
surge of interest recently, including online/batch
learning (Watanabe, 2012; Cherry and Foster, 2012),
large-scale training (Simianer et al, 2012; He
and Deng, 2012), and new discriminative objec-
tives (Gimpel and Smith, 2012; Zheng et al, 2012;
Bazrafshan et al, 2012). However, few works
have investigated the multi-metric tuning problem in
depth. Linear combination of BLEU and TER is re-
ported in (Zaidan, 2009; Dyer et al, 2009; Servan
and Schwenk, 2011); an alternative is to optimize on
BLEU with MERT while enforcing that TER does
not degrade per iteration (He and Way, 2009). Stud-
ies on metric tunability (Liu et al, 2011; Callison-
Burch et al, 2011; Chen et al, 2012) have found
that the metric used for evaluation may not be the
best metric used for tuning. For instance, (Mauser et
al., 2008; Cer et al, 2010) report that tuning on lin-
ear combinations of BLEU-TER is more robust than
a single metric like WER.
The approach in (Devlin and Matsoukas, 2012)
modifies the optimization function to include traits
such as output length so that the hypotheses pro-
duced by the decoder have maximal score according
to one metric (BLEU) but are subject to an output
length constraint, e.g. that the output is 5% shorter.
This is done by rescoring an N-best list (forest) for
the metric combined with each trait condition and
then the different trait hypothesis are combined us-
ing a system combination step. The traits are in-
dependent of the reference (while tuning). In con-
trast, our method is able to combine multiple metrics
(each of which compares to the reference) during the
tuning step and we do not depend on N-best list (or
forest) rescoring or system combination.
Duh et. al. (2012) proposed a Pareto-based ap-
proach to SMT multi-metric tuning, where the lin-
ear combination weights do not need to be known in
advance. This is advantageous because the optimal
weighting may not be known in advance. However,
the notion of Pareto optimality implies that multiple
?best? solutions may exist, so the MT system devel-
oper may be forced to make a choice after tuning.
These approaches require the MT system devel-
oper to make a choice either before tuning (e.g. in
terms of linear combination weights) or afterwards
(e.g. the Pareto approach). Our method here is dif-
ferent in that we do not require any choice. We
use ensemble decoding (Razmara et al, 2012) (see
sec 3) to combine the different solutions resulting
from the multi-metric optimization, providing an el-
egant solution for deployment. We extend this idea
further and introduce ensemble tuning, where the
metrics have separate set of weights. The tuning
process alternates between ensemble decoding and
the update step where the weights for each metric
are optimized separately followed by joint update of
metric (meta) weights.
3 Ensemble Decoding
We now briefly review ensemble decoding (Razmara
et al, 2012) which is used as a component in the al-
gorithms we present. The prevalent model of statis-
tical MT is a log-linear framework using a vector of
feature functions ?:
p(e|f) ? exp
(
w ? ?
)
(1)
The idea of ensemble decoding is to combine sev-
eral models dynamically at decode time. Given mul-
tiple models, the scores are combined for each par-
tial hypothesis across the different models during
decoding using a user-defined mixture operation ?.
p(e|f) ? exp
(
w1 ? ?1 ? w2 ? ?2 ? . . .
)
(2)
(Razmara et al, 2012) propose several mixture
operations, such as log-wsum (simple linear mix-
ture), wsum (log-linear mixture) and max (choose lo-
948
cally best model) among others. The different mix-
ture operations allows the user to encode the be-
liefs about the relative strengths of the models. It
has been applied successfully for domain adaptation
setting and shown to perform better approaches that
pre-compute linear mixtures of different models.
4 Multi-Metric Optimization
In statistical MT, the multi-metric optimization
problem can be expressed as:
w? = argmax
w
g
(
[M1(H), . . . ,Mk(H)]
)
(3)
where H = N (f ;w)
where N (f ;w) is the decoding function generating
a set of candidate hypotheses H based on the model
parameters w, for the source sentences f . For each
source sentence fi ? f there is a set of candidate
hypotheses {hi} ? H . The goal of the optimiza-
tion is to find the weights that maximize the func-
tion g(.) parameterized by different evaluation met-
rics M1, . . . ,Mk.
For the Pareto-optimal based approach such as
PMO (Duh et al, 2012), we can replace g(?) above
with gPMO(?) which returns the points in the Pareto
frontier. Alternately a weighted averaging function
gwavg(?) would result in a linear combination of the
metrics being considered, where the tuning method
would maximize the joint metric. This is similar to
the (TER-BLEU)/2 optimization (Cer et al, 2010;
Servan and Schwenk, 2011).
We introduce four methods based on the above
formulation and each method uses a different type
of g(?) function for combining different metrics and
we compare experimentally with existing methods.
4.1 PMO Ensemble
PMO (Duh et al, 2012) seeks to maximize the num-
ber of points in the Pareto frontier of the metrics con-
sidered. The inner routine of the PMO-PRO tuning
is described in Algorithm 1. This routine is con-
tained within an outer loop that iterates for a fixed
number iterations of decoding the tuning set and op-
timizing the weights.
The tuning process with PMO-PRO is inde-
pendently repeated with different set of weights
for metrics1 yielding a set of equivalent solutions
1For example Duh et al (2012) use five different weight
Algorithm 1 PMO-PRO (Inner routine for tuning)
1: Input: Hypotheses H = N (f ;w); Weights w
2: Initialize T = {}
3: for each f in tuning set f do
4: {h} = H(f)
5: {M({h})} = ComputeMetricScore({h}, e?)
6: {F} = FindParetoFrontier({M({h})})
7: for each h in {h} do
8: if h ? F then add (1, h) to T
9: else add (`, h) to T (see footnote 1)
10: wp ? PRO(T ) (optimize using PRO)
11: Output: Pareto-optimal weights wp
{ps1 , . . . , psn} which are points on the Pareto fron-
tier. The user then chooses one solution by making a
trade-off between the performance gains across dif-
ferent metrics. However, as noted earlier this a pos-
teriori choice ignores other solutions that are indis-
tinguishable from the chosen one.
We alleviate this by complementing PMO with
ensemble decoding, which we call PMO ensemble,
in which each point in the Pareto solution is a dis-
tinct component in the ensemble decoder. This idea
can also be used in other MMO approaches such as
linear combination of metrics (gwavg(.)) mentioned
above. In this view, PMO ensemble is a special case
of ensemble combination, where the decoding is per-
formed by an ensemble of optimal solutions.
The ensemble combination model introduces new
hyperparameters ? that are the weights of the en-
semble components (meta weights). These ensem-
ble weights could set to be uniform in a na??ve
implementation. Or the user can encode her be-
liefs or expectations about the individual solutions
{ps1 , . . . , psn} to set the ensemble weights (based
on the relative importance of the components). Fi-
nally, one could also include a meta-level tuning step
to set the weights ?.
The PMO ensemble approach is graphically il-
lustrated in Figure 1; we will also refer to this fig-
ure while discussing other methods.2 The orig-
settings for metrics (M1,M2), viz. (0.0, 1.0), (0.3, 0.7),
(0.5, 0.5), (0.7, 0.3) and (1.0, 0.0). They combine the met-
ric weights qi with the sentence-level metric scores Mi as
` =
(?
k qkMk
)
/k where ` is the target value for negative
examples (the else line in Alg 1) in the optimization step.
2The illustration is based on two metrics, metric-1 and
metric-2, but could be applied to any number of metrics. With-
out loss of generality we assume accuracy metrics, i.e. higher
949
Me
tric
?2
Metric?1
Figure 1: Illustration of different MMO approaches involving
two metrics. Solid (red) arrows indicate optimizing two met-
rics independently and the dashed (green) arrow optimize them
jointly. The Pareto frontier is indicated by the curve.
inal PMO-PRO seeks to maximize the points on
the Pareto frontier (blue curve in the figure) lead-
ing to Pareto-optimal solutions. On the other hand,
the PMO ensemble combines the different Pareto-
optimal solutions and potentially moving in the di-
rection of dashed (green) arrows to some point that
has higher score in either or both dimensions.
4.2 Lateen MMO
Lateen EM has been proposed as a way of jointly
optimizing multiple objectives in the context of de-
pendency parsing (Spitkovsky et al, 2011). It uses
a secondary hard EM objective to move away, when
the primary soft EM objective gets stuck in a local
optima. The course correction could be performed
under different conditions leading to variations that
are based on when and how often to shift from one
objective function to another during optimization.
The lateen technique can be applied to the multi-
metric optimization in SMT by treating the differ-
ent metrics as different objective functions. While
the several lateen variants are also applicable for our
task, our objective here is to improve performance
across the different metrics (being optimized). Thus,
we restrict ourselves to the style where the search
alternates between the metrics (in round-robin fash-
ion) at each iteration. Since the notion of conver-
gence is unclear in lateen setting, we stop after a
fixed number of iterations optimizing the tuning set.
In terms of Figure 1, lateen MMO corresponds to al-
ternately maximizing the metrics along two dimen-
sions as depicted by the solid arrows.
By the very nature of lateen-alternation, the
metric score is better.
weights obtained at each iteration are likely to be
best for the metric that was optimized in that itera-
tion. Thus, one could use weights from the last k
iterations (for lateen-tuning with as many metrics)
and then decode the test set with an ensemble of
these weights as in PMO ensemble. However in
practice we find the weights to converge and we sim-
ply use the weights from the final iteration to decode
the test set in our lateen experiments.
4.3 Union of Metrics
At each iteration lateen MMO excludes all but one
metric for optimization. An alternative would be to
consider all the metrics at each iteration so that the
optimizer could try to optimize them jointly. This
has been the general motivation for considering the
linear combination of metrics (Cer et al, 2010; Ser-
van and Schwenk, 2011) resulting in a joint metric,
which is then optimized.
However due to the scaling differences between
the scores of different metrics, the linear combi-
nation might completely suppress the metric hav-
ing scores in the lower-range. As an example, the
RIBES scores that are typically in the high 0.7-0.8
range, dominate the BLEU scores that is typically
around 0.3. While the weighted linear combination
tries to address this imbalance, they introduce ad-
ditional parameters that are manually fixed and not
separately tuned.
We avoid this linear combination pitfall by taking
the union of the metrics under which we consider
the union of training examples from all metrics and
optimize them jointly. Mathematically,
w? = argmax
w
g(M1(H)) ? . . . ? g(Mk(H)) (4)
Most of the optimization approaches involve two
phases: i) select positive and negative examples and
ii) optimize parameters to favour positive examples
while penalizing negative ones. In the union ap-
proach, we independently generate positive and neg-
ative sets of examples for all the metrics and take
their union. The optimizer now seeks to move to-
wards positive examples from all metrics, while pe-
nalizing others.
This is similar to the PMO-PRO approach except
that here the optimizer tries to simultaneously max-
imize the number of high scoring points across all
950
metrics. Thus, instead of the entire Pareto frontier
curve in Figure 1, the union approach optimizes the
two dimensions simultaneously in each iteration.
5 Ensemble Tuning
These methods, even though novel, under utilize the
power of ensembles as they combine the solution
only at the end of the tuning process. We would
prefer to tightly integrate the idea of ensembles into
the tuning. We thus extend the ensemble decoding
to ensemble tuning. The feature weights are repli-
cated separately for each evaluation metric, which
are treated as components in the ensemble decoding
and tuned independently in the optimization step.
Initially the ensemble decoder decodes a devset us-
ing a weighted ensemble to produce a single N-best
list. For the optimization, we employ a two-step ap-
proach of optimizing the feature weights (of each
ensemble component) followed by a step for tun-
ing the meta (component) weights. The optimized
weights are then used for decoding the devset in the
next iteration and the process is repeated for a fixed
number of iterations.
Modifying the MMO representation in Equa-
tion 3, we formulate ensemble tuning as:
Hens = Nens
(
f ; {wM};?;?
)
(5)
w? =
{
argmax
wMi
Hens | 1?i?k
}
(6)
? = argmax
?
g ({Mi(Hens)|1?i?k} ;w?) (7)
Here the ensemble decoder function Nens(.)
is parameterized by an ensemble of weights
wM1 , . . . , wMk (denoted as {wM} in Eq 5) for each
metric and a mixture operation (?). ? represents the
weights of the ensemble components.
Pseudo-code for ensemble tuning is shown in Al-
gorithm 2. In the beginning of each iteration (line 2),
the tuning process ensemble decodes (line 4) the
tuning set using the weights obtained from the pre-
vious iteration. Equation 5 gives the detailed expres-
sion for the ensemble decoding, whereHens denotes
the N-best list generated by the ensemble decoder.
The method now uses a dual tuning strategy in-
volving two phases to optimize the weights. In the
first step it optimizes each of the k metrics indepen-
dently (lines 6-7) along its respective dimension in
Algorithm 2 Ensemble Tuning Algorithm
1: Input: Tuning set f ,
Metrics M1, . . . ,Mk (ensemble components)
Initial weights {wM} ? wM1 , . . . wMk and
Component (meta) weights ?
2: for j = 1, . . . do
3: {w(j)M } ? {wM}
4: Ensemble decode the tuning set
Hens = Nens(f ; {w
(j)
M };?;?)
5: {wM} = {}
6: for each metric Mi ? {M} do
7: w?Mi ? PRO(Hens, wMi) (use PRO)
8: Add w?Mi to {wM}
9: ?? PMO-PRO(Hens, {wM}) (Alg 1)
10: Output: Optimal weights {wM} and ?
the multi-metric space (as shown by the solid arrows
along the two axes in Figure 1). This yields a new
set of weights w? for the features in each metric.
The second tuning step (line 9) then optimizes
the meta weights (?) so as to maximize the multi-
metric objective along the joint k-dimensional space
as shown in Equation 7. This is illustrated by the
dashed arrows in the Figure 1. While g(.) could be
any function that combines multiple metrics, we use
the PMO-PRO algorithm (Alg. 1) for this step.
The main difference between ensemble tuning and
PMO ensemble is that the former is an ensemble
model over metrics and the latter is an ensemble
model over Pareto solutions. Additionally, PMO en-
semble uses the notion of ensembles only for the fi-
nal decoding after tuning has completed.
5.1 Implementation Notes
All the proposed methods fit naturally within the
usual SMT tuning framework. However, some
changes are required in the decoder to support en-
semble decoding and in the tuning scripts for op-
timizing with multiple metrics. For ensemble de-
coding, the decoder should be able to use multiple
weight vectors and dynamically combine them ac-
cording to some desired mixture operation. Note
that, unlike Razmara et al (2012), our approach uses
just one model but has different weight vectors for
each metric and the required decoder modifications
are simpler than full ensemble decoding.
While any of the mixture operations proposed
by Razmara et al (2012) could be used, in this pa-
951
per we use log-wsum ? the linear combination of the
ensemble components and log-wmax ? the combina-
tion that prefers the locally best component. These
are simpler to implement and also performed com-
petitively in their domain adaptation experiments.
Unless explicitly noted otherwise, the results pre-
sented in Section 6 are based on linear mixture oper-
ation log-wsum, which empirically performed better
than the log-wmax for ensemble tuning.
6 Experiments
We evaluate the different methods on Arabic-
English translation in single as well as multiple ref-
erences scenario. Corpus statistics are shown in
Table 1. For all the experiments in this paper,
we use Kriya, our in-house Hierarchical phrase-
based (Chiang, 2007) (Hiero) system, and inte-
grated the required changes for ensemble decoding.
Kriya performs comparably to the state of the art in
phrase-based and hierarchical phrase-based transla-
tion over a wide variety of language pairs and data
sets (Sankaran et al, 2012).
We use PRO (Hopkins and May, 2011) for op-
timizing the feature weights and PMO-PRO (Duh
et al, 2012) for optimizing meta weights, wher-
ever applicable. In both cases, we use SVM-
Rank (Joachims, 2006) as the optimizer.
We used the default parameter settings for dif-
ferent MT tuning metrics. For METEOR, we tried
both METEOR-tune and METEOR-hter settings
and found the latter to perform better in BLEU and
TER scores, even though the former was marginally
better in METEOR3 and RIBES scores. We ob-
served the margin of loss in BLEU and TER to out-
weigh the gains in METEOR and RIBES and we
chose METEOR-hter setting for both optimization
and evaluation of all our experiments.
6.1 Evaluation on Tuning Set
Unlike conventional tuning methods, PMO (Duh et
al., 2012) was originally evaluated on the tuning set
to avoid potential mismatch with the test set. In
order to ensure robustness of evaluation, they re-
decode the devset using the optimal weights from
the last tuning iteration and report the scores on 1-
3This behaviour was also noted by Denkowski and Lavie
(2011) in their analysis of Urdu-English system for tunable met-
rics task in WMT11.
best candidates.
Corpus Training size Tuning/ test set
ISI corpus 1.1 M
1664/ 1313 (MTA)
1982/ 987 (ISI)
Table 1: Corpus Statistics (# of sentences) for Arabic-English.
MTA (4-refs) and ISI (1-ref).
We follow the same strategy and compare our
PMO-ensemble approach with PMO-PRO (denoted
P) and a linear combination4 (denoted L) base-
line. Similar to Duh et al (2012), we use
five different BLEU:RIBES weight settings, viz.
(0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and
(1.0, 0.0), marked L1 through L5 or P1 through P5.
The Pareto frontier is then computed from 80 points
(5 runs and 15 iterations per run) on the devset.
Figure 2(a) shows the Pareto frontier of L and P
baselines using BLEU and RIBES as two metrics.
The frontier of the P dominates that of L for most
part showing that the PMO approach benefits from
picking Pareto points during the optimization.
We use the PMO-ensemble approach to combine
the optimized weights from the 5 tuning runs and
re-decode the devset employing ensemble decoding.
This yields the points LEns and PEns in the plot,
which obtain better scores than most of the indi-
vidual runs of L and P. This ensemble approach of
combining the final weights also generalizes to the
unseen test set as we show later.
Figure 2(b) plots the change in BLEU during tun-
ing in the multiple references and the single refer-
ence scenarios. We show for each baseline method L
and P, plots for two different weight settings that ob-
tain high BLEU and RIBES scores. In both datasets,
our ensemble tuning approach dominates the curves
of the (L and P) baselines. In summary, these results
confirm that the ensemble approach achieves results
that are competitive with previous MMO methods
on the devset Pareto curve. We now provide a more
comprehensive evaluation on the test set.
6.2 Evaluation on Test Set
This section contains multi-metric optimization re-
sults on the unseen test sets, one test set has multi-
ple references and the other has a single-reference.
4Linear combination is a generalized version of the com-
bined (TER-BLEU)/2 metric and its variants.
952
 0.765
 0.766
 0.767
 0.768
 0.769
 0.77
 0.771
 0.772
 0.773
 0.33  0.335  0.34  0.345  0.35
R
I
B
E
S
BLEU
 L1
 L2
L3  L4  
 L5
P1   P2 P3
 P4 P5
LEns  
 PEns
PMO-PROLin-Comb
(a) Pareto frontier and BLEU-RIBES scores: MTA 4-refs devset
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
 2  4  6  8  10  12  14
B
L
E
U
Iterations
L1-mtaL5-mtaP1-mtaP5-mtaEns-Tune-mtaL1-isiL3-isiP1-isiP3-isiEns-Tune-isi
(b) Tuning BLEU scores: MTA 4-refs and ISI 1-ref devsets
Figure 2: Devset (redecode): Comparison of Lin-comb (L) and PMO-PRO (P) with Ensemble decoding (Lens and PEns) and
Ensemble tuning (Ens-Tune)
We plot BLEU scores against other metrics (RIBES,
METEOR and TER) and this allows us to compare
the performance of each metric relative to the de-
facto standard BLEU metric.
Baseline points are identified by single letters B
for BLEU, T for TER, etc. and the baseline (single-
metric optimized) score for each metric is indicated
by a dashed line on the corresponding axis. MMO
points use a series of single letters referring to the
metrics used, e.g. BT for BLEU-TER. The union of
metrics method is identified with the suffix ?J? and
lateen method with suffix ?L? (thus BT-L refers to the
lateen tuning with BLEU-TER). MMO points with-
out any suffix use the ensemble tuning approach.
Figures 3 and 4(a) plot the scores for the MTA test
set with 4-references. We see noticeable and some
statistically significant improvements in BLEU and
RIBES (see Table 2 for BLEU improvements).
All our MMO approaches, except for the union
method, show gains on both BLEU and RIBES axes.
Figures 3(b) and 4(a) show that none of the proposed
methods managed to improve the baseline scores for
METEOR and TER. However, several of our en-
semble tuning combinations work well for both ME-
TEOR (BR, BMRTB3, etc.) and TER (BMRT and
BRT) in that they improved or were close to the
baseline scores in either dimension. We again see in
these figures that the MMO approaches can improve
the BLEU-only tuning by 0.3 BLEU points, without
much drop in other metrics. This is in tune with the
finding that BLEU could be tuned easily (Callison-
Burch et al, 2011) and also explains why it remains
Approach and Tuning Metric(s)
BLEU
MTA ISI
Single Objective Baselines
BLEU 36.06 37.20
METEOR 35.05 36.91
RIBES 33.35 36.60
TER 33.92 35.85
Ensemble Tuning: 2 Metrics
B-M 36.02 37.26
B-R 36.15 37.37
B-T 35.72 36.31
Ensemble Tuning: 3 Metrics
B-M-R 36.36 37.37
B-M-T 36.22 36.89
B-R-T 35.97 36.72
Ensemble Tuning: > 3 Metrics
B-M-R-T 35.94 36.84
B-M-R-T-B3 36.16 37.12
B-M-R-T-B3-B2-B1 36.08 37.24
Table 2: BLEU Scores on MTA (4 refs) and ISI (1 ref) test sets
using the standard mteval script. Boldface scores indicate scores
that are comparable to or better than the baseline BLEU-only
tuning. Italicized scores indicate statistically significant differ-
ences at p-value 0.05 computed with bootstrap significance test.
a popular choice for optimizing SMT systems.
Among the different MMO methods the ensem-
ble tuning performs better than lateen or union ap-
proaches. In terms of the number of metrics being
optimized jointly, we see substantial gains when us-
ing a small number (typically 2 or 3) of metrics. Re-
sults seem to suffer beyond this number; probably
because there might not be a space that contain so-
lution(s) optimal for all the metrics that are jointly
optimized.
We hypothesize that each metric correlates well
953
 0.808
 0.81
 0.812
 0.814
 0.816
 0.818
 0.82
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
R
I
B
E
S
BLEU
 B
 M
 R
 T  PEns
 LEns
BM 
BR  
 BT
MR  
 BMR
 BMT
 BRT
BMRT  
 BMRTB3
 BMRTB3B2B1
 BM-J
 BR-J
 BT-J
BMT-J  
 BM-L
BR-L  
BMR-L  
(a) BLEU-RIBES scores
 0.495
 0.5
 0.505
 0.51
 0.515
 0.52
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
M
E
T
E
O
R
BLEU
B
 M
 R
 T
 PEns(BR)
LEns(BR)  BM  BR
 BT
MR  
 BMR
 BMT
 BRT
 BMRTB3BMRTB3B2B1  
 BM-J
 BR-J
 BT-J
BMT-J  
 BM-L
BR-L  
BMR-L  
(b) BLEU-METEOR scores
Figure 3: MTA 4-refs testset: Comparison of different MMO approaches. The dashed lines correspond to baseline scores tuned on
the respective metrics in the axes. The union of metrics method is identified with the suffix J and lateen with suffix L.
 0.4
 0.405
 0.41
 0.415
 0.42
 0.425
 0.43
 0.435
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
n
T
E
R
BLEU
B  
 M
 R
 T
 PEns(BR)
LEns(BR)  
BM   BR
 BT
MR   BMR
 BMTBRT  
 BMRT
 BMRTB3
 BM-J
BR-J  
 BT-J BMT-J  
BM-L  
BR-L  
BMR-L  
(a) MTA (4-refs)
 0.44
 0.445
 0.45
 0.455
 0.46
 0.465
 0.355  0.36  0.365  0.37  0.375
n
T
E
R
BLEU
B  
M  
 R
 T
PEns(BR)   LEns(BR)
  BM
 BR
 BMR
 B3MT
BRT  
BMRT  
 BMRTB3
BMRTB3B2B1  
 BR-J
BR-L  
(b) ISI (1-ref)
Figure 4: BLEU-TER scores: Comparison of different MMO approaches. We plot nTER (1-TER) scores for easy reading of the
plots. The dashed lines correspond to baseline scores tuned on the respective metrics in the axes.
(in a looser sense) with few others, but not all. For
example, union optimizations BR-J and BMT-J per-
form close to or better than RIBES and TER base-
lines, but get very poor score in METEOR. On the
other hand BM-J is close to the METEOR baseline,
while doing poorly on the RIBES and TER. This be-
haviour is also evident from the single-metric base-
lines, where R and T-only settings are clearly distin-
guished from the M-only system. It is not clear if
such distinct classes of metrics could be bridged by
some optimal solution and the metric dichotomy re-
quires further study as this is key to practical multi-
metric tuning in SMT.
The lateen and union approaches appear to be
very sensitive to the number of metrics and they
generally perform well for two metrics case and
show degradation for more metrics. Unlike other
approaches, the union approach failed to improve
over the baseline BLEU and this could be attributed
to the conflict of interest among the metrics, while
choosing example points for the optimization step.
The positive example preferred by a particular met-
ric could be a negative example for the other metric.
This would only confuse the optimizer resulting in
poor solutions. Our future line of work would be to
study the effect of avoiding such of conflicting ex-
amples in the union approach.
For the single-reference (ISI) dataset, we only
plot the BLEU-TER case in Figure 4(b) due to lack
of space. The results are similar to the multiple
references set indicating that MMO approaches are
equally effective for single references5. Table 2
5One could argue that MMO methods require multiple ref-
erences since each metric might be picking out a different ref-
954
Metric
Single-metric Tuning Ensemble Tuning
B-only M-only B-M-R
BLEU 37.89 37.18 39.01
HBLEU 51.93 53.59 53.14
METEOR 61.31 61.56 61.68
HMETEOR 72.35 72.39 72.74
TER 0.520 0.532 0.516
HTER 0.361 0.370 0.346
Table 3: Post-editing Human Evaluation: Regular (untargeted)
and human-targeted scores. Human targeted scores are com-
puted against the post-edited reference and regular scores are
computed with the original references. Best scores are in bold-
face and statistically significant ones (at p = 0.05) are italicized.
shows the BLEU scores for our ensemble tuning
method (for various combinations) and we again see
improvements over the baseline BLEU-only tuning.
6.3 Human Evaluation
So far we have shown that multi-metric optimiza-
tion can improve over single-metric tuning on a sin-
gle metric like BLEU and we have shown that our
methods find a tuned model that performs well with
respect to multiple metrics. Is the output that scores
higher on multiple metrics actually a better trans-
lation? To verify this, we conducted a post-editing
human evaluation experiment. We compared our en-
semble tuning approach involving BLEU, METEOR
and RIBES (B-M-R) with systems optimized for
BLEU (B-only) and METEOR (M-only).
We selected 100 random sentences (that are at
least 15 words long) from the Arabic-English MTA
(4 references) test set and translated them using the
three systems (two single metric systems and BMR
ensemble tuning). We shuffled the resulting trans-
lations and split them into 3 sets such that each set
has equal number of the translations from three sys-
tems. The translations were edited by three human
annotators in a post-editing setup, where the goal
was to edit the translations to make them as close
to the references as possible, using the Post-Editing
Tool: PET (Aziz et al, 2012). The annotators were
not Arabic-literate and relied only on the reference
translations during post-editing. The identifiers that
link each translation to the system that generated it
are removed to avoid annotator bias.
In the end we collated post-edited translations for
each system and then computed the system-level
erence sentence. Our experiment shows that even with a single
reference MMO methods can work.
human-targeted (HBLEU, HMETEOR, HTER)
scores, by using respective post-edited translations
as the reference. First comparing the HTER (Snover
et al, 2006) scores shown in Table 3, we see
that the single-metric system optimized for ME-
TEOR performs slightly worse than the one op-
timized for BLEU, despite using METEOR-hter
version (Denkowski and Lavie, 2011). Ensemble
tuning-based system optimized for three metrics (B-
M-R) improves HTER by 4% and 6.3% over BLEU
and METEOR optimized systems respectively.
The single-metric system tuned with M-only set-
ting scores high on HBLEU, closely followed by the
ensemble system. We believe this to be caused by
chance rather than any systematic gains by the M-
only tuning; the ensemble system scores high on
HMETEOR compared to the M-only system. While
HTER captures the edit distance to the targeted ref-
erence, HMETEOR and HBLEU metrics capture
missing content words or synonyms by exploiting
n-grams and paraphrase matching.
We also computed the regular variants (BLEU,
METEOR and TER), which are scored against orig-
inal references. The ensemble system outperformed
the single-metric systems in all the three metrics.
The improvements were also statistically significant
at p-value of 0.05 for BLEU and TER.
7 Conclusion
We propose and present a comprehensive study of
several multi-metric optimization (MMO) methods
in SMT. First, by exploiting the idea of ensemble de-
coding (Razmara et al, 2012), we propose an effec-
tive way to combine multiple Pareto-optimal model
weights from previous MMO methods (e.g. Duh et
al. (2012)), obviating the need for manually trading
off among metrics. We also proposed two new vari-
ants: lateen-style MMO and union of metrics.
We also extended ensemble decoding to a new
tuning algorithm called ensemble tuning. This
method demonstrates statistically significant gains
for BLEU and RIBES with modest reduction in ME-
TEOR and TER. Further, in our human evaluation,
ensemble tuning obtains the best HTER among com-
peting baselines, confirming that optimizing on mul-
tiple metrics produces human-preferred translations
compared to the conventional optimization approach
involving a single metric.
955
References
Michael Auli and Adam Lopez. 2011. Training a log-
linear parser with loss functions via softmax-margin.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 333?
343, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Wilker Aziz, Sheila Castilho Monteiro de Sousa, and
Lucia Specia. 2012. PET: a tool for post-editing
and assessing machine translation. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Associa-
tion (ELRA).
Marzieh Bazrafshan, Tagyoung Chung, and Daniel
Gildea. 2012. Tuning as linear regression. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 543?547, Montre?al, Canada. ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. ACL.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the ACL, pages 555?
563. ACL.
Boxing Chen, Roland Kuhn, and Samuel Larkin. 2012.
Port: a precision-order-recall mt evaluation metric for
tuning. In Proceedings of the 50th Annual Meeting
of the ACL (Volume 1: Long Papers), pages 930?939,
Jeju Island, Korea. ACL.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 427?436, Montre?al, Canada. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233. ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, Edinburgh, Scotland, July. ACL.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-based
hypothesis selection for machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 528?532. ACL.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proceedings of
the 50th Annual Meeting of the ACL, Jeju Island, Ko-
rea. ACL.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the North American Chapter of the ACL: Human
Language Technologies, pages 221?231, Montre?al,
Canada. ACL.
Keith Hall, Ryan T. McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
Proceedings of the Empirical Methods in Natural Lan-
guage Processing, pages 1489?1499.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 292?301, Jeju Island, Ko-
rea. ACL.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, Edin-
burgh, Scotland. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944?
952, Cambridge, MA. ACL.
Thorsten Joachims. 2006. Training linear svms in linear
time. In Proceedings of the 12th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 217?226.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105?115.
956
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineer-
ing. Structural and Multidisciplinary Optimization,
26(6):369?395, April.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167. ACL.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3):181?193.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of Asso-
ciation of Computational Linguistics, pages 311?318.
ACL.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the ACL, Jeju, Re-
public of Korea. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics (PBML), 97(97):83?98.
Christophe Servan and Holger Schwenk. 2011. Optimis-
ing multiple metrics with mert. Prague Bull. Math.
Linguistics, 96:109?118.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11?21, Jeju Island, Korea. ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas, pages 223?231.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the Empirical Methods in
Natural Language Processing, pages 1269?1280. As-
sociation of Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773. ACL.
Taro Watanabe. 2012. Optimized online rank learn-
ing for machine translation. In Proceedings of the
2012 Conference of the North American Chapter of the
ACL: Human Language Technologies, pages 253?262,
Montre?al, Canada, June. ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Daqi Zheng, Yifan He, Yang Liu, and Qun Liu. 2012.
Maximum rank correlation training for statistical ma-
chine translation. In MT Summit XIII.
957
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 429?433,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Is Machine Translation Ripe for Cross-lingual Sentiment Classification?
Kevin Duh and Akinori Fujino and Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
{kevin.duh,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
Abstract
Recent advances in Machine Translation (MT)
have brought forth a new paradigm for build-
ing NLP applications in low-resource scenar-
ios. To build a sentiment classifier for a
language with no labeled resources, one can
translate labeled data from another language,
then train a classifier on the translated text.
This can be viewed as a domain adaptation
problem, where labeled translations and test
data have some mismatch. Various prior work
have achieved positive results using this ap-
proach.
In this opinion piece, we take a step back and
make some general statements about cross-
lingual adaptation problems. First, we claim
that domain mismatch is not caused by MT
errors, and accuracy degradation will occur
even in the case of perfect MT. Second, we ar-
gue that the cross-lingual adaptation problem
is qualitatively different from other (monolin-
gual) adaptation problems in NLP; thus new
adaptation algorithms ought to be considered.
This paper will describe a series of carefully-
designed experiments that led us to these con-
clusions.
1 Summary
Question 1: If MT gave perfect translations (seman-
tically), do we still have a domain adaptation chal-
lenge in cross-lingual sentiment classification?
Answer: Yes. The reason is that while many trans-
lations of a word may be valid, the MT system might
have a systematic bias. For example, the word ?awe-
some? might be prevalent in English reviews, but in
translated reviews, the word ?excellent? is generated
instead. From the perspective of MT, this translation
is correct and preserves sentiment polarity. But from
the perspective of a classifier, there is a domain mis-
match due to differences in word distributions.
Question 2: Can we apply standard adaptation algo-
rithms developed for other (monolingual) adaptation
problems to cross-lingual adaptation?
Answer: No. It appears that the interaction between
target unlabeled data and source data can be rather
unexpected in the case of cross-lingual adaptation.
We do not know the reason, but our experiments
show that the accuracy of adaptation algorithms in
cross-lingual scenarios have much higher variance
than monolingual scenarios.
The goal of this opinion piece is to argue the need
to better understand the characteristics of domain
adaptation in cross-lingual problems. We invite the
reader to disagree with our conclusion (that the true
barrier to good performance is not insufficient MT
quality, but inappropriate domain adaptation meth-
ods). Here we present a series of experiments that
led us to this conclusion. First we describe the ex-
periment design (?2) and baselines (?3), before an-
swering Question 1 (?4) and Question 2 (?5).
2 Experiment Design
The cross-lingual setup is this: we have labeled data
from source domain S and wish to build a sentiment
classifier for target domain T . Domain mismatch
can arise from language differences (e.g. English vs.
translated text) or market differences (e.g. DVD vs.
Book reviews). Our experiments will involve fixing
429
T to a common testset and varying S. This allows us
to experiment with different settings for adaptation.
We use the Amazon review dataset of Pretten-
hofer (2010)1, due to its wide range of languages
(English [EN], Japanese [JP], French [FR], Ger-
man [DE]) and markets (music, DVD, books). Un-
like Prettenhofer (2010), we reverse the direction of
cross-lingual adaptation and consider English as tar-
get. English is not a low-resource language, but this
setting allows for more comparisons. Each source
dataset has 2000 reviews, equally balanced between
positive and negative. The target has 2000 test sam-
ples, large unlabeled data (25k, 30k, 50k samples
respectively for Music, DVD, and Books), and an
additional 2000 labeled data reserved for oracle ex-
periments. Texts in JP, FR, and DE are translated
word-by-word into English with Google Translate.2
We perform three sets of experiments, shown in
Table 1. Table 2 lists all the results; we will interpret
them in the following sections.
Target (T ) Source (S)
1 Music-EN Music-JP, Music-FR, Music-DE,
DVD-EN, Book-EN
2 DVD-EN DVD-JP, DVD-FR, DVD-DE,
Music-EN, Book-EN
3 Book-EN Book-JP, Book-FR, Book-DE,
Music-EN, DVD-EN
Table 1: Experiment setups: Fix T , vary S.
3 How much performance degradation
occurs in cross-lingual adaptation?
First, we need to quantify the accuracy degrada-
tion under different source data, without consider-
ation of domain adaptation methods. So we train
a SVM classifier on labeled source data3, and di-
rectly apply it on test data. The oracle setting, which
has no domain-mismatch (e.g. train on Music-EN,
test on Music-EN), achieves an average test accu-
racy of (81.6 + 80.9 + 80.0)/3 = 80.8%4. Aver-
1http://www.webis.de/research/corpora/webis-cls-10
2This is done by querying foreign words to build a bilingual
dictionary. The words are converted to tfidf unigram features.
3For all methods we try here, 5% of the 2000 labeled source
samples are held-out for parameter tuning.
4See column EN of Table 2, Supervised SVM results.
age cross-lingual accuracies are: 69.4% (JP), 75.6%
(FR), 77.0% (DE), so degradations compared to or-
acle are: -11% (JP), -5% (FR), -4% (DE).5 Cross-
market degradations are around -6%6.
Observation 1: Degradations due to market and
language mismatch are comparable in several cases
(e.g. MUSIC-DE and DVD-EN perform similarly
for target MUSIC-EN). Observation 2: The ranking
of source language by decreasing accuracy is DE >
FR > JP. Does this mean JP-EN is a more difficult
language pair for MT? The next section will show
that this is not necessarily the case. Certainly, the
domain mismatch for JP is larger than DE, but this
could be due to phenomenon other than MT errors.
4 Where exactly is the domain mismatch?
4.1 Theory of Domain Adaptation
We analyze domain adaptation by the concepts of
labeling and instance mismatch (Jiang and Zhai,
2007). Let pt(x, y) = pt(y|x)pt(x) be the target
distribution of samples x (e.g. unigram feature vec-
tor) and labels y (positive / negative). Let ps(x, y) =
ps(y|x)ps(x) be the corresponding source distribu-
tion. We assume that one (or both) of the following
distributions differ between source and target:
? Instance mismatch: ps(x) 6= pt(x).
? Labeling mismatch: ps(y|x) 6= pt(y|x).
Instance mismatch implies that the input feature
vectors have different distribution (e.g. one dataset
uses the word ?excellent? often, while the other uses
the word ?awesome?). This degrades performance
because classifiers trained on ?excellent? might not
know how to classify texts with the word ?awe-
some.? The solution is to tie together these features
(Blitzer et al, 2006) or re-weight the input distribu-
tion (Sugiyama et al, 2008). Under some assump-
tions (i.e. covariate shift), oracle accuracy can be
achieved theoretically (Shimodaira, 2000).
Labeling mismatch implies the same input has
different labels in different domains. For exam-
ple, the JP word meaning ?excellent? may be mis-
translated as ?bad? in English. Then, positive JP
5See ?Adapt by Language? columns of Table 2. Note
JP+FR+DE condition has 6000 labeled samples, so is not di-
rectly comparable to other adaptation scenarios (2000 samples).
Nevertheless, mixing languages seem to give good results.
6See ?Adapt by Market? columns of Table 2.
430
Target Classifier Oracle Adapt by Language Adapt by Market
EN JP FR DE JP+FR+DE MUSIC DVD BOOK
MUSIC-EN Supervised SVM 81.6 68.5 75.2 76.3 80.3 - 76.8 74.1
Adapted TSVM 79.6 73.0 74.6 77.9 78.6 - 78.4 75.6
DVD-EN Supervised SVM 80.9 70.1 76.4 77.4 79.7 75.2 - 74.5
Adapted TSVM 81.0 71.4 75.5 76.3 78.4 74.8 - 76.7
BOOK-EN Supervised SVM 80.0 69.6 75.4 77.4 79.9 73.4 76.2 -
Adapted TSVM 81.2 73.8 77.6 76.7 79.5 75.1 77.4 -
Table 2: Test accuracies (%) for English Music/DVD/Book reviews. Each column is an adaptation scenario using
different source data. The source data may vary by language or by market. For example, the first row shows that for
the target of Music-EN, the accuracy of a SVM trained on translated JP reviews (in the same market) is 68.5, while the
accuracy of a SVM trained on DVD reviews (in the same language) is 76.8. ?Oracle? indicates training on the same
market and same language domain as the target. ?JP+FR+DE? indicates the concatenation of JP, FR, DE as source
data. Boldface shows the winner of Supervised vs. Adapted.
reviews will be associated with the word ?bad?:
ps(y = +1|x = bad) will be high, whereas the true
conditional distribution should have high pt(y =
?1|x = bad) instead. There are several cases for
labeling mismatch, depending on how the polarity
changes (Table 3). The solution is to filter out these
noisy samples (Jiang and Zhai, 2007) or optimize
loosely-linked objectives through shared parameters
or Bayesian priors (Finkel and Manning, 2009).
Which mismatch is responsible for accuracy
degradations in cross-lingual adaptation?
? Instance mismatch: Systematic MT bias gener-
ates word distributions different from naturally-
occurring English. (Translation may be valid.)
? Label mismatch: MT error mis-translates a word
into something with different polarity.
Conclusion from ?4.2 and ?4.3: Instance mis-
match occurs often; MT error appears minimal.
Mis-translated polarity Effect
? ? 0 Loose a discriminative
e.g. (?good? ? ?the?) feature
0 ? ? Increased overlap in
e.g. (?the? ? ?good?) positive/negative data
+ ? ? and ? ? + Association with
e.g. (?good? ? ?bad?) opposite label
Table 3: Label mismatch: mis-translating positive (+),
negative (?), or neutral (0) words have different effects.
We think the first two cases have graceful degradation,
but the third case may be catastrophic.
4.2 Analysis of Instance Mismatch
To measure instance mismatch, we compute statis-
tics between ps(x) and pt(x), or approximations
thereof: First, we calculate a (normalized) average
feature from all samples of source S, which repre-
sents the unigram distribution of MT output. Simi-
larly, the average feature vector for target T approx-
imates the unigram distribution of English reviews
pt(x). Then we measure:
? KL Divergence between Avg(S) and Avg(T ),
where Avg() is the average vector.
? Set Coverage of Avg(T ) on Avg(S): how many
word (type) in T appears at least once in S.
Both measures correlate strongly with final accu-
racy, as seen in Figure 1. The correlation coefficients
are r = ?0.78 for KL Divergence and r = 0.71 for
Coverage, both statistically significant (p < 0.05).
This implies that instance mismatch is an important
reason for the degradations seen in Section 3.7
4.3 Analysis of Labeling Mismatch
We measure labeling mismatch by looking at dif-
ferences in the weight vectors of oracle SVM and
adapted SVM. Intuitively, if a feature has positive
weight in the oracle SVM, but negative weight in the
adapted SVM, then it is likely a MT mis-translation
7The observant reader may notice that cross-market points
exhibit higher coverage but equal accuracy (74-78%) to some
cross-lingual points. This suggests that MT output may be more
constrained in vocabulary than naturally-occurring English.
431
68 70 72 74 76 78 80 82
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Accuracy
KL
 D
iv
er
ge
nc
e
68 70 72 74 76 78 80 82
0.4
0.5
0.6
0.7
0.8
0.9
Accuracy
Te
st
 C
ov
er
ag
e
Figure 1: KL Divergence and Coverage vs. accuracy. (o)
are cross-lingual and (x) are cross-market data points.
is causing the polarity flip. Algorithm 1 (with
K=2000) shows how we compute polarity flip rate.8
We found that the polarity flip rate does not cor-
relate well with accuracy at all (r = 0.04). Conclu-
sion: Labeling mismatch is not a factor in perfor-
mance degradation. Nevertheless, we note there is a
surprising large number of flips (24% on average). A
manual check of the flipped words in BOOK-JP re-
vealed few MT mistakes. Only 3.7% of 450 random
EN-JP word pairs checked can be judged as blatantly
incorrect (without sentence context). The majority
of flipped words do not have a clear sentiment ori-
entation (e.g. ?amazon?, ?human?, ?moreover?).
5 Are standard adaptation algorithms
applicable to cross-lingual problems?
One of the breakthroughs in cross-lingual text clas-
sification is the realization that it can be cast as do-
main adaptation. This makes available a host of pre-
existing adaptation algorithms for improving over
supervised results. However, we argue that it may be
8The feature normalization in Step 1 is important to ensure
that the weight magnitudes are comparable.
Algorithm 1 Measuring labeling mismatch
Input: Weight vectors for source ws and target wt
Input: Target data average sample vector avg(T )
Output: Polarity flip rate f
1: Normalize: ws = avg(T ) * ws; wt = avg(T ) * wt
2: Set S+ = { K most positive features in ws}
3: Set S? = { K most negative features in ws}
4: Set T+ = { K most positive features in wt}
5: Set T? = { K most negative features in wt}
6: for each feature i ? T+ do
7: if i ? S? then f = f + 1
8: end for
9: for each feature j ? T? do
10: if j ? S+ then f = f + 1
11: end for
12: f = f2K
better to ?adapt? the standard adaptation algorithm
to the cross-lingual setting. We arrived at this con-
clusion by trying the adapted counterpart of SVMs
off-the-shelf. Recently, (Bergamo and Torresani,
2010) showed that Transductive SVMs (TSVM),
originally developed for semi-supervised learning,
are also strong adaptation methods. The idea is to
train on source data like a SVM, but encourage the
classification boundary to divide through low den-
sity regions in the unlabeled target data.
Table 2 shows that TSVM outperforms SVM in
all but one case for cross-market adaptation, but
gives mixed results for cross-lingual adaptation.
This is a puzzling result considering that both use
the same unlabeled data. Why does TSVM exhibit
such a large variance on cross-lingual problems, but
not on cross-market problems? Is unlabeled target
data interacting with source data in some unexpected
way?
Certainly there are several successful studies
(Wan, 2009; Wei and Pal, 2010; Banea et al, 2008),
but we think it is important to consider the possi-
bility that cross-lingual adaptation has some fun-
damental differences. We conjecture that adapting
from artificially-generated text (e.g. MT output)
is a different story than adapting from naturally-
occurring text (e.g. cross-market). In short, MT is
ripe for cross-lingual adaptation; what is not ripe is
probably our understanding of the special character-
istics of the adaptation problem.
432
References
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity analy-
sis using machine translation. In Proc. of Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Alessandro Bergamo and Lorenzo Torresani. 2010. Ex-
ploiting weakly-labeled web images to improve ob-
ject classification: a domain adaptation approach. In
Advances in Neural Information Processing Systems
(NIPS).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Jenny Rose Finkel and Chris Manning. 2009. Hierarchi-
cal Bayesian domain adaptation. In Proc. of NAACL
Human Language Technologies (HLT).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proc. of the As-
sociation for Computational Linguistics (ACL).
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural correspon-
dence learning. In Proc. of the Association for Com-
putational Linguistics (ACL).
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inferenc, 90.
Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima,
Hisashi Kashima, Paul von Bu?nau, and Motoaki
Kawanabe. 2008. Direct importance estimation for
covariate shift adaptation. Annals of the Institute of
Statistical Mathematics, 60(4).
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proc. of the Association for
Computational Linguistics (ACL).
Bin Wei and Chris Pal. 2010. Cross lingual adaptation:
an experiment on sentiment classification. In Proceed-
ings of the ACL 2010 Conference Short Papers.
433
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning to Translate with Multiple Objectives
Kevin Duh? Katsuhito Sudoh Xianchao Wu Hajime Tsukada Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
kevinduh@is.naist.jp, lastname.firstname@lab.ntt.co.jp
Abstract
We introduce an approach to optimize a ma-
chine translation (MT) system on multiple
metrics simultaneously. Different metrics
(e.g. BLEU, TER) focus on different aspects
of translation quality; our multi-objective ap-
proach leverages these diverse aspects to im-
prove overall quality.
Our approach is based on the theory of Pareto
Optimality. It is simple to implement on top of
existing single-objective optimization meth-
ods (e.g. MERT, PRO) and outperforms ad
hoc alternatives based on linear-combination
of metrics. We also discuss the issue of metric
tunability and show that our Pareto approach
is more effective in incorporating new metrics
from MT evaluation for MT optimization.
1 Introduction
Weight optimization is an important step in build-
ing machine translation (MT) systems. Discrimi-
native optimization methods such as MERT (Och,
2003), MIRA (Crammer et al, 2006), PRO (Hop-
kins and May, 2011), and Downhill-Simplex (Nelder
and Mead, 1965) have been influential in improving
MT systems in recent years. These methods are ef-
fective because they tune the system to maximize an
automatic evaluation metric such as BLEU, which
serve as surrogate objective for translation quality.
However, we know that a single metric such as
BLEU is not enough. Ideally, we want to tune to-
wards an automatic metric that has perfect corre-
lation with human judgments of translation quality.
?*Now at Nara Institute of Science & Technology (NAIST)
While many alternatives have been proposed, such a
perfect evaluation metric remains elusive.
As a result, many MT evaluation campaigns now
report multiple evaluation metrics (Callison-Burch
et al, 2011; Paul, 2010). Different evaluation met-
rics focus on different aspects of translation quality.
For example, while BLEU (Papineni et al, 2002)
focuses on word-based n-gram precision, METEOR
(Lavie and Agarwal, 2007) allows for stem/synonym
matching and incorporates recall. TER (Snover
et al, 2006) allows arbitrary chunk movements,
while permutation metrics like RIBES (Isozaki et
al., 2010; Birch et al, 2010) measure deviation in
word order. Syntax (Owczarzak et al, 2007) and se-
mantics (Pado et al, 2009) also help. Arguably, all
these metrics correspond to our intuitions on what is
a good translation.
The current approach of optimizing MT towards
a single metric runs the risk of sacrificing other met-
rics. Can we really claim that a system is good if
it has high BLEU, but very low METEOR? Simi-
larly, is a high-METEOR low-BLEU system desir-
able? Our goal is to propose a multi-objective op-
timization method that avoids ?overfitting to a sin-
gle metric?. We want to build a MT system that
does well with respect to many aspects of transla-
tion quality.
In general, we cannot expect to improve multi-
ple metrics jointly if there are some inherent trade-
offs. We therefore need to define the notion of Pareto
Optimality (Pareto, 1906), which characterizes this
tradeoff in a rigorous way and distinguishes the set
of equally good solutions. We will describe Pareto
Optimality in detail later, but roughly speaking, a
1
hypothesis is pareto-optimal if there exist no other
hypothesis better in all metrics. The contribution of
this paper is two-fold:
? We introduce PMO (Pareto-based Multi-
objective Optimization), a general approach for
learning with multiple metrics. Existing single-
objective methods can be easily extended to
multi-objective using PMO.
? We show that PMO outperforms the alterna-
tive (single-objective optimization of linearly-
combined metrics) in multi-objective space,
and especially obtains stronger results for met-
rics that may be difficult to tune individually.
In the following, we first explain the theory of
Pareto Optimality (Section 2), and then use it to
build up our proposed PMO approach (Section 3).
Experiments on NIST Chinese-English and PubMed
English-Japanese translation using BLEU, TER, and
RIBES are presented in Section 4. We conclude by
discussing related work (Section 5) and opportuni-
ties/limitations (Section 6).
2 Theory of Pareto Optimality
2.1 Definitions and Concepts
The idea of Pareto optimality comes originally from
economics (Pareto, 1906), where the goal is to char-
acterize situations when a change in allocation of
goods does not make anybody worse off. Here, we
will explain it in terms of MT:
Let h ? L be a hypothesis from an N-best list L.
We have a total of K different metrics Mk(h) for
evaluating the quality of h. Without loss of gen-
erality, we assume metric scores are bounded be-
tween 0 and 1, with 1 being perfect. Each hypoth-
esis h can be mapped to a K-dimensional vector
M(h) = [M1(h);M2(h); ...;MK(h)]. For exam-
ple, suppose K = 2, M1(h) computes the BLEU
score, and M2(h) gives the METEOR score of h.
Figure 1 illustrates the set of vectors {M(h)} in a
10-best list.
For two hypotheses h1, h2, we write M(h1) >
M(h2) if h1 is better than h2 in all metrics, and
M(h1) ? M(h2) if h1 is better than or equal
to h2 in all metrics. When M(h1) ? M(h2) and
Mk(h1) > Mk(h2) for at least one metric k, we say
that h1 dominates h2 and write M(h1) . M(h2).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
metric1
me
tric2
Figure 1: Illustration of Pareto Frontier. Ten hypotheses
are plotted by their scores in two metrics. Hypotheses
indicated by a circle (o) are pareto-optimal, while those
indicated by a plus (+) are not. The line shows the convex
hull, which attains only a subset of pareto-optimal points.
The triangle (4) is a point that is weakly pareto-optimal
but not pareto-optimal.
Definition 1. Pareto Optimal: A hypothesis h? ?
L is pareto-optimal iff there does not exist another
hypothesis h ? L such that M(h) . M(h?).
In Figure 1, the hypotheses indicated by circle
(o) are pareto-optimal, while those with plus (+) are
not. To visualize this, take for instance the pareto-
optimal point (0.4,0.7). There is no other point with
either (metric1 > 0.4 and metric2 ? 0.7), or (met-
ric1 ? 0.4 and metric2 > 0.7). On the other hand,
the non-pareto point (0.6,0.4) is ?dominated? by an-
other point (0.7,0.6), because for metric1: 0.7 > 0.6
and for metric2: 0.6 > 0.4.
There is another definition of optimality, which
disregards ties and may be easier to visualize:
Definition 2. Weakly Pareto Optimal: A hypothesis
h? ? L is weakly pareto-optimal iff there is no other
hypothesis h ? L such that M(h) > M(h?).
Weakly pareto-optimal points are a superset of
pareto-optimal points. A hypothesis is weakly
pareto-optimal if there is no other hypothesis that
improves all the metrics; a hypothesis is pareto-
optimal if there is no other hypothesis that improves
at least one metric without detriment to other met-
rics. In Figure 1, point (0.1,0.8) is weakly pareto-
optimal but not pareto-optimal, because of the com-
peting point (0.3,0.8). Here we focus on pareto-
optimality, but note our algorithms can be easily
2
modified for weakly pareto-optimality. Finally, we
can introduce the key concept used in our proposed
PMO approach:
Definition 3. Pareto Frontier: Given an N-best list
L, the set of all pareto-optimal hypotheses h ? L is
called the Pareto Frontier.
The Pareto Frontier has two desirable properties
from the multi-objective optimization perspective:
1. Hypotheses on the Frontier are equivalently
good in the Pareto sense.
2. For each hypothesis not on the Frontier, there
is always a better (pareto-optimal) hypothesis.
This provides a principled approach to optimiza-
tion: i.e. optimizing towards points on the Frontier
and away from those that are not, and giving no pref-
erence to different pareto-optimal hypotheses.
2.2 Reduction to Linear Combination
Multi-objective problems can be formulated as:
arg max
w
[M1(h);M2(h); . . . ;Mk(h)] (1)
where h = Decode(w, f)
Here, the MT system?s Decode function, parame-
terized by weight vector w, takes in a foreign sen-
tence f and returns a translated hypothesis h. The
argmax operates in vector space and our goal is to
find w leading to hypotheses on the Pareto Frontier.
In the study of Pareto Optimality, one central
question is: To what extent can multi-objective prob-
lems be solved by single-objective methods? Equa-
tion 1 can be reduced to a single-objective problem
by scalarizing the vector [M1(h); . . . ;Mk(h)] with
a linear combination:
arg max
w
K?
k=1
pkMk(h) (2)
where h = Decode(w, f)
Here, pk are positive real numbers indicating the rel-
ative importance of each metric (without loss of gen-
erality, assume
?
k pk = 1). Are the solutions to
Eq. 2 also solutions to Eq. 1 (i.e. pareto-optimal)
and vice-versa? The theory says:
Theorem 1. Sufficient Condition: If w? is solution
to Eq. 2, then it is weakly pareto-optimal. Further,
if w? is unique, then it is pareto-optimal.
Theorem 2. No Necessary Condition: There may
exist solutions to Eq. 1 that cannot be achieved by
Eq. 2, irregardless of any setting of {pk}.
Theorem 1 is a positive result asserting that lin-
ear combination can give pareto-optimal solutions.
However, Theorem 2 states the limits: in partic-
ular, Eq. 2 attains only pareto-optimal points that
are on the convex hull. This is illustrated in Fig-
ure 1: imagine sweeping all values of p1 = [0, 1]
and p2 = 1? p1 and recording the set of hypotheses
that maximizes
?
k pkMk(h). For 0.6 < p1 ? 1 we
get h = (0.9, 0.1), for p1 = 0.6 we get (0.7, 0.6),
and for 0 < p1 < 0.6 we get (0.4, 0.8). At no
setting of p1 do we attain h = (0.4, 0.7) which
is also pareto-optimal but not on the convex hull.1
This may have ramifications for issues like metric
tunability and local optima. To summarize, linear-
combination is reasonable but has limitations. Our
proposed approach will instead directly solve Eq. 1.
Pareto Optimality and multi-objective optimiza-
tion is a deep field with active inquiry in engineer-
ing, operations research, economics, etc. For the in-
terested reader, we recommend the survey by Mar-
ler and Arora (2004) and books by (Sawaragi et al,
1985; Miettinen, 1998).
3 Multi-objective Algorithms
3.1 Computing the Pareto Frontier
Our PMO approach will need to compute the Pareto
Frontier for potentially large sets of points, so we
first describe how this can be done efficiently. Given
a set of N vectors {M(h)} from an N-best list L,
our goal is extract the subset that are pareto-optimal.
Here we present an algorithm based on iterative
filtering, in our opinion the simplest algorithm to
understand and implement. The strategy is to loop
through the list L, keeping track of any dominant
points. Given a dominant point, it is easy to filter
out many points that are dominated by it. After suc-
cessive rounds, any remaining points that are not fil-
1We note that scalarization by exponentiated-combination
?
k pkMk(h)
q , for a suitable q > 0, does satisfy necessary
conditions for pareto optimality. However the proper tuning of q
is not known a priori. See (Miettinen, 1998) for theorem proofs.
3
Algorithm 1 FindParetoFrontier
Input: {M(h)}, h ? L
Output: All pareto-optimal points of {M(h)}
1: F = ?
2: while L is not empty do
3: h? = shift(L)
4: for each h in L do
5: if (M(h?) . M(h)): remove h from L
6: else if (M(h) . M(h?)): remove h from L; set
h? = h
7: end for
8: Add h? to Frontier Set F
9: for each h in L do
10: if (M(h?) . M(h)): remove h from L
11: end for
12: end while
13: Return F
tered are necessarily pareto-optimal. Algorithm 1
shows the pseudocode. In line 3, we take a point h?
and check if it is dominating or dominated in the for-
loop (lines 4-8). At least one pareto-optimal point
will be found by line 8. The second loop (lines 9-11)
further filters the list for points that are dominated by
h? but iterated before h? in the first for-loop.
The outer while-loop stops exactly after P iter-
ations, where P is the actual number of pareto-
optimal points in L. Each inner loop costs O(KN)
so the total complexity is O(PKN). Since P ? N
with the actual value depending on the probability
distribution of {M(h)}, the worst-case run-time is
O(KN2). For a survey of various Pareto algorithms,
refer to (Godfrey et al, 2007). The algorithm we de-
scribed here is borrowed from the database literature
in what is known as skyline operators.2
3.2 PMO-PRO Algorithm
We are now ready to present an algorithm for multi-
objective optimization. As we will see, it can be seen
as a generalization of the pairwise ranking optimiza-
tion (PRO) of (Hopkins and May, 2011), so we call
it PMO-PRO. PMO-PRO approach works by itera-
tively decoding-and-optimizing on the devset, sim-
2The inquisitive reader may wonder how is Pareto related
to databases. The motivation is to incorporate preferences into
relational queries(Bo?rzso?nyi et al, 2001). For K = 2 metrics,
they also present an alternative faster O(N logN) algorithm by
first topologically sorting along the 2 dimensions. All domi-
nated points can be filtered by one-pass by comparing with the
most-recent dominating point.
ilar to many MT optimization methods. The main
difference is that rather than trying to maximize a
single metric, we maximize the number of pareto
points, in order to expand the Pareto Frontier
We will explain PMO-PRO in terms of the
pseudo-code shown in Algorithm 2. For each sen-
tence pair (f, e) in the devset, we first generate an
N-best list L ? {h} using the current weight vector
w (line 5). In line 6, we evaluate each hypothesis
h with respect to the K metrics, giving a set of K-
dimensional vectors {M(h)}.
Lines 7-8 is the critical part: it gives a ?la-
bel? to each hypothesis, based on whether it is
in the Pareto Frontier. In particular, first we call
FindParetoFrontier (Algorithm 1), which re-
turns a set of pareto hypotheses; pareto-optimal hy-
potheses will get label 1 while non-optimal hypothe-
ses will get label 0. This information is added to
the training set T (line 8), which is then optimized
by any conventional subroutine in line 10. We will
follow PRO in using a pairwise classifier in line 10,
which finds w? that separates hypotheses with labels
1 vs. 0. In essence, this is the trick we employ to
directly optimize on the Pareto Frontier. If we had
used BLEU scores rather than the {0, 1} labels in
line 8, the entire PMO-PRO algorithm would revert
to single-objective PRO.
By definition, there is no single ?best? result
for multi-objective optimization, so we collect all
weights and return the Pareto-optimal set. In line 13
we evaluate each weight w on K metrics across the
entire corpus and call FindParetoFrontier
in line 14.3 This choice highlights an interesting
change of philosophy: While setting {pk} in linear-
combination forces the designer to make an a priori
preference among metrics prior to optimization, the
PMO strategy is to optimize first agnostically and
a posteriori let the designer choose among a set of
weights. Arguably it is easier to choose among so-
lutions based on their evaluation scores rather than
devising exact values for {pk}.
3.3 Discussion
Variants: In practice we find that a slight modifi-
cation of line 8 in Algorithm 2 leads to more sta-
3Note this is the same FindParetoFrontier algorithm as used
in line 7. Both operate on sets of points in K-dimensional
space, induced from either weights {w} or hypotheses {h}.
4
Algorithm 2 Proposed PMO-PRO algorithm
Input: Devset, max number of iterations I
Output: A set of (pareto-optimal) weight vectors
1: Initialize w. LetW = ?.
2: for i = 1 to I do
3: Let T = ?.
4: for each (f, e) in devset do
5: {h} =DecodeNbest(w,f )
6: {M(h)}=EvalMetricsOnSentence({h}, e)
7: {f} =FindParetoFrontier({M(h)})
8: foreach h ? {h}:
if h ? {f}, set l=1, else l=0; Add (l, h) to T
9: end for
10: w?=OptimizationSubroutine(T , w)
11: Add w? toW; Set w = w?.
12: end for
13: M(w) =EvalMetricsOnCorpus(w,devset) ?w ? W
14: Return FindParetoFrontier({M(w)})
ble results for PMO-PRO: for non-pareto hypothe-
ses h /? {f}, we set label l =
?
kMk(h)/K in-
stead of l= 0, so the method not only learns to dis-
criminate pareto vs. non-pareto but also also learns
to discriminate among competing non-pareto points.
Also, like other MT works, in line 5 the N-best list is
concatenated to N-best lists from previous iterations,
so {h} is a set with i ?N elements.
General PMO Approach: The strategy we out-
lined in Section 3.2 can be easily applied to other
MT optimization techniques. For example, by re-
placing the optimization subroutine (line 10, Algo-
rithm 2) with a Powell search (Och, 2003), one can
get PMO-MERT4. Alternatively, by using the large-
margin optimizer in (Chiang et al, 2009) and mov-
ing it into the for-each loop (lines 4-9), one can
get an online algorithm such PMO-MIRA. Virtually
all MT optimization algorithms have a place where
metric scores feedback into the optimization proce-
dure; the idea of PMO is to replace these raw scores
with labels derived from Pareto optimality.
4 Experiments
4.1 Evaluation Methodology
We experiment with two datasets: (1) The PubMed
task is English-to-Japanese translation of scientific
4A difference with traditional MERT is the necessity of
sentence-BLEU (Liang et al, 2006) in line 6. We use sentence-
BLEU for optimization but corpus-BLEU for evaluation here.
abstracts. As metrics we use BLEU and RIBES
(which demonstrated good human correlation in
this language pair (Goto et al, 2011)). (2) The
NIST task is Chinese-to-English translation with
OpenMT08 training data and MT06 as devset. As
metrics we use BLEU and NTER.
? BLEU = BP ? (?precn)1/4. BP is brevity
penality. precn is precision of n-gram matches.
? RIBES = (? + 1)/2 ? prec1/41 , with Kendall?s
? computed by measuring permutation between
matching words in reference and hypothesis5.
? NTER=max(1?TER, 0), which normalizes
Translation Edit Rate6 so that NTER=1 is best.
We compare two multi-objective approaches:
1. Linear-Combination of metrics (Eq. 2),
optimized with PRO. We search a range
of combination settings: (p1, p2) =
{(0, 1), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3), (1, 0)}.
Note (1, 0) reduces to standard single-metric
optimization of e.g. BLEU.
2. Proposed Pareto approach (PMO-PRO).
Evaluation of multi-objective problems can be
tricky because there is no single figure-of-merit.
We thus adopted the following methodology: We
run both methods 5 times (i.e. using the 5 differ-
ent (p1, p2) setting each time) and I = 20 iterations
each. For each method, this generates 5x20=100 re-
sults, and we plot the Pareto Frontier of these points
in a 2-dimensional metric space (e.g. see Figure 2).
A method is deemed better if its final Pareto Fron-
tier curve is strictly dominating the other. We report
devset results here; testset trends are similar but not
included due to space constraints.7
5from www.kecl.ntt.co.jp/icl/lirg/ribes
6from www.umd.edu/?snover/tercom
7An aside: For comparing optimization methods, we believe
devset comparison is preferable to testset since data mismatch
may confound results. If one worries about generalization, we
advocate to re-decode the devset with final weights and evaluate
its 1-best output (which is done here). This is preferable to sim-
ply reporting the achieved scores on devset N-best (as done in
some open-source scripts) since the learned weight may pick
out good hypotheses in the N-best but perform poorly when
re-decoding the same devset. The re-decode devset approach
avoids being overly optimistic while accurately measuring op-
timization performance.
5
Train Devset #Feat Metrics
PubMed 0.2M 2k 14 BLEU, RIBES
NIST 7M 1.6k 8 BLEU, NTER
Table 1: Task characteristics: #sentences in Train/Dev, #
of features, and metrics used. Our MT models are trained
with standard phrase-based Moses software (Koehn and
others, 2007), with IBM M4 alignments, 4gram SRILM,
lexical ordering for PubMed and distance ordering for the
NIST system. The decoder generates 50-best lists each
iteration. We use SVMRank (Joachims, 2006) as opti-
mization subroutine for PRO, which efficiently handle all
pairwise samples without the need for sampling.
4.2 Results
Figures 2 and 3 show the results for PubMed and
NIST, respectively. A method is better if its Pareto
Frontier lies more towards the upper-right hand cor-
ner of the graph. Our observations are:
1. PMO-PRO generally outperforms Linear-
Combination with any setting of (p1, p2).
The Pareto Frontier of PMO-PRO dominates
that of Linear-Combination. This implies
PMO is effective in optimizing towards Pareto
hypotheses.
2. For both methods, trading-off between met-
rics is necessary. For example in PubMed,
the designer would need to make a choice be-
tween picking the best weight according to
BLEU (BLEU=.265,RIBES=.665) vs. another
weight with higher RIBES but poorer BLEU,
e.g. (.255,.675). Nevertheless, both the PMO
and Linear-Combination with various (p1, p2)
samples this joint-objective space broadly.
3. Interestingly, a multi-objective approach can
sometimes outperform a single-objective opti-
mizer in its own metric. In Figure 2, single-
objective PRO focusing on optimizing RIBES
only achieves 0.68, but PMO-PRO using both
BLEU and RIBES outperforms with 0.685.
The third observation relates to the issue of metric
tunability (Liu et al, 2011). We found that RIBES
can be difficult to tune directly. It is an extremely
non-smooth objective with many local optima?slight
changes in word ordering causes large changes in
RIBES. So the best way to improve RIBES is to
0.2 0.21 0.22 0.23 0.24 0.25 0.26 0.270.665
0.67
0.675
0.68
0.685
0.69
0.695
bleu
ribe
s
 
 Linear CombinationPareto (PMO?PRO)
Figure 2: PubMed Results. The curve represents the
Pareto Frontier of all results collected after multiple runs.
0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.1640.694
0.695
0.696
0.697
0.698
0.699
0.7
0.701
0.702
0.703
0.704
bleu
nte
r
 
 Linear CombinationPareto (PMO?PRO)
Figure 3: NIST Results
not to optimize it directly, but jointly with a more
tunable metric BLEU. The learning curve in Fig-
ure 4 show that single-objective optimization of
RIBES quickly falls into local optimum (at iteration
3) whereas PMO can zigzag and sacrifice RIBES in
intermediate iterations (e.g. iteration 2, 15) leading
to a stronger result ultimately. The reason is the
diversity of solutions provided by the Pareto Fron-
tier. This finding suggests that multi-objective ap-
proaches may be preferred, especially when dealing
with new metrics that may be difficult to tune.
4.3 Additional Analysis and Discussions
What is the training time? The Pareto approach
does not add much overhead to PMO-PRO. While
FindParetoFrontier scales quadratically by size of
N-best list, Figure 5 shows that the runtime is triv-
6
0 2 4 6 8 10 12 14 16 18 200.63
0.64
0.65
0.66
0.67
0.68
0.69
iteration
ribe
s
 
 
Single?Objective RIBES
Pareto (PMO?PRO)
Figure 4: Learning Curve on RIBES: comparing single-
objective optimization and PMO.
0 100 200 300 400 500 600 700 800 900 10000
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Set size |L|
Run
time
 (sec
onds
)
 
 
Algorithm 1
TopologicalSort (footnote 2)
Figure 5: Avg. runtime per sentence of FindPareto
ial (0.3 seconds for 1000-best). Table 2 shows
the time usage breakdown in different iterations for
PubMed. We see it is mostly dominated by decod-
ing time (constant per iteration at 40 minutes on
single 3.33GHz processor). At later iterations, Opt
takes more time due to larger file I/O in SVMRank.
Note Decode and Pareto can be ?embarrasingly par-
allelized.?
Iter Time Decode Pareto Opt Misc.
(line 5) (line 7) (line 10) (line 6,8)
1 47m 85% 1% 1% 13%
10 62m 67% 6% 8% 19%
20 91m 47% 15% 22% 16%
Table 2: Training time usage in PMO-PRO (Algo 2).
How many Pareto points? The number of pareto
0 2 4 6 8 10 12 14 16 185
10
15
20
25
30
35
Iterations
Num
ber 
of P
aret
o P
oint
s
 
 
NIST
PubMed
Figure 6: Average number of Pareto points
hypotheses gives a rough indication of the diversity
of hypotheses that can be exploited by PMO. Fig-
ure 6 shows that this number increases gradually per
iteration. This perhaps gives PMO-PRO more direc-
tions for optimizing around potential local optimal.
Nevertheless, we note that tens of Pareto points is far
few compared to the large size of N-best lists used
at later iterations of PMO-PRO. This may explain
why the differences between methods in Figure 3
are not more substantial. Theoretically, the num-
ber will eventually level off as it gets increasingly
harder to generate new Pareto points in a crowded
space (Bentley et al, 1978).
Practical recommendation: We present the
Pareto approach as a way to agnostically optimize
multiple metrics jointly. However, in practice, one
may have intuitions about metric tradeoffs even if
one cannot specify {pk}. For example, we might
believe that approximately 1-point BLEU degra-
dation is acceptable only if RIBES improves by
at least 3-points. In this case, we recommend
the following trick: Set up a multi-objective prob-
lem where one metric is BLEU and the other is
3/4BLEU+1/4RIBES. This encourages PMO to ex-
plore the joint metric space but avoid solutions that
sacrifice too much BLEU, and should also outper-
form Linear Combination that searches only on the
(3/4,1/4) direction.
5 Related Work
Multi-objective optimization for MT is a relatively
new area. Linear-combination of BLEU/TER is
7
the most common technique (Zaidan, 2009), some-
times achieving good results in evaluation cam-
paigns (Dyer et al, 2009). As far as we known, the
only work that directly proposes a multi-objective
technique is (He and Way, 2009), which modifies
MERT to optimize a single metric subject to the
constraint that it does not degrade others. These
approaches all require some setting of constraint
strength or combination weights {pk}. Recent work
in MT evaluation has examined combining metrics
using machine learning for better correlation with
human judgments (Liu and Gildea, 2007; Albrecht
and Hwa, 2007; Gimnez and Ma`rquez, 2008) and
may give insights for setting {pk}. We view our
Pareto-based approach as orthogonal to these efforts.
The tunability of metrics is a problem that is gain-
ing recognition (Liu et al, 2011). If a good evalu-
ation metric could not be used for tuning, it would
be a pity. The Tunable Metrics task at WMT2011
concluded that BLEU is still the easiest to tune
(Callison-Burch et al, 2011). (Mauser et al, 2008;
Cer et al, 2010) report similar observations, in ad-
dition citing WER being difficult and BLEU-TER
being amenable. One unsolved question is whether
metric tunability is a problem inherent to the metric
only, or depends also on the underlying optimization
algorithm. Our positive results with PMO suggest
that the choice of optimization algorithm can help.
Multi-objective ideas are being explored in other
NLP areas. (Spitkovsky et al, 2011) describe a tech-
nique that alternates between hard and soft EM ob-
jectives in order to achieve better local optimum in
grammar induction. (Hall et al, 2011) investigates
joint optimization of a supervised parsing objective
and some extrinsic objectives based on downstream
applications. (Agarwal et al, 2011) considers us-
ing multiple signals (of varying quality) from online
users to train recommendation models. (Eisner and
Daume? III, 2011) trades off speed and accuracy of
a parser with reinforcement learning. None of the
techniques in NLP use Pareto concepts, however.
6 Opportunities and Limitations
We introduce a new approach (PMO) for training
MT systems on multiple metrics. Leveraging the
diverse perspectives of different evaluation metrics
has the potential to improve overall quality. Based
on Pareto Optimality, PMO is easy to implement
and achieves better solutions compared to linear-
combination baselines, for any setting of combi-
nation weights. Further we observe that multi-
objective approaches can be helpful for optimiz-
ing difficult-to-tune metrics; this is beneficial for
quickly introducing new metrics developed in MT
evaluation into MT optimization, especially when
good {pk} are not yet known. We conclude by draw-
ing attention to some limitations and opportunities
raised by this work:
Limitations: (1) The performance of PMO is
limited by the size of the Pareto set. Small N-best
lists lead to sparsely-sampled Pareto Frontiers, and
a much better approach would be to enlarge the hy-
pothesis space using lattices (Macherey et al, 2008).
How to compute Pareto points directly from lattices
is an interesting open research question. (2) The
binary distinction between pareto vs. non-pareto
points ignores the fact that 2nd-place non-pareto
points may also lead to good practical solutions. A
better approach may be to adopt a graded definition
of Pareto optimality as done in some multi-objective
works (Deb et al, 2002). (3) A robust evaluation
methodology that enables significance testing for
multi-objective problems is sorely needed. This will
make it possible to compare multi-objective meth-
ods on more than 2 metrics. We also need to follow
up with human evaluation.
Opportunities: (1) There is still much we do
not understand about metric tunability; we can learn
much by looking at joint metric-spaces and exam-
ining how new metrics correlate with established
ones. (2) Pareto is just one approach among many
in multi-objective optimization. A wealth of meth-
ods are available (Marler and Arora, 2004) and more
experimentation in this space will definitely lead to
new insights. (3) Finally, it would be interesting to
explore other creative uses of multiple-objectives in
MT beyond multiple metrics. For example: Can we
learn to translate faster while sacrificing little on ac-
curacy? Can we learn to jointly optimize cascaded
systems, such as as speech translation or pivot trans-
lation? Life is full of multiple competing objectives.
Acknowledgments
We thank the reviewers for insightful feedback.
8
References
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango,
and Xuanhui Wang. 2011. Click shaping to optimize
multiple objectives. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?11, pages 132?140,
New York, NY, USA. ACM.
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level mt evalu-
ation. In ACL.
J. L. Bentley, H. T. Kung, M. Schkolnick, and C. D.
Thompson. 1978. On the average number of max-
ima in a set of vectors and applications. Journal of the
Association for Computing Machinery (JACM), 25(4).
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing. Machine Translation, 24(1).
S. Bo?rzso?nyi, D. Kossmann, and K. Stocker. 2001. The
skyline operator. In Proceedings of the 17th Interna-
tional Conference on Data Engineering (ICDE).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Daniel Cer, Christopher Manning, and Daniel Jurafsky.
2010. The best lexical metric for phrase-based statis-
tical MT system optimization. In NAACL HLT.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passiveag-
gressive algorithms. Journal of Machine Learning Re-
search, 7.
Kalyanmoy Deb, Amrit Pratap, Sammer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2).
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Jason Eisner and Hal Daume? III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference algo-
rithms. In COST: NIPS 2011 Workshop on Computa-
tional Trade-offs in Statistical Learning.
Jesu?s Gimnez and Llu??s Ma`rquez. 2008. Heterogeneous
automatic mt evaluation through non-parametric met-
ric combinations. In ICJNLP.
Parke Godfrey, Ryan Shipley, and Jarek Gyrz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. VLDB Journal, 16.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of the NTCIR-9 Workshop Meeting.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1489?1499, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010. Automatic evaluation of translation quality for
distant language pairs. In EMNLP.
T. Joachims. 2006. Training linear SVMs in linear time.
In KDD.
P. Koehn et al 2007. Moses: open source toolkit for
statistical machine translation. In ACL.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for mt evaluation with high levels of cor-
relation with human judgments. In Workshop on Sta-
tistical Machine Translation.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In NAACL.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineering.
Structural and Multidisciplinary Optimization, 26.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
9
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
Kaisa Miettinen. 1998. Nonlinear Multiobjective Opti-
mization. Springer.
J.A. Nelder and R. Mead. 1965. The downhill simplex
method. Computer Journal, 7(308).
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL.
Vilfredo Pareto. 1906. Manuale di Economica Politica,
(Translated into English by A.S. Schwier as Manual of
Political Economy, 1971). Societa Editrice Libraria,
Milan.
Michael Paul. 2010. Overview of the iwslt 2010 evalua-
tion campaign. In IWSLT.
Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo
Tanino, editors. 1985. Theory of Multiobjective Opti-
mization. Academic Press.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen em: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1269?1280, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. In The Prague Bulletin of Mathe-
matical Linguistics.
10
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 100?104,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparative Study of Target Dependency Structures
for Statistical Machine Translation
Xianchao Wu?, Katsuhito Sudoh, Kevin Duh?, Hajime Tsukada, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan
wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp,
kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper presents a comparative study of
target dependency structures yielded by sev-
eral state-of-the-art linguistic parsers. Our ap-
proach is to measure the impact of these non-
isomorphic dependency structures to be used
for string-to-dependency translation. Besides
using traditional dependency parsers, we also
use the dependency structures transformed
from PCFG trees and predicate-argument
structures (PASs) which are generated by an
HPSG parser and a CCG parser. The experi-
ments on Chinese-to-English translation show
that the HPSG parser?s PASs achieved the best
dependency and translation accuracies.
1 Introduction
Target language side dependency structures have
been successfully used in statistical machine trans-
lation (SMT) by Shen et al (2008) and achieved
state-of-the-art results as reported in the NIST 2008
Open MT Evaluation workshop and the NTCIR-9
Chinese-to-English patent translation task (Goto et
al., 2011; Ma and Matsoukas, 2011). A primary ad-
vantage of dependency representations is that they
have a natural mechanism for representing discon-
tinuous constructions, which arise due to long-
distance dependencies or in languages where gram-
matical relations are often signaled by morphology
instead of word order (McDonald and Nivre, 2011).
It is known that dependency-style structures can
be transformed from a number of linguistic struc-
?Now at Baidu Inc.
?Now at Nara Institute of Science & Technology (NAIST)
tures. For example, using the constituent-to-
dependency conversion approach proposed by Jo-
hansson and Nugues (2007), we can easily yield de-
pendency trees from PCFG style trees. A seman-
tic dependency representation of a whole sentence,
predicate-argument structures (PASs), are also in-
cluded in the output trees of (1) a state-of-the-art
head-driven phrase structure grammar (HPSG) (Pol-
lard and Sag, 1994; Sag et al, 2003) parser, Enju1
(Miyao and Tsujii, 2008) and (2) a state-of-the-art
CCG parser2 (Clark and Curran, 2007). The moti-
vation of this paper is to investigate the impact of
these non-isomorphic dependency structures to be
used for SMT. That is, we would like to provide a
comparative evaluation of these dependencies in a
string-to-dependency decoder (Shen et al, 2008).
2 Gaining Dependency Structures
2.1 Dependency tree
We follow the definition of dependency graph and
dependency tree as given in (McDonald and Nivre,
2011). A dependency graph G for sentence s is
called a dependency tree when it satisfies, (1) the
nodes cover all the words in s besides the ROOT;
(2) one node can have one and only one head (word)
with a determined syntactic role; and (3) the ROOT
of the graph is reachable from all other nodes.
For extracting string-to-dependency transfer
rules, we use well-formed dependency structures,
either fixed or floating, as defined in (Shen et al,
2008). Similarly, we ignore the syntactic roles
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
2http://groups.inf.ed.ac.uk/ccg/software.html
100
 when the fluid pressure cylinder 31 is used , fluid is gradually applied . 
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 
c2 c5 c7 c9 c11 c12 c14 c15 c17 c20 c22 c24 c25 
c3 
c4 
c6 
c8 
c10 c13 
c18 
c19 
c21 
c23 
c16 
c1 
c0 
conj_ 
arg12 
det_ 
arg1 
adj_ 
arg1 
noun_ 
arg1 
noun_ 
arg0 
adj_ 
arg1 
aux_ 
arg12 
verb_ 
arg12 
punct_ 
arg1 
noun_ 
arg0 
aux_ 
arg12 
adj_ 
arg1 
verb_ 
arg12 
* + 
* + 
* + 
* 
+ 
* + 
* + 
* + 
*  
* + 
* + 
* + 
* + 
* + 
+ 
Figure 1: HPSG tree of an example sentence. ?*?/
?+?=syntactic/semantic heads. Arrows in red (upper)=
PASs, orange (bottom)=word-level dependencies gener-
ated from PASs, blue=newly appended dependencies.
both during rule extracting and target dependency
language model (LM) training.
2.2 Dependency parsing
Graph-based and transition-based are two predom-
inant paradigms for data-driven dependency pars-
ing. The MST parser (McDonald et al, 2005) and
the Malt parser (Nivre, 2003) stand for two typical
parsers, respectively. Parsing accuracy comparison
and error analysis under the CoNLL-X dependency
shared task data (Buchholz and Marsi, 2006) have
been performed by McDonald and Nivre (2011).
Here, we compare them on the SMT tasks through
parsing the real-world SMT data.
2.3 PCFG parsing
For PCFG parsing, we select the Berkeley parser
(Petrov and Klein, 2007). In order to generate word-
level dependency trees from the PCFG tree, we use
the LTH constituent-to-dependency conversion tool3
written by Johansson and Nugues (2007). The head
finding rules4 are according to Magerman (1995)
and Collins (1997). Similar approach has been orig-
inally used by Shen et al (2008).
2.4 HPSG parsing
In the Enju English HPSG grammar (Miyao et al,
2003) used in this paper, the semantic content of
3http://nlp.cs.lth.se/software/treebank converter/
4http://www.cs.columbia.edu/ mcollins/papers/heads
a sentence/phrase is represented by a PAS. In an
HPSG tree, each leaf node generally introduces a
predicate, which is represented by the pair made up
of the lexical entry feature and predicate type fea-
ture. The arguments of a predicate are designated by
the arrows from the argument features in a leaf node
to non-terminal nodes (e.g., t0?c3, t0?c16).
Since the PASs use the non-terminal nodes in the
HPSG tree (Figure 1), this prevents their direct us-
age in a string-to-dependency decoder. We thus need
an algorithm to transform these phrasal predicate-
argument dependencies into a word-to-word depen-
dency tree. Our algorithm (refer to Figure 1 for an
example) for changing PASs into word-based depen-
dency trees is as follows:
1. finding, i.e., find the syntactic/semantic head
word of each argument node through a bottom-
up traversal of the tree;
2. mapping, i.e., determine the arc directions
(among a predicate word and the syntac-
tic/semantic head words of the argument nodes)
for each predicate type according to Table 1.
Then, a dependency graph will be generated;
3. checking, i.e., post modifying the dependency
graph according to the definition of dependency
tree (Section 2.1).
Table 1 lists the mapping from HPSG?s PAS types
to word-level dependency arcs. Since a non-terminal
node in an HPSG tree has two kinds of heads, syn-
tactic or semantic, we will generate two dependency
graphs after mapping. We use ?PAS+syn? to repre-
sent the dependency trees generated from the HPSG
PASs guided by the syntactic heads. For semantic
heads, we use ?PAS+sem?.
For example, refer to t0 = when in Figure 1.
Its arg1 = c16 (with syntactic head t10), arg2
= c3 (with syntactic head t6), and PAS type =
conj arg12. In Table 1, this PAS type corresponds
to arg2?pred?arg1, then the result word-level de-
pendency is t6(is)?t0(when)?t10(is).
We need to post modify the dependency graph af-
ter applying the mapping, since it is not guaranteed
to be a dependency tree. Referring to the definition
of dependency tree (Section 2.1), we need the strat-
egy for (1) selecting only one head from multiple
101
PAS Type Dependency Relation
adj arg1[2] [arg2 ?] pred ? arg1
adj mod arg1[2] [arg2 ?] pred ? arg1 ? mod
aux[ mod] arg12 arg1/pred ? arg2 [? mod]
conj arg1[2[3]] [arg2[/arg3]] ? pred ? arg1
comp arg1[2] pred ? arg1 [? arg2]
comp mod arg1 arg1 ? pred ? mod
noun arg1 pred ? arg1
noun arg[1]2 arg2 ? pred [? arg1]
poss arg[1]2 pred ? arg2 [? arg1]
prep arg12[3] arg2[/arg3] ? pred ? arg1
prep mod arg12[3] arg2[/arg3] ? pred ? arg1 ? mod
quote arg[1]2 [arg1 ?] pred ? arg2
quote arg[1]23 [arg1/]arg3 ? pred ? arg2
lparen arg123 pred/arg2 ? arg3 ? arg1
relative arg1[2] [arg2 ?] pred ? arg1
verb arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]] ? pred
verb mod arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]]?pred?mod
app arg12,coord arg12 arg2/pred ? arg1
det arg1,it arg1,punct arg1 pred ? arg1
dtv arg2 pred ? arg2
lgs arg2 arg2 ? pred
Table 1: Mapping fromHPSG?s PAS types to dependency
relations. Dependent(s)? head(s), / = and, [] = optional.
heads and (2) appending dependency relations for
those words/punctuation that do not have any head.
When one word has multiple heads, we only keep
one. The selection strategy is that, if this arc was
deleted, it will cause the biggest number of words
that can not reach to the root word anymore. In case
of a tie, we greedily pack the arc that connect two
words wi and wj where |i? j| is the biggest. For all
the words and punctuation that do not have a head,
we greedily take the root word of the sentence as
their heads. In order to fully use the training data,
if there are directed cycles in the result dependency
graph, we still use the graph in our experiments,
where only partial dependency arcs, i.e., those target
flat/hierarchical phrases attached with well-formed
dependency structures, can be used during transla-
tion rule extraction.
2.5 CCG parsing
We also use the predicate-argument dependencies
generated by the CCG parser developed by Clark
and Curran (2007). The algorithm for generating
word-level dependency tree is easier than processing
the PASs included in the HPSG trees, since the word
level predicate-argument relations have already been
included in the output of CCG parser. The mapping
from predicate types to the gold-standard grammat-
ical relations can be found in Table 13 in (Clark and
Curran, 2007). The post-processing is like that de-
scribed for HPSG parsing, except we greedily use
the MST?s sentence root when we can not determine
it based on the CCG parser?s PASs.
3 Experiments
3.1 Setup
We re-implemented the string-to-dependency de-
coder described in (Shen et al, 2008). Dependency
structures from non-isomorphic syntactic/semantic
parsers are separately used to train the transfer
rules as well as target dependency LMs. For intu-
itive comparison, an outside SMT system is Moses
(Koehn et al, 2007).
For Chinese-to-English translation, we use the
parallel data from NIST Open Machine Translation
Evaluation tasks. The training data contains 353,796
sentence pairs, 8.7M Chinese words and 10.4M En-
glish words. The NIST 2003 and 2005 test data
are respectively taken as the development and test
set. We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) to obtain word alignments. The
Berkeley Language Modeling Toolkit, berkeleylm-
1.0b35 (Pauls and Klein, 2011), was employed to
train (1) a five-gram LM on the Xinhua portion of
LDC English Gigaword corpus v3 (LDC2007T07)
and (2) a tri-gram dependency LM on the English
dependency structures of the training data. We re-
port the translation quality using the case-insensitive
BLEU-4 metric (Papineni et al, 2002).
3.2 Statistics of dependencies
We compare the similarity of the dependencies with
each other, as shown in Table 2. Basically, we in-
vestigate (1) if two dependency graphs of one sen-
tence share the same root word and (2) if the head of
one word in one sentence are identical in two depen-
dency graphs. In terms of root word comparison, we
observe that MST and CCG share 87.3% of iden-
tical root words, caused by borrowing roots from
MST to CCG. Then, it is interesting that Berkeley
and PAS+syn share 74.8% of identical root words.
Note that the Berkeley parser is trained on the Penn
treebank (Marcus et al, 1994) yet the HPSG parser
is trained on the HPSG treebank (Miyao and Tsujii,
5http://code.google.com/p/berkeleylm/
102
Dependency Precision Recall BLEU-Dev BLEU-Test # phrases # hier rules # illegal dep trees # directed cycles
Moses-1 - - 0.3349 0.3207 5.4M - - -
Moses-2 - - 0.3445 0.3262 0.7M 4.5M - -
MST 0.744 0.750 0.3520 0.3291 2.4M 2.1M 251 0
Malt 0.732 0.738 0.3423 0.3203 1.5M 1.3M 130,960 0
Berkeley 0.800 0.806 0.3475 0.3312 2.4M 2.2M 282 0
PAS+syn 0.818 0.824 0.3499 0.3376 2.2M 1.9M 10,411 5,853
PAS+sem 0.777 0.782 0.3484 0.3343 2.1M 1.6M 14,271 9,747
CCG 0.701 0.705 0.3442 0.3283 1.7M 1.3M 61,015 49,955
Table 3: Comparison of dependency and translation accuracies. Moses-1 = phrasal, Moses-2 = hierarchical.
Malt Berkeley PAS PAS CCG
+syn +sem
MST 70.5 62.5 69.2 53.3 87.3
(77.3) (64.6) (58.5) (58.1) (61.7)
Malt 66.2 73.0 46.8 62.9
(63.2) (57.7) (56.6) (58.1)
Berkeley 74.8 44.2 56.5
(64.3) (56.0) (59.2)
PAS+ 59.3 62.9
syn (79.1) (61.0)
PAS+ 60.0
sem (58.8)
Table 2: Comparison of the dependencies of the English
sentences in the training data. Without () = % of similar
root words; with () = % of similar head words.
2008). In terms of head word comparison, PAS+syn
and PAS+sem share 79.1% of identical head words.
This is basically due to that we used the similar
PASs of the HPSG trees. Interestingly, there are only
59.3% identical root words shared by PAS+syn and
PAS+sem. This reflects the significant difference be-
tween syntactic and semantic heads.
We also manually created the golden dependency
trees for the first 200 English sentences in the train-
ing data. The precision/recall (P/R) are shown in
Table 3. We observe that (1) the translation accura-
cies approximately follow the P/R scores yet are not
that sensitive to their large variances, and (2) it is
still tough for domain-adapting from the treebank-
trained parsers to parse the real-world SMT data.
PAS+syn performed the best by avoiding the errors
of missing of arguments for a predicate, wrongly
identified head words for a linguistic phrase, and in-
consistency dependencies inside relatively long co-
ordinate structures. These errors significantly influ-
ence the number of extractable translation rules and
the final translation accuracies.
Note that, these P/R scores on the first 200 sen-
tences (all from less than 20 newswire documents)
shall only be taken as an approximation of the total
training data and not necessarily exactly follow the
tendency of the final BLEU scores. For example,
CCG is worse than Malt in terms of P/R yet with a
higher BLEU score. We argue this is mainly due to
that the number of illegal dependency trees gener-
ated by Malt is the highest. Consequently, the num-
ber of flat/hierarchical rules generated by using Malt
trees is the lowest. Also, PAS+sem has a lower P/R
than Berkeley, yet their final BLEU scores are not
statistically different.
3.3 Results
Table 3 also shows the BLEU scores, the number of
flat phrases and hierarchical rules (both integrated
with target dependency structures), and the num-
ber of illegal dependency trees generated by each
parser. From the table, we have the following ob-
servations: (1) all the dependency structures (except
Malt) achieved a significant better BLEU score than
the phrasal Moses; (2) PAS+syn performed the best
in the test set (0.3376), and it is significantly better
than phrasal/hierarchical Moses (p < 0.01), MST
(p < 0.05), Malt (p < 0.01), Berkeley (p < 0.05),
and CCG (p < 0.05); and (3) CCG performed as
well as MST and Berkeley. These results lead us to
argue that the robustness of deep syntactic parsers
can be advantageous in SMT compared with tradi-
tional dependency parsers.
4 Conclusion
We have constructed a string-to-dependency trans-
lation platform for comparing non-isomorphic tar-
get dependency structures. Specially, we proposed
an algorithm for generating word-based dependency
trees from PASs which are generated by a state-of-
the-art HPSG parser. We found that dependency
trees transformed from these HPSG PASs achieved
the best dependency/translation accuracies.
103
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments and suggestions.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
In Proceedings of NODALIDA, Tartu, Estonia, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Jeff Ma and Spyros Matsoukas. 2011. Bbn?s systems
for the chinese-english sub-task of the ntcir-9 patentmt
evaluation. In Proceedings of NTCIR-9, pages 579?
584.
David Magerman. 1995. Statistical decision-tree models
for parsing. In In Proceedings of of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on HLT, pages 114?119,
Plainsboro.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197?230.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 91?98, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT,
pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585, Colum-
bus, Ohio.
104
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678?683,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptation Data Selection using Neural Language Models:
Experiments in Machine Translation
Kevin Duh, Graham Neubig
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Japan
kevinduh@is.naist.jp
neubig@is.naist.jp
Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Labs.
NTT Corporation
2-4 Hikaridai, Seika, Kyoto, Japan
sudoh.katsuhito@lab.ntt.co.jp
tsukada.hajime@lab.ntt.co.jp
Abstract
Data selection is an effective approach
to domain adaptation in statistical ma-
chine translation. The idea is to use lan-
guage models trained on small in-domain
text to select similar sentences from large
general-domain corpora, which are then
incorporated into the training data. Sub-
stantial gains have been demonstrated in
previous works, which employ standard n-
gram language models. Here, we explore
the use of neural language models for data
selection. We hypothesize that the con-
tinuous vector representation of words in
neural language models makes them more
effective than n-grams for modeling un-
known word contexts, which are prevalent
in general-domain text. In a comprehen-
sive evaluation of 4 language pairs (En-
glish to German, French, Russian, Span-
ish), we found that neural language mod-
els are indeed viable tools for data se-
lection: while the improvements are var-
ied (i.e. 0.1 to 1.7 gains in BLEU), they
are fast to train on small in-domain data
and can sometimes substantially outper-
form conventional n-grams.
1 Introduction
A perennial challenge in building Statistical Ma-
chine Translation (SMT) systems is the dearth
of high-quality bitext in the domain of interest.
An effective and practical solution is adaptation
data selection: the idea is to use language models
(LMs) trained on in-domain text to select similar
sentences from large general-domain corpora. The
selected sentences are then incorporated into the
SMT training data. Analyses have shown that this
augmented data can lead to better statistical esti-
mation or word coverage (Duh et al, 2010; Had-
dow and Koehn, 2012).
Although previous works in data selection (Ax-
elrod et al, 2011; Koehn and Haddow, 2012; Ya-
suda et al, 2008) have shown substantial gains, we
suspect that the commonly-used n-gram LMs may
be sub-optimal. The small size of the in-domain
text implies that a large percentage of general-
domain sentences will contain words not observed
in the LM training data. In fact, as many as 60% of
general-domain sentences contain at least one un-
known word in our experiments. Although the LM
probabilities of these sentences could still be com-
puted by resorting to back-off and other smoothing
techniques, a natural question remains: will alter-
native, more robust LMs do better?
We hypothesize that the neural language model
(Bengio et al, 2003) is a viable alternative, since
its continuous vector representation of words is
well-suited for modeling sentences with frequent
unknown words, providing smooth probability es-
timates of unseen but similar contexts. Neu-
ral LMs have achieved positive results in speech
recognition and SMT reranking (Schwenk et al,
2012; Mikolov et al, 2011a). To the best of our
knowledge, this paper is the first work that exam-
ines neural LMs for adaptation data selection.
2 Data Selection Method
We employ the data selection method of (Ax-
elrod et al, 2011), which builds upon (Moore
and Lewis, 2010). The intuition is to select
general-domain sentences that are similar to in-
domain text, while being dis-similar to the average
general-domain text.
To do so, one defines the score of an general-
domain sentence pair (e, f) as:
[INE(e)?GENE(e)] + [INF (f)?GENF (f)]
(1)
where INE(e) is the length-normalized cross-
entropy of e on the English in-domain LM.
GENE(e) is the length-normalized cross-entropy
678
Figure 1: Recurrent neural LM.
of e on the English general-domain LM, which
is built from a sub-sample of the general-domain
text. Similarly, INF (f) and GENF (f) are the
cross-entropies of f on Foreign-side LM. Finally,
sentence pairs are ranked according to Eq. 1 and
those with scores lower than some empirically-
chosen threshold are added to the bitext for trans-
lation model training.
2.1 Neural Language Models
The four LMs used to compute Eq. 1 have con-
ventionally been n-grams. N-grams of the form
p(w(t)|w(t ? 1), w(t ? 2), . . .) predict words by
using multinomial distributions conditioned on the
context (w(t?1), w(t?2), . . .). But when the con-
text is rare or contains unknown words, n-grams
are forced to back-off to lower-order models, e.g.
p(w(t)|w(t ? 1)). These backoffs are unfortu-
nately very frequent in adaptation data selection.
Neural LMs, in contrast, model word probabili-
ties using continuous vector representations. Fig-
ure 1 shows a type of neural LMs called recurrent
neural networks (Mikolov et al, 2011b).1 Rather
than representing context as an identity (n-gram
hit-or-miss) function on [w(t ? 1), w(t ? 2), . . .],
neural LMs summarize the context by a hidden
state vector s(t). This is a continuous vector of
dimension |S| whose elements are predicted by
the previous word w(t ? 1) and previous state
s(t ? 1). This is robust to rare contexts because
continuous representations enable sharing of sta-
tistical strength between similar contexts. Bengio
(2009) shows that such representations are better
than multinomials in alleviating sparsity issues.
1Another major type of neural LMs are the so-called
feed-forward networks (Bengio et al, 2003; Schwenk, 2007;
Nakamura et al, 1990). Both types of neural LMs have seen
many improvements recently, in terms of computational scal-
ability (Le et al, 2011) and modeling power (Arisoy et al,
2012; Wu et al, 2012; Alexandrescu and Kirchhoff, 2006).
We focus on recurrent networks here since there are fewer
hyper-parameters and its ability to model infinite context us-
ing recursion is theoretically attractive. But we note that feed-
forward networks are just as viable.
Now, given state vector s(t), we can predict the
probability of the current word. Figure 1 is ex-
pressed formally in the following equations:
w(t) = [w0(t), . . . , wk(t), . . . w|W |(t)] (2)
wk(t) = g
?
?
|S|?
j=0
sj(t)Vkj
?
? (3)
sj(t)=f
?
?
|W |?
i=0
wi(t? 1)Uji +
|S|?
i?=0
si?(t? 1)Aji?
?
?
(4)
Here, w(t) is viewed as a vector of dimension
|W | (vocabulary size) where each element wk(t)
represents the probability of the k-th vocabulary
item at sentence position t. The function g(zk) =
ezk/?k ezk is a softmax function that ensures the
neural LM outputs are proper probabilities, and
f(z) = 1/(1 + e?z) is a sigmoid activation that
induces the non-linearity critical to the neural net-
work?s expressive power. The matrices V , U , and
A are trained by maximizing likelihood on train-
ing data using a ?backpropagation-through-time?
method.2 Intuitively, U and A compress the con-
text (|S| < |W |) such that contexts predictive of
the same word w(t) are close together.
Since proper modeling of unknown contexts is
important in our problem, training text for both n-
gram and neural LM is pre-processed by convert-
ing all low-frequency words in the training data
(frequency=1 in our case) to a special ?unknown?
token. This is used only in Eq. 1 for selecting
general-domain sentences; these words retain their
surface forms in the SMT train pipeline.
3 Experiment Setup
We experimented with four language pairs in the
WIT3 corpus (Cettolo et al, 2012), with English
(en) as source and German (de), Spanish (es),
French (fr), Russian (ru) as target. This is the
in-domain corpus, and consists of TED Talk tran-
scripts covering topics in technology, entertain-
ment, and design. As general-domain corpora,
we collected bitext from the WMT2013 campaign,
including CommonCrawl and NewsCommentary
for all 4 languages, Europarl for de/es/fr, UN for
es/fr, Gigaword for fr, and Yandex for ru. The in-
domain data is divided into a training set (for SMT
2The recurrent states are unrolled for several time-steps,
then stochastic gradient descent is applied.
679
en-de en-es en-fr en-ru
In-domain Training Set
#sentence 129k 140k 139k 117k
#token (en) 2.5M 2.7M 2.7M 2.3M
#vocab (en) 26k 27k 27k 25k
#vocab (f) 42k 39k 34k 58k
General-domain Bitext
#sentence 4.4M 14.7M 38.9M 2.0M
#token (en) 113M 385M 1012M 51M
%unknown 60% 58% 64% 65%
Table 1: Data statistics. ?%unknown?=fraction of
general-domain sentences with unknown words.
pipeline and neural LM training), a tuning set (for
MERT), a validation set (for choosing the optimal
threshold in data selection), and finally a testset of
1616 sentences.3 Table 1 lists data statistics.
For each language pair, we built a baseline in-
data SMT system trained only on in-domain data,
and an alldata system using combined in-domain
and general-domain data.4 We then built 3 systems
from augmented data selected by different LMs:
? ngram: Data selection by 4-gram LMs with
Kneser-Ney smoothing (Axelrod et al, 2011)
? neuralnet: Data selection by Recurrent neu-
ral LM, with the RNNLM Toolkit.5
? combine: Data selection by interpolated LM
using n-gram & neuralnet (equal weight).
All systems are built using standard settings in
the Moses toolkit (GIZA++ alignment, grow-diag-
final-and, lexical reordering models, and SRILM).
Note that standard n-grams are used as LMs for
SMT; neural LMs are only used for data selection.
Multiple SMT systems are trained by thresholding
on {10k,50k,100k,500k,1M} general-domain sen-
tence subsets, and we empirically determine the
single system for testing based on results on a sep-
arate validation set (in practice, 500k was chosen
for fr and 1M for es, de, ru.).
3The original data are provided by http://wit3.fbk.eu and
http://www.statmt.org/wmt13/. Our domain adaptation sce-
nario is similar to the IWSLT2012 campaign but we used our
own random train/test splits, since we wanted to ensure the
testset for all languages had identical source sentences for
comparison purposes. For replicability, our software is avail-
able at http://cl.naist.jp/?kevinduh/a/acl2013.
4More advanced phrase table adaptation methods are pos-
sible. but our interest is in comparing data selection methods.
The conclusions should transfer to advanced methods such as
(Foster et al, 2010; Niehues and Waibel, 2012).
5http://www.fit.vutbr.cz/?imikolov/rnnlm/
4 Results
4.1 LM Perplexity and Training Time
First, we measured perplexity to check the gen-
eralization ability of our neural LMs as language
models. Recall that we train four LMs to com-
pute each of the components of Eq. 1. In Table 2,
we compared each of the four versions of ngram,
neuralnet, and combine LMs on in-domain test
sets or general-domain held-out sets. It re-affirms
previous positive results (Mikolov et al, 2011a),
with neuralnet outperforming ngram by 20-30%
perplexity across all tasks. Also, combine slightly
improves the perplexity of neuralnet.
Task ngram neuralnet combine
In-Domain Test Set
en-de de 157 110 (29%) 110 (29%)
en-de en 102 81 (20%) 78 (24%)
en-es es 129 102 (20%) 98 (24%)
en-es en 101 80 (21%) 77 (24%)
en-fr fr 90 67 (25%) 65 (27%)
en-fr en 102 80 (21%) 77 (24%)
en-ru ru 208 167 (19%) 155 (26%)
en-ru en 103 83 (19%) 79 (23%)
General-Domain Held-out Set
en-de de 234 174 (25%) 161 (31%)
en-de en 218 168 (23%) 155 (29%)
en-es es 62 43 (31%) 43 (31%)
en-es en 84 61 (27%) 59 (30%)
en-fr fr 64 43 (33%) 43 (33%)
en-fr en 95 67 (30%) 65 (32%)
en-ru ru 242 199 (18%) 176 (27%)
en-ru en 191 153 (20%) 142 (26%)
Table 2: Perplexity of various LMs. Number in
parenthesis is percentage improvement vs. ngram.
Second, we show that the usual concern of neu-
ral LM training time is not so critical for the in-
domain data sizes used domain adaptation. The
complexity of training Figure 1 is dominated by
computing Eq. 3 and scales as O(|W | ? |S|) in
the number of tokens. Since |W | can be large, one
practical trick is to cluster the vocabulary so that
the output dimension is reduced. Table 3 shows
the training times on a 3.3GHz XeonE5 CPU by
varying these two main hyper-parameters (|S| and
cluster size). Note that the setting |S| = 200 and
cluster size of 100 already gives good perplexity
in reasonable training time. All neural LMs in this
paper use this setting, without additional tuning.
680
|S| Cluster Time Perplexity
200 100 198m 110
100 |W | 12915m 110
200 400 208m 113
100 100 52m 118
100 400 71m 120
Table 3: Training time (in minutes) for various
neural LM architectures (Task: en-de de).
4.2 End-to-end SMT Evaluation
Table 4 shows translation results in terms of BLEU
(Papineni et al, 2002), RIBES (Isozaki et al,
2010), and TER (Snover et al, 2006). We observe
that all three data selection methods essentially
outperform alldata and indata for all language
pairs, and neuralnet tend to be the best in all met-
rics. E.g., BLEU improvements over ngram are
in the range of 0.4 for en-de, 0.5 for en-es, 0.1
for en-fr, and 1.7 for en-ru. Although not all im-
provements are large in absolute terms, many are
statistically significant (95% confidence).
We therefore believe that neural LMs are gen-
erally worthwhile to try for data selection, as it
rarely underperform n-grams. The open question
is: what can explain the significant improvements
in, for example Russian, Spanish, German, but the
lack thereof in French? One conjecture is that
neural LMs succeeded in lowering testset out-of-
vocabulary (OOV) rate, but we found that OOV
reduction is similar across all selection methods.
The improvements appear to be due to better
probability estimates of the translation/reordering
models. We performed a diagnostic by decoding
the testset using LMs trained on the same test-
set, while varying the translation/reordering ta-
bles with those of ngram and neuralnet; this is a
kind of pseudo forced-decoding that can inform us
about which table has better coverage. We found
that across all language pairs, BLEU differences of
translations under this diagnostic become insignif-
icant, implying that the raw probability value is
the differentiating factor between ngram and neu-
ralnet. Manual inspection of en-de revealed that
many improvements come from lexical choice in
morphological variants (?meinen Sohn? vs. ?mein
Sohn?), segmentation changes (?baking soda? ?
?Backpulver? vs. ?baken Soda?), and handling of
unaligned words at phrase boundaries.
Finally, we measured the intersection between
the sentence set selected by ngram vs neural-
Task System BLEU RIBES TER
en-de indata 20.8 80.1 59.0
alldata 21.5 80.1 59.1
ngram 21.5 80.3 58.9
neuralnet 21.9+ 80.5+ 58.4
combine 21.5 80.2 58.8
en-es indata 30.4 83.5 48.7
alldata 31.2 83.2 49.9
ngram 32.0 83.7 48.4
neuralnet 32.5+ 83.7 48.3+
combine 32.5+ 83.8 48.3+
en-fr indata 31.4 83.9 51.2
alldata 31.5 83.5 51.4
ngram 32.7 83.7 50.4
neuralnet 32.8 84.2+ 50.3
combine 32.5 84.0 50.5
en-ru indata 14.8 72.5 69.5
alldata 23.4 75.0 62.3
ngram 24.0 75.7 61.4
neuralnet 25.7+ 76.1 60.0+
combine 23.7 75.9 61.9?
Table 4: End-to-end Translation Results. The best
results are bold-faced. We also compare neural
LMs to ngram using pairwise bootstrap (Koehn,
2004): ?+? means statistically significant im-
provement and ??? means significant degradation.
net. They share 60-75% of the augmented train-
ing data. This high overlap means that ngram
and neuralnet are actually not drastically different
systems, and neuralnet with its slightly better se-
lections represent an incremental improvement.6
5 Conclusions
We perform an evaluation of neural LMs for
adaptation data selection, based on the hypothe-
sis that their continuous vector representations are
effective at comparing general-domain sentences,
which contain frequent unknown words. Com-
pared to conventional n-grams, we observed end-
to-end translation improvements from 0.1 to 1.7
BLEU. Since neural LMs are fast to train in the
small in-domain data setting and achieve equal or
incrementally better results, we conclude that they
are an worthwhile option to include in the arsenal
of adaptation data selection techniques.
6This is corroborated by another analysis: taking the
union of sentences found by ngram and neuralnet gives sim-
ilar BLEU scores as neuralnet.
681
Acknowledgments
We thank Amittai Axelrod for discussions about
data selection implementation details, and an
anonymous reviewer for suggesting the union idea
for results analysis. K. D. would like to credit Spy-
ros Matsoukas (personal communication, 2010)
for the trick of using LM-based pseudo forced-
decoding for error analysis.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Pa-
pers, NAACL-Short ?06, pages 1?4, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20?28, Montre?al, Canada,
June. Association for Computational Linguistics.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage models. JMLR.
Yoshua Bengio. 2009. Learning Deep Architectures
for AI, volume Foundations and Trends in Machine
Learning. NOW Publishers.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261?268, Trento, Italy,
May.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation for
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT) - Technical Papers Track.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on smt systems. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 422?432, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 944?952, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In WMT.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural net-
work language model. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
training large scale neural network language model.
In ASRU.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model.
In Proceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Masami Nakamura, Katsuteru Maruyama, Takeshi
Kawabata, and Kiyohiro Shikano. 1990. Neural
network approach to word category prediction for
english texts. In Proceedings of the 13th conference
on Computational linguistics - Volume 3, COLING
?90, pages 213?218, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jan Niehues and Alex Waibel. 2012. Detailed analysis
of different strategies for phrase table adaptation in
SMT. In AMTA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
682
HLT, pages 11?19, Montre?al, Canada, June. Associ-
ation for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518,
July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Youzheng Wu, Xugang Lu, Hitoshi Yamamoto,
Shigeki Matsuda, Chiori Hori, and Hideki Kashioka.
2012. Factored language model based on recurrent
neural network. In Proceedings of COLING 2012,
pages 2835?2850, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In ICJNLP.
683
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143?149,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
On the Elements of an Accurate
Tree-to-String Machine Translation System
Graham Neubig, Kevin Duh
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
{neubig,kevinduh}@is.naist.jp
Abstract
While tree-to-string (T2S) translation the-
oretically holds promise for efficient, ac-
curate translation, in previous reports T2S
systems have often proven inferior to other
machine translation (MT) methods such as
phrase-based or hierarchical phrase-based
MT. In this paper, we attempt to clarify
the reason for this performance gap by
investigating a number of peripheral ele-
ments that affect the accuracy of T2S sys-
tems, including parsing, alignment, and
search. Based on detailed experiments
on the English-Japanese and Japanese-
English pairs, we show how a basic T2S
system that performs on par with phrase-
based systems can be improved by 2.6-4.6
BLEU, greatly exceeding existing state-
of-the-art methods. These results indi-
cate that T2S systems indeed hold much
promise, but the above-mentioned ele-
ments must be taken seriously in construc-
tion of these systems.
1 Introduction
In recent years, syntactic parsing is being viewed
as an ever-more important element of statistical
machine translation (SMT) systems, particularly
for translation between languages with large dif-
ferences in word order. There are many ways of
incorporating syntax into MT systems, including
the use of string-to-tree translation (S2T) to ensure
the syntactic well-formedness of the output (Gal-
ley et al, 2006; Shen et al, 2008), tree-to-string
(T2S) using source-side parsing as a hint during
the translation process (Liu et al, 2006), or pre-
or post-ordering to help compensate for reorder-
ing problems experienced by non-syntactic meth-
ods such as phrase-based MT (PBMT) (Collins et
al., 2005; Sudoh et al, 2011). Among these, T2S
translation has a number of attractive theoretical
properties, such as joint consideration of global re-
ordering and lexical choice while maintaining rel-
atively fast decoding times.
However, building an accurate T2S system is
not trivial. On one hand, there have been multiple
reports (mainly from groups with a long history
of building T2S systems) stating that systems us-
ing source-side syntax greatly out-perform phrase-
based systems (Mi et al, 2008; Liu et al, 2011;
Zhang et al, 2011; Tamura et al, 2013). On the
other hand, there have been also been multiple re-
ports noting the exact opposite result that source-
side syntax systems perform worse than Hiero,
S2T, PBMT, or PBMT with pre-ordering (Ambati
and Lavie, 2008; Xie et al, 2011; Kaljahi et al,
2012). In this paper, we argue that this is due to the
fact that T2S systems have the potential to achieve
high accuracy, but are also less robust, with a num-
ber of peripheral elements having a large effect on
translation accuracy.
Our motivation in writing this paper is to pro-
vide a first step in examining and codifying the
more important elements that make it possible to
construct a highly accurate T2S MT system. To do
so, we perform an empirical study of the effect of
parsing accuracy, packed forest input, alignment
accuracy, and search. The reason why we choose
these elements is that past work that has reported
low accuracy for T2S systems has often neglected
to consider one or all of these elements.
As a result of our tests on English-Japanese (en-
ja) and Japanese-English (ja-en) machine transla-
tion, we find that a T2S system not considering
these elements performs only slightly better than a
standard PBMT system. However, after account-
ing for all these elements we see large increases of
accuracy, with the final system greatly exceeding
not only standard PBMT, but also state-of-the-art
methods based on syntactic pre- or post-ordering.
143
2 Experimental Setup
2.1 Systems Compared
In our experiments, we use a translation model
based on T2S tree transducers (Graehl and Knight,
2004), constructed using the Travatar toolkit (Neu-
big, 2013). Rules are extracted using the GHKM
algorithm (Galley et al, 2006), and rules with
up to 5 composed minimal rules, up to 2 non-
terminals, and up to 10 terminals are used.
We also prepare 3 baselines not based on T2S
to provide a comparison with other systems in the
literature. The first two baselines are standard sys-
tems using PBMT or Hiero trained using Moses
(Koehn et al, 2007). We use default settings, ex-
cept for setting the reordering limit or maximum
chart span to the best-performing value of 24. As
our last baselines, we use two methods based on
syntactic pre- or post-ordering, which are state-of-
the-art methods for the language pairs. Specifi-
cally, for en-ja translation we use the head finaliza-
tion pre-ordering method of (Isozaki et al, 2010b),
and for ja-en translation, we use the syntactic post-
ordering method of (Goto et al, 2012). For all
systems, T2S or otherwise, the language model is
a Kneser-Ney 5-gram, and tuning is performed to
maximize BLEU score using minimum error rate
training (Och, 2003).
2.2 Data and Evaluation
We perform all of our experiments on en-ja
and ja-en translation over data from the NTCIR
PatentMT task (Goto et al, 2011), the most stan-
dard benchmark task for these language pairs. We
use the training data from NTCIR 7/8, a total of
approximately 3.0M sentences, and perform tun-
ing on the NTCIR 7 dry run, testing on the NTCIR
7 formal run data. As evaluation measures, we use
the standard BLEU (Papineni et al, 2002) as well
as RIBES (Isozaki et al, 2010a), a reordering-
based metric that has been shown to have high
correlation with human evaluations on the NTCIR
data. We measure significance of results using
bootstrap resampling at p < 0.05 (Koehn, 2004).
In tables, bold numbers indicate the best system
and all systems that were not significantly differ-
ent from the best system.
2.3 Motivational Experiment
Before going into a detailed analysis, we first
present results that stress the importance of the el-
ements described in the introduction. To do so,
en-ja ja-en
System BLEU RIBES BLEU RIBES
PBMT 35.84 72.89 30.49 69.80
Hiero 34.45 72.94 29.41 69.51
Pre/Post 36.69 77.05 29.42 73.85
T2S-all 36.23 76.60 31.15 72.87
T2S+all 40.84 80.15 33.70 75.94
Table 1: Overall results for five systems.
we compare the 3 non-T2S baselines with two
T2S systems that vary the settings of the parser,
alignment, and search, as described in the follow-
ing Sections 3, 4, and 5. The first system ?T2S-
all? is a system that uses the worst settings
1
for
each of these elements, while the second system
?T2S+all? uses the best settings.
2
The results for
the systems are shown in Table 1.
The most striking result is that T2S+all signif-
icantly exceeds all of the baselines, even includ-
ing the pre/post-ordering baselines, which provide
state-of-the-art results on this task. The gains are
particularly striking on en-ja, with a gain of over 4
BLEU points over the closest system, but still sig-
nificant on the ja-en task, where the use of source-
side syntax has proven less effective in previous
work (Sudoh et al, 2011). The next thing to notice
is that if we had instead used T2S-all, our conclu-
sion would have been much different. This system
is able to achieve respectable accuracy compared
to PBMT or Hiero, but does not exceed the more
competitive pre/post-ordering systems.
3
With this
result in hand, we will investigate the contribution
of each of these elements in detail in the following
sections. In the remainder of the paper settings
follow T2S+all except when otherwise noted.
3 Parsing
3.1 Parsing Overview
As T2S translation uses parse trees both in train-
ing and testing of the system, an accurate syntactic
parser is required. In order to test the extent that
parsing accuracy affects translation, we use two
1
Stanford/Eda, GIZA++, pop-limit 5000 cube pruning.
2
Egret forests, Nile, pop-limit 5000 hypergraph search.
3
We have also observed similar trends on other genres and
language pairs. For example, in a Japanese-Chinese/English
medical conversation task (Neubig et al, 2013), forests,
alignment, and search resulted in BLEU increases of en-ja
24.55?30.81, ja-en 19.28?22.46, zh-ja 15.22?20.67, ja-zh
30.88?33.89.
144
different syntactic parsers and examine the trans-
lation accuracy realized by each parser.
For English, the two most widely referenced
parsers are the Stanford Parser and Berkeley
Parser. In this work, we compare the Stanford
Parser?s CFG model, with the Berkeley Parser?s
latent variable model. In previous reports, it has
been noted (Kummerfeld et al, 2012) that the la-
tent variable model of the Berkeley parser tends to
have the higher accuracy of the two, so if the accu-
racy of a system using this model is higher then it
is likely that parsing accuracy is important for T2S
translation. Instead of the Berkeley Parser itself,
we use a clone Egret,
4
which achieves nearly iden-
tical accuracy, and is able to output packed forests
for use in MT, as mentioned below. Trees are
right-binarized, with the exception of phrase-final
punctuation, which is split off before any other el-
ement in the phrase.
For Japanese, our first method uses the MST-
based pointwise dependency parser of Flannery et
al. (2011), as implemented in the Eda toolkit.
5
In order to convert dependencies into phrase-
structure trees typically used in T2S translation,
we use the head rules implemented in the Travatar
toolkit. In addition, we also train a latent variable
CFG using the Berkeley Parser and use Egret for
parsing. Both models are trained on the Japanese
Word Dependency Treebank (Mori et al, 2014).
In addition, Mi et al (2008) have proposed a
method for forest-to-string (F2S) translation us-
ing packed forests to encode many possible sen-
tence interpretations. By doing so, it is possible to
resolve some of the ambiguity in syntactic inter-
pretation at translation time, potentially increasing
translation accuracy. However, the great majority
of recent works on T2S translation do not consider
multiple syntactic parses (e.g. Liu et al (2011),
Zhang et al (2011)), and thus it is important to
confirm the potential gains that could be acquired
by taking ambiguity into account.
3.2 Effect of Parsing and Forest Input
In Table 2 we show the results for Stanford/Eda
with 1-best tree input vs. Egret with trees or
forests as input. Forests are those containing all
edges in the 100-best parses.
First looking at the difference between the two
parsers, we can see that the T2S system using
4
http://code.google.com/p/egret-parser
5
http://plata.ar.media.kyoto-u.ac.jp/tool/EDA
en-ja ja-en
System BLEU RIBES BLEU RIBES
Stan/Eda 38.95 78.47 32.56 73.03
Egret-T 39.26 79.26 32.97 74.94
Egret-F 40.84 80.15 33.70 75.94
Table 2: Results for Stanford/Eda, Egret with tree
input, and Egret with forest input.
1 0 100
Forest n-best Cutoff
0.0
0.2
0.4
0.6
0.8
1.0
B
LE
U 39
40
41
42
1.75
2.03
2.58
3.06 3.21 3.70
3.73
4.61en-ja
ja-en
1 10 100
32
33
34
35
1.21 1.32
1.39 1.50
1.61 1.74 2.05
2.07
Figure 1: BLEU scores using various levels of for-
est pruning. Numbers in the graph indicate decod-
ing time in seconds/sentence.
Egret achieves greater accuracy than that using the
other two parsers. This improvement is particu-
larly obvious in RIBES, indicating that an increase
in parsing accuracy has a larger effect on global
reordering than on lexical choice. When going
from T2S to F2S translation using Egret, we see
another large gain in accuracy, although this time
with the gain in BLEU being more prominent. We
believe this is related to the observation of Zhang
and Chiang (2012) that F2S translation is not nec-
essarily helping fixing parsing errors, but instead
giving the translation system the freedom to ignore
the parse somewhat, allowing for less syntactically
motivated but more fluent translations.
As passing some degree of syntactic ambigu-
ity on to the decoder through F2S translation has
proven useful, a next natural question is how much
of this ambiguity we need to preserve in our forest.
The pruning criterion that we use for the forest is
based on including all edges that appear in one or
more of the n-best parses, so we perform transla-
tion setting n to 1 (trees), 3, 6, 12, 25, 50, 100, and
200. Figure 1 shows results for these settings with
regards to translation accuracy and speed. Over-
all, we can see that every time we double the size
of the forest we get an approximately linear in-
145
crease in BLEU at the cost of an increase in decod-
ing time. Interestingly, the increases in BLEU did
not show any sign of saturating even when setting
the n-best cutoff to 200, although larger cutoffs re-
sulted in exceedingly large translation forests that
required large amounts of memory.
4 Alignment
4.1 Alignment Overview
The second element that we investigate is align-
ment accuracy. It has been noted in many previ-
ous works that significant gains in alignment accu-
racy do not make a significant difference in trans-
lation results (Ayan and Dorr, 2006; Ganchev et
al., 2008). However, none of these works have ex-
plicitly investigated the effect on T2S translation,
so it is not clear whether these results carry over to
our current situation.
As our baseline aligner, we use the GIZA++ im-
plementation of the IBM models (Och and Ney,
2003) with the default options. To test the effect
of improved alignment accuracy, we use the dis-
criminative alignment method of Riesa and Marcu
(2010) as implemented in the Nile toolkit.
6
This
method has the ability to use source- and target-
side syntactic information, and has been shown to
improve the accuracy of S2T translation.
We trained Nile and tested both methods on
the Japanese-English alignments provided with
the Kyoto Free Translation Task (Neubig, 2011)
(430k parallel sentences, 1074 manually aligned
training sentences, and 120 manually aligned test
sentences).
7
As creating manual alignment data is
costly, we also created two training sets that con-
sisted of 1/4 and 1/16 of the total data to test if
we can achieve an effect with smaller amounts of
manually annotated data. The details of data size
and alignment accuracy are shown in Table 3.
4.2 Effect of Alignment on Translation
In Table 4, we show results when we vary the
aligner between GIZA++ and Nile. For reference,
we also demonstrate results when using the same
alignments for PBMT and Hiero.
From this, we can see that while for PBMT and
Hiero systems the results are mixed, as has been
noted in previous work (Fraser and Marcu, 2007),
6
http://code.google.com/p/nile
7
This data is from Wikipedia articles about Kyoto City,
and is an entirely different genre than our MT test data. It is
likely that creating aligned data that matches the MT genre
would provide larger gains in MT accuracy.
Name Sent. Prec. Rec. F-meas
GIZA++ 0 60.46 55.48 57.86
Nile/16 68 70.21 60.81 65.17
Nile/4 269 72.85 62.70 67.40
Nile 1074 72.73 63.97 68.07
Table 3: Alignment accuracy (%) by method and
number of manually annotated training sentences.
en-ja ja-en
System BLEU RIBES BLEU RIBES
PBMT-G 35.84 72.89 30.49 69.80
PBMT-N 36.05 71.84 30.77 69.75
Hiero-G 34.45 72.94 29.41 69.51
Hiero-N 33.90 72.63 28.90 69.83
T2S-G 39.57 78.94 32.62 75.19
T2S-N/16 40.79 80.05 32.82 74.89
T2S-N/4 40.97 80.32 33.35 75.46
T2S-N 40.84 80.15 33.70 75.94
Table 4: Results varying the aligner (GIZA++ vs.
Nile), including results for Nile when using 1/4 or
1/16 of the annotated training data.
Figure 2: Probabilities for SVO?SOV rules.
improving the alignment accuracy gives signifi-
cant gains for T2S translation. The reason for this
difference is two-fold. The first is that in rule
extraction in syntax-based translation (Galley et
al., 2006), a single mistaken alignment crossing
phrase boundaries results not only in a bad rule be-
ing extracted, but also prevents the extraction of a
number of good rules. This is reflected in the size
of the rule table; the en-ja system built using Nile
contains 92.8M rules, while the GIZA++ system
contains only 83.3M rules, a 11.2% drop.
The second reason why alignment is important
is that while one of the merits of T2S models is
their ability to perform global re-ordering, it is dif-
ficult to learn good reorderings from bad align-
ments. We show an example of this in Figure 2.
When translating SVO English to SOV Japanese,
we expect rules containing a verb and a following
noun phrase (VO) to have a high probability of be-
ing reversed (to OV), possibly with the addition of
146
the Japanese direct object particle ?wo.? From the
figure, we can see that the probabilities learned by
Nile match this intuition, while the probabilities
learned by GIZA heavily favor no reordering.
Finally, looking at the amount of data needed to
train the model, we can see that a relatively small
amount of manually annotated data proves suffi-
cient for large gains in alignment accuracy, with
even 68 sentences showing a 7.31 point gain in F-
measure over GIZA++. This is because Nile?s fea-
ture set uses generalizable POS/syntactic informa-
tion and also because mis-alignments of common
function words (e.g. a/the) will be covered even
by small sets of training data. Looking at the MT
results, we can see that even the smaller data sets
allow for gains in accuracy, although the gains are
more prominent for en-ja.
5 Search
5.1 Search Overview
Finally, we examine the effect that the choice of
search algorithm has on the accuracy of transla-
tion. The most standard search algorithm for T2S
translation is bottom-up beam search using cube
pruning (CP, Chiang (2007)). However, there are
a number of other search algorithms that have
been proposed for tree-based translation in gen-
eral (Huang and Chiang, 2007) or T2S systems
in particular (Huang and Mi, 2010; Feng et al,
2012). In this work, we compare CP and the hy-
pergraph search (HS) method of Heafield et al
(2013), which is also a bottom-up pruning algo-
rithm but performs more efficient search by group-
ing together similar language model states.
5.2 Effect of Search
Figure 3 shows BLEU and decoding speed results
using HS or CP on T2S and F2S translation, us-
ing a variety of pop limits. From this, we can see
that HS out-performs CP for both F2S and T2S,
especially with smaller pop limits. Comparing the
graphs for F2S and T2S translation, it is notable
that the shapes of the graphs for the two meth-
ods are strikingly similar. This result is somewhat
surprising, as the overall search space of F2S is
larger and it would be natural for the characteris-
tics of the search algorithm to vary between these
two settings. Finally, comparing ja-en and en-ja,
search is simpler for the former, a result of the fact
that the Japanese sentences contain more words,
and thus more LM evaluations per sentence.
100 0 10000
Pop Limit
0.0
0.2
0.4
0.6
0.8
1.0
B
LE
U
 (
F2
S)
37
38
39
40
41
42
0.33
0.42
0.72 1.07 1.81
3.73 5.60 9.59
0.33
0.43
0.66 1.04
1.77 4.43
9.60 17.40
en-ja HS
en-ja CP
100 1000 10000
30
31
32
33
34
35
0.22
0.30 0.41 0.58 0.91 2.05 3.54 6.44
0.24
0.32
0.43 0.71
1.01 2.29 4.73 9.18
ja-en HS
ja-en CP
100 0 10000
Pop Limit
0.0
0.2
0.4
0.6
0.8
1.0
B
LE
U
 (
T2
S)
36
37
38
39
40
41
0.08
0.12
0.25 0.43
0.76 1.75 2.96 4.80
0.08
0.09
0.27
0.38 0.71
1.75 4.34 8.73
en-ja HS
en-ja CP
100 1000 10000
29
30
31
32
33
34
0.10 0.14 0.25 0.37 0.57 1.21 2.22 3.97
0.10
0.13
0.24 0.44 0.64
1.60 3.83 5.74
ja-en HS
ja-en CP
Figure 3: Hypergraph search (HS) and cube
pruning (CP) results for F2S and T2S. Numbers
above and below the lines indicate time in sec-
onds/sentence for HS and CP respectively.
6 Conclusion
In this paper, we discussed the importance of three
peripheral elements that contribute greatly to the
accuracy of T2S machine translation: parsing,
alignment, and search. Put together, a T2S sys-
tem that uses the more effective settings for these
three elements greatly outperforms a system that
uses more standard settings, as well as the current
state-of-the-art on English-Japanese and Japanese-
English translation tasks.
Based on these results we draw three conclu-
sions. The first is that given the very competitive
results presented here, T2S systems do seem to
have the potential to achieve high accuracy, even
when compared to strong baselines incorporating
syntactic reordering into a phrase-based system.
The second is that when going forward with re-
search on T2S translation, one should first be sure
to account for these three elements to ensure a
sturdy foundation for any further improvements.
Finally, considering the fact that parsing and align-
ment for each of these languages is far from per-
fect, further research investment in these fields
may very well have the potential to provide ad-
ditional gains in accuracy in the T2S framework.
Acknowledgments: This work was supported
by JSPS KAKENHI Grant Number 25730136.
147
References
Vamshi Ambati and Alon Lavie. 2008. Improving syn-
tax driven translation models by re-structuring diver-
gent and non-isomorphic parse tree structures. In
Proc. AMTA, pages 235?244.
Necip Ayan and Bonnie Dorr. 2006. Going beyond
AER: an extensive analysis of word alignments and
their impact on MT. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531?540.
Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn.
2012. Left-to-right tree-to-string decoding with pre-
diction. In Proc. EMNLP, pages 1191?1200.
Daniel Flannery, Yusuke Miyao, Graham Neubig, and
Shinsuke Mori. 2011. Training dependency parsers
from partially annotated corpora. In Proc. IJCNLP,
pages 776?784.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL, pages 961?968.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
Proc. ACL.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR, volume 9, pages 559?578.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proc. ACL, pages
311?316.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT, pages 105?112.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words to
speed k?best extraction from hypergraphs. In Proc.
NAACL, pages 958?968.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. ACL, pages 144?151.
Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proc. EMNLP, pages 273?283.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proc. WMT
and MetricsMATR.
Rasoul Samad Zadeh Kaljahi, Raphael Rubino, Johann
Roturier, and Jennifer Foster. 2012. A detailed
analysis of phrase-based and syntax-based machine
translation: The search for systematic differences.
In Proc. AMTA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Jonathan K Kummerfeld, David Hall, James R Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: an empirical investigation of er-
ror types in parser output. In Proc. EMNLP, pages
1048?1059.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL.
Yang Liu, Qun Liu, and Yajuan Lu?. 2011. Adjoin-
ing tree-to-string translation. In Proc. ACL, pages
1278?1287.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL, pages 192?199.
Shinsuke Mori, Hideki Ogura, and Tetsuro Sasada.
2014. A Japanese word dependency corpus. In
Proc. LREC.
Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi
Nakamura, Yuji Matsumoto, Ryosuke Isotani, and
Yukichi Ikeda. 2013. Towards high-reliability
speech translation in the medical domain. In Proc.
MedNLP, pages 22?29.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Graham Neubig. 2013. Travatar: A forest-to-string
machine translation engine based on tree transduc-
ers. In Proc. ACL Demo Track, pages 91?96.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
148
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proc. ACL, pages
157?166.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ACL, pages 577?585.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering
in statistical machine translation. In Proc. MT Sum-
mit.
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-
roya Takamura, and Manabu Okumura. 2013. Part-
of-speech induction in dependency trees for statisti-
cal machine translation. In Proc. ACL, pages 841?
851.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proc. EMNLP, pages 216?226.
Hui Zhang and David Chiang. 2012. An exploration
of forest-to-string translation: Does translation help
or hurt parsing? In Proc. ACL, pages 317?321.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized forest to string translation. In Proc.
ACL, pages 835?845.
149
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 383?386,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MSS: Investigating the Effectiveness of Domain Combinations and
Topic Features for Word Sense Disambiguation
Sanae Fujita Kevin Duh Akinori Fujino Hirotoshi Taira Hiroyuki Shindo
NTT Communication Science Laboratories
{sanae, kevinduh, taira, a.fujino, shindo}@cslab.kecl.ntt.co.jp
Abstract
We participated in the SemEval-2010
Japanese Word Sense Disambiguation
(WSD) task (Task 16) and focused on
the following: (1) investigating domain
differences, (2) incorporating topic fea-
tures, and (3) predicting new unknown
senses. We experimented with Support
Vector Machines (SVM) and Maximum
Entropy (MEM) classifiers. We achieved
80.1% accuracy in our experiments.
1 Introduction
We participated in the SemEval-2010 Japanese
Word Sense Disambiguation (WSD) task (Task 16
(Okumura et al, 2010)), which has two new char-
acteristics: (1) Both training and test data across
3 or 4 domains. The training data include books
or magazines (called PB), newspaper articles (PN),
and white papers (OW). The test data also include
documents from a Q&A site on the WWW (OC);
(2) Test data include new senses (called X) that are
not defined in dictionary.
There is much previous research on WSD. In
the case of Japanese, unsupervised approaches
such as extended Lesk have performed well (Bald-
win et al, 2010), although they are outperformed
by supervised approaches (Tanaka et al, 2007;
Murata et al, 2003). Therefore, we selected a su-
pervised approach and constructed Support Vector
Machines (SVM) and Maximum Entropy (MEM)
classifiers using common features and topic fea-
tures. We performed extensive experiments to in-
vestigate the best combinations of domains for
training.
We describe the data in Section 2, and our sys-
tem in Section 3. Then in Section 4, we show the
results and provide some discussion.
2 Data Description
2.1 Given Data
We show an example of Iwanami Kokugo Jiten
(Nishio et al, 1994), which is a dictionary used as
a sense inventory. As shown in Figure 1, each en-
try has POS information and definition sentences
including example sentences.
We show an example of the given training data
in (1). The given data are morphologically ana-
lyzed and partly tagged with Iwanami?s sense IDs,
such as '37713-0-0-1-1( in (1).
(1) <mor pos='??-?}( rd='??( bfm='?
?( sense= '37713-0-0-1-1( >1<</mor>
This task includes 50 target words that were
split into 219 senses in Iwanami; among them, 143
senses including two Xs that were not defined in
Iwanami, appear in the training data. In the test
data, 150 senses including eight Xs appear. The
training and test data share 135 senses including
two Xs; that is, 15 senses including six Xs in the
test data are unseen in the training data.
2.2 Data Pre-processing
We performed two preliminary pre-processing
steps. First, we restored the base forms because
the given training and test data have no informa-
tion about the base forms. (1) shows an example
of the original morphological data, and then we
added the base form (lemma), as shown in (2).
(2) <mor pos=' ? ?-? } ( rd=' ? ? (
bfm=' ? ? ( sense='37713-0-0-1-1(
lemma='1d(>1<</mor>
Secondly, we extracted example sentences from
Iwanami, which is used as a sense inventory. To
compensate for the lack of training data, we an-
alyzed examples with a morphological analyzer,
Mecab1 UniDic version, because the training and
test data were tagged with POS based on UniDic.
1http://mecab.sourceforge.net/
383
??
?
?
?
HEADWORD Ad91d[ddNd:take (?; Transitive Verb)
37713-0-0-1-0
[
<1> ??<8[GCBk3D?= to get something left into one?s hand
]
37713-0-0-1-1
[
<y> 3??=53k-<??(6
take and hold by hand. 'to lead someone by the hand(
]
?
?
?
?
?
Figure 1: Simplified Entry for Iwanami Kokugo Jiten: Ad take
For example, from the entry for Ad take, as
shown in Figure 1, we extracted an example sen-
tence and morphologically analyzed it, as shown
in (3)2, for the second sense, 37713-0-0-1-1. In
(3), the underlined part is the headword and is
tagged with 37713-0-0-1-1.
(3) 3
hand
k
ACC
1<
take
?
and
?(
lead
?(I) take someone?s hand and lead him/her?
3 System Description
3.1 Features
In this section, we describe the features we gener-
ated.
3.1.1 Baseline Features
For each target word w, we used the surface form,
the base form, the POS tag, and the top POS cat-
egories, such as nouns, verbs, and adjectives of
w. Here the target is the ith word, so we also
used the same information of i? 2, i? 1, i+ 1, and
i+2th words. We used bigrams, trigrams, and skip-
bigrams back and forth within three words. We re-
fer to the model that uses these baseline features
as bl.
3.1.2 Bag-of-Words
For each target word w, we got all base forms of
the content words within the same document or
within the same article for newspapers (PN). We
refer to the model that uses these baseline features
as bow.
3.1.3 Topic Features
In the SemEval-2007 English WSD tasks, a sys-
tem incorporating topic features achieved the
highest accuracy (Cai et al, 2007). Inspired by
(Cai et al, 2007), we also used topic features.
Their approach uses Bayesian topic models (La-
tent Dirichlet Allocation: LDA) to infer topics in
an unsupervised fashion. Then the inferred topics
2We use ACC as an abbreviation of accusative
postposition.
are added as features to reduce the sparsity prob-
lem with word-only features.
In our proposed approach, we use the inferred
topics to find 'related?( words and directly add
these word counts to the bag-of-words representa-
tion.
We applied gibbslda++3 to the training and test
data to obtain multiple topic classification per doc-
ument or article for newspapers (PN). We used the
document or article topics for newspapers (PN) in-
cluding the target word. We refer to the model
that uses these topic features as tpX, where X is
the number of topics and tpdistX with the topics
weighted by distributions. In particular, the topic
distribution of each document/article is inferred by
the LDA topic model using standard Gibbs sam-
pling.
We also add the most typical words in the topic
as a bag-of-words. For example, one topic might
include ? city, ?? Tokyo, ? train line, ? ward
and so on. A second topic might include ?? dis-
section, ? after, ?? medicine, U grave and so
on. If a document is inferred to contain the first
topic, then the words (? city, ?? Tokyo, ? train
line, ...) are added to the bag-of-words feature. We
refer to these features as twdY, including the most
typical Y words as bag-of-words.
3.2 Investigation between Domains
In preliminary experiments, we used both SVM4
and MEM (Nigam et al, 1999), with optimization
method L-BFGS (Liu and Nocedal, 1989) to train
the WSD model.
First, we investigated the effect between do-
mains (PN, PB, and OW). For training data, we se-
lected words that occur in more than 50 sentences,
separated the training data by domain, and tested
different domain combinations.
Table 1 shows the SVM results of the domain
combinations. For Table 1, we did a 5-fold cross
validation for the self domain and for comparison
3http://gibbslda.sourceforge.net/
4http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
384
Table 1: Investigation of Domain Combinations
on Training data (features: bl + bow, SVM)
Target Words 77, No. of Instances > 50
Domain Acc.(%) Diff. Comment
PN 78.7 - 63 words,
PN +OW 79.25 0.55 1094 instances
PN +PB 79.43 0.73
PN +ALL 79.34 0.64
PB 79.29 - 75 words,
PB +PN 78.85 -0.45 2463 instances
PB +OW 78.56 -0.73
PB +ALL 78.4 -0.89
OW 87.91 - 42 words,
OW +PN 89.05 1.14 703 instances
OW +PB 88.34 0.43
OW +ALL 89.05 1.14
with the results after adding the other domain data.
In Table 1, Diff. shows the differences to the self
domain.
As shown in Table 1, for PN and OW, using other
domains improved the results, but for PB, other do-
mains degraded the results. So we decided to se-
lect the domains for each target word.
In the formal run, for each pair of domain and
target words, we selected the combination of do-
main and dictionary examples that got the best
cross-validation result in the training data. Note
that in the case of no training data for the test data
domain, for example, since no OCs have training
data, we used all training data and dictionary ex-
amples.
We show the number of selected domain combi-
nations for each target domain in Table 2. Because
the distribution of target words is very unbalanced
in domains, not all types of target words appear in
every domain, as shown in Table 2.
3.3 Method for Predicting New Senses
We also tried to predict new senses (X) that didn?t
appear in the training data by calculating the en-
tropy for each target given in the MEM. We as-
sumed that high entropy (when the probabilities
of classes are uniformly dispersed) was indicative
of X; i.e., if [entropy > threshold] => predict X;
else => predict with MEM?s output sense tag.
Note that we used the words that were tagged
with Xs in the training data, except for the target
words. We compared the entropies of X and not
X of the words and heuristically tuned the thresh-
old based on the differences among entropies. Our
three official submissions correspond to different
thresholds.
Table 2: Used Domain Combinations
Used MEM SVM
Domain No. (%) No. (%)
Target: PB (48 types of target words)
ALL +EX 26 54.2 23 47.9
ALL 4 8.3 6 12.5
PB 11 22.9 8 16.7
PB +EX 1 2.1 1 2.1
PB +OW 1 2.1 3 6.3
PB +PN 5 10.4 7 14.6
Target: PN (46 types of target words)
ALL +EX 30 65.2 30 65.2
ALL 4 8.7 4 8.7
PN 4 8.7 1 2.2
PN +EX 0 0 1 2.2
PN +OW 2 4.3 2 4.3
PN +PB 6 13 8 17.4
Target: OW (16 types of target words)
ALL +EX 5 31.3 5 31.3
ALL 2 12.5 1 6.3
OW 6 37.5 3 18.8
OW +PB 3 18.8 3 18.8
OW +PN 0 0 4 25.0
Target: OC (46 types of target words)
ALL +EX 46 100 46 100
4 Results and Discussions
Our cross-validation experiments on the training
set showed that selecting data by domain combi-
nations works well, but unfortunately this failed
to achieve optimal results on the formal run. In
this section, we show the results using all of the
training data with no domain selections (also after
fixing some bugs).
Table 3 shows the results for the combination
of features on the test data. MEM greatly outper-
formed SVM. Its effective features are also quite
different. In the case of MEM, baseline features
(bl) almost gave the best result, and the topic fea-
tures improved the accuracy, especially when di-
vided into 200 topics. But for SVM, the topic
features are not so effective, and the bag-of-words
features improved accuracy.
For MEM with bl +tp200, which produced the
best result, the following are the best words: ?
outside (accuracy is 100%), C^ economy (98%),
?!d think (98%), d& big (98%), and %Z
culture (98%). On the other hand, the following
are the worst words: 1d take (36%), ? good
(48%), ?+d raise (48%), w2 put out (50%),
and ?= stand up (54%).
In Table 4, we show the results for each POS (bl
+tp200, MEM). The results for the verbs are com-
parably lower than the others. In future work, we
will consider adding syntactic features that may
improve the results.
385
Table 3: Comparisons among Features and Test data
TYPE Precision (%)
MEM SVM Explain
Base Line 68.96 68.96 Most Frequent Sense
bl 79.3 69.6 Base Line Features
bl +bow 77.0 70.8 + Bag-of-Words (BOW)
bl +bow +tp100 76.4 70.7 +BOW + Topics (100)
bl +bow +tp200 77.0 70.7 +BOW + Topics (200)
bl +bow +tp300 77.4 70.7 +BOW + Topics (300)
bl +bow +tp400 76.8 70.7 +BOW + Topics (400)
bl +bow +tpdist300 77.0 70.8 +BOW + Topics (300)*distribution
bl +bow +tp300 +twd100 76.2 70.8 + Topics (300) with 100 topic words
bl +bow +tp300 +twd200 76.0 70.8 + Topics (300) with 200 topic words
bl +bow +tp300 +twd300 75.9 70.8 + Topics (300) with 300 topic words
without bow
bl +tp100 79.3 69.6 + Topics (100)
bl +tp200 80.1 69.6 + Topics (200)
bl +tp300 79.6 69.6 + Topics (300)
bl +tp400 79.6 69.6 + Topics (400)
bl +tpdist100 79.3 69.6 + Topics (100)*distribution
bl +tpdist200 79.3 69.6 + Topics (200)*distribution
bl +tpdist300 79.3 69.6 + Topics (300)*distribution
bl +tp200 +twd100 74.6 69.6 + Topics (200) with 100 topic words
bl +tp300 +twd10 74.4 69.4 + Topics (300) with 10 topic words
bl +tp300 +twd20 75.2 69.3 + Topics (300) with 20 topic words
bl +tp300 +twd50 74.8 69.2 + Topics (300) with 50 topic words
bl +tp300 +twd200 74.6 69.6 + Topics (300) with 200 topic words
bl +tp300 +twd300 75.0 69.6 + Topics (300) with 300 topic words
bl +tp400 +twd100 74.1 69.6 + Topics (400) with 100 topic words
bl+tpdist100 +twd20 79.3 69.6 + Topics (100)*distribution with 20 topic words
bl+tpdist200 +twd20 79.3 69.6 + Topics (200)*distribution with 20 topic words
bl+tpdist400 +twd20 79.3 69.6 + Topics (400)*distribution with 20 topic words
Table 4: Results for each POS (bl +tp200, MEM)
POS No. of Types Acc. (%)
Nouns 22 85.5
Adjectives 5 79.2
Transitive Verbs 15 76.9
Intransitive Verbs 8 71.8
Total 50 80.1
In the formal run, we selected training data
for each pair of domain and target words and
used entropy to predict new unknown senses. Al-
though these two methods worked well in our
cross-validation experiments, they did not perform
well for the test data, probably due to domain mis-
match.
Finally, we also experimented with SVM and
MEM, and MEM gave better results.
References
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2010. A Re-
examination of MRD-based Word Sense Disambiguation.
Transactions on Asian Language Information Process, As-
sociation for Computing Machinery (ACM), 9(4):1?21.
Jun Fu Cai, Wee Sun Lee, and YW Teh. 2007. Improv-
ing Word Sense Disambiguation using Topic Features. In
Proceedings of EMNLP-CoNLL-2007, pp. 1015?1023.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited Mem-
ory BFGS Method for Large Scale Optimization. Math.
Programming, 45(3, (Ser. B)):503?528.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and Hitoshi Isahara. 2003. CRL at Japanese
dictionary-based task of SENSEVAL-2. Journal of Nat-
ural Language Processing, 10(3):115?143. (in Japanese).
Kamal Nigam, John Lafferty, and Andrew McCallum. 1999.
Using Maximum Entropy for Text Classification. In
IJCAI-99 Workshop on Machine Learning for Information
Filtering, pp. 61?67.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and
Hikaru Yokono. 2010. SemEval-2010 Task: Japanese
WSD. In SemEval-2: Evaluation Exercises on Semantic
Evaluation.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae Fu-
jita, and Chikara Hashimoto. 2007. Word Sense Disam-
biguation Incorporating Lexical and Structural Semantic
Information. In Proceedings of EMNLP-CoNLL-2007, pp.
477?485.
386
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 244?251,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Head Finalization: A Simple Reordering Rule for SOV Languages
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, Kevin Duh
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,sudoh,tsukada,kevinduh}@cslab.kecl.ntt.co.jp
Abstract
English is a typical SVO (Subject-Verb-
Object) language, while Japanese is a typ-
ical SOV language. Conventional Statis-
tical Machine Translation (SMT) systems
work well within each of these language
families. However, SMT-based translation
from an SVO language to an SOV lan-
guage does not work well because their
word orders are completely different. Re-
cently, a few groups have proposed rule-
based preprocessing methods to mitigate
this problem (Xu et al, 2009; Hong et al,
2009). These methods rewrite SVO sen-
tences to derive more SOV-like sentences
by using a set of handcrafted rules. In this
paper, we propose an alternative single re-
ordering rule: Head Finalization. This
is a syntax-based preprocessing approach
that offers the advantage of simplicity. We
do not have to be concerned about part-
of-speech tags or rule weights because the
powerful Enju parser allows us to imple-
ment the rule at a general level. Our ex-
periments show that its result, Head Final
English (HFE), follows almost the same
order as Japanese. We also show that this
rule improves automatic evaluation scores.
1 Introduction
Statistical Machine Translation (SMT) is useful
for building a machine translator between a pair of
languages that follow similar word orders. How-
ever, SMT does not work well for distant language
pairs such as English and Japanese, since English
is an SVO language and Japanese is an SOV lan-
guage.
Some existing methods try to solve this word-
order problem in language-independent ways.
They usually parse input sentences and learn a re-
ordering decision at each node of the parse trees.
For example, Yamada and Knight (2001), Quirk et
al. (2005), Xia and McCord (2004), and Li et al
(2007) proposed such methods.
Other methods tackle this problem in language-
dependent ways (Katz-Brown and Collins, 2008;
Collins et al, 2005; Nguyen and Shimazu, 2006).
Recently, Xu et al (2009) and Hong et al (2009)
proposed rule-based preprocessing methods for
SOV languages. These methods parse input sen-
tences and reorder the words using a set of hand-
crafted rules to get SOV-like sentences.
If we could completely reorder the words in in-
put sentences by preprocessing to match the word
order of the target language, we would be able to
greatly reduce the computational cost of SMT sys-
tems.
In this paper, we introduce a single reordering
rule: Head Finalization. We simply move syntac-
tic heads to the end of the corresponding syntactic
constituents (e.g., phrases and clauses). We use
only this reordering rule, and we do not have to
consider part-of-speech tags or rule weights be-
cause the powerful Enju parser allows us to im-
plement the rule at a general level.
Why do we think this works? The reason is
simple: Japanese is a typical head-final language.
That is, a syntactic head word comes after non-
head (dependent) words. SOV is just one as-
pect of head-final languages. In order to imple-
ment this idea, we need a parser that outputs syn-
tactic heads. Enju is such a parser from the
University of Tokyo (http://www-tsujii.is.s.
u-tokyo.ac.jp/enju). We discuss other parsers
in section 5.
There is another kind of head: semantic heads.
Hong et al (2009) used Stanford parser (de Marn-
effe et al, 2006), which outputs semantic head-
based dependencies; Xu et al (2009) also used the
same representation.
The use of syntactic heads and the number
of dependents are essential for the simplicity of
244
Head Finalization (See Discussion). Our method
simply checks whether a tree node is a syntactic
head. We do not have to consider what we are
moving and how to move it. On the other hand, Xu
et al had to introduce dozens of weighted rules,
probably because they used the semantic head-
based dependency representation without restric-
tion on the number of dependents.
The major difference between our method and
the above conventional methods, other than its
simplicity, is that our method moves not only verbs
and adjectives but also functional words such as
prepositions.
2 Head Finalization
Figure 1 shows Enju?s XML output for the simple
sentence: ?John hit a ball.? The tag <cons>
indicates a nonterminal node and <tok> indicates
a terminal node or a word (token). Each node has
a unique id. Head information is given by the
node?s head attribute. For instance, node c0?s head
is node c3, and c3 is a VP, or verb phrase. Thus,
Enju treats not only words but also non-terminal
nodes as heads.
Enju outputs at most two child nodes for each
node. One child is a head and the other is a depen-
dent. c3?s head is c4, which is VX, or a fragment of
a verb phrase. c4?s head is t1 or hit, which is VBD
or a past-tense verb. The upper picture of Figure 2
shows the parse tree graphically. Here, ? indicates
an edge that is linked from a ?head.?
Our Head Finalization rule simply swaps two
children when the head child appears before the
dependent child. In the upper picture of Fig. 2, c3
has two children c4 and c5. Here, c3?s head c4
appears before c5, so c4 and c5 are swapped.
The lower picture shows the swapped result.
Then we get John a ball hit, which has the
same word order as its Japanese translation jon wa
bohru wo utta except for the functional words a,
wa, and wo.
We have to add Japanese particles wa (topic
marker) or ga (nominative case marker) for John
and wo (objective case marker) for ball to get an
acceptable Japanese sentence.
It is well known that SMT is not good at gen-
erating appropriate particles from English, whitch
does not have particles. Particle generation was
tackled by a few research groups (Toutanova and
Suzuki, 2007; Hong et al, 2009).
Here, we use Enju?s output to generate seeds
?sentence id=?s0? parse status=?success??
?cons id=?c0? cat=?S? xcat=?? head=?c3??
?cons id=?c1? cat=?NP? xcat=?? head=?c2??
?cons id=?c2? cat=?NX? xcat=?? head=?t0??
?tok id=?t0? cat=?N? pos=?NNP?
base=?john??John?/tok?
?/cons?
?/cons?
?cons id=?c3? cat=?VP? xcat=?? head=?c4??
?cons id=?c4? cat=?VX? xcat=?? head=?t1??
?tok id=?t1? cat=?V? pos=?VBD? base=?hit?
arg1=?c1? arg2=?c5??hit?/tok?
?/cons?
?cons id=?c5? cat=?NP? xcat=?? head=?c7??
?cons id=?c6? cat=?DP? xcat=?? head=?t2?
?tok id=?t2? cat=?D? pos=?DT? base=?a?
arg1=?c7??a?/tok?
?/cons?
?cons id=?c7? cat=?NX? xcat=?? head=?t3??
?tok id=?t3? cat=?N? pos=?NN?
base=?ball??ball?/tok?
?/cons?
?/cons?
?/cons?
?/cons?
.?/sentence?
Figure 1: Enju?s XML output (some attributes are
removed for readability).
t0
John
t1
hit
t2
a
t3
ball
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Original English?
t0
John
jon (wa)
t1
hit
utta
t2
a
?
t3
ball
bohru (wo)
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Head Final English?
Figure 2: Head Finalization of a simple sentence
(? indicates a head).
245
2
John
5
went
7
to
9
the
10
police
12
because
15
Mary
17
lost
19
his
20
wallet
1? 14?8? 18?
6?
4? 16?
13?
11?
3?
0 Original English?
2
John
jon (wa)
5
Mary
meari (ga)
19
his
kare no
20
wallet
saifu (wo)
17
lost
nakushita
12
because
node
9
the
?
10
police
keisatsu
7
to
ni
5
went
itta
1? 14? 8?18?
6?
4?16?
13?
11?
3 ?
0 Head Final English?
Figure 3: Head-Finalizing a complex sentence.
for particles. As Fig. 1 shows, the verb hit has
arg1="c1" and arg2="c5". This indicates that c1
(John) is the subject of hit and c5 (a ball) is
the object of hit. We add seed words va1 after
arg1 and va2 after arg2. Then, we obtain John
va1 a ball va2 hit. We do not have to add
arg2 for be because be?s arg2 is not an object but
a complement. We introduced the idea of particle
seed words independently but found that it is very
similar to Hong et al (2009)?s method for Korean.
Figure 3 shows Enju?s parse tree for a
more complicated sentence ?John went to the
police because Mary lost his wallet.? For
brevity, we hide the terminal nodes, and we re-
moved the nonterminal nodes? prefix c.
Conventional Rule-Based Machine Translation
(RBMT) systems swap X and Y of ?X because Y?
and move verbs to the end of each clause. Then we
get ?Mary his wallet lost because John the police
to went.? Its word-to-word translation is a fluent
Japanese sentence: meari (ga) kare no saifu (wo)
nakushita node jon (wa) keisatsu ni itta.
On the other hand, our Head Finalization with
particle seed words yields a slightly different word
order ?John va1 Mary va1 his wallet va2 lost
because the police to went.? Its word-to-word
translation is jon wa meari ga kare no saifu wo
nakushita node keisatsu ni itta. This is also an ac-
ceptable Japanese sentence.
This difference comes from the syntactic role
of ?because.? In our method, Enju states that
because is a dependent of went, whereas RBMT
systems treat because as a clause conjunction.
When we use Xu et al?s preprocessing method,
?because? moves to the beginning of the sentence.
We do not know a good monotonic translation of
the result.
Preliminary experiments show that HFE looks
good as a first approximiation of Japanese word
order. However, we can make it better by intro-
ducing some heuristic rules. (We did not see the
test set to develop these heuristic rules.)
From a preliminary experiment, we found that
coordination expressions such as A and B and A
or B are reordered as B and A and B or A. Al-
though A and B have syntactically equal positions,
the order of these elements sometimes matters.
Therefore, we decided to stop swapping them at
coordination nodes, which are indicated cat and
xcat attributes of the Enju output. We call this
the coordination exception rule. In addition,
we avoid Enju?s splitting of numerical expressions
such as ?12,345? and ?(1)? because this splitting
leads to inappropriate word orders.
246
3 Experiments
In order to show how closely our Head Finaliza-
tion makes English follow Japanese word order,
we measured Kendall?s ? , a rank correlation co-
efficient. We also measured BLEU (Papineni et
al., 2002) and other automatic evaluation scores to
show that Head Finalization can actually improve
the translation quality.
We used NTCIR7 PAT-MT?s Patent corpus (Fu-
jii et al, 2008). Its training corpus has 1.8 mil-
lion sentence pairs. We used MeCab (http://
mecab.sourceforge.net/) to segment Japanese
sentences.
3.1 Rough evaluation of reordering
First, we examined rank correlation between Head
Final English sentences produced by the Head Fi-
nalization rule and Japanese reference sentences.
Since we do not have handcrafted word alignment
data for an English-to-Japanese bilingual corpus,
we used GIZA++ (Och and Ney, 2003) to get au-
tomatic word alignment.
Based on this automatic word alignment, we
measured Kendall?s ? for the word order between
HFE sentences and Japanese sentences. Kendall?s
? is a kind of rank correlation measure defined as
follows. Suppose a list of integers such as L = [2,
1, 3, 4]. The number of all integer pairs in this list
is 4C2 = 4 ? 3/(2 ? 1) = 6. The number of in-
creasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4),
and (3, 4). Kendall?s ? is defined by
? = #increasing pairs
#all pairs
? 2? 1.
In this case, we get ? = 5/6? 2? 1 = 0.667.
For each sentence in the training data,
we calculate ? based on a GIZA++ align-
ment file, en-ja.A3.final. (We also tried
ja-en.A3.final, but we got similar results.) It
looks something like this:
John hit a ball .
NULL ({3}) jon ({1}) wa ({}) bohru ({4})
wo ({}) utta ({2}) . ({5})
Numbers in ({ }) indicate corresponding En-
glish words. The article ?a? has no correspond-
ing word in Japanese, and such words are listed
in NULL ({ }). From this alignment information,
we get an integer list [1, 4, 2, 5]. Then, we get
? = 5/4C2 ? 2? 1 = 0.667.
For HFE in Figure 2, we will get the following
alignment.
John va1 a ball va2 hit .
NULL ({3}) jon ({1}) wa ({2}) bohru ({4})
wo ({5}) utta ({6}) . ({7})
Then, we get [1, 2, 4, 5, 6, 7] and ? = 1.0. We
use ? or the average of ? over all training sentences
to observe the tendency.
Sometimes, one Japanese word corresponds to
an English phrase:
John went to Costa Rica .
NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5})
ni ({3}) itta ({2}) . ({6})
We get [1, 4, 5, 3, 2, 6] from this alignment.
When the same word (or derivative words) ap-
pears twice or more in a single English sentence,
two or more non-consecutive words in the English
sentence are aligned to a single Japanese word:
rate of change of speed
NULL ({}) sokudo ({5}) henka ({3})
no ({2 4}) wariai ({1})
We excluded the ambiguously aligned words (2
4) from the calculation of ? . We use only [5, 3,
1] and get ? = ?1.0. The exclusion of these
words will be criticized by statisticians, but even
this rough calculation of ? sheds light on the weak
points of Head Finalization.
Because of this exclusion, the best value ? =
1.0 does not mean that we obtained the perfect
word ordering, but low ? values imply failures. In
section 4, we use ? to analyze failures.
By examining low ? sentences, we found that
patent documents have a lot of expressions such
as ?motor 2.? These are reordered (2 motor) and
slightly degrade ? . We did not notice this problem
until we handled the patent corpus because these
expressions are rare in other documents such as
news articles. Here, we added a rule to keep these
expressions.
We did not use any dictionary in our experi-
ment, but if we add dictionary entries to the train-
ing data, it raises ? because most entries are short.
One-word entries do not affect ? because we can-
not calculate ? . Most multi-word entries are short
noun phrases that are not reordered (? = 1.0).
Therefore, we should exclude dictionary entries
from the calculation of ? .
3.2 Quality of translation
It must be noted that the rank correlation does not
directly measure the quality of translation. There-
fore, we also measured BLEU and other automatic
evaluation scores of the translated sentences. We
used Moses (Koehn, 2010) for Minimum Error
Rate Training and decoding.
247
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of English sentences
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of Head Finalized English sentences
Figure 4: Distribution of ?
We used the development set (915 sentences) in
the NTCIR7 PAT-MT PSD data as well as the for-
mal run test set (1,381 sentences).
In the NTCIR7 PAT-MT workshop held in 2008,
its participants used different methods such as hi-
erarchical phrase-based SMT, RBMT, and EBMT
(Example-Based Machine Translation). However,
the organizers? Moses-based baseline system ob-
tained the best BLEU score.
4 Results
First, we show ? values to evaluate word order,
and then we show BLEU and other automatic eval-
uation scores.
4.1 Rank correlation
The original English sentences have ? = 0.451.
Head Finalization improved it to 0.722. Figure
4 shows the distribution of ? for all training sen-
tences. HFE reduces the percentage of low ? sen-
tences: 49.6% of the 1.8 million HFE sentences
have ? ? 0.8 and 15.1% have ? = 1.0.
We also implemented Xu et al?s method with
the Stanford parser 1.6.2. Its ? was 0.624. The
rate of the sentences with ? ? 0.8 was 30.6% and
the rate of ? = 1.0 was 4.3%.
We examined low ? sentences of our method
and found the following reasons for low ? values.
? The sentence pair is not an exact one-to-one
translation. A Japanese reference sentence
for ?I bought the cake.? can be some-
thing like ?The cake I bought.? or ?The
person who bought the cake is me.?
? Mistakes in Enju?s tagging or parsing. We
encountered certain POS tag mistakes:
? VBZ/NNS mistake: ?advances? of ?. . .
device advances along . . .? is VBZ,
main cause count
tagging/parsing mistakes 12
VBN/VBD mistake (4)
VBZ/NNS mistake (2)
comma or and (2)
inexact translation 7
wrong alignment 1
Table 1: Main causes of 20 worst sentences
but NNS is assigned.
? VBN/VBD mistake: ?encoded? of
?. . . the error correction encoded
data is supplied . . .? is VBN, but
VBD is assigned.
These tagging mistakes lead to global parsing
mistakes. In addition, just like other parsers,
Enju tends to make mistakes when a sentence
has a comma or ?and.?
? Mistakes/Ambiguity of GIZA++ automatic
word alignment. Ambiguity happens when
a single sentence has two or more occur-
rences of a word or derivatives of a word
(e.g., difference/different/differential). As we
described above, ambiguously aligned words
are removed from calculation of ? , and small
reordering mistakes in other words are em-
phasized.
We analyzed the 20 worst sentences with ? <
?0.5 when we used only 400,000 sentences for
GIZA++. Their causes are summarized in Table
1. In general, low ? sentences have two or more
causes, but here we show only the most influen-
tial cause for each sentence. This table shows that
mistakes in tagging and parsing are major causes
of low ? values. When we used all of 1.8 million
248
Method BLEU WER TER
proposed (0) 30.79 0.663 0.554
proposed (3) 30.97 0.665 0.554
proposed (6) 31.21 0.660 0.549
proposed (9) 31.11 0.661 0.549
proposed (12) 30.98 0.662 0.551
proposed (15) 31.00 0.662 0.552
no va (6) 30.99 0.669 0.559
Organizer 30.58 0.755 0.592
Table 2: Automatic Evaluation of Translation
Quality (Numbers in parentheses indicate distor-
tion limits).
sentence pairs, only 11 sentences had ? < ?0.5
among the 1.8 million sentences.
4.2 Automatic Evaluation of Translation
Quality
In general, it is believed that translation between
English and Japanese requires a large distortion
limit (dl), which restricts how far a phrase can
move. SMT reasearchers working on E-J or J-
E translation often use dl=?1 (unlimited) as a
default value, and this takes a long translation
time.
For PATMT J-E translation, Katz-Brown and
Collins (2008) showed that dl=unlimited is the
best and it requires a very long translation time.
For PATMT E-J translation, Kumai et al (2008)
claimed that they achieved the best result ?when
the distortion limit was 20 instead of ?1.?
Table 2 compares the single-reference BLEU
score of the proposed method and that of the
Moses-based system by the NTCIR-7 PATMT
organizers. This organizers? system was better
than all participants (Fujii et al, 2008) in terms
of BLEU. Here, we used Bleu Kit (http://
www.mibel.cs.tsukuba.ac.jp/norimatsu/
bleu kit/) following the PATMT?s overview
paper (Fujii et al, 2008). The table shows that
dl=6 gives the best result, and even dl=0 (no
reordering in Moses) gives better scores than the
organizers? Moses.
Table 2 also shows Word Error Rates (WER)
and Translation Error Rates (TER) (Snover et al,
2006). Since they are error rates, smaller is better.
Although the improvement of BLEU is not very
impressive, the score of WER is greatly reduced.
This difference comes from the fact that BLEU
measures only local word order, while WER mea-
Method ROUGE-L IMPACT PER
proposed (6) 0.480 0.369 0.390
no va (6) 0.475 0.368 0.398
Organizer 0.403 0.339 0.384
Table 3: Improvement in word order
sures global word order. Another line ?no va?
stands for our method without vas or particle
seeds. Without particle seeds, all scores slightly
drop.
Echizen-ya et al (2009) showed that IMPACT
and ROUGE-L are highly correlated to human
evaluation in evaluating J-E patent translation.
Therefore, we also used these evaluation methods
here for E-J translation. Table 3 shows that the
proposed method is also much better than the or-
ganizers? Moses in terms of these measures. With-
out particle seeds, these scores also drop slightly.
On the other hand, Position-independent Word
Error Rate (PER), which completely disregards
word order, does not change very much. These
facts indicate that our method improves word or-
der, which is the most important problem in E-J
translation.
The organizers? Moses uses dl=unlimited, and
it has been reported that its MERT training took
two weeks. On the other hand, our MERT training
with dl=6 took only eight hours on a PC: Xeon
X5570 2.93 GHz. Our method takes extra time to
parse sentences by Enju, but it is easy to run the
parser in parallel.
5 Discussion
Our method used an HPSG parser, which gives
rich information, but it is not easy to build such a
parser. It is much easier to build word dependency
parsers and Penn Treebank-style parsers. In order
use these parsers, we have to add some heuristic
rules.
5.1 Word Dependency Parsers
At first, we thought that we could substitute a word
dependency parser for Enju by simply rephrasing
a head with a modified word. Xu et al (2009)
used a semantic head-based dependency parser for
a similar purpose. Even when we use a syntac-
tic head-based dependency parser instead, we en-
countered their ?excessive movement? problem.
A straightforward application of their rules
changes
249
3
John
5
hit
7
the
8
ball
10
but
13
Sam
15
threw
17
the
18
ball
16?
14?
12?
11?
9?
6?
4?
2?
1?
0?
xcat="COOD"
cat="COOD"
Figure 5: Head Finilization does not mix up
clauses
(0) John hit the ball but Sam threw the ball.
to
(1) John the ball but Sam the ball threw hit.
Here, the two clauses are mixed up. To prevent
this, they disallow any movement across punctua-
tion and conjunctions. Then they get a better re-
sult:
(2) John the ball hit but Sam the ball threw.
When we used Enju, these clauses were not
mixed up. Enju-based Head Finalization gave the
same word order as (2):
(3) John va1 ball va2 hit but Sam va1 ball va2
throw.
Figure 5 shows Enju?s parse tree. When Head Fi-
nalization swaps the children of a mother node,
the children do not move beyond the range of
the mother node. Therefore, Head Finalization
based on Enju does not mix up the first clause
John hit the ball covered by Node 1 with the
second clause Sam threw the ball covered by
Node 11. Moreover, our coordination exception
rule keeps the order of these clauses. Thus, non-
terminal nodes in Enju?s output are useful to pro-
tect clauses.
When we use a word-dependency parser, we as-
sume that the modified words are heads. Further-
more, the Head Finalization rule is rephrased as
?move modified words after modifiers.? There-
fore, hit is moved after threw just like (2), and
the two clauses become mixed up. Consequently,
we need a heuristic rule like Xu?s.
5.2 Penn Treebank-style parsers
We also tried Charniak-Johnson?s parser (Char-
niak and Johnson, 2005). PyInputTree
(http://www.cs.brown.edu/?dmcc/software/
PyInputTree/) gives heads. Enju outputs at
most two children for a mother node, but Penn
Treebank-style parsers do not have such a limita-
tion on the number of children. This fact causes a
problem.
When we use Enju, ?This toy is popular in
Japan? is reordered as ?This toy va1 Japan in
popular is.? Its monotonic translation is fluent:
kono omocha wa nihon de ninki ga aru.
On the other hand, Charniak-Johnson?s parser
outputs the following S-expression for this sen-
tence (we added asterisks (*) to indicate heads).
(S (NP (DT This) (NN* toy))
(VP* (AUX* is)
(ADJP (JJ* popular))
(PP (IN* in) (NP (NNP* Japan)))))
Simply moving heads to the end introduces
?Japan in? between ?is? and ?popular?: this toy
va1 popular Japan in is. It is difficult to translate
this monotonically because of this interruption.
Reversing the children order (Xu et al, 2009)
reconnects is and popular. We get ?This toy
(va1) Japan in popular is? from the follow-
ing reversed S-expression.
(S (NP (DT This) (NN* toy))
(VP* (PP (IN* in) (NP (NNP* Japan)))
(ADJP (JJ* popular))
(AUX* is)))
5.3 Limitation of Head Finalization
Head Finalization gives a good first approximation
of Japanese word order in spite of its simplicity.
However, it is not perfect. In fact, a small distor-
tion limit improved the performance.
Sometimes, the Japanese language does not
have an appropriate word for monotonic transla-
tion. For instance, ?I have no time? becomes
?I va1 no time va2 have.? Its monotonic trans-
lation is ?watashi wa nai jikan wo motteiru,?
but this sentence is not acceptable. An acceptable
literal translation is ?watashi wa jikan ga nai.?
Here, ?no? corresponds to ?nai? at the end of the
sentence.
6 Conclusion
To solve the word-order problem between SVO
languages and SOV langugages, we introduced
a new reordering rule called Head Finalization.
This rule is simple, and we do not have to consider
POS tags or rule weights. We also showed that this
reordering improved automatic evaluation scores
of English-to-Japanese translation. Improvement
of the BLEU score is not very impressive, but
other evaluation scores (WER, TER, LOUGE-L,
and IMPACT) are greatly improved.
250
However, Head Finalization requires a sophis-
ticated HPSG tagger such as Enju. We showed
that severe failures are caused by Enju?s POS tag-
ging mistakes. We discussed the problems of other
parsers and how to solve them.
Our future work is to build our own parser that
makes fewer errors and to apply Head Finalization
to other SOV languages such as Korean.
Acknowledgements
We would like to thank Dr. Yusuke Miyao for
his useful advice on the usage of Enju. We also
thank anonymous reviewers for their valuable sug-
gestions.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
173?180.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of the Language Resources and Evaluation
Conference (LREC), pages 449?454.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimo-
hata, Atsushi Fujii, Masao Utiyama, Mikio Ya-
mamoto, Takehito Utsuro, and Noriko Kando. 2009.
Meta-evaluation of automatic evaluation methods
for machine translation using patent translation data
in NTCIR-7. In Proceedings of the 3rd Workshop on
Patent Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging morpho-syntactic gap be-
tween source and target sentences for English-
Korean statistical machine translation. In Proc. of
ACL-IJCNLP, pages 233?236.
Jason Katz-Brown and Michael Collins. 2008. Syn-
tactic reordering in preprocessing for Japanese ?
English translation: MIT system description for
NTCIR-7 patent translation task. In Working Notes
of the NTCIR Workshop Meeting (NTCIR).
Philipp Koehn, 2010. MOSES, Statistical Machine
Translation System, User Manual and Code Guide.
Hiroyuki Kumai, Hirohiko Segawa, and Yasutsugu
Morimoto. 2008. NTCIR-7 patent translation ex-
periments at Hitachi. In Working Notes of the NT-
CIR Workshop Meeting (NTCIR), pages 441?444.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 720?727.
Thai Phuong Nguyen and Akira Shimazu. 2006.
Improving phrase-based statistical machine transla-
tion with morphosyntactic transformation. Machine
Translation, 20(3):147?166.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 271?279.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Kristina Toutanova and Hisami Suzuki. 2007. Gener-
ating case markers in machine translation. In Proc.
of NAACL-HLT, pages 49?56.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the International Con-
ference on Computational Linguistics (COLING),
pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
of NAACL-HLT, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 523?530.
251
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 375?383,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
N-best Reranking by Multitask Learning
Kevin Duh Katsuhito Sudoh Hajime Tsukada Hideki Isozaki Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{kevinduh,sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We propose a new framework for N-best
reranking on sparse feature sets. The idea
is to reformulate the reranking problem as
a Multitask Learning problem, where each
N-best list corresponds to a distinct task.
This is motivated by the observation that
N-best lists often show significant differ-
ences in feature distributions. Training a
single reranker directly on this heteroge-
nous data can be difficult.
Our proposed meta-algorithm solves this
challenge by using multitask learning
(such as ?1/?2 regularization) to discover
common feature representations across N-
best lists. This meta-algorithm is simple to
implement, and its modular approach al-
lows one to plug-in different learning algo-
rithms from existing literature. As a proof
of concept, we show statistically signifi-
cant improvements on a machine transla-
tion system involving millions of features.
1 Introduction
Many natural language processing applications,
such as machine translation (MT), parsing, and
language modeling, benefit from the N-best
reranking framework (Shen et al, 2004; Collins
and Koo, 2005; Roark et al, 2007). The advan-
tage of N-best reranking is that it abstracts away
the complexities of first-pass decoding, allowing
the researcher to try new features and learning al-
gorithms with fast experimental turnover.
In the N-best reranking scenario, the training
data consists of sets of hypotheses (i.e. N-best
lists) generated by a first-pass system, along with
their labels. Given a new N-best list, the goal is
to rerank it such that the best hypothesis appears
near the top of the list. Existing research have fo-
cused on training a single reranker directly on the
entire data. This approach is reasonable if the data
is homogenous, but it fails when features vary sig-
nificantly across different N-best lists. In partic-
ular, when one employs sparse feature sets, one
seldom finds features that are simultaneously ac-
tive on multiple N-best lists.
In this case, we believe it is more advantageous
to view the N-best reranking problem as a multi-
task learning problem, where each N-best list cor-
responds to a distinct task. Multitask learning, a
subfield of machine learning, focuses on how to
effectively train on a set of different but related
datasets (tasks). Our heterogenous N-best list data
fits nicely with this assumption.
The contribution of this work is three-fold:
1. We introduce the idea of viewing N-best
reranking as a multitask learning problem.
This view is particularly apt to any general
reranking problem with sparse feature sets.
2. We propose a simple meta-algorithm that
first discovers common feature representa-
tions across N-bests (via multitask learning)
before training a conventional reranker. Thus
it is easily applicable to existing systems.
3. We demonstrate that our proposed method
outperforms the conventional reranking ap-
proach on a English-Japanese biomedical
machine translation task involving millions
of features.
The paper is organized as follows: Section 2 de-
scribes the feature sparsity problem and Section 3
presents our multitask solution. The effectiveness
of our proposed approach is validated by experi-
ments demonstrated in Section 4. Finally, Sections
5 and 6 discuss related work and conclusions.
2 The Problem of Sparse Feature Sets
For concreteness, we will describe N-best rerank-
ing in terms of machine translation (MT), though
375
our approach is agnostic to the application. In MT
reranking, the goal is to translate a foreign lan-
guage sentence f into an English sentence e by
picking from a set of likely translations. A stan-
dard approach is to use a linear model:
e? = argmax
e?N(f)
wT ? h(e, f) (1)
where h(e, f) is a D-dimensional feature vector,
w is the weight vector to be trained, and N(f) is
the set of likely translations of f , i.e. the N-best
list. The feature h(e, f) can be any quantity de-
fined in terms of the sentence pair, such as transla-
tion model and language model probabilities.
Here we are interested in situations where the
feature definitions can be quite sparse. A com-
mon methodology in reranking is to first design
feature templates based on linguistic intuition and
domain knowledge. Then, numerous features are
instantiated based on the training data seen. For
example, the work of (Watanabe et al, 2007) de-
fines feature templates based on bilingual word
alignments, which lead to extraction of heavily-
lexicalized features of the form:
h(e, f) =
?
?
?
?
?
?
?
1 if foreign word ?Monsieur?
and English word ?Mr.?
co-occur in e,f
0 otherwise
(2)
One can imagine that such features are sparse
because it may only fire for input sentences that
contain the word ?Monsieur?. For all other input
sentences, it is an useless, inactive feature.
Another common feature involves word ngram
templates, for example:
h(e, f) =
?
?
?
1 if English trigram
?Mr. Smith said? occurs in e
0 otherwise
(3)
In this case, all possible trigrams seen in the N-
best list are extracted as features. One can see
that this kind of feature can be very sensitive to
the first-pass decoder: if the decoder has loose re-
ordering constraints, then we may extract expo-
nentially many nonsense ngram features such as
?Smith said Mr.? and ?said Smith Mr.?. Granted,
the reranker training algorithm may learn that
these nonsense ngrams are indicative of poor hy-
potheses, but it is unlikely that the exact same non-
sense ngrams will appear given a different test sen-
tence.
In summary, the following issues compound to
create extremely sparse feature sets:
1. Feature templates are heavily-lexicalized,
which causes the number of features to grow
unbounded as the the amount of data in-
creases.
2. The input (f ) has high variability (e.g. large
vocabulary size), so that features for different
inputs are rarely shared.
3. The N-best list output also exhibits high vari-
ability (e.g. many different word reorder-
ings). Larger N may improve reranking per-
formance, but may also increase feature spar-
sity.
When the number of features is too large, even
popular reranking algorithms such as SVM (Shen
et al, 2004) and MIRA (Watanabe et al, 2007;
Chiang et al, 2009) may fail. Our goal here is to
address this situation.
3 Proposed Reranking Framework
In the following, we first give an intuitive com-
parison between single vs. multiple task learning
(Section 3.1), before presenting the general meta-
algorithm (Section 3.2) and particular instantia-
tions (Section 3.3).
3.1 Single vs. Multiple Tasks
Given a set of I input sentences {f i}, the training
data for reranking consists of a set of I N-best lists
{(Hi,yi)}i=1,...,I , where Hi are features and yi
are labels.
To clarify the notation:1 for an input sentence
f i, there is a N-best list N(f i). For a N-best list
N(f i), there are N feature vectors corresponding
to the N hypotheses, each with dimension D. The
collection of feature vectors for N(f i) is repre-
sented by Hi, which can be seen as a D ? N
matrix. Finally, the N -dimensional vector of la-
bels yi indicates the translation quality of each hy-
pothesis in N(f i). The purpose of the reranker
training algorithm is to find good parameters from
{(Hi,yi)}.
1Generally we use bold font h to represent a vector, bold-
capital font H to represent a matrix. Script h and h(?) may
be scalar, function, or sentence (depends on context).
376
The conventional method of training a single
reranker (single task formulation) involves opti-
mizing a generic objective such as:
argmin
w
I
?
i=1
L(w,Hi,yi) + ??(w) (4)
where w ? RD is the reranker trained on all lists,
and L(?) is some loss function. ?(w) is an op-
tional regularizer, whose effect is traded-off by the
constant ?. For example, the SVM reranker for
MT (Shen et al, 2004) defines L(?) to be some
function of sentence-level BLEU score, and ?(w)
to be the large margin regularizer.2
On the other hand, multitask learning involves
solving for multiple weights, w1,w2, . . . ,wI ,
one for each N-best list. One class of multitask
learning algorithms, Joint Regularization, solves
the following objective:
arg min
w1,..,wI
I
?
i=1
L(wi,Hi,yi) + ??(w1, ..,wI )
(5)
The loss decomposes by task but the joint regu-
larizer ?(w1, ..,wI) couples together the different
weight parameters. The key is to note that multi-
ple weights allow the algorithm to fit the heteroge-
nous data better, compared to a single weight vec-
tor. Yet these weights are still tied together so that
some information can be shared across N-best lists
(tasks).
One instantiation of Eq. 5 is ?1/?2 regular-
ization: ?(w1, ..,wI) , ||W||1,2, where W =
[w1|w2| . . . |wI ]T is a I-by-D matrix of stacked
weight vectors. The norm is computed by first tak-
ing the 2-norm on columns of W, then taking a
1-norm on the resulting D-length vector. This en-
courages the optimizer to choose a small subset of
features that are useful across all tasks.
For example, suppose two different sets of
weight vectors Wa and Wb for a 2 lists, 4 fea-
tures reranking problem. The ?1/?2 norm for Wa
is 14; the ?1/?2 norm for Wb is 12. If both have
the same loss L(?) in Eq. 5, the multitask opti-
mizer would prefer Wb since more features are
shared:
Wa :
?
4 0 0 3
0 4 3 0
?
Wb :
?
4 3 0 0
0 4 3 0
?
4 4 3 3 ? 14 4 5 3 0 ? 12
2In MT, evaluation metrics like BLEU do not exactly de-
compose across sentences, so for some training algorithms
this loss is an approximation.
3.2 Proposed Meta-algorithm
We are now ready to present our general reranking
meta-algorithm (see Algorithm 1), termed Rerank-
ing by Multitask Learning (RML).
Algorithm 1 Reranking by Multitask Learning
Input: N-best data {(Hi,yi)}i=1,...,I
Output: Common feature representation hc(e, f)
and weight vector wc
1: [optional] RandomHashing({Hi})
2: W = MultitaskLearn({(Hi ,yi)})
3: hc = ExtractCommonFeature(W)
4: {Hic} = RemapFeature({Hi}, hc)
5: wc = ConventionalReranker({(Hic ,yi)})
The first step, random hashing, is optional. Ran-
dom hashing is an effective trick for reducing the
dimension of sparse feature sets without suffer-
ing losses in fidelity (Weinberger et al, 2009;
Ganchev and Dredze, 2008). It works by collaps-
ing random subsets of features. This step can be
performed to speed-up multitask learning later. In
some cases, the original feature dimension may be
so large that hashed representations may be neces-
sary.
The next two steps are key. A multitask learn-
ing algorithm is run on the N-best lists, and a com-
mon feature space shared by all lists is extracted.
For example, if one uses the multitask objective
of Eq. 5, the result of step 2 is a set of weights
W. ExtractCommonFeature(W) then returns the
feature id?s (either from original or hashed repre-
sentation) that receive nonzero weight in any of
W.3 The new features hc(e, f) are expected to
have lower dimension than the original features
h(e, f). Section 3.3 describes in detail different
multitask methods that can be plugged-in to this
step.
The final two steps involve a conventional
reranker. In step 4, we remap the N-best list
data according to the new feature representations
hc(e, f). In step 5, we train a conventional
reranker on this common representation, which by
now should have overcome sparsity issues. Us-
ing a conventional reranker at the end allows us
to exploit existing rerankers designed for specific
NLP applications. In a sense, our meta-algorithm
simply involves a change of representation for
the conventional reranking scenario, where the
3For example in Wb, features 1-3 have nonzero weights
and are extracted. Feature 4 is discarded.
377
new representation is found by multitask methods
which are well-suited to heterogenous data.
3.3 Multitask Objective Functions
Here, we describe various multitask methods that
can be plugged in Step 2 of Algorithm 1. Our
goal is to demonstrate that a wide range of existing
methods from the multitask learning literature can
be brought to our problem. We categorize multi-
task methods into two major approaches:
1. Joint Regularization: Eq. 5 is an exam-
ple of joint regularization, with ?1/?2 norm being
a particular regularizer. The idea is to use the reg-
ularizer to ensure that the learned functions of re-
lated tasks are close to each other. The popular
?1/?2 objective can be optimized by various meth-
ods, such as boosting (Obozinski et al, 2009) and
convex programming (Argyriou et al, 2008). Yet
another regularizer is the ?1/?? norm (Quattoni et
al., 2009), which replaces the 2-norm with a max.
One could also define a regularizer to ensure
that each task-specific wi is close to some average
parameter, e.g.
?
i ||wi ? wavg||2. If we inter-
pret wavg as a prior, we begin to see links to Hier-
archical Bayesian methods for multitask learning
(Finkel and Manning, 2009; Daume, 2009).
2. Shared Subspace: This approach assumes
that there is an underlying feature subspace that
is common to all tasks. Early works on multi-
task learning implement this by neural networks,
where different tasks have different output layers
but share the same hidden layer (Caruana, 1997).
Another method is to write the weight vector
as two parts w = [u;v] and let the task-specific
function be uT ? h(e, f) + vT ? ? ? h(e, f) (Ando
and Zhang, 2005). ? is a D??D matrix that maps
the original features to a subspace common to all
tasks. The new feature representation is computed
by the projection hc(e, f) , ? ? h(e, f).
Multitask learning is a vast field and relates to
areas like collaborative filtering (Yu and Tresp,
2005) and domain adaptation. Most methods as-
sume some common representation and is thus ap-
plicable to our framework. The reader is urged to
refer to citations in, e.g. (Argyriou et al, 2008) for
a survey.
4 Experiments and Results
As a proof of concept, we perform experiments
on a MT system with millions of features. We
use a hierarchical phrase-based system (Chiang,
100 101 102 103 104
10?7
10?6
10?5
10?4
10?3
10?2
10?1
100
P(
fea
tur
e o
cc
urs
 in
 x 
lis
ts)
x
Figure 1: This log-log plot shows that there are
many rare features and few common features. The
probability that a feature occurs in x number of N-
best lists behaves according to the power-law x??,
where ? = 2.28.
2007) to generate N-best lists (N=100). Sparse
features used in reranking are extracted according
to (Watanabe et al, 2007). Specifically, the major-
ity are lexical features involving joint occurrences
of words within the N-best lists and source sen-
tences.
It is worth noting that the fact that the first pass
system is a hierarchical system is not essential to
the feature extraction step; similar features can be
extracted with other systems as first-pass, e.g. a
phrase-based system. That said, the extent of the
feature sparsity problem may depend on the per-
formance of the first-pass system.
We experiment with medical domain MT, where
large numbers of technical vocabulary cause spar-
sity challenges. Our corpora consists of English
abstracts from PubMed4 with their Japanese trans-
lations. The first-pass system is built on hierarchi-
cal phrases extracted from 17k sentence pairs and
target (Japanese) language models trained on 800k
medical-domain sentences. For our reranking ex-
periments, we used 500 lists as the training set5,
500 lists as held-out, and another 500 for test.
4.1 Data Characteristics
We present some statistics to illustrate the feature
sparsity problem: From 500 N-best lists, we ex-
tracted a total of 2.4 million distinct features. By
type, 75% of these features occur in only one N-
best list in the dataset. Less than 3% of features
4A database of the U.S. National Library of Medicine.
5In MT, training data for reranking is sometimes referred
to as ?dev set? to distinguish from the data used in first-pass.
Also, while the 17k bitext may seem small compared to other
MT work, we note that 1st pass translation quality (around 28
BLEU) is high enough to evaluate reranking methods.
378
occur in ten or more lists. The distribution of fea-
ture occurrence is clearly Zipfian, as seen in the
power-law plot in Figure 1.
We can also observe the feature growth rate (Ta-
ble 1). This is the number of new features intro-
duced when an additional N-best list is seen. It is
important to note that on average, 2599 new fea-
tures are added everytime a new N-best list is seen.
This is as much as 2599/4188 = 62% of the ac-
tive features. Imagine an online training algorithm
(e.g. MIRA or perceptron) on this kind of data:
whenever a loss occurs and we update the weight
vector, less than half of the weight vector update
applies to data we have seen thus far. Herein lies
the potential for overfitting.
From observing the feature grow rate, one may
hypothesize that adding large numbers of N-best
lists to the training set (500 in the experiments
here) may not necessarily improve results. While
adding data potentially improves the estimation
process, it also increases the feature space dramat-
ically. Thus we see the need for a feature extrac-
tion procedure.
(Watanabe et al, 2007) also reports the possibil-
ity of overfitting in their dataset (Arabic-English
newswire translation), especially when domain
differences are present. Here we observe this ten-
dency already on the same domain, which is likely
due to the highly-specialized vocabulary and the
complex sentence structures common in research
paper abstracts.
4.2 MT Results
Our goal is to compare different feature represen-
tations in reranking: The baseline reranker uses
the original sparse feature representation. This is
compared to feature representations discovered by
three different multitask learning methods:
? Joint Regularization (Obozinski et al, 2009)
? Shared Subspace (Ando and Zhang, 2005)
? Unsupervised Multitask Feature Selection
(Abernethy et al, 2007).6
We use existing implementations of the above
methods.7 The conventional reranker (Step 5, Al-
6This is not a standard multitask algorithm since most
multitask algorithms are supervised. We include it to see
if unsupervised or semi-supervised multitask algorithms is
promising. Intuitively, the method tries to select subsets of
features that are correlated across multiple tasks using ran-
dom sampling (MCMC). Features that co-occur in different
tasks form a high probability path.
7Available at http://multitask.cs.berkeley.edu
Nbest id #NewFt #SoFar #Active
1 3900 3900 3900
2 7535 11435 7913
3 6078 17513 7087
4 3868 21381 4747
5 1896 23277 2645
6 3542 26819 4747
....
100 2440 289118 4299
101 1639 290757 2390
102 3468 294225 4755
103 2350 296575 3824
Average 2599 ? 4188
Table 1: Feature growth rate: For N-best list i in
the table, we have (#NewFt = number of new fea-
tures introduced since N-best i ? 1) ; (#SoFar =
Total number of features defined so far); and (#Ac-
tive = number of active features for N-best i). E.g.,
we extracted 7535 new features from N-best 2;
combined with the 3900 from N-best 1, the total
features so far is 11435.
gorithm 1) used in all cases is SVMrank.8 Our
initial experiments show that the SVM baseline
performance is comparable to MIRA training, so
we use SVM throughout. The labels for the SVM
are derived as in (Shen et al, 2004), where top
10% of hypotheses by smoothed sentence-BLEU
is ranked before the bottom 90%. All multitask
learning methods work on hashed features of di-
mension 4000 (Step 1, Algorithm 1). This speeds
up the training process.
All hyperparameters of the multitask method
are tuned on the held-out set. In particular, the
most important is the number of common features
to extract, which we pick from {250, 500, 1000}.
Table 2 shows the results by BLEU (Papineni
et al, 2002) and PER. The Oracle results are ob-
tained by choosing the best hypothesis per N-best
list by sentence-level BLEU, which achieved 36.9
BLEU in both Train and Test. A summary of our
observations is:
1. The baseline (All sparse features) overfits. It
achieves the oracle BLEU score on the train
set (36.9) but performs poorly on the test
(28.6).
2. Similar overfitting occurs when traditional ?1
regularization is used to select features on
8Available at http://svmlight.joachims.org
379
the sparse feature representation9 . ?1 reg-
ularization is a good method of handling
sparse features for classification problems,
but in reranking the lack of tying between
lists makes this regularizer inappropriate. A
small set of around 1200 features are chosen:
they perform well independently on each task
in the training data, but there is little sharing
with the test data.
3. All three multitask methods obtained features
that outperformed the baseline. The BLEU
scores are 28.8, 28.9, 29.1 for Unsupervised
Feature Selection, Joint Regularization, and
Shared Subspace, respectively, which all out-
perform the 28.6 baseline. All improvements
are statistically significant by bootstrap sam-
pling test (1000 samples, p < 0.05) (Zhang
et al, 2004).
4. Shared Subspace performed the best. We
conjecture this is because its feature projec-
tion can create new feature combinations that
is more expressive than the feature selection
used by the two other methods.
5. PER results are qualitatively similar to BLEU
results.
6. As a further analysis, we are interested in see-
ing whether multitask learning extracts novel
features, especially those that have low fre-
quency. Thus, we tried an additional feature
representation (feature threshold) which only
keeps features that occur in more than x N-
bests, and concatenate these high-frequency
features to the multitask features. The fea-
ture threshold alone achieves nice BLEU re-
sults (29.0 for x > 10), but the combination
outperforms it by statistically significant mar-
gins (29.3-29.6). This implies that multitask
learning is extracting features that comple-
ment well with high frequency features.
For the multitask features, improvements of 0.2
to 1.0 BLEU are modest but consistent. Figure
2 shows the BLEU of bootstrap samples obtained
as part of the statistical significance test. We see
that multitask almost never underperform base-
line in any random sampling of the data. This im-
plies that the proposed meta-algorithm is very sta-
9Optimized by the Vowpal Wabbit toolkit:
http://hunch.net/vw/
ble, i.e. it is not a method that sometimes improves
and sometimes degrades.
Finally, a potential question to ask is: what
kinds of features are being selected by the
multitask learning algorithms? We found that
that two kinds of features are usually selected:
one is general features that are not lexicalized,
such as ?count of phrases?, ?count of dele-
tions/insertions?, ?number of punctuation marks?.
The other kind is lexicalized features, such as
those in Equations 2 and 3, but involving functions
words (like the Japanese characters ?wa?, ?ga?,
?ni?, ?de?) or special characters (such as numeral
symbol and punctuation). These are features that
can be expected to be widely applicable, and it is
promising that multitask learning is able to recover
these from the millions of potential features. 10
?0.2 0 0.2 0.4 0.6 0.8 1 1.2
0
50
100
150
200
250
300
BLEU(shared subspace)?BLEU(baseline sparse feature)
Bo
ot
st
ra
p 
sa
m
pl
es
Figure 2: BLEU difference of 1000 bootstrap sam-
ples. 95% confidence interval is [.15, .90] The
proposed approach therefore seems to be a stable
method.
5 Related Work in NLP
Previous reranking work in NLP can be classified
into two different research focuses:
1. Engineering better features: In MT, (Och
and others, 2004) investigates features extracted
from a wide variety of syntactic representations,
such as parse tree probability on the outputs. Al-
though their results show that the proposed syntac-
tic features gave little improvements, they point to
some potential reasons, such as domain mismatch
for the parser and overfitting by the reranking
10Note: In order to do this analysis, we needed to run Joint
Regularization on the original feature representation, since
the hashed representations are less interpretable. This turns
out to be computationally prohibitive in the time being so we
only ran on a smaller data set of 50 lists. Recently new op-
timization methods that are orders of magnitude faster have
been developed (Liu et al, 2009), which makes larger-scale
experiments possible.
380
Train Test Test
Feature Representation #Feature BLEU BLEU PER
(baselines)
First pass 20 29.5 28.5 38.3
All sparse features (Main baseline) 2.4M 36.9 28.6 38.2
All sparse features w/ ?1 regularization 1200 36.5 28.5 38.6
Random hash representation 4000 33.0 28.5 38.2
(multitask learning)
Unsupervised FeatureSelect 500 32.0 28.8 37.7
Joint Regularization 250 31.8 28.9 37.5
Shared Subspace 1000 32.9 29.1 37.3
(combination w/ high-frequency features)
(a) Feature threshold x > 100 3k 31.7 27.9 38.2
(b) Feature threshold x > 10 60k 35.8 29.0 37.9
Unsupervised FeatureSelect + (b) 60.5k 36.2 29.3 37.6
Joint Regularization + (b) 60.25k 36.1 29.4 37.5
Shared Subspace + (b) 61k 36.2 29.6 37.3
Oracle (best possible) ? 36.9 36.9 33.1
Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER. All
multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared
Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency
features also give significant improvements over the high frequency features alone.
method. Recent work by (Chiang et al, 2009) de-
scribes new features for hierarchical phrase-based
MT, while (Collins and Koo, 2005) describes
features for parsing. Evaluation campaigns like
WMT (Callison-Burch et al, 2009) and IWSLT
(Paul, 2009) also contains a wealth of information
for feature engineering in various MT tasks.
2. Designing better training algorithms: N-
best reranking can be seen as a subproblem of
structured prediction, so many general structured
prediction algorithms (c.f. (Bakir et al, 2007))
can be applied. In fact, some structured predic-
tion algorithms, such as the MIRA algorithm used
in dependency parsing (McDonald et al, 2005)
and MT (Watanabe et al, 2007) uses iterative
sets of N-best lists in its training process. Other
training algorithms include perceptron-style algo-
rithms (Liang et al, 2006), MaxEnt (Charniak and
Johnson, 2005), and boosting variants (Kudo et al,
2005).
The division into two research focuses is conve-
nient, but may be suboptimal if the training algo-
rithm and features do not match well together. Our
work can be seen as re-connecting the two focuses,
where the training algorithm is explicitly used to
help discover better features.
Multitask learning is currently an active subfield
within machine learning. There has already been
some applications in NLP: For example, (Col-
lobert and Weston, 2008) uses a deep neural net-
work architecture for multitask learning on part-
of-speech tagging, chunking, semantic role label-
ing, etc. They showed that jointly learning these
related tasks lead to overall improvements. (De-
selaers et al, 2009) applies similar methods for
machine transliteration. In information extraction,
learning different relation types can be naturally
cast as a multitask problem (Jiang, 2009; Carlson
et al, 2009). Our work can be seen as following
the same philosophy, but applied to N-best lists.
In other areas, (Reichart et al, 2008) introduced
an active learning strategy for annotating multitask
linguistic data. (Blitzer et al, 2006) applies the
multitask algorithm of (Ando and Zhang, 2005)
to domain adaptation problems in NLP. We expect
that more novel applications of multitask learning
will appear in NLP as the techniques become scal-
able and standard.
6 Discussion and Conclusion
N-best reranking is a beneficial framework for ex-
perimenting with large feature sets, but unfortu-
nately feature sparsity leads to overfitting. We ad-
dressed this by re-casting N-best lists as multitask
381
learning data. Our MT experiments show consis-
tent statistically significant improvements.
From the Bayesian view, multitask formulation
of N-best lists is actually very natural: Each N-
best is generated by a different data-generating
distribution since the input sentences are different,
i.e. p(e|f1) 6= p(e|f2). Yet these N-bests are re-
lated since the general p(e|f) distribution depends
on the same first-pass models.
The multitask learning perspective opens up
interesting new possibilities for future work, e.g.:
? Different ways to partition data into tasks,
e.g. clustering lists by document structure, or
hierarchical clustering of data
? Multitask learning on lattices or N-best lists
with larger N. It is possible that a larger hy-
pothesis space may improve the estimation of
task-specific weights.
? Comparing multitask learning to sparse on-
line learning of batch data, e.g. (Tsuruoka et
al., 2009).
? Modifying the multitask objective to incorpo-
rate application-specific loss/decoding, such
as Minimum Bayes Risk (Kumar and Byrne,
2004)
? Using multitask learning to aid large-scale
feature engineering and visualization.
Acknowledgments
We have received numerous helpful comments
throughout the course of this work. In partic-
ular, we would like to thank Albert Au Yeung,
Jun Suzuki, Shinji Watanabe, and the three anony-
mous reviewers for their valuable suggestions.
References
Jacob Abernethy, Peter Bartlett, and Alexander
Rakhlin. 2007. Multitask learning with expert ad-
vice. In COLT.
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. JMLR.
Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Pontil. 2008. Convex multitask feature learn-
ing. Machine Learning, 73(3).
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola,
B. Taskar, and S. V. N. Vishwanathan, editors. 2007.
Predicting structured data. MIT Press.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In EMNLP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In
WMT.
Andrew Carlson, Justin Betteridge, Estevam Hruschka,
and Tom Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In NAACL
Workshop on Semi-supervised learning for NLP
(SSLNLP).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural langauge parsing. Computa-
tional Linguistics, 31(1).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.
Hal Daume. 2009. Bayesian multitask learning with
latent hierarchies. In UAI.
Thomas Deselaers, Sasa Hasan, Oliver Bender, and
Hermann Ney. 2009. A deep learning approach to
machine transliteration. In WMT.
Jenny Rose Finkel and Chris Manning. 2009. Hier-
archical Bayesian domain adaptation. In NAACL-
HLT.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In ACL-
2008 Workshop on Mobile Language Processing.
Jing Jiang. 2009. Multitask transfer learning for
weakly-supervised relation extraction. In ACL.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In ACL.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
382
J. Liu, S. Ji, and J. Ye. 2009. Multi-task feature learn-
ing via efficient l2,1-norm minimization. In UAI.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large margin training of de-
pendency parsers. In ACL.
Guillaume Obozinski, Ben Taskar, and Michael Jor-
dan. 2009. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing.
F.J. Och et al 2004. A smorgasbord of features for
statistical machine translation. In HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Michael Paul. 2009. Overview of the iwslt 2009 eval-
uation campaign. In IWSLT.
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projection
for L1-Linfinity regularization. In ICML.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2).
Libin Shen, Anoop Sarkar, and Franz Och. 2004. Dis-
criminative reranking for machine translation. In
HLT-NAACL.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In ACL-IJCNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In EMNLP-
CoNLL.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In ICML.
Kai Yu and Volker Tresp. 2005. Learning to learn and
collaborative filtering. In NIPS-2005 Workshop on
Inductive Transfer.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
LREC.
383
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418?427,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Divide and Translate: Improving Long Distance Reordering in Statistical
Machine Translation
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
sudoh@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a novel method
for long distance, clause-level reordering
in statistical machine translation (SMT).
The proposed method separately translates
clauses in the source sentence and recon-
structs the target sentence using the clause
translations with non-terminals. The non-
terminals are placeholders of embedded
clauses, by which we reduce complicated
clause-level reordering into simple word-
level reordering. Its translation model
is trained using a bilingual corpus with
clause-level alignment, which can be au-
tomatically annotated by our alignment
algorithm with a syntactic parser in the
source language. We achieved signifi-
cant improvements of 1.4% in BLEU and
1.3% in TER by using Moses, and 2.2%
in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT, for
the English-to-Japanese translation of re-
search paper abstracts in the medical do-
main.
1 Introduction
One of the common problems of statistical ma-
chine translation (SMT) is to overcome the differ-
ences in word order between the source and target
languages. This reordering problem is especially
serious for language pairs with very different word
orders, such as English-Japanese. Many previous
studies on SMT have addressed the problem by
incorporating probabilistic models into SMT re-
ordering. This approach faces the very large com-
putational cost of searching over many possibili-
ties, especially for long sentences. In practice the
search can be made tractable by limiting its re-
ordering distance, but this also renders long dis-
tance movements impossible. Some recent stud-
ies avoid the problem by reordering source words
prior to decoding. This approach faces difficul-
ties when the input phrases are long and require
significant word reordering, mainly because their
reordering model is not very accurate.
In this paper, we propose a novel method for
translating long sentences that is different from
the above approaches. Problematic long sentences
often include embedded clauses1 such as rela-
tive clauses. Such an embedded (subordinate)
clause can usually be translated almost indepen-
dently of words outside the clause. From this
viewpoint, we propose a divide-and-conquer ap-
proach: we aim to translate the clauses sepa-
rately and reconstruct the target sentence using the
clause translations. We first segment a source sen-
tence into clauses using a syntactic parser. The
clauses can include non-terminals as placeholders
for nested clauses. Then we translate the clauses
with a standard SMT method, in which the non-
terminals are reordered as words. Finally we re-
construct the target sentence by replacing the non-
terminals with their corresponding clause transla-
tions. With this method, clause-level reordering is
reduced to word-level reordering and can be dealt
with efficiently. The models for clause translation
are trained using a bilingual corpus with clause-
level alignment. We also present an automatic
clause alignment algorithm that can be applied to
sentence-aligned bilingual corpora.
In our experiment on the English-to-Japanese
translation of multi-clause sentences, the proposed
method improved the translation performance by
1.4% in BLEU and 1.3% in TER by using Moses,
and by 2.2% in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT.
The main contribution of this paper is two-fold:
1Although various definitions of a clause can be
considered, this paper follows the definition of ?S?
(sentence) in Enju. It basically follows the Penn Tree-
bank II scheme but also includes SINV, SQ, SBAR. See
http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enju-
output-spec.html#correspondence for details.
418
1. We introduce the idea of explicit separa-
tion of in-clause and outside-clause reorder-
ing and reduction of outside-clause reorder-
ing into common word-level reordering.
2. We propose an automatic clause alignment
algorithm, by which our approach can be
used without manual clause-level alignment.
This paper is organized as follows. The next
section reviews related studies on reordering. Sec-
tion 3 describes the proposed method in detail.
Section 4 presents and discusses our experimen-
tal results. Finally, we conclude this paper with
our thoughts on future studies.
2 Related Work
Reordering in SMT can be roughly classified into
two approaches, namely a search in SMT decod-
ing and preprocessing.
The former approach is a straightforward way
that models reordering in noisy channel transla-
tion, and has been studied from the early period
of SMT research. Distance-based reordering is a
typical approach used in many previous studies re-
lated to word-based SMT (Brown et al, 1993) and
phrase-based SMT (Koehn et al, 2003). Along
with the advances in phrase-based SMT, lexical-
ized reordering with a block orientation model was
proposed (Tillmann, 2004; Koehn et al, 2005).
This kind of reordering is suitable and commonly
used in phrase-based SMT. On the other hand,
a syntax-based SMT naturally includes reorder-
ing in its translation model. A lot of research
work undertaken in this decade has used syntac-
tic parsing for linguistically-motivated translation.
(Yamada and Knight, 2001; Graehl and Knight,
2004; Galley et al, 2004; Liu et al, 2006). Wu
(1997) and Chiang (2007) focus on formal struc-
tures that can be extracted from parallel corpora,
instead of a syntactic parser trained using tree-
banks. These syntactic approaches can theoret-
ically model reordering over an arbitrary length,
however, long distance reordering still faces the
difficulty of searching over an extremely large
search space.
The preprocessing approach employs deter-
ministic reordering so that the following trans-
lation process requires only short distance re-
ordering (or even a monotone). Several previ-
ous studies have proposed syntax-driven reorder-
ing based on source-side parse trees. Xia and
McCord (2004) extracted reordering rules auto-
matically from bilingual corpora for English-to-
French translation; Collins et al (2005) used
linguistically-motivated clause restructuring rules
for German-to-English translation; Li et al (2007)
modeled reordering on parse tree nodes by us-
ing a maximum entropy model with surface and
syntactic features for Chinese-to-English trans-
lation; Katz-Brown and Collins (2008) applied
a very simple reverse ordering to Japanese-to-
English translation, which reversed the word order
in Japanese segments separated by a few simple
cues; Xu et al (2009) utilized a dependency parser
with several hand-labeled precedence rules for re-
ordering English to subject-object-verb order like
Korean and Japanese. Tromble and Eisner (2009)
proposed another reordering approach based on a
linear ordering problem over source words with-
out a linguistically syntactic structure. These pre-
processing methods reorder source words close
to the target-side order by employing language-
dependent rules or statistical reordering models
based on automatic word alignment. Although
the use of language-dependent rules is a natural
and promising way of bridging gaps between lan-
guages with large syntactic differences, the rules
are usually unsuitable for other language groups.
On the other hand, statistical methods can be ap-
plied to any language pairs. However, it is very
difficult to reorder all source words so that they are
monotonic with the target words. This is because
automatic word alignment is not usually reliable
owing to data sparseness and the weak modeling
of many-to-many word alignments. Since such
a reordering is not complete or may even harm
word ordering consistency in the source language,
these previous methods further applied reordering
in their decoding. Li et al (2007) used N-best
reordering hypotheses to overcome the reordering
ambiguity.
Our approach is different from those of previous
studies that aim to perform both short and long dis-
tance reordering at the same time. The proposed
method distinguishes the reordering of embedded
clauses from others and efficiently accomplishes it
by using a divide-and-conquer framework. The re-
maining (relatively short distance) reordering can
be realized in decoding and preprocessing by the
methods described above. The proposed frame-
work itself does not depend on a certain language
pair. It is based on the assumption that a source
419
language clause is translated to the corresponding
target language clause as a continuous segment.
The only language-dependent resource we need is
a syntactic parser of the source language. Note
that clause translation in the proposed method is a
standardMT problem and therefore any reordering
method can be employed for further improvement.
This work is inspired by syntax-based meth-
ods with respect to the use of non-terminals. Our
method can be seen as a variant of tree-to-string
translation that focuses only on the clause struc-
ture in parse trees and independently translates the
clauses. Although previous syntax-based methods
can theoretically model this kind of derivation, it
is practically difficult to decode long multi-clause
sentences as described above.
Our approach is also related to sentence sim-
plification and is intended to obtain simple and
short source sentences for better translation. Kim
and Ehara (1994) proposed a rule-based method
for splitting long Japanese sentences for Japanese-
to-English translation; Furuse et al (1998) used
a syntactic structure to split ill-formed inputs in
speech translation. Their splitting approach splits
a sentence sequentially to obtain short segments,
and does not undertake their reordering.
Another related field is clause identification
(Tjong et al, 2001). The proposed method is not
limited to a specific clause identification method
and any method can be employed, if their clause
definition matches the proposed method where
clauses are independently translated.
3 Proposed Method
The proposed method consists of the following
steps illustrated in Figure 1.
During training:
1) clause segmentation of source sentences with
a syntactic parser (section 3.1)
2) alignment of target words with source clauses
to develop a clause-level aligned corpus (section
3.2)
3) training the clause translation models using
the corpus (section 3.3)
During testing:
1) clause translation with the clause translation
models (section 3.4)
2) sentence reconstruction based on non-
terminals (section 3.5)
Bilingual
Corpus
(Training)
source
target
parse & clause
segmentation
parse &
clause
segmen-
tation
Source Sentences
(clause-segmented)
Word Alignment
Model
Target Word Bigram
Language Model
LM training
word
alignment
Bilingual Corpus
(clause-aligned)
automatic clause alignment
Clause
Translation Models
(Phrase Table, N-gram LMs, ...)
training from scratch
Bilingual
Corpus
(Development)
(clause-segmented)
MERT
Test Sentence
Sentence
Translation
clause
clause
clause
clause
translation
clause
translation
clause
translation
sentence reconstruction
based on non-terminals
translation
Original (sentence-aligned)
corpus can also be used
Figure 1: Overview of proposed method.
3.1 Clause Segmentation of Source Sentences
Clauses in source sentences are identified by a
syntactic parser. Figure 2 shows a parse tree for
the example sentence below. The example sen-
tence has a relative clause modifying the noun
book. Figure 3 shows the word alignment of this
example.
English: John lost the book that was borrowed
last week from Mary.
Japanese: john wa (topic marker) senshu (last
week) mary kara (from) kari (borrow) ta
(past tense marker) hon (book) o (direct ob-
ject marker) nakushi (lose) ta (past tense
marker) .
We segment the source sentence at the clause level
and the example is rewritten with two clauses as
follows.
? John lost the book s0 .
? that was borrowed last week from Mary
s0 is a non-terminal symbol the serves as a place-
holder of the relative clause. We allow an arbitrary
420
SS
John
lost
the
book
that
was
borrowed
from Mary
last week
Figure 2: Parse tree for example English sentence.
Node labels are omitted except S.
John
lo
st
the
book
that
w
as
borrow
ed
from
M
ary
last
w
eek
john
wa
ta
nakushi
o
hon
ta
kari
kara
mary
senshu
Figure 3: Word alignment for example bilingual
sentence.
number of non-terminals in each clause2. A nested
clause structure can be represented in the same
manner using such non-terminals recursively.
3.2 Alignment of Target Words with Source
Clauses
To translate source clauses with non-terminal sym-
bols, we need models trained using a clause-level
aligned bilingual corpus. A clause-level aligned
corpus is defined as a set of parallel, bilingual
clause pairs including non-terminals that represent
embedded clauses.
We assume that a sentence-aligned bilingual
corpus is available and consider the alignment of
target words with source clauses. We can manu-
ally align these Japanese words with the English
clauses as follows.
? john wa s0 hon o nakushi ta .
2In practice not so many clauses are embedded in a single
sentence but we found some examples with nine embedded
clauses for coordination in our corpora.
John lost the book s0 .
? senshu mary kara kari ta
that was borrowed last week from Mary
Since the cost of manual clause alignment is
high especially for a large-scale corpus, a natu-
ral question to ask is whether this resource can be
obtained from a sentence-aligned bilingual corpus
automatically with no human input. To answer
this, we now describe a simple method for deal-
ing with clause alignment data from scratch, us-
ing only the word alignment and language model
probabilities inferred from bilingual and monolin-
gual corpora.
Our method is based on the idea that automatic
clause alignment can be viewed as a classification
problem: for an English sentence with N words (e
= (e1, e2, . . . , eN )) andK clauses (e?1,e?2,. . . ,e?K),
and its Japanese translation with M words (f
= (f1, f2, . . . , fM )), the goal is to classify each
Japanese word into one of {1, . . . ,K} classes. In-
tuitively, the probability that a Japanese word fm
is assigned to class k ? {1, . . . ,K} depends on
two factors:
1. The probability of translating fm into the En-
glish words of clause k (i.e.
?
e?e?k p(e|fm)).
We expect fm to be assigned to a clause
where this value is high.
2. The language model probability
(i.e. p(fm|fm?1)). If this value is high,
we expect fm and fm?1 to be assigned to the
same clause.
We implement this intuition using a graph-
based method. For each English-Japanese sen-
tence pair, we construct a graph with K clause
nodes (representing English clauses) and M word
nodes (representing Japanese words). The edge
weights between word and clause nodes are de-
fined as the sum of lexical translation probabilities
?
e?e?k p(e|fm). The edge weights between words
are defined as the bigram probability p(fm|fm?1).
Each clause node is labeled with a class ID k ?
{1, . . . ,K}. We then propagate these K labels
along the graph to label the M word nodes. Fig-
ure 4 shows the graph for the example sentence.
Many label propagation algorithms are avail-
able. The important thing is to use an algo-
rithm that encourages node pairs with strong edge
weights to receive the same label. We use the label
propagation algorithm of (Zhu et al, 2003). If we
421
John  lost  the  book  that  was  borrowed ...
clause(1) clause(2)
John Mary fromlast weektopicmarker
p(John |           )
+ p(lost |           )
+ ...
p(that |        )
+ p(was |        )
+ ...
p(     |         ) p(         |            ) p(        |         )p(            |     )
john kara
karajohn
john wa senshu mary kara
wa  john senshu  wa mary  senshu kara mary
Figure 4: Graph-based representation of the ex-
ample sentence. We propagate the clause labels to
the Japanese word nodes on this graph to form the
clause alignments.
assume the labels are binary, the following objec-
tive is minimized:
argmin
l?RK+M
?
i,j
wij(li ? lj)2 (1)
where wij is the edge weight between nodes i
and j (1 ? i ? K + M , 1 ? j ? K +
M ), and l (li ? {0, 1}) is a vector of labels
on the nodes. The first K elements of l, lc =
(l1, l2, ..., lK)T , are constant because the clause
nodes are pre-labeled. The remaining M ele-
ments, lf = (lK+1, lK+2, ..., lK+M )T , are un-
known and to be determined. Here, we consider
the decomposition of the weight matrixW = [wij ]
into four blocks after the K-th row and column as
follows:
W =
[
W cc W cf
W fc W ff
]
(2)
The solution of eqn. (1), namely lf , is given by the
following equation:
lf = (Dff ?W ff )?1W fc lc (3)
where D is the diagonal matrix with di =
?
j wij
and is decomposed similarly to W . Each element
of lf is in the interval (0, 1) and can be regarded
as the label propagation probability. A detailed ex-
planation of this solution can be found in Section 2
of (Zhu et al, 2003). For our multi-label problem
with K labels, we slightly modified the algorithm
by expanding the vector l to an (M + K) ? K
binary matrix L = [ l1 l2 ... lK ].
After the optimization, we can normalize Lf
to obtain the clause alignment scores t(lm =
k|fm) between each Japanese word fm and En-
glish clause k. Theoretically, we can simply out-
put the clause id k? for each fm by finding k? =
argmaxk t(lm = k|fm). In practice, this may
sometimes lead to Japanese clauses that have too
many gaps, so we employ a two-stage procedure
to extract clauses that are more contiguous.
First, we segment the Japanese sentence into K
clauses based on a dynamic programming algo-
rithm proposed by Malioutov and Barzilay (2006).
We define an M ? M similarity matrix S = [sij ]
with sij = exp(?||li?lj ||) where li is (K + i)-th
row vector in the label matrix L. sij represents
the similarity between the i-th and j-th Japanese
words with respect to their clause alignment score
distributions; if the score distributions are sim-
ilar then sij is large. The details of this algo-
rithm can be found in (Malioutov and Barzilay,
2006). The clause segmentation gives us contigu-
ous Japanese clauses f?1, f?2, ..., f?K , thus min-
imizing inter-segment similarity and maximizing
intra-segment similarity. Second, we determine
the clause labels of the segmented clauses, based
on clause alignment scores T = [Tkk? ] for English
and automatically-segmented Japanese clauses:
Tkk? =
?
fm?f? k?
t(lm = k|fm) (4)
where f?k? is the j?-th Japanese clause. In descend-
ing order of the clause alignment score, we greed-
ily determine the clause label 3.
3.3 Training Clause Translation Models
We train clause translation models using the
clause-level aligned corpus. In addition we can
also include the original sentence-aligned corpus.
We emphasize that we can use standard techniques
for heuristically extracted phrase tables, word n-
gram language models, and so on.
3.4 Clause Translation
By using the source language parser, a multi-
clause source sentence is reduced to a set of
clauses. We translate these clauses with a common
SMT method using the clause translation models.
Here we present another English example I
bought the magazine which Tom recommended
yesterday. This sentence is segmented into clauses
as follows.
3Although a full search is available when the number of
clauses is small, we employ a greedy search in this paper.
422
? I bought the magazine s0 .
? which Tom recommended yersterday
These clauses are translated into Japanese:
? watashi (I) wa (topic marker) s0
zasshi (magazine) o (direct object marker)
kat (buy) ta (past tense marker).
? tom ga (subject marker) kino (yesterday)
susume (recommend) ta (past tense marker)
3.5 Sentence Reconstruction
We reconstruct the target sentence from the clause
translations, based on non-terminals. Starting
from the clause translation of the top clause, we re-
cursively replace non-terminal symbols with their
corresponding clause translations. Here, if a non-
terminal is eventually deleted in SMT decoding,
we simply concatenate the translation behind its
parent clause.
Using the example above, we replace the non-
terminal symbol s0 with the second clause and
obtain the Japanese sentence:
watashi wa tom ga kino susume ta zasshi o kat ta .
4 Experiment
We conducted the following experiments on the
English-to-Japanese translation of research paper
abstracts in the medical domain. Such techni-
cal documents are logically and formally writ-
ten, and sentences are often so long and syntac-
tically complex that their translation needs long
distance reordering. We believe that the medical
domain is suitable as regards evaluating the pro-
posed method.
4.1 Resources
Our bilingual resources were taken from the med-
ical domain. The parallel corpus consisted of
research paper abstracts in English taken from
PubMed4 and the corresponding Japanese transla-
tions.
The training portion consisted of 25,500 sen-
tences (no-clause-seg.; original sentences with-
out clause segmentation). 4,132 English sen-
tences in the corpus were composed of multi-
ple clauses and were separated at the clause level
4http://www.ncbi.nlm.nih.gov/pubmed/
by the procedure in section 3.1. As the syntac-
tic parser, we used the Enju5 (Miyao and Tsu-
jii, 2008) English HPSG parser. For these train-
ing sentences, we automatically aligned Japanese
words with each English clause as described in
section 3.2 and developed a clause-level aligned
corpus, called auto-aligned corpus. We prepared
manually-aligned (oracle) clauses for reference,
called oracle-aligned clauses. The clause align-
ment error rate of the auto-aligned corpus was
14% (number of wrong clause assignments di-
vided by total number of words). The develop-
ment and test portions each consisted of 1,032
multi-clause sentences. because this paper focuses
only on multi-clause sentences. Their English-
side was segmented into clauses in the same man-
ner as the training sentences, and the development
sentences had oracle clause alignment for MERT.
We also used the Life Science Dictionary6 for
training. We extracted 100,606 unique English
entries from the dictionary including entries with
multiple translation options, which we expanded
to one-to-one entries, and finally we obtained
155,692 entries.
English-side tokenization was obtained using
Enju, and we applied a simple preprocessing that
removed articles (a, an, the) and normalized plu-
ral forms to singular ones. Japanese-side tokeniza-
tion was obtained using MeCab7 with ComeJisyo8
(dictionary for Japanese medical document tok-
enization). Our resource statistics are summarized
in Table 1.
4.2 Model and Decoder
We used two decoders in the experiments,
Moses9 (Koehn et al, 2007) and our in-
house hierarchical phrase-based SMT (almost
equivalent to Hiero (Chiang, 2007)). Moses
used a phrase table with a maximum phrase
length of 7, a lexicalized reordering model with
msd-bidirectional-fe, and a distortion
limit of 1210. Our hierarchical phrase-based SMT
used a phrase table with a maximum rule length of
7 and a window size (Hiero?s ?) of 12 11. Both
5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
6http://lsd.pharm.kyoto-u.ac.jp/en/index.html
7http://mecab.sourceforge.net/
8http://sourceforge.jp/projects/comedic/ (in Japanese)
9http://www.statmt.org/moses/
10Unlimited distortion was also tested but the results were
worse.
11A larger window size could not be used due to its mem-
ory requirements.
423
Table 1: Data statistics on training, development,
and test sets. All development and test sentences
are multi-clause sentences.
Training
Corpus Type #words #sentences
Parallel E 690,536
(no-clause-seg.) J 942,913
25,550
Parallel E 135,698
(auto-aligned) J 183,043
4,132
(oracle-aligned) J 183,147
(10,766 clauses)
E 263,175 155.692Dictionary
J 291,455 (entries)
Development
Corpus Type #words #sentences
Parallel E 34,417 1,032
(oracle-aligned) J 46,480 (2,683 clauses)
Test
Corpus Type #words #sentences
Parallel E 34,433 1,032
(clause-seg.) J 45,975 (2,737 clauses)
decoders employed two language models: a word
5-gram language model from the Japanese sen-
tences in the parallel corpus and a word 4-gram
language model from the Japanese entries in the
dictionary. The feature weights were optimized
for BLEU (Papineni et al, 2002) by MERT, using
the development sentences.
4.3 Compared Methods
We compared four different training and test con-
ditions with respect to the use of clauses in training
and testing. The development (i.e., MERT) condi-
tions followed the test conditions. Two additional
conditions with oracle clause alignment were also
tested for reference.
Table 2 lists the compared methods. First,
the proposed method (proposed) used the auto-
aligned corpus in training and clause segmen-
tation in testing. Second, the baseline method
(baseline) did not use clause segmentation in ei-
ther training or testing. Using this standard base-
line method, we focused on the advantages of the
divide-and-conquer translation itself. Third, we
tested the same translation models as used with
the proposed method for test sentences without
clause segmentation, (comp.(1)). Although this
comparison method cannot employ the proposed
clause-level reordering, it was expected to be bet-
ter than the baseline method because its transla-
tion model can be trained more precisely using the
finely aligned clause-level corpus. Finally, the sec-
ond comparison method (comp.(2)) translated seg-
mented clauses with the baseline (without clause
segmentation) model, as if each of them was a sin-
gle sentence. Its translation of each clause was
expected to be better than that of the baseline be-
cause of the efficient search over shortened inputs,
while its reordering of clauses (non-terminals) was
unreliable due to the lack of clause information
in training. Its sentence reconstruction based on
non-terminals was the same as with the proposed
method. Although non-terminals in the second
comparison method were out-of-vocabulary words
and may be deleted in decoding, all of them sur-
vived and we could reconstruct sentences from
translated clauses throughout the experiments. In
addition, two other conditions were tested: us-
ing oracle-aligned clauses in training: the pro-
posed method trained using oracle-aligned (ora-
cle) clauses and the first comparison method using
oracle-aligned (oracle-comp.) clauses.
4.4 Results
Table 3 shows the results in BLEU, Transla-
tion Edit Rate (TER) (Snover et al, 2006),
and Position-independent Word-error Rate (PER)
(Och et al, 2001), obtained with Moses and our
hierarchical phrase-based SMT, respectively. Bold
face results indicate the best scores obtained with
the compared methods (excluding oracles).
The proposed method consistently outper-
formed the baseline. The BLEU improve-
ments with the proposed method over the base-
line and comparison methods were statistically
significant according to the bootstrap sampling
test (p < 0.05, 1,000 samples) (Zhang et al,
2004). With Moses, the improvement when us-
ing the proposed method was 1.4% (33.19% to
34.60%) in BLEU and 1.3% (57.83% to 56.50%)
in TER, with a slight improvement in PER
(35.84% to 35.61%). We observed: oracle ?
proposed ? comp.(1) ? baseline ? comp.(2)
by the Bonferroni method, where the symbol
A ? B means ?A?s improvement over B is
statistically significant.? With the hierarchical
phrase-based SMT, the improvement was 2.2%
(32.39% to 34.55%) in BLEU, 3.5% (58.36% to
54.87%) in TER, and 1.5% in PER (36.42% to
34.79%). We observed: oracle ? proposed ?
424
Table 2: Compared methods.
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. proposed comp.(2) oracle
no-clause-seg. comp.(1) baseline oracle-comp.
{comp.(1), comp.(2)} ? baseline by the Bon-
ferroni method. The oracle results were better than
these obtained with the proposed method but the
differences were not very large.
4.5 Discussion
We think the advantage of the proposed method
arises from three possibilities: 1) better translation
model training using the fine-aligned corpus, 2) an
efficient decoder search over shortened inputs, and
3) an effective clause-level reordering model real-
ized by using non-terminals.
First, the results of the first comparison method
(comp.(1)) indicate an advantage of the transla-
tion models trained using the auto-aligned corpus.
The training of the translation models, namely
word alignment and phrase extraction, is difficult
for long sentences due to their large ambiguity.
This result suggests that the use of clause-level
alignment provides fine-grained word alignments
and precise translation models. We can also ex-
pect that the model of the proposed method will
work better for the translation of single-clause sen-
tences.
Second, the average and median lengths (in-
cluding non-terminals) of the clause-seg. test set
were 13.2 and 10 words, respectively. They were
much smaller than those of no-clause-seg. at 33.4
and 30 words and are expected to help realize
an efficient SMT search. Another observation is
the relationship between the number of clauses
and translation performance, as shown in Fig-
ure 5. The proposed method achieved a greater im-
provement in sentences with a greater number of
clauses. This suggests that our divide-and-conquer
approach works effectively for multi-clause sen-
tences. Here, the results of the second comparison
method (comp.(2)) with Moses were worse than
the baseline results, while there was an improve-
ment with our hierarchical phrase-based SMT.
This probably arose from the difference between
the decoders when translating out-of-vocabulary
words. The non-terminals were handled as out-of-
vocabulary words under the comp.(2) condition.
52
54
56
58
60
62
64
66
2 4 53
TE
R
 (%
)
The number of clauses
baseline
proposed
comp.(2)
Figure 5: Relationship between TER and number
of clauses for proposed, baseline, and comp.(2)
when using our hierarchical phrase-based SMT.
Moses generated erroneous translations around
such non-terminals that can be identified at a
glance, while our hierarchical phrase-based SMT
generated relatively good translations. This may
be a decoder-dependent issue and is not an essen-
tial problem.
Third, the results obtained with the proposed
method reveal an advantage in reordering in ad-
dition to the previous two advantages. The differ-
ence between the PERs with the proposed method
and the baseline with Moses was small (0.2%)
in spite of the large differences in BLEU and
TER (about 1.5%). This suggests that the pro-
posed method is better in word ordering and im-
plies our method is also effective in reordering.
With the hierarchical phrase-based SMT, the pro-
posed method showed a large improvement from
the baseline and comparison methods, especially
in TER which was better than the best Moses
configuration (proposed). This suggests that the
decoding of long sentences with long-distance
reordering is not easy even for the hierarchical
phrase-based SMT due to its limited window size,
while the hierarchical framework itself can natu-
rally model a long-distance reordering. If we try to
find a derivation with such long-distance reorder-
ing, we will probably be faced with an intractable
search space and computation time. Therefore,
we can conclude that the proposed divide-and-
425
Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,
TER, and PER.
Moses : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.60 / 56.50 / 35.61 32.14 / 58.78 / 36.08 35.31 / 55.12 / 34.42
no-clause-seg. 34.22 / 56.90 / 35.20 33.19 / 57.83 / 35.84 34.24 / 56.67 / 35.03
Hierarchical : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.55 / 54.87 / 34.79 33.03 / 56.70 / 36.03 35.08 / 54.22 / 34.77
no-clause-seg. 33.41 / 57.02 / 35.86 32.39 / 58.36 / 36.42 33.83 / 56.26 / 34.96
conquer approach provides more practical long-
distance reordering at the clause level.
We also analyzed the difference between auto-
matic and manual clause alignment. Since auto-
aligned corpus had many obvious alignment er-
rors, we suspected these noisy clauses hurt the
clause translation model. However, they were not
serious in terms of final translation performance.
So we can conclude that our proposed divide-and-
conquer approach is promising for long sentence
translation. Although we aimed to see whether we
could bootstrap using existing bilingual corpora in
this paper, we imagine better clause alignment can
be obtained with some supervised classifiers.
One problem with the divide-and-conquer ap-
proach is that its independently-translated clauses
potentially cause disfluencies in final sentence
translations, mainly due to wrong inflections. A
promising solution is to optimize a whole sentence
translation by integrating search of each clause
translation but this may require a much larger
search space for decoding. More simply, we may
be able to approximate it using n-best clause trans-
lations. This problem should be addressed for fur-
ther improvement in future studies.
5 Conclusion
In this paper we proposed a clause-based divide-
and-conquer approach for SMT that can re-
duce complicated clause-level reordering to sim-
ple word-level reordering. The proposed method
separately translates clauses with non-terminals by
using a well-known SMT method and reconstructs
a sentence based on the non-terminals, to reorder
long clauses. The clause translation models are
trained using a bilingual corpus with clause-level
alignment, which can be obtained with an un-
supervised graph-based method using sentence-
aligned corpora. The proposed method improves
the translation of long, multi-clause sentences and
is especially effective for language pairs with
large word order differences, such as English-to-
Japanese.
This paper focused only on clauses as segments
for division. However, other long segments such
as prepositional phrases are similarly difficult to
reorder correctly. The divide-and-conquer ap-
proach itself can be applied to long phrases, and
it is worth pursuing such an extension. As another
future direction, we must develop a more sophis-
ticated method for automatic clause alignment if
we are to use the proposed method for various lan-
guage pairs and domains.
Acknowledgments
We thank the U. S. National Library of Medicine
for the use of PubMed abstracts and Prof. Shuji
Kaneko of Kyoto University for the use of Life
Science Dictionary. We also thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531?540.
426
Osamu Furuse, Setsuo Yamada, and Kazuhide Ya-
mamoto. 1998. Splitting long or ill-formed in-
put for robust spoken-language translation. In Proc.
COLING-ACL, pages 421?427.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT-NAACL, pages 105?
112.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for Japanese-English
translation: MIT system description for NTCIR-7
patent translation task. In Proc. NTCIR-7, pages
409?414.
Yeun-Bae Kim and Terumasa Ehara. 1994. A method
for partitioning of long Japanese sentences with sub-
ject resolution in J/E machine translation. In Proc.
International Conference on Computer Processing
of Oriental Languages, pages 467?473.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 263?270.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. ACL, pages 720?727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String alignment template for statistical machine
translation. In Proc. Coling-ACL, pages 609?616.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Coling-ACL, pages 25?32.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statis-
tical machine translation. In Proc. the ACL Work-
shop on Data-Driven Methods in Machine Transla-
tion, pages 55?62.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proc.
HLT-NAACL, pages 101?104.
Erik F. Tjong, Kim Sang, and Herve? De?jean. 2001. In-
troduction to the CoNLL-2001 shared task: Clause
identification. In Proc. CoNLL, pages 53?57.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING, pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
HLT-NAACL, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523?530.
Ying Zhang, Stephan Vogel, and Alex Weibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proc. LREC, pages 2051?2054.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proc. ICML, pages
912?919.
427
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Head Finalization Reordering for Chinese-to-Japanese
Machine Translation
Han Dan+ Katsuhito Sudoh? Xianchao Wu??
Kevin Duh?? Hajime Tsukada? Masaaki Nagata?
+The Graduate University For Advanced Studies, Tokyo, Japan
?NTT Communication Science Laboratories, NTT Corporation
+handan@nii.ac.jp, ?wuxianchao@baidu.com, ?kevinduh@is.naist.jp
?{sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp
Abstract
In Statistical Machine Translation, reorder-
ing rules have proved useful in extracting
bilingual phrases and in decoding during
translation between languages that are struc-
turally different. Linguistically motivated
rules have been incorporated into Chinese-
to-English (Wang et al, 2007) and English-
to-Japanese (Isozaki et al, 2010b) transla-
tion with significant gains to the statistical
translation system. Here, we carry out a lin-
guistic analysis of the Chinese-to-Japanese
translation problem and propose one of the
first reordering rules for this language pair.
Experimental results show substantially im-
provements (from 20.70 to 23.17 BLEU)
when head-finalization rules based on HPSG
parses are used, and further gains (to 24.14
BLEU) were obtained using more refined
rules.
1 Introduction
In state-of-the-art Statistical Machine Translation
(SMT) systems, bilingual phrases are the main
building blocks for constructing a translation given
a sentence from a source language. To extract
those bilingual phrases from a parallel corpus,
the first step is to discover the implicit word-
to-word correspondences between bilingual sen-
tences (Brown et al, 1993). Then, a symmetriza-
tion matrix is built (Och and Ney, 2004) by us-
ing word-to-word alignments, and a wide variety
?Now at Baidu Japan Inc.
? Now at Nara Institute of Science and Technology
(NAIST)
of heuristics can be used to extract the bilingual
phrases (Zens et al, 2002; Koehn et al, 2003).
This method performs relatively well when the
source and the target languages have similar word
order, as in the case of French, Spanish, and En-
glish. However, when translating between lan-
guages with very different structures, as in the case
of English and Japanese, or Japanese and Chinese,
the quality of extracted bilingual phrases and the
overall translation quality diminishes.
In the latter scenario, a simple but effective strat-
egy to cope with this problem is to reorder the
words of sentences in one language so that it re-
sembles the word order of another language (Wu
et al, 2011; Isozaki et al, 2010b). The advan-
tages of this strategy are two fold. The first ad-
vantage is at the decoding stage, since it enables
the translation to be constructed almost monoton-
ically. The second advantage is at the training
stage, since automatically estimated word-to-word
alignments are likely to be more accurate and sym-
metrization matrices reveal more evident bilingual
phrases, leading to the extraction of better quality
bilingual phrases and cleaner phrase tables.
In this work, we focus on Chinese-to-Japanese
translation, motivated by the increasing interaction
between these two countries and the need to im-
prove direct machine translation without using a
pivot language. Despite the countries? close cul-
tural relationship, their languages significantly dif-
fer in terms of syntax, which poses a severe diffi-
culty in statistical machine translation. The syntac-
tic relationship of this language pair has not been
carefully studied before in the machine translation
57
field, and our work aims to contribute in this direc-
tion as follows:
? We present a detailed syntactic analysis of
several reordering issues in Chinese-Japanese
translation using the information provided by
an HPSG-based deep parser.
? We introduce novel reordering rules based on
head-finalization and linguistically inspired
refinements to make words in Chinese sen-
tences resemble Japanese word order. We em-
pirically show its effectiveness (e.g. 20.70 to
24.23 BLEU improvement).
The paper is structured as follows. Section 2 in-
troduces the background and gives an overview of
similar techniques related to this work. Section 3
describes the proposed method in detail. Exper-
imental evaluation of the performance of the pro-
posed method is described in section 4. There is an
error analysis on the obtained results in section 5.
Conclusions and a short description on future work
derived from this research are given in the final
section.
2 Background
2.1 Head Finalization
The structure of languages can be characterized
by phrase structures. The head of a phrase is the
word that determines the syntactic category of the
phrase, and its modifiers (also called dependents)
are the rest of the words within the phrase. In En-
glish, the head of a phrase can be usually found
before its modifiers. For that reason, English is
called a head-initial language (Cook and Newson,
1988). Japanese, on the other hand, is head-final
language (Fukui, 1992), since the head of a phrase
always appears after its modifiers.
In certain applications, as in the case of ma-
chine translation, word reordering can be a promis-
ing strategy to ease the task when working with
languages with different phrase structures like En-
glish and Japanese. Head Finalization is a success-
ful syntax-based reordering method designed to re-
order sentences from a head-initial language to re-
semble the word order in sentences from a head-
final language (Isozaki et al, 2010b). The essence
of this rule is to move the syntactic heads to the
end of its dependency by swapping child nodes in
a phrase structure tree when the head child appears
before the dependent child.
Isozaki et al (2010b) proposed a simple method
of Head Finalization, by using an HPSG-based
deep parser for English (Miyao and Tsujii, 2008)
to obtain phrase structures and head information.
The score results from several mainstream evalua-
tion methods indicated that the translation quality
had been improved; the scores of Word Error Rate
(WER) and Translation Edit Rate (TER) (Snover
et al, 2006) had especially been greatly reduced.
2.2 Chinese Deep Parsing
Syntax-based reordering methods need parsed sen-
tences as input. Isozaki et al (2010b) used Enju,
an HPSG-based deep parser for English, but they
also discussed using other types of parsers, such
as word dependency parsers and Penn Treebank-
style parsers. However, to use word dependency
parsers, they needed an additional heuristic rule to
recover phrase structures, and Penn Treebank-style
parsers are problematic because they output flat
phrase structures (i.e. a phrase may have multiple
dependents, which causes a problem of reorder-
ing within a phrase). Consequently, compared to
different types of parsers, Head-Final English per-
forms the best on the basis of English Enju?s pars-
ing result.
In this paper, we follow their observation, and
use the HPSG-based parser for Chinese (Chinese
Enju) (Yu et al, 2011) for Chinese syntactic pars-
ing. Since Chinese Enju is based on the same pars-
ing model as English Enju, it provides rich syn-
tactic information including phrase structures and
syntactic/semantic heads.
Figure 1 shows an example of an XML output
from Chinese Enju for the sentence ?wo (I) qu (go
to) dongjing (Tokyo) he (and) jingdu (Ky-
oto).? The label <cons> and <tok> represent
the non-terminal nodes and terminal nodes, respec-
tively. Each node is identified by a unique ?id?
and has several attributes. The attribute ?head?
indicates which child node is the syntactic head.
In this figure, <head=?c4? id=?c3?> means that
the node that has id=?c4? is the syntactic head of
the node that has id=?c3?.
58
Figure 1: An XML output for a Chinese sentence from
Chinese Enju. For clarity, we only draw information
related to the phrase structure and the heads.
2.3 Related Work
Reordering is a popular strategy for improving
machine translation quality when source and tar-
get languages are structurally very different. Re-
searchers have approached the reordering problem
in multiple ways. The most basic idea is pre-
ordering (Xia and McCord, 2004; Collins et al,
2005), that is, to do reordering during preprocess-
ing time, where the source side of the training and
development data and sentences from a source lan-
guage that have to be translated are first reordered
to ease the training and the translation, respec-
tively. In (Xu et al, 2009), authors used a depen-
dency parser to introduce manually created pre-
ordering rules to reorder English sentences when
translating into five different SOV(Subject-Object-
Verb) languages. Other authors (Genzel, 2010; Wu
et al, 2011) use automatically generated rules in-
duced from parallel data. Tillmann (2004) used a
lexical reordering model, and Galley et al (2004)
followed a syntactic-based model.
In this work, however, we are centered in the
design of manual rules inspired by the Head Final-
ization (HF) reordering (Isozaki et al, 2010b). HF
reordering is one of the simplest methods for pre-
ordering that significantly improves word align-
ments and leads to a better translation quality. Al-
though the method is limited to translation where
the target language is head-final, it requires neither
training data nor fine-tuning. To our knowledge,
HF is the best method to reorder languages when
translating into head-final languages like Japanese.
The implementation of HF method for English-
to-Japanese translation appears to work well. A
reasonable explanation for this is the close match
between the concept of ?head? in this language
pair. However, for Chinese-to-Japanese, there are
differences in the definitions of numbers of impor-
tant syntactic concepts, including the definition of
the syntactic head. We concluded that the diffi-
culties we encountered in using HF to Chinese-to-
Japanese translation were the result of these differ-
ences in the definition of ?head?. As we believe
that such differences are also likely to be observed
in other language pairs, the present work is gener-
ally important for head-initial to head-final trans-
lation as it shows a systematic linguistic analysis
that consistently improves the effectivity of the HF
method.
3 Syntax-based Reordering Rules
This section describes our method for syntax-
based reordering for Chinese-to-Japanese transla-
tion. We start by introducing Head Finalization
for Chinese (HFC), which is a simple adaptation
of Isozaki et al (2010b)?s method for English-to-
Japanese translation. However, we found that this
simple method has problems when applied to Chi-
nese, due to peculiarities in Chinese syntax. In
Section 3.2, we analyze several distinctive cases of
the problem in detail. And following this analysis,
Section 3.3 proposes a refinement of the original
HFC, with a couple of exception rules for reorder-
ing.
3.1 Head Finalization for Chinese (HFC)
Since Chinese and English are both known to be
head-initial languages1, the reordering rule intro-
duced in (Isozaki et al, 2010b) ideally would re-
order Chinese sentences to follow the word order
1As Gao (2008) summarized, whether Chinese is a head-
initial or a head-final language is open for debate. Neverthe-
less, we take the view that most Chinese sentence structures
are head-initial since the written form of Chinese mainly be-
haves as an head-initial language.
59
Figure 2: Simple example for Head-Final Chinese. The left figure shows the parsing tree of the original sentence
and its English translation. The right figure shows the reordered sentence along with its Japanese translation.
( ?*? indicate the syntactic head).
of their Japanese counterparts.
Figure 2 shows an example of a head finalized
Chinese sentence based on the output from Chi-
nese Enju shown in Figure 1. Notice that the
coordination exception rule described in (Isozaki
et al, 2010b) also applies to Chinese reordering.
This exception rule says that child nodes are not
swapped if the node is a coordination2. Another
exception rule is for punctuation symbols, which
are also preserved in their original order. In this
case, as can be seen in the example in Figure 2, the
nodes of c3, c6, and c8 had not been swapped with
their dependency. In this account, only the verb
?qu? had been moved to the end of the sentence,
following the same word order as its Japanese
translation.
3.2 Discrepancies in Head Definition
Head Finalization relies on the idea that head-
dependent relations are largely consistent among
different languages while word orders are differ-
ent. However, in Chinese, there has been much
debate on the definition of head3, possibly because
Chinese has fewer surface syntactic features than
other languages like English and Japanese. This
causes some discrepancies between the definitions
2Coordination is easily detected in the output of
Enju; it is marked by the attributes xcat="COOD" or
schema="coord-left/right" as shown in Figure 1.
3In this paper, we only consider the syntactic head.
of the head in Chinese and Japanese, which leads
to undesirable reordering of Chinese sentences.
Specifically, in preliminary experiments we ob-
served unexpected reorderings that are caused by
the differences in the head definitions, which we
describe below.
3.2.1 Aspect Particle
Although Chinese has no syntactic tense marker,
three aspect particles following verbs can be used
to identify the tense semantically. They are ?le0?
(did), ?zhe0? (doing), and ?guo4? (done), and
their counterparts in Japanese are ?ta?, ?teiru?,
and ?ta?, respectively. Both the first word and
third word can represent the past tense, but the
third one is more often used in the past perfect.
The Chinese parser4 treated aspect particles as
dependents of verbs, whereas their Japanese coun-
terparts are identified as the head. For exam-
ple in Table 15, ?qu? (go) and ?guo? (done)
aligned with ?i? and ?tta?, respectively. How-
ever, since ?guo? is treated as a dependent of
?qu?, by directly implementing the Head Final
Chinese (HFC), the sentence will be reordered like
4The discussions in this section presuppose the syntactic
analysis done by Chinese Enju, but most of the analysis is
consistent with the common explanation for Chinese syntax.
5English translation (En); Chinese original sentence
(Ch); reordered Chinese by Head-Final Chinese (HFC); re-
ordered Chinese by Refined Head-Final Chinese (R-HFC)
and Japanese translation (Ja).
60
HFC in Table 1, which does not follow the word
order of the Japanese (Ja) translation. In contrast,
the reordered sentence from refined-HFC (R-HFC)
can be translated monotonically.
En I have been to Tokyo.
Ch wo qu guo dongjing.
HFC wo dongjing guo qu.
R-HFC wo dongjing qu guo.
Ja watashi (wa) Tokyo (ni) i tta.
Table 1: An example for Aspect Particle. Best word
alignment Ja-Ch (En): ?watashi? ? ?wo?(I); ?Tokyo? ?
?dongjing? (Tokyo); ?i? ? ?qu? (been); ?tta? ? ?guo?
(have).
3.2.2 Adverbial Modifier ?bu4?
Both in Chinese and Japanese, verb phrase mod-
ifiers typically occur in pre-verbal positions, espe-
cially when the modifiers are adverbs. Since ad-
verbial modifiers are dependents in both Chinese
and Japanese, head finalization works perfectly for
them. However, there is an exceptional adverb,
?bu4?, which means negation and is usually trans-
lated into ?nai?, which is always at the end of the
sentence in Japanese and thus is the head. For ex-
ample in Table 2, the word ?kan? (watch) will be
identified as the head and the word ?bu? is its de-
pendent; on the contrary, in the Japanese transla-
tion (Ja), the word ?nai?, which is aligned with
?bu?, will be identified as the head. Therefore,
the Head Final Chinese is not in the same order,
but the reordered sentence by R-HFC obtained the
same order with the Japanese translation.
En I do not watch TV.
Ch wo bu kan dianshi.
HFC wo dianshi bu kan.
R-HFC wo dianshi kan bu.
Ja watashi (wa) terebi (wo) mi nai.
Table 2: An example for Adverbial Modifier bu4.
Best word alignment Ja-Ch (En): ?watashi? ? ?wo? (I);
?terebi? ? ?dianshi? (TV); ?mi? ? ?kan? (watch); ?nai?
? ?bu? (do not).
3.2.3 Sentence-final Particle
Sentence-final particles often appear at the end
of a sentence to express a speaker?s attitude:
e.g. ?ba0, a0? in Chinese, and ?naa, nee? in
Japanese. Although they appear in the same posi-
tion in both Chinese and Japanese, in accordance
with the differences of head definition, they are
identified as the dependent in Chinese while they
are the head in Japanese. For example in Table 3,
since ?a0? was identified as the dependent, it had
been reordered to the beginning of the sentence
while its Japanese translation ?nee? is at the end
of the sentence as the head. Likewise, by refining
the HFC, we can improve the word alignment.
En It is good weather.
Ch tianqi zhenhao a.
HFC a tianqi zhenhao.
R-HFC tianqi zhenhao a.
Ja ii tennki desu nee.
Table 3: An example for Sentence-final Particle.
Best word alignment Ja-Ch (En): ?tennki? ? ?tianqi?
(weather); ?ii? ? ?zhenhao? (good); ?nee? ? ?a? (None).
3.2.4 Et cetera
In Chinese, there are two expressions for rep-
resenting the meaning of ?and other things? with
one Chinese character: ?deng3? and ?deng3
deng3?, which are both identified as dependent
of a noun. In contrast, in Japanese, ?nado? is al-
ways the head because it appears as the right-most
word in a noun phrase. Table 4 shows an example.
En Fruits include apples, etc.
Ch shuiguo baokuo pingguo deng.
HFC shuiguo deng pingguo baokuo.
R-HFC shuiguo pingguo deng baokuo.
Ja kudamono (wa) ringo nado (wo)
fukunde iru.
Table 4: An example for Et cetera. Best word alignment
Ja-Ch (En): ?kudamono? ? ?shuiguo? (Fruits); ?ringo?
? ?pingguo? (apples); ?nado? ? ?deng? (etc.); ?fukunde
iru? ? ?baokuo? (include).
61
AS Aspect particle
SP Sentence-final particle
ETC et cetera (i.e. deng3 and deng3 deng3)
IJ Interjection
PU Punctuation
CC Coordinating conjunction
Table 5: The list of POSs for exception reordering rules
3.3 Refinement of HFC
In the preceding sections, we have discussed syn-
tactic constructions that cause wrong application
of Head Finalization to Chinese sentences. Fol-
lowing the observations, we propose a method to
improve the original Head Finalization reordering
rule to obtain better alignment with Japanese.
The idea is simple: we define a list of POSs,
and when we find one of them as a dependent
child of the node, we do not apply reordering. Ta-
ble 5 shows the list of POSs we define in the cur-
rent implementation6. While interjections are not
discussed in detail, we should obviously not re-
order to interjections because they are position-
independent. The rules for PU and CC are ba-
sically equivalent to the exception rules proposed
by (Isozaki et al, 2010b).
4 Experiments
The corpus we used as training data comes
from the China Workshop on Machine Transla-
tion (CWMT) (Zhao et al, 2011). This is a
Japanese-Chinese parallel corpus in the news do-
main, containing 281, 322 sentence pairs. We also
collected another Japanese-Chinese parallel cor-
pus from news containing 529, 769 sentences and
merged it with the CWMT corpus to create an ex-
tended version of the CWMT corpus. We will re-
fer to this corpus as ?CWMT ext.? We split an in-
verted multi-reference set into a development and a
test set containing 1, 000 sentences each. In these
two sets, the Chinese input was different, but the
Japanese reference was identical. We think that
this split does not pose any severe problem to the
comparison fairness of the experiment, since no
new phrases are added during tuning and the ex-
perimental conditions remain equal for all tested
6The POSs are from Penn Chinese Treebank.
Ch Ja
CWMT
Sentences 282K
Run. words 2.5M 3.2M
Avg. sent. leng. 8.8 11.5
Vocabulary 102K 42K
CWMT ext.
Sentences 811K
Run. words 14.7M 17M
Avg. sent. leng. 18.1 20.9
Vocabulary 249K 95K
Dev.
Sentences 1000
Run. words 29.9K 35.7K
Avg. sent. leng. 29.9 35.7
OoV w.r.t. CWMT 485 106
OoV w.r.t. CWMT ext. 244 53
Test
Sentences 1000
Run. words 25.8K 35.7K
Avg. sent. leng. 25.8 35.7
OoV w.r.t. CWMT 456 106
OoV w.r.t. CWMT ext. 228 53
Table 6: Characteristics of CWMT and extended
CWMT Chinese-Japanese corpus. Dev. stands for De-
velopment, OoV for ?Out of Vocabulary? words, K for
thousands of elements, and M for millions of elements.
Data statistics were collected after tokenizing.
methods. Detailed Corpus statistics can be found
in Table 6.
To parse Chinese sentences, we used Chinese
Enju (Yu et al, 2010), an HPSG-based parser
trained with the Chinese HPSG treebank converted
from Penn Chinese Treebank. Chinese Enju re-
quires segmented and POS-tagged sentences to
do parsing. We used the Stanford Chinese seg-
menter (Chang et al, 2008) and Stanford POS-
tagger (Toutanova et al, 2003) to obtain the seg-
mentation and POS-tagging of the Chinese side of
the training, development, and test sets.
The baseline system was trained following
the instructions of recent SMT evaluation cam-
paigns (Callison-Burch et al, 2010) by using the
MT toolkit Moses (Koehn et al, 2007) in its de-
fault configuration. Phrase pairs were extracted
from symmetrized word alignments and distor-
tions generated by GIZA++ (Och and Ney, 2003)
using the combination of heuristics ?grow-diag-
final-and? and ?msd-bidirectional-fe?. The lan-
guage model was a 5-gram language model es-
timated on the target side of the parallel cor-
pora by using the modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999) implemented in
62
the SRILM (Stolcke, 2002) toolkit. The weights
of the log-linear combination of feature functions
were estimated by using MERT (Och, 2003) on the
development set described in Table 6.
The effectiveness of the reorderings proposed
in Section 3.3 was assessed by using two preci-
sion metrics and two error metrics on translation
quality. The first evaluation metric is BLEU (Pap-
ineni et al, 2002), a very common accuracy metric
in SMT that measures N -gram precision, with a
penalty for too short sentences. The second eval-
uation metric was RIBES (Isozaki et al, 2010a), a
recent precision metric used to evaluate translation
quality between structurally different languages. It
uses notions on rank correlation coefficients and
precision measures. The third evaluation metric is
TER (Snover et al, 2006), another error metric that
computes the minimum number of edits required
to convert translated sentences into its correspond-
ing references. Possible edits include insertion,
deletion, substitution of single words, and shifts of
word sequences. The fourth evaluation metric is
WER, an error metric inspired in the Levenshtein
distance at word level. BLEU, WER, and TER
were used to provide a sense of comparison but
they do not significantly penalize long-range word
order errors. For this reason, RIBES was used to
account for this aspect of translation quality.
The baseline system was trained and tuned us-
ing the same configuration setup described in this
section, but no reordering rule was implemented at
the preprocessing stage.
Three systems have been run to translate the test
set for comparison when the systems were trained
using the two training data sets. They are the
baseline system, the system consisting in the na??ve
implementation of HF reordering, and the system
with refined HFC reordering rules. Assessment of
translation quality can be found in Table 7.
As can be observed in Table 7, the translation
quality, as measured by precision and error met-
rics, was consistently and significantly increased
when the HFC reordering rule was used and was
significantly improved further when the refinement
proposed in this work was used. Specifically, the
BLEU score increased from 19.94 to 20.79 when
the CWMT corpus was used, and from 23.17 to
24.14 when the extended CWMT corpus was used.
AS SP ETC IJ PU COOD
3.8% 0.8% 1.3% 0.0%* 21.0% 38.3%
Table 8: Weighted recall of each exception rule during
reordering on CWMT ext. training data, dev data, and
test data. (* actual value 0.0016%.)
Table 8 shows the recall of each exception rule
listed in Section 3, and was computed by counting
the times an exception rule was triggered divided
by the number of times the head finalization rule
applied. Data was collected for CWMT ext. train-
ing, dev and test sets. Although the exception rules
related to aspect particles, Et cetera, sentence-final
particles and interjections have a comparatively
lower frequency of application than punctuation
or coordination exception rules, the improvements
they led to are significant.
5 Error Analysis
In Section 3 we have analyzed syntactic differ-
ences between Chinese and Japanese that led to
the design of an effective refinement. A manual
error analysis of the results of our refined reorder-
ing rules showed that some more reordering issues
remain and, although they are not side effects of
our proposed rule, they are worth mentioning in
this separate section.
5.1 Serial Verb Construction
Serial verb construction is a phenomenon occur-
ring in Chinese, where several verbs are put to-
gether as one unit without any conjunction be-
tween them. The relationship between these
verbs can be progressive or parallel. Apparently,
Japanese has a largely corresponding construc-
tion, which indicates that no reordering should
be applied. An example to illustrate this fact in
Chinese is ?weishi (maintain) shenhua (deepen)
zhongriguanxi (Japan-China relations) de
(of) gaishan (improvement) jidiao (basic
tone).?7 The two verbs ?weishi? (in Japanese,
iji) and ?shenhua? (in Japanese, shinka) are
used together, and they follow the same order as
in Japanese: ?nicchukankei (Japan-China re-
7English translation: Maintain and deepen the improved
basic tone of Japan-China relations.
63
CWMT CWMT ext.
BLEU RIBES TER WER BLEU RIBES TER WER
baseline 16.74 71.24 70.86 77.45 20.70 74.21 66.10 72.36
HFC 19.94 73.49 65.19 71.39 23.17 75.35 61.38 67.74
refined HFC 20.79 75.09 64.91 70.39 24.14 77.17 59.67 65.31
Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for
training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed
refinement of head finalization reordering rules.
lations) no (of) kaizan (improvement) kityo
(basic tone) wo iji (maintain) shinka (deepen)
suru (do).?
5.2 Complementizer
A ?complementizer? is a particle used to intro-
duce a complement. In English, a very common
complementizer is the word ?that? when making a
clausal complement, while in Chinese it can de-
note other types of word, such as verbs, adjec-
tives or quantifiers. The complementizer is iden-
tified as the dependent of the verb that it modi-
fies. For instance, a Chinese sentence: ?wo (I)
mang wan le (have finished the work).? This
can be translated into Japanese: ?watashi (I) wa
shigoto (work) wo owa tta (have finished).? In
Chinese, the verb ?mang? is the head while ?wan?
is the complementizer, and its Japanese counter-
part ?owa tta? has the same word order.
However, during the reordering, ?mang? will be
placed at the end of the sentence and ?wan? in the
beginning, leading to an inconsistency with respect
to the Japanese translation where the complemen-
tizer ?tta? is the head.
5.3 Verbal Nominalization and Nounal
Verbalization
As discussed by Guo (2009), compared to English
and Japanese, Chinese has little inflectional mor-
phology, that is, no inflection to denote tense, case,
etc. Thus, words are extremely flexible, making
verb nominalization and noun verbalization appear
frequently and commonly without any conjugation
or declension. As a result, it is difficult to do dis-
ambiguation during POS tagging and parsing. For
example, the Chinese word ?kaifa? may have
two syntactic functions: verb (develop) and noun
(development). Thus, it is difficult to reliably tag
without considering the context. In contrast, in
Japanese, ?suru? can be used to identify verbs.
For example, ?kaihatu suru? (develop) is a
verb and ?kaihatu? (development) is a noun.
This ambiguity is prone to not only POS tagging
error but also parsing error, and thus affects the
identification of heads, which may lead to incor-
rect reordering.
5.4 Adverbial Modifier
Unlike the adverb ?bu4? we discussed in Sec-
tion 3.2, the ordinary adverbial modifier comes
directly before the verb it modifies both in Chi-
nese and Japanese, but not in English. Nev-
ertheless, in accordance with the principle of
identifying the head for Chinese, the adverb
will be treated as the dependent and it will
not be reordered following the verb it modi-
fied. As a result, the alignment between adverbs
and verbs is non-monotonic. This can be ob-
served in the Chinese sentence ?guojia (coun-
try) yanli (severely) chufa (penalize) jiage
(price) weifa (violation) xingwei (behavior)?8,
and its Japanese translation: ?kuni (country) wa
kakaku (price) no ihou (violation) koui (be-
havior) wo kibisiku (severely) syobatu (penal-
ize).? Both in Chinese and Japanese, the adverbial
modifier ?yanli? and ?kibisiku? are directly
in front of the verb ?chufa? and ?syobatu?, re-
spectively. However, the verb in Chinese is identi-
fied as the head and will be reordered to the end of
the sentence without the adverb.
8English translation: The country severely penalizes vio-
lations of price restrictions.
64
5.5 POS tagging and Parsing Errors
There were word reordering issues not caused
solely by differences in syntactic structures. Here
we summarize two that are difficult to remedy dur-
ing reordering and that are hard to avoid since re-
ordering rules are highly dependent on the tagger
and parser.
? POS tagging errors
In Chinese, for example, the word ?Iran?
was tagged as ?VV? or ?JJ? instead of ?NR?.
This led to identifying ?Iran? as a head in
accordance with the head definition in Chi-
nese, and it was reordered undesirably.
? Parsing errors
For example, in the Chinese verb phrase
?touzi (invest) 20 yi (200 million)
meiyuan (dollars)?, ?20? and ?yi? were
identified as dependent of ?touzi? and
?meiyuan?, respectively, which led to an
unsuitable reordering for posterior word
alignment.
6 Conclusion and Future Work
In the present work, we have proposed novel
Chinese-to-Japanese reordering rules inspired
in (Isozaki et al, 2010b) based on linguistic analy-
sis on Chinese HPSG and differences among Chi-
nese and Japanese. Although a simple implemen-
tation of HF to reorder Chinese sentences per-
forms well, translation quality was substantially
improved further by including linguistic knowl-
edge into the refinement of the reordering rules.
In Section 5, we found more patterns on reorder-
ing issues when reordering Chinese sentences to
resemble Japanese word order. The extraction of
those patterns and their effective implementation
may lead to further improvements in translation
quality, so we are planning to explore this possi-
bility.
In this work, syntactic information from a deep
parser has been used to reorder words better. We
believe that using semantic information can fur-
ther increase the expressive power of reordering
rules. With that objective, Chinese Enju can be
used since it provides the semantic head of nodes
and can interpret sentences by using their semantic
dependency.
Acknowledgments
This work was mainly developed during an intern-
ship at NTT Communication Science Laborato-
ries. We would like to thank Prof. Yusuke Miyao
for his invaluable support on this work.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of ma-
chine translation. In Computational Linguistics, vol-
ume 19, pages 263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the joint 5th workshop on Statistical Ma-
chine Translation and MetricsMATR. Association
for Computational Linguistics, July.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word seg-
mentation for machine translation performance. In
Proceedings of the 3rd Workshop on SMT, pages
224?232, Columbus, Ohio. Association for Compu-
tational Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Vivian James Cook and Mark Newson. 1988. Chom-
sky?s Universal Grammar: An introduction. Oxford:
Basil Blackwell.
Naoki Fukui. 1992. Theory of Projection in Syntax.
CSLI Publisher and Kuroshio Publisher.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. Whats in a translation rule?
In Proceedings of HLT-NAACL.
Qian Gao. 2008. Word order in mandarin: Reading and
speaking. In Proceedings of the 20th North Ameri-
can Conference on Chinese Linguistics (NACCL-20),
volume 2, pages 611?626.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
65
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuqing Guo. 2009. Treebank-based acquisition of
Chinese LFG resources for parsing and generation.
Ph.D. thesis, Dublin City University.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of Empirical Methods on Nat-
ural Language Processing (EMNLP).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMTMetricsMATR, pages 244?251.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings
HLT/NAACL?03, pages 48?54.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL Demo and Poster Sessions, 2007, pages
177?180, June 25?27.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34:35?80, March.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st annual conference of the Association for Com-
putational Linguistics, 2003, pages 160?167, July 7?
12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual conference of the Association for Com-
putational Linguistics, 2002, pages 311?318, July 6?
12.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on Spoken Language Pro-
cessing, 2002, pages 901?904, September 16?20.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings OF HLT-NAACL, pages 252?259.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 737?
745, Prague, Czech Republic, June. Association for
Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29?
37, Chiang Mai, Thailand, November. Asian Feder-
ation of Natural Language Processing.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 245?253, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kun Yu, Yusuke Miyao, Xiangli Wang, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2010. Semi-
automatically developing chinese hpsg grammar
from the penn chinese treebank for deep parsing. In
COLING (Posters)?10, pages 1417?1425.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
the 12th International Conference on Parsing Tech-
nologies, pages 48?57.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of
KI?02, pages 18?32.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th china workshop on machine translation
(cwmt2011). The 7th China Workshop on Machine
Translation (CWMT2011).
66
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 503?511,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Hidden Markov Tree Model for Word Alignment
Shuhei Kondo Kevin Duh Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara, 630-0192, Japan
{shuhei-k,kevinduh,matsu}@is.naist.jp
Abstract
We propose a novel unsupervised word
alignment model based on the Hidden
Markov Tree (HMT) model. Our model
assumes that the alignment variables have
a tree structure which is isomorphic to the
target dependency tree and models the dis-
tortion probability based on the source de-
pendency tree, thereby incorporating the
syntactic structure from both sides of the
parallel sentences. In English-Japanese
word alignment experiments, our model
outperformed an IBM Model 4 baseline
by over 3 points alignment error rate.
While our model was sensitive to poste-
rior thresholds, it also showed a perfor-
mance comparable to that of HMM align-
ment models.
1 Introduction
Automatic word alignment is the first step in the
pipeline of statistical machine translation. Trans-
lation models are usually extracted from word-
aligned bilingual corpora, and lexical translation
probabilities based on word alignment models are
also used for translation.
The most widely used models are the IBM
Model 4 (Brown et al, 1993) and Hidden Markov
Models (HMM) (Vogel et al, 1996). These mod-
els assume that alignments are largely monotonic,
possibly with a few jumps. While such assump-
tion might be adequate for alignment between sim-
ilar languages, it does not necessarily hold be-
tween a pair of distant languages like English and
Japanese.
Recently, several models have focused on in-
corporating syntactic structures into word align-
ment. As an extension to the HMM alignment,
Lopez and Resnik (2005) present a distortion
model conditioned on the source-side dependency
tree, and DeNero and Klein (2007) propose a
distortion model based on the path through the
source-side phrase-structure tree. Some super-
vised models receive syntax trees as their input
and use them to generate features and to guide the
search (Riesa and Marcu, 2010; Riesa et al, 2011),
and other models learn a joint model for pars-
ing and word alignment from word-aligned par-
allel trees (Burkett et al, 2010). In the context of
phrase-to-phrase alignment, Nakazawa and Kuro-
hashi (2011) propose a Bayesian subtree align-
ment model trained with parallel sampling. None
of these models, however, can incorporate syntac-
tic structures from both sides of the language pair
and can be trained computationally efficiently in
an unsupervised manner at the same time.
The Hidden Markov Tree (HMT) model
(Crouse et al, 1998) is one such model that sat-
isfies the above-mentioned properties. The HMT
model assumes a tree structure of the hidden vari-
ables, which fits well with the notion of word-to-
word dependency, and it can be trained from un-
labeled data via the EM algorithm with the same
order of time complexity as HMMs.
In this paper, we propose a novel word align-
ment model based on the HMT model and show
that it naturally enables unsupervised training
based on both source and target dependency trees
in a tractable manner. We also compare our HMT
word alignment model with the IBM Model 4 and
the HMM alignment models in terms of the stan-
dard alignment error rates on a publicly available
English-Japanese dataset.
2 IBM Model 1 and HMM Alignment
We briefly review the IBM Model 1 (Brown et
al., 1993) and the Hidden Markov Model (HMM)
word alignment (Vogel et al, 1996) in this section.
Both are probabilistic generative models that fac-
503
tor as
p(f |e) =
?
a
p(a, f |e)
p(a, f |e) =
J?
j=1
pd(aj |aj )pt(fj |eaj )
where e = {e1, ..., eI} is an English (source) sen-
tence and f = {f1, ..., fJ} is a foreign (target)
sentence. a = {a1, ..., aJ} is an alignment vec-
tor such that aj = i indicates the j-th target word
aligns to the i-th source word and aj = 0 means
the j-th target word is null-aligned. j is the index
of the last non null-aligned target word before the
index j.
In both models, pt(fj |eaj ) is the lexical transla-
tion probability and can be defined as conditional
probability distributions. As for the distortion
probability pd(aj |aj ), pd(aj = 0|aj = i?) = p0
where p0 is NULL probability in both models.
pd(aj = i|aj = i?) is uniform in the Model 1
and proportional to the relative count c(i ? i?) in
the HMM for i 6= 0. DeNero and Klein (2007)
proposed a syntax-sensitive distortion model for
the HMM alignment, in which the distortion prob-
ability depends on the path from the i-th word to
the i?-th word on the source-side phrase-structure
tree, instead of the linear distance between the two
words.
These models can be trained efficiently using
the EM algorithm. In practice, models in two di-
rections (source to target and target to source) are
trained and then symmetrized by taking their in-
tersection, union or using other heuristics. Liang
et al (2006) proposed a joint objective of align-
ment models in both directions and the probability
of agreement between them, and an EM-like algo-
rithm for training.
They also proposed posterior thresholding for
decoding and symmetrization, which take
a = {(i, j) : p(aj = i|f , e) > ?}
with a threshold ? . DeNero and Klein (2007) sum-
marized some criteria for posterior thresholding,
which are
? Soft-Union
?
pf (aj = i|f , e) ? pr(ai = j|f , e)
? Soft-Intersection
pf (aj = i|f , e) + pr(ai = j|f , e)
2
? Hard-Union
max(pf (aj = i|f , e), pr(ai = j|f , e))
? Hard-Intersection
min(pf (aj = i|f , e), pr(ai = j|f , e))
where pf (aj = i|f , e) is the alignment probabil-
ity under the source-to-target model and pr(ai =
j|f , e) is the one under the target-to-source model.
They also propose a posterior decoding heuris-
tic called competitive thresholding. Given a j ? i
matrix of combined weights c and a threshold ? , it
choose a link (j, i) only if its weight cji ? ? and it
is connected to the link with the maximum weight
both in row j and column i.
3 Hidden Markov Tree Model
The Hidden Markov Tree (HMT) model was first
introduced by Crouse et al (1998). Though it has
been applied successfully to various applications
such as image segmentation (Choi and Baraniuk,
2001), denoising (Portilla et al, 2003) and biol-
ogy (Durand et al, 2005), it is largely unnoticed
in the field of natural language processing. To
the best of our knowledge, the only exception is
Z?abokrtsky` and Popel (2009) who used a variant
of the Viterbi algorithm for HMTs in the transfer
phase of a deep-syntax based machine translation
system.
An HMT model consists of an observed random
tree X = {x1, ..., xN} and a hidden random tree
S = {s1, ..., sN}, which is isomorphic to the ob-
served tree.
The parameters of the model are
? P (s1 = j), the initial hidden state prior
? P (st = j|s?(t) = i), transition probabilities
? P (xt = h|st = j), emission probabilities,
where ?() is a function that maps the index of a
hidden node to the index of its parent node. These
parameters can be trained via the EM algorithm.
The ?upward-downward? algorithm proposed
in Crouse et al (1998), an HMT analogue of the
forward-backward algorithm for HMMs, can be
used in the E-step. However, it is based on the de-
composition of joint probabilities and suffers from
numerical underflow problems.
Durand et al (2004) proposed a smoothed vari-
ant of the upward-downward algorithm, which is
504
based on the decomposition of smoothed probabil-
ities and immune to underflow. In the next section,
we will explain this variant in the context of word
alignment.
4 Hidden Markov Tree Word Alignment
We present a novel word alignment model based
on the HMT model. Given a target sentence f =
{f1, ..., fJ}with a dependency treeF and a source
sentence e = {e1, ..., eI} with a dependency tree
E, an HMT word alignment model factors as
p(f |e) =
?
a
p(a, f |e)
p(a, f |e) =
J?
j=1
pd(aj |aj )pt(fj |eaj ).
While these equations appear identical to the ones
for the HMM alignment, they are different in that
1) e, f and a are not chain-structured but tree-
structured, and 2) j is the index of the non null-
aligned lowest ancestor of the j-th target word1,
rather than that of the last non null-aligned word
preceding the j-th word as in the HMM alignment.
Note that A, the tree composed of alignment vari-
ables a = {a1, ..., aJ}, is isomorphic to the target
dependency tree F.
Figure 1 shows an example of a target depen-
dency tree with an alignment tree, and a source
dependency tree. Note that English is the target
(or foreign) language and Japanese is the source
(or English) language here. We introduce the fol-
lowing notations following Durand et al (2004),
slightly modified to better match the context of
word alignment.
? ?(j) denotes the index of the head of the j-th
target word.
? c(j) denotes the set of indices of the depen-
dents of the j-th target word.
? Fj = f j denotes the target dependency sub-
tree rooted at the j-th word.
As for the parameters of the model, the initial
hidden state prior described in Section 3 can be
defined by assuming an artificial ROOT node for
both dependency trees, forcing the target ROOT
node to be aligned only to the source ROOT
1This dependence on aj can be implemented as a first-
order HMT, analogously to the case of the HMM alignment
(Och and Ney, 2003).
node and prohibiting other target nodes from be-
ing aligned to the source ROOT node. The lexi-
cal translation probability pt(fj |eaj ), which corre-
sponds to the emission probability, can be defined
as conditional probability distributions just like in
the IBM Model 1 and the HMM alignment.
The distortion probability pd(aj = i|aj = i?),
which corresponds to the transition probability,
depends on the distance between the i-th source
word and the i?-th source word on the source de-
pendency tree E, which we denote d(i, i?) here-
after. We model the dependence of pd(aj =
i|aj = i?) on d(i, i?) with the counts c(d(i, i?)).
In our model, d(i, i?) is represented by a pair
of non-negative distances (up, down), where up
is the distance between the i-th word and the
lowest common ancestor (lca) of the two words,
down is the one between the i?-th word and the
lca. For example in Figure 1b, d(0, 2) = (0, 4),
d(2, 5) = (2, 2) and d(4, 7) = (3, 0). In practice,
we clip the distance by a fixed window size w and
store c(d(i, i?)) in a two-dimensional (w + 1 ) ?
(w + 1 ) matrix. When w = 3, for example, the
distance d(0, 2) = (0, 3) after clipping.
We can use the smoothed variant of upward-
downward algorithm (Durand et al, 2004) for the
E-step of the EM algorithm. We briefly explain
the smoothed upward-downward algorithm in the
context of tree-to-tree word alignment below. For
the detailed derivation, see Durand et al (2004).
In the smoothed upward-downward algorithm,
we first compute the state marginal probabilities
p(aj = i)
=
?
i?
p(a?(j) = i?)pd(aj = i|a?(j) = i?)
for each target node and each state, where
pd(aj = i|a?(j) = i?) = p0
if the j-th word is null-aligned, and
pd(aj = i|a?(j) = i?)
= (1? p0) ?
c(d(i?, i))?
i?? 6=0 c(d(i?, i??))
if the j-th word is aligned. Note that we must ar-
tificially normalize pd(aj = i|a?(j) = i?), because
unlike in the case of the linear distance, multiple
words can have the same distance from the j-th
word on a dependency tree.
505
a0 a1 a2 a3 a4 a5
? ? ? ? ? ?
?ROOT?f0 Thatf1 wasf2 Mountf3 Kuramaf4 .f5
(a) Target sentence with its dependency/alignment tree. Target words {f0, ..., f5} are emitted from
alignment variables {a0, ..., a5}. Ideally, a0 = 0, a1 = 1, a2 = 7, a3 = 5, a4 = 4 and a5 = 9.
?ROOT?e0 ??e1 ?e2 ?e3 ??e4 ?e5 ?e6 ??e7 ?e8 ?e9
that mountain Kurama mountain be .
(b) Source sentence with its dependency tree. None of the target words are aligned to e2, e3, e6 and e8.
Figure 1: An example of sentence pair under the Hidden Markov Tree word alignment model. If we
ignore the source words to which no target words are aligned, the dependency structures look similar to
each other.
In the next phase, the upward recursion, we
compute p(aj = i|Fj = f j) in a bottom-up man-
ner. First, we initialize the upward recursion for
each leaf by
?j(i) = p(aj = i|Fj = fj)
= pt(fj |ei)p(aj = i)Nj
,
where
Nj = p(Fj = fj) =
?
i
pt(fj |ei)p(aj = i).
Then, we proceed from the leaf to the root with the
following recursion,
?j(i) = p(aj = i|Fj = f j)
=
{?j??c(j) ?j,j?(i)}pt(fj |ei)p(aj = i)
Nj
,
where
Nj =
p(Fj = f j)?
j??c(j) p(Fj? = f j?)
=
?
i
{
?
j??c(j)
?j,j?(i)}pt(fj |ei)p(aj = i)
and
??(j),j(i) =
p(Fj = f j |a?(j) = i)
p(Fj = f j)
=
?
i?
?j(i?)pd(aj = i?|a?(j) = i)
p(aj = i?)
.
After the upward recursion is completed, we
compute p(aj = i|F0 = f0) in the downward
recursion. It is initialized at the root node by
?0(i) = p(a0 = i|F0 = f0).
Then we proceed in a top-down manner, comput-
ing
?j(i) = p(aj = i|F0 = f0)
= ?j(i)p(aj = i)
?
?
i?
pd(aj = i|a?(j) = i?)??(j)(i?)
??(j),j(i?)
.
for each node and each state.
The conditional probabilities
p(aj = i, a?(j) = i?|F0 = f0)
=
?j(i)pd(aj = i|a?(j) = i?)??(j)(i?)
p(aj = i)??(j),j(i?)
,
506
which is used for the estimation of distortion prob-
abilities, can be extracted during the downward re-
cursion.
In the M-step, the lexical translation model can
be updated with
pt(f |e) =
c(f, e)
c(e) ,
just like the IBM Models and HMM alignments,
where c(f, e) and c(e) are the count of the word
pair (f, e) and the source word e. However, the
update for the distortion model is a bit compli-
cated, because the matrix that stores c(d(i, i?))
does not represent a probability distribution. To
approximate the maximum likelihood estimation,
we divide the counts c(d(i, i?)) calculated during
the E-step by the number of distortions that have
the distance d(i, i?) in the training data. Then we
normalize the matrix by
c(d(i, i?)) = c(d(i, i
?))?w
i=0
?w
i?=0 c(d(i, i?))
.
Given initial parameters for the lexical trans-
lation model and the distortion counts, an HMT
aligner collects the expected counts c(f, e), c(e)
and c(d(i, i?)) with the upward-downward algo-
rithm in the E-step and re-estimate the parameters
in the M-Step. Dependency trees for the sentence
pairs in the training data remain unchanged during
the training procedure.
5 Experiment
We evaluate the performance of our HMT align-
ment model in terms of the standard alignment er-
ror rate2 (AER) on a publicly available English-
Japanese dataset, and compare it with the IBM
Model 4 (Brown et al, 1993) and HMM alignment
with distance-based (HMM) and syntax-based (S-
HMM) distortion models (Vogel et al, 1996;
Liang et al, 2006; DeNero and Klein, 2007).
We use the data from the Kyoto Free Transla-
tion Task (KFTT) version 1.3 (Neubig, 2011). Ta-
ble 1 shows the corpus statistics. Note that these
numbers are slightly different from the ones ob-
served under the dataset?s default training proce-
dure because of the difference in the preprocessing
scheme, which is explained below.
2Given sure alignments S and possible alignments P , the
alignment error rate of alignments A is 1 ? |A?S|+|A?P ||A|+|S|
(Och and Ney, 2003).
The tuning set of the KFTT has manual align-
ments. As the KFTT doesn?t distinguish between
sure and possible alignments, F-measure equals
1?AER on this dataset.
5.1 Preprocessing
We tokenize the English side of the data using the
Stanford Tokenizer3 and parse it with the Berkeley
Parser4 (Petrov et al, 2006). We use the phrase-
structure trees for the Berkeley Aligner?s syntactic
distortion model, and convert them to dependency
trees for our dependency-based distortion model5.
As the Berkeley Parser couldn?t parse 7 (out of
about 330K) sentences in the training data, we re-
moved those lines from both sides of the data. All
the sentences in the other sets were parsed suc-
cessfully.
For the Japanese side of the data, we first con-
catenate the function words in the tokenized sen-
tences using a script6 published by the author
of the dataset. Then we re-segment and POS-
tag them using MeCab7 version 0.996 and parse
them using CaboCha8 version 0.66 (Kudo and
Matsumoto, 2002), both with UniDic. Finally,
we modify the CoNLL-format output of CaboCha
where some kind of symbols such as punctuation
marks and parentheses have dependent words. We
chose this procedure for a reasonable compromise
between the dataset?s default tokenization and the
dependency parser we use.
As we cannot use the default gold alignment due
to the difference in preprocessing, we use a script9
published by the author of the dataset to modify
the gold alignment so that it better matches the
new tokenization.
5.2 Training
We initialize our models in two directions with
jointly trained IBM Model 1 parameters (5 itera-
tions) and train them independently for 5 iterations
3http://nlp.stanford.edu/software/
4We use the model trained on the WSJ portion of
Ontonotes (Hovy et al, 2006) with the default setting.
5We use Stanford?s tool (de Marneffe et al, 2006)
with options -conllx -basic -makeCopulaHead
-keepPunct for conversion.
6https://github.com/neubig/
util-scripts/blob/master/
combine-predicate.pl
7http://code.google.com/p/mecab/
8http://code.google.com/p/cabocha/
9https://github.com/neubig/
util-scripts/blob/master/
adjust-alignments.pl
507
Sentences English Tokens Japanese Tokens
Train 329,974 5,912,543 5,893,334
Dev 1,166 24,354 26,068
Tune 1,235 30,839 33,180
Test 1,160 26,730 27,693
Table 1: Corpus statistics of the KFTT.
Precision Recall AER
HMT (Proposed) 71.77 55.23 37.58
IBM Model 4 60.58 57.71 40.89
HMM 69.59 56.15 37.85
S-HMM 71.60 56.14 37.07
Table 2: Alignment error rates (AER) based on
each model?s peak performance.
with window size w = 4 for the distortion model.
The entire training procedure takes around 4 hours
on a 3.3 GHz Xeon CPU.
We train the IBM Model 4 using GIZA++ (Och
and Ney, 2003) with the training script of the
Moses toolkit (Koehn et al, 2007).
The HMM and S-HMM alignment models are
initialized with jointly trained IBM Model 1 pa-
rameters (5 iterations) and trained independently
for 5 iterations using the Berkeley Aligner. We
find that though initialization with jointly trained
IBM Model 1 parameters is effective, joint train-
ing of HMM alignment models harms the perfor-
mance on this dataset (results not shown).
5.3 Result
We use posterior thresholding for the HMT and
HMM alignment models, and the grow-diag-final-
and heuristic for the IBM Model 4.
Table 2 and Figure 2 show the result. As
the Soft-Union criterion performed best, we don?t
show the results based on other criteria. On the
other hand, as the peak performance of the HMT
model is better with competitive thresholding and
those of HMM models are better without it, we
compare Precision/Recall curves and AER curves
both between the same strategy and the best per-
forming strategy for each model.
As shown in Table 2, the peak performance of
the HMT alignment model is better than that of
the IBM Model 4 by over 3 point AER, and it was
somewhere between the HMM and the S-HMM.
Taking into account that our distortion model is
simpler than that of S-HMM, these results seem
natural, and it would be reasonable to expect that
replacing our distortion model with more sophisti-
cated one might improve the performance.
When we look at Precision/Recall curves and
AER curves in Figures 2a and 2d, the HMT model
is performing slightly better in the range of 50 to
60 % precision and 0.15 to 0.35 posterior thresh-
old with the Soft-Union strategy. Results in Fig-
ures 2b and 2e show that the HMT model performs
better around the range around 60 to 70 precision
and it corresponds to 0.2 to 0.4 posterior thresh-
old with the competitive thresholding heuristic. In
addition, results on both strategies show that per-
formance curve of the HMT model is more peaked
than those of HMM alignment models.
We suspect that a part of the reason behind such
behavior can be attributed to the fact that the HMT
model?s distortion model is more uniform than that
of HMM models. For example, in our model, all
sibling nodes have the same distortion probability
from their parent node. This is in contrast with the
situation in HMM models, where nodes within a
fixed distance have different distortion probabili-
ties. With more uniform distortion probabilities,
many links for a target word may have a consider-
able amount of posterior probability. If that is true,
too many links will be above the threshold when it
is set low, and too few links can exceed the thresh-
old when it is set high. More sophisticated distor-
tion model may help mitigate such sensitivity to
the posterior threshold.
6 Related Works
Lopez and Resnik (2005) consider an HMM
model with distortions based on the distance in
dependency trees, which is quite similar to our
model?s distance. DeNero and Klein (2007) pro-
pose another HMM model with syntax-based dis-
tortions based on the path through constituency
trees, which improves translation rule extraction
for tree-to-string transducers. Both models as-
508
(a) Precision/Recall Curve with Soft-Union. (b) Precision/Recall Curve with Soft-Union + Competi-
tive Thresholding.
(c) Precision/Recall Curve with the Best Strategy. (d) Alignment Error Rate with Soft-Union.
(e) Precision/Recall Curve with Soft-Union + Competi-
tive Thresholding.
(f) Alignment Error Rate with with the Best Strategy.
Figure 2: Precision/Recall Curve and Alignment Error Rate with Different Models and Strategies.
509
sume a chain structure for hidden variables (align-
ment) as opposed to a tree structure as in our
model, and condition distortions on the syntactic
structure only in one direction.
Nakazawa and Kurohashi (2011) propose
a dependency-based phrase-to-phrase alignment
model with a sophisticated generative story, which
leads to an increase in computational complexity
and requires parallel sampling for training.
Several supervised, discriminative models use
syntax structures to generate features and to guide
the search (Burkett et al, 2010; Riesa and Marcu,
2010; Riesa et al, 2011). Such efforts are orthog-
onal to ours in the sense that discriminative align-
ment models generally use statistics obtained by
unsupervised, generative models as features and
can benefit from their improvement. It would be
interesting to incorporate statistics of the HMT
word alignment model into such discriminative
models.
Z?abokrtsky` and Popel (2009) use HMT mod-
els for the transfer phase in a tree-based MT sys-
tem. While our model assumes that the tree struc-
ture of alignment variables is isomorphic to tar-
get side?s dependency tree, they assume that the
deep-syntactic tree of the target side is isomorphic
to that of the source side. The parameters of the
HMT model is given and not learned by the model
itself.
7 Conclusion
We have proposed a novel word alignment model
based on the Hidden Markov Tree (HMT) model,
which can incorporate the syntactic structures of
both sides of the language into unsupervised word
alignment in a tractable manner. Experiments on
an English-Japanese dataset show that our model
performs better than the IBM Model 4 and com-
parably to the HMM alignment models in terms
of alignment error rates. It is also shown that the
HMT model with a simple tree-based distortion
is sensitive to posterior thresholds, perhaps due to
the flat distortion probabilities.
As the next step, we plan to improve the dis-
tortion component of our HMT alignment model.
Something similar to the syntax-sensitive distor-
tion model of DeNero and Klein (2007) might be
a good candidate.
It is also important to see the effect of our
model on the downstream translation. Apply-
ing our model to recently proposed models that
directly incorporate dependency structures, such
as string-to-dependency (Shen et al, 2008) and
dependency-to-string (Xie et al, 2011) models,
would be especially interesting.
Last but not least, though the dependency struc-
tures don?t pose a hard restriction on the align-
ment in our model, it is highly likely that parse
errors have negative effects on the alignment ac-
curacy. One way to estimate the effect of parse
errors on the accuracy is to parse the input sen-
tences with inferior models, for example trained
on a limited amount of training data. Moreover,
preserving some ambiguities using k-best trees or
shared forests might help mitigate the effect of 1-
best parse errors.
Acknowledgments
We thank anonymous reviewers for insightful sug-
gestions and comments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational linguistics,
19(2):263?311.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint Parsing and Alignment with Weakly Synchro-
nized Grammars. In Proceedings of NAACL HLT
2010, pages 127?135.
Hyeokho Choi and Richard G. Baraniuk. 2001. Mul-
tiscale Image Segmentation Using Wavelet-Domain
Hidden Markov Models. IEEE Transactions on Im-
age Processing, 10(9):1309?1321.
Matthew S. Crouse, Robert D. Nowak, and Richard G.
Baraniuk. 1998. Wavelet-Based Statistical Signal
Processing Using Hidden Markov Models. IEEE
Transactions on Signal Processing, 46(4):886?902.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC?06, pages 449?454.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In
Proceedings of ACL 2007, pages 17?24.
Jean-Baptiste Durand, Paulo Gonc?alve`s, and Yann
Gue?don. 2004. Computational Methods for Hid-
den Markov Tree Models-An Application to Wavelet
Trees. IEEE Transactions on Signal Processing,
52(9):2551?2560.
510
J.-B. Durand, Y. Gue?don, Y. Caraglio, and E. Costes.
2005. Analysis of the plant architecture via tree-
structured statistical models: the hidden Markov tree
models. New Phytologist, 166(3):813?825.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In Proceedings of
HLT-NAACL 2006, Short Papers, pages 57?60.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, pages 177?180.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of CoNLL-2002, pages 63?69.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of HLT-NAACL
2006, pages 104?111.
Adam Lopez and Philip Resnik. 2005. Im-
proved HMM Alignment Models for Languages
with Scarce Resources. In Proceedings of the ACL
Workshop on Building and Using Parallel Texts,
pages 83?86.
Toshiaki Nakazawa and Sadao Kurohashi. 2011.
Bayesian Subtree Alignment Model based on De-
pendency Trees. In Proceedings of IJCNLP 2011,
pages 794?802.
Graham Neubig. 2011. The Kyoto Free Translation
Task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational linguistics, 29(1):19?51.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING/ACL 2006, pages 433?440.
Javier Portilla, Vasily Strela, Martin J. Wainwright,
and Eero P. Simoncelli. 2003. Image Denoising
Using Scale Mixtures of Gaussians in the Wavelet
Domain. IEEE Transactions on Image Processing,
12(11):1338?1351.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
Search for Word Alignment. In Proceedings of ACL
2010, pages 157?166.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-Rich Language-Independent Syntax-Based
Alignment for Statistical Machine Translation. In
Proceedings of EMNLP 2011, pages 497?507.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-Based Word Alignment in Statisti-
cal Translation. In Proceedings of COLING 1996,
pages 836?841.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A
Novel Dependency-to-String Model for Statistical
Machine Translation. In Proceedings of EMNLP
2011, pages 216?226.
Zdene?k Z?abokrtsky` and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proceedings of ACL-IJCNLP 2009,
Short Papers, pages 145?148.
511
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212?221,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Topic Models + Word Alignment = A Flexible Framework for Extracting
Bilingual Dictionary from Comparable Corpus
Xiaodong Liu, Kevin Duh and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{xiaodong-l,kevinduh,matsu}@is.naist.jp
Abstract
We propose a flexible and effective frame-
work for extracting a bilingual dictionary
from comparable corpora. Our approach
is based on a novel combination of topic
modeling and word alignment techniques.
Intuitively, our approach works by con-
verting a comparable document-aligned
corpus into a parallel topic-aligned cor-
pus, then learning word alignments us-
ing co-occurrence statistics. This topic-
aligned corpus is similar in structure to the
sentence-aligned corpus frequently used in
statistical machine translation, enabling us
to exploit advances in word alignment re-
search. Unlike many previous work, our
framework does not require any language-
specific knowledge for initialization. Fur-
thermore, our framework attempts to han-
dle polysemy by allowing multiple trans-
lation probability models for each word.
On a large-scale Wikipedia corpus, we
demonstrate that our framework reliably
extracts high-precision translation pairs on
a wide variety of comparable data condi-
tions.
1 Introduction
A machine-readable bilingual dictionary plays a
very important role in many natural language pro-
cessing tasks. In machine translation (MT), dic-
tionaries can help in the domain adaptation set-
ting (Daume III and Jagarlamudi, 2011). In
cross-lingual information retrieval (CLIR), dictio-
naries serve as efficient means for query trans-
lation (Resnik et al, 2011). Many other multi-
lingual applications also rely on bilingual dictio-
naries as integral components.
One approach for building a bilingual dictio-
nary resource uses parallel sentence-aligned cor-
pora. This is often done in the context of Statis-
tical MT, using word alignment algorithms such
as the IBM models (Brown et al, 1993; Och and
Ney, 2003). Unfortunately, parallel corpora may
be scarce for certain language-pairs or domains of
interest (e.g., medical and microblog).
Thus, the use of comparable corpora for bilin-
gual dictionary extraction has become an active
research topic (Haghighi et al, 2008; Vulic? et
al., 2011). Here, a comparable corpus is defined
as collections of document pairs written in dif-
ferent languages but talking about the same topic
(Koehn, 2010), such as interconnected Wikipedia
articles. The challenge with bilingual dictionary
extraction from comparable corpus is that exist-
ing word alignment methods developed for paral-
lel corpus cannot be directly applied.
We believe there are several desiderata for bilin-
gual dictionary extraction algorithms:
1. Low Resource Requirement: The approach
should not rely on language-specific knowl-
edge or a large scale seed lexicon.
2. Polysemy Handling: One should handle the
fact that a word form may have multiple
meanings, and such meanings may be trans-
lated differently.
3. Scalability: The approach should run effi-
ciently an massively large-scale datasets.
Our framework addresses the above desired
points by exploiting a novel combination of topic
models and word alignment, as shown in Figure 1.
Intuitively, our approach works by first converting
a comparable document-aligned corpus into a par-
212
Figure 1: Proposed Framework
allel topic-aligned corpus, then apply word align-
ment methods to model co-occurence within top-
ics. By employing topic models, we avoid the
need for seed lexicon and operate purely in the
realm of unsupervised learning. By using word
alignment on topic model results, we can easily
model polysemy and extract topic-dependent lexi-
cons.
Specifically, let we be an English word and
wf be a French word. One can think of tradi-
tional bilingual dictionary extraction as obtaining
(we, wf ) pairs in which the probability p(we|wf )
or p(wf |we) is high. Our approach differs
by modeling p(we|wf , t) or p(wf |we, t) instead,
where t is a topic. The key intuition is that it is
easier to tease out the translation of a polysemous
word e given p(wf |we, t) rather than p(wf |we).
A word may be polysemous, but given a topic,
there is likely a one-to-one correspondence for the
most appropriate translation. For example, un-
der the simple model p(wf |we), the English word
?free? may be translated into the Japanese word
?? (as in free speech) or ?? (as in free
beer) with equal 0.5 probability; this low proba-
bility may cause both translation pairs to be re-
jected by the dictionary extraction algorithm. On
the other hand, given p(wf |we, t), where t is ?pol-
itics? or ?shopping?, we can allow high probabili-
ties for both words depending on context.
Our contribution is summarized as follows:
? We propose a bilingual dictionary extrac-
tion framework that simultaneously achieves
all three of the desiderata: low resource re-
quirement, polysemy handling, and scalabil-
ity. We are not aware of any previous works
that address all three.
? Our framework is extremely flexible and
simple-to-implement, consisting of a novel
combination of existing topic modeling tools
from machine learning and word alignment
tools from machine translation.
2 Related Work
There is a plethora of research on bilingual lexi-
con extraction from comparable corpora, starting
with seminal works of (Rapp, 1995; Fung and Lo,
1998). The main idea is to assume that translation
pairs have similar contexts, i.e. the distributional
hypothesis, so extraction consists of 3 steps: (1)
identify context windows around words, (2) trans-
late context words using a seed bilingual dictio-
nary, and (3) extract pairs that have high result-
ing similarity. Methods differ in how the seed
dictionary is acquired (Koehn and Knight, 2002;
De?jean et al, 2002) and how similarity is defined
(Fung and Cheung, 2004; Tamura et al, 2012).
Projection-based approaches have also been pro-
posed, though they can be shown to be related
to the aforementioned distributional approaches
(Gaussier et al, 2004); for example, Haghighi
(2008) uses CCA to map vectors in different lan-
guages into the same latent space. Laroche (2010)
presents a good summary.
Vulic? et al (2011) pioneered a new approach
to bilingual dictionary extraction based on topic
modeling approach which requires no seed dictio-
nary. While our approach is motivated by (Vulic?
et al, 2011), we exploit the topic model in a very
different way (explained in Section 4.2). They do
not use word alignments like we do and thus can-
not model polysemy. Further, their approach re-
quires training topic models with a large number
of topics, which may limit the scalability of the
approach.
Recently, there has been much interest in mul-
tilingual topic models (MLTM) (Jagarlamudi and
Daume, 2010; Mimno et al, 2009; Ni et al, 2009;
Boyd-Graber and Blei, 2009). Many of these mod-
els give p(t|e) and p(t|f), but stop short of extract-
ing a bilingual lexicon. Although topic models can
group related e and f in the same topic cluster, the
extraction of a high-precision dictionary requires
additional effort. One of our contributions here is
an effective way to do this extraction using word
alignment methods.
3 System Components: Background
This section reviews MLTMs and Word Align-
ment, the main components of our framework.
The knowledgeable readers may wish to skim this
section for notation and move to Section 4, which
describes our contribution.
3.1 Multilingual Topic Model
Any multilingual topic model may be used with
our framework. We use the one by Mimno et
213
al. (2009), which extends the monolingual La-
tent Dirichlet Allocation model (Blei et al, 2003).
Given a comparable corpus E in English and F
in a foreign language, we assume that the docu-
ment pair boundaries are known. For each doc-
ument pair di = [dei , dfi ] consisting of English
document dei and Foreign document dfi (where
i ? {1, . . . , D}, D is number of document pairs),
we know that dei and dfi talk about the same
topics. While the monolingual topic model lets
each document have its own so-called document-
specific distribution over topics, the multilingual
topic model assumes that documents in each tu-
ple share the same topic prior (thus the compara-
ble corpora assumption) and each topic consists of
several language-specific word distributions. The
generative story is shown in Algorithm 1.
for each topic k do
for l ? {e, f} do
sample ?lk ? Dirichlet(?l);end
end
for each document pair di do
sample ?i ? Dirichlet(?);
for l ? {e, f} do
sample zl ?Multinomial(?i);
for each word wl in dli do
sample wl ? p(wl|zl, ?l);
end
end
end
Algorithm 1: Generative story for (Mimno et al,
2009). ?i is the topic proportion of document
pair di. Words wl are drawn from language-
specific distributions p(wl|zl, ?l), where lan-
guage l indexes English e or Foreign f . Here
pairs of language-specific topics ?l are drawn
from Dirichlet distributions with prior ?l.
3.2 Statistical Word Alignment
For a sentence-pair (e,f), let e =
[we1, we2, . . . we|e|] be the English sentence with |e|
words and f = [wf1 , wf2 , . . . wf|f |] be the foreign
sentence with |f | words. For notation, we will
index English words by i and foreign words
by j. The goal of word alignment is to find an
alignment function a : i ? j mapping words in e
to words in f (and vice versa).
We will be using IBM Model 1 (Brown et al,
1993; Och and Ney, 2003), which proposes the
following probabilistic model for alignment:
p(e, a, |f) ?
|e|?
i=1
p(wei |wfa(i)) (1)
Here, p(wei |wfa(i)) captures the translation prob-ability of the English word at position i from the
foreign word at position j = a(i), where the ac-
tual alignment a is a hidden variable, and training
can be done via EM. Although this model does not
incorporate much linguistic knowledge, it enables
us to find correspondence between distinct objects
from paired sets. In machine translation, the dis-
tinct objects are words from different languages
while the paired sets are sentence-aligned corpora.
In our case, our distinct objects are also words
from distinct languages but our pair sets will be
topic-aligned corpora.
4 Proposed Framework for Bilingual
Dictionary Extraction
The general idea of our proposed framework is
sketched in Figure 1: First, we run a multilin-
gual topic model to convert the comparable cor-
pora to topic-aligned corpora. Second, we run
a word alignment algorithm on the topic-aligned
corpora in order to extract translation pairs. The
innovation is in how this topic-aligned corpora is
defined and constructed, the link between the two
stages. We describe how this is done in Section 4.1
and show how existing approaches are subsumed
in our general framework in Section 4.2.
4.1 Topic-Aligned Corpora
Suppose the original comparable corpus has D
document pairs [dei , dfi ]i=1,...,D. We run a mul-
tilingual topic model with K topics, where K
is user-defined (Section 3.1). The topic-aligned
corpora is defined hierarchically as a set of sets:
On the first level, we have a set of K topics,
{t1, . . . , tk, . . . , tK}. On the second level, for
each topic tk, we have a set of D ?word col-
lections? {Ck,1, . . . , Ck,i, . . . , Ck,D}. Each word
collection Ck,i represents the English and foreign
words that occur simultaneously in topic tk and
document di.
For clarity, let us describe the topic-aligned
corpora construction process step-by-step together
with a flow chart in Figure 2:
1. Train a multilingual topic model.
214
Figure 2: Construction of topic-aligned corpora.
2. Infer a topic assignment for each token in the
comparable corpora, and generate a list of word
collections Ck,i occurring under a given topic.
3. Re-arrange the word collections such that Ck,i
belonging to the same topic are grouped together.
This resulting set of sets is called topic-aligned
corpora, since it represents word collections linked
by the same topics.
4. For each topic tk, we run IBM Model 1 on
{Ck,1, . . . , Ck,i, . . . , Ck,D}. In analogy to statis-
tical machine translation, we can think of this
dataset as a parallel corpus of D ?sentence pairs?,
where each ?sentence pair? contains the English
and foreign word tokens that co-occur under the
same topic and the same document. Note that
word alignment is run independently for each
topic, resulting in K topic-dependent lexicons
p(we|wf , tk).
5. To extract a bilingual dictionary, we find pairs
(we, wf ) with high probability under the model:
p(we|wf ) =
?
k
p(we|wf , tk)p(tk|wf ) (2)
The first term is the topic-dependent bilingual lex-
icon from Step 4; the second term is the topic pos-
terior from the topic model in Step 1.
In practice, we will compute the probabilities
of Equation 2 in both directions: p(we|, wf ) as in
Eq. 2 and p(wf |we) =?k p(wf |we, tk)p(tk|we).
The bilingual dictionary can then be extracted
based on a probabilities threshold or some bidirec-
tional constraint. We choose to use a bidirectional
constraint because it gives very high-precision
dictionaries and avoid the need to tune probability
thresholds. A pair (e?, f?) is extracted if the
following holds:
e? = argmax
e
p(e|f = f?); f? = argmax
f
p(f |e = e?)
(3)
To summarize, the main innovation of our ap-
proach is that we allow for polysemy as topic-
dependent translation explicitly in Equation 2, and
use a novel combination of topic modeling and
word alignment techniques to compute the term
p(we|wf , tk) in an unsupervised fashion.
4.2 Alternative Approaches
To the best of our knowledge, (Vulic? et al, 2011)
is the only work focuses on using topic models
for bilingual lexicon extraction like ours, but they
exploit the topic model results in a different way.
Their ?Cue Method? computes:
p(we|wf ) =
?
k
p(we|tk)p(tk|wf ) (4)
This can be seen as a simplification of
our Eq. 2, where Eq. 4 replaces p(we|tk, wf )
with the simpler p(we|tk). Another vari-
ant is the so-called Kullback-Liebler (KL)
method, which scores translation pairs by
??k p(tk|we) log p(tk|we)/p(tk|wf ). In either
case, their contribution is the use of topic-word
distributions like p(tk|wf ) or p(wf |tk) to compute
translation probabilities.1 Our formulation can be
considered more general because we do not have
the strong assumption that we is independent of
1A third variant uses TF-IDF weighting, but is conceptu-
ally similar and have similar results.
215
wf given tk, and focus on estimating p(we|wf , tk)
directly with word alignment methods.
5 Experimental Setup
5.1 Data Set
We perform experiments on the Kyoto Wiki Cor-
pus2. We chose this corpus because it is a parallel
corpus, where the Japanese edition of Wikipedia
is translated manually into English sentence-by-
sentence. This enables us to use standard word
alignment methods to create a gold-standard lexi-
con for large-scale automatic evaluation.3
From this parallel data, we prepared several
datasets at successively lower levels of compara-
bility. As shown in Table 1, Comp100% is a com-
parable version of original parallel data, deleting
all the sentence alignments but otherwise keeping
all content on both Japanese and English sides.
Comp50% and Comp20% are harder datasets
that keep only 50% and 20% (respectively) of ran-
dom English sentences per documents. We further
use a real comparable corpus (Wiki)4, which is
prepared by crawling the online English editions
of the corresponding Japanese articles in the Ky-
oto Wiki Corpus. The Comp datasets are con-
trolled scenarios where all English content is guar-
anteed to have Japanese translations; no such guar-
antee exists in our Wiki data.
5.2 Experimental Results
1. How does the proposed framework compare
to previous work?
We focus on comparing with previous topic-
modeling approaches to bilingual lexicon extrac-
tion, namely (Vulic? et al, 2011). The methods are:
? Proposed: The proposed method which
exploits a combination of topic modeling
and word alignment to incorporate topic-
dependent translation probabilities (Eq. 2).
? Cue: From (Vulic? et al, 2011), i.e. Eq. 4.
2http://alaginrc.nict.go.jp/WikiCorpus/index E.html
3We trained IBM Model 4 using GIZA++ for both direc-
tions p(e|f) and p(f |e). Then, we extract word pair (e?, f?) as
a ?gold standard? bilingual lexicon if it satisfies Eq. 3. Due
to the large data size and the strict bidirectional requirement
imposed by Eq. 3, these ?gold standard? bilingual dictionary
items are of high quality (94% precision by a manual check
on 100 random items). Note sentence alignments are used
only for creating this gold-standard.
4The English corresponding dataset, gold-standard and
ML-LDA software used in our experiments are available at
https://sites.google.com/site/buptxiaodong/home/resource
Dataset #doc #sent(e/j) #voc(e/j)
Comp100% 14k 472k/472k 152k/116k
Comp50% 14k 236k/472k 100k/116k
Comp20% 14k 94k/472k 62k/116k
Wiki 3.6k 127k/163k 88k/61k
Table 1: Datasets: the number of document
pairs (#doc), sentences (#sent) and vocabulary
size (#voc) in English (e) and Japanese (j). For
pre-processing, we did word segmentation on
Japanese using Kytea (Neubig et al, 2011) and
Porter stemming on English. A TF-IDF based
stop-word lists of 1200 in each language is ap-
plied. #doc is smaller for Wiki because not all
Japanese articles in Comp100% have English ver-
sions in Wikipedia during the crawl.
? JS: From (Vulic? et al, 2011). Symmetrizing
KL by Jensen-Shannon (JS) divergence im-
proves results, so we report this variant.5
We also have a baseline that uses no topic models:
IBM-1 runs IBM Model 1 directly on the compa-
rable dataset, assuming each document pair is a
?sentence pair?.
Figure 3 shows the ROC (Receiver Operat-
ing Characteristic) Curve on the Wiki dataset.
The ROC curve lets us observe the change in
Recall as we gradually accept more translation
pairs as dictionary candidates. In particular,
it measures the true positive rate (i.e. recall =
|{Gold(e, f)}?{Extracted(e, f)}|/#Gold)
and false positive rate (fraction of false extractions
over total number of extractions) at varying levels
of thresholds. This is generated by first computing
p(e|f) + p(f |e) as the score for pair (e, f) for
each method, then sorting the pairs by this score
and successive try different thresholds.
The curve of the Proposed method dominates
those of all other methods. It is also the best
in Area-Under-Curve scores (Davis and Goadrich,
2006), which are 0.96, 0.90, 0.85 and 0.71, for
Proposed, IBM-1, Cue, and JS, respectively.6
ROC is insightful if we are interested in com-
paring methods for all possible thresholds, but in
practice we may desire a fixed operating point.
Thus we apply the bidirectional heuristic of Eq.
5Topic model hyperparameters for Proposed, Cue, and
JS are ? = 50/K and ? = 0.1 following (Vulic? et al, 2011).
6The Precision-Recall curve gives a similar conclusion.
We do not show it here since the extremely low precision of
JS makes the graph hard to visualize. Instead see Table 2.
216
Figure 3: ROC curve on the Wiki dataset. Curves
on upper-left is better. Cue, JS, Proposed all use
K=400 topics. Note that Proposed is best.
K Method Prec ManP #Extracted
100
Cue 0.027 0.02 3800
JS 0.013 0.01 3800
Proposed 0.412 0.36 3800
400
Cue 0.059 0.02 2310
JS 0.075 0.02 2310
Proposed 0.631 0.56 2310
- IBM-1 0.514 0.42 2310
- IBM-1* 0.493 0.39 3714
Table 2: Precision on the Wiki dataset.
K=number of topics. Precision (Prec) is defined
as |{Gold(e,f)}
?{Extracted(e,f)}|
#Extracted . ManP is preci-sion evaluated manually on 100 random items.
3 to extract a fixed set of lexicon for Proposed.
For the other methods, we calibrated the thresh-
olds to get the same number of extractions. Then
we compare the precision, as shown in Table 2.
1. Proposed outperforms other methods,
achieving 63% (automatic) precision and
56% (manual) precision.
2. The JS and Cue methods suffer from ex-
tremely poor precision. We found that this
is due to insufficient number of topics, and
is consistent with the results by (Vulic? et al,
2011) which showed best results with K >
2000. However, we could not train JS/Cue
on such a large number of topics since it is
computationally-demanding for a corpus as
large as ours.7 In this regard, the Proposed
7The experiments in (Vulic? et al, 2011) has vocabulary
Figure 4: Robustness of method under different
data conditions.
method is much more scalable, achieving
good results with low K, satisfying one of
original desiderata.8
3. IBM-1 is doing surprisingly well, consider-
ing that it simply treats document pairs as
sentence pairs. This may be due to some
extent to the structure of the Kyoto Wiki
dataset, which contains specialized topics
(about Kyoto history, architecture, etc.), lead-
ing to a vocabulary-document co-occurrence
matrix with sparse block-diagonal structure.
Thus there may be enough statistics train
IBM-1 on documents.
2. How does the proposed method perform un-
der different degrees of ?comparability??
We next examined how our methods perform un-
der different data conditions. Figure 4 plots the re-
sults in terms of Precision evaluated automatically.
We observe that Proposed (K=400) is relatively
stable, with a decrease of 14% Precision going
from fully-comparable to real Wikipedia compa-
rable corpora. The degradation for K=100 is much
larger (31%) and therefore not recommended. We
believe that robustness depends on K, because the
size of 10k, compared to 150k in our experiments. We have
attempted large K ? 1000 but Cue did not finish after days.
8We have a hypothesis as to why Cue and JS depend on
largeK. Eq. 2 is a valid expression for p(we|wf ) that makes
little assumptions. We can view Eq. 4 as simplifying the first
term of Eq. 2 from p(we|tk, wf ) to p(we|tk). Both prob-
ability tables have the same output-space (we), so the same
number of parameters is needed in reality to describe this dis-
tribution. By throwing outwf , which has large cardinality, tk
needs to grow in cardinality to compensate for the loss of ex-
pressiveness.
217
5 10 15 20
010
000
30000
010
000
30000
Word 
Count
Topic Count
en
jp
Figure 5: Power-law distribution of number of
word types with X number of topics.
topic model of (Mimno et al, 2009) assumes one
topic distribution per document pair. For low-
levels of comparability, a small number of topics
may not sufficiently model the differences in top-
ical content. This suggests the use of hierarchical
topic models (Haffari and Teh, 2009) or other vari-
ants in future work.
3. What are the statistical characteristics of
topic-aligned corpora?
First, we show the word-topic distribution from
multilingual topic modeling in the K = 400 sce-
nario (first step of Proposed, Cue, and JS). For
each word type w, we count the number of topics
it may appear in, i.e. nonzero probabilities accord-
ing to p(w|t). Fig. 5 shows the number of word
types that have x number of topics. This power-
law is expected since we are modeling all words.9
Next we compute the statistics after construct-
ing the topic-aligned corpora (Step 3 of Fig. 2).
For each part of the topic-aligned corpora, we
compute the ratio of distinct English word types
vs. distinct Japanese word types. If the ratio is
close to one, that means the partition into topic-
aligned corpora effectively separates the skewed
word-topic distribution of Fig 5. We found that
the mean ratio averaged across topics is low at
1.721 (variance is 1.316), implying that within
each topic, word alignment is relatively easy.
4. What kinds of errors are made?
We found that the proposed method makes sev-
eral types of incorrect lexicon extractions. First,
Word Segmentation ?errors? on Japanese could
9This means that it is not possible to directly extract lexi-
con by taking the cross-product (wf , we) of the top-n words
in p(wf |tk) and p(we|tk) for the same topic tk, as suggested
by (Mimno et al, 2009). When we attempted to do this, us-
ing top-2 words per p(wf |tk) and p(we|tk), we could only
obtain precision of 0.37 for 1600 extractions. This skewed
distribution similarly explains the poor performance of Cue.
make it impossible to find a proper English trans-
lation (e.g., ???? should translate to ?Prince-
Takechi? but system proposes ?Takechi?). Sec-
ond, an unrelated word pair (we, wf ) may be in-
correctly placed in the same topic, leading to an
Incorrect Topic error. Third, even if (we, wf ) in-
tuitively belong to the same topic, they may not be
direct translations; an extraction in this case would
be a Correct Topic, Incorrect Alignment error
(e.g. ??????, a particular panfried snack,
is incorrectly translated as ?panfry?).
Table 3 shows the distribution of error types by
a manual classification. Incorrect Alignment er-
rors are most frequent, implying the topic models
are doing a reasonable job of generating the topic-
aligned corpus. The amount of Incorrect Topic is
not trivial, though, so we would still imagine more
advanced topic models to help. Segmentation er-
rors are in general hard to solve, even with a better
word segmenter, since in general one-to-one cross-
lingual word correspondence is not consistent?we
believe the solution is a system that naturally han-
dles multi-word expressions (Baldwin, 2011).
Word Segmentation Error 14
Incorrect Topic 29
Correct Topic, Incorrect Alignment 40
Reason Unknown 7
Table 3: Counts of various error types.
5. What is the computation cost?
Timing results on a 2.4GHz Opteron CPU for var-
ious steps of Proposed and Cue are shown in Ta-
ble 5. The proposed method is 5-8 times faster
than Cue. For Proposed, computation time is
dominated by topic modeling while GIZA++ on
topic-aligned corpora is extremely fast. Cue addi-
tionally suffers from computational complexity in
calculating Eq.4, especially when both p(we|tk)
and p(tk|wf ) have high cardinality. In compari-
son, calculating Eq.2 is fast since p(we|wf , tk) is
in practice quite sparse.
6. What topic-dependent lexicons are learned
and do they capture polysemy?
In our evaluation so far, we have only produced an
one-to-one bilingual dictionary (due to the bidirec-
tionality constraint of Eq.3). We have seen how
topic-dependent translation models p(wf |we, tk)
is important in achieving good results. However,
Eq.2 marginalizes over the topics so we do not
know what topic-dependent lexicons are learned.
218
English Japanese1(gloss), Japanese2(gloss)
interest ?? (a sense of concern),?? (a charge of money borrowing)
count ??(act of reciting numbers),?? (nobleman)
free ??(as in ?free? speech),?? (as in ?free? beer)
blood ??(line of descent),? (the red fluid)
demand ??(as noun),??(as verb)
draft ??(as verb),?? (as noun)
page ??? (one leaf of e.g. a book),?? (youthful attendant)
staff ????(general personel),?? (as in political ?chief of staff?)
director ?? (someone who controls),?? (board of directors)?? (movie director)
beach ?(area of sand near water),???(leisure spot at beach)
actor ?? (theatrical performer),?? (movie actor)
Table 4: Examples of topic-dependent translations given by p(wf |we, tk). The top portion shows ex-
amples of polysemous English words. The bottom shows examples where English is not decisively
polysemous, but indeed has distinct translations in Japanese based on topic.
K topic giza Eq.2 Eq.4 Prp Cue
100 180 3 20 1440 203 1620
200 300 3 33 2310 336 2610
400 780 5 42 3320 827 4100
Table 5: Wall-clock times in minutes for Topic
Modeling (topic), Word Alignment (giza), and
p(we|wf ) calculation. Overall time for Pro-
posed (Prp) is topic+giza+Eq.2 and for Cue is
topic+Eq.4.
Here, we explore the model p(wf |we, tk) learned
at Step 4 of Figure 2 to see whether it captures
some of the polysemy phenomenon mentioned in
the desiderata. It is not feasible to automatically
evaluate topic-dependent dictionaries, since this
requires ?gold standard? of the form (e, f, t). Thus
we cannot claim whether our method successfully
extracts polysemous translations. Instead we will
present some interesting examples found by our
method. In Table 4, we look at potentially pol-
ysemous English words we, and list the highest-
probability Japanese translations wf conditioned
on different tk. We found many promising cases
where the topic identification helps divide the dif-
ferent senses of the English word, leading to the
correct Japanese translation achieving the highest
probability.
6 Conclusion
We proposed an effective way to extract bilin-
gual dictionaries by a novel combination of topic
modeling and word alignment techniques. The
key innovation is the conversion of a compara-
ble document-aligned corpus into a parallel topic-
aligned corpus, which allows word alignment
techniques to learn topic-dependent translation
models of the form p(we|wf , tk). While this kind
of topic-dependent translation has been proposed
for the parallel corpus (Zhao and Xing, 2007),
we are the first to enable it for comparable cor-
pora. Our large-scale experiments demonstrated
that the proposed framework outperforms existing
baselines under both automatic metrics and man-
ual evaluation. We further show that our topic-
dependent translation models can capture some of
the polysemy phenomenon important in dictionary
construction. Future work includes:
1. Exploring other topic models (Haffari and Teh,
2009) and word alignment techniques (DeNero
and Macherey, 2011; Mermer and Saraclar, 2011;
Moore, 2004) in our framework.
2. Extract lexicon from massive multilingual col-
lections. Mausum (2009) and Shezaf (2010) show
that language pivots significantly improve the pre-
cision of distribution-based approaches. Since
multilingual topic models can easily be trained on
more than 3 languages, we expect it will give a big
boost to our approach.
Acknowledgments
We thank Mamoru Komachi, Shuhei Kondo and
the anonymous reviewers for valuable discussions
and comments. Part of this research was executed
under the Commissioned Research of National In-
stitute of Information and Communications Tech-
nology (NICT), Japan.
219
References
Timothy Baldwin. 2011. Mwes and topic mod-
elling: enhancing machine learning with linguis-
tics. In Proceedings of the Workshop on Multiword
Expressions: from Parsing and Generation to the
Real World, MWE ?11, pages 1?1, Stroudsburg, PA,
USA. Association for Computational Linguistics.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
Jordan Boyd-Graber and David M. Blei. 2009. Multi-
lingual topic models for unaligned text. In UAI.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2).
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by min-
ing unseen words. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
407?412, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Jesse Davis and Mark Goadrich. 2006. The relation-
ship between precision-recall and ROC curves. In
ICML.
Herve? De?jean, E?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th international conference
on Computational linguistics - Volume 1, COLING
?02, pages 1?7.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the Association for Com-
putational Linguistics (ACL).
Pascale Fung and Percy Cheung. 2004. Mining
verynon-parallel corpora: Parallel sentence and lex-
icon extraction via bootstrapping and em. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Pascale Fung and Yuen Yee Lo. 1998. Translating un-
known words using nonparallel, comparable texts.
In COLING-ACL.
Eric Gaussier, J.M. Renders, I. Matveeva, C. Goutte,
and H. Dejean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 526?533, Barcelona, Spain, July.
Ghloamreza Haffari and Yee Whye Teh. 2009. Hier-
archical dirichlet trees for information retrieval. In
NAACL.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Jagadeesh Jagarlamudi and Hal Daume. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. In ECIR.
Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
617?625, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In ACL.
Coskun Mermer and Murat Saraclar. 2011. Bayesian
word alignment for statistical machine translation.
In ACL.
David Mimno, Hanna Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
Robert Moore. 2004. Improving IBM word alignment
model 1. In ACL.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In The 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT) Short Paper Track, pages 529?533, Portland,
Oregon, USA, 6.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from wikipedia.
In WWW.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics.
220
Philip Resnik, Douglas Oard, and Gina Levow. 2011.
Improved cross-language retrieval using backoff
translation. In Proceedings of the First International
Conference on Human Language Technology.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual
lexicon generation using non-aligned signatures. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 98?107. Association for Computational Lin-
guistics.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 24?36,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compa-
rable corpora using latent topic models. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 479?484, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM:
Bilingual Topic Exploration, Word Alignment, and
Translation. In NIPS.
221
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109?113,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
Identifying collocations using cross-lingual association measures
Lis Pereira
1
, Elga Strafella
2
, Kevin Duh
1
and Yuji Matsumoto
1
1
Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{lis-k, kevinduh, matsu}@is.naist.jp
2
National Institute for Japanese Language and Linguistics, 10-2 Midoricho, Tachikawa, Tokyo 190-8561, Japan
strafelga@gmail.com
Abstract
We introduce a simple and effective cross-
lingual approach to identifying colloca-
tions. This approach is based on the obser-
vation that true collocations, which cannot
be translated word for word, will exhibit
very different association scores before
and after literal translation. Our exper-
iments in Japanese demonstrate that our
cross-lingual association measure can suc-
cessfully exploit the combination of bilin-
gual dictionary and large monolingual cor-
pora, outperforming monolingual associa-
tion measures.
1 Introduction
Collocations are part of the wide range of linguis-
tic phenomena such as idioms (kick the bucket),
compounds (single-mind) and fixed phrases (by
and large) defined as Multiword Expressions
(MWEs). MWEs, and collocations, in particu-
lar, are very pervasive not only in English, but
in other languages as well. Although handling
MWEs properly is crucial in many natural lan-
guage processing (NLP) tasks, manually annotat-
ing them is a very costly and time consuming task.
The main goal of this work-in-progress is,
therefore, to evaluate the effectiveness of a simple
cross-lingual approach that allows us to automat-
ically identify collocations in a corpus and subse-
quently distinguish them according to one of their
intrinsic properties: the meaning of the expression
cannot be predicted from the meaning of the parts,
i.e. they are characterized by limited composition-
ality (Manning and Sch?utze, 1999). Given an ex-
pression, we predict whether the expression(s) re-
sulted from the word by word translation is also
commonly used in another language. If not, that
might be evidence that the original expression is
a collocation (or an idiom). This can be cap-
tured by the ratio of association scores, assigned
by association measures, in the target vs. source
language. The results indicate that our method
improves the precision comparing with standard
methods of MWE identification through monolin-
gual association measures.
2 Related Work
Most previous works on MWEs and, more specifi-
cally, collocation identification (Evert, 2008; Sere-
tan, 2011; Pecina, 2010; Ramisch, 2012) employ
a standard methodology consisting of two steps:
1) candidate extraction, where candidates are ex-
tracted based on n-grams or morphosyntactic pat-
terns and 2) candidate filtering, where association
measures are applied to rank the candidates based
on association scores and consequently remove
noise. One drawback of such method is that as-
sociation measures might not be able to perform a
clear-cut distinction between collocation and non-
collocations, since they only assign scores based
on statistical evidence, such as co-occurrence fre-
quency in the corpus. Our cross-lingual associa-
tion measure ameliorates this problem by exploit-
ing both corpora in two languages, one of which
may be large.
A few studies have attempted to identify non-
compositional MWE?s using parallel corpora and
dictionaries. Melamed (1997) investigates how
non-compositional compounds can be detected
from parallel corpus by identifying translation di-
vergences in the component words. Pichotta and
DeNero (2013) analyses the frequency statistics
of an expression and its component words, us-
ing many bilingual corpora to identifying phrasal
verbs in English. The disadvantage of such ap-
proach is that large-scale parallel corpora is avail-
able for only a few language pairs. On the other
hand, monolingual data is largely and freely avail-
able for many languages. Our approach requires
only a bilingual dictionary and non-parallel mono-
lingual corpora in both languages.
109
Salehi and Cook (2013) predict the degree of
compositionality using the string distance between
the automatic translation into multiple languages
of an expression and the individual translation
of its components. They use an online database
called Panlex (Baldwin et al., 2010), that can
translate words and expressions from English into
many languages. Tsvetkov and Wintner (2013) is
probably the closest work to ours. They trained
a Bayesian Network for identfying MWE?s and
one of the features used is a binary feature that
assumes value is 1 if the literal translation of the
MWE candidate occurs more than 5 times in a
large English corpus.
3 Identifying Collocations
In this research, we predict whether the expres-
sion(s) resulted from the translation of the com-
ponents of a Japanese collocation candidate is/are
also commonly used in English. For instance, if
we translate the Japanese collocation ????
? mendou-wo-miru ?to care for someone? (care-
?-see)1 into English word by word, we obtain
?see care?, which sounds awkward and may not
appear in an English corpus very often. On the
other hand, the word to word translation of the free
combination ????? eiga-wo-miru ?to see a
movie? (movie-?-see) is more prone to appear
in an English corpus, since it corresponds to the
translation of the expression as well. In our work,
we focus on noun-verb expressions in Japanese.
Our proposed method consists of three steps:
1) Candidate Extraction: We focus on noun-
verb constructions in Japanese. We work with
three construction types: object-verb, subject-verb
and dative-verb constructions, represented respec-
tively as ?noun wo verb (noun-?-verb)?, ?noun ga
verb (noun-?-verb)? and ?noun ni verb (noun-?-
verb)?, respectively. The candidates are extracted
from a Japanese corpus using a dependency parser
(Kudo and Matsumoto, 2002) and ranked by fre-
quency.
2) Translation of the component words: for
each noun-verb candidate, we automatically ob-
tain all the possible English literal translations of
the noun and the verb using a Japanese/English
dictionary. Using that information, all the possi-
ble verb-noun combinations in English are then
generated. For instance, for the candidate ??
1
In Japanese,? is a case marker that indicates the object-
verb dependency relation.
?? hon-wo-kau ?to buy a book? (buy-?-book),
we take the noun ? hon and the verb ?? kau
and check their translation given in the dictio-
nary. ? has translations like ?book?, ?main? and
?head? and ?? is translated as ?buy?. Based
on that, possible combinations are ?buy book? or
?buy main? (we filter out determiners, pronouns,
etc.).
3) Ranking of original and derived word to
word translated expression: we compare the
association score of the original expression in
Japanese (calculated using a Japanese corpus) and
its corresponding derived word to word translated
expressions. If the original expression has a much
higher score than its literal translations, it might be
a good evidence that we are dealing with a collo-
cation, instead of a free combination.
There is no defined criteria in choosing one par-
ticular association measure when applying it in
a specific task, since different measures highlight
different aspects of collocativity (Evert, 2008). A
state-of-the-art, language independent framework
that employs the standard methodology to identify
MWEs is mwetoolkit (Ramisch, 2012). It ranks
the extracted candidates using four different as-
sociation measures: log-likelihood-ratio, Dice co-
efficient, pointwise mutual information and Stu-
dent?s t-score. We previously conducted exper-
iments with these four measures for Japanese
(results are ommited), and Dice coefficient per-
formed best. Using Dice coefficient, we calcu-
late the ratio between the score of the original ex-
pression and the average score of its literal trans-
lations. Finally, the candidates are ranked by the
ratio value. Those that have a high value are ex-
pected to be collocations, while those with a low
value are expected to be free combinations.
4 Experiment Setup
4.1 Data Set
The following resources were used in our experi-
ments:
Japanese/English dictionary: we used Edict
(Breen, 1995), a freely available Japanese/English
Dictionary in machine-readable form, containing
110,424 entries. This dictionary was used to find
all the possible translations of each Japanese word
involved in the candidate (noun and verb). For our
test set, all the words were covered by the dictio-
nary. We obtained an average of 4.5 translations
per word. All the translations that contains more
110
than three words are filtered out. For the transla-
tions of the Japanese noun, we only consider the
first noun appearing in each translation. For the
translations of the Japanese verb, we only consider
the first verb/phrasal verb appearing in each trans-
lation. For instance, in the Japanese collocation?
???? koi-ni-ochiru ?to fall in love? (love-?-
fall down)
2
, the translations in the dictionary and
the ones we consider (shown in bold type) of the
noun? koi ?love? and the verb??? ochiru ?to
fall down? are:
?: love , tender passion
???: to fall down, to fail, to crash, to
degenerate, to degrade
Bilingual resource: we used Hiragana Times
corpus, a Japanese-English bilingual corpus of
magazine articles of Hiragana Times
3
, a bilingual
magazine written in Japanese and English to intro-
duce Japan to non-Japanese, covering a wide range
of topics (culture, society, history, politics, etc.).
The corpus contains articles from 2003-2102, with
a total of 117,492 sentence pairs. We used the
Japanese data to extract the noun-verb collocation
candidates using a dependency parser, Cabocha
(Kudo and Matsumoto, 2002). For our work, we
focus on the object-verb, subject-verb and dative-
verb dependency relations. The corpus was also
used to calculate the Dice score of each Japanese
candidate, using the Japanese data.
Monolingual resource: we used 75,377 En-
glish Wikipedia articles, crawled in July 2013. It
contains a total of 9.5 million sentences. The data
was used to calculate the Dice score of each can-
didate?s derived word to word translated expres-
sions. The corpus was annotated with Part-of-
Speech (POS) information, from where we de-
fined POS patterns to extract all the verb-noun
and noun-verb sequences, using the MWE toolkit
(Ramisch, 2012), which is an integrated frame-
work for MWE treatment, providing corpus pre-
processing facilities.
Table 1 shows simple statistics on the Hiragana
Times corpus and on the Wikipedia corpus.
4.2 Test set
In order to evaluate our system, the top 100 fre-
quent candidates extracted from Hiragana Times
corpus were manually annotated by 4 Japanese
native speakers. The judges were asked to make
2? is the dative case marker in Japanese.
3
http://www.hiraganatimes.com
Hiragana
Times
Wikipedia
# jp sentences 117,492 -
# en sentences 117,492 9,500,000
# jp tokens 3,949,616 -
# en tokens 2,107,613 247,355,886
# jp noun-verb 31,013 -
# en noun-verb - 266,033
# en verb-noun - 734,250
Table 1: Statistics on the Hiragana Times corpus
and Wikipedia corpus, showing the number of sen-
tences, number of words and number of noun-
verb and verb-noun expressions in English and
Japanese.
a ternary judgment for each of the candidates on
whether the candidate is a collocation, idiom or
free combination. For each category, a judge was
shown the definition and some examples. We de-
fined collocations as all those expressions where
one of the component words preserves its lit-
eral meaning, while the other element assumes a
slightly different meaning and its use is blocked
(i.e. it cannot be substituted by a synonym). Id-
ioms were defined as the semantically and syntac-
tically fixed expressions where all the component
words loose their original meaning. Free combi-
nations were defined as all those expressions fre-
quently used where the components preserve their
literal meaning. The inter-annotator agreement
is computed using Fleiss? Kappa statistic (Fleiss,
1971), since it involves more than 2 annotators.
Since our method does not differentiate colloca-
tions from idioms (although we plan to work on
that as future work), we group collocations and id-
ioms as one class. We obtained a Kappa coeffi-
cient of 0.4354, which is considered as showing
moderate agreement according to Fleiss (1971).
Only the candidates identically annotated by the
majority of judges (3 or more) were added to the
test set, resulting in a number of 87 candidates
(36 collocations and 51 free combinations). Af-
ter that, we obtained a new Kappa coefficient of
0.5427, which is also considered as showing mod-
erate agreement (Fleiss, 1971).
4.3 Baseline
We compare our proposed method with two base-
lines: an association measure based system and
a Phrase-Based Statistical Machine Translation
111
(SMT) based system.
Monolingual Association Measure: The sys-
tem ranks the candidates in the test set according
to their Dice score calculated using the Hiragana
Times Japanese data.
Phrase-Based SMT system: a standard non-
factored phrase-based SMT system was built us-
ing the open source Moses toolkit (Koehn et al.,
2007) with parameters set similar to those of Neu-
big (2011), who provides a baseline system pre-
viously applied to a Japanese-English corpus built
from Wikipedia articles. For training, we used Hi-
ragana Times bilingual corpus. The Japanese sen-
tences were word-segmented and the English sen-
tences were tokenized and lowercased. All sen-
tences with size greater than 60 tokens were previ-
ously eliminated. The whole English corpus was
used as training data for a 5-gram language model
built with the SRILM toolkit (Stolcke, 2002).
Similar to what we did for our proposed
method, for each candidate in the test set, we find
all the possible literally translated expressions (as
described in Section 3). In the phrase-table gen-
erated after the training step, we look for all the
entries that contain the original candidate string
and check if at least one of the possible literal
translations appear as their corresponding transla-
tion. For the entries found, we compute the av-
erage of the sum of the candidate?s direct and in-
verse phrase translation probability scores. The di-
rect phrase translation probability and the inverse
phrase translation probability (Koehn et al., 2003)
are respectively defined as:
?(e|f) =
count(f, e)
?
f
count(f, e)
(1)
?(f |e) =
count(f, e)
?
e
count(f, e)
(2)
Where f and e indicate a foreign phrase and a
source phrase, independently.
The candidates are ranked according to the av-
erage score as described previously.
5 Evaluation
In our evaluation, we average the precision con-
sidering all true collocations and idioms as thresh-
old points, obtaining the mean average precision
(MAP). Differently from the traditional approach
used to evaluate an association measure, using
MAP we do not need to set a hard threshold.
Table 2 presents the MAP values for our pro-
posed method and for the two baselines. Our
cross-lingual method performs best in terms of
MAP values against the two baselines. We found
out that it performs statistically better only com-
pared to the Monolingual Association Measure
baseline
4
. The Monolingual Association Measure
baseline performed worst, since free combinations
were assigned high scores as well, and the system
was not able to perform a clear separation into col-
locations and non-collocations. The Phrase-Based
SMT system obtained a higher MAP value than
Monolingual Association measure, but the score
may be optimistic since we are testing in-domain.
One concern is that there are only a very few bilin-
gual/parallel corpora for the Japanese/English lan-
guage pair, in case we want to test with a different
domain and larger test set. The fact that our pro-
posed method outperforms SMT implies that us-
ing such readily-available monolingual data (En-
glish Wikipedia) is a better way to exploit cross-
lingual information.
Method MAP value
Monolingual Association Measure 0.54
Phrase-Based SMT 0.67
Proposed Method 0.71
Table 2: Mean average precision of proposed
method and baselines.
Some cases where the system could not per-
form well include those where a collocation can
also have a literal translation. For instance,
in Japanese, there is the collocation ????
kokoro-wo-hiraku ?to open your heart? (heart-?-
open), where the literal translation of the noun ?
kokoro ?heart? and the verb ?? hiraku ?open?
correspond to the translation of the expression as
well.
Another case is when the candidate expression
has both literal and non-literal meaning. For in-
stance, the collocation ???? hito-wo-miru
(person-?-see) can mean ?to see a person?, which
is the literal meaning, but when used together with
the noun ? me ?eye?, for instance, it can also
can mean ?to judge human character?. When an-
notating the data, the judges classified as idioms
some of those expressions, for instance, because
the non-literal meaning is mostly used compared
4
Statistical significance was calculated using a two-tailed
t-test for a confidence interval of 95%.
112
with the literal meaning. However, our system
found that the literal translated expressions are
also commonly used in English, which caused the
performance decrease.
6 Conclusion and Future Work
In this report of work in progress, we propose
a method to distinguish free combinations and
collocations (and idioms) by computing the ratio
of association measures in source and target lan-
guages. We demonstrated that our method, which
can exploit existing monolingual association mea-
sures on large monolingual corpora, performed
better than techniques previously applied in MWE
identification.
In the future work, we are interested in increas-
ing the size of the corpus and test set used (for
instance, include mid to low frequent MWE?s),
as well as applying our method to other collo-
cational patterns like Noun-Adjective, Adjective-
Noun, Adverb-Verb, in order to verify our ap-
proach. We also believe that our approach can
be used for other languages as well. We intend to
conduct a further investigation on how we can dif-
ferentiate collocations from idioms. Another step
of our research will be towards the integration of
the acquired data into a web interface for language
learning and learning materials for foreign learn-
ers as well.
Acknowledgments
We would like to thank Masashi Shimbo, Xi-
aodong Liu, Mai Omura, Yu Sawai and Yoriko
Nishijima for their valuable help and anonymous
reviewers for the helpful comments and advice.
References
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all
words of all languages of the world. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Demonstrations, pages 37?
40. Association for Computational Linguistics.
Jim Breen. 1995. Building an electronic japanese-
english dictionary. In Japanese Studies Association
of Australia Conference. Citeseer.
Stefan Evert. 2008. Corpora and collocations. Corpus
Linguistics. An International Handbook, 2.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
proceedings of the 6th conference on Natural lan-
guage learning-Volume 20, pages 1?7. Association
for Computational Linguistics.
Christopher D Manning and Hinrich Sch?utze. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. EMNLP.
Graham Neubig. 2011. The kyoto free translation task.
Available on line at http://www. phontron. com/kftt.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language resources and
evaluation, 44(1-2):137?158.
Karl Pichotta and John DeNero. 2013. Identifying
phrasal verbs using many bilingual corpora. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636?
646.
Carlos Ramisch. 2012. A generic framework for mul-
tiword expressions treatment: from acquisition to
applications. In Proceedings of ACL 2012 Student
Research Workshop, pages 61?66. Association for
Computational Linguistics.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages.
Violeta Seretan. 2011. Syntax-based collocation ex-
traction, volume 44. Springer.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov and Shuly Wintner. 2013. Identifica-
tion of multi-word expressions by combining multi-
ple linguistic information sources.
113
