R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 34 ? 45, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Automatic Image Annotation Using  
Maximum Entropy Model 
Wei Li and Maosong Sun 
State Key Lab of Intelligent Technology and Systems, 
Department of Computer Science and Technology, Tsinghua University, 
Beijing 100084, China 
wei.lee04@gmail.com, sms@mail.tsinghua.edu.cn 
Abstract. Automatic image annotation is a newly developed and promising 
technique to provide semantic image retrieval via text descriptions. It concerns 
a process of automatically labeling the image contents with a pre-defined set of 
keywords which are exploited to represent the image semantics. A Maximum 
Entropy Model-based approach to the task of automatic image annotation is 
proposed in this paper. In the phase of training, a basic visual vocabulary con-
sisting of blob-tokens to describe the image content is generated at first; then 
the statistical relationship is modeled between the blob-tokens and keywords by 
a Maximum Entropy Model constructed from the training set of labeled images. 
In the phase of annotation, for an unlabeled image, the most likely associated 
keywords are predicted in terms of the blob-token set extracted from the given 
image. We carried out experiments on a medium-sized image collection with 
about 5000 images from Corel Photo CDs. The experimental results demon-
strated that the annotation performance of this method outperforms some tradi-
tional annotation methods by about 8% in mean precision, showing a potential 
of the Maximum Entropy Model in the task of automatic image annotation. 
1   Introduction 
Last decade has witnessed an explosive growth of multimedia information such as 
images and videos. However, we can?t access to or make use of the relevant informa-
tion more leisurely unless it is organized so as to provide efficient browsing and que-
rying. As a result, an important functionality of next generation multimedia informa-
tion management system will undoubtedly be the search and retrieval of images and 
videos on the basis of visual content.  
In order to fulfill this ?intelligent? multimedia search engines on the world-wide-
web, content-based image retrieval techniques have been studied intensively during 
the past few years. Through the sustained efforts, a variety of state-of-the-art methods 
employing the query-by-example (QBE) paradigm have been well established. By this 
we mean that queries are images and the targets are also images. In this manner, vis-
ual similarity is computed between user-provided image and database images based 
on the low-level visual features such as color, texture, shape and spatial relationships. 
However, two important problems still remain. First, due to the limitation of object 
recognition and image understanding, semantics-based image segmentation algorithm 
 Automatic Image Annotation Using Maximum Entropy Model 35 
is unavailable, so segmented region may not correspond to users? query object. Sec-
ond, visual similarity is not semantic similarity which means that low-level features 
are easily extracted and measured, but from the users? point of view, they are non-
intuitive. It is not easy to use them to formulate the user?s needs. We encounter a so-
called semantic gap here. Typically the starting point of the retrieval process is the 
high-level query from users. So extracting image semantics based on the low-level 
visual features is an essential step. As we know, semantic information can be repre-
sented more accurately by using keywords than by using low-level visual features. 
Therefore, building relationship between associated text and low-level image features 
is considered to an effective solution to capture the image semantics. By means of this 
hidden relationship, images can be retrieved by using textual descriptions, which is 
also called query-by-keyword (QBK) paradigm. Furthermore, textual queries are a 
desirable choice for semantic image retrieval which can resort to the powerful text-
based retrieval techniques. The key to image retrieval using textual queries is image 
annotation. But most images are not annotated and manually annotating images is a 
time-consuming, error-prone and subjective process. So, automatic image annotation 
is the subject of much ongoing research. Its main goal is to assign descriptive words 
to whole images based on the low-level perceptual features, which has been recog-
nized as a promising technique for bridging the semantic gap between low-level im-
age features and high-level semantic concepts. 
Given a training set of images labeled with text (e.g. keywords, captions) that de-
scribe the image content, many statistical models have been proposed by research-
ers to construct the relation between keywords and image features. For example, co-
occurrence model, translation model and relevance-language model. By exploiting 
text and image feature co-occurrence statistics, these methods can extract hidden 
semantics from images, and have been proven successful in constructing a nice 
framework for the domain of automatic image annotation and retrieval.  
In this paper, we propose a novel approach for the task of automatic image anno-
tation using Maximum Entropy Model. Though Maximum Entropy method has 
been successfully applied to a wide range of application such as machine transla-
tion, it is not much used in computer vision domain, especially in image auto  
annotation.  
This paper is organized as follows: Section 2 presents related work. Section 3 de-
scribes the representation of labeled and unlabeled images, gives a brief introduc-
tion to Maximum Entropy Model and then details how to use it for automatically 
annotating unlabeled images. Section 4 demonstrates our experimental results. Sec-
tion 5 presents conclusions and a comment for future work. 
2   Related Work 
Recently, many statistical models have been proposed for automatic image annotation 
and retrieval. The work of associating keywords with low-level visual features can be 
addressed from two different perspectives.  
36 W. Li and M. Sun 
2.1   Annotation by Keyword Propagation  
This kind of approach usually formulates the process of automatic image annotation 
as one of supervised classification problems. With respect to this method, accurate 
annotation information is demanded. That is to say, given a set of training images 
labeled with semantic keywords, detailed labeling information should be provided. 
For example, from training samples, we can know which keyword corresponds to 
which image region or what kind of concept class describes a whole-image. So each 
or a set of annotated keyword can be considered as an independent concept class, 
followed by training each class model with manually labeled images, then the model 
is applied to classify each unlabeled image into a relevant concept class, and finally 
producing annotation by propagating the corresponding class words to unlabeled 
images.  
Wang and Li [8] introduced a 2-D multi- resolution HMM model to automate lin-
guistic indexing of images. Clusters of fixed-size blocks at multiple resolution and the 
relationships between these clusters is summarized both across and within the resolu-
tions. To annotate the unlabeled image, words of the highest likelihood is selected 
based on the comparison between feature vectors of new image and the trained con-
cept models. Chang et al[5] proposed content-based soft annotation (CBSA) for pro-
viding images with semantic labels using (BPM) Bayesian Point Machine. Starting 
with labeling a small set of training images, an ensemble of binary classifier for each 
keyword is then trained for predicting label membership for images. Each image is 
assigned one keyword vector, with each keyword in the vector assigned a confidence 
factor. In the process of annotation, words with high confidence are considered to be 
the most likely descriptive words for the new images. The main practical problem 
with this kind of approaches is that a large labeled training corpus is needed. More-
over, during the training and application stages, the training set is fixed and not in-
cremented. Thus if a new domain is introduced, new labeled examples must be pro-
vided to ensure the effectiveness of such classifiers. 
2.2   Annotation by Statistical Inference 
More recently, there have been some efforts to solve this problem in a more general 
way. The second approach takes a different strategy which focuses on discovering the 
statistical links between visual features and words using unsupervised learning meth-
ods. During training, a roughly labeled image datasets is provided where a set of se-
mantic labels is assigned to a whole image, but the word-to-region information is 
hidden in the space of image features and keywords. So an unsupervised learning 
algorithm is usually adopted to estimate the joint probability distribution of words and 
image features.  
Mori et al[4] were the earliest to model the statistics using a co-occurrence prob-
abilistic model, which predicate the correct probability of associating keywords by 
counting the co-occurrence of words with image regions generated using a fixed-size 
blocks. Blocks are vector quantized to form clusters which inherit the whole set of  
 
 Automatic Image Annotation Using Maximum Entropy Model 37 
keywords assigned to each image. Then clusters are in turn used to predict the key-
words for unlabeled images. The disadvantage is that the model is a little simple and 
the rough fixed-size blocks are unable to model objects effectively, leading to poor 
annotation accuracy. Instead of using fixed-size blocks, Barnard et al[1] performed 
Blobworld segmentation and Normalized cuts to produce semantic meaningful re-
gions. They constructed a hierarchical model via EM algorithm. This model combines 
both asymmetric clustering model which maps words and image regions into clusters 
and symmetric clustering model which models the joint distribution of words and 
regions. Duygulu et al[2] proposed a translation model to map keywords to individ-
ual image regions. First, image regions are created by using a segmentation algorithm. 
For each region, visual features are extracted and then blob-tokens are generated by 
clustering the features for each region across whole image datasets. Each image can 
be represented by a certain number of these blob-tokens. Their Translation Model 
uses machine translation model ?of IBM to annotate a test set of images based on a 
large number of annotated training images. Another approach using cross-media rele-
vance models (CMRM) was introduced by Jeon et al[3]. They assumed that this 
could be viewed as analogous to the cross-lingual retrieval problem and a set of key-
words{ }nwww ...,,, 21  is related to the set of blob-tokens{ }nbbb ...,,, 21 , rather 
than one-to-one correspondence between the blob-tokens and keywords. Here the 
joint distribution of blob-tokens and words was learned from a training set of anno-
tated images to perform both automatic image annotation and ranked retrieval. Jeon et 
al [9] introduced using Maximum Entropy to model the fixed-size block and key-
words, which gives us a good hint to implement it differently. Lavrenko et al[11] 
extended the cross-media relevance model using actual continuous-valued features 
extracted from image regions. This method avoids the clustering and constructing the 
discrete visual vocabulary stage. 
3   The Implementation of Automatic Annotation Model 
3.1   The Hierarchical Framework of Automatic Annotation and Retrieval 
The following Fig. 1 shows the framework for automatic image annotation and key-
word-based image retrieval. Given a training dataset of images labeled with key-
words. First, we segment a whole image into a collection of sub-images, followed by 
extracting a set of low-level visual features to form a feature vector to describe the 
visual content of each region. Second, a visual vocabulary of blob-tokens is generated 
by clustering all the regions across the whole dataset so that each image can be repre-
sented by a number of blob-tokens from a finite set of visual symbols. Third, both 
textual information and visual information is provided to train the Maximum Entropy 
model, and the learned model is then applied to automatically generate keywords to 
describe the semantic content of an unlabeled image based on the low-level features. 
Consequently, both the users? information needs and the semantic content of images 
can be represented by textual information, which can resort to the powerful text IR 
techniques to implement this cross-media retrieval, suggesting the importance of 
textual information in semantics-based image retrieval. 
38 W. Li and M. Sun 
 
Fig. 1. Hierarchical Framework of Automatic Annotation and Retrieval 
                 learning correlations between blob-tokens and textual annotations 
                 applying correlations to generate annotations for unlabeled images 
3.2   Image Representation and Pre-processing 
A central issue in content-based image annotation and retrieval is how to describe the 
visual information in a way compatible with human visual perception. But until now, 
no general framework is proposed. For different tasks and goals, different low-level 
features are used to describe and analyze the visual content of images. On the whole, 
there are two kinds of interesting open questions remain unresolved. First, what fea-
ture sets should be selected to be the most expressive for any image region. Second, 
how blob-tokens can be generated, that is to say, how can one create such a visual 
vocabulary of blob-tokens to represent each image in the collection using a number of 
symbols from this finite set? In our method, we carried out these following two steps: 
First, segment images into sub-images, Second, extract appropriate features for any 
sub-images, cluster similar regions by k-means and then use the centroid in each clus-
 Automatic Image Annotation Using Maximum Entropy Model 39 
ter as a blob-token. The first step can be employed by either using a segmentation 
algorithm to produce semantically meaningful units or partitioning the image into 
fixed-size rectangular grids. Both methods have pros and cons, a general purpose 
segmentation algorithm may produce semantic regions, but due to the limitation in 
computer vision and image processing, there are also the problems of erroneous and 
unreliable region segmentation. The advantage of regular grids is that is does not need 
to perform complex image segmentation and is easy to be conducted. However, due to 
rough fixed-size rectangular grids, the extracted blocks are unable to model objects 
effectively, leading to poor annotation accuracy in our experiment.  
                   
Fig. 2. Segmentation Results using Normalized cuts and JSEG 
In this paper, we segment images into a number of meaningful regions using Nor-
malized cuts [6] against using JSEG. Because the JSEG is only focusing on local 
features and their consistencies, but Ncuts aims at extracting the global impression of 
an image data. So Ncuts may get a better segmentation result than JSEG. Fig. 2 shows 
segmentation result using Normalized cuts and JSEG respectively, the left is the origi-
nal image, the mid and the right are the segmentation result using Ncuts and JSEG 
respectively. After segmentation, each image region is described by a feature vector 
formed by HSV histograms and Gabor filters. Similar regions will be grouped to-
gether based on k-means clustering to form the visual vocabulary of blob-tokens. Too 
much clusters may cause data sparseness and too few can not converge. Then each of 
the labeled and unlabeled images can be described by a number of blob-tokens, in-
stead of the continuous-valued feature vectors. So we can avoid the image data mod-
eling in a high-dimensional and complex feature space. 
3.3   The Annotation Strategy Based on Maximum Entropy 
Maximum Entropy Model is a general purpose machine learning and classification 
framework whose main goal is to account for the behavior of a discrete-valued ran-
dom process. Given a random process whose output value y may be influenced by 
some specific contextual information x, such a model is a method of estimating the 
conditional probability. 
?
=
=
k
j
yxfj
j
xZ
xyp
1
),(
)(
1)|( ?                                              (1) 
In the process of annotation, images are segmented using normalized cuts, every 
image region is represented by a feature vector consisting of HSV color histogram 
and the Gabor filters, and then a basic visual vocabulary containing 500 blob-tokens 
is generated by k-means clustering. Finally, each segmented region is assigned to the 
label of its closest blob-token. Thus the complex visual contents of images can be 
40 W. Li and M. Sun 
represented by a number of blob-tokens. Due to the imbalanced distribution of key-
words frequency and the data sparseness problem, the size of the pre-defined keyword 
vocabulary is reduced from 1728 to 121 keywords, by keeping only the keywords 
appearing more than 30 times in the training dataset. 
We use a series of feature function ( )ji wbf ,Label,FC  to model the co-occurrence 
statistics of blob-tokens ib  and keywords jw , where FC denote the context of feature 
constraints for each blob-token. The following example represents the co-occurrence 
of the blob-token 
?
b  and the keyword ?water? in an image I. 
( ) ( )
??
? ====
=
otherwise
truebFCandwaterwif
wbf iwjji 0
''1
,water ,FC w       (2) 
If blob-token ib  satisfies the context of feature constraints and keyword ?water? 
also occurs in image I. In other words, if the color and texture feature components are 
coordinated with the semantic label ?water?, and then the value of the feature function 
is 1, otherwise 0. 
The following Fig. 3 shows the annotation procedure that using MaxEnt captures 
the hidden relationship between blob-tokens and keywords from a roughly labeled 
training image sets. 
 
Fig. 3. Learning the statistics of blob-tokens and words 
In the recent past, many models for automatic image annotation are limited by the 
scope of the representation. In particular, they fail to exploit the context in the images 
and words. It is the context in which an image region is placed that gives it meaning-
ful interpretation. 
 
 Automatic Image Annotation Using Maximum Entropy Model 41 
In our annotation procedure, each annotated word is predicted independently by the 
Maximum Entropy Model, word correlations are not taken into consideration. How-
ever, correlations between annotated words are essentially important in predicting 
relevant text descriptions. For example, the words ?trees? and ?grass? are more likely 
to co-occur than the words ?trees? and ?computers?. In order to generate appropriate 
annotations, a simple language model is developed that takes the word-correlation 
information into account, and then the textual description is determined not only by 
the model linking keywords and blob-tokens but also by the word-to-word correla-
tion. We simply count the co-occurrence information between words in the pre-
defined textual set to produce a simple word correlation model to improve the annota-
tion accuracy. 
4   Experiments and Analysis 
We carried out experiments using a mid-sized image collection, comprising about 
5,000 images from Corel Stock Photo CDs, 4500 images for training and 500 for 
testing. The following table 1 shows the results of automatic image annotation using 
Maximum Entropy. 
Table 1. Automatic image annotation results 
Images Original Annotation Automatic Annotation 
 
sun city sky mountain Sun sky mountain 
clouds 
 
flowers tulips mountain sky Flowers sky trees grass 
 
tufa snow sky grass snow sky grass stone 
 
polar bear snow post bear snow sky rocks 
42 W. Li and M. Sun 
For our training datasets, the visual vocabulary and the pre-defined textual set con-
tain 500 blob-tokens and 121 keywords respectively, so the number of the training 
pairs ( )ji wb ,  is 60500. After the procedure of feature selection, only 9550 pairs left. 
For model parameters estimation, there are a few algorithms including Generalized 
Iterative Scaling and Improved Iterative Scaling which are widely used. Here we use 
Limited Memory Variable Metric method which has been proved effective for Maxi-
mum Entropy Model [10]. Finally, we can get the model linking blob-tokens and 
keywords, and then the trained model ( )xyp  is applied to predict textual annota-
tions { }nwww ,,, 21 K  given an unseen image formed by{ }mbbb ,,, 21 K . 
To further verify the feasibility and effectiveness of Maximum Entropy model, we 
have implemented the co-occurrence model as one of the baselines whose conditional 
probability ( )ij bwp  can be estimated as follows: 
( ) ( ) ( )
( ) ( )
( )( )
( )( ) i
ij
N
k
ik
ij
N
k
kkik
jjij
N
k
iki
iji
ij M
m
m
m
Nnnm
Nnnm
wpwbp
wpwbp
bwp ==?=
???
=== 111
    (3) 
Where ijm denote the co-occurrence of ib  and jw , jn denote the occurring num-
ber of  jw in the total N words. 
The following Fig. 4 shows the some of the retrieval results using the keyword  
?water? as a textual query.  
 
Fig. 4. Some of retrieved images using ?water? as a query 
The following Fig. 5 and Fig. 6 show the precision and recall of using a se of high-
frequency keywords as user queries. We implemented two statistical models to link 
blob-tokens and keywords.  
 Automatic Image Annotation Using Maximum Entropy Model 43 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
w
a
t
e
r
s
k
y
t
r
e
e
s
p
e
o
p
l
e
g
r
a
s
s
s
n
o
w
c
l
o
u
d
s
f
l
o
w
e
r
s
m
o
u
n
t
a
i
n
s
b
u
i
l
d
i
n
g
s
t
o
n
e
b
u
i
l
d
i
n
g
s
s
t
r
e
e
t
s
a
n
d
f
i
e
l
d
b
e
a
r
b
e
a
c
h
t
r
e
e
j
e
t
P
r
e
c
i
s
i
o
n
Maximum Entropy
Co-occurrence
 
Fig. 5. Precision of retrieval using some high-frequency keywords 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
w
a
t
e
r
s
k
y
t
r
e
e
s
p
e
o
p
l
e
g
r
a
s
s
s
n
o
w
c
l
o
u
d
s
f
l
o
w
e
r
s
m
o
u
n
t
a
i
n
s
b
u
i
l
d
i
n
g
s
t
o
n
e
b
u
i
l
d
i
n
g
s
s
t
r
e
e
t
s
a
n
d
f
i
e
l
d
b
e
a
r
b
e
a
c
h
t
r
e
e
j
e
t
R
e
c
a
l
l
Maximum Entropy
Co-Occurrence
 
Fig. 6. Recall of retrieval using some high-frequency keywords 
The annotation accuracy is evaluated by using precision and recall indirectly. After 
posing a keyword query for images, the measure of precision and recall can be de-
fined as follows: 
BA
Aprecision
+
=              
CA
A
recall
+
=                              (4) 
Where A denote the number of relevant images retrieved, B denote the number of 
irrelevant images retrieved, C denote the number of relevant images not retrieved in 
the image datasets, and images whose labels containing the query keyword is consid-
ered relevant, otherwise irrelevant.  
Table 2. Experimental results with average precision and mean 
Method Mean precision Mean recall 
Co-occurrence 0.11 0.18 
Maximum Entropy 0.17 0.25 
44 W. Li and M. Sun 
The above experimental results in table 2 show that our method outperforms the 
Co-occurrence model [4] in the average precision and recall. Since our model uses the 
blob-tokens to represent the contents of the image regions and converts the task of 
automatic image annotation to a process of translating information from visual lan-
guage (blob-tokens) to textual language (keywords). So Maximum Entropy Model is 
a natural and effective choice for our task, which has been successfully applied to the 
dyadic data in which observations are made from two finite sets of objects. But disad-
vantages also exist. There are two fold problems to be considered. First, since Maxi-
mum Entropy is constrained by the equation ( ) ( )fpfp ~= , which assumes that the 
expected value of output of the stochastic model should be the same as the expected 
value of the training sample. However, due to the unbalanced distribution of key-
words frequency in the training subset of Corel data, this assumption will lead to an 
undesirable problem that common words with high frequency are usually associated 
with too many irrelevant blob-tokens, whereas uncommon words with low frequency 
have little change to be selected as annotations for any image regions, consider word 
?sun? and ?apple? , since both words may be related to regions with ?red? color and 
?round? shape, but it is difficult to make a decision between the word ?sun? and ?ap-
ple?. However, since ?sun? is a common word as compared to ?apple? in the lexical 
set, the word ?sun? will definitely used as the annotation for these kind of regions. To 
address this kind of problems, our future work will mainly focus on the more sophis-
ticated language model to improve the statistics between image features and key-
words. Second, the effects of segmentation may also affect the annotation perform-
ance. As we know, semantic image segmentation algorithm is a challenging and com-
plex problem, current segmentation algorithm based on the low-level visual features 
may break up the objects in the images, that is to say, segmented regions do not defi-
nitely correspond to semantic objects or semantic concepts, which may cause the 
Maximum Entropy Model to derive a wrong decision given an unseen image. 
5   Conclusion and Future Work 
In this paper, we propose a novel approach for automatic image annotation and re-
trieval using Maximum Entropy Model. Compared to other traditional classical meth-
ods, the proposed model gets better annotation and retrieval results. But three main 
challenges are still remain: 
1) Semantically meaningful segmentation algorithm is still not available, so the 
segmented region may not correspond to a semantic object and region features 
are insufficient to describe the image semantics. 
2) The basic visual vocabulary construction using k-means is only based on the 
visual features, which may lead to the fact that two different semantic objects 
with similar visual features fall into the same blob-token. This may degrade the 
annotation quality. 
3) Our annotation task mainly depend on the trained model linking image features 
and keywords, the spatial context information of image regions and the word cor-
relations are not fully taken into consideration. 
In the future, more work should be done on image segmentation techniques, clus-
tering algorithms, appropriate feature extraction and contextual information between 
regions and words to improve the annotation accuracy and retrieval performance. 
 Automatic Image Annotation Using Maximum Entropy Model 45 
Acknowledgements 
We would like to express our deepest gratitude to Kobus Barnard and J.Wang for 
making their image datasets available. This research is supported by the National 
Natural Science Foundation of China under grant number 60321002 and the National 
863 Project of China under grant number 2001AA114210-03, and the ALVIS Project 
co-sponsored by EU PF6 and NSFC. 
References 
1. K. Barnard, P. Dyugulu, N. de Freitas, D. Forsyth, D. Blei, and M. I. Jordan. Matching 
words and pictures. Journal of Machine Learning Research, 3: 1107-1135, 2003. 
2. P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth. Ojbect recognition as machine 
translation: Learning a lexicon fro a fixed image vocabulary. In Seventh European Conf. 
on Computer Vision, 97-112, 2002. 
3. J. Jeon, V. Lavrenko and R. Manmatha. Automatic image annotation and retrieval using 
cross-media relevance models. In Proceedings of the 26th intl. SIGIR Conf, 119-126, 2003. 
4. Y. Mori, H. Takahashi, and R. Oka, Image-to-word transformation based on dividing and 
vector quantizing images with words. First International Workshop on Multimedia Intelli-
gent Storage and Retrieval Management, 1999. 
5. Edward Chang, Kingshy Goh, Gerard Sychay and Gang Wu. CBSA: Content-based soft 
annotation for multimodal image retrieval using bayes point machines. IEEE Transactions 
on Circuts and Systems for Video Technology Special Issue on Conceptual and Dynamical 
Aspects of Multimedia Content Descriptions, 13(1): 26-38, 2003. 
6. J. shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions On Pat-
tern Analysis and Machine Intelligence, 22(8): 888-905, 2000. 
7. A. Berger, S. Pietra and V. Pietra. A maximum entropy approach to natural language proc-
essing. In Computational Linguistics, 39-71, 1996. 
8. J. Li and J. A. Wang. Automatic linguistic indexing of pictures by a statistical modeling 
approach. IEEE Transactions on PAMI, 25(10): 175-1088, 2003. 
9. Jiwoon Jeon, R. Manmatha. Using maximum entropy for automatic image annotation. In 
proceedings of third international conference on image and video retrieval, 24-31, 2004. 
10. Robert Malouf. A comparison of algorithms for maximum entropy parameter estimation. 
In Proceedings of the 6th Workshop on Computational Language Learning, 2003. 
11. V. Lavrenko, R. Manmatha and J. Jeon. A model for learning  the semantics of pictures. In 
Proceedings of the 16th Annual Conference on Neural Information Processing Systems, 
2004. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 414 ? 425, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Preliminary Work on Classifying Time Granularities 
of Temporal Questions 
Wei Li1, Wenjie Li1, Qin Lu1, and Kam-Fai Wong2  
1
 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong 
{cswli, cswjli, csluqin}@comp.polyu.edu.hk 
2
 Department of Systems Engineering, the Chinese University of Hong Kong, 
Shatin, Hong Kong 
kfwong@se.cuhk.edu.hk 
Abstract. Temporal question classification assigns time granularities to tempo-
ral questions ac-cording to their anticipated answers. It is very important for an-
swer extraction and verification in the literature of temporal question answer-
ing. Other than simply distinguishing between "date" and "period", a more fine-
grained classification hierarchy scaling down from "millions of years" to "sec-
ond" is proposed in this paper. Based on it, a SNoW-based classifier, combining 
user preference, word N-grams, granularity of time expressions, special patterns 
as well as event types, is built to choose appropriate time granularities for the 
ambiguous temporal questions, such as When- and How long-like questions. 
Evaluation on 194 such questions achieves 83.5% accuracy, almost close to 
manually tagging accuracy 86.2%. Experiments reveal that user preferences 
make significant contributions to time granularity classification. 
1   Introduction 
Temporal questions, such as the questions with the interrogatives ?when?, ?how long? 
and ?which year?, seek for the occurrence time of the events or the temporal attributes 
of the entities. Temporal question classification plays an important role in the litera-
ture of question answering and temporal information processing. In the evaluation of 
TREC 10 Question-Answering (QA) track [1], more than 10% of questions in the test 
question corpus are temporal questions. Different from TREC QA track, Workshop 
TERQAS (http://www.timeml.org/terqas/) particularly investigated on temporal ques-
tion answering instead of a general one. It focused on temporal and event recognition 
in question answering systems and paid great attention to temporal relations among 
states, events and time expressions in temporal questions. TimeML (http://www.ti-
meml.org), a temporal information (e.g. time expression, tense & aspect) annotation 
standard, has also been used for temporal question answering in this workshop [2]. 
Correct understanding of a temporal question will greatly help extracting and verify-
ing its answers and certainly improve the performance of any question answering 
system. Look at the following examples. 
[Ea]. What is the birthday of Abraham Lincoln? 
[Eb]. When did the Neanderthal man live? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 415 
In a general question answering system, the question classifier commonly classifies 
temporal questions into two classes, i.e. ?date? and ?period?. With such a system, the 
above two questions are both assigned a ?date?. Whereas it is natural for the question 
[Ea] to be answered with a particular data (e.g. ?12/02/1809?), it is not the case for 
question [Eb], because a proper answer could be ?35,000 years ago?. However, if it is 
known that the time granularity concerned is ?thousands of years?, answer extraction 
turn to be more targeted. The need for a more fine-grained classification is obvious. 
Although there were different question classification hierarchies, as reported 
[3,4,12,13,14], few inclined to introducing the classification hierarchy (e.g. ?year?, 
?month? and ?day?) which could give a clearer direction to guide answer extraction 
and verification of temporal questions. In the following, we try to find out whether 
temporal questions can be further classified into finer time granularity and how to 
classify them. 
By examining a temporal question corpus consisting of 348 questions, 293 of 
which are gathered from UIUC question answering labelled data (http://l2r.cs. 
uiuc.edu/~cogcomp/Data/QA/QC), and the rest 55 from TREC 10 test corpus, we find 
two different cases. On the one hand, some questions are very straightforward in ex-
pressing the time granularities of the answers expected, e.g. the questions beginning 
with ?which year? or ?for how many years?. On the other hand, some questions are 
not so obvious, e.g. the questions headed by ?when? or ?for how long?. We call such 
questions ambiguous questions. Not surprisingly, the ambiguous When- and How 
long-like questions account for a large proportion in this temporal question corpus, 
i.e. 197 from 348 in total. 
We further investigate on those 197 ambiguous questions in order to find out 
whether they can be classified into finer time granularity. Three experimenters are 
requested to tag a time granularity to each question independently1. Answers are not 
provided. The tag with two agreements is taken as the time granularity class of the 
corresponding temporal question. Otherwise the tag ?UNKNOWN? is assigned. Ref-
erence answers for the questions are extracted from AltaVista Web Search 
(http://www.altavista.com). Comparing the time granularities tagged manually with 
those provided by the reference answers, we find that only 27 out of 197 questions are 
incorrectly tagged, in other words, the manually tagging accuracy is 86.2%. Errors 
exist though, the relatively high agreement between users? tagging and reference 
answers lights the hope of automatically determining the time granularities of tempo-
ral questions. 
Analysing the tagging results, it is revealed that the tagging errors arouse from 
three sources: insufficient world knowledge, different speaking habits and different 
expected information granularity among human. See the following examples: 
[Ec]. When did the Neanderthal man live?   
User: year; Ref.: thousands of years 
[Ed]. How long is human gestation?  
User: month; Ref.: week 
[Ee]. When was the first Wall Street Journal published?   
User: year; Ref.: day 
                                                          
1
  The granularity hierarchy and the tagging principle will be detailed later. 
416 W. Li et al 
For question [Ec], the time granularity should be ?thousands of years?, rather than 
?year?. This error could be corrected if one knows that Neanderthal man existed 
35,000 years ago. The time granularity of question [Ed] should be ?week?, but not 
?month? in accordance with the habit. For question [Ee], users? tag is ?year?, different 
from the reference answer?s tag ?day?. However, both granularities are acceptable in 
commonsense, because the different users may want coarser or finer information. This 
observation suggests that incorporating question context, world knowledge, and 
speaking habits would help determine the time granularities of temporal questions. 
In this paper, we propose a fine-grained temporal question classification scheme, 
i.e. time granularity hierarchy, consisting of sixteen non-exclusive classes and scaling 
down from ?millions of years? to ?second?. The SNoW-based classifier is then built 
to combine linguistic features (including word N-grams, granularity of time expres-
sions and special patterns), user preferences and event types, and assign one of the 
sixteen classes to each temporal question. In our work, user preference, which charac-
terizes world knowledge and speaking habits, is estimated by means of the time 
granularities of the entities and/or events involved. The SNoW-based classifier 
achieves 83.5% accuracy, almost close to 86.2% of manually tagging accuracy. Ex-
periments also show that user preference makes a great contribution to time granular-
ity classification. 
The rest of this paper is organized as follows. In the next section various related 
works in this literature are introduced. In Sect. 3, we demonstrate the time granularity 
hierarchy and principles. User preference is fully investigated in Sect. 4. Feature de-
sign is depicted in Sect. 5. Time granularity classifiers are introduced in Sect. 6 and 
the experiment results are presented in Sect. 7. We finally conclude this paper in the 
last section. 
2   Related Works 
In TREC QA track, almost every QA system joining in the evaluation has a question 
classification module. This makes question classification a hot topic. Questions can be 
classified from several aspects. Most classification hierarchies [3,4,12,13,14] adopt the 
anticipated answer types as its classification criteria. Abney et al [4] gave a coarse 
classification hierarchy with seven classes (person, location, etc.). Hovy et al [13] 
introduced a finer classification with forty-seven classes manually constructed from 
17,000 practical questions. Li et al [3] proposed a two-level classification hierarchy, a 
coarser one with six classes and a finer one with fifty classes. In all these classification 
hierarchies, temporal questions are simply classified into two classes, i.e. ?date? and 
?period?. Some works classified temporal questions from other aspects. In [2], a tem-
poral question classification hierarchy is proposed according to the temporal relation 
among state, event and time expression. In [5], temporal questions are classified into 
three types with regard to question structure: non-temporal, simple and complex. Diaz 
F. et al [6] did an interesting work on the statistics of the number of topics along time-
line. According to whether questions or topics have a clear distribution along timeline, 
they can be classified into three types: atemporal, temporal clear and temporal ambigu-
ous.  Focusing on ambiguous temporal questions, e.g. when and how long-like ques-
tions, we introduce a classification hierarchy in terms of the anticipated answer types. 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 417 
It is an extension of two classes ?date? and ?period? and includes sixteen non-
exclusive classes scaling down from ?millions of years? to ?second?. 
Related to the work of features design, Li et al [3] built the question classifier 
based on three types of features, including surface text (e.g. N-grams), syntactic fea-
tures (e.g. part-of-speech and name entity tags), and semantic related words (words 
that often occur with a specific question class). Later works of Li et al [10] intro-
duced semantic information and world knowledge from external resources such as 
WordNet. In this paper, we introduce a new feature, user preference, which is ex-
pected to imply the world knowledge in time granularity in the experiment. User 
preference is estimated from statistics with which Diaz F. et al [6] determine whether 
a question is temporal ambiguous or not. E. Saquete et al [5] suggested that questions 
had different structures, i.e. non-temporal, simple and complex, which is helpful to 
handle questions more orderly. It gives us inspiration to use question focus, i.e. 
whether a question is event-based or entity-based.  
Many machine-learning methods have been used in question classification, such 
as language model [7], SNoW [3,10], maximum entropy [15] and support vector ma-
chine [8,9]. In our experiments, language model is selected as the baseline model, and 
SNoW is selected to tackle to the large feature space and build the classifier. In fact, 
SNoW has already been used in many other fields, such as text categorization, word 
sense disambiguation and even facial feature detection. 
3   Time Granularity Hierarchy and Tagging Principles 
In traditional question answering systems, only two question types are time-related, i.e. 
?date? and ?period?. For the reasons explained in Sect. 1, we propose a more detailed 
temporal question classification scheme, namely time granularity hierarchy scaling 
down from ?millions of years? to ?second? in order to facilitate answer extraction and 
verification. The initial time granularity hierarchy includes the following twelve 
classes: ?second?, ?minute?, ?hour?, ?day?, ?week?, ?month?, ?season?, ?year?, ?dec-
ade?, ?century?,  ?thousands of years? and ?millions of years?.  
Granularity ?weekday? is added to the initial hierarchy because some temporal 
questions favor ?weekday? instead of ?day?, although both of them indicate one day. 
Some questions favour a region of time granularity. Look at the following examples. 
[Ef]. What time of year has the most air travel? 
[Eg]. What time of day did Emperor Hirohito die? 
For [Ef] question, its time granularity could be ?season?, ?month? or even ?day?; and 
for question [Eg], the time granularity could be ?hour? or ?minute?. We can only 
determine that their time granularities are less than ?year? or ?day? respectively, but 
cannot go any further. Such situations only occur to time granularity ?year? and 
?day?, so we expand the original classification hierarchy by adding another two types: 
?less than day?, ?less than year?. Besides, the questions asking for festivals are classi-
fied into ?special date?.  
Up to now, the time granularity hierarchy has sixteen classes. The less frequent 
temporal measures, such as ?microsecond? and ?billions of years? are ignored. As 
mentioned above, the class ?less than day? overlaps several granularities, e.g. ?hour? 
and ?minute?, so the time granularity hierarchy we proposed is non-exclusive.  
418 W. Li et al 
In reality, some temporal questions can be answered in several different time 
granularities. For example, question ?when was Abraham Lincoln born??,  its answers 
can be a ?day? (?12/02/1809?) or a ?year? (?1809?). To resolve this confliction, we 
adopt two principles for time granularity annotation.  
[Pa]. Assign the minimum time granularity we can determine to a given temporal 
question if several time granularities are applicable. 
[Pb]. Select the time granularity with regard to speaking habits or user preferences.  
When the two principles conflict to each other, principle [Pb] takes the priority. With 
principle [Pa], time granularity of the above question can only be ?day?. 
4   User Preference 
In general, temporal questions have two different focuses: entity-based and event-based. 
[a]. Entity-based question: temporal interrogative words + (be) + entity, e.g. 
?When was the World War II?? 
[b]. Event-based question: temporal interrogatives + event, e.g. ?When did 
Mount St. Helen last have a significant eruption?? 
Time granularities of entities (or events) have great significance to those of entity-
based (or event-based) temporal questions. So, in the following, we make estimation 
of the time granularities of entities and events from statistics, based on the intuition 
that some entities or events may favor certain types of time granularities, which is 
called user preference here.  
4.1   Estimation of Time Granularities of Entities and Events 
4.1.1   Time Granularity of Entities 
The time granularity of the entity is derived by counting the co-occurrences of the 
entity and time granularities. The statistics is gathered from AltaVista Web Search. 
The sentences containing both the entity and time expressions are extracted from the 
first one hundred results returned by AltaVista with the entity as the searching key-
word. The probability P of a time granularity class tgi on the occurrence of the entity 
is calculated as the following Equation (1).  
)(#
)(#)|(
entity
entitytg
entitytgP ii
?
=
  )|(max)( entitytgPArgentityTG itgi=          (1) 
#( ) is the number of the sentences containing the expressions between the parenthe-
sis. TG(entity) represents the time granularity of the entity. 
4.1.2   Time Granularity of Events 
The time granularities of the events are not directly extracted as what is done to the 
entities, because they have little chance to be reused on the observation that there are 
rarely two identical events in a question corpus. As an alternative, the time granularity 
of an event is estimated from a sequence of entity-verb-entity? approximating the 
event. The time granularity of the verb is determined as Equation (1) by substituting 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 419 
?verb? for ?entity?. We choose two strategies for the estimation: maximum product 
and one-win-all.  
Maximum  product: )'|()|()|(1)|( entitytgPverbtgPentitytgP
Z
eventtgP iiii =
 
                                 )|(max)( eventtgPArgeventTG itgi=                                        (2) 
TG(event) represents time granularity of event. Z is used for normalization. 
One-win-all: )}'|(),|(),|({max)( entitytgPverbtgPentitytgPArgeventTG iiitgi=               (3) 
Equation (1) is smoothed in order to avoid 0 values in Equation (2). 
                          
i
i
i tgttw
wtg
wtgP =
+
+?
= )(#
1)(#)|(                           (4) 
t is the number of the time granularity classes, w is either an entity or a verb. 
4.1.3   Experiment: Evaluating the Estimation 
In the 197 ambiguous questions, 12 questions are entity-based, and the rest 185 ques-
tions are event-based. If all the 197 questions are arbitrarily assigned a tag ?year?, the 
tagging accuracy is 48.2%. 
For each entity-based or event-based question, the time granularity of the entity or 
event within it are assumed as the time granularity of the question. Compared with the 
time granularity of the reference answer, for the entity-based questions, we achieve 
75% accuracy; for the event-based question, the accuracy of maximum product strat-
egy and one-win-all strategy are 67.0% and 64.3% respectively. It seems that maxi-
mum product strategy is more effective than one-win-all strategy in this application. 
With maximum product strategy, the overall accuracy on all the 197 ambiguous ques-
tions is 67.4%. Notice that the accuracy of arbitrarily tagging is only 48.2%, so the 
estimation of the time granularities of the entities and the events is useful for deter-
mining the time granularities of temporal questions. 
4.2   Distribution of the Time Granularity of Entities and Events 
4.2.1   Observation of Distribution 
In the experiments of estimation, we find that some entities or events tend to favor 
only one certain time granularity, some others tend to favor several time granularities, 
and the rest may have a uniform distribution almost on every time granularity. 
-1 0 1 2 3 4 5 6 7 8 9
0
20
40
60
80
100
Season
Week
Pr
o
p o
rti
o n
(%
)
Time Granularity of "gestation"
   
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
60
Century
Decade
Year
Month
Day
Pr
o
po
rti
o
n(%
)
Time Granularity of "Lincoln born"
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
Year
Month
Weekday
Day
Hour
Pr
op
or
tio
n
(%
)
Time Granularity of "take place"
 
(a)   (b)   (c) 
Fig. 1. Distribution of the time granularities of the entities and events 
420 W. Li et al 
In Fig. 1(a), time granularity ?day? takes a preponderant proportion, i.e. more than 
80%, in the distribution of ?gestation?, which is called single-peak-distribution. In 
Fig. 1(b), both ?day? and ?year? take a large proportion, so ?Lincoln born? is multi-
peak-distributed. In Fig. 1(c), for ?take place?, all the time granularities almost take a 
similar proportion and it is a uniform distribution. 
4.2.2   Experiments on Distribution 
Assume an entity (or event) E, its possible time granularities {tgi, i=1,?t} and the 
corresponding probabilities {Pi, i=1,?t} (calculated by Equation 1 and 2).  
       ?= i iPt1? ;  ?= i iPId ),( ? ; ?
??
?
>
??
?
=
i
i
i P
P
PI
0
1),(                        (5) 
d is the number of time granularities tgi with higher probability Pi than average prob-
ability ? . For simplicity, distribution DE of the time granularity of E is determined as 
follows, 
                                              
3
31
1
>
?<
=
??
??
?
=
d
d
d
Uniform
Multi
Single
DE
                                                  (6)  
Observing the experiment results in Sect. 4.1.3, 88.7%, 56.3% and 18.9% accuracy 
are achieved on the questions within which the time granularities of the entities or 
events are estimated to be single-peak-, multi-peak-, and uniform-distributed respec-
tively. So whether the estimated time granularity of the entity or event is single-peak-, 
multi-peak-, or uniform-distributed highlights the confidence on the estimation, which 
can be taken as a feature associated with the estimation of the time granularities. 
5   Feature Design 
As described in the above section, estimation of the time granularities of the entities 
and the events is useful for determining the time granularities of temporal questions; 
whether a question is entity-based or not and the distribution of time granularities of 
the entities and events within the questions will also be taken as associated features. 
These three features are named user preference feature in total. Besides, another four 
types of features are considered. 
Word N-grams 
Word N-grams feature, e.g. unigram and bigram is the most straightforward feature 
and commonly used in question classification. In general question classification, uni-
gram ?when? indicates a temporal question. In temporal question classification, uni-
gram ?birthday? always implies a ?day? while bigram ?when ? born? is a strong 
evidence of the time granularity ?day?. From this aspect, word N-grams also reflect 
user preference on time granularity. 
Granularity of Time Expressions 
Time expressions are common in temporal questions, e.g. ?July 11, 1998? and date 
modifier ?1998? in ?1998 Superbowl?. We take the granularities of time expressions 
as features, for example, 
TG(?in 1998?) = ?year?  TG(?July 11, 1998?) = ?day? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 421 
Granularities of time expressions impose the constraints on the time granularities of 
temporal questions. If there is a time expression whose time granularity is tg in a 
temporal question, time granularity of this question can not be tg. For example, ques-
tion ?When is the 1998 SuperBowl??, its time granularity can not be ?Year?, i.e. the 
time granularity of  ?1998?. 
Special Patterns 
In word N-gram features, words are equally processed, however, some special words 
combining with the verbs or the temporal connectives (e.g. ?when?, ?before? and 
?since?) will produce special patterns and affect the time granularities of temporal 
questions. Look at the following examples. 
[Eh]. Since when hasn?t John Sununu been able to fly on government planes for 
personal business? 
[Ei]. What time of the day does Michael Milken typically wake up? 
For question [Eh], the temporal preposition ?since? combined with ?when? highlights 
that this question is seeking for a beginning point time, which implies a finer time 
granularity; for question [Ei], ?typically? combined with verb ?wake up? indicates a 
generally occurred event, and implies that its time granularity could be ?less than 
day? or ?less than year?. 
Event Types 
In general, there are four event types: states, activities, accomplishments, and 
achievements. States and activities favour larger time granularities, while accom-
plishments and achievements favour smaller ones. For example, the activity ?stay? 
will favour larger time granularity than the accomplishment event ?take place?.  
6   Classifier Building 
In this work, we choose the Sparse Network of Winnow (SNoW) model as the time 
granularity classifier and compare it with a commonly used Language Model (LM) 
classifier. 
6.1   Language Model (LM) 
As language model has already been used in question classification [7], it is taken as 
the baseline model in the experiments. Language model mainly combines two types 
of features, i.e. unigram and bigram. Given a temporal question Q, its time granularity 
TG(Q) is calculated by Equation (7). 
              ?? =
=
+
=
=
?+=
nj
j jji
mj
j jitg wwtgPwtgPArgQTG i 1 11 )|()1()|(max)( ??              (7)  
w represents words. m and n are the numbers of unigrams and bigrams in questions 
respectively. ?  assigns different weights to unigrams and bigrams. In the experiment, 
best accuracy is achieved when 7.0=?  (see Sect. 7.3.1). 
6.2   Sparse Network of Winnow (SNoW) 
SNoW is a learning framework and applicable to the tasks with a very large number 
of features. It selects active features by updating weights of features, and learns a 
422 W. Li et al 
linear function from a corpus consisting of positive and negative examples. Let 
Ac={i1, ?, im} be the set of features that are active and linked to target class c. Let si 
be the real valued strength associated with feature in the example. Then the example?s 
class is c if and only if, 
                                                   ?
?
?
Aci
ciic sw ?,                                                     (8) 
icw , is weight of feature i connected with class c, which is learned from the training 
corpus. SNoW has already been used in question classification [3,10] and good results 
are reported. As mentioned in Sect. 5, five types of features are selected for our task. 
They are altogether counted to more than ten thousand features. Since it is a large 
feature set, SNoW is a good choice.  
7   Experiments 
7.1   Setup 
In this 348-question-corpus (see Sect. 1), time granularities of 151 questions are 
straightforward, while those of the rest 197 questions are ambiguous. For the sixteen 
time granularity classes, we only consider ten classes including more than four ques-
tions. Questions with unconsidered time granularity classes excluded, the question cor-
pus has 339 questions in total, 145 for training and 194 for testing. As a result, the task 
is to learn a model from the 145-question training corpus and classify questions in the 
194-question test corpus into ten classes: ?second?, ?minute?, ?hour?, ?day?, ?week-
day?, ?week?, ?month?, ?season?, ?year? and ?century?. The SNoW classifier is 
downloaded from UIUC (http://l2r.cs.uiuc.edu/~cogcomp/download.php?key=SNOW). 
7.2   Evaluation Criteria 
The primary evaluation standard is accuracy1, i.e. the proportion of the correct classi-
fied questions out of the test questions (see Equation 9). However, if a question seek-
ing for a finer time granularity, e.g. ?day?, has been incorrectly determined as a 
coarser one, e.g. ?year?, it should also be taken as partly correct, which is reflected in 
accuracy2 (see Equation 10).  
                                         
)(#
)(#
1 test
correctAccuracy =                                                  (9) 
#( ) is number of questions. 
       
)(#
)(
2 test
QRR
Accuracy i i?=
??
??
?
>
<
=
+?
=
)()'(
)()'(
)()'(
)1)()'((1
0
1
)(
QQ
QQ
QQ
QQ tgRtgR
tgRtgR
tgRtgR
tgRtgR
QRR
        (10) 
Qtg  and 'Qtg  are the reference and classification result respectively. )( QtgR is the 
rank of the time granularity class Qtg , scaling down from ?millions of years? to 
?second?. Rank of ?second? is 1, while rank of ?year? is 9. The ranks of the last three 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 423 
time granularities, i.e. ?special date?, ?less than day? and ?less than year? are 14, 15 
and 16 respectively. Likewise, )'( QtgR is the rank of 'Qtg .  
7.3   Experimental Results and Analysis 
In the experiments, language model is taken as the baseline model. Performance of 
SNoW-based classifier will be compared with that of language model. Different com-
binations of features are tested in SNoW-based classifier and their performances are 
investigated. 
7.3.1   LM Classifier 
The LM classifier takes two types of features: unigram and bigram. Experiment re-
sults are presented in Fig. 2.  
Accuracy varies with different feature weight ?  and best accuracy (accuracy1 
68.0% and accuracy2 68.9%) achieves when ? =0.7. Accuracy when ? =1.0 is higher 
than that when ? =0. It indicates that, in the framework of language model, unigrams 
achieves better performance than bigrams, which accounts from the sparseness of 
bigram features. 
-0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
45
50
55
60
65
70
 Accuracy1
 Accuracy2
Ac
cu
ra
cy
(%
)
?
 
Fig. 2. Accuracy of LM classifier. Data in circle is the best performance achieved. 
7.3.2   SNOW Classifier 
Our SNoW classifier requires binary features. We then encode each feature with an 
integer label. When a feature is observed in a question, its label will appear in the 
extracted feature set of this question. There are six types of features: 15 user prefer-
ences (10 for the estimation of time granularities, 3 for the estimation distributions, 
and 2 for question focuses) (F1), 951 unigrams (F2), 9277 bigrams (F3), 10 granularity 
of time expressions (F4), 14 special patterns (F5), and 4 event types (F6). Although the 
number of all features is more than ten thousand, the features in one question are no 
more than twenty in general. Accuracies of SNoW classifier on 194 test questions are 
presented in Table 1. It shows that simply using unigram features, SNoW classifier 
has already achieved better accuracy than LM classifier (accuracy1: 69.5% vs. 68.0%; 
accuracy2: 70.3% vs. 68.9%). From this view, SNoW classifier outperforms LM clas-
sifier in handling sparse features. When all the six types of features are used, SNoW 
classifier achieves 83.5% in accuracy1 and 83.9% in accuracy2, almost close to the 
accuracy of user tagging, i.e. 86.2%. 
424 W. Li et al 
Table 1. Accuracy (%) of SNoW classifier 
Feature Set F2 F2, 3 F1~6 
Accuracy1 69.5 72.1 83.5 
Accuracy2 70.3 72.7 83.9 
Table 2. Accuracy1 (%) on different types of time granularities 
TG second minute hour day weekday 
Accuracy1 100 100 100 64.2 100 
TG week month season year century 
Accuracy1 100 60 100 90.5 66.7 
Table 3. Accuracy (%) on combination of different types of features 
Feature Set F2,3 F1,2,3 F2,3,4 F2,3,5 F2,3,6 
Accuracy1 72.1 79.8 73.7 74.7 72.6 
Accuracy2 72.7 80.6 74.7 75.2 73.1 
With all the six types of features, accuracy1 on the questions with different types of 
time granularity is illustrated in Table 2. It reveals that the classification errors mainly 
come from time granularity of ?month?, ?day? and ?century?. Low accuracy on 
?month? and ?century? accounts from absence of enough examples, i.e. examples for 
training and testing both less than five. Many ?day? questions are incorrectly classi-
fied into ?year?, which accounts for the low accuracy on ?day?.  The reason lies in 
that there are more ?year? questions than ?day? questions in the training question 
corpus (116 vs. 56).  
In general, we can extract three F1 features, one F4 feature, less than two F5 fea-
tures, and one F6 feature from one question. It is hard for SNoW classifier to train and 
test independently on each of these types of the features because of the small feature 
number in one example question. However, the numbers of F2 and F3 features in a 
question are normally more than ten. So we take unigrams (F2) and bigrams (F3) as 
the basic feature set. Table 3 presents the accuracy when the rest four types of fea-
tures are added into the basic feature set respectively. As expected user preference 
makes the most significant improvement, 7.82% in accuracy1 and 7.90% in accuracy2. 
Special patterns also play an important role, which makes 2.6% accuracy1 improve-
ment. It is strange that event type makes such a modest improvement (0.5%). After 
analyzing the experimental results, we find that as there are only four event types, it 
makes limited contribution to 10-class time granularity classification. 
8   Conclusion 
Various features for time granularity classification of temporal questions are investi-
gated in this paper. User preference is shown to make a significant contribution  
to classification performance. SNoW classifier, combining user preference, word  
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 425 
N-grams, granularity of time expressions, special patterns and event types, achieves 
83.5% accuracy in classification, close to manually tagging accuracy 86.2%.  
Acknowledgement 
This project is partially supported by Hong Kong RGC CERG (Grant No: 
PolyU5181/03E), and partially by CUHK Direct Grant (No: 2050330).  
References 
1) TREC (ed.): The TREC-8 Question Answering Track Evaluation. Text Retrieval Confer-
ence TREC-8, Gaithersburg, MD (1999) 
2) Radev D. and Sundheim B.: Using TimeML in Question Answering. 
http://www.cs.brandeis.edu/~jamesp/arda/time/documentation/TimeML-use-in-qa-
v1.0.pdf, (2002) 
3) Li, X. and Roth, D.: Learning Question Classifiers. Proceedings of the 19th International 
Conference on Computational Linguistics (2002) 556-562 
4) S. Abney, M. Collins, and A. Singhal: Answer Extraction. Proceedings of the 6th ANLP 
Conference (2000) 296-301 
5) Saquete E., Mart?nez-Barco P., Mu?oz R.: Splitting Complex Temporal Questions for 
Question Answering Systems. Proceedings of the 42nd Annual Meeting of the Association 
for Computational Linguistics (2004) 567-574 
6) Diaz, F. and Jones, R.: Temporal Profiles of Queries. Yahoo! Research Labs Technical 
Report YRL-2004-022 (2004) 
7) Wei Li: Question Classification Using Language Modeling. CIIR Technical Report (2002) 
8) Dell Zhang and Wee Sun Lee: Question Classification Using Support Vector Machines. 
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and 
Development in Information Retrieval (2003) 26-32 
9) Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda: Question Classification 
Using HDAG Kernel. Proceedings of Workshop on Multilingual Summarization and 
Question Answering (2003) 61-68 
10) Li X., Roth D., and Small K.: The Role of Semantic Information in Learning Question 
Classifiers. Proceedings of the International Joint Conference on Natural Language Proc-
essing (2004) 
11) Schilder, Frank & Habel, Christopher: Temporal Information Extraction for Temporal 
Question Answering. In New Directions in Question Answering. Papers from the 2003 
AAAI Spring Symposium TR SS-03-07 (2003) 34-44 
12) Rohini K. Srihari, Wei Li: A Question Answering System Supported by Information Ex-
traction. Proceedings of Association for Computational Linguistics (2000) 166-172 
13) Eduard Hovy, Laurie Geber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran: 
Towards Semantics-Based Answer Pinpointing. Proceedings of the DARPA Human Lan-
guage Technology Conference (2001) 
14) Hermjacob U.: Parsing and Question Classification for Question Answering. Proceedings 
of the Association for Computational Linguists Workshop on Open-Domain Question An-
swering (2001) 17-22 
15) Ittycheriah, Franz M., Zhu W., Ratnaparki A. and Mammone R.: Question Answering Us-
ing Maximum Entropy Components. Proceedings of the North American chapter of the 
Association for Computational Linguistics (2001) 33-39  
Early Results for
Named Entity Recognition with Conditional Random Fields,
Feature Induction and Web-Enhanced Lexicons
Andrew McCallum and Wei Li
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003
{mccallum,weili}@cs.umass.edu
1 Introduction
Models for many natural language tasks benefit from the
flexibility to use overlapping, non-independent features.
For example, the need for labeled data can be drastically
reduced by taking advantage of domain knowledge in
the form of word lists, part-of-speech tags, character n-
grams, and capitalization patterns. While it is difficult to
capture such inter-dependent features with a generative
probabilistic model, conditionally-trained models, such
as conditional maximum entropy models, handle them
well. There has been significant work with such mod-
els for greedy sequence modeling in NLP (Ratnaparkhi,
1996; Borthwick et al, 1998).
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are undirected graphical models, a special case of
which correspond to conditionally-trained finite state ma-
chines. While based on the same exponential form as
maximum entropy models, they have efficient procedures
for complete, non-greedy finite-state inference and train-
ing. CRFs have shown empirical successes recently in
POS tagging (Lafferty et al, 2001), noun phrase segmen-
tation (Sha and Pereira, 2003) and Chinese word segmen-
tation (McCallum and Feng, 2003).
Given these models? great flexibility to include a wide
array of features, an important question that remains is
what features should be used? For example, in some
cases capturing a word tri-gram is important, however,
there is not sufficient memory or computation to include
all word tri-grams. As the number of overlapping atomic
features increases, the difficulty and importance of con-
structing only certain feature combinations grows.
This paper presents a feature induction method for
CRFs. Founded on the principle of constructing only
those feature conjunctions that significantly increase log-
likelihood, the approach builds on that of Della Pietra et
al (1997), but is altered to work with conditional rather
than joint probabilities, and with a mean-field approxi-
mation and other additional modifications that improve
efficiency specifically for a sequence model. In compari-
son with traditional approaches, automated feature induc-
tion offers both improved accuracy and significant reduc-
tion in feature count; it enables the use of richer, higher-
order Markov models, and offers more freedom to liber-
ally guess about which atomic features may be relevant
to a task.
Feature induction methods still require the user to cre-
ate the building-block atomic features. Lexicon member-
ship tests are particularly powerful features in natural lan-
guage tasks. The question is where to get lexicons that are
relevant for the particular task at hand?
This paper describes WebListing, a method that obtains
seeds for the lexicons from the labeled data, then uses the
Web, HTML formatting regularities and a search engine
service to significantly augment those lexicons. For ex-
ample, based on the appearance of Arnold Palmer in the
labeled data, we gather from the Web a large list of other
golf players, including Tiger Woods (a phrase that is dif-
ficult to detect as a name without a good lexicon).
We present results on the CoNLL-2003 named entity
recognition (NER) shared task, consisting of news arti-
cles with tagged entities PERSON, LOCATION, ORGANI-
ZATION and MISC. The data is quite complex; for exam-
ple the English data includes foreign person names (such
as Yayuk Basuki and Innocent Butare), a wide diversity of
locations (including sports venues such as The Oval, and
rare location names such as Nirmal Hriday), many types
of organizations (from company names such as 3M, to
acronyms for political parties such as KDP, to location
names used to refer to sports teams such as Cleveland),
and a wide variety of miscellaneous named entities (from
software such as Java, to nationalities such as Basque, to
sporting competitions such as 1,000 Lakes Rally).
On this, our first attempt at a NER task, with just a few
person-weeks of effort and little work on development-
set error analysis, our method currently obtains overall
English F1 of 84.04% on the test set by using CRFs, fea-
ture induction and Web-augmented lexicons. German F1
using very limited lexicons is 68.11%.
2 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et al, 2001)
are undirected graphical models used to calculate the con-
ditional probability of values on designated output nodes
given values assigned to other designated input nodes.
In the special case in which the output nodes of the
graphical model are linked by edges in a linear chain,
CRFs make a first-order Markov independence assump-
tion, and thus can be understood as conditionally-trained
finite state machines (FSMs). In the remainder of this
section we introduce the likelihood model, inference and
estimation procedures for CRFs.
Let o = ?o1, o2, ...oT ? be some observed input data
sequence, such as a sequence of words in text in a doc-
ument, (the values on n input nodes of the graphical
model). Let S be a set of FSM states, each of which
is associated with a label, l ? L, (such as ORG). Let
s = ?s1, s2, ...sT ? be some sequence of states, (the val-
ues on T output nodes). By the Hammersley-Clifford the-
orem, CRFs define the conditional probability of a state
sequence given an input sequence to be
P?(s|o) =
1
Zo
exp
(
T?
t=1
?
k
?kfk(st?1, st,o, t)
)
,
where Zo is a normalization factor over all state se-
quences, fk(st?1, st,o, t) is an arbitrary feature func-
tion over its arguments, and ?k is a learned weight for
each feature function. A feature function may, for exam-
ple, be defined to have value 0 in most cases, and have
value 1 if and only if st?1 is state #1 (which may have
label OTHER), and st is state #2 (which may have la-
bel LOCATION), and the observation at position t in o
is a word appearing in a list of country names. Higher ?
weights make their corresponding FSM transitions more
likely, so the weight ?k in this example should be pos-
itive. More generally, feature functions can ask pow-
erfully arbitrary questions about the input sequence, in-
cluding queries about previous words, next words, and
conjunctions of all these, and fk(?) can range ??...?.
CRFs define the conditional probability of a label
sequence based on total probability over the state se-
quences, P?(l|o) =
?
s:l(s)=l P?(s|o), where l(s) is
the sequence of labels corresponding to the labels of the
states in sequence s.
Note that the normalization factor, Zo, is the sum
of the ?scores? of all possible state sequences, Zo =
?
s?ST exp
(?T
t=1
?
k ?kfk(st?1, st,o, t)
)
, and that
the number of state sequences is exponential in the in-
put sequence length, T . In arbitrarily-structured CRFs,
calculating the normalization factor in closed form is
intractable, but in linear-chain-structured CRFs, as in
forward-backward for hidden Markov models (HMMs),
the probability that a particular transition was taken be-
tween two CRF states at a particular position in the input
sequence can be calculated efficiently by dynamic pro-
gramming. We define slightly modified forward values,
?t(si), to be the ?unnormalized probability? of arriving
in state si given the observations ?o1, ...ot?. We set ?0(s)
equal to the probability of starting in each state s, and
recurse:
?t+1(s) =
?
s?
?t(s
?) exp
(
?
k
?kfk(s
?, s,o, t)
)
.
The backward procedure and the remaining details of
Baum-Welch are defined similarly. Zo is then
?
s ?T (s).
The Viterbi algorithm for finding the most likely state
sequence given the observation sequence can be corre-
spondingly modified from its HMM form.
2.1 Training CRFs
The weights of a CRF, ?={?, ...}, are set to maximize the
conditional log-likelihood of labeled sequences in some
training set, D = {?o, l?(1), ...?o, l?(j), ...?o, l?(N)}:
L? =
N?
j=1
log
(
P?(l(j)|o(j))
)
?
?
k
?2k
2?2
,
where the second sum is a Gaussian prior over parameters
(with variance ?) that provides smoothing to help cope
with sparsity in the training data.
When the training labels make the state sequence un-
ambiguous (as they often do in practice), the likelihood
function in exponential models such as CRFs is con-
vex, so there are no local maxima, and thus finding the
global optimum is guaranteed. It has recently been shown
that quasi-Newton methods, such as L-BFGS, are signifi-
cantly more efficient than traditional iterative scaling and
even conjugate gradient (Malouf, 2002; Sha and Pereira,
2003). This method approximates the second-derivative
of the likelihood by keeping a running, finite-sized win-
dow of previous first-derivatives.
L-BFGS can simply be treated as a black-box opti-
mization procedure, requiring only that one provide the
first-derivative of the function to be optimized. Assum-
ing that the training labels on instance j make its state
path unambiguous, let s(j) denote that path, and then the
first-derivative of the log-likelihood is
?L
??k
=
?
?
N?
j=1
Ck(s(j),o(j))
?
??
?
?
N?
j=1
?
s
P?(s|o(j))Ck(s,o(j))
?
??
?k
?2
where Ck(s,o) is the ?count? for feature k given s
and o, equal to
?T
t=1 fk(st?1, st,o, t), the sum of
fk(st?1, st,o, t) values for all positions, t, in the se-
quence s. The first two terms correspond to the differ-
ence between the empirical expected value of feature fk
and the model?s expected value: (E?[fk]?E?[fk])N . The
last term is the derivative of the Gaussian prior.
3 Efficient Feature Induction for CRFs
Typically the features, fk, are based on some number of
hand-crafted atomic observational tests (such as word is
capitalized or word is ?said?, or word appears in lexi-
con of country names), and a large collection of features
is formed by making conjunctions of the atomic tests in
certain user-defined patterns; (for example, the conjunc-
tions consisting of all tests at the current sequence po-
sition conjoined with all tests at the position one step
ahead?specifically, for instance, current word is capi-
talized and next word is ?Inc?). There can easily be
over 100,000 atomic tests (mostly based on tests for the
identity of words in the vocabulary), and ten or more
shifted-conjunction patterns?resulting in several million
features (Sha and Pereira, 2003). This large number of
features can be prohibitively expensive in memory and
computation; furthermore many of these features are ir-
relevant, and others that are relevant are excluded.
In response, we wish to use just those time-shifted
conjunctions that will significantly improve performance.
We start with no features, and over several rounds of fea-
ture induction: (1) consider a set of proposed new fea-
tures, (2) select for inclusion those candidate features that
will most increase the log-likelihood of the correct state
path s(j), and (3) train weights for all features. The pro-
posed new features are based on the hand-crafted obser-
vational tests?consisting of singleton tests, and binary
conjunctions of tests with each other and with features
currently in the model. The later allows arbitrary-length
conjunctions to be built. The fact that not all singleton
tests are included in the model gives the designer great
freedom to use a very large variety of observational tests,
and a large window of time shifts.
To consider the effect of adding a new feature, define
the new sequence model with additional feature, g, hav-
ing weight ?, to be
P?+g,?(s|o) =
P?(s|o) exp
(?T
t=1 ? g(st?1, st,o, t)
)
Zo(?, g, ?)
;
Zo(?, g, ?)
def
=
?
s? P?(s
?|o) exp(
?T
t=1 ? g(s
?
t?1, s
?
t,o, t))
in the denominator is simply the additional portion of
normalization required to make the new function sum to
1 over all state sequences.
Following (Della Pietra et al, 1997), we efficiently as-
sess many candidate features in parallel by assuming that
the ? parameters on all included features remain fixed
while estimating the gain, G(g), of a candidate feature, g,
based on the improvement in log-likelihood it provides,
G?(g) = max
?
G?(g, ?) = max
?
L?+g? ? L?.
where L?+g? includes ??2/2?2.
In addition, we make this approach tractable for CRFs
with two further reasonable and mutually-supporting ap-
proximations specific to CRFs. (1) We avoid dynamic
programming for inference in the gain calculation with
a mean-field approximation, removing the dependence
among states. (Thus we transform the gain from a se-
quence problem to a token classification problem. How-
ever, the original posterior distribution over states given
each token, P?(s|o) = ?t(s|o)?t+1(s|o)/Zo, is still
calculated by dynamic programming without approxima-
tion.) Furthermore, we can calculate the gain of aggre-
gate features irrespective of transition source, g(st,o, t),
and expand them after they are selected. (2) In many
sequence problems, the great majority of the tokens are
correctly labeled even in the early stages of training. We
significantly gain efficiency by including in the gain cal-
culation only those tokens that are mislabeled by the cur-
rent model. Let {o(i) : i = 1...M} be those tokens, and
o(i) be the input sequence in which the ith error token
occurs at position t(i). Then algebraic simplification us-
ing these approximations and previous definitions gives
G?(g, ?) =
M?
i=1
log
(
exp
(
? g(st(i),o(i), t(i))
)
Zo(i)(?, g, ?)
)
?
?2
2?2
= M?E?[g] ?
M?
i=1
log(E?[exp(? g)|o(i)] ?
?2
2?2
,
where Zo(i)(?, g, ?) (with non-bold o) is simply?
s P?(s|o(i)) exp(?g(s,o(i), t(i))). The optimal val-
ues of the ??s cannot be solved in closed form, but New-
ton?s method finds them all in about 12 quick iterations.
There are two additional important modeling choices:
(1) Because we expect our models to still require sev-
eral thousands of features, we save time by adding many
of the features with highest gain each round of induction
rather than just one; (including a few redundant features
is not harmful). (2) Because even models with a small se-
lect number of features can still severely overfit, we train
the model with just a few BFGS iterations (not to con-
vergence) before performing the next round of feature in-
duction. Details are in (McCallum, 2003).
4 Web-augmented Lexicons
Some general-purpose lexicons, such a surnames and lo-
cation names, are widely available, however, many nat-
ural language tasks will benefit from more task-specific
lexicons, such as lists of soccer teams, political parties,
NGOs and English counties. Creating new lexicons en-
tirely by hand is tedious and time consuming.
Using a technique we call WebListing, we build lexi-
cons automatically from HTML data on the Web. Previ-
ous work has built lexicons from fixed corpora by deter-
mining linguistic patterns for the context in which rele-
vant words appear (Collins and Singer, 1999; Jones et al,
1999). Rather than mining a small corpus, we gather data
from nearly the entire Web; rather than relying on fragile
linguistic context patterns, we leverage robust formatting
regularities on the Web. WebListing finds co-occurrences
of seed terms that appear in an identical HTML format-
ting pattern, and augments a lexicon with other terms on
the page that share the same formatting. Our current im-
plementation uses GoogleSets, which we understand to
be a simple implementation of this approach based on us-
ing HTML list items as the formatting regularity. We are
currently building a more sophisticated replacement.
5 Results
To perform named entity extraction on the news articles
in the CoNLL-2003 English shared task, several families
of features are used, all time-shifted by -2, -1, 0, 1, 2: (a)
the word itself, (b) 16 character-level regular expressions,
mostly concerning capitalization and digit patterns, such
as A, A+, Aa+, Aa+Aa*, A., D+, where A, a and D indi-
cate the regular expressions [A-Z], [a-z] and [0-9],
(c) 8 lexicons entered by hand, such as honorifics, days
and months, (d) 15 lexicons obtained from specific web
sites, such as countries, publicly-traded companies, sur-
names, stopwords, and universities, (e) 25 lexicons ob-
tained by WebListing (including people names, organi-
zations, NGOs and nationalities), (f) all the above tests
with prefix firstmention from any previous duplicate of
the current word, (if capitalized). A small amount of
hand-filtering was performed on some of the WebList-
ing lexicons. Since GoogleSets? support for non-English
is severely limited, only 5 small lexicons were used for
German; but character bi- and tri-grams were added.
A Java-implemented, first-order CRF was trained for
about 12 hours on a 1GHz Pentium with a Gaussian prior
variance of 0.5, inducing 1000 or fewer features (down
to a gain threshold of 5.0) each round of 10 iterations of
L-BFGS. Candidate conjunctions are limited to the 1000
atomic and existing features with highest gain. Perfor-
mance results for each of the entity classes can be found
in Figure 1. The model achieved an overall F1 of 84.04%
on the English test set using 6423 features. (Using a set
of fixed conjunction patterns instead of feature induction
results in F1 73.34%, with about 1 million features; trial-
and-error tuning the fixed patterns would likely improve
this.) Accuracy gains are expected from experimentation
with the induction parameters and improved WebListing.
Acknowledgments
We thank John Lafferty, Fernando Pereira, Andres Corrada-
Emmanuel, Drew Bagnell and Guy Lebanon, for helpful
input. This work was supported in part by the Center
for Intelligent Information Retrieval, SPAWARSYSCEN-SD
grant numbers N66001-99-1-8912 and N66001-02-1-8903, Ad-
vanced Research and Development Activity under contract
number MDA904-01-C-0984, and DARPA contract F30602-
01-2-0566.
References
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998.
Exploiting diverse knowledge sources via maximum entropy
in named entity recognition. In Proceedings of the Sixth
Workshop on Very Large Corpora, Association for Compu-
tational Linguistics.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the Joint SIG-
DAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Stephen Della Pietra, Vincent J. Della Pietra, and John D. Laf-
ferty. 1997. Inducing Features of Random Fields. IEEE
English devel. Precision Recall F?=1
LOC 93.82% 91.78% 92.79
MISC 83.99% 78.52% 81.17
ORG 84.23% 82.03% 83.11
PER 92.64% 93.65% 93.14
Overall 89.84% 88.10% 88.96
English test Precision Recall F?=1
LOC 87.23% 87.65% 87.44
MISC 74.44% 71.37% 72.87
ORG 79.52% 78.33% 78.92
PER 91.05% 89.98% 90.51
Overall 84.52% 83.55% 84.04
German devel. Precision Recall F?=1
LOC 68.55% 68.84% 68.69
MISC 72.66% 45.25% 55.77
ORG 70.64% 54.88% 61.77
PER 82.21% 64.31% 72.17
Overall 73.60% 59.01% 65.50
German test Precision Recall F?=1
LOC 71.92% 69.28% 70.57
MISC 69.59% 42.69% 52.91
ORG 63.85% 48.90% 55.38
PER 90.04% 74.14% 81.32
Overall 75.97% 61.72% 68.11
Table 1: English and German named entity extraction.
Transactions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
Rosie Jones, Andrew McCallum, Kamal Nigam, and Ellen
Riloff. 1999. Bootstrapping for Text Learning Tasks. In
IJCAI-99 Workshop on Text Mining: Foundations, Tech-
niques and Applications.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. ICML.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Sixth Workshop on
Computational Language Learning (CoNLL-2002).
Andrew McCallum and Fang-Fang Feng. 2003. Chinese
Word Segmentation with Conditional Random Fields and In-
tegrated Domain Knowledge. In Unpublished Manuscript.
Andrew McCallum. 2003. Efficiently Inducing Features of
Conditional Random Fields. In Nineteenth Conference on
Uncertainty in Artificial Intelligence (UAI03). (Submitted).
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Eric Brill and Kenneth Church,
editors, Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 133?142. Asso-
ciation for Computational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of Human Lan-
guage Technology, NAACL.
A Question Answering System Supported by Information Extraction* 
Rohini Srihari 
Cymfony Inc. 
5500 Main Street 
Williamsville, NY 14221 
rohini@cymfony.com 
Wei Li 
Cymfony Inc. 
5500 Main Street 
Williamsville, NY14221 
wei@cymfony.com 
Abstract 
This paper discusses an information 
extraction (IE) system, Textract, in natural 
language (NL) question answering (QA) and 
examines the role of IE in QA application. It 
shows: (i) Named Entity tagging is an 
important component for QA, (ii) an NL 
shallow parser provides a structural basis for 
questions, and (iii) high-level domain 
independent IE can result in a QA 
breakthrough. 
Introduction 
With the explosion of information in Internet, 
Natural language QA is recognized as a 
capability with great potential. Traditionally, 
QA has attracted many AI researchers, but most 
QA systems developed are toy systems or games 
confined to lab and a very restricted omain. 
More recently, Text Retrieval Conference 
(TREC-8) designed a QA track to stimulate the 
research for real world application. 
Due to little linguistic support from text 
analysis, conventional IR systems or search 
engines do not really perform the task of 
information retrieval; they in fact aim at only 
document retrieval. The following quote from the 
QA Track Specifications (www.research.att.com/ 
-singhal/qa-track-spec.txt) in the TREC 
community illustrates this point. 
Current information retrieval systems allow 
us to locate documents hat might contain the 
pertinent information, but most of them leave 
it to the user to extract he useful information 
from a ranked list. This leaves the (often 
unwilling) user with a relatively large 
amount of text o consume. There is an urgent 
need for tools that would reduce the amount 
of text one might have to read in order to 
obtain the desired information. This track 
aims at doing exactly that for a special (and 
popular) class of information seeking 
behavior: QUESTION ANSWERING. People 
have questions and they need answers, not 
documents. Automatic question answering 
will definitely be a significant advance in the 
state-of-art information retrieval technology. 
Kupiec (1993) presented a QA system 
MURAX using an on-line encyclopedia. This 
system used the technology of robust shallow 
parsing but suffered from the lack of basic 
information extraction support. In fact, the most 
siginifcant IE advance, namely the NE (Named 
Entity) technology, occured after Kupiec (1993), 
thanks to the MUC program (MUC-7 1998). 
High-level IE technology beyond NE has not 
been in the stage of possible application until 
recently. 
AskJeeves launched a QA portal 
(www.askjeeves.com). It is equipped with a 
fairly sophisticated natural language question 
parser, but it does not provide direct answers to 
the asked questions. Instead, it directs the user to 
the relevant web pages, just as the traditional 
search engine does. In this sense, AskJeeves has 
only done half of the job for QA. 
We believe that QA is an ideal test bed for 
demonstrating the power of IE. There is a natural 
co-operation between IE and IR; we regard QA 
as one major intelligence which IE can offer IR. 
* This work was supported in part by the SBIR grants F30602-98-C-0043 and F30602-99-C-0102 from Air Force 
Research Laboratory (AFRL)/IFED. 
166 
An important question then is, what type of 
IE can support IR in QA and how well does it 
support it? This forms the major topic of this 
paper. We structure the remaining part of the 
paper as follows. In Section 1, we first give an 
overview of the underlying IE technology which 
our organization has been developing. Section 2 
discusses the QA system. Section 3 describes the 
limitation of the current system. Finally, in 
Section 4, we propose a more sophisticated QA 
system supported by three levels of IE. 
1 Overview of Textract IE 
The last decade has seen great advance and 
interest in the area of IE. In the US, the DARPA 
sponsored Tipster Text Program \[Grishman 
1997\] and the Message Understanding 
Conferences (MUC) \[MUC-7 1998\] have been 
the driving force for developing this technology. 
In fact, the MUC specifications for various IE 
tasks have become de facto standards in the IE 
research community. It is therefore necessary to 
present our IE effort in the context of the MUC 
program. 
MUC divides IE into distinct tasks, 
namely, NE (Named Entity), TE (Template 
Element), TR (Template Relation), CO 
(Co-reference), and ST (Scenario Templates) 
\[Chinchor & Marsh 1998\]. Our proposal for 
three levels of IE is modelled after the MUC 
standards using MUC-style representation. 
However, we have modified the MUC IE task 
definitions in order to make them more useful 
and more practical. More precisely, we propose a
hierarchical, 3-level architecture for developing a 
kernel IE system which is domain-independent 
throughout. 
The core of this system is a state-of-the-art 
NE tagger \[Srihari 1998\], named Textract 1.0. 
The Textract NE tagger has achieved speed and 
accuracy comparable tothat of the few deployed 
NE systems, such as NetOwl \[Krupka & 
Hausman 1998\] and Nymble \[Bikel et al1997\]. 
It is to be noted that in our definition of NE, 
we significantly expanded the type of 
information to be extracted. In addition to all the 
MUC defined NE types (person, organization, 
location, time, date, money and percent), the 
following types/sub-types of information are also 
identified by the TextractNE module: 
? duration, frequency, age 
? number, fraction, decimal, ordinal, math 
equation 
? weight, length, temperature, angle, area, 
capacity, speed, rate 
? product, software 
? address, email, phone, fax, telex, www 
? name (default proper name) 
Sub-type information like company, 
government agency, school (belonging to the 
type organization) and military person, religious 
person (belonging to person) are also identified. 
These new sub-types provide a better foundation 
for defining multiple relationships between the 
identified entities and for supporting question 
answering functionality. For example, the key to 
a question processor is to identify the asking 
point (who, what, when, where, etc.). In many 
cases, the asking point corresponds to an NE 
beyond the MUC definition, e.g. the 
how+adjective questions: how long (duration or 
length), how far (length), how often (frequency), 
how old (age), etc. 
Level-2 IE, or CE (Correlated Entity), is 
concerned with extracting pre-defined multiple 
relationships between the entities. Consider the 
person entity as an example; the TextractCE 
prototype is capable of extracting the key 
relationships uch as age, gender, affiliation, 
position, birthtime, birth__place, spouse, 
parents, children, where.from, address, phone, 
fax, email, descriptors. As seen, the information 
in the CE represents a mini-CV or profile of the 
entity. In general, the CE template integrates and 
greatly enriches the information contained in 
MUC TE and TR. 
The final goal of our IE effort is to further 
extract open-ended general events (GE, or level 3 
IE) for information like who did what (to whom) 
when (or how often) and where. By general 
events, we refer to argument structures centering 
around verb notions plus the associated 
information of time/frequency and location. We 
show an example of our defined GE extracted 
from the text below: 
Julian Hill, a research chemist whose 
accidental discovery of a tough, taffylike 
compound revolutionized everyday life after 
it proved its worth in warfare and courtship, 
167 
died on Sunday in Hockessin, Del. 
\[1\] <GE_TEMPLATE> := 
PREDICATE: die 
ARGUMENTI: Julian Hill 
TIME: Sunday 
LOCATION: Hockessin, Del 
Figure 1 is the overall system architecture for 
the IE system Textract hat our organization has 
been developing. 
Kernet IE Modutes  L|ngui_sti_cLM_odu!es 
I . . . . . . . . . . . . .  I I . . . . . . . . . .  I 
I I I I ,,l l I , ! I 
I I I I 
'l J ' I I 
! I 
I I 
I I I I ,i I i ', I , 
I ! I I , l ? i  , , 
I I I I 
I I 
F L - -  - - - -~  . . . .  . L - -  - -  - -  . - -  - -  - -  | . . . .  
Apptication Modutes 
NE: NIiml~ EnlilyTitl~klll QA: Que~tlon Answering 
CE: Come,led Entity ExtrmClkm BR: In~lllgenl ~ws lng  
GE: Gcn~mI Evenl Ex~ct~on AS; Auio~ SUl len  
co :  ce-  mfcmnc ~1 momial l  s ~ p~ 
Figure 1: Textract IE System Architecture 
The core of the system consists of three 
kernel IE modules and six linguistic modules. 
The multi-level linguistic modules erve as an 
underlying support system for different levels of 
IE. The IE results are stored in a database which 
is the basis for IE-related applications like QA, 
BR (Browsing, threading and visualization) and 
AS (Automatic Summarization). The approach 
to IE taken here, consists of a unique blend of 
machine learning and FST (finite state 
transducer) rule-based system \[Roche & Schabes 
1997\]. By combining machine learning with an 
FST rule-based system, we are able to exploit he 
best of both paradigms while overcoming their 
respective weaknesses \[Srihari 1998, Li & Srihari 
2000\]. 
2 NE-Supported QA 
This section presents the QA system based on 
Named Entity tagging. Out of the 200 questions 
that comprised the TREC-8 QA track 
competition, over 80% asked for an NE, e.g. who 
(PERSON), when (T IME\ [  DATE), where 
(LOCATION), how far (LENGTH). Therefore, 
the NE tagger has been proven to be very helpful. 
Of course, the NE of the targeted type is only 
necessary but not complete in answering such 
questions because NE by nature only extracts 
isolated individual entities from the text. 
Nevertheless, using even crude methods like "the 
nearest NE to the queried key words" or "the NE 
and its related key words within the same line (or 
same paragraph, etc.)", in most cases, the QA 
system was able to extract ext portions which 
contained answers in the top five list. 
Figure 2 illustrates the system design of 
TextractQA Prototype. There are two 
components for the QA prototype: Question 
Processor and Text Processor. The Text Matcher 
module links the two processing results and tries 
to find answers to the processed question. 
Matching is based on keywords, plus the NE 
type and their common location within a same 
sentence. 
Quest ion  Prc~:essor 
i 
: :eXt P r~_~ . . . . . . . . . . . .  ? ~  
i i ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
i . . . .  i 
Figure 2: Textract/QA 1.0 Prototype Architecture 
The general algorithm for question 
answering is as follows: 
168 
Process Question 
Shallow parse question 
Determine Asking Point 
Question expansion (using word lists) 
Process Documents 
Tokenization, POS tagging, NE Indexing 
Shallow Parsing (not yet utilized) 
Text Matcher 
Intersect search engine results with NE 
rank answers 
2.1 Question Processing 
The Question Processing results are a list of 
keywords plus the information for asking point. 
For example, the question: 
\[2\] Who won the 1998 Nobel Peace Prize? 
contains the following keywords: won, 1998, 
Nobel, Peace, Prize. The asking point Who refers 
to the NE type person. The output before 
question expansion is a simple 2-feature template 
as shown below: 
\[3\] asking_point: PERSON 
key_word: { won, 1998, Nobel, 
Peace, Prize } 
The following is an example where the 
asking point does not correspond to any type of 
NE in our definition. 
\[3\] Why did David Koresh ask the FBI for a 
word processor ? 
The system then maps it to the following 
question template : 
\[4\] asking_point: 
key_word: 
REASON 
{ ask, David, Koresh, 
FBI, word, processor }
The question processor scans the question to 
search for question words (wh-words) and maps 
them into corresponding NE types/sub-types or 
pre-defined notions like REASON. 
We adopt wo sets of pattern matching rules 
for this purpose: (i) structure based pattern 
matching rules; (ii) simple key word based 
pattern matching rules (regarded as default rules). 
It is fairly easy to exhaust the second set of rules 
as interrogative question words/phrases form a 
closed set. In comparison, the development of
the first set of rules are continuously being 
fine-tuned and expanded. This strategy of using 
two set of rules leads to the robustness of the 
question processor. 
The first set of rules are based on shallow 
parsing results of the questions, using Cymfony 
FST based Shallow Parser. This parser identifies 
basic syntactic onstructions like BaseNP (Basic 
Noun Phrase), BasePP (Basic Prepositional 
Phrase) and VG (Verb Group). 
The following is a sample of the first set of 
rules: 
\[6\] Name NP (city I country I company) --> 
CITYICOUNTRYICOMPANY 
\[7\] Name NP(person_w) --> PERSON 
\[8\] Name NP(org_w) --> ORGANIZATION 
\[9\] Name NP(NOT person_w, NOT org_w) 
--> NAME 
Rule \[6\] checks the head word of the NP. It 
covers cases like VG\[Name\] NP\[a country\] that 
VG\[is developing\] NP\[a magnetic levitation 
railway system\]. Rule \[7\] works for cases like 
VG\[Name\] NP\[the first private citizen\] VG\[to 
fly\] PP\[in space\] as citizen belongs to the word 
class person_w. Rule \[9\] is a catch-all rule: if the 
NP is not of class person (person_w) or 
organization (org_w), then the asking point is a 
proper name (default NE), often realized in 
English in capitalized string of words. Examples 
include Name a film that has won the Golden 
Bear in the Berlin Film Festival. 
The word lists org_w and person_w are 
currently manually maintained based on 
inspection of large volumes of text. An effort is 
underway to automate the learning of such word 
lists by utilizing machine learning techniques. 
We used the following pattern 
transformations to expand our ruleset: 
(Please) name NP\[X\] 
--> what/which Aux(be) (the name of) NP\[X\] 
--> NP(what/which...X) 
In other words, the four rules are expanded to 
12 rules. For example, Rule \[10\] below 
corresponds to Rule \[6\]; Rule \[11\] is derived 
169 
from Rule \[7\]. 
\[10\] what/which Aux(be) NP (city \[ country \[ 
company) --> 
CITY I COUNTRY \[ COMPANY 
\[11\] NP(what/which ... person_w) --> 
PERSON 
Rule \[10\] extracts the asking point from 
cases like NP\[What\] Aux\[is\] NP\[the largest 
country\] PP\[in the world\]. Rule \[11\] covers the 
following questions: NP\[What costume 
designer\] VG\[decided\] that NP\[Michael 
Jacksonl VG\[should only wear\] NP\[one glove\], 
NP\[Which former Ku Klux Klan member\] 
VG\[won\] NP\[an elected office\] PP\[in the U.S.\], 
NP\[What Nobel laureate\] VG\[was expelled\] 
PP\[from the Philippines\] PP\[before the 
conference\] PP\[on East Timor\], NP\[What 
famous communist leader\] VG\[died\] PP\[in 
Mexico City\], etc. 
As seen, shallow parsing helps us to capture a
variety of natural anguage question expressions. 
However, there are cases where some simple key 
word based pattern matching would be enough to 
capture the asking point. That is our second set 
of rules. These rules are used when the first set of 
rules has failed to produce results. The following 
is a sample of such rules: 
\[ 12\] who/whom --> PERSON 
\[13\] when --> TIME/DATE 
\[14\] where/what place --> LOCATION 
\[15\] what time (of day) --> TIME 
\[16\] what day (of the week) --> DAY 
\[17\] what/which month --> MONTH 
\[18\] what age/how old --> AGE 
\[19\] what brand --> PRODUCT 
\[20\] what --> NAME 
\[21\] how far/tall/high --> LENGTH 
\[22\] how large/hig/small --> AREA 
\[23\] how heavy --> WEIGHT 
\[24\] how rich --> MONEY 
\[25\] how often --> FREQUENCY 
\[26\] how many --> NUMBER 
\[27\] how long --> LENGTH/DURATION 
\[28\] why/for what --> REASON 
In the stage of question expansion, the 
template in \[4\] would be expanded to the 
template shown in \[29\]: 
\[29\] asking_point: 
key_word: 
{because{because of\] 
due to{thanks to{since I 
in order{to VB} 
{ asklaskslasked\[asking, 
David,Koresh,FBI, 
word, processor} 
The last item in the asking._point list attempts 
to find an infinitive by checking the word to 
followed by a verb (with the part-of-speech tag 
VB). As we know, infinitive verb phrases are 
often used in English to explain a reason for some 
action. 
2.2 Text Processing 
On the text processing side, we first send the 
question directly to a search engine in order to 
narrow down the document pool to the first n, say 
200, documents for IE processing. Currently, 
this includes tokenization, POS tagging and NE 
tagging. Future plans include several evels of 
parsing as well; these are required to support CE 
and GE extraction. It should be noted that all 
these operations are extremely robust and fast, 
features necessary for large volume text 
indexing. Parsing is accomplished through 
cascaded finite state transducer grammars. 
2.3 Text Matching 
The Text Matcher attempts to match the question 
template with the processed ocuments for both 
the asking point and the key words. There is a 
preliminary ranking standard built-in the matcher 
in order to find the most probable answers. The 
primary rank is a count of how many unique 
keywords are contained within a sentence. The 
secondary ranking is based on the order that the 
keywords appear in the sentence compared to 
their order in the question. The third ranking is 
based on whether there is an exact match or a 
variant match for the key verb. 
In the TREC-8 QA track competition, 
Cymfony QA accuracy was 66.0%. Considering 
we have only used NE technology to support QA 
in this run, 66.0% is a very encouraging result. 
3 Limitation 
The first limitation comes from the types of 
questions. Currently only wh-questions are 
handled although it is planned that yes-no 
questions will be handled once we introduce CE 
170 
and GE templates to support QA. Among the 
wh-questions, the why-question and 
how-question t are more challenging because the 
asking point cannot be simply mapped to the NE 
types/sub-types. 
The second limitation is from the nature of 
the questions. Questions like Where can l find 
the homepage for Oscar winners or Where can I 
find info on Shakespeare's works might be 
answerable asily by a system based on a 
well-maintained data base of home pages. Since 
our system is based on the processing of the 
underlying documents, no correct answer can be 
provided if there is no such an answer (explicitly 
expressed in English) in the processed 
documents. In TREC-8 QA, this is not a problem 
since every question is guaranteed to have at least 
one answer in the given document pool. 
However, in the real world scenario such as a QA 
portal, it is conceived that the IE results based on 
the processing of the documents hould be 
complemented by other knowledge sources uch 
as e-copy of yellow pages or other manually 
maintained and updated ata bases. 
The third limitation is the lack of linguistic 
processing such as sentence-level parsing and 
cross-sentential co-reference (CO). This problem 
will be gradually solved when high-level IE 
technology is introduced into the system. 
4 Future Work: Multi-level IE Supported QA 
A new QA architecture is under development; i  
will exploit all levels of the IE system, including 
CE and GE. 
The first issue is how much CE can 
contribute to a better support of QA. It is found 
that there are some frequently seen questions 
which can be better answered once the CE 
information is provided. These questions are of 
two types: (i) what/who questions about an NE; 
(ii) relationship questions. 
Questions of the following format require CE 
templates as best answers: who/what is NE? For 
example, Who is Julian Hill? Who is Bill 
Clinton? What is Du Pont? What is Cymfony? 
To answer these questions, the system can simply 
1 For example, How did one make a chocolate cake? 
How+Adjective questions (e.g. how long, how big, 
how old, etc.) are handled fairly well. 
retrieve the corresponding CE template to 
provide an "assembled" answer, as shown below. 
Q: Who is Julian Hill? 
A: name: Julian Werner Hill 
type: PERSON 
age: 91 
gender: MALE 
position: research chemist 
affiliation: Du Pont Co. 
education: Washington University; 
MIT 
Q: What is Du Pont? 
A: name: Du Pont Co, 
type: COMPANY 
staff: Julian Hill; Wallace Carothers. 
Questions specifically about a CE 
relationship include: For which company did 
Julian Hill work? (affiliation relationship) Who 
are employees of Du Pont Co.? (staff 
relationship) What does Julian Hill do? 
(position/profession relationship) Which 
university did Julian Hill graduate from? 
(education relationship), etc. 2 
The next issue is the relationships between 
GE and QA. It is our belief that the GE 
technology will result in a breakthrough for QA. 
In order to extract GE templates, the text 
goes through a series of linguistic processing as 
shown in Figure 1. It should be noted that the 
question processing is designed to go through 
parallel processes and share the same NLP 
resources until the point of matching and ranking. 
The merging of question templates and GE 
templates in Template Matcher are fairly 
straightforward. As they both undergo the same 
NLP processing, the resulting semantic templates 
are of the same form. Both question templates 
and GE templates correspond to fairly 
standard/predictable patterns (the PREDICATE 
value is open-ended, but the structure remains 
stable). More precisely, a user can ask questions 
on general events themselves (did what) and/or 
on the participants of the event (who, whom, 
what) and/or the time, frequency and place of 
events (when, how often, where). This addresses 
2 An alpha version of TextractQA supported by both 
NE and CE has been implemented and is being tested. 
171 
by far the most types of general questions of a 
potential user. 
For example, if a user is interested in 
company acquisition events, he can ask questions 
like: Which companies ware acquired by 
Microsoft in 1999? Which companies did 
Microsoft acquire in 1999? Our system will then 
parse these questions into the templates as shown 
below: 
\[31\] <Q_TEMPLATE> := 
PREDICATE: acquire 
ARGUMENT1: Microsoft 
ARGUMENT2: WHAT(COMPANY) 
TIME: 1999 
If the user wants to know when some 
acquisition happened, he can ask: When was 
Netscape acquired? Our system will then 
translate it into the pattern below: 
\[32\] <QTEMPLATE> := 
PREDICATE: acquire 
ARGUMENT1: WHO 
ARGUMENT2: Netscape 
TIME: WHEN 
Note that WHO, WHAT, WHEN above are 
variable to be instantiated. Such question 
templates serve as search constraints o filter the 
events in our extracted GE template database. 
Because the question templates and the extracted 
GE template share the same structure, a simple 
merging operation would suffice. Nevertheless, 
there are two important questions to be answered: 
(i) what if a different verb with the same meaning 
is used in the question from the one used in the 
processed text? (ii) what if the question asks 
about something beyond the GE (or CE) 
information? These are issues that we are 
currently researching. 
References 
Bikel D.M. et al (1997) Nymble: aHigh-Performance 
Learning Name-finder. "Proceedings of the Fifth 
Conference on Applied Natural Language 
Processing", Morgan Kaufmann Publishers, pp. 
194-201 
Chinchor N. and Marsh E. (1998) MUC- 7 Information 
Extraction Task Definition (version 5.1), 
"Proceedings ofMUC-7". 
Grishman R. (1997) TIPSTER Architecture Design 
Document Version 2.3. Technical report, DARPA 
Krupka G.R. and Hausman K. (1998) IsoQuest Inc.: 
Description of the NetOwl (TM) Extractor System 
as Used for MUC-7, "Proceedings ofMUC-7". 
Kupiec J. (1993) MURAX: A Robust Linguistic 
Approach For Question Answering Using An 
On-Line Encyclopaedia, "Proceedings of 
SIGIR-93 93" Pittsburgh, Penna. 
Li, W & Srihari, R. 2000. Flexible Information 
Extraction Learning Algorithm, Final Technical 
Report, Air Force Research Laboratory, Rome 
Research Site, New York 
MUC-7 (1998) Proceedings of the Seventh Message 
Understanding Conference (MUC-7), published on 
the website _http://www.muc.saic.com/ 
Roche E. and Schabes Y. (1997) Finite-State 
Language Processing, MIT Press, Cambridge, MA 
Srihari R. (1998) A Domain Independent Event 
Extraction Toolkit, AFRL-IF-RS-TR-1998-152 
Final Technical Report, Air Force Research 
Laboratory, Rome Research Site, New York 
172 
A Hybrid Approach for Named Entity and Sub-Type Tagging* 
Rohini Srihari 
Cymfony Net, Inc. 
5500 Main Street 
Williamsville, NY 14260 
rohini @ cymfony.com 
Cheng Niu and Wei Li 
Cymfony Net, Inc. 
5500 Main Street 
Williamsville, NY 14260 
chengniu@cymfony.com 
wei@cymfony.com 
Abstract 
This paper presents a hybrid approach for 
named entity (NE) tagging which combines 
Maximum Entropy Model (MaxEnt), Hidden 
Markov Model (HMM) and handcrafted 
grammatical rules. Each has innate strengths 
and weaknesses; the combination results in a 
very high precision tagger. MaxEnt includes 
external gazetteers in the system. Sub-category 
generation is also discussed. 
Introduction 
Named entity (NE) tagging is a task in which 
location names, person names, organization 
names, monetary amounts, time and percentage 
expressions are recognized and classified in 
unformatted text documents. This task provides 
important semantic information, and is a critical 
first step in any information extraction system. 
Intense research has been focused on 
improving NE tagging accuracy using several 
different echniques. These include rule-based 
systems \[Krupka 1998\], Hidden Markov Models 
(HMM) \[Bikel et al 1997\] and Maximum 
Entropy Models (MaxEnt) \[Borthwick 1998\]. A 
system based on manual rules may provide the 
best performance; however these require 
painstaking intense skilled labor.. Furthermore, 
shifting domains involves significant effort and 
may result in performance degradation. The 
strength of HMM models lie in their capacity for 
modeling local contextual information. HMMs 
have been widely used in continuous peech 
recognition, part-of-speech tagging, OCR, etc., 
and are generally regarded as the most successful 
statistical modelling paradigm in these domains. 
MaxEnt is a powerful tool to be used in situations 
where several ambiguous information sources 
need to be combined. Since statistical techniques 
such as HMM are only as good as the data they 
are trained on, they are required to use back-off 
models to compensate for unreliable statistics. In  
contrast o empirical back-off models used in 
HMMs, MaxEnt provides a systematic method 
by which a statistical model consistent with all 
obtained knowledge can be trained. \[Borthwick 
et al 1998\] discuss atechnique for combining the 
output of several NE taggers in a black box 
fashion by using MaxEnt. They demonstrate the 
superior performance of this system; however, 
the system is computationally inefficient since 
many taggers need to be run. 
In this paper we propose ahybrid method for 
NE tagging which combines all the modelling 
techniques mentioned above. NE tagging is a 
complex task and high-performance systems are 
required in order to be practically usable. 
Furthermore, the task demonstrates 
characteristics that can be exploited by all three 
techniques. For example, time and monetary 
expressions are fairly predictable and hence 
processed most efficiently with handcrafted 
grammar rules. Name, location and organization 
entities are highly variable and thus lend 
themselves tostatistical training algorithms such 
as HMMs. Finally, many conflicting pieces of 
information regarding the class of a tag are 
* This work was supported in part by the SBIR grant F30602-98-C-0043 from Air Force Research Laboratory 
(AFRL)/IFED. 
247 
frequently present. This includes information 
from less than perfect gazetteers. For this, a 
MaxEnt approach works well in utilizing diverse 
sources of information in determining the final 
tag. The structure of our system is shown in 
Figure 1. 
I I 
I to=~m, I~mm 
I I 
l I o , ,=- . . , I  
I MJE3;,= G,,: ;,IE 
1 
I~1 Sb~lued hE Tasp" 
The first module is a rule-based tagger 
containing pattern match rules, or templates, for 
time, date, percentage, and monetary 
expressions. These tags include the standard 
MUC tags \[Chinchor 1998\], as well as several 
other sub-categories defined by our organization. 
More details concerning the sub-categories are 
presented later. The pattern matcher is based on 
Finite State Transducer (FST) technology 
\[Roches & Schabes 1997\] that has been 
implemented in-house. The subsequent modules 
are focused on location, person and organization 
names. The second module assigns tentative 
person and location tags based on external person 
and location gazetteers. Rather than relying on 
simple lookup of the gazetteer which is very error 
prone, this module employs MaxEnt to build a 
statistical model that incorporates gazetteers with 
common contextual information. The core 
module of the system is a bigram-based HMM 
\[Bikel et a1.1997\]. Rules designed to correct 
errors in NE segmentation are incorporated into a 
constrained HMM network. These rules serve as 
constraints on the HMM model and enable it to 
utilize information beyond bigrams and remove 
obvious errors due to the limitation of the training 
corpus. HMM generates the standard MUC tags, 
person, location and organization. Based on 
MaxEnt, the last module derives sub-categories 
such as city, airport, government, etc. from the 
basic tags. 
Section 1 describes the FST rule module. 
Section 2 discusses combining gazetteer 
information using MaxEnt. The constrained 
HMM is described in Section 3. Section 4 
discusses ub-type generation by MaxEnt. The 
experimental results and conclusion are 
presented finally. 
1 FST-based Pattern Matching Rules for 
Textract NE 
The most attractive feature of the FST (Finite 
State Transducer) formalism lies in its superior 
time and space efficiency \[Mohri 1997\] \[Roche 
& Schabes 1997\]. Applying a deterministic FST 
depends linearly only on the input size of the text. 
Our experiments also show that an FST rule 
system is extraordinarily robust. In addition, it 
has been verified by many research programs 
\[Krupka & Hausman 1998\] \[Hobbs 1993\] 
\[Silberztein 1998\] \[Srihari 1998\] \[Li & Srihari 
2000\], that FST is also a convenient tool for 
capturing linguistic phenomena, especially for 
idioms and semi-productive expressions like time 
NEs and numerical NEs. 
The rules which we have currently 
implemented include a grammar for temporal 
expressions (time, date, duration, frequency, age, 
etc.), a grammar for numerical expressions 
(money, percentage, length, weight, etc.), and a 
grammar for other non-MUC NEs (e.g. contact 
information like address, email). 
The following sample pattern rules give an 
idea of what our NE grammars look like. These 
rules capture typical US addresses, like: 5500 
Main St., Williamsville, NY14221; 12345 Xyz 
Avenue, Apt. 678, Los Angeles, CA98765-4321. 
The following notation is used: @ for macro; I
for logical OR; + for one or more; (...) for 
optionality. 
9 -~- 
number = 
uppercase =
0111213141516171819 
@0_9+ 
AIBICIDIEIFIGIHIIIJI 
KILIMINIOIPIQIRISIT 
UIVIWIXIYIZ 
248 
lowercase = a \[ b \[ c I d \[ e I f l g I h \[i I J I k \[ I I 
mln lo lp lq l r l s l t lu lv lw\ [  
x ly l z  
letter = @uppercase \[ @lowercase 
word = @letter+ 
delimiter = (",") .... + 
zip = @0_9 @0_9 @09 @0_9 @0_9 
("-" @0_9 @0_9 @0_9 @0_9) 
street = \[\[St l ST I Rd I RD I Dr I DRI 
Ave\[AVE \] C.")\] I Street\[ 
Road\[Drive\[Avenue 
city = @word (@word) 
state = @uppercase (".") @uppercase (".") 
us-- USA IU.S.AIUSIU.S.I 
(The) United States (of America) 
street_addr = @number @word @street 
apt_addr = \[APT C.") I Apt (".") \[ 
Apartment\] @number 
local_addr = @ street_addr 
(@delimiter @apt_addr) 
address = @ local_addr 
@delimiter @city 
@delimiter @state @zip 
(@delimiter @us) 
Our work is similar to the research on FST 
local grammars at LADL/University Paris VII 
\[Silberztein 1998\] 1, but that research was not 
turned into a functional rule based NE system. 
The rules in our NE grammars cover 
expressions with very predictable patterns. They 
were designed to address the weaknesses of our 
statistical NE tagger. For example, the following 
missings (underlined) and mistagging originally 
made by our statistical NE tagger have all been 
correctly identified by our temporal NE 
grammar. 
began <TIMEX TYPE="DATE">Dec. 15, 
the</TIMEX> space agency 
on Jan. 28, <TIMEX 
TYPE="DATE"> 1986</TIMEX>, 
in September <TIMEX 
TYPE="DATE">1994</TIMEX>on <TIMEX 
1 They have made public their esearch results at their 
website (http://www.ladl.jussieu.fr/index.html), 
including a grammar for certain temporal expressions 
and a grammar for stock exchange sub-language. 
TYPE="TIME">Saturday at</TIMEX> 2:42 
a.m. ES<ENAMEX 
TYPE="PERSON">T.</ENAMEX> 
He left the United States in <TIMEX 
TYPE="DATE">1984 and</TIMEX> moved 
in early <TIMEX TYPE="DATE"> 1962 
and</TIMEX> 
in <TIMEX TYPE="DATE">1987 the 
Bonn</TIMEX> government ruled 
2 Incorporating Gazetteers with the 
Maximum Entropy Model 
We use two gazetteers in our system, one for 
person and one for location. The person gazetteer 
consists of 3,000 male names, 5,000 female 
names and 14,000 family names. The location 
gazetteer consists of 250,000 location ames with 
their categories uch as CITY, PROVINCE, 
COUNTRY, AIRPORT, etc. The containing and 
being-contained relationship among locations is 
also provided. 
The following is a sample line in the location 
gazetteer, which denotes "Aberdeen" as a city in 
"California", and "California" as a province of 
"United States". 
Aberdeen (CITY) California (PROVINCE) 
United States (COUNTRY) 
Although gazetteers obviously contain useful 
name entity information, a straightforward word 
match approach may even degrade the system 
performance since the information from 
gazetteers i  too ambiguous. There are a lot of 
common words that exist in the gazetteers, uch 
as 'T', "A", "Friday", "June", "Friendship", etc. 
Also, there is large overlap between person 
names and location names, such as "Clinton", 
"Jordan", etc. 
Here we propose a machine learning 
approach to incorporate the gazetteer information 
with other common contextual information based 
on MaxEnt. Using MaxEnt, the system may 
learn under what situation the occurrence in 
gazetteers is a reliable vidence for a name entity. 
We first define "LFEATURE" based on 
occurrence in the location gazetteer as follows: 
249 
COUNTRY 
USSTATE 
MULTITOKEN 
of multiple tokens) 
BIGCITY 
in OXFD dictionary) 
COEXIST 
(country name) 
(US state name) 
(a location ame consisting 
(a location ame occurring 
(where COEXIST(A,B) is 
true iff A and B are in the same US state, or in 
the same foreign country) 
OTHER 
There is precedence from the first 
LFEATURE to the last one. Each token in the 
input document is assigned a unique 
"LFEATURE". We also define "NFEATURE" 
based on occurrence in the name gazetteer as 
follows: 
FAMILY 
MALE 
FEMALE 
FAMILYANDMALE 
name) 
FAMILYANDFEMALE 
name) 
OTHER 
(family name) 
(male name) 
(female name) 
(family and male 
(family and female 
With these two extra features, every token in 
the document is regarded as a three-component 
vector (word, LFEATURE, NFEATURE). We 
can build a statistical model to evaluate the 
conditional probability based on these contextual 
and gazetteer features. Here "tag" represents one 
of the three possible tags (Person, Location, 
Other), and history represents any possible 
contextual history. Generally, we have: 
p (tag, history) 
tag 
(1) 
A maximum entropy solution for probability has 
the form \[Rosenfeld 1994\] \[Ratnaparkhi 1998\] 
H ~/i (history,tag) 
p(tag,history) =
Z(history) 
Z(history) = ~.~ 1-I \[~t'ifi(hist?ry'tag ) 
tag i 
(e) 
(3) 
where fi (history, tag) are binary-valued feature 
functions that are dependent on whether the 
feature is applicable to the current contextual 
history. Here is an example of our feature 
function: 
f(history,tag)={~ ifcurrenttokenisaeountryname, ndtagisloeatiOnotherwise 
(4) 
In (2) and (3) a i are weights associated tofeature 
functions. 
The weight evaluation scheme is as follows: 
We first compute the average value of each 
feature function according to a specific training 
corpus. The obtained average observations are 
set as constraints, and the Improved Iterative 
Scaling (IIS) algorithm \[Pietra et al 1995\] is 
employed to evaluate the weights. The resulting 
probability distribution (2) possesses the 
maximum entropy among all the probability 
distributions consistent with the constraints 
imposed by feature function average values. 
In the training stage, our gazetteer module 
contains two sub-modules: feature function 
induction and weight evaluation \[Pietra et al 
1995\]. The structure is shown in Figure 2. 
Rule ~|ect|on Module \[ 
~elect next rule reduce the entropy most 
"-~ Evaluate weiEht for each Selected rule 
IteraUve $?atinB (US) t 
Fig.2, Structure ofMaxEnt learning Process 
We predefine twenty-four feature function 
templates. The following are some examples and 
others have similar structures: 
10 if LFEATURE = , and tag = _ f (history, tag) = 
else 
250 
f(history, tag)={lo 
f(history, tag)={~ 
f(history,tag)={lo 
f(history, tag)={; 
i f  NFEATURE = _ ,  and  tag  = _ 
e l se  
i f  cur rent  word  = _ ,  and  tag  = _ 
e l se  
i f  p rev ious  word  = _ ,  and  tag  = _ 
e l se  
i f  fo l low ing  word  = _ ,  and  tag  = _ 
e l se  
where the symbol .... denotes any possible 
values which may be  inserted into that field. 
Different fields will be filled different values. 
Then, using a training corpus containing 
230,000 tokens, we set up a feature function 
candidate space based on the feature function 
templates. The "Feature Function Induction 
Module" can select next feature function that 
reduces the Kullback-Leibler divergence the 
most \[Pietra et al 1995\]. To make the weight 
evaluation computation tractable at the feature 
function induction stage, when trying a new 
feature function, all previous computed weights 
are held constant, and we only fit one new 
constraint hat is imposed by the candidate 
feature function. Once the next feature function 
is selected, we recalculate the weights by IIS to 
satisfy all the constraints, and thus obtain the next 
tentative probability. The feature function 
induction module will stop when the 
Log-likelihood gain is less than a pre-set 
threshold. 
The gazetteer module recognizes the person 
and location names in the document despite the 
fact that some of them may be embedded in an 
organization ame. For example, "New York 
Fire Department" may be tagged as 
<LOCATION> New York </NE> Fire 
Department. In the input stream for HMM, each 
token being tagged as location is accordingly 
transformed into one of the built-in tokens 
"CITY", "PROVINCE", "COUNTRY". The 
HMM may group "CITY Fire Department" into 
an organization ame. A similar technique is 
applied for person names. 
Since the tagged tokens from the gazetteer 
module are regarded by later modules as either 
person or location names, we require that the 
current module generates results with the highest 
possible precision. For each tagged token we will 
compute the entropy of the answer. If the entropy 
is higher than a pre-set hreshold, the system will 
not be certain enough about the answer, and the 
word will be untagged. The missed location or 
person names may be recognized by the 
following HMM module. 
3 Improving NE Segmentation through 
constrained HMM 
Our original HMM is similar to the Nymble 
\[Bikel et al 1997\] system that is based on bigram 
statistics. To correct some of the leading errors, 
we incorporate manual segmentation rules with 
HMM. These syntactic rules may provide 
information beyond bigram and balance the 
limitation of the training corpus. 
Our manual rules focus on improving the NE 
segmentation. For example, in the token 
sequence "College of William and Mary", we 
have rules based on global sequence checking to 
determine if the words "and" or "of" are common 
words or parts of organization name. 
The output of the rules are some constraints 
on the HMM transition etwork, such as "same 
tags for tokens A, B", or "common word for 
token A". The Viterbi algorithm will select he 
optimized path that is consistent with such 
constraints. 
The manual rules are divided into three 
categories: (i) preposition disambiguation, (ii) 
spurious capitalized word disambiguation, and 
(iii) spurious NE sequence disambiguation. 
The rules of preposition disambiguation are 
responsible for determination of boundaries 
involving prepositions ("of", "and", "'s", etc.). 
For example, for the sequence "A of B", we have 
the following rule: A and B have same tags if the 
lowercase of A and B both occur in OXFD 
dictionary. A "global word sequence checking" 
\[Mikheev, 1999\] is also employed. For the 
sequence "Sprint and MCI", we search the 
document globally. If the word "Sprint" or 
251 
"MCI" occurs individually somewhere lse, we 
mark "and" as a common word. 
The rules of spurious capitalized word 
disambiguation are designed to recognize the 
first word in the sentence. If the first word is 
unknown in the training corpus, but occurs in 
OXFD as a common word in lowercase, HHM's 
unknown word model may be not accurate 
enough. The rules in the following paragraph are 
designed to treat such a situation. 
If the second word of the same sentence is in 
lowercase, the first word is tagged as a common 
word since it never occurs as an isolated NE 
token in the training corpus unless it has been 
recognized as a NE elsewhere in the document. 
If the second word is capitalized, we will check 
globally if the same sequence occurs somewhere 
else. If so, the HMM is constrained to assign the 
same tag to the two tokens. Otherwise, the 
capitalized token is tagged as a common word. 
The rules of spurious NE sequence 
disambiguation are responsible for finding 
spurious NE output from HMM, adding 
constraints, and re-computing NE by HMM. For 
example, in a sequence "Person Organization", 
we will require the same output ag for these two 
tokens and run HMM again. 
4 NE Sub-Type Tagging using Maximum 
Entropy Model 
The output document from constrained HMM 
contains MUC-standard NE.tags such as person, 
location and organization. However, for a real 
information extraction system, the 
MUC-standard NE tag may not be enough and 
further detailed NE information might be 
necessary. We have predefined the following 
sub-types for person, location and organization: 
Person: Military Person 
Religious Person 
Man 
Woman 
Location: City 
Province 
Country 
Continent 
Lake 
River 
Mountain 
Road 
Region 
District 
Airport 
Organization: Company 
Government 
Army 
School 
Association 
Mass Medium 
If a NE is not covered by any of the above 
sub-categories, it should remain a MUC-standard 
tag. Obviously, the sub-categorization requires 
much more information beyond bigram than 
MUC-standard tagging. For example, it is hard 
to recognize CNN as a Mass Media company by 
bigram if the token "CNN" never occurs in the 
training corpus. External gazetteer information is 
critical for some sub-category recognition, and 
trigger word models may also play an important 
role. 
With such considerations, we use the 
Maximum entropy model for sub-categorization, 
since MaxEnt is powerful enough to incorporate 
into the system gazetteer or other information 
sources which might become available at some 
later time. 
Similar to the gazetteer module in Section 2, 
the sub-categorization module in the training 
stage contains two sub-modules, (i) feature 
function induction and (ii) weight evaluation. 
We have the following seven feature function 
templates: 
10 if MUC_tag = _, and tag = _ 
f (history, tag) = else 
{ 10 if MUC_tag = _, LFEATURE = _, and tag = _ 
f (history, tag) = else 
1 if contain word(__), MUC tag(history) = _,and tag = 
f (history, tag ) = - - - 
0 else 
10 if Previous_Word = _, MUC_tag = _,and tag = _ 
f (history, tag)= else 
f(history, tag)= {10 if following_Word= _,MUC_tag = _ ,ande lse  tag=_  
f(history, tag)={lo i fMUC_tag= ,contain_male_name, and tag 
252 
= 1l  if  ,oc_ta  =_,co. .in_fema,e_.ame,a.d,ag=_ f (history, tag ) to else 
We have trained 1,000 feature functions by 
the feature function induction module according 
to the above templates. 
Because much more external gazetteer 
information is necessary for the 
sub-categorization and there is an overlap 
between male and female name gazetteers, the 
result from the current MaxEnt module is not 
sufficiently accurate. Therefore, a conservative 
strategy has been applied. If the entropy of the 
output answer is higher than a threshold, we will 
back-off to the MUC-standard tags. Unlike 
MUC NE categories, local contextual 
information is not sufficient for 
sub-categorization. In the future more external 
gazetteers focusing on recognition of 
government, company, army, etc. will be 
incorporated into our system. And we are 
considering using trigger words \[Rosenfeld, 
1994\] to recognize some sub-categories. For 
example, "psalms" may be a trigger word for 
"religious person", and "Navy" may be a trigger 
word for "military person". 
Experiment and Conclusion 
We have tested our system on MUC-7 dry run 
data; this data consists of 22,000 words and 
represents articles from The New York Times. 
Since a key was provided with the data, it is 
possible to properly evaluate the performance of 
our NE tagger. The scoring program computes 
both the precision and recall, and combines these 
two measures into f-measure as the weighted 
harmonic mean \[Chinchor, 1998\]. The formulas 
are as follows: 
number of correct responses Precision = 
number esponses 
number of  correct responses 
Recall = 
number correct in key 
F - (/32 + 1)Precision * Recall 
(f l2Recall) + Precision 
The score of our system is as follows: 
Recall Precision 
Organization 95 95 
Person 96 
Location 96 
Percentage 
93 
94 
Date 92 91 
Time 92 91 
Money 100 86 
100 75 
F-measure =93.39 
If the gazetteer module is removed from our 
system, and the constrained HMM is restored to 
the standard HMM, the f-measures for person, 
location, and organization are as follows: 
Recall Precision 
Organization 94 92 
Person 95 91 
Location 95 92 
Obviously, our gazetteer model and 
constrained HMM have greatly increased the 
system accuracy on the recognition of persons, 
locations, and organizations. Currently, there are 
some errors in our gazetteers. Some common 
words such as "Changes", "USER", 
"Administrator", etc. are mistakenly included in 
the person name gazetteer. Also, too many 
person names are included into the location 
gazetteer. By cleaning up the gazetteers, we can 
continue improving the precision on person name 
and locations. 
We also ran our NE tagger on the formal test 
files of MUC-7. The following are the results: 
Recall Precision 
Person 92 95 
Organization 85 86 
Location 90 92 
Date 95 85 
253 
Time 79 72 
Money 95 82 
Percentage 97 80 
-Overall F-measure 89 
There is some performance degradation i  
the formal test. This decrease is because that the 
formal test is focused on satellite and rocket 
domains in which our system has not been 
trained. There are some person/location names 
used as spacecraft or robot names (ex. Mir, Alvin, 
Columbia...), and there are many high-tech 
company names which do not occur in our HMM 
training corpus. Since the finding of organization 
names totally relies on the HMM model, it suffers 
most from domain shift (10% degradation). This 
difference implies that gazetteer information may 
be useful in overcoming the domain dependency. 
This paper has demonstrated improved 
performance in an NE tagger by combining 
symbolic and statistical approaches. MaxEnt has 
been demonstrated to be a viable technique for 
integrating diverse sources of information and 
has been used in NE sub-categorization. 
A. Ratnaparkhi, Maximum Entropy Models for 
Natural Language Ambiguity resolution, PHD 
thesis, Univ. of Pennsylvania, (1998) 
S. D. Pietra, Vincent Della Pietra, and John Lafferty, 
Inducing Features of Random Fields, Tech Report, 
Carnegie Mellon University, (1995) 
A. Mikheev, A Knowledge-free Method for 
Capitalized Word Disambiguation, in Proceedings 
of the 37th Annual Meeting of the Association for 
Computational Linguistics, (1999), pp. 159-166 
J. R. Hobbs, 1993. FASTUS: A System for Extracting 
Information from Text, Proceedings ofthe DARPA 
workshop on Human Language Technology", 
Princeton, NJ, pp. 133-137. 
Emmanuel Roche & Yves Schabes, 1997. Finite-State 
Language Processing, The MIT Press, Cambridge, 
MA. 
Li, W & Srihari, R. 2000. Flexible Information 
Extraction Learning Algorithm, Final Technical 
Report, Air Force Research Laboratory, Rome 
Research Site, New York 
M. Silberztein, 1998. Tutorial Notes: Finite State 
Processing with INTEX, COLING-ACL'98, 
Montreal (also available at 
http://www.ladl.\] ussieu.fr) 
M. Mohri,. 1997. Finite-State Transducers in 
Language and Speech Processing, Computational 
Linguistics, Vol. 23, No. 2, pp. 269-311. 
R. Srihari, 1998. A Domain Independent Event 
Extraction Toolkit, AFRL-IF-RS-TR-1998-152 
Final Technical Report, Air Force Research 
Laboratory, Rome Research Site, New York 
References 
G. R Krupka and K. Hausman, "IsoQuest Inc: 
Description of the NetOwl "Fext Extraction System 
as used for MUC-7" in Proceedings of Seventh 
Machine Understanding Conference (MUC-7) 
(1998) 
D. M. Bikel, "Nymble: a high-performance learning 
name-finder" in Proceedings of the Fifth 
Conference on Applied Natural Language 
Processing, 1997, pp. 194-201, Morgan Kaufmann 
Publishers. 
A. Borthwick, et al, Description of the MENE named 
Entity System, In Proceedings of the Seventh 
Machine Understanding Conference (MUC-7) 
(1998) 
R. Rosenfeld, Adaptive Statistical language Modeling, 
PHD thesis, Carnegie Mellon University, (1994) 
254 
 Location Normalization for Information Extraction* 
 
 
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
(hli, rohini, cniu, wei)@cymfony.com 
 
 
Abstract  
 
Ambiguity is very high for location names. For 
example, there are 23 cities named ?Buffalo? in 
the U.S.  Country names such as ?Canada?, 
?Brazil? and ?China? are also city names in the 
USA. Almost every city has a Main Street or 
Broadway. Such ambiguity needs to be handled 
before we can refer to location names for 
visualization of related extracted events. This 
paper presents a hybrid approach for location 
normalization which combines (i) lexical 
grammar driven by local context constraints, (ii) 
graph search for maximum spanning tree and 
(iii) integration of semi-automatically derived 
default senses. The focus is on resolving 
ambiguities for the following types of location 
names: island, town, city, province, and country. 
The results are promising with 93.8% accuracy 
on our test collections. 
 
1 Introduction 
The task of location normalization is to identify 
the correct sense of a possibly ambiguous 
location Named Entity (NE). Ambiguity is very 
serious for location NEs. For example, there are 
23 cities named ?Buffalo?, including the city in 
New York State and in Alabama State. Even 
country names such as ?Canada?, ?Brazil?, and 
?China? are also city names in the USA. Almost 
every city has a Main Street or Broadway. Such 
ambiguity needs to be properly handled before 
converting location names into some normal 
form to support entity profile construction, event 
merging and visualization of extracted events on 
 
*This work was partly supported by a grant from the 
Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-00-C-0090.  
a map for an Information Extraction (IE) System.  
Location normalization is a special 
application of word sense disambiguation 
(WSD). There is considerable research on WSD. 
Knowledge-based work, such as (Hirst, 1987; 
McRoy, 1992; Ng and Lee, 1996) used 
hand-coded rules or supervised machine learning 
based on annotated corpus to perform WSD. 
Recent work emphasizes corpus-based 
unsupervised approach (Dagon and Itai, 1994; 
Yarowsky, 1992; Yarowsky, 1995) that avoids 
the need for costly truthed training data. Location 
normalization is different from general WSD in 
that the selection restriction often used for WSD 
in many cases is not sufficient to distinguish the 
correct sense from the other candidates.  
For example, in the sentence ?The White 
House is located in Washington?, the selection 
restriction from the collocation ?located in? can 
only determine that ?Washington? should be a 
location name, but is not sufficient to decide the 
actual sense of this location. Location 
normalization depends heavily on co-occurrence 
constraints of geographically related location 
entities mentioned in the same discourse. For 
example, if ?Buffalo?, ?Albany? and ?Rochester? 
are mentioned in the same document, the most 
probable senses of ?Buffalo?, ?Albany? and 
?Rochester? should refer to the cities in New 
York State. There are certain fixed 
keyword-driven patterns from the local context, 
which decide the sense of location NEs. These 
patterns use keywords such as ?city?, ?town?, 
?province?, ?on?, ?in? or other location names. For 
example, the pattern ?X + city? can determine 
sense tags for cases like ?New York city?; and the 
pattern ?City + comma + State? can disambiguate 
cases such as ?Albany, New York? and 
?Shanghai, Illinois?. In the absence of these 
patterns, co-occurring location NEs in the same 
discourse can be good evidence for predicting the 
most probable sense of a location name.  
  
Unrestricted
text Tokenizer
POS Tagging
Shallow Parsing
Coreference
Semantic Parsing
Pragmatic Filter
NE Tagging
LocNZ
Profile
Event
Question 
Answering
O
ut
pu
t(I
E 
D
at
ab
as
e)
Intelligent
Browsing
Summari-
zationVisualization
Kernel IE Modules Linguistic Modules
Note: NE: name entity tagging; LocNZ: location normalization
Application Modules
O
ut
pu
t(I
E 
D
at
ab
as
e)
 
Figure 1. InfoXtract system architecture 
 
 
Event type: Job change
Keyword: hired
Company : Microsoft
Person in: Mary
Position: sales person
Location: Beijing
Date: January 1
Event type: Job change
Keyword: replaced
Company: Microsoft
Person out : he(Dick)
Position: sales person
Location: Beijing
Date: Yesterday
Event 1 Event 2
Event type: Job change
Keyword: hired
Keyword: replaced
Company: Microsoft
Person in: Mary
Person out : he(Dick)
Position: salesperson
Location: <LocationProfile101>
Date: 2000-01-01
Figure 2. Location verification in Event 
merging. 
For choosing the best matching sense set 
within a document, we simply construct a graph 
where each node represents a sense of a location 
NE, and each edge represents the relationship 
between two location name senses. A graph 
spanning algorithm can be used to select the best 
senses from the graph. If there exist nodes that 
cannot be resolved in this step, we will apply 
default location senses that were extracted 
semi-automatically by statistical processing. The 
location normalization module, or ?LocNZ?, is 
applied after the NE tagging module in our 
InfoXtract IE system as shown in Figure 1. 
This paper focuses on how to resolve 
ambiguity for the names of island, town, city, 
province, and country. Three applications of 
LocNZ in Information Extraction are illustrated 
in Section 2. Section 3 presents location sense 
identification using local context; Section 4 
describes disambiguation process using 
information within a document through graph 
processing; Section 5 shows how to 
semi-automatically collect default senses of 
locations from a corpus; Section 6 presents an 
algorithm for location normalization with 
experimental results. The summary and 
conclusions are given in Section 7. Sample text 
and the results of location tagging are given in the 
Appendix. 
 
2   Applications of Location Normalization 
Several applications are enabled through location  
normalization. 
? Event extraction and merging 
Event extraction is an advanced IE task. 
Extracted events can be merged to provide key 
content in a document. The merging process 
consists of several steps including checking 
information compatibility such as checking 
synonyms, name aliases and co-reference of 
anaphors, time and location normalization. Two 
events cannot be merged if there is a conflicting 
condition such as time and location. Figure 2 
shows an example of event merging where the 
events occurred in Microsoft at Beijing, not in 
Seattle. 
? Event visua lization 
Visualization applications can illustrate where an 
event occurred with support of location 
normalization. Figure 3 demonstrates a 
visualized event on a map based on the 
normalized location names associated with the 
events. The input to visualization consists of 
extracted events from a news story pertaining to 
Julian Hill?s life. The arrow points to the city 
where the event occurred. 
? Entity profile construction 
An entity profile is an information object for 
entities such as person, organization and location.  
It is defined as an Attribute Value Matrix (AVM)  
to represent key aspects of information about 
entities, including their relationships with other 
entities. Each attribute slot embodies some 
  
Event type: <Die: Event 200>
Who:       <Julian Werver Hill: PersonProfile 001>
When:     1996-01-0 7
Where :     <Loca t ionPro f i l e103>
Preceding_event:  <hospitalize: Event 260>
Subsequent_event:  <bury:  Event  250>
Event Visualization
;  ; 
; ; 
Predicate: Die
Who: Julian Werner Hill
When:
Where: <LocationProfile 103>
Hockessin, Delaware, USA,
19707,75.688873,39.77604
1996-01-07
Figure 3. Event visulization with location. 
information about the entity in one aspect. Each 
relationship is represented by an attribute slot in 
the Profile AVM.  Sample Profile AVMs 
involving the reference of locations are 
illustrated below. 
<PersonProfile 001> :: 
Name:   Julian Werner Hill  
Position: Research chemist 
Age:        91  
Birth-place: <LocationProfile100> 
Affiliation:  Du Pont Co.  
Education:  MIT  
 
<LocationProfile 100> :: 
Name:   St. Louis  
State:    Missouri 
Country: United States of America  
Zipcode:  63101 
Lattitude : 90.191313  
Longitude:  38.634616 
Related_profiles: <PersonProfile 001>  
 
Several other applications such as question 
answering and classifying documents by location 
areas can also be enabled through LocNZ. 
3 Lexical Grammar Processing in 
Local Context 
Named Entity tagging systems (Krupka and 
Hausman, 1998; Srihari et al, 2000) attempt to 
tag information such as names of people, 
organizations, locations, time, etc. in running 
text.  In InfoXtract, we combine Maximum 
Entropy Model (MaxEnt) and Hidden Markov 
Model for NE tagging (Shrihari et al,, 2000). The 
Maximum Entropy Models incorporate local 
contextual evidence in handling ambiguity of 
information from a location gazetteer. In the 
Tipster Location gazetteer used by InfoXtract, 
there are a lot of common words, such as I, A, 
June, Friendship , etc. Also, there is large overlap 
between person names and location names, such 
as Clinton, Jordan, etc. Using MaxEnt, systems 
learn under what situation a word is a location 
name, but it is very difficult to determine the 
correct sense of an ambiguous location name. If a 
word can represent a city or state at the same 
time, such as New York or Washington, it is 
difficult to decide if it refers to city or state. The 
NE tagger in InfoXtract only assigns the location 
super-type tag NeLOC to the identified location 
words and leaves the task of location sub-type 
tagging such as NeCITY or NeSTATE and its 
normalization to the subsequent module LocNZ. 
For representation of LocNZ results, we add 
an unique zip code and position information 
that is longitude and latitude for the cities for 
event visualization. 
The first step of LocNZ is to use local context 
that is the co-occurring words around a location 
name. Local context can be a reliable source in 
deciding the sense of a location. The following 
are most commonly used patterns for this 
purpose.  
 
(1) location+comma+NP(headed by ?city?)  
e.g. Chicago, an old city  
(2) ?city of? +location1+comma+location2 
e.g. city of Albany, New York 
(3) ?city of? +location 
(4) ?state of?+location  
(5) location1+{,}+location2+{,}+location3 
e.g. (i) Williamsville, New York, USA 
       (ii) New York, Buffalo,USA 
     (6) {on, in}+location 
 e.g. on Strawberry ? NeIsland 
 in Key West ? NeCity 
Patterns (1) , (3), (4) and (6) can be used to decide 
if the location is a city, a state or an island, while 
patterns (2) and (5) can be used to determine both 
the sub-tag and its sense. These patterns are 
implemented in  our finite state transducer 
formalism. 
 4 Maximum Spanning Tree 
Calculation with Global Information 
Although local context can be reliable evidence 
for disambiguating location senses, there are still 
many cases which cannot be captured by the 
above patterns. Information in the entire 
document (i.e. discourse information) should be 
considered. Since all location names in a 
document have meaning relationships among 
them, a way to represent the best sense 
combination within the document is needed.  
The LocNZ process constructs a weighted 
graph where each node represents a location 
sense, and each edge represents similarity weight 
between location names. Apparently there will be 
no links among the different senses of a location 
name, so the graph will be partially complete. We 
calculate the maximum weight spanning tree 
(MaxST) using Kruskal?s MinST algorithm 
(Cormen et al 1990). The nodes on the 
resulting MaxST are the most promising senses 
of the location names.  
We define three criteria for similarity weight 
assignment between two nodes:  
(1) More weight will be given to the edge 
between a city and the province (or the 
country) to which it belongs.  
(2) Distance between location names mentioned 
in the document is taken into consideration. 
The shorter the distance, the more we assign  
the weight between the nodes.  
(3) The number of word occurrences affects the 
weight calculation. For multiple mentions of 
a location name, only one node will be 
represented in the graph. We assume that all 
the same location mentions have the same 
meaning in a document following one sense 
per discourse principle (Gale, Church, and 
Yarowsky, 1992).  
When calculating the weight between two 
location names, the predefined similarity values 
shown in Table 1, the number of location name 
occurrences and the distance between them in a 
text are taken into consideration. After selecting 
each edge, the senses that are connected will be 
chosen, and other senses of the same location 
name will be discarded so that they will not be 
considered again in the MaxST calculation. A 
weight value is calculated with equation (1), 
where sij indicate the jth sense of wordi, a reflects 
the number of location name occurrences in a 
text, and b refers to the distance between the two 
location names. Figure 4 shows the graph for 
calculating MaxST. Dots in a circle mean the 
number of senses of a location name. 
Table 1. Similarity value sim(si,si) between 
location sense pairs. 
Loc1 Loc2 Relationship  Sim(si,si) 
C1 P1 P1 includes C1  5 
IL Ctr1 Ctr1 includes IL 5 
C1 Ctr1 Ctr1 is direct parent 5 
C1 C2 C1 and C2 in same 
province/state 
3 
C1 C2 C1 and C2 in same 
country 
2 
C1 P1 C1 and P1 are in same 
country but C1 is not 
in P1 
2 
C1 Ctr1 Ctr1 is not a direct 
parent of C1  
3 
P1 Ctr1 P1 is in Ctr1 1 
P1 P2 P1 and P2 in same 
country 
1 
Loc1 Loc2 Loc1 and Loc2 are two 
sense nodes of the 
same location name 
-? 
Loc1 Loc2 Other cases 0 
Note: Ci: city; Pi: province/state; IL: island; Ctri: 
country; Loci: location. 
),(),(
/))()((),(
/),(),(),(),(
jijkij
jijkij
jkijjkijjkijjkij
wwdistss
numAllwnumwnumss
numAllsssssssimssScore
=
+=
-+=
b
a
ba
                          (1) 
5 Default Sense Extraction 
In our experiments, we found that the system 
performance suffers greatly from the lack of 
lexical information on default senses. For 
example, people refer to ?Los Angeles? as the 
city at California more than the city in 
Philippines, Chile, Puerto Rico, or the city in 
Texas in the USA. This problem becomes a 
bottleneck in the system performance. As 
mentioned before, a location name usually has a 
dozen senses that need sufficient evidence in a 
document for selecting one sense among them. 
 Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio ,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec ,
Quebec Prov,
Connecticut,
?}
3*4 l ines
2*3 lines
4*11 lines
11*10 lines
3*10 lines
8*3 l ines
2*8 lines
2*43*11
3*10
3*33*8
2*10
2*3
8*4
8*11
8*10
8*4
10*4
3*11
 
Figure 4. Graph for calculating maximum weight 
spanning tree. 
But in many cases there is no explicit clue in a 
document, so the system has to choose the default 
senses that most people may refer to under 
common sense.  
The Tipster Gazetteer (http://crl.nmsu.edu/ 
cgi-bin/Tools/CLR/clrcat) used in our system has 
171,039 location entries with 237,916 total 
senses that cover most location names all over the 
world. Each location in the gazetteer may have 
several senses. Among them 30,711 location 
names have more than one sense. Although it has 
ranking tags on some location entries, a lot of 
them have no tags attached or the same rank is 
assigned to the entries of the same name. 
Manually calculating the default senses for over 
30,000 location names will be difficult and it is 
subject to inconsistency due to the different 
knowledge background of the human taggers. To 
solve this problem in calculating the default 
senses of location names, we propose to extract 
the knowledge from a corpus using statistical 
processing method.  
With the TREC-8 (Text Retrieval Conference) 
corpus, we can only extract default senses for 
1687 location names, which cannot satisfy our 
requirement. This result shows that the general 
corpus is not sufficient to suit our purpose due to 
the serious ?data sparseness? problem. Through a 
series of experiments, we found that we could 
download highly useful information from Web 
search engines such as Google, Yahoo, and 
Northern Light by searching ambiguous location 
names in the Gazetteer. Web search engines can 
provide the closest content by their built-in 
ranking mechanisms. Among those engines, we 
found that the Yahoo search engine is the best 
one for our purpose.  We wrote a script to 
download web-pages from Yahoo! using each 
ambiguous location name as a search string.   
In order to derive default senses automatically 
from the downloaded web-pages, we use the 
similarity features and scoring values between 
location-sense pairs described in Section 3. For 
example, if ?Los Angeles? co-occurs with 
?California? in the same web-page, then its sense 
will be most probably set to the city in California 
by the system.   Suppose a location word w has 
several city senses si: Sense(w) indicates the 
default sense of w; sim(wi,xjk) means the 
similarity value between two senses of the  word 
w and the j th co-occuring word xj; num(w) is the   
number of w in the document, and NumAll is the 
total number of locations.  a  is a parameter that 
reflects the importance of the co-occurring 
location names and is determined empirically. 
The default sense of w is wi that maximizes the 
similarity value with all co-occurring location 
names. The maximum similarity should be larger 
than a threshold to keep meaningful default 
senses. The threshold can be determined 
empirically through experimentation. 
)))(/())((*                
*),((maxmax)(
1 11
wnumNumAllxnum
xssimwSense
j
n
j
jkipkmi
-
= ?
= ????
a
(2) 
 
For each of 30,282 ambiguous location names, 
we used the name itself as search term in Yahoo 
to download its corresponding web-page. The 
system produced default senses for 18,446 
location names. At the same time, it discarded the 
remaining location names because the 
corresponding web-pages do not contain 
sufficient evidence to reach the threshold. We  
observed that the results reflect the correct senses 
in most cases, and found that the discarded 
location names have low references in the search 
results of other Web search engines. This means 
they will not appear frequently in text, hence 
minimal impact on system performance. We 
manually modified some of the default sense 
results based on the ranking tags in the Tipster 
Gazetteer and some additional information on 
population of the locations in order to consolidate 
the default senses.  
 6 Algorithm and Experiment 
With the information from local context, 
discourse context and the knowledge of default 
senses, the location normalization process turned 
out to be very efficient and precise. The 
processing flow is divided into 5 steps: 
Step 1. Look up the location gazetteer to 
associate candidate senses for each location NE; 
Step 2. Call the pattern matching sub-module to 
resolve the ambiguity of the NEs involved in 
local patterns like ?Williamsville, New York, 
USA? to retain only one sense for the NE as early 
as possible; 
Step 3. Apply the ?one sense per discourse? 
principle for each disambiguated location name 
to propagate the selected sense to its other 
occurrences within a document; 
Step 4. Call the global sub-module, which is a 
graph search algorithm, to resolve the remaining 
ambiguities; 
Step 5. If the decision score for a location name is 
lower than a threshold, we choose a default sense 
of that name as a result. 
For evaluating the system performance, 53 
documents from a travel site 
(http://www.worldtravelguide.net/navigate/region/na
m.asp), CNN News and New York Times are 
used. Table 2 shows some sample results from 
our test collections. For results shown in Column 
4, we first applied default senses of location 
names available from the Tipster Gazetteer in 
accordance with the rules specified in the 
gazetteer document. If there is no ranking value 
tagged for a location name, we select the first 
sense in the gazetteer as its default. This 
experiment showed accuracy of 42%. For 
Column 5, we tagged the corpus with default 
senses we derived with the method described in 
section 5, and found that it can resolve 78% 
location name ambiguity. Column 6 in Table 2 is 
the result of our LocNZ system using the 
algorithm described above as well as default 
senses we derived. The system showed promising 
results with 93.8% accuracy.  
7 Conclusion 
This paper presents a method of location 
normalization for information extraction with 
experimental results and its applications. In 
future work, we will integrate a expanded 
location gazetteer including names of landmarks, 
mountains and lakes such as Holland Tunnel (in 
New York, not in Holland) and Hoover Dam (in 
Arizona, not in Alabama), to enlarge the system 
coverage, and adjust the scoring weight given in 
Table 1 for better normalization results. Using 
context information other than location names 
can be a subtask for determining specific location 
names such as bridge or area names. 
Table 2. Experimental evaluation for location name normalization. 
 
Correctly tagged locations Document Type No. of 
Ambigu-
ous Loc 
Names 
No. of 
Ambigu-
ous 
senses  
With Tipster 
Gazetteer 
default sense 
and rule only 
With LocNZ 
default senses 
only 
LocNZ 
Precision 
(%) of 
LocNZ  
California Intro. 26 326 13 18 25 96 
Canada Intro. 14 75 13 13 14 100 
Florida Intro 22 221 10 18 20 90 
Texas Intro. 13 153 9 11 12 93 
CNN News 1 27 486 10 23 25 92 
CNN News 2 26 360 10 22 24 92 
CNN News 3 16 113 4 10 14 87.5 
New York Times 1 8 140 1 7 8 100 
New York Times 2 10 119 2 7 10 100 
New York Times 3 18 218 5 13 17 94 
Total 180 2211 77 (42%) 142 (78%) 169 (93.8%)  93.8 
 
 8 Acknowledgement 
The authors wish to thank Carrie Pine of AFRL 
for supporting this work.  Other members of 
Cymfony?s R&D team, including Sargur N. 
Srihari, have also contributed in various ways. 
References 
Cormen, Thomas H., Charles E. Leiserson, and 
Ronald L. Rivest. 1990. Introduction to 
Algorithm. The MIT Press, pp. 504-505. 
Dagon, Ido and Alon Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational 
Linguistics, Vol.20, pp. 563-596. 
Gale, W.A., K.W. Church, and D. Yarowsky. 
1992. One Sense Per Discourse. In 
Proceedings of the 4th DARPA Speech and 
Natural Language Workshop. pp. 233-237. 
Hirst, Graeme. 1987. Semantic Interpretation 
and the Resolution of Ambiguity. Cambridge 
University Press, Cambridge. 
Krupka, G.R. and K. Hausman.  1998.  IsoQuest 
Inc.: Description of the NetOwl (TM) Extractor 
System as Used for MUC-7.  Proceedings of 
MUC.  
McRoy, Susan W. 1992. Using Multiple 
Knowledge Sources for Word Sense 
Discrimination. Computational Linguistics, 
18(1): 1-30. 
Ng, Hwee Tou and Hian Beng Lee. 1996. 
Integrating Multiple Knowledge Sources to 
Disambiguate Word Sense: an Exemplar-based 
Approach. In Proceedings of 34th Annual 
Meeting of the Association for Computational 
Linguistics, pp. 40-47, California. 
Srihari, Rohini, Cheng Niu, and Wei Li. 2000. A 
Hybrid Approach for Named Entity and 
Sub-Type Tagging. In Proceedings of ANLP 
2000, Seattle. 
Yarowsky, David. 1992. Word-sense 
Disambiguation Using Statistical Models of 
Roget?s Categories Trained on Large Corpora. 
In Proceedings of the 14 th Internaional 
Conference on Computational Linguistics 
(COLING-92), pp. 454-460, Nates, France. 
Yarowsky, David. 1995. Unsupervised Word 
Sense Disambiguation Rivaling Supervised 
Methods. In Proceedings of the 33rd Annual 
Meeting of the Association for Computational 
Linguistics, Cambridge, Massachusetts. 
Appendix: Sample text and tagged result 
Few countries in the world offer as many choices to 
the world traveler as Canada. Whether your passion is 
skiing, sailing, museum-combing or indulging in 
exceptional cuisine, Canada has it all.  
Western Canada is renowned for its stunningly 
beautiful countryside. Stroll through Vancouver's 
Park, overlooking the blue waters of English Bay or 
ski the slopes of world-famous Whistler-Blackcomb, 
surrounded by thousands of hectares of pristine 
forestland. For a cultural experience, you can take an 
Aboriginal nature hike to learn about Canada's First 
Nations' history and cuisine, while outdoorsmen can 
river-raft, hike or heli-ski the thousands of kilometers 
of Canada's backcountry, where the memories of gold 
prospectors and pioneers still flourish today.  
By contrast, Canada mixes the flavor and charm of 
Europe with the bustle of trendy New York. Toronto 
boasts an irresistible array of ethnic restaurants, 
bakeries and shops to tempt the palate, while 
Charlottetown, Canada's birthplace, is located amidst 
the rolling fields and sandy Atlantic beaches of Prince 
Edward Island. Between the two, ancient Quebec City 
is a world unto itself: the oldest standing citadel in 
North America and the heart of Quebec hospitality.  
 
 
Location City Province Country 
Canada - - Canada 
Vancouver Vancouver British 
Columbia 
Canada 
New York New York New York USA 
Toronto Toronto Ontario Canada 
Charlotte- 
town 
Charlotte- 
town 
Prince 
Edward 
Island 
Canada 
Prince 
Edward 
Island 
- Prince 
Edward 
Island 
Canada 
Quebec Quebec Quebec Canada 
 
Bootstrapping for Named Entity Tagging Using Concept-based Seeds 
Cheng Niu, Wei Li, Jihong Ding, Rohini K. Srihari 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221. USA. 
{cniu, wei, jding, rohini}@cymfony.com
 
Abstract 
A novel bootstrapping approach to 
Named Entity ?NE?tagging using con-
cept-based seeds and successive learners 
is presented. This approach only requires 
a few common noun or pronoun seeds 
that correspond to the concept for the tar-
geted NE, e.g. he/she/man/woman for 
PERSON NE. The bootstrapping proce-
dure is implemented as training two suc-
cessive learners. First, decision list is used 
to learn the parsing-based NE rules. Then, 
a Hidden Markov Model is trained on a 
corpus automatically tagged by the first 
learner. The resulting NE system ap-
proaches supervised NE performance for 
some NE types.  
1 Overview 
Recognizing and classifying proper names is a 
fundamental task for information extraction. Three 
types of proper names are defined in the Message 
Understanding Conference (MUC) Named Entity 
(NE) standards, namely, PERSON (PER), 
ORGANIZATION (ORG), and LOCATION 
(LOC). [MUC-7 1998]  
There is considerable research on NE tagging 
using supervised machine learning [e.g. Bikel et al 
1997; Borthwick 1998]. To overcome the knowl-
edge bottleneck of supervised learning, unsuper-
vised machine learning has been applied to NE. 
[Cucchiarelli & Velardi 2001] discussed boosting 
the performance of an existing NE tagger by unsu-
pervised learning based on parsing structures. 
[Cucerzan & Yarowsky 1999], [Collins & Singer 
1999] and [Kim et al 2002] presented various 
techniques using co-training schemes for NE ex-
traction seeded by a small list of proper names or 
hand-crafted NE rules. NE tagging has two tasks: 
(i) NE chunking; (ii) NE classification. Parsing-
supported unsupervised NE learning systems in-
cluding ours only need to focus on NE classifica-
tion, assuming the NE chunks have been 
constructed by the parser.  
This paper presents a new bootstrapping ap-
proach using successive learning and concept-
based seeds.  The successive learning is as follows. 
First, parsing-based NE rules are learned with high 
precision but limited recall. Then, these rules are 
applied to a large raw corpus to automatically gen-
erate a tagged corpus. Finally, a high-performance 
HMM-based NE tagger is trained using this cor-
pus.  
Unlike co-training, our bootstrapping does not 
involve iterative learning between the two learners, 
hence it suffers little from error propagation which 
is commonly associated with iterative learning.  
To derive the parsing-based learner, the system 
only requires a few common noun or pronoun 
seeds that correspond to the concept for the tar-
geted NE, e.g. he/she/man/woman for PERSON 
NE. Such concept-based seeds share grammatical 
structures with the corresponding NEs, hence a 
parser is utilized to support bootstrapping. Since 
pronouns and common nouns occur more often 
than NE instances, the parsing-based NE rules can 
be learned in one iteration to avoid iterative learn-
ing. 
The benchmarking shows that this system ap-
proaches the performance of supervised NE tag-
gers for two of the three proper name NE types in 
MUC, namely, PER NE and LOC NE. This ap-
proach also supports tagging user-defined NE 
types. 
2 Implementation 
Figure 1 shows the overall system architecture. 
Before the bootstrapping is started, a large raw 
training corpus is parsed. The bootstrapping ex-
periment reported in this paper is based on a cor-
pus containing ~100,000 news articles and totally 
~88,000,000 words. The parsed corpus is saved 
into a repository, which supports fast retrieval by 
keyword based indexing scheme. 
 
Repository
(parsed corpus)
Decision List 
NE Learning
HMM 
NE Learning
Concept-based Seeds
parsing-based NE rules
training corpus 
based on tagged NEs
NE tagging using   parsing-based rules
NE 
Tagger
Bootstrapping Procedure
Bootstrapping Procedure
 
Figure 1. Bootstrapping System Architecture 
 
The unsupervised bootstrapping is performed as 
follows: 
1. User provides concept-based seeds; 
2. Retrieve parsing structures involving con-
cept-based seeds from the repository to train 
a decision list for NE classification; 
3.  Apply the learned rules to the NE candidates 
retrieved from the repository; 
4.  Construct an NE annotated corpus using the 
tagged proper names and their neighboring 
words; 
5. Train an HMM based on the annotated cor-
pus. 
A parser is necessary for concept-based NE 
bootstrapping. This is due to the fact that concept-
based seeds only share pattern similarity with the 
corresponding NEs at structural level, not at string 
sequence level.  In fact, the anaphoric function of 
pronouns and common nouns to represent antece-
dent NEs indicates the substitutability of proper 
names by the noun phrases headed by the corre-
sponding common nouns or pronouns. For exam-
ple, this man can substitute the proper name John 
Smith in almost all structural patterns. 
Five binary dependency relationships decoded 
by our parser are used for parsing-based NE rule 
learning:  (i) a Has_Predicate(b): from logical sub-
ject a to verb b; (ii) a Object_Of(b): from logical 
object a to verb b; (iii) a Has_Amod(b): from noun 
a to its adjective modifier b; (iv) a Possess(b): 
from the possessive noun-modifier a to head noun 
b; (v) a IsA(b):  equivalence relation (including 
appositions)  from one NP a to another NP b. 
The concept-based seeds used in the experi-
ments are: (i) he, she, his, her, him, man, woman 
for PER; (ii) city, province, town, village for LOC; 
(iii) company, firm, organization, bank, airline, 
army, committee, government, school, university 
for ORG.  
From the parsed corpus in the repository, all in-
stances (821,267) of the concept-based seeds in-
volved in the five dependency relations are 
retrieved. Each seed instance was assigned a con-
cept tag corresponding to NE. For example, each 
instance of he is marked as PER. The instances 
with concept tagging plus their associated parsing 
relationships are equivalent to an annotated NE 
corpus. Based on this training corpus, the Decision 
List Learning algorithm [Segal & Etzioni 1994] is 
used. The accuracy of each rule was evaluated us-
ing Laplace smoothing as follows, 
No.category  NEnegativepositive
1positive
++
+
=
accuracy
 
As the PER tag dominates the corpus due to the 
high occurrence frequency of he and she, learning 
is biased towards PER as the answer. To correct 
this bias, we employ the following modification 
scheme for instance count. Suppose there are a to-
tal of PERN  PER instances, LOCN  LOC instances, 
ORGN ORG instances, then in the process of rule 
accuracy evaluation, the involved instance count 
for any NE type will be adjusted by the coefficient 
NE
ORGLOCPERmin
N
) , N, N(N . 
A total of 1,290 parsing-based NE rules, shown 
in samples below, are learned, with accuracy 
higher than 0.9.  
 
Possess(wife)   PER 
Has_Predicate(divorce)  PER 
Object_Of(deport)  PER 
Possess(mayor)  LOC 
Has_AMod(coastal)  LOC 
Possess(ceo)  ORG 
Has_AMod(non-profit)  ORG 
Has_AMod(non-governmental)  ORG 
???? 
Due to the unique equivalence nature of the IsA 
relation, we add the following IsA-based rules to 
the top of the decision list: IsA(seed) tag of the 
seed, e.g. IsA(man)  PER 
The parsing-based first learner is used to tag a 
raw corpus. First, we retrieve all the named entity 
candidates associated with at least one of the five 
parsing relationships from the repository. After 
applying the decision list to the retrieved 1,607,709 
NE candidates, 33,104 PER names, 16,426 LOC 
names, and 11,908 ORG names are tagged. In or-
der to improve the bootstrapping performance, we 
use the heuristic one tag per domain for multi-
word NE in addition to the one sense per discourse 
principle [Gale et al1992]. These heuristics are 
found to be very helpful in both increasing positive 
instances (i.e. tag propagation) and decreasing the 
spurious instances (i.e. tag elimination). The tag 
propagation/elimination scheme is adopted from 
[Yarowsky 1995]. After this step, a total of 
367,441 proper names are classified, including 
134,722 PER names, 186,488 LOC names, and 
46,231 ORG names.  
The classified proper name instances lead to the 
construction of an automatically tagged training 
corpus, consisting of the NE instances and their 
two (left and right) neighboring words within the 
same sentence.  
In the final stage, a bi-gram HMM is trained 
based on the above training corpus. The HMM 
training process follows [Bikel 1997].  
3 Benchmarking 
We used the same blind testing corpus of 300,000 
words containing 20,000 PER, LOC and ORG in-
stances to measure performance degradation of 
unsupervised learning from the existing supervised 
NE tagger (Table 1, P for Precision, R for Recall, F 
for F-measure and F/D for F-measure degradation). 
 
Table 1: Supervised-to-Unsupervised NE Degradation 
 Supervised NE Unsupervised NE  
TYPE P R F P R F F/D 
PER 92.3% 93.1% 92.7% 86.6% 88.9% 87.7% 5.0%
LOC 89.0% 87.7% 88.3% 82.9% 81.7% 82.3% 6.0%
ORG 85.7% 87.8% 86.7% 57.1% 48.9% 52.7% 34.0%
 
The performance for PER and LOC are above 
80%, and approaching the performance of super-
vised learning. The reason of the unsatisfactory 
performance of ORG (52.7%) is not difficult to 
understand. There are numerous sub-types of ORG 
that cannot be represented by the less than a dozen 
concept-based seeds used for this experiment.  
In addition to the key NE types in MUC, we 
also tested this method for recognizing user-
defined NE types. We use the following concept-
based seeds for PRODUCT (PRO) NE: car, truck, 
vehicle, product, plane, aircraft, computer, soft-
ware, operating system, database, book, platform, 
network. Table 2 shows the benchmarks for 
PRODUCT tagging. 
 
Table 2: Performance for PRODUCT NE  
TYPE PRECISION RECALL F-MEASURE
PRODUCT 67.27% 72.52% 69.80%
References 
Bikel, D. M. 1997. Nymble: a high-performance learn-
ing name-finder. Proceedings of ANLP?97, 194-201, 
Morgan Kaufmann Publishers. 
Borthwick, A. et al 1998. Description of the MENE 
named Entity System. Proceedings of MUC-7. 
Collins, M. and Y. Singer. 1999. Unsupervised Models    
for Named Entity Classification. Proceedings of the 
Joint SIGAT Conference on EMNLP and 
VLC. ???Association for Computational    Linguis-
tics, 1999. 
Cucchiarelli, A. and P. Velardi. 2001. Unsupervised 
Named Entity Recognition Using Syntactic and Se-
mantic Contextual Evidence. Computational Linguis-
tics, Volume 27, Number 1, 123-131. 
Cucerzan, S. and D. Yarowsky. 1999. Language    Inde-
pendent Named Entity Recognition Combining    
Morphological and Contextual Evidence.     Proceed-
ings of the Joint SIGDAT Conference on    EMNLP 
and VLC, 90-99. 
Gale, W., K. Church, and D. Yarowsky. 1992. One 
Sense Per Discourse. Proceedings of the 4th DARPA 
Speech and Natural Language Workshop. 233-237. 
Kim, J., I. Kang, and K. Choi. 2002. Unsupervised 
Named Entity Classification Models and their En-
sembles. Proceedings of COLING 2002. 
MUC-7, 1998.  Proceedings of the Seventh Message 
Understanding Conference (MUC-7), published on 
the website http://www.muc.saic.com/ 
Segal, R. and O. Etzioni. 1994. Learning decision lists 
using homogeneous rules. Proceedings of the 12th 
National Conference on Artificial Intelligence.  
Yarowsky, David. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Method. Pro-
ceedings of ACL 1995. 
A Bootstrapping Approach to Named Entity Classification Using 
Successive Learners 
Cheng Niu, Wei Li, Jihong Ding, Rohini K. Srihari 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221. USA. 
{cniu, wei, jding, rohini}@cymfony.com 
 
Abstract 
This paper presents a new bootstrapping 
approach to named entity (NE) 
classification. This approach only requires 
a few common noun/pronoun seeds that 
correspond to the concept for the target 
NE type, e.g. he/she/man/woman for 
PERSON NE. The entire bootstrapping 
procedure is implemented as training two 
successive learners: (i) a decision list is 
used to learn the parsing-based high 
precision NE rules; (ii) a Hidden Markov 
Model is then trained to learn string 
sequence-based NE patterns. The second 
learner uses the training corpus 
automatically tagged by the first learner. 
The resulting NE system approaches 
supervised NE performance for some NE 
types. The system also demonstrates 
intuitive support for tagging user-defined 
NE types. The differences of this 
approach from the co-training-based NE 
bootstrapping are also discussed. 
1 Introduction 
Named Entity (NE) tagging is a fundamental task 
for natural language processing and information 
extraction. An NE tagger recognizes and classifies 
text chunks that represent various proper names, 
time, or numerical expressions. Seven types of 
named entities are defined in the Message 
Understanding Conference (MUC) standards, 
namely, PERSON (PER), ORGANIZATION 
(ORG), LOCATION (LOC), TIME, DATE, 
MONEY, and PERCENT1 (MUC-7 1998). 
                                                 
1 This paper only focuses on classifying proper names. Time and 
numerical NEs are not yet explored using this method.  
There is considerable research on NE tagging 
using different techniques. These include systems 
based on handcrafted rules (Krupka 1998), as well 
as systems using supervised machine learning, 
such as the Hidden Markov Model (HMM) (Bikel  
1997) and the Maximum Entropy Model 
(Borthwick 1998).   
The state-of-the-art rule-based systems and 
supervised learning systems can reach near-human 
performance for NE tagging in a targeted domain. 
However, both approaches face a serious 
knowledge bottleneck, making rapid domain 
porting difficult. Such systems cannot effectively 
support user-defined named entities. That is the 
motivation for using unsupervised or weakly-
supervised machine learning that only requires a 
raw corpus from a given domain for this NE 
research. 
(Cucchiarelli & Velardi 2001) discussed 
boosting the performance of an existing NE tagger 
by unsupervised learning based on parsing 
structures. (Cucerzan & Yarowsky 1999), (Collins 
& Singer 1999) and (Kim 2002) presented various 
techniques using co-training schemes for NE 
extraction seeded by a small list of proper names 
or handcrafted NE rules. NE tagging has two tasks: 
(i) NE chunking; (ii) NE classification. Parsing-
supported NE bootstrapping systems including 
ours only focus on NE classification, assuming NE 
chunks have been constructed by the parser.       
The key idea of co-training is the separation of 
features into several orthogonal views. In case of 
NE classification, usually one view uses the 
context evidence and the other relies on the lexicon 
evidence. Learners corresponding to different 
views learn from each other iteratively. 
One issue of co-training is the error propagation 
problem in the process of the iterative learning. 
The rule precision drops iteration-by-iteration. In 
the early stages, only few instances are available 
for learning. This makes some powerful statistical 
models such as HMM difficult to use due to the 
extremely sparse data.  
This paper presents a new bootstrapping 
approach using successive learning and concept-
based seeds.  The successive learning is as follows. 
First, some parsing-based NE rules are learned 
with high precision but limited recall. Then, these 
rules are applied to a large raw corpus to 
automatically generate a tagged corpus. Finally, an 
HMM-based NE tagger is trained using this 
corpus. There is no iterative learning between the 
two learners, hence the process is free of the error 
propagation problem. The resulting NE system 
approaches supervised NE performance for some 
NE types. 
To derive the parsing-based learner, instead of 
seeding the bootstrapping process with NE 
instances from a proper name list or handcrafted 
NE rules as (Cucerzan & Yarowsky 1999), 
(Collins & Singer 1999) and (Kim 2002), the 
system only requires a few common noun or 
pronoun seeds that correspond to the concept for 
the targeted NE, e.g. he/she/man/woman for 
PERSON NE. Such concept-based seeds share 
grammatical structures with the corresponding 
NEs, hence a parser is utilized to support 
bootstrapping. Since pronouns and common nouns 
occur more often than NE instances, richer 
contextual evidence is available for effective 
learning. Using concept-based seeds, the parsing-
based NE rules can be learned in one iteration so 
that the error propagation problem in the iterative 
learning can be avoided.  
This method is also shown to be effective for 
supporting NE domain porting and is intuitive for 
configuring an NE system to tag user-defined NE 
types. 
The remaining part of the paper is organized as 
follows. The overall system design is presented in 
Section 2. Section 3 describes the parsing-based 
NE learning. Section 4 presents the automatic 
construction of annotated NE corpus by parsing-
based NE classification. Section 5 presents the 
string level HMM NE learning. Benchmarks are 
shown in Section 6. Section 7 is the Conclusion. 
2 System Design 
Figure 1 shows the overall system architecture. 
Before the bootstrapping is started, a large raw 
training corpus is parsed by the English parser 
from our InfoXtract system (Srihari et al 2003).  
The bootstrapping experiment reported in this 
paper is based on a corpus containing ~100,000 
news articles and a total of ~88,000,000 words. 
The parsed corpus is saved into a repository, which 
supports fast retrieval by a keyword-based 
indexing scheme. 
Although the parsing-based NE learner is found 
to suffer from the recall problem, we can apply the 
learned rules to a huge parsed corpus. In other 
words, the availability of an almost unlimited raw 
corpus compensates for the modest recall. As a 
result, large quantities of NE instances are 
automatically acquired. An automatically 
annotated NE corpus can then be constructed by 
extracting the tagged instances plus their 
neighboring words from the repository. 
 
Repository
(parsed corpus)
Decision List 
NE Learning
HMM 
NE Learning
Concept-based Seeds
parsing-based NE rules
training corpus 
based on tagged NEs
NE tagging using   parsing-based rules
NE 
Tagger
Bootstrapping Procedure
Bootstrapping Procedure
 
Figure 1. Bootstrapping System Architecture 
 
The bootstrapping is performed as follows: 
1. Concept-based seeds are provided by the 
user. 
2. Parsing structures involving concept-based 
seeds are retrieved from the repository to 
train a decision list for NE classification. 
3. The learned rules are applied to the NE 
candidates stored in the repository. 
4. The proper names tagged in Step 3 and 
their neighboring words are put together as 
an NE annotated corpus. 
5. An HMM is trained based on the annotated 
corpus. 
3 Parsing-based NE Rule Learning 
The training of the first NE learner has three major 
properties: (i) the use of concept-based seeds, (ii) 
support from the parser, and (iii) representation as 
a decision list.  
This new bootstrapping approach is based on 
the observation that there is an underlying concept 
for any proper name type and this concept can be 
easily expressed by a set of common nouns or 
pronouns, similar to how concepts are defined by 
synsets in WordNet (Beckwith 1991).  
Concept-based seeds are conceptually 
equivalent to the proper name types that they 
represent. These seeds can be provided by a user 
intuitively. For example, a user can use pill, drug, 
medicine, etc. as concept-based seeds to guide the 
system in learning rules to tag MEDICINE names. 
This process is fairly intuitive, creating a favorable 
environment for configuring the NE system to the 
types of names sought by the user. 
An important characteristic of concept-based 
seeds is that they occur much more often than 
proper name seeds, hence they are effective in 
guiding the non-iterative NE bootstrapping.  
A parser is necessary for concept-based NE 
bootstrapping. This is due to the fact that concept-
based seeds only share pattern similarity with the 
corresponding NEs at structural level, not at string 
sequence level. For example, at string sequence 
level, PERSON names are often preceded by a set 
of prefixing title words Mr./Mrs./Miss/Dr. etc., but 
the corresponding common noun seeds 
man/woman etc. cannot appear in such patterns. 
However, at structural level, the concept-based 
seeds share the same or similar linguistic patterns 
(e.g. Subject-Verb-Object patterns) with the 
corresponding types of proper names.  
The rationale behind using concept-based seeds 
in NE bootstrapping is similar to that for parsing-
based word clustering (Lin 1998): conceptually 
similar words occur in structurally similar context.  
In fact, the anaphoric function of pronouns and 
common nouns to represent antecedent NEs 
indicates the substitutability of proper names by 
the corresponding common nouns or pronouns. For 
example, this man can be substituted for the proper 
name John Smith in almost all structural patterns. 
Following the same rationale, a bootstrapping 
approach is applied to the semantic lexicon 
acquisition task [Thelen & Riloff. 2002]. 
The InfoXtract parser supports dependency 
parsing based on the linguistic units constructed by 
our shallow parser (Srihari et al 2003). Five types 
of the decoded dependency relationships are used 
for parsing-based NE rule learning.  These are all 
directional, binary dependency links between 
linguistic units:   
 
(1) Has_Predicate: from logical subject to verb 
e.g.  He said she would want him to join.  
he: Has_Predicate(say) 
she: Has_Predicate(want) 
him: Has_Predicate(join) 
(2) Object_Of : from logical object to verb 
e.g.  This company was founded to provide  
new telecommunication services.  
company: Object_Of(found) 
service: Object_Of(provide) 
(3) Has_Amod: from noun to its adjective modifier 
e.g. He is a smart, handsome young man.  
man: Has_AMod(smart) 
man: Has_AMod(handsome) 
man: Has_AMod(young)  
(4) Possess: from the possessive noun-modifier to 
head noun 
e.g. His son was elected as mayor of the city.  
his: Possess(son) 
city: Possess(mayor) 
(5) IsA:  equivalence relation from one NP to 
another NP  
e.g. Microsoft spokesman John Smith is a  
popular man.  
spokesman: IsA(John Smith) 
John Smith: IsA(man) 
 
The concept-based seeds used in the 
experiments are: 
 
1. PER: he, she, his, her, him, man, woman 
2. LOC: city, province, town, village 
3. ORG: company, firm, organization, bank, 
airline, army, committee, government, 
school, university 
4. PRO: car, truck, vehicle, product, plane, 
aircraft, computer, software, operating 
system, data-base, book, platform, network 
 
Note that the last target tag PRO (PRODUCT) 
is beyond the MUC NE standards: we added this 
NE type for the purpose of testing the system?s 
capability in supporting user-defined NE types.  
From the parsed corpus in the repository, all 
instances of the concept-based seeds associated 
with one or more of the five dependency relations 
are retrieved:  821,267 instances in total in our 
experiment. Each seed instance was assigned a 
concept tag corresponding to NE. For example, 
each instance of he is marked as PER. The marked 
instances plus their associated parsing relationships 
form an annotated NE corpus, as shown below: 
 
he/PER:   Has_Predicate(say) 
she/PER:   Has_Predicate(get) 
company/ORG:  Object_Of(compel) 
city/LOC:   Possess(mayor) 
car/PRO:  Object_Of(manufacture) 
HasAmod(high-quality) 
???? 
 
This training corpus supports the Decision List 
Learning which learns homogeneous rules (Segal 
& Etzioni 1994). The accuracy of each rule was 
evaluated using Laplace smoothing: 
 
No.category  NEnegativepositive
1positive
++
+
=accuracy  
 
It is noteworthy that the PER tag dominates the 
corpus due to the fact that the pronouns he and she 
occur much more often than the seeded common 
nouns. So the proportion of NE types in the 
instances of concept-based seeds is not the same as 
the proportion of NE types in the proper name 
instances. For example, considering a running text 
containing one instance of John Smith and one 
instance of a city name Rochester, it is more likely 
that John Smith will be referred to by he/him than 
Rochester by (the) city. Learning based on such a 
corpus is biased towards PER as the answer. 
To correct this bias, we employ the following 
modification scheme for instance count. Suppose 
there are a total of PERN  PER instances, LOCN  
LOC instances, ORGN  ORG instances, PRON  PRO 
instances, then in the process of rule accuracy 
evaluation, the involved instance count for any NE 
type will be adjusted by the coefficient  
NE
PRO,ORGLOCPERmin
N
) N, N, N(N . For example, if 
the number of the training instances of PER is ten 
times that of PRO, then when evaluating a rule 
accuracy, any positive/negative count associated 
with PER will be discounted by 0.1 to correct the 
bias.  
A total of 1,290 parsing-based NE rules are 
learned, with accuracy higher than 0.9. The 
following are sample rules of the learned decision 
list: 
 
Possess(wife)  PER 
Possess(husband)  PER 
Possess(daughter)  PER 
Possess(bravery)  PER 
Possess(father)  PER 
Has_Predicate(divorce)  PER 
Has_Predicate(remarry)  PER 
Possess(brother)  PER 
Possess(son)  PER 
Possess(mother)  PER 
Object_Of(deport)  PER 
Possess(sister)  PER 
Possess(colleague)  PER 
Possess(career)  PER 
Possess(forehead)  PER 
Has_Predicate(smile)  PER 
Possess(respiratory system)  PER 
{Has_Predicate(threaten),   
  Has_Predicate(kill)} PER 
???? 
Possess(concert hall)  LOC 
Has_AMod(coastal)  LOC 
Has_AMod(northern)  LOC 
Has_AMod(eastern)  LOC 
Has_AMod(northeastern)  LOC 
Possess(undersecretary)  LOC 
Possess(mayor)  LOC 
Has_AMod(southern)  LOC 
Has_AMod(northwestern)  LOC 
Has_AMod(populous)  LOC 
Has_AMod(rogue)  LOC 
Has_AMod(southwestern)  LOC 
Possess(medical examiner)  LOC 
Has_AMod(edgy)  LOC 
???? 
Has_AMod(broad-base)  ORG 
Has_AMod(advisory)  ORG 
Has_AMod(non-profit)  ORG 
Possess(ceo)  ORG 
Possess(operate loss)  ORG 
Has_AMod(multinational)  ORG 
Has_AMod(non-governmental)  ORG 
Possess(filings)  ORG 
Has_AMod(interim)  ORG 
Has_AMod(for-profit)  ORG 
Has_AMod(not-for-profit)  ORG 
Has_AMod(nongovernmental)  ORG 
Object_Of(undervalue)  ORG 
???? 
Has_AMod(handheld)  PRO 
Has_AMod(unman)  PRO 
Has_AMod(well-sell)  PRO 
Has_AMod(value-add)  PRO 
Object_Of(refuel)  PRO 
Has_AMod(fuel-efficient)  PRO 
Object_Of(vend)  PRO 
Has_Predicate(accelerate)  PRO 
Has_Predicate(collide)  PRO 
Object_Of(crash)  PRO 
Has_AMod(scalable)  PRO 
Possess(patch)  PRO 
Object_Of(commercialize)PRO  
Has_AMod(custom-design)  PRO 
Possess(rollout)  PRO 
Object_Of(redesign)  PRO 
???? 
 
Due to the unique equivalence nature of the IsA 
relation, the above bootstrapping procedure can 
hardly learn IsA-based rules. Therefore, we add the 
following IsA-based rules to the top of the decision 
list: IsA(seed) tag of the seed, for example: 
 
IsA(man)  PER 
IsA(city)  LOC 
IsA(company)  ORG 
IsA(software)  PRO 
4 Automatic Construction of Annotated 
NE Corpus 
In this step, we use the parsing-based first learner 
to tag a raw corpus in order to train the second NE 
learner. 
One issue with the parsing-based NE rules is 
modest recall. For incoming documents, 
approximately 35%-40% of the proper names are 
associated with at least one of the five parsing 
relations. Among these proper names associated 
with parsing relations, only ~5% are recognized by 
the parsing-based NE rules. 
So we adopted the strategy of applying the 
parsing-based rules to a large corpus (88 million 
words), and let the quantity compensate for the 
sparseness of tagged instances. A repository level 
consolidation scheme is also used to improve the 
recall.  
The NE classification procedure is as follows. 
From the repository, all the named entity 
candidates associated with at least one of the five 
parsing relationships are retrieved. An NE 
candidate is defined as any chunk in the parsed 
corpus that is marked with a proper name Part-Of-
Speech (POS) tag (i.e. NNP or NNPS). A total of 
1,607,709 NE candidates were retrieved in our 
experiment. A small sample of the retrieved NE 
candidates with the associated parsing 
relationships are shown below: 
 
Deep South : Possess(project) 
Ramada : Possess(president) 
Argentina : Possess(first lady) 
???? 
 
After applying the decision list to the above the 
NE candidates, 33,104 PER names, 16,426 LOC 
names, 11,908 ORG names and 6,280 PRO names 
were extracted. 
It is a common practice in the bootstrapping 
research to make use of heuristics that suggest 
conditions under which instances should share the 
same answer. For example, the one sense per 
discourse principle is often used for word sense 
disambiguation (Gale et al 1992). In this research, 
we used the heuristic one tag per domain for multi-
word NE in addition to the one sense per discourse 
principle. These heuristics were found to be very 
helpful in improving the performance of the 
bootstrapping algorithm for the purpose of both 
increasing positive instances (i.e. tag propagation) 
and decreasing the spurious instances (i.e. tag 
elimination). The following are two examples to 
show how the tag propagation and elimination 
scheme works.  
Tyco Toys occurs 67 times in the corpus, and 11 
instances are recognized as ORG, only one 
instance is recognized as PER. Based on the 
heuristic one tag per domain for multi-word NE, 
the minority tag of PER is removed, and all the 67 
instances of Tyco Toys are tagged as ORG. 
Three instances of Postal Service are 
recognized as ORG, and two instances are 
recognized as PER. These tags are regarded as 
noise, hence are removed by the tag elimination 
scheme.  
The tag propagation/elimination scheme is 
adopted from (Yarowsky 1995). After this step, a 
total of 386,614 proper names were recognized, 
including 134,722 PER names, 186,488 LOC 
names, 46,231 ORG names and 19,173 PRO 
names. The overall precision was ~90%. The 
benchmark details will be shown in Section 6. 
The extracted proper name instances then led to 
the construction of a fairly large training corpus 
sufficient for training the second NE learner. 
Unlike manually annotated running text corpus, 
this corpus consists of only sample string 
sequences containing the automatically tagged NE 
instances and their left and right neighboring 
words within the same sentence. The two 
neighboring words are always regarded as common 
words while constructing the corpus. This is based 
on the observation that the proper names usually 
do not occur continuously without any punctuation 
in between. 
A small sample of the automatically 
constructed corpus is shown below: 
 
in <LOC> Argentina </LOC> . 
<LOC> Argentina </LOC> 's 
and <PER> Troy Glaus </PER> walk 
call <ORG> Prudential Associates </ORG> .  
, <PRO> Photoshop </PRO> has 
not <PER> David Bonderman </PER> , 
???? 
 
This corpus is used for training the second NE 
learner based on evidence from string sequences, 
to be described in Section 5 below. 
5 String Sequence-based NE Learning 
String sequence-based HMM learning is set as our 
final goal for NE bootstrapping because of the 
demonstrated high performance of this type of NE 
taggers.  
In this research, a bi-gram HMM is trained 
based on the sample strings in the annotated corpus 
constructed in section 4. During the training, each 
sample string sequence is regarded as an 
independent sentence. The training process is 
similar to (Bikel 1997).  
The HMM is defined as follows: Given a word 
sequence nn00 fwfwsequenceW =  (where 
jf denotes a single token feature which will be 
defined below), the goal for the NE tagging task is 
to find the optimal NE tag sequence 
n210 ttttsequence T = , which maximizes the 
conditional probability sequence)W |sequence Pr(T  
(Bikel 1997). By Bayesian equality, this is 
equivalent to maximizing the joint probability 
sequence) Tsequence,Pr(W . This joint probability 
can be computed by bi-gram HMM as follows:  
 
? ?=
i
)t,f,w|t,f,wPr(
sequence) T sequence,Pr(W 
1i1-i1-iiii
 
The back-off model is as follows,  
 
)t,w|)Pr(tt,t|f,wPr()-(1
)t,f,w|t,f,w(P
)t,f,w|t,f,wPr(
1i1ii1iiii1
1i1-i1-iiii01
1i1-i1-iiii
???
?
?
+
=
?
?  
 
where V denotes the size of the vocabulary, the 
back-off coefficients ??s are determined using the 
Witten-Bell smoothing algorithm. The quantities 
)t,,w|t,f,w(P 1i11iiii0 ??? if , 
)t,t|f,w(P 1iiii0 ? , )t,w|(tP 1i1-ii0 ? ,
)t|f,w(P iii0 , )t|(fP ii0 , )w|(tP 1-ii0 , )(tP i0 , and 
)t|(wP ii0  are computed by the maximum 
likelihood estimation.  
We use the following single token feature set 
for HMM training. The definitions of these 
features are the same as in (Bikel 1997). 
 
 
 
)t | f,w Pr( ) - (1 )t,t | f, w (P 
)t,t |f,w Pr(
iii 2 1iiii02
1iiii
? ? + = 
?
?
 
)w | Pr(t ) -(1 )t ,w|(tP 
)t ,w | Pr(t
1 - i i 3 1i1-ii03
1i1-ii
? ? + = 
?
?
 
)t | (f)Pt | (w Pr ) -(1)t |f,w (P 
)t|f,w Pr(
ii0i i 4 iii04
iii
? ? + = 
) t ( P ) - (1 ) w | (t P ) w | Pr(t i0 5 1 - ii051-ii ? ? + = 
V 
1 ) - (1 )t |(wP)t| Pr(w 6 ii06ii ? ? + = 
twoDigitNum, fourDigitNum, 
containsDigitAndAlpha,  
containsDigitAndDash, 
containsDigitAndSlash,  
containsDigitAndComma,  
containsDigitAndPeriod, otherNum, allCaps,  
capPeriod, initCap, lowerCase, other.  
6 Benchmarking and Discussion 
Two types of benchmarks were measured: (i) the 
quality of the automatically constructed NE 
corpus, and (ii) the performance of the HMM NE 
tagger. The HMM NE tagger is considered to be 
the resulting system for application. The 
benchmarking shows that this system approaches 
the performance of supervised NE tagger for two 
of the three proper name NE types in MUC, 
namely, PER NE and LOC NE. 
We used the same blind testing corpus of 
300,000 words containing 20,000 PER, LOC and 
ORG instances that were truthed in-house 
originally for benchmarking the existing 
supervised NE tagger (Srihari, Niu & Li 2000). 
This has the benefit of precisely measuring 
performance degradation from the supervised 
learning to unsupervised learning. The 
performance of our supervised NE tagger using the 
MUC scorer is shown in Table 1. 
 
Table 1. Performance of Supervised NE Tagger 
Type Precision Recall F-Measure
PERSON 92.3% 93.1% 92.7% 
LOCATION 89.0% 87.7% 88.3% 
ORGANIZATION 85.7% 87.8% 86.7% 
 
To benchmark the quality of the automatically 
constructed corpus (Table 2), the testing corpus is 
first processed by our parser and then saved into 
the repository. The repository level NE 
classification scheme, as discussed in section 4, is 
applied. From the recognized NE instances, the 
instances occurring in the testing corpus are 
compared with the answer key.  
 
Table 2. Quality of the Constructed Corpus 
Type Precision 
PERSON 94.3% 
LOCATION 91.7% 
ORGANIZATION 88.5% 
To benchmark the performance of the HMM 
tagger, the testing corpus is parsed. The noun 
chunks with proper name POS tags (NNP and 
NNPS) are extracted as NE candidates. The 
preceding word and the succeeding word of the NE 
candidates are also extracted.  Then we apply the 
HMM to the NE candidates with their neighboring 
context. The NE classification results are shown in 
Table 3.  
 
Table 3. Performance of the second HMM NE 
Type Precision Recall F-Measure
PERSON 86.6% 88.9% 87.7% 
LOCATION 82.9% 81.7% 82.3% 
ORGANIZATION 57.1% 48.9% 52.7% 
 
Compared with our existing supervised NE 
tagger, the degradation using the presented 
bootstrapping method for PER NE, LOC NE, and 
ORG NE are 5%, 6%, and 34% respectively.  
The performance for PER and LOC are above 
80%, approaching the performance of supervised 
learning. The reason for the low recall of ORG 
(~50%) is not difficult to understand. For PERSON 
and LOCATION, a few concept-based seeds seem 
to be sufficient in covering their sub-types (e.g. the 
sub-types COUNTRY, CITY, etc for 
LOCATION). But there are hundreds of sub-types 
of ORG that cannot be covered by less than a 
dozen concept-based seeds, which we used. As a 
result, the recall of ORG is significantly affected. 
Due to the same fact that ORG contains many 
more sub-types, the results are also noisier, leading 
to lower precision than that of the other two NE 
types. Some threshold can be introduced, e.g. 
perplexity per word, to remove spurious ORG tags 
in improving the precision. As for the recall issue, 
fortunately, in a real-life application, the 
organization type that a user is interested in usually 
is in a fairly narrow spectrum. We believe that the 
performance will be better if only company names 
or military organization names are targeted. 
In addition to the key NE types in MUC, our 
system is able to recognize another NE type, 
namely, PRODUCT (PRO) NE. We instructed our 
truthing team to add this NE type into the testing 
corpus which contains ~2,000 PRO instances. 
Table 4 shows the performance of the HMM on the 
PRO tag. 
 
Table 4. Performance of PRODUCT NE 
TYPE PRECISION RECALL F-MEASURE
PRODUCT 67.3% 72.5% 69.8% 
 
Similar to the case of ORG NEs, the number of 
concept-based seeds is found to be insufficient to 
cover the variations of PRO subtypes. So the 
performance is not as good as PER and LOC NEs. 
Nevertheless, the benchmark shows the system 
works fairly effectively in extracting the user-
specified NEs. It is noteworthy that domain 
knowledge such as knowing the major sub-types of 
the user-specified NE type is valuable in assisting 
the selection of appropriate concept-based seeds 
for performance enhancement. 
The performance of our HMM tagger is 
comparable with the reported performance in 
(Collins & Singer 1999). But our benchmarking is 
more extensive as we used a much larger data set 
(20,000 NE instances in the testing corpus) than 
theirs (1,000 NE instances).  
7 Conclusion 
A novel bootstrapping approach to NE 
classification is presented. This approach does not 
require iterative learning which may suffer from 
error propagation. With minimal human 
supervision in providing a handful of concept-
based seeds, the resulting NE tagger approaches 
supervised NE performance in NE types for 
PERSON and LOCATION. The system also 
demonstrates effective support for user-defined NE 
classification. 
Acknowledgement 
This work was partly supported by a grant from the 
Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-01-C-0035. The authors wish to thank 
Carrie Pine and Sharon Walter of AFRL for 
supporting and reviewing this work. 
References 
 
Bikel, D. M. 1997. Nymble: a high-performance 
learning name-finder. Proceedings of ANLP 1997, 
194-201, Morgan Kaufmann Publishers. 
Beckwith, R. et al 1991.  WordNet: A Lexical Database 
Organized on Psycholinguistic Principles.  Lexicons: 
Using On-line Resources to build a Lexicon, Uri 
Zernik, editor, Lawrence Erlbaum, Hillsdale, NJ. 
Borthwick, A. et al 1998. Description of the MENE 
named Entity System. Proceedings of MUC-7. 
Collins, M. and Y. Singer. 1999. Unsupervised Models    
for Named Entity Classification. Proceedings of the 
1999 Joint SIGDAT Conference on EMNLP and VLC. 
Cucchiarelli, A. and P. Velardi. 2001. Unsupervised 
Named Entity Recognition Using Syntactic and Se-
mantic Contextual Evidence. Computational 
Linguistics, Volume 27, Number 1, 123-131. 
Cucerzan, S. and D. Yarowsky. 1999. Language    
Independent Named Entity Recognition Combining    
Morphological and Contextual Evidence.     
Proceedings of the 1999 Joint SIGDAT Conference on    
EMNLP  and VLC, 90-99. 
Gale, W., K. Church, and D. Yarowsky. 1992. One 
Sense Per Discourse. Proceedings of the 4th DARPA 
Speech and Natural Language Workshop. 233-237. 
Kim, J., I. Kang, and K. Choi. 2002. Unsupervised 
Named Entity Classification Models and their 
Ensembles. COLING 2002. 
Krupka, G. R. and K. Hausman. 1998. IsoQuest Inc: 
Description of the NetOwl Text Extraction System as 
used for MUC-7. Proceedings of MUC-7. 
Lin, D.K. 1998. Automatic Retrieval and Clustering of 
Similar Words. COLING-ACL 1998. 
MUC-7, 1998.  Proceedings of the Seventh Message 
Understanding Conference (MUC-7).  
Thelen, M. and E. Riloff. 2002. A Bootstrapping 
Method for Learning Semantic Lexicons using 
Extraction Pattern Contexts. Proceedings of EMNLP 
2002.  
Segal, R. and O. Etzioni. 1994. Learning decision lists 
using homogeneous rules. Proceedings of the 12th 
National Conference on Artificial Intelligence.  
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: An Information Discovery Engine 
Supported by New Levels of Information Extraction.  
Proceeding of HLT-NAACL 2003 Workshop on 
Software Engineering and Architecture of Language 
Technology Systems, Edmonton, Canada. 
Srihari, R., C. Niu, & W. Li. 2000. A Hybrid Approach 
for Named Entity and Sub-Type Tagging.  
Proceedings of ANLP 2000, Seattle.  
Yarowsky, David. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Method. ACL 
1995.  
An Expert Lexicon Approach to Identifying English Phrasal Verbs 
 
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, Rohini Srihari  
 
Cymfony Inc. 
600 Essjay Road 
Williamsville, NY 14221, USA 
{wei, xzhang, cniu, yjiang, rohini}@Cymfony.com
 
Abstract 
Phrasal Verbs are an important feature 
of the English language. Properly 
identifying them provides the basis for 
an English parser to decode the related 
structures. Phrasal verbs have been a 
challenge to Natural Language 
Processing (NLP) because they sit at 
the borderline between lexicon and 
syntax. Traditional NLP frameworks 
that separate the lexicon module from 
the parser make it difficult to handle 
this problem properly.  This paper 
presents a finite state approach that 
integrates a phrasal verb expert lexicon 
between shallow parsing and deep 
parsing to handle morpho-syntactic 
interaction. With precision/recall 
combined performance benchmarked 
consistently at 95.8%-97.5%, the 
Phrasal Verb identification problem 
has basically been solved with the 
presented method.    
1 Introduction 
Any natural language processing (NLP) system 
needs to address the issue of handling  multiword 
expressions, including Phrasal Verbs (PV) [Sag 
et al 2002; Breidt et al 1996]. This paper 
presents a proven approach to identifying 
English PVs based on pattern matching using a 
formalism called Expert Lexicon.   
Phrasal Verbs are an important feature of the 
English language since they form about one third 
of the English verb vocabulary. 1  Properly 
                                                     
1 For the verb vocabulary of our system based on  
machine-readable dictionaries and two Phrasal Verb 
dictionaries, phrasal verb entries constitute 33.8% of 
the entries. 
recognizing PVs is an important condition for  
English parsing. Like single-word verbs, each 
PV has its own lexical features including 
subcategorization features that determine its 
structural patterns [Fraser 1976; Bolinger 1971; 
Pelli 1976; Shaked 1994], e.g., look for has 
syntactic subcategorization and semantic features 
similar to those of search; carry?on shares 
lexical features with continue. Such lexical 
features can be represented in the PV lexicon in 
the same way as those for single-word verbs, but 
a parser can only use them when the PV is 
identified. 
Problems like PVs are regarded as ?a pain in 
the neck for NLP? [Sag et al 2002]. A proper 
solution to this problem requires tighter 
interaction between syntax and lexicon than 
traditionally available [Breidt et al 1994].  
Simple lexical lookup leads to severe 
degradation in both precision and recall, as our 
benchmarks show (Section 4). The recall 
problem is mainly due to separable PVs such as 
turn?off which allow for syntactic units to be 
inserted inside the PV compound, e.g., turn it off, 
turn the radio off.  The precision problem is 
caused by the ambiguous function of the particle. 
For example, a simple lexical lookup will mistag 
looked for as a phrasal verb in sentences such as 
He looked for quite a while but saw nothing. 
In short, the traditional NLP framework that 
separates the lexicon module from a parser 
makes it difficult to handle this problem properly.  
This paper presents an expert lexicon approach 
that integrates the lexical module with contextual 
checking based on shallow parsing results.  
Extensive blind benchmarking shows that this 
approach is very effective for identifying phrasal 
verbs, resulting in the precision/recall combined 
F-score of about 96%.   
 The remaining text is structured as follows. 
Section 2 presents the problem and defines the 
task. Section 3 presents the Expert Lexicon 
formalism and illustrates the use of this 
formalism in solving this problem. Section 4 
shows the benchmarking and analysis, followed 
by conclusions in Section 5. 
2 Phrasal Verb Challenges  
This section defines the problems we intend to 
solve, with a checklist of tasks to accomplish.  
2.1 Task Definition 
First, we define the task as the identification of 
PVs in support of deep parsing, not as the parsing 
of the structures headed by a PV. These two are 
separated as two tasks not only because of 
modularity considerations, but more importantly 
based on a natural labor division between NLP 
modules.  
Essential to the second argument is that these 
two tasks are of a different linguistic nature: the 
identification task belongs to (compounding) 
morphology (although it involves a syntactic 
interface) while the parsing task belongs to 
syntax. The naturalness of this division is 
reflected in the fact that there is no need for a 
specialized, PV-oriented parser. The same parser, 
mainly driven by lexical subcategorization 
features, can handle the structural problems for 
both phrasal verbs and other verbs. The 
following active and passive structures involving 
the PVs look after (corresponding to watch) and 
carry?on (corresponding to continue) are 
decoded by our deep parser after PV 
identification: she is being carefully ?looked 
after? (watched); we should ?carry on? (continue) 
the business for a while. 
There has been no unified definition of PVs 
among linguists. Semantic compositionality is 
often used as a criterion to distinguish a PV from 
a syntactic combination between a verb and its 
associated adverb or prepositional phrase 
[Shaked 1994]. In reality, however, PVs reside in 
a continuum from opaque to transparent in terms 
of semantic compositionality [Bolinger 1971]. 
There exist fuzzy cases such as take something 
away2 that may be included either as a PV or as a 
regular syntactic sequence. There is agreement 
                                                     
2  Single-word verbs like ?take? are often 
over-burdened with dozens of senses/uses. Treating 
marginal cases like ?take?away? as independent 
phrasal verb entries has practical benefits in relieving 
the burden and the associated noise involving ?take?.   
on the vocabulary scope for the majority of PVs, 
as reflected in the overlapping of PV entries from 
major English dictionaries.   
English PVs are generally classified into three 
major types. Type I usually takes the form of an 
intransitive verb plus a particle word that 
originates from a preposition. Hence the resulting 
compound verb has become transitive, e.g., look 
for, look after, look forward to, look into, etc. 
Type II typically takes the form of a transitive 
verb plus a particle from the set {on, off, up, 
down}, e.g., turn?on, take?off, wake?up, 
let?down. Marginal cases of particles may also 
include {out, in, away} such as take?away, 
kick ?in, pull?out.3   
Type III takes the form of an intransitive verb 
plus an adverb particle, e.g., get by, blow up, burn 
up, get off, etc. Note that Type II and Type III 
PVs have considerable overlapping in  
vocabulary, e.g., The bomb blew up vs. The 
clown blew up the balloon. The overlapping 
phenomenon can be handled by assigning both a 
transitive feature and an intransitive feature to the 
identified PVs in the same way that we treat the 
overlapping of single-word verbs.   
The first issue in handling PVs is inflection. A 
system for identifying PVs should match the 
inflected forms, both regular and irregular, of the 
leading verb.  
The second is the representation of the lexical 
identity of recognized PVs. This is to establish a 
PV (a compound word) as a syntactic atomic unit 
with all its lexical properties determined by the 
lexicon [Di Sciullo and Williams 1987]. The 
output of the identification module based on a PV 
lexicon should support syntactic analysis and 
further processing. This translates into two 
sub-tasks: (i) lexical feature assignment, and (ii) 
canonical form representation. After a PV is 
identified, its lexical features encoded in the PV 
lexicon should be assigned for a parser to use. 
The representation of a canonical form for an 
identified PV is necessary to allow for individual 
rules to be associated with identified PVs in 
further processing and to facilitate verb retrieval 
in applications. For example, if we use turn_off 
as the canonical form for the PV turn?off, 
identified in both he turned off the radio and he 
                                                     
3 These three are arguably in the gray area. Since they 
do not fundamentally affect the meaning of the 
leading verb, we do not have to treat them as phrasal 
verbs.  In principle, they can also be treated as  adverb 
complements of verbs.   
turned the radio off, a search for turn_off will 
match all and only the mentions of this PV.  
The fact that PVs are separable hurts recall. In 
particular, for Type II, a Noun Phrase (NP) object 
can be inserted inside the compound verb. NP 
insertion is an intriguing linguistic phenomenon 
involving the morpho-syntactic interface: a 
morphological compounding process needs to 
interact with the formation of a syntactic unit.  
Type I PVs also have the separability problem, 
albeit to a lesser degree.  The possible inserted 
units are adverbs in this case, e.g., look 
everywhere for, look carefully after.   
What hurts precision is spurious matches of 
PV negative instances. In a sentence with the 
structure V+[P+NP], [V+P] may be mistagged as 
a PV, as seen in the following pairs of examples 
for Type I and Type II:  
 
(1a) She [looked for] you yesterday. 
(1b) She looked [for quite a while] (but saw  
nothing). 
(2a) She [put on] the coat. 
(2b) She put [on the table] the book she  
borrowed yesterday. 
 
To summarize, the following is a checklist of 
problems that a PV identification system should 
handle: (i) verb inflection, (ii) lexical identity 
representation, (iii) separability, and (iv) 
negative instances. 
2.2 Related Work 
Two lines of research are reported in addressing 
the PV problem: (i) the use of a high-level 
grammar formalism that integrates the 
identification with parsing, and (ii) the use of a 
finite state device in identifying PVs as a lexical 
support for the subsequent parser. Both 
approaches have their own ways of handling the 
morpho-syntactic interface. 
[Sag et al 2002] and [Villavicencio et al 
2002] present their project LinGO-ERG that 
handles PV identification and parsing together. 
LingGO-ERG is based on Head-driven Phrase 
Structure Grammar (HPSG), a unification-based 
grammar formalism. HPSG provides a 
mono-stratal lexicalist framework that facilitates 
handling intricate morpho-syntactic interaction. 
PV-related morphological and syntactic 
structures are accounted for by means of a lexical 
selection mechanism where the verb morpheme 
subcategorizes for its syntactic object in addition 
to its particle morpheme. 
The LingGO-ERG lexicalist approach is 
believed to be effective. However, their coverage 
and testing of the PVs seem preliminary. The 
LinGO-ERG lexicon contains 295 PV entries, 
with no report on benchmarks.  
In terms of the restricted flexibility and 
modifiability of a system, the use of high-level 
grammar formalisms such as HPSG to integrate 
identification in deep parsing cannot be 
compared with the alternative finite state 
approach [Breidt et al 1994]. 
[Breidt et al1994]?s approach is similar to our 
work. Multiword expressions including idioms, 
collocations, and compounds as well as PVs are 
accounted for by using local grammar rules 
formulated as regular expressions. There is no 
detailed description for English PV treatment 
since their work focuses on multilingual, 
multi-word expressions in general. The authors 
believe that the local grammar implementation of 
multiword expressions can work with general 
syntax either implemented in a high-level 
grammar formalism or implemented as a local 
grammar for the required morpho-syntactic 
interaction, but this interaction is not 
implemented into an integrated system and hence 
it is impossible to properly measure performance 
benchmarks. 
There is no report on implemented solutions 
covering the entire English PVs that are fully 
integrated into an NLP system and are well tested 
on sizable real life corpora, as is presented in this 
paper.   
3 Expert Lexicon Approach  
This section illustrates the system architecture 
and presents the underlying Expert Lexicon (EL) 
formalism, followed by the description of the 
implementation details.  
3.1 System Architecture 
Figure 1 shows the system architecture that 
contains the PV Identification Module based on 
the PV Expert Lexicon.  
This is a pipeline system mainly based on 
pattern matching implemented in local grammars 
and/or expert lexicons [Srihari et al2003]. 4 
                                                     
4 POS and NE tagging are hybrid systems involving 
both hand-crafted rules and statistical learning.  
English parsing is divided into two tasks: shallow 
parsing and deep parsing. The shallow parser 
constructs Verb Groups (VGs) and basic Noun 
Phrases (NPs), also called BaseNPs [Church 
1988]. The deep parser utilizes syntactic 
subcategorization features and semantic features 
of a head (e.g., VG) to decode both syntactic and 
logical dependency relationships such as 
Verb-Subject, Verb-Object, Head-Modifier, etc. 
 
 
Part-of-Speech  
(POS) Tagging 
General
Lexicon Lexical lookup 
Named Entity  
(NE) Taggig 
Shallow Parsing 
PV Identification 
Deep parsing 
General
Lexicon 
PV Expert 
Lexicon 
Figure 1. System Architecture 
 
The general lexicon lookup component 
involves stemming that transforms regular or 
irregular inflected verbs into the base forms to 
facilitate the later phrasal verb matching. This 
component also performs indexing of the word 
occurrences in the processed document for 
subsequent expert lexicons.  
The PV Identification Module is placed 
between the Shallow Parser and the Deep Parser. 
It requires shallow parsing support for the 
required syntactic interaction and the PV output 
provides lexical support for deep parsing. 
Results after shallow parsing form a proper 
basis for PV identification. First, the inserted NPs 
and adverbial time NEs are already constructed 
by the shallow parser and NE tagger. This makes 
it easy to write pattern matching rules for 
identifying separable PVs. 
Second, the constructed basic units NE, NP 
and VG provide conditions for 
constraint-checking in PV identification. For 
example, to prevent spurious matches in 
sentences like she put the coat on the table, it is 
necessary to check that the post-particle unit 
should NOT be an NP.  The VG chunking also 
decodes the voice, tense and aspect features that 
can be used as additional constraints for PV 
identification. A sample macro rule 
active_V_Pin that checks the ?NOT passive? 
constraint and the ?NOT time?, ?NOT location? 
constraints is shown in 3.3.  
3.2 Expert Lexicon Formalism 
The Expert Lexicon used in our system is an 
index-based formalism that can associate pattern 
matching rules with lexical entries. It is 
organized like a lexicon, but has the power of a 
lexicalized local grammar.   
All Expert Lexicon entries are indexed, 
similar to the case for the finite state tool in 
INTEX [Silberztein 2000]. The pattern matching 
time is therefore reduced dramatically compared 
to a sequential finite state device [Srihari et al 
2003].5   
The expert lexicon formalism is designed to 
enhance the lexicalization of our system, in 
accordance with the general trend of lexicalist 
approaches to NLP. It is especially beneficial in 
handling problems like PVs and many individual 
or idiosyncratic linguistic phenomena that can 
not be covered by non-lexical approaches.  
Unlike the extreme lexicalized word expert 
system in [Small and Rieger 1982] and similar to 
the IDAREX local grammar formalism [Breidt et 
al.1994], our EL formalism supports a 
parameterized macro mechanism that can be 
used to capture the general rules shared by a set 
of individual entries. This is a particular useful 
mechanism that will save time for computational 
lexicographers in developing expert lexicons, 
especially for phrasal verbs, as shall be shown in 
Section 3.3 below. 
The Expert Lexicon tool provides a flexible 
interface for coordinating lexicons and syntax: 
any number of expert lexicons can be placed at 
any levels, hand-in-hand with other 
non-lexicalized modules in the pipeline 
architecture of our system.    
                                                     
5 Some other unique features of our EL formalism 
include: (i) providing the capability of proximity 
checking as rule constraints in addition to pattern 
matching using regular expressions so that the rule 
writer or lexicographer can exploit the combined 
advantages of both, and (ii) the propagation 
functionality of semantic tagging results, to 
accommodate principles like one sense per discourse. 
3.3 Phrasal Verb Expert Lexicon 
To cover the three major types of PVs, we use the 
macro mechanism to capture the shared patterns. 
For example, the NP insertion for Type II PV is 
handled through a macro called V_NP_P, 
formulated in pseudo code as follows.  
 
V_NP_P($V,$P,$V_P,$F1, $F2,?)  := 
Pattern:  
$V  
NP 
(?right?|?back?|?straight?) 
$P  
NOT NP 
Action:  
$V: %assign_feature($F1, $F2,?)   
%assign_canonical_form($V_P) 
$P: %deactivate 
 
This macro represents cases like Take the coat 
off, please; put it back on, it?s raining now. It 
consists of two parts: ?Pattern? in regular 
expression form (with parentheses for optionality, 
a bar for logical OR, a quoted string for checking 
a word or head word) and ?Action? (signified by 
the prefix %). The parameters used in the macro 
(marked by the prefix $) include the leading verb 
$V, particle $P, the canonical form $V_P, and 
features $Fn.  After the defined pattern is matched, 
a Type II separable verb is identified. The Action 
part ensures that the lexical identity be 
represented properly, i.e. the assignment of the 
lexical features and the canonical form. The 
deactivate action flags the particle as being part 
of the phrasal verb.  
In addition, to prevent a spurious case in (3b), 
the macro V_NP_P checks the contextual 
constraints that no NP (i.e. NOT NP) should 
follow a PV particle. In our shallow parsing, NP 
chunking does not include identified time NEs, 
so it will not block the PV identification in (3c). 
    
(3a) She [put the coat on]. 
(3b) She put the coat [on the table]. 
(3c) She [put the coat on] yesterday. 
 
All three types of PVs when used without NP 
insertion are handled by the same set of macros, 
due to the formal patterns they share. We use a 
set of macros instead of one single macro, 
depending on the type of particle and the voice of 
the verb, e.g., look for calls the macro 
[active_V_Pfor | passive_V_Pfor], fly in calls the 
macro [active_V_Pin | passive_V_Pin], etc.  
The distinction between active rules and 
passive rules lies in the need for different 
constraints. For example, a passive rule needs to 
check the post-particle constraint [NOT NP] to 
block the spurious case in (4b).  
 
(4a) He [turned on] the radio. 
(4b)  The world [had been turned] [on its 
head] again. 
 
As for particles, they also require different 
constraints in order to block spurious matches. 
For example, active_V_Pin (formulated below) 
requires the constraints ?NOT location NOT 
time? after the particle while active_V_Pfor only 
needs to check ?NOT time?, shown in (5) and (6). 
 
(5a) Howard [had flown in] from Atlanta. 
(5b) The rocket [would fly] [in 1999]. 
(6a) She was [looking for] California on the 
map. 
(6b) She looked [for quite a while]. 
 
active_V_Pin($V, in, $V_P,$F1, $F2,?)  := 
Pattern:  
$V NOT passive 
(Adv|time) 
$P  
NOT location NOT time 
Action:  
$V: %assign_feature($F1, $F2, ?)   
%assign_canonical_form($V_P) 
$P: %deactivate 
 
The coding of the few PV macros requires 
skilled computational grammarians and a 
representative development corpus for rule 
debugging. In our case, it was approximately 15 
person-days of skilled labor including data 
analysis, macro formulation and five iterations of 
debugging against the development corpus. But 
after the PV macros are defined, lexicographers 
can quickly develop the PV entries: it only cost 
one person-day to enter the entire PV vocabulary 
using the EL formalism and the implemented 
macros. We used the Cambridge International 
Dictionary of Phrasal Verbs and Collins Cobuild 
Dictionary of Phrasal Verbs as the major 
reference for developing our PV Expert 
Lexicon. 6  This expert lexicon contains 2,590 
entries. The EL-rules are ordered with specific 
rules placed before more general rules. A sample 
of the developed PV Expert Lexicon is shown 
below (the prefix @ denotes a macro call): 
 
abide:  @V_P_by(abide, by, abide_by, V6A, 
APPROVING_AGREEING) 
accede: @V_P_to(accede, to, accede_to, V6A, 
APPROVING_AGREEING) 
add:  @V_P(add, up, add_up, V2A, 
MATH_REASONING); 
 @V_NP_P(add, up, add_up, V6A, 
MATH_REASONING) 
???? 
 
In the above entries, V6A and V2A are 
subcategorization features for transitive and 
intransitive verb respectively, while 
APPROVING_AGREEING and 
MATH_REASONING are semantic features. 
These features provide the lexical basis for the 
subsequent parser. 
The PV identification method as described 
above resolves all the problems in the checklist. 
The following sample output shows the 
identification result: 
 
NP[That]  
VG[could slow: slow_down/V6A/MOVING] 
NP[him]  
down/deactivated . 
4 Benchmarking 
Blind benchmarking was done by two 
non-developer testers manually checking the 
results. In cases of disagreement, a third tester 
was involved in examining the case to help 
resolve it. We ran benchmarking on both the 
formal style and informal style of English text.  
4.1 Corpus Preparation 
Our development corpus (around 500 KB) 
consists of the MUC-7 (Message Understanding 
                                                     
6 Some entries that are listed in these dictionaries do 
not seem to belong to phrasal verb categories, e.g., 
relieve?of (as used in relieve somebody of something), 
remind?of (as used in remind somebody of 
something), etc.  It is generally agreed that such cases 
belong to syntactic patterns in the form of 
V+NP+P+NP that can be captured by 
subcategorization.  We have excluded these cases.  
Conference-7) dryrun corpus and an additional 
collection of news domain articles from TREC 
(Text Retrieval Conference) data.  The PV expert 
lexicon rules, mainly the macros, were developed 
and debugged using the development corpus.    
The first testing corpus (called English-zone 
corpus) was downloaded from a website that is 
designed to teach PV usage in Colloquial English 
(http://www.english-zone.com/phrasals/w-phras
als.html). It consists of 357 lines of sample 
sentences containing 347 PVs. This addresses the 
sparseness problem for the less frequently used 
PVs that rarely get benchmarked in running text 
testing. This is a concentrated corpus involving 
varieties of PVs from text sources of an informal 
style, as shown below.7 
 
"Would you care for some dessert? We have 
ice cream, cookies, or cake." 
Why are you wrapped up in that blanket? 
After John's wife died, he had to get through 
his sadness. 
After my sister cut her hair by herself, we had 
to take her to a hairdresser to even her 
hair out! 
After the fire, the family had to get by without 
a house. 
 
We have prepared two collections from the 
running text data to test written English of a more 
formal style in the general news domain:  (i) the 
MUC-7 formal run corpus (342 KB) consisting 
of 99 news articles, and (ii) a collection of 23,557 
news articles (105MB) from the TREC data.   
4.2 Performance Testing 
There is no available system known to the NLP 
community that claims a capability for PV 
treatment and could thus be used for a reasonable 
performance comparison. Hence, we have 
devised a bottom-line system and a baseline 
system for comparison with our EL-driven 
system. The bottom-line system is defined as a 
simple lexical lookup procedure enhanced with 
the ability to match inflected verb forms but with 
no capability of checking contextual constraints. 
There is no discussion in the literature on what 
                                                     
7 Proper treatment of PVs is most important in parsing 
text sources involving Colloquial English, e.g., 
interviews, speech transcripts, chat room archives. 
There is an increasing demand for NLP applications in 
handling this type of data.    
constitutes a reasonable baseline system for PV. 
We believe that a baseline system should have 
the additional, easy-to-implement ability to jump 
over inserted object case pronouns (e.g., turn it 
on) and adverbs (e.g., look everywhere for) in PV 
identification.  
Both the MUC-7 formal run corpus and the 
English-zone corpus were fed into the 
bottom-line  and the baseline systems as well as 
our EL-driven system described in Section 3.3. 
The benchmarking results are shown in Table 1 
and Table 2. The F-score is a combined measure 
of precision and recall, reflecting the overall 
performance of a system. 
Table 1.  Running Text Benchmarking 1 
 Bottom-line Baseline EL 
Correct 303 334 338 
Missing 58 27 23 
Spurious 33 34 7 
Precision 90.2% 88.4% 98.0% 
Recall 83.9% 92.5% 93.6% 
F-score 86.9% 91.6% 95.8% 
Table 2.  Sampling Corpus Benchmarking 
 Bottom-line Baseline EL 
Correct 215 244 324 
Missing 132 103 23 
Spurious 0 0 0 
Precision 100% 100% 100% 
Recall 62.0% 70.3% 93.4% 
F-score 76.5% 82.6% 96.6% 
 
Compared with the bottom-line performance 
and the baseline performance, the F-score for the 
presented method has surged 9-20 percentage 
points and 4-14 percentage points, respectively. 
The high precision (100%) in Table 2 is due to 
the fact that, unlike running text, the sampling 
corpus contains only positive instances of PV. 
This weakness, often associated with sampling 
corpora, is overcome by benchmarking running 
text corpora (Table 1 and Table 3).   
To compensate for the limited size of the 
MUC formal run corpus, we used the testing 
corpus from the TREC data. For such a large 
testing corpus (23,557 articles, 105MB), it is 
impractical for testers to read every article to 
count mentions of all PVs in benchmarking. 
Therefore, we selected three representative PVs 
look for, turn?on and blow?up and used the 
head verbs (look, turn, blow), including their 
inflected forms, to retrieve all sentences that 
contain those verbs. We then ran the retrieved 
sentences through our system for benchmarking 
(Table 3).  
All three of the blind tests show fairly 
consistent benchmarking results (F-score 
95.8%-97.5%), indicating that these benchmarks 
reflect the true capability of the presented system, 
which targets the entire PV vocabulary instead of 
a selected subset. Although there is still some 
room for further enhancement (to be discussed 
shortly), the PV identification problem is 
basically solved. 
Table 3.  Running Text Benchmarking 2 
 ?look for? ?turn?on? ?blow?up?
 Correct 1138 128 650 
 Missing 76 0 33 
 Spurious 5 9 0 
 Precision 99.6% 93.4% 100.0% 
 Recall 93.7% 100.0% 95.2% 
 F-score 96.6% 97.5% 97.5% 
4.3 Error Analysis 
There are two major factors that cause errors: (i) 
the impact of errors from the preceding modules 
(POS and Shallow Parsing), and (ii) the mistakes 
caused by the PV Expert Lexicon itself.  
The POS errors caused more problems than 
the NP grouping errors because the inserted NP 
tends to be very short, posing little challenge to 
the BaseNP shallow parsing. Some verbs 
mis-tagged as nouns by POS were missed in PV 
identification. 
There are two problems that require the 
fine-tuning of the PV Identification Module. First, 
the macros need further adjustment in their 
constraints. Some constraints seem to be too 
strong or too weak.  For example, in the Type I 
macro, although we expected the possible 
insertion of an adverb, however, the constraint on 
allowing for only one optional adverb and not 
allowing for a time adverbial is still too strong. 
As a result, the system failed to identify 
listening?to and meet?with in the following 
cases: ?was not listening very closely on 
Thursday to American concerns about human 
tights? and ... meet on Friday with his Chinese... 
The second type of problems cannot be solved 
at the macro level. These are individual problems 
that should be handled by writing specific rules 
for the related PV. An example is the possible 
spurious match of the PV have?out in the 
sentence ...still have our budget analysts out 
working the numbers. Since have is a verb with 
numerous usages, we should impose more 
individual constraints for NP insertion to prevent 
spurious matches, rather than calling a common 
macro shared by all Type II verbs.  
4.4 Efficiency Testing 
To test the efficiency of the index-based PV 
Expert Lexicon in comparison with a sequential  
Finite State Automaton (FSA) in the PV 
identification task, we conducted the following 
experiment.  
The PV Expert Lexicon was compiled as a 
regular local grammar into a large automaton that 
contains 97,801 states and 237,302 transitions. 
For a file of 104 KB (the MUC-7 dryrun corpus 
of 16,878 words), our sequential FSA  runner 
takes over 10 seconds for processing on the  
Windows NT platform with a Pentium PC. This 
processing only requires 0.36 second using the 
indexed PV Expert Lexicon module. This is 
about 30 times faster.   
5 Conclusion 
An effective and efficient approach to phrasal 
verb identification is presented. This approach 
handles both separable and inseparable phrasal 
verbs in English. An Expert Lexicon formalism 
is used to develop the entire phrasal verb lexicon 
and its associated pattern matching rules and 
macros.  This formalism allows the phrasal verb 
lexicon to be called between two levels of 
parsing for the required morpho-syntactic 
interaction in phrasal verb identification. 
Benchmarking using both the running text corpus 
and sampling corpus shows that the presented 
approach provides a satisfactory solution to this 
problem. 
In future research, we plan to extend the 
successful experiment on phrasal verbs to other 
types of multi-word expressions and idioms 
using the same expert lexicon formalism. 
Acknowledgment 
This work was partly supported by a grant from 
the Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under 
contract F30602-03-C-0044. The authors wish to 
thank Carrie Pine and Sharon Walter of AFRL 
for supporting and reviewing this work. Thanks 
also go to the anonymous reviewers for their 
constructive comments. 
References 
Breidt. E., F. Segond and G. Valetto. 1994. Local 
Grammars for the Description of Multi-Word 
Lexemes and Their Automatic Recognition in 
Text.  Proceedings of Comlex-2380 - Papers 
in Computational Lexicography, Linguistics 
Institute, HAS, Budapest, 19-28. 
Breidt, et al 1996. Formal description of 
Multi-word Lexemes with the Finite State 
formalism: IDAREX. Proceedings of 
COLING 1996, Copenhagen.  
Bolinger, D. 1971. The Phrasal Verb in English.  
Cambridge, Mass., Harvard University Press. 
Church, K. 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. 
Proceedings of ANLP 1988.  
Di Sciullo, A.M. and E. Williams. 1987.  On The 
Definition of Word. The MIT Press, 
Cambridge, Massachusetts. 
Fraser, B. 1976. The Verb Particle Combination 
in English.  New York: Academic Press. 
Pelli, M. G. 1976. Verb Particle Constructions in 
American English.  Zurich: Francke Verlag 
Bern. 
Sag, I., T. Baldwin, F. Bond, A. Copestake and D. 
Flickinger. 2002. Multiword Expressions: A 
Pain in the Neck for NLP. Proceedings of 
CICLING 2002, Mexico City, Mexico, 1-15. 
Shaked, N. 1994. The Treatment of Phrasal 
Verbs in a Natural Language Processing 
System, Dissertation, CUNY. 
Silberztein, M. 2000. INTEX: An FST Toolbox. 
Theoretical Computer Science, Volume 
231(1): 33-46. 
Small, S. and C. Rieger. 1982. Parsing and 
comprehending with word experts (a theory 
and its realisation). W. Lehnert and M. 
Ringle, editors, Strategies for Natural 
Language Processing. Lawrence Erlbaum 
Associates, Hillsdale, NJ.  
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: An Information Discovery Engine 
Supported by New Levels of Information 
Extraction. Proceeding of HLT-NAACL 
Workshop on Software Engineering and 
Architecture of Language Technology 
Systems, Edmonton, Canada. 
Villavicencio, A. and A. Copestake. 2002. 
Verb-particle constructions in a 
computational grammar of English.  
Proceedings of the Ninth International 
Conference on Head-Driven Phrase Structure 
Grammar, Seoul, South Korea. 
Weakly Supervised Learning for Cross-document Person Name 
Disambiguation Supported by Information Extraction  
Cheng Niu, Wei Li, and Rohini K. Srihari 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA. 
{cniu, wei, rohini}@cymfony.com 
 
Abstract 
It is fairly common that different people are 
associated with the same name. In tracking 
person entities in a large document pool, it is 
important to determine whether multiple 
mentions of the same name across documents 
refer to the same entity or not.  Previous 
approach to this problem involves measuring 
context similarity only based on co-occurring 
words. This paper presents a new algorithm 
using information extraction support in 
addition to co-occurring words. A learning 
scheme with minimal supervision is developed 
within the Bayesian framework. Maximum 
entropy modeling is then used to represent the 
probability distribution of context similarities 
based on heterogeneous features.  Statistical 
annealing is applied to derive the final entity 
coreference chains by globally fitting the 
pairwise context similarities. Benchmarking 
shows that our new approach significantly 
outperforms the existing algorithm by 25 
percentage points in overall F-measure. 
1 Introduction 
Cross document name disambiguation is 
required for various tasks of knowledge discovery 
from textual documents, such as entity tracking, 
link discovery, information fusion and event 
tracking.  This task is part of the co-reference task: 
if two mentions of the same name refer to same 
(different) entities, by definition, they should 
(should not) be co-referenced. As far as names are 
concerned, co-reference consists of two sub-tasks: 
(i) name disambiguation to handle the problem of 
different entities happening to use the same name; 
(ii) alias association to handle the problem of the 
same entity using multiple names (aliases). 
Message Understanding Conference (MUC) 
community has established within-document co-
reference standards [MUC-7 1998]. Compared 
with within-document name disambiguation which 
can leverage highly reliable discourse heuristics 
such as one sense per discourse [Gale et al1992], 
cross-document name disambiguation is a much 
harder problem. 
Among major categories of named entities (NEs, 
which in this paper refer to entity names, excluding 
the MUC time and numerical NEs), company and 
product names are often trademarked or uniquely 
registered, and hence less subject to name 
ambiguity. This paper focuses on cross-document 
disambiguation of person names. 
Previous research for cross-document name 
disambiguation applies vector space model (VSM) 
for context similarity, only using co-occurring 
words [Bagga & Baldwin 1998]. A pre-defined 
threshold decides whether two context vectors are 
different enough to represent two different entities. 
This approach faces two challenges: i) it is difficult 
to incorporate natural language processing (NLP) 
results in the VSM framework; 1 ii) the algorithm 
focuses on the local pairwise context similarity, 
and neglects the global correlation in the data: this 
may cause inconsistent results, and hurts the 
performance. 
This paper presents a new algorithm that 
addresses these problems. A learning scheme with 
minimal supervision is developed within the 
Bayesian framework. Maximum entropy modeling 
is then used to represent the probability 
distribution of context similarities based on 
heterogeneous features covering both co-occurring 
words and natural language information extraction 
(IE) results.  Statistical annealing is used to derive 
the final entity co-reference chains by globally 
fitting the pairwise context similarities. 
Both the previous algorithm and our new 
algorithm are implemented, benchmarked and 
                                                     
1 Based on our experiment, only using co-occurring 
words often cannot fulfill the name disambiguation task. 
For example, the above algorithm identifies the 
mentions of Bill Clinton as referring to two different 
persons, one represents his role as U. S. president, and 
the other is strongly associated with the scandal, 
although in both mention clusters, Bill Clinton has been 
mentioned as U.S. president. Proper name 
disambiguation calls for NLP/IE support which may 
have extracted the key person?s identification 
information from the textual documents. 
compared.  Significant performance enhancement 
up to 25 percentage points in overall F-measure is 
observed with the new approach. The generality of 
this algorithm ensures that this approach is also 
applicable to other categories of NEs. 
The remaining part of the paper is structured as 
follows. Section 2 presents the algorithm design 
and task definition. The name disambiguation 
algorithm is described in Sections 3, 4 and 5, 
corresponding to the three key aspects of the 
algorithm, i.e. minimally supervised learning 
scheme, maximum entropy modeling and 
annealing-based optimization. Benchmarks are 
shown in Section 6, followed by Conclusion in 
Section 7. 
2 Task Definition and Algorithm Design 
Given n  name mentions, we first introduce the 
following symbols. iC  refers to the context of the  
i -th mention. iP  refers to the entity for the i -th 
mention. iName  refers to the name string of the i  
-th mention. jiCS ,  refers to the context similarity 
between the i -th mention and the j -th mention, 
which is a subset of the predefined context 
similarity features. ?f  refers to the? -th 
predefined context similarity feature. So jiCS ,  
takes the form of { }?f . 
The name disambiguation task is defined as hard 
clustering of the multiple mentions of the same 
name. Its final solution is represented as { }MK ,  
where K refers to the number of distinct entities, 
and M represents the many-to-one mapping (from 
mentions to a cluster) such that 
( ) K]. [1,j n],[1,i j,iM ??=  
One way of combining natural language IE 
results with traditional co-occurring words is to 
design a new context representation scheme and 
then define the context similarity measure based on 
the new scheme.  The challenge to this approach 
lies in the lack of a proper weighting scheme for 
these high-dimensional heterogeneous features. In 
our research, the algorithm directly models the 
pairwise context similarity. 
For any given context pair, a set of predefined 
context similarity features are defined. Then with n 
mentions of a same name, 2
)1( ?nn  context 
similarities [ ] [ )( )ijniCS ji ,1,,1 , ??  are 
computed. The name disambiguation task is 
formulated as searching for { }MK ,  which 
maximizes the following conditional probability:  
{ }( ) [ ] [ )( )ijniCSMK ji ,1,,1       }{,Pr , ??  
Based on Bayesian Equity, this is equivalent to 
maximizing the following joint probability 
 
{ }( ) [ ] [ )( )
{ }( ) { }( )
{ }( ) { }( )MKMKCS
MKMKCS
ijniCSMK
ij
Ni
ji
ji
ji
,Pr,Pr
,Pr,}{Pr
,1,,1       }{,,Pr
1,1
,1
,
,
,
?
?=
=
?
=
??
(1) 
 
Eq. (1) contains a prior probability distribution 
of name disambiguation { }( )MK ,Pr . Because 
there is no prior knowledge available about what 
solution is preferred, it is reasonable to take an 
equal distribution as the prior probability 
distribution. So the name disambiguation is 
equivalent to searching for { }MK ,  which 
maximizes Expression (2). 
 
{ }( )?
?=
=
1,1
,1
, ,Pr
ij
Ni
ji MKCS      (2) 
 
where 
{ }( ) ( ) ( ) ( )( )





?
==
= otherwise ,Pr
jMiM if ,Pr,Pr
,
,
,
jiji
jiji
ji PPCS
PPCSMKCS
       (3) 
 
To learn the conditional probabilities ( )jiji PPCS =|Pr ,  and ( )jiji PPCS ?|Pr ,  in Eq. 
(3), we use a machine learning scheme which only 
requires minimal supervision. Within this scheme, 
maximum entropy modeling is used to combine 
heterogeneous context features. With the learned 
conditional probabilities in Eq. (3), for a given 
{ }MK ,  candidate, we can compute the conditional 
probability of Expression (2).  In the final step, 
optimization is performed to search for { }MK ,  
that maximizes the value of Expression (2). 
To summarize, there are three key elements in 
this learning scheme: (i) the use of automatically 
constructed corpora to estimate conditional 
probabilities of Eq. (3); (ii) maximum entropy 
modeling for combining heterogeneous context 
similarity features; and (iii) statistical annealing for 
optimization. 
3 Learning Using Automatically Constructed 
Corpora 
This section presents our machine learning 
scheme to estimate the conditional probabilities ( )jiji PPCS =|Pr ,  and ( )jiji PPCS ?|Pr ,  in Eq. 
(3). Considering jiCS ,  is in the form of { }?f , we 
re-formulate the two conditional probabilities as 
{ }( )ji PPf =|Pr ?  and { }( )ji PPf ?|Pr ? . 
The learning scheme makes use of automatically 
constructed large corpora. The rationale is 
illustrated in the figure below. The symbol + 
represents a positive instance, namely, a mention 
pair that refers to the same entity.  The symbol ? 
represents a negative instance, i.e. a mention pair 
that refers to different entities. 
 
Corpus I  Corpus II 
+++++---++++++         ---------------------- 
+-----+++--+++++           --+------------------ 
   ++++++++++--++           --------------+------ 
   +++++++---++++         ----------------------- 
   +++----++++++++         --------+------------- 
 
As shown in the figure, two training corpora are 
automatically constructed. Corpus I contains 
mention pairs of the same names; these are the 
most frequently mentioned names in the document 
pool. It is observed that frequently mentioned 
person names in the news domain are fairly 
unambiguous, hence enabling the corpus to contain 
mainly positive instances.2 Corpus II contains 
mention pairs of different person names, these 
pairs overwhelmingly correspond to negative 
instances (with statistically negligible exceptions). 
Thus, typical patterns of negative instances can be 
learned from Corpus II. We use these patterns to 
filter away the negative instances in Corpus I. The 
purified Corpus I can then be used to learn patterns 
for positive instances. The algorithm is formulated 
as follows. 
Following the observation that different names 
usually refer to different entities, it is safe to derive 
Eq. (4).  
 
( ) ( )2121 }{Pr}{Pr namenamefPPf ?=? ??   
(4) 
 
For ( )21}{Pr PPf =? , we can derive the 
following relation (Eq. 5): 
 
                                                     
2 Based on our data analysis, there is no observable 
difference in linguistic expressions involving frequently 
mentioned vs. occasionally occurring person names.  
Therefore, the use of frequently mentioned names in the 
corpus construction process does not affect the 
effectiveness of the learned model to be applicable to all 
the person names in general. 
( )
( )[
( )]
( )[
( )( )]2121
21
2121
21
21
Pr1*  
}{Pr
Pr*  
}{Pr
}{Pr
namenamePP
PPf
namenamePP
PPf
namenamef
==?
?+
==
==
=
?
?
?
 (5) 
 
So ( )21}{Pr PPf =?  can be determined if 
( ))()(}{Pr 21 PnamePnamef =? , 
( ))()(}{Pr 21 PnamePnamef ?? , and 
( ))()(Pr 2121 PnamePnamePP ==  are all known. 
By using Corpus I and Corpus II to estimate the 
above three probabilities, we achieve Eq. (6.1) and 
Eq. (6.2) 
 
( )21}{Pr PPf =?  
( ) ( ) ( )
X
Xff ??
=
1*}{Pr}{Pr maxEntIImaxEntI ?? .                  
     (6.1) 
 
( ) })({Pr}{Pr maxEntII21 ?? fPPf =?            (6.2) 
 
where ( )}{Pr maxEntI ?f  denotes the maximum 
entropy model of ( ))()(}{Pr 21 PnamePnamef =?  
using Corpus I,  ( )}{Pr maxEntII ?f  denotes the 
maximum entropy model of 
( ))()(}{Pr 21 PnamePnamef ??  using Corpus II, 
and X  stands for the Maximum Likelihood 
Estimation (MLE) of 
( ))()(Pr 2121 PnamePnamePP ==  using Corpus I. 
Maximum entropy modeling is used here due to its 
strength of combining heterogeneous features. 
It is worth noting that ( )}{Pr maxEntI ?f  and 
( )}{Pr maxEntII ?f  can be automatically computed 
using Corpus I and Corpus II. Only X requires 
manual truthing. Because X is context 
independent, the required truthing is very limited 
(in our experiment, only 100 truthed mention pairs 
were used). The details of corpus construction and 
truthing will be presented in the next section. 
4 Maximum Entropy Modeling 
This section presents the definition of context 
similarity features }{ ?f , and how to estimate the 
maximum entropy model of  ( )}{Pr maxEntI ?f  and 
( )}{Pr maxEntII ?f . 
First, we describe how Corpus I and Corpus II 
are constructed. Before the person name 
disambiguation learning starts, a large pool of 
textual documents are processed by an IE engine 
InfoXtract [Srihari et al2003]. The InfoXtract 
engine contains a named entity tagger, an aliasing 
module, a parser and an entity relationship 
extractor. In our experiments, we used ~350,000 
AP and WSJ news articles (a total of ~170 million 
words) from the TIPSTER collection. All the 
documents and the IE results are stored into an IE 
Repository. The top 5,000 most frequently 
mentioned multi-token person names are retrieved 
from the repository. For each name, all the 
contexts are retrieved while the context is defined 
as containing three categories of features: 
 
(i)     The surface string sequence centering around 
a key person name (or its aliases as identified 
by the aliasing module) within a predefined 
window size equal to 50  
tokens to both sides of the key name. 
 
(ii)  The automatically tagged entity names co 
occurring with the key name (or its aliases) 
within the same predefined window as in (i). 
 
(iii) The automatically extracted relationships 
associated with the key name (or its aliases). 
The relationships being utilized are listed 
below: 
 
Age, Where-from, Affiliation, Position, 
Leader-of, Owner-of, Has-Boss, Boss-of, 
Spouse-of, Has-Parent, Parent-of, Has-
Teacher, Teacher-of, Sibling-of, Friend-of, 
Colleague-of, Associated-Entity, Title, 
Address, Birth-Place, Birth-Time, Death-
Time, Education, Degree, Descriptor, 
Modifier, Phone, Email, Fax. 
 
A recent manual benchmarking of the InfoXtract 
relationship extraction in the news domain is 86% 
precision and 67% recall (75% F-measure).    
To construct Corpus I, a person name is 
randomly selected from the list of the top 5,000 
frequently mentioned multi-token names. For each 
selected name, a pair of contexts are extracted, and 
inserted into Corpus I. This process repeats until 
10,000 pairs of contexts are selected. 
It is observed that, in the news domain, the top 
frequently occurring multi-token names are highly 
unambiguous. For example, Bill Clinton 
exclusively stands for the previous U.S. president 
although in real life, although many other people 
may also share this name. Based on manually 
checking 100 sample pairs in Corpus I, we have 
( ) 95.0Pr 21 ?== PPX I , which means for the 100 
sample pairs mentioning the same person name, 
only 5 pairs are found to refer to different person 
entities. Note that the value of X?1  represents the 
estimation of the noise in Corpus I, which is used 
in Eq (6.1) to correct the bias caused by the noise 
in the corpus.  
To construct Corpus II, two person names are 
randomly selected from the same name list. Then a 
context for each of the two names is extracted, and 
this context pair is inserted into Corpus II. This 
process repeats until 10,000 pairs of contexts are 
selected.  
Based on the above three categories of context 
features, four context similarity features are 
defined:  
 
(1)  VSM-based context similarity using co-
occurring words  
 
The surface string sequence centering around the 
key name is represented as a vector, and the word i 
in context j is weighted as follows. 
 
)(log*),(),( idf
Djitfjiweight =   (7) 
 
where ),( jitf is the frequency of word i in the  
j-th surface string sequence; D is the number of 
documents in the pool; and )(idf  is the number of 
documents containing the word i. Then, the cosine 
of the angle between the two resulting vectors is 
used as the context similarity measure.  
 
(2) Co-occurring NE Similarity 
 
The latent semantic analysis (LSA) [Deerwester 
et al1990] is used to compute the co-occurring NE 
similarities.  LSA is a technique to uncover the 
underlining semantics based on co-occurrence 
data. The first step of LSA is to construct word-
vs.-document co-occurrence table. We use 100,000 
documents from the TIPSTER corpus, and select 
the following types of top n most frequently 
mentioned words as base words: 
 
top 20,000 common nouns 
top 10,000 verbs 
top 10,000 adjectives 
top 2,000 adverbs 
top 10,000 person names 
top 15,000 organization names 
top 6,000 location names 
top 5,000 product names 
 
Then, a word-vs.-document co-occurrence table 
Matrix  is built so that 
)(log*),( idf
DjitfMatrixij = . The second step of 
LSA is to perform singular value decomposition 
(SVD) on the co-occurrence matrix.  SVD yields 
the following Matrix  decomposition:  
 
TDSTMatrix 000=    (8)  
 
where T  and D are orthogonal matrices (the row 
vector is called singular vectors), and S  is a 
diagonal matrix with the diagonal elements (called 
singular values) sorted decreasingly. 
The key idea of LSA is to reduce noise or 
insignificant association patterns by filtering the 
insignificant components uncovered by SVD. This 
is done by keeping only top k singular values. In 
our experiment, k is set to 200, following the 
practice reported in [Deerwester et al 1990] and 
[Landauer & Dumais, 1997]. This procedure yields 
the following approximation to the co-occurrence 
matrix: 
TTSDMatrix ?    (9) 
 
where S  is attained from 0S by deleting non-top k 
elements,  and T ( D ) is obtained from 0T ( 0D ) by 
deleting the corresponding columns. 
It is believed that the approximate matrix is more 
proper to induce underlining semantics than the 
original one. In the framework of LSA, the co-
occurring NE similarities are computed as follows: 
suppose the first context in the pair contains NEs 
{ }it0 , and the second context in the pair contains 
NEs { }it1 . Then the similarity is computed as 

 
=
ii
ii
titi
titi
TwTw
TwTwS
10
10
10
10 where iw0 and iw1 are 
term weights defined in Eq (7). 
 
(3) Relationship Similarity 
 
We define four different similarity values based 
on entity relationship sharing: (i) sharing no 
common relationships, (ii) relationship conflicts 
only, (iii) relationship with consistence and 
conflicts, and (iv) relationship with consistence 
only. The  consistency checking between extracted 
relationships is supported by the InfoXtract 
number normalization and time normalization as 
well as entity aliasing procudures. 
 
(4) Detailed Relationship Similarity 
 
For each  relationship type, four different 
similarity values are defined based on sharing of 
that specific relationship i: (i) no sharing of 
relationship i, (ii) conflicts for relationship i, (iii) 
consistence and conflicts for relationship i, and 
(iv) consistence for relationship i. 
 
To facilitate the maximum entropy modeling in 
the later stage, the values of the first and second 
categories of similarity measures are discretized 
into integers. The number of integers being used 
may impact the final performance of the system. If 
the number is too small, significant information 
may be lost during the discretization process. On 
the other hand, if the number is too large, the 
training data may become too sparse. We trained a 
conditional maximum entropy model to 
disambiguate context pairs between Corpus I and 
Corpus II. The performance of this model is used 
to select the optimal number of integers. There is 
no significant  performance change when the 
integer number is within the range of [5,30], with 
12 as the optimal number. 
Now the context similarity for a context pair is a 
vector of similarity features, e.g.  
 
{VSM_Similairty_equal_to_2, 
NE_Similarity_equal_to_1, 
Relationship_Conflicts_only, 
No_Sharing_for_Age, 
   Conflict_for_Affiliation}. 
Besides the four categories of basic context 
similarity features defined above, we define 
induced context similarity features by combining 
basic context similarity features using the logical 
AND operator. With induced features, the context 
similarity vector in the previous example is 
represented as 
{VSM_Similairty_equal_to_2, 
NE_Similarity_equal_to_1, 
Relationship_Conflicts_only, 
No_Sharing_for_Age, 
Conflict_for_Affiliation,  
[VSM_Similairty_equal_to_2 and 
NE_Similarity_equal_to_1], 
[VSM_Similairty=2 and 
Relationship_Conflicts_only],  
??  
[VSM_Similairty_equal_to_2 and 
NE_Similarity_equal_to_1 and 
Relationship_Conflicts_only and 
No_Sharing_for_Age and 
Conflict_for_Affiliation] 
  }. 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. Combining basic features and induced 
features under a smoothing scheme, maximum 
entropy modeling may achieve optimal 
performance.  
Now the maximum entropy modeling can be 
formulated as follows: given a pairwise context 
similarity vector }{ ?f  the probability of }{ ?f is 
given as 
 
( )
{ }
?
?
=
?
?
ff
fwZf
1}{Pr maxEnt   (10) 
 
where Z is the normalization factor, fw  is the 
weight associated with feature f . The Iterative 
Scaling algorithm combined with Monte Carlo 
simulation [Pietra, Pietra & Lafferty 1995] is used 
to train the weights in this generative model. 
Unlike the commonly used conditional maximum 
entropy modeling which approximates the feature 
configuration space as the training corpus 
[Ratnaparkhi 1998], Monte Carlo techniques are 
required in the generative modeling to simulate the 
possible feature configurations. The exponential 
prior smoothing scheme [Goodman 2003] is 
adopted. The same training procedure is performed 
using Corpus I and Corpus II to estimate 
( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  respectively. 
5 Annealing-based Optimization  
With the maximum entropy modeling presented 
in the last section, for a given name 
disambiguation candidate solution{ }MK , , we can 
compute the conditional probability of Expression 
(2). Statistical annealing [Neal 1993]-based 
optimization is used to search for { }MK ,  which 
maximizes Expression (2). 
The optimization process consists of two steps. 
First, a local optimal solution{ }0, MK is computed 
by a greedy algorithm. Then by setting { }0, MK as 
the initial state, statistical annealing is applied to 
search for the global optimal solution. 
Given n  same name mentions, assuming the 
input of 2
)1( ?nn  probabilities ( )jiji PPCS =,Pr  
and 2
)1( ?nn  probabilities ( )jiji PPCS ?,Pr , the 
greedy algorithm performs as follows: 
 
1. Set the initial state { }MK , as nK = , 
and [ ]n1,i  ,)( ?= iiM ; 
2. Sort ( )jiji PPCS =,Pr  in decreasing  
order; 
3. Scan the sorted probabilities one by one.  
If the current probability is  ( )jiji PPCS =,Pr , )(  )( jMiM ? , and  
there exist no such l  and m that  
( ) ( ) ( ) ( )jMmMiMlM == ,  
and ( ) ( )mlmljiji PPCSPPCS ?<= ,, PrPr  
then update { }MK ,  by merging cluster 
)(iM and )( jM . 
4.   Output { }MK ,  as a local optimal solution. 
 
Using the output { }0, MK of the greedy 
algorithm as the initial state, the statistical 
annealing is described using the following pseudo-
code:  
 
Set { } { }0,, MKMK = ; 
for( 1.01?*;?? ;?? final0 =<= ) 
   { 
    iterate pre-defined number of times 
    { 
          set { } { }MKMK ,, 1 = ; 
          update { }1, MK  by randomly changing    
          the  number of clusters K and the    
          content of   each cluster.  
            set 
{ }( )
{ }( )?
?
?=
=
?=
=
=
1,1
,1
,
1,1
,1
1,
,Pr
,Pr
ij
Ni
ji
ij
Ni
ji
MKCS
MKCS
x  
           if(x>=1) 
          { 
             set { } { }1,, MKMK =  
          } 
          else 
         { 
             set { } { }1,, MKMK =  with probability  
              ?x . 
         } 
      if 
{ }( )
{ }( ) 1,Pr
,Pr
1,1
,1
0,
1,1
,1
,
>?
?
?=
=
?=
=
ij
Ni
ji
ij
Ni
ji
MKCS
MKCS
 
      set { } { }MKMK ,, 0 =  
   } 
} 
output { }0, MK  as the optimal state. 
6 Benchmarking 
To evaluate the effectiveness of our new 
algorithm, we implemented the previous algorithm 
described in [Bagga & Baldwin 1998] as our 
baseline. The threshold is selected as 0.19 by 
optimizing the pairwise disambiguation accuracy 
using the 80 truthed mention pairs of ?John 
Smith?. To clearly benchmark the performance 
enhancement from IE support, we also 
implemented a system using the same weakly 
supervised learning scheme but only VSM-based 
similarity as the pairwise context similarity 
measure. We benchmarked the three systems for 
comparison. The following three scoring measures 
are implemented. 
 
(1) Precision (P): 

=
iN
P i  ofcluster  output   in the  mentions of #
i  ofcluster  output   in the  mentionscorrect   of #1  
 
(2) Recall (R): 

=
iN
P i  ofcluster  key    in  the  mentions of #
i   ofcluster  output    in  the  mentionscorrect   of #1  
 
(3) F-measure (F): 
RP
RPF
+
=
*2  
 
The name co-reference precision and recall used 
here is adopted from the B_CUBED scoring 
scheme used in [Bagga & Baldwin 1998], which is 
believed to be an appropriate benchmarking 
standard for this task.  
Traditional benchmarking requires manually 
dividing person name mentions into clusters, 
which is labor intensive and difficult to scale up. In 
our experiments, an automatic corpus construction 
scheme is used in order to perform large-scale 
testing for reliable benchmarks. 
The intuition is that in the general news domain, 
some multi-token names associated with mass 
media celebrities is highly unambiguous. For 
example, ?Bill Gates?, ?Bill Clinton?, etc. 
mentioned in the news almost always refer to 
unique entities. Therefore, we can retrieve contexts 
of these unambiguous names, and mix them 
together. The name disambiguation algorithm 
should recognize mentions of the same name. The 
capability of recognizing mentions of an 
unambiguous name is equivalent to the capability 
of disambiguating ambiguous names. 
For the purpose of benchmarking, we 
automatically construct eight testing datasets 
(Testing Corpus I), listed in Table 1. 
Table 1. Constructed Testing Corpus I 
# of Mentions Name 
Set 1a Set 1b 
Mikhail S. Gorbachev 20 50
Dick Cheney 20 10
Dalai Lama 20 10
Bill Clinton 20 10
 Set 2a Set 2b 
Bob Dole 20 50
Hun Sen 20 10
Javier Perez de Cuellar 20 10
Kim Young Sam 20 10
 Set 3a Set 3b 
Jiang Qing 20 10
Ingrid Bergman 20 10
Margaret Thatcher 20 50
Aung San Suu Kyi 20 10
 Set 4a Set 4b 
Bill Gates 20 10
Jiang Zemin 20 10
Boris Yeltsin 20 50
Kim Il Sung 20 10
  
Table 2.  Testing Corpus I Benchmarking 
 P R F P R F 
 Set 1a Set 1b 
Baseline 0.79 0.37 0.58 0.78 0.34 0.56 
VSMOnly 0.86 0.33 0.60 0.78 0.23 0.51 
Full 0.98 0.75 0.86 0.90 0.79 0.85 
  Set 2a Set 2b 
Baseline 0.82 0.58 0.70 0.94 0.50 0.72 
VSMOnly 0.90 0.54 0.72 0.98 0.45 0.71 
Full 0.93 0.84 0.88 1.00 0.93 0.96 
 Set 3a Set 3b 
Baseline 0.84 0.69 0.77 0.80 0.34 0.57 
VSMOnly 0.95 0.72 0.83 0.93 0.29 0.61 
Full 0.95 0.86 0.90 0.98 0.57 0.77 
 Set 4a Set 4b 
Baseline 0.88 0.74 0.81 0.80 0.49 0.64 
VSMOnly 0.93 0.77 0.85 0.88 0.42 0.65 
Full 0.95 0.93 0.94 0.98 0.84 0.91 
Overall P R F 
Baseline 0.83 0.51 0.63 
VSMOnly 0.90 0.47 0.69 
Full 0.96 0.82 0.88 
 
Table 2 shows the benchmarks for each dataset, 
using the three measures just defined. The new 
algorithm when only using VSM-based similarity 
(VSMOnly) outperforms the existing algorithm 
(Baseline) by 5%. The new algorithm using the full 
context similarity measures including IE features 
(Full) significantly outperforms the existing 
algorithm (Baseline) in every test:  the overall F-
measure jumps from 64% to 88%, with 25 
percentage point enhancement.  This performance 
breakthrough is mainly due to the additional 
support from IE, in addition to the optimization 
method used in our algorithm. 
We have also manually truthed an additional 
testing corpus of two datasets containing mentions 
associated with the same name (Testing Corpus II). 
Truthed Dataset 5a contains 25 mentions of Peter 
Sutherland and Truthed Dataset 5b contains 68 
mentions of John Smith. John Smith is a highly 
ambiguous name. With its 68 mentions, they 
represent totally 29 different entities. On the other 
hand, all the mentions of Peter Sutherland are 
found to refer to the same person. The benchmark 
using this corpus is shown below. 
Table 3. Testing Corpus II Benchmarking 
 P R F P R F 
 Set 5a Set 5b 
Baseline 0.96 0.92 0.94 0.62 0.57 0.60 
VSMOnly 0.96 0.92 0.94 0.75 0.51 0.63 
Full 1.00 0.92 0.96 0.90 0.81 0.85 
 
Based on these benchmarks, using either 
manually truthed corpora or automatically 
constructed corpora, using either ambiguous 
corpora or unambiguous corpora, our algorithm 
consistently and significantly outperforms the 
existing algorithm. In particular, our system 
achieves a very high precision (0.96 precision). 
This shows the effective use of IE results which 
provide much more fine-grained evidence than co-
occurring words. It is interesting to note that the 
recall enhancement is greater than the precision 
enhancement (0.31 recall enhancement vs. 0.13 
precision enhancement). This demonstrates the 
complementary nature between evidence from the 
co-occurring words and the evidence carried by IE 
results. The system recall can be further improved 
once the recall of the currently precision-oriented 
IE engine is enhanced over time. 
7 Conclusion 
We have presented a new person name 
disambiguation algorithm which demonstrates a 
successful use of natural language IE support in 
performance enhancement. Our algorithm is 
benchmarked to outperform the previous algorithm 
by 25 percentage points in overall F-measure, 
where the effective use of IE contributes to 20 
percentage points. The core of this algorithm is a 
learning system trained on automatically 
constructed large corpora, only requiring minimal 
supervision in estimating a context-independent 
probability.   
8 Acknowledgements 
This work was partly supported by a grant from 
the Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-03-C-0170.  The authors wish to thank 
Carrie Pine of AFRL for supporting and reviewing 
this work.   
References  
Bagga, A., and B. Baldwin. 1998. Entity-Based 
Cross-Document Coreferencing Using the 
Vector Space Model. In Proceedings of 
COLING-ACL'98.  
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Gale, W., K. Church, and D. Yarowsky. 1992.  
One Sense Per Discourse.  In Proceedings of the 
4th DARPA Speech and Natural Language 
Workshop.  
Goodman, J. 2003. Exponential Priors for 
Maximum Entropy Models. 
Landauer, T. K., & Dumais, S. T. 1997. A solution 
to Plato's problem: The Latent Semantic 
Analysis theory of the acquisition, induction, and 
representation of knowledge. Psychological 
Review, 104, 211-240, 1997. 
MUC-7. 1998.  Proceedings of the Seventh 
Message Understanding Conference. 
Neal, R. M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto.  
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence. 
Srihari, R. K., W. Li, C. Niu and T. Cornell. 
InfoXtract: An Information Discovery Engine 
Supported by New Levels of Information 
Extraction. In Proceeding of HLT-NAACL 2003 
Workshop on Software Engineering and 
Architecture of Language Technology Systems, 
Edmonton, Canada. 
Extracting Exact Answers to Questions Based on Structural Links?
Wei Li, Rohini K. Srihari, Xiaoge Li, M. Srikanth, Xiuhong Zhang, Cheng Niu
Cymfony Inc.
600 Essjay Road, Williamsville, NY 14221. USA.
{wei, rohini, xli, srikanth, xzhang, cniu}@cymfony.com
Keywords:  Question Answering, Information Extraction, Semantic Parsing, Dependency Link
?  This  work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate 
(AFRL/IF), Rome, NY, under contracts F30602-00-C-0037 and F30602-00-C-0090.
Abstract
This paper presents a novel approach to
extracting phrase-level answers in a question 
answering system. This approach uses
structural support provided by an integrated 
Natural Language Processing (NLP) and
Information Extraction (IE) system. Both
questions and the sentence-level candidate
answer strings are parsed by this NLP/IE
system into binary dependency structures.
Phrase-level answer extraction is modelled by 
comparing the structural similarity involving 
the question-phrase and the candidate answer-
phrase.
There are two types of structural support. The 
first type involves predefined, specific entity 
associa tions such as Affiliation, Position, Age 
for a person entity. If a question asks about 
one of these associations, the answer-phrase
can be determined as long as the system
decodes such pre-defined dependency links 
correctly, despite the syntactic difference
used in expressions between the question and 
the candidate answer string. The second type 
involves generic grammatical relationships
such as V-S (verb-subject), V-O (verb-
object).
Preliminary experimental results show an
improvement in both precision and recall in 
extracting phrase-level answers, compared
with a baseline system which only uses Named 
Entity constraints. The proposed methods are 
particularly effective in cases where the
question-phrase does not correspond to a
known named entity type and in cases where 
there are multiple candidate answer-phrases
satisfying the named entity constraints.
Introduction
Natural language Question Answering (QA) is 
recognized as a capability with great potential.
The NIST-sponsored Text Retrieval Conference
(TREC) has been the driving force for developing 
this technology through its QA track since TREC-8
(Voorhees 1999). There has been significant
progress and interest in QA research in recent
years (Voorhees 2000, Pasca and Harabagiu 2001).
QA is different than search engines in two aspects: 
(i) instead of a string of keyword search terms, the 
query is a natural language question, necessitating 
question parsing, (ii) instead of a list of documents 
or URLs, a list of candidate answers at phrase level 
or sentence level are expected to be returned in 
response to a query, hence the need for text
processing beyond keyword indexing, typically
supported by Natural Language Processing (NLP) 
and Information Extraction (IE) (Chinchor and
Marsh 1998, Hovy, Hermjakob and Lin 2001, Li 
and Srihari 2000). Examples of the use of NLP and 
IE in Question Answering include shallow parsing 
(Kupiec, 1993), semantic parsing (Litkowski
1999), Named Entity tagging (Abney et al 2000, 
Srihari and Li 1999) and high-level IE (Srihari 
and Li, 2000).
Identifying exact or phrase-level answers is a
much more challenging task than sentence-level
answers. Good performance on the latter can be 
achieved by using sophisticated passage retrieval 
techniques and/or shallow level NLP/IE
processing (Kwok et al 2001, Clarke et al 2001). 
The phrase-level answer identification involves
sophisticated NLP/IE and it is difficult to apply 
only IR techniques for this task (Prager et al 
1999). These two tasks are closely related. Many 
systems (e.g. Prager et al1999; Clark et al2001) 
take a two-stage approach. The first stage
involves retrieving sentences or paragraphs in
documents as candidate answer strings. Stage
Two focuses on extracting phrase-level exact
answers from the candidate answer strings.
This paper focuses on methods involving Stage 
Two. The input is a sentence pair consisting of a 
question and a sentence-level candidate answer 
string. The output is defined to be a phrase, called 
answer-point, extracted from the candidate
answer string. In order to identify the answer-
point, the pair of strings are parsed by the same 
system to generate binary dependency structures 
for both specific entity associations and generic 
grammatical relationships. An integrated Natural 
Language Processing (NLP) and Information
Extraction (IE) engine is used to extract named 
entities (NE) and their associations and to decode 
grammatical relationships. The system searches
for an answer-point by comparing the structural 
similarity involving the question-phrase and a
candidate answer-phrase. Generic grammatical
relationships are used as a back-off for specific 
entity associations when the question goes beyond 
the scope of the specific associations or when the 
system fails to identify the answer-point which 
meets the specific  entity association constraints. 
The proposed methods are particularly helpful in 
cases where the question-phrase does not
correspond to a known named entity type and in 
cases where there are multiple candidate answer-
points to select from.
The rest of the paper is structured as follows: 
Section 1 presents the NLP/IE engine used,
sections 2 discusses how to identify and formally 
represent what is being asked, section 3 presents 
the algorithm on identifying exact answers
leveraging structural support, section 4 presents 
case studies and benchmarks, and section 5 is the 
conclusion.
Kernel IE Modules Linguistic  Modules
Entity
Association
Named
Entity
Part-Of-
Speech
Asking-point
Identification
O
ut
pu
t(
En
ti
ty
, 
Ph
ra
se
 a
nd
 S
tr
uc
tu
ra
l 
Li
nk
s)
Shallow
Parsing
Semantic
Parsing
Tokenizer
Input
Figure 1: InfoXtract? NLP/IE System Architecture
1 NLP/IE Engine Description
The NLP/IE engine used in the QA system
described here is named InfoXtract?. It consists 
of an NLP component and IE component, each 
consisting of a set of pipeline modules (Figure 1). 
The NLP component serves as underlying support 
for IE. A brief description of these modules is 
given below.
? Part-of-Speech Tagging: tagging syntactic
categories such as noun, verb, adjective, etc. 
? Shallow Parsing: grouping basic linguistic
units as building blocks for structural links, 
such as Basic Noun Phrase, Verb Group, etc. 
? Asking-point Identification: analysis of
question sentences to determine what is being 
asked
? Semantic Parsing: decoding grammatical
dependency relationships at the logical level 
between linguistic units, such as Verb-Subject
(V-S), Verb-Object (V-O), Head-Modifier
(H-M) relationships; both active patterns and 
passive patterns will be parsed into the same 
underlying logical S-V-O relationships
? Named Entity Tagger: classifying proper
names and other phrases to different
categories such as Person, Organization,
Location, Money, etc.
? Entity Association Extractor: relating named 
entities with predefined associations such as 
Affiliation, Position, Age, Spouse, Address,
etc.
The NE tagger in our system is benchmarked to 
achieve close to human performance, around or 
above 90% precision and recall for most
categories of NE. This performance provides
fundamental support to QA. Many questions
require a named entity or information associated 
with a named entity as answers. A subset of the 
NE hierarchy used in our system is illustrated
below:
Person: woman, man
Organization: company, government,
association, school, army, mass-media
Location: city, province, country, continent, 
ocean, lake, etc.
Time Expressions: hour, part-of-day, day-of-
week, date, month, season, year, decade, 
century, duration
Numerical Expressions: percentage, money, 
number, weight, length, area, etc.
Contact expressions: email, address,
telephone, etc.
The Entity Association module correlates named 
entities and extracts their associations with other 
entities or phrases.  These are specific, predefined 
relationships for entities of person and
organization. Currently, our system can extract
the following entity associations with high
precision (over 90%) and modest recall ranging 
from 50% to 80% depending on the size of
grammars written for each specific association. 
Person: affiliation, position, age, spouse,
birth-place, birth-time, etc.
Organization: location, staff, head, products,
found-time, founder, etc.
Entity associations are semantic structures very
useful in supporting QA. For example, from the 
sentence Grover Cleveland , who in June 1886
married 21-year-old Frances Folsom,?the IE
engine can identify the following associations:
Spouse: Grover Cleveland ?Frances Folsom
Spouse: Frances?Grover Cleveland 
Age:  Frances Folsom?21-year-old
A question asking about such an association, say, 
Q11: Who was President Cleveland ?s wife, will be 
parsed into the following association link between 
a question-phrase ?Who? and the entity ?Cleveland? 
(see Section 2): Spouse: Cleveland ? Who. The 
semantic similarity between this structure and the 
structure Spouse: Grover Cleveland ? Frances 
Folsom can determine the answer point to be
?Frances Folsom?.
The Semantic Parsing module decodes the
grammatical dependency relationships: V-S, V-O,
V-C (Verb-Complement), H-M of time, location, 
reason, manner, purpose, result, etc. This module 
extends the shallow parsing module through the 
use of a cascade of handcrafted pattern matching 
rules.  Manual benchmarking shows results with 
the following performance:
H-M: Precision 77.5%
V-O: Precision 82.5%
V-S: Precision 74%
V-C: Precision 81.4%
In our semantic parsing, not only passive patterns
will be decoded into the same underlying
structures as active patterns, but structures for
verbs such as acquire and for de-verbal nouns such 
as acquisition lead to the same dependency links, 
as shown below.
AOL acquired Netscape in 1998. ?
V-S: acquired? AOL
V-O: acquired ? Netscape
H-M: acquired ? in 1998 (time-modifier)
Netscape was acquired by AOL in 1998. ?
V-S: was acquired ? by AOL
V-O: was acquired ? Netscape
H-M: was acquired ? in 1998 (time-modifier)
the acquisition of Netscape by AOL in 1998??
V-S: acquisition ? by AOL
V-O: acquisition ? of Netscape 
H-M: acquired ? in 1998 (time-modifier)
These links can be used as structural support to 
answer questions like Who acquired Netscape or
which company was acquired by AOL.
Obviously, our semantic parser goes one step
further than parsers which only decode syntactic 
relationships. It consumes some surface structure 
variations to provide the power of comparing the 
structural similarity at logical level. However,
compared with the entity association structures 
which sits at deep semantic level, the logical SVO 
(Subject-Verb-Object) structures still cannot
capture semantic relations which are expressed
using different head verbs with different
structures. An example is the pair : X borrows Y 
from Z versus Z lends Y to X.
2 Asking Point Link Identification
Asking point link identification is a crucial step in 
a QA system. It provides the necessary
information decoded from question processing for 
a system to locate the corresponding answer-
points from candidate answer strings. 
The Asking-point (Link) Identification Module is 
charged with the task of parsing wh-phrases in 
their context into three categories: NE Asking-
point, Asking-point Association  Link and
Asking-point Grammar  Link. Asking Point refers
to the question phrases with its constraints  that a 
corresponding answer-point should satisfy in
matching. Asking-point Link is the decoded binary 
relationship from the asking point to another unit 
in the question. 
The identification of the NE asking point is
essentially mapping the wh-phrase to the NE
types or subtypes. For example, which year is 
mapped to [which year]/NeYear, how old mapped 
to [how old]/NeAge, and how long mapped to 
[how long]/NeLength or [how long]/NeDuration, 
etc.
The identification of the Asking-point Association
Link is to decide whether the incoming question 
asks about a predefined association relationship. 
For Asking-point Association  Link, the module 
needs to identify the involved entity and the asked 
association. For example, the Asking-point
Association  Link for How old is John Smith is the 
AGE relationship of the NePerson John Smith,
represented as AGE: John Smith ? [how
old]/NeAge.
The wh-phrases which may or may not be mapped 
to NE asking points and whose dependency links 
are beyond predefined associations lead to Asking-
point Grammar Links, e.g. How did Julian Hill 
discover nylon? This asking-point link is
represented as H-M: discover ? [How]/manner-
modifier. As seen, an asking-point grammar link 
only involves generic grammatical constraints: in 
this case, the constraints for a candidate answer-
point to satisfy during matching are H-M link with 
?discover? as head and a phrase which must be a 
modifier of manner. 
These three types of asking points and their
possible links form a natural hierarchy that can be 
used to facilitate the backoff strategy for the
answer-point extraction module (see Section 3): 
Asking-point Association Link ? Asking-point
Grammar Link ? NE Asking Point.  This
hierarchy defines the sequence of matching steps 
which should be followed during the answer-point
extraction.
The backoff from Asking-point Association  Link 
to Asking-point Grammar  Link is necessary as the 
latter represents more generic structural constraints 
than the former. For example, in the sentence
where is IBM located, the Asking-point
Association Link is LOCATION: IBM ?
[where]/NeLocation while the default Grammar
Link is H-M: located ? [where]/location-
modifier. When the specific association constraints 
cannot be satisfied, the system should attempt to 
locate an answer-point by searching for a location-
modifier of the key verb ?located?.
The NE asking point constraints are also marked 
for asking-point association links and those asking-
point grammar links whose wh-phrases can be
mapped to NE asking points. Backing off to the 
NE asking point is required in cases where the 
asking-point association constraints and
grammatical structural constraints cannot be
satisfied. For How old is John Smith, the asking-
point grammar  link is represented as H-M: John 
Smith ? [how old]/NeAge. If the system cannot 
find a corresponding AGE association or a
modifier of NeAge for the entity John Smith to
satisfy the structural constraints, it will at least 
attempt to locate a candidate answer-point by
enforcing the NE asking point constraints NeAge. 
When there is only one NeAge in the answer
string, the system can extract it as the only
possible answer-point even if the structural
constraints are not honored.
3 Answer Point Identification
The answer-point identification is accomplished 
through  matching the asking-point to candidate 
answer-points using the following back-off
algorithm based on the processing results of the 
question and the sentence-level candidate answer 
string.
(1) if there is Asking-point Association
Link, call Match(asking-point association 
link, candidate answer-point association 
link) to search for the corresponding
association to locate answer-point
(2) if step (1) fails and there is an asking-
point grammar link, call Match(asking-
point grammar link, candidate answer-
point grammar link) to search for the
corresponding grammar link to locate the 
answer-point
(3) if step (2) fails and there is an NE asking 
point, search for the corresponding NEs: 
if there is only one corresponding NE, 
then extract this as the answer-point else 
mark all corresponding NEs as candidate 
answer-points
The function Match(asking-point link, candidate 
answer-point link) is defined as (i) exact match or 
synonym match of the related units (synonym
match currently confined to verb vs. de-verbal
noun); (ii) match the relation type directly (e.g. V-
S matches V-S, AGE matches AGE, etc.); (iii) 
match the type of asking point and answer point 
(e.g. NePerson asking point matches NePerson and
its sub-types NeMan and NeWoman; ?how?
matches manner-modifier; etc.): either through
direct link or indirect link based on conjunctive 
link (ConjLink) or equivalence link (S-P, subject-
predicative or appositive relations between two
NPs).
Step (1) and Step (2) attempt to leverage the
structural support from parsing and high-level
information extraction beyond NE. It is worth
noticing that in our experiment, the structural
support used for answer-point identification only 
checks the binary links involving the asking point 
and the candidate answer points, instead of full 
template matching as proposed in (Srihari and Li, 
2000).
Full template matching is best exemplified by the 
following example. If the incoming question is 
Who won the Nobel Prize in 1991, and the
candidate answer string is John Smith won the
Nobel Prize in 1991, the question template and 
answer template are shown below:
win
V-S: NePerson [Who]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
win
V-S: NePerson [John Smith]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
The template matching will match the asking point 
Who with the answer point John Smith because for 
all the dependency links in the trees, the
information is all compatible (in this case, exact
match). This is the ideal case of full template
matching and guarantees the high precision of the 
extracted answer point.
However, in practice, full template matching is 
neither realistic for most of cases nor necessary for 
achieving the objective of extracting answer points 
in a two-stage approach. It is not realistic because 
natural language semantic parsing is such a
challenging problem that a perfect dependency tree 
(or full template) which pieces together every
linguistic unit is not always easy to decode. For
InfoXtract,, in most cases, the majority, but not 
all, of the decoded binary dependency links are 
accurate, as shown in the benchmarks above. In 
such situations, insisting on checking every
dependency link of a template tree is too strong a 
condition to meet. On the other hand, it is actually 
not necessary to check all the links in the
dependency trees for full template matching. With 
the modular design and work division between
sentence level candidate answer string generation 
module (Stage One) and answer-point extraction 
from the candidate answer strings (Stage Two), 
all the candidate answer strings are already
determined by previous modules as highly
relevant. In this situation, a simplified partial
template matching, namely, ?asking/answer point 
binary relation matching?, will be sufficient to 
select the answer-point, if present, from the
candidate answer string. In other words, the
system only needs to check this one dependency 
link in extracting the answer-point. For the
previous example, only the asking/answer point 
binary dependency links need to be matched as 
illustrated below:
V-S win?[Who]/NePerson
V-S win?[John Smith]/NeMan
Some sample results are given in section 4 to
illustrate how answer-points are identified based 
on matching binary relations involving
asking/answer points. 
4 Experiments and Results
In order to conduct the feasibility study on the 
proposed method, we selected the first 100
questions from the TREC-8 QA track pool and 
the corresponding first candidate answer
sentences for this preliminary experiment. The 
Stage One processing for generating candidate 
answer sentences was conducted by the existing 
ranking module of our QA system. The Stage
Two processing for answer-point identification
was accomplished by using the algorithm
described in Section 3.
As shown in Table 1, out of the 100 question-
answer pairs we selected, 9 have detected
association links involving asking/answer points, 
44 are found to have grammar links involving 
asking/answer points. 
Table 1: Experiment Results
detected correct fail precision recall
Association
Links 9 8 1 89% 8%
Grammar
Links 44 39 6 89% 39%
NE Points 
(Baseline) 76 41 35 54% 41%
Overall
performance 86 71 14 83% 71%
As for NE asking points, 76 questions were
identified to require some type of NE as answers.
Assume that a baseline answer-point identification 
system only uses NE asking points as constraints, 
out of the 76 questions requiring NEs as answers, 
41 answer-points were identified successfully
because there was only one NE in the answer
string which matches the required NE type. The 
failed cases in matching NE asking point
constraints include two situations: (i) no NE exists 
in the answer string; (ii) multiple NEs satisfy the 
type constraints of NE asking points (i.e. more 
than one candidate answer-points found from the 
answer string) or there is type conflict during the 
matching of NE asking/answer points. Therefore, 
the baseline system would achieve 54% precision 
and 41% recall based on the standard precision and 
recall formulas: 
Precision = Correct / Detected
Recall = Correct / Relevant. 
In comparison, in our answer-point identification 
system which leverages structural support from
both the entity association links and grammar links 
as well as the NE asking points, both the precision 
and recall are raised: from the baseline 54% to 
83% for precision and from 41% to 71% for recall. 
The significant improvement in precision and
recall is attributed to the performance of structural 
matching in identifying exact answers. This
demonstrates the benefits of making use of
sophisticated NLP/IE technology, beyond NE and 
shallow parsing.
Using grammar links alone, exact answers were
identified for 39 out of the 44 candidate answer-
points satisfying the types of grammar links in 100 
cases. During matching, 6 cases failed either due to 
the parsing error or due to the type conflict
between the asking/answer points (e.g. violating 
the type constraints such as manner-modifier on 
the answer-point for ?how? question). The high 
precision and modest recall in using the grammar 
constraints is understandable as the grammar links 
impose very strong constraints on both the nodes 
and the structural type. The high precision
performance indicates that grammar links not
only have the distinguishing power to identify
exact answers in the presence of multiple NE 
options but also recognize answers in the absence 
of asking point types.
Even stronger structural support comes from the 
semantic relations decoded by the entity
association extraction module.  In this case, the 
performance is naturally high-precision (89%)
low-recall (8%) as predefined association links 
are by nature more sparse than generic
grammatical relations.
In the following, we illustrate with some
examples with questions from the TREC-8 QA 
task on how the match function identified in
Section 3 applies to different question types.
Q4: How much did Mercury spend on
advertising in 1993? ? asking-point grammar 
link:
V-O spend ? [How much]/NeMoney
A: Last year the company spent Pounds 12m
on advertising. ? candidate answer-point
grammar link:
V-O spent?[Pounds 12m]/NeMoney
Answer-point Output: Pounds 12m
This case requires (i) exact match in its original 
verb form between spend and spent; (ii) V-O type 
match; and (iii) asking/answer point type
NeMoney match through direct link.
Q63: What nuclear-powered Russian
submarine sank in the Norwegian Sea on April 
7, 1989?? asking-point grammar link: 
H-M submarine?[What]
A: NEZAVISIMAYA GAZETA on the
Komsomolets nuclear-powered submarine
which sank in the Norwegian Sea five years 
ago:? candidate answer-point grammar link:
H-M submarine?Komsomolets
Answer-point Output: Komsomolets
This case requires (i) exact match of submarine;
(ii) H-M type match; and (iii) asking/answer point 
match through direct link:  there are no asking
point type constraints because the asking point 
goes beyond existing NE. This case highlights the 
power of semantic parsing in answer-point
extraction. Since there are no type constraints on 
answer point,1 candidate answer points cannot be 
extracted without bringing in structural context by 
checking the NE type. Most of what-related asking 
points such as those in the patterns
?what/which?N?, ?what type/kind of ?N? go
beyond NE and require this type of structural
relation checking to locate the exact answer. The 
case below is another example.
Q79: What did Shostakovich write for
Rostropovich?? asking-point grammar link: 
V-O write?[What]
A: The Polonaise from Tchaikovsky?s opera
Eugene was a brief but cracking opener and its 
brilliant bluster was no sooner in our ears than 
forcibly contradicted by the bleak depression of 
Shostakovich?s second cello concerto, Op. 126,
a late work written for Rostropovich in 1966 
between the thirteenth and fourteenth
symphonies. ? candidate answer-point
grammar link:
V-O written?[a late work]/NP
S-P [Op. 126]/NP ?[a late work]/NP
Answer-point Output: Op. 126
This case requires (i) exact match in its original 
verb form between ?written? and ?write?;
(ii) V-O type match; and (iii) asking/answer point 
match through indirect link based on equivalence 
link S-P. When there are no NE constraints on the 
answer point, a proper name or an initial-
capitalized NP is preferred over an ordinary,
lower-case NP as an answer point. This heuristic is 
built-in so that ?Op. 126? is output as the answer-
point in this case instead of ?a late work?.
1 Strictly speaking, there are some type constraints on 
the answer point. The type constraints are something to 
the effect of ?a name for a kind of ship? which goes 
beyond the existing NE types defined.
Conclusion
This paper presented an approach to exact answer 
identification to questions using only binary
structural links involving the question-phrases.
Based on the experiments conducted, some
preliminary conclusions can be arrived at.
? The Entity Association extraction helps in 
pinpointing exact answers precisely
? Grammar dependency links enable the
system to not only identify exact answers 
but answer questions not covered by the 
predefined set of available
NEs/Associations
? Binary dependency links instead of full 
structural templates provide sufficient and 
effective structural leverage for extracting 
exact answers 
Some cases remain difficult however, beyond the 
current level of NLP/IE.  For example,
Q92: Who released the Internet worm in the 
late 1980s?? asking point link: 
V-S (released, NePerson[Who])
A: Morris, suspended from graduate studies at 
Cornell University at Syracuse, N,Y,, is
accused of designing and disseminating in
November, 1988, a rogue program or ?worm? 
that immobilized some 6,000 computers linked 
to a research network, including some used by 
NASA and the Air Force.? answer point link:
V-S (disseminating, NePerson[Morris]) 
In order for this case to be handled, the following 
steps are required: (i) the semantic parser should 
be able to ignore the past participle postmodifier 
phrase headed by ?suspended?; (ii) the V-O
dependency should be decoded between ?is
accused? and ?Morris?; (iii) the V-S dependency 
should be decoded between ?designing and
disseminating? and ?Morris? based on the pattern 
rule ?accuse NP of Ving?? V-S(Ving, NP); (iv) 
the conjunctive structure should map the V-S
(?designing and disseminating?, ?Morris?) into two 
V-S links; (v)  ?disseminate? and ?release? should
be linked somehow for synonym expansion.  It 
may be unreasonable to expect an NLP/IE system 
to accomplish all of these, but each of the above 
challenges indicates some directions for further 
research in this topic.
We would like to extend the experiments on a 
larger set of questions to further investigate the 
effectiveness of structural support in extracting
exact answers. The TREC-9 and TREC 2001 QA 
pool and the candidate answer sentences generated 
by both NLP-based or IR-based QA systems would 
be ideal for further testing this method.
5 Acknowledgement
The authors wish to thank Walter Gadz and Carrie 
Pine of AFRL for supporting this work. Thanks 
also go to anonymous reviewers for their valuable 
comments.
References
Abney, S., Collins, M and Singhal, A. (2000) Answer
Extraction. In Proceedings of ANLP -2000, Seattle.
Chinchor, N. and Marsh, E. (1998) MUC -7 Information 
Extraction Task Definition (version 5.1), In
?Proceedings of MUC-7?. Also published at
http://www.muc.saic.com/
Clarke, C. L. A., Cormack, G. V. and Lynam, T. R. 
(2001), Exploiting Redundancy in Question
Answering. In Proceedings of SIGIR?01, New
Orleans, LA.
Hovy, E.H., U. Hermjakob, and Chin-Yew Lin. 2001. 
The Use of External Knowledge of Factoid QA. In 
Proceedings of the 10th Text Retrieval Conference 
(TREC 2001), Gaithersburg, MD, U.S.A., November 
13-16, 2001
Kupiec, J. (1993) MURAX: A Robust Linguistic
Approach For Question Answering Using An On-Line
Encyclopaedia . In Proceedings of SIGIR-93,
Pittsburgh, PA.
Kwok, K. L., Grunfeld, L., Dinstl, N. and Chan, M. 
(2001), TREC2001 Question-Answer, Web and Cross 
Language Experiments using PIRCS. In Proceedings 
of TREC-10, Gaithersburg, MD.
Li, W. and Srihari, R. (2000) A Domain Independent 
Event Extraction Toolkit , Phase 2 Final Technical 
Report, Air Force Research Laboratory/Rome, NY.
Litkowski, K. C. (1999) Question-Answering Using
Semantic Relation Triples. In Proceedings of TREC-
8, Gaithersburg, MD.
Pasca, M. and Harabagiu, S. M. High Performance
Question/Answering. In Proceedings of SIGIR 2001: 
pages 366-374
Prager, J., Radev, D., Brown, E., Coden, A. and Samn, 
V., The use of predictive annotation for question
answering in TREC8. In Proceedings of TREC-8,
Gaithersburg, MD.
Srihari, R. and Li, W. (1999) Information Extraction 
supported Question Answering. In Proceedings of
TREC-8, Gaithersberg, MD.
Srihari, R and Li, W. (2000b). A Question Answering 
System Supported by Information Extraction. In 
Proceedings of ANLP 2000, Seattle.
Voorhees, E. (1999), The TREC-8 Question Answering 
Track Report, In Proceedings of TREC-8,
Gaithersburg, MD. 
Voorhees, E. (2000), Overview of the TREC-9
Question Answering Track , In Proceedings of
TREC-9, Gaithersburg, MD. 
InfoXtract location normalization: a hybrid approach to geographic 
references in information extraction ? 
 
 
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
(hli, rohini, cniu, wei)@cymfony.com 
 
 
                                                     
? This work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate (AFRL/IF), Rome, 
NY, under contract F30602-01-C-0035. The authors wish to thank Carrie Pine of AFRL for supporting and commenting this work. 
Abstract  
Ambiguity is very high for location names. For 
example, there are 23 cities named ?Buffalo? in the 
U.S.  Based on our previous work, this paper presents 
a refined hybrid approach to geographic references 
using our information extraction engine InfoXtract. 
The InfoXtract location normalization module 
consists of local pattern matching and discourse 
co-occurrence analysis as well as default senses.  
Multiple knowledge sources are used in a number of 
ways: (i) pattern matching driven by local context, 
(ii) maximum spanning tree search for discourse 
analysis, and (iii) applying default sense heuristics 
and extracting default senses from the web. The 
results are benchmarked with 96% accuracy on our 
test collections that consist of both news articles and 
tourist guides. The performance contribution for each 
component of the module is also benchmarked and 
discussed. 
 
1 Introduction 
The task of location normalization is to decode 
geographic references for extracted location  Named 
Entities (NE). Ambiguity is a very serious problem for 
location NEs. For example, there are 23 cities named 
?Buffalo?, including the city in New York State and in 
the state of Alabama. Country names such as 
?Canada?, ?Brazil?, and ?China? are also city names in 
the USA. Such ambiguity needs to be properly 
handled before converting location names into normal 
form to support Entity Profile (EP) construction, 
information merging/consolidation as well as 
visualization of location-stamped extracted events on 
a map.  
Location normalization is a special application of 
word sense disambiguation (WSD). There is 
considerable research on WSD. Knowledge-based 
work, such as [Hirst 1987; McRoy 1992; Ng and 
Lee 1996] used hand-coded rules or supervised 
machine learning based on an annotated corpus to 
perform WSD. Recent work emphasizes a 
corpus-based unsupervised approach [Dagon and 
Itai 1994; Yarowsky 1992; Yarowsky 1995] that 
avoids the need for costly truthed training data.  
Location normalization is different from general 
WSD in that the selection restriction often used for 
WSD in many cases is not sufficient to distinguish 
the correct sense from the other candidates. For 
example, in the sentence ?The White House is 
located in Washington?, the selection restriction 
from the collocation ?located in? can only 
determine that ?Washington? should be a location 
name, but is not sufficient to decide the actual sense 
of this location.  
In terms of local context, we found that there are 
certain fairly predictable keyword-driven patterns 
which can decide the senses of location NEs. These 
patterns use keywords such as ?city?, ?town?, 
?province?, ?on?, ?in? or candidate location subtypes 
that can be assigned from a location gazetteer. For 
example, the pattern ?X + city? can determine sense 
tags for cases like ?New York City?; and the pattern 
?Candidate-city-name + comma + 
Candidate-state-name? can disambiguate cases 
such as ?Albany, New York? and ?Shanghai, 
Illinois?.  
In the absence of these patterns, co-occurring 
location NEs in the same discourse provide 
evidence for predicting the most probable sense of a 
location name. More specifically, location 
normalization depends on co-occurrence 
constraints of geographically related location entities 
mentioned in the same document. For example, if 
?Buffalo?, ?Albany? and ?Rochester? are mentioned in 
the same  document, the most probable senses of 
?Buffalo?, ?Albany? and ?Rochester? should refer to 
the cities in New York State.   
For choosing the best matching sense set within a 
document, we simply construct a graph where each 
node represents a sense of a location NE, and each 
edge represents the relationship between two location 
name senses. A graph  spanning algorithm can be used 
to select the best senses from the graph.  
Last but not least, proper assignment of default 
senses is found to play a significant role in the 
performance of a location normalizer. This involves 
two issues: (i) determining default senses using 
heuristics and/or other methods, such as statistical 
processing for semi-automatic default sense extraction 
from the web [Li et al 2002]; and (ii) setting the 
conditions/thresholds and the proper levels when 
assigning default senses, to coordinate with local and 
discourse evidence for enhanced performance. The 
second issue can be resolved through experimentation. 
In the light of the above overview, this paper 
presents an effective hybrid location normalization 
approach which consists of local pattern matching and 
discourse co-occurrence analysis as well as default 
senses. Multiple knowledge sources are used in a 
number of ways: (i) pattern matching driven by local 
context, (ii) maximum spanning tree search for 
discourse analysis, and (iii) applying heuristics-based 
default senses and web-extracted default senses in 
proper stages.  
In the remaining text, Section 2 introduces the 
background for this research. Section 3 describes our 
previous work in this area and Section 4 presents the 
modified algorithm to address the issues with the 
previous method. Experiment and benchmarks are 
described in Section 5. Section 6 is the conclusion. 
 
2 Background 
The design and implementation of the location 
normalization module is an integrated part of 
Cymfony?s core information extraction (IE) engine 
InfoXtract. InfoXtract extracts and normalizes entities, 
relationships and events from natural language text.  
Figure 1 shows the overall system architecture of 
InfoXtract, involving multiple modules in a pipeline 
structure.  
InfoXtract involves a spectrum of linguistic 
processing and relationship/event extraction. This 
engine, in its current state, involves over 100 levels of 
processing and 12 major components. Some 
components are based on hand-crafted pattern 
matching rules, some are statistical models or 
procedures, and others are hybrid (e.g. NE, 
Co-reference, Location Normalization). The basic 
information extraction task is NE tagging [Krupka 
and Hausman 1998; Srihari et al 2000].  The NE 
tagger identifies and classifies proper names of type 
PERSON, ORGANIZATION, PRODUCT, 
NAMED-EVENTS, LOCATION (LOC) as well as 
numerical expressions such as MEASUREMENT 
(e.g. MONEY, LENGTH, WEIGHT, etc) and time 
expressions (TIME, DATE, MONTH, etc.). 
Parallel to location normalization, InfoXtract also 
involves time normalization and measurement 
normalization.  
   
Document Processor
Knowledge Resources
Lexicon
Resources
Grammars
Process
Manager
Tokenlist
Legend
Output
Manager
Source
Document
Linguistic Processor(s)Tokenizer
Tokenlist
Lexicon Lookup
 POS Tagging
NE Tagging
Shallow
Parsing
Relationship
Extraction
Document
pool
NE
CE
EP
SVO
Time
Normalization
Profile/Event
Consolidation
Event
Extraction
Abbreviations
NE = Named Entity
CE = Correlated Entity
EP = Entity Profile
SVO = Subject-Verb-Object
GE = General Event
PE = Predefined Event
Rule-based
Pattern Matching
Procedure or
Statistical Model
Hybrid
Module
GE
Statistical
Models
PE
IE
Repository
Deep Parsing
Coreference
Location
Normalization
Measurement
Normalization
 Figure 1:  System Architecture of InfoXtract 
 
InfoXtract combines the Maximum Entropy 
Model (MaxEnt) and Hidden Markov Model for 
NE tagging [Srihari et al 2000]. Maximum 
Entropy Models incorporate local contextual 
evidence to handle ambiguity of information from a 
location gazetteer. In the Tipster Location 
Gazetteer used by InfoXtract, there are many 
common words, such as I, A, June, Friendship, etc. 
Also, there is large overlap between person names 
and location names, such as Clinton, Jordan, etc. 
Using MaxEnt, systems learn under what situation 
a word is a location name, but it is very difficult to 
determine the correct sense of an ambiguous 
location name. The NE tagger in InfoXtract only 
assigns the location super-type tag LOC to the 
identified location words and leaves the task of 
location sub-type tagging such as CITY or STATE 
and its disambiguation to the subsequent module 
Location Normalization.  
Beyond NE, the major information objects 
extracted by InfoXtract are Correlated Entity (CE) 
relationships (e.g. AFFILIATION and POSITION), 
Entity Profile (EP) that is a collection of extracted 
entity-centric information, Subject-Verb-Object 
(SVO) which refers to dependency links between 
logical subject/object and its verb governor, General 
Event (GE) on who did what when and where and 
Predefined Event (PE) such as Management 
Succession and Company Acquisition.  
It is believed that these information objects capture 
the key content of the processed text. When 
normalized location, time and measurement NEs are 
associated with information objects (events, in 
particular) based on parsing, co-reference and/or 
discourse propagation, these events are stamped. The 
processing results are stored in IE Repository, a 
dynamic knowledge warehouse used to support 
cross-document consolidation, text mining for hidden 
patterns and IE applications. For example, 
location-stamped events can support information 
visualization on maps (Figure 2); time-stamped 
information objects can support visualization along a 
timeline; measurement-stamped objects will allow 
advanced retrieval such as find all Company 
Acquisition events that involve money amount greater 
than 2 million US dollars. 
 
Event type: <Die: Event 200>
Who:       <Julian Werver Hill: PersonProfile 001>
When:     1996-01-07
Where:    <LocationProfile103>
Preceding_event: <hospitalize: Event 260>
Subsequent_event: <bury: Event 250>
Event Visualization
;  ; 
; ; 
Predicate: DieWho: Julian Werner Hill
When:
Where: <LocationProfile 103>
Hockessin, Delaware, USA,
19707,75.688873,39.77604
1996-01-07
Figure 2:  Location-stamped Information 
Visualization 
 
3 Previous Work and Issues 
This paper is follow-up research based on our previous 
work [Li et al 2002]. Some efficiency and 
performance issues are identified and addressed by the 
modified approach.  
The previous algorithm [Li et al 2002] for location 
normalization consisted of five steps. 
 
Step 1. Look up location names in the 
gazetteer to associate candidate senses for 
each location NE; 
Step 2. Call the pattern matching sub-module 
to resolve the ambiguity of the NEs involved 
in local patterns like ?Williamsville, New 
York, USA? to retain only one sense for the 
NE as early as possible; 
Step 3. Apply the ?one sense per discourse? 
principle [Gale et al1992] for each 
disambiguated location name to propagate 
the selected sense to its other mentions 
within a document; 
Step 4. Call the discourse sub-module, 
which is a graph search algorithm 
(Kruskal?s algorithm), to resolve the 
remaining ambiguities; 
Step 5. If the decision score for a location 
name is lower than a threshold, we choose a 
default sense of that name as a result. 
In this algorithm, Step 2, Step 4, and Step 5 
complement each other, and help produce better 
overall performance.  
Step 2 uses local context that is the co-occurring 
words around a location name. Local context can be 
a reliable source in deciding the sense of a location. 
The following are the most commonly used 
patterns for this purpose.  
 
(1) LOC + ?,? + NP (headed by ?city?)  
e.g. Chicago, an old city  
(2) ?city of? + LOC1 + ?,? + LOC2 
e.g. city of Albany, New York 
(3) ?city of? + LOC 
(4) ?state of? + LOC  
(5) LOC1+ ?,? + LOC2 + ?,? + LOC3 
e.g. (i) Williamsville, New York, USA 
       (ii) New York, Buffalo, USA 
     (6) ?on?/ ?in? + LOC 
 e.g. on Strawberry  ISLAND 
 in Key West  CITY 
 
Patterns (1) , (3), (4) and (6) can be used to decide if 
the location is a city, a state or an island, while 
patterns (2) and (5) can be used to determine both 
the sub-tag and its sense. 
Step 4 constructs a weighted graph where each 
node represents a location sense, and each edge 
represents similarity weight between location 
names. The graph is partially complete since there 
are no links among the different senses of a location 
name. The maximum weight spanning tree (MST) 
is calculated using Kruskal?s MinST algorithm 
[Cormen et al 1990]. The nodes on the resulting 
MST are the most promising senses of the location 
names.  
Figure 3 and Figure 4 show the graphs for 
calculating MST. Dots in a circle mean the number 
of senses of a location name. 
Through experiments, we found an efficiency 
problem in Step 4 which adopted Kruskal?s 
algorithm for MST search to capture the impact of 
location co-occurrence in a discourse. While this 
algorithm works fairly well for short documents (e.g. 
most news articles), there is a serious time complexity 
issue when numerous location names are contained in 
long documents. A weighted graph is constructed by 
linking sense nodes for each location with the sense 
nodes for other locations. In addition, there is also an 
associated performance issue: the value weighting for 
the calculated edges using the previous method is not 
distinctive enough.  We observe that the number of 
location mentions and the distance between the 
location names impact the selection of location senses, 
but the previous method could not reflect these factors 
in distinguishing the weights of candidate senses. 
 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Quebec Prov,
Connecticut,
?}
3*4 lines
2*3 lines
4*11 lines
11*10 lines
3*10 lines8*3 lines
2*8 lines
2*43*11
3*10
3*33*8
2*10
2*3
8*4
8*11
8*10
8*4
10*4
3*11
 
Figure 3:  Graph and its Spanning Tree 
 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{city in New York
Port in canada}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Prov in Canada,
Connecticut,
?}
3.6
3.6
3.66
3.6
3.6
0
1
2
3
4
5
6
7
 
Figure 4:  Max Spanning Tree 
 
Finally, our research shows that default senses play 
a significant role in location normalization. For 
example, people refer to ?Los Angeles? as the city in 
California more than the city in the Philippines, Chile, 
Puerto Rico, or the city in Texas in the USA. 
Unfortunately, the available Tipster Gazetteer 
(http://crl.nmsu.edu/cgi-bin/Tools/CLR/clrcat) does 
not mark default senses for most entries. It has 
171,039 location entries with 237,916  senses, among 
which 30,711 location names are ambiguous. 
Manually tagging the default senses for over 30,000 
location names is difficult; moreover, it is also subject 
to inconsistency due to the different knowledge 
backgrounds of the human taggers. This problem 
was solved by developing a procedure to 
automatically extract default senses from web 
pages using the Yahoo! search engine [Li et al 
2002]. Such a procedure has the advantage of 
enabling ?re-training? of default senses when 
necessary. If the web pages obtained through Yahoo! 
represent a typical North American ?view? of what 
default sense should be assigned to location names, 
it may be desirable to re-train the default senses of 
location names  using other views (e.g. an Asian 
view or African view) when the system needs to 
handle overseas documents that contain many 
foreign location names.  
In addition to the above automatic default sense 
extraction, we later found that a few simple default 
sense heuristics, when used at proper levels, can 
further enhance performance. This finding is 
incorporated in our modified approach described in 
Section 3 below.  
 
4 Modified Hybrid Approach 
To address the issues identified in Section 2, we 
adopt Prim?s algorithm, which traverses each node 
of a graph to choose the most promising senses. 
This algorithm has much less search space and 
shows the advantage of being able to reflect the 
number of location mentions and their distances in 
a document.  
The following is the description of our adapted 
Prim?s algorithm for the weight calculation.  
The weight of each sense of a node is calculated 
by considering the effect of linked senses of other 
location nodes based on a predefined weight table 
(Table 1) for the sense categories of co-occurring 
location names. For example, when a location name 
with a potential city sense co-occurs with a location 
name with a potential state/province sense and the 
city is in the state/province, the impact weight of 
the state/province name on the city name is fairly 
high, with the weight set to 3 as shown in the 3rd 
row of Table 1.   
Table 1. Impact weight of Sense2 on Sense1 
Sense1 Sense2 Condition  Weight 
City City in same state 2 
 City in same country 1 
 State in same state 3 
 Country in country without 
state (e.g. in Europe) 
4 
 
Let W(Si) be the calculated weight of a sense Sj of 
a location; weight(Sj->Si) means the weight of Si 
influenced by sense Sj; Num(Loci) is the number of 
location mentions; and ?/dist(Loci, Locj) is the 
measure of distance between two locations.  The final 
sense of a location is the one that has maximum 
weight. A location name may be mentioned a number 
of times in a document.  For each location name, we 
only count the location mention that has the maximum 
sense weight summation in equation (1) and 
eventually propagate the selected sense of this 
location mention to all its other mentions based on one 
sense per discourse principle.  Equation (2) refers to 
the sense with the maximum weight for Loci. 
 
(1)
( )

=
?
=
m
j
jijij
i
LocLocdistLocNumSSweight
SW
0
),(/*)(*)(
)(
?
 
(2) ))(()( maxarg j
j
i SWLocS =   
wj ??0  
Through experiments, we also found that it is 
beneficial to select default senses when candidate 
location senses in the discourse analysis turn out to be 
of the same weight. We included two kinds of default 
senses: heuristics-based default senses and the default 
senses extracted semi-automatically from the web 
using Yahoo. For the first category of default senses, 
we observe that if a name has a country sense and 
other senses, such as ?China? and ?Canada?, the 
country senses are dominant in most cases. The 
situation is the same for a name with province sense 
and for a name with country capital sense (e.g. London, 
Beijing). The updated algorithm for location 
normalization is as follows. 
 
Step 1. Look up the location gazetteer to 
associate candidate senses for each location 
NE; 
Step 2. If a location has sense of country, then 
select that sense as the default sense of that 
location (heuristics); 
Step 3. Call the pattern matching sub-module 
for local patterns like ?Williamsville, New 
York, USA?; 
Step 4. Apply the ?one sense per discourse? 
principle for each disambiguated location 
name to propagate the selected sense to its 
other mentions within a document; 
Step 5. Apply default sense heuristics for a 
location with province or capital senses; 
Step 6. Call Prim?s algorithm in the 
discourse sub-module to resolve the 
remaining ambiguities (Figure 5); 
Step 7. If the difference between the sense 
with the maximum weight and the sense 
with next largest weight is equal to or lower 
than a threshold, choose the default sense of 
that name from lexicon.  Otherwise, choose 
the sense with the maximum weight as 
output. 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Quebec Prov,
Connecticut,
?}
 
 Figure 5:  Weight assigned to Sense Nodes 
 
5 Experiment and Benchmark 
With the information from local context, discourse 
context and the knowledge of default senses, the 
location normalization process is  efficient and 
precise.  
The testing documents were randomly selected 
from CNN news and from travel guide web pages. 
Table 2 shows the preliminary testing results using 
different configurations.  
As shown, local patterns (Column 4) alone 
contribute 12% to the overall performance while 
proper use of defaults senses and the heuristics 
(Column 5) can achieve close to 90%. In terms of 
discourse co-occurrence evidence, the new method 
using Prim?s algorithm (Column 7) is clearly better 
than the previous method using Kruskal?s 
algorithm (Column 6), with 13% enhancement 
(from 73.8% to 86.6%). But both methods cannot 
outperform default senses.  Finally, when using all 
three types of evidence, the new hybrid method 
presented in this paper shows significant 
performance enhancement (96% in Column 9) over 
the previous method (81.9% in Column 8), in 
addition to a satisfactory solution to the efficiency 
problem.  
Table 2. Experimental evaluation for location normalization 
File # of 
ambiguous 
location 
names 
# of 
mentions 
Pattern 
hits 
Def-
senses 
 
Kruskal 
Algo. 
only 
Prim 
Algo 
only  
Kruskal 
+Pattern 
+Def 
(previous) 
Prim 
+Pattern 
+Def 
(new) 
Cnn1 26 39 4 20 21 24 26  26 
Cnn2 12 20 5 11 7 10 11 11 
Cnn3 14 29 0 12 10 12 10 14 
Cnn4 8 14 2 8 4 4 4 8 
Cnn5 11 26 1 9 5 8 5 9 
Cnn6 19 35 6 16 11 16 13 18 
Cnn7 11 27 0 11 4 7 6 10 
Calif. 16 30 0 16 16 16 16 16 
Florida 19 28 0 19 19 19 18 19 
Texas 13 13 0 12 13 13 13 12 
Total 149 261 12% 89.9% 73.8% 86.6% 81.9% 96% 
 
We observed that if a file contains more 
concentrated locations, such as the state introductions 
in the travel guides for California, Florida and Texas, 
the accuracy is higher than the relatively short news 
articles from CNN.  
 
6 Conclusion and Future Work 
This paper presented an effective hybrid method of 
location normalization for information extraction with 
promising experimental results. In the future, we will 
integrate an expanded location gazetteer including 
names of landmarks, mountains and lakes such 
as Holland Tunnel (in New York, not in Holland) and 
Hoover Dam (in Arizona, not in Alabama), to enlarge 
the system coverage. Meanwhile, more extensive 
benchmarking is currently being planned in order to 
conduct a detailed analysis of different evidence 
sources and their interaction and contribution to 
system performance.  
References 
Cormen, Thomas H., Charles E. Leiserson, and 
Ronald L. Rivest. 1990. Introduction to Algorithm. 
The MIT Press, 504-505. 
Dagon, Ido and Alon Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics, 
Vol.20, 563-596. 
Gale, W.A., K.W. Church, and D. Yarowsky. 1992. 
One Sense Per Discourse. Proceedings of the 4th 
DARPA Speech and Natural Language Workshop. 
233-237. 
Hirst, Graeme. 1987. Semantic Interpretation and the 
Resolution of Ambiguity. Cambridge University 
Press, Cambridge. 
Huifeng Li, Rohini K. Srihari, Cheng Niu, Wei Li. 
2002. Location Normalization for Information 
Extraction, COLING 2002, Taipei, Taiwan. 
Krupka, G.R. and K. Hausman.  1998.  IsoQuest 
Inc.: Description of the NetOwl (TM) Extractor 
System as Used for MUC-7.  Proceedings of 
MUC.  
McRoy, Susan W. 1992. Using Multiple 
Knowledge Sources for Word Sense 
Discrimination. Computational Linguistics, 
18(1): 1-30. 
Ng, Hwee Tou and Hian Beng Lee. 1996. 
Integrating Multiple Knowledge Sources to 
Disambiguate Word Sense: an Exemplar-based 
Approach. ACL 1996, 40-47, California. 
Srihari, Rohini, Cheng Niu, and Wei Li. 2000. A 
Hybrid Approach for Named Entity and 
Sub-Type Tagging. ANLP 2000, Seattle. 
Yarowsky, David. 1992. Word-sense 
Disambiguation Using Statistical Models of 
Roget?s Categories Trained on Large Corpora. 
COLING 1992, 454-460, Nantes, France. 
Yarowsky, David. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
ACL 1995, Cambridge, Massachusetts. 
  
InfoXtract: A Customizable Intermediate Level Information 
Extraction Engine?  
 
Rohini K. Srihari 
Cymfony, Inc. 
State University of New York at Buffalo 
rohini@Cymfony.com
Wei Li, Cheng Niu and Thomas Cornell 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
{wei, cniu, cornell}@Cymfony.com
 
Keywords: Information Extraction, Named Entity Tagging, Machine Learning, Domain Porting 
 
                                                     
? This work was supported in part by SBIR grants F30602-01-C-0035, F30602-03-C-0156, and 
F30602-02-C-0057 from the Air Force Research Laboratory (AFRL)/IFEA. 
 
Abstract 
Information extraction (IE) systems assist 
analysts to assimilate information from 
electronic documents. This paper focuses on 
IE tasks designed to support information 
discovery applications. Since information 
discovery implies examining large volumes 
of documents drawn from various sources for 
situations that cannot be anticipated a priori, 
they require IE systems to have breadth as 
well as depth. This implies the need for a 
domain-independent IE system that can 
easily be customized for specific domains: 
end users must be given tools to customize 
the system on their own. It also implies the 
need for defining new intermediate level IE 
tasks that are richer than the 
subject-verb-object (SVO) triples produced 
by shallow systems, yet not as complex as the 
domain-specific scenarios defined by the 
Message Understanding Conference (MUC). 
This paper describes a robust, scalable IE 
engine designed for such purposes. It 
describes new IE tasks such as entity profiles, 
and concept-based general events which 
represent realistic goals in terms of what can 
be accomplished in the near-term as well as 
providing useful, actionable information. 
These new tasks also facilitate the correlation 
of output from an IE engine with existing 
structured data. Benchmarking results for the 
core engine and applications utilizing the 
engine are presented. 
1 Introduction 
This paper focuses on new intermediate level 
information extraction tasks that are defined and 
implemented in an IE engine, named InfoXtract. 
InfoXtract is a domain independent, but portable 
information extraction engine that has been designed 
for information discovery applications. 
The last decade has seen great advances in the area 
of IE. In the US, MUC [Chinchor & Marsh 1998] has 
been the driving force for developing this technology.  
The most successful IE task thus far has been 
Named Entity (NE) tagging. The state-of-the-art 
exemplified by systems such as NetOwl [Krupka & 
Hausman 1998], IdentiFinder [Miller et al1998] and 
InfoXtract [Srihari et al2000] has reached near human 
performance, with 90% or above F-measure. On the 
other hand, the deep level MUC IE task Scenario 
Template (ST) is designed to extract detailed 
information for predefined event scenarios of interest. 
It involves filling the slots of complicated templates. It 
is generally felt that this task is too ambitious for 
commercial application at present.  
Information Discovery (ID) is a term which has 
traditionally been used to describe efforts in data 
mining [Han 1999]. The goal is to extract novel 
patterns of transactions which may reveal interesting 
trends. The key assumption is that the data is already 
in a structured form. ID in this paper is defined within 
the context of unstructured text documents; it is the 
ability to extract, normalize/disambiguate, merge and 
link entities, relationships, and events which provides 
significant support for ID applications. Furthermore, 
there is a need to accumulate information across 
documents about entities and events. Due to rapidly 
changing events in the real world, what is of no 
interest one day, may be especially interesting the 
following day. Thus, information discovery 
applications demand breadth and depth in IE 
technology.  
A variety of IE engines, reflecting various goals in 
terms of extraction as well as architectures are now 
available. Among these, the most widely used are the 
GATE system from the University of Sheffield 
[Cunningham et al2003], the IE components from 
Clearforest (www.clearforest.com), SIFT from BBN 
[Miller et al1998], REES from SRA [Aone & 
Ramon-Santacruz 1998] and various tools provided 
by Inxight (www.inxight.com). Of these, the GATE 
system most closely resembles InfoXtract in terms of 
its goals as well as the architecture and customization 
tools. Cymfony differentiates itself by using a hybrid 
  
model that efficiently combines statistical and 
grammar-based approaches, as well as by using an 
internal data structure known as a token-list that can 
represent hierarchical linguistic structures and IE 
results for multiple modules to work on.  
The research presented here focuses on a new 
intermediate level of information extraction which 
supports information discovery. Specifically, it 
defines new IE tasks such as Entity Profile (EP) 
extraction, which is designed to accumulate 
interesting information about an entity across 
documents as well as within a discourse. Furthermore, 
Concept-based General Event (CGE) is defined as a 
domain-independent, representation of event 
information but more feasible than MUC ST.  
InfoXtract represents a hybrid model for extracting 
both shallow and intermediate level IE: it exploits 
both statistical and grammar-based paradigms. A key 
feature is the ability to rapidly customize the IE engine 
for a specific domain and application. Information 
discovery applications are required to process an 
enormous volume of documents, and hence any IE 
engine must be able to scale up in terms of processing 
speed and robustness; the design and architecture of 
InfoXtract reflect this need.  
In the remaining text, Section 2 defines the new 
intermediate level IE tasks. Section 3 presents 
extensions to InfoXtract to support cross-document 
IE. Section 4 presents the hybrid technology. Section 
5 delves into the engineering architecture and 
implementation of InfoXtract. Section 6 discusses 
domain porting. Section 7 presents two applications 
which have exploited InfoXtract, and finally, Section 
8 summarizes the research contributions. 
2 InfoXtract: Defining New IE Tasks 
InfoXtract [Li & Srihari 2003, Srihari et al2000] is a 
domain-independent and domain-portable, inter- 
mediate level IE engine. Figure 1 illustrates the 
overall architecture of the engine. 
A description of the increasingly sophisticated IE 
outputs from the InfoXtract engine is given below: 
 
? NE:  Named Entity objects represent key items 
such as proper names of person, organization, 
product, location, target, contact information 
such as address, email, phone number, URL, time 
and numerical expressions such as date, year and 
various measurements weight, money, 
percentage, etc.  
? CE:  Correlated Entity objects capture relation- 
ship mentions between entities such as the 
affiliation relationship between a person and his 
employer. The results will be consolidated into 
the information object Entity Profile (EP) based 
on co-reference and alias support. 
? EP:  Entity Profiles are complex rich information 
objects that collect entity-centric information, in 
particular, all the CE relationships that a given 
entity is involved in and all the events this entity 
is involved in. This is achieved through 
document-internal fusion and cross-document 
fusion of related information based on support 
from co-reference, including alias association. 
Work is in progress to enhance the fusion by 
correlating the extracted information with 
information in a user-provided existing database. 
? GE:  General Events are verb-centric information 
objects representing ?who did what to whom 
when and where? at the logical level. 
Concept-based GE (CGE) further requires that 
participants of events be filled by EPs instead of 
NEs and that other values of the GE slots (the 
action, time and location) be disambiguated and 
normalized.  
? PE:  Predefined Events are domain specific or 
user-defined events of a specific event type, such 
as Product Launch and Company Acquisition in 
the business domain. They represent a simplified 
version of MUC ST. InfoXtract provides a toolkit 
that allows users to define and write their own 
PEs based on automatically generated PE rule 
templates.  
 
The InfoXtract engine has been deployed both 
internally to support Cymfony?s Brand Dashboard? 
product and externally to a third-party integrator for 
building IE applications in the intelligence domain.  
 
Document Processor
Knowledge Resources
Lexicon
Resources
Grammars
Process
Manager
Tokenlist
Legend
Output
Manager
Source
Document
NLP/IE Processor(s)Tokenizer
Tokenlist
Lexicon Lookup
 POS Tagging
Named Entity
Detection
Shallow
Parsing
Deep Parsing
Relationship
Detection
Document
pool
NE
CE
EP
SVO
Time
Normalization
Alias and
Coreference
Profile/Event
Linking/Merging
Abbreviations
POS = Part of Speech
NE = Named Entity
CE = Correlated Entity
EP = Entity Profile
SVO = Subject-Verb-Object
GE = General Event
PE = Predefined Event
Grammar Module
Procedure or
Statistical Model
Hybrid
Module
GE
Statistical
Models
Location
Normalizationli ti
PE
InfoXtract
Repository
Event
Extraction
Case Restoration
 
Figure 1:  InfoXtract Engine Architecture 
  
3 Hybrid Technology 
InfoXtract represents a hybrid model for IE since it 
combines both grammar formalisms as well as 
machine learning. Achieving the right balance of these 
two paradigms is a major design objective of 
InfoXtract. The core of the parsing and information 
extraction process in InfoXtract is organized very 
simply as a pipeline of processing modules. All 
modules operate on a single in-memory data structure, 
called a token list. A token list is essentially a 
sequence of tree structures, overlaid with a graph 
whose edges define relations that may be either 
grammatical or informational in nature. The nodes of 
these trees are called tokens. InfoXtract?s typical 
mode of processing is to skim along the roots of the 
trees in the token list, building up structure 
?strip-wise?. So even non-terminal nodes behave, in 
the typical case, as complex tokens. Representing a 
marked up text using trees explicitly, rather than 
implicitly as an interpretation of paired bracket 
symbols, has several advantages. For example, it 
allows a somewhat richer organization of the 
information contained ?between the brackets,? 
allowing us to construct direct links from a root node 
to its semantic head, for example. 
The processing modules that act on token lists can 
range from lexical lookup to the application of hand 
written grammars to statistical analysis based on 
machine learning all the way to arbitrary procedures 
written in C++. The configuration of the InfoXtract 
processing pipeline is controlled by a configuration 
file, which handles pre-loading required resources as 
well as ordering the application of modules. Despite 
the variety of implementation strategies available, 
InfoXtract Natural Language Processing (NLP) 
modules are restricted in what they can do to the token 
list to actions of the following three types : 
 
1. Assertion and erasure of token properties 
(features, normal forms, etc.) 
2. Grouping token sequences into higher level 
constituent tokens. 
3. Linking token pairs with a relational link. 
 
Grammatical analysis of the input text makes use of a 
combination of phrase structure and relational 
approaches to grammar. Basically, early modules 
build up structure to a certain level (including 
relatively simple noun phrases, verb groups and 
prepositional phrases), after which further 
grammatical structure is represented by asserting 
relational links between tokens. This mix of phrase 
structural and relational approaches is very similar to 
the approach of Lexical Functional Grammar (LFG) 
[Kaplan & Bresnan 1982], much scaled down.  
Our grammars are written in a formalism 
developed for our own use, and also in a modified 
formalism developed for outside users, based on our 
in-house experiences. In both cases, the formalism 
mixes regular expressions with boolean expressions. 
Actions affecting the token list are implemented as 
side effects of pattern matching. So although our 
processing modules are in the technical sense token 
list transducers, they do not resemble Finite State 
Transducers (FSTs) so much as the regular expression 
based pattern-action rules used in Awk or Lex. 
Grammars can contain (non-recursive) macros, with 
parameters. 
This means that some long-distance dependencies, 
which are very awkward to represent directly in finite 
state automata can be represented very compactly in 
macro form. While this has the advantage of 
decreasing grammar sizes, it does increase the size of 
the resulting automata. Grammars are compiled to a 
special type of finite state automata. These token list 
automata can be thought of as an extension of tree 
walking automata [M?nnich et al2001, Aho & 
Ullman 1971, Engelfriet et al1999]. These are linear 
automata (as opposed to standard finite state tree 
automata [G?cseg &  Steinby 1997], which are more 
naturally thought of as parallel) which run over trees. 
The problem with linear automata on trees is that there 
can be a number of ?next? nodes to move the read 
head to: right sister, left sister, parent, first child, etc. 
So the vocabulary of the automaton is increased to 
include not only symbols that might appear in the text 
(test instructions) but also symbols that indicate where 
to move the read head (directive instructions). We 
have extended the basic tree walking formalism in 
several directions. First we extend the power of test 
instructions to allow them to check features of the 
current node and to perform string matching against 
the semantic head of the current node (so that a 
syntactically complex constituent can be matched 
against a single word). Second, we include symbols 
for action instructions, to implement side effects. 
Finally, we allow movement not only along the root 
sequence (string-automaton style) and branches of a 
tree (tree-walking style) but also along the the 
terminal frontier of the tree and along relational links. 
These extensions to standard tree walking 
automata extend the power of that formalism 
tremendously, and could pose problems. However, the 
grammar formalisms that compile into these token list 
walking automata are restrictive, in the sense that 
there exist many token list transductions that are 
implementable as automata that are not 
implementable as grammars. Also the nature of the 
shallow parsing task itself is such that we only need to 
dip into the reserves of power that this representation 
affords us on relatively rare occasions. As a result, the 
automata that we actually plug into the InfoXtract 
NLP pipeline generally run very fast. 
Recently, we have developed an extended finite 
state formalism named Expert Lexicon, following the 
general trend of lexicalist approaches to NLP. An 
  
expert lexicon rule consists of both grammatical 
components as well as proximity-based keyword 
matching. All Expert Lexicon entries are indexed, 
similar to the case for the finite state tool in INTEX 
[Silberztein 2000]. The pattern matching time is 
therefore reduced dramatically compared to a 
sequential finite state device.  
Some unique features of this formalism include: (i) 
the flexibility of inserting any number of Expert 
Lexicons at any level of the process; (ii) the capability 
of proximity checking within a window size as rule 
constraints in addition to pattern matching using an 
FST call, so that the rule writer can exploit the 
combined advantages of both; and (iii) support for the 
propagation of semantic tagging results, to 
accommodate principles like one sense per discourse. 
Expert lexicons are used in customization of lexicons, 
named entity glossaries, and alias lists, as well as 
concept tagging. 
Both supervised machine learning and unsuper- 
vised learning are used in InfoXtract. Supervised 
learning is used in hybrid modules such as NE [Srihari 
et al2000], NE Normalization [Li et al2002] and 
Co-reference. It is also used in the preprocessing 
module for orthographic case restoration of case 
insensitive input [Niu et al2003]. Unsupervised 
learning involves acquisition of lexical knowledge 
and rules from a raw corpus. The former includes 
word clustering, automatic name glossary acquisition 
and thesaurus construction. The latter involves 
bootstrapped learning of NE and CE rules, similar to 
the techniques used in [Riloff 1996]. The results of 
unsupervised learning can be post-edited and added as 
additional resources for InfoXtract processing.  
 
Table 1: SVO/CE Benchmarking 
 SVO CE 
 CORRECT 196 48 
 INCORRECT 13 0 
 SPURIOUS 10 2 
 MISSING 31 10 
 PRECISION 89.50% 96.0% 
 RECALL 81.67% 82.8% 
 F-MEASURE 85.41% 88.9% 
 
Accuracy 
InfoXtract has been benchmarked using the MUC-7 
data sets which are recognized as standards by the 
research community. Precision and recall figures for 
the person and location entity types were above 90%. 
For organization entity types, precision and recall 
were in the high 80?s reflecting the fact that 
organization names tend to be very domain specific. 
InfoXtract provides the ability to create customized 
named entity glossaries, which will boost the 
performance of organization tagging for a given 
domain. No such customization was done in the 
testing just described. The accuracy of shallow 
parsing is well over 90% reflecting very high 
performance part-of-speech tagging and named entity 
tagging. Table 1 shows the benchmarks for CE 
relationships which are the basis for EPs and for the 
SVO parsing which supports event extraction.  
4 Engineering Architecture 
The InfoXtract engine has been developed as a 
modular, distributed application and is capable of 
processing up to 20 MB per hour on a single 
processor. The system has been tested on very large (> 
1 million) document collections. The architecture 
facilitates the incorporation of the engine into external 
applications requiring an IE subsystem. Requests to 
process documents can be submitted through a web 
interface, or via FTP. The results of processing a 
document can be returned in XML. Since various 
tools are available to automatically populate databases 
based on XML data models, the results are easily 
usable in web-enabled database applications. 
Configuration files enable the system to be used with 
different lexical/statistical/grammar resources, as well 
as with subsets of the available IE modules.  
InfoXtract supports two modes of operation, active 
and passive. It can act as an active retriever of 
documents to process or act as a passive receiver of 
documents to process. When in active mode, 
InfoXtract is capable of retrieving documents via 
HTTP, FTP, or local file system. When in passive 
mode, InfoXtract is capable of accepting documents 
via HTTP. Figure 2 illustrates a multiple processor 
configuration of InfoXtract focusing on the typical 
deployment of InfoXtract within an application. 
 
Server B
Server C
Server A
Processor 4
Processor 6
Processor 2
Document
Retriever
InfoXtract
Controller
Document
Manager
Processor 1
Processor 3
Processor 5
Extracted info
database
Documents
External Content
Provider
Java InfoXtract
(JIX)
External
Application
Figure 2:  High Level Architecture 
 
The architecture facilitates scalability by 
supporting multiple, independent Processors. The 
Processors can be running on a single server (if 
multiple CPUs are available) and on multiple servers. 
The Document Manager distributes requests to 
process documents to all available Processors. Each 
component is an independent application. All direct 
  
inter-module communication is accomplished using 
the Common Object Request Broker Architecture 
(CORBA). CORBA provides a robust, programming 
language independent, and platform neutral 
mechanism for developing and deploying distributed 
applications. Processors can be added and removed 
without stopping the InfoXTract engine. All modules 
are self-registering and will announce their presence 
to other modules once they have completed 
initialization.  
The Document Retriever module is only used in 
the active retriever mode. It is responsible for 
retrieving documents from a content provider and 
storing the documents for use by the InfoXtract 
Controller. The Document Retriever handles all 
interfacing with the content provider?s retrieval 
process, including interface protocol (authentication, 
retrieve requests, etc.), throughput management, and 
document packaging. It is tested to be able to retrieve 
documents from content providers such as Northern 
Light, Factiva, and LexisNexis. Since the Document 
Retriever and the InfoXtract Controller do not 
communicate directly, it is possible to run the 
Document Retriever standalone and process all 
retrieved documents in a batch mode at a later time.  
The InfoXtract Controller module is used only in 
the active retriever mode. It is responsible for 
retrieving documents to be processed, submitting 
documents for processing, storing extracted 
information, and system logging. The InfoXtract 
Controller is a multi-threaded application that is 
capable of submitting multiple simultaneous requests 
to the Document Manager. As processing results are 
returned, they are stored to a repository or database, an 
XML file, or both. 
The Document Manager module is responsible for 
managing document submission to available 
Processors. As Processors are initialized, they register 
with the Document Manager. The Document Manager 
uses a round robin scheduling algorithm for sending 
documents to available Processors. A document queue 
is maintained with a size of four documents per 
Processor. The Processor module forms the core of the 
IE engine. InfoXtract utilizes a multi-level approach 
to NLP. Each level utilizes the results of the previous 
levels in order to achieve more sophisticated parsing. 
The JIX module is a web application that is 
responsible for accepting requests for documents to be 
processed. This module is only used in the passive 
mode. The document requests are received via the 
HTTP Post request. Processing results are returned in 
XML format via the HTTP Post response.  
In Table 2 we present an example of the 
performance that can be expected based on the 
application of all modules within the engine. It should 
be noted that considerably faster processing per 
processor can be achieved if output is restricted to a 
certain IE level, such as named entity tagging only. 
The output in this benchmark includes all major tasks 
such as NE, EP, parsing and event extraction as well 
as XML generation.  
This configuration provides throughput of 
approximately 12,000 documents (avg. 10KB) per 
day. A smaller average document size will increase 
the document throughput. Increased throughput can 
be achieved by dedicating a CPU for each running 
Processor. Each Processor instance requires 
approximately 500 MB of RAM to run efficiently. 
Processing speed increases linearly with additional 
Processors/CPUs, and CPU speed. In the current state, 
with no speed optimization, using a bank of eight 
processors, it is able to process approximately 
100,000 documents per day. Thus, InfoXtract is 
suitable for high volume deployments. The use of 
CORBA provides seamless inter-process and 
over-the-wire communication between modules. 
Computing resources can be dynamically assigned to 
handle increases in document volume.  
 
Table 2:  Benchmark for Efficiency 
Server 
Configuration 
2 CPU @ 1 GHz, 2 GB 
RAM 
Operating System Redhat Linux 7.2 
Document 
Collection Size 
500 Documents, 5 MB 
total size 
Engine 
Configuration 
InfoXtract Controller, 
Document Manager, 
and 2 Processors 
running on a single 
server 
Processing Time 30 Minutes 
 
A standard document input model is used to 
develop effective preprocessing capabilities. 
Preprocessing adapts the engine to the source by 
presenting metadata, zoning information in a 
standardized format and performing restoration tasks 
(e.g. case restoration). Efforts are underway to 
configure the engine such that zone-specific 
processing controls are enabled. For example, zones 
identified as titles or subtitles must be tagged using 
different criteria than running text. The engine has 
been deployed on a variety of input formats including 
HUMINT documents (all uppercase), the Foreign 
Broadcast Information Services feed (FBIS), live 
feeds from content providers such as Factiva (Dow 
Jones/Reuters), LexisNexis, as well as web pages. A 
user-trainable, high-performance case restoration 
module [Niu et al2003] has been developed that 
transforms case insensitive input such as speech 
transcripts into mixed-case before being processed by 
the engine. The case restoration module eliminates the 
need for separate IE engines for case-insensitive and 
case-sensitive documents; this is easier and more cost 
effective to maintain. 
  
5 Corpus-level IE 
Efforts have extended IE from the document level to 
the corpus level. Although most IE systems perform 
corpus-level information consolidation at an 
application level, it is felt that much can be gained by 
doing this as an extended step in the IE engine. A 
repository has been developed for InfoXtract that is 
able to hold the results of processing an entire corpus. 
A proprietary indexing scheme for indexing token-list 
data has been developed that enables querying over 
both the linguistic structures as well as statistical 
similarity queries (e.g., the similarity between two 
documents or two entity profiles). The repository is 
used by a fusion module in order to generate 
cross-document entity profiles as well as for text 
mining operations. The results of the repository 
module can be subsequently fed into a relational 
database to support applications. This has the 
advantage of filtering much of the noise from the 
engine level and doing sophisticated information 
consolidation before populating a relational database. 
The architecture of these subsequent stages is shown 
in Figure 3. 
 
Databases
Fusion
Module
Corpus-
level IEInfoXtract
Text
Mining
FBIS, Newswire
Documents
InfoXtract
Repository 1
InfoXtract
Repository 2
IDP
Figure 3:  Extensions to InfoXtract 
 
Information Extraction has two anchor points: (i) 
entity-centric information which leads to an EP, and 
(ii) action-centric information which leads to an event 
scenario. Compared with the consolidation of 
extracted events into cross-document event scenario, 
cross-document EP merging and consolidation is a 
more tangible task, based mainly on resolving aliases. 
Even with modest recall, the corpus-level EP 
demonstrates tremendous value in collecting 
information about an entity.  This is as shown in Table 
3 for only part of the profile of ?Mohamed Atta? from 
one experiment based on a collection of news articles. 
The extracted EP centralizes a significant amount of 
valuable information about this terrorist. 
6 Domain Porting 
Considerable efforts have been made to keep the core 
engine as domain independent as possible; domain 
specialization or tuning happens with minimum 
change to the core engine, assisted by automatic or 
semi-automatic domain porting tools we have 
developed.  
Cymfony has taken several distinct approaches in 
achieving domain portability: (i) the use of a standard 
document input model, pre-processors and 
configuration scripts in order to tailor input and output 
formats for a given application, (ii) the use of tools in 
order to customize lexicons and grammars, and (iii) 
unsupervised machine learning techniques for 
learning new named entities (e.g. weapons) and 
relationships based on sample seeds provided by a 
user.  
Table 3:  Sample Entity Profile 
Name Mohamed Atta 
Aliases Atta; Mohamed  
Position apparent mastermind;  
ring leader; engineer; leader  
Age 33; 29; 33-year-old; 
34-year-old  
Where-from United Arab Emirates; 
Spain; Hamburg; Egyptian; 
?? 
Modifiers on the first plane; evasive; 
ready; in Spain; in seat 8D? 
Descriptors hijacker; al-Amir; purported 
ringleader; a square-jawed 
33-year-old pilot; ?? 
Association bin Laden; Abdulaziz 
Alomari; Hani Hanjour; 
Madrid; American Media 
Inc.; ?? 
Involved-events move-events (2); 
accuse-events (9), 
convict-events (10), 
confess-events (2), 
arrest-events (3), 
 rent-events (3),  ..... 
 
It has been one of Cymfony?s primary objectives 
to facilitate domain portability [Srihari 1998] [Li & 
Srihari 2000a,b, 2003].  This has resulted in a 
development/customization environment known as 
the Lexicon Grammar Development Environment 
(LGDE). The LGDE permits users to modify named 
entity glossaries, alias lexicons and general-purpose 
lexicons.  It also supports example-based grammar 
writing; users can find events of interest in sample 
documents, process these through InfoXtract and 
modify the constraints in the automatically generated 
rule templates for event detection. With some basic 
training, users can easily use the LGDE to customize 
InfoXtract for their applications. This facilitates 
customization of the system in user applications 
where access to the input data to InfoXtract is 
restricted. 
  
7 Applications 
The InfoXtract engine has been used in two 
applications, the Information Discovery Portal (IDP) 
and Brand Dashboard (www.branddashboard. 
com). The IDP supports both the traditional top-down 
methods of browsing through large volumes of 
information as well as novel, data-driven browsing. A 
sample user interface is shown in Figure 4.  
Users may select ?watch lists? of entities (people, 
organizations, targets, etc.) that they are interested in 
monitoring. Users may also customize the sources of 
information they are interested in processing. 
Top-down methods include topic-centric browsing 
whereby documents are classified by topics of 
interest. IE-based browsing techniques include 
entity-centric and event-centric browsing. 
Entity-centric browsing permits users to track key 
entities (people, organizations, targets) of interest and 
monitor information pertaining to them. Event-centric 
browsing focuses on significant actions including 
money movement and people movement events.  
Visualization of extracted information is a key 
component of the IDP.  The Information Mesh enables 
a user to visualize an entity, its attributes and its 
relation to other entities and events. Starting from an 
entity (or event), relationship chains can be traversed 
to explore related items. Timelines facilitate 
visualization of information in the temporal axis. 
Information Discovery Portal 
Associations 
Who/what is being 
associated with al-
Qaeda ?
Organizations
    Religious
    Political
    Terrorist
     - al-Jihad (34)
     - HAMAS (16)
     - Hizballah (5)
     - ?more
People
Incidents
  - Attacks (125)
  - Bombing (64)
  - Threats (45)
  - ?more
Locations
Weapons
Governments
Overall 
Coverage 
Events Info. Sources Documents 
Track... Organizations People Targets 
al-Qaeda 
Overall Coverage of  al-Qaeda Over Time 
0 10 
20 30 
40 50 
5/7/2001 5/14/2001 5/21/2001 5/28/2001 6/4/2001 6/11/2001 6/18/2001 6/25/2001 7/2/2001 7/9/2001 7/16/2001 7/23/2001 7/30/2001 
# 
Re
por
ts 
Alerts for Week of August 6, 2001 
(3)  new reports of al-Qaeda terrorist activity 
(1)  new report of  bin Laden sighting 
(4)  new quotes by bin Laden 
(1)  new target identified 
Figure 4:  Information Discovery Portal 
Recent efforts have included a tight integration of 
InfoXtract with visualization tools such as the 
Web-based Timeline Analysis System (WebTAS) 
(http://www.webtas.com). The IDP reflects the ability 
for users to select events of interest and automatically 
export them to WebTAS for visualization. Efforts are 
underway to integrate higher-level event scenario 
analysis tools such as the Terrorist Modus Operandi 
Detection System (TMODS) (www.21technologies 
.com) into the IDP. 
 
Brand Dashboard is a commercial application for 
marketing and public relations organizations to 
measure and assess media perception of consumer 
brands. The InfoXtract engine is used to analyze 
several thousand electronic sources of information 
provided by various content aggregators (Factiva, 
LexisNexis, etc.). The engine is focused on tagging 
and generating brand profiles that also capture salient 
information such as the descriptive phrases used in 
describing brands (e.g. cost-saving, non-habit 
forming) as well as user-configurable specific 
messages that companies are trying to promote and 
track (safe and reliable, industry leader, etc.). The 
output from the engine is fed into a database-driven 
web application which then produces report cards for 
brands containing quantitative metrics pertaining to 
brand perception, as well as qualitative information 
describing characteristics. A sample screenshot from 
Brand Dashboard is presented in Figure 5. It depicts a 
report card for a particular brand, highlighting brand 
strength as well as highlighting metrics that have 
changed the most in the last time period. The ?buzz 
box? on the right hand side illustrates 
companies/brands, people, analysts, and messages 
most frequently associated with the brand in question.  
Figure 5:  Report Card from Brand Dashboard 
8 Summary and Future Work 
This paper has described the motivation behind 
InfoXtract, a domain independent, portable, 
intermediate-level IE engine. It has also discussed the 
architecture of the engine, both from an algorithmic 
perspective and software engineering perspective. 
Current efforts to improve InfoXtract include the 
following: support for more diverse input formats, 
more use of metadata in the extraction tasks, support 
 
  
for structured data, and capabilities for processing 
foreign languages. Finally, support for more intuitive 
domain customization tools, especially the 
semi-automatic learning tools is a major focus.  
Acknowledgments 
The authors wish to thank Carrie Pine of AFRL for 
reviewing and supporting this work. 
References 
[Aho & Ullman 1971] Alfred V. Aho and Jeffrey 
D. Ullman. Translations on a context-free grammar. 
Information and Control, 19(5):439?475, 1971. 
[Aone & Ramos-Santacruz 1998] REES: A 
Large-Scale Relation and Event Extraction System.  
url: http://acl.ldc.upenn.edu/A/A00/A00-1011.pdf 
[Chinchor & Marsh 1998] Chinchor, N. & Marsh, 
E. 1998. MUC-7 Information Extraction Task 
Definition (version 5.1), Proceedings of MUC-7.  
[Cunningham et al2003] Hamish Cunningham et 
al.  Developing Language Processing Components 
with GATE: A User Guide. 
http://gate.ac.uk/sale/tao/index.html#annie 
[Engelfriet et al1999] Joost Engelfriet, Hendrik 
Jan Hoogeboom, and Jan-Pascal Van Best. Trips on 
trees. Acta Cybernetica, 14(1):51?64, 1999. 
[G?cseg & Steinby 1997] Ferenc G?cseg and 
Magnus Steinby. Tree languages. In Grzegorz 
Rozenberg and Arto Salomaa, editors, Handbook of 
Formal Languages: Beyond Words, volume 3, pages 
1?68, Berlin, 1997. Springer 
[Han 1999] Han, J. Data Mining. 1999.  In J. 
Urban and P. Dasgupta (eds.), Encyclopedia of 
Distributed Computing, Kluwer Academic Publishers. 
[Hobbs 1993] J. R. Hobbs, 1993.  FASTUS: A 
System for Extracting Information from Text, 
Proceedings of the DARPA workshop on Human 
Language Technology?, Princeton, NJ, 133-137. 
[Kaplan & Bresnan 1982] Ronald M. Kaplan and 
Joan Bresnan. Lexical-Functional Grammar: A formal 
system for grammatical representation. In Joan 
Bresnan, editor, The Mental Representation of 
Grammatical Relations, pages 173?281. The MIT 
Press, Cambridge, MA, 1982.  
[Krupka & Hausman 1998] G. R Krupka and K. 
Hausman, ?IsoQuest Inc: Description of the NetOwl 
Text Extraction System as used for MUC-7?, MUC-7 
[Li et al2002] Li, H., R. Srihari, C. Niu, and W. Li 
(2002).  Localization Normalization for Information 
Extraction.  COLING 2002, 549?555, Taipei, Taiwan. 
[Li, W & R. Srihari 2000a]. A Domain 
Independent Event Extraction Toolkit, Final 
Technical Report, Air Force Research Laboratory, 
Information Directorate, Rome Research Site, New 
York 
[Li, W & R. Srihari 2000b]. Flexible Information 
Extraction Learning Algorithm, Final Technical 
Report, Air Force Research Laboratory, Information 
Directorate, Rome Research Site, New York  
[Li & Srihari 2003] Li, W. and R. K. Srihari (2003) 
Intermediate-Level Event Extraction for Temporal 
and Spatial Analysis and Visualization, Final 
Technical Report AFRL-IF-RS-TR-2002-245, Air 
Force Research Laboratory, Information Directorate, 
Rome Research Site, New York. 
[Miller et al1998] Miller, Scott; Crystal, Michael; 
Fox, Heidi; Ramshaw, Lance; Schwartz, Richard; 
Stone, Rebecca; Weischedel, Ralph; and Annotation 
Group, the 1998. Algorithms that Learn to Extract 
Information; BBN: Description of the SIFT System as 
Used for MUC-7.  
[M?nnich et al2001] Uwe M?nnich, Frank 
Morawietz, and Stephan Kepser. A regular query for 
context-sensitive relations. In Steven Bird, Peter 
Buneman, and Mark Liberman, editors, IRCS 
Workshop Linguistic Databases 2001, pages 
187?195, 2001 
[Niu et al2003] Niu, C., W. Li, J. Ding, and R.K. 
Srihari (to appear 2003).  Orthographic Case 
Restoration Using Supervised Learning Without 
Manual Annotation.  Proceedings of The 16th 
FLAIRS, St. Augustine, FL 
[Riloff 1996] [Automatically Generating 
Extraction Patterns from Untagged Text. AAAI-96. 
[Roche & Schabes 1997] Emmanuel Roche & 
Yves Schabes, 1997. Finite-State Language 
Processing, The MIT Press, Cambridge, MA. 
[Silberztein 1999] Max Silberztein, (1999). 
INTEX: a Finite State Transducer toolbox, in 
Theoretical Computer Science #231:1, Elsevier 
Science 
[Srihari 1998]. A Domain Independent Event 
Extraction Toolkit, AFRL-IF-RS-TR-1998-152 Final 
Technical Report, Air Force Research Laboratory, 
Information Directorate, Rome Research Site, New 
York  
[Srihari et al2000] Srihari, R, C. Niu and W. Li. 
(2000).  A Hybrid Approach for Named Entity and 
Sub-Type Tagging.  In Proceedings of ANLP 2000, 
247?254, Seattle, WA. 
Question Answering on a Case Insensitive Corpus 
 
Wei Li, Rohini Srihari, Cheng Niu, Xiaoge Li  
 
Cymfony Inc. 
600 Essjay Road 
Williamsville, NY 14221, USA 
{wei, rohini, cniu, xli}@cymfony.com
 
 
 
 
 
 
 
Abstract 
Most question answering (QA) systems 
rely on both keyword index and Named 
Entity (NE) tagging. The corpus from 
which the QA systems attempt to retrieve 
answers is usually mixed case text.  
However, there are numerous corpora that 
consist of case insensitive documents, e.g. 
speech recognition results.  This paper 
presents a successful approach to QA on a 
case insensitive corpus, whereby a 
preprocessing module is designed to 
restore the case-sensitive form. The 
document pool with the restored case then 
feeds the QA system, which remains 
unchanged. The case restoration 
preprocessing is implemented as a Hidden 
Markov Model trained on a large raw 
corpus of case sensitive documents. It is 
demonstrated that this approach leads to 
very limited degradation in QA 
benchmarking (2.8%), mainly due to the 
limited degradation in the underlying 
information extraction support. 
1 Introduction 
Natural language Question Answering (QA) is 
recognized as a capability with great potential. The 
NIST-sponsored Text Retrieval Conference 
(TREC) has been the driving force for developing 
this technology through its QA track since TREC-8 
[Voorhees 1999] [Voorhees 2000]. There has been 
significant progress and interest in QA research in 
recent years [Pasca & Harabagiu. 2001] [Voorhees 
2000]. 
In real-life QA applications, a system should be 
robust enough to handle diverse textual media 
degraded to different degrees. One of the 
challenges from degraded text is the treatment of 
case insensitive documents such as speech 
recognition results, broadcast transcripts, and the 
Foreign Broadcast Information Service (FBIS) 
sources. In the intelligence domain, the majority of 
archives consist of documents in all uppercase.   
The orthographic case information for written 
text is an important information source. In 
particular, the basic information extraction (IE) 
support for QA, namely Named Entity (NE) 
tagging, relies heavily on the case information for 
recognizing proper names. Almost all NE systems 
(e.g. [Bikel et al 1997], [Krupka & Hausman 
1998]) utilize case-related features. When this 
information is not available, if the system is not re-
trained or adapted, serious performance 
degradation will occur. In the case of the statistical 
NE tagger, without adaptation the system simply 
does not work. The degradation for proper name 
NE tagging is more than 70% based on our testing. 
The key issue here is how to minimize the 
performance degradation by adopting some 
strategy for the system adaptation.     
For search engines, the case information is often 
ignored in keyword indexing and retrieval for the 
sake of efficiency and robustness/recall. However, 
QA requires fine-grained text processing beyond 
keyword indexing since, instead of a list of 
documents or URLs, a list of candidate answers at 
phrase level or sentence level is expected to be 
returned in response to a query.  Typically QA is 
supported by Natural Language Processing (NLP) 
and IE [Chinchor & Marsh 1998] [Hovy et al 
2001] [Srihari & Li 2000]. Examples of using NLP 
and IE in Question Answering include shallow 
parsing [Kupiec 1993] [Srihari & Li 2000], deep 
parsing [Li et al 2002] [Litkowski 1999] 
[Voorhees 1999], and IE [Abney et al 2000] 
[Srihari & Li 2000]. Almost all state-of-the-art QA 
systems rely on NE in searching for candidate 
answers. 
For a system based on language models, a 
feature exclusion approach is used to re-train the 
models, excluding features related to the case 
information [Kubala et al 1998] [Miller et al 
2000] [Palmer et al 2000]. In particular, the 
DARPA HUB-4   program evaluates NE systems 
on speech recognizer output in SNOR (Standard 
Normalized Orthographic Representation) that is 
case insensitive and has no punctuations [Chincor 
et al 1998]. Research on case insensitive text has 
so far been restricted to NE and the feature 
exclusion approach [Chieu & Ng 2002] [Kubala et 
al. 1998] [Palmer et al 2000] [Robinson et al 
1999]. When we examine a system beyond the 
shallow processing of NE, the traditional feature 
exclusion approach may not be feasible. A 
sophisticated QA system usually involves several 
components with multiple modules, involving 
NLP/IE processing at various levels. Each 
processing module may involve some sort of case 
information as constraints. It is too costly and 
sometimes impossible to maintain two versions of 
a multi-module QA system for the purpose of 
handling two types of documents, with or without 
case.   
This paper presents a case restoration approach 
to this problem, as applied to QA. The focus is to 
study the feasibility of QA on a case insensitive 
corpus using the presented case restoration 
approach. For this purpose, we use an existing QA 
system as the baseline in experiments; we are not 
concerned with enhancing the QA system itself. A 
preprocessing module is designed to restore the 
case-sensitive form to feed to this QA system. The 
case restoration module is based on a Hidden 
Markov Model (HMM) trained on a large raw 
corpus of case sensitive documents, which are 
drawn from a given domain with no need for 
human annotation. With the plug-in of this 
preprocessing module, the entire QA system with 
its underlying NLP/IE components needs no 
change or adaptation in handling the case 
insensitive corpus. Using the TREC corpus with 
the case information artificially removed, this 
approach has been benchmarked with very good 
results, leading to only 2.8% degradation in QA 
performance. In the literature, this is the first time 
a QA system is applied to case insensitive corpora.  
Although the artificially-made case insensitive 
corpus is an easier case than some real life corpora 
from speech recognition, the insight and 
techniques gained in this research are helpful in 
further exploring solutions of spoken language 
QA. In addition, by using the TREC corpus and the 
TREC benchmarking standards, the QA 
degradation benchmarking is easy to interpret and 
to compare with other QA systems in the 
community.    
The case restoration approach has the following 
advantages: (i) the training corpus is almost 
limitless, resulting in a high performance model, 
with no knowledge bottleneck as faced by many 
supervised learning scenarios, (ii) the case 
restoration approach is applicable no matter 
whether the core system is statistical model, a 
hand-crafted rule system or a hybrid, (iii) when the 
core system consists of multiple modules, as is the 
case for the QA system used in the experiments 
that is based on multi-level NLP/IE, the case 
restoration approach relieves the burden of having 
to re-train or adapt each module in respect of case 
insensitive input, and (iv) the  restoration approach 
reduces the system complexity: the burden of 
handling degraded text (case in this case) is 
reduced to a preprocessing module while all other 
components need no changes. 
The remaining text is structured as follows. 
Section 2 presents the QA system. Section 3 
describes the language model for case restoration. 
Section 4  benchmarks the IE engine and Section 5 
benchmarks the IE-supported QA application. In 
both benchmarking sections, we compare the 
performance degradation from case sensitive input 
to case insensitive input. Section 5 is the 
Conclusion. 
2 Question Answering Based on IE 
We use a QA system supported by increasingly 
sophisticated levels of IE [Srihari & Li 2000] [Li et 
al. 2002]. Figure 1 presents the underlying IE 
engine InfoXtract [Srihari et al 2003] that forms 
the basis for the QA system. The major 
information objects extracted by InfoXtract include 
NEs,1 Correlated Entity (CE) relationships (e.g. 
Affiliation, Position etc.), Subject-Verb-Object 
(SVO) triples, entity profiles, and general or 
predefined events. These information objects 
capture the key content of the processed text, 
preparing a foundation for answering factoid 
questions.  
Document Processor
Knowledge Resources
Lexicon
Resources
Grammars
Process
Manager
Tokenlist
Legend
Output
Manager
Source
Document
Linguistic Processor(s)Tokenizer
Tokenlist
Lexicon Lookup
Pragmatic
Filtering
 POS Tagging
Named Entity
Detection
Shallow
Parsing
Deep
Parsing
Relationship
Detection
Document
pool
NE
CE
EP
SVO
Time
Normalization
Alias/Coreference
Linking
Profile/Event
Linking
Profile/Event
Merge
Abbreviations
NE = Named Entity
CE = Correlated Entity
EP = Entity Profile
SVO=Subject-Verb-Object
GE = General Event
PE = Predefined Event
Grammar Module
Procedure or
Statistical Model
Hybrid
Module
GEStatisticalModels
Location
Normalizationli ti
PE
IE
Index
 
Figure 1: System Architecture of InfoXtract 
Figure 2 shows the architecture of the QA 
system. This system consists of three components: 
(i) Question Processing, (ii) Text Processing, and 
(iii) Answer Ranking. In text processing, the case 
insensitive corpus is first pre-processed for case 
restoration before being parsed by InfoXtract. In 
addition, keyword indexing on the corpus is 
required. For question processing, a special module 
for Asking Point Identification is called for. 
Linking the two processing components is the 
Answer Ranking component that consists of two 
modules: Snippet Retrieval and Feature Ranking.2 
                                                 
1 It is worth noting that there are two types of NE: (i) proper names 
NeName (including NePerson, NeOrganization, NeLocation, etc.) and 
(ii) non-name NEs (NeItem) such as time NE (NeTimex) and 
numerical NE (NeNumex). Close to 40% of the NE questions target 
non-name NEs. Proper name NEs are more subject to the case effect 
because recognizing a name in the running text often requires case 
information. Non-name NEs generally appear in predictable patterns. 
Pattern matching rules that perform case-insensitive matching are 
most effective in capturing them. 
2 There is a third, optional module Answer Point Identification in our 
QA system [10], which relies on deep parsing for generating phrase-
Answer Ranking relies on access to information 
from both the Keyword Index as well as the IE 
Index.  
 
IE Index
Case 
Insensitive
Corpus
Multi-level 
TemplateQuestion
Keyword 
IndexKeyword indexing
InfoXtract Asking PointIdentification
Feature RankingSnippet Retrieval Snippet-levelAnswer
Question Processing
Text Processing
Answer Ranking
Case Restoration InfoXtract
 
Figure 2:  Architecture of QA Based on NLP/IE 
 
Snippet Retrieval 
 
Snippet retrieval generates the top n (we chose 
200) most relevant sentence-level candidate 
answer snippets based on the question processing 
results. 
We use two types of evidence for snippet 
retrieval:  (i) keyword occurrence statistics at 
snippet level (with stop words removed), and (ii) 
the IE results, including NE Asking Points, Asking 
Point CE Link, head word of a phrase, etc. 
If the Question Processing component detects an 
Asking Point CE Link, the system first attempts to 
retrieve snippets that contain the corresponding CE 
relationship.  If it fails, it backs off to the 
corresponding NE Asking Point. This serves as a 
filter in the sense that only the snippets that contain 
at least one NE that matches the NE Asking Point 
are extracted.  For questions that do not contain NE 
Asking Points, the system backs off to keyword-
based snippet retrieval.   
A synonym lexicon is also constructed for query 
expansion to help snippet retrieval.  This includes 
irregular verbs (go/went/gone, etc.), verb-noun 
conversion (develop/development; satisfy/ 
satisfaction; etc.), and a human-modified 
                                                                             
level answers from snippet-level answers. This module was not used 
in the experiments reported in this paper. 
conservative synonym list (e.g. adjust/adapt; 
adjudicate/judge; etc.). 
Factors that contribute to relevancy weighting in 
snippet retrieval include giving more weight to the 
head words of phrases (e.g. ?disaster? in the noun 
phrase ?the costliest disaster?), more weight to 
words that are linked with question words (e.g. 
?calories? in ?How many calories?? and 
?American? in ?Who was the first American in 
space?), and discounting the weight for synonym 
matching. 
 
Feature Ranking 
 
The purpose of Feature Ranking is to re-rank the 
candidate snippets based on a list of ranking 
features. 
Given a list of top n snippets retrieved in the 
previous stage, the Feature Ranking module uses a 
set of re-ranking features to fine-tune relevancy 
measures of the initial list of snippets in order to 
generate the final top five answer strings that are 
required as output. Figure 3 gives the ranking 
model for the Feature Ranking module.   
 
RankingFeature 1
SnippetList QuestionTokenList
Weight - Wq1
RankingFeature 2
SnippetList QuestionTokenList
Weight - Wq2
RankingFeature mWeight - Wqm
SnippetList QuestionTokenList
Ranked List of Answers  
Figure 3: Pipeline for Ranking Features 
For a given question, Q, let {S1, S2,?,Sn} be the 
set of candidate answer snippets. Let {R1, R2, ?, 
Rk} be the ranking features. For a snippet Sj, let the 
ranking feature Ri assign a relevancy of rij 
quantifying the snippet?s relevance to the question. 
The ranking model is given by  
  

=
=
k
i
ijilj rwSQR
1
),(  
where l represents the question type of Q and wil 
gives the weight assigned to the ranking feature. 
Weights wil vary based on question type. 
We use both traditional IR ranking features such 
as Keyword Proximity and Inverse Document 
Frequency (IDF) as well as the ranking features 
supported by NLP/IE, listed below:  
? NE Asking Point  
? Asking Point CE Link 
? Headword Match for Basic Phrases 
? Phrase-Internal Word Order  
? Alias (e.g. ?IBM? and ?International 
Business Machine?) 
? NE Hierarchical Match (e.g. Company vs. 
Organization)  
? Structure-Based Matching (SVO Links, 
Head-Modifier Link, etc.)  
3 Case Restoration 
This section presents the case restoration approach 
[Niu et al 2003] that supports QA on case 
insensitive corpus. The flowchart for using Case 
Restoration as a plug-in preprocessing module to 
IE is shown in Figure 4. 
 
 
Document Input
Tokenization
Case Detection
InfoXtract 
Case 
Restoration 
Module 
No 
Yes 
Case Sensitive? 
Output
 
Figure 4: Case Restoration for IE 
The incoming documents first go through 
tokenization. In this process, the case information 
is recorded as features for each token. This token-
based case information provides basic evidence for 
the optional procedure called Case Detection to 
decide whether the Case Restoration module needs 
to be called.  
A simple bi-gram Hidden Markov Model [Bikel 
et al 1999] is selected as the choice of language 
model for this task. Currently, the system is based 
on a bi-gram model trained on a normal, case 
sensitive raw corpus in the chosen domain. 
Three orthographic tags are defined in this 
model: (i) initial uppercase followed by at least one 
lowercase, (ii) all lowercase, and (iii) all 
uppercase.  
To handle words with low frequency, each word 
is associated with one of five features: (i) 
PunctuationMark (e.g. &, ?, !?), (ii) LetterDot 
(e.g. A., J.P., U.S.A.,?), (iii) Number (e.g. 
102,?), (iv) Letters (e.g. GOOD, MICROSOFT, 
IBM, ?), or (v) Other.  
The HMM is formulated as follows. Given a 
word sequence nn00 fwfw W =  (where 
jf denotes a single token feature which are defined 
as above), the goal for the case restoration task is 
to find the optimal tag sequence n210 tttt T = , 
which maximizes the conditional probability 
W)| Pr(T  [Bikel et al 1999]. By Bayesian equality, 
this is equivalent to maximizing the joint 
probability T)Pr(W, . This joint probability can be 
computed by a bi-gram HMM as 
?
?
=
i
1i1-i1-iiii )t,f,w|t,f,wPr(T)Pr(W, . The 
back-off model is as follows, 
 
)t,w|)Pr(tt,t|f,wPr()-(1
)t,f,w|t,f,w(P
)t,f,w|t,f,wPr(
1i1ii1iiii1
1i1-i1-iiii01
1i1-i1-iiii
???
?
?
+
=
?
?  
)t|f,wPr()-(1)t,t|f,w(P
)t,t|f,wPr(
iii21iiii02
1iiii
?? +=
?
?  
)w|Pr(t)-(1)t,w|(tP
)t,w|Pr(t
1-ii31i1-ii03
1i1-ii
?? +=
?
?  
)t|(f)Pt|(wPr)-(1)t|f,w(P
)t|f,wPr(
ii0ii4iii04
iii
?? +=
 
)t(P)-(1)w|(tP)w|Pr(t i051-ii051-ii ?? +=  
V
1)-(1)t|(wP)t|Pr(w 6ii06ii ?? +=  
where V denotes the size of the vocabulary, the 
back-off coefficients ??s are determined using the 
Witten-Bell smoothing algorithm, and the 
quantities 
)t,f,w|t,f,w(P 1i1i1iiii0 ??? , )t,t|f,w(P 1iiii0 ? , 
)t,w|(tP 1i1-ii0 ? , )t|f,w(P iii0 , )t|(fP ii0 , 
)w|(tP 1-ii0 , )(tP i0 , and )t|(wP ii0  are computed by 
the maximum likelihood estimation. 
A separate HMM is trained for bigrams 
involving unknown words. The training corpus is 
separated into two parts, the words occurring in 
Part I but not in Part II and the words occurring in 
Part II but not in Part I are all replaced by a special 
symbol #Unknown#. Then an HMM for unknown 
words is trained on this newly marked corpus. In 
the stage of tagging, the unknown word model is 
used in case a word beyond the vocabulary occurs. 
4 IE Engine Benchmarking 
A series of benchmarks have been conducted in 
evaluating the approach presented in this paper. 
They indicate that this is a simple but very 
effective method to solve the problem of handling 
case insensitive input for NLP, IE and QA.  
 
Case Restoration 
 
A raw corpus of 7.6 million words in mixed case 
drawn from the general news domain is used in 
training case restoration. A separate testing corpus 
of 0.88 million words drawn from the same 
domain is used for benchmarking. Table 1 gives 
the case restoration performance benchmarks.  The 
overall F-measure is 98% (P for Precision, R for 
Recall and F for F-measure).  
Table 1: Case Restoration Performance 
P R F
0.96 1 0.98
0.97 0.99 0.98
0.93 0.84 0.88
Initial-Upper Case 0.87 0.84 0.85
All-Upper Case 0.77 0.6 0.67
Overall
Lower Case
Non-Lower Case
 
The score that is most important for IE is the  
F-measure of recognizing non-lowercase word. We 
found that the majority of errors involve missing 
the first word in a sentence due to the lack of a 
powerful sentence final punctuation detection 
module in the case restoration stage. But it is found 
that such ?errors? have almost no negative effect on 
the following IE tasks.3    
There is no doubt that the lack of case 
information from the input text will impact the 
NLP/IE/QA performance. The goal of the case 
restoration module is to minimize this impact. A 
series of degradation tests have been run to 
measure the impact. 
 
Degradation Tests on IE and Parsing 
 
Since IE is the foundation for our QA system, the 
IE degradation due to the case insensitive input 
directly affects the QA performance.  
The IE degradation benchmarking is designed as 
follows. We start with a testing corpus drawn from 
normal case sensitive text. We then feed the corpus 
into the IE engine for benchmarking. This is 
normal benchmarking for case sensitive text input 
as a baseline. After that, we artificially remove the 
case information by transforming the corpus into a 
corpus in all uppercase. The case restoration 
module is then plugged in to restore the case 
before feeding the IE engine. By comparing 
benchmarking using case restoration with baseline 
benchmarking, we can calculate the level of 
performance degradation from the baseline in 
handling case insensitive input. 
For NE, an annotated testing corpus of 177,000 
words is used for benchmarking (Table 3), using 
an automatic scorer following Message 
Understanding Conference (MUC) NE standards. 
Table 2: NE Degradation Benchmarking 
Type P R F 
NE on case sensitive input 89.1% 89.7% 89.4%
NE on case insensitive input using 
case restoration  86.8% 87.9% 87.3%
Degradation  2.3% 1.8% 2.1%
 
The overall F-measure for NE degradation, due 
to the loss of case information in the incoming 
corpus, is 2.1%. We have also implemented the 
traditional NE-retraining approach proposed by 
[Kubala et al 1998] [Miller et al 2000] [Palmer et 
al. 2000] and the re-trained NE model leads to 
                                                 
3 In fact, positive effects are observed in some cases. The normal 
English orthographic rule that the first word be capitalized can 
confuse the NE learning system due to the lack of the usual 
orthographic distinction between a candidate proper name and a 
common word.       
6.3% degradation in the NE F-measure, a drop of 
more than four percentage points when compared 
with the case restoration two-step approach. Since 
this comparison between two approaches is based 
on the same testing corpus using the same system, 
the conclusion can be derived that the case 
restoration approach is clearly better than the 
retraining approach for NE.   
Beyond NE, some fundamental InfoXtract 
support  for QA comes from the CE relationships 
and the SVO parsing results. We benchmarked 
their degradation as follows.  
From a processed corpus drawn from the news 
domain, we randomly picked 250 SVO structural 
links and 60 AFFILIATION and POSITION 
relationships for manual checking (Table 3, COR 
for Correct, INC for Incorrect, SPU for Spurious,  
MIS for Missing, and DEG for Degradation). 
Surprisingly, there is almost no statistically 
significant difference in the SVO performance. 
The degradation due to the case restoration was 
only 0.07%. This indicates that parsing is less 
subject to the case factor to a degree that the 
performance differences between a normal case 
sensitive input and a case restored input are not 
obviously detectable. 
Table 3: SVO/CE Degradation Benchmarking 
 SVO CE 
 Baseline
Case 
Restored Baseline 
Case 
Restored 
COR 196 195 48 43 
INC 13 12 0 1 
SPU 10 10 2 2 
MIS 31 33 
DEG 
 
 
 10 14 
DEG 
 
 
 
P 89.50% 89.86% -0.36% 96.0% 93.5% 2.5% 
R 81.67% 81.25% 0.42% 82.8% 74.1% 8.7% 
F 85.41% 85.34% 0.07% 88.9% 82.7% 6.2% 
 
The degradation for CE is about 6%. 
Considering there is absolutely no adaptation of 
the CE module, this degradation is reasonable. 
5 QA Degradation Benchmarking 
The QA experiments were conducted following the 
TREC-8 QA standards in the category of 250-byte 
answer strings. In addition to the TREC-8 
benchmarking standards Mean Reciprocal Rank 
(MRR), we also benchmarked precision for the top 
answer string (Table 4). 
Table 4: QA Degradation Benchmarking-1 
Type Top 1 Precision MRR 
QA on case sensitive corpus  130/198=65.7% 73.9%
QA on case insensitive corpus 124/198=62.6% 71.1%
Degradation  3.1% 2.8%
 
Comparing QA benchmarks with benchmarks 
for the underlying IE engine shows that the limited 
QA degradation is in proportion with the limited 
degradation in NE, CE and SVO. The following 
examples illustrate the chain effect: case 
restoration errors  NE/CE/SVO errors  QA 
errors. 
 
Q137: ?Who is the mayor of Marbella??  
 
This is a CE question, the decoded CE asking 
relationship is CeHead for the location entity 
?Marbella?. In QA on the original case sensitive 
corpus, the top answer string has a corresponding 
CeHead relationship extracted as shown below.  
Input: Some may want to view the results of the 
much-publicised activities of the mayor of 
Marbella, Jesus Gil y Gil, in cleaning up the 
town 
 [NE tagging] 
  Some may want to view the results of the 
much-publicised activities of the mayor of 
<NeCity>Marbella</NeCity> , 
<NeMan>Jesus Gil y Gil</NeMan>, in 
cleaning up the town 
 [CE extraction] 
CeHead: Marbella  Jesus Gil y Gil 
 
In contrast, the case insensitive processing is 
shown below: 
 
Input: SOME MAY WANT TO VIEW THE 
RESULTS OF THE MUCH-PUBLICISED 
ACTIVITIES OF THE MAYOR OF 
MARBELLA, JESUS GIL Y GIL, IN 
CLEANING UP THE TOWN 
 [case restoration] 
 some may want to view the results of the 
much-publicised activities of the mayor of 
marbella , Jesus Gil y Gil, in cleaning up the 
town 
 [NE tagging] 
 some may want to view the results of the 
much-publicised activities of the mayor of 
marbella , <NeMan>Jesus Gil y 
Gil</NeMan> , in cleaning up the town 
The CE module failed to extract the relationship 
for MARBELLA because this relationship is 
defined for the entity type NeOrganization or 
NeLocation which is absent due to the failed case 
restoration for ?MARBELLA?.  The next example 
shows an NE error leading to a problem in QA.  
 
Q119: ?What Nobel laureate was expelled from 
the Philippines before the conference on East 
Timor??  
 
In question processing, the NE Asking Point is 
identified as NePerson. Because Mairead Maguire  
was successfully tagged as NeWoman, the QA 
system got the correct answer string in the 
following snippet: Immigration officials at the 
Manila airport on Saturday expelled Irish Nobel 
peace prize winner Mairead Maguire. However, 
the case insensitive processing fails to tag any 
NePerson in this snippet. As a result the system 
misses this answer string. The process is illustrated 
below.  
 
Input: IMMIGRATION OFFICIALS AT THE 
MANILA AIRPORT ON SATURDAY 
EXPELLED IRISH NOBEL PEACE PRIZE 
WINNER MAIREAD MAGUIRE 
 [case restoration] 
immigration officials at the Manila airport 
on Saturday expelled Irish Nobel Peace Prize 
Winner Mairead Maguire  
 [NE tagging] 
immigration officials at the 
<NeCity>Manila</NeCity> airport on 
<NeDay>Saturday</NeDay> expelled 
<NeProduct>Irish Nobel Peace Prize Winner 
Mairead Maguire </NeProduct> 
 
As shown, errors in case restoration cause 
mistakes in the NE grouping and tagging: Irish 
Nobel Peace Prize Winner Mairead Maguire  is 
wrongly tagged as NeProduct. 
We also found one interesting case where case 
restoration actually leads to QA performance 
enhancement over the original case sensitive 
processing. A correct answer snippet is promoted 
from the 3rd candidate to the top in answering 
Q191 ?Where was Harry Truman born??. This 
process is shown below. 
Input: HARRY TRUMAN (33RD PRESIDENT): 
BORN MAY 8, 1884, IN LAMAR, MO.  
 [case restoration] 
Harry Truman ( 33rd President ) : born May 
8 , 1884  , in Lamar , MO .  
 [NE tagging] 
 <NeMan>Harry Truman</NeMan> ( 
<NeOrdinal>33rd</NeOrdinal> President ) : 
born <NeDay>May 8 , 1884</NeDay> , in 
<NeCity>Lamar , MO</NeCity> .  
 
As shown, LAMAR, MO gets correctly tagged as 
NeCity after case restoration. But LAMAR is mis-
tagged as NeOrg in the original case sensitive 
processing. The original case sensitive snippet is 
Harry Truman (33rd President): Born May 8, 
1884, in Lamar, Mo.  In our NE system, there is 
such a learned pattern as follows: 
 
X , TwoLetterUpperCase  NeCity.   
 
This rule fails to apply to the original text because 
the US state abbreviation appears in a less 
frequently seen format Mo instead of MO. 
However, the restoration HMM assigns all 
uppercase to ?MO? since this is the most frequently 
seen orthography for this token. This difference of 
the restored case from the original case enables the 
NE tagger to tag Lamar, MO as ?NeCity? which 
meets the NE Asking Point constraint 
?NeLocation?. 
 
QA and Case Insensitive Question 
 
We also conducted a test on case insensitive 
questions in addition to case insensitive corpus by 
calling the same case restoration module.  
Table 5: QA Degradation Benchmarking-2 
Type Top 1 Precision MRR
QA on case sensitive corpus  130/198=65.7% 73.9%
QA on case insensitive corpus,  
with case insensitive question 111/198=56.1% 64.4%
Degradation  9.6% 9.5%
 
This research is useful because, when interfacing 
a speech recognizer to a QA system to accept 
spoken questions, the case information is not 
available in the incoming question.4 We want to 
                                                 
4 In addition to missing the case information, there are other aspects of 
spoken questions that require treatment, e.g., lack of punctuation 
marks, spelling mistakes, repetitions. Whether the restoration 
approach is effective calls for more research.  
know how the same case restoration technique 
applies to question processing and gauge the 
degradation effect on the QA performance  
(Table 5). 
We notice that the question processor missed 
two originally detected NE Asking Points and one 
Asking Point CE Link. There are a number of other 
errors due to incorrectly restored case, including 
non-asking-point NEs in the question and grouping 
errors in shallow parsing as shown below for Q26 : 
?What is the name of the ?female? counterpart to 
El Nino, which results in cooling temperatures and 
very dry weather?? (Notation: NP for Noun Phrase, 
VG for Verb Group, PP for Prepositional Phrase 
and AP for Adjective Phrase).  
 
Input: WHAT IS THE NAME OF THE 
"FEMALE" COUNTERPART TO EL 
NINO ? ?  
 [case restoration] 
What is the name of the "Female" 
counterpart to El Nino, ??  
 [question shallow parsing] 
 NP[What] VG[is] NP[the name] PP[of the] " 
AP[Female] " NP[counterpart] PP[to El 
Nino] , ? ?  
 
In the original mixed-case question, after parsing, 
we get the following basic phrase grouping:  
 
NP[What] VG[is] NP[the name] PP[of the " female 
" counterpart] PP[to El Nino] , ? ?  
 
There is only one difference between the case-
restored question and the original mixed-case 
question, i.e. Female vs. female. This difference 
causes the shallow parsing grouping error for the 
PP of the "female" counterpart. This error affects 
the weights of the ranking features Headword 
Matching and Phrase-internal Word Order. As a 
result, the following originally correctly identified 
answer snippet was dropped: the greenhouse effect 
and El Nino -- as well as its "female" counterpart, 
La Nina -- have had a profound effect on weather 
nationwide. 
As question processing results are the starting 
point and basis for snippet retrieval and feature 
ranking, an error in question processing seems to 
lead to greater degradation, as seen in almost 10% 
drop compared with about 3% drop in the case 
when only the corpus is case insensitive.  
A related explanation for this degradation 
contrast is as follows. Due to the information 
redundancy in a large corpus, processing errors in 
some potential answer strings in the corpus can be 
compensated for by correctly processed equivalent 
answer strings. This is due to the fact that the same 
answer may be expressed in numerous ways in the 
corpus.  Some of those ways may be less subject to 
the case effect than others. Question processing 
errors are fatal in the sense that there is no 
information redundancy for its compensation. 
Once it is wrong, it directs the search for answer 
strings in the wrong direction. Since questions 
constitute a subset of the natural language 
phenomena with their own characteristics, case 
restoration needs to adapt to this subset for optimal 
performance, e.g. by including more questions in 
the case restoration training corpus. 
6 Conclusion 
An effective approach to perform QA on case 
insensitive corpus is presented with very little 
degradation (2.8%). This approach uses a high 
performance case restoration module based on 
HMM as a preprocessor for the NLP/IE processing 
of the corpus. There is no need for any changes on 
the QA system and the underlying IE engine which 
were originally designed for handling normal, case 
sensitive corpora. It is observed that the limited 
QA degradation is due to the limited IE 
degradation. 
An observation from the research of handling 
case insensitive questions is that question 
processing degradation has more serious 
consequence affecting the QA performance. The 
current case restoration training corpus is drawn 
from the general news articles which rarely contain 
questions. As a future effort, we plan to focus on 
enhancing the case restoration performance by 
including as many mixed-case questions as 
possible into the training corpus for case 
restoration. 
Acknowledgment 
This work was partly supported by a grant from the 
Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-03-C-0044. The authors wish to thank 
Carrie Pine and Sharon Walter of AFRL for 
supporting and reviewing this work. 
References 
Abney, S., Collins, M and Singhal. 2000. A. 
Answer Extraction. Proceedings of ANLP-
2000, Seattle. 
Bikel, D.M. et al 1997. Nymble: a High-
Performance Learning Name-finder.  
Proceedings of the Fifth Conference on ANLP, 
Morgan Kaufmann Publishers,  194-201. 
Bikel, D.M., R. Schwartz, and R.M. Weischedel. 
1999. An Algorithm that Learns What?s in a 
Name.  Machine Learning, Vol. 1,3, 1999, 
211-231. 
Chieu, H.L. and H.T. Ng. 2002. Teaching a 
Weaker Classifier: Named Entity Recognition 
on Upper Case Text. Proceedings of ACL-
2002, Philadelphia. 
Chinchor N. and E. Marsh. 1998. MUC-7 
Information Extraction Task Definition 
(version 5.1), Proceedings of MUC-7. 
Hovy, E.H., U. Hermjakob, and Chin-Yew Lin. 
2001. The Use of External Knowledge of 
Factoid QA. Proceedings of TREC-10, 2001, 
Gaithersburg, MD, U.S.A.. 
Krupka, G.R. and K. Hausman. 1998. IsoQuest 
Inc.: Description of the NetOwl (TM) 
Extractor System as Used for MUC-7, 
Proceedings of MUC-7. 
Kubala, F., R. Schwartz, R. Stone and R. 
Weischedel. 1998. Named Entity Extraction 
from Speech.  Proceedings of DARPA 
Broadcast News Transcription and 
Understanding Workshop. 
Kupiec J. 1993. MURAX: A Robust Linguistic 
Approach For Question Answering Using An 
On-Line Encyclopaedia. Proceedings of SIGIR 
Pittsburgh, PA.  
Li, W, R. Srihari, X. Li, M. Srikanth, X. Zhang and 
C. Niu. 2002. Extracting Exact Answers to 
Questions Based on Structural Links. 
Proceedings of Multilingual Summarization 
and Question Answering (COLING-2002 
Workshop), Taipei, Taiwan. 
Litkowski, K. C. 1999. Question-Answering Using 
Semantic Relation Triples. Proceedings of 
TREC-8, Gaithersburg, MD. 
Miller, D., S. Boisen, R. Schwartz, R. Stone, and 
R. Weischedel. 2000. Named Entity Extraction 
from Noisy Input: Speech and OCR. 
Proceedings of ANLP 2000, Seattle. 
Niu, C., W. Li, J. Ding and R. Srihari. 2003.  
Orthographic Case Restoration Using 
Supervised Learning Without Manual 
Annotation.  Proceedings of the 16th 
International FLAIRS Conference 2003, 
Florida  
Chincor, N., P. Robinson and E. Brown. 1998. 
HUB-4 Named Entity Task Definition Version 
4.8.   (www.nist.gov/speech/tests/bnr/hub4_98/ 
hub4_98.htm) 
Palmer, D., M. Ostendorf and J.D. Burger. 2000. 
Robust Information Extraction from 
Automatically Generated Speech 
Transcriptions.  Speech Communications, Vol. 
32, 2000, 95-109. 
Pasca, M. and S.M. Harabagiu. 2001. High 
Performance Question/Answering. 
Proceedings of SIGIR 2001, 366-374. 
Robinson, P., E. Brown, J. Burger, N. Chinchor, A. 
Douthat, L. Ferro, and L. Hirschman. 1999. 
Overview: Information Extraction from 
Broadcast News.  Proceedings of The DARPA 
Broadcast News Workshop Herndon, Virginia. 
Srihari, R and W. Li. 2000. A Question Answering 
System Supported by Information Extraction.  
Proceedings of ANLP 2000, Seattle.   
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: A Customizable Intermediate 
Level Information Extraction Engine. HLT-
NAACL03 Workshop on The Software 
Engineering and Architecture of Language 
Technology Systems (SEALTS). Edmonton, 
Canada  
Voorhees, E. 1999. The TREC-8 Question 
Answering Track Report. Proceedings of 
TREC-8. Gaithersburg, MD.  
Voorhees, E. 2000. Overview of the TREC-9 
Question Answering Track. Proceedings of 
TREC-9. Gaithersburg, MD.  
Context Clustering for Word Sense Disambiguation Based on  
Modeling Pairwise Context Similarities 
Cheng Niu, Wei Li, Rohini K. Srihari, Huifeng Li, Laurie Crist 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221. USA. 
{cniu, wei, rohini, hli, lcrist}@cymfony.com 
 
Abstract 
Traditionally, word sense disambiguation 
(WSD) involves a different context model for 
each individual word. This paper presents a 
new approach to WSD using weakly 
supervised learning. Statistical models are not 
trained for the contexts of each individual 
word, but for the similarities between context 
pairs at category level. The insight is that the 
correlation regularity between the sense 
distinction and the context distinction can be 
captured at category level, independent of 
individual words. This approach only requires 
a limited amount of existing annotated training 
corpus in order to disambiguate the entire 
vocabulary. A context clustering scheme is 
developed within the Bayesian framework. A 
maximum entropy model is then trained to 
represent the generative probability 
distribution of context similarities based on 
heterogeneous features, including trigger 
words and parsing structures. Statistical 
annealing is applied to derive the final context 
clusters by globally fitting the pairwise 
context similarity distribution. Benchmarking 
shows that this new approach significantly 
outperforms the existing WSD systems in the 
unsupervised category, and rivals supervised 
WSD systems. 
1 Introduction 
Word Sense Disambiguation (WSD) is one of the 
central problems in Natural Language Processing. 
The difficulty of this task lies in the fact that 
context features and the corresponding statistical 
distribution are different for each individual word. 
Traditionally, WSD involves modeling the 
contexts for each word.  [Gale et al 1992] uses the 
Na?ve Bayes method for context modeling which 
requires a manually truthed corpus for each 
ambiguous word. This causes a serious Knowledge 
Bottleneck. The situation is worse when 
considering the domain dependency of word 
senses. To avoid the Knowledge Bottleneck, 
unsupervised or weakly supervised learning 
approaches have been proposed. These include the 
bootstrapping approach [Yarowsky 1995] and the 
context clustering approach [Schutze 1998]. 
Although the above unsupervised or weakly 
supervised learning approaches are less subject to 
the Knowledge Bottleneck, some weakness exists: 
i) for each individual keyword, the sense number 
has to be provided and in the bootstrapping case, 
seeds for each sense are also required; ii) the 
modeling usually assumes some form of evidence 
independency, e.g. the vector space model used in 
[Schutze 1998] and [Niu et al 2003]: this limits the 
performance and its potential enhancement; iii) 
most WSD systems either use selectional 
restriction in parsing relations, and/or  trigger 
words which co-occur within a window size of the 
ambiguous word. We previously at-tempted 
combining both types of evidence but only 
achieved limited improvement due to the lack of a 
proper modeling of information over-lapping [Niu 
et al 2003]. 
This paper presents a new algorithm that 
addresses these problems. A novel context 
clustering scheme based on modeling the 
similarities between pairwise contexts at category 
level is presented in the Bayesian framework. A 
generative maximum entropy model is then trained 
to represent the generative probability distribution 
of pairwise context similarities based on 
heterogeneous features that cover both co-
occurring words and parsing structures. Statistical 
annealing is used to derive the final context 
clusters by globally fitting the pairwise context 
similarities. 
This new algorithm only requires a limited 
amount of existing annotated corpus to train the 
generative maximum entropy model for the entire 
vocabulary. This capability is based on the 
observation that a system does not necessarily 
require training data for word A in order to 
disambiguate A.  The insight is that the correlation 
regularity between the sense distinction and the 
context distinction can be captured at category 
level, independent of individual words. 
In what follows, Section 2 formulates WSD as a 
context clustering task based on the pairwise 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
context similarity model. The context clustering 
algorithm is described in Sections 3 and 4, 
corresponding to the two key aspects of the 
algorithm, i.e. the generative maximum entropy 
modeling and the annealing-based optimization. 
Section 5 describes benchmarks and conclusion. 
2 Task Definition and Algorithm Design 
Given n  mentions of a key word, we first 
introduce the following symbols. iC  refers to the 
i -th context.  iS  refers to the sense of the i -th 
context. jiCS ,  refers to the context similarity 
between the i -th context and the j -th context, 
which is a subset of the predefined context 
similarity features. ?f  refers to the ? -th 
predefined context similarity feature. So jiCS ,  
takes the form of { }?f . 
The WSD task is defined as the hard clustering 
of multiple contexts of the key word. Its final 
solution is represented as { }MK ,  where K refers 
to the number of distinct senses, and M represents 
the many-to-one mapping (from contexts to a 
cluster) such that ( ) K]. [1,j n],[1,i j,iM ??=  
For any given context pair, a set of context 
similarity features are defined. With n mentions of 
the same key word, 2
)1( ?nn  context similarities 
[ ] [ )( )ijniCS ji ,1,,1 , ??  are computed. The WSD task 
is formulated as searching for { }MK ,  which 
maximizes the following conditional probability: 
{ }( ) [ ] [ )( )ijniCSMK ji ,1,,1       }{,Pr , ??  
Based on Bayesian Equity, this is equivalent to 
maximizing the joint probability in Eq. (1), which 
contains a prior probability distribution of WSD, 
{ }( )MK ,Pr .  
 
{ }( ) [ ] [ )( )
{ }( ) { }( )
{ }( ) { }( )MKMKCS
MKMKCS
ijniCSMK
ij
Ni
ji
ji
ji
,Pr,Pr
,Pr,}{Pr
,1,,1       }{,,Pr
1,1
,1
,
,
,
?
?=
=
=
=
??
 (1) 
 
Because there is no prior knowledge available 
about what solution is preferred, it is reasonable to 
take an equal distribution as the prior probability 
distribution. So WSD is equivalent to searching for 
{ }MK ,  which maximizes Expression (2). 
 { }( )?
?=
=
1,1
,1
, ,Pr
ij
Ni
ji MKCS    (2) 
where 
{ }( ) ( ) ( ) ( )( )





?
==
= otherwise ,Pr
jMiM if ,Pr,Pr
,
,
,
jiji
jiji
ji SSCS
SSCSMKCS   
     (3) 
 
To learn the conditional probabilities ( )jiji SSCS =|Pr ,  and ( )jiji SSCS ?|Pr ,  in Eq. (3), a 
maximum entropy model is trained. There are two 
major advantages of this maximum entropy model: 
i) the model is independent of individual words; ii) 
the model takes no information independence 
assumption about the data, and hence is powerful 
enough to utilize heterogeneous features. With the 
learned conditional probabilities in Eq. (3), for a 
given { }MK ,  candidate, we can compute the 
conditional probability of Expression (2).  In the 
final step, optimization is performed to search for 
{ }MK ,  that maximizes the value of Expression 
(2). 
3 Maximum Entropy Modeling 
This section presents the definition of context 
similarity features, and how to estimate the 
generative probabilities of context similarity ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  using 
maximum entropy modeling. 
Using the Senseval-2 training corpus,1 we have 
constructed Corpus I and Corpus II for each Part-
of-speech (POS) tag. Corpus I is constructed using 
context pairs involving the same sense of a word.  
Corpus II is constructed using context pairs that 
refer to different senses of a word. Each corpus 
contains about 18,000 context pairs. The instances 
in the corpora are represented as pairwise context 
similarities, taking the form of { }?f . The two 
conditional probabilities ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  can be represented as 
( )}{Pr maxEntI ?f  and ( )}{Pr maxEntII ?f  which are 
generative probabilities by maximum entropy for 
Corpus I and Corpus II. 
We now present how to compute the context 
similarities. Each context contains the following 
two categories of features: 
i) Trigger words centering around the key word 
within a predefined window size equal to 50 
tokens to both sides of the key word. Trigger 
words are learned using the same technique as 
in [Niu et al 2003]. 
ii) Parsing relationships associated with the key 
word automatically decoded by our parser 
                                                     
1 Note that the words that appear in the Senseval-3 
lexical sample evaluation are removed in the corpus 
construction process. 
InfoXtract [Srihari et al 2003]. The 
relationships being utilized are listed below.  
 
Noun: subject-of, object-of, complement-of, 
has-adjective-modifier, has-noun-
modifier, modifier-of, possess, 
possessed-by, appositive-of 
 
Verb: has-subject, has-object, has-
complement, has-adverb-modifier, 
has-prepositional-modifier 
 
Adjective: modifier-of, has-adverb-modifier 
 
Based on the above context features, the 
following three categories of context similarity 
features are defined: 
(1) Context similarity based on a vector space 
model using co-occurring trigger words: the 
trigger words centering around the key word 
are represented as a vector, and the tf*idf 
scheme is used to weigh each trigger word. 
The cosine of the angle between two resulting 
vectors is used as a context similarity 
measure. 
(2) Context similarity based on Latent 
semantic analysis (LSA) using trigger words: 
LSA [Deerwester et al 1990] is a technique 
used to uncover the underlying semantics 
based on co-occurrence data. Using LSA, 
each word is represented as a vector in the 
semantic space. The trigger words are 
represented as a vector summation. Then the 
cosine of the angle between the two resulting 
vector summations is computed, and used as a 
context similarity measure. 
(3) LSA-based Parsing Structure Similarity: 
each relationship is in the form of )(wR? . 
Using LSA, each word w  is represented as 
semantic vector ( )wV . Then, the similarity 
between )( 1wR? and )( 2wR?  is represented as 
the cosine of angle between ( )1wV  and ( )2wV . 
Two special values are assigned to two 
exceptional cases: i) when  no relationship 
?R  is decoded in both contexts; ii) when the 
relationship ?R is decoded only for one 
context. 
To facilitate the maximum entropy modeling in 
the later stage, the resulting similarity measure is 
discretized into 10 integer values. Now the 
pairwise context similarity is a set of similarity 
features, e.g. 
 
{VSM-Similairty-equal-to-2, LSA-Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2}. 
 
In addition to the three categories of basic 
context similarity features defined above, we also 
define induced context similarity features by 
combining basic context similarity features using 
the logical AND operator. With induced features, 
the context similarity vector in the previous 
example is represented as 
 
{VSM-Similairty-equal-to-2, LSA- Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2,  
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1], [VSM-Similairty-
equal-to-2 and LSA-Subject-Similarity-equal-to-
2],  
???, 
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1 and LSA-Subject-
Similarity-equal-to-2]}. 
 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. To make the computation feasible, we 
regulate 3 as the maximum number of logical AND 
in the induced features. Combining basic features 
and induced features under a smoothing scheme, 
maximum entropy modeling may achieve optimal 
performance. 
Now the maximum entropy modeling can be 
formulated as follows: given a pairwise context 
similarity }{ ?f , the generative probability of 
}{ ?f in Corpus I or Corpus II is given as 
 
( )
{ }
?
?
=
?
?
ff
fwZf
1}{Pr maxEnt         (4) 
 
where Z is the normalization factor, fw  is the 
weight associated with feature f . The Iterative 
Scaling algorithm combined with Monte Carlo 
simulation [Pietra, Pietra, & Lafferty 1995] is used 
to train the weights in this generative model. 
Unlike the commonly used conditional maximum 
entropy modeling which approximates the feature 
configuration space as the training corpus 
[Ratnaparkhi 1998], Monte Carlo techniques are 
required in the generative modeling to simulate the 
possible feature configurations. The exponential 
prior smoothing scheme [Goodman 2003] is 
adopted. The same training procedure is performed 
using Corpus I and Corpus II to estimate 
( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  respectively. 
4 Statistical Annealing 
With the maximum entropy modeling presented 
above, the WSD task is performed as follows: i) 
for a given set of contexts, the pairwise context 
similarity measures are computed; ii) for each 
context similarity }{ if , the two generative 
probabilities ( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  are 
computed; iii) for a given WSD candidate 
solution{ }MK , , the conditional probability (2) can 
be computed. Optimization based on statistical 
annealing (Neal 1993) is used to search for { }MK ,  
which maximizes Expression (2). 
The optimization process consists of two steps. 
First, a local optimal solution{ }0, MK is computed 
by a greedy algorithm. Then by setting { }0, MK as 
the initial state, statistical annealing is applied to 
search for the global optimal solution. To reduce 
the search time, we set the maximum value of K  
to 5. 
5 Benchmarking and Conclusion 
To enter the Senseval-3 evaluation, we 
implemented the following procedure to map the 
context clusters to Senseval-3 standards: i) process 
the Senseval-3 training corpus and testing corpus 
using our parser; ii) for each word to be 
benchmarked, retrieve the related contexts from 
the corpora and cluster them; iii) Based on 10% of 
the sense tags in the Senseval-3 training corpus 
(10% data correspond roughly to an average of 2-3 
instances for each sense), the context cluster is 
mapped onto the most frequent WSD sense 
associated with the cluster members. By design, 
the context clusters correspond to distinct senses, 
therefore, we do not allow multiple context clusters 
to be mapped onto one sense. In case multiple 
clusters correspond to one sense, only the largest 
cluster is retained; iv), each instance in the testing 
corpus is tagged with the same sense as the one to 
which its context cluster corresponds.  
    We are not able to compare our performance 
with other systems in Senseval-3 because at the 
time of writing, the Senseval-3 evaluation results 
are not publicly available. As a note, compared 
with the Senseval-2 English Lexical Sample 
evaluation, the benchmarks of our new algorithm 
(Table 1) are significantly above the performance 
of the WSD systems in the unsupervised category, 
and rival the performance of the supervised WSD 
systems. 
 
Table 1. Senseval-3 Lexical Sample Evaluation  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 49.1 64.8 
Noun (20) 57.9 66.6 
Verb (32) 55.3 66.3 
Average 56.3% 66.4% 
 
6 Acknowledgements 
This work was supported by the Navy SBIR 
program under contract N00178-03-C-1047. 
References  
Gale, W., K. Church, and D. Yarowsky. 1992. A 
Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
In Proceedings of ACL 1995. 
Schutze, H. 1998. Automatic Word Sense 
Disambiguation. Computational Linguistics, 23. 
C. Niu, Zhaohui Zheng, R. Srihari, H. Li, and W. 
Li 2003. Unsupervised Learning for Verb Sense 
Disambiguation Using Both trigger Words and 
Parsing Relations. In Proceeding of PACLING 
2003, Halifax, Canada. 
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Goodman, J. 2003. Exponential Priors for 
Maximum Entropy Models. 
Neal, R.M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence.  
Adwait Ratnaparkhi. (1998). Maximum Entropy 
Models for Natural Language Ambiguity 
Resolution. Ph.D. Dissertation. University of 
Pennsylvania. 
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: A Customizable Intermediate Level 
Information Extraction Engine. In Proceedings 
of HLT/NAACL 2003 Workshop on SEALTS. 
Edmonton, Canada. 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 33?39, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
Abstract 
Traditionally, word sense disambiguation 
(WSD) involves a different context classifi-
cation model for each individual word. This 
paper presents a weakly supervised learning 
approach to WSD based on learning a word 
independent context pair classification 
model. Statistical models are not trained for 
classifying the word contexts, but for classi-
fying a pair of contexts, i.e. determining if a 
pair of contexts of the same ambiguous word 
refers to the same or different senses. Using 
this approach, annotated corpus of a target 
word A can be explored to disambiguate 
senses of a different word B. Hence, only a 
limited amount of existing annotated corpus 
is required in order to disambiguate the entire 
vocabulary. In this research, maximum en-
tropy modeling is used to train the word in-
dependent context pair classification model. 
Then based on the context pair classification 
results, clustering is performed on word men-
tions extracted from a large raw corpus. The 
resulting context clusters are mapped onto 
the external thesaurus WordNet. This ap-
proach shows great flexibility to efficiently 
integrate heterogeneous knowledge sources, 
e.g. trigger words and parsing structures. 
Based on Senseval-3 Lexical Sample stan-
dards, this approach achieves state-of-the-art 
performance in the unsupervised learning 
category, and performs comparably with the 
supervised Na?ve Bayes system. 
1 Introduction 
Word Sense Disambiguation (WSD) is one of the 
central problems in Natural Language Processing. 
The difficulty of this task lies in the fact that con-
text features and the corresponding statistical dis-
tribution are different for each individual word. 
Traditionally, WSD involves training the context 
classification models for each ambiguous word.  
(Gale et al 1992) uses the Na?ve Bayes method for 
context classification which requires a manually 
annotated corpus for each ambiguous word. This 
causes a serious Knowledge Bottleneck. The bot-
tleneck is particularly serious when considering the 
domain dependency of word senses. To overcome 
the Knowledge Bottleneck, unsupervised or weakly 
supervised learning approaches have been pro-
posed. These include the bootstrapping approach 
(Yarowsky 1995) and the context clustering ap-
proach (Sch?tze 1998). 
The above unsupervised or weakly supervised 
learning approaches are less subject to the Knowl-
edge Bottleneck. For example, (Yarowsky 1995) 
only requires sense number and a few seeds for 
each sense of an ambiguous word (hereafter called 
keyword). (Sch?tze 1998) may only need minimal 
annotation to map the resulting context clusters 
onto external thesaurus for benchmarking and ap-
plication-related purposes. Both methods are based 
on trigger words only. 
This paper presents a novel approach based on 
learning word-independent context pair classifica-
tion model. This idea may be traced back to 
(Sch?tze 1998) where context clusters based on 
generic Euclidean distance are regarded as distinct 
word senses. Different from (Sch?tze 1998), we 
observe that generic context clusters may not al-
ways correspond to distinct word senses. There-
fore, we used supervised machine learning to 
model the relationships between the context dis-
tinctness and the sense distinctness. 
Although supervised machine learning is used 
for the context pair classification model, our over-
all system belongs to the weakly supervised cate-
gory because the learned context pair classification 
Word Independent Context Pair Classification Model for Word 
Sense Disambiguation 
 
Cheng Niu, Wei Li, Rohini K. Srihari, and Huifeng Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA. 
{cniu, wei, rohini,hli}@cymfony.com 
33
model is independent of the keyword for disam-
biguation. Our system does not need human-
annotated instances for each target ambiguous 
word. The weak supervision is performed by using 
a limited amount of existing annotated corpus 
which does not need to include the target word set.   
The insight is that the correlation regularity be-
tween the sense distinction and the context distinc-
tion can be captured at Part-of-Speech category 
level, independent of individual words or word 
senses. Since context determines the sense of a 
word, a reasonable hypothesis is that there is some 
mechanism in the human comprehension process 
that will decide when two contexts are similar (or 
dissimilar) enough to trigger our interpretation of a 
word in the contexts as one meaning (or as two 
different meanings). We can model this mecha-
nism by capturing the sense distinction regularity 
at category level.  
In the light of this, a maximum entropy model is 
trained to determine if a pair of contexts of the 
same keyword refers to the same or different word 
senses. The maximum entropy modeling is based 
on heterogeneous context features that involve 
both trigger words and parsing structures. To en-
sure the resulting model?s independency of indi-
vidual words, the keywords used in training are 
different from the keywords used in benchmarking. 
For any target keyword, a collection of contexts is 
retrieved from a large raw document pool. Context 
clustering is performed to derive the optimal con-
text clusters which globally fit the local context 
pair classification results. Here statistical annealing 
is used for its optimal performance. In benchmark-
ing, a mapping procedure is required to correlate 
the context clusters with external ontology senses. 
In what follows, Section 2 formulates the maxi-
mum entropy model for context pair classification. 
The context clustering algorithm, including the 
object function of the clustering and the statistical 
annealing-based optimization, is described in Sec-
tion 3. Section 4 presents and discusses bench-
marks, followed by conclusion in Section 5. 
2 Maximum Entropy Modeling for Con-
text Pair Classification 
Given n  mentions of a keyword, we first introduce 
the following symbols. iC  refers to the i -th con-
text.  iS  refers to the sense of the i -th context. 
jiCS ,  refers to the context similarity between the 
i -th context and the j -th context, which is a subset 
of the predefined context similarity features. ?f  
refers to the ? -th predefined context similarity 
feature. So jiCS ,  takes the form of { }?f . 
In this section, we study the context pair classi-
fication task, i.e. given a pair of contexts iC and 
jC  of the same target word, are they referring to 
the same sense? This task is formulated as compar-
ing the following conditional probabilities: ( )jiji CSSS ,Pr =  and ( )jiji CSSS ,Pr ? . Unlike 
traditional context classification for WSD where 
statistical model is trained for each individual 
word, our context pair classification model is 
trained for each Part-of-speech (POS) category. 
The reason for choosing POS as the appropriate 
category for learning the context similarity is that 
the parsing structures, hence the context represen-
tation, are different for different POS categories. 
The training corpora are constructed using the 
Senseval-2 English Lexical Sample training cor-
pus. To ensure the resulting model?s independency 
of individual words, the target words used for 
benchmarking (which will be the ambiguous words 
used in Senseval-3 English Lexicon Sample task) 
are carefully removed in the corpus construction 
process. For each POS category, positive and nega-
tive instances are constructed as follows.  
Positive instances are constructed using context 
pairs referring to the same sense of a word.  Nega-
tive instances are constructed using context pairs 
that refer to different senses of a word.  
For each POS category, we have constructed 
about 36,000 instances, half positive and half nega-
tive. The instances are represented as pairwise con-
text similarities, taking the form of { }?f . 
Before presenting the context similarity features 
we used, we first introduce the two categories of 
the involved context features: 
 
i) Co-occurring trigger words within a prede-
fined window size equal to 50 words to both 
sides of the keyword. The trigger words are 
learned from a TIPSTER document pool con-
taining ~170 million words of AP and WSJ 
news articles. Following (Sch?tze 1998), ?2 is 
used to measure the cohesion between the 
keyword and a co-occurring word.  In our ex-
34
periment, all the words are first sorted based 
on its ?2 with the keyword, and then the top 
2,000 words are selected as trigger words. 
 
ii) Parsing relationships associated with the 
keyword automatically decoded by a broad-
coverage parser, with F-measure (i.e. the pre-
cision-recall combined score) at about 85% 
(reference temporarily omitted for the sake of 
blind review). The logical dependency rela-
tionships being utilized are listed below. 
 
Noun:  subject-of,  
object-of, 
complement-of,  
has-adjective-modifier,  
has-noun-modifier,  
modifier-of,  
possess, 
 possessed-by,  
appositive-of 
 
Verb:   has-subject,  
has-object, 
 has-complement,  
has-adverb-modifier,  
has-prepositional-phrase-modifier 
 
Adjective: modifier-of,  
has-adverb-modifier 
 
Based on the above context features, the follow-
ing three categories of context similarity features 
are defined: 
 
(1)  VSM-based (Vector Space Model based) 
trigger word similarity: the trigger words 
around the keyword are represented as a vec-
tor, and the word i in context j is weighted as 
follows: 
)(log*),(),( idf
Djitfjiweight =  
where ),( jitf  is the frequency of word i in 
the j-th context; D is the number of docu-
ments in the pool; and )(idf  is the number of 
documents containing the word i. D and 
)(idf are estimated using the document pool 
introduced above. The cosine of the angle be-
tween two resulting vectors is used as the 
context similarity measure. 
 
(2)  LSA-based (Latent Semantic Analysis based) 
trigger word similarity: LSA (Deerwester et 
al. 1990) is a technique used to uncover the 
underlying semantics based on co-occurrence 
data. The first step of LSA is to construct 
word-vs.-document co-occurrence matrix. 
Then singular value decomposition (SVD) is 
performed on this co-occurring matrix. The 
key idea of LSA is to reduce noise or insig-
nificant association patterns by filtering the 
insignificant components uncovered by SVD. 
This is done by keeping only the top k singu-
lar values. By using the resulting word-vs.-
document co-occurrence matrix after the fil-
tering, each word can be represented as a vec-
tor in the semantic space. 
  
In our experiment, we constructed the original 
word-vs.-document co-occurring matrix as 
follows: 100,000 documents from the 
TIPSTER corpus were used to construct the 
co-occurring matrix. We processed these 
documents using our POS tagger, and se-
lected the top n most frequently mentioned 
words from each POS category as base 
words: 
 
top 20,000 common nouns 
top 40,000 proper names 
top 10,000 verbs 
top 10,000 adjectives 
top 2,000 adverbs 
 
In performing SVD, we set k (i.e. the number 
of nonzero singular values) as 200, following 
the practice reported in (Deerwester et al 
1990) and (Landauer & Dumais, 1997). 
 
Using the LSA scheme described above, each 
word is represented as a vector in the seman-
tic space. The co-occurring trigger words are 
represented as a vector summation. Then the 
cosine of the angle between the two resulting 
vector summations is computed, and used as 
the context similarity measure. 
 
(3) LSA-based parsing relationship similarity: 
each relationship is in the form of )(wR? . 
Using LSA, each word w  is represented as a 
35
semantic vector ( )wV . The similarity between 
)( 1wR? and )( 2wR?  is represented as the co-
sine of the angle between ( )1wV  and ( )2wV . 
Two special values are assigned to two excep-
tional cases: (i) when no relationship ?R  is 
decoded in both contexts; (ii) when the rela-
tionship ?R is decoded only for one context. 
 
In matching parsing relationships in a context 
pair, if only exact node match counts, very few 
cases can be covered, hence significantly reducing 
the effect of the parser in this task. To solve this 
problem, LSA is used as a type of synonym expan-
sion in matching.  For example, using LSA, the 
following word similarity values are generated: 
 
similarity(good, good)   1.00 
similarity(good, pretty) 0.79 
similarity(good, great) 0.72 
?? 
 
Given a context pair of a noun keyword, suppose 
the first context involves a relationship has-
adjective-modifier whose value is good, and the 
second context involves the same relationship has-
adjective-modifier with the value pretty, then the 
system assigns 0.79 as the similarity value for this 
relationship pair. 
 
To facilitate the maximum entropy modeling in 
the later stage, all the three categories of the result-
ing similarity values are discretized into 10 inte-
gers. Now the pairwise context similarity is 
represented as a set of similarity features, e.g. 
 
{VSM-Trigger-Words-Similairty-equal-to-2,  
  LSA-Trigger-Words-Similarity-equal-to-1,      
  LSA-Subject-Similarity-equal-to-2}. 
 
In addition to the three categories of basic con-
text similarity features defined above, we also de-
fine induced context similarity features by 
combining basic context similarity features using 
the logical and operator. With induced features, the 
context similarity vector in the previous example is 
represented as 
 
{VSM-Trigger-Word-Similairty-equal-to-2,  
  LSA- Trigger-Word-Similarity-equal-to-1,  
  LSA-Subject-Similarity-equal-to-2,  
  [VSM-Similairty-equal-to-2 and  
   LSA-Trigger-Word-Similarity-equal-to-1],    
  [VSM-Similairty-equal-to-2 and  
   LSA-Subject-Similarity-equal-to-2],  
  ??? 
  [VSM-Trigger-Word-Similairty-equal-to-2   
and LSA-Trigger-Word-Similarity-equal-to-1 
and LSA-Subject-Similarity-equal-to-2] 
} 
 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. Combining basic features and induced fea-
tures under a smoothing scheme, maximum en-
tropy modeling may achieve optimal performance. 
Using the context similarity features defined 
above, the training corpora for the context pair 
classification model is in the following format: 
 
Instance_0 tag=?positive? {VSM-Trigger-Word-
Similairty-equal-to-2, ?} 
Instance_1 tag=?negative? {VSM-Trigger-Word-
Similairty-equal-to-0, ?} 
????? 
where positive tag denotes a context pair associ-
ated with same sense, and negative tag denotes a 
context pair associated with different senses.  
 
The maximum entropy modeling is used to com-
pute the conditional probabilities ( )jiji CSSS ,Pr =  and ( )jiji CSSS ,Pr ? : once the 
context pair jiCS ,  is represented as }{ ?f , the con-
ditional probability is given as 
 
( )
{ }
?
?
=
?
?
ff
ftwZft ,
1}{Pr         (1) 
where { }jiji SSSSt ?=? , , Z is the normaliza-
tion factor, ftw ,  is the weight associated with tag t 
and feature f . Using the training corpora con-
structed above, the weights can be computed based 
on Iterative Scaling algorithm (Pietra etc. 1995) 
The exponential prior smoothing scheme (Good-
man 2003) is adopted in the training.  
36
3 Context Clustering based on Context 
Pair Classification Results 
Given n  mentions { }iC of a keyword, we use the 
following context clustering scheme. The discov-
ered context clusters correspond to distinct word 
senses.  
For any given context pair, the context similarity 
features defined in Section 2 are computed. With n 
mentions of the same keyword, 2
)1( ?nn  context 
similarities [ ] [ )( )ijniCS ji ,1,,1 , ??  are computed. 
Using the context pair classification model, each 
pair is associated with two scores ( )( )jijiji CSSSsc ,0, Prlog ==  and 
( )( )jijiji CSSSsc ,1, Prlog ==  which correspond to 
the probabilities of two situations: the pair refers to 
the same or different word senses. 
Now we introduce the symbol { }MK ,  which re-
fers to the final context cluster configuration, 
where K refers to the number of distinct sense, and 
M represents the many-to-one mapping (from con-
texts to a sense) such that 
( ) K]. [1,j n],[1,i j,iM ??= Based on the pairwise 
scores { } 0, jisc and  { } 1, jisc , WSD is formulated as 
searching for { }MK , which maximizes the follow-
ing global scores: 
 
{ }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs  (2) 
where ( ) ( ) ( )



=
= otherwise
jMiMifjik      ,1
 ,0,  
 
Similar clustering scheme has been used success-
fully for the task of co-reference in (Luo etc. 
2004), (Zelenko, Aone and Tibbetts, 2004a) and 
(Zelenko, Aone and Tibbetts, 2004b). 
In this paper, statistical annealing-based optimi-
zation (Neal 1993) is used to search for { }MK ,  
which maximizes Expression (2). 
The optimization process consists of two steps. 
First, an intermediate solution { }0, MK  is com-
puted by a greedy algorithm. Then by setting 
{ }0, MK as the initial state, statistical annealing is 
applied to search for the global optimal solution. 
The optimization algorithm is as follows. 
1. Set the initial state { }MK , as nK = , and 
[ ]n1,i  ,)( ?= iiM ; 
2. Select a cluster pair for merging that 
maximally increases 
{ }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs  
3. If no cluster pair can be merged to in-
crease { }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs , output 
{ }MK , as the intermediate solution; 
otherwise, update { }MK ,  by the merge 
and go to step 2. 
 
Using the intermediate solution { }0, MK of the 
greedy algorithm as the initial state, the statistical 
annealing is implemented using the following 
pseudo-code:  
       Set { } { }0,, MKMK = ; 
       for( 1.01?*;?? ;?? final0 =<= ) 
{ 
    iterate pre-defined number of times 
    { 
          set { } { }MKMK ,, 1 = ; 
     update { }1, MK  by randomly changing 
cluster number and cluster contents;  
            set { }( ){ }( )MK,c
MK,c 1
s
sx =  
           if(x>=1) 
           { 
             set { } { }1,, MKMK =  
           } 
          else 
          { 
             set { } { }1,, MKMK =  with probability  
              ?x . 
          } 
         if { }( ) { }( )0MK,cMK,c ss >  
         then set { } { }MKMK ,, 0 =  
     } 
  } 
  output { }0, MK  as the optimal state. 
 
37
4 Benchmarking 
Corpus-driven context clusters need to map to a 
word sense standard to facilitate performance 
benchmark. Using Senseval-3 evaluation stan-
dards, we implemented the following procedure to 
map the context clusters:  
 
i) Process TIPSTER corpus and the origi-
nal unlabeled Senseval-3 corpora (in-
cluding the training corpus and the 
testing corpus) by our parser, and save 
all the parsing results into a repository.  
 
ii) For each keyword, all related contexts in 
Senseval-3 corpora and up-to-1,000 re-
lated contexts in TIPSTER corpus are 
retrieved from the repository.  
 
iii) All the retrieved contexts are clustered 
based on the context clustering algo-
rithm presented in Sect. 2 and 3. 
 
iv) For each keyword sense, three annotated 
contexts from Senseval-3 training cor-
pus are used for the sense mapping. The 
context cluster is mapped onto the most 
frequent word sense associated with the 
cluster members. By design, the context 
clusters correspond to distinct senses, 
therefore, we do not allow multiple con-
text clusters to be mapped onto one 
sense. In case multiple clusters corre-
spond to one sense, only the largest 
cluster is retained.  
 
v) Each context in the testing corpus is 
tagged with the sense to which its con-
text cluster corresponds to. 
 
As mentioned above, Sensval-2 English lexical 
sample training corpora is used to train the context 
pair classification model. And Sensval-3 English 
lexical sample testing corpora is used here for 
benchmarking. There are several keyword occur-
ring in both Senseval-2 and Senseval-3 corpora. 
The sense tags associated with these keywords are 
not used in the context pair classification training 
process.  
In order to gauge the performance of this new 
weakly supervised learning algorithm, we have 
also implemented a supervised Na?ve Bayes sys-
tem following (Gale et al 1992).  This system is 
trained based on the Senseval-3 English Lexical 
Sample training corpus.  In addition, for the pur-
pose of quantifying the contribution from the pars-
ing structures in WSD, we have run our new 
system with two configurations: (i) using only 
trigger words; (ii) using both trigger words and 
parsing relationships. All the benchmarking is per-
formed using the Senseval-3 English Lexical Sam-
ple testing corpus and standards.  
The performance benchmarks for the two sys-
tems in three runs are shown in Table 1, Table 2 
and Table 3. When using only trigger words, this 
algorithm has 8 percentage degradation from the 
supervised Na?ve Bayes system (see Table 1 vs. 
Table 2). When adding parsing structures, per-
formance degradation is reduced, with about 5 per-
centage drop (see Table 3 vs. Table 2). Comparing 
Table 1 with Table 3, we observe about 3% en-
hancement due to the contribution from the parsing 
support in WSD. The benchmark of our algorithm 
using both trigger words and parsing relationships 
is one of the best in unsupervised category of the 
Senseval-3 Lexical Sample evaluation. 
 
Table 1. New Algorithm Using Only Trigger Words  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 46.3 60.8 
Noun (20) 54.6 62.8 
Verb (32) 54.1 64.2 
Overall  54.0 63.4 
 
Table 2. Supervised Na?ve Bayes System 
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 44.7 56.6 
Noun (20) 66.3 74.5 
Verb (32) 58.6 70.0 
Overall 61.6 71.5 
 
Table 3. New Algorithm Using Both Trigger Words and 
Parsing  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 49.1 64.8 
Noun (20) 57.9 66.6 
Verb (32) 55.3 66.3 
Overall 56.3 66.4 
38
 
It is noted that Na?ve Bayes algorithm has many 
variation, and its performance has been greatly 
enhanced during recent research. Based on Sen-
seval-3 results, the best Na?ve Bayse system out-
perform our version (which is implemented based 
on Gale et al 1992) by 8%~10%. So the best su-
pervised WSD systems output-perform our weakly 
supervised WSD system by 13%~15% in accuracy. 
5 Conclusion 
We have presented a weakly supervised learning 
approach to WSD. Statistical models are not 
trained for the contexts of each individual word, 
but for context pair classification. This approach 
overcomes the knowledge bottleneck that chal-
lenges supervised WSD systems which need la-
beled data for each individual word. It captures the 
correlation regularity between the sense distinction 
and the context distinction at Part-of-Speech cate-
gory level, independent of individual words and 
senses. Hence, it only requires a limited amount of 
existing annotated corpus in order to disambiguate 
the full target set of ambiguous words, in particu-
lar, the target words that do not appear in the train-
ing corpus.   
The weakly supervised learning scheme can 
combine trigger words and parsing structures in 
supporting WSD. Using Senseval-3 English Lexi-
cal Sample benchmarking, this new approach 
reaches one of the best scores in the unsupervised 
category of English Lexical Sample evaluation. 
This performance is close to the performance for 
the supervised Na?ve Bayes system. 
In the future, we will implement a new scheme 
to map context clusters onto WordNet senses by 
exploring WordNet glosses and sample sentences. 
Based on the new sense mapping scheme, we will 
benchmark our system performance using Senseval 
English all-words corpora.  
 
References  
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Gale, W., K. Church, and D. Yarowsky. 1992. A 
Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26. 
Goodman, J. 2003. Exponential Priors for Maxi-
mum Entropy Models. In Proceedings of HLT-
NAACL 2004. 
Landauer, T. K., & Dumais, S. T. 1997. A solution 
to Plato's problem: The Latent Semantic Analy-
sis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological 
Review, 104, 211-240, 1997. 
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla and 
S. Roukos. A Mention-Synchronous Corefer-
ence Resolution Algorithm Based on the Bell 
Tree. In The Proceedings of ACL 2004. 
Neal, R.M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence. 
Sch?tze, H. 1998. Automatic Word Sense Disam-
biguation. Computational Linguistics, 23. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
In Proceedings of ACL 1995.  
Zelenko, D., C. Aone and J. 2004. Tibbetts. 
Coreference Resolution for Information Extrac-
tion. In Proceedings of ACL 2004 Workshop on 
Reference Resolution and its Application.  
Zelenko, D., C. Aone and J. 2004. Tibbetts. Binary 
Integer Programming for Information Extrac-
tion. In Proceedings of ACE 2004 Evaluation 
Workshop.  
 
39
