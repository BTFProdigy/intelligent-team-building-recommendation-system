Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210?218,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Extending Statistical Machine Translation with
Discriminative and Trigger-Based Lexicon Models
Arne Mauser and Sa
?
sa Hasan and Hermann Ney
Human Language Technology and Pattern Recognition Group
Chair of Computer Science 6, RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this work, we propose two extensions of
standard word lexicons in statistical ma-
chine translation: A discriminative word
lexicon that uses sentence-level source in-
formation to predict the target words and
a trigger-based lexicon model that extends
IBM model 1 with a second trigger, allow-
ing for a more fine-grained lexical choice
of target words. The models capture de-
pendencies that go beyond the scope of
conventional SMT models such as phrase-
and language models. We show that the
models improve translation quality by 1%
in BLEU over a competitive baseline on a
large-scale task.
1 Introduction
Lexical dependencies modeled in standard phrase-
based SMT are rather local. Even though the deci-
sion about the best translation is made on sentence
level, phrase models and word lexicons usually do
not take context beyond the phrase boundaries into
account. This is especially problematic since the
average source phrase length used during decod-
ing is small. When translating Chinese to English,
e.g., it is typically close to only two words.
The target language model is the only model
that uses lexical context across phrase boundaries.
It is a very important feature in the log-linear setup
of today?s phrase-based decoders. However, its
context is typically limited to three to six words
and it is not informed about the source sentence.
In the presented models, we explicitly take advan-
tage of sentence-level dependencies including the
source side and make non-local predictions for the
target words. This is an important aspect when
translating from languages like German and Chi-
nese where long-distance dependencies are com-
mon. In Chinese, for example, tenses are often en-
coded by indicator words and particles whose po-
sition is relatively free in the sentence. In German,
prefixes of verbs can be moved over long distances
towards the end of the sentence.
In this work, we propose two models that can
be categorized as extensions of standard word lex-
icons: A discriminative word lexicon that uses
global, i.e. sentence-level source information to
predict the target words using a statistical classi-
fier and a trigger-based lexicon model that extends
the well-known IBM model 1 (Brown et al, 1993)
with a second trigger, allowing for a more fine-
grained lexical choice of target words. The log-
linear framework of the discriminative word lexi-
con offers a high degree of flexibility in the selec-
tion of features. Other sources of information such
as syntax or morphology can be easily integrated.
The trigger-based lexicon model, or simply
triplet model since it is based on word triplets,
is not trained discriminatively but uses the classi-
cal maximum likelihood approach (MLE) instead.
We train the triplets iteratively on a training cor-
pus using the Expectation-Maximization (EM) al-
gorithm. We will present how both models al-
low for a representation of topic-related sentence-
level information which puts them close to word
sense disambiguation (WSD) approaches. As will
be shown later, the experiments indicate that these
models help to ensure translation of content words
that are often omitted by the baseline system. This
is a common problem in Chinese-English transla-
tion. Furthermore, the models are often capable to
produce a better lexical choice of content words.
210
The structure of the paper is as follows: In Sec-
tion 2, we will address related work and briefly
pin down how our models differentiate from pre-
vious work. Section 3 will describe the discrimi-
native lexical selection model and the triplet model
in more detail, explain the training procedures and
show how the models are integrated into the de-
coder. The experimental setup and results will be
given in Section 4. A more detailed discussion
will be presented in Section 5. In the end, we con-
clude our findings and give an outlook for further
research in Section 6.
2 Related Work
Several word lexicon models have emerged in the
context of multilingual natural language process-
ing. Some of them were used as a machine transla-
tion system or as a part of one such system. There
are three major types of models: Heuristic models
as in (Melamed, 2000), generative models as the
IBM models (Brown et al, 1993) and discrimina-
tive models (Varea et al, 2001; Bangalore et al,
2006).
Similar to this work, the authors of (Varea et
al., 2001) try to incorporate a maximum entropy
lexicon model into an SMT system. They use
the words and word classes from the local con-
text as features and show improvements with n-
best rescoring.
The models in this paper are also related to
word sense disambiguation (WSD). For example,
(Chan et al, 2007) trained a discriminative model
for WSD using local but also across-sentence un-
igram collocations of words in order to refine
phrase pair selection dynamically by incorporat-
ing scores from the WSD classifier. They showed
improvements in translation quality in a hierar-
chical phrase-based translation system. Another
WSD approach incorporating context-dependent
phrasal translation lexicons is given in (Carpuat
and Wu, 2007) and has been evaluated on sev-
eral translation tasks. Our model differs from the
latter in three ways. First, our approach mod-
els word selection of the target sentence based on
global sentence-level features of the source sen-
tence. Second, instead of disambiguating phrase
senses as in (Carpuat and Wu, 2007), we model
word selection independently of the phrases used
in the MT models. Finally, the training is done in a
different way as will be presented in Sections 3.1.1
and 3.2.1.
Recently, full translation models using discrim-
inative training criteria emerged as well. They
are designed to generate a translation for a given
source sentence and not only score or disam-
biguate hypotheses given by a translation system.
In (Ittycheriah and Roukos, 2007), the model can
predict 1-to-many translations with gaps and uses
words, morphologic and syntactic features from
the local context.
The authors of (Venkatapathy and Bangalore,
2007) propose three different models. The first
one is a global lexical selection model which in-
cludes all words of the source sentence as features,
regardless of their position. Using these features,
the system predicts the words that should be in-
cluded in the target sentence. Sentence structure is
then reconstructed using permutations of the gen-
erated bag of target words. We will also use this
type of features in our model.
One of the simplest models in the context of
lexical triggers is the IBM model 1 (Brown et
al., 1993) which captures lexical dependencies be-
tween source and target words. It can be seen
as a lexicon containing correspondents of transla-
tions of source and target words in a very broad
sense since the pairs are trained on the full sen-
tence level. The trigger-based lexicon model used
in this work follows the training procedure intro-
duced in (Hasan et al, 2008) and is integrated di-
rectly in the decoder instead of being applied in
n-best list reranking. The model is very close to
the IBM model 1 and can be seen as an extension
of it by taking another word into the condition-
ing part, i.e. the triggering items. Thus, instead
of p(f |e), it models p(f |e, e
?
). Furthermore, since
the second trigger can come from any part of the
sentence, there is a link to long-range monolin-
gual triggers as presented in (Tillmann and Ney,
1997) where a trigger language model was trained
using the EM algorithm and helped to reduce per-
plexities and word error rates in a speech recog-
nition experiment. In (Rosenfeld, 1996), another
approach was chosen to model monolingual trig-
gers using a maximum-entropy based framework.
Again, this adapted LM could improve speech
recognition performance significantly.
A comparison of a variant of the trigger-based
lexicon model applied in decoding and n-best list
reranking can be found in (Hasan and Ney, 2009).
In order to reduce the number of overall triplets,
the authors use the word alignments for fixing the
211
first trigger to the aligned target word. In general,
this constraint performs slightly worse than the un-
constrained variant used in this work, but allows
for faster training and decoding.
3 Extended Lexicon Models
In this section, we present the extended lexicon
models, how they are trained and integrated into
the phrase-based decoder.
3.1 Discriminative Lexicon Model
Discriminative models have been shown to outper-
form generative models on many natural language
processing tasks. For machine translation, how-
ever, the adaptation of these methods is difficult
due to the large space of possible translations and
the size of the training data that has to be used to
achieve significant improvements.
In this section, we propose a discriminative
word lexicon model that follows (Bangalore et al,
2007) and integrate it into the standard phrase-
based machine translation approach.
The core of our model is a classifier that pre-
dicts target words, given the words of the source
sentence. The structure of source as well as tar-
get sentence is neglected in this model. We do
not make any assumptions about the location of
the words in the sentence. This is useful in many
cases, as words and morphology can depend on in-
formation given at other positions in the sentence.
An example would be the character? in Chinese
that indicates a completed or past action and does
not need to appear close to the verb.
We model the probability of the set of target
words in a sentence e given the set of source words
f . For each word in the target vocabulary, we can
calculate a probability for being or not being in-
cluded in the set. The probability of the whole set
then is the product over the entire target vocabu-
lary V
E
:
P (e|f) =
?
e?e
P (e
+
|f) ?
?
e?V
E
\e
P (e
?
|f) (1)
For notational simplicity, we use the event e
+
when the target word e is included in the target
sentence and e
?
if not. We model the individual
factors p(e|f) of the probability in Eq. 1 as a log-
linear model using the source words from f as bi-
nary features
?(f, f) =
{
1 if f ? f
0 else
(2)
and feature weights ?
f,?
:
P (e
+
|f) =
exp
(
?
f?f
?
f,e
+ ?(f, f)
)
?
e?{e
+
,e
?
}
exp
(
?
f?f
?
f,e
?(f, f)
)
(3)
Subsequently, we will call this model discrimina-
tive word lexicon (DWL).
Modeling the lexicon on sets and not on se-
quences has two reasons. Phrase-based MT along
with n-gram language models is strong at predict-
ing sequences but only uses information from a lo-
cal context. By using global features and predict-
ing words in a non-local fashion, we can augment
the strong local decisions from the phrase-based
systems with sentence-level information.
For practical reasons, translating from a set to
a set simplifies the parallelization of the training
procedure. The classifiers for the target words can
be trained separately as explained in the following
section.
3.1.1 Training
Common classification tasks have a relatively
small number of classes. In our case, the num-
ber of classes is the size of the target vocabulary.
For large translation tasks, this is in the range of a
hundred thousand classes. It is far from what con-
ventional out-of-the-box classifiers can handle.
The discriminative word lexicon model has the
convenient property that we can train a separate
model for each target word making paralleliza-
tion straightforward. Discussions about possible
classifiers and the choice of regularization can
be found in (Bangalore et al, 2007). We used
the freely available MegaM Toolkit
1
for training,
which implements the L-BFGS method (Byrd et
al., 1995). Regularization is done using Gaussian
priors. We performed 100 iterations of the train-
ing algorithm for each word in the target vocabu-
lary. This results in a large number of classifiers to
be trained. For the Arabic-English data (cf. Sec-
tion 4), the training took an average of 38 seconds
per word. No feature cutoff was used.
3.1.2 Decoding
In search, we compute the model probabilities as
an additional model in the log-linear model com-
bination of the phrase-based translation approach.
To reduce the memory footprint and startup time
of the decoding process, we reduced the number of
1
http://www.cs.utah.edu/
?
hal/megam/
212
parameters by keeping only large values ?
f,e
since
smaller values tend to have less effect on the over-
all probability. In experiments we determined that
we could safely reduce the size of the final model
by a factor of ten without losing predictive power.
In search, we compute the model probabilities as
an additional model in the log-linear combination.
When scoring hypotheses from the phrase-based
system, we see the translation hypothesis as the
set of target words that are predicted. Words from
the target vocabulary which are not included in
the hypothesis are not part of the set. During the
search process, however, we also have to score in-
complete hypotheses where we do not know which
words will not be included. This problem is cir-
cumvented by rewriting Eq. 1 as
P (e|f) =
?
e?V
E
P (e
?
|f) ?
?
e?e
P (e
+
|f)
P (e
?
|f)
.
The first product is constant given a source sen-
tence and therefore does not affect the search. Us-
ing the model assumption from Eq. 3, we can fur-
ther simplify the computation and compute the
model score entirely in log-space which is numer-
ically stable even for large vocabularies. Exper-
iments showed that using only the first factor of
Eq. 1 is sufficient to obtain good results.
In comparison with the translation model from
(Bangalore et al, 2007) where a threshold on the
probability is used to determine which words are
included in the target sentence, our approach relies
on the phrase model to generate translation candi-
dates. This has several advantages: The length of
the translation is determined by the phrase model.
Words occurring multiple times in the translation
do not have to be explicitly modeled. In (Banga-
lore et al, 2007), repeated target words are treated
as distinct classes.
The main advantage of the integration being
done in a way as presented here is that the phrase
model and the discriminative word lexicon model
are complementary in the way they model the
translation. While the phrase model is good in
predicting translations in a local context, the dis-
criminative word lexicon model is able to predict
global aspects of the sentence like tense or vocabu-
lary changes in questions. While the phrase model
is closely tied to the structure of word and phrase
alignments, the discriminative word lexicon model
completely disregards the structure in source and
target sentences.
3.2 Trigger-based Lexicon Model
The triplets of the trigger-based lexicon model,
i.e. p(e|f, f
?
), are composed of two words in the
source language triggering one target language
word. We chose this inverse direction since it
can be integrated directly into the decoder and,
thus, does not rely on a two-pass approach us-
ing reranking, as it is the case for (Hasan et al,
2008). The triggers can originate from words of
the whole source sentence, also crossing phrase
boundaries of the conventional bilingual phrase
pairs. The model is symmetric though, mean-
ing that the order of the triggers is not relevant,
i.e. (f, f
?
? e) = (f
?
, f ? e). Nevertheless,
the model is able to capture long-distance effects
such as verb splits or adjustments to lexical choice
of the target word given the topic-triggers of the
source sentence. In training, we determine the
probability of a target sentence e
I
1
given the source
sentence f
J
1
within the model by
p(e
I
1
|f
J
1
) =
I
?
i=1
p(e
i
|f
J
1
)
=
I
?
i=1
2
J(J + 1)
J
?
j=0
J
?
j
?
=j+1
p(e
i
|f
j
, f
j
?
), (4)
where f
0
denotes the empty word and, thus, for
f
j
= ?, allows for modeling the conventional (in-
verse) IBM model 1 lexical probabilities as well.
Since the second trigger f
j
?
always starts right of
the current first trigger, the model is symmetric
and does not need to look at all trigger pairs. Eq. 4
is used in the iterative EM training on all sentence
pairs of the training data which is described in
more detail in the following.
3.2.1 Training
For training the trigger-based lexicon model, we
apply the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). The goal is to max-
imize the log-likelihood F
trip
of this model for
a given bilingual training corpus {(f
J
n
1
, e
I
n
1
)}
N
1
consisting of N sentence pairs:
F
trip
:=
N
?
n=1
log p(e
I
n
1
|f
J
n
1
),
where I
n
and J
n
are the lengths of the n-th tar-
get and source sentence, respectively. An aux-
iliary function Q(?; ??) is defined based on F
trip
213
where ?? is the updated estimate within an itera-
tion which is to be derived from the current esti-
mate ?. Here, ? stands for the entire set of model
parameters, i.e. the set of all {?(e|f, f
?
)} with the
constraint
?
e
?(e|f, f
?
) = 1. The accumulators
?(?) are therefore iteratively trained on the train-
ing data by using the current estimate, i.e. deriv-
ing the expected value (E-step), and maximizing
their likelihood afterwards to reestimate the distri-
bution. Thus, the perplexity of the training data is
reduced in each iteration.
3.2.2 Decoding
In search, we can apply this model directly when
scoring bilingual phrase pairs. Given a trained
model for p(e|f, f
?
), we compute the feature score
h
trip
(?) of a phrase pair (e?,
?
f) as
h
trip
(e?,
?
f, f
J
0
) = (5)
?
?
i
log
(
2
J ? (J + 1)
?
j
?
j
?
>j
p(e?
i
|f
j
, f
j
?
)
)
,
where i moves over all target words in the phrase
e?, the second sum selects all source sentence
words f
J
0
including the empty word, and j
?
> j
incorporates the rest of the source sentence right of
the first trigger. We take negative log-probabilities
and normalize to obtain the final score (represent-
ing costs) for the given phrase pair. Note that in
search, we can only use this direction, p(e|f, f
?
),
since the whole source sentence is available for
triggering effects whereas not all target words
have been generated so far, as it would be neces-
sary for the standard direction, p(f |e, e
?
).
Due to the enormous number of triplets, we
trained the model on a subset of the overall train-
ing data. The subcorpus, mainly consisting of
newswire articles, contained 1.4M sentence pairs
with 32.3M running words on the English side.
We trained two versions of the triplet lexicon, one
using 4 EM iterations and another one that was
trained for 10 EM iterations. Due to trimming
of triplets with small probabilities after each it-
eration, the version based on 10 iterations was
slightly smaller, having 164 million triplets but
also performed slightly worse. Thus, for the ex-
periments, we used the version based on 4 itera-
tions which contained 291 million triplets.
Note that decoding with this model can be quite
efficient if caching is applied. Since the given
source sentence does not change, we have to cal-
culate p(e|f, f
?
) for each e only once and can re-
train (C/E) test08 (NW/WT)
Sent. pairs 9.1M 480 490
Run. words 259M/300M 14.8K 12.3K
Vocabulary 357K/627K 3.6K 3.2K
Table 1: GALE Chinese-English corpus statistics
including two test sets: newswire and web text.
train C/E ? A/E nist08 C/A
Sent. pairs 7.3M 4.6M 1357
Words (M) 185/196 142/139 36K/46K
Vocab. (K) 163/265 351/361 6.4K/9.6K
Table 2: NIST Chinese-English and Arabic-
English corpus statistics including the official
2008 test sets.
trieve the probabilities from the cache for consec-
utive scorings of the same target word e. This sig-
nificantly speeds up the decoding process.
4 Experimental Evaluation
In this section we evaluate our lexicon models on
the GALE Chinese-English task for newswire and
web text translation and additionally on the of-
ficial NIST 2008 task for both Chinese-English
and Arabic-English. The baseline system was
built using a state-of-the art phrase-based MT sys-
tem (Zens and Ney, 2008). We use the standard
set of models with phrase translation probabilities
for source-to-target and target-to-source direction,
smoothing with lexical weights, a word and phrase
penalty, distance-based and lexicalized reordering
and a 5-gram (GALE) or 6-gram (NIST) target
language model.
We used training data provided by the Linguis-
tic Data Consortium (LDC) consisting of 9.1M
parallel Chinese-English sentence pairs of vari-
ous domains for GALE (cf. Table 1) and smaller
amounts of data for the NIST systems (cf. Ta-
ble 2). The DWL and Triplet models were inte-
grated into the decoder as presented in Section 3.
For the GALE development and test set, we sep-
arated the newswire and web text parts and did
separate parameter tuning for each genre using
the corresponding development set which consists
of 485 sentences for newswire texts and 533 sen-
tences of web text. The test set has 480 sentences
for newswire and 490 sentences for web text. For
NIST, we tuned on the official 2006 eval set and
used the 2008 evaluation set as a blind test set.
214
GALE NW WT
test08 BLEU TER BLEU TER
[%] [%] [%] [%]
Baseline 32.3 59.38 25.3 64.40
DWL 33.1 58.90 26.2 63.75
Triplet 32.9 58.59 26.2 64.20
DWL+Trip. 33.3 58.23 26.3 63.87
Table 3: Results on the GALE Chinese-English
test set for the newswire and web text setting
(case-insensitive evaluation).
4.1 Translation Results
The translation results on the two GALE test
sets are shown in Table 3 for newswire and web
text. Both the discriminative word lexicon and the
triplet lexicon can individually improve the base-
line by approximately +0.6?0.9% BLEU and -0.5?
0.8% TER. For the combination of both lexicons
on the newswire setting, we observe only a slight
improvement on BLEU but also an additional
boost in TER reduction, arriving at +1% BLEU
and -1.2% TER. For web text, the findings are sim-
ilar: The combination of the discriminative and
trigger-based lexicons yields +1% BLEU and de-
creases TER by -0.5%.
We compared these results against an inverse
IBM model 1 but the results were inconclusive
which is consistent with the results presented in
(Och et al, 2004) where no improvements were
achieved using p(e|f). In our case, inverse IBM1
improves results by 0.2?0.4% BLEU on the devel-
opment set but does not show the same trend on
the test sets. Furthermore, combining IBM1 with
DWL or Triplets often even degraded the transla-
tion results, e.g. only 32.8% BLEU was achieved
on newswire for a combination of the IBM1, DWL
and Triplet model. In contrast, combinations of
the DWL and Triplet model did not degrade per-
formance and could benefit from each other.
In addition to the automatic scoring, we also
did a randomized subjective evaluation where the
hypotheses of the baseline was compared against
the hypotheses generated using the discrimina-
tive word lexicon and triplet models. We evalu-
ated 200 sentences from newswire and web text.
In 80% of the evaluated sentences, the improved
models were judged equal or better than the base-
line.
We tested the presented lexicon models also on
another large-scale system, i.e. NIST, for two lan-
NIST Chinese-Eng. Arabic-Eng.
nist08 BLEU TER BLEU TER
[%] [%] [%] [%]
Baseline 26.8 65.11 42.0 50.55
DWL 27.6 63.56 42.4 50.01
Triplet 27.7 63.60 42.9 49.76
DWL+Trip. 27.9 63.56 43.0 49.15
Table 4: Results on the test sets for the NIST 2008
Chinese-English and Arabic-English task (case-
insensitive evaluation).
guage pairs, namely Chinese-English and Arabic-
English. Interestingly, the results obtained for
Arabic-English are similar to the findings for
Chinese-English, as can be seen in Table 4. The
overall improvements for this language pair are
+1% BLEU and -1.4% TER. In contrast to the
GALE Chinese-English task, the triplet lexicon
model for the Arabic-English language pair per-
forms slightly better than the discriminative word
lexicon.
These results strengthen the claim that the pre-
sented models are capable of improving lexical
choice of the MT system. In the next section, we
discuss the observed effects and analyze our re-
sults in more detail.
5 Discussion
In terms of automatic evaluation measures, the re-
sults indicate that it is helpful to incorporate the
extended lexicon models into the search process.
In this section, we will analyze some more details
of the models and take a look at the lexical choice
they make and what differentiates them from the
baseline models. In Table 5, we picked an ex-
ample sentence from the GALE newswire test set
and show the different hypotheses produced by our
system. As can be seen, the baseline does not
produce the present participle of the verb restore
which makes the sentence somewhat hard to un-
derstand. Both the discriminative and the trigger-
based lexicon approach are capable of generating
this missing information, i.e. the correct use of
restoring. Figure 1 gives an example how discon-
tinuous triggers affect the word choice on the tar-
get side. Two cases are depicted where high proba-
bilities of triplets including emergency and restor-
ing on the target side influence the overall hypoth-
esis selection. The non-local modeling advantages
of the triplet model can be observed as well: The
215
?? , ?? ?? ? ?? ?? ?? ?? ?? .source
target [...] the emergency rescue group is [...] restoring  the ventilation system.
p(restoring | ??,  ? ) = 0.1572p(emergency | ??, ??) = 0.3445
Figure 1: Triggering effect for the example sentence using the triplet lexicon model. The Chinese source
sentence is shown in its segmented form. Two triplets are highlighted that have high probability and
favor the target words emergency and restoring.
Figure 2: Ranking of words for the example sentence for IBM1, Triplet and DWL model. Ranks are
sorted at IBM1, darker colors indicate higher probabilities within the model.
triggering events do not need to be located next
to each other or within a given phrase pair. They
move across the whole source sentence, thus al-
lowing for capturing of long-range dependencies.
Table 6 shows the top ten content words that are
predicted by the two models, discriminative word
lexicon and triplet lexicon model. IBM model 1
ranks are indicated by subscripts in the column
of the triplet model. Although the triplet model
is similar to IBM1, we observe differences in the
word lists. Comparing this to the visualization of
the probability distribution for the example sen-
tence, cf. Figure 2, we argue that, although the
IBM1 and Triplet distributions look similar, the
triplet model is sharper and favors words such as
the ones in Table 6, resulting in different word
choice in the translation process. In contrast, the
DWL approach gives more distinct probabilities,
selecting content words that are not chosen by the
other models.
Table 7 shows an example from the web text
test set. Here, the baseline hypothesis contains
an incorrect word, anna, which might have been
mistaken for the name ying. Interestingly, the hy-
potheses of the DWL lexicon and the combina-
tion of DWL and Triplet contain the correct con-
tent word remarks. The triplet model makes an er-
ror by selecting music, an artifact that might come
from words that co-occur frequently with the cor-
responding Chinese verb to listen, i.e. ? , in the
data. Although the TER score of the baseline is
better than the one for the alternative models for
this particular example, we still think that the ob-
served effects show how our models help produc-
ing different hypotheses that might lead to subjec-
tively better translations.
An Arabic-English translation example is
shown in Table 8. Here, the term incidents of mur-
der in apartments was chosen over the baseline?s
killings inside the flats. Both translations are un-
derstandable and the difference in the wording is
only based on synonyms. The translation using
the discriminative and trigger-based lexicons bet-
ter matches the reference translation and, thus, re-
flects a better lexical choice of the content words.
6 Conclusion
We have presented two lexicon models that use
global source sentence context and are capable
of predicting context-specific target words. The
models have been directly integrated into the de-
coder and have shown to improve the translation
quality of a state-of-the-art phrase-based machine
translation system. The first model was a dis-
criminative word lexicon that uses sentence-level
features to predict if a word from the target vo-
cabulary should be included in the translation or
not. The second model was a trigger-based lexi-
216
Source ?? , ?? ?? ? ?? ??
?????? .
Baseline at present, the accident and rescue
teams are currently emergency re-
covery ventilation systems.
DWL at present, the emergency rescue
teams are currently restoring the
ventilation system.
Triplet at present, the emergency rescue
group is in the process of restoring
the ventilation system.
DWL
+Triplet
at present, the accident emergency
rescue teams are currently restor-
ing the ventilation system.
Reference right now, the accident emergency
rescue team is making emergency
repair on the ventilation system.
Table 5: Translation example from the GALE
newswire test set, comparing the baseline and the
extended lexicon models given a reference trans-
lation. The Chinese source sentence is presented
in its segmented form.
con that uses triplets to model long-range depen-
dencies in the data. The source word triggers can
move across the whole sentence and capture the
topic of the sentence and incorporate more fine-
grained lexical choice of the target words within
the decoder.
Overall improvements are up to +1% in BLEU
and -1.5% in TER on large-scale systems for
Chinese-English and Arabic-English. Compared
to the inverse IBM model 1 which did not yield
consistent improvements, the presented models
are valuable additional features in a phrase-based
statistical machine translation system. We will test
this setup for other language pairs and expect that
languages like German where long-distance ef-
fects are common can benefit from these extended
lexicon models.
In future work, we plan to extend the discrimi-
native word lexicon model in two directions: ex-
tending context to the document level and feature
engineering. For the trigger-based model, we plan
to investigate more model variants. It might be
interesting to look at cross-lingual trigger mod-
els such as p(f |e, f
?
) or constrained variants like
p(f |e, e
?
) with pos(e
?
) < pos(e), i.e. the second
trigger coming from the left context within a sen-
tence which has already been generated. These
DWL Triplet
emergency 0.894 emergency
1
0.048
currently 0.330 system
2
0.032
current 0.175 rescue
8
0.027
emergencies 0.133 accident
3
0.022
present 0.133 ventilation
7
0.021
accident 0.119 work
33
0.021
recovery 0.053 present
5
0.011
group 0.046 currently
9
0.010
dealing 0.042 rush
60
0.010
ventilation 0.034 restoration
31
0.009
Table 6: The top 10 content words predicted by
each model for the GALE newswire example sen-
tence. Original ranks for the related IBM model 1
are given as subscripts for the triplet model.
Source ?? ???? , ?????
? .
Baseline i have listened to anna, happy and
laugh.
DWL i have listened to the remarks,
happy and laugh.
Triplet i have listened to the music, a roar
of laughter.
DWL
+Triplet
i have listened to the remarks,
happy and laugh.
Reference hearing ying?s remark, i laughed
aloud happily.
Table 7: Translation example from the GALE web
text test set. In this case, the baseline has a bet-
ter TER but we can observe a corrected content
word (remark) for the extended lexicon models.
The Chinese source sentence is shown in its seg-
mented form.
extensions could be integrated directly in search
as well and would enable the system to combine
both directions (standard and inverse) to some ex-
tent which was previously shown to help when ap-
plying the standard direction p(f |e, e
?
) as an addi-
tional reranking step, cf. (Hasan and Ney, 2009).
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023, and was partly realized as part of
the Quaero Programme, funded by OSEO, French
State agency for innovation.
The authors would like to thank Christian Buck
217
Source
	
?j
.
??@ ?

I
	
?Q?

K ?



?? @

HBAm
?
'@
	
?? @XY?

HQ?

?
	
 Y

?

?K


X????@
	
?j??@
	
??K
.

I
	
KA? ?
. A?
Q



	
? ?

?

?

??@ ?
	
g@X ?

J

?? @

HX@?k
	
??K
.
??
	
Y? ? P
Q

.
?
	
??X
Baseline some saudi newspapers have published a number of cases that had been subjected to
imprisonment without justification, as well as some killings inside the flats and others.
DWL
+Triplet
some of the saudi newspapers have published a number of cases which were subjected
to imprisonment without justification, as well as some incidents of murder in apartments
and others.
Reference some saudi newspapers have published a number of cases in which people were unjusti-
fiably imprisoned, as well as some incidents of murder in apartments and elsewhere.
Table 8: Translation example from the NIST Arabic-English test set. The DWL and Triplet models
improve lexical word choice by favoring incidents of murder in apartments instead of killings inside the
flats. The Arabic source is shown in its segmented form.
and Juri Ganitkevitch for their help training the ex-
tended lexicon models.
References
S. Bangalore, P. Haffner, and S. Kanthak. 2006. Se-
quence classification for machine translation. In
Ninth International Conf. on Spoken Language Pro-
cessing, Interspeech 2006 ? ICSLP, pages 1722?
1725, Pitsburgh, PA, September.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statis-
tical machine translation through global lexical se-
lection and sentence reconstruction. In 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 152?159, Prague, Czech Republic,
June.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?312, June.
R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. 1995. A
limited memory algorithm for bound constrained op-
timization. SIAM Journal on Scientific Computing,
16(5):1190?1208.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL 2007),
Prague, Czech Republic, June.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In 45th Annual Meeting of the Association
of Computational Linguistics, pages 33?40, Prague,
Czech Republic, June.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1?22.
S. Hasan and H. Ney. 2009. Comparison of extended
lexicon models in search and rescoring for SMT. In
NAACL HLT 2009, Companion Volume: Short Pa-
pers, pages 17?20, Boulder, Colorado, June.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr?es-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In EMNLP, pages 372?381, Honolulu,
Hawaii, October.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In HLT-NAACL 2007: Main Conference,
pages 57?64, Rochester, New York, April.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. pages 161?168, Boston, MA, May.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10(3):187?228.
C. Tillmann and H. Ney. 1997. Word triggers
and the EM algorithm. In Proc. Special Interest
Group Workshop on Computational Natural Lan-
guage Learning (ACL), pages 117?124, Madrid,
Spain, July.
I. Garc??a Varea, F. J. Och, H. Ney, and F. Casacu-
berta. 2001. Refined lexicon models for statistical
machine translation using a maximum entropy ap-
proach. In ACL ?01: 39th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 204?
211, Morristown, NJ, USA.
S. Venkatapathy and S. Bangalore. 2007. Three
models for discriminative machine translation us-
ing global lexical selection and sentence reconstruc-
tion. In SSST, NAACL-HLT 2007 / AMTA Workshop
on Syntax and Structure in Statistical Translation,
pages 96?102, Rochester, New York, April.
R. Zens and H. Ney. 2008. Improvements in dynamic
programming beam search for phrase-based statis-
tical machine translation. In International Work-
shop on Spoken Language Translation, Honolulu,
Hawaii, October.
218
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 755?762, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Translating with non-contiguous phrases
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman,
Eric Gaussier, Cyril Goutte, Kenji Yamada
Xerox Research Centre Europe
FirstName.FamilyName@xrce.xerox.com
Philippe Langlais
RALI/DIRO Universite? de Montre?al
felipe@iro.umontreal.ca
Arne Mauser
RWTH Aachen University
arne.mauser@rwth-aachen.de
Abstract
This paper presents a phrase-based statis-
tical machine translation method, based
on non-contiguous phrases, i.e. phrases
with gaps. A method for producing such
phrases from a word-aligned corpora is
proposed. A statistical translation model
is also presented that deals such phrases,
as well as a training method based on the
maximization of translation accuracy, as
measured with the NIST evaluation met-
ric. Translations are produced by means of
a beam-search decoder. Experimental re-
sults are presented, that demonstrate how
the proposed method allows to better gen-
eralize from the training data.
1 Introduction
Possibly the most remarkable evolution of recent
years in statistical machine translation is the step
from word-based models to phrase-based models
(Och et al, 1999; Marcu and Wong, 2002; Yamada
and Knight, 2002; Tillmann and Xia, 2003). While
in traditional word-based statistical models (Brown
et al, 1993) the atomic unit that translation operates
on is the word, phrase-based methods acknowledge
the significant role played in language by multi-
word expressions, thus incorporating in a statistical
framework the insight behind Example-Based Ma-
chine Translation (Somers, 1999).
However, Phrase-based models proposed so far
only deal with multi-word units that are sequences
of contiguous words on both the source and the tar-
get side. We propose here a model designed to deal
with multi-word expressions that need not be con-
tiguous in either or both the source and the target
side.
The rest of this paper is organised as follows. Sec-
tion 2 provides motivations, definition and extrac-
tion procedure for non-contiguous phrases. The log-
linear conditional translation model we adopted is
the object of Section 3; the method used to train
its parameters is described in Section 4. Section 5
briefly describes the decoder. The experiments we
conducted to asses the effectiveness of using non-
contiguous phrases are presented in Section 6.
2 Non-contiguous phrases
Why should it be a good thing to use phrases
composed of possibly non-contiguous sequences of
words? In doing so we expect to improve trans-
lation quality by better accounting for additional
linguistic phenomena as well as by extending the
effect of contextual semantic disambiguation and
example-based translation inherent in phrase-based
MT. An example of a phenomenon best described
using non-contiguous units is provided by English
phrasal verbs. Consider the sentence ?Mary switches
her table lamp off?. Word-based statistical mod-
els would be at odds when selecting the appropri-
ate translation of the verb. If French were the target
language, for instance, corpus evidence would come
from both examples in which ?switch? is translated
as ?allumer? (to switch on) and as ?e?teindre? (to
switch off). If many-to-one word alignments are not
allowed from English to French, as it is usually the
755
2 31
Pierre
Pierre
ne mange pas
does not eat
Figure 1: An example of a complex alignment asso-
ciated with different syntax for negation in English
and French.
case, then the best thing a word-based model could
do in this case would be to align ?off? to the empty
word and hope to select the correct translation from
?switch? only, basically a 50-50 bet. While han-
dling inseparable phrasal verbs such as ?to run out?
correctly, previously proposed phrase-based models
would be helpless in this case. A comparable behav-
ior is displayed by German separable verbs. More-
over, non-contiguous linguistic units are not limited
to verbs. Negation is formed, in French, by inserting
the words ?ne? and ?pas? before and after a verb re-
spectively. So, the sentence ?Pierre ne mange pas?
and its English translation display a complex word-
level alignment (Figure 1) current models cannot ac-
count for.
Flexible idioms, allowing for the insertion of lin-
guistic material, are other phenomena best modeled
with non-contiguous units.
2.1 Definition and library construction
We define a bi-phrase as a pair comprising a source
phrase and a target phrase: b = ?s?, t??. Each of the
source and target phrases is a sequence of words and
gaps (indicated by the symbol ?); each gap acts as
a placeholder for exactly one unspecified word. For
example, w? = w1w2?w3?? w4 is a phrase of length
7, made up of two contiguous words w1 and w2, a
first gap, a third word w3, two consecutive gaps and
a final word w4. To avoid redundancy, phrases may
not begin or end with a gap. If a phrase does not
contain any gaps, we say it is contiguous; otherwise
it is non-contiguous. Likewise, a bi-phrase is said to
be contiguous if both its phrases are contiguous.
The translation of a source sentence s is produced
by combining together bi-phrases so as to cover the
source sentence, and produce a well-formed target-
language sentence (i.e. without gaps). A complete
translation for s can be described as an ordered se-
quence of bi-phrases b1...bK . When piecing together
the final translation, the target-language portion t?1
of the first bi-phrase b1 is first layed down, then each
subsequent t?k is positioned on the first ?free? posi-
tion in the target language sentence, i.e. either the
leftmost gap, or the right end of the sequence. Fig-
ure 2 illustrates this process with an example.
To produce translations, our approach therefore
relies on a collection of bi-phrases, what we call a
bi-phrase library. Such a library is constructed from
a corpus of existing translations, aligned at the word
level.
Two strategies come to mind to produce non-
contiguous bi-phrases for these libraries. The first is
to align the words using a ?standard? word aligne-
ment technique, such as the Refined Method de-
scribed in (Och and Ney, 2003) (the intersection of
two IBM Viterbi alignments, forward and reverse,
enriched with alignments from the union) and then
generate bi-phrases by combining together individ-
ual alignments that co-occur in the same pair of sen-
tences. This is the strategy that is usually adopted in
other phrase-based MT approaches (Zens and Ney,
2003; Och and Ney, 2004). Here, the difference is
that we are not restricted to combinations that pro-
duce strictly contiguous bi-phrases.
The second strategy is to rely on a word-
alignment method that naturally produces many-to-
many alignments between non-contiguous words,
such as the method described in (Goutte et al,
2004). By means of a matrix factorization, this
method produces a parallel partition of the two texts,
seen as sets of word tokens. Each token therefore
belongs to one, and only one, subset within this par-
tition, and corresponding subsets in the source and
target make up what are called cepts. For example,
in Figure 1, these cepts are represented by the circles
numbered 1, 2 and 3; each cept thus connects word
tokens in the source and the target, regardless of po-
sition or contiguity. These cepts naturally constitute
bi-phrases, and can be used directly to produce a bi-
phrase library.
Obviously, the two strategies can be combined,
and it is always possible to produce increasingly
large and complex bi-phrases by combining together
co-occurring bi-phrases, contiguous or not. One
problem with this approach, however, is that the re-
sulting libraries can become very large. With con-
756
danser le tango
to tango
I do not want to tango anymore
I do not want anymore
doI want
Je ne veux plus danser le tango
Je
I
ne plus
veux
wantdo
not anymore
I
source =
bi?phrase 1 =
bi?phrase 2 =
bi?phrase 3 =
bi?phrase 4 =
target =
Figure 2: Combining bi-phrases to produce a translation.
tiguous phrases, the number of bi-phrases that can
be extracted from a single pair of sentences typically
grows quadratically with the size of the sentences;
with non-contiguous phrases, however, this growth
is exponential. As it turns out, the number of avail-
able bi-phrases for the translation of a sentence has
a direct impact on the time required to compute the
translation; we will therefore typically rely on vari-
ous filtering techniques, aimed at keeping only those
bi-phrases that are more likely to be useful. For ex-
ample, we may retain only the most frequently ob-
served bi-phrases, or impose limits on the number of
cepts, the size of gaps, etc.
3 The Model
In statistical machine translation, we are given a
source language input sJ1 = s1...sJ , and seek the
target-language sentence tI1 = t1...tI that is its most
likely translation:
t?I1 = argmaxtI1Pr(t
I
1|s
J
1 ) (1)
Our approach is based on a direct approximation
of the posterior probability Pr(tI1|sJ1 ), using a log-
linear model:
Pr(tI1|s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 )
)
In such a model, the contribution of each feature
function hm is determined by the corresponding
model parameter ?m; ZsJ1 denotes a normalization
constant. This type of model is now quite widely
used for machine translation (Tillmann and Xia,
2003; Zens and Ney, 2003)1.
Additional variables can be introduced in such a
model, so as to account for hidden characteristics,
and the feature functions can be extended accord-
ingly. For example, our model must take into ac-
count the actual set of bi-phrases that was used to
produce this translation:
Pr(tI1, b
K
1 |s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 , b
K
1 )
)
Our model currently relies on seven feature func-
tions, which we describe here.
? The bi-phrase feature function hbp: it rep-
resents the probability of producing tI1 using
some set of bi-phrases, under the assump-
tion that each source phrase produces a target
phrase independently of the others:
hbp(t
I
1, s
J
1 , b
K
1 ) =
K?
k=1
logPr(t?k|s?k) (2)
Individual bi-phrase probabilities Pr(t?k|s?k)
are estimated based on occurrence counts in the
word-aligned training corpus.
? The compositional bi-phrase feature function
hcomp: this is introduced to compensate for
1Recent work from Chiang (Chiang, 2005) addresses simi-
lar concerns to those motivating our work by introducing a Syn-
chronous CFG for bi-phrases. If on one hand SCFGs allow to
better control the order of the material inserted in the gaps, on
the other gap size does not seem to be taken into account, and
phrase dovetailing such as the one involving ?do ?want? and
?not ???anymore? in Fig. 2 is disallowed.
757
hbp?s strong tendency to overestimate the prob-
ability of rare bi-phrases; it is computed as in
equation (2), except that bi-phrase probabilities
are computed based on individual word transla-
tion probabilities, somewhat as in IBM model
1 (Brown et al, 1993):
Pr(t?|s?) =
1
|s?||t?|
?
t?t?
?
s?s?
Pr(t|s)
? The target language feature function htl: this
is based on a N -gram language model of the
target language. As such, it ignores the source
language sentence and the decomposition of
the target into bi-phrases, to focus on the actual
sequence of target-language words produced
by the combination of bi-phrases:
htl(t
I
1, s
J
1 , b
K
1 ) =
I?
i=1
logPr(ti|t
i?1
i?N+1)
? The word-count and bi-phrase count feature
functions hwc and hbc: these control the length
of the translation and the number of bi-phrases
used to produce it:
hwc(tI1, s
J
1 , b
K
1 ) = I hbc(t
I
1, s
J
1 , b
K
1 ) = K
? The reordering feature function
hreord(tI1, s
J
1 , b
K
1 ): it measures the amount of
reordering between bi-phrases of the source
and target sentences.
? the gap count feature function hgc: It takes as
value the total number of gaps (source and tar-
get) within the bi-phrases of bK1 , thus allowing
the model some control over the nature of the
bi-phrases it uses, in terms of the discontigui-
ties they contain.
4 Parameter Estimation
The values of the ? parameters of the log-linear
model can be set so as to optimize a given crite-
rion. For instance, one can maximize the likely-
hood of some set of training sentences. Instead, and
as suggested by Och (2003), we chose to maximize
directly the quality of the translations produced by
the system, as measured with a machine translation
evaluation metric.
Say we have a set of source-language sentences
S. For a given value of ?, we can compute the set of
corresponding target-language translations T . Given
a set of reference (?gold-standard?) translations R
for S and a function E(T,R) which measures the
?error? in T relative to R, then we can formulate the
parameter estimation problem as2:
?? = argmin?E(T,R)
As pointed out by Och, one notable difficulty with
this approach is that, because the computation of T
is based on an argmax operation (see eq. 1), it is not
continuous with regard to ?, and standard gradient-
descent methods cannot be used to solve the opti-
mization. Och proposes two workarounds to this
problem: the first one relies on a direct optimiza-
tion method derived from Powell?s algorithm; the
second introduces a smoothed (continuous) version
of the error function E(T,R) and then relies on a
gradient-based optimization method.
We have opted for this last approach. Och shows
how to implement it when the error function can be
computed as the sum of errors on individual sen-
tences. Unfortunately, this is not the case for such
widely used MT evaluation metrics as BLEU (Pa-
pineni et al, 2002) and NIST (Doddington, 2002).
We show here how it can be done for NIST; a simi-
lar derivation is possible for BLEU.
The NIST evaluation metric computes a weighted
n-gram precision between T and R, multiplied by
a factor B(S, T,R) that penalizes short translations.
It can be formulated as:
B(S, T,R) ?
N?
n=1
?
s?S In(ts, rs)
?
s?S Cn(ts)
(3)
where N is the largest n-gram considered (usually
N = 4), In(ts, rs) is a weighted count of common
n-grams between the target (ts) and reference (rs)
translations of sentence s, and Cn(ts) is the total
number of n-grams in ts.
To derive a version of this formula that is a con-
tinuous function of ?, we will need multiple trans-
lations ts,1, ..., ts,K for each source sentence s. The
general idea is to weight each of these translations
2For the sake of simplicity, we consider a single reference
translation per source sentence, but the argument can easily be
extended to multiple references.
758
by a factor w(?, s, k), proportional to the score
m?(ts,k|s) that ts,k is assigned by the log-linear
model for a given ?:
w(?, s, k) =
[
m?(ts,k|s)
?
k? m?(ts,k? |s)
]?
where ? is the smoothing factor. Thus, in
the smoothed version of the NIST function, the
term In(ts, rs) in equation (3) is replaced by?
k w(?, s, k)In(ts,k, rs), and the term Cn(ts) is
replaced by
?
k w(?, s, k)Cn(ts,k). As for the
brevity penalty factor B(S, T,R), it depends on
the total length of translation T , i.e.
?
s |ts|. In
the smoothed version, this term is replaced by
?
s
?
k w(?, s, k)|ts,k|. Note that, when ? ? ?,
then w(?, s, k) ? 0 for all translations of s, except
the one for which the model gives the highest score,
and so the smooth and normal NIST functions pro-
duce the same value. In practice, we determine some
?good? value for ? by trial and error (5 works fine).
We thus obtain a scoring function for which we
can compute a derivative relative to ?, and which can
be optimized using gradient-based methods. In prac-
tice, we use the OPT++ implementation of a quasi-
Newton optimization (Meza, 1994). As observed by
Och, the smoothed error function is not convex, and
therefore this sort of minimum-error rate training is
quite sensitive to the initialization values for the ?
parameters. Our approach is to use a random set of
initializations for the parameters, perform the opti-
mization for each initialization, and select the model
which gives the overall best performance.
Globally, parameter estimation proceeds along
these steps:
1. Initialize the training set: using random pa-
rameter values ?0, for each source sentence of
some given set of sentences S, we compute
multiple translations. (In practice, we use the
M -best translations produced by our decoder;
see Section 5).
2. Optimize the parameters: using the method de-
scribed above, we find ? that produces the best
smoothed NIST score on the training set.
3. Iterate: we then re-translate the sentences of S
with this new ?, combine the resulting multiple
translations with those already in the training
set, and go back to step 2.
Steps 2 and 3 can be repeated until the smooothed
NIST score does not increase anymore3.
5 Decoder
We implemented a version of the beam-search stack
decoder described in (Koehn, 2003), extended to
cope with non-contiguous phrases. Each transla-
tion is the result of a sequence of decisions, each of
which involves the selection of a bi-phrase and of a
target position. The final result is obtained by com-
bining decisions, as in Figure 2. Hypotheses, cor-
responding to partial translations, are organised in a
sequence of priority stacks, one for each number of
source words covered. Hypotheses are extended by
filling the first available uncovered position in the
target sentence; each extended hypotheses is then
inserted in the stack corresponding to the updated
number of covered source words. Each hypothesis is
assigned a score which is obtained as a combination
of the actual feature function values and of admissi-
ble heuristics, adapted to deal with gaps in phrases,
estimating the future cost for completing a transla-
tion. Each stack undergoes both threshold and his-
togram pruning. Whenever two hypotheses are in-
distinguishable as far as the potential for further ex-
tension is concerned, they are merged and only the
highest-scoring is further extended. Complete trans-
lations are eventually recovered in the ?last? priority
stack, i.e. the one corresponding to the total num-
ber of source words: the best translation is the one
with the highest score, and that does not have any
remaining gaps in the target.
6 Evaluation
We have conducted a number of experiments to eval-
uate the potential of our approach. We were par-
ticularly interested in assessing the impact of non-
contiguous bi-phrases on translation quality, as well
as comparing the different bi-phrase library contruc-
tion strategies evoked in Section 2.1.
3It can be seen that, as the set of possible translations for
S stabilizes, we eventually reach a point where the procedure
converges to a maximum. In practice, however, we can usually
stop much earlier.
759
6.1 Experimental Setting
All our experiments focused exclusively on French
to English translation, and were conducted using the
Aligned Hansards of the 36th Parliament of Canada,
provided by the Natural Language Group of the USC
Information Sciences Institute, and edited by Ulrich
Germann. From this data, we extracted three dis-
tinct subcorpora, which we refer to as the bi-phrase-
building set, the training set and the test set. These
were extracted from the so-called training, test-1
and test-2 portions of the Aligned Hansard, respec-
tively. Because of efficiency issues, we limited our-
selves to source-language sentences of 30 words or
less. More details on the evaluation data is presented
in Table 14.
6.2 Bi-phrase Libraries
From the bi-phrase-building set, we built a number
of libraries. A first family of libraries was based on
a word alignment ?A?, produced using the Refined
method described in (Och and Ney, 2003) (com-
bination of two IBM-Viterbi alignments): we call
these the A libraries. A second family of libraries
was built using alignments ?B? produced with the
method in (Goutte et al, 2004): these are the B li-
braries. The most notable difference between these
two alignments is that B contains ?native? non-
contiguous bi-phrases, while A doesn?t.
Some libraries were built by simply extracting the
cepts from the alignments of the bi-phrase-building
corpus: these are the A1 and B1 libraries, and vari-
ants. Other libraries were obtained by combining
cepts that co-occur within the same pair of sen-
tences, to produce ?composite? bi-phrases. For in-
stance, the A2 libraries contain combinations of 1
or 2 cepts from alignment A; B3 contains combina-
tions of 1, 2 or 3 cepts, etc.
Some libraries were built using a ?gap-size? filter.
For instance library A2-g3 contains those bi-phrases
obtained by combining 1 or 2 cepts from alignment
A, and in which neither the source nor the target
phrase contains more than 3 gaps. In particular, li-
brary B1-g0 does not contain any non-contiguous
bi-phrases.
4Preliminary experiments on different data sets allowed us
to establish that 800 sentences constituted an acceptable size
for estimating model parameters. With such a corpus, the esti-
mation procedure converges after just 2 or 3 iterations.
Finally, all libraries were subjected to the same
two filtering procedures: the first excludes all bi-
phrases that occur only once in the training corpus;
the second, for any given source-language phrase,
retains only the 20 most frequent target-language
equivalents. While the first of these filters typically
eliminates a large number of entries, the second only
affects the most frequent source phrases, as most
phrases have less than 20 translations.
6.3 Experiments
The parameters of the model were optimized inde-
pendantly for each bi-phrase library. In all cases,
we performed only 2 iterations of the training proce-
dure, then measured the performance of the system
on the test set in terms of the NIST and BLEU scores
against one reference translation. As a point of com-
parison, we also trained an IBM-4 translation model
with the GIZA++ toolkit (Och and Ney, 2000), using
the combined bi-phrase building and training sets,
and translated the test set using the ReWrite decoder
(Germann et al, 2001)5.
Table 2 describes the various libraries that were
used for our experiments, and the results obtained
for each.
System/library bi-phrases NIST BLEU
ReWrite 6.6838 0.3324
A1 238 K 6.6695 0.3310
A2-g0 642 K 6.7675 0.3363
A2-g3 4.1 M 6.7068 0.3283
B1-g0 193 K 6.7898 0.3369
B1 267 K 6.9172 0.3407
B2-g0 499 K 6.7290 0.3391
B2-g3 3.3 M 6.9707 0.3552
B1-g1 206 K 6.8979 0.3441
B1-g2 213 K 6.9406 0.3454
B1-g3 218 K 6.9546 0.3518
B1-g4 222 K 6.9527 0.3423
Table 2: Bi-phrase libraries and results
The top part of the table presents the results for
the A libraries. As can be seen, library A1 achieves
approximately the same score as the baseline sys-
tem; this is expected, since this library is essentially
5Both the ReWrite and our own system relied on a trigram
language model trained on the English half of the bi-phrase
building set.
760
Subset sentences source words target words
bi-phrase-building set 931,000 17.2M 15.2M
training set 800 11,667 10,601
test set 500 6726 6041
Table 1: Data sets.
made up of one-to-one alignments computed using
IBM-4 translation models. Adding contiguous bi-
phrases obtained by combining pairs of alignments
does gain us some mileage (+0.1 NIST)6. Again, this
is consistent with results observed with other sys-
tems (Tillmann and Xia, 2003). However, the addi-
tion of non-contiguous bi-phrases (A2-g3) does not
seem to help.
The middle part of Table 2 presents analogous re-
sults for the corresponding B libraries, plus the B1-
g0 library, which contains only those cepts from the
B alignment that are contiguous. Interestingly, in
the experiments reported in (Goutte et al, 2004),
alignment method B did not compare favorably to A
under the widely used Alignment Error Rate (AER)
metric. Yet, the B1-g0 library performs better than
the analogous A1 library on the translation task.
This suggests that AER may not be an appropriate
metric to measure the potential of an alignment for
phrase-based translation.
Adding non-contiguous bi-phrases allows another
small gain. Again, this is interesting, as it sug-
gests that ?native? non-contiguous bi-phrases are in-
deed useful for the translation task, i.e. those non-
contiguous bi-phrases obtained directly as cepts in
the B alignment.
Surprisingly, however, combining cepts from the
B alignment to produce contiguous bi-phrases (B2-
G0) does not turn out to be fruitful. Why this
is so is not obvious and, certainly, more experi-
ments would be required to establish whether this
tendency continues with larger combinations (B3-
g0, B4-g0...). Composite non-contiguous bi-phrases
produced with the B alignments (B2-g3) seem
to bring improvements with regard to ?basic? bi-
phrases (B1), but it is not clear whether these are
significant.
6While the differences in scores in these and other experi-
ments are relatively small, we believe them to be significant, as
they have been confirmed systematically in other experiments
and, in our experience, by visual inspection of the translations.
Visual examination of the B1 library reveals
that many non-contiguous bi-phrases contain long-
spanning phrases (i.e. phrases containing long se-
quences of gaps). To verify whether or not these
were really useful, we tested a series of B1 libraries
with different gap-size filters. It must be noted that,
because of the final histogram filtering we apply on
libraries (retain only the 20 most frequent transla-
tions of any source phrase), library B1-g1 is not
a strict subset of B1-g2. Therefore, filtering on
gap-size usually represents a tradeoff between more
frequent long-spanning bi-phrases and less frequent
short-spanning ones.
The results of these experiments appear in the
lower part of Table 2. While the differences in score
are small, it seems that concentrating on bi-phrases
with 3 gaps or less affords the best compromise.
For small libraries such as those under consideration
here, this sort of filtering may not be very important.
However, for higher-order libraries (B2, B3, etc.) it
becomes crucial, because it allows to control the ex-
ponential growth of the libraries.
7 Conclusions
In this paper, we have proposed a phrase-based sta-
tistical machine translation method based on non-
contiguous phrases. We have also presented a esti-
mation procedure for the parameters of a log-linear
translation model, that maximizes a smooth version
of the NIST scoring function, and therefore lends
itself to standard gradient-based optimization tech-
niques.
From our experiments with these new methods,
we essentially draw two conclusions. The first and
most obvious is that non-contiguous bi-phrases can
indeed be fruitful in phrase-based statistical machine
translation. While we are not yet able to character-
ize which bi-phrases are most helpful, some of those
that we are currently capable of extracting are well
suited to cover some short-distance phenomena.
761
The second conclusion is that alignment quality is
crucial in producing good translations with phrase-
based methods. While this may sound obvious, our
experiments shed some light on two specific aspects
of this question. The first is that the alignment
method that produces the most useful bi-phrases
need not be the one with the best alignment error
rate (AER). The second is that, depending on the
alignments one starts with, constructing increasingly
large bi-phrases does not necessarily lead to better
translations. Some of our best results were obtained
with relatively small libraries (just over 200,000 en-
tries) of short bi-phrases. In other words, it?s not
how many bi-phrases you have, it?s how good they
are. This is the line of research that we intend to
pursue in the near future.
Acknowledgments
The authors are grateful to the anonymous reviewers
for their useful suggestions. 7
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, Michigan.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast Decoding and Optimal Decoding
for Machine Translation. In Proceedings of ACL 2001,
Toulouse, France.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Proc.
ACL?04, pages 503?510.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
7This work was supported in part by the IST Programme
of the European Community, under the PASCAL Network of
Excellence, IST-2002-506778. This publication only reflects
the authors? views.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP 02), Philadel-
phia, PA.
J. C. Meza. 1994. OPT++: An Object-Oriented Class
Library for Nonlinear Optimization. Technical Report
SAND94-8225, Sandia National Laboratories, Albu-
querque, USA, March.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of ACL 2000, pages
440?447, Hongkong, China, October.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VCL 99), College Park,
MD.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL?03: 41st Ann. Meet.
of the Assoc. for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 311?318, Philadel-
phia, USA.
Harold Somers. 1999. Review Article: Example-based
Machine Translation. Machine Translation, 14:113?
157.
Christoph Tillmann and Fei Xia. 2003. A phrase-based
unigram model for statistical machine translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), Philadelphia, PA.
Richard Zens and Hermann Ney. 2003. Improvements
in Phrase-Based Statistical Machine Translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
762
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 475?484,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Training Phrase Translation Models with Leaving-One-Out
Joern Wuebker and Arne Mauser and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
Several attempts have been made to learn
phrase translation probabilities for phrase-
based statistical machine translation that
go beyond pure counting of phrases
in word-aligned training data. Most
approaches report problems with over-
fitting. We describe a novel leaving-
one-out approach to prevent over-fitting
that allows us to train phrase models that
show improved translation performance
on the WMT08 Europarl German-English
task. In contrast to most previous work
where phrase models were trained sepa-
rately from other models used in transla-
tion, we include all components such as
single word lexica and reordering mod-
els in training. Using this consistent
training of phrase models we are able to
achieve improvements of up to 1.4 points
in BLEU. As a side effect, the phrase table
size is reduced by more than 80%.
1 Introduction
A phrase-based SMT system takes a source sen-
tence and produces a translation by segmenting the
sentence into phrases and translating those phrases
separately (Koehn et al, 2003). The phrase trans-
lation table, which contains the bilingual phrase
pairs and the corresponding translation probabil-
ities, is one of the main components of an SMT
system. The most common method for obtain-
ing the phrase table is heuristic extraction from
automatically word-aligned bilingual training data
(Och et al, 1999). In this method, all phrases of
the sentence pair that match constraints given by
the alignment are extracted. This includes over-
lapping phrases. At extraction time it does not
matter, whether the phrases are extracted from a
highly probable phrase alignment or from an un-
likely one.
Phrase model probabilities are typically defined
as relative frequencies of phrases extracted from
word-aligned parallel training data. The joint
counts C(f? , e?) of the source phrase f? and the tar-
get phrase e? in the entire training data are normal-
ized by the marginal counts of source and target
phrase to obtain a conditional probability
pH(f? |e?) =
C(f? , e?)
C(e?)
. (1)
The translation process is implemented as a
weighted log-linear combination of several mod-
els hm(eI1, s
K
1 , f
J
1 ) including the logarithm of the
phrase probability in source-to-target as well as in
target-to-source direction. The phrase model is
combined with a language model, word lexicon
models, word and phrase penalty, and many oth-
ers. (Och and Ney, 2004) The best translation e?I?1
as defined by the models then can be written as
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(2)
In this work, we propose to directly train our
phrase models by applying a forced alignment pro-
cedure where we use the decoder to find a phrase
alignment between source and target sentences of
the training data and then updating phrase transla-
tion probabilities based on this alignment. In con-
trast to heuristic extraction, the proposed method
provides a way of consistently training and using
phrase models in translation. We use a modified
version of a phrase-based decoder to perform the
forced alignment. This way we ensure that all
models used in training are identical to the ones
used at decoding time. An illustration of the basic
475
Figure 1: Illustration of phrase training with
forced alignment.
idea can be seen in Figure 1. In the literature this
method by itself has been shown to be problem-
atic because it suffers from over-fitting (DeNero
et al, 2006), (Liang et al, 2006). Since our ini-
tial phrases are extracted from the same training
data, that we want to align, very long phrases can
be found for segmentation. As these long phrases
tend to occur in only a few training sentences, the
EM algorithm generally overestimates their prob-
ability and neglects shorter phrases, which better
generalize to unseen data and thus are more useful
for translation. In order to counteract these effects,
our training procedure applies leaving-one-out on
the sentence level. Our results show, that this leads
to a better translation quality.
Ideally, we would produce all possible segmen-
tations and alignments during training. However,
this has been shown to be infeasible for real-world
data (DeNero and Klein, 2008). As training uses
a modified version of the translation decoder, it is
straightforward to apply pruning as in regular de-
coding. Additionally, we consider three ways of
approximating the full search space:
1. the single-best Viterbi alignment,
2. the n-best alignments,
3. all alignments remaining in the search space
after pruning.
The performance of the different approaches is
measured and compared on the German-English
Europarl task from the ACL 2008 Workshop on
Statistical Machine Translation (WMT08). Our
results show that the proposed phrase model train-
ing improves translation quality on the test set by
0.9 BLEU points over our baseline. We find that
by interpolation with the heuristically extracted
phrases translation performance can reach up to
1.4 BLEU improvement over the baseline on the
test set.
After reviewing the related work in the fol-
lowing section, we give a detailed description
of phrasal alignment and leaving-one-out in Sec-
tion 3. Section 4 explains the estimation of phrase
models. The empirical evaluation of the different
approaches is done in Section 5.
2 Related Work
It has been pointed out in literature, that training
phrase models poses some difficulties. For a gen-
erative model, (DeNero et al, 2006) gave a de-
tailed analysis of the challenges and arising prob-
lems. They introduce a model similar to the one
we propose in Section 4.2 and train it with the EM
algorithm. Their results show that it can not reach
a performance competitive to extracting a phrase
table from word alignment by heuristics (Och et
al., 1999).
Several reasons are revealed in (DeNero et al,
2006). When given a bilingual sentence pair, we
can usually assume there are a number of equally
correct phrase segmentations and corresponding
alignments. For example, it may be possible to
transform one valid segmentation into another by
splitting some of its phrases into sub-phrases or by
shifting phrase boundaries. This is different from
word-based translation models, where a typical as-
sumption is that each target word corresponds to
only one source word. As a result of this am-
biguity, different segmentations are recruited for
different examples during training. That in turn
leads to over-fitting which shows in overly deter-
minized estimates of the phrase translation prob-
abilities. In addition, (DeNero et al, 2006) found
that the trained phrase table shows a highly peaked
distribution in opposition to the more flat distribu-
tion resulting from heuristic extraction, leaving the
decoder only few translation options at decoding
time.
Our work differs from (DeNero et al, 2006)
in a number of ways, addressing those problems.
476
To limit the effects of over-fitting, we apply the
leaving-one-out and cross-validation methods in
training. In addition, we do not restrict the train-
ing to phrases consistent with the word alignment,
as was done in (DeNero et al, 2006). This allows
us to recover from flawed word alignments.
In (Liang et al, 2006) a discriminative transla-
tion system is described. For training of the pa-
rameters for the discriminative features they pro-
pose a strategy they call bold updating. It is simi-
lar to our forced alignment training procedure de-
scribed in Section 3.
For the hierarchical phrase-based approach,
(Blunsom et al, 2008) present a discriminative
rule model and show the difference between using
only the viterbi alignment in training and using the
full sum over all possible derivations.
Forced alignment can also be utilized to train a
phrase segmentation model, as is shown in (Shen
et al, 2008). They report small but consistent
improvements by incorporating this segmentation
model, which works as an additional prior proba-
bility on the monolingual target phrase.
In (Ferrer and Juan, 2009), phrase models are
trained by a semi-hidden Markov model. They
train a conditional ?inverse? phrase model of the
target phrase given the source phrase. Addition-
ally to the phrases, they model the segmentation
sequence that is used to produce a phrase align-
ment between the source and the target sentence.
They used a phrase length limit of 4 words with
longer phrases not resulting in further improve-
ments. To counteract over-fitting, they interpolate
the phrase model with IBM Model 1 probabilities
that are computed on the phrase level. We also in-
clude these word lexica, as they are standard com-
ponents of the phrase-based system.
It is shown in (Ferrer and Juan, 2009), that
Viterbi training produces almost the same results
as full Baum-Welch training. They report im-
provements over a phrase-based model that uses
an inverse phrase model and a language model.
Experiments are carried out on a custom subset of
the English-Spanish Europarl corpus.
Our approach is similar to the one presented in
(Ferrer and Juan, 2009) in that we compare Viterbi
and a training method based on the Forward-
Backward algorithm. But instead of focusing on
the statistical model and relaxing the translation
task by using monotone translation only, we use a
full and competitive translation system as starting
point with reordering and all models included.
In (Marcu and Wong, 2002), a joint probability
phrase model is presented. The learned phrases
are restricted to the most frequent n-grams up to
length 6 and all unigrams. Monolingual phrases
have to occur at least 5 times to be considered
in training. Smoothing is applied to the learned
models so that probabilities for rare phrases are
non-zero. In training, they use a greedy algorithm
to produce the Viterbi phrase alignment and then
apply a hill-climbing technique that modifies the
Viterbi alignment by merge, move, split, and swap
operations to find an alignment with a better prob-
ability in each iteration. The model shows im-
provements in translation quality over the single-
word-based IBM Model 4 (Brown et al, 1993) on
a subset of the Canadian Hansards corpus.
The joint model by (Marcu and Wong, 2002)
is refined by (Birch et al, 2006) who use
high-confidence word alignments to constrain the
search space in training. They observe that due to
several constraints and pruning steps, the trained
phrase table is much smaller than the heuristically
extracted one, while preserving translation quality.
The work by (DeNero et al, 2008) describes
a method to train the joint model described in
(Marcu and Wong, 2002) with a Gibbs sampler.
They show that by applying a prior distribution
over the phrase translation probabilities they can
prevent over-fitting. The prior is composed of
IBM1 lexical probabilities and a geometric distri-
bution over phrase lengths which penalizes long
phrases. The two approaches differ in that we ap-
ply the leaving-one-out procedure to avoid over-
fitting, as opposed to explicitly defining a prior
distribution.
3 Alignment
The training process is divided into three parts.
First we obtain all models needed for a normal
translations system. We perform minimum error
rate training with the downhill simplex algorithm
(Nelder and Mead, 1965) on the development data
to obtain a set of scaling factors that achieve a
good BLEU score. We then use these models and
scaling factors to do a forced alignment, where
we compute a phrase alignment for the training
data. From this alignment we then estimate new
phrase models, while keeping all other models un-
477
changed. In this section we describe our forced
alignment procedure that is the basic training pro-
cedure for the models proposed here.
3.1 Forced Alignment
The idea of forced alignment is to perform a
phrase segmentation and alignment of each sen-
tence pair of the training data using the full transla-
tion system as in decoding. What we call segmen-
tation and alignment here corresponds to the ?con-
cepts? used by (Marcu and Wong, 2002). We ap-
ply our normal phrase-based decoder on the source
side of the training data and constrain the transla-
tions to the corresponding target sentences from
the training data.
Given a source sentence fJ1 and target sentence
eI1, we search for the best phrase segmentation and
alignment that covers both sentences. A segmen-
tation of a sentence into K phrase is defined by
k ? sk := (ik, bk, jk), for k = 1, . . . ,K
where for each segment ik is last position of kth
target phrase, and (bk, jk) are the start and end
positions of the source phrase aligned to the kth
target phrase. Consequently, we can modify Equa-
tion 2 to define the best segmentation of a sentence
pair as:
s?K?1 = argmax
K,sK1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(3)
The identical models as in search are used: condi-
tional phrase probabilities p(f?k|e?k) and p(e?k|f?k),
within-phrase lexical probabilities, distance-based
reordering model as well as word and phrase
penalty. A language model is not used in this case,
as the system is constrained to the given target sen-
tence and thus the language model score has no
effect on the alignment.
In addition to the phrase matching on the source
sentence, we also discard all phrase translation
candidates, that do not match any sequence in the
given target sentence.
Sentences for which the decoder can not find
an alignment are discarded for the phrase model
training. In our experiments, this is the case for
roughly 5% of the training sentences.
3.2 Leaving-one-out
As was mentioned in Section 2, previous ap-
proaches found over-fitting to be a problem in
phrase model training. In this section, we de-
scribe a leaving-one-out method that can improve
the phrase alignment in situations, where the prob-
ability of rare phrases and alignments might be
overestimated. The training data that consists ofN
parallel sentence pairs fn and en for n = 1, . . . , N
is used for both the initialization of the transla-
tion model p(f? |e?) and the phrase model training.
While this way we can make full use of the avail-
able data and avoid unknown words during train-
ing, it has the drawback that it can lead to over-
fitting. All phrases extracted from a specific sen-
tence pair fn, en can be used for the alignment of
this sentence pair. This includes longer phrases,
which only match in very few sentences in the
data. Therefore those long phrases are trained to
fit only a few sentence pairs, strongly overesti-
mating their translation probabilities and failing to
generalize. In the extreme case, whole sentences
will be learned as phrasal translations. The aver-
age length of the used phrases is an indicator of
this kind of over-fitting, as the number of match-
ing training sentences decreases with increasing
phrase length. We can see an example in Figure
2. Without leaving-one-out the sentence is seg-
mented into a few long phrases, which are unlikely
to occur in data to be translated. Phrase boundaries
seem to be unintuitive and based on some hidden
structures. With leaving-one-out the phrases are
shorter and therefore better suited for generaliza-
tion to unseen data.
Previous attempts have dealt with the over-
fitting problem by limiting the maximum phrase
length (DeNero et al, 2006; Marcu and Wong,
2002) and by smoothing the phrase probabilities
by lexical models on the phrase level (Ferrer and
Juan, 2009). However, (DeNero et al, 2006) expe-
rienced similar over-fitting with short phrases due
to the fact that the same word sequence can be seg-
mented in different ways, leading to specific seg-
mentations being learned for specific training sen-
tence pairs. Our results confirm these findings. To
deal with this problem, instead of simple phrase
length restriction, we propose to apply the leaving-
one-out method, which is also used for language
modeling techniques (Kneser and Ney, 1995).
When using leaving-one-out, we modify the
phrase translation probabilities for each sentence
pair. For a training example fn, en, we have to
remove all phrases Cn(f? , e?) that were extracted
from this sentence pair from the phrase counts that
478
Figure 2: Segmentation example from forced alignment. Top: without leaving-one-out. Bottom: with
leaving-one-out.
we used to construct our phrase translation table.
The same holds for the marginal counts Cn(e?) and
Cn(f?). Starting from Equation 1, the leaving-one-
out phrase probability for training sentence pair n
is
pl1o,n(f? |e?) =
C(f? , e?)? Cn(f? , e?)
C(e?)? Cn(e?)
(4)
To be able to perform the re-computation in an
efficient way, we store the source and target phrase
marginal counts for each phrase in the phrase ta-
ble. A phrase extraction is performed for each
training sentence pair separately using the same
word alignment as for the initialization. It is then
straightforward to compute the phrase counts after
leaving-one-out using the phrase probabilities and
marginal counts stored in the phrase table.
While this works well for more frequent obser-
vations, singleton phrases are assigned a probabil-
ity of zero. We refer to singleton phrases as phrase
pairs that occur only in one sentence. For these
sentences, the decoder needs the singleton phrase
pairs to produce an alignment. Therefore we retain
those phrases by assigning them a positive proba-
bility close to zero. We evaluated with two differ-
ent strategies for this, which we call standard and
length-based leaving-one-out. Standard leaving-
one-out assigns a fixed probability ? to singleton
phrase pairs. This way the decoder will prefer us-
ing more frequent phrases for the alignment, but is
able to resort to singletons if necessary. However,
we found that with this method longer singleton
phrases are preferred over shorter ones, because
fewer of them are needed to produce the target sen-
tence. In order to better generalize to unseen data,
we would like to give the preference to shorter
phrases. This is done by length-based leaving-
one-out, where singleton phrases are assigned the
probability ?(|f? |+|e?|) with the source and target
Table 1: Avg. source phrase lengths in forced
alignment without leaving-one-out and with stan-
dard and length-based leaving-one-out.
avg. phrase length
without l1o 2.5
standard l1o 1.9
length-based l1o 1.6
phrase lengths |f? | and |e?| and fixed ? < 1. In our
experiments we set ? = e?20 and ? = e?5. Ta-
ble 1 shows the decrease in average source phrase
length by application of leaving-one-out.
3.3 Cross-validation
For the first iteration of the phrase training,
leaving-one-out can be implemented efficiently as
described in Section 3.2. For higher iterations,
phrase counts obtained in the previous iterations
would have to be stored on disk separately for each
sentence and accessed during the forced alignment
process. To simplify this procedure, we propose
a cross-validation strategy on larger batches of
data. Instead of recomputing the phrase counts for
each sentence individually, this is done for a whole
batch of sentences at a time. In our experiments,
we set this batch-size to 10000 sentences.
3.4 Parallelization
To cope with the runtime and memory require-
ments of phrase model training that was pointed
out by previous work (Marcu and Wong, 2002;
Birch et al, 2006), we parallelized the forced
alignment by splitting the training corpus into
blocks of 10k sentence pairs. From the initial
phrase table, each of these blocks only loads the
phrases that are required for alignment. The align-
479
ment and the counting of phrases are done sep-
arately for each block and then accumulated to
build the updated phrase model.
4 Phrase Model Training
The produced phrase alignment can be given as a
single best alignment, as the n-best alignments or
as an alignment graph representing all alignments
considered by the decoder. We have developed
two different models for phrase translation proba-
bilities which make use of the force-aligned train-
ing data. Additionally we consider smoothing by
different kinds of interpolation of the generative
model with the state-of-the-art heuristics.
4.1 Viterbi
The simplest of our generative phrase models esti-
mates phrase translation probabilities by their rel-
ative frequencies in the Viterbi alignment of the
data, similar to the heuristic model but with counts
from the phrase-aligned data produced in training
rather than computed on the basis of a word align-
ment. The translation probability of a phrase pair
(f? , e?) is estimated as
pFA(f? |e?) =
CFA(f? , e?)
?
f? ?
CFA(f?
?, e?)
(5)
where CFA(f? , e?) is the count of the phrase pair
(f? , e?) in the phrase-aligned training data. This can
be applied to either the Viterbi phrase alignment
or an n-best list. For the simplest model, each
hypothesis in the n-best list is weighted equally.
We will refer to this model as the count model as
we simply count the number of occurrences of a
phrase pair. We also experimented with weight-
ing the counts with the estimated likelihood of the
corresponding entry in the the n-best list. The sum
of the likelihoods of all entries in an n-best list is
normalized to 1. We will refer to this model as the
weighted count model.
4.2 Forward-backward
Ideally, the training procedure would consider all
possible alignment and segmentation hypotheses.
When alternatives are weighted by their posterior
probability. As discussed earlier, the run-time re-
quirements for computing all possible alignments
is prohibitive for large data tasks. However, we
can approximate the space of all possible hypothe-
ses by the search space that was used for the align-
ment. While this might not cover all phrase trans-
lation probabilities, it allows the search space and
translation times to be feasible and still contains
the most probable alignments. This search space
can be represented as a graph of partial hypothe-
ses (Ueffing et al, 2002) on which we can com-
pute expectations using the Forward-Backward al-
gorithm. We will refer to this alignment as the full
alignment. In contrast to the method described in
Section 4.1, phrases are weighted by their poste-
rior probability in the word graph. As suggested in
work on minimum Bayes-risk decoding for SMT
(Tromble et al, 2008; Ehling et al, 2007), we use
a global factor to scale the posterior probabilities.
4.3 Phrase Table Interpolation
As (DeNero et al, 2006) have reported improve-
ments in translation quality by interpolation of
phrase tables produced by the generative and the
heuristic model, we adopt this method and also re-
port results using log-linear interpolation of the es-
timated model with the original model.
The log-linear interpolations pint(f? |e?) of the
phrase translation probabilities are estimated as
pint(f? |e?) =
(
pH(f? |e?)
)1??
?
(
pgen(f? |e?)
)(?)
(6)
where ? is the interpolation weight, pH the
heuristically estimated phrase model and pgen the
count model. The interpolation weight ? is ad-
justed on the development corpus. When inter-
polating phrase tables containing different sets of
phrase pairs, we retain the intersection of the two.
As a generalization of the fixed interpolation of
the two phrase tables we also experimented with
adding the two trained phrase probabilities as ad-
ditional features to the log-linear framework. This
way we allow different interpolation weights for
the two translation directions and can optimize
them automatically along with the other feature
weights. We will refer to this method as feature-
wise combination. Again, we retain the intersec-
tion of the two phrase tables. With good log-
linear feature weights, feature-wise combination
should perform at least as well as fixed interpo-
lation. However, the results presented in Table 5
480
Table 2: Statistics for the Europarl German-
English data
German English
TRAIN Sentences 1 311 815
Run. Words 34 398 651 36 090 085
Vocabulary 336 347 118 112
Singletons 168 686 47 507
DEV Sentences 2 000
Run. Words 55 118 58 761
Vocabulary 9 211 6 549
OOVs 284 77
TEST Sentences 2 000
Run. Words 56 635 60 188
Vocabulary 9 254 6 497
OOVs 266 89
show a slightly lower performance. This illustrates
that a higher number of features results in a less
reliable optimization of the log-linear parameters.
5 Experimental Evaluation
5.1 Experimental Setup
We conducted our experiments on the German-
English data published for the ACL 2008
Workshop on Statistical Machine Translation
(WMT08). Statistics for the Europarl data are
given in Table 2.
We are given the three data sets TRAIN ,DEV
and TEST . For the heuristic phrase model, we
first use GIZA++ (Och and Ney, 2003) to compute
the word alignment on TRAIN . Next we obtain
a phrase table by extraction of phrases from the
word alignment. The scaling factors of the trans-
lation models have been optimized for BLEU on
the DEV data.
The phrase table obtained by heuristic extraction
is also used to initialize the training. The forced
alignment is run on the training data TRAIN
from which we obtain the phrase alignments.
Those are used to build a phrase table according
to the proposed generative phrase models. After-
ward, the scaling factors are trained on DEV for
the new phrase table. By feeding back the new
phrase table into forced alignment we can reiterate
the training procedure. When training is finished
the resulting phrase model is evaluated on DEV
Table 3: Comparison of different training setups
for the count model on DEV .
leaving-one-out max phr.len. BLEU TER
baseline 6 25.7 61.1
none 2 25.2 61.3
3 25.7 61.3
4 25.5 61.4
5 25.5 61.4
6 25.4 61.7
standard 6 26.4 60.9
length-based 6 26.5 60.6
and TEST . Additionally, we can apply smooth-
ing by interpolation of the new phrase table with
the original one estimated heuristically, retrain the
scaling factors and evaluate afterwards.
The baseline system is a standard phrase-based
SMT system with eight features: phrase transla-
tion and word lexicon probabilities in both transla-
tion directions, phrase penalty, word penalty, lan-
guage model score and a simple distance-based re-
ordering model. The features are combined in a
log-linear way. To investigate the generative mod-
els, we replace the two phrase translation prob-
abilities and keep the other features identical to
the baseline. For the feature-wise combination
the two generative phrase probabilities are added
to the features, resulting in a total of 10 features.
We used a 4-gram language model with modified
Kneser-Ney discounting for all experiments. The
metrics used for evaluation are the case-sensitive
BLEU (Papineni et al, 2002) score and the trans-
lation edit rate (TER) (Snover et al, 2006) with
one reference translation.
5.2 Results
In this section, we investigate the different as-
pects of the models and methods presented be-
fore. We will focus on the proposed leaving-one-
out technique and show that it helps in finding
good phrasal alignments on the training data that
lead to improved translation models. Our final
results show an improvement of 1.4 BLEU over
the heuristically extracted phrase model on the test
data set.
In Section 3.2 we have discussed several meth-
ods which aim to overcome the over-fitting prob-
481
Figure 3: Performance on DEV in BLEU of the
count model plotted against size n of n-best list
on a logarithmic scale.
lems described in (DeNero et al, 2006). Table 3
shows translation scores of the count model on the
development data after the first training iteration
for both leaving-one-out strategies we have in-
troduced and for training without leaving-one-out
with different restrictions on phrase length. We
can see that by restricting the source phrase length
to a maximum of 3 words, the trained model is
close to the performance of the heuristic phrase
model. With the application of leaving-one-out,
the trained model is superior to the baseline, the
length-based strategy performing slightly better
than standard leaving-one-out. For these experi-
ments the count model was estimated with a 100-
best list.
The count model we describe in Section 4.1 esti-
mates phrase translation probabilities using counts
from the n-best phrase alignments. For smaller n
the resulting phrase table contains fewer phrases
and is more deterministic. For higher values of
n more competing alignments are taken into ac-
count, resulting in a bigger phrase table and a
smoother distribution. We can see in Figure 3
that translation performance improves by moving
from the Viterbi alignment to n-best alignments.
The variations in performance with sizes between
n = 10 and n = 10000 are less than 0.2 BLEU.
The maximum is reached for n = 100, which we
used in all subsequent experiments. An additional
benefit of the count model is the smaller phrase
table size compared to the heuristic phrase extrac-
tion. This is consistent with the findings of (Birch
et al, 2006). Table 4 shows the phrase table sizes
for different n. With n = 100 we retain only 17%
of the original phrases. Even for the full model, we
Table 4: Phrase table size of the count model for
different n-best list sizes, the full model and for
heuristic phrase extraction.
N # phrases % of full table
1 4.9M 5.3
10 8.4M 9.1
100 15.9M 17.2
1000 27.1M 29.2
10000 40.1M 43.2
full 59.6M 64.2
heuristic 92.7M 100.0
do not retain all phrase table entries. Due to prun-
ing in the forced alignment step, not all translation
options are considered. As a result experiments
can be done more rapidly and with less resources
than with the heuristically extracted phrase table.
Also, our experiments show that the increased per-
formance of the count model is partly derived from
the smaller phrase table size. In Table 5 we can see
that the performance of the heuristic phrase model
can be increased by 0.6 BLEU on TEST by fil-
tering the phrase table to contain the same phrases
as the count model and reoptimizing the log-linear
model weights. The experiments on the number of
different alignments taken into account were done
with standard leaving-one-out.
The final results are given in Table 5. We can
see that the count model outperforms the base-
line by 0.8 BLEU on DEV and 0.9 BLEU on
TEST after the first training iteration. The perfor-
mance of the filtered baseline phrase table shows
that part of that improvement derives from the
smaller phrase table size. Application of cross-
validation (cv) in the first iteration yields a perfor-
mance close to training with leaving-one-out (l1o),
which indicates that cross-validation can be safely
applied to higher training iterations as an alterna-
tive to leaving-one-out. The weighted count model
clearly under-performs the simpler count model.
A second iteration of the training algorithm shows
nearly no changes in BLEU score, but a small im-
provement in TER. Here, we used the phrase table
trained with leaving-one-out in the first iteration
and applied cross-validation in the second itera-
tion. Log-linear interpolation of the count model
with the heuristic yields a further increase, show-
ing an improvement of 1.3 BLEU onDEV and 1.4
BLEU on TEST over the baseline. The interpo-
482
Table 5: Final results for the heuristic phrase table
filtered to contain the same phrases as the count
model (baseline filt.), the count model trained with
leaving-one-out (l1o) and cross-validation (cv),
the weighted count model and the full model. Fur-
ther, scores for fixed log-linear interpolation of the
count model trained with leaving-one-out with the
heuristic as well as a feature-wise combination are
shown. The results of the second training iteration
are given in the bottom row.
DEV TEST
BLEU TER BLEU TER
baseline 25.7 61.1 26.3 60.9
baseline filt. 26.0 61.6 26.9 61.2
count (l1o) 26.5 60.6 27.2 60.5
count (cv) 26.4 60.7 27.0 60.7
weight. count 25.9 61.4 26.4 61.3
full 26.3 60.0 27.0 60.2
fixed interpol. 27.0 59.4 27.7 59.2
feat. comb. 26.8 60.1 27.6 59.9
count, iter. 2 26.4 60.3 27.2 60.0
lation weight is adjusted on the development set
and was set to ? = 0.6. Integrating both models
into the log-linear framework (feat. comb.) yields
a BLEU score slightly lower than with fixed inter-
polation on both DEV and TEST . This might
be attributed to deficiencies in the tuning proce-
dure. The full model, where we extract all phrases
from the search graph, weighted with their poste-
rior probability, performs comparable to the count
model with a slightly worse BLEU and a slightly
better TER.
6 Conclusion
We have shown that training phrase models can
improve translation performance on a state-of-
the-art phrase-based translation model. This is
achieved by training phrase translation probabil-
ities in a way that they are consistent with their
use in translation. A crucial aspect here is the use
of leaving-one-out to avoid over-fitting. We have
shown that the technique is superior to limiting
phrase lengths and smoothing with lexical prob-
abilities alone.
While models trained from Viterbi alignments
already lead to good results, we have demonstrated
that considering the 100-best alignments allows to
better model the ambiguities in phrase segmenta-
tion.
The proposed techniques are shown to be supe-
rior to previous approaches that only used lexical
probabilities to smooth phrase tables or imposed
limits on the phrase lengths. On the WMT08 Eu-
roparl task we show improvements of 0.9 BLEU
points with the trained phrase table and 1.4 BLEU
points when interpolating the newly trained model
with the original, heuristically extracted phrase ta-
ble. In TER, improvements are 0.4 and 1.7 points.
In addition to the improved performance, the
trained models are smaller leading to faster and
smaller translation systems.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR001-06-C-0023. Any opinions,
ndings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reect the views of the DARPA.
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constraining the
phrase-based, joint probability statistical translation
model. In smt2006, pages 154?157, Jun.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, pages 200?208, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?312, June.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 25?28, Morristown, NJ,
USA. Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proceedings of the
483
Workshop on Statistical Machine Translation, pages
31?38, New York City, June.
John DeNero, Alexandre Buchard-Co?te?, and Dan
Klein. 2008. Sampling Alignment Structure under
a Bayesian Translation Model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314?323, Honolulu,
October.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum bayes risk decoding for bleu. In ACL ?07:
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 101?104, Morristown, NJ, USA. Association
for Computational Linguistics.
Jesu?s-Andre?s Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine
translation. In Procedings of European Association
for Machine Translation (EAMT), Barcelona, Spain,
May. European Association for Machine Translation.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modelling. In
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing (ICASSP), pages 181?184, Detroit, MI,
May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, pages 48?54, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Percy Liang, Alexandre Buchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An End-to-End Discriminative
Approach to Machine Translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 761?
768, Sydney, Australia.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), July.
J.A. Nelder and R. Mead. 1965. A Simplex Method
for Function Minimization. The Computer Journal),
7:308?313.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449,
December.
F.J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical
Methods in Natural Language Processing and Very
Large Corpora (EMNLP99), pages 20?28, Univer-
sity of Maryland, College Park, MD, USA, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Wade Shen, Brian Delaney, Tim Anderson, and Ray
Slyh. 2008. The MIT-LL/AFRL IWSLT-2008 MT
System. In Proceedings of IWSLT 2008, pages 69?
76, Hawaii, U.S.A., October.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA, pages 223?231, Aug.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
620?629, Honolulu, Hawaii, October. Association
for Computational Linguistics.
N. Ueffing, F.J. Och, and H. Ney. 2002. Genera-
tion of word graphs in statistical machine translation.
In Proc. of the Conference on Empirical Methods
for Natural Language Processing, pages 156?163,
Philadelphia, PA, USA, July.
484
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156?164,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Deciphering Foreign Language by Combining Language Models and
Context Vectors
Malte Nuhn and Arne Mauser? and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper we show how to train statis-
tical machine translation systems on real-
life tasks using only non-parallel monolingual
data from two languages. We present a mod-
ification of the method shown in (Ravi and
Knight, 2011) that is scalable to vocabulary
sizes of several thousand words. On the task
shown in (Ravi and Knight, 2011) we obtain
better results with only 5% of the computa-
tional effort when running our method with
an n-gram language model. The efficiency
improvement of our method allows us to run
experiments with vocabulary sizes of around
5,000 words, such as a non-parallel version of
the VERBMOBIL corpus. We also report re-
sults using data from the monolingual French
and English GIGAWORD corpora.
1 Introduction
It has long been a vision of science fiction writers
and scientists to be able to universally communi-
cate in all languages. In these visions, even previ-
ously unknown languages can be learned automati-
cally from analyzing foreign language input.
In this work, we attempt to learn statistical trans-
lation models from only monolingual data in the
source and target language. The reasoning behind
this idea is that the elements of languages share sta-
tistical similarities that can be automatically identi-
fied and matched with other languages.
This work is a big step towards large-scale and
large-vocabulary unsupervised training of statistical
translation models. Previous approaches have faced
constraints in vocabulary or data size. We show how
?Author now at Google Inc., amauser@google.com.
to scale unsupervised training to real-life transla-
tion tasks and how large-scale experiments can be
done. Monolingual data is more readily available,
if not abundant compared to true parallel or even
just translated data. Learning from only monolin-
gual data in real-life translation tasks could improve
especially low resource language pairs where few or
no parallel texts are available.
In addition to that, this approach offers the op-
portunity to decipher new or unknown languages
and derive translations based solely on the available
monolingual data. While we do tackle the full unsu-
pervised learning task for MT, we make some very
basic assumptions about the languages we are deal-
ing with:
1. We have large amounts of data available in
source and target language. This is not a very
strong assumption as books and text on the in-
ternet are readily available for almost all lan-
guages.
2. We can divide the given text in tokens and
sentence-like units. This implies that we know
enough about the language to tokenize and
sentence-split a given text. Again, for the vast
majority of languages, this is not a strong re-
striction.
3. The writing system is one-dimensional left-to-
right. It has been shown (Lin and Knight, 2006)
that the writing direction can be determined
separately and therefore this assumption does
not pose a real restriction.
Previous approaches to unsupervised training for
SMT prove feasible only for vocabulary sizes up to
around 500 words (Ravi and Knight, 2011) and data
156
sets of roughly 15,000 sentences containing only
about 4 tokens per sentence on average. Real data
as it occurs in texts such as web pages or news texts
does not meet any of these characteristics.
In this work, we will develop, describe, and
evaluate methods for large vocabulary unsupervised
learning of machine translation models suitable for
real-world tasks. The remainder of this paper is
structured as follows: In Section 2, we will review
the related work and describe how our approach ex-
tends existing work. Section 3 describes the model
and training criterion used in this work. The im-
plementation and the training of this model is then
described in Section 5 and experimentally evaluated
in Section 6.
2 Related Work
Unsupervised training of statistical translations sys-
tems without parallel data and related problems have
been addressed before. In this section, we will re-
view previous approaches and highlight similarities
and differences to our work. Several steps have been
made in this area, such as (Knight and Yamada,
1999), (Ravi and Knight, 2008), or (Snyder et al,
2010), to name just a few. The main difference of
our work is, that it allows for much larger vocab-
ulary sizes and more data to be used than previous
work while at the same time not being dependent on
seed lexica and/or any other knowledge of the lan-
guages.
Close to the methods described in this work,
Ravi and Knight (2011) treat training and transla-
tion without parallel data as a deciphering prob-
lem. Their best performing approach uses an EM-
Algorithm to train a generative word based trans-
lation model. They perform experiments on a
Spanish/English task with vocabulary sizes of about
500 words and achieve a performance of around
20 BLEU compared to 70 BLEU obtained by a sys-
tem that was trained on parallel data. Our work uses
the same training criterion and is based on the same
generative story. However, we use a new training
procedure whose critical parts have constant time
and memory complexity with respect to the vocab-
ulary size so that our methods can scale to much
larger vocabulary sizes while also being faster.
In a different approach, Koehn and Knight (2002)
induce a bilingual lexicon from only non-parallel
data. To achieve this they use a seed lexicon which
they systematically extend by using orthographic as
well as distributional features such as context, and
frequency. They perform their experiments on non-
parallel German-English news texts, and test their
mappings against a bilingual lexicon. We use a
greedy method similar to (Koehn and Knight, 2002)
for extending a given lexicon, and we implicitly also
use the frequency as a feature. However, we perform
fully unsupervised training and do not start with a
seed lexicon or use linguistic features.
Similarly, Haghighi et al (2008) induce a one-
to-one translation lexicon only from non-parallel
monolingual data. Also starting with a seed lexi-
con, they use a generative model based on canoni-
cal correlation analysis to systematically extend the
lexicon using context as well as spelling features.
They evaluate their method on a variety of tasks,
ranging from inherently parallel data (EUROPARL)
to unrelated corpora (100k sentences of the GIGA-
WORD corpus). They report F-measure scores of the
induced entries between 30 to 70. As mentioned
above, our work neither uses a seed lexicon nor or-
thographic features.
3 Translation Model
In this section, we describe the statistical training
criterion and the translation model that is trained us-
ing monolingual data. In addition to the mathemat-
ical formulation of the model we describe approxi-
mations used.
Throughout this work, we denote the source lan-
guage words as f and target language words as e.
The source vocabulary is Vf and we write the size
of this vocabulary as |Vf |. The same notation holds
for the target vocabulary with Ve and |Ve|.
As training criterion for the translation model?s
parameters ?, Ravi and Knight (2011) suggest
arg max
?
?
?
?
?
f
?
e
P (e) ? p?(f |e)
?
?
?
(1)
We would like to obtain ? from Equation 1 using
the EM Algorithm (Dempster et al, 1977). This
becomes increasingly difficult with more complex
translation models. Therefore, we use a simplified
157
translation model that still contains all basic phe-
nomena of a generic translation process. We formu-
late the translation process with the same generative
story presented in (Ravi and Knight, 2011):
1. Stochastically generate the target sentence ac-
cording to an n-gram language model.
2. Insert NULL tokens between any two adjacent
positions of the target string with uniform prob-
ability.
3. For each target token ei (including NULL)
choose a foreign translation fi (including
NULL) with probability P?(fi|ei).
4. Locally reorder any two adjacent foreign words
fi?1, fi with probability P (SWAP) = 0.1.
5. Remove the remaining NULL tokens.
In practice, however, it is not feasible to deal with
the full parameter table P?(fi|ei) which models the
lexicon. Instead we only allow translation models
where for each source word f the number of words
e? with P (f |e?) 6= 0 is below some fixed value. We
will refer to this value as the maximum number of
candidates of the translation model and denote it
with NC . Note that for a given e this does not nec-
essarily restrict the number of entries P (f ?|e) 6= 0.
Also note that with a fixed value of NC , time and
memory complexity of the EM step isO(1) with re-
spect to |Ve| and |Vf |.
In the following we divide the problem of maxi-
mizing Equation 1 into two parts:
1. Determining a set of active lexicon entries.
2. Choosing the translation probabilities for the
given set of active lexicon entries.
The second task can be achieved by running the
EM algorithm on the restricted translation model.
We deal with the first task in the following section.
4 Monolingual Context Similarity
As described in Section 3 we need some mecha-
nism to iteratively choose an active set of translation
candidates. Based on the assumption that some of
the active candidates and their respective probabili-
ties are already correct, we induce new active candi-
dates. In the context of information retrieval, Salton
et al (1975) introduce a document space where each
document identified by one or more index terms is
represented by a high dimensional vector of term
weights. Given two vectors v1 and v2 of two doc-
uments it is then possible to calculate a similarity
coefficient between those given documents (which
is usually denoted as s(v1, v2)). Similar to this we
represent source and target words in a high dimen-
sional vector space of target word weights which we
call context vectors and use a similarity coefficient
to find possible translation pairs. We first initialize
these context vectors using the following procedure:
1. Using only the monolingual data for the target
language, prepare the context vectors vei with
entries vei,ej :
(a) Initialize all vei,ej = 0
(b) For each target sentence E:
For each word ei in E:
For each word ej 6= ei in E:
vei,ej = vei,ej + 1.
(c) Normalize each vector vei such that
?
ej
(vei,ej )
2 != 1 holds.
Using the notation ei =
(
ej : vei,ej , . . .
)
these
vectors might for example look like
work = (early : 0.2, late : 0.1, . . . )
time = (early : 0.2, late : 0.2, . . . ).
2. Prepare context vectors vfi,ej for the source
language using only the monolingual data for
the source language and the translation model?s
current parameter estimate ?:
(a) Initialize all vfi,ej = 0
(b) Let E??(F ) denote the most probable
translation of the foreign sentence F ob-
tained by using the current estimate ?.
(c) For each source sentence F :
For each word fi in F :
For each word ej 6= E?(fi)1 in
E?(F ):
vfi,ej = vfi,ej + 1
(d) Normalize each vector vfi such that
?
ej
(vfi,ej )
2 != 1 holds.
1denoting that ej is not the translation of fi in E?(F )
158
Adapting the notation described above, these
vectors might for example look like
Arbeit = (early : 0.25, late : 0.05, . . . )
Zeit = (early : 0.15, late : 0.25, . . . )
Once we have set up the context vectors ve and
vf , we can retrieve translation candidates for some
source word f by finding those words e? that maxi-
mize the similarity coefficient s(ve? , vf ), as well as
candidates for a given target word e by finding those
words f ? that maximize s(ve, vf ?). In our implemen-
tation we use the Euclidean distance
d(ve, vf ) = ||ve ? vf ||2. (2)
as distance measure.2 The normalization of context
vectors described above is motivated by the fact that
the context vectors should be invariant with respect
to the absolute number of occurrences of words.3
Instead of just finding the best candidates for a
given word, we are interested in an assignment that
involves all source and target words, minimizing the
sum of distances between the assigned words. In
case of a one-to-one mapping the problem of assign-
ing translation candidates such that the sum of dis-
tances is minimal can be solved optimally in poly-
nomial time using the hungarian algorithm (Kuhn,
1955). In our case we are dealing with a many-
to-many assignment that needs to satisfy the max-
imum number of candidates constraints. For this,
we solve the problem in a greedy fashion by simply
choosing the best pairs (e, f) first. As soon as a tar-
get word e or source word f has reached the limit
of maximum candidates, we skip all further candi-
dates for that word e (or f respectively). This step
involves calculating and sorting all |Ve| ? |Vf | dis-
tances which can be done in time O(V 2 ? log(V )),
with V = max(|Ve|, |Vf |). A simplified example of
this procedure is depicted in Figure 1. The example
already shows that the assignment obtained by this
algorithm is in general not optimal.
2We then obtain pairs (e, f) that minimize d.
3This gives the same similarity ordering as using un-
normalized vectors with the cosine similarity measure
ve?vf
||ve||2?||vf ||2
which can be interpreted as measuring the cosine
of the angle between the vectors, see (Manning et al, 2008).
Still it is noteworthy that this procedure is not equivalent to the
tf-IDF context vectors described in (Salton et al, 1975).
x
y
time (e)
Arbeit (f)
work (e)
Zeit (f)
Figure 1: Hypothetical example for a greedy one-to-one
assignment of translation candidates. The optimal assign-
ment would contain (time,Zeit) and (work,Arbeit).
5 Training Algorithm and Implementation
Given the model presented in Section 3 and the
methods illustrated in Section 4, we now describe
how to train this model.
As described in Section 4, the overall procedure
is divided into two alternating steps: After initializa-
tion we first perform EM training of the translation
model for 20-30 iterations using a 2-gram or 3-gram
language model in the target language. With the ob-
tained best translations we induce new translation
candidates using context similarity. This procedure
is depicted in Figure 2.
5.1 Initialization
Let NC be the maximum number of candidates per
source word we allow, Ve and Vf be the target/source
vocabulary and r(e) and r(f) the frequency rank of
a source/target word. Each word f ? Vf with fre-
quency rank r(f) is assigned to all words e ? Ve
with frequency rank
r(e) ? [ start(f) , end(f) ] (3)
where
start(f) = max(0 , min
(
|Ve| ?Nc ,
?
|Ve|
|Vf |
? r(f)?
Nc
2
?
)
)
(4)
end(f) = min (start(f) +Nc, |Ve|) . (5)
This defines a diagonal beam4 when visualizing
the lexicon entries in a matrix where both source
and target words are sorted by their frequency rank.
However, note that the result of sorting by frequency
4The diagonal has some artifacts for the highest and lowest
frequency ranks. See, for example, left side of Figure 2.
159
In
it
ia
li
za
ti
on
ta
rg
et
w
or
ds
source words
E
M
It
er
at
io
n
s
ta
rg
et
w
or
ds
source words C
on
te
xt
V
ec
to
rs
ta
rg
et
w
or
ds
source words
E
M
It
er
at
io
n
s
. . .
Figure 2: Visualization of the training procedure. The big rectangles represent word lexica in different stages of the
training procedure. The small rectangles represent word pairs (e, f) for which e is a translation candidate of f , while
dots represent word pairs (e, f) for which this is not the case. Source and target words are sorted by frequency so that
the most frequent source words appear on the very left, and the most frequent target words appear at the very bottom.
and thus the frequency ranks are not unique when
there are words with the same frequency. In this
case, we initially obtain some not further specified
frequency ordering, which is then kept throughout
the procedure.
This initialization proves useful as we show by
taking an IBM1 lexicon P (f |e) extracted on the
parallel VERBMOBIL corpus (Wahlster, 2000): For
each word e we calculate the weighted rank differ-
ence
?ravg(e) =
?
f
P (f |e) ? |(r(e)? r(f)| (6)
and count how many of those weighted rank dif-
ferences are smaller than a given value NC2 . Here
we see that for about 1% of the words the weighted
rank difference lies withinNC = 50, and even about
3% for NC = 150 respectively. This shows that the
initialization provides a first solid guess of possible
translations.
5.2 EM Algorithm
The generative story described in Section 3 is im-
plemented as a cascade of a permutation, insertion,
lexicon, deletion and language model finite state
transducers using OpenFST (Allauzen et al, 2007).
Our FST representation of the LM makes use of
failure transitions as described in (Allauzen et al,
2003). We use the forward-backward algorithm on
the composed transducers to efficiently train the lex-
icon model using the EM algorithm.
5.3 Context Vector Step
Given the trained parameters ? from the previous run
of the EM algorithm we set the context vectors ve
and vf up as described in Section 4. We then calcu-
late and sort all |Ve|?|Vf | distances which proves fea-
sible in a few CPU hours even for vocabulary sizes
of more than 50,000 words. This is achieved with
the GNU SORT tool, which uses external sorting for
sorting large amounts of data.
To set up the new lexicon we keep the bNC2 c
best translations for each source word with respect
to P (e|f), which we obtained in the previous EM
run. Experiments showed that it is helpful to also
limit the number of candidates per target words. We
therefore prune the resulting lexicon using P (f |e)
to a maximum of bN
?
C
2 c candidates per target word
afterwards. Then we fill the lexicon with new can-
didates using the previously sorted list of candidate
pairs such that the final lexicon has at most NC
candidates per source word and at most N ?C can-
didates per target word. We set N ?C to some value
N ?C > NC . All experiments in this work were run
with N ?C = 300. Values of N
?
C ? NC seem to pro-
duce poorer results. Not limiting the number of can-
didates per target word at all also typically results in
weaker performance. After the lexicon is filled with
candidates, we initialize the probabilities to be uni-
form. With this new lexicon the process is iterated
starting with the EM training.
6 Experimental Evaluation
We evaluate our method on three different corpora.
At first we apply our method to non-parallel Span-
ish/English data that is based on the OPUS corpus
(Tiedemann, 2009) and that was also used in (Ravi
and Knight, 2011). We show that our method per-
forms better by 1.6 BLEU than the best performing
method described in (Ravi and Knight, 2011) while
160
Name Lang. Sent. Words Voc.
OPUS
Spanish 13,181 39,185 562
English 19,770 61,835 411
VERBMOBIL
German 27,861 282,831 5,964
English 27,862 294,902 3,723
GIGAWORD
French 100,000 1,725,993 68,259
English 100,000 1,788,025 64,621
Table 1: Statistics of the corpora used in this paper.
being approximately 15 to 20 times faster than their
n-gram based approach.
After that we apply our method to a non-parallel
version of the German/English VERBMOBIL corpus,
which has a vocabulary size of 6,000 words on the
German side, and 3,500 words on the target side and
which thereby is approximately one order of magni-
tude larger than the previous OPUS experiment.
We finally run our system on a subset of the non-
parallel French/English GIGAWORD corpus, which
has a vocabulary size of 60,000 words for both
French and English. We show first interesting re-
sults on such a big task.
In case of the OPUS and VERBMOBIL corpus,
we evaluate the results using BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) to reference
translations. We report all scores in percent. For
BLEU higher values are better, for TER lower val-
ues are better. We also compare the results on these
corpora to a system trained on parallel data.
In case of the GIGAWORD corpus we show lexi-
con entries obtained during training.
6.1 OPUS Subtitle Corpus
6.1.1 Experimental Setup
We apply our method to the corpus described in
Table 6. This exact corpus was also used in (Ravi
and Knight, 2011). The best performing methods
in (Ravi and Knight, 2011) use the full 411 ? 579
lexicon model and apply standard EM training. Us-
ing a 2-gram LM they obtain 15.3 BLEU and with
a whole segment LM, they achieve 19.3 BLEU. In
comparison to this baseline we run our algorithm
with NC = 50 candidates per source word for both,
a 2-gram and a 3-gram LM. We use 30 EM iterations
between each context vector step. For both cases we
run 7 EM+Context cycles.
6.1.2 Results
Figure 3 and Figure 4 show the evolution of BLEU
and TER scores for applying our method using a 2-
gram and a 3-gram LM.
In case of the 2-gram LM (Figure 3) the transla-
tion quality increases until it reaches a plateau after
5 EM+Context cycles. In case of the 3-gram LM
(Figure 4) the statement only holds with respect to
TER. It is notable that during the first iterations TER
only improves very little until a large chunk of the
language unravels after the third iteration. This be-
havior may be caused by the fact that the corpus only
provides a relatively small amount of context infor-
mation for each word, since sentence lengths are 3-4
words on average.
0 1 2 3 4 5 6 7 88
10
12
14
16 Full EM best (BLEU)
Iteration
BL
EU
66
68
70
72
74
76
78
80
TE
R
BLEU
TER
Figure 3: Results on the OPUS corpus with a 2-gram LM,
NC = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
2-gram LM in (Ravi and Knight, 2011).
Table 2 summarizes these results and compares
them with (Ravi and Knight, 2011). Our 3-gram
based method performs by 1.6 BLEU better than
their best system which is a statistically significant
improvement at 95% confidence level. Furthermore,
Table 2 compares the CPU time needed for training.
Our 3-gram based method is 15-20 times faster than
running the EM based training procedure presented
in (Ravi and Knight, 2011) with a 3-gram LM5.
5(Ravi and Knight, 2011) only report results using a 2-gram
LM and a whole-segment LM.
161
0 1 2 3 4 5 6 7 88
10
12
14
16
18
20
22
24
Full EM best (BLEU)
Iteration
BL
EU
64
66
68
70
72
TE
R
BLEU
TER
Figure 4: Results on the OPUS corpus with a 3-gram LM,
NC = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
whole-segment LM in (Ravi and Knight, 2011)
Method CPU BLEU TER
EM, 2-gram LM
411 cand. p. source word
(Ravi and Knight, 2011)
?850h6 15.3 ?
EM, Whole-segment LM
411 cand. p. source word
(Ravi and Knight, 2011)
?7 19.3 ?
EM+Context, 2-gram LM
50 cand. p. source word
(this work)
50h8 15.2 66.6
EM+Context, 3-gram LM
50 cand. p. source word
(this work)
200h8 20.9 64.5
Table 2: Results obtained on the OPUS corpus.
To summarize: Our method is significantly faster
than n-gram LM based approaches and obtains bet-
ter results than any previously published method.
6Estimated by running full EM using the 2-gram LM using
our implementation for 90 Iterations yielding 15.2 BLEU.
7?4,000h when running full EM using a 3-gram LM, using
our implementation. Estimated by running only the first itera-
tion and by assuming that the final result will be obtained after
90 iterations. However, (Ravi and Knight, 2011) report results
using a whole segment LM, assigning P (e) > 0 only to se-
quences seen in training. This seems to work for the given task
but we believe that it can not be a general replacement for higher
order n-gram LMs.
8Estimated by running our method for 5? 30 iterations.
6.2 VERBMOBIL Corpus
6.2.1 Experimental Setup
The VERBMOBIL corpus is a German/English
corpus dealing with short sentences for making ap-
pointments. We prepared a non-parallel subset of
the original VERBMOBIL (Wahlster, 2000) by split-
ting the corpus into two parts and then selecting only
the German side from the first half, and the English
side from the second half such that the target side
is not the translation of the source side. The source
and target vocabularies of the resulting non-parallel
corpus are both more than 9 times bigger compared
to the OPUS vocabularies. Also the total amount of
word tokens is more than 5 times larger compared
to the OPUS corpus. Table 6 shows the statistics of
this corpus. We run our method for 5 EM+Context
cycles (30 EM iterations each) using a 2-gram LM.
After that we run another five EM+Context cycles
using a 3-gram LM.
6.2.2 Results
Our results on the VERBMOBIL corpus are sum-
marized in Table 3. Even on this more complex
task our method achieves encouraging results: The
Method BLEU TER
5? 30 Iterations EM+Context
50 cand. p. source word, 2-gram LM
11.7 67.4
+ 5? 30 Iterations EM+Context
50 cand. p. source word, 3-gram LM
15.5 63.2
Table 3: Results obtained on the VERBMOBIL corpus.
translation quality increases from iteration to itera-
tion until the algorithm finally reaches 11.7 BLEU
using only the 2-gram LM. Running further five
cycles using a 3-gram LM achieves a final perfor-
mance of 15.5 BLEU. Och (2002) reports results of
48.2 BLEU for a single-word based translation sys-
tem and 56.1 BLEU using the alignment template
approach, both trained on parallel data. However, it
should be noted that our experiment only uses 50%
of the original VERBMOBIL training data to simulate
a truly non-parallel setup.
162
Iter. e p(f1|e) f1 p(f2|e) f2 p(f3|e) f3 p(f4|e) f4 p(f5|e) f5
1. the 0.43 la 0.31 l? 0.11 une 0.04 le 0.04 les
2. several 0.57 plusieurs 0.21 les 0.09 des 0.03 nombreuses 0.02 deux
3. where 0.63 ou` 0.17 mais 0.06 indique 0.04 pre?cise 0.02 appelle
4. see 0.49 e?viter 0.09 effet 0.09 voir 0.05 envisager 0.04 dire
5. January 0.25 octobre 0.22 mars 0.09 juillet 0.07 aou?t 0.07 janvier
? Germany 0.24 Italie 0.12 Espagne 0.06 Japon 0.05 retour 0.05 Suisse
Table 4: Lexicon entries obtained by running our method on the non-parallel GIGAWORD corpus. The first column
shows in which iteration the algorithm found the first correct translations f (compared to a parallely trained lexicon)
among the top 5 candidates
6.3 GIGAWORD
6.3.1 Experimental Setup
This setup is based on a subset of the monolingual
GIGAWORD corpus. We selected 100,000 French
sentences from the news agency AFP and 100,000
sentences from the news agency Xinhua. To have a
more reliable set of training instances, we selected
only sentences with more than 7 tokens. Note that
these corpora form true non-parallel data which, be-
sides the length filtering, were not specifically pre-
selected or pre-processed. More details on these
non-parallel corpora are summarized in Table 6. The
vocabularies have a size of approximately 60,000
words which is more than 100 times larger than the
vocabularies of the OPUS corpus. Also it incor-
porates more than 25 times as many tokens as the
OPUS corpus.
After initialization, we run our method with
NC = 150 candidates per source word for 20 EM
iterations using a 2-gram LM. After the first context
vector step with NC = 50 we run another 4 ? 20
iterations with NC = 50 with a 2-gram LM.
6.3.2 Results
Table 4 shows example lexicon entries we ob-
tained. Note that we obtained these results by us-
ing purely non-parallel data, and that we neither
used a seed lexicon, nor orthographic features to as-
sign e.g. numbers or proper names: All results are
obtained using 2-gram statistics and the context of
words only. We find the results encouraging and
think that they show the potential of large-scale un-
supervised techniques for MT in the future.
7 Conclusion
We presented a method for learning statistical ma-
chine translation models from non-parallel data. The
key to our method lies in limiting the translation
model to a limited set of translation candidates and
then using the EM algorithm to learn the probabil-
ities. Based on the translations obtained with this
model we obtain new translation candidates using
a context vector approach. This method increased
the training speed by a factor of 10-20 compared
to methods known in literature and also resulted
in a 1.6 BLEU point increase compared to previ-
ous approaches. Due to this efficiency improvement
we were able to tackle larger tasks, such as a non-
parallel version of the VERBMOBIL corpus having
a nearly 10 times larger vocabulary. We also had a
look at first results of our method on an even larger
Task, incorporating a vocabulary of 60,000 words.
We have shown that, using a limited set of trans-
lation candidates, we can significantly reduce the
computational complexity of the learning task. This
work serves as a big step towards large-scale unsu-
pervised training for statistical machine translation
systems.
Acknowledgements
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation. The authors would like to thank Su-
jith Ravi and Kevin Knight for providing us with the
OPUS subtitle corpus and David Rybach for kindly
sharing his knowledge about the OpenFST library.
163
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 40?47. Association for
Computational Linguistics.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Jan Holub and Jan Zda?rek, editors, CIAA,
volume 4783 of Lecture Notes in Computer Science,
pages 11?23. Springer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, B, 39.
Aria Haghighi, Percy Liang, T Berg-Kirkpatrick, and
Dan Klein. 2008. Learning Bilingual Lexicons from
Monolingual Corpora. In Proceedings of ACL08 HLT,
pages 771?779. Association for Computational Lin-
guistics.
Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
ACL Workshop on Unsupervised Learning in Natural
Language Processing, number 1, pages 37?44. Cite-
seer.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL02 workshop on Unsupervised lex-
ical acquisition, number July, pages 9?16. Association
for Computational Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Shou-de Lin and Kevin Knight. 2006. Discovering
the linear writing order of a two-dimensional ancient
hieroglyphic script. Artificial Intelligence, 170:409?
421, April.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schuetze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, RWTH Aachen University, Aachen, Ger-
many, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sujith Ravi and Kevin Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram mod-
els. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 812?819, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 12?21,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Gerard M. Salton, Andrew K. C. Wong, and Chang S.
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613?620, November.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In 48th Annual Meeting of the Association for
Computational Linguistics, number July, pages 1048?
1057.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia, Borovets, Bul-
garia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of speech-to-speech translations. Springer-
Verlag, Berlin.
164
