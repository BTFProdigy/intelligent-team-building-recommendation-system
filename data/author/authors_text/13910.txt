33
34
35
36
37
38
39
40
From Controlled Document Authoring
to Interactive Document Normalization
Aure?lien Max
Groupe d?E?tude pour la Traduction Automatique
GETA-CLIPS
Grenoble, France
aurelien.max@imag.fr
Abstract
This paper presents an approach to nor-
malize documents in constrained domains.
This approach reuses resources developed
for controlled document authoring and is
decomposed into three phases. First, can-
didate content representations for an input
document are automatically built. Then,
the content representation that best corres-
ponds to the document according to an ex-
pert of the class of documents is identified.
This content representation is finally used to
generate the normalized version of the docu-
ment. The current version of our prototype
system is presented, and its limitations are
discussed.
1 Document normalization
The authoring of documents in constrained
domains and their translation into other lan-
guages is a very important activity in industrial
settings. In some cases, the distinction between
technical writers and technical translators
has started to blur, so as to minimize the
time and efforts needed to obtain multilingual
documents. The paradigm of translation for
monolinguals introduced by Kay in 1973 (Kay,
1997)1 led the way to a new conception of
the authoring task, which first materialized
with systems involving human disambiguation
(e.g. (Boitet, 1989; Somers et al, 1990)). A
related paradigm emerged in the 90s (Hartley
and Paris, 1997), whereby a technical author
is responsible for providing the content of a
document and a generation system produces
multilingual versions of it. Updating docu-
ments is then done by updating the document
content, and only some postediting may take
place instead of full translation by a human
translator.
Systems implementing this paradigm range
from template-based multilingual document
1This is a reedition of the original article.
Figure 1: Architecture of a MDA system
creation to systems presenting the user with
the evolving text of the document (often called
the feedback or control text) in her language,
following from the WYSIWYM (What You See
Is What You Meant) approach (Power and
Scott, 1998).2 Anchors (or active zones) in
the text of the evolving document allow the
user to specify further its semantics by making
choices presented to her in her language. The
underlying content representation is then used
to generate the text of the document in as many
languages as the system supports. The MDA
(Multilingual Document Authoring) system
(Dymetman et al, 2000; Brun et al, 2000)
follows the WYSIWYM approach, but puts
a strong emphasis on the well-formedness of
document semantic content. More particularly,
document content can be specified in terms
of communicative goals, allowing the selection
of messages which are contrastive within
the modelled class of documents in no more
steps than is needed to identify a predefined
communicative goal. Figure 1 illustrates the ar-
chitecture of a MDA system. A MDA grammar
specifies the possible content representations of
a document in terms of trees of typed semantic
objects in a formalism inspired from Definite
Clause Grammars (Pereira and Warren, 1980).
2We have done a review of these systems in (Max,
2003a) in which we have identified and compared five
families of approaches.
Figure 2: Document normalization in a given
class of documents
Considering all the possibilities offered by
having the semantic description of a docu-
ment, for example the exploitation within
the Semantic Web, it seemed very interesting
to reuse resources developed for controlled
document authoring to analyze existing docu-
ments. Also, a corpus study of drug leaflets
that we conducted (Max, 2003a) showed that
documents from the same class of documents
could contain a lot of variation, which can
hamper the reader?s understanding. We de-
fined document normalization as the process of
the identification of the content representation
produced by an existing document model
corresponding best to an input document,
followed by the automatic generation of the
text of the normalized document from that
content representation. This is illustrated in
figure 2.
In the next section, we briefly describe our
paradigm for document content analysis, which
exploits the MDA formalism in a reverse way.
Candidate content representations expressed
in the MDA formalism are first produced and
ranked automatically, and a human expert then
identifies the one that best accounts for the
communicative content of the original docu-
ment. The core of this paper is devoted to our
implementation of interactive negotiation for
document normalization. Finally, we discuss
our results and propose ways of improving the
system.
2 Document normalization system
A MDA grammar can enumerate the well-
formed content representations for documents
of a given class and associate textual reali-
zations to them (Dymetman et al, 2000).
Content representations are typed abstract
semantic trees in which dependencies can be
established through unification of variables.
Generation of text is done in a compositional
manner from the semantic representation.
Figure 3 shows an excerpt of a MDA grammar
describing well-formed commands for the Unix
shell. Such a grammar describes both the
abstract semantic syntax and the concrete
syntax for a particular language, English in this
case. The first rule reads as follows: lsCommand
is a semantic object of type shellCommand type,
which is composed of an object of type fileSe-
lection type, an object of type sortCriteria type,
and an object of type displayOptions type. Text
strings appearing in the right-hand side of
the rules are used together with the strings
associated with the present semantic objects to
compose the normalized text associated with
the described abstract semantic trees.
Our approach to normalize documents has
been described in (Max, 2003b). A heuristic
search procedure in the space of content repre-
sentations defined by a MDA grammar is first
performed. Its evaluation function measures
a similarity score between the document to
be normalized and the normalized documents
that can be produced from a partial content
representation. The similarity score is inspired
from information retrieval, and takes into
account common descriptors and their relative
informativity in the class of documents. The
admissibility property of the search procedure
guarantees that the first complete content
representation found is the one with the best
global similarity score. This process uses text
generation to measure some kind of similarity,
and has been called fuzzy inverted generation.
In order to better cover the space of texts
conveying the same communicative content,
the MDA formalism has been extended to
support non-deterministic generation, allowing
the production of competing texts from the
same content representation, as is illustrated in
figure 4. For each considered content represen-
tation, texts are produced and compared to the
document to be normalized, thus allowing the
ranking of candidate content representations
% semantic object of type ?shellCommand_type? describing the ?ls? command
lsCommand(FileSelection, SortCriteria, DisplayOptions)::shellCommand_type-e-[] -->
[?List ?],
FileSelection::fileSelection_type-e-[],
SortCriteria::sortCriteria_type-e-[],
DisplayOptions::displayOptions_type-e-[].
fileSelection(ListOfFilesAndDirectories, HiddenFilesSelection, DirectoriesContentsListing, LinksReferenceListing)::
fileSelection_type-e-[] -->
ListOfFilesAndDirectories::listOfFilesAndDirectories_type-e-[], [?.?],
HiddenFilesSelection::hiddenFilesSelection_type-e-[],
DirectoriesContentsListing::directoriesContentsListing_type-e-[],
LinksReferenceListing::linksReferencesListing_type-e-[].
% ...
% description for the type ?linksReferencesListing_type?
type_display(e, linksReferencesListing_type, ?specifies how links are shown?).
% description for the objects ?displayLinksReferences? and ?dontDisplayLinksReferences?
functor_display(e, displayLinksReferences,?show the files and directories that are referenced by links?).
functor_display(e, dontDisplayLinksReferences,?show links as such (not the files and directories they point to)?).
displayLinksReferences::linksReferencesListing_type-e-[] -->
[? Display referenced files and directories instead of links. ?].
dontDisplayLinksReferences::linksReferencesListing_type-e-[] -->
[? Display links as such. ?].
Figure 3: MDA grammar extract for the description of the ls Unix command
Figure 4: Identification of the best content
representation through fuzzy inverted genera-
tion
by decreasing the similarity score.
Given the limitations of the similarity
measure inspired from information retrieval,
the search is continued to find the N first
documents with the best similarity scores. The
identification of the content representation that
represents best the communicative content
of the original document is then done by
interactive negotiation between an expert of
the class of the document and the system based
on the candidates previously extracted.
To demonstrate how the implemented system
works, we will consider the normalization of the
following description in English of a command
for the Unix shell with the grammar of figure
3: List all files. Do not show hidden files and visit
subdirectories recursively. Sort results by date of
last modification in long format in single-column
in reverse chronological order. Give file size in
bytes.
2.1 Finding candidate document
representations: fuzzy inverted
generation
The MDA grammar used is first precompiled
offline by a separate tool, in order to associate
profiles of text descriptors to semantic objects
and types in the grammar (see (Max, 2003b)
for details). In our current implementation,
descriptors are WordNet synsets. The text
of the input document is then lemmatized
and the descriptors are extracted, yielding the
profile of descriptors for the input document.
The grammar is then used to construct partial
abstract semantic trees, which are ordered in
a list of candidates according to the similarity
score computed between their profile and that
of the input document. At each iteration, the
search algorithm considers the most promising
candidate content representation and performs
one step of derivation on it, which corresponds
to instantiating a variable in the tree with a
value for its type. The first complete candidate
(i.e. an abstract tree not containing any
variable) found is then kept, and the search
continues until a given number of candidates
has been found. This number defines a value
of system confidence, which can be selected
by the user of our normalization system: the
higher the confidence, the fewer candidates are
kept, at the risk that the best one according
to an expert may not be present. Given the
size of the grammar used and the complexity
of the analysed document, a small number of
candidates can be kept (20 in our example).
This process restricts the search space from
a large collection of virtual documents3 to
a comparatively smaller number of concrete
textual documents, associated with their se-
mantic structure. A factorization process then
builds a unique content representation that
contains all the different alternative subtrees
found in the candidates. Each semantic object
in the resulting factorized semantic tree is
then decorated by a list of all the candidates
to which it belongs. Competing semantic
objects are ranked according to the score of
the candidate with the highest score to which
they belong. This compact representation
permits to consider underspecifications from
the analysis of the input document present at
any depth in the candidate semantic trees.
2.2 Identifying the best document
representation: interactive
negotiation
Document normalization implies a normative
view on the analysis of a document. Because the
communicative content that will be ultimately
retained may not be exactly that of the original
document, some negotiation must take place to
determine which alternative semantic content, if
any, is acceptable. This is analoguous to what
happens in translation. As (Kay et al, 1994)
put it:
Translation is not a meaning-
preserving function from a source
to a target text. Indeed, it is probably
3We call virtual documents all the documents that
can be produced by a given grammar.
Figure 5: Resolving underspecifications by in-
teractive negotiation
not helpful to think of it as a function
at all, but rather as a matter of
compromise.
In our view, a human expert should be
responsible for making difficult decisions that
the machine cannot make without significant
interpretation capabilities. Furthermore, these
decisions encompass cases where no explicit
content in the input document can be used to
determine content that is expected in order
to obtain a well-formed representation in the
semantic model used.4 This will be illustrated
below with the negotiation dialogue of figure 8.
A naive way to select the candidate content
representation found by the system that best
corresponds to the input document would
be to show to an expert all the normalized
texts corresponding to the candidates. This
would however be a tedious and error-prone
task. The compact representation built at the
end of fuzzy inverted generation allows the
discrimination of candidates based on local
underspecifications corresponding to compe-
ting semantic choices. We have implemented
three methods for supporting interactive
negotiation that will be described below.
They allow an expert to resolve underspeci-
fications and therefore update the factorized
content representation by eliminating incorrect
hypotheses. This is iterated until the facto-
rized content representation does not contain
any underspecification, as illustrated in figure 5.
Figure 6 shows the main interface of our nor-
malization system after the automatic selection
4This suggests that document normalization can be
used as a corrective mecanism applied on ill-formed do-
cuments that can be incomplete or semantically incohe-
rent relatively to a given semantic model.
Figure 6: Interface of our document normalization system
of candidate content representations and the
construction of the compact representation.
Semantic view The middle panel on the
right of the window contains the semantic view,
which is a graphical view of the factorized
abstract semantic tree that can be interpreted
by the expert. It uses the text descriptions
for semantic objects and types as described by
the functor display and type display predicates
present in the original MDA formalism (see
figure 3). The tick symbol represents a
semantic object that dominates a semantic
subtree containing no underspecifications. In
our example, this is the case for the object
described as output type and detail level for
display. The arrow symbol describes a
semantic object that does not take part in
an underspecification, but which dominates a
subtree that contains at least one. The ex-
clamation mark symbol denotes a semantic
type that is underspecified, and for which at
least two semantic objects are in competition.
Semantic objects in competition are denoted
by the interrogation mark symbol , and are
ordered according to the highest score of the
candidate representation to which they belong.
This view can be used by the expert to
Figure 7: Validation of a semantic choice within
the semantic view
navigate at any depth inside the compact
representation. By clicking on a semantic
object in competition, the expert can decide
whether this object belongs to the solution or
not. On the example of figure 7, the expert has
selected the first possibility (subdirectories are
recursively visited) for an underspecified type
(specifies how subdirectories are visited), which
is itself dominated by another underspecified
type (specifies whether only directory names are
shown or. . . ). The menu that pops up allows
the validation of the selected object: this will
have for effect to prune the factorized tree of
any subtree that does not belong to at least
one of the candidates of the validated object.
In the present case, not only will it prune the
alternative subtree dominated by subdirectories
are not recursively visited, but also the subtree
dominated by only show directory names (not
Figure 8: Negotiation dialogue about how links
should be shown
their content) present at a shallower level in
the tree. Furthermore, subtrees that would
be incompatible elsewhere in the compact
representation because of failed parameter
unification would disappear. Conversely, the
invalidation operation prunes all the subtrees
which have at least one candidate in common
with the invalidated object. The expert can
also ask for a negotiation dialogue, which will
be introduced shortly.
MDA view It seemed very natural to propose
a view with which a user of aMDA system would
already be familiar. Such a view shows the nor-
malized text corresponding to all the objects
from the root object that are not in competi-
tion. Underspecified semantic types appear as
underlined text spans called active zones, which
trigger a pop up menu when clicked. Whereas
in the MDA authoring mode all the possible ob-
jects for the semantic type that do not violate
any semantic dependencies are shown, our sys-
tem only proposes those that belong to can-
didates that are still in competition. Further-
more, these semantic objects are not ordered
by their order in appearance in the grammar,
but by the score of their most likely candidate
according to our system. Selecting an object
corresponds to validating it, implying that the
invalidation operation is not accessible from this
view. Also, underspecified semantic types do-
minated by other underspecified types cannot
be resolved using this view, as they do not ap-
pear in the text.5 However, dealing with a text
in natural language corresponding to the nor-
malized document may be a more intuitive in-
terface to some users, although it may require
more operations.
Negotiation dialogues The key element in
this task is the minimization of the number of
5We thought that showing these types using cascade
menus would be too confusing for the user.
operations by the user. The two previous views
allow the expert to choose some underspecifica-
tion to resolve. The List of underspecifications
panel on the left of the window in figure 6
contains an enumeration of all underspecifi-
cations found in the compact representation.
They are ordered by decreasing score, where
the score can indicate the average score of
the objects in competition, or the inverse of
the average number of candidates per object
in competition. Therefore, the expert can
choose to resolve first underspecifications that
contain likely objects, or underspecifications
that involve few candidates so that the valida-
tion of an object will prune more candidates
from the compact representations. Clicking
on an underspecification in the list triggers a
negotiation dialogue similar to that of figure 8.
The semantic type on that dialogue, specifies
how links are shown, is not supported by any
evidence in the input document. The expert
can however choose a value for it. When the
underspecification is resolved, all the views
are refreshed to reflect the new state of the
compact representation, and a negotiation
dialogue for the underspecification then ranked
first in the list is shown. The expert can either
discard it, or continue in the dialogue mode,
with the possibility to skip the resolution of an
underspecification.
3 Discussion and perspectives
We have presented an approach to normalize
documents in constrained domains and its
implementation. Our approach combines the
strictness of well-formed content representa-
tions and and the flexibility of information
retrieval techniques, and makes use of human
expertise to resolve difficult interpretation
problems in an attempt to build an opera-
tional system. Although our initial results are
promising, our approach could be improved in
several ways.
First of all, an important evaluation factor of
our approach is how much effort has to be done
by the human expert. We have only conducted
informal experiments of evaluation by the task,
which have revealed that normalization can
be performed quite fast when the user has a
good command of the different views available.
Nevertheless, it seems crucial to be able to
present the expert with at least some evidence
from the text of the input document to support
competing semantic objects. Morever, the
evidence extracted from the input document
could be used as the basis for learning new
formulations for particular communicative
goals that would match better subsequent
similar input. Although our system already
supports non-deterministic generation, we have
not implemented a mechanism that would
allow supervised learning of new formulations
yet. We expect this ?normalization memory?
functionality to have an important impact for
the normalization of documents from the same
origin, as it should improve the automatic
selection of content representations.
In case the candidates returned by fuzzy
inverted generation do not contain the content
representation representing best the input
document, the user can choose to reanalyze
the document. This will start search again
from the (N+1)th content representation. But
because the expert might have already resolved
some underspecifications and thus identified
subparts that should belong to the solution,
this information should be taken into account
while reanalyzing the document, which is not
the case in the current implementation. If
the solution has to be present in the list of
candidates returned, it should be as close to
the top of the list as possible, so that the first
choices for each underspecification represent
the actual best choices. To this end, we intend
to implement a second-pass analysis that
would rerank the candidates produced by fuzzy
inverted generation by computing text similari-
ties over short passages such as those proposed
in (Hatzivassiloglou et al, 1999). These techni-
ques were much harder to implement during
the search in the virtual space of documents
produced by the document model, because par-
tial content representations are not actual texts.
4 Acknowledgements
Many thanks to Marc Dymetman, who super-
vised my work at Xerox Research Centre Eu-
rope and who originally came up with the con-
cept of fuzzy inverted generation. Many thanks
also to Christian Boitet, my university PhD su-
pervisor, and to Anne-Lise Bully, Ce?dric Leray
and Abdelkhalek Rherad for their programming
work on the interface of the presented system.
This work was funded by a PhD grant from
ANRT and XRCE.
References
Christian Boitet. 1989. Speech Synthesis and
Dialogue Based Machine Translation. In Pro-
ceedings of the ATR Symposium on Basic Re-
search for Telephone Interpretation, Kyoto,
Japan.
Caroline Brun, Marc Dymetman, and Veronika
Lux. 2000. Document Structure and Multi-
lingual Authoring. In Proceedings of INLG
2000, Mitzpe Ramon, Israel.
Marc Dymetman, Veronika Lux, and Aarne
Ranta. 2000. XML and Multilingual Doc-
ument Authoring: Convergent Trends. In
Proceedings of COLING 2000, Saarbrucken,
Germany.
Anthony F. Hartley and Ce?cile L. Paris. 1997.
Multilingual Document Production - From
Support for Translating to Support for Au-
thoring. Machine Translation, 12:109?128.
Vasileios Hatzivassiloglou, Judith L. Klavans,
and Eleazar Eskin. 1999. Detecting Text
Similarity over Short Passages: Exploring
Linguistic Feature Combinations via Machine
Learning. In Proceedings of EMNLP/VLC-
99, College Park, United States.
Martin Kay, Jean Mark Gawron, and Peter
Norvig. 1994. Verbmobil ? A Translation
System for Face-to-Face Dialog. CSLI Lec-
ture Notes.
Martin Kay. 1997. The Proper Place of Men
and Machines in Language Translation. Ma-
chine Translation, 12:3?23.
Aure?lien Max. 2003a. De la cre?ation de docu-
ments normalise?s a` la normalisation de doc-
uments en domaine contraint. PhD thesis,
Universite? Joseph Fourier, Grenoble.
Aure?lien Max. 2003b. Reversing Controlled
Document Authoring to Normalize Docu-
ments. In Proceedings of the EACL-03 Stu-
dent Research Workshop, Budapest, Hungary.
Fernando Pereira and David Warren. 1980.
Definite Clauses for Language Analysis. Ar-
tificial Intelligence, 13.
Richard Power and Donia Scott. 1998. Multi-
lingual Authoring using Feedback Texts. In
Proceedings of COLING/ACL-98, Montre?al,
Canada.
H. Somers, J.-I. Tsujii, and D. Jones. 1990.
Machine Translation without a Source Text.
In Proceedings of COLING-90, Helsinki, Fin-
land, volume 3, pages 217?276.
Interpreting Communicative Goals in Constrained Domains
using Generation and Interactive Negotiation
Aure?lien Max
Groupe d?Etude pour la Traduction Automatique
GETA-CLIPS
Grenoble, France
aurelien.max@imag.fr
Abstract
This article presents an approach to in-
terpret the content of documents in con-
strained domains at the level of com-
municative goals. The kind of knowl-
edge used contains descriptions of well-
formed document contents and texts
that can be produced from them. The
automatic analysis of text content is
followed by an interactive negotiation
phase involving an expert of the class
of documents. Motivating reasons are
given for an application of this ap-
proach, document normalization, and
an implemented system is briefly intro-
duced.1
1 Introduction
A classical view on text interpretation is to have
a syntactic parsing process followed by seman-
tic interpretation derived from syntactic structures
(Allen, 1995). In practice, however, building
broad-coverage syntactically-driven parsing gram-
mars that are robust to the variation in the input
is a very difficult task. Sometimes, it may not be
relevant to perform a fine-grained analysis of the
semantic content of text. Indeed, there are cases
where what should be recognized is the high-level
communicative intentions of the author. Depend-
ing on the kind of interpretation that is targeted
1The author was affiliated to Xerox Research Centre Eu-
rope (XRCE) and GETA when this work was carried out un-
der a PhD grant.
from a text, some semantic distinctions need not
be recognized. For example, the two following
sentences found in a drug leaflet may not carry sig-
nificantly different communicative goals in spite
of their clear semantic differences:
? Consult your doctor in case of pregnancy be-
fore taking this product.
? Consult a health professional in case of preg-
nancy before taking this product.
We have identified a domain of application,
document normalization, where text interpretation
can be limited in many cases to the interpretation
of a text in terms of the communicative goals it
conveys (Max, 2003a). We have defined document
normalization as the process that first derives the
normalized communicative content of a text in a
constrained domain (e.g. drug leaflets), and then
generates the normalized version of the text in the
language of the original document. We consid-
ered three levels in a normalization model for doc-
uments in constrained domain:
1. Communicative goals: the communicative
goals that can appear in a document in con-
strained domain belong to a predefined reper-
toire.
2. Communicative structure: the communica-
tive structure describes the content of a docu-
ment in terms of compatible communicative
goals, as well as how these communicative
goals are organized in a document.
3. Natural language: the language used should
be as comprehensible as possible. To this
end, every communicative goal should be as-
sociated with an expression that could be
considered as ?gold standard?.
Figure 1 shows a warning section found in the
drug leaflet for a pain reducer. Manually deriv-
ing a normalized version of this document ex-
tract using a normalization model requires identi-
fying the communicative goals present in the doc-
ument, which may be deduced from textual evi-
dence found at different places in the document.
Once identified, these communicative goals must
be compared with the normalized ones in the pre-
defined repertoire. We consider the four following
cases:
1. A communicative goal in the document is
clearly identified as belonging to the prede-
fined repertoire.
2. A communicative goal in the document be-
longs to the predefined repertoire, but several
normalized communicative goals are in com-
petition due to some evidence found in the
document.
3. A communicative goal in the document does
not belong to the predefined repertoire, but it
is deemed close to a normalized communica-
tive goal.
4. A communicative goal in the document can-
not be matched with any normalized commu-
nicative goal.
Once the normalized communicative goals have
been identified, the communicative structure can
be built (provided there are no incompatibilities)
and the corresponding normalized textual version
produced. A possible normalized text correspond-
ing to the input document of figure 1 is given on
figure 2.
The very general Warnings section has been
split into several subsections. Communicative
goals that were expressed in the same sentence
have been isolated and reformulated in separate
sentences, as is the case for the communicative
goal indicating that the product should not be
taken in case of allergy to aspirin. This commu-
nicative goal was found in a complex sentence,
Do not take this product if you have asthma, an
allergy to aspirin, stomach problems. . . , and was
reformulated as DO NOT TAKE THIS DRUG IF
YOU ARE ALLERGIC TO ASPIRIN in the section
about product warnings.
The communicative goal warning about the
risk of Reye?s syndrome in children is expressed
in a long and complex sentence: Children and
teenagers should not use this medicine for chicken
pox or flu symptoms before a doctor is consulted
about Reye syndrome, a rare but serious illness
reported to be associated with aspirin. Consid-
ering the fact that no other communicative goals
should be in competition with this one in this
class of documents when Reye?s syndrome is
involved, its identification can be quite simple.2
In fact, it illustrates the fact that the interpretation
of communicative goals within documents in
constrained domains may not always require a
very fine-grained semantic analysis, and that some
indicators can already be quite informative.
However, it is unquestionable that in general
identifying communicative goals and comparing
them to predefined communicative goals clearly
requires high-level interpretation capabilities,
which would normally be those of an expert of
the domain. With our application to normalize
documents as target, we have proposed an ap-
proach to extract the communicative content of
documents in constrained domains automatically.
Considering that we wanted to obtain a practical
normalization system, we further defined an
approach to allow a human expert identifying the
correct communicative content of a document
from the set of hypotheses produced automati-
cally.
This task should not be confused with text
paraphrasing, for example for rewriting into a
2We do not claim that this is necessarily true in expert
medical terms. Nonetheless, the normalization model that
we used only considered this communicative goal involving
Reye?s Syndrome.
Drug Interaction Precautions:
Do not take this product if you are taking a prescription drug for anticoagulation (thinning the blood),
diabetes or gout unless directed by a doctor.
Warnings: Children and teenagers should not use this medicine for chicken pox or flu symptoms
before a doctor is consulted about Reye syndrome, a rare but serious illness reported to be associated
with aspirin. Do not take this product if you have asthma, an allergy to aspirin, stomach problems (such
as heartburn, upset stomach, or stomach pain) that persist or recur, ulcers or bleeding problems, or if
ringing in the ears or a loss of hearing occurs, unless directed by a doctor. Do not take this product for
pain for more than 10 days unless directed by a doctor. If pain persists or gets worse, if new symptoms
occur, or if redness or swelling is present , consult a doctor because these could be signs of a serious
condition. As with any drug. If you are pregnant or nursing a baby, seek the advice of a health profes-
sional before using this product. It is especially important not to use aspirin during the last 3 months
of pregnancy unless specifically directed to do so by a doctor because it may cause problems in the
unborn child or complications during delivery. Keep this and all drugs out of the reach of children. In
case of accidental overdose, seek professional assistance or contact a poison control center immediately.
Alcohol Warning: If you consume 3 or more alcoholic drinks every day, ask you doctor whether you
should take aspirin or other pain relievers or fever reducers. Aspirin may cause stomach bleeding.
Figure 1: Example of a warning sections in a drug leaflet for a pain reducer
WARNINGS
Product warnings. DO NOT TAKE THIS DRUG IF YOU ARE ALLERGIC TO ASPIRIN. Do not
take this product for more than 10 days unless directed by a health professional. Consult your doctor if
pain persists or gets worse.
Alcohol. Do not take alcohol when you take this drug or ask your doctor for an alternative pain reducer.
Particular conditions. A doctor should be consulted before taking this drug if you have any of the
following conditions:
- asthma
- stomach problems
- ulcers
- bleeding problems
Children and teenagers. CONSULT A DOCTOR BEFORE ADMINISTERING THIS PRODUCT
TO A CHILD OR A TEENAGER, AS IT CAN INCREASE THE RISKS OF A SERIOUS ILLNESS
CALLED REYE?S SYNDROME.
Pregnancy. Consult a doctor before taking this drug if you are pregnant. Using aspirin during the last
3 months of pregnancy may cause problems to the unborn child or complications during delivery.
Overdose. Stop taking this drug immediately and call a poison control control center or a health
professional if you have taken too much of this drug.
Figure 2: Normalized text corresponding to the warning section of figure 1
controlled language (see e.g. (Nasr, 1996)). The
main objective of our task is to identify which
communicative goals from a given repertoire
occur in a document, and to build a well-formed
communicative structure that contains them.3
Because the speech acts conveying a com-
municative goal (such as one that says that a
doctor should be consulted before taking a given
drug in case of pregnancy) can be performed
under a wide range of surface forms, text para-
phrasing would have to transform very different
surface forms into the same target normalized text.
Through document normalization, we want to
enforce 4 properties of document well-formedness
that should be encodable into the normalization
model used:
? Well-formedness of the communicative
structure of documents: sentences should be
well articulated to form a coherent discourse.
? Consistency of the communicative content:
incompatible communicative goals should
not coexist in the same document.
? Completeness of the communicative content:
communicative content imposed by some
communicative goal must be present.
? Comprehensibility and coherence of the lan-
guage used: readers should be able to identify
easily the communicative intentions across
documents of the same class.
Text paraphrasing into a controlled language
at the level of the sentence would only enforce
the last property, because if controlled language
rules can enforce some level of semantic well-
formedness, they cannot guarantee the three other
properties.
3It is true, however, that document normalization of a
given document with very particular properties relative to a
normalization model could be achieved by text paraphrasing
at the level of the sentence, but this is too specific to us.
2 Automatic analysis of the
communicative content of a document
in constrained domain
Several approaches have already been experi-
mented to analyze the content of documents in
constrained domains, which can vary depending
on the amount of surface analysis of the text.
One type of approach uses information extraction
techniques such as pattern matching that use
strong predictions on the content and attempt to
fill templates derived from a model of the domain
(e.g. (Blanchon, 2002)), thus not giving too much
importance to syntactic structure. Another type
of approach first performs a syntactic analysis
of the text, from which semantic dependencies
can be extracted. The system presented in (Brun
and Hage`ge, 2003) derives normalized predicates
encoding the meaning of documents from seman-
tic dependencies found by a robust parser. This
allows obtaining identical semantic interpretations
for paraphrases such as ProductX is a colorless,
non flammable liquid and ProductX is a liquid
that has no colour and that does not burn easily.
These approaches require an encoding of tem-
plates and extraction or normalization rules that
may be difficult to build and to maintain. Fur-
thermore, if they seem appropriate for extracting
surface semantic information, interpreting com-
municative goals using these techniques may be
more difficult. Indeed, communicative goals can
be expressed with different surface texts carrying
semantic differences that may not bear any signifi-
cance for our purpose and may not always be con-
sidered as paraphrases. In the following examples
from pain reducer leaflets, it may be acceptable
that a particular normalization model consider the
three following sentences as carrying one and only
communicative goal:
1. This product should not be taken for more
than 14 days without first consulting a health
professional.
2. If pain persist after 14 days, consult your doc-
tor before taking any more of this product.
3. If symptoms persist for 2 weeks, stop using
this product and see a physician.
In order to be able to identify communicative
goals, we believe that it is important to consider
them within a well-formed communicative struc-
ture. Therefore, we think that the central objects
for analysis should be well-formed descriptions
of document communicative content4, as it may
be counterproductive to spend too much effort
on the fine-grained analysis of surface text. If
semantic dependencies can be expressed in these
descriptions, then the space of possible contents
will filter out incompatible communicative goals
and thus disambiguate without always requiring a
more fine-grained semantic analysis.
We have proposed an approach for the deep
content analysis of documents in contrained
domain, fuzzy inverted generation (Max and
Dymetman, 2002). Well-formed document con-
tent representations are produced for the class of
the input document. From these representations,
normalized texts are generated, and a score of
semantic similarity taking into account common
descriptors is computed between the normalized
texts and the text of the input document. The un-
derlying hypotheses are, as we said earlier on, that
considering well-formed content representations
can restrict the space of the communicative goals
to consider, and that the presence of informative
textual indicators can help identifying commu-
nicative goals.
However, the space of content representations
being potentially huge, a heuristic search can be
performed to find the candidate representations
with the best global scores. Moreover, in order to
better cover the space of possible texts, the gener-
ation of the text can be done non-deterministically,
so that several texts will compete over the input
document from the same content representation.
Figure 3 shows how several texts produced from a
content representation can span several documents
from the space of possible texts. The content
representation that corresponds to the text with the
4This is under the assumption that the input documents
are semantically well-formed and complete, but if they are
not then the model used can indicate for what reasons they
are ill-formed, and document normalization can be used to
correct those documents so that they become valid relative to
the normalization model.
Figure 3: Fuzzy inverted generation
highest similarity score with the input document
is then considered to be the most likely candidate.
3 Interactive validation of the correct
communicative content
Relying solely on information retrieval techniques
to associate a normalized content representation
to an input document is unfortunately unlikely to
yield good results, even if linguistically-oriented
techniques can improve accuracy (Arampatzis
et al, 2000). We have advocated an interactive
approach to text understanding (Dymetman et al,
2003) where the input text is used as a source of
information to assist the user in re-authoring its
content. Following fuzzy inverted generation, an
interactive negotiation can take place between the
system and its hypotheses (the candidate content
representations) on the one hand, and a human
expert on the second. A naive way would be to
let the expert choose which hypothesis is correct
based on the normalized text associated with each
one of them. But this would be a tedious and
error-prone process. Rather, underspecifications
from analysis can be found by building a compact
representation of the candidates, and then used to
engage in negotiations over local interpretation
issues.
Using interactive validation with generated
texts has already been used in several domains:
for example, (Blanchon, 1994) proposed disam-
biguation dialogues involving reformulations for
dialogue-based machine translation; (Overmyer
et al, 2001) proposed a text that can be used
to inspect the domain object model automa-
tically built from a text describing a software
engineering domain model. In the following
section, we introduce our implementation of a
prototype system for interactive document nor-
malization based on the two presented approaches.
4 Interactive document normalization
system
Systems implementing controlled document
authoring (Hartley and Paris, 1997) are based on
an interaction with an author who makes semantic
choices that define the content of a document,
from which multilingual textual versions can be
produced. Therefore, these systems integrate
resources that can be used to represent document
content and to generate textual versions of the
documents. The MDA system developed at
XRCE (Dymetman et al, 2000; Brun et al,
2000) uses a formalism inspired from Definite
Clause Grammars (Pereira and Warren, 1980)
that encodes both the abstract semantic syntax of
well-formed documents and the concrete syntax
for the documents in several languages.5 MDA
grammars contain the definition of semantic
objects of a given semantic type, which are used
to build typed abstract semantic trees. Impor-
tantly, the formalism can encode the three levels
for a normalization model that we described in
our introduction: semantic objects can be of
any granularity and can thus be communicative
goals; the communicative structure is described
by the abstract semantic syntax, which can be
used to express semantic dependencies across
subtrees; and the text generated is entirely under
control, so normalized texts can be associated
with communicative goals.
5This is achieved by developing parallel grammars that
share the same abstract semantic syntax, but specify concrete
syntax for a particular language.
Figure 4: Architecture of our document normal-
ization system
For the reasons given above, we used the
formalism of MDA for our implementation. The
architecture of our normalization system is shown
on figure 4. Textual descriptors (WordNet synsets
in our current implementation) are first extracted
from the text of the input document to build the
profile of the input document. The MDA grammar
used was previously compiled offline in order to
associate profiles to each semantic objects and
types described in the grammar. Fuzzy inverted
generation is then performed from the profile of
the document and the profiled grammar. Details
on the implementation using MDA grammars have
been described elsewhere (Max, 2003a; Max,
2003b).
The set of abstract semantic trees extracted by
fuzzy inverted generation is then used to build
Figure 5: Factorized abstract semantic tree
a compact representation (a factorized abstract
semantic tree) for interactive negotiation with an
expert. The output of this phase is a single abstract
semantic tree, such as the one shown on figure
5 that is used for interactive validation. The
icon represents a semantic object that dominates
a semantic subtree containing no underspecifica-
tions; the icon represents a semantic object that
does not take part in any underspecification, but
which dominates a subtree that contains at least
one; the icon represents a semantic type that
is underspecified, that is for which at least two
semantic objects are in competition; finally, the
icon denotes semantic objects in competition,
which are ordered for a given type by decreasing
score of plausibility.
The MDA grammar used for analysis can then
be used to produce the text associated with this
tree, which corresponds to the normalized version
of the input document that was validated by the
expert.
The interface of our system displays an enumer-
Figure 6: Example of a negotiation dialogue
ation of all the underspecifications found in the
compact representation. They are ordered by de-
creasing score, where the score can indicate the
average score of the objects in competition, or
the inverse of the average number of candidates
per object in competition. Therefore, the expert
can choose to resolve first underspecifications that
contain likely objects, or underspecifications that
involve few candidates so that the validation of an
object will prune more candidates from the com-
pact representations. Clicking on an underspeci-
fication in the list triggers a negotiation dialogue
similar to that on figure 6. The semantic type on
that dialogue, specifies how links are shown, is
not supported by any evidence in the input docu-
ment. The expert can however choose a value for
it.
5 Perspectives
We have presented a practical approach to content
analysis at the level of communicative goals,
in which a strong emphasis is put on document
content well-formedness. Providing the expert is
willing to spend enough time, the communicative
content of a document can be interactively built.
The better the system performance, the less
time is needed to identify the correct candidate
content representation. The fact that the expert
can read the corresponding normalized text
(on the MDA view) can help guarantee that the
whole validation process was carried out correctly.
We now need to grow our grammars for Unix
commands and drug leaflets, and to enrich our
test corpus of annotated documents (raw text and
abstract semantic structure)6 for these classes in
6Documents for the test corpus can be obtained by using
order to be able to carry out evaluation. Evaluation
should be performed on two aspects. First, the
performance of fuzzy inverted generation could be
measured, for a given normalization model and on
a given source of documents, by the position and
relative score of the candidate content representa-
tion corresponding to the normalized document.
Second, we want to evaluate the usability of our
user interface supporting interactive negotiation.
An evaluation corresponding to the number of
steps and the time needed to obtain the normalized
version of a document would be a good indicator.
Moreover, we plan to implement the possibility
for the expert to add new formulations found in
documents to better match communicative goals
in subsequent normalizations. It will then be
interesting to evaluate the impact of this kind of
supervised learning on system performance and
user acceptance. Our next challenge will be to
investigate how our approach can be applied to
documents in less-constrained domains for which
normalization models cannot be entirely built a
priori.
Acknowledgments
Many thanks to Marc Dymetman and Christian
Boitet for their supervision of this work.
References
James Allen. 1995. Natural Language Understanding.
Benjamin/Cummings Publishing, 2nd edition.
Avi Arampatzis, Th. P. ven der Weide, P. van Bommel,
and C.H.A Koster. 2000. Linguistically Motivated
Information Retrieval. Encyclopedia of Library and
Information Science, 69.
Herve? Blanchon. 1994. LIDIA-1: une premire ma-
quette vers la TA interactive pour tous. Phd thesis,
Universite? Joseph Fourier, Grenoble.
Herve? Blanchon. 2002. A Pattern-based Analyzer for
French in the Context of Spoken Language Transla-
tion: First Prototype and Evaluation. In Proceedings
of COLING-02, Taipei.
our system on each document, or by re-creating the content
with the MDA system. Building a significant corpus is a time-
consuming task that we have not finished yet.
Caroline Brun and Caroline Hage`ge. 2003. Normal-
ization and Paraphrasing using Symbolic Methods.
In Proceedings of the 2nd International Workshop
on Paraphrasing (IWP2003) at ACL-03, Sapporo,
Japan.
Caroline Brun, Marc Dymetman, and Veronika Lux.
2000. Document Structure and Multilingual Author-
ing. In Proceedings of INLG 2000, Mitzpe Ramon,
Israel.
Marc Dymetman, Veronika Lux, and Aarne Ranta.
2000. XML and Multilingual Document Author-
ing: Convergent Trends. In Proceedings of COLING
2000, Saarbrucken, Germany.
Marc Dymetman, Aure?lien Max, and Kenji Yamada.
2003. Towards Interactive Text Understanding. In
Proceeding of ACL-03, interactive posters session,
Sapporo, Japan.
Anthony F. Hartley and Ce?cile L. Paris. 1997. Mul-
tilingual Document Production - From Support for
Translating to Support for Authoring. Machine
Translation, 12:109?128.
Aure?lien Max and Marc Dymetman. 2002. Document
Content Analysis through Inverted Generation. In
Actes de l?atelier Using (and Acquiring) Linguis-
tic (and World) Knowledge for Information Access
du AAAI Spring Symposium Series, Universite? Stan-
ford, Etats-Unis.
Aure?lien Max. 2003a. De la cre?ation de docu-
ments normalise?s a` la normalisation de documents
en domaine contraint. Phd thesis, Universite? Joseph
Fourier, Grenoble.
Aure?lien Max. 2003b. Reversing Controlled Docu-
ment Authoring to Normalize Documents. In Pro-
ceedings of the EACL-03 Student Research Work-
shop, Budapest, Hungary.
Alexis Nasr. 1996. Un mode`le de reformulation de
phrases fonde? sur la the?orie Sens-Texte. Application
aux langues contro?le?es. Phd thesis, Universite? Paris
7.
Scott P. Overmyer, Benoit Lavoie, and Owen Ram-
bow. 2001. Conceptual Modeling through Linguis-
tic Analysis using LIDA. In Proceedings of the 23rd
international conference on Software Engineering,
ICSE, Toronto, Canada.
Fernando Pereira and David Warren. 1980. Definite
Clauses for Language Analysis. Artificial Intelli-
gence, 13.
The Syntax Student?s Companion:
an eLearning Tool designed for
(Computational) Linguistics Students
Aure?lien Max
Groupe d?Etude pour la Traduction Automatique
(GETA-CLIPS)
Grenoble, France
aurelien.max@imag.fr
Abstract
This paper advocates the use of free and
easily accessible computer programs in
teaching. The motivating reasons for a par-
ticular program supporting the learning of
syntax are given, and a first version of the
program is presented and illustrated. Initial
evaluation results led to additional specifi-
cations and to the development of a new
version of the program that is introduced.
Finally, several perspectives for such a sup-
port tool are drawn.
1 Introduction
Doing exercises to manipulate the concepts
taught in a course is essential to both teachers
and students. While the former want to ensure
that their students have a good grasp of the
material that they teach them, the latter often
want to illustrate that material with some
concrete practice. Linguistics or computational
linguistics students who are introduced to the
intricacies of grammar are no less concerned
than any others. A typical exercise consists
in asking students to analyze a sentence by
means of its description as a syntactic tree.
In introductory courses, either a context-free
grammar is given to them before the exercise
begins, or they have to build one of their
own that can be used to analyze the sentence
given. Obviously, the more exercises look like
challenging ?games? and the more they are
easy to use and accessible, the more likely
students are to invest time and effort in trying
to do them (see e.g. (van Halteren, 2002;
Gibbon and Carson-Berndsen, 1999)). If they
spend a lot of time drawing, erasing parts of
their trees, drawing them again or correcting
them, and then waiting for minutes before their
teaching assistant is available again, they may
not find the whole exercise very captivating
very long. But this type of exercise is essential
to understand how the most basic of grammar
formalism works and therefore to build a solid
ground for the study of language analysis.
Computers play a growing role in educa-
tion, as the number of workshops dedicated
to eLearning and related domains shows.
While many institutions experience financial
cuts, often reflected in the reduction of the
time devoted to supervised work, the use of
computer support has also its roots in other
reasons. It should be clear that computer
tools are not meant to dispense entirely with
teachers, but rather to have them concentrate
on the pedagogical content. Machines are
good at supporting well-defined tasks, and can
therefore allow students to practise concepts
that have been encoded into a well designed
computer program. The issues of what type of
practice can be done in a satisfactory manner
with computers today and of the extent to
which it can actually help students or assess
their performance are open to debate and the
object of research. Importantly to us, past
projects have shown that the computer-assisted
learning of syntax can produce a high level
of engagement by students (e.g.(Larson, 1996)).
This paper concentrates more on the stu-
dent?s perspective, inspired from the author?s
own experience as a former computer science
student taking courses in linguistics. The first
section presents the motivating reasons for the
creation of a computer program intended to
support the practice of syntax exercises. The
program is described and its use is illustrated
by concrete examples. Preliminary elements
of evaluation are inferred from the use of the
program by university students and teachers,
showing that this type of support yields
promising results in spite of a few issues. We
then present our current work by describing the
design of a new version of the program, where
modularity and extensibility play a central
role. It is hoped that this new version will be
more suited to both students? and teachers?
needs, and that this practical experience will
contribute to the development of the field of
computer-assisted learning. We finally propose
several tracks for the evolution of this type of
tool.
2 Motivating reasons for the
creation of the program
Supervised time in university courses tends
more to diminish than to augment. It is however
particularly crucial in introductory courses that
students can get a good grasp of the concepts
by regular supervised practice. Exercise sheets
are often found useful only if sufficient time in
the classroom can be devoted to go throught all
the different subtleties encoded in the exercises.
It can therefore be advantageous to offer stu-
dents a means to practise outside of the class-
room, while still being able to ask their teachers
for help. There are a number of criteria that
should be taken into account when designing a
computer program for supporting this kind of
practice, including the following:
? The program should be attractive to
students. It is well known in computer en-
gineering that good programs can end up
not being used if the user was not taken
into consideration from the very beginning
of the engineering process. Students are a
particular kind of users who may not be
willing to use programs that are tedious
or overcomplicated to use, and seen as not
helpful as a result.
? Teachers should have the feeling that
they can control what the program
does. Not only should it be simple for
teachers to add new data conforming to
predefined exercise types, but it should also
be possible to extend the program.1
? The program should provide useful
feedback to students. While it is proba-
bly the case that an asynchronous mode of
practice whereby a student would do exer-
cises on a computer and then send the re-
sults electronically to a teaching supervi-
sor would yield good results in some con-
1It is not expected that teachers would write com-
puter code themselves, but the program could be ex-
tended by means of predefined building bricks or by the
addition of code by a computer engineer with access to
a clear application programming interface (API).
texts, students will expect the program to
assess their answers and possibly provide
feedback, and therefore support self-study
to some extent.
? The use of the program should be in-
dependent from place and time. It
is our own experience as teacher at the
university level that a significant propor-
tion of post-2000 students prefer to work
from home when they have this possibili-
ty. Booking computer rooms for practice
for specific courses may work for some stu-
dents, but certainly not for all of them.
This said, supervised sessions with com-
puters may still be a fruitful option. This,
of course, further implies that the program
should not be too costly for both the uni-
versity and the students, if not free.
When we first worked on the development
of a program that would support the practice
of syntax exercises, back in 1999, there were
already programs in this area. Trees 2 2,
developed at the University of Pennsylvania,
allowed students to visually build syntactic
trees, but in such a way that they could only be
valid relative to the grammar used. Moreover,
at the time the program could only be run lo-
cally on Macintosh computers and required the
purchase of a licence. Syntactica3, developed at
Sunny Brooks University, allowed students to
build grammars and then ask the program to
build the syntactic tree for them, which they
could subsequently modify. Again, at the time
the program only existed for NeXT computers.
The free Java applet from the University of
Bangor, The Syntax Tutor4, permitted a student
to enter a set of context-free rules and to ask
the system to parse a sentence with it.
Except for the case of the The Syntax Tutor,
these programs had to be bought, and could
only be run on specific computer families. Ne-
vertheless, their existence shows that there was
a very promising trend, supported by encoura-
ging evaluation (see e.g. (Larson, 1996; Phillips,
1998)), to offer students computer programs for
the study of syntax.
2http://www.ling.upenn.edu/ kroch/Trees.html
3http://semlab2.sbs.sunysb.edu/Users/rlarson/Syntactica/
syntactica.html
4http://www.bangor.ac.uk/ling/java/lt/LingTutor.html.
This link has been down for some time.
3 Program design considerations
When designing the program, we had two types
of considerations in mind, pedagogical and
technical. The basic idea was to let students
build syntactic trees in a simple way, and to
edit or consult the underlying grammars. What
seemed very important was to let the students
the possibility to make errors, considering that
trial and error, providing appropriate feedback
is given, can be part of a sound learning
process. Therefore, students should be able to
draw syntactic trees that are not valid relative
to a given grammar, which was given to them
or was build by them, and was accessible and
modifiable or hidden. The syntactic theory
used would initially be the X? theory5, and the
types of exercises would include the drawing of
ambiguous sentences based on some data, and
the modification of existing trees to illustrate
syntactic transformations.
Technical considerations included the fact
that the program should be runnable anywhere
and on any computer family. The Java pro-
gramming language (Sun Microsystems, 1995)
was the obvious choice, as it was already quite
mature and could be run over the Internet on
any platform that had a Java virtual machine.
Furthermore, a Java program can exist in two
flavors, as an application that can be installed
and run locally on a personal computer, and
as an applet that can be downloaded at exe-
cution time over the Internet and run by the
virtual machine of a web browser installed on
computers of a university department without
any installation nor maintenance.
Furthermore, exercises and resources for the
program had to be modifiable. For a local use
with the application version, the user should
be able to create new exercises using a simple
description language. For a distributed use
with the applet version, the administrator
of the website where the applet is hosted
should be able to add resources that would be
immediately accessible to all the remote users.
Modifiable resources include grammars, trees,
exercise definitions, and language resource files
for running the program in the language of the
user. XML (W3C, 2000) was chosen as the
format for most of the resources, and a simple
5This choice was based on a particular introductory
course taught at McGill University, which used (O?Grady
and Dobrovolsky, 1996) as its coursebook.
Figure 1: The main window of the program
schema was designed to allow the creation of
new resources. It was initially believed that
this provided a simple way of creating new
resources and modifying existing ones.
4 Presentation of the program
Our program is called the Syntax Student?s
Companion. Figure 1 shows its main interface
running in English.6 The top panel contains
the active grammar (Simple CFG for English in
the example), a button to launch the grammar
editor, the active mode (Free drawing mode)
and a button to switch to the exercise mode.
The panel on the left contains buttons for all
the nonterminal and terminal categories of the
active grammar, and a list for the words in
the lexicon. The main panel is a scrollable
zone called the workspace where trees can be
drawn. Menus contain commands relative to
the customization of the program, user modes,
grammars, and trees.
Clicking on a syntactic category or on a
lexicon word allows dropping it onto the
workspace at a chosen location.7 Trees are
built by combining subtrees, as illustrated in
6The program can be run in 7 languages thanks to
localized resource files contributed by various people.
7The Trees program proposes to drop on the
workspace subtrees corresponding to partial structures
described in the grammar used. We plan to add this
feature in the next version of the program, as it allows
students to concentrate on more advanced notions.
Figure 2: Steps for attaching a subtree to a node
figure 2. First, the root node of the tree that
will become a subtree of another tree should
be selected with the mouse (1), and dragged
onto the node that will become its mother (2).
If that node has not any children yet, then
the attachment is done. Otherwise, the user
has to select the position of the new subtree
among the daughters of its mother (3). When
the position has been chosen, the attachment
is done, and the new layout of the tree is
produced (4), so as to ensure that the trees are
always well-balanced.8 Alternatively, categories
and words can be directly dropped onto the
workspaces as children of existing nodes. Trees
or subtrees can be copied and pasted onto the
workspace, allowing faster construction. To
detach a subtree, the root of the subtree should
simply be dragged away from its parent tree.
Trees and subtrees can also be removed from
the workspace by using the rubber tool.
All these adjunction operations can be done
regardless of the rules defined in the active
grammar. Therefore, students may make errors
and be aware of them only after they try to
validate their trees with the active grammar.
Indeed, contexts where students could use
a tree drawing application with grammars
designed in such a way that irrelevant errors
were not possible revealed in some cases that
the students had become too dependent on the
helping hand of the program and were not able
to perform as well without it (Phillips, 1998).
The current version only supports simple
context-free grammars. Grammars can either
come from a remote or a local file, or they
can be created from scratch by the student.
The grammar editor (see figure 3) allows the
8We are aware that some textbooks use trees with up-
right lefthand branches and sloping righthand branches,
so we will add this possibility as a new parameter. Like-
wise, we will allow trees to be built bottom up, with all
the words of a sentence aligned horizontally.
Figure 3: The dialog box of the editor for
context-free grammars
consultation and modification of the current
grammar. It shows all the derivation rules cor-
responding to a given nonterminal category9,
and allows specifying of how they are presented
on the window of the main interface.
Once students have built trees, they can ask
the program to check their validity according
to the active grammar. If the active grammar
is modifiable, they can modify it so as to ensure
that the coverage of the grammar include
their trees. If the active grammar is hidden
(i.e. not accessible), the validation of their
trees indicates whether they conform to an
9In the presented implementation, lexical categories
appear as just any other nonterminal categories in the
grammar editor dialog box, but that may be confusing
for students. We therefore think that the lexicon should
be distinguished from the grammar itself, as it is done
on the left panel of the main interface (see figure 1).
Figure 4: Checking of the validity of a tree relatively to the current grammar
implicit grammar specification (such as one
that would have been described during lecture
sessions). Tree nodes that violate the rules of
the grammar are shown in red, and passing
the mouse cursor over them displays a message
indicating the nature of the error, as illustrated
in figure 4, subfigure (3).
Three modes of exercises have been defined
and can be encoded in XML resource files. The
drawing of non-ambiguous trees requires the
student to draw the tree for a given sentence
using a given grammar, whereby the analysis
of the sentence is unambiguous. An example of
such an exercise encoded into XML format is
given in figure 5 for the Spanish phrase convo-
catoria de proyectos de innovacio?n educativa.
Figure 6 illustrates the ambiguous tree drawing
exercise type. The student is asked to draw the
syntactic tree for a sentence (Time flies like
an arrow in this case) given several data that
permit to disambiguate the sentence and find
the correct syntactic derivation. The last type
of exercise asks students to modify trees (see
figure 7) to reflect syntactic transformations.
Instead of asking the student to draw the
syntactic tree for the sentence (in the example,
Who will come tomorrow? ), she is provided
with a base tree (in the example, the tree for
the sentence Bobby-Joe will come tomorrow),
in order to better illustrate the transformations
that take place.
5 Initial evaluation
As we are not ourselves involved in syntax
teaching10, we have not been able to perform
any formal evaluation of the presented version
of the program. It is however crucial to be
able to assess the effectiveness of such a tool,
both in terms of the type of help it gives to
10Our initial motivation was to offer such a program
to fellow students.
<?xml version="1.0"?>
<exercices type="" author="">
<exercice name="convocatoria de
proyectos de innovacion
educativa"
language="espanol"
type="Unambiguous tree drawing">
<sentence>convocatoria de proyectos
de innovacion educativa</sentence>
<grammar name="" type="" author="">
<rules>
fsust -> nucleo mod;
ncleo -> sust;
mod -> fprep;
fprep -> director termino;
director -> prep;
termino -> fsust;
mod -> adj;
sust -> convocatoria;
prep -> de;
sust -> proyectos;
sust -> innovacion;
adj -> educativa;
</rules>
<categories_display>
<row>fsust nucleo</row>
<row>mod fprep sust</row>
<row>adj prep termino</row>
<row>director</row>
</categories_display>
</grammar>
</exercice>
</exercices>
Figure 5: Sample exercise definition for unam-
biguous tree drawing
the student and the support it provides to the
teacher. The initial evaluation elements we
have been able to gather from emails sent to
us via the website of the project constitute the
Figure 6: Ambiguous tree drawing exercise
Figure 7: Tree transformation exercise
basis for an updated specification for the new
version of the program that we will introduce
in the next section.
Several teachers have reported that they
had used the program at some point in their
teaching, but we suspect that in most cases
the program was demonstrated to students
(for example, using a data projector in the
classroom), hoping that they would use it for
self-study. The most important limitation
user feedback told us was the difficulty to
add new exercises for teachers. Only few
people contributed exercises in XML format11,
suggesting that this way of specifying resources
was probably not adequate for linguistics
teachers. Although the program can support
any grammar theory based on context-free
grammars, the default grammars made some
users think that only the X? theory could be
used, and some users had difficulty to see
that the grammars could in fact be edited
and totally new sets of categories defined.
Unsurprisingly, some teachers said they were
interested in the support of feature structures.
A not-so-expected use of the program was
for producing graphical trees for inclusion
into documents. This, corroborated with
several user testimonies, seems to indicate that
the program is considered easy to use. Its
simplicity was in fact often mentionned as one
of the preferred characteristics by students
who used the program without any prior
recommandation from a teacher. We also think
that the availability of the program and its
online user manual in several languages may
have contributed to this.12
Some technical issues were also reported.
Most users of the program, who are not
supposed to be computer scientists, found it
difficult to set up the Java program and run
it as an application. Moreover, some web
browsers did not run the applet perfectly. The
existing version of the program is based on
the Java technology that existed in 1999, and
the language is now more mature and better
supported, so it is now simpler to set up a Java
virtual machine on one?s computer and to run
Java programs, and support for Java in web
browsers is much better than it used to be.
As regards the evaluation we would like
to be able to conduct, we believe that user
questionnaires and logging of student activity
would be good indicators of its effectiveness.
Also, it would be interesting to see if the use
11Some people may have written exercises of which we
are not aware.
12Evaluation results for the Syntactica grammar work-
bench revealed that the use of this kind of computer-
assisted instruction surprisingly increased the need for
instructor support (Larson, 1996). We assume that this
was partly due first to the number of functions of the
program, as well as the fact that at the time linguis-
tics students were for the most part new to the use of
computers.
of the program can make significant differences
in the evaluation of the performance of student
groups.
6 Current work
We have specified a new version of the program
that will be partly developed by two Mas-
ters students during a computer engineering
project. We present the main changes from the
existing version in this section, and we conclude
with some perspectives in the next section.
First of all, the main lesson we can draw from
user feedback is that no matter how much time
is spent on specification, not all features that
would be useful to users could be imagined.
Therefore, it seems a good idea that such a
non-commercial program be extensible by other
contributors who would like to add new features
such as new exercise types, or support for other
grammatical theories. The new version will
have an OpenSource licence, which implies that
we pay a particular attention to the genericity,
modularity and documentation of the source
code, and that the program will continue to be
free to use, which seems essential to us.
A bottleneck to a more widespread use of
the program is certainly the difficulty to create
new resources, mainly exercises. A particular
mode for the definition of exercises will be
integrated into the program. This mode will
allow a teacher to describe an exercise and
its solution in a way as similar as possible
to the exercise mode itself. We also want
to support the description of possible errors
and their appropriate corrections and com-
ments, in order to provide better feedback to
students. Once the exercises are defined, it
would be possible to submit them to a reposi-
tory on a web server, on a collaborative mode.13
A novel use of the applet version will allow
using it inline in web pages, instead of as a
separate application window. This will not only
allow the dynamic drawing of tree descriptions
specified as parameters to the Java applet (and
possibly tree animations), but also the insertion
of exercises within online course material. We
13Collaborative projects, such as the Papillon project
for multilingual lexical resources, show that this ap-
proach can work if submitters can also benefit from the
submissions of other contributors.
plan to use this for the tutorial of the program.
On the content side, several ideas have been
submitted and will be implemented depending
on time. Notably, it seems particularly inte-
resting to provide actual linguistic data from
corpora to students from which grammars
can be inferred, as in (Borin and Dahllof,
1999). A new exercise type will ask students to
write a grammar accounting for a given small
corpus, which could already be morphologically
annotated or not. Lexicons will be separated
from grammars, in order to make them reusable
when possible. Feature structures will also be
supported, both for the edition of grammars
and for the validation of syntactic derivations.
A number of new features concern the
graphical display of trees. Notably, it will
be possible to collapse or expand subtrees
(using the triangle notation), and to draw trees
top-down with the terminal symbols immedia-
tely under the non-terminal that dominates
them, or bottom-up with the terminal symbols
aligned horizontally.14 It will also be possible
to specify display properties (such as font and
color) at the level of nodes and subtrees, and to
export trees as bitmap files for easy inclusion
into documents like assignments and course
notes.
7 Perspectives and conclusions
One could think of many other features that
would probably make the program even more
useful for learning. We only mention a few and
we hope that OpenSource contributions will
extend the list.
A key aspect of this kind of support tool
certainly lies in the nature of the feedback that
is provided to students. We have already said
that the mode for defining exercises will allow
the teacher to specify possible wrong solutions
and to associate them with an appropriate
correction. An interesting extension would be
a mode where students could send the results
of their exercise session (possibly containing a
series of coherent exercises) to a supervisor by
14In the latter case, it will be possible to specify that
the trees be developed with an upright lefthand branch
and sloping righthand ones, as this layout is used in some
textbook and is therefore more familiar to students using
them.
email from the program. Then, the annotated
corrections of exercises could feed a database
and be reused in subsequent unsupervised
exercises. We think that there is indeed much
to be gained from past corrections, as shown in
the research on vicarious learning using past
dialogues between learners and their teachers
(Cox et al, 1999), which, incidentally, was also
based on the teaching of syntax.
The range of topics covered by the program
could be extended. The learning of syntax
could probably be supported by the integration
of parsers, which could be of particular interest
to computational linguistics students (see e.g.
(Meurers et al, 2002; van Halteren, 2002)).
The integration of generators would also allow
students to inspect the productions of their
grammars to attempt to identify why they
could overgenerate. Furthermore, we would like
to reuse what already exists for the morpholo-
gical analysis of words in terms of inflections
and derivations, as well as for compositional
semantic analysis.
The program we have presented puts a par-
ticular emphasis on its central users, who are
students in (computational) linguistics. Initial
evaluation has shown that this kind of support
was very welcome by the learners? community,
and we hope that it will be more widely adopted
by the teachers? community in its new version
that attempts to reduce known limitations. We
look forward to new developments in the field
of research in computer-assisted learning, and
in particular on methodologies for the evalua-
tion of systems.
Acknowledgements
Many thanks go to the people who have directly
contributed to this unfunded project on a vo-
lunteer basis, in particular Se?verine Gedzelman
and Be?ne?dicte Grizolle for their work on the
new version of the program, and A`gnes Sandor,
Su-Ying Hsiao, Tanja Hieber, Susana Sotelo
Doc??o, Thierry van Steenberghe, Nicola Can-
cedda and Christophe Terrasson for their con-
tribution. Many thanks also to Lisa Travis and
Nathan Friedman from McGill University, and
to all the students and teachers who have sent
encouraging feedback on their use of the tool.
References
Lars Borin and Mats Dahllof. 1999. A Corpus-
Based Grammar Tutor for Education in Lan-
guage and Speech Technology. In Proceedings
of the workshop Computer and Internet sup-
ported education in language and speech tech-
nology, EACL?99, Bergen, Norway.
Richard Cox, Jean McKendree, Richard Tobin,
John Lee, and Terry Mayes. 1999. Vicarious
learning from dialogue and discourse. Journal
of Instructional Science, 27:431?458.
Dafydd Gibbon and Julie Carson-Berndsen.
1999. Web tools for introductory computa-
tional linguistics. In Proceedings of the work-
shop Computer and Internet supported ed-
ucation in language and speech technology,
EACL?99, Bergen, Norway.
Richard K. Larson. 1996. Grammar as a labo-
ratory science. In Presented at the American
Association for the Advancement of Science
Meetings, Special Session ?From Curiousity
to Science Through Linguistic Inquiry? Bal-
timore, U.S.A.
W. Detmar Meurers, Gerald Penn, and Frank
Richter. 2002. A Web-based Instructional
Platform for Contraint-Based Grammar For-
malisms and Parsing. In Proceedings of the
ACL-02 Workshop on Effective Tools and
Methodologies for Teaching Natural Language
Processing and Computational Linguistics,
Philadelphia, U.S.A, pages 19?26.
William O?Grady and Michael Dobrovolsky.
1996. Contemporary Linguistic Analysis.
Copp Clarck, Toronto, 3rd edition.
Colin Phillips. 1998. Teaching Syntax with
Trees. GLOT International, 3.7.
Sun Microsystems. 1995. The Java program-
ming language. http://www.javasoft.com.
Hans van Halteren. 2002. Teaching NLP/CL
through Games: the Case of Parsing. In
Proceedings of the ACL-02 Workshop on Ef-
fective Tools and Methodologies for Teaching
Natural Language Processing and Computa-
tional Linguistics, Philadelphia, U.S.A, pages
1?9.
W3C. 2000. XML 1.0: The eXtensi-
ble Markup Language (2nd edition).
October 2000 W3C recommandation,
http://www.w3.org/TR/Rec-xml.
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 100?104,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?09
Alexandre Allauzen, Josep Crego, Aur?lien Max and Fran?ois Yvon
LIMSI/CNRS and Universit? Paris-Sud 11, France
BP 133, 91403 Orsay C?dex
firstname.lastname@limsi.fr
Abstract
This paper describes our Statistical Ma-
chine Translation systems for the WMT09
(en:fr) shared task. For this evaluation, we
have developed four systems, using two
different MT Toolkits: our primary sub-
mission, in both directions, is based on
Moses, boosted with contextual informa-
tion on phrases, and is contrasted with a
conventional Moses-based system. Addi-
tional contrasts are based on the Ncode
toolkit, one of which uses (part of) the En-
glish/French GigaWord parallel corpus.
1 Introduction
This paper describes our Statistical Machine
Translation systems for the WMT09 (en:fr) shared
task. For this evaluation, we have developed four
systems, using two different MT toolkits: our
primary submission, in both direction, is based
on Moses, boosted with contextual information
on phrases; we also provided a contrast with a
vanilla Moses-based system. Additional contrasts
are based on the N-code decoder, one of which
takes advantage of (part of) the English/French Gi-
gaWord parallel corpus.
2 System architecture and resources
In this section, we describe the main characteris-
tics of the baseline phrase-based systems used in
this evaluation and the resources that were used to
train our models.
2.1 Pre- and post-processing tools
All the available textual corpora were processed
and normalized using in-house text processing
tools. Our last year experiments (D?chelotte et
al., 2008) revealed that using better normalization
tools provides a significant reward in BLEU, a fact
that we could observe again this year. The down-
side is the need to post-process our outputs so as
to ?detokenize? them for scoring purposes, which
is unfortunately an error-prone process.
Based again on last year?s experiments, our sys-
tems are built in ?true case?: the first letter of each
sentence is lowercased when it should be, and the
remaining tokens are left as is.
Finally, the N-code (see 2.5) and the context-
aware (see 3) systems require the source to be
morpho-syntactically analysed. This was per-
formed using the TreeTagger1 for both languages.
2.2 Alignment and translation models
Our baseline translation models (see 2.4 and 2.5)
use all the parallel corpora distributed for this eval-
uation: Europarl V4, news commentary (2006-
2009) and the additional news data, totalling 1.5M
sentences. Our preliminary attempts with larger
translation models using the GigaWord corpus are
reported in section 3.2. All these corpora were
aligned with GIZA++2 using default settings.
2.3 Language Models
To train our language models (LMs), we took ad-
vantage of the a priori information that the test
set would be of newspaper/newswire genre. We
1http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger.
2http://www.fjoch.com/GIZA++.html.
100
Source Period M. words
News texts 1994-06 3 317
En BN transcripts 2000-07 341
WMT 86
Newswires 1994-07 723
Newspapers 1987-06 486
Fr WEB 2008 23
WMT 46
News-train08 167
Table 1: Corpora used to train the target language
models in English and French.
thus built much larger LMs for translating both to
French and to English, and optimized their combi-
nation on the first part of the official development
data (dev2009a).
Corpora and vocabulary Statistics regarding
the training material are summarized in table 1 in
terms of source, time period, and millions of oc-
currences. ?WMT? stands for all text provided
for the evaluation. Development sets and the large
training corpora (news-train08 and the GigaWord
corpus) were not included. Altogether, these data
contain a total number of 3.7 billion tokens for En-
glish and 1.4 billion tokens for French.
To estimate such large LMs, a vocabulary was
first defined for both languages by including all to-
kens in the WMT parallel data. This initial vocab-
ulary of 130K words was then extended by adding
the most frequent words observed in the additional
training data. This procedure yielded a vocabulary
of one million words in both languages.
Language model training The training data
were divided into several sets based on dates on
genres (resp. 7 and 9 sets for English and French).
On each set, a standard 4-gram LM was estimated
from the 1M word vocabulary with in-house tools
using absolute discounting interpolated with lower
order models. The resulting LMs were then lin-
early interpolated using interpolation coefficients
chosen so as to minimise perplexity of the devel-
opment set (dev2009a). Due to memory limita-
tions, the final LMs were pruned using perplexity
as pruning criterion.
Out of vocabulary word and perplexity To
evaluate our vocabulary and LMs, we used the of-
ficial devtest and test sets. The out-of-vocabulary
(OOV) rate was drastically reduced by increasing
the vocabulary size, the mean OOV rate decreas-
ing from 2.5% to 0.7%, a trend observed in both
languages.
For French, using a small LM trained on the
"WMT" data only resulted in a perplexity of 301
on the devtest corpus and 299 on the test set. Us-
ing all additional data yielded a large decrease in
perplexity (106 on the devtest and 108 on the test);
again the same trend was observed for English.
2.4 A Moses baseline
Our baseline system was a vanilla phrase-based
system built with Moses (Koehn et al, 2007) us-
ing default settings. Phrases were extracted using
the ?grow-diag-final-and? heuristics, using a max-
imum phrase length of 7; non-contextual phrase
scores contain the 4 translation model scores, plus
a fixed phrase penalty; 6 additional scores param-
eterize the lexicalized reordering model. Default
decoding options were used (20 alternatives per
phrase, maximum distortion distance of 7, etc.)
2.5 A N-code baseline
N-code implements the n-gram-based approach
to Statistical Machine Translation (Mari?o et al,
2006). In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training
such a model requires to reorder source sentences
so as to match the target word order. This is also
performed via a stochastic finite-state reordering
model, which uses part-of-speech information to
generalise reordering patterns beyond lexical reg-
ularities. The reordering model is trained on a ver-
sion of the parallel corpora where the source sen-
tences have been reordered via the unfold heuris-
tics (Crego and Mari?o, 2007). A conventional n-
gram language model of the target language pro-
vides the third component of the system.
In all our experiments, we used 4-gram reorder-
ing models and bilingual tuple models built using
Kneser-Ney backoff (Chen and Goodman, 1996).
The maximum tuple size was also set to 7.
2.6 Tuning procedure
The Moses-based systems were tuned using the
implementation of minimum error rate train-
ing (MERT) (Och, 2003) distributed with the
Moses decoder, using the development corpus
(dev2009a). For the context-less systems, tun-
ing concerned the 14 usual weights; tuning the
101
22 weights of the context-aware systems (see 3.1)
proved to be much more challenging, and the
weights used in our submissions are probably far
from optimal. The N-code systems only rely on
9 weights, since they dispense with the lexical re-
ordering model; these weights were tuned on the
same dataset, using an in-house implementation of
the simplex algorithm.
3 Extensions
3.1 A context-aware system
In phrase-based translation, source phrases are
translated irrespective of their (source) context.
This is often not perceived as a limitation as
(i) typical text domains usually contain only few
senses for polysemous words, thus limiting the
use of word sense disambiguation (WSD); and (ii)
using long-span target language models (4-grams
and more) often capture sufficient context to se-
lect the more appropriate translation for a source
phrase based on the target context. In fact, at-
tempts at using source contexts in phrase-based
SMT have to date failed to show important gains
on standard evaluation test sets (Carpuat and Wu,
2007; Stroppa et al, 2007; Gimpel and Smith,
2008; Max et al, 2008). Importantly, in all con-
ditions where gains have been obtained, the tar-
get language was the ?morphologically-poor? En-
glish.
Nonetheless, there seems to be a clear consen-
sus on the importance of better exploiting source
contexts in SMT, so as to improve phrase disam-
biguation. The following sentence extract from
the devtest corpus is a typical example where the
lack of context in our phrase-based system yields
an incorrect translation:
Source: the long weekend comes with a price . . .
Target: Le long week-end vient avec un prix . . .
(the long weekend comes accompanied by a price)
While grammatically correct, the French trans-
lation sounds unnatural, and getting the correct
meaning requires knowledge of the idiom in the
source language. In such a situation, the right con-
text of the phrase comes with can be successfully
used to propose a better translation.3
From an engineering perspective, integrating
context into phrase-based SMT systems can be
performed by (i) transforming source words into
unique tokens, so as to record the original context
3Our context-aware phrase-based system indeed proposes
the appropriate translation: Le long week-end a un prix.
of each entry of the phrase table; and by (ii) adding
one or several contextual scores to the phrase ta-
ble. Using standard MERT, the corresponding
weights can be optimized on development data.
A typical contextual score corresponds to
p(e|f , C(f)), where C(f) is some contextual in-
formation about the source phrase f . An exter-
nal disambiguation system can be used to pro-
vide one global context score (Stroppa et al, 2007;
Carpuat and Wu, 2007; Max et al, 2008)); alter-
natively, several scores based on single features
can be estimated using relative frequencies (Gim-
pel and Smith, 2008):
p(e|f , C(f)) =
count(e, f , C(f))
?
e? count(e?, f , C(f))
For these experiments, we followed the latter ap-
proach, restricting ourselves to features represent-
ing the local context up to a fixed distance d (using
the values 1 and 2 in our experiments) from the
source phrase f endstart:
? lexical context features:
? left context: p(e|f , f start?1start?d )
? right context: p(e|f , f end+dend+1 )
? shallow syntactic features (denoting tF1 the
sequence of POS tags for the source sen-
tence):
? left context: p(e|f , tstart?1start?d)
? right context: p(e|f , tend+dend+1)
As in (Gimpel and Smith, 2008), we filtered out
all translations for which p(e|f) < 0.0002. This
was necessary to make score computation practi-
cal given our available hardware resources.
Results on the devtest corpus for
English?French were similar for the context-
aware phrase-based and the baseline phrase-based
system; small gains were achieved in the reverse
direction (see Table 2). The same trend was
observed on the test data.
Manual inspection of the output of the base-
line and context-aware systems on the devtest
corpus for English?French translation confirmed
two facts: (1) performing phrase translation dis-
ambiguation is only useful if a more appropriate
translation has been seen during training ; and (2)
phrase translation disambiguation can capture im-
portant source dependencies that the target lan-
guage model can not recover. The following ex-
102
ample, involving an unseen sense4 (ball in the se-
mantic field of dance rather than sports), illus-
trates our first remark:
Source: about 500 people attended the ball .
Baseline : Environ 500 personnes ont assist? ? la
balle.
+Context: Environ 500 personnes ont particip? ?
la balle.
The next example is a case where contextual in-
formation helped selecting an appropriate transla-
tion, in constrast to the baseline system.
Source: . . . the new method for calculating pen-
sions due to begin next year . . .
Baseline : . . . le nouveau mode de calcul des pen-
sions due ? commencer l?ann?e prochaine . . .
+Context: . . . la nouvelle m?thode de calcul des
pensions qui va d?buter l?ann?e prochaine . . .
3.2 Preliminary experiments with the
GigaWord parallel corpus
One exciting novelty of this year?s campaign was
the availability of a very large parallel corpus for
the en:fr pair, containing about 20M aligned sen-
tences.
Our preliminary work consisted in selecting the
most useful pairs of sentences, based on their av-
erage perplexity, as computed on our develop-
ment language models. The top ranking sen-
tences (about 8M sentences) were then fed into the
usual system development procedure: alignment,
reordering (for the N-code system), phrase pair
extraction, model estimation. Given the unusual
size of this corpus, each of these steps proved
extremely resource intensive, and, for some sys-
tems, actually failed to complete. Contrarily, the
N-code systems, conceptually simpler, proved to
scale nicely.
Given the very late availability of this cor-
pus, our experiments were very limited and we
eventually failed to deliver the test submissions
of our ?GigaWord? system. Preliminary exper-
iments using the N-code systems (see Table 2),
however, showed a clear improvement of perfor-
mance. There is no reason to doubt that similar
gains would be observed with the Moses systems.
3.3 Experiments
The various systems presented above were all de-
veloped according to the same procedure: train-
ing used all the available parallel text; tuning was
4This was confirmed after careful inspection of the phrase
tables of the baseline system.
en ? fr fr ? en
Moses Ncode Moses Ncode
small LM 20.06 18.98 21.14 20.41
Large LM 22.93 21.95 22.20 22.28
+context 23.06 22.69
+giga 23.21 23.14
Table 2: Results on the devtest set
performed on dev2009a (1000 sentences), and our
internal tests were performed on dev2009b (1000
sentences). Results are reported in table 2.
Our primary submission corresponds to
the +context entry, our first contrast to
Moses+LargeLM, and our second contrast to
Ncode+largeLM. Due to lack of time, no official
submission was submitted for the +giga variant.
For the record, the score we eventually obtained
on the test corpus was 26.81, slightly better than
our primary submission which obtained a score of
25.74 (all these numbers were computed on the
complete test set).
4 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT?09 shared task. We
used last year experiments to build competitive
systems, which greatly benefited from in-house
normalisation and language modeling tools.
One motivation for taking part in this campaign
was to use the GigaWord corpus. Even if time did
not allow us to submit a system based on this data,
it was a interesting opportunity to confront our-
selves with the technical challenge of scaling up
our system development tools to very large paral-
lel corpora. Our preliminary results indicate that
this new resource can actually help improve our
systems.
Naturally, future work includes adapting our
systems so that they can use models learnt from
corpora of the size of the GigaWord corpus. In
parallel, we intend to keep on working on context-
aware systems to study the impact of more types
of scores, e.g. based on grammatical dependencies
as in (Max et al, 2008). Given the difficulties we
had tuning our systems, we feel that a preliminary
task should be improving our tuning tools before
addressing these developments.
103
Acknowledgments
This work was partly realised as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
References
M. Carpuat and D. Wu. 2007. Context-Dependent
Phrasal Translation Lexicons for Statistical Machine
Translation. In Proceedings of Machine Translation
Summit XI, pages 73?80, Copenhagen, Denmark.
F. Casacuberta and E. Vidal. 2004. Machine transla-
tion with inferred stochastic finite-state transducers.
Computational Linguistics, 30(3):205?225.
S. F. Chen and J. T. Goodman. 1996. An empirical
study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 310?318, Santa Cruz, NM.
J. M. Crego and J. B. Mari?o. 2007. Improving
SMT by coupling reordering and decoding. Ma-
chine Translation, 20(3):199?215.
D. D?chelotte, G. Adda, A. Allauzen, O. Galibert, J.-L.
Gauvain, H. Meynard, and F. Yvon. 2008. Limsi?s
statistical translation systems for WMT?08. In Pro-
ceedings of the NAACL-HTL Statistical Machine
Translation Workshop, pages 107-100, Columbus,
Ohio.
K. Gimpel and N. A. Smith. 2008. Rich Source-Side
Context for Statistical Machine Translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 9?17, Columbus, Ohio.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session, Prague, Czech Republic.
A. Max, R. Makhloufi, and P. Langlais. 2008. Explo-
rations in using grammatical dependencies for con-
textual phrase translation disambiguation. In Pro-
ceedings of EAMT, poster session, Hamburg, Ger-
many.
J. B. Mari?o, R. E. Banchs R, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M. R. Costa-
Juss?. 2006. N-gram-based machine translation.
Computational Linguistics, 32(4):527?549.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In Proceedings of the 11th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI?07),
pages 231?240, Sk?vde, Sweden.
104
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 18?26,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Sub-sentential Paraphrasing by Contextual Pivot Translation
Aure?lien Max
LIMSI-CNRS
Universite? Paris-Sud 11
Orsay, France
aurelien.max@limsi.fr
Abstract
The ability to generate or to recognize
paraphrases is key to the vast majority of
NLP applications. As correctly exploit-
ing context during translation has been
shown to be successful, using context in-
formation for paraphrasing could also lead
to improved performance. In this arti-
cle, we adopt the pivot approach based
on parallel multilingual corpora proposed
by (Bannard and Callison-Burch, 2005),
which finds short paraphrases by finding
appropriate pivot phrases in one or several
auxiliary languages and back-translating
these pivot phrases into the original lan-
guage. We show how context can be ex-
ploited both when attempting to find pivot
phrases, and when looking for the most
appropriate paraphrase in the original sub-
sentential ?envelope?. This framework al-
lows the use of paraphrasing units ranging
from words to large sub-sentential frag-
ments for which context information from
the sentence can be successfully exploited.
We report experiments on a text revision
task, and show that in these experiments
our contextual sub-sentential paraphrasing
system outperforms a strong baseline sys-
tem.
1 Introduction
The ability to generate or to recognize paraphrases
is key to the vast majority of NLP applications.
Most current research efforts on paraphrase gener-
ation attempt to push the limits of their respective
methods and resources without recourse to deep
meaning interpretation, an admitedly long-term
research objective. A step towards meaning-aware
paraphrasing can be done by appropriate use of the
context in which a paraphrasing occurrence oc-
curs. At the lowest level, deciding automatically
when a word can be substituted with a synonym is
a complex issue (Connor and Roth, 2007). When
attempting paraphrasing on a higher level, such as
arbitrary phrases or full sentences (Barzilay and
Lee, 2003; Pang et al, 2003; Quirk et al, 2004;
Bannard and Callison-Burch, 2005; Zhao et al,
2008a), a first issue concerns the acquisition of el-
ementary units, which in the general case do not
exist in predefined dictionaries. Some paraphras-
ing strategy must then follow, which may consider
the context of a substitution to guide the selection
of appropriate units (Callison-Burch, 2008; Max,
2008). An important limitation to this family of
works is the scarcity of corpora that can be used as
reliable supervised training data. Indeed, strictly
parallel sentence pairs, for instance, are not nat-
urally produced in human activities.1 As a con-
sequence, works on paraphrasing have recourse to
costly human evaluation procedures, and an objec-
tive of automatic evaluation metrics is to rely on
as little gold standard data as possible (Callison-
Burch et al, 2008).
A text revision task is an application of para-
phrase generation where context may be used in
an effective way. When a local change is made to
a text, it occurs within a textual ?envelope? within
which a paraphrase should fit. In particular, if the
original sentence was grammatical, the substituted
sentence should remain grammatical and convey
essentially the same meaning.2 The manner in
which such a context can be exploited depends
of course on the type of automatic paraphrasing
technique used. In this article, we adopt the pivot
1Recent works such as (Nelken and Yamangil, 2008)
have proposed mining the revision histories of collabora-
tive authoring resources like Wikipedia, offering interesting
prospects in paraphrasing and rewriting studies.
2We posit here that the revision activity does not involve
important semantic changes, as opposed to the rewriting ac-
tivity. In future work, we will attempt to consider cases of
paraphrasing involving meaning changes corresponding to
textual entailment phenomena.
18
approach based on parallel multilingual corpora
proposed by (Bannard and Callison-Burch, 2005),
which finds short paraphrases by finding appropri-
ate pivot phrases in one or several auxiliary lan-
guages and back-translating these pivot phrases
into the original language. We show how con-
text can be exploited both when attempting to find
pivot phrases, and when looking for the most ap-
propriate paraphrase in the original sub-sentential
envelope. This framework allows the use of para-
phrasing units ranging from words to large sub-
sentential fragments for which context informa-
tion from the sentence can be successfully ex-
ploited.
This article is organized as follows. In section 2,
we briefly review related work in paraphrasing and
context-aware Machine Translation. We describe
the main characteristics of our approach to sub-
sentential paraphrasing in section 3. We then de-
scribe an evaluation protocol for evaluating our
proposal and report the results of a human evalua-
tion in section 4. We finally conclude and present
our future work in section 5.
2 Related work
Different sources have been considered for para-
phrase acquisition techniques. (Pang et al, 2003),
for example, apply syntactic fusion to multiple
translations of individual sentences. (Barzilay and
Lee, 2003; Dolan et al, 2004) acquire short para-
phrases from comparable corpora, while (Bha-
gat and Ravichandran, 2008) considered the is-
sue of acquiring short paraphrase patterns from
huge amounts of comparable corpora. (Bannard
and Callison-Burch, 2005) introduced a pivot ap-
proach to acquire short paraphrases from multi-
lingual parallel corpora, a resource much more
readily available than their monolingual counter-
part. (Zhao et al, 2008b) acquire paraphrase pat-
terns from bilingual corpora and report the vari-
ous types obtained.3 (Callison-Burch, 2008) im-
proves the pivot paraphrase acquisition technique
by using syntactic constraints at the level of con-
stituents during phrase extraction. This works also
uses syntactic constraints during phrase substitu-
tion, resulting in improvements in both grammat-
3The types of their paraphrase patterns are the follow-
ing (numbers in parentheses indicate frequency in their
database): phrase replacements (267); trivial changes (79);
structural paraphrases (71); phrase reorderings (56); and ad-
dition of deletion of information that are claimed to not alter
meaning (27).
icality and meaning preservation in a large-scale
experiment on English. (Max, 2008) explored the
use of syntactic dependency preservation during
phrase substitution on French.
This family of works considered the acquisi-
tion of short paraphrases and their use in local
paraphrasing of known units. Several works have
tackled full sentence paraphrasing as a monolin-
gual translation task relying on Statistical Ma-
chine Translation (SMT). For instance, (Quirk et
al., 2004) used a phrase-based SMT decoder that
uses local paraphrases acquired from compara-
ble corpora to produce monotone sentential para-
phrases. (Zhao et al, 2008a) acquired monolin-
gual biphrases from various sources and used them
with a phrase-based SMT decoder, and (Madnani
et al, 2007) combined rules of their hierarchical
decoders by pivot to obtain a monolingual gram-
mar. These works were not motivated by the gen-
eration of high-quality paraphrases that could, for
example, be reused in documents. The lack of
structural information, the local nature of the para-
phrasing performed and the fact that the context of
the original sentences was not taken into account
in the phrase-based approaches make it difficult to
control meaning preservation during paraphrasing.
Context has been shown to play a crucial role
in Machine Translation, where in particular proper
Word Sense Disambiguation (WSD) is required in
many cases. A variety of works have integrated
context with some success into phrase-based and
hierarchical decoders. For example, (Carpuat and
Wu, 2007) disambiguate phrases using a state-of-
the-art WSD classifier, and (Stroppa et al, 2007)
use a global memory-based classifier to find ap-
propriate phrase translations in context. Context
is often defined as local linguistic features such
as surrounding words and their part-of-speech, but
some works have experimented with more syntac-
tic features (e.g. (Gimpel and Smith, 2008; Max
et al, 2008; Haque et al, 2009)).
Using an intermediate pivot language with
bilingual translation in which a given language
pair is low-resourced has led to improvements
in translation performance (Wu and Wang, 2007;
Bertoldi et al, 2008), but to our knowledge this ap-
proach has not been applied to full sentence para-
phrasing. Several reasons may explain this, in par-
ticular the relative low quality of current MT ap-
proaches on full sentence translation, and the diffi-
culties in controlling what is paraphrased and how.
19
3 Contextual pivot SMT for
sub-sentential paraphrasing
Although many works have addressed the issue of
local paraphrase acquisition, effective use of such
paraphrases for paraphrase generation has only be
achieved at the level of text units corresponding
to short contiguous phrases. Recent works have
proposed approaches to exploit context in order
to correctly replace a text fragment with a para-
phrase, but they are limited to known text units
and therefore suffer from a scarcity of data.4
In this work, we address the case of sub-
sentential paraphrase generation, an intermediate
case between local paraphrasing using text units
for which paraphrases are available and full sen-
tence paraphrasing. Data sparcity is addressed by
using a pivot translation mechanism, which can
produce back-translations for text fragments for
which paraphrases cannot be acquired beforehand
by some paraphrase acquisition technique. Sub-
sentential paraphrasing by pivot allows the ex-
ploitation of context during both source-to-pivot
translation, where the source context is avail-
able, and during pivot-to-source back-translation,
where the target context is known. The success
of this approach is then directly dependent on the
availability of high quality MT engines and on
their ability to exploit these source and target con-
texts.
3.1 Paraphrasing by pivot translation
Whereas attempts at using two translation sys-
tems in pivot have met with some success for low-
resourced language pairs, it is unlikely that cur-
rent SMT systems can be successfully called in
succession to obtain high-quality sentential para-
phrases.5 Several works have shown that mono-
lingual biphrases obtained by multilingual pivots
can be used by decoders, but although gains can
for example be obtained by using sentential para-
phrases as alternative reference corpora for opti-
mizing SMT systems (Madnani et al, 2007), re-
sulting paraphrases seem to be of too low quality
4Current approaches based on paraphrase patterns are
only a partial solution to this issue, as the variables used are
limited to simple types.
5In particular, back-translation can introduce lexical er-
rors due to incorrect word sense disambiguation and there-
fore severely hamper understanding, as illustrated by the in-
famous MT textbook example of the sentence The spirit is
willing but the flesh is weak being translated into Russian and
back-translated into English as The vodka is good, but the
meat is rotten.
for most other possible application contexts. In
this work, we propose to use a pivot approach from
a source language to a pivot language and back to
the source language, but for sub-sentential frag-
ments. In this way, the source context in which
they occur can be exploited for both translating
into the pivot language and for back-translating
into the original language. This is illustrated on
Figure 1.
Step (1) performs a N-best decoding (a single
example is shown here) in which a segmentation
of the source sentence is forced to ensure that
a given fragment (mettre en danger la richesse
e?cologique in the example) is translated indepen-
dently of its surrounding context.6 Only trans-
lations which respect this segmentation are kept,
yielding a variety of pivot sentences. We are
mostly interested in the pivot translation of our
paraphrased fragment, but its prefix and suffix
pivot context can be exploited by contextual SMT
to guide pivot-to-source translation, although the
lower quality of automatically generated sentences
might not help as much as before.
Step (2) produces from each obtained N-best
hypothesis a new N-best list of hypotheses, this
time in the source language. The decoder is once
more asked to use a given segmentation, and is fur-
ther given imposed translations for the pivot pre-
fix and suffix, as shown by the arrows going di-
rectly from the sentence at the top to the sentence
at the bottom of Figure 1. Step (2) can be fol-
lowed by a reranking procedure on the obtained
N-best list of hypotheses, whose individual score
can be obtained by combining the scores of the
two translation hypotheses that led to it. As op-
posed to the pivot approach for phrases of (Ban-
nard and Callison-Burch, 2005), it is not possi-
ble to sum over all possible pivots for a given
pair ?original sentence, paraphrased sentence?, as
the search space would make this computation im-
practical. We can instead look for the paraphrase
that maximizes the product of the probabilities of
the two translation steps according to the scores
produced by the decoders used.
A further step can eliminate paraphrases by ap-
plying heuristics designed to define sought or un-
desirable properties for paraphrases, although this
6It is in fact incorrect to say that translation of the vari-
ous fragments would take place independently of each other,
as various models such as a source context models or target
language models will use information from surrounding frag-
ments.
20
Figure 1: Example of sub-sentential paraphrasing by contextual pivot translation
could be directly integrated in the reranking step.
For example, we may not be interested by identity
paraphrases, or by paraphrases including or being
included in the original fragment, or we may pre-
fer paraphrases in which a given word has been
replaced, etc.
3.2 Source context for pivot SMT
Using the context of a phrase is necessary to trans-
late it correctly, most notably when several word
senses corresponding to distinct translations are
involved. The following examples show a case of
a polysemous English word, which can be trans-
lated into three distinct French words and back-
translated into various English fragments:
? Follow the instructions outlined below to
save that file. ? sauvegarder ce fichier ?
write the file on disk
? Quitting smoking is a sure-fire way to save
some money. ? e?conomiser de l?argent ?
have some money on your bank account
? Brown?s gamble may save the banks but the
economy cannot wait. ? sauver les banques
? salvage the banks
Our approach for source context aware SMT,
based on that of (Stroppa et al, 2007), is illus-
trated by the system architecture on Figure 2. A
memory-based classification approach was cho-
sen as it allows for efficient training with large
example sets, can handle any number of output
classes and produces results that can be directly
used to estimate the required conditional probabil-
ities. We add context-informed features to the log-
linear framework of our SMT system based on the
conditional probability of a target phrase e
i
given
a source phrase f
i
and its context, C(f
i
):
h
m
(f
i
, C(f
i
), e
i
) = logP (e
i
|f
i
, C(f
i
))
Figure 2: Architecture of our contextual phrase-
based SMT system
21
Memory-based classification performs implicit
smoothing, which addresses in part the problem
of data sparcity, which worsen with the inclu-
sion of context features and makes direct estima-
tion of those probabilities problematic. Given a
fixed-length vector, ?f
i
, C(f
i
)?, a set of weighted
class labels corresponding to target phrases is
returned by the classifier, which give access to
P (e
i
|f
i
, C(f
i
)) after normalization.
Because each source phrase potentially occurs
in a unique context, they must be given a unique
entry in the phrase table. To this end, we added
a preprocessor component whose role is to dy-
namically build a modified source file containing
unique tokens and to produce a modified trans-
lation table containing those tokens. Phrase ex-
traction uses both phrase alignment results and
linguistic analysis of the source corpus to pro-
duce standard biphrases and biphrases with con-
textual information. The latter are used to train the
memory-based classifier. The source file under-
goes the same linguistic analysis whose output is
then aligned to unique tokens (e.g. president@45),
and each possible phrase which is also present in
the standard translation table is classified using its
context information. The output is used to create a
set of entries in the contextual translation tables, in
which a new score corresponding to our context-
based feature are added.
Most existing context-aware SMT approaches
rely on context features from the immediate con-
text of a source phrase. In this work, we initially
restricted ourselves to a limited set of features: up
to two lemmas to the left and to the right of a seg-
ment and their part-of-speech.7
3.3 Target context for pivot SMT
When decoding from the pivot hypothesis, we
force our decoder to use provided sentence pre-
fix and suffix corresponding to the ?envelope? of
the original fragment. Target context will thus be
taken into account by the decoder.
Furthermore, based on the hypothesis that a
paraphrase for an unmodified envelope should pre-
serve the syntactic dependencies between the para-
phrased fragment and its envelope (inter-fragment
dependencies), we optionaly add a ?hard? rerank-
ing step where we filter the N-best list of hypothe-
7We will integrate richer syntactic context as in (Gimpel
and Smith, 2008; Max et al, 2008) in our short-term future
work, as we expect it to be particularly useful for our para-
phrasing task.
ses to keep only those which preserve these depen-
dencies. Note however that for a dependency to be
marked as preserved, we only need to find its label
and its target word in the envelope (governor or de-
pendent), as the word in the paraphrased fragment
might have changed. This of course has practical
implications on the nature of the paraphrases that
can be produced.
In part due to various deficiencies of phrase
alignments discussed in (Callison-Burch, 2008),
we further apply heuristics to filter out some un-
desirable paraphrase candidates. Our current set
of heuristics includes:
? no reordering should have taken place be-
tween the original source phrase and its con-
text8;
? considering the set of full word lemmas for
the original fragment and the paraphrased
fragment, at least one lemma should not be-
long to both sets9;
? neither the original fragment nor its para-
phrase must be included into the other (only
taking full words into account).
4 Experiments
We have conducted experiments motivated by a
text revision task that we report in this section
by describing our baseline and context-aware sub-
sentential paraphrasing systems and the results of
a small-scale manual evaluation.
4.1 Data and systems
We built two-way French-English SMT sys-
tems using 188,115 lines of the Europarl cor-
pus (Koehn, 2005) of parliamentary debates with
moses (Koehn et al, 2007) 10. Our corpus was
analyzed by the XIP robust parser (A??t-Mokhtar
et al, 2002) and its output tokenization was used.
We built standard systems, as well as a contextual
system for French?English as described in sec-
tion 3.2 using an additional contextual score ob-
8Reordering is allowed in the paraphrased fragment.
9As a consequence, minimal paraphrases may differ by
only one full word. This can however be used advantageously
when the sought type of paraphrasing aims at ?normalizing?
a text fragment and when the most appropriate rewording is
very similar to an original text fragment.
10We used revision 2234 available on the moses SVN web-
site: http://mosesdecoder.sourceforge.net/
svn.php. In particular, it allows the use of XML annota-
tions to guide the translation of particular fragments.
22
Baseline fr?en 30.56
Contextual fr?en 31.17
Baseline en?fr 32.10
Table 1: BLEU scores for the translation systems
used by our paraphrasing system
tained through memory-based classification per-
formed with the TiMBL package (Daelemans et
al., 2007). Standard MERT was used to optimize
model weights. BLEU scores for the three systems
are reported on Table 1. The contextual system
obtains a slightly higher score than the baseline
system, which can participate to some extent to a
better exploitation of context for paraphrasing.11
Two paraphrasing systems we built: S
bas
is a
baseline system which uses standard phrase tables
and post-filtering heuritics, but does not include
reranking based on syntactic dependencies. S
cont
is a contextual system which uses the contex-
tual French?English translation system, rerank-
ing based on syntactic dependencies and post-
filtering heuristics.
We used 1000-best lists of hypotheses for the
source-to-pivot translation, and restricted our-
selves to much smaller 10-best lists for pivot-
to-source translation (integrating early more con-
straints directly into decoding could help in ob-
taining better and smaller N-best lists).12
4.2 Evaluation protocol
A native speaker was asked to study a held-out test
file of Europarl data in French and to identify at
most one fragment per sentence that would be a
good candidate for revision and for which the an-
notator could think of reasonable paraphrases that
did not involve changes to the envelope. Candidate
fragments were accepted if they were not found in
the French?English translation table. This step
resulted in a corpus of 151 sentences with as many
test fragments, with sizes ranging from 2 to 12
words, an average size of 5.38 words and a me-
dian size of 4 words.
Two native speakers, including the previous an-
notator, were asked to evaluate all paraphrased
sentences on grammaticality and meaning. Con-
trary to previous works, we decided to use a
11The unexpected worse performance of the fr?en system
may be explained by issues related to tokenization after anal-
ysis by the parser.
12In our future work, we intend to investigate the possible
use of lattices rather than N-best lists.
smaller evaluation scale with only 3 values, as
using more values tend to result in low inter-
annotator agreement:
? 2: indicates that the paraphrase is perfect or
almost perfect;
? 1: indicates that the paraphrase could become
grammatical with one minor change, or that
its meaning is almost clear;
? 0: indicates that more than one minor change
is required to make the paraphrase grammat-
ical or understandable.
4.3 Results and discussion
We ran both systems and took their one-best hy-
pothesis for evaluation. Table 2 shows the results
of a contrastive evaluation of the results obtained.
For the 143 sentences for which paraphrases could
be produced, we obtained 72 results common to
both systems, and 71 which were specific to each
system. The fact that for half of the cases the two
systems produced the same paraphrases reveals
that either context did not play an important role
in these cases, and/or that the search space was
rather limited due to the presence of rare words in
the original fragment. Systems S
cont
and S
bas
are
compared based on the number of cases were one
was found to be better or worse than the other for
the 71 cases where they proposed different para-
phrases, either by the two judges (denoted by the
< and > signs) or by one of the two judges while
the other found the two systems to be of compara-
ble performance (denoted by the ? and ? signs).
As can be seen from the table, there is a clear pref-
erence for our S
cont
system, with a 31:37 ratio of
cases where it is preferred for grammar, and 33:49
for meaning.
Table 3 shows absolute results for the same run
of evaluation. First, it can be noted that both sys-
tems perform at a reasonable level, both for short
and long text fragments. Several reasons may ex-
plain this: first, sentences to paraphrase are from
the same domain as the training corpora for our
SMT systems, which is a positive bias towards
the paraphrasing systems. Also, the post-filtering
heuristics and the fact that both systems could
benefit from the knowledge of the target enve-
lope during pivot-to-source back-translation cer-
tainly helped in filtering out incorrect paraphrases.
These results confirm the trend observed on con-
trastive results that our S
cont
system is the best
23
Scont
< S
bas
S
cont
? S
bas
S
cont
? S
bas
S
cont
> S
bas
? Total
Grammar 3 3 10 21 34 71
Meaning 3 13 13 20 22 71
Table 2: Contrastive results. The notation S
cont
< S
bas
stands for cases in which S
cont
is found to be
worse than S
bas
by both judges; S
cont
? S
bas
for cases where S
cont
was found to be worse by one judge
while the other found the two systems equivalent; similarly for other cases. ??? stands for cases where
judges disagreed.
count Grammar Meaning System
- + ? - + ? - + ?
S
bas
and S
cont
72 0 69 3 1 67 4 0 66 6
S
bas
only 71 13 46 12 18 41 12 9 39 23
S
cont
only 71 5 63 3 8 56 7 4 55 12
S
bas
: 2 ? size ? 5 81 6 69 6 10 63 8 4 61 16
S
cont
: 2 ? size ? 5 81 2 78 1 6 72 3 2 71 8
S
bas
: 6 ? size ? 12 62 7 46 9 9 45 8 5 44 13
S
cont
: 6 ? size ? 12 62 4 54 4 3 51 8 2 50 10
Table 3: Absolute results for manual evaluation. ?+? indicates that both judges agree on a positive
judgement (score 1 or 2), ?-? that both judges agree on a negative judgment (score 0), and ??? that judges
disagreed. ?System? judgments include judgments for both Grammar and Meaning.
performer for that task and that test set. It is
however noteworthy that results were significantly
better when they were produced by both systems,
which may correspond to the easiest cases with re-
spect to the training data and/or the task but also
suggests the application of consensus techniques
as done in MT system output combination.
Table 4 shows paraphrasing examples produced
by S
cont
. As can be noted from positive exam-
ples (a-c), the obtained paraphrases are mostly of
the same syntactic types as the original phrases,
which may be due to the proximity between the
main language and the pivot language, as well as
to the constraint on syntactic dependency preser-
vation. Example (a) shows a case of what may
be seen as some sort of normalization, as the con-
cept of ?confidence of people? (w.r.t. the English
pivot language) may be more frequently expressed
as la confiance des citoyens (citizens) than as la
confiance des gens (people) in the reference cor-
pus. Example (b), although showing a correct
paraphrasing, contains an agreement error which
is a result of the use of the gender neutral English
pivot and the fact that the language model used by
the pivot-to-source SMT system was not able to
choose the correct agreement. Example (c) illus-
trates a case of correct paraphrasing involving re-
ordering strongly influenced by the reordering re-
quired by the pivot language. The incorrect para-
phrase of example (d) mainly results from the in-
ability of the source-to-pivot SMT system to cor-
rectly translate the selected fragment; in particular,
the syntactic structure was not correctly translated,
and the noun palier (stage) was incorrectly trans-
lated as the verb heal and back-translated as the
verb traiter (heal, cure). Lastly, example (e) con-
tains an error in word sense disambiguation be-
tween the pivot noun act and the noun loi (law)13,
as well as the incorrect deletion of the adverb tre`s
fermement (firmly) during source-to-pivot transla-
tion.
Several conclusions can be drawn from these
results and observations. First, it is not surpris-
ing that the performance of the SMT systems used
has an important impact on the results. This can
be mitigated in several ways: by attempting para-
phrasing on in-domain sentences; by using an ap-
propriate pivot language with respect to the nature
of the text fragment to paraphrase; by using one or
several pivot languages (as proposed by (Bannard
and Callison-Burch, 2005) for phrase paraphras-
ing) and consensus on the obtained paraphrases.
13This example might call for better lexical checking be-
tween original and paraphrased sentences, as well as exploit-
ing context-aware SMT on the lower quality input of pivot-
to-source translation.
24
(a) En tant que parti de gauche, nous avons du?, avec beaucoup de peine, nous rendre compte que les institutions ne sont
pas des jeux de construction montables, transformables et de?montables a` souhait, mais qu?elles ont leur propre histoire
et doivent be?ne?ficier de la confiance des gens qui les soutiennent.
As the left, we have had, with a great deal of trouble, we see that the institutions are not games montables construction,
transformables de?montables and to wish, but they have their own history and must enjoy the confidence of people
who support them.
? doivent avoir la confiance des citoyens
(b) Monsieur le pre?sident, je suis inquie`te au sujet de l?attitude qui risque de se de?velopper au sein de l?UE concernant la
liberte? des e?changes.
Mr President, I am concerned about the attitude which might develop within the EU on free trade.
? je suis pre?occupe? par
(c) Ces accords constituent un cadre contractuel entie`rement nouveau pour les pays de la re?gion.
These agreements constitute an entirely new contractual framework for the countries of the region.
? un tout nouveau cadre contractuel
(d) Aujourd?hui, le durcissement paralle`le des inde?pendantistes albanais et des autorite?s serbes fait franchir a` la crise un
nouveau palier tre`s inquie?tant dans la monte?e des tensions.
Today, the inflexibility parallel with the Albanian independent and the Serbian authorities to overcome the crisis is a
new heal very worrying in the rise of tension.
? (*) de surmonter la crise est une nouvelle traiter tre`s pre?occupant
(e) La commission condamne tre`s fermement cet acte et prend note de la de?cision de constituer un comite? spe?cial au sein
de la fiscalia general de la nacio?n afin d?enque?ter sur cet assassinat.
The Commission condemn this act and takes note of the decision to set up a special committee fiscalia within the
general de la nacin in order to investigate this murder.
? (*) condamne cette loi
Table 4: Examples of sub-sentential paraphrasings produced by our S
cont
system.
Another remark is that our systems could be im-
proved as regards their ability to exploit source
context.14
5 Conclusion and future work
In this article, we have presented an approach to
obtain sub-sentential paraphrases by using pivot
SMT systems. Although our results showed that
we were able to build a strong baseline on our test
set, they also showed that integrating context both
when translating from source-to-pivot and when
back-translating from pivot-to-source can lead to
improved performance. Our approach has the dis-
tinctive feature that it targets text fragments that
can be larger than phrases traditionally captured
by statistical techniques, while not targeting full
sentences for which it would be harder to exploit
context successfully. More generally, it addresses
the case of the paraphrasing of text units for which
no paraphrases are directly available.
We have identified several issues in our exper-
iments that will constitute our future work. We
intend to experiment with several pivot languages
and to make them compete to obtain the N-best
lists, as done in some approaches to multisource
translation (Och and Ney, 2001) and/or to use a
consensus technique to select the best paraphrase.
14It should be noted, however, that the reported experi-
ments used relatively small amounts of training data as in
most comparable works on context-aware Machine Transla-
tion.
As regards our context-aware SMT systems, we
plan to exploit more complex syntactic knowledge
and to learn correspondances for inter-fragment
dependencies so as to make our rescoring based
on syntactic dependencies more flexible. We are
currently working on extracting revision instances
from Wikipedia?s revision history, which will pro-
vide us with a corpus of genuine revision occur-
rences as well as with an out-domain test corpus
with reference paraphrases. Lastly, we want to in-
vestigate the use of our approach for two types of
applications: text normalization, in which a text
is revised to select approved phraseology and ter-
minology, through the use of a carefully chosen
training corpus for our pivot-to-source SMT sys-
tem ; and interactive translation output revision for
cases with or without a source text for professional
translators or monolingual users.
Acknowledgments
This work was funded by a grant from LIMSI.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
cremental deep parsing. Natural Language Engi-
neering, 8(3):121?144.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL, Ann Arbor, USA.
25
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
NAACL/HLT, Edmonton, Canada.
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-based
statistical machine translation with pivot languages.
In Proceeding of IWSLT, pages 143?149, Hawaii,
USA.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL: HLT, Columbus,
USA.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. Parametric: An automatic evaluation
metric for paraphrasing. In Proceedings of COL-
ING, Manchester, UK.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, Hawai, USA.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statisti-
cal machine translation. In Proceedings of Machine
Translation Summit XI, Copenhagen, Denmark.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a single unsupervised classi-
fier. In Proceedings of ECML, Warsaw, Poland.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2007. TiMBL: Tilburg
Memory Based Learner, version 6.1, Reference
Guide. Technical report, ILK 07-xx. Available from
http://ilk.uvt.nl/downloads/pub/pap
ers/ilk0703.pdf.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of Coling 2004, pages 350?356,
Geneva, Switzerland.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proceedings of WMT at ACL, Columbus, USA.
Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma,
and Andy Way. 2009. Using supertags as source
language context in smt. In Proceedings of EAMT,
Barcelona, Spain.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, demo session, Prague, Czech
Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit, Phuket, Thailand.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of Workshop on Machine Translation at ACL,
Prague, Czech Republic.
Aure?lien Max, Rafik Makhloufi, and Philippe Langlais.
2008. Explorations in using grammatical dependen-
cies for contextual phrase translation disambigua-
tion. In Proceedings of EAMT, Hamburg, Germany.
Aure?lien Max. 2008. Local rephrasing suggestions for
supporting the work of writers. In Proceedings of
GoTAL, Gothenburg, Sweden.
Rani Nelken and Elif Yamangil. 2008. Mining
wikipedia?s article revision history for training com-
putational linguistics algorithms. In Proceedings of
the AAAI Workshop on Wikipedia and Artificial In-
telligence: An Evolving Synergy, Chicago, USA.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Proceedings of MT
Summit, Santiago de Compostela, Spain.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of NAACL/HLT, Edmonton,
Canada.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of EMNLP,
Barcelona, Spain.
Nicolas Stroppa, Antal van den Bosch, and Andy Way.
2007. Exploiting source similarity for smt using
context-informed features. In Proceedings of TMI,
Skvde, Sweden.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
Prague, Czech Republic.
Shiqi Zhao, Cheng Niu, Ming Zhou, and Sheng Li.
2008a. Combining multiple resources to improve
smt-based paraphrasing model. In Proceedings of
ACL-HLT, Columbus, USA.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL-HLT, Columbus, USA.
26
Towards Interactive Text Understanding 
Marc Dymetman* Aur?lien Max*+ Kenji Yamada* 
(*) Xerox Research Centre Europe, Grenoble  
(+) CLIPS-GETA, Universit? Joseph Fourier, Grenoble 
{marc.dymetman,aurelien.max,kenji.yamada@xrce.xerox.com} 
 
Abstract 
This position paper argues for an interactive 
approach to text understanding. The proposed 
model extends an existing semantics-based 
text authoring system by using the input text 
as a source of information to assist the user in 
re-authoring its content. The approach per-
mits a reliable deep semantic analysis by 
combining automatic information extraction 
with a minimal amount of human interven-
tion. 
1 Introduction 
Answering emails sent to a company by its cus-
tomers ? to take just one example among many 
similar text-processing tasks ? requires a reli-
able understanding of the content of incoming 
messages. This understanding can currently only 
be done by humans, and represents the main bot-
tleneck to a complete automation of the process-
ing chain: other aspects could be delegated to 
such procedures as database requests and text 
generation. Current technology in natural lan-
guage understanding or in information extraction 
is not at a stage where the understanding task can 
be accomplished reliably without human inter-
vention.  
In this paper, which aims at proposing a fresh 
outlook on the problem of text understanding 
rather than at describing a completed implemen-
tation, we advocate an interactive approach 
where:  
1. The building of the semantic representation 
is under the control of a human author;  
2. In order to build the semantic representa-
tion, the author interacts with an intuitive textual 
interface to that representation (obtained from it 
through an NLG process), where some ?active? 
regions of the text are associated with menus that 
display a number of semantic choices for incre-
menting the representation;  
3. The raw input text to be analyzed serves as 
a source of information to the authoring system 
and permits to associate likelihood levels with 
the various authoring choices; in each menu the 
choices are then ranked according to their likeli-
hood, allowing a speedier selection by the au-
thor; when the likelihood of a choice exceeds a 
certain threshold, this choice is performed auto-
matically by the system (but in a way that re-
mains revisable by the author).  
4. The system acts as a flexible understanding 
aid to the human operator: by tuning the thresh-
old at a low level, it can be used as a purely 
automatic, but somewhat unreliable, information 
extraction or understanding system; by tuning the 
threshold higher, it can be used as a powerful 
interactive guide to building a semantic interpre-
tation, with the advantage of a plain textual inter-
face to that representation that is easily 
accessible to general users. 
The paper is organized as follows. In section 
2, we present a document authoring system, 
MDA,  where the author constructs an internal 
semantic representation, but interacts with a tex-
tual realization of that representation. In section 
3, we explain how such a system may be ex-
tended into an Interactive Text Understanding 
(ITU) aid. A raw input document acts as an in-
formation source that serves to rank the choices 
proposed to the author according to their likeli-
hood of ?accounting? for information present in 
the input document. In section 4, we present cur-
rent work on using MDA for legacy-document 
normalization and show that this work can pro-
vide a first approach to an ITU implementation. 
In section 5, we indicate some links between 
these ideas and current work on interactive statis-
tical MT (TransType), showing directions to-
wards more efficient implementations of ITU. 
2 MDA: A semantics-based document au-
thoring system 
The MDA (Multilingual Document Authoring) 
system [Brun et al2000] is an instance (de-
scended from Ranta?s Grammatical Framework 
[Ranta 2002]) of a text-mediated interactive 
natural language generation system, a notion in-
troduced by [Power and Scott 1998] under the 
name of WYSIWYM. In such systems, an author 
gradually constructs a semantic representation, 
but rather than accessing the evolving representa-
tion directly, she actually interacts with a natural 
language text generated from the representation; 
some regions of the text are active, and corre-
spond to still unspecified parts of the representa-
tion; they are associated with menus presenting 
collections of choices for extending the semantic 
representation; the choices are semantically ex-
plicit and the resulting representation contains no 
ambiguities. The author thus has the feeling of 
only interacting with text, while in fact she is 
building a formal semantic object. One applica-
tion of this approach is in multilingual authoring: 
the author interacts with a text in her own lan-
guage, but the internal representation can be used 
to generate reliable translations in other lan-
guages. Fig. 1 gives an overview of the MDA 
architecture and Fig. 2 is a screenshot of the 
MDA interface. 
 
 
 
Fig. 1: Authoring in MDA. A ?semantic grammar? defines 
an enumerable collection of well-formed partial semantic 
structures, from which an output text containing active re-
gions is generated, with which the author interacts. 
 
 
 
 
Fig. 2: Snapshot of the MDA system applied to the author-
ing of drug leaflets.  
3 Interactive Text Understanding 
In the current MDA system, menu choices are 
ordered statically once and for all in the semantic 
grammar1. However, consider the situation of an 
author producing a certain text while using some 
input document as an informal reference source. 
It would be quite natural to assume that the au-
thoring system could use this document as a 
source of information in order to prime some of 
the menu choices.  
 
Thus, when authoring the description of a phar-
maceutical drug, the presence in the input docu-
ment of the words tablet and solution could serve 
to highlight corresponding choices in the menu 
corresponding to the pharmaceutical form of the 
drug. This would be relatively simple to do, but 
one could go further: rank menu choices and as-
sign them confidence weights according to tex-
tual and contextual hints found in the input 
document. When the confidence is sufficiently 
high, the choice could then be performed auto-
matically by the authoring system, which would 
produce a new portion of the output text, with the 
author retaining the ability of accepting or reject-
ing the system?s suggestion. In case the confi-
dence is not high enough, the author?s choice 
would still be sped up through displaying the 
most likely choices on top of the menu list. 
 
 
 
 
Fig. 3: Interactive Text Understanding. 
 
This kind of functionality is what we call a text-
mediated interactive text understanding system, 
or for short, an ITU system (see Fig. 3).2 
                                                          
1
 While the order between choices listed in a menu does not 
vary, certain choices may be filtered out depending on the 
current authoring context; this mechanism relies on unifica-
tion constraints in the semantic grammar.  
2
  Note that we do not demand that the semantic representa-
tion built with an ITU system be a complete representation 
of the input document, rather it can be a structured descrip-
tion of some thematic aspects of that document. Similarly, it 
is OK for the input document not to contain enough infor-
mation permitting the system or even the author to ?answer? 
certain menus: then some active regions of the output text 
remain unspecified. 
We will now consider some directions to im-
plement an ITU system.  
4 From document normalization to ITU 
A first route towards achieving an ITU system is 
through an extension of ongoing work on docu-
ment normalization [Max and Dymetman 2002, 
Max 2003]. The departure point is the following. 
Assume an MDA system is available for author-
ing a certain type of documents (for instance a 
certain class of drug leaflets), and suppose one is 
presented a ?legacy? document of the same type, 
that is, a document containing the same type of 
information, but produced independently of the 
MDA system; using the system, a human could 
attempt to ?re-author? the content of the input 
legacy document, thus obtaining a normalized 
version of it, as well as an associated semantic 
representation. 
An attempt to automate the re-authoring proc-
ess works as follows. Consider the virtual space 
of semantic representations enumerated by the 
MDA grammar. For each such representation, 
produce, through the standard MDA realization 
process3 a certain more or less rough ?descriptor? 
of what the input text should contain if its con-
tent should correspond to that semantic represen-
tation; then define a similarity measure between 
this descriptor and the input text; finally perform 
an admissible  heuristic search [Nilsson 1998] of 
the virtual space to find the semantics whose de-
scriptor has the best similarity with the input text. 
This architecture can accomodate more or less 
sophisticated descriptors: from bags of content-
words to be intersected with the input text, up to 
predicted ?top-down? predicate-argument tuples 
to be matched with ?bottom-up? tuples extracted 
from the input text through a rough information-
extraction process. 
Up to now the emphasis of this work has been 
more on automatic reconstruction of a legacy 
document than on interaction, but we have re-
cently started to think about adapting the ap-
proach to ITU. The heuristic search that we 
mentioned above associates with a menu choice 
an estimate of the best similarity score that could 
be obtained by some complete semantic structure 
extending that choice. It is then possible to rank 
choices according to that heuristic estimate (or 
some refinement of it obtained by deepening the 
                                                          
3
 Which was initially designed to produce parallel texts in 
several languages, but can be easily adapted to the produc-
tion of non-textual ?renderings? of the semantic representa-
tions. 
search a few steps down the line), and then to 
propose to the author a re-ranked menu. 
While we are currently pursuing this promis-
ing line of research because of its conceptual and 
algorithmic simplicity, it has some weaknesses. 
It relies on similarity scores between an input 
text and a descriptor that are defined in a some-
what ad hoc manner, it depends on parameters 
that are fixed a priori rather than by training, and 
it is difficult to associate with confidence levels 
having a clear interpretation.  
A way of solving these problems is to move 
towards a more probabilistic approach that com-
bines advantages of being built on accepted prin-
ciples and of having a well-developed learning 
theory. We finally turn our attention to existing 
work in this area that holds promise for improv-
ing ITU. 
5 Towards statistical ITU 
Recent research on the interactive statistical ma-
chine translation system TransType [Foster et al 
1997; Foster et al 2002] holds special interest in 
relation to ITU. This system, outlined in Fig. 4, 
aims at helping a translator type her (uncon-
strained) translation of a source text by predict-
ing sequences of characters that are likely to 
follow already typed characters in the target text; 
this prediction is done on the basis of informa-
tion present in the source text. The approach is 
similar to standard statistical MT4, but instead of 
producing one single best translation, the system 
ranks several completion proposals according to 
a probabilistic confidence measure and uses this 
measure to optimize the length of completions 
proposed to the translator for validation. Evalua-
tions of the first version of TransType have al-
ready shown significant gains in terms of the 
number of keystrokes needed for producing a 
translation, and work is continuing for making 
the approach effective in real translation envi-
ronments. 
 
If we now compare Fig. 3 and Fig. 4, we see 
strong parallels between TransType and ITU: 
language model enumerating word sequences vs 
                                                          
4
 Initially statistical MT used a noisy-channel approach 
[Brown et al 1993]; but recently [Och and Ney 2002] have 
introduced a more general framework based on the maxi-
mum-entropy principle, which shows nice prospects in 
terms of flexibility and learnability. An interesting research 
thread is to use more linguistic structure in a statistical 
translation model [Yamada and Knight 2001], which has 
some relevance to ITU since we need to handle structured 
semantic data. 
grammar enumerating semantic structures, 
source text vs input text as information sources, 
match between source text and target text vs 
match between input text and semantic structure. 
In TransType the interaction is directly with the 
target text, while in ITU the interaction with the 
semantic structure is mediated through an output 
text realization of that structure. We can thus 
hope to bring some of the techniques developed 
for TransType to ITU, but let us note that some 
of the challenges are different: for instance train-
ing the semantic grammars in ITU cannot be 
done on a directly observable corpus of texts.5  
 
 
 
Fig. 4: TransType. 
6 Conclusion 
We have introduced an interactive approach to 
text understanding, based on an extension to the 
MDA document authoring system. ITU at this 
point is more a research program than a com-
pleted realization. However we think it repre-
sents an exciting direction towards permitting a 
reliable deep semantic analysis of input docu-
ments by complementing automatic information 
                                                          
5
 Let us briefly mention that we are not the first to note for-
mal connections between natural language understanding 
and statistical MT. Thus, [Epstein 1996], working in a non-
interactive framework, draws the following parallel between 
the two tasks: while in MT, the aim is to produce a target 
text from a source text, in NLU, the aim is to produce a 
semantic representation from an input text. He then goes on 
to adapt the conventional noisy channel MT model of 
[Brown et al1993] to NLU, where extracting a semantic 
representation from an input text corresponds to finding: 
argmax(Sem) {p(Input|Sem) p(Sem)}, where p(Sem) is a 
model for generating semantic representations, and 
p(Input|Sem) is a model for the relation between semantic 
representations and corresponding texts. See also [Berger 
and Lafferty 1999] and [Knight and Marcu 2002] for paral-
lels between statistical MT and Information Retrieval and 
Summarization respectively. On a different plane, in the 
context of interactive NLG, [Nickerson 2003] has recently 
proposed to rank semantic choices according to probabilities 
estimated from a corpus; but here the purpose is not text 
understanding, but improving the speed of authoring a new 
document from scratch. 
extraction with a minimal amount of human in-
tervention for those aspects of understanding that 
presently resist automation. 
Acknowledgements 
Thanks for discussions and advice to C. Boitet, 
C. Brun, E. Fanchon, E. Gaussier, P. Isabelle, G. 
Lapalme, V. Lux and S. Pogodalla. 
References 
[Berger and Lafferty 1999] Information Retrieval as 
Statistical Translation, SIGIR-99 
[Brown, Della Pietra, Della Pietra and Mercer 1993] 
The Mathematics of Statistical Machine Transla-
tion: Parameter Estimation. Computational Linguis-
tics 19(2), 1993  
[Brun, Dymetman and Lux 2000]. Document Struc-
ture and Multilingual Text Authoring, INLG-2000 
[Epstein 1996] Statistical Source Channel Models for 
Natural Language Understanding, PhD Thesis, New 
York University, 1996.  
[Foster, Isabelle and Plamondon, 1997] Target-Text 
Mediated Interactive Machine Translation, Machine 
Translation, 12:1-2, 175-194, Dordrecht, Kluwer, 
1997. 
[Foster, Langlais and Lapalme, 2002] User-Friendly 
Text Prediction for Translators, EMNLP-02 
[Knight and Marcu 2002] Summarization beyond 
sentence extraction: A Probabilistic Approach to 
Sentence Compression, Artificial Intelligence, 
139(1), 2002.   
[Max and Dymetman 2002] Document Content 
Analysis through Fuzzy Inverted Generation, in 
AAAI 2002 Spring Symposium on Using (and Ac-
quiring) Linguistic (and World) Knowledge for In-
formation Access, 2002 
[Max 2003]. Reversing Controlled Document Author-
ing to Normalize Documents. In the proceedings of 
the EACL-03 Student Research Workshop, 2003 
[Nickerson 2003]. Statistical Models for Organizing 
Semantic Options in Knowledge Editing Interfaces. 
In AAAI Spring Symposium workshop on natural 
language generation in spoken and written dialogue, 
2003.  
[Nilsson 1998] Artificial Intelligence: a New Synthe-
sis. Morgan Kaufmann, 1998. 
[Och and Ney 2002] Discriminative Training and 
Maximum Entropy Models for Statistical Machine 
Translation, ACL02 
[Power and Scott 1998] Multilingual Authoring using 
Feedback Texts. COLING/ACL-98. 
[Ranta 2002] Grammatical Framework: A Type-
Theoretical Grammar Formalism, Journal of Func-
tional Programming, September 2002. 
[Yamada and Knight 2001] A Syntax-based Transla-
tion Model, ACL-01. 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 232?240,
Beijing, August 2010
Local lexical adaptation in Machine Translation through triangulation:
SMT helping SMT
Josep Maria Crego
LIMSI-CNRS
jmcrego@limsi.fr
Aur?lien Max
LIMSI-CNRS
Univ. Paris Sud
amax@limsi.fr
Fran?ois Yvon
LIMSI-CNRS
Univ. Paris Sud
yvon@limsi.fr
Abstract
We present a framework where auxiliary
MT systems are used to provide lexical
predictions to a main SMT system. In
this work, predictions are obtained by
means of pivoting via auxiliary languages,
and introduced into the main SMT sys-
tem in the form of a low order language
model, which is estimated on a sentence-
by-sentence basis. The linear combination
of models implemented by the decoder
is thus extended with this additional lan-
guage model. Experiments are carried out
over three different translation tasks using
the European Parliament corpus. For each
task, nine additional languages are used
as auxiliary languages to obtain the trian-
gulated predictions. Translation accuracy
results show that improvements in trans-
lation quality are obtained, even for large
data conditions.
1 Introduction
Important improvements are yet to come regard-
ing the performance of Statistical Machine Trans-
lation systems. Dependence on training data and
limited modelling expressiveness are the focus of
many research efforts, such as using monolingual
corpora for the former and syntactic models for
the latter.
Another promising approach consists in ex-
ploiting complementary sources of information
in order to build better translations, as done by
consensus-based system combination (e.g. (Ma-
tusov et al, 2008)). This, however, requires to
have several systems available for the same lan-
guage pair. Considering that the same training
data would be available to all systems, differences
in translation modelling are expected to produce
redundant and complementary hypotheses. Mul-
tisource translation (e.g. (Och and Ney, 2001;
Schwartz, 2008)) is a variant, involving source
texts available in several languages which can be
translated by systems for different language pairs
and whose outputs can be successfully combined
into better translations (Schroeder et al, 2009).
One theoretical expectation of multisource trans-
lation is that it can successfully reduce ambiguity
of the original source text, but does so under the
rare conditions of availability of existing (accu-
rate) translations. In contrast, pivot-based system
combination (e.g. (Utiyama and Isahara, 2007;
Wu and Wang, 2007)) aims at compensating the
lack of training data for a given language pair by
producing translation hypotheses obtained by piv-
oting via an intermediary language for which bet-
ter systems are available.
These techniques generally produce a search
space that differs from that of the direct transla-
tion systems. As such, they create a new transla-
tion system out of various systems for which di-
agnosis becomes more difficult.
This paper instead focusses on improving a sin-
gle system, which should be state-of-the-art as
regards data and models. We propose a frame-
work in which information coming from external
sources is used to boost lexical choices and guide
the decoder into making more informed choices.1
1We performed initial experiments where the comple-
mentary information was exploited during n-best list rerank-
ing (Max et al, 2010), but except for the multisource condi-
tion the list of hypotheses contained too little useful variation
232
Complementary sources can be of different na-
ture: they can involve other automatic systems
(for the same or different language pairs) and/or
human knowledge. Furthermore, complementary
information is injected at the lexical level, thus
making targeted fine-grained lexical predictions
useful. Importantly, those predictions are ex-
ploited at the sentence level2, so as to allow for
efficient use of source contextual information.
The second contribution of this paper is an in-
stantiation of the proposed framework. Auto-
matically pivoting via auxiliary languages is used
to make complementary predictions that are ex-
ploited through language model adaptation by the
decoder for a given language pair. For this appar-
ently difficult condition, where predictions result
from automatic translations involving two sys-
tems, we manage to report significant improve-
ments, measured with respect to the target and the
source text, under various configurations.
This paper is organized as follows. We first re-
view related work in section 2.1, and describe the
distinctive characteristics of our approach in Sec-
tion 2.2. Section 2.3 presents our instantiation of
the framework based on lexical boosting via aux-
iliary language triangulation. Experiments involv-
ing three language pairs of various complexity and
different amounts of training data are described in
Section 3. We finally conclude by discussing the
prospects offered by our proposed framework in
Section 4.
2 A framework for sentence-level lexical
boosting
2.1 Related work
The idea of using more than one translation sys-
tem to improve translation performance is not new
and has been implemented in many different ways
which we briefly review here.
System combination An often used strategy
consists in combining the output of several sys-
tems for a fixed language pair, and to rescore the
resulting set of hypotheses taking into account
all the available translations and scores. Various
to lead to measurable improvements.
2We plan to experiment next on using predictions at the
document level.
proposals have been made to efficiently perform
such a combination, using auxiliary data struc-
tures such as n-best lists, word lattices or con-
sensus networks (see for instance (Kumar and
Byrne, 2004; Rosti et al, 2007; Matusov et al,
2008; Hildebrand and Vogel, 2008; Tromble et al,
2008)). Theses techniques have proven extremely
effective and have allowed to deliver very signifi-
cant gains in several recent evaluation campaigns
(Callison-Burch et al, 2008).
Multisource translation A related, yet more re-
sourceful approach, consists in trying to combine
several systems providing translations from differ-
ent sources into the same target, provided such
multilingual sources are available. (Och and Ney,
2001) propose to select the most promising trans-
lation amongst the hypotheses produced by sev-
eral Foreign?English systems, where output se-
lection is based on the translation scores. The
intuition that if a system assigns a high figure
of merits to the translation of a particular sen-
tence, then this translation should be preferred,
is implemented in the MAX combination heuris-
tics, whose relative (lack of) success is discussed
in (Schwartz, 2008). A similar idea is explored in
(Nomoto, 2004), where the sole target language
model score is used to rank competing outputs.
(Schroeder et al, 2009) propose to combine the
available sources prior to translation, under the
form of a multilingual lattice, which is decoded
with a multisource phrase table. (Chen et al,
2008) integrate the available auxiliary information
in a different manner, and discuss how to improve
the translation model of the primary system: the
idea is to use the entries in the phrase table of
the auxiliary system to filter out those acciden-
tal correspondences that pollute the main transla-
tion model. The most effective implementation of
multisource translation to date however consists
in using mono-source system combination tech-
niques (Schroeder et al, 2009).
Translation through pivoting The use of aux-
iliary systems has also been proposed in another
common situation, as a possible remedy to the
lack of parallel data for a particular language pair,
or for a particular domain. Assume, for instance,
that one wishes to build a translation system for
233
the pair A ? B, for which the parallel data
is sparse; assuming further that such parallel re-
sources exist for pairs A ? C and for C ? B,
it is then tempting to perform the translation in-
directly through pivoting, by first translating from
A to C, then from C to B. Direct implementa-
tions of this idea are discussed e.g. in (Utiyama
and Isahara, 2007). Pivoting can also intervene
earlier in the process, for instance as a means
to automatically generate the missing parallel re-
source, an idea that has also been considered to
adapt an existing translation systems to new do-
mains (Bertoldi and Federico, 2009). Pivoting can
finally be used to fix or improve the translation
model: (Cohn and Lapata, 2007) augments the
phrase table for a baseline bilingual system with
supplementary phrases obtained by pivoting into
a third language.
Triangulation in translation Triangulation
techniques are somewhat more general and only
require the availabily of one auxiliary system (or
one auxiliary parallel corpus). For instance, the
authors of (Chen et al, 2008) propose to use the
translation model of an auxiliary C ? B system
to filter-out the phrase-table of a primary A ? B
system.
2.2 Our framework
As in other works, we propose to make use of sev-
eral MT systems (of any type) to improve trans-
lation performance, but contrarily to these works
we concentrate on improving one particular sys-
tem. Our framework is illustrated on Figure 1.
The main system (henceforth, direct system), cor-
responding to configuration 1, is a SMT system,
translating from German to English in the exam-
ple. Auxiliary information may originate from
various sources (2-6) and enter into the decoder.
A new model is dynamically built and is used to
guide the exploration of the search space to the
best hypothesis. Several auxiliary models can be
used at once and can be weighted by standard op-
timization techniques using development data, so
that bad sources are not used in practice, or by
exploiting a priori information. In the implemen-
tation described in section 2.3, this information is
updated by the auxiliary source at each sentence.
Figure 1: Lexical boosting framework with vari-
ous configurations for auxiliary predictions
We now briefly describe various possible con-
figurations to make some links to previous works
explicit. Configuration 2 translates the same
source text by means of another system for the
same language pair, as would be done in system
combination, except that here a new complete de-
coding is performed by the direct system. Con-
figuration 3, which will be detailed in section 2.3,
uses translations obtained by triangulating via an
auxiliary language (Spanish in the example). Us-
ing this two-step translation is common to pivot
approaches, but our approach is different in that
the result of the triangulation is only used as aux-
iliary information for the decoding of the direct
system. Configurations 4 and 5 are instances of
multisource translation, where a paraphrase or a
translation of the source text is available. Lastly,
configuration 6 illustrates the case where a human
translator, with knowledge of the target language
and at least of one of the available source lan-
guages, could influence the decoding by provid-
ing desired3 words (e.g. only for source words or
phrases that would be judged difficult to translate).
This human supervision through a feedback text in
real time is similar to the proposal of (Dymetman
et al, 2003).
Given this framework, several questions arise,
3The proposal as it is limits the hypotheses produced by
the system to those that are attainable given its training data.
It is conceivable, however, to find ways of introducing new
knowledge in this framework.
234
the most important underlying this work being
whether the performance of SMT systems can be
improved by using other SMT systems. Another
point of interest is whether improvements made
to auxiliary systems can yield improvement to the
direct system, without the latter undergoing any
modification.
2.3 Lexical boosting via triangulation
Auxiliary translations obtained by pivoting can be
viewed as a source of adaptation data for the target
language model of the direct system. Assuming
we have computed n-best translation hypotheses
of a sentence in the target language, we can then
boost the likeliness of the words and phrases oc-
curring in these hypotheses by deriving an auxil-
iary language model for each test sentence. This
allows us to integrate this auxiliary information
during the search and thus provides a tighter in-
tegration with the direct system. This idea has
successfully been used in speech recognition, us-
ing for instance close captions (Placeway and Laf-
ferty, 1996) or an imperfect translation (Paulik et
al., 2005) to provide auxiliary in-domain adap-
tation data for the recognizer?s language model.
(Simard and Isabelle, 2009) proposed a similar ap-
proach in Machine Translation in which they use
the target-side of an exact match in a translation
memory to build language models on a per sen-
tence basis used in their decoder.
This strategy can be implemented in a straight-
forward manner, by simply training a language
model using the n-best list as an adaptation cor-
pus. Being automatically generated, hypotheses
in the n-best list are not entirely reliable: in par-
ticular, they may contain very unlikely target se-
quences at the junction of two segments. It is how-
ever straightforward to filter these out using the
available phrase alignment information.
This configuration is illustrated on Figure 2: the
direct system (configuration 1) makes use of pre-
dictions from pivoting through an auxiliary lan-
guage (configuration 2), where n-best lists can be
used to produce several hypotheses. In order to
get a upper bound on the potential gains of this ap-
proach, we can run the artificial experiment (con-
figuration 3) where a reference in the target lan-
guage is used as a ?perfect? source of information.
Furthermore, we are interested in the performance
of the simple pivot system alone (configuration 4),
as it gives an indication of the quality of the data
used for LM adaptation.
Figure 2: Architecture of a German?English sys-
tem for lexical boosting via triangulation through
Spanish
3 Experiments and results
3.1 Translation engine
In this study, we used our own machine trans-
lation engine, which implements the n-gram-
based approach to statistical machine translation
(Mari?o et al, 2006). The translation model
is implemented as a stochastic finite-state trans-
ducer trained using a n-gram language model of
(source,target) pairs.
In addition to a bilingual n-gram model, our
SMT system uses six additional models which
are linearly combined following a discriminative
modeling framework: two lexicalized reorder-
ing (Tillmann, 2004) models,a target-language
model, two lexicon models, a ?weak? distance-
based distortion model, a word bonus model and
a translation unit bonus model. Coefficients in
this linear combination are tuned over develop-
ment data with the MERT optimization toolkit4,
slightly modified to use our decoder?s n-best lists.
For this study, we used 3-gram bilingual and
3-gram target language models built using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996); model estimation was performed with the
SRI language modeling toolkit.5 Target language
4http://www.statmt.org/moses
5http://wwww.speech.sri.com/projects/
srilm
235
models were trained on the target side of the bi-
text corpora.
After preprocessing the corpora with standard
tokenization tools, word-to-word alignments are
performed in both directions, source-to-target and
target-to-source. In our system implementation,
the GIZA++ toolkit6 is used to compute the word
alignments. Then, the grow-diag-final-and heuris-
tic is used to obtain the final alignments from
which translation units are extracted. Convergent
studies have showed that systems built accord-
ing to these principles typically achieve a per-
formance comparable to that of the widely used
MOSES phrase-based system for the language
pairs under study.
3.2 Corpora
We have used the Europarl corpus7 for our main
and auxiliary languages. The eleven languages
are: Danish (da), German (de), English (en),
Spanish (es), Finnish (fi), French (fr), Greek
(el), Italian (it), Dutch (nl), Portuguese (pt) and
Swedish (sv).
We focussed on three translation tasks: one
for which translation accuracy, as measured by
automatic metrics, is rather high (fr ? en),
and two for which translation accuracy is lower
(de ? en) and (fr ? de). This will allow us
to check whether the improvements provided by
our method carry over even in situations where the
baseline is strong; conversely, it will allow us to
assess whether the proposed techniques are appli-
cable when the baseline is average or poor.
In order to measure the contribution of each of
the auxiliary languages we used a subset of the
training corpus that is common to all language
pairs, hereinafter referred to as the intersection
data condition. We used the English side of all
training language pairs to collect the same sen-
tences in all languages, summing up to 320, 304
sentence pairs. Some statistics on the data used in
this study are reported in Table 1. Finally, in order
to assess the impact of the training data size over
the results obtained, we also considered a much
more challenging condition for the fr ? de pair,
where we used the entire Europarl data (V5) made
6http://www.fjoch.com/GIZA++.html
7http://www.statmt.org/europarl
available for the fifth Workshop on Statistical Ma-
chine Translation8 for training, and test our sys-
tem on out-of-domain news data. The training
corpus in this condition contains 43.6M French
words and 37.2M German words.
Development and test data for the first con-
dition (intersection) were obtained by leaving
out respectively 500 and 1000 sentences from
the common subset (same sentences for all lan-
guages), while the first 500 sentences of news-
test2008 and the entire newstest2009 official test
sets were used for the full data condition.
Train Dev Test
Words Voc. Words Voc. OOV Words Voc. OOV
da 8.5M 133.5k 13.4k 3.2k 104 25.9k 5.1k 226
de 8.5M 145.3k 13.5k 3.5k 120 26.0k 5.5k 245
en 8.9M 53.7k 14.0k 2.8k 39 27.2k 4.0k 63
es 9.3M 85.3k 14.6k 3.3k 56 28.6k 5.0k 88
fi 6.4M 274.9k 10.1k 4.3k 244 19.6k 7.1k 407
fr 10.3M 67.8k 16.1k 3.2k 47 31.5k 4.8k 87
el 8.9M 128.3k 14.1k 3.9k 72 27.2k 6.2k 159
it 9.0M 78.9k 14.3k 3.4k 61 28.1k 5.1k 99
nl 8.9M 105.0k 14.2k 3.1k 76 27.5k 4.8k 162
pt 9.2M 87.3k 14.5k 3.4k 49 28.3k 5.2k 118
sv 8.0M 140.8k 12.7k 3.3k 116 24.5k 5.2k 226
Table 1: Statistics for the training, development
and test sets of the intersection data condition
3.3 Results
In this section, we report on the experiments car-
ried out to assess the benefits of introducing an
auxiliary language model to the linear combina-
tion of models implemented in our SMT system.
Table 2 reports translation accuracy (BLEU) re-
sults for the main translation tasks considered in
this work (fr ? de), (fr ? en) and (de ? en),
as well as for multiple intermediate tasks needed
for pivoting via auxiliary systems.
For each triplet of languages (src, aux, trg),
columns 4th to 6th show BLEU scores for systems
performing (src ? aux), (aux ? trg) and pivot
translations using aux as the bridge language.
The last two columns display BLEU scores for
the main translation tasks (fr ? de), (fr ? en)
and (de? en). Column src-trg refers to the base-
line (direct) systems, for which no additional lan-
8http://www.statmt.org/wmt10
236
src aux trg src-aux aux-trg pivot src-trg +auxLM
Intersection data condition
fr - de - - - 18.02
da 22.78 20.02 16.27 +0.44
el 24.54 18.51 15.86 +0.76
en 29.53 17.31 15.69 +0.50
es 34.94 18.31 16.76 +0.96
fi 10.71 14.15 11.39 +0.65
it 31.60 16.86 16.54 -0.05
nl 22.71 21.44 16.76 +0.55
pt 33.61 17.47 16.34 -0.12
sv 20.73 19.59 13.73 -0.14
average +0.39
- - ref - - - - +6.46
fr - en - - - 29.53
da 22.78 29.54 25.48 +0.02
de 18.02 24.66 23.50 +0.05
el 24.54 29.37 25.31 +0.07
es 34.94 31.05 27.76 +0.61
fi 10.71 20.56 19.15 +0.44
it 31.60 25.75 25.79 +0.32
nl 22.71 24.49 25.15 +0.01
pt 33.61 29.44 27.27 +0.01
sv 20.73 30.98 23.74 +0.50
average +0.22
- - ref - - - - +11.30
de - en - - - 24.66
da 24.59 29.54 22.73 +0.96
el 19.72 29.37 20.88 +1.02
es 25.48 31.05 21.23 +0.77
fi 12.42 20.56 18.02 +0.94
fr 25.93 29.53 21.55 +0.19
it 18.82 25.75 18.05 +0.19
nl 24.97 24.49 22.62 +0.64
pt 23.15 29.44 21.93 +0.87
sv 19.80 30.98 21.35 +0.69
average +0.69
- - ref - - - - +9.53
Full data condition
fr - de - - - 19.94
es 38.76 20.18 19.36 +0.61
Table 2: Translation accuracy (BLEU) results.
guage model is used; column +auxLM refers to
the same system augmented with the additional
language model. Additional language models are
built from hypotheses obtained by means of pivot
translations, using aux as auxiliary language. The
last score is shown in the form of the difference
(improvement) with respect to the score of the
baseline system.
This table additionally displays the BLEU re-
sults obtained when building the additional lan-
guage models directly from the English reference
translations (see last row of each translation task).
These numbers provide an upper-bound of the ex-
pected improvements. Note finally that numbers
in boldface correspond to the best numbers in their
column for a given language pair.
As detailed above, the additional language
models are built using trg hypotheses obtained by
pivoting via an auxiliary language: (src ? aux)
+ (aux ? trg). Hence, column pivot shows the
quality (measured in terms of BLEU) of the hy-
potheses used to estimate the additional model.
Note that we did not limit the language model to
be estimated from the 1-best pivot hypotheses. In-
stead, we uses n-best translation hypotheses of the
(src ? aux) system and m-best hypotheses of
the (aux ? trg) system. Hence, n ? m target
hypotheses were used as training data to estimate
the additional models. Column +auxLM shows
BLEU scores over the test set after performing
four system optimizations on the development set
to select the best combination of values used for n
and m among: (1, 1), (10, 1), (10, 1) and (10, 10).
All hypotheses used to estimate a language model
are considered equally likely. Language models
are learnt using Witten-Bell discounting. Approx-
imately?1.0 point must be added to BLEU scores
shown in the last 2 columns for 95% confidence
levels.
As expected, pivot translations yield lower
quality scores than the corresponding direct trans-
lations hypotheses. However, pivot hypotheses
may contain better lexical predictions, that the ad-
ditional model helps transfer into the baseline sys-
tem, yielding translations with a higher quality, as
shown in many cases the +auxLM systems results.
The case of using Finnish as an auxiliary language
is particularly remarkable. Even though pivot hy-
potheses obtained through Finnish have the low-
est scores9, they help improve the baseline perfor-
mance as additional language models.
As expected, the translation results of the pair
9Given the agglutinative nature of morphological pro-
cesses in Finnish, reflected in a much lower number of words
per sentence, and a higher number of types (see Table 1),
BLEU scores for this language do not compare directly with
the ones obtained for other languages.
237
with a highest baseline (fr ? en) were on av-
erage less improved than those of the pairs with
lower baselines.
As can also be seen, the contribution of each
auxiliary language varies for each of the three
translation tasks. For instance, Danish (da) pro-
vides a clear improvement to (de ? en) transla-
tions, while no gain is observed for (fr ? en).
No clear patterns seems to emerge, though, and
the correlation between the quality of the pivot
translation and the boost provided by using these
pivot hypotheses remains to be better analyzed.
In order to assess whether the improvements
obtained carry over larger data conditions, we
trained our (fr ? de), (fr ? es) and (es? de)
systems over the entire EPPS data. Results are re-
ported in the bottom part of Table 2. As can be
seen, the (fr ? de) system is still improved by
using the additional language model. However,
the absolute value of the gain under the full condi-
tion (+0.61) is lower than that of the intersection
data condition (+0.96).
3.4 Contrastive evaluation of lexical
translation
In some cases, automatic metrics such as BLEU
cannot show significant differences that can be re-
vealed by fine-grained focussed human evaluation
(e.g. (Vilar et al, 2006)). Furthermore, comput-
ing some similarity between a system?s hypothe-
ses and gold standard references puts a strong
focus on the target side of translation, and does
not allow evaluating translation performance from
the source words that were actually translated.
We therefore use the evaluation methodology de-
scribed in (Max et al, 2010) for a complementary
measure of translation performance that focuses
on the contrastive ability of two systems to ade-
quately translate source words.
Source words from the test corpus were first
aligned with target words in the reference, by au-
tomatically aligning the union of the training and
test corpus using GIZA++.10 The test corpus was
analyzed by the TREETAGGER11 so as to identify
10The obtained alignments are thus strongly influenced by
alignments from the training corpus. It could be noted that
alignments could be manually corrected.
11http://www.ims.uni-stuttgart.de/
Source words? part-of-speech
aux ADJ ADV NOM PRO VER all +Bleu
el - 27 21 114 25 99 286 +0.07+ 62 29 136 27 114 368
es - 33 25 106 26 110 300 +0.61+ 64 38 136 22 117 377
fi - 44 40 106 20 92 302 +0.44+ 49 31 120 23 106 329
it - 55 39 128 35 119 376 +0.32+ 55 39 145 36 121 396
sv - 40 30 138 29 109 346 +0.50+ 69 46 144 23 134 416
Table 3: Contrastive lexical evaluation re-
sults per part-of-speech between the baseline
French?English system and our systems using
various auxiliary languages. ?-? (resp. ?+?) val-
ues indicate numbers of words that only the base-
line system (resp. our system) correctly translated
with respect to the reference translation.
content words, which have a more direct impact
on translation adequacy. When source words are
aligned to several target words, each target word
should be individually searched for in the candi-
date translation, and words from the reference can
only be matched once.
Table 3 shows contrastive results per part-of-
speech between the baseline fr?en system and
systems using various auxiliary languages. Val-
ues in the ?-? row indicate the number of words
that only the baseline system translated as in the
reference translation, and values in the ?+? row
the number of words that only our corresponding
system translated as in the reference. The most
striking result is the contribution of Greek, which,
while giving no gain in terms of BLEU, improved
the translation of 82 content words. This could
be explained, in addition to the lower Bleu3 and
Bleu4 precision, by the fact that the quality of
the translation of grammatical words may have
decreased. On the contrary, Italian brings little
improvement for content words save for nouns.
The mostly negative results on the translation of
pronouns were expected, because this depends on
their antecedent in English and is not the object of
specific modelling from the systems. The trans-
lation of nouns and adjectives benefits the most
from auxiliary translations.
projekte/corplex/TreeTagger
238
Figure 3 illustrates this evaluation by means of
two examples. It should be noted that a recurrent
type of improvement was that of avoiding missing
words, which is here a direct result of their being
boosted in the auxiliary hypotheses.
4 Conclusions and future work
We have presented a framework where auxiliary
MT systems are used to provide useful informa-
tion to a main SMT system. Our experiments
on auxiliary language triangulation have demon-
strated its validity on a difficult configuration and
have shown that improvements in translation qual-
ity could be obtained even under large training
data conditions.
The fact that low quality sources such as pivot
translation can provide useful complementary in-
formation calls for a better understanding of the
phenomena at play. It is very likely that, look-
ing at our results on the contribution of auxiliary
languages, improving the quality of an auxiliary
source can also be achieved by identifying what
a source is good for. For example, in the stud-
ied language configurations predictions of transla-
tions for pronouns in the source text by auxiliary
triangulation does not give access to useful infor-
mation. On the contrary, triangulation with Greek
when translating from French to English seems to
give useful information regarding the translation
of adjectives, a result which was quite unexpected.
Also, it would be interesting to use richer pre-
dictions than short n-grams, such as syntactic
dependencies, but this would require significant
changes on the decoders used. Using dynamic
models at the discourse level rather than only at
the sentence level would also be a useful improve-
ment. Besides the improvements just mentioned,
our future work includes working on several con-
figurations of the framework described in sec-
tion 2.2, in particular investigating the new type
of system combination.
Acknowledgements
This work has been partially funded by OSEO un-
der the Quaero program.
References
Bertoldi, Nicola and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
WMT, Athens, Greece.
Callison-Burch, Chris, Cameron Shaw Fordyce,
Philipp Koehn, Christof Monz, and Josh Schroeder.
2008. Further meta-evaluation of machine transla-
tion. In Proceedings of WMT, Columbus, USA.
Chen, Stanley F. and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of ACL, Santa
Cruz, USA.
Chen, Yu, Andreas Eisele, and Martin Kay. 2008. Im-
proving statistical machine translation efficiency by
triangulation. In Proceedings of LREC, Marrakech,
Morocco.
Cohn, Trevor and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use
of multi-parallel corpora. In Proceedings of ACL,
Prague, Czech Republic.
Dymetman, Marc, Aur?lien Max, and Kenji Yamada.
2003. Towards interactive text understanding. In
Proceedings of ACL, short paper session, Sapporo,
Japan.
Hildebrand, Almut Silja and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
Proceedings of AMTA, Honolulu, USA.
Kumar, Shankar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of NAACL-HLT, Boston, USA.
Mari?o, Jos?, Rafael E. Banchs, Josep Maria Crego,
Adria de Gispert, Patrick Lambert, J.A.R. Fonol-
losa, and Martha Costa-juss?. 2006. N-gram based
machine translation. Computational Linguistics,
32(4):527?549.
Matusov, Evgeny, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose Mari?o,
Matthias Paulik, Salim Roukos, Holger Schwenk,
and Hermann Ney. 2008. System combination for
machine translation of spoken and written language.
IEEE Transactions on Audio, Speech and Language
Processing, 16(7):1222?1237, September.
Max, Aur?lien, Josep M. Crego, and Fran?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of LREC, Valletta,
Malta.
239
ref #357 this concession to the unions ignores the reality that all airlines have different safety procedures which even differ
between aircrafts within each airline .
bas this concession unions ignores the fact that all airlines have different safety procedures which are even within each
of the companies in accordance with the types of equipment .
w.r.t. src cette concession aux syndicats ignore la r?alit? selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?
diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ? appareils .
+aux this concession to the trade unions ignores the reality according to which all the airlines have different safety pro-
cedures which differ even within each of the companies in accordance with the types of equipment .
w.r.t. src cette concession aux syndicats ignore la r?alit? selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?
diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ? appareils .
Figure 3: Example of automatic translations from French to English for the baseline system and when
using Spanish as the auxiliary language. Bold marking indicates source/target words which were cor-
rectly translated according to the reference translation.
Nomoto, Tadashi. 2004. Multi-engine machine trans-
lation with voted language model. In Proceedings
of ACL, Barcelona, Catalunya, Spain.
Och, Franz Josef and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Proceedings of MT
Summit, Santiago de Compostela, Spain.
Paulik, Matthias, Christian F?gen, Thomas Schaaf,
Tanja Schultz, Sebastian St?ker, and Alex Waibel.
2005. Document driven machine translation en-
hanced automatic speech recognition. In Proceed-
ings of InterSpeech, Lisbon, Portugal.
Placeway, Paul and John Lafferty. 1996. Cheating
with imperfect transcripts. In Proceedings of IC-
SLP, Philadelphia, USA.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bin Xiang,
Spyros Matsoukas, Richard Schwatz, and Bonnie J.
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Proceedings of
NAACL-HTL, Rochester, USA.
Schroeder, Josh, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation. In
Proceedings of EACL, Athens, Greece.
Schwartz, Lane. 2008. Multi-source translation meth-
ods. In Proceedings of AMTA, Honolulu, USA.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of Machine
Translation Summit XII, Ottawa, Canada.
Tillmann, Christoph. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of NAACL-HLT, Boston, USA.
Tromble, Roy, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP, Honolulu, USA.
Utiyama, Masao and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Proceedings of NAACL-
HLT, Rochester, USA.
Vilar, David, Jia Xu, Luis Fernando d?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proceedings of LREC,
Genoa, Italy.
Wu, Hua and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine trans-
lation. In Proceedings of ACL, Prague, Czech Re-
public.
240
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 656?666,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Example-based Paraphrasing for Improved Phrase-Based
Statistical Machine Translation
Aure?lien Max
LIMSI-CNRS & Univ. Paris Sud
Orsay, France
aurelien.max@limsi.fr
Abstract
In this article, an original view on how to
improve phrase translation estimates is pro-
posed. This proposal is grounded on two main
ideas: first, that appropriate examples of a
given phrase should participate more in build-
ing its translation distribution; second, that
paraphrases can be used to better estimate this
distribution. Initial experiments provide ev-
idence of the potential of our approach and
its implementation for effectively improving
translation performance.
1 Introduction
Phrase translation estimation in Statistical Phrase-
based Translation (Koehn et al, 2003) is hampered
by the availability of both too many and too few
training instances. Recent results on tera-scale SMT
(Lopez, 2008) show that access to many training
examples1 can lead to significant improvements in
translation quality. Also, providing indirect train-
ing instances via synonyms or paraphrases for pre-
viously unseen phrases can result in gains in trans-
lation quality, which are more apparent when little
training data is originally available (Callison-Burch
et al, 2006; Marton et al, 2009; Mirkin et al, 2009;
Aziz et al, 2010). Although there is a consensus on
the importance of using more parallel data in SMT,
it has never been formally shown that all additional
training instances are actually useful in predicting
contextually appropriate translation hypotheses.
1To be more accurate, works such as that of (Lopez, 2008)
have recourse to random sampling to build models of a manage-
able size in a reasonable amount of time.
Attempts at limiting training parallel sentences to
those resembling test data through thematic adapta-
tion (Hildebrand et al, 2005) indeed confirm that
large quantities of training data cannot compen-
sate for the requirement for contextually appropriate
training instances. In fact, it is important that phrase
translation models adequatly reflect contextual pref-
erences for each phrase occurrence in a text. A vari-
ety of recent works have used dynamically adapted
translation models, where each phrase occurrence
has its own translation distribution (Carpuat and Wu,
2007; Stroppa et al, 2007; Max et al, 2008; Gim-
pel and Smith, 2008; Haque et al, 2009) derived
from local contextual information in the training ex-
amples.2 These approaches are supported by the
study of (Wisniewski et al, 2010) which shows that
phrase-based SMT systems are expressive enough to
achieve very high translation performance and there-
fore suggests a better scoring of phrases.
The apparent tradeoff between the number of
training examples and their appropriateness in each
indivual context naturally asks for means of increas-
ing the number of appropriate examples. Exploiting
comparable corpora for acquiring translation equiva-
lents (Munteanu and Marcu, 2005; Abdul-Rauf and
Schwenk, 2009) offers interesting prospects to this
issue, but so far focus has not been so much on con-
text appropriateness as on globally increasing the
number of biphrase examples.
2The study of (Carpuat, 2009) shows that the one transla-
tion per discourse hypothesis holds in some cases, but to our
knowledge no SMT systems have attempted to exploit it yet.
However, in our view, this finding does not contradict the need
for estimating translation distributions at the individual phrase
level, but they should be integrated as additional information.
656
The approach we take in this article is motivated
by the fact that natural language allows for multiple
text views on a given content, and that if two phrases
are good paraphrases in context, then considering
appropriate training examples of one of the phrases
could provide larger quantities of training data for
translating the other. In other words, we hypothe-
size that there may be more training data to learn a
phrase?s translations in a bilingual corpus than what
SMT approaches typically use.
In contrast to previous attempts at using para-
phrases to improve Statistical Machine Translation,
which require external data in the form of additional
parallel bilingual corpora (Callison-Burch et al,
2006), monolingual corpora (Marton et al, 2009),
lexico-semantic resources (Mirkin et al, 2009; Aziz
et al, 2010), or sub-sentential (Resnik et al, 2010)
or sentential paraphrases of the input (Schroeder et
al., 2009), the approach we take here can be endoge-
nous with respect to the original training data. It
also significantly departs from previous work in that
paraphrasing is not simply considered as a way of
finding alternative wordings that can be translated
given the original training data for out-of-vocabulary
phrases only (Callison-Burch et al, 2006; Marton
et al, 2009; Mirkin et al, 2009; Aziz et al, 2010),
but as a means to better estimate translations for any
possible phrase. Also, as opposed to the work by
(Schroeder et al, 2009; Onishi et al, 2010; Du et
al., 2010), we do not encode paraphrases into input
lattices to have them compete against each other to
belong to the source sentential paraphrase that will
lead to the highest scoring output sentence3. Instead,
we make use of all contextually appropriate para-
phrases of a source phrase, which collectively eval-
uate the quality of each translation for that phrase.
This work can thus be seen as a contribution to-
wards shifting from global phrase translation dis-
tributions to contextual translation distributions for
contextually equivalent source units. The remainder
of this paper is organized as followed. In section 2
we review relevant previous works and discuss how
they differ from our approach. Section 3 provides a
description of the details of our approach. We de-
scribe an experimental setup in section 4 and com-
3This highly depends on how well estimated translations for
each independent paraphrase are.
ment on our results. Finally, we discuss our future
work in section 5.
2 Relation to previous work
2.1 Contextual estimation of phrase
translations
In standard approaches to phrase-based SMT, evi-
dence of a translation is accumulated uniformely ev-
ery time it is found associated with a source phrase
in the training corpus. In addition to the fact that
errors in automatic word alignment and non literal
translations often produce useless biphrases, this re-
sults in rare but appropriate translations being very
unlikely to be considered during decoding. Some
approaches on source context modelling (Carpuat
and Wu, 2007; Stroppa et al, 2007; Max et al, 2008;
Haque et al, 2009) build classifiers offline for the
phrases in a test set, so that context similarity can
for example reinforce scores associated with rare
but appropriate translations. However, heavy offline
computation makes scaling to larger corpora an is-
sue. Other approaches (Callison-Burch et al, 2005;
Lopez, 2008) instead focus on accessing very large
corpora. Indexing by suffix arrays is used to allow
fast access to phrase instances in the corpus, and ran-
dom sampling to avoid collecting the full set of ex-
amples has been shown to perform well. However,
these approaches consider all instances of a phrase
as equivalent for the estimation of its translations.
These works converge on the need for accessing
a sufficient number of examples that are relevant for
any source phrase in context, fast enough to permit
on-the-fly phrase table building. This paper pro-
poses an intermediate step: the full set of phrase
examples is found efficiently, and a measure of the
adequacy of each example with a phrase in context
provides evidence for its translation that depends on
this value of adequacy. In this way, the translation
associated with an example for a different sense of
a polysemous word would in the best scenario only
be considered marginally when computing the trans-
lation distribution. As in most previous works, ad-
equacy can be approximated by context similarity
between phrase occurrences and training examples.
Ideally, one would stop extracting examples when
enough appropriate examples have been found to es-
timate a reliable translation distribution. (Callison-
657
sample size 100 500 1K 5K 10k 50k unlim.
BLEU score 28.8 28.8 28.8 28.9 29.1 28.9 29.0
Figure 1: Effect of number of samples on translation
quality (measured on German to English translation on
Europarl data) reported by (Callison-Burch et al, 2005)
Burch et al, 2005) measured the impact on transla-
tion quality of the sample size in random sampling
of source phrase examples in the training corpus to
estimate a phrase?s translation probabilities. As Ta-
ble 1 shows, quality (in terms of BLEU scores) al-
most remains constant for samples of size 100 or
more. This apparent confirmation of the efficiency
of random sampling is backed up by the authors
with the following possible explanations: 1) the
most probable translations remain the same for dif-
ferent sample sizes; 2) misestimated probabilities
are ruled out by the target language model; and
3) longer or less frequent phrases, which are not
affected by sampling, are preferred. However, as
said previously, random sampling cannot guarantee
that contextually-appropriate examples are selected.
In fact, (Lopez, 2008) points out to using discrimi-
natively trained models with contextual features of
source phrases in conjunction with phrase sampling
as an open problem. This work does not attempt to
directly address it, but instead resorts to complete
analysis of the training data to guarantee that all
contextually-appropriate examples are considered.
2.2 Using paraphrases for translating
For some phrases, not enough examples can be
found in the training corpus to estimate reliable
translation probabilities in context. In such cases,
one might be interested in finding more appropri-
ate examples, which seems at first impossible us-
ing the sole original bilingual corpus. We can in
fact consider the set of source phrases that have
similar translations in context. This set is roughly
made up of a subset of what can be referred to as
paraphrases. One possible approach to extract lo-
cal (i.e. phrasal) paraphrases precisely exploits sim-
ilarity on the target side in another language by ex-
tracting source phrases that share common transla-
tions (Bannard and Callison-Burch, 2005), but re-
cent approaches have combined this approach with
Source phrase Paraphrases
Balkan War Balkan war (0.25) Balkans War
(0.125) Balkans (0.125) Balkans
war (0.125) war in the Balkans
(0.125) Balkan conflict (0.125)
British forces British troops (0.29) British
armed forces (0.19)
Czech president President of the Czech Republic
(0.5)
Dalai Lama?s of the Dalai Lama (0.27)
I don?t see I do not believe (0.18) I do not think
(0.18) I do not see (0.15)
Figure 2: Examples of paraphrases obtained by pivoting
via French; values indicate paraphrase probability as de-
fined in (Bannard and Callison-Burch, 2005).
similarity computation in the ?source? (i.e. original)
language (Callison-Burch, 2008; Max, 2008; Kok
and Brockett, 2010). Figure 2 provides examples of
English paraphrases obtained by automatically piv-
oting via French. As can be seen, some examples
would be clearly useful to better estimate transla-
tions of the original source phrase: (Balkan War
? war in the Balkans) are syntactic variants that
can generally substitute with each other, (Balkan
War ? Balkans war) are character-level variants4.
Other examples, however, clearly illustrate the need
for validation in context: (Dalai Lama?s ? of the
Dalai Lama) require different syntactic contexts,
and (I don?t see ? I do not believe) are only inter-
changeable in specific semantic contexts.
Previous attempts at exploiting paraphrases in
SMT have first concentrated on obtaining transla-
tions for phrases absent from the training corpus
(Callison-Burch et al, 2006; Marton et al, 2009;
Mirkin et al, 2009)5, with modest gains in trans-
lation performance as measured by automatic met-
rics. (Callison-Burch et al, 2006) obtain para-
phrases by pivoting via additional bilingual corpora
and use the translations of known paraphrases to
translate unseen phrases, which requires that the ad-
ditional bilingual corpora contain the unseen source
phrases and that some of the extracted paraphrases
be present in the original corpus. (Marton et al,
4To our knowledge, most implementations of SMT decoders
do not integrate flexible matching of phrases.
5The work by (Mirkin et al, 2009) in fact considers both
paraphrases and entailed texts to increase the number of prop-
erly translated texts.
658
2009) proceed similarly but obtain their paraphrases
from comparatively much larger monolingual cor-
pora by following the distributionality hypothesis.
In both cases, gains are only obtained in very spe-
cific conditions where very few training data are
available and where useful additional knowledge can
be brought in from external resources. Furthermore,
the described implementations do not consider ac-
ceptability of the paraphrases in context, as their un-
derlying hypothesis is that it might be more desir-
able to translate some paraphrase than not to trans-
late a given phrase.6 In contrast, the work by (Mirkin
et al, 2009) attempts to model context when using
replacements for words (synonyms or hypernyms).
The natural next step that we take here is to
exploit the complementarity of the original bilin-
gual training corpus for finding paraphrases and the
monolingual (source) side of the same corpus for
validating them in context. Furthermore, our fo-
cus here is not on paraphrasing unseen phrases7, but
possibly any phrase, or any phrase seen less than a
given number of times, or any types of difficult-to-
translate phrases (Mohit and Hwa, 2007).
The recent work of (Resnik et al, 2010)
uses crowdsourcing to obtain paraphrases for
source phrases corresponding to mistranslated target
phrases. The spotting of the incorrect target phrases
and the paraphrasing of the source phrases can be
automated. Promising oracle figures are obtained,
validating the claim that some variations of the input
sentence might be more easily translated than oth-
ers by a given system. Paraphrases have also been
used to represent alternative inputs encoded in lat-
tices using existing (Schroeder et al, 2009) or au-
tomatically built paraphrases (Onishi et al, 2010;
Du et al, 2010). In this scenario, paraphrases are in
fact competing with each other, whereas in our pro-
posal paraphrases collectively participate in evalu-
ating the quality of each translation for a source
phrase. We believe that if two phrases are indeed
paraphrases in context, then their respective set of
translations are both relevant to translate the two
phrases. The target language model nevertheless
still has an important role to play to select appro-
6The default strategy for most decoders is to copy out-of-
vocabulary tokens into the final text.
7Doing it in conjunction with our approach for improving
the translation of known phrases is part of our future work.
priate translations among semantically-compatible
translations (i.e., target side paraphrases) in the spe-
cific context of a generated target hypothesis.
Lastly, automatic sentential paraphrasing has also
been used in SMT to build alternative reference
translations for parameter optimization (Madnani
et al, 2008) and to build alternative training cor-
pora (Bond et al, 2008).
3 Towards better exploitation of training
corpora in phrase-based SMT
In typical phrase-based SMT settings (Koehn et al,
2003), words from the source side of the corpus
are first aligned to words on the target side and
biphrases are extracted from each training sentence
using some heuristics on the word alignments. A
source phrase f in a sentence being translated may
therefore be aligned to a variety of target phrases.
In the example on Figure 3, f is aligned some num-
ber of times in the training corpus to target phrases
e1, e2, e3 and e5. Using the number of times f is
paired with some target phrase ei, count(f, ei), rela-
tive frequency estimation can be used to compute the
probability of translation ei given source phrase f :
prel(ei|f) =
count(f, ei)
?
j count(f, ej)
(1)
This value, together with other estimates of how
appropriate a translation pair (f, ei) is, are recorded
in a phrase table, which typically discards all con-
textual information.8 Therefore, the translation dis-
tribution of some phrase is globally estimated from
a training corpus independently of the actual context
of that phrase.9 On Figure 3, phrase f has at least
two distinct senses: one represented by set E , which
in our example corresponds to the appropriate sense
for a particular occurrence of f in a test sentence;
and one which corresponds to translation e5. A typ-
ical problem, due to the lack of context modeling,
8See (Carpuat and Wu, 2007; Stroppa et al, 2007; Max et
al., 2008; Gimpel and Smith, 2008; Haque et al, 2009) for no-
table exceptions.
9Context is in fact taken into account to some extent by the
target language model, which should score higher translations
that are more appropriate given a target translation hypothesis
being built. In fact, in this work we consider the target language
model as the main source of information for selecting among
acceptable target phrases (target language paraphrases).
659
Figure 3: Example of possible source equivalents and
translations for phrase occurrence f ?un bon avocat? in
the sentence ?L?embauche d?un bon avocat est cruciale
quelle que soit l?activite?? (?Hiring a good lawyer is cru-
cial to any business?). Set E represents target phrase
types that are acceptable translations given the particu-
lar context of f , and set F represents source phrase types
that can be in a paraphrasing relation to f depending on
the context they appear in.
is that in situations such as ?ei ? E , count(f, e5)
count(f, ei), it is very unlikely that a correct trans-
lation will be selected during decoding against the
incorrect but much more frequent one. Taking an
extreme view on this issue, it is in fact desirable that
when estimating phrase translation probabilities for
a phrase f , translations of incompatible senses be
not considered.10 Of course, this raises the diffi-
cult issue of sense clustering of phrases. We propose
here an intermediary solution, which consists in con-
sidering each occurrence in the training corpus as
counting a number of times that depends on its con-
textual similarity with the occurrence of f from the
test file, through the following additional translation
model :
pcont(ei|f) =
?
?fk,ei?
simcont(C(f), C(fk))
?
?fk,ej?
simcont(C(f), C(fk))
(2)
where f is some source phrase to translate and fk
an example of f in the training corpus, ?fk, ei? is a
10Put differently, is it more acceptable to copy a source word
in the target hypothesis or to incorrectly translate it when the
confidence about its being incorrect is high?
biphrase from the training corpus, C(f) the context
of some source phrase, C(fk) the context of a par-
ticular example of f in the training corpus, simphr
a function indicating the contextual similarity be-
tween two phrase contexts, and ej is any possible
translation of f .
The problem of modeling phrase translation is
however not limited to inappropriate training exam-
ples. For various reasons, legitimate occurrences of
source phrases may not be considered when building
a phrase?s translation distribution. We describe those
cases by considering the possible source phrases pi
from Figure 3:
? p1?s only translation, e1, is a common transla-
tion with f ; each contextually-appropriate ex-
ample of p1 should reinforce the probability of
e1 being a translation for f .
? Contextually-appropriate examples of p2 can
reinforce e3. Translation e6 should correspond
to contextually-inappropriate examples of p2,
so e6 should not be considered as a new po-
tential translation for f .
? Contrarily to the examples of p2 translating as
e6, examples of p3 translating as e4 are much
more likely of being contextually-appropriate
with f , meaning that f could be substituted
with most p3 examples. Therefore, e4, which
was not initially considered as a possible trans-
lation of f , could now be considered as such.
? p4 shares a translation with f , e2, but this is
due to the polysemous nature of this transla-
tion. Again, all examples of p4 should be found
contextually-inappropriate with f , and their
translations should not be considered when es-
timating the translations of f .
? Lastly, the case of the common translation e5
between f and p5 illustrates a consequence of
the polysemous nature of the source phrase cor-
responding to word sequence f : translations
corresponding to other senses of f should not
get reinforced by paraphrase examples such as
those of p5 as these examples should be found
contextually-inappropriate with f .
660
We build a separate translation model for transla-
tions estimated through paraphrases, defined as fol-
lows:
ppara(ei|f) =
?
?pk,ei?
simpara(C(f), C(pk))
?
?pk,ej?
simpara(C(f), C(pk))
(3)
where pk is a paraphrase of f , ?pk, ei? is a biphrase
from the training corpus such that ei is also a transla-
tion of f , C(f) the context of a given source phrase
for which we are estimating the translation distribu-
tion, C(pk) the context of a particular example of pk
in the training corpus, simpara a function indicat-
ing the contextual similarity between a phrase con-
text and a paraphrase context, and ej is any possible
translation of f .
Several requirements can be drawn from the pre-
vious description:
1. List of potential paraphrases: some mech-
anism for finding potential paraphrases for
source phrases is required, and several such
mechanisms could be combined. Pivoting via
bilingual corpora, a natural strategy given the
issue at hand, is just one among many different
proposed strategies (Madnani and Dorr, 2010).
2. Contextual similarity measure: a similarity
measure between the contexts of two phrases
or two potential local paraphrases is required.
This automatic measure should ideally be able
to model not only syntactic but also semantic
and pragmatic information.
3. Robust translation evaluation: our ap-
proach is designed to reinforce estimates for
any contextually-appropriate translations of a
phrase, as shown by set E on Figure 3. It is
therefore important to have some means of ac-
cepting them as subparts of valid translations.
Robustness in Machine Translation evaluation
is an active domain, and potential candidates
include using BLEU-like metrics with multiple
references, Human-targeted Translation Error
Rate (Snover et al, 2006) and the use of para-
phrases for reference translations (Kauchak and
Barzilay, 2006).
train dev. test
# sent. # tok. # sent. # tok. # sent. # tok.
en 318K 9.1M 500 14,0K 500 13,6K
fr 318K 10.3M 500 16,1K 500 15,7k
Figure 4: Statistics of the corpora used.
In this paper, we want to evaluate whether an en-
dogenous approach for finding paraphrases can lead
to some improvement in translation performance.
Note that we will not consider in this initial work
the possibility of adding new translations to phrases
(such as e4 for f on Figure 3) as it adds complexity
and should be investigated when the other simpler
cases can be handled successfully.
In the following section, we describe experiments
in which the original bilingual corpus is the only re-
source used to find potential paraphrases and to esti-
mate phrase translations in context. We chose a very
simple measure of similarity, and let to our future
work the task of improving context modeling. As
regards evaluation, we will resort to various ways to
measure the impact of our implementation on trans-
lation performance.
4 Experiments and results
4.1 Data and baseline SMT systems
We have conducted our experiments using the
MOSES11 package to build state-of-the-art phrase-
based SMT systems for phrases of up to 5 tokens,
using standard parameters and MERT for optimizing
model weights. We used a subpart of the Europarl
corpus12 in French and English as our training cor-
pus and built baseline MOSES systems (bsl) in both
directions. The target side of the training corpus
was used to train 3-gram target language model with
modified Kneser-Ney smoothing. Held-out datasets
were used for development and testing. The charac-
teristics of all corpora are described in Figure 4.
4.2 Example-based Paraphrasing SMT systems
We also built systems that exploit phrase and para-
phrase context under the form of two additional
models pcont and ppara described in section 3. These
11http://www.statmt.org/moses
12http://www.statmt.org/europarl
661
phrase table size num. entries
en?fr
baseline 240Mb 2.4M
our systems 5.0Gb 37.5M
fr?en
baseline 193Mb 1.9M
our systems 4.0Gb 30.2M
Figure 5: Statistics on the size and the number of entries
of the phrase tables filtered on the development set.
models are added to the list of models used to eval-
uate the various translations of a phrase in the ap-
propriate phrase tables, and are optimized with the
other models by standard MERT.
In order to model context, we modified the source
texts so that each phrase becomes unique in the
phrase table, i.e. it has its own translation distribu-
tion. This is done (as in other works (Carpuat and
Wu, 2007; Stroppa et al, 2007)) by transforming
each token into a unique token, e.g. token ? to-
ken@337. This therefore leads to a significant in-
crease in the size of the phrase table, as illustrated
on Figure 5, as all occurrences for the same phrase
are not factored anymore.13
We chose a very simple initial definition of con-
text similarity based on the presence of common
n-grams in the immediate vicinity of two phrases.
Let lengthleft (resp. lengthright) be the length of
the longest common n-gram in the immediate vicin-
ity on the left (resp. right) of two phrases in context
(C(f) and C(fi)). For instance, given the two fol-
lowing contexts (phrases under focus are in bold and
common n-grams are underlined):
1. the commission accepts the substance of the
amendments@11257 proposed@11258
by@11259 the committee on fisheries ...
2. this is why we shall support all of the amend-
ments put forward by the committee on agri-
culture and rural development ...
lengthleft = 2 and lengthright = 3. We further
define length as:
13These volumes of data and our available hardware facilities
for these experiments led us to initially limit the size of our data
sets. We will discuss in section 5 how we intend to address this
limitation in our future work.
length =
?
????
????
lengthleft + lengthright
if lengthleft > 0
and lengthright > 0
0 otherwise
(4)
We can now define the two similarity functions
used in Equations 2 and 3 that we used for our ex-
periments:
simcont(C(f), C(fi)) = (1 + length)
? (5)
simpara(C(f), C(pi)) = (length)
? (6)
The rationale for these functions is the follow-
ing. Exact phrase examples add at least a transla-
tion count of 1, i.e. their translation is always taken
into account to estimate pcont. Paraphrase exam-
ples add a translation count of 0 if length = 0,
i.e. their translation is not taken into account at all
if surrounding n-gram similarity is too low. We used
? = ? = 1.5. Algorithm 1 describes how the two
models are estimated from the training data.
foreach phrase f in training file do
extract C(f);
/* phrase count */
foreach unique phrase fi in test(f)) do
extract C(fi);
compute simcont(C(f), C(fi));
end
/* paraphrase count */
foreach phrase pi in para(f) do
foreach unique phrase fj in test(pi) do
extract C(fj);
compute simpara(C(f), C(fj));
end
end
end
estimate pcont and para;
Algorithm 1: Model estimation for pcont and
ppara. Function test(f) returns all unique
phrases corresponding to phrase f from the test
file. Function para(f) return all phrases for
which f is a known paraphrase.
We implemented the following strategy to find
paraphrases for phrases in the test file. We extract all
662
Left context phrase/paraphrase Right context
IS#1 at the moment it is up to each member state to decide, and practice dif-
fers considerably from country to country
PE#1 ... as regards the terminal portion in the
cycle of nuclear fuel, it is
the responsability of each member state to define its own policy .
PT#1 la responsabilite? de chaque
IS#2 that is why i find it extremely regrettable that the amendment on harmonising the re-
registration of cars that have been involved
in accidents ...
PE#2 for all these reasons and given your most
excellent statement , i find it
a pity that the new legal base for the daphne pro-
gramme is so restrictive ...
PT#2 dommage que
Figure 6: Examples of paraphrases in context from the development file. The input sentence (IS) contains a source
phrase of interest (in bold), the paraphrase example (PE) contains a paraphrase of that source phrase (in bold) for
which a paraphrase translation (PT) is known.
paraphrases p for a phrase f by pivot: all target lan-
guage phrases e aligned to f are first extracted, and
all source language phrases p aligned to e are ex-
tracted. The following constraints are then applied
to define which paraphrases are kept:
? string p is not included in string f and vice
versa (in order to minimize the impact of align-
ment errors in the training corpus);
? the paraphrasing probability is greater than a
fixed threshold: para(f, p) ? 10?2, where
para(f, p) =
?
e p(e|f)p(p|e) (Bannard and
Callison-Burch, 2005);
? the number of occurrences of phrase f and
paraphrase p are equal or less than indepen-
dent thresholds: numOccs(f) ? 100 and
numOccs(p) ? 1000.14
Figure 6 shows examples of paraphrases in con-
text with high similarity with some original phrase,
and Figure 7 provides various statistics on the para-
phrases extracted on the test file.
4.3 Results and analysis
Automatic evaluation results are reported in Table 8
for various configurations. We also wanted to focus
our measures on content words, which are known
14The first threshold value was chosen as (Callison-Burch et
al., 2005) report it to be an optimal sample size for estimating
phrase translation probabilities. The relatively low value for the
second threshold was selected to reduce computation time.
phrase # phrases # paraphrased # paraphrases
length en fr en fr en fr
1 13,620 15,707 458 725 1,824 2,684
2 13,120 15,207 4,127 4,481 18,871 19,700
3 12,620 14,707 4,782 5,715 24,111 27,377
4 12,120 14,208 2,859 4,078 15,071 20,345
5 11,623 13,711 1,171 2,275 6,077 12,132
Figure 7: Statistics on numbers of phrases, numbers
of paraphrased phrases and numbers of paraphrases per
phrase length.
to be important as regards information content in
translation. We applied the contrastive lexical eval-
uation (CLE) methodology described in (Max et
al., 2010), which indicates how many times source
words grouped into user-defined classes were cor-
rectly translated or not across systems. These addi-
tional results are reported on Figure 9.
On English to French translation, both additional
features lead to improvements over the baseline
with all metrics, including CLE, and their combi-
nation shows a strong improvement in TER (-1.55).
CLE on content words reveals that the para feature
seems particularly effective in reducing the number
of words in all categories that only the baseline sys-
tem translated correctly.
Results on French to English translation are less
positive: neither cont nor para alone improve over
the baseline with any metrics. However, their com-
bination improves over the baseline with all met-
rics except BLEU, including a reduction of -1.07
in TER. Detailed analysis of CLE results shows
that the translation of adjectives and nouns benefited
663
more from using our two additional models. Verbs,
whose translation improved slightly, are strongly in-
flected in French, so finding examples for a given
form is more difficult than for less inflected word
categories, as is finding paraphrases with the appro-
priate inflection. Also, pivoting via English is one
reason why paraphrases obtained via a low-inflected
language can be of varying quality. Furthermore, the
simplicity of our context modeling may have been
ineffective in filtering out some bad examples. Over-
all, para was more effective with the low-inflected
English as the source language, improving over the
baseline with all metrics.
These results confirm that translation perfor-
mance can be improved by exploiting context and
paraphrases in the original training corpus only. We
next attempted to measure whether some improve-
ment in the quality of the paraphrases used would
have some measurable impact on translation perfor-
mance. To this end, we devised a semi-oracle ex-
periment in the following way: the source and target
test files were automatically aligned, and for each
source phrase possible target phrases (i.e., reference
translations) were extracted, and used as pivots to
extract potential paraphrases, which were then fil-
tered with the same constraints as previously. In
this way, we exploit the information that paraphrases
can at least produce the desired translation, but they
may also propose other incorrect translations and/or
be present in very few examples. Results appear
in the inf rows of Tables 8 and 9. We obtain the
most important improvement over the baseline in
BLEU for the two language pairs (resp. +0.99 and
+0.44), though the results for the other metrics for
French to English translation are more difficult to
interpret. For this language pair, possible reasons
include again that the pivot language may not be
appropriate, and also that the limitation to a sin-
gle pivot15 may not have produced more monolin-
gual variation that might have proved useful. CLE
on English to French, however, reveals significant
gains with a relative improvement over the baseline
of +116 content words. Under this condition, this
result shows that the higher the quality of the para-
phrases used, the more translation quality can be im-
15Several pivot phrases may in fact have been automatically
extracted for a given phrase, some of which being possible bad
candidates.
BLEU NIST TER METEOR
en?fr
bsl 30.28 - 6.66 - 57.86 - 54.79 -
+cont 31.11 +0.83 6.77 +0.11 57.24 -0.62 55.22 +0.43
+para 30.97 +0.69 6.74 +0.08 57.38 -0.48 55.39 +0.60
all 30.93 +0.65 6.84 +0.18 56.31 -1.55 55.28 +0.49
inf 31.27 +0.99 6.78 +0.12 57.22 -0.64 55.80 +1.01
fr?en
bsl 29.90 - 6.90 - 54.64 - 61.36 -
+cont 29.56 -0.34 6.89 -0.01 54.95 +0.31 60.98 -0.38
+para 29.70 -0.20 6.92 +0.02 54.64 +0.00 61.10 -0.26
all 29.75 -0.15 7.03 +0.13 53.57 -1.07 61.63 +0.27
inf 30.34 +0.44 6.93 +0.03 54.90 +0.26 60.99 -0.37
Figure 8: Automatic scores for the MOSES baseline sys-
tems (bsl), systems additionnally using the contextual
feature (+cont), systems additionnally using the para-
phrasing feature (+para), systems using both features
(all), and pivot-informed systems (inf).
Adj Adv Noun Verb
?
en?fr
+cont - 74 28 113 60 275+ 55 35 114 85 289 +14
+para - 62 12 82 46 202+ 58 32 111 78 279 +77
all - 72 25 91 72 260+ 50 37 118 97 302 +42
inf - 58 20 108 56 242+ 65 43 147 103 358 +116
fr?en
+cont - 30 16 80 69 195+ 15 21 69 46 151 -44
+para - 32 19 72 60 183+ 12 18 65 43 138 -45
all - 21 18 67 61 167+ 30 18 94 48 190 +23
inf - 38 21 83 66 208+ 31 23 106 57 217 +9
Figure 9: Contrastive lexical evaluation results per part-
of-speech measured on the test file. ?-? (resp. ?+?) rows
indicate the number of source words that only bsl (resp.
the compared system) correctly translated.
proved, which is in line with works that make use
of human-made paraphrases to improve translation
quality (Schroeder et al, 2009; Resnik et al, 2010).
Table 10 presents a typology of paraphrases found
in our development set and classifies the impact of
using them for phrase translation estimation. As can
be seen, more work is needed to better understand
the characteristics of the phrases that should be para-
phrased and of their paraphrases.
664
Type Impact Examples
Morphological variants +/- (yugoslav republic? yugoslavian republic), (go far? goes far)
Synonymy + (duties? obligations), (to look into? to study)
Grammatical word substitution ?/- (states in the? the states of the), (amendments by? amendments to)
Word deletion or insertion ?/- (first reading, the? first reading the), (amendments by? amendments
proposed by)
Syntactic rewritings + (approval of the majority ? majority support), (capacity of the euro-
pean union? european union?s ability)
Phrasal idiomatic substitutions + (must be said that the? goes without saying that the), (is fully in line
? is totally coherent), (is amazing? strikes me)
Context-dependent substitu-
tions
+/- (is not right? is unacceptable), (offer my? express my)
Alignment and translation prob-
lems
- (unnecessary if ? vital if), (the crime ? organized), (ill-advised ?
wise), (to begin by thanking? to begin by congratulating)
Figure 10: Main types of paraphrase pairs found in our dev. and training corpora. Pairs shown have length > 0.
5 Conclusion and future work
We have introduced an original way of exploiting
both context and paraphrasing for the estimation
of phrase translations in phrase-based SMT. To our
knowledge, this is the first time that paraphrases ac-
quired in an endogenous manner have been shown
to improve translation performance, which shows
that bilingual corpora can be better exploited than
they typically are. Our experiments further showed
the promises of our approach when paraphrases of
higher quality are available.
In the light of our results and our initial typology
of paraphrases presented on Figure 10, as well as
previous work on paraphrasing for SMT, the diffi-
cult question of what units should be paraphrased
for what success should be addressed, taking into ac-
count parameters such as language pairs, quantity of
training data and availability of external resources.
Our future work includes three main areas: first,
we want to improve the modeling of context, by no-
tably working on techniques inspired from Informa-
tion Retrieval to quickly access contextually-similar
examples of source phrases in bilingual corpora.
Such contextual sampling on large bilingual corpora
for phrases and their paraphrases, which could inte-
grate more complex linguistic information, will al-
low us to assess our approach on more challenging
conditions. This would also allow us to build con-
textual models on-the-fly, and experiment with us-
ing lattices to encode contextually estimated para-
phrases. Second, we will combine paraphrases ob-
tained via different techniques and resources, which
will allow us to also learn translation distributions
for phrases absent from the original corpus. Lastly,
we want to also exploit paraphrases for the addi-
tional translations that they propose (such as e4 on
Figure 3) and that would be contextually similar in
the target language to other existing translations of
a given phrase or that could even represent a new
sense of the original phrase.
Acknowledgements
This work was partly supported by ANR project
Trace (ANR-09-CORD-023). The author would like
to thank the anonymous reviewers for their helpful
questions and comments.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
Use of Comparable Corpora to Improve SMT perfor-
mance. In Proceedings of EACL, Athens, Greece.
Wilker Aziz, Marc Dymetman, Shachar Mirkin, Lucia
Specia, Nicola Cancedda, and Ido Dagan. 2010.
Learning an Expert from Human Annotations in Sta-
tistical Machine Translation: the Case of Out-of-
Vocabulary Words. In Proceedings of EAMT, Saint-
Raphael, France.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving statistical machine
translation by paraphrasing the training data. In Pro-
ceedings of IWSLT, Hawai, USA.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling Phrase-Based Statis-
665
tical Machine Translation to Larger Corpora and
Longer Phrases. In Proceedings of ACL, Ann Arbor,
USA.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Transla-
tion Using Paraphrases. In Proceedings of NAACL,
New York, USA.
Chris Callison-Burch. 2008. Syntactic Constraints on
Paraphrases Extracted from Parallel Corpora. In Pro-
ceedings of EMNLP, Hawai, USA.
Marine Carpuat and Dekai Wu. 2007. Context-
Dependent Phrasal Translation Lexicons for Statisti-
cal Machine Translation. In Proceedings of Machine
Translation Summit XI, Copenhagen, Denmark.
Marine Carpuat. 2009. One Translation Per Discourse.
In Proceedings of the NAACL-HLT Workshop on Se-
mantic Evaluations, Boulder, USA.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
Translation Using Source Language Paraphrase Lat-
tices. In Proceedings of EMNLP, Cambridge, USA.
Kevin Gimpel and Noah A. Smith. 2008. Rich Source-
Side Context for Statistical Machine Translation. In
Proceedings of the ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma, and
Andy Way. 2009. Using Supertags as Source Lan-
guage Context in SMT. In Proceedings of EAMT,
Barcelona, Spain.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the Transla-
tion Model for Statistical Machine Translation Based
on Information Retrieval. In Proceedings of EAMT,
Budapest, Hungary.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
NAACL HLT, New York, USA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of NAACL HLT, Edmonton, Canada.
Stanley Kok and Chris Brockett. 2010. Hitting the Right
Paraphrases in Good Time. In Proceedings of NAACL,
Los Angeles, USA.
Adam Lopez. 2008. Tera-Scale Translation Models
via Pattern Matching. In Proceedings of COLING,
Manchester, UK.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal & Sentential Paraphrases: A Survey of Data-
Driven Methods. Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are Multiple Reference
Translations Necessary? Investigating the Value of
Paraphrased Reference Translations in Parameter Op-
timization. In Proceedings of AMTA, Waikiki, USA.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceedings
of EMNLP, Singapore.
Aure?lien Max, Rafik Makhloufi, and Philippe Langlais.
2008. Explorations in using grammatical dependen-
cies for contextual phrase translation disambiguation.
In Proceedings of EAMT, Hamburg, Germany.
Aure?lien Max, Josep M. Crego, and Franc?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of LREC, Valletta, Malta.
Aure?lien Max. 2008. Local rephrasing suggestions for
supporting the work of writers. In Proceedings of Go-
TAL, Gothenburg, Sweden.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-Language Entailment Modeling for Translat-
ing Unknown Terms. In Proceedings of ACL, Singa-
pore.
Behrang Mohit and Rebecca Hwa. 2007. Localization
of Difficult-to-Translate Phrases. In Proceedings of
the ACL Workshop on Statistical Machine Translation,
Prague, Czech Republic.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Exploit-
ing Non-parallel Corpora. Computational Linguistics,
31(4).
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of ACL, short paper ses-
sion, Uppsala, Sweden.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kron-
rod, Alex Quinn, and Benjamin B. Bederson. 2010.
Improving Translation via Targeted Paraphrasing. In
Proceedings of EMNLP, Cambridge, USA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word Lattices for Multi-Source Translation. In Pro-
ceedings of EACL, Athens, Greece.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of AMTA, Boston, USA.
Nicolas Stroppa, Antal van den Bosch, and Andy Way.
2007. Exploiting Source Similarity for SMT using
Context-Informed Features. In Proceedings of TMI,
Skovde, Sweden.
Guillaume Wisniewski, Alexandre Allauzen, and
Franc?ois Yvon. 2010. Assessing Phrase-based Trans-
lation Models with Oracle Decoding. In Proceedings
of EMNLP, Cambridge, USA.
666
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 721?731, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generalizing Sub-sentential Paraphrase Acquisition
across Original Signal Type of Text Pairs
Aure?lien Max Houda Bouamor
LIMSI-CNRS & Univ. Paris Sud
Orsay, France
firstname.lastname@limsi.fr
Anne Vilnat
Abstract
This paper describes a study on the impact of
the original signal (text, speech, visual scene,
event) of a text pair on the task of both man-
ual and automatic sub-sentential paraphrase
acquisition. A corpus of 2,500 annotated sen-
tences in English and French is described, and
performance on this corpus is reported for
an efficient system combination exploiting a
large set of features for paraphrase recogni-
tion. A detailed quantified typology of sub-
sentential paraphrases found in our corpus
types is given.
1 Introduction
Sub-sentential paraphrases can be acquired from text
pairs expressing the same meaning (Madnani and
Dorr, 2010). If the semantic similarity of a text
pair has a direct impact on the quality of the ac-
quired paraphrases, it has, to our knowledge, never
been shown what impact the type of original sig-
nal has on paraphrase acquisition. In this work,
we consider four types of corpora, which we think
are representative of the main types of original
semantic signals: text pairs (roughly, sentences)
originating a) from independent translations of a
text (TEXT), b) from independent translations of a
speech (SPEECH), c) from independent descriptions
of a visual scene (SCENE), and d) from independent
descriptions of some event (EVENT). We will report
the results of experiments on sub-sentential para-
phrase acquisition on all these corpus types in two
languages, English and French, and provide some
answers to the following questions: What types of
paraphrases can be found by human annotators, with
what confidence and in which quantities? How well
can representative paraphrase acquisition systems
perform on each corpus type, and how performance
can be improved through combination? On what
corpus types can performance be improved by using
training material from other corpus types? Our ex-
perimental results will provide several indications of
the differences and complementarities of the corpus
types under study, and will notably show that perfor-
mance on the most readily available corpus type can
be improved by using training data from the set of
all other corpus types.
We will first describe the building procedures
and characteristics of our corpora (section 2), and
then describe our experimental settings for evalu-
ating paraphrase acquisition (section 3.1). Our ex-
periments will first consist of the description (sec-
tion 3.2) and evaluation (section 3.3) of a system
combination on each corpus type and then of our
system provided with additional training data from
the other corpus types (section 3.4). We will finally
briefly review related work (section 4) and discuss
our main findings and future work (section 5).
2 Collection of sentence pair corpora
In this study, we will focus on paraphrase acquisition
from related sentence pairs characteristic of 4 corpus
types, which correspond to different original signal
types of text pairs illustrated by the word alignment
matrices on Figure 1. A corpus for each type has
been collected for 2 languages, English and French,
and comprises 625 sentence pairs per language. We
now briefly describe how each corpus was built.
721
It
is
estimated
that
the
total
annual
volume
of
import
and
export
will
exceed
9
billion
US
dollars.
It is an
tic
ipa
ted
tha
t
the an
nu
al
tot
al
fo
rei
gn
tra
de
vo
lum
e
wi
ll
ex
ce
ed
US
$9
bil
lio
n
.
So
he
uses
the
photo
booths
to
remind
people
what
he
looks
like
.
He us
es
th
os
e
m
ac
hi
ne
s
to re
m
in
d
th
e
liv
in
g
of hi
s
fa
ce
.
a
boy
is
riding
on
a
bicycle
fast
.
a bo
y
rid
es
a bi
ke
on a di
rt
ro
ad
.
Pigeons
have
an
understanding
of
numbers
on
par
with
primates
Pi
ge
on
s
ha
ve
nu
m
er
ica
l
ab
ili
tie
s
ju
st
lik
e
pr
im
ate
s
Figure 1: Example reference alignment matrices for
(from top to bottom) TEXT, SPEECH, SCENE and
EVENT. Sure alignments appear in green or gray (identi-
ties) and possible alignments in yellow.
TEXT For English, we used the MTC corpus1 (de-
scribed in (Cohn et al2008)) consisting of sets
of news article translations from Chinese, and for
French the CESTA corpus2 consisting of sets of
news article translations from English. For each
sentence cluster, we selected sentence pairs with
minimal edit distance above an empirically-selected
threshold, covering all clusters first and then select-
ing from already used clusters to reach the target
number of sentence pairs.
e.g. It is estimated that the total annual volume of import
and export will exceed 9 billion US dollars. ? It is an-
ticipated that the annual total foreign trade volume will
exceed US$9 billion.
SPEECH For English, we used two freely avail-
able subtitle files3 of the French movies Le Fabuleux
Destin d?Ame?lie Poulain and Les Choristes, and for
French we used two subtitle files from the Desperate
Housewives TV series. We first aligned each paral-
lel corpus using the algorithm described in (Tiede-
mann, 2007), based on time frames and developed
for bilingual subtitles, we then filtered out sentence
pairs below a minimal edit distance threshold, and
manually removed obvious errors made by the algo-
rithm.
e.g. So he uses the photo booths to remind people what
he looks like. ? He uses those machines to remind the
living of his face.
SCENE We used the Multiple Video Description
Corpus (Chen and Dolan, 2011) obtained from mul-
tiple descriptions of short videos. Similarly to what
we did for TEXT, we selected sentence pairs from
clusters by minimal edit distance above a threshold.
An important fact is that for English we were able
to use what is described as ?verified? descriptions.
There were, however, far fewer descriptions avail-
able for French, and none had the ?verified? status.
We decided to use this corpus nonetheless, but with
the knowledge that this source for French is of a sub-
stantially lower quality (this corpus type will there-
fore appear as ?(SCENE)? in all tables to reflect this).
e.g. a boy is riding on a bicycle fast. ? a boy rides a bike
on a dirt road.
1http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2002T01
2http://www.elda.org/article125.html
3http://www.opensubtitles.org
722
Corpus statistics Annotator agreements Tokens in paraphrase statistics
500 sentence pairs 50 sentence pairs not considering identity paraphrases
sure para. possible para.
# tokens # tokens per sent. sure para. possible para. % tokens # tokens % tokens # tokens
ENGLISH
TEXT 21,473 21.0 66.1 20.4 18.6 4004 12.3 2651
SPEECH 11,049 10.5 79.1 10.9 17.5 1942 31.6 3500
SCENE 7,783 7.5 80.5 35.2 10.9 851 14.0 1094
EVENT 8,609 8.0 65.3 20.5 17.5 1506 14.5 1251
FRENCH
TEXT 24,641 24.0 64.6 16.6 29.2 7218 6.2 1527
SPEECH 11,850 11.5 82.7 20.8 22.5 2667 16.7 1981
(SCENE) 7,012 6.5 42.8 9.3 3.9 275 9.4 664
EVENT 9,121 9.1 67.8 3.8 19.6 1793 9.6 876
Table 1: Description of all corpora and paraphrase reference sets for English (top) and French (bottom). Note that
SCENE for French appears within parentheses as we do not consider it of the same quality as the other corpora.
EVENT We used titles of news article clusters
from the Google News4 news aggregation service.
We further refined the clustering algorithm by filter-
ing out article pairs whose publication dates differed
from more than one day. We repeated the same se-
lection procedure as for TEXT and SCENE to have
a maximal cluster coverage and select more similar
pairs first.
e.g. Pigeons Have an Understanding of Numbers on Par
With Primates ? Pigeons Have Numerical Abilities Just
Like Primates
Table 1 provides various statistics for these cor-
pora. The first observation is that TEXT contains sig-
nificantly larger sentences than the other types, more
than twice as long as those of SPEECH. Annotation
was performed following the guidelines proposed by
Cohn et al2008)5 using the YAWAT tool (Germann,
2008), except that alignments where not initially ob-
tained automatically so as not to bias our annota-
tors? work (there were two annotators per language).
The main guidelines that they had to follow were
that sure and possible paraphrases must be distin-
guished, smaller alignments were to be prefered but
any-to-any alignments may be used, and sentences
should be aligned as much as possible. Henceforth,
we will only consider for all reported statistics and
experiments those paraphrases that are not identity
pairs (e.g. (a nice day ? a nice day)), as they are
4http://news.google.com
5See http://staffwww.dcs.shef.ac.uk/
people/T.Cohn/paraphrase_guidelines.pdf
considered trivial as far as acquisition is concerned.
Table 1 also reports inter-annotator agreement6
values computed on sets of 50 sentence pairs. We
find that acceptable values are obtained for sure
paraphrases, but that low values are obtained for
possible paraphrases. This was somehow expected,
given the many possible interpretations of possible
paraphrases, but was not a problem for our experi-
ments: as we will describe in section 3.1, the evalua-
tion metrics we use will not count them as expected
solutions, but will simply not count them as false
when proposed as candidates.
Table 1 finally shows proportions and absolute
numbers of paraphrases of each type for all corpora.
We find that there are approximately the same to-
tal number of paraphrases for English (16,799) and
French (17,001), but that English corpora collec-
tively have an equivalent number of sure and pos-
sible paraphrases (8,303 vs. 8,496) and French have
more sure paraphrases (11,953 vs. 5,048). This may
be explained by the fact that our annotators worked
independently and that the corpora used have dif-
ferences by nature, as our experiments will show.
Other salient results include the fact that TEXT con-
tains more sure paraphrases in number than the other
corpora, that SPEECH contains relatively more pos-
sible paraphrases than the other corpora, and that
SCENE has significantly fewer paraphrases, both in
proportion and number. In Figure 2 various mea-
6For each paraphrase type, we used the average of recall
values obtained for each annotator set as the reference .
723
synonymy typography tense inclusion pragmatics syntax morphology number
ENGLISH
TEXT 51.2 7.6 5.1 12.1 0.6 4.4 12.1 6.4
SPEECH 39.8 25.6 3.5 12.3 1.7 3.5 3.5 9.7
SCENE 50.0 1.3 13.5 21.6 0.0 1.3 5.4 6.7
EVENT 36.9 15.0 8.2 19.1 1.3 6.8 6.8 5.4
FRENCH
TEXT 46.9 9.0 8.7 2.1 3.6 6.6 3.0 19.8
SPEECH 45.5 14.2 8.0 8.0 2.6 11.6 3.5 6.2
(SCENE) 46.4 5.3 3.5 8.9 0.0 5.3 0.0 30.3
EVENT 28.3 19.7 6.1 16.0 7.4 8.6 7.4 6.1
Table 2: Percentages of paraphrase classes in 50 randomly selected sentence pairs for reference paraphrases for English
(top) and French (bottom). Classes are illustrated by the following examples: (mutual understanding ? consensus)
(synonymy), (California ? CA) (typography), (letting ? having let) (tense), (Asian Development Bank ? Asian
Bank) (inclusion), (police dispatcher ? woman) (pragmatics), (grief-stricken ? struck with grief ) (syntactic), (Viet-
name ? Vietnam) (morphology), (mortgage ? mortgages) (number).
sures of sentence pair similarities are given. TEXT
contains the most similar sentence pairs according to
all metrics, with EVENT at a similar level on French.
SCENE has sentence pairs that are more similar than
those in SPEECH for English, but this is not the case
for French. While the metrics used can only provide
a crude account of semantic equivalence at the sen-
tence level, these results clearly indicate that trans-
lating from text yields more similar sentences than
translating from speech.
Table 2 provides a typology of paraphrases found
in all our corpora and two languages, where each
class has been quantified with respect to the refer-
ence alignments.7 The main observation here is that
phrasal synonymy (e.g. mutual understanding ?
consensus) is the most present phenomenon. It is
also interesting to note that the EVENT corpus type,
which is easy to collect on a daily basis, contains ref-
erence paraphrases spread over all classes. Lastly, it
is expected that paraphrases in the pragmatics class
(e.g. police dispatcher ? woman) would be diffi-
cult to acquire, as this would often rely on document
context and costly world knowledge.8
7Note that typologies of paraphrases have already been pro-
posed in the literature (e.g. (Culicover, 1968; Vila et al2011)),
but that the choice of our classes has been primarily moti-
vated by potential subsequent uses of the acquired paraphrases
(paraphrases could be annotated as belonging to more than one
class). Note also that our experiments will also include results
focused on the synonymy class only (cf. Table 5).
8Reusing such types of paraphrases into applications would
however often be too strongly context-dependent.
  COSINE*100 BLEU 1-TER METEOR010
203040
506070 TEXT SPEECH SCENE EVENT
  COSINE*100 BLEU 1-TER METEOR010
203040
506070
Figure 2: Sentence pair average similarities for all cor-
pora for English (left) and French (right) using the co-
sine of token vectors, BLEU (Papineni et al2002),
TER (Snover et al2006) and METEOR (Lavie and
Agarwal, 2007).
3 Bilingual experiments across corpus
types
3.1 Evaluation of paraphrase acquisition
We followed the PARAMETRIC methodology de-
scribed in (Callison-Burch et al2008) for assess-
ing the performance of systems on the task of sub-
sentential paraphrase acquisition. In this methodol-
ogy, a set of paraphrase candidates extracted from
a sentence pair is compared with a set of reference
paraphrases, obtained through human annotation, by
computing usual measures of precision (P ) and re-
call (R). The first value corresponds to the propor-
tion of paraphrase candidates, denoted H, produced
by a system and that are correct relative to the ref-
erence set containing sure and possible paraphrases,
denoted Rall. Recall is obtained by measuring the
proportion of the reference set of sure paraphrases,
724
  TEXT      SPEECH     SCENE       EVENT
GIZA
FASTR
TERp
PIVOT
biphrases
biphrases
biphrases
biphrases
combinationsystem
paraphrases
training and test instances of sentencepairs
union of biphraseswith features
Original signal type of text pairs
Figure 3: Architecture of our combination system for
paraphrase identification.
denoted Rsure, that are found by a system. We also
computed an F-measure value (F1), which consid-
ers recall and precision as equally important. These
values are thus given by the following formulae:
P =
|H ? Rall|
|H|
R =
|H ? Rsure|
|Rsure|
F1 =
2PR
P +R
Note that the way the sets Rall and Rsure of refer-
ence paraphrase pairs are defined ensures that para-
phrase pair candidates that include possible refer-
ence paraphrases will not penalize precision while
not increasing recall.
All performance values reported in the follow-
ing sections will be obtained using 10-fold cross-
validation and averaging the results on each sub-test.
All data sets of cross-validation contain 500 sen-
tence pairs per corpus type, and 125 pairs are kept
for development.
3.2 A framework for sub-sentential paraphrase
identification
We now describe the systems that will be tested
on the various corpora described in section 2 using
the methodology described in section 3.1. Follow-
ing (Bouamor et al2012), a combination system
is used to automatically weight paraphrase pair can-
didates produced by individual systems using a set
of features aiming at recognizing paraphrases, as il-
lustrated on Figure 3. Four individual systems have
been used and are described below: the reasons for
considering those systems include their free avail-
ability, the possibility of using comparable resources
when relevant for our two languages, and the spe-
cific characteristics of the techniques used.
Statistical learning of word alignments (GIZA)
The GIZA++ tool (Och and Ney, 2004) com-
putes statistical word alignment models of increas-
ing complexity from parallel corpora. It was run
on each monolingual corpus of sentence pairs in
both directions, symmetrized alignments were kept
and classical phrase extraction heuristics were ap-
plied (Koehn et al2003), without growing phrases
with unaligned tokens.
Linguistic knowledge on term variation (FASTR)
The FASTR tool (Jacquemin, 1999) spots term vari-
ants in large corpora, where variants are described
through metarules expressing how the morphosyn-
tactic structure of a term variant can be derived
from a given term by means of regular expressions
on morphosyntactic categories. Paradigmatic varia-
tion can also be expressed with constraints between
words, imposing that they be of the same morpho-
logical or semantic family using existing resources
available in our two languages. Variants for all
phrases from one sentence of a pair are extracted
from the other sentence, and the intersection of the
sets for both directions is kept.
Edit rate on word sequences (TERp) The TERp
tool (Snover et al2010) can be used to compute an
optimal set of word and phrase edits that can trans-
form one sentence into another one.9 Edit types are
parameterized by one or more weights which were
optimized towards F-measure by hill climbing with
100 random restarts using the held-out data set con-
sisting of 125 sentence pairs for each corpus type.
Translational equivalence (PIVOT) We exploited
the paraphrase probability defined by Bannard and
Callison-Burch (2005) on bilingual parallel corpora.
We used the Europarl corpus10 of parliamentary de-
bates in English and French, consisting of approx-
imately 1.7 million parallel sentences, using each
language as source and pivot in turn. GIZA++
9Note that contrarily to what TERp allows, we did not used
the possibility of using word or phrase equivalents as those are
only made available for English. This type of knowledge is
however captured in part by the FASTR and PIVOT systems.
10http://statmt.org/europarl
725
Phrase pair features ? edit distance between paraphrases, stem identity, bag-of-tokens similarity, phrase
length ratio
Sentence pair features ? sentence pair similarity (cosine, BLEU, TER, METEOR), relative position of
paraphrases, presence of common tokens at paraphrase boundaries, presence of another paraphrase pair
from each system at paraphrase boundaries, presence of a paraphrase at a different position in the other
sentence
Distributional features ? similarity of token context vectors for each phrase of a paraphrase (derived
from counts in the large English-French parallel corpus from WMT?11 (http://www.statmt.org/
wmt11/translation-task.html) (approx. 30 million parallel sentences)
System features ? combination of the individual systems that proposed the paraphrase pair
Table 3: Features used by our classifiers. Discretized intervals based on median values are used for real values, and
binarized values are used for combinations.
was used for word alignment and phrase transla-
tion probabilities were estimated from them by the
MOSES system (Koehn et al2007). For each
phrase of a sentence pair, we built its set of para-
phrases, and extracted its paraphrase from the other
sentence with highest probability. We repeated this
process in both directions, and finally kept for each
phrase its paraphrase pair from any direction with
highest probability.
Automatic validation of candidate paraphrases
Taking the union of all paraphrase pair candidates
from all the above systems for each sentence pair, we
perform a Maximum Entropy two-class classifica-
tion11, which allows us to include features that were
not necessarily exploited or straightforward to ex-
ploit by individual systems to determine the proba-
bility that each candidate is a good paraphrase. More
generally, this allows us to attempt to learn a more
generic characterization of paraphrases, which could
trivially accept any number of systems as inputs.
Positive examples for the classifier are those from
the union of candidates that are also in the reference
set Rsure, while negative examples are the remaining
ones from the union. The features that we used are
summarized in Table 3.
3.3 Experimental results
Results for individual systems, their union and our
validation system trained on each corpus type are
given on Table 4. First, we find that all individual
systems fare better on TEXT, for which more train-
ing data were available and where semantic equiv-
11Using the implementation at: http://homepages.
inf.ed.ac.uk/lzhang10/maxent_toolkit.html
alence of sentence pairs is most likely. EVENT ap-
pears to be the most difficult corpus type, whereas
one could say that being the most readily data source
this is a disapointing result: we will return to this in
section 3.4. In terms of performance on F-measure
per corpus type, GIZA performs best for TEXT and
SPEECH, containing long sentences with possible
repetitions, while TERp performs on par with GIZA
for SCENE and best for EVENT, where equivalences
that are rare at the corpus level are more present.
FASTR achieves a very low recall, showing that the
encoded definitions of term variants do not cover all
types of paraphrases, and also possibly that the lex-
ical resource that it uses has incomplete coverage.
It nonetheless obtains high precision values, most
notably on TEXT. One last comment regarding in-
dividual systems is that PIVOT is by far the most
precise of all the techniques used, but with a recall
much lower than those of GIZA and TERp: as is
the case for FASTR, which makes use of manually-
encoded lexical resources, PIVOT encodes in some
sense some kind of semantic knowledge.12
In all cases, our combination system manages
to increase F-measure substantially over the best
individual system for a corpus type and the sim-
ple union. Improvements are strong on TEXT
(resp. +12.5 and +11.6 on English and French)
and on SPEECH (+11.7 and +11.1) and quite good
on SCENE (+3.2 and +6.4) and on EVENT (+5.4
12Note that the fact that English and French were used as the
pivot for one another may have had some positive effect here,
but, incidentally, the two corpora obtained by translating from
the other language (TEXT and SPEECH) are not those where
PIVOT fares better. The difference observed may however lie in
the higher complexity of the sentences in these corpus types.
726
Individual systems Combination systems
GIZA FASTR TERp?F PIVOT union validation
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
ENGLISH
TEXT 48.2 58.9 53.0 63.1 5.9 10.7 41.2 66.4 50.9 73.4 25.8 38.2 20.8 80.8 33.1 68.4 62.8 65.5
SPEECH 39.7 44.2 41.8 27.1 3.5 6.3 25.0 50.3 33.4 79.2 15.3 25.7 25.5 71.4 37.6 51.0 56.3 53.5
SCENE 44.8 57.7 50.5 47.4 5.2 9.5 40.1 67.9 50.4 84.6 14.6 25.0 36.2 83.4 50.5 44.9 66.8 53.7
EVENT 19.0 33.9 24.3 62.9 3.1 6.0 28.8 68.7 40.6 97.4 11.2 20.1 20.8 75.5 32.7 35.0 67.1 46.0
FRENCH
TEXT 52.5 58.9 55.5 56.9 4.9 9.1 46.4 61.4 52.8 64.5 30.3 41.2 41.5 77.9 54.1 74.7 61.0 67.1
SPEECH 44.0 54.9 48.9 30.7 4.3 7.6 34.8 60.2 44.1 75.5 19.0 30.4 31.4 76.2 44.5 60.2 59.7 60.0
(SCENE) 14.4 43.6 21.7 53.0 4.0 7.4 13.8 75.3 23.4 94.6 5.21 9.8 12.7 86.4 22.2 19.9 59.8 29.8
EVENT 28.7 44.2 34.8 34.4 2.3 4.3 29.9 58.9 39.7 79.5 15.0 25.2 25.2 72.5 37.4 40.0 56.3 46.8
Table 4: Evaluation results for individual systems (left) and combination systems (right) on all corpus types for English
(top) and French (bottom). Values in bold are for highest values for a given metric for each corpus type and language.
and +6.1). Recall from Table 1 that TEXT and
SPEECH were the two corpus types with the highest
number of sure paraphrase examples for both lan-
guages: results show that our classifier was able to
efficiently use them.
Recall values for the union are quite strong for
all corpus types, ranging from 71.4 (SPEECH in En-
glish) to 83.4 (SCENE in English). There is, how-
ever, a substantial decrease between the unions and
the results of our combination systems, although
recall values for our systems are roughly between
56 and 67, which may be considered an acceptable
range on such a task. Further study of false neg-
atives should help with engineering new features to
improve paraphrase recognition. Lastly, we note that
precision is in general highest for a specific system
(PIVOT), and reaches high values for our validation
system on TEXT, where we have the most examples
(resp. 68.4 and 74.7 for English and French).
As seen in Table 2, synonymy is the most present
phenomenon in all our corpora; it is also proba-
bly one of the most useful type of knowledge for
many applications. We now therefore focus on this
class, for which all the sure paraphrases in our cor-
pora falling in this class have been annotated. Ta-
ble 5 shows F-measure values for the individual
techniques and our combination systems on all cor-
pus types. We first observe that our combination sys-
tem also always improves here over the best individ-
ual system, albeit not by a large margin on EVENT.
GIZA FASTR TERp PIVOT validation
ENGLISH
TEXT 52.2 6.1 47.3 47.1 68.1
SPEECH 42.6 5.0 30.3 39.5 54.9
SCENE 51.8 6.0 48.0 26.0 56.3
EVENT 22.5 2.1 34.8 24.7 35.5
FRENCH
TEXT 55.3 3.9 50.7 50.5 70.3
SPEECH 49.8 1.6 40.9 36.2 57.2
(SCENE) 19.6 4.2 23.1 0.0 24.7
EVENT 36.8 3.5 35.3 25.6 39.9
Table 5: F-measure values for test instances in the syn-
onymy class (see Table 2) for all individual systems and
our validation system for English (top) and French (bot-
tom).
Also, we find that PIVOT performs relatively closer
to GIZA and TERp on TEXT and SPEECH than for
the full set of classes, confirming the intuition that
translational equivalence may be appropriate to rec-
ognize synonymy.
3.4 Experiments across corpus types
To test how different the corpora under study are as
regards paraphrase identification, we now consider
using as additional training data for our classifiers
corpora of the other types, both individually and col-
lectively. Results are given on Table 6.13
13Note that our results are still given by performing cross-
validation averaging over 10 test sets for each tested corpus
type.
727
+TEXT +SPEECH +SCENE +EVENT +All
ENGLISH
# ex+ 7,342 2,296 1,784 1,171 12,593
TEXT 65.5 66.2 65.1 66.2 65.1
SPEECH 56.0 53.5 52.8 54.8 56.6
SCENE 49.7 54.3 53.7 53.8 42.7
EVENT 51.1 45.3 42.5 46.0 56.2
FRENCH
# ex+ 12,961 3,340 966 2,160 19,427
TEXT 67.1 67.2 66.7 67.0 66.6
SPEECH 57.6 60.0 56.4 59.6 57.9
(SCENE) 23.7 22.0 29.8 23.9 21.1
EVENT 45.2 45.6 44.3 46.8 49.3
Table 6: Evaluation results (F1 scores) for all corpus
types for English (top) and French (bottom) when adding
training material from other corpus types (values with
gray background on the diagonal are when no additional
training data are used). ?#ex+? rows indicate numbers of
positive paraphrase examples for each additional corpus
type.
The most notable observation is that EVENT is
substantially improved by using all available addi-
tional training data for English (+10.2), and to a
lesser extent for French (+2.5) . It should be noted
that no individual corpus type, save TEXT, individu-
ally improves results on EVENT, and that results are
yet substantially improved over the use of training
data from TEXT when using all available data, re-
vealing a collective contribution of all corpus types.
The second major observation is that all other cor-
pus types seem to be quite specific in nature, as no
addition of training data from other types yields any
improvement (with the exception of SPEECH on En-
glish), but they often in fact decrease performance.
For instance, SCENE in English is substantially neg-
atively impacted by the use of the numerous exam-
ples of TEXT (-4 in F-measure) and even more when
using all other training data (-9). This underlines
the specific nature of this corpus type: independent
descriptions of the same scene in a video may be
worded with much variation that mostly differ from
that present in other corpus types.
Our main conclusion here is therefore that all our
corpora under study are quite specific in nature, but
that EVENT can benefit from all training data from
the other corpus types. We can further note that the
fact that TEXT is almost not impacted by additional
data may also be explained by the fact that this cor-
pus type contains more than half of the total number
of examples for the two languages. Finally, there are
substantially more positive paraphrase examples for
French (19,427) than for English (12,593).
4 Related work
Over the years, paraphrase acquisition and genera-
tion have attracted a wealth of research works that
are too many to adequatly summarize here: (Mad-
nani and Dorr, 2010) presents a complete and up-
to-date review of the main approaches. Sentential
paraphrase collection has been tackled from specific
resources increasing the probability of sentences be-
ing paraphrases (Dolan et al2004; Bernhard and
Gurevych, 2008; Wubben et al2009), from com-
parable monolingual corpora (Barzilay and Elhadad,
2003; Fung and Cheung, 2004; Nelken and Shieber,
2006), and even at web scale (Pasc?a and Dienes,
2005; Bhagat and Ravichandran, 2008).
Various techniques have been proposed for para-
phrase acquisition from related sentence pairs
(Barzilay and McKeown, 2001; Pang et al2003)
and from bilingual parallel corpora (Bannard and
Callison-Burch, 2005; Kok and Brockett, 2010).
The issue of corpus construction for developing and
evaluating paraphrase acquisition techniques are ad-
dressed in (Cohn et al2008; Callison-Burch et al
2008). To the best of our knowledge, this is the first
time that a study in paraphrase acquisition is con-
ducted on several corpus types and for 2 languages.
Faruqui and Pado? (2011) study the acquisition of en-
tailment pairs (premise and hypothesis), with ex-
periments in 3 languages and various domains of
newspaper corpora for one language. Although their
work is not directly comparable to ours, they report
that robustness across domains is difficult to achieve.
Laslty, the evaluation of automatically generated
paraphrases has recently received some attention
(Liu et al2010; Chen and Dolan, 2011; Met-
zler et al2011) although it remains a difficult is-
sue. Application-driven paraphrase generation pro-
vides indirect means of evaluating paraphrase gen-
eration (Zhao et al2009). For instance, the field of
Statistical Machine Translation has produced works
showing both the usefulness of human-produced
728
(Schroeder et al2009; Resnik et al2010) and au-
tomatically produced paraphrases (Madnani et al
2008; Marton et al2009; Max, 2010; He et al
2011) for improving translation performance.
5 Discussion and future work
This work has addressed the issue of sub-sentential
paraphrase acquisition from text pairs. Analogu-
ously to bilingual parallel corpora, which are still
to date the most reliable resources for automatic ac-
quisition of sub-sentential translations, monolingual
parallel corpora are generally regarded as very ap-
propriate for paraphrase acquisition. However, their
low availability makes searching for less parallel
corpora a necessity. In this study, we have attempted
to identify corpora of various degrees of semantic
textual similarity by considering text pairs originat-
ing from various signal types. These signal types
allow various degrees of freedom as to how to for-
mulate a text: a text is read and translated into a dif-
ferent language (TEXT); some speech is listened to
in the context of a visual story and translated into a
different language (SPEECH); some action is looked
at and described (SCENE); and some event that took
place is concisely reported (EVENT).
The results presented in this paper have shown
how these corpora differed in various aspects. First,
they contain varying quantities of paraphrases that
are differently distributed into paraphrase classes.
Individual acquisition techniques, based on statis-
tical learning of word alignments (GIZA), linguis-
tic knowledge on term variation (FASTR), edit rate
on word sequence (TERp), and translational equiv-
alence (PIVOT), for which different performances
were observed among them on the same corpus
type, were shown to achieve different performances
across corpus types. An efficient combination of
candidate paraphrases from these individual tech-
niques exploiting additional features to character-
ize paraphrases has yielded substantial increases in
performance on all corpus types; however, it is in-
teresting to note that the highest amplitude in per-
formance across corpus types was not so much on
recall (amplitude of 10.5 on English and 4.7 on
French) than on precision (amplitude of 33.4 on En-
glish and 34.714 on French). This, some other fac-
14Not considering (SCENE) for French.
tors aside, emphasizes the fact that the correct idenfi-
cation of paraphrases is facilitated when equivalence
of semantic content is more probable. Many works
have accordingly attempted to identify text units that
are as parallel as possible from large corpora, and
the task of measuring semantic textual similarity,
which can find many uses, has received some atten-
tion lately (Agirre et al2012). However, it itself
relies on some knowledge on paraphrasing.
Our avenues for future work lie in three main ar-
eas. The first one is to continue our current line of
work and study the impact of additional individual
acquisition techniques and better characterizations
of paraphrases in context, in tandem with working
on identifying parallel text pairs in large corpora.
Another avenue is to start from the output of high
recall techniques and to attempt to characterize the
contexts of possible substitution for candidate para-
phrases from large corpora as a means to acquire
precise paraphrases. As the examples from Table 7
show, some classes of paraphrases, and in particular
in the continuum from our synonymy to pragmat-
ics classes, require the joint acquisition of contextual
information that license substitution. Lastly, we plan
to apply such knowledge in text-to-text applications.
synonymy
TEXT take part in ? participate in
great assistance ? enormous help
SPEECH make a deal ? come to an agreement
I don?t care ? I don?t give a damn
SCENE riding a bicycle ? cycling
lady ? woman
EVENT jail escapee ? prison fugitive
apologizes ? expresses regret
pragmatics
TEXT flew in ? arrived in
flood-control materials ? needed supplies
SPEECH face ? picture
want to sleep ? dream about sleeping
SCENE a man ? someone
bento ? food
EVENT violence ? bloodshed
anger ? emotion
Table 7: Examples in English for the synonymy and
pragmatics classes.
729
Acknowledgements
The authors would like to thank the reviewers for
their comments and suggestions. This work was
partly funded by ANR project Edylex (ANR-09-
CORD-008).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
SemEval, Montre?al, Canada.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP, Sapporo, Japan.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL, Toulouse, France.
Delphine Bernhard and Iryna Gurevych. 2008. Answer-
ing Learners? Questions by Retrieving Question Para-
phrases from Social Q&A Sites. In Proceedings of the
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, Columbus, USA.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2012.
Validation of sub-sentential paraphrases acquired from
parallel monolingual corpora. In EACL, Avignon,
France.
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
2008. Parametric: An automatic evaluation metric for
paraphrasing. In Proceedings of COLING, Manch-
ester, UK.
David Chen and William Dolan. 2011. Collecting highly
parallel data for paraphrase evaluation. In Proceedings
of ACL, Portland, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4).
P. W. Culicover. 1968. Paraphrase Generation and Infor-
mation Retrieval from Stored Text. Mechanical Trans-
lation and Computational Linguistics, 11:78?88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING, Geneva, Switzerland.
Manaal Faruqui and Sebastian Pado?. 2011. Acquiring
entailment pairs across languages and domains: A data
analysis. In Proceedings of the International Con-
ference on Computational Semantics (IWCS), Oxford,
UK.
Pascale Fung and Percy Cheung. 2004. Multi-level
bootstrapping for extracting parallel sentences from a
quasi-comparable corpus. In Proceedings of COLING,
Geneva, Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-HLT,
demo session, Columbus, USA.
Wei He, Shiqi Zhao, Haifeng Wang, and Ting Liu. 2011.
Enriching SMT Training Data via Paraphrasing. In
Proceedings of IJCNLP, Chiang Mai, Thailand.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, College Park, USA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of NAACL-HLT, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL, demo session, Prague, Czech Repub-
lic.
Stanley Kok and Chris Brockett. 2010. Hitting the Right
Paraphrases in Good Time. In Proceedings of NAACL,
Los Angeles, USA.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the ACL Workshop on Statistical Machine Translation,
Prague, Czech Republic.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
PEM: A paraphrase evaluation metric exploiting par-
allel texts. In Proceedings of EMNLP, Cambridge,
USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
Driven Methods . Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are multiple reference
translations necessary? investigating the value of
paraphrased reference translations in parameter opti-
mization. In Proceedings of AMTA, Waikiki, USA.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceedings
of EMNLP, Singapore.
730
Aure?lien Max. 2010. Example-Based Paraphrasing for
Improved Phrase-Based Statistical Machine Transla-
tion. In Proceedings of EMNLP, Cambridge, USA.
Donald Metzler, Eduard Hovy, and Chunliang Zhang.
2011. An empirical evaluation of data-driven para-
phrase generation techniques. In Proceedings of ACL-
HLT, Portland, USA.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of EACL, Trento, Italy.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
Philadelphia, USA.
Marius Pasc?a and Peter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across the
Web. In Proceedings of IJCNLP, Jeju Island, South
Korea.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin B. Bederson. 2010. Im-
proving translation via targeted paraphrasing. In Pro-
ceedings of EMNLP, Cambridge, USA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word Lattices for Multi-Source Translation. In Pro-
ceedings of EACL, Athens, Greece.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of AMTA, Boston, USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Jo?rg Tiedemann. 2007. Building a Multilingual Paral-
lel Subtitle Corpus. In Proceedings of the Conference
on Computational Linguistics in the Netherlands, Leu-
ven, Belgium.
Marta Vila, M. Anto`nia Mart??, and Horacio Rodr??guez.
2011. Paraphrase Concept and Typology. A Linguisti-
cally Based and Computationally Oriented Approach.
Procesamiento del Lenguaje Natural, (462-3).
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and maching
headlines for automatic paraphrase acquisition. In
Proceedings of the European Workshop on Natural
Language Generation, Athens, Greece.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven Statistical Paraphrase Generation.
In Proceedings of ACL-IJCNLP, Singapore.
731
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 716?725,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Validation of sub-sentential paraphrases acquired
from parallel monolingual corpora
Houda Bouamor Aure?lien Max
LIMSI-CNRS & Univ. Paris Sud
Orsay, France
firstname.lastname@limsi.fr
Anne Vilnat
Abstract
The task of paraphrase acquisition from re-
lated sentences can be tackled by a variety
of techniques making use of various types
of knowledge. In this work, we make the
hypothesis that their performance can be
increased if candidate paraphrases can be
validated using information that character-
izes paraphrases independently of the set of
techniques that proposed them. We imple-
ment this as a bi-class classification prob-
lem (i.e. paraphrase vs. not paraphrase),
allowing any paraphrase acquisition tech-
nique to be easily integrated into the com-
bination system. We report experiments on
two languages, English and French, with
5 individual techniques on parallel mono-
lingual parallel corpora obtained via multi-
ple translation, and a large set of classifi-
cation features including surface to contex-
tual similarity measures. Relative improve-
ments in F-measure close to 18% are ob-
tained on both languages over the best per-
forming techniques.
1 Introduction
The fact that natural language allows messages
to be conveyed in a great variety of ways consti-
tutes an important difficulty for NLP, with appli-
cations in both text analysis and generation. The
term paraphrase is now commonly used in the
NLP litterature to refer to textual units of equiva-
lent meaning at the phrasal level (including single
words). For instance, the phrases six months and
half a year form a paraphrase pair applicable in
many different contexts, as they would appropri-
ately denote the same concept. Although one can
envisage to manually build high-coverage lists of
synonyms, enumerating meaning equivalences at
the level of phrases is too daunting a task for hu-
mans. Because this type of knowledge can how-
ever greatly benefit many NLP applications, au-
tomatic acquisition of such paraphrases has at-
tracted a lot of attention (Androutsopoulos and
Malakasiotis, 2010; Madnani and Dorr, 2010),
and significant research efforts have been devoted
to this objective (Callison-Burch, 2007; Bhagat,
2009; Madnani, 2010).
Central to acquiring paraphrases is the need of
assessing the quality of the candidate paraphrases
produced by a given technique. Most works to
date have resorted to human evaluation of para-
phrases on the levels of grammaticality and mean-
ing equivalence. Human evaluation is however
often criticized as being both costly and non re-
producible, and the situation is even more compli-
cated by the inherent complexity of the task that
can produce low inter-judge agreement. Task-
based evaluation involving the use of paraphras-
ing into some application thus seem an acceptable
solution, provided the evaluation methodologies
for the given task are deemed acceptable. This,
in turn, puts the emphasis on observing the im-
pact of paraphrasing on the targeted application
and is rarely accompanied by a study of the intrin-
sic limitations of the paraphrase acquisition tech-
nique used.
The present work is concerned with the task of
sub-sentential paraphrase acquisition from pairs
of related sentences. A large variety of tech-
niques have been proposed that can be applied
to this task. They typically make use of differ-
ent kinds of automatically or manually acquired
knowledge. We make the hypothesis that their
performance can be increased if candidate para-
716
phrases can be validated using information that
characterize paraphrases in complement to the set
of techniques that proposed them. We propose to
implement this as a bi-class classification problem
(i.e. paraphrase vs. not paraphrase), allowing
any paraphrase acquisition technique to be easily
integrated into the combination system. In this
article, we report experiments on two languages,
English and French, with 5 individual techniques
based on a) statistical word alignment models,
b) translational equivalence, c) handcoded rules of
term variation, d) syntactic similarity, and e) edit
distance on word sequences. We used parallel
monolingual parallel corpora obtained via mul-
tiple translation from a single language as our
sources of related sentences, and a large set of
features including surface to contextual similarity
measures. Relative improvements in F-measure
close to 18% are obtained on both languages over
the best performing techniques.
The remainder of this article is organized as
follows. We first briefly review previous work
on sub-sentential paraphrase acquisition in sec-
tion 2. We then describe our experimental setting
in section 3 and the individual techniques that we
have studied in section 4. Section 5 is devoted to
our approach for validating paraphrases proposed
by individual techniques. Finally, section 6 con-
cludes the article and presents some of our future
work in the area of paraphrase acquisition.
2 Related work
The hypothesis that if two words or, by exten-
sion, two phrases, occur in similar contexts then
they may be interchangeable has been extensively
tested. The distributional hypothesis, attributed to
Zellig Harris, was for example applied to syntac-
tic dependency paths in the work of Lin and Pan-
tel (2001). Their results take the form of equiva-
lence patterns with two arguments such as {X asks
for Y, X requests Y, X?s request for Y, X wants Y,
Y is requested by X, . . .}.
Using comparable corpora, where the same in-
formation probably exists under various linguis-
tic forms, increases the likelihood of finding very
close contexts for sub-sentential units. Barzilay
and Lee (2003) proposed a multi-sequence align-
ment algorithm that takes structurally similar sen-
tences and builds a compact lattice representation
that encodes local variations. The work by Bhagat
and Ravichandran (2008) describes an application
of a similar technique on a very large scale.
The hypothesis that two words or phrases are
interchangeable if they share a common trans-
lation into one or more other languages has
also been extensively studied in works on sub-
sentential paraphrase acquisition. Bannard and
Callison-Burch (2005) described a pivoting ap-
proach that can exploit bilingual parallel corpora
in several languages. The same technique has
been applied to the acquisition of local paraphras-
ing patterns in Zhao et al(2008). The work of
Callison-Burch (2008) has shown how the mono-
lingual context of a sentence to paraphrase can be
used to improve the quality of the acquired para-
phrases.
Another approach consists in modelling local
paraphrasing identification rules. The work of
Jacquemin (1999) on the identification of term
variants, which exploits rewriting morphosyntac-
tic rules and descriptions of morphological and
semantic lexical families, can be extended to ex-
tract the various forms corresponding to input pat-
terns from large monolingual corpora.
When parallel monolingual corpora aligned at
the sentence level are available (e.g. multiple
translations into the same language), the task of
sub-sentential paraphrase acquisition can be cast
as one of word alignment between two aligned
sentences (Cohn et al 2008). Barzilay and
McKeown (2001) applied the distributionality hy-
pothesis on such parallel sentences, and Pang et
al. (2003) proposed an algorithm to align sen-
tences by recursive fusion of their common syn-
tactic constituants.
Finally, they has been a recent interest in auto-
matic evaluation of paraphrases (Callison-Burch
et al 2008; Liu et al 2010; Chen and Dolan,
2011; Metzler et al 2011).
3 Experimental setting
We used the main aspects of the methodology
described by Cohn et al(2008) for constructing
evaluation corpora and assessing the performance
of techniques on the task of sub-sentential para-
phrase acquisition. Pairs of related sentences are
hand-aligned to define a set of reference atomic
paraphrase pairs at the level of words or phrases,
denoted asRatom1.
1Note that in this study we do not distinguish between
?Sure? and ?Possible? alignments, and when reusing anno-
717
single language multiple language video descriptions multiply-translated news headlines
translation translation subtitles
# tokens 4,476 4,630 1,452 2,721 1,908
# unique tokens 656 795 357 830 716
% aligned tokens (excluding identities) 60.58 48.80 23.82 29.76 14.46
lexical overlap (tokens) 77.21 61.03 59.50 32.51 39.63
lexical overlap (lemmas content words) 83.77 71.04 64.83 39.54 45.31
translation edit rate (TER) 0.32 0.55 0.76 0.68 0.62
penalized n-gram prec. (BLEU) 0.33 0.15 0.13 0.14 0.39
Table 1: Various indicators of sentence pair comparability for different corpus types. Statistics are reported for
French on sets of 100 sentence pairs.
We conducted a small-scale study to assess dif-
ferent types of corpora of related sentences:
1. single language translation Corpora ob-
tained by several independent human trans-
lation of the same sentences (e.g. (Barzilay
and McKeown, 2001)).
2. multiple language translation Same as
above, but where a sentence is translated
from 4 different languages into the same lan-
guage (Bouamor et al 2010).
3. video descriptions Descriptions of short
YouTube videos obtained via Mechanical
Turk (Chen and Dolan, 2011).
4. multiply-translated subtitles Aligned mul-
tiple translations of contributed movie subti-
tles (Tiedemann, 2007).
5. comparable news headlines News head-
lines collected from Google News clusters
(e.g. (Dolan et al 2004)).
We collected 100 sentence pairs of each type
in French, for which various comparability mea-
sures are reported on Table 1. In particular, the
?% aligned tokens? row indicates the propor-
tion of tokens from the sentence pairs that could
be manually aligned by a native-speaker annota-
tor.2 Obviously, the more common tokens two
sentences from a pair contain, the fewer sub-
sentential paraphrases may be extracted from that
pair. However, high lexical overlap increases the
probability that two sentences be indeed para-
phrases, and in turn the probability that some of
their phrases be paraphrases. Furthermore, the
tated corpora using them we considered all alignments as be-
ing correct.
2The same annotator hand-aligned the 5*100=500 para-
phrase pairs using the YAWAT (Germann, 2008) manual
alignment tool.
presence of common token may serve as useful
clues to guide paraphrase extraction.
For our experiments, we chose to use parallel
monolingual corpora obtained by single language
translation, the most direct resource type for ac-
quiring sub-sentential paraphrase pairs. This al-
lows us to define acceptable references for the
task and resort to the most consensual evaluation
technique for paraphrase acquisition to date. Us-
ing such corpora, we expect to be able to extract
precise paraphrases (see Table 1), which will be
natural candidates for further validation, which
will be addressed in section 5.3.
Figure 1 illustrates a reference alignment ob-
tained on a pair of English sentential paraphrases
and the list of atomic paraphrase pairs that can be
extracted from it, against which acquisition tech-
niques will be evaluated. Note that we do not con-
sider pairs of identical units during evaluation, so
we filter them out from the list of reference para-
phrase pairs.
The example in Figure 1 shows different cases
that point to the inherent complexity of this task,
even for human annotators: it could be argued,
for instance, that a correct atomic paraphrase
pair should be reached ? amounted to rather
than reached ? amounted. Also, aligning in-
dependently 260 ? 0.26 and million ? billion
is assuredly an error, while the pair 260 mil-
lion? 0.26 billion would have been appropriate.
A case of alignment that seems non trivial can be
observed in the provided example (during the en-
tire year ? annual). The abovementioned rea-
sons will explain in part the difficulties in reach-
ing high performance values using such gold stan-
dards.
Reference composite paraphrase pairs (denoted
as R), obtained by joining adjacent atomic para-
phrase pairs from Ratom up to 6 tokens3, will
3We used standard biphrase extraction heuristics (Koehn
718
the
amount
of
foreign
capital
actually
utilized
during
the
entire
year
reached
260
million
us
dollars
.
th
e
an
nu
al
fo
re
ig
n
in
ve
stm
en
t
ac
tu
al
ly
us
ed
am
ou
nt
ed
to us
$
0.
26
bi
lli
on
capital ? investment
utilized ? used
during the entire year ? annual
reached ? amounted
260 ? 0.26
million ? billion
us dollars ? us$
Figure 1: Reference alignments for a pair of English
sentential paraphrases from the annotation corpus of
Cohn et al(2008) (note that possible and sure align-
ments are not distinguished here) and the list of atomic
paraphrase pairs extracted from these alignments.
also be considered when measuring performance.
Evaluated techniques have to output atomic can-
didate paraphrase pairs (denoted as Hatom) from
which composite paraphrase pairs (denoted as
H) are computed. The usual measures of pre-
cision (P ), recall (R) and F-measure (F1) can
then be defined in the following way (Cohn et al
2008):
P =
|Hatom ?R|
|Hatom|
R =
|H ? Ratom|
|Ratom|
F1 =
2pr
p+ r
We conducted experiments using two different
corpora in English and French. In each case,
a held-out development corpus of 150 sentential
paraphrase pairs was used for development and
tuning, and all techniques were evaluated on the
same test set consisting of 375 sentential para-
phrase pairs. For English, we used the MTC
et al 2007) : all words from a phrase must be aligned to at
least one word from the other and not to words outside, but
unaligned words at phrase boundaries are not used.
corpus described in (Cohn et al 2008), consist-
ing of multiply-translated Chinese sentences into
English, and used as our gold standard both the
alignments marked as ?Sure? and ?Possible?. For
French, we used the CESTA corpus of news ar-
ticles4 obtained by translating into French from
English.
We used the YAWAT (Germann, 2008) manual
alignment tool. Inter-annotator agreement val-
ues (averaging with each annotation set as the
gold standard) are 66.1 for English and 64.6 for
French, which we interpret as acceptable val-
ues. Manual inspection of the two corpora reveals
that the French corpus tends to contain more lit-
eral translations, possibly due to the original lan-
guages of the sentences, which are closer to the
target language than Chinese is to English.
4 Individual techniques for paraphrase
acquisition
As discussed in section 2, the acquisition of sub-
sentential paraphrases is a challenging task that
has previously attracted a lot of work. In this
work, we consider the scenario where sentential
paraphrases are available and words and phrases
from one sentence can be aligned to words and
phrases from the other sentence to form atomic
paraphrase pairs. We now describe several tech-
niques that perform the task of sub-sentential unit
alignment. We have selected and implemented
five techniques which we believe are representa-
tive of the type of knowledge that these techniques
use, and have reused existing tools, initially devel-
oped for other tasks, when possible.
4.1 Statistical learning of word alignments
(Giza)
The GIZA++ tool (Och and Ney, 2004) computes
statistical word alignment models of increasing
complexity from parallel corpora. While origi-
nally developed in the bilingual context of Statis-
tical Machine Translation, nothing prevents build-
ing such models on monolingual corpora. How-
ever, in order to build reliable models, it is nec-
essary to use enough training material includ-
ing minimal redundancy of words. To this end,
we provided GIZA++ with all possible sentence
pairs from our mutiply-translated corpus to im-
prove the quality of its word alignments (note that
4http://www.elda.org/article125.html
719
we used symmetrized alignments from the align-
ments in both directions). This constitutes a sig-
nificant advantage for this technique that tech-
niques working on each sentence pair indepen-
dently do not have.
4.2 Translational equivalence (Pivot)
Translational equivalence can be exploited to de-
termine that two phrases may be paraphrases.
Bannard and Callison-Burch (2005) defined a
paraphrasing probability between two phrases
based on their translation probability through all
possible pivot phrases as:
Ppara(p1, p2) =
?
piv
Pt(piv|p1)Pt(p2|piv)
where Pt denotes translation probabilies. We used
the Europarl corpus5 of parliamentary debates in
English and French, consisting of approximately
1.7 million parallel sentences : this allowed us
to use the same resource to build paraphrases for
English, using French as the pivot language, and
for French, using English as the pivot language.
The GIZA++ tool was used for word alignment
and the MOSES Statistical Machine Translation
toolkit (Koehn et al 2007) was used to com-
pute phrase translation probabilities from these
word alignments. For each sentential paraphrase
pair, we applied the following algorithm: for each
phrase, we build the entire set of paraphrases us-
ing the previous definition. We then extract its
best paraphrase as the one exactly appearing in the
other sentence with maximum paraphrase proba-
bility, using a minimal threshold value of 10?4.
4.3 Linguistic knowledge on term variation
(Fastr)
The FASTR tool (Jacquemin, 1999) was designed
to spot term/phrase variants in large corpora.
Variants are described through metarules express-
ing how the morphosyntactic structure of a term
variant can be derived from a given term by means
of regular expressions on word morphosyntactic
categories. Paradigmatic variation can also be ex-
pressed by expressing constraints between words,
imposing that they be of the same morphologi-
cal or semantic family. Both constraints rely on
preexisting repertoires available for English and
French. To compute candidate paraphrase pairs
using FASTR, we first consider all phrases from
5http://statmt.org/europarl
the first sentence and search for variants in the
other sentence, then do the reverse process and
finally take the intersection of the two sets.
4.4 Syntactic similarity (Synt)
The algorithm introduced by Pang et al(2003)
takes two sentences as input and merges them by
top-down syntactic fusion guided by compatible
syntactic substructure. A lexical blocking mecha-
nism prevents constituents from fusionning when
there is evidence of the presence of a word in an-
other constituent of one of the sentence. We use
the Berkeley Probabilistic parser (Klein and Man-
ning, 2003) to obtain syntactic trees for English
and its adapted version for French (Candito et al
2010). Because this process is highly sensitive to
syntactic parse errors, we use in our implemen-
tation k-best parses and retain the most compact
fusion from any pair of candidate parses.
4.5 Edit rate on word sequences (TERp)
TERp (Translation Edit Rate Plus) (Snover et al
2010) is a score designed for the evaluation of
Machine Translation output. Its typical use takes
a system hypothesis to compute an optimal set of
word edits that can transform it into some exist-
ing reference translation. Edit types include ex-
act word matching, word insertion and deletion,
block movement of contiguous words (computed
as an approximation), as well as optionally vari-
ants substitution through stemming, synonym or
paraphrase matching.6 Each edit type is parame-
terized by at least one weight which can be opti-
mized using e.g. hill climbing. TERp being a tun-
able metric, our experiments will include tuning
TERp systems towards either precision (? P ),
recall (? R), or F-measure (? F1).7
4.6 Evaluation of individual techniques
Results for the 5 individual techniques are given
on the left part of Table 2. It is first apparent
that all techniques but TERp fared better on the
French corpus than on the English corpus. This
can certainly be explained by the fact that the for-
mer results from more literal translations (from
6Note that for these experiments we did not use the stem-
ming module, the interface to WordNet for synonym match-
ing and the provided paraphrase table for English, due to the
fact that these resources were available for English only.
7Hill climbing was used for all tunings as done by Snover
et al(2010), and we used one iteration starting with uniform
weights and 100 random restarts.
720
Individual techniques Combinations
GIZA PIVOT FASTR SYNT TERp union validation? P ? R ? F1
English
P 31.01 31.78 37.38 52.17 50.00 29.15 33.37 21.44 50.51
R 38.30 18.50 6.71 2.53 5.83 45.19 45.37 60.87 41.19
F1 34.27 23.39 11.38 4.83 10.44 35.44 38.46 31.71 45.37
French
P 28.99 29.53 52.48 62.50 31.35 30.26 31.43 17.58 40.77
R 45.98 26.66 8.59 8.65 44.22 44.60 44.10 63.36 45.85
F1 35.56 28.02 14.77 15.20 36.69 36.05 36.70 27.53 43.16
Table 2: Results on the test set on English and French for the 5 individual paraphrase acquisition techniques (left
part) and for the 2 combination techniques (right part).
English to French, compared with from Chinese
to English), which should be consequently eas-
ier to word-align. This is for example clearly
shown by the results of the statistical aligner
GIZA, which obtains a 7.68 advantage on recall
for French over English.
The two linguistically-aware techniques,
FASTR and SYNT, have a very strong precision
on the more parallel French corpus, but fail to
achieve an acceptable recall on their own. This
is not surprising : FASTR metarules are focussed
on term variant extraction, and SYNT requires
two syntactic trees to be highly comparable
to extract sub-sentential paraphrases. When
these constrained conditions are met, these two
techniques appear to perform quite well in terms
of precision.
GIZA and TERp perform roughly in the same
range on French, with acceptable precision and
recall, TERp performing overall better, with e.g.
a 1.14 advantage on F-measure on French and
4.19 on English. The fact that TERp performs
comparatively better on English than on French8,
with a 1.76 advantage on F-measure, is not con-
tradictory: the implemented edit distance makes
it possible to align reasonably distant words and
phrases independently from syntax, and to find
alignments for close remaining words, so the dif-
ferences of performance between the two lan-
guages are not necessarily expected to be com-
parable with the results of a statistical alignment
technique. English being a poorly-inflected lan-
guage, alignment clues between two sentential
paraphrases are expected to be more numerous
8Recall that all specific linguistic modules for English
only from TERp had been disabled, so the better perfor-
mance on English cannot be explained by a difference in
terms of resources used.
than for highly-inflected French.
PIVOT is on par with GIZA as regards preci-
sion, but obtains a comparatively much lower re-
call (differences of 19.32 and 19.80 on recall on
French and English respectively). This may first
be due in part to the paraphrasing score threshold
used for PIVOT, but most certainly to the use of
a bilingual corpus from the domain of parliamen-
tary debates to extract paraphrases when our test
sets are from the news domain: we may be ob-
serving differences inherent to the domain, and
possibly facing the issue of numerous ?out-of-
vocabulary? phrases, in particular for named en-
tities which frequently occur in the news domain.
Importantly, we can note that we obtain at best
a recall of 45.98 on French (GIZA) and of 45.37
on English (TERp). This may come as a disap-
pointment but, given the broad set of techniques
evaluated, this should rather underline the inher-
ent complexity of the task. Also, recall that the
metrics used do not consider identity paraphrases
(e.g. at the same time ? at the same time), as
well as the fact that gold standard alignment is
a very difficult process as shown by interjudge
agreement values and our example from section 3.
This, again, confirms that the task that is ad-
dressed is indeed a difficult one, and provides fur-
ther justification for initially focussing on parallel
monolingual corpora, albeit scarce, for conduct-
ing fine-grained studies on sub-sentential para-
phrasing.
Lastly, we can also note that precision is not
very high, with (at best, using TERp?P ) average
values for all techniques of 40.97 and 40.46 on
French and English, respectively. Several facts
may provide explanations for this observation.
First, it should be noted that none of those tech-
niques, except SYNT, was originally developed
721
for the task of sub-sentential paraphrase acqui-
sition from monolingual parallel corpora. This
results in definitions that are at best closely re-
lated to this task.9 Designing new techniques
was not one of the objectives of our study, so we
have reused existing techniques, originally devel-
oped with different aims (bilingual parallel cor-
pora word alignment (GIZA), term variant recog-
nition (FASTR), Machine Translation evaluation
(TERp)). Also, techniques such as GIZA and
TERp attempt to align as many words as possi-
ble in a sentence pair, when gold standard align-
ments sometimes contain gaps.10 Finally, the met-
rics used will count as false small variations of
gold standard paraphrases (e.g. missing function
word): the acceptability or not of such candi-
dates could be either evaluated in a scenario where
such ?acceptable? variants would be taken into
account, and could be considered in the context
of some actual use of the acquired paraphrases
in some application. Nonetheless, on average the
techniques in our study produce more candidates
that are not in the gold standard: this will be an
important fact to keep in mind when tackling the
task of combining their outputs. In particular, we
will investigate the use of features indicating the
combination of techniques that predicted a given
paraphrase pair, aiming to capture consensus in-
formation.
5 Paraphrase validation
5.1 Technique complementarity
Before considering combining and validating the
outputs of individual techniques, it is informative
to look at some notion of ?complementarity? be-
tween techniques, in terms of how many correct
paraphrases a technique would add to a combined
set. The following formula was used to account
for the complementarity between the set of can-
didates from some technique i, ti, and the set for
some technique j, tj :
C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj))
9Recall, however, that our best performing technique on
F-measure, TERp, was optimized to our task using a held
out development set.
10It is arguable whether such cases should happen in sen-
tence pairs obtained by translating the same original sentence
into the same language, but this clearly depends on the inter-
pretation of the expected level of annotation by the annota-
tors.
Results on the test set for the two languages
are given in Table 3. A number of pairs of tech-
niques have strong complementarity values, the
strongest one being for GIZA and TERp for both
languages. According to these figures, PIVOT
identify paraphrases which are slightly more sim-
ilar to those of TERp than those of GIZA. Inter-
estingly, FASTR and SYNT exhibit a strong com-
plementarity, where in French, for instance, they
only have a very small proportion of paraphrases
in common. Considering the set of all other tech-
niques, GIZA provides the more new paraphrases
on French and TERp on English.
GIZA PIVOT FASTR SYNT TERp?R all others
English
GIZA - 4.65 2.83 0.59 10.31 8.31
PIVOT 4.65 - 2.30 1.88 3.12 3.72
FASTR 2.83 2.30 - 2.42 1.71 0.53
SYNT 0.59 1.88 2.42 - 0.59 0.00
TERp?R 10.31 3.12 1.71 0.59 - 12.20
French
GIZA - 9.79 3.64 2.20 10.73 8.91
PIVOT 9.79 - 2.26 5.22 7.84 3.39
FASTR 3.64 2.26 - 7.28 3.01 0.19
SYNT 2.20 5.22 7.28 - 1.76 0.44
TERp?R 10.73 7.84 3.01 1.76 - 5.65
Table 3: Values of complementarity on the test set for
both languages, where the following formula was used
for the set of technique outputs T = {t1, t2, ..., tn} :
C(ti, tj) = recall(ti?tj)?max(recall(ti), recall(tj)).
Complementarity values are computed between all
pairs of individual techniques, and each individual
technique and the set of all other techniques. Values in
bold indicate highest values for the technique of each
row.
5.2 Naive combination by union
We first implemented a naive combination ob-
tained by taking the union of all techniques. Re-
sults are given in the first column of the right part
of Table 2. The first result is quite encouraging:
in both languages, more than 6 paraphrases from
the gold standard out of 10 are found by at least
one of the techniques, which, given our previous
discussion, constitutes a good result and provide
a clear justification for combining different tech-
niques for improving performance on this task.
Precision is mechanically lowered to account for
roughly 1 correct paraphrase over 5 candidates
for both languages. F-measure values are much
lower than those of TERp and GIZA, showing
that the union of all techniques is only interest-
ing for recall-oriented paraphrase acquisition. In
722
the next section, we will show how the results of
the union can be validated using machine learning
to improve these figures.
5.3 Paraphrase validation via automatic
classification
A natural improvement to the naive combination
of paraphrase candidates from all techniques can
consist in validating candidate paraphrases by us-
ing several models that may be good indicators of
their paraphrasing status. We can therefore cast
our problem as one of biclass classification (i.e.
?paraphrase? vs. ?not paraphrase?).
We have used a maximum entropy classifier11
with the following features, aiming at capturing
information on the paraphrase status of a candi-
date pair:
Morphosyntactic equivalence (POS) It may
be the case that some sequences of part-of-speech
can be rewritten as different sequences, e.g. as
a result of verb nominalization. We therefore
use features to indicate the sequences of part-of-
speech for a pair of candidate paraphrases. We
used the preterminal symbols of the syntactic
trees of the parser used for SYNT.
Character-based distance (CAR) Morpholog-
ical variants often have close word forms, and
more generally close word forms in sentential
paraphase pairs may indicate related words. We
used features for discretized values of the edit
distance between the two phrases of a candidate
paraphrase pair as measured by the Levenshtein
distance.
Stem similarity (STEM) Inflectional morphol-
ogy, which is quite productive in languages such
as French, can increase vocabulary size signifi-
cantly, while in sentential paraphrases common
stems may indicate related words. We used a
binary feature indicating whether the stemmed
phrases of a candidate paraphrase pair match.12
Token set identity (BOW) Syntactic rearrange-
ments may involve the same sets of words in var-
ious orders. We used discretized features indicat-
ing the proportion of common tokens in the set
11We used the implementation available at:
http://homepages.inf.ed.ac.uk/lzhang10/
maxent_toolkit.html
12We use the implementations of the Snowball stem-
mer from English and French available from: http://
snowball.tartarus.org
of tokens for the two phrases of a candidate para-
phrase pair.
Context similarity (CTXT) It can be derived
from the distributionality hypothesis that the more
two phrases will be seen in similar contexts, the
more they are likely to be paraphrases. We used
discretized features indicating how similar the
contexts of occurrences of two paraphrases are.
For this, we used the full set of bilingual English-
French data available for the translation task of
the Workshop on Statistical Machine Transla-
tion13, totalling roughly 30 million parallel sen-
tences: this again ensures that the same resources
are used for experiments in the two languages. We
collect all occurrences for the phrases in a pair,
and build a vector of content words cooccurring
within a distance of 10 words from each phrase.
We finally compute the cosine between the vec-
tors of the two phrases of a candidate paraphrase
pair.
Relative position in a sentence (REL) De-
pending on the language in which parallel sen-
tences are analyzed, it may be the case that sub-
sentential paraphrases occur at close locations in
their respective sentence. We used a discretized
feature indicating the relative position of the two
phrases in their original sentence.
Identity check (COOC) We used a binary fea-
ture indicating whether one of the two phrases
from a candidate pair, or the two, occurred at
some other location in the other sentence.
Phrase length ratio (LEN) We used a dis-
cretized feature indicating phrase length ratio.
Source techniques (SRC) Finally, as our set-
ting validates paraphrase candidates produced by
a set of techniques, we used features indicat-
ing which combination of techniques predicted a
paraphrase candidate. This can allow learning that
paraphrases in the intersection of the predicted
sets for some techniques may produce good re-
sults.
We used a held out training set consisting of
150 sentential paraphrase pairs from the same cor-
pora as our previous developement and test sets
for both languages. Positive examples were taken
from the candidate paraphrase pairs from any of
13http://www.statmt.org/wmt11/
translation-task.html
723
the 5 techniques in our study which belong to
the gold standard, and we used a corresponding
number of negative examples (randomly selected)
from candidate pairs not in the gold standard. The
right part of Table 2 provides the results for our
validation experiments of the union set for all pre-
vious techniques.
We obtain our best results for this study using
the output of our validation classifier over the set
of all candidate paraphrase pairs. On French, it
yields an improvement in F-measure (43.16) of
+6.46 over the best individual technique (TERp)
and of +15.63 over the naive union from all indi-
vidual techniques. On English, the improvement
in F-measure (45.37) is for the same conditions of
respectively +6.91 (over TERp) and +13.66. We
unfortunately observe an important decrease in re-
call over the naive union, of respectively -17.54
and -19.68 for French and English. Increasing our
amount of training data to better represent the full
range of paraphrase types may certainly overcome
this in part. This would indeed be sensible, as bet-
ter covering the variety of paraphrase types as a
one-time effort would help all subsequent valida-
tions. Figure 2 shows how performance varies on
French with number of training examples for var-
ious feature configurations. However, some para-
phrase types will require integration of more com-
plex knowledge, as is the case, for instance, for
paraphrase pairs involving some anaphora and its
antecedent (e.g. China? it).
While these results, which are very comparable
for the two languages studied, are already satisfy-
ing given the complexity of our task, further in-
spection of false positives and negatives may help
us to develop additional models that will help us
obtain a better classification performance.
6 Conclusions and future work
In this article, we have addressed the task of com-
bining the results of sub-sentential paraphrase ac-
quition from parallel monolingual corpora using a
large variety of techniques. We have provided jus-
tifications for using highly parallel corpora con-
sisting of multiply translated sentences from a
single language. All our experiments were con-
ducted on both English and French using com-
parable resources, so although the results cannot
be directly compared they give some acceptable
comparison points. The best recall of any indi-
vidual technique is around 45 for both language,
10 20 30 40 50 60 70 80 90 10031
33
35
37
39
41
43
All\POS\SRC\CTXT\STEM\LEN\COOC
F-mea
sure
% of examples from training corpus
Figure 2: Learning curves obtained on French by re-
moving features individually.
and F-measure in the range 36-38, indicating that
the task under study is a very challenging one.
Our validation strategy based on bi-class classi-
fication using a broad set of features applicable to
all candidate paraphrase pairs allowed us to obtain
a 18% relative improvement in F-measure over
the best individual technique for both languages.
Our future work include performing a deeper
error analysis of our current results, to better com-
prehend what characteristics of paraphrase still
defy current validation. Also, we want to inves-
tigate adding new individual techniques to pro-
vide so far unseen candidates. Another possible
approach would be to submit all pairs of sub-
sentential paraphrase pairs from a sentence pair
to our validation process, which would obviously
require some optimization and devising sensible
heuristics to limit time complexity. We also in-
tend to collect larger corpora for all other corpus
types appearing in Table 1 and conducting anew
our acquisition and validation tasks.
Acknowledgements
The authors would like to thank the reviewers for
their comments and suggestions, as well as Guil-
laume Wisniewski for helpful discussions. This
work was partly funded by ANR project Edylex
(ANR-09-CORD-008).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
724
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, Ann Arbor, USA.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of NAACL-HLT, Edmonton, Canada.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL, Toulouse, France.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Rahul Bhagat. 2009. Learning Paraphrases from Text.
Ph.D. thesis, University of Southern California.
Houda Bouamor, Aure?lien Max, and Anne Vilnat.
2010. Comparison of Paraphrase Acquisition Tech-
niques on Sentential Paraphrases. In Proceedings of
IceTAL, Rejkavik, Iceland.
Chris Callison-Burch, Trevor Cohn, and Mirella La-
pata. 2008. Parametric: An automatic evaluation
metric for paraphrasing. In Proceedings of COL-
ING, Manchester, UK.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP, Hawai, USA.
Marie Candito, Beno??t Crabbe?, and Pascal Denis.
2010. Statistical French dependency parsing: tree-
bank conversion and first results. In Proceedings of
LREC, Valletta, Malta.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of ACL, Portland, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4).
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of COLING, Geneva, Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-HLT,
demo session, Columbus, USA.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, College Park, USA.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
Sapporo, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Proceedings of ACL, demo session,
Prague, Czech Republic.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. PEM: A paraphrase evaluation metric ex-
ploiting parallel texts. In Proceedings of EMNLP,
Cambridge, USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generat-
ing Phrasal and Sentential Paraphrases: A Survey
of Data-Driven Methods . Computational Linguis-
tics, 36(3).
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, University of Maryland College Park.
Donald Metzler, Eduard Hovy, and Chunliang Zhang.
2011. An empirical evaluation of data-driven para-
phrase generation techniques. In Proceedings of
ACL-HLT, Portland, USA.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of NAACL-HLT, Edmonton,
Canada.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase,
semantic, and alignment enhancements to Transla-
tion Edit Rate. Machine Translation, 23(2-3).
Jo?rg Tiedemann. 2007. Building a Multilingual Paral-
lel Subtitle Corpus. In Proceedings of the Confer-
ence on Computational Linguistics in the Nether-
lands, Leuven, Belgium.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings
of ACL-HLT, Columbus, USA.
725
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 395?400,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Monolingual Alignment by Edit Rate Computation
on Sentential Paraphrase Pairs
Houda Bouamor Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
Orsay, France
{firstname.lastname}@limsi.fr
Anne Vilnat
Abstract
In this paper, we present a novel way of tack-
ling the monolingual alignment problem on
pairs of sentential paraphrases by means of
edit rate computation. In order to inform the
edit rate, information in the form of subsenten-
tial paraphrases is provided by a range of tech-
niques built for different purposes. We show
that the tunable TER-PLUS metric from Ma-
chine Translation evaluation can achieve good
performance on this task and that it can effec-
tively exploit information coming from com-
plementary sources.
1 Introduction
The acquisition of subsentential paraphrases has at-
tracted a lot of attention recently (Madnani and Dorr,
2010). Techniques are usually developed for extract-
ing paraphrase candidates from specific types of cor-
pora, including monolingual parallel corpora (Barzi-
lay and McKeown, 2001), monolingual comparable
corpora (Dele?ger and Zweigenbaum, 2009), bilin-
gual parallel corpora (Bannard and Callison-Burch,
2005), and edit histories of multi-authored text (Max
and Wisniewski, 2010). These approaches face two
main issues, which correspond to the typical mea-
sures of precision, or how appropriate the extracted
paraphrases are, and of recall, or how many of the
paraphrases present in a given corpus can be found
effectively. To start with, both measures are often
hard to compute in practice, as 1) the definition of
what makes an acceptable paraphrase pair is still
a research question, and 2) it is often impractical
to extract a complete set of acceptable paraphrases
from most resources. Second, as regards the pre-
cision of paraphrase acquisition techniques in par-
ticular, it is notable that most works on paraphrase
acquisition are not based on direct observation of
larger paraphrase pairs. Even monolingual corpora
obtained by pairing very closely related texts such as
news headlines on the same topic and from the same
time frame (Dolan et al, 2004) often contain unre-
lated segments that should not be aligned to form a
subsentential paraphrase pair. Using bilingual cor-
pora to acquire paraphrases indirectly by pivoting
through other languages is faced, in particular, with
the issue of phrase polysemy, both in the source and
in the pivot languages.
It has previously been noted that highly parallel
monolingual corpora, typically obtained via mul-
tiple translation into the same language, consti-
tute the most appropriate type of corpus for ex-
tracting high quality paraphrases, in spite of their
rareness (Barzilay and McKeown, 2001; Cohn et
al., 2008; Bouamor et al, 2010). We build on this
claim here to propose an original approach for the
task of subsentential alignment based on the compu-
tation of a minimum edit rate between two sentential
paraphrases. More precisely, we concentrate on the
alignment of atomic paraphrase pairs (Cohn et al,
2008), where the words from both paraphrases are
aligned as a whole to the words of the other para-
phrase, as opposed to composite paraphrase pairs
obtained by joining together adjacent paraphrase
pairs or possibly adding unaligned words. Figure 1
provides examples of atomic paraphrase pairs de-
rived from a word alignment between two English
sentential paraphrases.
395
China
will
continue continue?carry on
implementing
the
financial financial opening
up?open financialopening
up
policy
Ch
ina
wi
ll
ca
rry
on op
en
fin
an
cia
l
po
lic
y
Figure 1: Reference alignments for a pair of English
sentential paraphrases and their associated list of atomic
paraphrase pairs extracted from them. Note that identity
pairs (e.g. China ? China) will never be considered in
this work and will not be taken into account for evalua-
tion.
The remainder of this paper is organized as fol-
lows. We first briefly describe in section 2 how we
apply edit rate computation to the task of atomic
paraphrase alignment, and we explain in section 3
how we can inform such a technique with paraphrase
candidates extracted by additional techniques. We
present our experiments and discuss their results in
section 4 and conclude in section 5.
2 Edit rate for paraphrase alignment
TER-PLUS (Translation Edit Rate Plus) (Snover et
al., 2010) is a score designed for evaluation of Ma-
chine Translation (MT) output. Its typical use takes
a system hypothesis to compute an optimal set of
word edits that can transform it into some existing
reference translation. Edit types include exact word
matching, word insertion and deletion, block move-
ment of contiguous words (computed as an approx-
imation), as well as variants substitution through
stemming, synonym or paraphrase matching. Each
edit type is parameterized by at least one weight
which can be optimized using e.g. hill climbing.
TER-PLUS is therefore a tunable metric. We will
henceforth design as TERMT the TER metric (basi-
cally, without variants matching) optimized for cor-
relation with human judgment of accuracy in MT
evaluation, which is to date one of the most used
metrics for this task.
While this metric was not designed explicitely for
the acquisition of word alignments, it produces as a
by-product of its approximate search a list of align-
ments involving either individual words or phrases,
potentially fitting with the previous definition of
atomic paraphrase pairs. When applying it on a
MT system hypothesis and a reference translation,
it computes how much effort would be needed to
obtain the reference from the hypothesis, possibly
independently of the appropriateness of the align-
ments produced. However, if we consider instead
a pair of sentential paraphrases, it can be used to
reveal what subsentential units can be aligned. Of
course, this relies on information that will often go
beyond simple exact word matching. This is where
the capability of exploiting paraphrase matching can
come into play: TER-PLUS can exploit a table of
paraphrase pairs, and defines the cost of a phrase
substitution as ?a function of the probability of the
paraphrase and the number of edits needed to align
the two phrases without the use of phrase substitu-
tions?. Intuitively, the more parallel two sentential
paraphrases are, the more atomic paraphrase pairs
will be reliably found, and the easier it will be for
TER-PLUS to correctly identify the remaining pairs.
But in the general case, and considering less appar-
ently parallel sentence pairs, its work can be facil-
itated by the incorporation of candidate paraphrase
pairs in its paraphrase table. We consider this possi-
ble type of hybridation in the next section.
3 Informing edit rate computation with
other techniques
In this article, we use three baseline techniques
for paraphrase pair acquisition, which we will only
briefly introduce (see (Bouamor et al, 2010) for
more details). As explained previously, we want to
evaluate whether and how their candidate paraphrase
pairs can be used to improve paraphrase acquisition
on sentential paraphrases using TER-PLUS. We se-
lected these three techniques for the complementar-
ity of types of information that they use: statistical
word alignment without a priori linguistic knowl-
edge, symbolic expression of linguistic variation ex-
ploiting a priori linguistic knowledge, and syntactic
similarity.
396
Statistical Word Alignment The GIZA++
tool (Och and Ney, 2004) computes statistical word
alignment models of increasing complexity from
parallel corpora. While originally developped in the
bilingual context of Machine Translation, nothing
prevents building such models on monolingual
corpora. However, in order to build reliable models
it is necessary to use enough training material
including minimal redundancy of words. To this
end, we will be using monolingual corpora made
up of multiply-translated sentences, allowing us to
provide GIZA++ with all possible sentence pairs
to improve the quality of its word alignments (note
that following common practice we used symetrized
alignments from the alignments in both directions).
This constitutes an advantage for this technique that
the following techniques working on each sentence
pair independently do not have.
Symbolic expression of linguistic variation The
FASTR tool (Jacquemin, 1999) was designed to spot
term variants in large corpora. Variants are de-
scribed through metarules expressing how the mor-
phosyntactic structure of a term variant can be de-
rived from a given term by means of regular ex-
pressions on word categories. Paradigmatic varia-
tion can also be expressed by defining constraints
between words to force them to belong to the same
morphological or semantic family, both constraints
relying on preexisting repertoires available for En-
glish and French. To compute candidate paraphrase
pairs using FASTR, we first consider all the phrases
from the first sentence and search for variants in the
other sentence, do the reverse process and take the
intersection of the two sets.
Syntactic similarity The algorithm introduced
by Pang et al (2003) takes two sentences as in-
put and merges them by top-down syntactic fusion
guided by compatible syntactic substructure. A
lexical blocking mechanism prevents sentence con-
stituents from fusionning when there is evidence of
the presence of a word in another constituent of one
of the sentence. We use the Berkeley Probabilistic
parser (Petrov and Klein, 2007) to obtain syntac-
tic trees for English and its Bonsai adaptation for
French (Candito et al, 2010). Because this process
is highly sensitive to syntactic parse errors, we use
k-best parses (with k = 3 in our experiments) and
retain the most compact fusion from any pair of can-
didate parses.
4 Experiments and discussion
We used the methodology described by Cohn et al
(2008) for constructing evaluation corpora and as-
sessing the performance of various techniques on the
task of paraphrase acquisition. In a nutshell, pairs of
sentential paraphrases are hand-aligned and define a
set of reference atomic paraphrase pairs at the level
of words or blocks or words, denoted as Ratom, and
also a set of reference composite paraphrase pairs
obtained by joining adjacent atomic paraphrase pairs
(up to a given length), denoted as R. Techniques
output word alignments from which atomic candi-
date paraphrase pairs, denoted as Hatom, as well as
composite paraphrase pairs, denoted as H, can be
extracted. The usual measures of precision, recall
and f-measure can then be defined in the following
way:
p =
|Hatom ?R|
|Hatom|
r =
|H ? Ratom|
|Ratom|
f1 =
2pr
p + r
To evaluate our individual techniques and their
use by the tunable TER-PLUS technique (hence-
forth TERP), we measured results on two different
corpora in French and English. In each case, a held-
out development corpus of 150 paraphrase pairs was
used for tuning the TERP hybrid systems towards
precision (? p), recall (? r), or F-measure (?
f1).1 All techniques were evaluated on the same test
set consisting of 375 paraphrase pairs. For English,
we used the MTC corpus described in (Cohn et al,
2008), which consists of multiply-translated Chi-
nese sentences into English, with an average lexical
overlap2 of 65.91% (all tokens) and 63.95% (content
words only). We used as our reference set both the
alignments marked as ?Sure? and ?Possible?. For
French, we used the CESTA corpus of news articles3
obtained by translating into French from various lan-
guages with an average lexical overlap of 79.63%
(all tokens) and 78.19% (content words only). These
1Hill climbing was used for tuning as in (Snover et al,
2010), with uniform weights and 100 random restarts.
2We compute the percentage of lexical overlap be-
tween the vocabularies of two sentences S1 and S2 as :
|S1 ? S2|/min(|S1|, |S2|)
3http://www.elda.org/article125.html
397
Individual techniques Hybrid systems (TERPpara+X)
Giza++ Fastr Pang TMT TERPpara +G +F +P +G + F + P
G F P ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1 ? p ? r ? f1
French French
p 28.99 52.48 62.50 25.66 31.35 30.26 31.43 41.99 30.55 41.14 36.74 29.65 34.84 54.49 20.94 33.89 42.27 27.06 42.80
r 45.98 8.59 8.65 41.15 44.22 44.60 44.10 35.88 45.67 35.25 40.96 43.85 44.41 13.61 40.40 40.46 31.36 44.10 31.61
f1 35.56 14.77 15.20 25.66 36.69 36.05 36.70 38.70 36.61 37.97 38.74 35.38 39.05 21.78 27.58 36.88 36.01 33.54 36.37
English English
p 18.28 33.02 36.66 20.41 31.19 19.14 19.35 26.89 19.85 21.25 41.57 20.81 22.51 31.32 18.02 18.92 29.45 16.81 29.42
r 14.63 5.41 2.23 17.37 2.31 19.38 19.69 11.92 18.47 17.10 6.94 21.02 20.28 3.41 18.94 16.44 13.57 19.30 16.35
f1 16.25 9.30 4.21 18.77 4.31 19.26 19.52 16.52 19.14 18.95 11.91 20.92 21.33 6.15 18.47 17.59 18.58 17.96 21.02
Figure 2: Results on the test set on French and English for the individual techniques and TERP hybrid systems.
Column headers of the form ?? c? indicate that TERP was tuned on criterion c.
figures reveal that the French corpus tends to contain
more literal translations, possibly due to the original
languages of the sentences, which are closer to the
target language than Chinese is to English. We used
the YAWAT (Germann, 2008) interactive alignment
tool and measure inter-annotator agreement over a
subset and found it to be similar to the value reported
by Cohn et al (2008) for English.
Results for all individual techniques in the two
languages are given on Figure 2. We first note that
all techniques fared better on the French corpus than
on the English corpus. This can certainly be ex-
plained by the fact that the former results from more
literal translations, which are consequently easier to
word-align.
TERMT (i.e. TER tuned for Machine Transla-
tion evaluation) performs significantly worse on all
metrics for both languages than our tuned TERP ex-
periments, revealing that the two tasks have differ-
ent objectives. The two linguistically-aware tech-
niques, FASTR and PANG, have a very strong pre-
cision on the more parallel French corpus, and also
on the English corpus to a lesser extent, but fail to
achieve a high recall (note, in particular, that they
do not attempt to report preferentially atomic para-
phrase pairs). GIZA++ and TERPpara perform in
the same range, with acceptable precision and re-
call, TERPpara performing overall better, with e.g. a
1.14 advantage on f-measure on French and 3.27 on
English. Recall that TERP works independently on
each paraphrase pair, while GIZA++ makes use of
artificial repetitions of paraphrases of the same sen-
tence.
Figure 3 gives an indication of how well each
technique performs depending on the difficulty of
the task, which we estimate here as the value
(1? TER(para1, para2)), whose low values cor-
respond to sentences which are costly to trans-
form into the other using TER. Not surprisingly,
TERPpara and GIZA++, and PANG to a lesser ex-
tent, perform better on ?more parallel? sentential
paraphrase pairs. Conversely, FASTR is not affected
by the degree of parallelism between sentences, and
manages to extract synonyms and more generally
term variants, at any level of difficulty.
We have further tested 4 hybrid configurations
by providing TERPpara with the output of the other
individual techniques and of their union, the latter
simply obtained by taking paraphrase pairs output
by at least one of these techniques. On French,
where individual techniques achieve good perfor-
mance, any hybridation improves the F-measure
over both TERPpara and the technique used, the best
performance, using FASTR, corresponding to an im-
provement of respectively +2.35 and +24.28 over
TERPpara and FASTR. Taking the union of all tech-
niques does not yield additional gains: this might
be explained by the fact that incorrect predictions
are proportionnally more present and consequently
have a greater impact when combining techniques
without weighting them, possibly at the level of each
398
  <0.1 <0.2 <0.3 <0.4 <0.5 <0.6 <0.7 <0.8 <0.90
1020
3040
5060
7080
90100 TERpParaF1Giza++FastrPang
Difficulty (1-TER)
F-measu
re
  <0.1 <0.2 <0.3 <0.4 <0.5 <0.6 <0.7 <0.8 <0.90
1020
3040
5060
7080
90100 TERpParaF1Giza++FastrPang
Difficulty (1-TER)
F-measu
re
(a) French (b) English
Figure 3: F-measure values for our 4 individual techniques on French and English depending on the complexity of
paraphrase pairs measured with the (1-TER) formula. Note that each value corresponds to the average of F-measure
values for test examples falling in a given difficulty range, and that all ranges do not necessarily contain the same
number of examples.
prediction.4 Successful hybridation on English seem
harder to obtain, which may be partly attributed to
the poor quality of the individual techniques relative
to TERPpara. We however note anew an improve-
ment over TERPpara of +1.81 when using FASTR.
This confirms that some types of linguistic equiva-
lences cannot be captured using edit rate computa-
tion alone, even on this type of corpus.
5 Conclusion and future work
In this article, we have described the use of edit rate
computation for paraphrase alignment at the sub-
sentential level from sentential paraphrases and the
possibility of informing this search with paraphrase
candidates coming from other techniques. Our ex-
periments have shown that in some circumstances
some techniques have a good complementarity and
manage to improve results significantly. We are
currently studying hard-to-align subsentential para-
phrases from the type of corpora we used in order to
get a better understanding of the types of knowledge
required to improve automatic acquisition of these
units.
4Indeed, measuring the precision on the union yields a poor
performance of 23.96, but with the highest achievable value of
50.56 for recall. Similarly, the maximum value for precision
with a good recall can be obtained by taking the intersection of
the results of TERPpara and GIZA++, which yields a value of
60.39.
Our future work also includes the acquisition of
paraphrase patterns (e.g. (Zhao et al, 2008)) to gen-
eralize the acquired equivalence units to more con-
texts, which could be both used in applications and
to attempt improving further paraphrase acquisition
techniques. Integrating the use of patterns within an
edit rate computation technique will however raise
new difficulties.
We are finally also in the process of conducting
a careful study of the characteristics of the para-
phrase pairs that each technique can extract with
high confidence, so that we can improve our hybri-
dation experiments by considering confidence val-
ues at the paraphrase level using Machine Learning.
This way, we may be able to use an edit rate com-
putation algorithm such as TER-PLUS as a more
efficient system combiner for paraphrase extraction
methods than what was proposed here. A poten-
tial application of this would be an alternative pro-
posal to the paraphrase evaluation metric PARAMET-
RIC (Callison-Burch et al, 2008), where individual
techniques, outputing word alignments or not, could
be evaluated from the ability of the informated edit
rate technique to use correct equivalence units.
Acknowledgments
This work was partly funded by a grant from LIMSI.
The authors wish to thank the anonymous reviewers
for their useful comments and suggestions.
399
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL, Toulouse, France.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2010.
Comparison of Paraphrase Acquisition Techniques on
Sentential Paraphrases. In Proceedings of IceTAL, Re-
jkavik, Iceland.
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
2008. Parametric: An automatic evaluation metric for
paraphrasing. In Proceedings of COLING, Manch-
ester, UK.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical French dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4).
Louise Dele?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of Coling 2004, pages 350?356, Geneva,
Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-08: HLT
Demo Session, Columbus, USA.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, pages 341?348, College Park, USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
Driven Methods . Computational Linguistics, 36(3).
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia?s Revision History. In Proceedings of
LREC, Valletta, Malta.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase Pat-
terns from Bilingual Corpora. In Proceedings of ACL-
HLT, Columbus, USA.
400
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 77?85
Manchester, August 2008
Looking up phrase rephrasings via a pivot language
Aure?lien Max
LIMSI-CNRS & Universite? Paris-Sud 11
Orsay, France
aurelien.max@limsi.fr
Michael Zock
LIF-CNRS
Marseilles, France
michael.zock@lif.univ-mrs.fr
Abstract
Rephrasing text spans is a common task
when revising a text. However, traditional
dictionaries often cannot provide direct as-
sistance to writers in performing this task.
In this article, we describe an approach
to obtain a monolingual phrase lexicon
using techniques used in Statistical Ma-
chine Translation. A part to be rephrased
is first translated into a pivot language,
and then translated back into the origi-
nal language. Models for assessing flu-
ency, meaning preservation and lexical di-
vergence are used to rank possible rephras-
ings, and their relative weight can be tuned
by the user so as to better address her
needs. An evaluation shows that these
models can be used successfully to select
rephrasings that are likely to be useful to a
writer.
1 Introduction
Once an initial draft of a text is ready, writers face
the difficult phase of text revision. Changes may
be made for various reasons: correcting spelling or
grammatical errors, making the text locally more
fluent (for example, in case it contains wordings
that are literal translations from another language),
avoiding close repetitions or enforcing terminolog-
ical consistency, or better conveying the writer?s
ideas. All these changes can affect text spans of
various sizes, and can globally be seen as cases
of rephrasing. Paraphrasing involves rephrasings
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
that are semantically equivalent, but targets termi-
nology and style that are more suited to the con-
text of use of a text. In a broad sense, rephrasing
may involve wordings that convey different mean-
ings in an attempt to correct or make the writer?s
thoughts more precise. Research concerned with
the study of changes between writers? drafts (tex-
tual genetic criticism) can help in understanding
writers? rewriting processes, and can be supported
by automatic tools (e.g. (Bourdaillet et al, 2007)).
In this work, we address the issue of how writ-
ers can be assisted in finding wordings that corre-
spond to multi-word phrases of any nature. Given
an original text span, the writer is presented with
a list of rephrasings that are organized by taking
into account the context of the rephrasing and user-
specified preferences. Our proposal can therefore
be used as a lexicon operating at the phrasal level,
which can be used either when writers are faced
with a tip-of-the-tongue lexical access problem, or
when they are not completely satisfied with some
initial wording. In the former case, they may be
able to come up with some words or phrases that
would be different in meaning from what they are
looking for, and in the latter they may be looking
for a near-synonymous wording that is more ap-
propriate to a given context, for example to avoid
close repetitions. To define such a phrase lexi-
con and its possible mode of use, the following
questions should be considered: (a) how the lex-
icon entries are obtained, (b) what can be the entry
points and how can one navigate in the results, and
(c) how the results are displayed.
Rephrasing can be more or less complex and
problematic depending on the consequences at the
various levels:
? In the simplest case, replacing one element
77
by another does not have any consequences
overall. This is often the case when a word is
replaced by its synonym or a similar word.
? An entire expression or sentence is replaced
by its equivalent. In this case the problem is
generally to obtain a good fit with regard to
the surrounding text, the replacing unit being
well-formed by definition.
? The replacing element may require syntactic
changes of the matrix, i.e. the text in which it
is embedded. This occurs if the source word
and the target word have different syntactic
requirements, and this can be seen as a good
reason to replace entire sentences, or at least
sentence fragments. This assumes a pattern
dictionary, where patterns achieving the same
conceptual goal are grouped together.
In the next section, we discuss limitations of tra-
ditional dictionaries with respect to the targeted
task, and describe an approach to obtain phrase
rephrasings through a pivot translation into another
language. In section 3, we discuss the issue of the
organization of the results along various axis: flu-
ency of rephrasings, preservation of meaning, and
lexical divergence between original text spans and
rephrasings. We then present an initial evaluation
of our approach on French rephrasing in section 4.
Related work is presented in section 5, and we fi-
nally discuss our approach and our future work in
section 6.
2 Lexicon of phrase rephrasings
Dictionaries and semantic resources such as the-
sauri can be used to find words by following links
of different kinds from a given entry point. Word-
Net (Fellbaum, 1998) is one such resource. For a
proposal of other kinds of links and navigational
aids see also (Zock and Bilac, 2004; Zock, 2006;
Zock, 2007).
Words are the traditional units that people ex-
pect to find in dictionaries. Whereas some types
of dictionaries can contain multiword expressions,
such as compound nouns and terms, those corre-
spond to linguistically-motivated units. In order
to rephrase phrases of any type with a dictionary, a
writer may have to look up several words, combine
various information and validate the result using
her experience of the language or throught the use
of a concordancer. Moreover, dictionary lookups
are in most cases insensitive to the actual context
of words in an existing text. It is therefore the re-
sponsibility of its users to ensure that a choice is
appropriate for a given context, which can be quite
difficult, for example when writing in a second lan-
guage.
One way of obtaining phrase rephrasings is by
looking at phrases that occur in similar contexts
in a monolingual corpus (e.g. (Munteanu and
Marcu, 2006)). In order to extract a comprehensive
phrase lexicon, a very large number of sentences
should be compared to extract potential rephras-
ings, which furthermore may often correspond to
phrases that are too remotely connected. Parallel
corpora provide the interesting advantage that it is
reasonable to assume that elements from one side
of the corpus should be aligned to elements on the
other side, and that associations of elements can be
reinforced by the number of times they occur in the
corpus. Various approaches for word alignment
from parallel corpora have been proposed (see e.g.
(Och and Ney, 2003)), and the phrase-based ap-
proach to Statistical Machine Translation (Koehn
et al, 2003) has led to the development of heuris-
tics for obtaining alignments between phrases of
any number of words.
Unfortunately, monolingual parallel corpora
aligned at the sentence level, such as various trans-
lations of a novel in a foreign language, are re-
sources that are extremely scarce. Using bilingual
parallel corpora, a much more common resource,
one can obtain various possible phrase translations
for a given source phrase, as well as some estimate
of the distribution of probabilities for the various
translations of that phrase. Such N ? M aligne-
ments can capture lexical translations (e.g. exi-
geons ? ask for, call for, demand, expect, request,
etc.) and phrasal literal or idiomatic translations
(e.g. un bon de?but ? a good approach, a good
first move, a good starting point, a positive initia-
tive, an encouraging start, the right road, etc.), but
can also capture noise depending on the alignment
heuristics used (e.g. les e?tats candidats (candi-
date countries) ? Member States, the candidate
countries were to, the accession countries have
called for, candidate, the, etc.) Different target
phrases associated with a given source phrase can
either represent paraphrases or phrases with differ-
ent meanings. Among the limitations of this type
of phrasal alignments are their inability to model
non-consecutive words and to generalize the con-
78
tents of phrases, and the fact that their translations
are not conditioned on their context.
If phrase extraction is performed in two oppo-
site directions, then it is possible to find the pos-
sible translations of a given phrase (and their con-
ditional probabilities), and then to translate back
those phrases into the original language. In this ap-
proach proposed by (Bannard and Callison-Burch,
2005), the second language acts as a pivot, as il-
lustrated on figure 1. Because of the nature of the
possible alignments, this pivot can represent vari-
ous senses, which in context can be equivalent or
comparable to that of the original phrase. In turn,
the same phenomena can take place when translat-
ing back from the pivot phrases to the original lan-
guage, and the resulting rephrasings can be equiv-
alent or comparable in meaning to that of the orig-
inal phrase in some context, may also be incom-
plete and/or require other changes in the rephrased
sentence.
Bannard and Callison-Burch have defined a
paraphrase probability between two phrases p
1
and p
2
(with p
1
6= p
2
) that uses conditional proba-
bilities between phrases and sums over all possible
pivot phrases:
P (p
2
|p
1
) = argmax
p
2
6=p
1
?
pivot
P (pivot|p
1
)P (p
2
|pivot)
(1)
(Callison-Burch, 2007) measured the impor-
tance of various factors impacting the quality of
the paraphrases obtained. Using manually built
alignments yields a significant improvement in
paraphrase quality, showing that if better align-
ments are available the proposed approach can
produce better paraphrases. Alignments between
several languages can be used for finding pivot
phrases, and using several simulateously tend to
improve alignment quality and therefore para-
phrases themselves. Using a language model to
find paraphrases that maximize its score in the
original sentencial context leads to improved flu-
ency, but has a negative impact on meaning preser-
vation. Lastly, restricting pivot phrases to those
actually aligned in a test aligned bilingual corpus
improves paraphrase quality, which illustrates the
importance of disambiguating source phrases rela-
tively to the pivot language.
The rephrasings obtained can be classified into
several categories when used in context:
? A rephrasing can be a paraphrase that is valid
in all contexts (e.g. je vous donne raison ?
je suis d?accord avec vous), in specific gram-
matical contexts (e.g. pouvoir accueillir dans
de bonnes conditions les pays ? comme il se
doit) and/or pragmatic contexts (e.g. c?est un
bon de?but ? nous partons du bon pied).
? A rephrasing can contain shifts in meaning
with the original phrase which might be ac-
ceptable or not (e.g. nous voulons apporter
notre contribution a` ce de?bat ? donner de
la valeur). Some such rephrasings reveal a
natural bias towards the bilingual corpus used
(e.g. le prochain e?largissement constitue la
principale ta?che ? l? objectif principal).
? A rephrasing can be ill-formed but still con-
tain elements of interest to a writer (e.g. ceux
qui disent que . . . se trompent ? devrions
a` nouveau re?fle?chir; here a rephrasing such
as devraient a` nouveau re?fle?chir could be
deemed acceptable in some contexts).
? A rephrasing may introduce a contradiction
in a specific context (e.g. ce n?est pas le mo-
ment de se montrer he?sitant ? il est trop to?t
pour)
? A rephrasing may be inexploitable because it
is syntactically ill-formed in context and does
not contain any element of interest, or is too
close to the original phrase.
The most natural entry point to such a resource
is by entering a phrase or selecting it in a text under
revision. Approximate search can also be of use,
as done in some concordancer software, for exam-
ple by allowing the user to enter word-based reg-
ular expressions mixing literal words, word lem-
mas, word part-of-speech or even word classes
(e.g. types of named entities). Boolean queries
on indexes of word lemmas can also be used to of-
fer yet more flexibility to search the lexicon, but at
the cost of more candidate results. Once results are
returned, they can recursively be reused as source
phrases, so as to offer a means to navigate by iter-
ative refining.
3 Evaluation of rephrasings in context
for ranking results
Each candidate phrase rephrasing for a given
phrase must be evaluated in order to define a rank-
ing order for presentation to the user, and possibly
79
Figure 1: Example of rephrasing for the French phrase ce n?est pas le moment de using English as pivot.
to discard some of them. The proposed ranking
should reflect as best as possible the preferences of
the user for the task at hand in order to minimize
reading time and maintain the user?s interest in us-
ing the phrase lexicon. It is essential to give the
user some control over how the results are returned
depending on what is more important to her. For
example, (Ferret and Zock, 2006) have proposed
to present results from a dictionary enriched with
topical associations in chunks to allow for catego-
rial search. There will be cases where the user may
find acceptable only grammatical results, while in
other cases the user might accept agrammatical re-
sults provided they contain interesting suggestions.
Moreover, it seems extremely important that result
ranking can take into account the phrase substitu-
tion into the original context.
Considering how the proposed phrase lexicon is
built, the pivot paraphrasing probability of equa-
tion 1 (PIV) can be used as a baseline ordering.
Such a model reflects some strength of association
between a rephrased phrase and the original phrase
using the extracted phrases and conditional prob-
abilities derived from a bilingual training corpus.
It is therefore expected that results will be biased
towards that corpus if the latter belongs to a partic-
ular genre or theme. Nonetheless, one can expect
that some associations will be general enough to
be of general interest.
In addition, several models that users can in-
terpret as ranking criterion can be used simulate-
neously using the log-linear framework tradition-
ally used in SMT systems. However, contrary to
what is done in SMT, the weight of the models
cannot be automatically optimized if we do not use
an automatic evaluation of rephrasing quality, the
definition of which depending heavily on the sub-
jective appreciation of a user. Equation 2 shows
how the score of a rephrasing p
2
of p
1
can be com-
puted, where M is the set of models used, h
m
is
the logarithm of the normalized score of a model
and ?
m
its weight (with?
m?M
?
m
= 1), and C
is the original sentence and the placeholder for the
rephrased phrase.
s(p
2
, p
1
, C) =
?
m?M
?
m
h
m
(p
1
, p
2
, C) (2)
3.1 Control over fluency
As noted by (Mutton et al, 2007), the notion of
sentence-level fluency is not uniformely agreed
upon, and its evaluation by human judges is some-
times found subjective, but in practice judges can
obtain high levels of agreement about what can
be considered fluent or not. Like (Callison-Burch,
2007), we can use a language model (LM) to as-
sess the local fluency of a sentence after a phrase
has been substituted with a rephrasing. A degra-
dation in score (with a fluent original sentence)
can indicate that the rephrasing segment should be
adapted to the sentence, and/or that the sentence
itself should be modified in order to integrate the
new phrase as is.
Syntax parsers can produce various information
that can be relevant for assessing the fluency of
sentences, which can be used as features from dif-
ferent parsers for classification that can correlate
well with human judgment (Mutton et al, 2007).
When substituting a part of a sentence with an-
other phrase and if this substitution does not re-
quire other changes in the sentence, then at least
the dependency relationships between words out-
side that phrase should be preserved. This seems
coherent with our objective of focussing on the
task of phrase rephrasing when it is possible to
modify only a given phrase and obtain an accept-
able result.
80
3.2 Control over meaning preservation
The preservation of dependency relationships out-
side of the rephrased phrase can also play a role
in terms of meaning preservation. Dependency
relationships connecting words in the phrase and
words outside the phrase (i.e., whose governor is
outside the phrase and dependant inside it, or the
opposite) should still exist after such a substitution,
but possibly with a modified dependency target in
the phrase. Indeed, those relationships denote the
grammatical role of the words of the phrase rela-
tive to their context, and if those are preserved then
it is more likely that meaning is preserved.
We use a model based on dependency preser-
vation (DEP) which involves relationships outside
the rephrased phrase and relationships crossing
a boundary of that phrase. The score is based
on some proportion of the number of such de-
pendencies found after substitution over the num-
ber of original dependencies (see (Max, 2008) for
details). Another way of controlling for mean-
ing preservation is to ensure that only the pivot
phrases with the same meaning as the original
phrase are kept (and then their back translations).
(Callison-Burch, 2007) has shown the positive im-
pact on paraphrase quality of using a controlled
pivot present in an aligned sentence in a test bilin-
gual corpora. Phrase disambiguation techniques
have been proposed for SMT and could be applied
to the problem at hand (e.g. (Stroppa et al, 2007)).
In an interactive context, it makes sense to let the
user the opportunity to control for phrase sense by
rejecting bad pivot phrases if she wants to, which
is then similar to Callison-Burch?s experiment set-
tings. This manual selection must of course be op-
tional, but can be used when a user prefers a stricter
control on meaning. Another possibly interesting
use is to disambiguate in a pivot language corre-
sponding to one?s native language when writing in
a foreign language.
3.3 Control over lexical divergence
There will be cases when possible rephrasings will
be very close to their original phrase, differing
for example by only punctuation marks or verbal
forms1. Writers may sometimes prefer rephras-
ings that differ by just one word, or on the con-
trary rephrasings that use a set of completely dif-
ferent words. To account for differents words be-
1This is particularly the case when aligning between low
and highly inflected languages.
Figure 2: Bilingual phrase lexicon statistics
tween an original phrase and its rephrasing, we use
a model (LEM) that returns a proportion of lem-
mas for full words that only belong to a rephrasing
over all such lemmas for an initial phrase and its
rephrasing (see (Max, 2008)).
4 Experiments and evaluation
We carried out an evaluation on the local rephras-
ing of French sentences, using English as the
pivot language.2 We extracted phrase align-
ments of up to 7 word forms using the Giza++
alignment tool (Och and Ney, 2003) and the
grow-diag-final-and heuristics described
in (Koehn et al, 2003) on 948,507 sentences
of the French-English part of the Europarl cor-
pus (Koehn, 2005) and obtained some 42 million
phrase pairs for which probabilities were estimated
using maximum likelihood estimation. Statistics
for the extracted lexicons are reported on figure 2.
Entries of the monolingual phrase lexicon are built
dynamically from the entries of the monolingual
lexicons.
For the LM model, we used a 5-gram language
model trained on the French part of the corpus us-
ing Kneser-Ney smoothing. The robust parser for
French SYNTEX (Bourigault et al, 2005) was used
to obtain lemmas for word and labeled dependency
relationships between words, used respectively for
the LEM and DEP models. Robust parsers provide
the advantage that they can provide partial analysis
for correct chunks in agrammatical sentences, but
they can also recover information from agrammat-
ical chunks which can be undesirable in this case.3
A test corpus of 82 sentences that were not used
for extracting phrase alignments and learning the
2The main motivation for this choice was that we could
easily have access to French native speakers for manual eval-
uation. We plan however to start new experiments using En-
glish, as well as experiments using another highly inflected
language as pivot such as Spanish.
3We intend to use several parsers for English implement-
ing different approaches as in (Mutton et al, 2007), but we
had access to only one parser for French.
81
language model was built. A human judge selected
one phrase of length 3 words or more per sen-
tence that would be a good candidate for rephras-
ing, and which was accepted if it belonged to the
French-English lexicon4. We kept at most the 20
first rephrasings obtained using the baseline PIV
model, and asked two French native speakers to
evaluate on a 5-level scale each the 1648 refor-
mulated sentences obtained on fluency, meaning
preservation, and authoring value, where the lat-
ter was described in the following way: (5) the
rephrasing can be directly reused for revising a
text, (4) the rephrasing can be used with a mi-
nor change, (3) the rephrasing contains elements
that could be used for a good rephrasing, (2) the
rephrasing contains elements that could suggest a
rephrasing, and (1) the rephrasing is useless.
After the judges had completed manual annota-
tion, smoothing of the scores was done by keep-
ing mean scores for each sentence. We measured
a value of 0.59 standard deviation for score differ-
ences between judges for grammaticality, 0.7 for
meaning preservation and 0.8 for authoring value.
Those values can indicate a growing difficulty in
judging those characteristics, and in particular that
judging authoring value on the proposed scale is
more dependant on personal judgment. Results of
mean scores for the first rank solutions with vari-
ous model combinations with uniform weights are
reported on figure 3, and results for mean author-
ing value scores depending on the number of top
results presented to the user are reported on fig-
ure 4.
Authoring value scores are lower, which can be
explained by the fact that rephrasings with bad
fluency and/or meaning preservation scores will
penalize authoring value scores according to our
scale. The best results are obtained when combin-
ing all models, which remains true when consider-
ing mean results up to at least 8 rephrasings.
The baseline PIV model seems to have the most
impact, but all other models also contribute in
different ways. This suggests that which model
should be used (or its weight in our framework)
could be chosen by a user. In the following ex-
ample, the LEM model helped select a rephrasing
which obtained good scores:
Original sentence: ce que je vous propose donc,
4This is a limitation of our evaluation, as our annotator
was not strictly speaking revising a text that she wrote. We
hope to be able to conduct task-based experiments in the fu-
ture.
fluency meaning authoring
PIV (baseline) 4.46 4.18 3.62
LM 4.28 3.62 3.45
DEP 4.35 3.68 3.43
LEM 4.05 3.21 3.28
PIV+LM 4.65 4.06 3.82
PIV+DEP 4.58 4.27 3.66
PIV+LEM 4.37 4.00 3.76
LM+DEP 4.49 3.81 3.68
LM+LEM 4.28 3.59 3.56
PIV+LM+DEP 4.65 4.05 3.92
PIV+LM+LEM 4.61 4.02 3.97
PIV+DEP+LEM 4.57 4.17 4.02
LM+DEP+LEM 4.37 3.69 3.64
PIV+LM+DEP+LEM 4.68 4.09 4.05
Figure 3: Mean results at first rank for various
model combinations (uniform weighting)
Figure 4: Mean authoring value scores depending
on the number of results presented to the user
c?est de travailler dans cette direction ... (what I
therefore propose is to work towards this . . .)
Rephrased sentence: ce que je vous pro-
pose donc, c?est de coope?rer dans ce sens ...
(work towards this goal . . .)
Figures 5 and 6 show two examples of rephras-
ings in French, whereby for each rephrasing the
ranks given by PIV, LM and the combination of
all mentioned models are shown.
5 Related work
While the traditional view of lexicons is word-
based, we may as well consider larger units, in-
cluding sentences. Corpus Pattern Analysis (CPA)
(Hanks and Pustejovsky, 2005) is concerned with
the prototypical syntagmatic patterns with which
words in use are associated. For example, the
meaning of take place is different from the mean-
82
Rephrasings Ranks given by model(s)
PIV LM PIV+LM+DEP+LEM
quelques points essentiels 1 3 1
les points essentiels 19 1 2
plusieurs questions importantes 17 4 3
des points essentiels 8 6 4
deux ou trois questions importantes 5 9 5
plusieurs points importants 11 2 5
un certain nombre de questions importantes 17 7 7
certains points importants 2 5 8
un certain nombre de points importants 3 8 9
certains e?le?ments tre`s importants 13 11 10
une se?rie de points importants 4 12 11
quelques accents importants 5 15 11
des choses extre?mement importantes 13 14 11
quelques remarques importantes , 8 16 14
des points importants 12 10 15
quelques choses tre`s importantes 13 17 16
certains points importants , 8 13 17
quelques points essentiels sur 20 18 17
de certains e?le?ments tre`s importants 13 19 19
placer quelques accents importants 5 20 20
Figure 5: Examples of rephrasings for the phrase quelques points importants in je voudrais mentionner
quelques points importants de la directive
Rephrasings Ranks given by model(s)
PIV LM PIV+LM+DEP+LEM
vous avez raison 1 1 1
je suis d? accord avec vous 2 2 2
je suis d? accord 3 6 3
je conviens avec vous 6 5 4
je partage votre avis 7 4 5
vous avez raison de dire 10 3 5
je pense comme vous 7 8 7
je suis parfaitement d? accord avec vous 12 7 8
je partage votre point de vue 12 9 9
je vous rejoins 7 10 10
, je vous donne raison 3 12 11
la` , je vous donne raison 3 13 12
tu as raison 16 11 12
vous avez raison de 10 14 14
je partage votre point 12 15 15
je partage votre point de 12 16 16
Figure 6: Examples of rephrasings for the phrase je vous donne raison in a` cet e?gard bien pre?cis , je vous
donne raison , monsieur le commissaire
83
ing of take his place, due to the possessive deter-
miner. The actual meaning of words depends on
the context in which they are used. The work done
by the team of Gross on lexicon-grammar (e.g.
(Gross, 1984)) showed that a relatively small set of
clause patterns and syntactic constraints suffices to
cover most of common French.
Comparable monolingual corpora have been
used for automatic paraphrasing. Barzilay and
Lee (Barzilay and Lee, 2003) learned paraphras-
ing patterns as pairs of word lattices, which are
then used to produce sentence level paraphrases.
Their corpus contained news agency articles on the
same events, which allows precise sentence para-
phrasing, but on a small sets of phenomena and
for a limited domain. As sentential paraphras-
ing is more likely to alter meaning, Quirk et al
(Quirk et al, 2004) approached paraphrasing as
a monotonous decoding by a phrase-based SMT
system. Their corpus consisted of monolingual
sentences extracted from a comparable corpus that
were automatically aligned so as to allow aligned
phrase extraction. Pang et al (Pang et al, 2003)
used parallel monolingual corpora built from news
stories that had been independantly translated sev-
eral times to learn lattices from a syntax-based
alignment process.
Bannard and Callison-Burch (Bannard and
Callison-Burch, 2005) proposed to use pivot trans-
lation for paraphrasing phrases. Fujita (Fujita,
2005) proposed a transfer-and-revision framework
using linguistic knowledge for generating para-
phrases in Japanese and a model for error detec-
tion. At the lexical level, a recent evaluation on En-
glish lexical substitution was held (McCarthy and
Navigli, 2007) in which systems had to find lexical
synonyms and disambiguate the context.
6 Discussion and future work
In this article, we have presented an approach for
obtaining rephrasings for short text spans from par-
allel bilingual corpora. These rephrasings can be
ranked according to user-defined preferences, and
the weights of the models used can be dynamically
adjusted by a user depending on what features are
more important to her, for instance after an initial
list of candidates has been proposed by the sys-
tem. Indeed, good candidates include paraphrases,
but also more generally phrases that could help a
writer revise a text with some shifts in meaning,
even if at the cost of some corrections to make the
resulting text grammatical. Furthermore, search
for rephrasings can be iteratively performed using
candidate rephrasings as source phrases, and the
user can have some fine-grained control if select-
ing or rejecting possible pivot phrases manually.
Possible user interfaces to this proposed bilingual
phrase lexicon could include rephrasing memory
features to learn from interaction with the user, and
concordancing features to display the context of
use in the bilingual corpus of the segments used to
build the relevant lexicon entries. In the latter case,
the similarity used to select examples could take
the context of the phrases into account in terms of
dependency relationships.
There are several open issues to the presented
work. Important issues are where the phrases
can come from and the bias introduced by the re-
source used. Using a bilingual corpora such as
the Europarl corpus with this pivot approach yields
both generic and domain/genre-specific rephras-
ings, and it is important to be able to determine
their appropriate context of use. It would also
be interesting to investigate enriching this frame-
work with phrases learnt from monolingual cor-
pora from a given domain or genre, and to use fea-
tures from the current text under revision. More
generally, we would need to get some idea of the
degree of possible reuse of a given rephrasing.
Another important group of issues concerns lim-
itations due to the nature of phrases for the task
at hand. As we have said, phrases as units of
rephrasing are limited because they cannot model
non-consecutive words and because of the rigidity
of their content. Various types of entry points to
the rephrasing lexicon such as using word-based
regular expressions can in some way alleviate this
problem, but work could be done on the lexicon
itself. As shown by Callison-Burch (Callison-
Burch, 2007), much can be gained by using bet-
ter alignments. Alignments techniques using syn-
tactic information could eliminate weak rephras-
ing candidates (i.e. increase in overall precision),
but interesting phrasal alignments could be lost as
well (decrease in overall recall). Furthermore, in-
formation from the context of alignments could
also be used to disambiguate the source phrase and
get only pivot phrases that are compatible with the
context of a given rephrasing, in similar ways as
recently done for SMT (Stroppa et al, 2007).
84
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
NAACL/HLT, Edmonton, Canada.
Bourdaillet, Julien, Jean-Gabriel Ganascia, and Ire`ne
Fenoglio. 2007. Machine assisted study of writ-
ers? rewriting processes. In Proceedings of NLPCS,
poster session, Madeire, Portugal.
Bourigault, Didier, Ce?cile Fabre, Ce?cile Frrot, Marie-
Paule Jacques, and Sylvia Ozdowska. 2005. Syntex,
analyseur syntaxique de corpus. In Proceedings of
TALN, Dourdan, France.
Callison-Burch, Chris. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh.
Fellbaum, Christiane, editor, 1998. WordNet: An Elec-
tronic Lexical Database and some of its Applica-
tions. MIT Press.
Ferret, Olivier and Michael Zock. 2006. Enhancing
electronic dictionaries with an index based on asso-
ciations. In Proceedings of COLING/ACL, Sydney,
Australia.
Fujita, Atsushi. 2005. Automatic Generation of Syn-
tactically Well-formed and Semantically Appropriate
Paraphrases. Ph.D. thesis, Nara Institute of Science
and Technology.
Gross, Maurice. 1984. Lexicon-grammar and the anal-
ysis of french. In Proc. of the 11th COLING, pages
275?282, Stanford, CA.
Hanks, Patrick and James Pustejovsky. 2005. A pattern
dictionary for natural language processing. Revue
Franc?aise de linguistique applique?e, 10(2):63?82.
Koehn, Philipp, Franz Josef Och, , and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL/HLT, Edmonton, Canada.
Koehn, Philipp. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit, Phuket, Thailand.
Max, Aure?lien. 2008. Local rephrasing suggestions for
supporting the work of writers. In Proceedings of
GoTAL, Gothenburg, Sweden.
McCarthy, Diana and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Semeval-2007 Workshop at ACL,
Prague, Czech Republic.
Munteanu, Dragos S. and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of COLING/ACL
2006, Sydney, Australia.
Mutton, Andrew, Mark Dras, Stephen Wan, and Robert
Dale. 2007. GLEU : Automatic evaluation of
sentence-level fluency. In Proceedings of ACL,
Prague, Czech Republic.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Pang, Bo, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences.
In Proceedings of NAACL/HLT, Edmonton, Canada.
Quirk, Chris, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of EMNLP,
Barcelona, Spain.
Stroppa, Nicolas, Antal van den Bosch, and Andy Way.
2007. Exploiting source similarity for smt using
context-informed features. In Proceedings of TMI,
Skvde, Sweden.
Zock, Michael and Slaven Bilac. 2004. Word lookup
on the basis of associations : from an idea to a
roadmap. In Workshop on ?Enhancing and using
electronic dictionaries?, pages 29?35, Geneva. COL-
ING.
Zock, Michael. 2006. Navigational aids, a critical
factor for the success of electronic dictionaries. In
Rapp, Reinhard, P. Sedlmeier, and G. Zunker-Rapp,
editors, Perspectives on Cognition: A Festschrift for
Manfred Wettler, pages 397?414. Pabst Science Pub-
lishers, Lengerich.
Zock, Michael. 2007. If you care to find what you
are looking for, make an index: the case of lexical
access. ECTI, Transaction on Computer and Infor-
mation Technology, 2(2):71?80.
85
Workshop on Monolingual Text-To-Text Generation, pages 10?19,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 10?19,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Web-based validation for contextual targeted paraphrasing
Houda Bouamor
LIMSI-CNRS
Univ. Paris Sud
hbouamor@limsi.fr
Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
amax@limsi.fr
Gabriel Illouz
LIMSI-CNRS
Univ. Paris Sud
gabrieli@limsi.fr
Anne Vilnat
LIMSI-CNRS
Univ. Paris Sud
anne@limsi.fr
Abstract
In this work, we present a scenario where con-
textual targeted paraphrasing of sub-sentential
phrases is performed automatically to support
the task of text revision. Candidate para-
phrases are obtained from a preexisting reper-
toire and validated in the context of the orig-
inal sentence using information derived from
the Web. We report on experiments on French,
where the original sentences to be rewrit-
ten are taken from a rewriting memory au-
tomatically extracted from the edit history of
Wikipedia.
1 Introduction
There are many instances where it is reasonable to
expect machines to produce text automatically. Tra-
ditionally, this was tackled as a concept-to-text real-
ization problem. However, such needs apply some-
times to cases where a new text should be derived
from some existing texts, an instance of text-to-text
generation. The general idea is not anymore to pro-
duce a text from data, but to transform a text so as to
ensure that it has desirable properties appropriate for
some intended application (Zhao et al, 2009). For
example, one may want a text to be shorter (Cohn
and Lapata, 2008), tailored to some reader pro-
file (Zhu et al, 2010), compliant with some spe-
cific norms (Max, 2004), or more adapted for sub-
sequent machine processing tasks (Chandrasekar et
al., 1996). The generation process must produce
a text having a meaning which is compatible with
the definition of the task at hand (e.g. strict para-
phrasing for document normalization, relaxed para-
phrasing for text simplification), while ensuring that
it remains grammatically correct. Its complexity,
compared with concept-to-text generation, mostly
stems from the fact that the semantic relationship
between the original text and the new one is more
difficult to control, as the mapping from one text to
another is very dependent on the rewriting context.
The wide variety of techniques for acquiring phrasal
paraphrases, which can subsequently be used by text
paraphrasing techniques (Madnani and Dorr, 2010),
the inherent polysemy of such linguistic units and
the pragmatic constraints on their uses make it im-
possible to ensure that potential paraphrase pairs
will be substitutable in any context, an observation
which was already made at a lexical level (Zhao et
al., 2007). Hence, automatic contextual validation of
candidate rewritings is a fundamental issue for text
paraphrasing with phrasal units.
In this article, we tackle the problem of what we
call targeted paraphrasing, defined as the rewriting
of a subpart of a sentence, as in e.g. (Resnik et al,
2010) where it is applied to making parts of sen-
tences easier to translate automatically. While this
problem is simpler than full sentence rewriting, its
study is justified as it should be handled correctly
for the more complex task to be successful. More-
over, being simpler, it offers evaluation scenarios
which make the performance on the task easier to
assess. Our particular experiments here aim to as-
sist a Wikipedia contributor in revising a text to im-
prove its quality. For this, we use a collection of
phrases that have been rewritten in Wikipedia, and
test the substitutability of paraphrases coming from
a repertoire of sub-sentential paraphrases acquired
10
from different sources. We thus consider that preex-
isting repertoires of sub-sentential paraphrase pairs
are available, and that each potential candidate has to
be tested in the specific context of the desired rewrit-
ing. Due to the large variety of potential phrases
and their associated known paraphrases, we do not
rely on precomputed models of substitutability, but
rather build them on-the-fly using information de-
rived from web queries.1
This article is organized as follows. In section 2,
we first describe the task of text revision, where a
subpart of a sentence is rewritten, as an instance
of targeted paraphrasing. Section 3 presents previ-
ous works on the acquisition of sub-sentential para-
phrases and describes the knowledge sources that we
have used in this work. We then describe in section 4
how we estimate models of phrase substitution in
context by exploiting information coming from the
web. We present our experiments and their results in
section 5, and finally discuss our current results and
future work in section 6.
2 Targeted paraphrasing for text revision
One of the important processes of text revision is
the rewording of parts of sentences. Some reword-
ings are not intended to alter meaning significantly,
but rather to make text more coherent and easier to
comprehend. Those instances which express close
meanings are sub-sentential paraphrases: in their
simpler form, they can involve synonym substitu-
tion, but they can involve more complex deeper
lexical-syntactic transformations.
Such rephrasings are commonly found in record-
ings of text revisions, which now exist in large
quantities in the collaborative editing model of
Wikipedia. In fact, revision histories of the encyclo-
pedia contain a significant amount of sub-sentential
paraphrases, as shown by the study of (Dutrey et al,
2011). This study also reports that there is an impor-
tant variety of rephrasing phenomena, as illustrated
by the difficulty of reaching a good identification
coverage using a rule-based term variant identifica-
tion engine.
1Note that using the web may not always be appropriate, or
that at least it should be used in a different way than what we
propose in this article, in particular in cases where the desired
properties of the rewritten text are better described in controlled
corpora.
The use of automatic targeted paraphrasing as an
authoring aid has been illustrated by the work of
Max and Zock (2008), in which writers are pre-
sented with potential paraphrases of sub-sentential
fragments that they wish to reword. The automatic
paraphrasing technique used is a contextual vari-
ant of bilingual translation pivoting (Bannard and
Callison-Burch, 2005). It has also been proposed
to externalize various text editing tasks, including
proofreading, by having crowdsourcing functions on
text directly from word processors (Bernstein et al,
2010).
Text improvements may also be more specifi-
cally targeted for automatic applications. In the
work by Resnik et al (2010), rephrasings for spe-
cific phrases are acquired through crowdsourcing.
Difficult-to-translate phrases in the source text are
first identified, and monolingual contributors are
asked to provide rephrasings in context. Collected
rephrasings can then be used as input for a Ma-
chine Translation system, which can positively ex-
ploit the increased variety in expression to pro-
duce more confident translations for better estimated
source units (Schroeder et al, 2009).2 For instance,
the phrase in bold in the sentence The number of
people known to have died has now reached 358
can be rewritten as 1) who died, 2) identified to
have died and 3) known to have passed away. All
such rephrasings are grammatically correct, the first
one being significantly shorter, and they all convey
a meaning which is reasonably close to the original
wording.
The task of rewriting complete sentences has also
been addressed in various works (e.g. (Barzilay and
Lee, 2003; Quirk et al, 2004; Zhao et al, 2010)). It
poses, however, numerous other challenges, in par-
ticular regarding how it could be correctly evalu-
ated. Human judgments of whole sentence trans-
formations are complex and intra- and inter-judge
coherence is difficult to attain with hypotheses of
comparable quality. Using sentential paraphrases
to support a given task (e.g. providing alternative
reference translations for optimizing Statistical Ma-
chine Translation systems (Madnani et al, 2008))
2It is to be noted that, in the scenario presented in (Resnik et
al., 2010), monolingual contributors cannot predict how useful
their rewritings will be to the underlying Machine Translation
engine used.
11
can be seen as a proxy for extrinsic evaluation of
the quality of paraphrases, but it is not clear from
published results that improvements on the task are
clearly correlated with the quality of the produced
paraphrases. Lastly, automatic metrics have been
proposed for evaluating the grammaticality of sen-
tences (e.g. (Mutton et al, 2007)). Automatic evalu-
ation of sentential paraphrases has not produced any
consensual results so far, as they do not integrate
task-specific considerations and can be strongly bi-
ased towards some paraphrasing techniques.
In this work, we tackle the comparatively more
modest task of sub-sentential paraphrasing applied
to text revision. In order to use an unbiased
task, we use a corpus of naturally-occurring rewrit-
ings from an authoring memory of Wikipedia ar-
ticles. We use the WICOPACO corpus (Max and
Wisniewski, 2010), a collection of local rephras-
ings from the edit history of Wikipedia which con-
tains instances of lexical, syntactical and semantic
rephrasings (Dutrey et al, 2011), the latter type be-
ing illustrated by the following example:
Ce vers de Nuit rhe?nane d?Apollinaire [qui para??t
presque sans structure rythmique? dont la ce?sure
est comme masque?e]. . . 3
The appropriateness of this corpus for our work
is twofold: first, the fact that it contains naturally-
occurring rewritings provides us with an interest-
ing source of text spans in context which have been
rewritten. Moreover, for those instances where the
meaning after rewriting was not significantly al-
tered, it provides us with at least one candidate
rewriting that should be considered as a correct para-
phrase, which can be useful for training validation
algorithms.
3 Automatic sub-sentential paraphrase
acquisition and generation
The acquisition of paraphrases, and in particular
of sub-sentential paraphrases and paraphrase pat-
terns, has attracted a lot of works with the advent of
data-intensive Natural Language Processing (Mad-
nani and Dorr, 2010). The techniques proposed have
a strong relationship to the type of text corpus used
3This verse from Apollinaire?s Nuit Rhe?nane [which seems
almost without rhythmic structure ? whose cesura is as if
hidden]. . .
for acquisition, mainly:
? pairs of sentential paraphrases (monolingual
parallel corpora) allow for a good precision
but evidently a low recall (e.g. (Barzilay and
McKeown, 2001; Pang et al, 2003; Cohn et
al., 2008; Bouamor et al, 2011))
? pairs of bilingual sentences (bilingual parallel
corpora) allow for a comparatively better re-
call (e.g. (Bannard and Callison-Burch, 2005;
Kok and Brockett, 2010))
? pairs of related sentences (monolingual com-
parable corpora) allow for even higher recall
but possibly lower precision (e.g. (Barzilay
and Lee, 2003; Li et al, 2005; Bhagat and
Ravichandran, 2008; Dele?ger and Zweigen-
baum, 2009)
Although the precision of such techniques can in
some cases be formulated with regards to a prede-
fined reference set (Cohn et al, 2008), it should
more generally be assessed in the specific context
of some use of the paraphrase pair. This refers to
the problem of substituability in context (e.g. (Con-
nor and Roth, 2007; Zhao et al, 2007)), which is a
well studied field at the lexical level and the object of
evaluation campains (McCarthy and Navigli, 2009).
Contextual phrase substitution poses the additional
challenge that phrases are rarer than words, so that
building contextual and grammatical models to en-
sure that the generated rephrasings are both seman-
tically compatible and grammatical is more compli-
cated (e.g. (Callison-Burch, 2008)).
The present work does not aim to present any
original technique for paraphrase acquisition, but
rather focusses on the task of sub-sentential para-
phrase validation in context. We thus resort to some
existing repertoire of phrasal paraphrase pairs. As
explained in section 2, we use the WICOPACO cor-
pus as a source of sub-sentential paraphrases: the
phrase after rewriting can thus be used as a potential
paraphrase in context.4 To obtain other candidates
of various quality, we used two knowledge sources.
The first uses automatic pivot translation (Bannard
and Callison-Burch, 2005), where a state-of-the-art
4Note, however, that in our experiments we will ask our hu-
man judges to assess anew its paraphrasing status in context.
12
general-purpose Statistical Machine Translation sys-
tem is used in a two-way translation. The second
uses manual acquisition of paraphrase candidates.
Web-based acquisition of this type of knowledge has
already been done before (Chklovski, 2005; Espan?a
Bonet et al, 2009), and could be done by crowd-
sourcing, a technique growing in popularity in recent
years. We have instead formulated manual acquisi-
tion as a web-based game. Players can take parts in
two parts of the game, illustrated on Figure 3.
First, players propose sub-sentential paraphrases
in context for selected text spans in web documents
(top of Figure 3), and then raters can take part in as-
sessing paraphrases proposed by other players (bot-
tom of Figure 3). In order to avoid any bias, players
cannot evaluate games in which they played. Eval-
uation is sped up by using a compact word lattice
view for eliciting human judgments, built using the
syntactic fusion algorithm of (Pang et al, 2003).
Data acquisition was done in French to remain co-
herent with our experiments on the French corpus
of WICOPACO, and both players and raters were
native speakers. An important point is that in our
experiments the context of acquisition and of evalu-
ation were different: players were asked to generate
paraphrases in contexts that are different from those
of the WICOPACO corpus used for evaluation. To
this end, web snippets were automatically retrieved
for the various phrases of our dataset without con-
texts, so that sentences from the Web (but not from
Wikipedia) were used for manual paraphrase acqui-
sition. This allows us to simulate the availability of a
preexisting repertoire of (contextless) sub-sentential
paraphrases, and to assess the performance of our
contextual validation techniques on a possibly in-
compatible context.
4 Web-based contextual validation
Given a repertoire of potential phrasal paraphrases
and a context for a naturally-occurring rewriting, our
task consists in deciding automatically which poten-
tial paraphrases can be substituted with good confi-
dence for the original phrase. A concrete instantia-
tion of it could correspond to the proposal of Max
and Zock (2008), where such candidate rephrasings
could be presented in order of decreasing suitability
to a word processor user, possibly during the revi-
sion of a Wikipedia article.
The specific nature of the text units that we are
dealing with calls for a careful treatment: in the
general scenario, it is unlikely that any supervised
corpus would contain enough information for ap-
propriate modeling of the substituability in context
decision. It is therefore tempting to consider using
the Web as the largest available information source,
in spite of several of its known limitations, includ-
ing that data can be of varying quality. It has how-
ever been shown that a large range of NLP applica-
tions can be improved by exploiting n-gram counts
from the Web (using Web document counts as a
proxy) (Lapata and Keller, 2005).
Paraphrase identification has been addressed pre-
viously, both using features computed from an of-
fline corpus (Brockett and Dolan, 2005) and fea-
tures computed from Web queries (Zhao et al,
2007). However, to our knowledge previous work
exploiting information from the Web was limited to
the identification of lexical paraphrases. Although
the probability of finding phrase occurrences sig-
nificantly increases by considering the Web, some
phrases are still very rare or not present in search
engine indexes.
As in (Brockett and Dolan, 2005), we tackle our
paraphrase identification task as one of monolingual
classification. More precisely, considering an orig-
inal phrase p within the context of sentence s, we
seek to determine whether a candidate paraphrase p?
would be a grammatical paraphrase of p within the
context of s. We make use of a Support Vector Ma-
chine (SVM) classifier which exploits the features
described in the remainder of this section.
Edit distance model score Surface similarity on
phrase pairs can be a good indicator that they share
semantic content. In order to account for the cost
of transforming one string into the other, rather
than simply counting common words, we use the
score produced by the Translation Edit Rate met-
ric (Snover et al, 2010). Furthermore, we perform
this computation on strings of lemmas rather than
surface forms:5
5Note that because we computed the TER metric on French
strings, stemming and semantic matching through WordNet
were not activated.
13
Figure 1: Interface of our web-based game for paraphrase acquisition and evaluation. On the top, players reformulate
all text spans highlighted by the game creator on any webpage (a Wikipedia article on the example). On the bottom,
raters evaluate paraphrases proposed by sets of players using a compact word-lattice view. Note that in its standard
definition, the game attributes higher scores to paraphrase candidates that are highly rated and rarer.
hedit = TER(Lemorig, Lempara) (1)
Note that this model is not derived from informa-
tion from the Web, in contrast to all the models de-
scribed next.
Language model score The likelihood of a sen-
tence can be a good indicator of its grammatical-
ity (Mutton, 2006). Language model probabilities
can now be obtained from Web counts. In our ex-
periments, we used the Microsoft Web N-gram Ser-
vice6 for research (Wang et al, 2010) to obtain log
likelihood scores for text units.7 However, this score
is certainly not sufficient as it does not take the orig-
inal wording into account. We therefore used a ratio
of the language model score of the paraphrased sen-
tence with the language model score of the original
6http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
7Note that in order to query on French text, we had to re-
move all diacritics for the service to behave correctly, indepen-
dently of encodings: careful examination of ranked hypotheses
showed that this trick allowed us to obtain results coherent with
expectations.
sentence, after normalization by sentence length of
the language model scores (Onishi et al, 2010):
hLM ratio =
LM(para)
LM(orig)
=
lm(para)1/length(para)
lm(orig)1/length(orig)
(2)
Contextless thematic model scores Cooccurring
words are used in distributional semantics to account
for common meanings of words. We build vector
representations of cooccurrences for both the origi-
nal phrase p and its paraphrase p?. Our contextless
thematic model is built in the following fashion: we
query a search engine to retrieve the top N docu-
ment snippets for phrase p. We then count frequen-
cies for all content words in these snippets, and keep
the set W of words appearing more than a fraction
of N . We then build a vector T (thematic profile)
of dimension |W | where values are computed by the
following formula:
Tnocontorig [w] =
count(p, w)
count(p)
(3)
14
where count(x) correspond to the number of docu-
ments containing a given exact phrase or word ac-
cording to the search engine used and count(x, y)
correspond to the number of documents containing
simultaneously both. We then compute the same
thematic profile for the paraphrase p?, using only the
subset of words W :
Tnocontpara [w] =
count(p?, w)
count(p)
(4)
Finally, we compute a similarity between the two
profiles by taking the cosinus between their two vec-
tors:
hnocontthem =
Tnocontorig ? T
nocont
para
||Tnocontorig || ? ||T
nocont
para ||
(5)
In all our experiments, we used the Yahoo! Search
BOSS8 Web service for obtaining Web counts and
retrieving snippets. Assuming that the distribution
of words in W is not biased by the result ordering
of the search engine, our model measures some sim-
ilarity between the most cooccurring content words
with p and the same words with p?.
Context-aware thematic model scores Our
context-aware thematic model takes into account
the words of sentence s in which the substitution
of p with p? is attempted. We now consider the set
of content words from s (s being the part of the
sentence without phrase p) in lieu of the previous
set of cooccurring words W , and compute the
same profile vectors and similarity between that of
the original sentence and that of the paraphrased
sentence:
hcontthem =
T contorig ? T
cont
para
||T contorig || ? ||T
cont
para||
(6)
However, words from s might not be strongly
cooccurring with p. In order to increase the likeli-
hood of finding thematically related words, we also
build an extended context model, hextcontthem where
content words from s are supplemented with their
most cooccurring words. This is done using the
same procedure as that previously used for finding
content words cooccurring with p.
8http://developer.yahoo.com/search/boss/
5 Experiments
In this section we report on experiments conducted
to assess the performance of our proposed approach
for validating candidate sub-sentential paraphrases
using information from the Web.
5.1 Data used
We randomly extracted 150 original sentences in
French and their rewritings from the WICOPACO
corpus which were marked as paraphrases. Of those,
we kept 100 for our training corpus and the remain-
ing 50 for testing. The number of original phrases of
each length is reported on Figure 2.
phrase length 1 2 3 4 5 6 7 8
original phrases 0 3 29 8 6 2 2 0
paraphrases 39 64 74 36 21 10 5 1
Figure 2: Distribution of number of phrases per phrase
length in tokens for the test corpus
For each original sentence, we collected 5 candi-
date paraphrases to simulate the fact that we had a
repertoire of paraphrases with the required entries:9
? WICOPACO: the original paraphrase from the
WICOPACO corpus;
? GAME: two candidate paraphrases from users
of our Web-based game;
? PIVOTES and PIVOTZH: two candidate para-
phrases obtained by translation by pivot, using
the Google Translate10 online SMT system and
one language close to French as pivot (Span-
ish), and another one more distant (Chinese).
We then presented the original sentence and its 5
paraphrases (in random order) to two judges. Four
native speakers took part in our experiments: they
all took part in the data collection for one half of
the sentences of the training and test corpora and to
the evaluation of paraphrases for the other half. For
the annotation with two classes (paraphrase vs. not
paraphrase), we obtain as inter-judge agreement11 a
9Note that, as a consequence, we did not carry any experi-
ment related to the recall of any technique here.
10http://translate.google.com
11We used R (http://www.r-project.org) to com-
pute this Cohen?s ? value.
15
Figure 3: Example of an original sentence and its 5 associated candidate paraphrases. The phrase in bold from the
original sentence (The brand is at the origin of many concepts that have revolutionized computing.) is paraphrased
as est le promoteur (is the promoter), a popularise? (popularized), origine (origin), est a` la source (is the source), and
l?origine (the origin).
value of ? = 0.65, corresponding to a substantial
agreement according to the literature. An example
of the interface used is provided in Figure 3.
We considered that our technique could not pro-
pose reliable results when web phrase counts were
too low. From the distribution of counts of phrases
and paraphrases from our training set (see Figure 4),
we empirically chose a threshold of 10 for the min-
imum count of any phrase. Our corpus was conse-
quently reduced from 750=150*5 to 434 examples
for the training corpus, and from 250=50*5 to 215
for the test corpus.
  <10 <100 <1000 <10000 <100000 <1000000 >10000000
1020
3040
5060
7080
90100 # of original phrases# of paraphrases
Range of number of counts
Figure 4: Number of phrases and paraphrases per web
count range
Results will be reported for three conditions:
? Possible: the gold standard for instances where
at least one of the judges indicated ?para-
phrases? records the pair as a paraphrase. In
this condition, the test set has 116 instances that
are paraphrases and 99 that are not.
? Sure: the gold standard for instances where not
all judges indicated ?paraphrases? records the
pair as not paraphrase. In this condition, the
test set has 76 instances that are paraphrases
and 139 that are not.
? Surer: only those instances where both judges
agree are recorded. This reduces our training
and test set to respectively 287 and 175 exam-
ples. Thus, results on this subcorpora will not
be directly comparable with the other results.
In this condition, the test set has 76 instances
that are paraphrases and 99 that are not.
5.2 Baseline techniques
Web-count based baselines We used two base-
lines based on simple Web counts. The first one,
WEBLM, considers a candidate sentence a para-
phrase of the original sentence whenever its Web
language model score is higher than that of the orig-
inal phrase. The second one, BOUNDLM, considers
a sentence as a paraphrase whenever the counts for
the bigrams crossing the left and right boundary of
the sub-sentential paraphrase is higher than 10.
Syntactic dependency baseline When rewriting a
subpart of a sentence, the fact that syntactic depen-
dencies between the rewritten phrase and its con-
text are the same than those of the original phrase
and the same context can provide some information
16
about the grammatical and semantic substituability
of the two phrases (Zhao et al, 2007; Max and Zock,
2008). We thus build syntactic dependencies for
both the original and rewritten sentence, using the
French version (Candito et al, 2010) of the Berkeley
probabilistic parser (Petrov and Klein, 2007), and
consider the subset of dependencies for the two sen-
tences that exist between a word inside the phrase
under focus and a word outside it (Deporig and
Deppara). Our CONTDEP baseline considers a sen-
tence as a paraphrase iff Deppara = Deporig.
5.3 Evaluation results
We used the models described in Section 4 to build
a SVM classifier using the LIBSVM package (Chang
and Lin, 2001). Accuracy results are reported on
Figure 5.
WEBLM BOUNDLM CONTDEP CLASSIFIER
POSSIBLE 62.79 54.88 48.53 57.67
SURE 68.37 36.27 51.90 70.69
SURER 56.79 51.41 42.69 62.85
Figure 5: Accuracy results for the three baselines and our
classifier on the test set for the three conditions. Note that
the SURER condition cannot be directly compared with
the other two as the number of training and test examples
are not the same.
The first notable observation is that our task is not
surprisingly a difficult one. The best performance
achieved is an accuracy of 70.69 with our system in
the SURE condition. There are, however, some im-
portant variations across conditions, with a result as
low as 57.67 for our system in the POSSIBLE condi-
tion (recall that in this condition candidates are con-
sidered paraphrases when only one of the two judges
considered it a paraphrase, i.e. when the two judges
disagreed).
Overall, the WEBLM baseline and our system ap-
pear as stronger than the two other baselines. The
two lower baselines, BOUNDLM and CONTDEP, at-
tempt to model local grammatical constraints, which
are not surprisingly not sufficient for paraphrase
identification. WEBLM is comparatively a much
more competitive baseline, but its accuracy in the
SURER condition is not very strong. As this latter
condition considers only consensual judgements for
the two judges, we can hypothesize that the interpre-
tation of its results is more reliable. In this condi-
WICOPACO GAMERS PIVOTES PIVOTZH
POSSIBLE 89.33 67.00 47.33 20.66
SURE 64.00 44.50 31.33 10.66
SURER 86.03 57.34 37.71 12.60
Figure 6: Paraphrase accuracy of our different paraphrase
acquisition methods for the three conditions.
tion, our system obtains the best performance, with
a +6.06 advantage over WEBLM. As found in other
works (e.g. (Bannard and Callison-Burch, 2005)),
using language models for paraphrase validation is
not sufficient as it cannot model meaning preserva-
tion, and our results show that this is also true even
when counts are estimated from the Web. Using a
ratio of normalized LM scores may have improved
the situation a bit.12
Lastly, we report in Figure 6 the paraphrase
accuracy of each individual acquisition technique
(i.e. source of paraphrases from the preexisting
repertoire). The original rewritting from WICO-
PACO obtains not surprisingly a very high para-
phrase accuracy, in particular in the POSSIBLE and
SURER conditions. Paraphrases obtained through
our Web-based game have an acceptable accuracy:
the numbers confirm that paraphrase pairs are highly
context-dependent, because the pairs which were
likely to be paraphrases in the context of the game
are not necessarily so in a different context. This,
of course, may be due to a number of reasons that
we will have to investigate. Lastly, there is a signif-
icant drop in accuracy for the automatic pivot para-
phrasers, but pivoting through Spanish obtained, not
suprisingly again, a much better performance than
pivoting through Chinese.
6 Discussion and future work
We have presented an approach to the task of
targeted paraphrasing in the context of text revi-
sion, a scenario which was supported by naturally-
occurring data from the rephrasing memory of
Wikipedia. Our framework takes a repertoire of ex-
isting sub-sentential paraphrases, coming from pos-
12A possible explanation for the relative good performance of
WEBLM may lie in the fact that our two automatic paraphrasers
using Google Translate as a pivot translation engine tend to pro-
duce strings that are very likely according to the language mod-
els used by the translation system, which we assume to be very
comparable to those that were used in our experiments.
17
sibly any source including manual acquisition, and
validates all candidate paraphrases using informa-
tion from the Web. Our experiments have shown
that the current version of our classifier outperforms
several baselines when considering paraphrases with
consensual judgements in the gold standard refer-
ence.
Although our initial experiments are positive, we
believe that they can be improved in a number of
ways. We intend to broaden our exploration of the
various characteristics at play. We will try more fea-
tures, including e.g. a model of syntactic depen-
dencies derived from the Web, and extend our work
to new languages. We will also attempt to analyze
more precisely our results to identify problematic
cases, some of which could turn to be almost im-
possible to model without resorting to world knowl-
edge, which was beyond our attempted modeling.
Finally, we will also be interested in considering the
applicability of this approach as a framework for the
evaluation of paraphrase acquisition techniques.
Acknowledgments
This work was partly supported by ANR project
Trace (ANR-09-CORD-023). The authors would
like to thank the anonymous reviewers for their help-
ful questions and comments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT,
Edmonton, Canada.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, Toulouse, France.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceedings
of the ACM symposium on User interface software and
technology.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2011.
Monolingual alignment by edit rate computation on
sentential paraphrase pairs. In Proceedings of ACL,
Short Papers session, Portland, USA.
Chris Brockett and William B. Dolan. 2005. Support
vector machines for paraphrase identification and cor-
pus construction. In Proceedings of The 3rd Inter-
national Workshop on Paraphrasing IWP, Jeju Island,
South Korea.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, Hawai, USA.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
R. Chandrasekar, Christine Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of COLING, Copenhagen, Denmark.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Timothy Chklovski. 2005. Collecting paraphrase cor-
pora from volunteer contributors. In Proceedings of
KCAP 2005, Banff, Canada.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, Manchester, UK.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Comput. Linguist.,
34(4):597?614.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of ECML, Warsaw, Poland.
Louise Dele?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in wikipedia?s revision history. SEPLN
journal, 46:51?58.
Cristina Espan?a Bonet, Marta Vila, M. Anto`nia Mart??,
and Horacio Rodr??guez. 2009. Coco, a web interface
for corpora compilation. SEPLN journal, 43.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of NAACL-
HLT, Los Angeles, USA.
18
Mirella Lapata and Frank Keller. 2005. Web-based Mod-
els for Natural Language Processing. ACM Transac-
tions on Speech and Language Processing, 2(1):1?31.
Weigang Li, Ting Liu, Yu Zhang, Sheng Li, and Wei
He. 2005. Automated generalization of phrasal para-
phrases from the web. In Proceedings of the IJCNLP
Workshop on Paraphrasing, Jeju Island, South Korea.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are multiple reference
translations necessary? investigating the value of
paraphrased reference translations in parameter opti-
mization. In Proceedings of AMTA, Waikiki, USA.
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia?s Revision History. In Proceedings of
LREC 2010, Valletta, Malta.
Aure?lien Max and Michael Zock. 2008. Looking up
phrase rephrasings via a pivot language. In Proceed-
ings of the COLING Workshop on Cognitive Aspects
of the Lexicon, Manchester, United Kingdom.
Aure?lien Max. 2004. From controlled document au-
thoring to interactive document normalization. In Pro-
ceedings of COLING, Geneva, Switzerland.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2).
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In Proceedings of ACL, Prague, Czech
Republic.
Andrew Mutton. 2006. Evaluation of sentence grammat-
icality using Parsers and a Support Vector Machine.
Ph.D. thesis, Macquarie University.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of ACL, Short Papers ses-
sion, Uppsala, Sweden.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP, Barcelona,
Spain.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin B. Bederson. 2010. Im-
proving translation via targeted paraphrasing. In Pro-
ceedings of EMNLP, Cambridge, MA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word lattices for multi-source translation. In Proceed-
ings of EACL, Athens, Greece.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Kuansan Wang, Chris Thrasher, Evelyne Viegas, Xiao-
long Li, and Bo-june (Paul) Hsu. 2010. An Overview
of Microsoft Web N-gram Corpus and Applications.
In Proceedings of the NAACL-HLT Demonstration
Session, Los Angeles, USA.
Shiqi Zhao, Ting Liu, Xincheng Yuan, Sheng Li, and
Yu Zhang. 2007. Automatic acquisition of context-
specific lexical paraphrases. In Proceedings of IJCAI
2007, Hyderabad, India.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint ACL-IJCNLP, Singapore.
Shiqi Zhao, Haifeng Wang, Ting Liu, , and Sheng Li.
2010. Leveraging multiple mt engines for paraphrase
generation. In Proceedings of COLING, Beijing,
China.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of COLING,
Beijing, China.
19
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
Workshop on Computational Linguistics for Literature, pages 36?44,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Aligning Bilingual Literary Works: a Pilot Study
Qian Yu and Aure?lien Max and Franc?ois Yvon
LIMSI/CNRS and Univ. Paris Sud
rue John von Neumann F-91 403 Orsay, France
{fistname.lastname}@limsi.fr
Abstract
Electronic versions of literary works abound on the In-
ternet and the rapid dissemination of electronic read-
ers will make electronic books more and more com-
mon. It is often the case that literary works exist in
more than one language, suggesting that, if properly
aligned, they could be turned into useful resources for
many practical applications, such as writing and lan-
guage learning aids, translation studies, or data-based
machine translation. To be of any use, these bilin-
gual works need to be aligned as precisely as possible,
a notoriously difficult task. In this paper, we revisit
the problem of sentence alignment for literary works
and explore the performance of a new, multi-pass,
approach based on a combination of systems. Ex-
periments conducted on excerpts of ten masterpieces
of the French and English literature show that our
approach significantly outperforms two open source
tools.
1 Introduction
The alignment of bitexts, i.e. of pairs of texts as-
sumed to be mutual translations, consists in find-
ing correspondences between logical units in the in-
put texts. The set of such correspondences is called
an alignment. Depending on the logical units that
are considered, various levels of granularity for the
alignment are obtained. It is usual to align para-
graphs, sentences, phrases or words (see (Wu, 2010;
Tiedemann, 2011) for recent reviews). Alignments
are used in many fields, ranging from Translation
Studies and Computer Assisted Language Learn-
ing (CALL) to Multilingual Natural Language Pro-
cessing (NLP) applications (Cross-Lingual Informa-
tion Retrieval, Writing Aids for Translators, Multi-
lingual Terminology Extraction and Machine Trans-
lation (MT)). For all these applications, sentence
alignments have to be computed.
Sentence alignment is generally thought to be
fairly easy and many efficient sentence alignment
programs are freely available1. Such programs rely
on two main assumptions: (i) the relative order of
sentences is the same on the two sides of the bi-
text, and (ii) sentence parallelism can be identified
using simple surface cues. Hypothesis (i) warrants
efficient sentence alignment algorithms based on
dynamic programming techniques. Regarding (ii),
various surface similarity measures have been pro-
posed: on the one hand, length-based measures
(Gale and Church, 1991; Brown et al, 1991) rely
on the fact that the translation of a short (resp. long)
sentence is short (resp. long). On the other hand,
lexical matching approaches (Kay and Ro?scheisen,
1993; Simard et al, 1993) identify sure anchor
points for the alignment using bilingual dictionar-
ies or surface similarities of word forms. Length-
based approaches are fast but error-prone, while lex-
ical matching approaches seem to deliver more re-
liable results. Most state-of-the-art approaches use
both types of information (Langlais, 1998; Simard
and Plamondon, 1998; Moore, 2002; Varga et al,
2005; Braune and Fraser, 2010).
In most applications, only high-confidence one-
to-one sentence alignments are considered useful
and kept for subsequent processing stages. Indeed,
when the objective is to build subsentential align-
1See, for instance, the Uplug toolbox which integrates sev-
eral sentence alignment tools in a unified framework:
http://sourceforge.net/projects/uplug/
36
ments (at the level of words, terms or phrases), other
types of mappings between sentences are deemed
to be either insufficiently reliable or inappropriate.
As it were, the one-to-one constraint is viewed as a
proxy to literalness/compositionality of the transla-
tion and warrants the search of finer-grained align-
ments. However, for certain types of bitexts2, such
as literary texts, translation often departs from a
straight sentence-by-sentence alignment and using
such a constraint can discard a significant propor-
tion of the bitext. For MT, this is just a regrettable
waste of potentially useful training material (Uszko-
reit et al, 2010), all the more so as parallel liter-
ary texts constitute a very large reservoir of par-
allel texts online. For other applications implying
to mine, visualize or read the actual translations in
their context (second language learning (Nerbonne,
2000; Kraif and Tutin, 2011), translators training,
automatic translation checking (Macklovitch, 1994),
etc.), the entire bitext has to be aligned. Further-
more, areas where the translation is only partial or
approximative need to be identified precisely.
The work reported in this study aims to explore
the quality of existing sentence alignment tech-
niques for literary work and to explore the usability
of a recently proposed multiple-pass approach, espe-
cially designed for recovering many-to-one pairings.
In a nutshell, this approach uses sure one-to-one
mappings detected in a first pass to train a discrim-
inative sentence alignment system, which is then
used to align the regions which remain problem-
atic. Our experiments on the BAF corpus (Simard,
1998) and on a small literary corpus consisting of ten
books show that this approach produces high quality
alignments and also identifies the most problematic
passages better than its competitors.
The rest of this paper is organized as follows:
we first report the results of a pilot study aimed at
aligning our corpus with existing alignment meth-
ods (Section 2). In Section 3, we briefly describe our
two-pass method, including some recent improve-
ments, and present experimental performance on the
BAF corpus. Attempts to apply this technique to our
larger literary corpus are reported and discussed in
2Actual literary bitexts are not so easily found over the Inter-
net, notably due to (i) issues related to variations in the source
text and (ii) issues related to the variations, over time, of the
very notion of what a translation should be like.
Section 4. We discuss further prospects and con-
clude in Section 5.
2 Book alignment with off-the-shelf tools
2.1 A small bilingual library
The corpus used in this study contains a random se-
lection of ten books written mostly in the 19th and
in the early 20th century: five are English classics
translated into French, and five are French classics
translated into English. These books and their trans-
lation are freely available3 from sources such as the
Gutenberg project4 or wikisource5, and are repre-
sentative of the kinds of collections that can be easily
collected from the Internet. These texts have been
preprocessed and tokenized using in-house tools,
yielding word and sentence counts in Table 1.
2.2 Baseline sentence alignments
2.2.1 Public domain tools
Baseline alignments are computed using two
open-source sentence alignment packages, the sen-
tence alignment tool of Moore (2002)6, and Hu-
nalign (Varga et al, 2005). These two tools were
chosen as representative of the current state-of-the-
art in sentence alignment. Moore?s approach im-
plements a two-pass, coarse-to-fine, strategy: a first
pass, based on sentence length cues, computes a
first alignment according to the principles of length-
based approaches (Brown et al, 1991; Gale and
Church, 1991). This alignment is used to train a sim-
plified version of IBM model 1 (Brown et al, 1993),
which provides the alignment system with lexical
association scores; these scores are then used to re-
fine the measure of association between sentences.
This approach is primarily aimed at delivering high
confidence, one-to-one, sentence alignments to be
used as training material for data-intensive MT. Sen-
tences that cannot be reliably aligned are discarded
from the resulting alignment.
3Getting access to more recent books (or their translation) is
problematic, due to copyright issues: literary works fall in the
public domain 70 years after the death of their author.
4http://www.gutenberg.org
5http://wikisource.org
6http://research.microsoft.com/en-us/downloads/
aafd5dcf-4dcc-49b2-8a22-f7055113e656/
37
French side English side
# sents # words # sents # words
English books and their French translation
Emma, J. Austen EM 5,764 134,950 7,215 200,223
Jane Eyre, C. Bronte? JE 9,773 240,032 9,441 237,487
The last of the Mohicans, F. Cooper LM 6,088 189,724 5,629 177,303
Lord Jim, J. Conrad LJ 7962 175,876 7,685 162,498
Vanity fair, W. Thackeray VF 14,534 395,702 12,769 372,027
French books and their English translation
Les confessions, J.J. Rousseau CO 9,572 324,597 8,308 318,658
5 semaines en ballon, J. Verne 5S 7,250 109,268 7,894 121,231
La faute de l?Abbe? Mouret, E. Zola AM 8,604 156,514 7,481 156,692
Les travailleurs de la mer, V. Hugo TM 10,331 170,015 9,613 178,427
Du co?te? de chez Swann, M. Proust SW 4,853 208,020 4,738 232,514
Total 84,731 2,104,698 80,773 2,157,060
Table 1: A small bilingual library
Hunalign7, with default settings, also implements
a two-pass strategy which resembles the approach of
Moore. Their main difference is that Hunalign also
produces many-to-one and one-to-many alignment
links, which are needed to ensure that all the input
sentences appear in the final alignment.
Both systems also deliver confidence measures
for the automatic alignment: a value between 0 and
1 for Moore?s tool, which can be interpreted as a
posterior probability; the values delivered by Hu-
nalign are less easily understood, and range from?1
to some small positive real values (greater than 1).
2.2.2 Evaluation metrics
Sentence alignment tools are usually evaluated
using standard recall [R] and precision [P] mea-
sures, combined in the F-measure [F], with respect
to some manually defined gold alignment (Ve?ronis
and Langlais, 2000). These measures can be com-
puted at various levels of granularity: the level of
alignment links, of sentences, of words, and of char-
acters. As gold references only specify alignment
links, the other references are automatically derived
in the most inclusive way. For instance, if the refer-
ence alignment links state that the pair of source sen-
tences f1, f2 is aligned with target e, the reference
sentence alignment will contain both (f1, e) and
7ftp://ftp.mokk.bme.hu/Hunglish/src/hunalign; we have
used the version that ships with Uplug.
(f2, e); likewise, the reference word alignment will
contain all the possible word alignments between
tokens in the source and the target side. For such
metrics, missing the alignment of a large ?block?
of sentences gets a higher penalty than missing a
small one; likewise, misaligning short sentences is
less penalized than misaligning longer ones. As a
side effect, all metrics, but the more severe one, ig-
nore null alignments. Our results are therefore based
on the link-level and sentence-level F-measure, to
reflect the importance of correctly predicting un-
aligned sentences in our applicative scenario.
2.2.3 Results
Previous comparisons of these alignment tools
on standard benchmarks have shown that both typ-
ically yield near state-of-the-art performance. For
instance, experiments conducted using the literary
subpart of the BAF corpus (Simard, 1998), con-
sisting of a hand-checked alignment of the French
novel De la Terre a` la Lune (From the Earth to
the Moon), by Jules Verne, with a slightly abridged
translation available from the Gutenberg project8,
have yielded the results in Table 2 (Moore?s system
was used with its default parameters, Hunalign with
the --realign option).
All in all, for this specific corpus, Moore?s strat-
egy delivers slightly better sentence alignments than
8http://www.gutenberg.org/ebooks/83
38
P R F % 1-1 links
Alignment based metrics
Hunalign 0.51 0.60 0.55 0.77
Moore 0.85 0.65 0.74 1.00
Sentence based metrics
Hunalign 0.76 0.70 0.73 -
Moore 0.98 0.62 0.76 -
Table 2: Baseline alignment experiments
Figure 1: Percentage of one-to-one links and pseudo-
paragraph size for various baselines
Hunalign does; in particular, it is able to identify 1-
to-1 links with a very high precision.
2.3 Aligning a small library
In a first series of experiments, we simply run the
two alignment tools on our small collection to see
howmuch of it can be aligned with a reasonable con-
fidence. The main results are reproduced in Figure 1,
where we display both the number of 1-to-1 links
extracted by the baselines (as dots on the Figure), as
well as the average size of pseudo-paragraphs (see
definition below) in French and English. As ex-
pected, less 1-to-1 links almost always imply larger
blocks.
As expected, these texts turn out to be rather
difficult to align: in the best case (Swann?s way
(SW)), only about 80% of the total sentences are
aligned by Moore?s system; in the more problem-
atic cases (Emma (EM) and Vanity Fair (VF)), more
than 50% of the book content is actually thrown
away when one only looks at Moore?s alignments.
Hunalign?s results look more positive, as a signifi-
cantly larger number of one-to-one correspondences
is found. Given that this system is overall less reli-
able than Moore?s approach, it might be safe to filter
these alignments and keep only the surer ones (here,
keeping only links having a score greater than 0.5).
The resulting number of sentences falls way below
what is obtained by Moore?s approach.
To conclude, both systems seem to have more dif-
ficulties with the literary material considered here
than with other types of texts. In particular, the
proportion of one-to-one links appears to be signif-
icantly smaller than what is typically reported for
other genres; note, however, that even in the worst
case, one-to-one links still account for about 50% of
the text. Another finding is that the alignment scores
which are output are not very useful: for Moore, fil-
tering low scoring links has very little effect; for Hu-
nalign, there is a sharp transition (around a threshold
of 0.5): below this value, filtering has little effect;
above this value, filtering is too drastic, as shown on
Figure 1.
3 Learning sentence alignments
In this section, we outline the main principles of
the approach developed in this study to improve the
sentence alignments produced by our baseline tools,
with the aim to salvage as many sentences as possi-
ble, which implies to come up with a way for better
detecting many-to-one and one-to-many correspon-
dences. Our starting point is the set of alignments
delivered by Moore?s tool. As discussed above,
these alignments have a very high precision, at the
expense of an unsatisfactory recall. Our sentence
alignment method considers these sentence pairs as
being parallel and uses them to train a binary classi-
fier for detecting parallel sentences. Using the pre-
dictions of this tool, it then attempts to align the re-
maining portions of the bitext (see Figure 2).
In Figure 2, Moore?s links are displayed with
solid lines; these lines delineate parallel pseudo-
paragraphs in the bitexts (appearing in boxed areas),
which we will try to further decompose. Note that
two configurations need to be distinguished: (i) one
side of a paragraph is empty: no further analysis
is performed and a 0-to-many alignment is output;
(ii) both sides of a paragraph are non-empty and de-
fine a i-to-j alignment that will be processed by the
block alignment algorithm described below.
39
Figure 2: Filling alignment gaps
3.1 Detecting parallelism
Assuming the availability of a set of example paral-
lel sentences, the first step of our approach consists
in training a function for scoring candidate align-
ments. Following (Munteanu and Marcu, 2005), we
train a Maximum Entropy classifier9 (Rathnaparkhi,
1998); in principle, many other binary classifiers
would be possible here. Our motivation for using
a maxent approach was to obtain, for each possible
pair of sentences (f ,e), a link posterior probability
P (link|f , e).
We take the sentence alignments of the first step
as positive examples. Negative examples are artifi-
cially generated as follows: for all pairs of positive
instances (e, f) and (e?, f ?) such that e? immediately
follows e, we select the pair (e, f ?) as a negative ex-
ample. This strategy produced a balanced corpus
containing as many negative pairs as positive ones.
However, this approach may give too much weight
on the length ratio feature and it remains to be seen
whether alternative approaches are more suitable.
Formally, the problem is thus to estimate a con-
ditional model for deciding whether two sentences
e and f should be aligned. Denoting Y the corre-
sponding binary variable, this model has the follow-
9Using the implementation available from http://homepages.
inf.ed.ac.uk/lzhang10/maxent toolkit.html.
ing form:
P (Y = 1|e, f) =
1
1 + exp[?
?K
k=1 ?kFk(e, f)]
,
where {Fk(e, f), k = 1 . . .K} denotes a set of fea-
ture functions testing arbitrary properties of e and f ,
and {?k, k = 1 . . .K} is the corresponding set of
parameter values.
Given a set of training sentence pairs, the opti-
mal values of the parameters are set by optimizing
numerically the conditional likelihood; optimization
is performed here using L-BFGS (Liu and Nocedal,
1989); a Gaussian prior over the parameters is used
to ensure numerical stability of the optimization.
In this study, we used the following set of feature
functions:
? lexical features: for each pair of words10 (e, f)
occurring in Ve ? Vf , there is a corresponding
feature Fe,f which fires whenever e ? e and
f ? f .
? length features: denoting le (resp. lf ) the
length of the source (resp. target) sentence,
measured in number of characters, we in-
clude features related to length ratio, defined
as Fr(e, f) =
|le?lf |
max(le,lf )
. Rather than taking the
numerical value, we use a simple discretization
scheme based on 6 bins.
? cognate features: we loosely define cog-
nates11 as words sharing a common prefix of
length at least 3. This gives rise to 4 features,
which are respectively activated when the num-
ber of cognates in the parallel sentence is 0, 1,
2, or greater than 2.
? copy features: an extreme case of similarity
is when a word is copied verbatim from the
source to the target. This happens with proper
nouns, dates, etc. We again derive 4 features,
depending on whether the number of identical
words in f and e is 0, 1, 2 or greater than 2.
10A word is an alphabetic string of characters, excluding
punction marks.
11Cognates are words that share a similar spelling in two or
more different languages, as a result of their similar meaning
and/or common etymological origin, e.g. (English-Spanish):
history - historia, harmonious - armonioso.
40
3.2 Filling alignment gaps
The third step uses the posterior alignment proba-
bilities computed in the second step to fill the gaps
in the first pass alignment. The algorithm can be
glossed as follows. Assume a bitext block compris-
ing the sentences from index i to j in the source
side of the bitext, and from k to l in the target side
such that sentences ei?1 (resp. ej+1) and fk?1 (resp.
el+1) are aligned12.
The first case is when j < i or k > l, in which
case we create a null alignment for fk:l or for ei:j . In
all other situations, we compute:
?i?, j?, k?, l?, i ? i? ? j? ? j, k ? k? ? l? ? l,
ai?,j?,k?,l? = P (Y = 1|ei?:j? , fk?:l?) ? ?S(i
?, j?, k?, l?)
where ei?:j? is obtained by concatenation of all the
sentences in the range [i?:j?], and S(i, j, k, l) = (j ?
i+1)(l?k+1)?1 is proportional to the block size.
The factor ?S(i?, j?, k?, l?) aims at penalizing large
blocks, which, for the sentence-based metrics, yield
much more errors than the small ones. This strategy
implies to compute O(|j ? i + 1|2 ? |k ? l + 1|2)
probabilities, which, given the typical size of these
blocks (see above), can be performed very quickly.
These values are then iteratively visited by de-
creasing order in a greedy fashion. The top-scoring
block i? : j?, k? : l? is retained in the final alignment;
all overlapping blocks are subsequently deleted from
the list and the next best entry is then considered.
This process continues until all remaining blocks
imply null alignments, in which case these n ? 0 or
0 ? n alignments are also included in our solution.
This process is illustrated in Figure 3: assuming
that the best matching link is f2-e2, we delete all
the links that include f2 or e2, as well as links that
would imply a reordering of sentences, meaning that
we also delete links such as f1-e3.
3.3 Experiments
In this section, we report the results of experiments
run using again Jules Verne?s book from the BAF
corpus. Figures are reported in Table 3 where we
contrast our approach with two simple baselines:
(i) keep only Moore?s links; (ii) complete Moore?s
links with one single many-to-many alignment for
12We enclose the source and target texts between begin and
end markers to enforce alignment of the first and last sentences.
Figure 3: Greedy alignment search
P R F
(maxent) (all) (all) (all)
link based
Moore only - 0.85 0.65 0.74
Moore+all links - 0.78 0.75 0.76
Maxent, ? = 0 0.44 0.74 0.81 0.77
Maxent, ? = 0.06 0.42 0.72 0.82 0.77
sentence based
Moore only - 0.98 0.62 0.76
Moore+all links - 0.61 0.88 0.72
Maxent, ? = 0 0.80 0.93 0.80 0.86
Maxent, ? = 0.06 0.91 0.97 0.79 0.87
Table 3: Performance of maxent-based alignments
each block. For the maxent-based approach, we also
report the precision on just those links that are not
predicted by Moore. A more complete set of experi-
ments conducted with other portions of the BAF are
reported elsewhere (Yu et al, 2012) and have shown
to deliver state-of-the-art results.
As expected, complementing the very accurate
prediction of Moore?s systems with our links sig-
nificantly boosts the sentence-based alignment per-
formance: recall rises from 0.62 to 0.80 for ? = 0,
which has a clear effect on the corresponding F-
measure (from 0.76 to 0.86). The performance dif-
ferences with the default strategy of keeping those
blocks unsegmented are also very clear. Sentence-
wise, maxent-based alignments are also quite pre-
cise, especially when the value of ? is chosen with
care (P=0.91 for ?=0.06); however, this optimiza-
tion has a very small overall effect, given that only a
limited number of alignment links are actually com-
puted by the maxent classifier.
41
4 Sentence alignment in the real world
In this section, we analyze the performance obtained
with our combined system, using excerpts of our
small corpus as test set. For this experiment, the
first two to three hundreds sentences in each book,
corresponding to approximately two chapters, were
manually aligned (by one annotator), using the same
guidelines that were used for annotating the BAF
corpus. Except for two books (EM and VF), produc-
ing these manual alignments was found to be quite
straightforward. Results are in Table 4.
A first comment is that both baselines are signifi-
cantly outperformed by our algorithm for almost all
conditions and books. For several books (LM, AM,
SW), the obtained sentence alignments are almost
as precise as those predicted by Moore and have a
much higher recall, resulting in very good overall
alignments. The situation is, of course, much less
satisfactory for other books (EM, VF, 5S). All in all,
our method salvages many useful sentence pairs that
would otherwise be left unaligned.
Moore?s method remains remarkably accurate
throughout the whole collection, even for the most
difficult books. It also outputs a significant propor-
tion of wrong links, which, for lack of reliable confi-
dence estimators, are difficult to spot and contribute
to introduce noise into the maxent training set.
The variation of performance can mostly be at-
tributed to idiosyncrasies in the translation. For in-
stance, Emma (EM) seems very difficult to align,
which can be attributed to the use of an old transla-
tion dating back to 1910 (by P. de Puliga), and which
often looks more like an adaptation than a transla-
tion. Some passages even question the possibility of
producing any sensible (human) alignment between
source and target13:
(en) Her sister, though comparatively but little removed by
matrimony, being settled in London, only sixteen miles off,
was much beyond her daily reach; and many a long October
and November evening must be struggled through at Hart-
field, before Christmas brought the next visit from Isabella
and her husband, and their little children, to fill the house,
and give her pleasant society again.
(fr) La s?ur d?Emma habitait Londres depuis son mariage,
c?est-a`-dire, en re?alite?, a` peu de distance; elle se trouvait
13In this excerpt, in addition to several approximations, the
end of the last sentence (and their children...) is not translated
in French.
ne?anmoins hors de sa porte?e journalie`re, et bien des longues
soire?es d?automne devraient e?tre passe?es solitairement a`
Hartfield avant que Noe?l n?amena?t la visite d?Isabelle et de
son mari.
Les confessions (CO) is much most faithful to the
content, yet, the translator has significantly departed
from Rousseau?s style14, mostly made up of short
sentences, and it is often the case that several French
sentences align with one single English sentence,
which is detrimental to Moore, and by ricochet, to
the quality of maxent predictions. A typical excerpt:
(fr) Pendant deux ans entiers je ne fus ni te?moin ni victime
d?un sentiment violent. Tout nourrissait dans mon coeur les
dispositions qu?il rec?ut de la nature.
(en) Everything contributed to strengthen those propensities
which nature had implanted in my breast, and during the
two years I was neither the victim nor witness of any violent
emotions.
The same goes for Thackeray (VF), with a lot of re-
structurations of the sentences as demonstrated by
the uneven number of sentences on both sides of the
bitext. Lord Jim (LJ) poses another type of diffi-
culty: approximately 100 sentences are missing on
the French side, the rest of the text being fairly paral-
lel (more than 82% of the reference links are actually
1-to-1). Du co?te? de chez Swann (SW) represents the
other extreme of the spectrum, where the translation
sticks as much as possible to the very peculiar style
of Proust: nearly 90% of the reference alignments
are 1-to-1, which explains the very good F-measure
for this book.
It is difficult to analyze more precisely our er-
rors; however, a fairly typical pattern is the infer-
ence of a 1-to-1 link rather than a 2-to-1 link made
up of a short and a long sentence. An example from
Hugo (TM), where our approach prefers to leave
the second English sentence unaligned, even though
the corresponding segment (un enfant...) is the in
French sentence:
(fr) Dans tout le tronc?on de route qui se?pare la premie`re tour
de la seconde tour, il n?y avait que trois passants, un enfant,
un homme et une femme.
(en) Throughout that portion of the highway which separates
the first from the second tower, only three foot-passengers
could be seen. These were a child, a man, and a woman.
A possible walk around for this problem would be
to also add a penalty for null alignments.
14Compare the number of sentences in Table 1.
42
Moore Hunalign Moore+maxent
links P R F links F S 6= 0 S = 0 P R F
fr en links link based
EM 160 217 164 84 0.76 0.39 0.52 173 0.43 72 10 0.52 0.53 0.52
JE 229 205 174 104 0.86 0.51 0.64 198 0.40 95 5 0.64 0.75 0.69
LM 232 205 197 153 0.97 0.76 0.85 203 0.63 64 2 0.79 0.87 0.83
LJ 580 682 515 403 0.94 0.73 0.82 616 0.60 155 15 0.82 0.81 0.76
VF 321 248 219 129 0.92 0.54 0.68 251 0.39 133 3 0.58 0.70 0.63
CO 326 236 213 104 0.86 0.42 0.56 256 0.28 135 3 0.62 0.70 0.66
5S 182 201 153 107 0.76 0.53 0.62 165 0.52 72 10 0.60 0.74 0.66
AM 258 226 222 179 1.00 0.81 0.90 222 0.71 55 0 0.88 0.93 0.90
TM 404 388 358 284 0.89 0.71 0.79 374 0.69 86 16 0.79 0.85 0.82
SW 492 495 463 431 0.94 0.87 0.90 474 0.80 59 9 0.85 0.92 0.88
fr en links sentence based
EM 160 217 206 84 0.85 0.34 0.49 199 0.60 124 0 0.62 0.63 0.62
JE 229 205 270 104 0.92 0.36 0.52 235 0.60 125 0 0.90 0.76 0.82
LM 232 205 238 153 0.99 0.64 0.78 234 0.79 62 0 0.97 0.88 0.92
LJ 580 682 645 403 0.96 0.60 0.74 625 0.78 212 0 0.85 0.81 0.83
VF 321 248 363 129 0.98 0.35 0.52 318 0.62 163 0 0.88 0.71 0.79
CO 326 236 380 104 0.94 0.26 0.41 306 0.48 226 0 0.88 0.76 0.82
5S 182 201 260 107 0.98 0.40 0.57 224 0.70 81 0 0.93 0.67 0.78
AM 258 226 264 179 1.00 0.68 0.81 262 0.84 72 0 0.98 0.94 0.96
TM 404 388 445 284 0.96 0.61 0.75 418 0.82 134 0 0.93 0.87 0.90
SW 492 495 532 431 0.99 0.80 0.88 512 0.88 55 0 0.99 0.90 0.94
Table 4: Evaluating alignment systems on a sample of ?real-world? books
For each book, we report the number of French and English test sentences, the number of reference links and standard performance
measures. For the maxent approach, we also report separately the number of empty (S = 0) and non-empty (S 6= 0) paragraphs.
5 Conclusions and future work
In this paper, we have presented a novel two-pass ap-
proach aimed at improving existing sentence align-
ment methods in contexts where (i) all sentences
need to be aligned and/or (ii) sentence alignment
confidence need to be computed. By running ex-
periments with several variants of this approach, we
have been able to show that it was able to signif-
icantly improve the bare results obtained with the
sole Moore alignment system. Our study shows
that the problem of sentence alignment for literary
texts is far from being solved and additional work
is needed to obtain alignments that could be used in
real applications, such as bilingual reading aids.
The maxent-based approach proposed here is thus
only a first step, and we intend to explore various
extensions: an obvious way to go is to use more
resources (larger training corpora, bilingual dictio-
naries, etc.) and add more features, such as part-of-
speech, lemmas, or alignment features as was done
in (Munteanu and Marcu, 2005). We also plan to
provide a much tighter integration with Moore?s al-
gorithm, which already computes such alignments,
so as to avoid having to recompute them. Finally,
the greedy approach to link selection can easily be
replaced with an exact search based on dynamic pro-
gramming techniques, including dependencies with
the left and right alignment links.
Regarding applications, a next step will be to pro-
duce and evaluate sentence alignments for a much
larger and more diverse set of books, comprising
more than 100 novels, containing books in 7 lan-
guages (French, English, Spanish, Italian, German,
Russian, Portuguese) from various origins. Most
were collected on the Internet from Gutenberg, wik-
isource and GoogleBooks15, and some were col-
lected in the course of the Carmel project (Kraif et
al., 2007). A number of these books are translated
in more than one language, and some are raw OCR
outputs and have not been cleaned from errors.
Acknowledgments
This work has been partly funded through the
?Google Digital Humanities Award? program.
15http://books.google.com
43
References
Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for symmet-
rical and asymmetrical parallel corpora. In Coling
2010: Posters, pages 81?89, Beijing, China. Coling
2010 Organizing Committee.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the 29th annual meeting on Association
for Computational Linguistics, 1991, Berkeley, Cali-
fornia, pages 169?176.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th annual meeting of the Associ-
ation for Computational Linguistics, pages 177?184,
Berkeley, California.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignement. Computational Linguistics,
19(1):121?142.
Olivier Kraif and Agne`s Tutin. 2011. Using a bilingual
annotated corpus as a writing aid: An application for
academic writing for efl users. In In Natalie Ku?bler
(Ed.), editor, Corpora, Language, Teaching, and Re-
sources: From Theory to Practice. Selected papers
from TaLC7, the 7th Conference of Teaching and Lan-
guage Corpora. Peter Lang, Bruxelles.
Olivier Kraif, Marc El-Be`ze, Re?gis Meyer, and Claude
Richard. 2007. Le corpus Carmel: un corpus multi-
lingue de re?cits de voyages. In Proceedings of Teach-
ing and Language Corpora : TaLC?200, Paris.
Philippe Langlais. 1998. A System to Align Com-
plex Bilingual Corpora. Technical report, CTT, KTH,
Stockholm, Sweden, Sept.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Elliot Macklovitch. 1994. Using bi-textual alignment for
translation validation: the TransCheck system. In Pro-
ceedings of the First Conference of the Association for
Machine Translation in the Americas (AMTA), pages
157?168, Columbia.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Stephen D.
Richardson, editor, Proceedings of the annual meet-
ing of tha Association for Machine Translation in
the Americas (AMTA?02), Lecture Notes in Computer
Science 2499, pages 135?144, Tiburon, CA, USA.
Springer Verlag.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
John Nerbonne, 2000. Parallel Texts in Computer-
Assisted Language Learning, chapter 15, pages 354?
369. Text Speech and Language Technology Series.
Kluwer Academic Publishers.
Ardwait Rathnaparkhi. 1998. Maximum Entropy Mod-
els for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Michel Simard and Pierre Plamondon. 1998. Bilingual
sentence alignment: Balancing robustness and accu-
racy. Machine Translation, 13(1):59?80.
Michel Simard, George F. Foster, and Pierre Isabelle.
1993. Using cognates to align sentences in bilingual
corpora. In Ann Gawman, Evelyn Kidd, and Per-
A?ke Larson, editors, Proceedings of the 1993 Confer-
ence of the Centre for Advanced Studies on Collabora-
tive Research, October 24-28, 1993, Toronto, Ontario,
Canada, 2 Volume, pages 1071?1082.
Michel Simard. 1998. The BAF: a corpus of English-
French bitext. In First International Conference on
Language Resources and Evaluation, volume 1, pages
489?494, Granada, Spain.
Jo?rg Tiedemann. 2011. Bitext Alignment. Number 14
in Synthesis Lectures on Human Language Technolo-
gies, Graeme Hirst (ed). Morgan & Claypool Publish-
ers.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 1101?1109, Beijing,
China.
Da?niel Varga, La?szlo? Ne?meth, Pe?ter Hala?csy, Andra?s Ko-
rnai, Viktor Tro?n, and Viktor Nagy. 2005. Parallel cor-
pora for medium density languages. In Proceedings of
RANLP 2005, pages 590?596, Borovets, Bulgaria.
Jean Ve?ronis and Philippe Langlais. 2000. Evaluation
of Parallel Text Alignment Systems. In Jean Ve?ronis,
editor, Parallel Text Processing, Text Speech and Lan-
guage Technology Series, chapter X, pages 369?388.
Kluwer Academic Publishers.
Dekai Wu. 2010. Alignment. In Nitin Indurkhya
and Fred Damerau, editors, CRC Handbook of Natu-
ral Language Processing, number 16, pages 367?408.
CRC Press.
Qian Yu, Aure?lien Max, and Franc?ois Yvon. 2012.
Revisiting sentence alignment algorithms for align-
ment visualization and evaluation. In Proceedings of
the Language Resource and Evaluation Conference
(LREC), Istambul, Turkey.
44
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIMSI @ WMT?12
Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,
Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,
Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submissions to
the shared translation task. We report results
for French-English and German-English in
both directions. Our submissions use n-code,
an open source system based on bilingual
n-grams. In this approach, both the transla-
tion and target language models are estimated
as conventional smoothed n-gram models; an
approach we extend here by estimating the
translation probabilities in a continuous space
using neural networks. Experimental results
show a significant and consistent BLEU im-
provement of approximately 1 point for all
conditions. We also report preliminary experi-
ments using an ?on-the-fly? translation model.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Seventh Workshop
on Statistical Machine Translation. LIMSI partic-
ipated in the French-English and German-English
tasks in both directions. For this evaluation, we
used n-code, an open source in-house Statistical
Machine Translation (SMT) system based on bilin-
gual n-grams1. The main novelty of this year?s
participation is the use, in a large scale system, of
the continuous space translation models described
in (Hai-Son et al, 2012). These models estimate the
n-gram probabilities of bilingual translation units
using neural networks. We also investigate an alter-
native approach where the translation probabilities
of a phrase based system are estimated ?on-the-fly?
1http://ncode.limsi.fr/
by sampling relevant examples, instead of consider-
ing the entire training set. Finally we also describe
the use in a rescoring step of several additional fea-
tures based on IBM1 models and word sense disam-
biguation information.
The rest of this paper is organized as follows. Sec-
tion 2 provides an overview of the baseline systems
built with n-code, including the standard transla-
tion model (TM). The continuous space translation
models are then described in Section 3. As in our
previous participations, several steps of data pre-
processing, cleaning and filtering are applied, and
their improvement took a non-negligible part of our
work. These steps are summarized in Section 5.
The last two sections report experimental results ob-
tained with the ?on-the-fly? system in Section 6 and
with n-code in Section 7.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o et al,
2006; Crego and Marin?o, 2006). In this framework,
translation is divided in two steps: a source reorder-
ing step and a (monotonic) translation step. Source
reordering is based on a set of learned rewrite rules
that non-deterministically reorder the input words.
Applying these rules result in a finite-state graph of
possible source reorderings, which is then searched
for the best possible candidate translation.
2.1 Features
Given a source sentence s of I words, the best trans-
lation hypothesis t? is defined as the sequence of J
words that maximizes a linear combination of fea-
330
ture functions:
t? = argmax
t,a
{
M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature func-
tion hm and a denotes an alignment between source
and target phrases. Among the feature functions, the
peculiar form of the translation model constitute one
of the main difference between the n-gram approach
and standard phrase-based systems. This will be fur-
ther detailled in section 2.2 and 3.
In addition to the translation model, fourteen
feature functions are combined: a target-language
model (Section 5.3); four lexicon models; six lexi-
calized reordering models (Tillmann, 2004; Crego
et al, 2011) aiming at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two lexi-
cal weights are estimated from the automatic word
alignments. The weights vector ? is learned us-
ing a discriminative training framework (Och, 2003)
(Minimum Error Rate Training (MERT)) using the
newstest2009 as development set and BLEU (Pap-
ineni et al, 2002) as the optimization criteria.
2.2 Standard n-gram translation models
n-gram translation models rely on a specific de-
composition of the joint probability of a sentence
pair P (s, t): a sentence pair is assumed to be
decomposed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL2. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing obtained by ?unfolding? initial word alignments.
In this framework, the basic translation units are
tuples, which are the analogous of phrase pairs and
represent a matching u = (s, t) between a source
s and a target t phrase (see Figure 1). Using the
n-gram assumption, the joint probability of a seg-
2From now on, (s, t) thus denotes an aligned sentence pair,
and we omit the alignment variable a in further developments.
mented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
During the training phase (Marin?o et al, 2006), tu-
ples are extracted from a word-aligned corpus (us-
ing MGIZA++3 with default settings) in such a
way that a unique segmentation of the bilingual
corpus is achieved. A baseline n-gram translation
model is then estimated over a training corpus com-
posed of tuple sequences using modified Knesser-
Ney Smoothing (Chen and Goodman, 1998).
2.3 Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, only those reorder-
ing hypotheses are translated and they are intro-
duced using a set of reordering rules automatically
learned from the word alignments.
In the example in Figure 1, the rule [prix no-
bel de la paix ; nobel de la paix prix] repro-
duces the invertion of the French words that is ob-
served when translating from French into English.
Typically, part-of-speech (POS) information is used
to increase the generalization power of these rules.
Hence, rewrite rules are built using POS rather than
surface word forms (Crego and Marin?o, 2006).
3 SOUL translation models
A first issue with the model described by equa-
tion (2) is that the elementary units are bilingual
pairs. As a consequence, the underlying vocabulary,
hence the number of parameters, can be quite large,
even for small translation tasks. Due to data sparsity
issues, such model are bound to face severe estima-
tion problems. Another problem with (2) is that the
source and target sides play symmetric roles: yet,
in decoding, the source side is known and only the
target side must be predicted.
3.1 A word factored translation model
To overcome these issues, the n-gram probability in
equation (2) can be factored by decomposing tuples
3http://www.kyloo.net/software/doku.php
331
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
in two parts (source and target), and by taking words
as the basic units of the n-gram TM. This may seem
to be a regression with respect to current state-of-
the-art SMT systems, as the shift from the word-
based model of (Brown et al, 1993) to the phrase-
based models of (Zens et al, 2002) is usually con-
sidered as a major breakthrough of the recent years.
Indeed, one important motivation for considering
phrases was to capture local context in translation
and reordering. It should however be emphasized
that the decomposition of phrases into words is only
re-introduced here as a way to mitigate the param-
eter estimation problems. Translation units are still
pairs of phrases, derived from a bilingual segmen-
tation in tuples synchronizing the source and target
n-gram streams. In fact, the estimation policy de-
scribed in section 4 will actually allow us to take into
account larger contexts than is possible with conven-
tional n-gram models.
Let ski denote the k
th word of source tuple si.
Considering the example of Figure 1, s111 denotes
the source word nobel, s411 the source word paix.
We finally denote hn?1(tki ) the sequence made of
the n? 1 words preceding tki in the target sentence:
in Figure 1, h3(t211) thus refers to the three words
context receive the nobel associated with t211 peace.
Using these notations, equation (2) is rewritten as:
P (a, s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (3)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words from ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is:
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
A benefit of this new formulation is that the vo-
cabularies involved only contain words, and are thus
much smaller that tuple vocabularies. These models
are thus less at risk to be plagued by data sparsity is-
sues. Moreover, the decomposition (3) now involves
two models: the first term represents a TM, the sec-
ond term is best viewed as a reordering model. In
this formulation, the TM only predicts the target
phrase, given its source and target contexts.
P (s, t) =
L?
i=1
[ |si|?
k=1
P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
?
|ti|?
k=1
P
(
tki |h
n?1(s1i ), h
n?1(tki )
)] (4)
4 The principles of SOUL
In section 3.1, we defined a n-gram translation
model based on equations (3) and (4). A major diffi-
culty with such models is to reliably estimate their
parameters, the numbers of which grow exponen-
tially with the order of the model. This problem
is aggravated in natural language processing due to
332
the well-known data sparsity issue. In this work,
we take advantage of the recent proposal of (Le et
al., 2011). Using a specific neural network architec-
ture (the Structured OUtput Layer or SOUL model),
it becomes possible to handle large vocabulary lan-
guage modeling tasks. This approach was experi-
mented last year for target language models only and
is now extended to translation models. More details
about the SOUL architecture can be found in (Le et
al., 2011), while its extension to translation models
is more precisely described in (Hai-Son et al, 2012).
The integration of SOUL models for large SMT
tasks is carried out using a two-pass approach: the
first pass uses conventional back-off n-gram trans-
lation and language models to produce a k-best list
(the k most likely translations); in the second pass,
the probability of a m-gram SOUL model is com-
puted for each hypothesis and the k-best list is ac-
cordingly reordered. In all the following experi-
ments, we used a context size for SOUL of m = 10,
and used k = 300. The two decompositions of equa-
tions (3) and (4) are used by introducing 4 scores
during the rescoring step.
5 Corpora and data pre-processing
Concerning data pre-processing, we started from our
submissions from last year (Allauzen et al, 2011)
and mainly upgraded the corpora and the associated
language-dependent pre-processing routines.
5.1 Pre-processing
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte et
al., 2008). Previous experiments have demonstrated
that better normalization tools provide better BLEU
scores: all systems are thus built in ?true-case?.
Compared to last year, the pre-processing of utf-8
characters was significantly improved.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to unknown
forms). When translating from German into En-
glish, the German side is thus normalized using a
specific pre-processing scheme (described in (Al-
lauzen et al, 2010; Durgar El-Kahlout and Yvon,
2010)), which aims at reducing the lexical redun-
dancy by (i) normalizing the orthography, (ii) neu-
tralizing most inflections and (iii) splitting complex
compounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition, for
German, fine-grained POS labels were also needed
for pre-processing and were obtained using the RF-
Tagger (Schmid and Laws, 2008).
5.2 Bilingual corpora
As for last year?s evaluation, we used all the avail-
able parallel data for the German-English language
pair, while only a subpart of the French-English par-
allel data was selected. Word alignment models
were trained using all the data, whereas the transla-
tion models were estimated on a subpart of the par-
allel data: the UN corpus was discarded for this step
and about half of the French-English Giga corpus
was filtered based on a perplexity criterion as in (Al-
lauzen et al, 2011)).
For French-English, we mainly upgraded the
training material from last year by extracting the
new parts from the common data. The word
alignment models trained last year were then up-
dated by running a forced alignment 4 of the new
data. These new word-aligned data was added to
last year?s parallel corpus and constitute the train-
ing material for the translation models and feature
functions described in Section 2. Given the large
amount of available data, three different bilingual
n-gram models are estimated, one for each source of
data: News-Commentary, Europarl, and the French-
English Giga corpus. These models are then added
to the weighted mixture defined by equation (1). For
German-English, we simply used all the available
parallel data to train one single translation models.
5.3 Monolingual corpora and language models
For the monolingual training data, we also used the
same setup as last year. For German, all the train-
ing data allowed in the constrained task were di-
vided into several sets based on dates or genres:
News-Commentary, the news crawled from the Web
grouped by year, and Europarl. For each subset,
a standard 4-gram LM was estimated using inter-
polated Kneser-Ney smoothing (Kneser and Ney,
4The forced alignment step consists in an additional EM it-
eration.
333
1995; Chen and Goodman, 1998). The resulting
LMs are then linearly combined using interpolation
coefficients chosen so as to minimize the perplexity
of the development set. The German vocabulary is
created using all the words contained in the parallel
data and expanded to reach a total of 500k words by
including the most frequent words observed in the
monolingual News data for 2011.
For French and English, the same monolingual
corpora as last year were used5. We did not observe
any perplexity decrease in our attempts to include
the new data specifically provided for this year?s
evaluation. We therefore used the same language
models as in (Allauzen et al, 2011).
6 ?On-the-fly? system
We also developped an alternative approach imple-
menting ?on-the-fly? estimation of the parameter of
a standard phase-based model, using Moses (Koehn
et al, 2007) as the decoder. Implementing on-the-
fly estimation for n-code, while possible in the-
ory, is less appealing due to the computational cost
of estimating a smoothed language model. Given
an input source file, it is possible to compute only
those statistics which are required to translate the
phrases it contains. As in previous works on on-
the-fly model estimation for SMT (Callison-Burch
et al, 2005; Lopez, 2008), we compute a suffix
array for the source corpus. This further enables
to consider only a subset of translation examples,
which we select by deterministic random sampling,
meaning that the sample is chosen randomly with
respect to the full corpus but that the same sample
is always returned for a given value of sample size,
hereafter denoted N . In our experiments, we used
N = 1, 000 and computed from the sample and the
word alignments (we used the same tokenization and
word alignments as in all other submitted systems)
the same translation6 and lexical reordering models
as the standard training scripts of the Moses system.
Experiments were run on the data sets used for
WMT English-French machine translation evalua-
tion tasks, using the same corpora and optimization
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
6An approximation is used for p(f |e), and coherent transla-
tion estimation is used; see (Lopez, 2008).
procedure as in our other experiments. The only no-
table difference is our use of the Moses decoder in-
stead of the n-gram-based system. As shown in Ta-
ble 1, our on-the-fly system achieves a result (31.7
BLEU point) that is slightly worst than the n-code
baseline (32.0) and slightly better than the equiva-
lent Moses baseline (31.5), but does it much faster.
Model estimation for the test file is reduced to 2
hours and 50 minutes, with an additional overhead
for loading and writing files of one and a half hours,
compared to roughly 210 hours for our baseline sys-
tems under comparable hardware conditions.
7 Experimental results
7.1 n-code with SOUL
Table 1 summarizes the experimental results sub-
mitted to the shared translation for French-English
and German-English in both directions. The perfor-
mances are measured in terms of BLEU on new-
stest2011, last year?s test set, and this year?s test
set newstest2012. For the former, BLEU scores are
computed with the NIST script mteva-v13.pl, while
we provide for newstest2012 the results computed
by the organizers 7. The Baseline results are ob-
tained with standard n-gram models estimated with
back-off, both for the bilingual and monolingual tar-
get models. With standard n-gram estimates, the or-
der is limited to n = 4. For instance, the n-code
French-English baseline achieves a 0.5 BLEU point
improvement over a Moses system trained with the
same data setup in both directions.
From Table 1, it can be observed that adding
the SOUL models (translation models and target
language model) consistently improves the base-
line, with an increase of 1 BLEU point. Con-
trastive experiments show that the SOUL target LM
does not bring significant gain when added to the
SOUL translation models. For instance, a gain of
0.3 BLEU point is observed when translating from
French to English with the addition of the SOUL tar-
get LM. In the other translation directions, the differ-
ences are negligible.
7All results come from the official website: http://
matrix.statmt.org/matrix/.
334
Direction System BLEU
test2011 test2012?
en2fr Baseline 32.0 28.9
+ SOUL TM 33.4 29.9
on-the-fly 31.7 28.6
fr2en Baseline 30.2 30.4
+ SOUL TM 31.1 31.5
en2de Baseline 15.4 16.0
+ SOUL TM 16.6 17.0
de2en Baseline 21.8 22.9
+ SOUL TM 22.8 23.9
Table 1: Experimental results in terms of BLEU scores
measured on the newstest2011 and newstest2012. For
newstest2012, the scores are provided by the organizers.
7.2 Experiments with additional features
For this year?s evaluation, we also investigated sev-
eral additional features based on IBM1 models and
word sense disambiguation (WSD) information in
rescoring. As for the SOUL models, these features
are added after the n-best list generation step.
In previous work (Och et al, 2004; Hasan, 2011),
the IBM1 features (Brown et al, 1993) are found
helpful. As the IBM1 model is asymmetric, two
models are estimated, one in both directions. Con-
trary to the reported results, these additional features
do not yield significant improvements over the base-
line system. We assume that the difficulty is to add
information to an already extensively optimized sys-
tem. Moreover, the IBM1 models are estimated on
the same training corpora as the translation system,
a fact that may explain the redundancy of these ad-
ditional features.
In a separate series of experiments, we also add
WSD features calculated according to a variation of
the method proposed in (Apidianaki, 2009). For
each word of a subset of the input (source lan-
guage) vocabulary, a simple WSD classifier pro-
duces a probability distribution over a set of trans-
lations8. During reranking, each translation hypoth-
esis is scanned and the word translations that match
one of the proposed variant are rewarded using an
additional score. While this method had given some
8The difference with the method described in (Apidianaki,
2009) is that no sense clustering is performed, and each transla-
tion is represented by a separate weighted source feature vector
which is used for disambiguation
small gains on a smaller dataset (IWSLT?11), we did
not observe here any improvement over the base-
line system. Additional analysis hints that (i) most
of the proposed variants are already covered by the
translation model with high probabilities and (ii) that
these variants are seldom found in the reference sen-
tences. This means that, in the situation in which
only one reference is provided, the hypotheses with
a high score for the WSD feature are not adequately
rewarded with the actual references.
8 Conclusion
In this paper, we described our submissions to
WMT?12 in the French-English and German-
English shared translation tasks, in both directions.
As for our last year?s participation, our main sys-
tems are built with n-code, the open source Statis-
tical Machine Translation system based on bilingual
n-grams. Our contributions are threefold. First, we
have experimented a new kind of translation mod-
els, where the bilingual n-gram distribution are es-
timated in a continuous space with neural networks.
As shown in past evaluations with target language
model, there is a significant reward for using this
kind of models in a rescoring step. We observed that,
in general, the continuous space translation model
yields a slightly larger improvement than the target
translation model. However, their combination does
not result in an additional gain.
We also reported preliminary results with a sys-
tem ?on-the-fly?, where the training data are sam-
pled according to the data to be translated in order
to train contextually adapted system. While this sys-
tem achieves comparable performance to our base-
line system, it is worth noticing that its total train-
ing time is much smaller than a comparable Moses
system. Finally, we investigated several additional
features based on IBM1 models and word sense dis-
ambiguation information in rescoring. While these
methods have sometimes been reported to help im-
prove the results, we did not observe any improve-
ment here over the baseline system.
Acknowledgment
This work was partially funded by the French State
agency for innovation (OSEO) in the Quaero Pro-
gramme.
335
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
77?85, Athens, Greece, March. Association for Com-
putational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard Un iversity.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
Ilknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franc?ois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Hai-Son, Alexandre Allauzen, and Franc?ois Yvon. 2012.
Continuous space translation models with neural net-
works. In NAACL ?12: Proceedings of the 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology.
Sas?a Hasan. 2011. Triplet Lexicon Models for Statisti-
cal Machine Translation. Ph.D. thesis, RWTH Aachen
University.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
336
May 2 - May 7. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
337
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?9,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
WSD for n-best reranking and local language modeling in SMT
Marianna Apidianaki, Guillaume Wisniewski?, Artem Sokolov, Aure?lien Max?, Franc?ois Yvon?
LIMSI-CNRS
? Univ. Paris Sud
BP 133, F-91403, Orsay Cedex, France
firstname.lastname@limsi.fr
Abstract
We integrate semantic information at two
stages of the translation process of a state-of-
the-art SMT system. A Word Sense Disam-
biguation (WSD) classifier produces a proba-
bility distribution over the translation candi-
dates of source words which is exploited in
two ways. First, the probabilities serve to
rerank a list of n-best translations produced by
the system. Second, the WSD predictions are
used to build a supplementary language model
for each sentence, aimed to favor translations
that seem more adequate in this specific sen-
tential context. Both approaches lead to sig-
nificant improvements in translation perfor-
mance, highlighting the usefulness of source
side disambiguation for SMT.
1 Introduction
Word Sense Disambiguation (WSD) is the task of
identifying the sense of words in texts by reference
to some pre-existing sense inventory. The selec-
tion of the appropriate inventory and WSD method
strongly depends on the goal WSD intends to serve:
recent methods are increasingly oriented towards
the disambiguation needs of specific end applica-
tions, and explicitly aim at improving the overall
performance of complex Natural Language Process-
ing systems (Ide and Wilks, 2007; Carpuat and Wu,
2007). This task-oriented conception of WSD is
manifested in the area of multilingual semantic pro-
cessing: supervised methods, which were previously
shown to give the best results, are being abandoned
in favor of unsupervised ones that do not rely on pre-
annotated training data. Accordingly, pre-defined
semantic inventories, that usually served to provide
the lists of candidate word senses, are being replaced
by senses relevant to the considered applications and
directly identified from corpora by means of word
sense induction methods.
In a multilingual setting, the sense inventories
needed for disambiguation are generally built from
all possible translations of words or phrases in a par-
allel corpus (Carpuat and Wu, 2007; Chan et al,
2007), or by using more complex representations
of the semantics of translations (Apidianaki, 2009;
Mihalcea et al, 2010; Lefever and Hoste, 2010).
However, integrating this semantic knowledge into
Statistical Machine Translation (SMT) raises sev-
eral challenges: the way in which the predictions of
the WSD classifier have to be taken into account;
the type of context exploited for disambiguation;
the target words to be disambiguated (?all-words?
WSD vs. WSD restricted to target words satisfy-
ing specific criteria); the use of a single classifier
versus building separate classifiers for each source
word; the quantity and type of data used for training
the classifier (e.g., use of raw data or of more ab-
stract representations, such as lemmatization, allow-
ing to deal with sparseness issues), and many oth-
ers. Seemingly, the optimal way to take advantage
of WSD predictions remains an open issue.
In this work, we carry out a set of experiments
to investigate the impact of integrating the predic-
tions of a cross-lingual WSD classifier into an SMT
system, at two different stages of the translation pro-
cess. The first approach exploits the probability dis-
tribution built by the WSD classifier over the set of
translations of words found in the parallel corpus,
1
for reranking the translations in the n-best list gen-
erated by the SMT system. Words in the list that
match one of the proposed translations are boosted
and are thus more likely to appear in the final trans-
lation. Our results on the English-French IWSLT?11
task show substantial improvements in translation
quality. The second approach provides a tighter in-
tegration of the WSD classifier with the rest of the
system: using the WSD predictions, an additional
sentence specific language model is estimated and
used during decoding. These additional local mod-
els can be used as an external knowledge source to
reinforce translation hypotheses matching the pre-
diction of the WSD system.
In the rest of the paper, we present related work
on integrating semantic information into SMT (Sec-
tion 2). The WSD classifier used in the current study
is described in Section 3. We then present the two
approaches adopted for integrating the WSD out-
put into SMT (Section 4). Evaluation results are
presented in Section 5, before concluding and dis-
cussing some avenues for future work.
2 Related work
Word sense disambiguation systems generally work
at the word level: given an input word and its con-
text, they predict its (most likely) meaning. At
the same time, state-of-the-art translation systems
all consider groups of words (phrases, tuples, etc.)
rather than single words in the translation process.
This discrepancy between the units used in MT and
those used in WSD is one of the major difficul-
ties in integrating word predictions into the decoder.
This was, for instance, one of the reasons for the
somewhat disappointing results obtained by Carpuat
and Wu (2005) when the output of a WSD system
was directly incorporated into a Chinese-English
SMT system. Because of this difficulty, other cross-
lingual semantics works have considered only sim-
plified tasks, like blank-filling, without addressing
the integration of the WSD models in full-scale MT
systems (Vickrey et al, 2005; Specia, 2006).
Since the pioneering work of Carpuat and Wu
(2005), several more successful ways to take WSD
predictions into account have been proposed. For
instance, Carpuat and Wu (2007) proposed to gen-
eralize the WSD system so that it performs a fully
phrasal multiword disambiguation. However, given
that the number of phrases is far larger than the num-
ber of words, this approach suffers from sparsity
and computational problems, as it requires training
a classifier for each entry of the phrase table.
Chan et al (2007) introduced a way to modify the
rule weights of a hierarchical translation system to
reflect the predictions of their WSD system. While
their approach and ours are built on the same intu-
ition (an adaptation of a model to incorporate word
predictions) their work is specific to hierarchical
systems, while ours can be applied to any decoder
that uses a language model. Haque et al (2009) et
Haque et al (2010) introduce lexico-syntactic de-
scriptions in the form of supertags as source lan-
guage context-informed features in a phrase-based
SMT and a state-of-the-art hierarchical model, re-
spectively, and report significant gains in translation
quality.
Closer to our work, Mauser et al (2009) and Pa-
try and Langlais (2011) train a global lexicon model
that predicts the bag of output words from the bag
of input words. As no explicit alignment between
input and output words is used, words are chosen
based on the (global) input context. For each input
sentence, the decoder considers these word predic-
tions as an additional feature that it uses to define a
new model score which favors translation hypothe-
ses containing words predicted by the global lexicon
model. A difference between this approach and our
work is that instead of using a global lexicon model,
we disambiguate a subset of the words in the input
sentence by employing a WSD classifier that cre-
ates a probability distribution over the translations
of each word in its context.
The unsupervised cross-lingual WSD classifier
used in this work is similar to the one proposed in
Apidianaki (2009). The original classifier disam-
biguates new instances of words in context by se-
lecting the most appropriate cluster of translations
among a set of candidate clusters found in an auto-
matically built bilingual sense inventory. The sense
inventory exploited by the classifier is created by
a cross-lingual word sense induction (WSI) method
that reveals the senses of source words by grouping
their translations into clusters according to their se-
mantic proximity, revealed by a distributional sim-
ilarity calculation. The resulting clusters represent
2
the source words? candidate senses. This WSD
method gave good results in a word prediction task
but, similarly to the work of Vickrey et al (2005)
and of Specia (2006), the predictions are not inte-
grated into a complete MT system.
3 The WSD classifier
Our WSD classifier is a variation of the one intro-
duced in Apidianaki (2009). The main difference
is that here the classifier serves to discriminate be-
tween unclustered translations of a word and to as-
sign a probability to each translation for new in-
stances of the word in context. Each translation is
represented by a source language feature vector that
the classifier uses for disambiguation. All experi-
ments carried out in this study are for the English
(EN) - French (FR) language pair.
3.1 Source Language Feature Vectors
Preprocessing The information needed by the clas-
sifier is gathered from the EN-FR training data pro-
vided for the IWSLT?11 evaluation task.1 The
dataset consists of 107,268 parallel sentences, word-
aligned in both translation directions using GIZA++
(Och and Ney, 2003). We disambiguate EN words
found in the parallel corpus that satisfy the set of
criteria described below.
Two bilingual lexicons are built from the align-
ment results and filtered to eliminate spurious align-
ments. First, translation correspondences with a
probability lower than a threshold are discarded;2
then translations are filtered by part-of-speech
(PoS), keeping for each word only translations per-
taining to the same grammatical category;3 finally,
only intersecting alignments (i.e., correspondences
found in the lexicons of both directions) are retained.
Given that the lexicons contain word forms, the in-
tersection is calculated based on lemmatization in-
formation in order to perform a generalization over
the contents of the lexicons. For instance, if the EN
adjective regular is translated by habituelle (femi-
1http://www.iwslt2011.org/
2The translation probabilities between word tokens are
found in the translation table produced by GIZA++; the thresh-
old is set to 0.01.
3For this filtering, we employ a PoS and lemmatization lex-
icon built after tagging both parts of the training corpus with
TreeTagger (Schmid, 1994).
nine singular form of the adjective habituel) in the
EN-FR lexicon, but is found to translate habituel
(masculine singular form) in the other direction,
the EN-FR correspondence regular/habituelle is re-
tained (because the two variants of the adjective are
reduced to the same lemma).
All lexicon entries satisfying the above criteria are
retained and used for disambiguation. In these initial
experiments, we disambiguate English words having
less than 20 French translations in the lexicon. Each
French translation of an English word that appears
more than once in the training corpus4 is character-
ized by a weighted English feature vector built from
the training data.
Vector building The feature vectors corresponding
to the translations are built by exploiting information
from the source contexts (Apidianaki, 2008; Grefen-
stette, 1994). For each translation of an EN word w,
we extract the content words that co-occur with w
in the corresponding source sentences of the parallel
corpus (i.e. the content words that occur in the same
sentence as w whenever it is translated by this trans-
lation). The extracted source language words con-
stitute the features of the vector built for the transla-
tion.
For each translation Ti of w, let N be the number
of features retained from the corresponding source
context. Each feature Fj (1 ? j ? N) receives a to-
tal weight tw(Fj,Ti) defined as the product of the
feature?s global weight, gw(Fj), and its local weight
with that translation, lw(Fj,Ti):
tw(Fj,Ti) = gw(Fj) ? lw(Fj,Ti) (1)
The global weight of a feature Fj is a function of
the number Ni of translations (Ti?s) to which Fj is re-
lated, and of the probabilities (pi j) that Fj co-occurs
with instances of w translated by each of the Ti?s:
gw(Fj) = 1?
?Ti pi j log(pi j)
Ni
(2)
Each of the pi j?s is computed as the ratio between
the co-occurrence frequency of Fj with w when
translated as Ti, denoted as cooc frequency(Fj,Ti),
4We do not consider hapax translations because they often
correspond to alignment errors.
3
and the total number of features (N) seen with Ti:
pi j =
cooc frequency(Fj,Ti)
N
(3)
Finally, the local weight lw(Fj,Ti) between Fj and Ti
directly depends on their co-occurrence frequency:
lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (4)
3.2 Cross-Lingual WSD
The weighted feature vectors corresponding to the
different translations of an English word are used
for disambiguation.5 As noted in Section 3.1, we
disambiguate source words satisfying a set of crite-
ria. Disambiguation is performed by comparing the
vector associated with each translation to the new
context of the words in the input sentences from the
IWSLT?11 test set.
More precisely, the information contained in each
vector is exploited by the WSD classifier to produce
a probability distribution over the translations, for
each new instance of a word in context. We dis-
ambiguate word forms (not lemmas) in order to di-
rectly use the selected translations in the translated
texts. However, we should note that in some cases
this reduces the role of WSD to distinguishing be-
tween different forms of one word and no different
senses are involved. Using more abstract represen-
tations (corresponding to senses) is one of the per-
spectives of this work.
The classifier assigns a score to each transla-
tion by comparing information in the corresponding
source vector to information found in the new con-
text. Given that the vector features are lemmatized,
the new context is lemmatized as well and the lem-
mas of the content words are gathered in a bag of
words. The adequacy of each translation for a new
instance of a word is estimated by comparing the
translation?s vector with the bag of words built from
the new context. If common features are found be-
tween the new context and a translation vector, an
association score is calculated corresponding to the
mean of the weights of the common features rela-
tively to the translation (i.e. found in its vector). In
5The vectors are not used for clustering the translations as
in Apidianaki (2009) but all translations are considered as can-
didate senses.
Equation (5), (CFj)|CF |j=1 is the set of common fea-
tures between the translation vector Vi and the new
context C and tw is the weight of a CF with transla-
tion Ti (cf. formula (1)).
assoc score(Vi,C) =
?|CF |j=1 tw(CFj,Ti)
|CF| (5)
The scores assigned to the different translations of a
source word are normalized to sum up to one.
In this way, a subset of the words that occur in the
input sentences from the test set are annotated with
their translations and the associated scores (contex-
tual probabilities), as shown in the example in Fig-
ure 1.6 The WSD classifier makes predictions only
for the subset of the words found in the source part
of the parallel test set that were retained from the ini-
tial EN-FR lexicon after filtering. Table 1 presents
the total coverage of the WSD method as well as its
coverage for words of different PoS, with a focus
on content words. We report the number of disam-
biguated words for each content PoS (cf. third col-
umn) and the corresponding percentage, calculated
on the basis of the total number of words pertaining
to this PoS (cf. second column). We observe that
the coverage of the method on nouns and adjectives
is higher than the one on verbs. Given the rich ver-
bal morphology of French, several verbs have a very
high number of translations in the bilingual lexicon
(over 20) and are not handled during disambigua-
tion. The same applies to function words (articles,
prepositions, conjunctions, etc.) included in the ?all
PoS? category.
4 Integrating Semantics into SMT
In this section, we present two ways to integrate
WSD predictions into an SMT decoder. The first
one (Section 4.1) is a simple method based on n-
best reranking. This method, already proposed in
the literature (Specia et al, 2008), allows us to eas-
ily evaluate the impact of WSD predictions on au-
tomatic translation quality. The second one (Sec-
tion 4.2) builds on the idea, introduced in (Crego et
al., 2010), of using an additional language model to
6Some source words are tagged with only one translation
(e.g. stones {pierres(1.000)}) because their other translations
in the lexicon occurred only once in the training corpus and,
consequently, were not considered.
4
PoS # of words # of WSD predictions %
Nouns 5535 3472 62.72
Verbs 5336 1269 23.78
Adjs 1787 1249 69.89
Advs 2224 1098 49.37
all content PoS 14882 7088 47.62
all PoS 27596 8463 30.66
Table 1: Coverage of the WSD method
you know, one of the intense {intenses(0.305), forte(0.306), intense(0.389)} pleasures of
travel {transport(0.334), voyage(0.332), voyager(0.334)} and one of the delights of ethnographic
research {recherche(0.225), research(0.167), e?tudes(0.218), recherches(0.222), e?tude(0.167)} is
the opportunity {possibilite?(0.187), chance(0.185), opportunite?s(0.199), occasion(0.222), opportu-
nite?(0.207)} to live amongst those who have not forgotten {oubli(0.401), oublie?s(0.279), ou-
blie?e(0.321)} the old {ancien(0.079), a?ge(0.089), anciennes(0.072), a?ge?es(0.100), a?ge?s(0.063), an-
cienne(0.072), vieille(0.093), ans(0.088), vieux(0.086), vieil(0.078), anciens(0.081), vieilles(0.099)}
ways {fac?ons(0.162), manie`res(0.140), moyens(0.161), aspects(0.113), fac?on(0.139), moyen(0.124),
manie`re(0.161)} , who still feel their past {passe?e(0.269), autrefois(0.350), passe?(0.381)} in the
wind {e?olienne(0.305), vent(0.392), e?oliennes(0.304)} , touch {touchent(0.236), touchez(0.235),
touche(0.235), toucher(0.293)} it in stones {pierres(1.000)} polished by rain {pluie(1.000)} ,
taste {gou?t(0.500), gou?ter(0.500)} it in the bitter {amer(0.360), ame`re(0.280), amertume(0.360)}
leaves {feuilles(0.500), feuillages(0.500)} of plants {usines(0.239), centrales(0.207), plantes(0.347),
ve?ge?taux(0.207)}.
Figure 1: Input sentence with WSD information
directly integrate the prediction of the WSD system
into the decoder.
4.1 N-best List Reranking
A simple way to influence translation hypotheses se-
lection with WSD information is to use the WSD
probabilities of translation variants to produce an ad-
ditional feature appended to the n-best list after its
generation. The feature value should reflect the de-
gree to which a particular hypothesis includes pro-
posed WSD variants for the respective words. Re-
running the standard MERT optimization procedure
on the augmented features gives a new set of model
weights, that are used to rescore the n-best list.
We propose the following method of features con-
struction. Given the phrase alignment information
between a source sentence and a hypothesis, we ver-
ify if one or more of the proposed WSD variants for
the source word occur in the corresponding phrase of
the translation hypothesis. If this is the case, the cor-
responding probabilities are additively accumulated
for the current hypothesis. At the end, two features
are appended to each hypothesis in the n-best list:
the total score accumulated for the hypothesis and
the same score normalized by the number of words
in the hypothesis.
Two MERT initialization schemes were consid-
ered: (1) all model weights are initialized to zero,
and (2) all the weights of ?standard? features are ini-
tialized to the values found by MERT and the new
WSD features to zero.
4.2 Local Language Models
We propose to adapt the approach introduced in
Crego et al (2010) as an alternative way to inte-
grate the WSD predictions within the decoder: for
each sentence to be translated, an additional lan-
guage model (LM) is estimated and taken into ac-
count during decoding. As this additional ?local?
model depends on the source sentence, it can be
used as an external source of knowledge to reinforce
translation hypotheses complying with criteria pre-
dicted from the whole source sentence. For instance,
the unigram probabilities of the additional LM can
be derived from the (word) predictions of a WSD
system, bigram probabilities from the prediction of
phrases and so on and so forth. Although this ap-
proach was suggested in (Crego et al, 2010), this
5
is, to the best of our knowledge, the first time it is
experimentally validated.
In practice, the predictions of the WSD system
described in Section 3 can be integrated by defining,
for each sentence, an additional unigram language
model as follows:
? each translation predicted by the WSD classi-
fier can be generated by the language model
with the probability estimated by the WSD
classifier; no information about the source
word that has been disambiguated is consid-
ered;
? the probability of unknown words is set to a
small arbitrary constant.
Even if most of the words composing the transla-
tion hypothesis are considered as unknown words,
hypotheses that contain the words predicted by the
WSD system still have a higher LM score and are
therefore preferred. Note that even if we only use
unigram language models in our experiments, as
senses are predicted at the word level, our approach
is able to handle disambiguation of phrases as well.
This approach has two main advantages over ex-
isting ways to integrate WSD predictions in an SMT
system. First, no hard decisions are made: errors
of the WSD can be ?corrected? by the translation.
Second, sense disambiguation at the word level is
naturally and automatically propagated at the phrase
level: the additional LM is influencing all phrase
pairs using one of the predicted words.
Compared to the reranking approach introduced
in the previous section, this method results in a
tighter integration with the decoder. In particu-
lar, the WSD predictions are applied before search-
space pruning and are therefore expected to have a
more important role.
5 Evaluation
5.1 Experimental Setting
In all our experiments, we considered the TED-
talk English to French data set provided by the
IWSLT?11 evaluation campaign, a collection of pub-
lic speeches on a variety of topics. We used the
Moses decoder (Koehn et al, 2007).
The TED-talk corpus is a small data set made
of a monolingual corpus (111,431 sentences) used
to estimate a 4-gram language model with KN-
smoothing, and a bilingual corpus (107,268 sen-
tences) used to extract the phrase table. All data
are tokenized, cleaned and converted to lowercase
letters using the tools provided by the WMT orga-
nizers.7 We then use a standard training pipeline to
construct the translation model: the bitext is aligned
using GIZA++, symmetrized using the grow-diag-
final-and heuristic; the phrase table is extracted and
scored using the tools distributed with Moses. Fi-
nally, systems are optimized using MERT on the
934 sentences of the dev-2010 set. All evalua-
tions are performed on the 1,664 sentences of the
test-2010 set.
5.2 Baseline
In addition to the models introduced in Section 4,
we considered two other supplementary models as
baselines. The first one uses the IBM 1 model esti-
mated during the SMT system training as a simple
WSD system: for each source sentence, a unigram
additional language model is defined by taking, for
each source, the 20 best translations according to the
IBM 1 model and their probability. Model 1 has
been shown to be one of the best performing fea-
tures to be added to an SMT system in a reranking
step (Och et al, 2004) and can be seen as a naive
WSD classifier.
To test the validity of our approach, we repli-
cate the ?oracle? experiments of Crego et al (2010)
and estimate the best gain our method can achieve.
These experiments consist in using the reference to
train a local n-gram language model (with n in the
range 1 to 3) which amounts, in the local language
model method of Section 4.2, to assuming that the
WSD system correctly predicted a single translation
for each source word.
5.3 Results
Table 2 reports the results of our experiments. It
appears that, for the considered task, sense disam-
biguation improves translation performance: n-best
rescoring results in a 0.37 BLEU improvement and
using an additional language model brings about an
improvement of up to a 0.88 BLEU. In both cases,
MERT assigns a large weight to the additional fea-
7http://statmt.org/wmt08/scripts.tgz
6
method BLEU METEOR
baseline ? 29.63 53.78
rescoring WSD (zero init) 30.00 54.26WSD (reinit) 29.58 53.96
additional LM
oracle 3-gram 43.56 64.64
oracle 2-gram 39.36 62.92
oracle 1-gram 42.92 69.39
IBM 1 30.18 54.36
WSD 30.51 54.38
Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions.
PoS baseline WSD
Nouns 67.57 69.06
Verbs 45.97 47.76
Adjectives 51.79 53.94
Adverbs 52.17 56.25
Table 3: Contrastive lexical evaluation: % of words correctly translated within each PoS class
tures during tuning. When rescoring n-best, an im-
provement is observed only when the weights are
initialized to zero and not to the weights resulting
from the previous optimization, maybe because of
the difficulty to exit the local minimum MERT had
found earlier.
As expected, integrating the WSD predictions
with an additional language model results in a larger
improvement than simple rescoring, which shows
the importance of applying this new source of in-
formation early in the translation pipeline, before
search space pruning. Also note that the system us-
ing the IBM 1 predictions is outperformed by the
system using the WSD classifier introduced in Sec-
tion 3, showing the quality of its predictions.
Oracle experiments stress the high potential of
the method introduced in (Crego et al, 2010) as a
way to integrate external sources of knowledge: all
three conditions result in large improvements over
the baseline and the proposed methods. It must,
however, be noted that contrary to the WSD method
introduced in Section 3, these oracle experiments
rely on sense predictions for all source words and
not only content words. Surprisingly enough, pre-
dicting phrases instead of words results only in a
small improvement. Additional experiments are re-
quired to explain why 2-gram oracle achieved such
a low performance.
5.4 Contrastive lexical evaluation
All the measures used for evaluating the impact
of WSD information on translation show improve-
ments, as discussed in the previous section. We
complement these results with another measure of
translation performance, proposed by Max et al
(2010), which allows for a more fine-grained con-
trastive evaluation of the translations produced by
different systems. The method permits to compare
the results produced by the systems on different
word classes and to take into account the source
words that were actually translated. We focus this
evaluation on the classes of content words (nouns,
adjectives, verbs and adverbs) on which WSD had
an important coverage. Our aim is, first, to ex-
plore how these words are handled by a WSD-
informed SMT system (the system using the lo-
cal language models) compared to the baseline sys-
tem that does not exploit any semantic informa-
tion; and, second, to investigate whether their dis-
ambiguation influences the translation of surround-
ing non-disambiguated words.
Table 3 reports the percentage of words cor-
rectly translated by the semantically-informed sys-
tem within each content word class: consistent gains
in translation quality are observed for all parts-of-
speech compared to the baseline, and the best results
are obtained for nouns.
7
baseline WSD
w?2 w?1 w+1 w+2 w?2 w?1 w+1 w+2
Nouns 64.01 68.69 75.17 64.6 65.47 70.46 76.3 66.6
Verbs 68.67 67.58 63 62.19 69.98 68.89 64.85 64.25
Adjectives 63.1 64.39 64.28 66.55 64.09 65.65 64.76 69.33
Adverbs 70.8 69.44 68.67 66.38 71 71.21 70 67.22
Table 4: Impact of WSD prediction on the surrounding words
Table 4 shows how the words surrounding a dis-
ambiguated word w (noun, verb, adjective or adverb)
in the text are handled by the two systems. More
precisely, we look at the translation of words in the
immediate context of w, i.e. at positions w?2, w?1,
w+1 and w+2. The left column reports the percent-
age of correct translations produced by the baseline
system (without disambiguation) for words in these
positions; the right column shows the positive im-
pact that the disambiguation of a word has on the
translation of its neighbors. Note that this time we
look at disambiguated words and their context with-
out evaluating the correctness of the WSD predic-
tions. Nevertheless, even in this case, consistent
gains are observed when WSD information is ex-
ploited. For instance, when a noun is disambiguated,
70.46% and 76.3% of the immediately preceding
(w?1) and following (w+1) words, respectively, are
correctly translated, versus 68.69% and 75.17% of
correct translations produced by the baseline system.
6 Conclusion and future work
The preliminary results presented in this paper on
integrating cross-lingual WSD into a state-of-the-
art SMT system are encouraging. Both adopted ap-
proaches (n-best rescoring and local language mod-
eling) benefit from the predictions of the proposed
cross-lingual WSD classifier. The contrastive eval-
uation results further show that WSD improves not
only the translation of disambiguated words, but also
the translation of neighboring words in the input
texts.
We consider various ways for extending this
work. First, future experiments will involve the use
of more abstract representations of senses than indi-
vidual translations, by applying a cross-lingual word
sense induction method to the training corpus prior
to disambiguation. We will also experiment with
disambiguation at the level of lemmas, to reduce
sparseness issues, and with different ways for han-
dling lemmatized predictions by the SMT systems.
Furthermore, we intend to extend the coverage of the
WSD method by exploring other filtering methods
for cleaning the alignment lexicons, and by address-
ing the disambiguation of words of all PoS.
Acknowledgments
This work was partly funded by the European Union
under the FP7 project META-NET (T4ME), Con-
tract No. 249119, and by OSEO, the French agency
for innovation, as part of the Quaero Program.
References
Marianna Apidianaki. 2008. Translation-oriented Word
Sense Induction Based on Parallel Corpora. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selection
in Translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-09), pages 77?85,
Athens, Greece.
Marine Carpuat and Dekai Wu. 2005. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
387?394, Ann Arbor, Michigan.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation. In Proceedings of the Joint EMNLP-CoNLL
Conference, pages 61?72, Prague, Czech Republic.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 33?40, Prague, Czech Republic.
8
Josep Maria Crego, Aure?lien Max, and Franc?ois Yvon.
2010. Local lexical adaptation in Machine Transla-
tion through triangulation: SMT helping SMT. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 232?
240, Beijing, China.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Rejwanual Haque, Sudip Naskar, Yanjun Ma, and Andy
Way. 2009. Using supertags as source language con-
text in SMT. In Proceedings of the 13th Annual Meet-
ing of the European Association for Machine Transla-
tion (EAMT 2009), pages 234?241, Barcelona, Spain.
Rejwanul Haque, Sudip Kumar Naskar, Antal Van Den
Bosch, and Andy Way. 2010. Supertags as source lan-
guage context in hierarchical phrase-based SMT. In
Proceedings of AMTA 2010: The Ninth Conference of
the Association for Machine Translation in the Ameri-
cas, pages 210?219, Denver, CO.
N. Ide and Y. Wilks. 2007. Making Sense About Sense.
In E. Agirre and P. Edmonds, editors, Word Sense Dis-
ambiguation, Algorithms and Applications, pages 47?
73. Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual ACL Meeting, Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2), ACL 2010, pages
15?20, Uppsala, Sweden.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 210?217,
Singapore, August.
Aure?lien Max, Josep Maria Crego, and Franc?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-Lingual Lexical Sub-
stitution. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), ACL
2010, pages 9?14, Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In Proceedings of HLT-NAACL 2004, pages 161?
168, Boston, Massachusetts, USA.
Alexandre Patry and Philippe Langlais. 2011. Going be-
yond word cooccurrences in global lexical selection
for statistical machine translation using a multilayer
perceptron. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
658?666, Chiang Mai, Thailand, November.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Lucia Specia, Baskaran Sankaran, and Maria Das
Grac?as Volpe Nunes. 2008. n-Best Reranking for the
Efficient Integration of Word Sense Disambiguation
and Statistical Machine Translation. In Proceedings of
the 9th international conference on Computational lin-
guistics and intelligent text processing, CICLing?08,
pages 399?410, Berlin, Heidelberg. Springer-Verlag.
Lucia Specia. 2006. A Hybrid Relational Approach for
WSD - First Results. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages 55?
60, Sydney, Australia.
David Vickrey, Luke Biewald, Marc Teyssier, and
Daphne Koller. 2005. Word-Sense Disambiguation
for Machine Translation. In Proceedings of the Joint
Conference on Human Language Technology / Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP), pages 771?778, Vancouver, Canada.
9
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 260?265,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
LIMSI?s Participation in the 2013 Shared Task
on Native Language Identification
Thomas Lavergne, Gabriel Illouz, Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
Orsay, France
{firstname.lastname}@limsi.fr
Ryo Nagata
LIMSI-CNRS & Konan University
8-9-1 Okamoto
Kobe 658-0072 Japan
rnagata@konan-u.ac.jp
Abstract
This paper describes LIMSI?s participation to
the first shared task on Native Language Iden-
tification. Our submission uses a Maximum
Entropy classifier, using as features character
and chunk n-grams, spelling and grammati-
cal mistakes, and lexical preferences. Perfor-
mance was slightly improved by using a two-
step classifier to better distinguish otherwise
easily confused native languages.
1 Introduction
This paper describes the submission from LIMSI to
the 2013 shared task on Native Language Identifica-
tion (Tetreault et al, 2013). The creation of this new
challenge provided us with a dataset (12,100 TOEFL
essays by learners of English of eleven native lan-
guages (Blanchard et al, 2013)) that was necessary
to us to develop an initial framework for studying
Native Language Identification in text. We expect
that this challenge will draw conclusions that will
provide the community with new insights into the
impact of native language in foreign language writ-
ing. We believe that such a research domain is
crucial, not only for improving our understanding
of language learning and language production pro-
cesses, but also for developing Natural Language
Processing applications to support text improve-
ment.
This article is organized as follows. We first de-
scribe in Section 2 our maximum entropy system
used for the classification of a given text in English
into the native languages of the shared task. We then
introduce the various sets of features that we have in-
cluded in our submission, comprising basic n-gram
features (3.1) and features to capture spelling mis-
takes (3.2), grammatical mistakes (3.3), and lexical
preference (3.4). We next report the performance of
each of our sets of features (4.1) and our attempt to
perform a two-step classification to reduce frequent
misclassifications (4.2). We finally conclude with a
short discussion (section 5).
2 A Maximum Entropy model
Our system is based on a classical maximum entropy
model (Berger et al, 1996):
p?(y|x) =
1
Z?(x)
exp(?>F (x, y))
whereF is a vector of feature functions, ? a vector of
associated parameter values, and Z?(x) the partition
function.
Given N independent samples (xi, yi), the model
is trained by minimizing, with respect to ?, the neg-
ative conditional log-likelihood of the observations:
L(?) = ?
N?
i=1
log p(yi|xi).
This term is complemented with an additional regu-
larization term so as to avoid overfitting. In our case,
an `1 regularization is used, with the additional ef-
fect to produce a sparse model.
The model is trained with a gradient descent algo-
rithm (L-BFGS) using the Wapiti toolkit (Lavergne
et al, 2010). Convergence is determined either by
error rate stability on an held-out dataset or when
limits of numerical precision are reached.
260
3 Features
Our submission makes use of basic features, includ-
ing n-grams of characters and part-of-speech tags.
We further experimented with several sets of fea-
tures that will be described and compared in the fol-
lowing sections.
3.1 Basic features
We used n-grams of characters up to length 4 as fea-
tures. In order to reduce the size of the feature space
and the sparsity of these features, we used a hash
kernel (Shi et al, 2009) of size 216 with a hash fam-
ily of size 4. This allowed us to significantly reduce
the training time with no noticeable impact on the
model?s performance.
Our set of basic features also includes n-grams of
part-of-speech (POS) tags and chunks up to length 3.
Both were computed using an in-house CRF-based
tagger trained on PennTreeBank (Marcus et al,
1993). The POS tags sequences were post-processed
so that word tokens were used in lieu of their cor-
responding POS tags for the following: coordinat-
ing conjunctions, determiners, prepositions, modals,
predeterminers, possessives, pronouns, and question
adverbs (Nagata, 2013).
For instance, from this sentence excerpt:
[NP Some/DT people/NNS] [VP
might/MD think/VB] [SBAR that/IN]
[VP traveling/VBG] [PP in/IN]. . .
we extract n-grams from the pseudo POS-tag se-
quence:
Some NNS MD VB that VBG in. . .
and n-grams from the chunk sequence:
NP VP SBAR VP PP. . .
The length of chunks is encoded as separate fea-
tures that correspond to mean length of each type of
chunks. As shown in (Nagata, 2013), length of noun
sequences is also informative and thus was encoded
as a feature.
3.2 Capturing spelling mistakes
We added a set of features to capture information
about spelling mistakes in the model, following the
intuition that some spelling mistakes may be at-
tributed to the influence of the writer?s native lan-
guage.
To extract these features, each document is pro-
cessed using the ispell1 spell checker. This re-
sults in a list of incorrectly written word forms and
a set of potential corrections. For each word, the
best correction is next selected using a set of rules,
which were built manually after a careful study of
the training dataset.
When a corrected word is found, the incorrect
fragment of the word is isolated by striping from
the original and corrected words common prefix and
suffix, keeping only the inner-most substring differ-
ence. For example, given the following mistake and
correction:
appartment? apartment
this procedure generates the following feature:
pp? p
Such a feature may for instance help to identify na-
tive languages (using latin scripts) where doubling
of letters is frequent.
3.3 Capturing grammatical mistakes
Errors at the grammatical level are captured using
the ?language tool? toolkit (Milkowski, 2010), a
rule-based grammar and style checker. Each rule fir-
ing in a document is mapped to an individual feature.
This triggers features such as
BEEN PART AGREEMENT, corresponding to
cases where the auxiliary be is not followed by a
past participle, or EN A VS AN, corresponding to
confusions between the correct form the articles a
and an.
3.4 Capturing lexical preferences
Learners of a foreign language may have some pref-
erence for lexical choice given some semantic con-
tent that they want to convey2. We made the follow-
ing assumption: the lexical variant chosen for each
word may correspond to the less ambiguous choice
if mapping from the native language to English3.
1http://www.gnu.org/software/ispell/
2We assumed that we should not expect thematic differences
in the contents of the essays across original languages, as the
prompts for the essays were evenly distributed.
3This assumption of course could not hold for advanced
learners of English, who should make their lexical choices in-
dependently of their native language.
261
Thus, for each word in an English essay, if we
knew a corresponding word (or sense) that a writer
may have thought of in her native language, we
would like to consider the most likely translation
into English, according to some reliable probabilis-
tic model of lexical translation into English, as the
lexical choice most likely to be made by a learner of
this native language.
As we obviously do not have access to the word
in the native language of the writer, we approximate
this information by searching for the word that max-
imizes the translation probability of translating back
from the native language after translating from the
original English word. This in fact corresponds to a
widely used way of computing paraphrase probabili-
ties from bilingual translation distributions (Bannard
and Callison-Burch, 2005):
e?l ? argmax
e
?
f
pl(f |e).pl(e|f)
where f ranges over all possible translations of En-
glish word e in a given native language l.
Preferably, we would like to obtain candidate
translations into the native language in context,
that is, by translating complete sentences and us-
ing a posteriori translation probabilities. We could
not do this for a number of reasons, the main one
being that we did not have the possibility of using
or building Statistical Machine Translation systems
for all the language pairs involving English and the
native languages of the shared task. We therefore
resorted to simply finding, for each English word,
the most likely back-translation into English via a
given native language. Using the Google Transla-
tion online Statistical Machine Translation service4,
which proposed translations from and to English and
all the native languages of the shared task, a further
approximation had to be made as, in practice, we
were only able to access the most likely translations
for words in isolation: we considered only the best
translation of the original English word in the native
language, and then kept its best back-translation into
English. We here note some common intuitions with
the use of roundtrip translation as a Machine Trans-
lation evaluation metrics (Rapp, 2009).
4http://translate.google.com
Table 1 provides various examples of back-
translations for English adjectives obtained via each
native language. The samples from the Table show
that our procedure produces a significant number of
non identical back-translations. They also illustrate
some types of undesirable results obtained, which
led us to only consider as features for our classi-
fier the proportion of words in essays for which
the above-defined back-translation yielded the same
word, considering all possible native languages. We
only considered content words, as out-of-context
back-translation for function words would be too un-
reliable. Table 2 shows values for some documents
of the training set. As can be seen, there are impor-
tant differences across languages, some languages
obtaining high scores on average (e.g. French and
Japanese) and others obtaining low scores on aver-
age (e.g. Korean, Turkish). Furthermore, the high-
est score is only rarely obtained for the actual native
language of each document, showing that keeping
the most probable language according to this value
alone would not allow to obtain a good classification
performance.
4 Experiments
4.1 Results per set of features
For all our experiments reported here, we used the
full training data provided using cross-validation to
tune the regularization parameter. Our results are
presented in the top part of Table 3. Using our com-
plete set of features yields our best performance on
accuracy, corresponding to a 0.75% absolute im-
provement over using our basic n-gram features
only. No type of features allows a significant im-
provement over the n-gram features when added in-
dividually.
4.2 Two-step classification
Table 4 contains the confusion matrix for our system
across languages. It clearly stands out that two lan-
guage pairs were harder to distinguish: Hindi (hin)
and Telugu (tel) on the one hand, and Korean (kor)
and Japanese (jpn) on the other.
In order to improve the performance of our model,
we performed a two-step classification focused on
these difficult pairs. For this, we built additional
classifiers for each difficult pairs. Both are built
262
eng abrupt affirmative amazing ambiguous anarchic atrocious attentive awkward
ara sudden positive amazing mysterious messy terrible heedful inappropriate
chi sudden sure amazing ambiguous anarchic atrocious careful awkward
fre sudden affirmative amazing ambiguous anarchic atrocious careful awkward
ger abrupt affirmative incredible ambiguous anarchical gruesome attentively awkward
hin suddenly positive amazing vague chaotic brutal observant clumsy
ita abrupt affirmative amazing ambiguous anarchist atrocious careful uncomfortable
jap sudden positive surprising ambiguous anarchy heinous cautious awkward
kor fortuitous positive amazing ambiguous anarchic severe kind awkward
spa abrupt affirmative surprising ambiguous anarchic atrocious attentive clumsy
tel abrupt affirmative amazing ambiguous anarchic formidable attentive awkward
tur sudden positive amazing uncertain anarchic brutal attentive strange
Table 1: Examples of back translations for English adjectives from the training set via each of the eleven native
languages of the shared task. Back-translations that differ from the original word are indicated using a bold face.
Doc id. Native l. ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
976 ARA 0.80 0.88 0.91 0.95 0.75 0.91 0.87 0.73 0.89 0.79 0.71
29905 CHI 0.84 0.81 0.93 0.87 0.79 0.89 0.89 0.56 0.93 0.62 0.75
61765 FRE 0.73 0.84 0.90 0.71 0.73 0.83 0.86 0.50 0.91 0.58 0.66
100416 GER 0.78 0.80 0.86 0.83 0.72 0.89 0.86 0.70 0.90 0.67 0.67
26649 HIN 0.68 0.75 0.88 0.89 0.67 0.85 0.86 0.69 0.86 0.75 0.77
39189 ITA 0.68 0.85 0.92 0.94 0.74 0.93 0.89 0.69 0.92 0.72 0.72
3044 JPN 0.83 0.81 0.89 0.83 0.68 0.94 0.91 0.71 0.94 0.83 0.70
3150 KOR 0.75 0.86 0.91 0.84 0.76 0.88 0.87 0.55 0.88 0.67 0.73
6614 SPA 0.79 0.90 0.86 0.85 0.78 0.85 0.92 0.67 0.90 0.70 0.68
12600 TEL 0.65 0.74 0.84 0.73 0.71 0.92 0.90 0.76 0.95 0.82 0.58
5565 TUR 0.70 0.77 0.88 0.78 0.70 0.84 0.86 0.72 0.84 0.74 0.71
Table 2: Values corresponding to the proportion of content words in a random essay for each native language for which
back-translation yielded the same word.
FRE GER ITA SPA TUR ARA HIN TEL KOR JPN CHI
FRE 79 4 4 3 2 3 0 0 2 2 1
GER 0 89 2 4 1 0 1 0 2 1 0
ITA 6 1 83 6 1 1 0 0 0 1 1
SPA 4 4 5 72 2 3 3 2 1 1 3
TUR 3 2 1 3 81 1 3 2 0 3 1
ARA 3 0 1 3 3 81 5 2 1 0 1
HIN 1 1 1 3 2 1 64 26 1 0 0
TEL 0 0 1 0 0 1 17 81 0 0 0
KOR 1 1 0 0 3 1 0 0 80 12 2
JPN 1 0 2 2 0 3 0 1 13 73 5
CHI 0 1 0 0 2 2 0 2 3 3 87
Table 4: Confusion matrix on the Test set.
263
Features X-Val Test
ngm 74.83% 75.27%
ngm+ort 74.98% 75.29%
ngm+grm 75.18% 75.63%
ngm+lex 74.85% 75.47%
all 75.57% 75.81%
2-step (a) 75.46% 75.69%
2-step (b) 75.89% 75.98%
Table 3: Accuracy results obtained by cross-validation
and using the provided Test set for various combina-
tions of features and our two 2-step strategies. The fea-
ture sets are: character and part-of-speech n-grams fea-
tures (ngm), spelling features (ort), grammatical features
(grm), and lexical preference features (lex).
from the same feature sets as for the first-step model
but with only three labels: one for each language of
the pair and one for any other language.
The training data used for these new models in-
clude all documents from both languages as well as
document misclassified as one of them by the first-
step classifier (using cross-validation to label the full
training set). The formers keep their original labels
while the later are relabeled as other.
Document classified in one of the difficult pairs
by the first-step classifier were post-processed with
these new models. When the new label predicted is
other, the second best choice of the first step is used.
We investigated two setups for the first classifier:
(a) using the original 11 native languages classi-
fier, and (b) using a new classifier with languages
of the difficult pairs merged, resulting in 9 native
?languages?.
Our results, shown in Figure 3 for easy com-
parison, improve over our system using all fea-
tures only when the first-pass classifier uses the set
of 9 merged pseudo-languages (b). We obtain a
moderate 0.32% absolute improvement in accuracy
over one-step classification on cross-validation, and
0.17% improvement on the Test set.
5 Discussion and conclusion
We have submitted on maximum entropy system to
the shared task on Native Language Identification,
for which our basic set of n-gram features already
obtained a level of performance, around 75% in ac-
curacy, close to the best performance reported in our
submission. The additional feature sets that we have
included in our system, while improving the model,
did not allow us to capture a deeper influence of the
native language.
A first analysis reveals that the model fails to fully
use the additional feature sets due to lack of context.
Future experiments will need to link more closely
these features to the documents for which they pro-
vide useful information.
Due to time constraints and engineering issues,
the two-pass system was not ready by the time of
submission. The results that we have included in
this report show that it is a promising approach that
we should continue to explore. We also plan to con-
duct experiments that exploit the information about
the level of English available in the essays, some-
thing that we did not consider for this submission.
While this information is not directly available, it
may be infered from the data as a first-step classifi-
cation. We believe that studying its influence on the
mistakes make learners of different native language
is a promising direction.
The approach that we have described in this sub-
mission, as most of previously published approaches
for this task, attempts to find mistakes in the text of
the documents. The most typical mistakes are then
used by the classifier to detect the native language.
This does not take into consideration the fact that na-
tive English writers also make errors. It would be in-
teresting to explore the divergence between various
sets of writers/learners, not from the mean of non-
native writers, but from the mean of native writers.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 597?604,
Ann Arbor, Michigan.
Adam Berger, Stephen Della Pietra, and Vincent
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1), March.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
264
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics, July.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
Marcin Milkowski. 2010. Developing an open-source,
rule-based proofreading tool. Software - Practice and
Experience, 40(7):543?566.
Ryo Nagata. 2013. Generating a language family tree
from indo-european non-native english texts (to ap-
pear). In Proceedings the 51th Annual Meeting of the
Association for Computational Linguistics (ACL). As-
sociation for Computational Linguistics.
Reinhard Rapp. 2009. The backtranslation score: Auto-
matic mt evalution at the sentence level without refer-
ence translations. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 133?136, Sun-
tec, Singapore.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. Journal of Machine
Learning Research, 10:2615?2637, December.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A Report on the First Native Language Identification
Shared Task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
265
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62?69,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI @ WMT?13
Alexandre Allauzen1,2, Nicolas Pe?cheux1,2, Quoc Khanh Do1,2, Marco Dinarelli2,
Thomas Lavergne1,2, Aure?lien Max1,2, Hai-Son Le3, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Vietnamese Academy of Science and Technology3, Hanoi, Vietnam
lehaison@ioit.ac.vn
Abstract
This paper describes LIMSI?s submis-
sions to the shared WMT?13 translation
task. We report results for French-English,
German-English and Spanish-English in
both directions. Our submissions use
n-code, an open source system based on
bilingual n-grams, and continuous space
models in a post-processing step. The
main novelties of this year?s participation
are the following: our first participation
to the Spanish-English task; experiments
with source pre-ordering; a tighter integra-
tion of continuous space language mod-
els using artificial text generation (for Ger-
man); and the use of different tuning sets
according to the original language of the
text to be translated.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Eighth Workshop on
Statistical Machine Translation. LIMSI partici-
pated in the French-English, German-English and
Spanish-English tasks in both directions. For this
evaluation, we used n-code, an open source in-
house Statistical Machine Translation (SMT) sys-
tem based on bilingual n-grams1, and continuous
space models in a post-processing step, both for
translation and target language modeling.
This paper is organized as follows. Section 2
contains an overview of the baseline systems built
with n-code, including the continuous space mod-
els. As in our previous participations, several
steps of data pre-processing, cleaning and filter-
ing are applied, and their improvement took a non-
negligible part of our work. These steps are sum-
marized in Section 3. The rest of the paper is de-
voted to the novelties of the systems submitted this
1http://ncode.limsi.fr/
year. Section 4 describes the system developed for
our first participation to the Spanish-English trans-
lation task in both directions. To translate from
German into English, the impact of source pre-
ordering is investigated, and experimental results
are reported in Section 5, while for the reverse di-
rection, we explored a text sampling strategy us-
ing a 10-gram SOUL model to allow a tighter in-
tegration of continuous space models during the
translation process (see Section 6). A final section
discusses the main lessons of this study.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o
et al, 2006; Crego and Marin?o, 2006). In this
framework, translation is divided in two steps: a
source reordering step and a (monotonic) transla-
tion step. Source reordering is based on a set of
learned rewrite rules that non-deterministically re-
order the input words. Applying these rules result
in a finite-state graph of possible source reorder-
ings, which is then searched for the best possible
candidate translation.
2.1 Features
Given a source sentence s of I words, the best
translation hypothesis t? is defined as the sequence
of J words that maximizes a linear combination of
feature functions:
t? = argmax
t,a
{ M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature
function hm and a denotes an alignment between
source and target phrases. Among the feature
functions, the peculiar form of the translation
model constitutes one of the main difference be-
tween the n-gram approach and standard phrase-
based systems.
62
In addition to the translation model (TM), four-
teen feature functions are combined: a target-
language model; four lexicon models; six lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two
lexical weights are estimated from the automatic
word alignments. The weight vector ? is learned
using the Minimum Error Rate Training frame-
work (MERT) (Och, 2003) and BLEU (Papineni
et al, 2002) measured on nt09 (newstest2009) as
the optimization criteria.
2.2 Translation Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to repro-
duce the word order modifications introduced dur-
ing the tuple extraction process. Hence, only those
reordering hypotheses are translated and are intro-
duced using a set of reordering rules automatically
learned from the word alignments. Part-of-speech
(POS) information is used to increase the gen-
eralization power of these rules. Hence, rewrite
rules are built using POS, rather than surface word
forms (Crego and Marin?o, 2006).
2.3 SOUL rescoring
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al, 2003; Schwenk
et al, 2006) as a potential means to improve dis-
crete language models (LMs). As for our last year
participation (Le et al, 2012c), we take advantage
of the recent proposal of Le et al (2011). Using
a specific neural network architecture (the Struc-
tured OUtput Layer or SOUL model), it becomes
possible to estimate n-gram models that use large
vocabulary, thereby making the training of large
neural network LMs (NNLMs) feasible both for
target language models and translation models (Le
et al, 2012a). We use the same models as last year,
meaning that the SOUL rescoring was used for all
systems, except for translating into Spanish. See
section 6 and (Le et al, 2012c) for more details.
3 Corpora and data pre-processing
Concerning data pre-processing, we started from
our submissions from last year (Le et al, 2012c)
and mainly upgraded the corpora and the associ-
ated language-dependent pre-processing routines.
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte
et al, 2008). Previous experiments have demon-
strated that better normalization tools provide bet-
ter BLEU scores: all systems are thus built using
the ?true-case? scheme.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to un-
known forms). When translating from German
into English, the German side is thus normalized
using a specific pre-processing scheme (Allauzen
et al, 2010; Durgar El-Kahlout and Yvon, 2010)
which aims at reducing the lexical redundancy by
(i) normalizing the orthography, (ii) neutralizing
most inflections and (iii) splitting complex com-
pounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition,
for German, fine-grained POS labels were also
needed for pre-processing and were obtained us-
ing the RFTagger (Schmid and Laws, 2008).
For Spanish, all the availaible data are tokenized
using FreeLing2 toolkit (Padro? and Stanilovsky,
2012), with default settings and some added rules.
Sentence splitting and morphological analysis are
disabled except for del ? de el and al ? a el.
Moreover, a simple ?true-caser? based on upper-
case word frequency is used, and the specific
Spanish punctuation signs ??? and ??? are removed
and heuristically reintroduced in a post-processing
step. All Spanish texts are POS-tagged also using
Freeling. The EAGLES tag set is however sim-
plified by truncating the category label to the first
two symbols, in order to reduce the sparsity of the
reordering rules estimated by n-code.
For the CommonCrawl corpus, we found that
many sentences are not in the expected language.
For example, in the French side of the French-
English version, most of the first sentences are
in English. Therefore, foreign sentence pairs are
filtered out with a MaxEnt classifier that uses n-
grams of characters as features (n is between 1
and 4). This filter discards approximatively 10%
2http://nlp.lsi.upc.edu/freeling/
63
of the sentence pairs. Moreover, we also observe
that a lot of sentence pairs are not translation of
each other. Therefore, an extra sentence alignment
step is carried out using an in-house implementa-
tion of the tool described in (Moore, 2002). This
last step discards approximately 20% of the cor-
pus. For the Spanish-English task, the same filter-
ing is applied to all the available corpora.
4 System development for the
Spanish-English task
This is our first participation to the Spanish-
English translation task in both directions. This
section provides details about the development of
n-code systems for this language pair.
4.1 Data selection and filtering
The CommonCrawl and UN corpora can be con-
sidered as very noisy and out-of-domain. As de-
scribed in (Allauzen et al, 2011), to select a subset
of parallel sentences, trigram LMs were trained for
both Spanish and English languages on a subset of
the available News data: the Spanish (resp. En-
glish) LM was used to rank the Spanish (resp. En-
glish) side of the corpus, and only those sentences
with perplexity above a given threshold were se-
lected. Finally, the two selected sets were in-
tersected. In the following experiments, the fil-
tered versions of these corpora are used to train
the translation systems unless explicitly stated.
4.2 Spanish language model
To train the language models, we assumed that the
test set would consist in a selection of recent news
texts and all the available monolingual data for
Spanish were used, including the Spanish Giga-
word, Third Edition. A vocabulary is first defined
by including all tokens observed in the News-
Commentary and Europarl corpora. This vocab-
ulary is then expanded with all words that occur
more than 10 times in the recent news texts (LDC-
2007-2011 and news-crawl-2011-2012). This pro-
cedure results in a vocabulary containing 372k
words. Then, the training data are divided into
7 sets based on dates or genres. On each set, a
standard 4-gram LM is estimated from the vocab-
ulary using absolute discounting interpolated with
lower order models (Kneser and Ney, 1995; Chen
and Goodman, 1998). The resulting LMs are then
linearly interpolated using coefficients chosen so
Corpora BLEU
dev nt11 test nt12
es2en N,E 30.2 33.2
N,E,C 30.6 33.7
N,E,U 30.3 33.6
N,E,C,U 30.6 33.7
N,E,C,U (nf) 30.7 33.6
en2es N,E 32.2 33.3
N,E,C,U 32.3 33.6
N,E,C,U (nf) 32.5 33.9
Table 1: BLEU scores achieved with different
sets of parallel corpora. All systems are base-
line n-code with POS factor models. The follow-
ing shorthands are used to denote corpora, : ?N?
stands for News-Commentary, ?E? for Europarl,
?C? for CommonCrawl, ?U? for UN and (nf) for
non filtered corpora.
as to minimise the perplexity evaluated on the de-
velopment set (nt08).
4.3 Experiments
All reported results are averaged on 3 MERT runs.
Table 1 shows the BLEU scores obtained with dif-
ferent corpora setups. We can observe that us-
ing the CommonCrawl corpus improves the per-
formances in both directions, while the impact of
the UN data is less important, especially when
combined with CommonCrawl. The filtering strat-
egy described in Section 4.2 has a slightly posi-
tive impact of +0.1 BLEU point for the Spanish-
to-English direction but yields a 0.2 BLEU point
decrease in the opposite direction.
For the following experiments, all the available
corpora are therefore used: News-Commentary,
Europarl, filtered CommonCrawl and UN. For
each of these corpora, a bilingual n-gram model
is estimated and used by n-code as one individual
model score. An additionnal TM is trained on the
concatenation all these corpora, resulting in a to-
tal of 5 TMs. Moreover, n-code is able to handle
additional ?factored? bilingual models where the
source side words are replaced by the correspond-
ing lemma or even POS tag (Koehn and Hoang,
2007). Table 2 reports the scores obtained with
different settings.
In Table 2, big denotes the use of a wider
context for n-gram TMs (n = 4, 5, 4 instead
of 3, 4, 3 respectively for word-based, POS-based
and lemma-based TMs). Using POS factored
64
Condition BLEU
dev nt11 test nt12
es2en base 30.3 33.5
pos 30.6 33.7
big-pos 30.7 33.7
big-pos-lem 30.7 33.8
en2es base 32.0 33.4
pos 32.3 33.6
big-pos 32.3 33.8
big-pos-pos+ 32.2 33.4
Table 2: BLEU scores for different configuration
of factored translation models. The big prefix de-
notes experiments with the larger context for n-
gram translation models.
models yields a significant BLEU improvement,
as well as using a wider context for n-gram TMs.
Since Spanish is morphologically richer than En-
glish, lemmas are introduced only on the Span-
ish side. An additionnal BLEU improvement is
achieved by adding factored models based on lem-
mas when translating from Spanish to English,
while in the opposite direction it does not seem
to have any clear impact.
For English to Spanish, we also experimented
with a 5-gram target factored model, using the
whole morphosyntactic EAGLES tagset, (pos+ in
Table 2), to add some syntactic information, but
this, in fact, proved harmful.
As several tuning sets were available, experi-
ments were carried out with the concatenation of
nt09 to nt11 as a tuning data set. This yields an im-
provement between 0.1 and 0.3 BLEU point when
testing on nt12 when translating from Spanish to
English.
4.4 Submitted systems
For both directions, the submitted systems are
trained on all the available training data, the cor-
pora CommonCrawl and UN being filtered as de-
scribed previously. A word-based TM and a POS
factored TM are estimated for each training set.
To translate from Spanish to English, the system
is tuned on the concatenation of the nt09 to nt11
datasets with an additionnal 4-gram lemma-based
factored model, while in the opposite direction, we
only use nt11.
dev nt09 test nt11
en2de 15.43 15.35
en-mod2de 15.06 15.00
Table 3: BLEU scores for pre-ordering experi-
ments with a n-code system and the approach pro-
posed by (Neubig et al, 2012)
5 Source pre-ordering for English to
German translation
While distorsion models can efficiently handle
short range reorderings, they are inadequate to
capture long-range reorderings, especially for lan-
guage pairs that differ significantly in their syn-
tax. A promising workaround is the source pre-
ordering method that can be considered similar,
to some extent, to the reordering strategy imple-
mented in n-code; the main difference is that the
latter uses one deterministic (long-range) reorder-
ing on top of conventional distortion-based mod-
els, while the former only considers one single
model delivering permutation lattices. The pre-
ordering approach is illustrated by the recent work
of Neubig et al (2012), where the authors use a
discriminatively trained ITG parser to infer a sin-
gle permutation of the source sentence.
In this section, we investigate the use of this
pre-ordering model in conjunction with the bilin-
gual n-gram approach for translating English into
German (see (Collins et al, 2005) for similar ex-
periments with the reverse translation direction).
Experiments are carried out with the same settings
as described in (Neubig et al, 2012): given the
source side of the parallel data (en), the parser is
estimated to modify the original word order and to
generate a new source side (en-mod); then a SMT
system is built for the new language pair (en-mod
? de). The same reordering model is used to re-
order the test set, which is then translated with the
en-mod? de system.
Results for these experiments are reported in Ta-
ble 3, where nt09 and nt11 are respectively used
as development and test sets. We can observe that
applying pre-ordering on source sentences leads to
small drops in performance for this language pair.
To explain this degradation, the histogram of to-
ken movements performed by the model on the
pre-ordered training data is represented in Fig-
ure 1. We can observe that most of the movements
are in the range [?4,+6] (92% of the total occur-
65
Figure 1: Histogram of token movement size ver-
sus its occurrences performed by the model Neu-
big on the source english data.
rences), which can be already taken into account
by the standard reordering model of the baseline
system. This is reflected also by the following
statistics: surprisingly, only 16% of the total num-
ber of sentences are changed by the pre-ordering
model, and the average sentence-wise Kendall?s ?
and the average displacement of these small parts
of modified sentences are, respectively, 0.027 and
3.5. These numbers are striking for two reasons:
first, English and German have in general quite
different word order, thus our experimental con-
dition should be somehow similar to the English-
Japanese scenario studied in (Neubig et al, 2012);
second, since the model is able to perform pre-
ordering basically at any distance, it is surprising
that a large part of the data remains unmodified.
6 Artificial Text generation with SOUL
While the context size for BOLMs is limited (usu-
ally up to 4-grams) because of sparsity issues,
NNLMs can efficiently handle larger contexts up
to 10-grams without a prohibitive increase of the
overall number of parameters (see for instance the
study in (Le et al, 2012b)). However the major
bottleneck of NNLMs is the computation cost dur-
ing both training and inference. In fact, the pro-
hibitive inference time usually implies to resort to
a two-pass approach: the first pass uses a conven-
tional BOLM to produce a k-best list (the k most
likely translations); in the second pass, the prob-
ability of a NNLM is computed for each hypoth-
esis, which is then added as a new feature before
the k-best list is reranked. Note that to produce the
k-best list, the decoder uses a beam search strategy
to prune the search space. Crucially, this pruning
does not use the NNLMs scores and results in po-
tentially sub-optimal k-best-lists.
6.1 Sampling texts with SOUL
In language modeling, a language is represented
by a corpus that is approximated by a n-gram
model. Following (Sutskever et al, 2011; Deoras
et al, 2013), we propose an additionnal approxi-
mation to allow a tighter integration of the NNLM:
a 10-gram NNLM is first estimated on the training
corpus; texts then are sampled from this model to
create an artificial training corpus; finally, this arti-
ficial corpus is approximated by a 4-gram BOLM.
The training procedure for the SOUL NNLM is
the same as the one described in (Le et al, 2012c).
To sample a sentence from the SOUL model, first
the sentence length is randomly drawn from the
empirical distribution, then each word of the sen-
tence is sampled from the 10-gram distribution es-
timated with the SOUL model.
The convergence of this sampling strategy can
be evaluated by monitoring the perplexity evolu-
tion vs. the number of sentences that are gener-
ated. Figure 2 depicts this evolution by measuring
perplexity on the nt08 set with a step size of 400M
sampled sentences. The baseline BOLM (std) is
estimated on all the available training data that
consist of approximately 300M of running words.
We can observe that the perplexity of the BOLM
estimated on sampled texts (generated texts) de-
creases when the number of sample sentences in-
creases, and tends to reach slowly the perplex-
ity of the baseline BOLM. Moreover, when both
BOLMs are interpolated, an even lower perplex-
ity is obtained, which further decreases with the
amount of sampled training texts.
6.2 Translation results
Experiments are run for translation into German,
which lacks a GigaWord corpus. An artificial cor-
pus containing 3 billions of running words is first
generated as described in Section 6.1. This corpus
is used to estimate a BOLM with standard settings,
that is then used for decoding, thereby approxi-
mating the use of a NNLM during the first pass.
Results reported in Table 4 show that adding gen-
erated texts improves the BLEU scores even when
the SOUL model is added in a rescoring step. Also
note that using the LM trained on the sampled cor-
pus yields the same BLEU score that using the
standard LM.
66
 190 200 210 220 230 240 250 260
 270 280
 2  4  6  8  10  12ppx times 400M sampled sentences
artificial textsartificial texts+stdstd
Figure 2: Perplexity measured on nt08 with the
baseline LM (std), with the LM estimated on the
sampled texts (generated texts), and with the inter-
polation of both.
Therefore, to translate from English to German,
the submitted system includes three BOLMs: one
trained on all the monolingual data, one on artifi-
cial texts and a third one that uses the freely avail-
able deWack corpus3 (1.7 billion words).
target LM BLEU
dev nt09 test nt10
base 15.3 16.5
+genText 15.5 16.8
+SOUL 16.4 17.6
+genText+SOUL 16.5 17.8
Table 4: Impact of the use of sampled texts.
7 Different tunings for different original
languages
As shown by Lembersky et al (2012), the original
language of a text can have a significant impact on
translation performance. In this section, this effect
is assessed on the French to English translation
task. Training one SMT system per original lan-
guage is impractical, since the required informa-
tion is not available for most of parallel corpora.
However, metadata provided by the WMT evalua-
tion allows us to split the development and test sets
according to the original language of the text. To
ensure a sufficient amount of texts for each con-
dition, we used the concatenation of newstest cor-
pora for the years 2008, 2009, 2011, and 2012,
leaving nt10 for testing purposes.
Five different development sets have been cre-
ated to tune five different systems. Experimental
results are reported in Table 7 and show a drastic
3http://wacky.sslmit.unibo.it/doku.php
baseline adapted
original language tuning
cz 22.31 23.83
en 36.41 39.21
fr 31.61 32.41
de 18.46 18.49
es 30.17 29.34
all 29.43 30.12
Table 5: BLEU scores for the French-to-English
translation task measured on nt10 with systems
tuned on development sets selected according to
their original language (adapted tuning).
improvement in terms of BLEU score when trans-
lating back to the original English and a significant
increase for original text in Czech and French. In
this year?s evaluation, Russian was introduced as
a new language, so for sentences originally in this
language, the baseline system was used. This sys-
tem is used as our primary submission to the eval-
uation, with additional SOUL rescoring step.
8 Conclusion
In this paper, we have described our submis-
sions to the translation task of WMT?13 for
the French-English, German-English and Spanish-
English language pairs. Similarly to last year?s
systems, our main submissions use n-code, and
continuous space models are introduced in a post-
processing step, both for translation and target lan-
guage modeling. To translate from English to
German, we showed a slight improvement with
a tighter integration of the continuous space lan-
guage model using a text sampling strategy. Ex-
periments with pre-ordering were disappointing,
and the reasons for this failure need to be better
understood. We also explored the impact of using
different tuning sets according to the original lan-
guage of the text to be translated. Even though the
gain vanishes when adding the SOUL model in a
post-processing step, it should be noted that due to
time limitation this second step was not tuned ac-
cordingly to the original language. We therefore
plan to assess the impact of using different tuning
sets on the post-processing step.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
67
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard Un iversity.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Marin?o.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, and
Kenneth Church. 2013. Approximate inference: A
sampling based modeling technique to capture com-
plex dependencies in a language model. Speech
Communication, 55(1):162 ? 177.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012b. Measuring the influence of long range de-
pendencies with neural network language models.
In Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages 1?
10, Montre?al, Canada.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012c. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012. Language models for machine trans-
lation: Original vs. translated texts. Comput. Lin-
guist., 38(4):799?825, December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Proceed-
ings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, AMTA
?02, pages 135?144, Tiburon, CA, USA. Springer-
Verlag.
68
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02:
Proc. of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318. Associ-
ation for Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. of Interna-
tional Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ?11, pages
1017?1024, New York, NY, USA, June. ACM.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
69
