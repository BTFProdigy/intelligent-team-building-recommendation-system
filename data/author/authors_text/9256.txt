Minimizing Word Error Rate in 
Textual Summaries of Spoken Language 
Klaus Zechner and A lex Waibel 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213, USA 
(zechner ,waibel}@cs. cmu. edu 
Abstract 
Automatic generation of text summaries for spoken 
language faces the problem of containing incorrect 
words and passages due to speech recognition er- 
rors. This paper describes comparative experiments 
where passages with higher speech recognizer confi- 
dence scores are favored in the ranking process. Re- 
sults show that a relative word error rate reduction 
of over 10% can be achieved while at the same time 
the accuracy of the summary improves markedly. 
1 Introduction 
The amount of audio data on-line has been grow- 
ing rapidly in recent years, and so methods for ef- 
ficiently indexing and retrieving non-textual infor- 
mation have become increasingly important (see, 
e.g., the TREC-7 branch for "Spoken Document Re- 
trieval" (Garofolo et al, 1999)). 
One way of compressing audio information is the  
automatic creation of textual summaries which can 
be skimmed much faster and stored much more effi- 
ciently than the audio itself. There has been plenty 
of research in the area of summarizing written lan- 
guage (see (Mani and Maybury, 1999) for a compre- 
hensive overview). So far, however, very little atten- 
tion has been given to the question how to create 
and evaluate a summary of spoken audio based on 
automatically generated transcripts from a speech 
recognizer. One fundamental problem with those 
summaries i that they contain incorrectly recog- 
nized words, i.e., the original text is to some extent 
"distorted". 
Several research groups have developed interac- 
tive "browsing" tools, where audio (and possibly 
video) can be accessed together with various types 
of textual information (transcripts, ummaries) via a 
graphical user interface (Waibel et al, 1998; Valenza 
et al, 1999; Hirschberg et al, 1999). With these 
tools, the problem of misrecognitions is alleviated 
in the sense that the user can always easily listen 
to the audio recording corresponding to a passage 
in a textual summary. In some instances, however, 
this approach may not be feasible or too expensive 
to pursue, and a short, stand-alone textual repre- 
sentation of the spoken audio may be preferred or 
even required. This paper addresses in particular 
this latter case and (a) explores means of making 
textual summaries less distorted (i.e., reducing their 
word error rate (WElt)), and (b) assesses how the 
accuracy of the summaries changes when methods 
for word error rate reduction ar e applied. Summary 
accuracy will be a function of how much relevant 
information is present in the sun'mary. 
Our results from experiments on four television 
shows with multiple speakers how that it is possi- 
ble to reduce word error rate while at the same time 
also improving the accuracy of the summary. Fur- 
thermore, this paper presents a novel method for 
evaluation of textual summaries from spoken lan- 
guage data. . . . .  
The paper is organized as follows: In the next : 
section, we review related work on spoken language 
summarization. In section 3 we describe our sum: 
marizer. Next, we present and discuss our proposal 
for an audio summarization evaluation metric (sec- 
tion 4). In section 5 we describe the Corpus that we 
use for our experiments and how i t  was annotated. 
Sections 6 and 7 describe xperixnents on both hu .... 
man and machine generated transcripts of the audio 
data. Finally, we discuss and summarize the results 
in sections 8 and 9. 
2 Related work ? " 
(Waibel et al, 1998) report results of their sum- 
marization system on automatically transcribed 
SWITCHBOARD data (Godfrey et al, 1992), the word 
error rate being about 30%. In a question-answer 
test with summaries offive dialogues, ubjects could 
identify most of the key concepts using a summary 
size of only five turns. However, the results vary 
widely across five different dialogues tested in this 
experiment (between 20% and 90% accuracy). 
(Valenza et al, 1999) went one step further and 
report that they were able to reduce the word error 
rate in summaries (as opposed to full texts) by using 
speech recognizer confidence scores. They combined 
inverse frequency weights with confidence scores for 
each recognized word. Using summaries composed of 
186 
one 30-gram per minute (approximately 15% length 
of the full text), the WER dropped from 25% for 
the full text to 10% for these summaries. They also 
conducted a qualitative study where human subjects 
were given summaries of n-grams of different length 
and also summaries with speaker utterances as min- 
imal units, either giving a high weight o the inverse 
frequency scores or to the confidence scores. The 
utterance summaries were considered best, followed 
closely by 30-gram summaries, both using high con- 
fidence score weights. This suggests that not only 
does the WER drop by extracting passages that are 
more likely to be correctly recognized but also do 
summaries seem to be "better" which are generated 
that way. 
While the results of (Valenza et al, 1999) are in- 
dicative for their approach, we want to investigate 
the benefits of using speech recognizer confidence 
scores in more detail and particularly find out about 
the trade-off between WER and summarization ac- 
curacy when we vary the influence of the confidence 
scores. To our knowledge, this paper addresses this 
trade-off for the first time in a clear, numerically de- 
scribable way. To be able to obtain numerical values 
for summary accuracy, we had our corpus annotated 
for relevance (section 5) and devised an evaluation 
scheme that allows the calculation of summary ac- 
curacy for both human and machine generated tran- 
scripts (section 4). 
3 Summar izat ion  sys tem 
Prior to summarizing, the input text is cleaned up 
for disfluencies, such as hesitations, filled pauses, 
and repetitions. I In the context of multi-topical 
recordings we use for our experiments, summaries 
are generated for each topical segment separately. 
The segment boundaries were determined to be at 
those places where the majority Cat least half) of the 
human annotators agreed (see section 5). Intercoder 
agreement for topical boundaries i fairly good (and 
higher than the agreement on relevant words or pas- 
sages).2 
To determine the content of the summaries, we 
use a "maximal marginal relevance" (MMR) based 
summarizer with speaker turns as minimal units (cf. 
(Carbonell and Goldstein, 1998)). 
The MMR formula is given in equation 1. It gen- 
erates a list of turns ranked by their relevance and 
states that the next turn to be put in this ranked 
list will be taken from the turns which were not yet 
ranked (tar) and has the following properties: it is 
(a) maximally similar to a "query" and (b) max- 
imally dissimilar to the turns which were already 
1 More details about this component and other parts of the 
summarization system can be found in (Zechner and Walbei, 
20oo). 
2For details ee (Zechner, 2000).  
ranked (tr). As "query" we use a frequency vector 
for all content words within a topical segment. The 
A-parameter (0.0 < A < 1.0) is used to trade off the 
influence of C a) vs. (b). 
Both similarity metrics (sire1, sire2) are inner 
vector products of (_stemmed) term frequencies (see 
equations 2 to 4); tft is a vector of stem frequencies 
in a turn; f ,  are in-segment frequencies of a stem; 
f ,  rna= are maximal segment frequencies of any stem 
in the topical segment, sirnl can be normalized or 
not. The formulae for tfa (equation 4) are inspired 
from Cornell's SMART system (Salton, 1971); we 
will call these parameters "smax', "log", and ?'freq", 
respectively. 
neztturn = argmax(Asima(tn,j,query) 
tnr,~ 
- (1 - A) maxsim2 (tnrd, tr,t~)) (1) 
tr ,  k 
siml : tf~tft or \[tf=tit~ (2) 
I / I  -I 
tftztft~ 
I I I  I 
tfi,, = 0.5 + 0.5 /~" or 1 + logfi., 
or ~,, (4) 
Using the MMR algorithm, we obtain a list of 
ranked turns for each topical segment. We com- 
pute this both for human and machine generated 
transcripts of the audio files ("reference text" vs. 
"hypothesis text") .3 
4 Eva luat ion  metr i cs  
The challenge of devising a meaningful evaluation 
metric for the task of audio summarization is that 
it has to be applicable to both the reference (hu- 
man transcript) and the hypothesis transcripts (au- 
tomatic speech recognizer (ASR) transcripts). We 
want to be able to assess the quality of the sum- 
mary with respect to the relevance markings of the 
human annotators (see section 5), as well as to re- 
late this "summary accuracy" to the word error rate 
present in the ASR transcripts. 
The approach we take is to align the words in the 
summary with the words in the reference transcript 
(wa). For ASR transcripts, word substitutions are 
aligned with their "true original" and word inser- 
tions are aligned with a NIL dummy. That way, 
3The human reference is cons idered  to be an "optimal" or 
"ideal" rendering of the words which were actually said in a 
conversation. Human transcription errors do occur, but are 
marginal and hence ignored in the context of this paper. 
187 
we can determine for each individual word wa in 
the summary (a) whether it occurs in a "relevant 
phrase" and (b) whether it is correctly recognized 
or a recognition error (for ASR transcripts). 
We define word error rate as WER = (S + 
I + D) / (S  + I + C) (I=insertion, D=deletion, 
S=substitution, C=correct). 
Each word's relevance score r is the average num- 
ber it occurs in the human annotators' relevant 
phrases (0.0 < r <_. 1.0). Relevance scores for in- 
sertions and substitutions are always 0.0. 
We choose to define the summary accuracy sa 
("relevance") as the sum of relevance scores of all 
n aligned words ~--~? r~, divided by the maximum 
achievable relevance score with the same number of 
n words somewhere in the text (i.e., 0.0 < sa <_ 1.0). 
Word deletions obviously do not show up in the sum- 
mary, but are accounted for, as well, to make the 
WER computation sound. 
To better illustrate how these metrics work, we 
demonstrate them on a simplified example of only 
two speaker turns (Figure 1). The first line repre- 
sents the relevance score r for each word (the number 
this word was within a "relevant phrase" divided by 
the number of annotators for that text). In turn 1, 
"this is to illustrate" was only marked relevant by 
two annotators, whereas "the idea" by 3 out of 4 
annotators. The second line provides the reference 
transcript, the third line the ASB. transcript. Line 
4 gives the type of word error, and line 5 the con- 
fidence score of the speech recognizer (between 0.0 
and 1.0, 1.0 meaning maximal confidence). 
Now let us assume that turn 2 shows up in the 
summary. The scores are computed as follows: 
? When summarizing the reference: Here, the 
word error rate is trivially 0.0; the summary 
accuracy sa is the sum of all relevance scores 
(-6.0) divided by the maximal achievable score 
with the same number of words (n = 7). l"hrn 2 
has 6 words which were marked relevant by all 
coders (r -- 1.0), turn l 's highest score is r = 
0.75. Therefore: sa2 = 6.0/(6.0 + 0.75) = 0.89. 
This is higher than the summary accuracy for 
turn 1: sal = 3.5/6.0 = 0.58(n = 6). 
? When summarizing the ASR transcript ("hy- 
pothesis"): Selecting turn 2 will give sa2 = 
0.02.25 = 0.0 (n = 5) .  For turn 1, sal = 
2.25/(0.75 + 0.5 + 0.5 + 0.5 + 0.0 + 0.0) = 1.0 
(n = 6; the sum in the denominator can only use 
relevance scores based on the aligned words wa 
which were correctly recognized, therefore the 
1.0-scores in turn 2 cannot be used). Turn 2 has 
WER=6\ [5=l .2 ,  turn 1 has WER=3/6=0.5 .  
Obviously, when summarizing the ASB. output, we 
would rather have turn 1 showing up in the summary 
than turn 2, because turn 2 is completely off from 
the truth and turn 1 only partially. The fact that 
turn 2 was considered to be more relevant by human 
coders cannot, in our opinion, be used to favor its 
inclusion in the summary. An exception would be 
a situation where the user has immediate access to 
the audio as well and is able to listen to selected 
passages from the summary (see section 1). In our 
case, where we focus on text-only summaries to be 
used stand-alone, we have to minimize their word 
error rate. 
Given that, turn 1 has to be favored over turn 2, 
both because of its lower WEB, and because of its 
higher accuracy with respect o the relevance anno- 
tations. 
In order to increase the likelihood that turns 
with lower WEB, are selected over turns with higher 
WEB., we make use of the speech recognizer's con- 
fidence scores which are attached to every word hy- 
pothesis and can be viewed as probabilities: they are 
in \[0.0,1.0\], high values reflecting a high confidence 
in the correctness of the respective word. 4 Follow- 
ing (Valenza et al, 1999) we conjecture that we can 
use these confidence scores to increase the probabil- 
ity of passages with lower WEB, to show up in the 
summary. To test how far this assumption is justi- 
fied, we correlated the WEB. with various metrics of 
confidence scores: (i) sum of scores, (ii) average of 
scores, (iii) number of scores above a threshold, (iv) 
the latter normalized by the number of all scores, 
and (v) the geometric mean of scores. Table 1 shows 
the correlation coefficients (Pearson r) for the four 
ASK transcripts we used in our experiments ( ee sec- 
tion 5). To prevent he influence of large differences 
in turn length, those computations were done for 
subsequent "buckets" of 50 words each. 
Since in most cases we achieve the highest corre- 
lation coefficient (absolute value) for method (iv = 
avgth) (average number of words whose confidence 
score is greater than a threshold of 0.95), we apply 
this metric to the computation ofturn-query similar- 
ities (sire1 in equation 1). We use the two following 
formulae to adjust the similarity,scores. (We shall 
call these adjustments MULT and EXP in the follow- 
ins.) 
\[mutt\] sirn~ = Siml (1 + aavgth) (5) 
\[ezp\] sim'l' = s imlavgth ~ (6) 
For both equations it holds that if a = 0.0, the 
scores don't change, whereas if c~ > 0.0, we en- 
hance the weights of turns with many high confi- 
dence scores ("boosting") and hence increase their 
likelihood of showing up earlier in the summary. 5 
Even though our evaluation method looks like it 
would "guarantee" an increase in summary accu- 
4The speech recognizer computes  these scores based on the 
acoustic stability of words during lattice rescoring. 
5For EXP, we define 0 ? ---- O. 
188 
TURN 
re1: 
REF: 
HYP: 
e r r :  
con:  
TURN 
re l  : 
REF: 
HYP: 
err: 
con:  
1: 
0.5 0.5 0.5 0.5 0.75 0.75 *** 
this is to illustrate the idea *** 
this is to  ILLUMINATE *** idea 
C C C S D C I 
1 1 1 0 .9  - 0 .8  0 .8  
2: 
0 1 1 1 1 1 1 
and here we have very  re levant  in fo rmat ion  
and HE ** BEHAVES **** IRREVERENT FORMATION 
C S D S D S S 
0.8  0 .7  - 0 .8  - 0 .8  0 .9  
Figure 1: Simplified example of two turns (for score computation) 
BACK 
(i} sum -0.43 
(ii) average -0.53 
Off) scores > 0.95 -0.55 
( iv) normal i zed  (ii i) -0.58 
(v) geometric mean -0.53 
19CENT BUCHANAN 
-0.51 -0.12 
-0.52 -0.43 
-0.48 -0.35 
-0.48 -0.48 
-0.53 -0.42 
GRAY 
-0.03 
-0.42 
-0.25 
-0.44 
-0.38 
Table 1: Pearson r correlation between WER and confidence scores 
racy when the word error rate is reduced, this is 
not necessarily the case. For example, it could turn 
out that while we can reduce WER by "boosting" 
passages with higher confidence scores, those pas- 
sages might have (much) fewer words marked rele- 
vant than those being present in the summary with- ~ 
out boosting. This way, it would be conceivable to 
create low word error summaries that contain also 
very few relevant pieces of information. However, as 
we will see later, WER reduction goes hand in hand 
with an increase of summary accuracy. 
5 Data  character i s t i cs  and  
annotat ion  
Table 2 describes the main features of the corpus we 
used for our experiments: we selected four audio ex- 
cerpts from four television shows, together with hu- 
man generated textual transcripts. All these shows 
are conversations between multiple speakers. The 
audio was sampled at 16kHz and then also automat- 
ically transcribed using a gender independent, vo- 
cal tract length normalized, large vocabulary speech 
recognizer which was trained on about 80 hours of 
Broadcast News data (Yu et al, 1999). The average 
word error rates for our 4 recordings ranged from 
25% to 50%. 
The reference transcripts of the four recordings 
were given to six human annotators who had to seg- 
ment them into topically coherent regions and to de- 
cide on the "most relevant phrases" to be included 
in a summary for each topical region. Those phrases 
usually do not coincide exactly with speaker turns 
and the annotators were encouraged to mark sec- 
tions of text freely such that they would form mean- 
ingful, concise, and informative phrases. Three an- ? 
notators could listen to the audio while annotat- 
ing the corpus, the other three only had the hu- 
man generated transcripts available. 2 of the 6 an- 
notators only finished the NewsHour data, so we 
have the opinion of 4 annotators for the recordings 
BUCHANAN and GRAY and of 6 annotators for BACK 
and 19CENT. 
6 Exper iments  on  human generated  
t ranscr ip ts  
We created summaries of the reference transcripts 
using different parameters for the MMR computa- 
tion: For tf  we used "freq", "log", and "smax"; fur- 
ther, we did or did not normalize these weights; fi- 
nally, we varied the MMR-A from 0.85 to 1.0. Sum- 
marization accuracy was determined at 5%, 10%, 
15%, 20%, and 25% of the text length of each sum- 
marized topical segment and then averaged over all 
sample points in all segments. Since these were 
word-based lengths, words were added incrementally 
to the summary in the order of the turns ranked via 
MMR; turns were cut off when the length limit was 
reached. As explained in the example in section 4, 
the accuracy score is defined as the fraction of the 
sum of all individual word relevance scores (as de- 
189 
TV show 
number of speakers 
speaker turns 
words in transcript 
length in minutes 
topical segments 
word error rate (in %) 
BACK 19CENT BUCHANAN GR.AY 
NewsHour 
5 
24 
1216 
8.6 
4 
25.6 
NewsHour 
2 
27 
1281 
8.6 
4 
32.6 
Crossfire 
4 
69 
3252 
17.3 
4 
32.5 
Table 2: Characteristics of the corpus 
Crossfire 
5 
7O 
2205 
11.9 
3 
49.8 
i 119CE T I BUCH*NAN I O A* I av0*a'? I 0,  
0.533 0.596 0.513 0.443 0.522 
Table 3: Reference summarization accuracy of MMR o~ 
summaries 
termined by human annotators) over the maximum 
possible score given the current number of words in 
the summary. 
Table 3 shows the summary accuracy results for 
the best parameter setting (if=log, no normaliza- 
tion) ~. 
7 Exper iments  on automat ica l ly  
generated  t ranscr ip ts  
Using the same summarizer as before, we now cre- 
ated summaries from ASR transcripts. Addition- 
ally to the summary accuracy, we evaluate now also 
the WER for each evaluation point. Again, we ran 
a series of experiments for different parameters of 
the MMR formula (if=log, smax, freq; with/without 
normalization). As before, we achieved the best re- 
sults for non normalized scores and tf=log. We var- 
ied a from 0.0 to 10.0 to see how much of an effect we 
would get from the "boosting" of turns with many 
high confidence scores (see equations 5 and 6). 
The ExP formula yielded better esults than MULl? 
(Table 4), the optimum for ExP was reached for 
= 3.0 with a WER of 26.6%, an absolute improve- 
ment of over 8% over the average of WER=35.1% 
for the complete ASR transcripts (non-summarized). 
The summarization accuracy peaks at 0.47, a 9% 
absolute improvement over the a = 0.0-baseline and 
only about 5% absolute lower than for reference sum- 
maries (Table 4 and Figure 2). 
When we compare the baseline of ~ = 0.0 (i.e., no 
"boosting" of high confidence turns) to the best re- 
sult (a = 3.0), we see that the WER drops markedly 
by about 12% relative from 30.1 to 26.6%. At the 
same time, the summarization accuracy increases by 
about 18% relative form 0.401 to 0.472. 
? I f  we use non-normal ized scores, the value of the MMR-X 
does not  have any measurab le  effect; we assigned it to be 0.95 
for all subsequent  experiments. 
0.$ 
0.25 
Summary accuracy vL Word mror rate 
FgXP Ioenula 
' ' i 0"2.35 0.4 0.45 0 5 0.65 
=unvnan/accuracy 
Figure 2: Summary accuracy vs. word error rates 
with ~.xP boosting (0 < a < 7) 
Results for the MULT formula confirm this trend, 
but it is considerably weaker: approximately 6% 
WER reduction and 14% accuracy improvement for 
c~ = 10.0 over the c~ = 0.0 baseline. 
An appendix (section 11) provides an example of 
actual summaries generated by our system for the 
first topical segment of the BACK conversation. It 
illustrates how WER reduction and summary ac- 
curacy improvement can be achieved by using our 
confidence boosting method. 
8 D iscuss ion  
The most significant result of our experiments i , 
in our opinion, the fact that the trade-off between 
word and summary accuracy indeed leads to an op- 
timal parameter setting for the creation of textual 
summaries for spoken language (Figure 2). Using 
a formula which emphasizes turns containing many 
high confidence scores leads to an average WER re- 
duction of over 10% and to an average improvement 
in summary accuracy of over 15%, compared to the 
baseline of a standard MMR-based summary. 
Comparing our results to those reported in 
(Valenza et al, 1999), we find that their relative 
190 
a = 0~0 
P.xP (o = 3.0) 
MOLT (0 = 10.0) 
BACK 19CENT 
acc I WER acc I WER 
0.411 26 .2  0.501 26.7 
0.648 18.8 0.501 26.7 
0.575 21.5 0.501 26.7 
BUCHANAN G RAY average  
acc WER acc WER ace \] WER 
0.412 30 .6  0.280 36.9 0.401 30.1 
0.444 26.9 0.296 34.0 0.472 26.6 
0.429 29.6 0.317 35.7 0.456 28.3 
Table 4: Effect of a on summary accuracy vs. WER (in %) transcripts with ExP and MULl" boosting methods 
I IBm?K\[ 19C NT BUOHA ANIO AYI 
avgth -0.79 -0.11 -0.43 -0.03 
Table 5: Correlation between WER and confidence 
scores on a turn basis 
WER reduction for summaries over full texts was 
considerably arger than ours (60% vs. 24%). We 
conjecture that reasons for this may be due to the 
different nature and quality of the confidence scores, 
and (not unrelated), to the different absolute WER 
of the two corpora (25% vs. 35%): in transcripts 
with higher WER, the confidence scores are usually 
less reliable (eft Table 1). 
Looking at the four audio recordings individually, 
we see that the improvements vary strongly across 
different recordings. We conjecture that one reason 
for this fact may be due to the high variation in 
the correlation between WER and confidence scores 
on a turn basis (Table 5). This would explain why, 
e.g., BACK'S improvements are much stronger than 
those of the BUCHANAN recording or why there are 
no improvements for the 19CENT recording. How- 
ever, GRAY does improve despite its very low abso- 
lute correlation. 
9 Summary  
In this paper, we presented experiments on sum- 
maries of both human and machine generated tran- 
scripts from four recordings of spoken language. We 
explored the trade-off of word accuracy vs. summary 
accuracy (relevance) using speech recognizer confi- 
dence scores to rank passages with lower word error 
rate higher in the summarization process. 
Results comparing our approach to a simple MMR 
ranking show that while the WER can be reduced by 
over 10%, summarization accuracy improves by over 
15% as measured against ranscripts with relevance 
annotations. 
10 Acknowledgements  
We thank the six human annotators for their tedious 
work of annotating the corpus with topical segment 
boundaries and relevance information. We also want 
to thank Alon Lavie and the three anonymous re- 
viewers for useful feedback and comments on earlier 
drafts of this paper. 
This work was funded in part by ATR - Inter- 
preting Telecommunications Re earch Laboratories 
of Japan, and the US Department ofDefense. 
Re ferences  
Jaime Carbonell and Jade Goldstein. 1998. The use 
of MMR, diversity-based reranking for reordering 
documents and producing summaries. In Proceed- 
ings of the ~Ist A CM-SIGIR International Con- 
ference on Research and Development in Informa- 
tion Retrieval, Melbourne, Australia. 
John S. Garofolo, Ellen M. Voorhees, Cedric G. P. 
Auzanue, and Vincent M. Stanford. 1999. Spoken 
document retrieval: 1998 evaluation and investi- 
gation of new metrics. In Proceedings of the ESCA 
workshop: Accessing information in spoken audio, 
pages 1-7. Cambridge, OK, April. 
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 
1992. SWITCHBOARD: telephone speech corpus 
for research and development. In Proceedings of 
the ICASSP-9~, volume 1, pages 517-520. 
Julia Hirschberg, Steve Whittaker, Don Hindle, Fer- 
nando Pereira, and Amit Singhal. 1999. Finding 
information i  audio: A new paradigm for audio 
browsing/retrieval. In Proceedings of the ESCA 
workshop: Accessing information in spoken audio, 
pages 117-122. Cambridge, OK, April. 
Inderjeet Mani and Mark T. Maybury, editors. 1999. 
Advances in automatic text summarization. MIT 
Press, Cambridge, MA. 
Gerard Salton, editor. 1971. The SMART Retrieval 
System -- Experiments in Automatic Text Pro- 
cessing. Prentice Hall, Englewood Cliffs, New Jer- 
sey. 
Robin Valenza, Tony Robinson, Marianne Hickey, 
and Roger Tucker. 1999. Summarisation f spo- 
ken audio through information extraction. In Pro- 
ceedings of the ESCA workshop: Accessing in- 
formation in spoken audio, pages 111-116. Cam- 
bridge, OK, April. 
Alex Waibel, Michael Bett, and Michael Finke. 
1998. Meeting browser: Tracking and summa- 
rizing meetings. In Proceedings of the DARPA 
Broadcast News Workshop. 
Hua Yu, Michael Finke, and Alex Waibel. 1999. 
Progress in automatic meeting transcription. 
In Proceedings of EUROSPEECH-99, Budapest, 
Hungary, September. 
191 
a relative sa WEB. in % turns in summary 
0.0 0.428 29.2 2, 1 \[beginning\] 
3.0 0.885 11.8 1, 5\[beginning\] 
Table 6: Relative summary accuracy, WER, and se- 
lected turns by the summarizer for (a) no boosting 
and (b) P.XP boosting. 
higher WER scores, case (b) (0 = 3.0) successfully 
ranks turn 1 first due to its higher confidence scores 
and hence both summary accuracy and WElt scores 
improve. 
turn avg. relevance score 
1 0.663 
2 0.369 
3 0.149 
4 0.212 
5 0.274 
WERin % avgth. 
9.5 0.84 
27.5 0.40 
26.9 0.39 
11.1 0.08 
27.7 0.17 
Table 7: Average relevance scores, WER, and confi- 
dence values for the five turns of BACK'S first topical 
segment. 
Klaus Zechner and Alex Waibel. 2000. Dia- 
summ: Flexible summarization of spontaneous 
dialogues in unrestricted domains. Available from 
http://www.cs.cmu.edu/?zechner/publications.html. 
Klaus Zechner. 2000. A word-based annota- 
tion and evaluation scheme for summariza- 
tion of spontaneous speech. Available from 
http://www .cs.cmu.edu/~zechner/publications.html. 
11 Append ix :  Example  summar ies  
This appendix provides summaries for the first topi- 
cal segment of the BACK conversation. The contents 
of this conversation revolves around former Illinois 
congressman Dan Rostenkowski who had been re- 
leased from prison and was ready to re-enter public 
life. 
Figure 3 shows the human transcript of this seg- 
ment which is about two minutes long and con- 
sists of 5 speaker turns. Figure 4 contrasts the 
machine generated summaries for this segment (a) 
without confidence boosting (a -- 0.0) and (b) using 
the optimal confidence boosting (c~ = 3.0, method 
ExP). Insertions and substitutions are capitalized 
and marked with I- or S- prefixes. Table 6 compares 
the relative summary accuracies ( a) and word error 
rates (WER in %) for these two summaries (aver- 
age over the 5 sample points from 5% to 25% sum- 
mary length). Additionally, the turns that show up 
in the summaries are listed in their ranking order. 
Table 7 provides the average relevance scores, word 
error rates, and confidence scores ("avgth") for each 
turn of this topical segment. 
We observe that the most relevant urn is turn 
1 which has, incidentally, also the lowest WER. 
Whereas in case (a) (o = 0.0), turn 2 is ranked 
first and therefore dominates the lower relevance and 
192 
1 e l i zabeth :  i t  has been e ight  months s ince  dan roetenkowsk i  ua lked  out  o f  a u i scons in  federa l  
p r i son  n ix  months s ince  he le f t  a ha l fuay  house in  ch icago  the  fo rmer  chai rman of  the  house ways and means 
committee i s  ready  to  s tep  back in to  the  pub l i c  eye 
2 e l i zabeth :  the  recept ion  was-warm the  banquet  ha l l  packed w i th  the  c i ty ' s  movers and shakers  
the  th i r ty  f i ve  do l la rs  a p la te  inv i ta t ion  re fer red  to  ros tenkowsk i  an mr. chai rman ros tenkowsk i  
made no re ference  to h i s  conv ic t ion  fo r  misus ing  federa l  funds  on ly  a br ie f  re fe rence  to  h i s  f i f teen  
months of  p r i son  t ime 
3 dan:  i g raduated  from oxford  and i rea l ly  had a rhodes scho larsh ip  the  past  th ree  years  have been a 
constant ly  cha l leng ing  t ime fo r  me change never  comes eas i ly  and g iven  the  c i rcumstances  
o f  my s i tuat ion  that  has par t i cu le~r ly  t rue  fo r  me at  t imes  th ings  have been dosnr ight  b leak  and i 
uou ldn ' t  want to  wish my exper ience  on my uors t  enemy but  there  were some s i l ver  l in ings  i ' ve  had an 
oppor tun i ty  to  read and re f lec t  in  a bay that  uasn ' t  poss ib le  when i was 
in  constant  moment in  these  
remarks  today  i 'd  l i ke  to  share  some of  my conc lus ions  
4 e l i zabeth :  the  conc lus ions  d id 'not  due l l  on the  demise of  dan ros tenkuwsk i ' s  career  but  
the  demise o f  par ty  po l i t i cs  
5 dan:  those  who say that  the  pres ident ' s  po l i t i ca l  poser  has been ueakened by scanda l  have t ru ly  shor t  
memories the  sad fac t  i s  that  p res ident  c l in ton  has never  had a democrat i c  base in  congress  a group 
of  peop le  uhom one cou ld  suppor t  the  wh i te  house on any g iven  i ssue  are  not  there  
Figure 3: Human transcript of first topical segment (BACK) 
1 e l i zabeth :  hen been e ight  months s ince  dan ros tenkoesk i  ba lked  out  o f  
u i scons in  federa l  p r i son  I -~YBE 
2 e l i zabeth :  wan S-ALARMED the  banquet  ha l l  packed s i th  the  c i ty ' s  
S-CDM~O~S S-IH S-CHAHBERSS-UHICH th i r ty  f i ve  S-DOLLAR a p la te  
S - IN ITAT IO| re fer red  to  routenkoeek i  an S-MR. chai rman I-LET 1-HE I-ASK 
ros tenkousk imade no re ference  to  h i s  conv ic t ion  fo r  I -HIS S-USIHG federa l  
funds  on ly  a br ie f  re fe rence  to  S- IS  f i f teen  months o f  p r i son  t ime 
1 e l i zabeth :  has been e ight  months s ince  dan routenkoesk i  
ea lked  out  o f  s i scons in  federa l  p r i son  I-SAYBE s ix  months s ince  he le f t  
S-THE ha l fuay  house in  ch icago  the  fo rmer  chairman of  the  house says  and 
means committee ready  to  s tep  back in to  the  pub l i c  eye 
5 dan:  S-ALSO say  that  the  pres ident ' s  po l i t i ca lpower  has been eeakened by 
scanda l  S-RIGHT S-ESPECIALLY shor t  S-MEMORY S-THAT S-DISSATISFACTI0| that  
p res ident  c l in ton  has never  
Figure 4: Machine generated summaries for (a) ~ = 0.0 and (b) a = 3.0 (25% of text length) 
193 
DIASUMM: Flexible Summarizat ion of 
Spontaneous Dialogues in Unrestricted Domains 
Klaus Zechner and Alex Waibel 
LaJlguage 'l~chnologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsbm:gh, PA 115213, USA 
{zechner., waibel}@cs, cmu. edu 
Abstract 
In this paper, we present a summa.rization system 
for spontaneous dialogues which consists of a novel 
multi-stage architectm'e. It is specifically aimed at 
addressing issues related to tlle nature of the l;exts 
being spoken vs. written and being diMogical vs. 
monologica.l. The system is embedded in a. graph- 
ical user interface ~md was developed and tested on 
transcripts of recorded telephone conversations in 
English and Spanish (CAI,LHOMI,;). 
1 Introduct ion 
Summa.rization of written docmnents has recently 
O' been a. focus for much research in NI,t ~ ( ~.o., (Mani 
and 1Vlasq~ury , 1997; AAAI, 1998; Mani el. al., 1998; 
ACL, 2000), to nanle some of tile Inajol: events in 
this field ill the past few years). Ilowever, very lit- 
tle a.ttention has been given so far to the summa- 
riza.tion of spol, r('~n language, even less of conversa- 
lions vs. monologic'al texts. We believe tha.t sum- 
mariza.tion of speech will bccoJne increasingly more 
important, a.s the ~ml(mnt of online audio daLa. grows 
and demand for r~tl)id browsing, skimming, a.nd a.e- 
cess of speech data increases. Another application 
which particulm:ly pertains to our interest in spo-- 
ken dialogue summarization would be the generation 
of meeting minutes for archival purposes a.nd/or to 
update l)a.rticil)a.nts .joining a.t la.ter stages on qm' 
progress of the conversa.tion so far. 
Sunmmrization of dialogues within l imilcd do- 
mains ha.s been attempted within the context of 
the VERBMOBII, pl:ojcct ("protocol generation", 
(Alexandersson and Poller, 1998)) or by SRI's MIMI 
summarizer (Kameyama et ~d., 1996). l{ecent work 
on spoken language summarization i unrestricted 
domains has focused ahnost exclusively on Broad- 
cast News, mostly due to the spoken hmguage track 
of recent TREC evaluations (Oarofolo et al, 1997; 
Garotblo et al, 1999). (Waibel et a.1., 1(.)98) describe 
a Meeting Browser where summaries earl be gener- 
ated using technology established for written texts. 
(Va.lenza. el. M., 1999) go one step further and incof  
pora.te knowledge from the speech recognizer (con- 
fidence scores) into their summarization system, as 
well. 
We a.rgue that the nature of spoken dialogues, to- 
gether with their textual representations a speech 
recognizer hypotheses, requires a. set of specific al> 
proa.ches to make summarization feasible for this 
text genre. 
As a demonstrable proof of concept, we present 
the multi-stage a.rchitecture of the summa.rization 
system I)IASUMM which can flexibly deal with spo- 
ken di,dogues in English and Spa.nish, without any 
restrictions of domahl. Since it cannot rely on a.ny 
domain specific knowledge base, it uses shallow sta- 
tisticaJ approaches and presents (possibly modified) 
ca:lracts from the original text. as summa.ry. 
We. present results of several evaluations of our 
system using human transcripts of spontaneous tele- 
phone conversations in English and Spanish from the 
(~,AI,LIIOME corl)/ls ((LI)C), 1996), in particular the 
accura.cy of the topic segmentation and in\[brmat.ion 
condensing components (sections (5 and 7). Also, Ibr 
I.he purpose of a global evaluation, a user study was 
l~ei:%i:med which a.ddresscd in\[or\]nation access t.inJe 
a.nd a.ccura.ey of retaine.d information eompa.ring dif- 
ferent versions of summaries (section 10). 
This paper is organized as follows: In the next sec- 
tion, we provide, a.n ow;rview a.bout t}ie in,till issues 
Ibr summa.rization of Sl)oken dialogues and indicate 
I;hc "~l)l)roaches we, are taking in our system. We 
then present he system a.rchitecture (section 3), fol- 
lowed by a. detailed description of the readier building 
blocks (sections <1 to 8). After a. brief elmra.cteriza- 
tion of the (2 UI (section 9) we describe a user study 
for global system evaluation in section 10. We con- 
clude the pa.per with a smmnary and a brief outlook 
in section 11. 
2 Issues and Approaches: Overview 
In this section, we give a,n overview about the main 
issues that a.ny sunmmrizat;ion system for spoken di- 
a.logues has to address mid indica.te the approach we 
are taking for each of these in I)IASUMM. 
In a generM sense, when dealing with written 
texts, usually there is plenty of information avail- 
able which can be used lbr the purpose of summa- 
968 
rization, such as capitalization, i)un(-tuation ~narks, 
t,itles, passage head(rs, i)aragral)h boundaries, or 
other ,nark-ul)S. (hfforl.mud.ely, however, ,,onc (.)f 
this holds for :q)ccch data whh:h arrives as a stream 
of word l,ok('w; from ;I recognizer, (:ut iuto "utt(.q'- 
antes" by using a silence heuristi('. 
2.1. Lack of  clause. 1)Oulldaries 
One of the mosl. serious issues is the lack el senten(:e 
or clause boundaries in spoken dialogues whi(:h ix 
particularly problemati(: .;in(:e scnten(:es, clauses, or 
l)aragral)hs a.re (.:onsidercd the "minimal re,its" in 
virtually all existil,g summarizat ion systcu,s. \'Vheu 
humans speak, they so,lletillles pause durinq a 
(:\]a.use, and not always at. l.he eml of a claus(', whi(:h 
means that the outl)ut of a r(;coguizer (whi(:h us,t- 
ally uses some silelme-heuristics to cut the segments) 
frequently does nol real,eli Iogi(:al sep, l,en(:e or clause 
boundaries, l,ooking at five I';nglish (~A,,I,HOM,,: (li- 
alogues with an average ii/11111)(".1' of :{20 iltl\[.('3'a,l('.c~.q 
eat.h, we find on average 30 such "(:ontinuations" of 
logical clauses over automa.ti(:ally detcrmiued a(:ous- 
tit" segment I)ounda.ries. lu a smmnary,  this can 
cause a. r(;du(:tion in coh(,,ren(:c and r<~dability of 
the outlmt. 
We address this issue I)y linking adjac(;nt tm'ns 
of th(; smue sl)eaker together if the silence between 
them ix less than a given col,sl.\[/llt (se(;tioll d). 
2.2 Distr i lml ;c .d int ' (n 'mat io l l  
Siuce we have multi-pari,y conversations as o\])l)oscd 
to Inonologi('al texts, sonmtimcs the cru(:ial in\['or- 
matiou is found in a question-auswer-l)air , i.e., it 
involv('s more than oue Sl)eaker; extracting ouly the 
question or only the auswer wo,ld be meaningless 
in ma.ny cases. We found that on average about 
10% el' the speaker turns belong to such question- 
answer l)airs in five examined English (~AIA,IIOME 
dialogues. Often, either the question or the answer 
ix very shoI:t and does not contain any words with 
high relevan(:c. In order not to "lose" these short 
tutus at a later stage, when only the n~ost, relevant 
turns are extracted, we link them to the matching 
question/answer ahead of/. ime, using two different 
methods to detect questions aud their answers (sec- 
tion 4). 
2.3 D is t luent  speech 
Speech disfluencies in spontaneous convers,ttions - -  
such as fillers, repetitions, repairs, or unfinished 
clauses -- can make transcril)ts (and summary  ex- 
tracts) quite ha.rd to read and also introduce all tin- 
wanted bias to relevance computat ions (e.g., word 
repetitions would cause a higher word count tbr the 
repeated content words; words in untinished clauses 
would be included in the word count.) 
'l'o alleviate this problem, we employ a clean-up 
tilter pipeline, which eliminates liller words and ,:el)- 
el.it.ions, and segments the tm'ns into short clauses 
(sectiou 5). \Ve also remove incomplete clauses, typ- 
ically sentem:c-iuitial repairs, at this stage of our 
'.syst?lu. This "clea.niug-up" serves two main pur- 
1)oscs: (i) it. im:rea~cs tim readabilit3~ (for the fiually 
(;xtracl.cd segments); and (ii)it. ~nakcs the text more 
tractable by subsequent modules. 
The following exalnl)le com\])arcs a turn before and 
after t.he clean-up component:  
before: I MEAN WE LOSE WE LOSE I CAN'T I 
CAN'T DO ANYTHING ABOUT IT SO 
after: we lose / i can't do anything 
about it 
2.4 Lack of tel)i(" l)oundaries 
(;AI,I,IIOME s\])c'e(;h data is lll/llti-to\])ica\] I)tlt does 
uot include mark<q) \['or pa.ragral)hs, nor al,y tolfie- 
inforlJ,ative headers. Tyl)ically, we lind about 5 I0 
(.lilt'erent opics within a 10-mimd;e segment of a di-- 
ah)gue, i.e., the. topic changes about every 1 2 min- 
utes in these conversations. To facilitate browsing 
and smHtlmrization, we thus have to discover topi- 
(:ally coherent, segl,lents automatical ly.  This is done 
using a TextTi l ing approach, adapted t'ron~ (l\]earst, 
\]997) (section (i). 
2.5 Speech. reeog l f i zer  e r rors  
Imst but not least, we face t.he l)roblcm of iml)er- 
t'e(:t word a(:cura(:y of sl)eech recognizers, l)articu- 
larly when (h'.a~ling with Sl)OUl.a\]mous t)eech over a 
large vo(:al)uhu'y aud over a low I);mdwi(Ith (:hamJe\], 
SIIC\]I \[~S l,h(~ (',AI,I,IIOME ({at;tl)asc's which we Juainly 
used for develol)lnent , testing, and evaluatiou of our 
syste/n. (hu'r(mt recognizers tyl)ically exhibit word 
error rates \['or l,hese (:orl)ora ill the order of 50%. In 
I)IASUMM's hfl'ormation condensation component,  
the relevaucc weights of speaker ttlr,ls (:all be ad- 
justed to take into acc.omd, their word confidence 
scores from 1.111; sl)eech recognizer. That  way we can 
reduce the likelihood of extra.eting passages with a 
larger amount of word lnisreeognitions (Zeclmer and 
\Vaibel, 201111). lu this 1)aper, however, the focus will 
be exclusively on results of our evaluations on hu- 
man generated transcripts. No information from the 
speech recognizer nor from the acoustic signal (other 
than inter-utterance pause durations) are used. We 
are aware that in particular prosodic information 
may be of help for tasks such as the detection of 
sentence boundaries, speech acts, or topic bound- 
aries (l\]irschberg ~md Nakatani, 1998; Shriberg et 
al., 1998; Stolcke et al, 2000), but the investigation 
of the integration of this additional source of i n fer  
marion is beyond the scope of this pal)er and lel't tbr 
future work. 
3 System Arch i tec ture  
The global system architecture of I)IASUMM is a 
1)ipeline of the tbllowing lbur major components: 
969 
inputtor \] 
CLEAN ~ Turn Linking 
and TELE ! 
i 
\] Clean-up Filter 
! 
I i 
\] 
J 
input fo r .  Topic Segmentation 
TRANS 
i l 
Information Condensation ~ TRANS 
i 
L 
1 71-  - - - \]7 7 -  ~ CLEAN 
Telegraphic Reduction TELE 
Fignre 1: System architecture 
turn linking; clean-up filter; topic segmentation; and 
information condensation. A. fifth component is 
added a.t the end for the purpose of telegraphic re- 
duction, so that we can maximize the information 
content in a given amount of space. The system ar- 
chitecture is shown in Figure 1. It also indicates the 
three major types of smnmaries which can be gener- 
ated by l)Ia SUMM: 'P\]~ANS ("transcript"): not using 
the linking and clean-up components; CLEAN: ris- 
ing the main four components; 'I'EI,E ("telegraphic" 
summary): additionally, using the telegraphic reduc- 
tion component. 
The following sections describe the components of 
DIASUMM ill more detail. 
4 Turn  L ink ing  
The two main objectives of this component are: (i) 
to form turns which contain a set of full (and not 
partial) clauses; and (ii) to forln turn-pairs in cases 
where we have a question-answer pair in the dia- 
logue. 
To achieve the first objective, we scan the input for 
adjacent turns of one speaker and link them together 
if their time-stamp distance is below a pre-specified 
threshold 0. If the threshold is too small, we don't 
get most of the (logical) turn continuations across 
utterance boundaries, if it is too large, we run the 
risk of "skipping" over short but potentiMly relevant 
Daglnents of the speaker on the other channel. We 
experimented with thresholds between 0.0 and 2.0 
seconds and determined a local performance maxi- 
mum around 0 = 1..0. 
For the second objective, to form turn-pairs which 
comprise a question-answer information exchange 
between two dialogue participants, we need to detect 
wh- and yes-uo-questions i  the dialogue. We tested 
\] English \] Spanish 
Annotated l)ata 
turns 1603 1185 
Wh-questions /12 78 
yes-no-questions /t3 98 
questions total 85 (5.3%) 176 (14.9%) 
Automatic Detection Results (F1) 
SA classifier 
POS rules 
raudom baseline 
0.24 0.22 
0.22 0.37 
0.02 0.13 
Tahle 1: Q-A-pair distribution in the data and ex- 
pel'imental results for automatic Q-A-detection 
two approa.ches: (a) a I tMM based speech a.ct (SA) 
classifier (\]/Jes, \] 999) and (b) a set of part-of-speech 
(POS) based rules. The SA classifier was trained oll 
dialogues which were manually annotated for speech 
acts, using parts of the SWITCIIBOARI) corpus (God- 
frey et al, 1992) for Fmglish and CALLIIOMF, for 
Spanish. The corresponding answers for the de- 
tected questions were hypothesized in the first turn 
with a. different sl)eaker , following the question-turn. 
Table 1 shows the results of these experiments for 5 
English and 5 Spanish CAI,L\]IOME dialogues, corn- 
payed to a baseline of randomly assigning n question 
speech acts, n being the number of question-turns 
marked by human a.nnotal~ors. We report Fl-seores, 
where F1 - ~ with P=preeision and /g--recall. 
We note that while the results \[br the SA-classifier 
and the rule-based approach are very similar for En- 
glish, the rule-based apl~roach yields better results 
tbr Spanish. The much higher random baseline for 
Spanish can be explained by the higher incidence of 
questions in the Spanish data (14.9?/(, vs. 5.3% for 
English). 
5 C lean-up  F i l te r  
The clean-up component is a sequence of modules 
which serve the purposes of (a) rendering the tran- 
scripts more readable, (b) simplifying the input for 
subsequent components, and (c) avoiding unwanted 
bias for relevance computations ( ee section 2). All 
this has to happen without losing essential informa- 
tion that could be relevant in a summary. While 
other work (\]\]eeman et al, 1996; Stolcke et al, 1998) 
was concerned with building classifiers that can de- 
tect and possibly correct wn:ious speech disfluencies, 
our implementntion is of a much simpler design. It 
does not require as much lnanual annota.ted train- 
ing data and uses individual components for every 
major category of disfluency.1 
t While we have not yet numerical ly evaluated the per fo f  
mance of this component,  its output  is deemed very natura l  to 
read by system users. Since the focus and goals of this contpo- 
nent are somewhat  different han l)reviotts work in that  area, 
meaningful  compar isons are hard to make. 
970 
Single or multiple word repetitions, fillers (e.g., 
"uhm"), and discourse markers without semantic 
content (e.g., "you know") a.re removed fl:om the in- 
put, some short forms axe expanded (e.g., "we'll" 
-+ "we will"), a.nd fl'cquent word sequences are 
combined into a single token (e.g., % lot of" -+ 
"a_lot_of"). 
Longer tm'ns are segmented into shorl clauses, 
which are defined a.s consisting of at least a. sub- 
ject and a.n inIlectcd verbal form. While (Stolcke 
and Shriberg, 1996) use n-gram models for this task, 
and (C~awald~t et al, 1997) use neura.l networks, we 
decided to use a. rule-based approach (using word 
a,nd POS information), whose performa.nce proved 
to be compat'able with the results in the cited \])~- 
pets (1,'~ > 0.85, error < 0.05). ~ 
leo, . several of tile clea.n-up filter's components, we 
ina.ke use of Brill's POS ta.gger (Ih:ill, I,(),qd). For 
Fmglish, we use ~t modified version of Brilt's original 
t~g set, and the tagger was adapted and retra.ined for 
Sl)oken langua.ge orl)ora, (CAIAAIOME a.lKl SWITCll-  
tlOalU)) (Zechner, 1997). For S1)anish, we crea.ted 
our own tag set., derived from the l,l)C lexicon and 
front the CI{ATEI/. project (LeOn, 1994), and trained 
the tagger on ma.nua.lly annotated (~;AI,I,IIOME dia- 
logues, l!'urthernlore, a. POS based sha.lk)w chunk 
parser (Zechner a.nd Wa.ibel, 1998) is used to fill.('.,' 
(,tit. likely ca.ndidates for incomplete, clauses dne to 
speech repair or interrul)tion by the other Slleaker. 
6 Topic Segmentation 
,~illce CAI,I,IIOME dialogues are a.lways multi-topica.I, 
segmenting them into tOl)ical units is an important 
:;tel) in our summa.riza.tion system. '.l'his allows us 
to l)rovi(le "signature?' information (frcqllenl; coil- 
tent words) about every topic to the user as a. hell) 
for faster 1)rowsing and accessing the dat.a., l,'ur- 
thel:more, the subsequent informa.tio, condensation 
COI\]l\])Ollent ca.ll ~,VolYk on smaller parts of the diaJogue 
a.nd thus opera.re more ellieiently. 
Following (l{oguraev and Ii{cnnedy, 1997; Ba.rzi- 
la.y and Elhadad, 1997) who use 'l'extTiling (llcarst, 
1997) for their summa.riza.tion systems of written 
text,  we adapted this algorithm (it.s block compar- 
ison version) R)r sl)eech data: we choose turns to 
be minimal units a.nd compute block simila.rity be- 
tween l)locl(s of k turns every d turns. We use 9 
English and 15 Spanish @ALI,tIOMI,; dialogues, man- 
ually annota.ted for topic bounda.ries, to determine 
the optinmm wdues for a set of TextTiling pm:am- 
eters and ~t. the same time to eva.lua.te the accu- 
racy of this algorithm. '.re do this, we ran a.n n-R)ld 
cross-wdidation (".jack-l~nifing") where ~dl dia.logues 
but one are used to determine the 1)est parameters 
"train set") m,d the remaining dia.logue is used as 
2'\]'lie COIIIIIDA'isoII W~:tS (\[OllC OI1 t.he S~-tllle <latat set as  used  
m (Gav;ddh ctal. ,  1997). 
English Spanish 
blocksize k 25 15 
sample distance d 2 2 
rounds of smoothing r 2 l 
smoothing width s 2 \] 
'l.'able 2: OptimM 'l>xt'.l.'iling pa.rameters for English 
and Spanish CAI,IAIOME dialogues 
nmnber of dbdogues 
r~mdom baseline 
test set avg. (%nseen data") 
train set a~vg. ("seen dat?') 
English Spanish 
9 15 
0.34 0.35 
0.58 0.53 
0.69 0.58 
'l'~d)le 3: Topic segmenta.tion results for English and 
Spa.nish CAI,IAIOMI,: dialogues (Fl-Scores) 
a held-out d~ta. set for eva.luation ("test set"). This 
process is rcpea.ted n times and average results are 
reported. Ta.ble 2 shows the set of p~u:ameters which 
worked best for most diak)gues ~md 'Fable 3 shows 
tile eva.hm.tion results of the cross-validation exper- 
iment. /,'~-scores improve I)y 18-2d% absohtte over 
the random baseline for unseen a.nd by 23 35% for 
seen data., the performance for E\]@ish being better 
than for Spanish. 'l'hese results, albeit achieved on 
a. quite different ext genre, are well in line with the 
results in (llea.rst, 1997) who reports a.n absolute im- 
provement of a, bout :20% over a, random baseline for 
seen data. 
7 Information Condensation 
The informa,tion condensa, tion COml)onent is the core 
o\[' our sysl,en:~, lilts pUrl)OSe is to determine weights 
for terms and turns (or linked turn-i)airs ) and then 
to rank the turns a.ccording to their relewmce within 
each topical segment of the dialogue. 
For term-weighting, lf*idf-insl)ired formula.e 
(Sa.lton and Buckley, 1990) are used to empha.size 
words which are in the "middle range" of fl:equency 
in the dialogue a.nd do not a.pl)eat: in a. stop list. :~ 
For turn--ranking, we use a version of the "maximal 
n,argina.l relevance" (MMI{) algorithm (Ca.rbonell 
and Goldstein, 1998), where emphasis is given to 
liurns which conta.in ma.ny highly weighted terms tot" 
the current segment ("sa.lience") a.nd are sutficiently 
dissimila.r to previously ranked turns (to minimize 
redunda.ncy). 
For 9 English and l d Spanish dialogues, the "most 
relevant" turns were nmrl~ed lay hmnan coders. We 
ran a. series of cross-validation experiments o (a,) op- 
timize the parameters of this component related to 
tJ'*idf a.nd MMR computa,tion and to (b) deterlnine 
31,'or l,;nglish, our stop list comprises 557 words, for Span- 
ish, 831 words. 
971 
how well this information condensing component can 
match tile human relewmce annotations. 
Summarization results are comlmted using 1 l-pt- 
avg precision scores t`or ranked turn lists where the 
maximum precision of the list of retrieved turns 
is averaged in the 11 evenly spaced intervals be- 
tween recall=\[0,0.1),\[0.1,0.2), . . \[1.0,1.:1)(Salton 
and McGill, 1.983). 4 Table 4 shows the results from 
these experiments. Similar to other experiments in
the summarization literature (Ma.ni et a.l., 1998), we 
find a wide performance variation across different 
texts. 
8 Telegraphic Reduction 
The purpose of this component is to maximize infor- 
mation in a tixed amount of space. We shorten the 
OUClmt of the summarizer to a "telegraphic style"; 
that way, more inrorma.tion can be included in a 
summary of k words (02: n bytes). Since we only 
use shallow methods for textual analysis that do 
not generate a. dependency structure, we cannot use 
complex methods for text reduction as described, 
e.g., in (Jing, 2000). Our method simply excludes 
words occurring in the stop list fl:om the summary, 
except for some highly inforlnative words such as 'T' 
or  ~11ot ~ . 
9 User  In ter face  and  System 
Per fo r lnance  
Since we want to enable interactive summarization 
which a.llows ~ user to browse through a dialogue 
qnickly Co search for information he is interested 
in, we have integrated our summarization system 
into a 3AVA-based graphical user interface ("Meet- 
ing Browser") (Bert et al, 2000). This interface also 
integrates the output of a speech recognizer (Yu et 
al., 1.999), and can display a wide variety of infer  
1nation about a conversation, including speech acts, 
dialogue games, and emotions. 
For sumlnarization, the user can determine the 
size of the summary and which topical segments 
he wants to have displayed. Ite can also rocus 
the summary on particular content words ("query- 
based summary")  or exclude words from considera- 
tion ("dynamic stop list expansion"). 
Smmnarizing a 10 minute segment of a CALL- 
hOME dialogue with our system takes on average less 
than 30 seconds on a 167 MHz 320 MB Sun Ultral 
workstation.S 
4 We are aware that  this annotat ion and evaluat ion scheme 
is far fl'om opt lmah it does neither reflect the fact that  turns 
are not necessari ly the best units for extract ion or that  the 
11-pt-avg precision score is not optimal ly suited for the sum- 
mar izat ion task. We thus have recently developed a new 
word-based method  for annotat ion  and evaluat ion of spon- 
taneous peech (Zechner, 2000). 
5The average was computed  over five English dialogues. 
10 Human Study  
1(1.1 Exper iment  Set;up 
Ill order to ewduate the system as a. whole, we con- 
ducted a study with humans in the loop to 1)e able Co 
colnpare three types of summaries (TITANS, CLEAN, 
TELE, see section 3) with the fllll original transcript. 
We address these two main questions in this study: 
(i) how fast can information be identified using dif- 
ferent types of summaries? (ii) how accurately is the 
information preserved, comparing different types of 
summaries? 
We did not only ask the user "narrow" questions 
for a specific piece of information - -  along the lines 
of the Q-A-evaluation part. of the SUMMAC confer- 
ence (Mani eC a.l., 1998) -- but also very "global", 
non-specific questions, tied Co a. parCicular (topical) 
segment of the dialogue. 
The experiment was conducted as follows: Sub- 
jeers were given 24 texts each, aceompa.nied by either 
a generic question ("What is the topic of the discus- 
sion in this text segment?") or three specitic ques- 
tions (e.g., "Which clothes did speaker A buy.'?"). 
The texts were drawn from five topical segments 
each rrom five English CAIAAIOME dialogues. (; They 
have four difl>rent formats: (a) fldl transcripts (i.e., 
the transcript of the whole segment) (FULL); (b) 
summa.ry of the raw transcripts (without linking and 
clea.n--up) ('rll.aNS); (c) cleaned-up summary (using 
all four major components of our  sys ten l )  (C,I,I,;AN); 
and (d) telegram suln21\]a, ry (der ived  r ron \ ]  (c),  us ing  
also Cite Celegraphic reduct.ion component) (TI';LE). 
'l'he texts or for,,,a.t,, (b), (c), a.nd (d) were gener- 
ated 1;o have the saaue length: 40% of (a), i.e., we 
use a 60% reduction rate. All these formats can 
be accotnpanied by either a. generic or three specitic 
questions: hence there are eight types of tasks for 
each of the 24: texts. 
We divided the subjects in eight groups such that 
no subject had to l)erform more than one task on 
the same text and we distributed the different Casks 
evenly \['or each group. Thus we cau make unbiased 
comparisons across texts and tasks. 
The answer accuracy vs. a pre-defined answer key 
was manually assessed on a 6 point discrete scale 
between 0.0 and 1.0. 
10.2 ll,esults and Discussion 
Of the 27 subjects taking part in this experiment, 
we included 24 subjects iu the evaluation; 3 sub- 
jects were excluded who were extreme outliers with 
respect o average answer time or score (not within 
/* + -2sCddev). 
From the results in Table 5 we observe the fol- 
lowing trends with respect to answer accuracy and 
response time: 
SOne of the 25 segments  was set aside for demonst rat ion  
purposes. 
972 
English Spanish 
nun+her of dialogues 9 14 
turns t)er dialogue ma.rked ;ts relevant I)y human coders 12% 25% 
I l-pt-a.vg precision (average over t.ol)i(:a.l segnlent.s) 0.45 0.5.0 
score variation between (liak)gues 0.2 0.49 0.15 0.8 
TM)Ie 4: Smmnarizat ion result;s for English and S1)anisll (I~AI,I,IIOME 
I,'ornmt tra ns (:lea.n \] tele 
'\]'ime vs. A(:c. T in . :  \] Ae(.  'l'ime \[ A( C. I T ime \[ Ac( 
generic (q = 72) 
specific (q = 216) 
L full T ime~ Ace. 
I 0._(,. 1D.): s 
, -~ec .  
-%~ \[ 07739 
'l'M)le 5: Average a.nsw('r times (i,, sect a.nd a.ccuracy scores (\[0.0-1.0\]) over eight dilferent tasks (number of 
subjects=2d; q:=mmd)er of questions l)er task type). 
summary  l,ype 
generic / indicative 
speci\[ic / informative 
\[ !)/).s I    wci.,l \] 
Lr Ls 1 ? t  .0 
' l 'able 6: Ilela.tive answer accuracies in % for dill'~,rent 
Sl)l\]llll~/ri(~S 
* ge~w'ric questions ("indicative summarie,s", the 
task being to identi\[y the topic o\[' a text): The 
tWO c leaned u D StlllnFla,ries tool(  M)out, the same 
Lime to in;ocess I)ui. had lower a eeura('y scores 
than tim v(;rsion directly u:dug the trans(:ril)l.. 
* spcc~/ir quest.ions ("ilfl'orlnal.ive sunllllaries", 
the (.ask being Io lilM Sl)ecilie intLrllml ion in t\]l(~ 
re?t): (I) The accuracy advant, age of the raw 
I,ranscripl, sun lmaries ('I'R, A NS) over  the c leal led 
u\]) versions (CLlCAN) is only small (,oZ :;Latis- 
tica.lly signitieant: L:-0.748) 7. (2) 'l'her(" is a 
sui)eriority of the 'l'l,;lA,,-StllnlHary to t)o(;h otJmr 
kinds ('rFLI.: is significa.nlJy more ;,iCCtllX/|l(2 (h~-/.ll 
CLEAN \[()r 1) "~ 0.0~r)). 
l,'rom this w(; conjecture thai. our methods for (:us- 
tomizaJ.ion of the summaries to spoken dialogues is 
mostly relewmt for inJ'ormativc, but llot so tUll(;h 
for indi,:,tivc smmmu'ization. We drink that el.her 
methods, such as lists of signature l)hrases would l)e 
n tor0 effective to use lbr the \]al;tcr \[mrl)ose. 
'l)dtle 6 shows the answer accuracy for the three 
different smmnary  tyl)es relative 1;o the accuracy of 
tile fldl transcripl, texts of l, he sa.me segmenl,s (':rela- 
tive ~mswer a.ccm:acy"). We, observe that; tit(: r('l~d;ive 
accuracy reduction for all smnn\]aries i markedly 
lower than the reduction of tc'xt size: all sunmmries 
were reduced from the full transcripts l)y 60%, 
whereas tile answer a(:(:uracy only drops between 9% 
(TITANS) a,tld 24% (CI,EAN) l()l" the generic quest, ions, 
7111 \['DA;\[,, ill 2, of 5 dialogues. I,\]m CI,I.1AN SIIllllllD, l'y scores 
m:e higher tllall th<>se of the 'I'IIANS summaries. 
and between 20% ('rF, l,l~,) and 29% (CI,F, AN) fOl: the 
speci\[ic questions. This proves that our systeln is 
able to retain most of the relevant information in 
tim summaries.  
As for average' answer times, we see a. ma.rked re- 
duction (3()0{,) of all sunmm.ries coulparcd to the full 
texts in l,hc .qcneric case; for the SlmCific ease, the 
t ime reduction is sonlewhat sma.ller (l 5% 25%). 
One shortcoming of the current, system is thai; it 
oper~d;es on turns (or \[;tlrll-pa.irs) as minimal units 
\['or extraction, tn \[Stture work, we will investigate 
possil)ilities to reduce the minimal units ot7 extrac-- 
l.ion l.o tim level of chmses or sent.<m<:es, wilhoul, giv 
like; Ul) the idea of linking cross-slxmker information. 
1 1 Summary  and  l g l tu re  Work  
\Ve have presented a sunmmrizat ion sysl,e~ for six) 
ken dialogues which is constructed to address key 
difl)renees of spolcen vs. written langua.ge, dia.logues 
vs. monologues, and inul|.i-topical vs. mono-topical  
texts. The system cleans up the input for speech 
disfluencies, links t.urns together into coherent in- 
formation units, determines tOlfica.l segments, and 
extracts the most relevant pieces of informal, ion in 
a user-customiza.ble way. I~;vahml,ions of major sys- 
tem (:Oral)Orients and of t.he systeJn as a. whole were 
1)erfornmd. 'l'hc results of a user sl, udy show that 
with a. sutmna ry size of d0%, between 71% and 911% 
of the inlbrma.tion of the fill\] text is ret.a.ined in the 
summary,  depending on tile type of summary  and 
tim Lyl)('s of quest, ions being asked. 
\?c' are currently extending the system to be able 
to ha.ndle different levels of granularity for extract;ion 
(clauses, sentences, turns), leurthermore, we plan to 
investigate the, integration of l)rosodic information 
into several (-onq)onents of our system. 
12 Acknowledgements  
We wa.nt, l,o tha.nk the almotators for their ell'errs aim 
Klaus Hies for providing l.he automatic speech a(:t 
973 
tagger. We appreciate comments and suggestions 
t?om Alon Lavie, Marsal Gawtld~/, Jade Goldstein, 
Thomas MacCracken, and the &llonymotls l:eviewers 
on earlier drafts of this paper. 
This work was funded in part by the VEf{BMOBI1, 
project of the Federal Republic of Oerma,ny, ATR - 
Interpreting Telecommunications Research L~l)ora- 
tories of Japan, and the US l)epartment of l)efense. 
Re ferences  
AAAI, editor. 1998. Proceedin9s of the AAAI-98 Spring 
Symposium on Intelligent Te.vt Summarization, Stan\]ord, 
CA. 
ACL. 2000. Proceedings of thc ANLP/NAACL-2000 Work- 
shop on Automatic Summarization, Seattle, WA, May. 
Jan Alexaudersson and Peter Poller. 1998. Towards mul- 
tilingual protocol generation for spontaneous speech dia- 
logues. In Proceedings of the INLG-98, Niagara-on-the- 
lahc, Canada, ilugust. 
f{cgina Barzilay and Michael Elhadad. 1997. Using lexical 
chains for text summarization. In ilCL/EACL-97 Work- 
shop on Intelligent and Scalable Te.vt Summarization. 
Michael Bert, l{alph Gross, llua Yu, Xiaojin Zhu, Yue Pan, 
Jie Yang, and Alex Waibel. 2000. Multimodal meeting 
tracker. In Proceedings o\] the Conference on Content- 
Based Multimedia Information Access, IHAO-2000, Paris, 
l<7'ance, April. 
Braniinir Boguraev and Chrlstol)hcr I(cnnedy. 1997. 
Salience-based characterisation of text documents. In 
A CID/EA CL- 97 Workshop on Intelligent and Scalable Text 
Summarization. 
Eric Brill. 1994. Some advances in transforlnation-I)~ed part 
of speech tagging. In Proceeedings o.f AAAI-9/~. 
Jaime Carbonell mid Jade Goldstein. 1998. The use of MMR, 
diversity-based reranking for reordering docunlents and 
producing summaries. In Proceedings o.f the 21st ACM- 
SIGIJg International Co,florence on Research and Devel- 
opment in lnJormation ll.ctrieval, Melbour~;c, Australia. 
Johl\] S. Garofolo, Ellen M. Voorhees, Vincent M. Stanford, 
and l(aren Sparck .\]ones. \]997. TI{I\]C-6 1997 spoken doc- 
IllllellL retriewfl track overview and results. In Proceed- 
in9s o.\[ the 1997 "17H?C-6 Conference, Gaithe'rsburg, MI), 
November, pages 83 -91. 
John S. Garofolo, Ellen M. Voorhees, Cedric G. P. Auzanne, 
and Vincent M. Stratford. 1999. Spoken doculnent re- 
trieval: 1998 evaluation aud investigation of new inetrics. 
In Proceedings of the ESCA workshop: Accessing informa- 
tion in spoken audio, pages 1-7. Camloridge, UK, April. 
Morsel Gawddh, Klaus Zechner, and Gregory Aist. 1997. 
Iligh perforlnauce s gnlentation f spontaneous speech us- 
ing part of speech and trigger word infornmtion. In Pro- 
eeedin9 s of the 5th ANLP Conference, Washington DO, 
pages 12-15. 
J. J. Godfrey, E. C. ltolliman, and J. Mcl)mfiel. 1992. 
SWITCttBOARD: telephone speech corpus for research mid 
development. In Proceedings of the IUASSP-92, vohnne 1, 
pages 517-520. 
Martl A. IIearst. 1997. TextTiling: Segmenting text into 
multi-paragraph subtopic passages. Computational Lin.- 
guistics, 2311):33-64, March. 
Peter A. IIeeman, Ieyung he Loken-Khn, and James 1:. Allen. 
1996. Oombining the detection and correction of speech 
repairs. In Proceedin9s of ICSLP-96. 
Julia Ilirsehberg mid Christine Nakatmfi. 1998. Acoustic 
indicators of topic segmentation. In Proceedings o.f the 
ICSLP-98, Sydney, Australia. 
IIongyan Jing. 2000. Sentence reduction for automatic text 
sum,narlzation. In Proceedings of ANIH~-NAA CL-2000, 
Seattle, WA, May, pages 310-;315. 
Megumi Kameyama, Goh Kawai, and isao Arima. 1996. A 
real-tinie systcni for summarizing human-human sponta- 
neous spoken dialogues. Ill Proceedings of the ICSLP-96, 
pages 681-684. 
Linguistic Data Consortium (LDC). 1996. CallHome alld 
CallFriend LVCSR databases. 
Fernando S~nchez \[,edn. 1994. Spanish l.agset for tile 
CI~.ATIBR project, http://xxx.lanl.gov/cinp-lg/9406023. 
lndetjeet Mani and Mark Maybury, editors. 1997. Proceed- 
in gs of the A CL/ICA CL '97 Workshop on Intelligent Scal- 
able Text Summarization, Madrid, Spain. 
\]ndet:ieet Mani, I)avid ltouse, Gary Klein, l,ynette 
Hirschman, Leo Obrst, Therese Firmin, Michael 
Chrzanowsld, and lJeth Sundheim. 1998. The 'I'\]P- 
STER SUMMAC text summarization evaluation. Mitre 
Technical Report MTIi 98W0000138, October 1998. 
Klaus liles. 1999. ItMM and neural network based speech 
act detectiou. \]n Proceedings o\] the ICASSP-99, Phoenix, 
Arizona, March. 
Gerard Salton and Chris Buckley. 1990. \]?lexlble text match- 
ing for information retrieval. 'Pcchnical report, Cornell 
University, Department ofComputer Science, TR. 90-1158, 
September. 
Germ'd Salton and Michael J. McGill. 1983. Introduction to 
Modern Information ltetrieval. McO,'aw IIill, q\~kyo etc. 
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke, Paul 
q)*ylor, Daniel aurafsky, Klaus f{ies, Noah Coccaro, l{achel 
Martin, Marie Meteer, and Carol Van Ess-Dykema. 1998. 
(Jan prosody aid the automatic classification ofdialog acts 
in conversational speech? Lan9aa9 e and Speech, ,1113- 
4):439 487. 
Andrew,s Stolcke and l~lizabeth Shriberg. 1996. Automatic 
linguistic segmentation f conversational speech. In Pro- 
ceedings o\] the I6'SL\]~-96, pages 1005-1008. 
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Marl 
Ostendorf, Dilek IIakkani, Madelei,m Plauche, (JSkhan 
Tfir, and Yu tin. 1998. Automatic detection of sentence 
1ooundm:ies and disfluencies based on recognized words. In 
Proceedings of the ICSLP-98, Sydney, Australia, Decen> 
bet, volunm 5, pages 2247--2250. 
Andreas 8tolcke, ISlizabeth Shriberg, l)ilek IIakkani-Tfir, and 
GSkhan q'fir. 2000. Prosody-based automatic segmenta- 
tion of speech into sentences and topics. Speech Comn~u- 
"nhcatio'a., 32(1-2). 
l/obin Valenza, 3kmy l~obinson, Marianne l\]ickcy, and l{oger 
Tucker. 199,(/. Sunnnarisation of Sl)oken audio through in- 
forniatiou extraction, tn Proceedings o,f the /'TSCA work- 
shop: Aceessin.9 i~fformatio'n i~ spoken audio, pages 111 
116. C.2ambridge, UK, April. 
Alex Waibel, Michael Belt, and Michael Finke. 1998. Meet- 
ing browser: Tracking and summarizillg meetings, in Pro- 
ceedings of the DARPA Broadcast News l/Vo'rkshop. 
Hue Yu, Michael Finke, and Alex Waibel. 1999. Progress 
ill atltonlatic meeting transcril)tion. \]n Proceedings qf 
EUI~OSI'EECI1-99, Budapest, lhm9ary, September. 
Klaus Zeehner and Alex \?aibel. 1998. Using chunk based 
partial parsing of spontaneous speech in unrestricted do- 
mains for reducing word error rate in speech recognition. 
In Proceedings of COLING-A CL 98, \]WIontreal, Canada. 
Klaus Zechner and Alex Waibel. 2000. Minimizing word error 
rate in textual suinnlaries of spoken lmiguage. \]u Procced- 
ings o\] the First Meeting o.f the North American Chapter o.f 
the Association for Computational Linguistics, NAACL- 
2000, Seattle, WA, April/May, pages 186-193. 
Klaus Zechner. 1997. Building chunk level represen- 
tations for spontmmous peech in unrestricted do- 
mains: The CHUNI';Y system and its al)plication to 
reranking N-best lists of a speech recognizer. Mas- 
ter's thesis (project report), Oh/I_U, available fl'om: 
http  : / /wuu.  es .  emu. edu/-zechner/publ icat  ons. html. 
Klaus Zechner. 2000. A word-based annota- 
tion and evaluation scheme for summariza- 
tion of Sl)ontancoIJs speech. Awfilablc fi'oni 
http://www.cs.?,,,,.eduFzechner/pubiications.i,1:ml. 
974 

c? 2002 Association for Computational Linguistics
Automatic Summarization of
Open-Domain Multiparty Dialogues in
Diverse Genres
Klaus Zechner?
Educational Testing Service
Automatic summarization of open-domain spoken dialogues is a relatively new research area. This
article introduces the task and the challenges involved and motivates and presents an approach
for obtaining automatic-extract summaries for human transcripts of multiparty dialogues of four
different genres, without any restriction on domain.
We address the following issues, which are intrinsic to spoken-dialogue summarization and
typically can be ignored when summarizing written text such as news wire data: (1) detection and
removal of speech disfluencies; (2) detection and insertion of sentence boundaries; and (3) detection
and linking of cross-speaker information units (question-answer pairs).
A system evaluation is performed using a corpus of 23 dialogue excerpts with an average
duration of about 10 minutes, comprising 80 topical segments and about 47,000 words total. The
corpus was manually annotated for relevant text spans by six human annotators. The global eval-
uation shows that for the two more informal genres, our summarization system using dialogue-
specific components significantly outperforms two baselines: (1) a maximum-marginal-relevance
ranking algorithm using TF*IDF term weighting, and (2) a LEAD baseline that extracts the first
n words from a text.
1. Introduction
Although the field of summarizing written texts has been explored for many decades,
gaining significantly increased attention in the last five to ten years, summarization
of spoken language is a comparatively recent research area. As the number of spoken
audio databases is growing rapidly, however, we predict that the need for high-quality
summarization of information contained in this medium will increase substantially.
Summarization of spoken dialogues, in particular, may aid in the archiving, indexing,
and retrieval of various records of oral communication, such as corporate meetings,
sales interactions, or customer support.
The purpose of this article is to explore the issues of spoken-dialogue summa-
rization and to describe and evaluate an implementation addressing some of the core
challenges intrinsic to the task. We will use an implementation of a state-of-the-art
text summarization method (maximum marginal relevance, or MMR) as the main
baseline for comparative evaluations, and then add a set of components addressing
issues specific to spoken dialogues to this MMR module to create our spoken dialogue
summarization system, which we call DIASUMM.
We consider the following dimensions to be relevant for our research; the combi-
nation of these dimensions distinguishes our work from most other work in the field
? Educational Testing Service, Rosedale Road MS 11-R, Princeton, NJ 08541. E-mail: kzechner@ets.org
448
Computational Linguistics Volume 28, Number 4
of summarization:
? spoken versus written language
? multiparty dialogues versus texts written by one author
? unrestricted versus restricted domains
? diverse genres versus a single genre
The main challenges this work has to address, in addition to the challenges of written-
text summarization, are as follows:
? coping with speech disfluencies
? identifying the units for extraction
? maintaining cross-speaker coherence
? coping with speech recognition errors
We will discuss these challenges in more detail in the following section. Although
we have addressed the issue of speech recognition errors in previous related work
(Zechner and Waibel 2000b), for the purpose of this article, we exclusively use human
transcripts of spoken dialogues.
Intrinsic evaluations of text summaries usually use sentences as their basic units.
For our data, however, sentence boundaries are typically not available in the first place.
Thus we devise a word-based evaluation metric derived from an average relevance
score from human relevance annotations (section 6.2).
The organization of this article is as follows: Section 2 provides the motivation
for our research, introducing and discussing the main challenges of spoken-dialogue
summarization, followed by a section on related work (section 3). Section 4 describes
the corpus we use to develop and evaluate our system, along with the procedures
employed for corpus annotation. The system architecture and its components are de-
scribed in detail in section 5, along with evaluations thereof. Section 6 presents the
global evaluation of our approach, before we conclude the article with a discussion
of our results, contributions, and directions for future research in this field (sections 7
and 8).
2. Motivation
Consider the following example from a phone conversation drawn from the English
CALLHOME database (LDC 1996). It is a transcript of a conversation between two
native speakers of American English; one person is in the New York area (speaker
a), the other one (speaker b) in Israel. It was recorded about a month after Yitzhak
Rabin?s assassination (1995). This dialogue segment is about one minute of real time.
The audio is segmented into speaker turns using silence heuristics,1 and each turn
is marked with a turn number and with the speaker label. Noises are removed to
increase readability.2
1 Therefore, in some cases, we can find several turns of one speaker following each other.
2 Hence there can be ?missing? turns (e.g., turn 37), in case they contain only noises and no actual words.
449
Zechner Automatic Summarization of Dialogues
28 a: oh
29 b: they didn?t know he was going to get shot but it
was at a peace rally so i mean it just worked out
30 b: i mean it was a good place for the poor guy to die
i mean because it was you know right after the rally
and everything was on film and everything
31 a: yeah
32 b: oh the whole country we just finished the thirty days
mourning for him now you know it?s uh oh everybody?s
still in shock it?s
33 a: oh
34 a: i know
35 b: terrible what?s going on over here
36 b: and this guy that killed him they show him on t v
smiling he?s all happy he did it and everything he
isn?t even sorry or anything
38 a: there are i
39 b: him him he and his brother you know the two of
them were in it together and there?s a whole group
now it?s like a a conspiracy oh it?s eh
40 a: mm
41 a: with the kahane chai
42 b: unbelievable
43 b: yeah yeah it?s all those people yeah you probably see
them running around new york don?t you they?re all
44 a: yeah
45 a: oh yeah they?re here
46 b: new york based yeah
47 a: oh there?s
48 a: all those fanatics
49 a: like the extreme
50 b: oh but
51 b: but wh- what?s the reaction in america really i mean
i mean do people care you know i mean you know do they
52 a: yeah mo- most pe- i mean uh
53 a: i don?t know what commu- i mean like the jewish community
54 a: a lot e- all of us were
55 a: very upset and there were lots all the
56 b: yeah
57 a: like two days after did it happen like on a sunday
58 b: yeah it hap- it happened on it happened on a saturday night
By looking at this transcript we can readily identify some of the phenomena that
would cause difficulties for conventional summarizers of written texts:
? Some turns (e.g., turn 51) contain many disfluencies that (1) make them
hard to read and (2) reduce the relevance of the information contained
therein.
? Some (important) pieces of information are distributed over a sequence
of turns (e.g., turns 53?54?55, 45?47?48?49); this is due to a silence-based
450
Computational Linguistics Volume 28, Number 4
segmentation algorithm that causes breaks in logically connected clauses.
A traditional summarizer might render these sequences incompletely.
? Some turns are quite long (e.g., 36, 39) and contain several sentences; a
within-turn segmentation seems necessary to avoid the extraction of too
much extraneous information when only parts of a turn contain relevant
information.
? Some of the information is constructed interactively by both speakers;
the prototypical cases are question-answer pairs (e.g., turns 51?52ff.,
turns 57?58). A traditional text summarizer might miss either question or
answer and hence produce a less meaningful summary.
We shall discuss these arising issues along with an indication of our computational
remedies in the following subsections. We want to stress beforehand, though, that
the originality of our system should not be seen in the particular implementation of
its individual components, but rather in their selection and specific composition to
address the issues at hand in an effective and also efficient way.
2.1 Disfluency Detection
The two main negative effects speech disfluencies have on summarization are that they
(1) decrease the readability of the summary and (2) increase its noncontent noise. In
particular for informal conversations, the percentage of disfluent words is quite high,
typically around 20% of the total words spoken.3 This means that this issue should,
in our opinion, be addressed to improve the quality (readability and conciseness) of
the generated summaries.
In section 5.3 we shall present three components for identifying most of the major
classes of speech disfluencies in the input of the summarization system, such as filled
pauses, repetitions, and false starts. All detected disfluencies are marked in this process
and can be selectively excluded during summary generation.
2.2 Sentence Boundary Detection
Unlike written texts, in which punctuation markers clearly indicate clause and sen-
tence boundaries, spoken language is generated as a sequence of streams of words, in
which pauses (silences between words) do not always match linguistically meaningful
segments: A speaker can pause in the middle of a sentence or even a phrase, or, on
the other hand, might not pause at all after the end of a sentence or clause.
This mismatch between acoustic and linguistic segmentation is reflected in the
output of a speech recognizer, which typically generates a sequence of speaker turns
whose boundaries are marked by periods of silence (or nonspeech). As a result, one
speaker?s turn may contain multiple sentences, or, on the other hand, a speaker?s
sentence might span more than one turn. In a test corpus of five English CALLHOME
dialogues with an average length of 320 turns, we found on average of about 30 such
continuations of logical clauses over automatically determined acoustic segments per
dialogue.
The main problems for a summarizer would thus be (1) the lack of coherence and
readability of the output because of incomplete sentences and (2) extraneous infor-
mation due to extracted units consisting of more than one sentence. In section 5.4 we
3 Although other studies have found percentages lower than this figure, we included content-less
categories such as discourse markers or rhetorical connectives, which are often not regarded as
disfluencies per se.
451
Zechner Automatic Summarization of Dialogues
describe a component for sentence segmentation that addresses both of these prob-
lems.
2.3 Distributed Information
Since we have multiparty conversations as opposed to monologues, sometimes the
crucial information is found in a sequence of turns from several speakers, the proto-
typical case of this being a question-answer pair. If the summarizer were to extract
only the question or only the answer, the lack of the corresponding answer or question
would often cause a severe reduction of coherence in the summary.
In some cases, either the question or the answer is very short and does not contain
any words with high relevance that would yield a substantial weight in the summa-
rizer. In order not to lose these short sentences at a later stage, when only the most
relevant sentences are extracted, we need to identify matching question-answer pairs
ahead of time, so that the summarizer can output the matching sentences during sum-
mary generation as one unit. We describe our approach to cross-speaker information
linking in section 5.5.
2.4 Other Issues
We see the work reported in this article as the first in-depth analysis and evaluation
in the area of open-domain spoken-dialogue summarization. Given the large scope of
this undertaking, we had to restrict ourselves to those issues that are, in our opinion,
the most salient for the task at hand.
A number of other important issues for summarization in general and for speech
summarization in particular are either simplified or not addressed in this article and
left for future work in this field. In the following, we briefly mention some of these
issues, indicating their potential relevance and promise.
2.4.1 Topic Segmentation. In many cases, spoken dialogues are multitopical. For the
English CALLHOME corpus, we determined an average topic length of about one to two
minutes? speaking time (or about 200?400 words). Summarization can be accomplished
faster and more concisely if it operates on smaller topical segments rather than on long
pieces of input consisting of diverse topics.
Although we have implemented a topic segmentation component as part of our
system for these reasons, all of the evaluations are based on the topical segments
determined by human annotators. Therefore, this component will not be discussed
in this article. Furthermore, topical segmentation is not an issue intrinsic to spoken
dialogues, which in our opinion justifies this simplification.
2.4.2 Anaphora Resolution. An analogous reasoning holds for the issue of anaphora
resolution: Although it would certainly be desirable, for the sake of increased coher-
ence and readability, to employ a well-working anaphora resolution component, this
issue is not specific to the task at hand, either. One could argue that particularly for
summarization of more informal conversations, in which personal pronouns are rather
frequent, anaphora resolution might be more helpful than for, say, summarization of
written texts. But we conjecture that this task might also prove more challenging than
written-text anaphora resolution. In our system, we did not implement a module for
anaphora resolution.
2.4.3 Discourse Structure. Previous work indicates that information about discourse
structure from written texts can help in identifying the more salient and relevant
sentences or clauses for summary generation (Marcu 1999; Miike et al 1994). Much
452
Computational Linguistics Volume 28, Number 4
less exploration has been done, however, in the area of automatic analysis of dis-
course structure for non-task-oriented spoken dialogues in unrestricted domains, such
as CALLHOME (LDC 1996). Research for those kinds of corpora reported in Jurafsky et
al. (1998), Stolcke et al (2000), Levin et al (1999), and Ries et al (2000) focuses more on
detecting localized phenomena such as speech acts, dialogue games, or functional ac-
tivities. We conjecture that there are two reasons for this: (1) free-flowing spontaneous
conversations have much less structure than task-oriented dialogues, and (2) the au-
tomatic detection of hierarchical structure would be much harder than it is for written
texts or dialogues based on a premeditated plan.
Although we believe that in the long run attempts to automatically identify the
discourse structure of spoken dialogues may benefit summarization, in this article, we
greatly simplify this matter and exclusively look at local contexts in which speakers
interactively construct shared information (the question-answer pairs).
2.4.4 Speech Recognition Errors. Throughout this article, our simplifying assumption
is that our input comes from a perfect speech recognizer; that is, we use human
textual transcripts of the dialogues in our corpus. Although there are cases in which
this assumption is justifiable, such as transcripts provided by news services in parallel
to the recorded audio data, we believe that in general a spoken dialogue summarizer
has to be able to accept corrupted input from an automatic speech recognizer (ASR),
as well. Our system is indeed able to work with ASR output; it is integrated in a larger
system (Meeting Browser) that creates, summarizes, and archives meeting records and
is connected to a speech recognition engine (Bett et al 2000). Further, we have shown
in previous work how we can use ASR confidence scores (1) to reduce the word error
rate within the summary and (2) to increase the summary accuracy (Zechner and
Waibel 2000b).
2.4.5 Prosodic Information. A further simplifying assumption of this work is that
prosodic information is not available, with the exception of start and end times of
speaker turns. Considering the results reported by Shriberg et al (1998) and Shriberg
et al (2000), we conjecture that future work in this field will demonstrate the addi-
tional benefit of incorporating prosodic information, such as stress, pitch, and intra-
turn pauses, into the summarization system. In particular, we would expect improved
system performance when speech recognition hypotheses are used as input: In that
case, the prosodic information could compensate to some extent for incorrect word
information.
3. Related Work
The vast majority of summarization research in the past clearly has focused exclu-
sively on written text. A good selection of both early seminal papers and more recent
work can be found in Mani and Maybury (1999). In general, most summarization
approaches can be classified as either corpus-based, statistical summarization (such
as Kupiec, Pedersen, and Chen [1995]), or knowledge-based summarization (such as
Reimer and Hahn [1988]) in which the text domain is restricted. (The MMR method
[Carbonell, Geng, and Goldstein 1997], which we are using as the summarization
engine for our DIASUMM system, belongs to the first category.) More recently, Marcu
(1999) presented work on using automatically detected discourse structure for summa-
rization. Knight and Marcu (2000) and Berger and Mittal (2000) presented approaches
in which summarization can be reformulated as a problem of machine translation:
453
Zechner Automatic Summarization of Dialogues
translating a long sentence into a shorter sentence, or translating a Web page into a
brief gist, respectively.
Two main areas are exceptions to the focus on text summarization in past work:
(1) summarization of task-oriented dialogues in restricted domains and (2) summa-
rization of spoken news in unrestricted domains. We shall discuss both of these areas
in the following subsections, followed by a discussion of prosody-based emphasis de-
tection in spoken language, and finally by a summary of research most closely related
to the topic of this work.
3.1 Summarization of Dialogues in Restricted Domains
During the past decade, there has been significant progress in the area of closed-
domain spoken-dialogue translation and understanding, even with automatic speech
recognition input. Two examples of systems developed in that time frame are JANUS
(Lavie et al 1997) and VERBMOBIL (Wahlster 1993).
In that context, several spoken-dialogue summarization systems have been de-
veloped whose goal it is to capture the essence of the task-based dialogues at hand.
The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996)
deals with the travel reservation domain and uses a cascade of finite-state pattern rec-
ognizers to find the desired information. Within VERBMOBIL, a more knowledge-rich
approach is used (Alexandersson and Poller 1998; Reithinger et al 2000). The domain
here is travel planning and negotiation of a trip. In addition to finite-state transducers
for content extraction and statistical dialogue act recognition, VERBMOBIL also uses a
dialogue processor and a summary generator that have access to a world knowledge
database, a domain model, and a semantic database. The abstract representations built
by this summarizer allow for summary generation in multiple languages.
3.2 Summarization of Spoken News
Within the context of the Text Retrieval Conference (TREC) spoken document retrieval
(SDR) conferences (Garofolo et al 1997; Garofolo et al 1999) as well as the recent
Defense Advanced Research Project Agency (DARPA) broadcast news workshops, a
number of research groups have been developing multimedia browsing tools for text,
audio, and video data, which should facilitate the access to news data, combining
different modalities.
Hirschberg et al (1999) and Whittaker et al (1999) present a system that supports
local navigation for browsing and information extraction from acoustic databases,
using speech recognizer transcripts in tandem with the original audio recording. Al-
though their interface helps users in the tasks of relevance ranking and fact finding,
it is less helpful in the creating of summaries, partly because of imperfect speech
recognition.
Valenza et al (1999) present an audio summarization system that combines acous-
tic confidence scores with relevance scores to obtain more accurate and reliable sum-
maries. An evaluation showed that human judges preferred summaries with a com-
pression rate of about 15% (30 words per minute at a speaking rate of about 200 words
per minute) and that the summary word error rate was significantly smaller than the
word error rate for the full transcript.
Hori and Furui (2000) use salience features in combination with a language model
to reduce Japanese broadcast news captions by about 30?40% while keeping the mean-
ing of about 72% of all sentences in the test set. Another speech-related reduction ap-
proach was presented recently by Koumpis and Renals (2000), who summarize voice
mail in the Small Message format.
454
Computational Linguistics Volume 28, Number 4
3.3 Prosody-Based Emphasis Detection in Spoken Audio
Whereas most approaches to summarizing acoustic data rely on the word informa-
tion (provided by a human or ASR transcript), there have been attempts to generate
summaries based on emphasized regions in a discourse, using only prosodic features.
Chen and Withgott (1992) train a hidden Markov model on transcripts of spontaneous
speech, labeled for different degrees of emphasis by a panel of listeners. Their ?au-
dio summaries? on an unseen (but rather small) test set achieve a remarkably good
agreement with human annotators (? > 0.5). Stifelman (1995) uses a pitch-based em-
phasis detection algorithm developed by Arons (1994) to find emphasized passages
in a 13-minute discourse. In her analysis, she finds good agreement between these
emphasized regions and the beginnings of manually marked discourse segments (in
the framework of Grosz and Sidner [1986]). Although these are promising results, be-
ing suggestive of the role of prosody for determining emphasis, relevance, or salience
in spoken discourse, in this work we restrict the use of prosody to the turn length
and interturn pause features. We conjecture, however, that the integration of prosodic
and word level information would be a fruitful research area that would have to be
explored in future work.
3.4 Spoken Dialogue Summarization in Unrestricted Domains
Waibel, Bett, and Finke (1998) report results of their summarizer on automatically
transcribed SWITCHBOARD (SWBD) data (Godfrey, Holliman, and McDaniel 1992), the
word error rate being about 30%. Their implementation used an algorithm inspired
by MMR, but they did not address any dialogue- or speech-related issues in their
summarizer. In a question-answer test with summaries of five dialogues, participants
could identify most of the key concepts using a summary size of only five turns.
These results varied widely (between 20% and 90% accuracy) across the five different
dialogues tested in this experiment.
Our own previous work (Zechner and Waibel 2000a) addressed for the first time
the combination of challenges of dialogue summarization with summarization of spo-
ken language in unrestricted domains. We presented a first prototype of DIASUMM
that addressed the issues of disfluency detection and removal and sentence boundary
detection, as well as cross-speaker information linking.
This work extends and expands these initial attempts substantially, in that we are
now focusing on (1) a systematic training of the major components of the DIASUMM
system, enabled by the recent availability of a large corpus of disfluency-annotated
conversations (LDC 1999b), and (2) the exploration of three more genres of spoken
dialogues in addition to the English CALLHOME corpus (NEWSHOUR, CROSSFIRE, GROUP
MEETINGS). Further, the relevance annotations are now performed by a set of six human
annotators, which makes the global system evaluation more meaningful, considering
the typical divergence among different annotators? relevance judgments.
4. Data Annotation
4.1 Corpus Characteristics
Table 1 provides the statistics on the corpus used for the development and evaluation
of our system. We use data from four different genres, two being more informal, two
more formal:
? English CALLHOME and CALLFRIEND: from the Linguistic Data
Consortium (LDC) collections, eight dialogues for the devtest set
455
Zechner Automatic Summarization of Dialogues
Table 1
Data characteristics for the corpus (average over dialogues). 8E-CH, 4E-CH: English
CallHome; NHOUR: NewsHour; XFIRE: CrossFire; G-MTG: Group Meetings.
Data Set 8E-CH 4E-CH NHOUR XFIRE G-MTG
Formal/informal informal informal formal formal informal
Topics predetermined no no yes yes yes
Dialogue excerpts (total) 8 4 3 4 4
Topical segments (total) 28 23 8 14 7
Different speakers 2.1 2 2 6 7.5
Turns 242 276 25 96 140
Sentences 280 366 101 281 304
Sentences per turn 1.2 1.3 4.1 2.9 2.2
Questions (in %) 3.7 6.4 6.3 9.8 4.0
False starts (in %) 12.1 11.0 2.0 7.2 13.9
Words 1685 1905 1224 3165 2355
Words per sentence 6.0 5.2 12.1 11.3 7.7
Disfluent (in %) 16.0 16.3 5.1 4.2 13.2
Disfluencies 222 259 48 95 266
Disfluencies per sentence 0.79 0.71 0.48 0.34 0.87
Empty coordinating conjunctions (in %) 30.3 30.4 64.8 50.7 24.3
Lexicalized filled pauses (in %) 18.8 21.0 17.2 23.5 13.9
Editing terms (in %) 3.6 1.6 3.4 5.7 3.3
Nonlexicalized filled pauses (in %) 20.8 29.9 0.7 2.3 29.5
Repairs (in %) 26.6 17.1 13.8 17.8 29.0
(8E-CH) and four dialogues for the eval set (4E-CH).4 These are
recordings of phone conversations between two family members or
friends, typically about 30 minutes in length; the excerpts we used were
matched with the transcripts, which typically represent 5?10 minutes of
speaking time.
? NEWSHOUR (NHOUR): Excerpts from PBS?s NewsHour television show
with Jim Lehrer (recorded in 1998).
? CROSSFIRE (XFIRE): Excerpts from CNN?s CrossFire television show with
Bill Press and Robert Novak (recorded in 1998).
? GROUP MEETINGS (G-MTG): Excerpts from recordings of project group
meetings in the Interactive Systems Labs at Carnegie Mellon University.
Furthermore, we used the Penn Treebank distribution of the SWITCHBOARD corpus,
annotated with disfluencies, to train the major components of the system (LDC 1999b).
From Table 1 we can see that the two more formal corpora, NEWSHOUR and
CROSSFIRE, have longer sentences, more sentences per turn, and fewer disfluencies
(particularly nonlexicalized filled pauses and false starts) than English CALLHOME
and the GROUP MEETINGS. This means that their flavor is more like that of written text
and not so close to the conversational speech typically found in the SWITCHBOARD or
CALLHOME corpora.
4 We used the devtest set corpus for system development and tuning and set aside the eval set for the
final global system evaluation. For the other three genres, two dialogue excerpts each were used for the
devtest set, the remainder for the eval set.
456
Computational Linguistics Volume 28, Number 4
4.2 Corpus Annotation
4.2.1 First Annotation Phase. All the annotations were performed on human-gener-
ated transcripts of the dialogues. The CALLHOME and GROUP MEETINGS dialogues
were automatically partitioned into speaker turns (by means of a silence heuristic);
the other corpora were segmented manually (based on the contents and flow of the
conversation).5
There were six naive human annotators performing the task;6 only four, however,
completed the entire set of dialogues. Thus, the number of annotations available for
each dialogue varies from four to six. Prior to the relevance annotations, the annotators
had to mark topical boundaries, because we want to be able to define and then create
summaries for each topical segment separately (as opposed to a whole conversation
consisting of multiple topics). The notion of a topic was informally defined as a region
in the text that ends, according to the annotation manual, ?when the speakers shift
their topic of discussion.?
Once the topical segments were marked, for each such segment, each annota-
tor had to identify the most relevant information units (IUs), called nucleus IUs, and
somewhat relevant IUs, called satellite IUs. IUs are often equivalent to sentences but
can span longer or shorter contiguous segments of text, dependent on the annotator?s
choice. The overall goal of this relevance markup was to create a concise and readable
summary containing the main information present in the topical segment. Annotators
were also asked to mark the most salient words within their annotated IUs with a +,
which would render a summary with a somewhat more telegraphic style (+-marked
words).
We also asked that the human annotators stay within a preset target length for
their summaries: The +-marked words in all IUs within a topical segment should
be 10?20% of all the words in the segment. The guideline was enforced by a checker
program that was run during and after annotation of a transcript and that also ensured
that no markup errors and no accidental word deletions occurred. We provide a brief
example here (n[, n] mark the beginning and end of a nucleus IU, the phrase they fly
to Boston was +-marked as the core content within this IU):
B: heck it might turn out that you know n[ if
+they +fly in +to +boston i can n]
4.2.2 Creation of Gold-Standard Summaries. After the first annotation phase, in
which each coder worked independently according to the guidelines described above,
we devised a second phase, in which two coders from the initial group were asked
to create a common-ground annotation, based on the majority opinion of the whole
group. To construct such a majority opinion guideline automatically, we assigned
weights to all words in nucleus IUs and satellite IUs and added all weights for all
marked words of all coders for every turn.7 The total turn weights were then sorted by
decreasing value to provide a guide for the two coders in the second phase as to which
turns they should focus their annotations on for the common-ground or gold-standard
summaries.
5 This fact may partially account for NEWSHOUR and CROSSFIRE turns being longer than CALLHOME and
GROUP MEETING turns.
6 Naive in this context means that they were nonexperts in linguistics or discourse analysis.
7 The weights were set as follows: nucleus IUs: 3.0 if +-marked, 2.0 otherwise; satellite IUs: 1.0 if
+-marked, 0.5 otherwise.
457
Zechner Automatic Summarization of Dialogues
Table 2
Nuclei and satellites: Length in tokens and relative frequency (in % of all tokens).
Annotator/ Avg. Nuc. Avg. Sat. Nuc-Tokens Nuc-+-Marked Sat-Tokens Sat-+-Marked
Data Set Length Length (in %) (in %) (in %) (in %)
LB 12.993 13.732 11.646 8.558 5.363 3.818
BR 16.507 14.551 11.978 8.339 10.558 7.645
SC 20.720 14.093 29.412 18.045 6.517 4.796
RW 22.899 19.576 19.352 11.332 2.757 1.718
RC 23.741 18.553 43.573 15.434 12.749 0.333
JK 39.203 9.794 26.355 11.204 0.711 0.465
Gold 21.763 6.462 13.934 6.573 0.179 0.000
CALLHOME 17.108 13.099 21.962 11.003 5.126 1.932
NEWSHOUR 25.828 16.733 29.536 13.530 4.300 2.947
CROSSFIRE 33.923 22.132 21.705 10.615 1.853 0.976
MEETINGS 37.674 23.413 23.034 9.222 7.456 1.123
All Dialogues 23.152 16.173 22.796 10.807 4.665 1.636
Other than this guideline, the requirements were almost exactly identical to those
in phase 1, except that (1) the pair of annotators was required to work together on this
task to be able to reach a consensus opinion, and (2) the preset relative word length
of the gold summary (10?20%) applied only to the nucleus IUs.
As for the topical boundaries, which obviously vary among coders, a list of bound-
ary positions chosen by the majority (at least half) of the coders in the first phase was
provided. In this gold-standard phase, the two coders mostly stayed with these sug-
gestions and changed less than 15% of the suggested topic boundaries, the majority
of which were minor (less than two turns? difference in boundary position).
4.2.3 General Annotation Analysis. Table 2 provides the statistics on the frequencies
of the annotated nucleus and satellite IUs. We make the following observations:
? On average, about 23% of all tokens were assigned to a nucleus IU and
5% to a satellite IU; counting only the +-marked tokens, this reduces to
about 11% and 2% of all tokens, respectively.
? The average total lengths of nuclei and satellites vary widely across
corpora: between 17.1 (13.1) tokens for CALLHOME and 37.7 (23.4) tokens
for GROUP MEETINGS data. This is most likely a reflection on the typical
length of turns in the different subcorpora.
? A similar variation is also observed across annotators: between 12 and 40
tokens for nucleus-IUs and between 9 and 20 tokens for satellites. The
granularity of IUs is quite different across annotators.
? Since some annotators mark a larger number of IUs than others, there is
an even larger discrepancy in the relative number of words assigned to
nucleus IUs and satellite IUs among the different annotators: 11?44%
(nucleus IUs) and 0?13% (satellite IUs).
? The ratio of nucleus versus satellite tokens also varies greatly among the
annotators: from about 1:1 to 40:1.
458
Computational Linguistics Volume 28, Number 4
? The ratio of nucleus and satellite tokens that are +-marked varies greatly:
between 36 and 77% for nucleus IUs and between 2 and 80% for
satellite IUs.
From these observations, we conclude that merging the nucleus and satellite IUs
into one class would yield a more consistent picture than keeping them separate. A
similar argument can be made for the +-marked passages, in which we also find a
quite high intercoder variation in relative +-marking. This led us to the decision of
giving equal weight to any word in an IU, irrespective of IU type or marking, for the
purpose of global system evaluation.
Finally, we conjecture that the average length of our extraction units should be
in the 10?40 words range, which roughly corresponds to about 3?12 seconds of real
time, assuming an average word length of 300 milliseconds. As a comparison, we
note that Valenza et al (1999) found summaries with 30-grams8 working well in their
experiments, a finding that is in line with our observations here on typical human IU
lengths.
4.2.4 Intercoder Agreement. Agreement between coders (and between automatic meth-
ods and coders) has been measured in the summarization literature with quite a wide
range of methods: Rath, Resnick, and Savage (1961) use Kendall?s ? ; Kupiec, Ped-
ersen, and Chen (1995) (among many others) use percentage agreement; and Aone,
Okurowski, and Gorlinsky (1997) (among others) use the notions of precision, recall,
and F1-score, which are commonly employed in the information retrieval community.
Similarly, in the literature on discourse segmentation and labeling, a variety of differ-
ent agreement measures have been used, including precision and recall (Hearst 1997;
Passonneau and Litman 1997), Krippendorff?s (1980) ? (Passonneau and Litman 1997)
and Cohen?s (1960) ? (Carletta et al 1997).
In this work, we use the two following metrics: (1) the ?-statistic in its extension for
more than two coders (Davies and Fleiss 1982); and (2) precision, recall, and F1-score.9
We will discuss the ?-statistic first.
For intercoder agreement with respect to topical boundaries, agreement is found
if boundaries fall within the same 50-word bin of a dialogue. Relevance agreements
are computed at the word level. For relevance markings, we compute ? both for the
three-way case (nucleus IUs, satellite IUs, unmarked) and the two-way case (any IUs,
unmarked).10 Topical-boundary agreement was not evaluated for two of the GROUP
MEETINGS dialogues, in which only one of four annotators marked any text-internal
topic boundary. We compute agreements for each dialogue separately and report the
arithmetic means for the five subcorpora in Table 3. We observe that agreement for top-
ical boundaries is much higher than for relevance markings. Furthermore, agreement
is generally higher for CALLHOME and comparatively low for the GROUP MEETINGS
corpus.
As a second evaluation metric, we compute precision, recall, and F1-scores for the
same four annotators and the same sets of subcorpora as before. For topical boundaries,
a match means that the boundaries fall within ?3 turns of each other, and for relevant
8 A 30-gram is a passage of text containing 30 adjacent words.
9 Precision is the ratio of correctly matched items over all items (boundaries, marked words); recall is the
ratio of correctly matched items over all items that need to be matched; and the F1-score combines
precision (P) and recall (R) in the following way: F1 = 2PRP+R .
10 These computations were performed for those four (out of six) annotators who completed the entire
corpus markup.
459
Zechner Automatic Summarization of Dialogues
Table 3
Intercoder annotation ? agreement for topical boundaries and relevance markings.
8E-CH 4E-CH NHOUR XFIRE G-MTG Overall
Topical boundaries 0.503 0.402 0.256 0.331 0.174 0.384
Relevance markings (3 way) 0.147 0.161 0.123 0.089 0.040 0.117
Relevance markings (2 way) 0.157 0.169 0.124 0.100 0.046 0.126
Table 4
Intercoder annotation F1-agreement for topical boundaries and relevance markings.
8E-CH 4E-CH NHOUR XFIRE G-MTG Overall
Topical boundaries .54 .44 .53 .38 .18 .45
Relevance markings (2 way) .38 .39 .38 .32 .32 .36
words a match means that the two words to be compared are both in a nucleus or
satellite IU. The results can be seen in Table 4.
4.2.5 Disfluency and Sentence Boundary Annotation. In addition to the annotation
for topic boundaries and relevant text spans, the corpus was also annotated for speech
disfluencies in the same style as the Penn Treebank SWITCHBOARD corpus (LDC 1999b).
One coder (different from the six annotators mentioned before) manually tagged the
corpus for disfluencies and sentence boundaries following the SWITCHBOARD disflu-
ency annotation style book (Meteer et al 1995).
4.2.6 Question-Answer Annotation. A final type of annotation was performed on the
entire corpus to mark all questions and their answers, for the purpose of training and
evaluation of the question-answer linking system component. Questions and answers
were annotated in the following way: Every sentence that is a question was marked as
either a Yes-No-question or a Wh-question. Exceptions were back-channel questions,
such as ?Is that right??; rhetorical questions, such as ?Who would lie in public??; and
other questions that do not refer to a propositional content. These were not supposed
to be marked (even if they have an apparent answer), since we see the latter class
of questions as irrelevant for the purpose of increasing the local coherence within
summaries. For each Yes-No-question and Wh-question that has an answer, the answer
was marked with its relative offset to the question to which it belongs. Some answers
are continued over several sentences, but only the core answer (which usually consists
of a single sentence) was marked. This decision was made to bias the answer detection
module toward brief answers and to avoid the question-answer regions? getting too
lengthy, at the expense of summary conciseness.
5. Dialogue Summarization System
5.1 System Architecture
The global system architecture of the spoken-dialogue summarization system pre-
sented in this article (DIASUMM) is depicted in Figure 1. The input data are a time-
ordered sequence of speaker turns with the following quadruple of information: start
time, end time, speaker label, and word sequence. The seven major components are
executed sequentially, yielding a pipeline architecture.
460
Computational Linguistics Volume 28, Number 4
False Start Detection
(+ Chunk Parser)
Sentence Boundary Detection
POS Tagger
Repetition Filter
Question & Answer Detection
Sentence Ranking & Selection
Topic Segmentation
Disfluency
Detection
Extraction Unit
Identification
dialogue transcript
dialogue summary
Figure 1
Global system architecture.
The following subsections describe the components of the system in more detail.
As argued earlier, the topic detection component is not relevant for the way we con-
duct the global system evaluation and hence is not discussed here. (We implemented a
variant of Hearst?s [1997] TextTiling algorithm.) The three components involved in dis-
fluency detection are the part-of-speech (POS) tagger, the false-start detection module,
and the repetition filter. They are discussed in subsection 5.3, followed by a subsection
on sentence boundary detection (5.4). The question-answer pair detection is described
in subsection 5.5, and the sentence selection module, performing relevance ranking,
in subsection 5.6.
5.2 Input Tokenization
We eliminate all human and nonhuman noises and incomplete words from the in-
put transcript. Further, we eliminate all information on case and punctuation, since
461
Zechner Automatic Summarization of Dialogues
we emulate the ASR output in that regard, which does not provide this informa-
tion.
Contractions such as don?t or I?ll are divided and treated as separate words?in
these examples we would obtain do n?t and I ?ll.
5.3 Disfluency Detection
5.3.1 Motivation. Conversational, informal spoken language is quite different from
written language in that a speaker?s utterances are typically much less well-formed
than a writer?s sentences. We can observe a set of disfluencies such as false starts, hes-
itations, repetitions, filled pauses, and interruptions. Additionally, in speech there is
no good match between linguistically motivated sentence boundaries and turn bound-
aries or recognition hypotheses from automatic speech recognition.
5.3.2 Types of Disfluencies. The classification of disfluencies in this work follows
Shriberg (1994), Meteer et al (1995), and Rose (1998). It is worth noting, however,
that any disfluency classification will be only an approximation of the assumed real
phenomena and that often boundaries between different classes are fuzzy and hard
to decide for human annotators (cf. Meteer et al [1995] on annotators? problems with
the classification of the word so).
? Filled pauses: We follow Rose?s (1998) classification of nonlexicalized
filled pauses (typically uh, um) and lexicalized filled pauses (e.g., like, you
know). Whereas the former are usually nonambiguous and hence easy to
detect, the latter are ambiguous and much harder to detect accurately.
? Restarts or repairs: These are fragments that are resumed, but without
completely abandoning the first attempt. We follow the notation in
Meteer et al (1995) and Shriberg (1994), which has these parts:
(1) reparandum, (2) interruption point (+), (3) interregnum (editing
phase, {. . . }), and (4) repair.
? Repetition: A restart with a verbatim repetition of a word or a
sequence of words: [ she is + she is ] happy.
? Insertion: A repetition of the reparandum, with some word(s)
inserted: [ she liked + {um} she really liked ] it.
? Substitution: The reparandum is not repeated: [ she + {uh} my
wife ] liked it.
? False starts: These are abandoned, incomplete clauses. In some cases,
they may occur at the end of an utterance, and they can be due to
interruption by another speaker. Example: so we didn?t?they have not
accepted our proposal.
5.3.3 Related Work. The past decade has produced a substantial amount of research in
the area of detecting intonational and linguistic boundaries in conversational speech,
as well as in the area of detecting and correcting speech disfluencies. Whereas earlier
work tended to look at these phenomena in isolation (Nakatani and Hirschberg 1994;
Stolcke and Shriberg 1996), more recent work has attempted to solve several tasks
within one framework (Heeman and Allen 1999; Stolcke et al 1998).
Most approaches use some kind of prosodic information, such as duration of
pauses, stress, and pitch contours, and most of them combine this prosodic informa-
tion with information about word identity and sequence (n-grams, hidden Markov
462
Computational Linguistics Volume 28, Number 4
models). In the study of Stolcke et al (1998), the goal was to detect sentence bound-
aries and a variety of speech disfluencies on a large portion of the SWITCHBOARD
corpus. An explicit comparison was made between prosodic and word-based models,
and the results showed that an n-gram model, enhanced with segmental informa-
tion about turn boundaries, significantly outperformed the prosodic model. Model
combination improved the overall results, but only to a small extent. In more recent
research, Shriberg et al (2000) reported that for sentence boundary detection in two
different corpora (BROADCAST NEWS and SWITCHBOARD), prosodic models outperform
word-based language models and a model combination yields additional performance
gains.
5.3.4 Overview. In the following, we will discuss the three components of the DIASUMM
system that perform disfluency detection:
? a POS tagger that tags, in addition to the standard SWITCHBOARD
Treebank-3 tag set (LDC 1999b), the following disfluent regions or words:
1. coordinating conjunctions that don?t serve their usual connective
role, but act more as links between subsequent speech acts of a
speaker (e.g., and then; we call these empty coordinating
conjunctions in this work)
2. lexicalized filled pauses (labeled as discourse markers in the
Treebank-3 corpus; e.g., you know, like)
3. editing terms within speech repairs (e.g., I mean)
4. nonlexicalized filled pauses (e.g., um, uh)
? a decision tree (supported by a shallow chunk parser) that decides
whether to label a particular sentence as a false start
? a repetition detection script (for repeated sequences of up to four words)
5.3.5 Training Corpus. For training, we used a part of the SWITCHBOARD transcripts
that was manually annotated for sentence boundaries, POS, and the following types
of disfluent regions (LDC 1999b):
? {A. . . }: asides (very rare; we ignore them in our experiments)
? {C. . . }: empty coordinating conjunctions (e.g., and then)
? {D. . . }: discourse markers (i.e., lexicalized filled pauses in our terminology,
e.g., you know)
? {E. . . }: editing terms (within repairs; e.g., I mean)
? {F. . . }: filled pauses (nonlexicalized; e.g., uh)
? [. . . + . . .]: repairs: the part before the + is called reparandum (to be
removed), the part after the + repair (proper)
Sentence boundaries can be at the end of completed sentences (E S) or of noncompleted
sentences, such as false starts or abandoned clauses (N S).
463
Zechner Automatic Summarization of Dialogues
Table 5
Precision, recall and F1-scores of the four disfluency tag categories for the SWITCHBOARD test
set.
Description Count Tag Precision Recall F1
Empty coordinating conjunctions 5,990 CO 0.84 0.93 0.88
Lexicalized filled pauses 5,787 DM 0.95 0.90 0.93
Editing terms 1,004 ET 0.98 0.94 0.96
Nonlexicalized filled pauses 12,926 UH 0.98 0.98 0.98
Table 6
POS tagging accuracy on five subcorpora (evaluated on 500-word samples).
8E-CH 4E-CH NHOUR XFIRE G-MTG
Known words 92.8 90.6 92.7 90.6 93.2
Unknown words (total) 48.0 (25) 44.4 (9) 69.6 (23) 86.4 (22) 92.6 (27)
Overall 90.6 89.8 91.6 90.4 93.2
5.3.6 POS Tagger. We are using Brill?s rule-based POS tagger (Brill 1994). Its basic
algorithm at run time (after training) can be described as follows:
1. Tag every word with its most likely tag, predicting tags of unknown
words based on rules.
2. Change every tag according to its right and left context (both words and
tags are considered), following a list of rules.
For preprocessing, we replaced the tags in the regions of {C. . . }, {D. . . }, and {E. . . }
with the tags CO (coordinating), DM (discourse marker), and ET (editing term), re-
spectively. (The filler regions {F. . . } are already tagged with UH in the corpus.) Lines
that contain typographical errors were excluded from the training corpus. We further
eliminated all incomplete words (XX tag) and combined multiwords, marked by a GW
tag, into a single word (hence eliminating the GW tag).11 The entire resulting new tag
set had 42 tags.12
Training of the POS tagger proceeded in three stages, using about 250,000 tagged
words for each stage. The trained POS tagger?s performance on an unseen test set of
about 185,000 words is 94.1% tag accuracy (untrained baseline: 84.8% accuracy).
Table 5 shows precision, recall, and F1-scores for the four categories of disfluency
tags, measured on the test set after the last training phase. We see that the nonlexical-
ized filler words are almost perfectly tagged (F1 = 0.98), whereas the hardest task for
the tagger is the empty coordinating conjunctions (F1 = 0.88): There are a few highly
ambiguous words in that set, such as and, so, and or.
Table 6 shows the POS tagging accuracy on the five subcorpora of our dialogue
corpus, evaluated on a sample of 500 words per subcorpus. We see that the POS-
tagging accuracy is slightly lower than for the SWITCHBOARD set that was used for
11 The sole function of the GW tag is to label words that are considered to be parts of other words but
were transcribed separately, such as: drug/GW testing/NN.
12 For a description of the POS tags used in that database see Santorini (1990) and LDC (1999a).
464
Computational Linguistics Volume 28, Number 4
Table 7
Disfluency tag detection (F1) for five subcorpora (results in parentheses: less than 10 tags to be
detected).
8E-CH 4E-CH NHOUR XFIRE G-MTG
CO .89 .89 .38 .77 .54
DM .93 .73 .90 .82 .30
ET .95 .95 (.94) .85 .88
UH .56 .62 (.14) (.28) .45
training (approximately 90?93%; global average: 91.1%). Further we observe that with
the exception of the CALLHOME corpora, the majority of unknown words were actually
tagged correctly. The most frequent errors were (1) conjunctions tagged as empty
coordinated conjunctions, (2) proper names tagged as regular nouns, and (3) adverbs
tagged as adjectives.
Finally, we look at the POS tagger?s performance for the four disfluency tags CO,
DM, ET, and UH in our five subcorpora; the results of this evaluation are presented in
Table 7. We can see that the detection accuracy is generally lower than for the corpus
on which we trained the tagger (SWITCHBOARD), but still quite good in general. The
major exceptions are the UH tags, on which the F1-scores are comparatively low for all
subcorpora. The reason for this can be found mostly in words like yes, no, uh-huh, right,
okay, and yeah, which are often tagged with UH in SWITCHBOARD but frequently are not
considered to be irrelevant words in our corpus and hence not marked as disfluent
(e.g., if they are considered to be the answer to a question or a summary-relevant
acknowledgment). We circumvent potential exclusion from the summary output of
these and other words that might be erroneously tagged as nonlexicalized filled pauses
(UH) by marking a small set of words as exempt from removal (see section 5.5.6).
5.3.7 False Start Detection. False starts are quite frequent in spontaneous speech,
occurring at a rate of about 10?15% of all sentences (SWITCHBOARD, CALLHOME). They
involve less than 10% of the total words of a dialogue; about 34% of the words in
these incomplete sentences are part of some other disfluencies, such as filled pauses
or repairs. (In complete sentences, only about 15% of the words are part of these
disfluencies.) For CALLHOME, the average length of complete sentences is about 6
words, of incomplete sentences about 4.1 words (including disfluencies).
We trained a C4.5 decision tree (Quinlan 1992) on 8,000 sentences of SWITCHBOARD.
As features we use the first and last four trigger words (words that have a high
incidence around sentence boundaries) and POS of every sentence, as well as the first
and last four chunks from a POS-based chunk parser. This chunk parser is based
on a simple context-free POS grammar for English. It outputs a phrasal bracketing
of the input string (e.g., noun phrases or prepositional phrases). Further, we encode
the length of the sentence in words and the number of the words not parsed by the
chunk parser. We observed that whereas the chunk information itself does not improve
performance over the baseline of using trigger words and POS information only, the
derived feature of ?number of not parsed words? actually does improve the results.
We ran the decision tree on data with perfect POS tags (for SWITCHBOARD only),
disfluency tags (except for repairs), and sentence boundaries. The evaluations were
performed on independent test sets of about 3,000 sentences for SWITCHBOARD and of
our complete dialogue corpus. Table 8 shows the results of these experiments. Typical
errors, where complete sentences were classified as incomplete, are inverted forms or
465
Zechner Automatic Summarization of Dialogues
Table 8
False start classification results for different corpora (F1).
SWBD 8E-CH 4E-CH NHOUR XFIRE G-MTG
False start frequency (in %) 12.3 12.1 11.0 2.0 7.2 13.9
False start detection (F1) .611 .545 .640 .286 .352 .557
Table 9
Detection accuracy for repairs on the basis of individual word tokens using the repetition filter.
8E-CH 4E-CH NHOUR XFIRE G-MTG
Repair tokens (%) 4.7 3.8 2.2 1.3 7.9
Precision .88 .78 .25 .35 .91
Recall .41 .32 .01 .04 .27
F1-score .56 .45 .02 .08 .41
ellipsis at the end of a sentence (e.g., neither do I, it seems to). The performance for
the informal corpora (CALLHOME, GROUP MEETINGS) is better than that for the formal
corpora (NEWSHOUR, CROSSFIRE); this is related to the fact that the relative frequency
of false starts is markedly lower in these latter data sets and that these corpora are
more dissimilar to the training corpus (SWITCHBOARD).
5.3.8 Repetition Detection. The repetition detection component is concerned with
(verbatim) repetitions within a speaker?s turn, the most frequently occurring case of all
speech repairs for informal dialogues (insertions and substitutions are comparatively
less frequent). Repeated phrases can potentially be interrupted by other disfluencies,
such as filled pauses or editing terms. Repetition detection is performed with a script
that can identify repetitions of word/POS sequences of length one to four (longer
repetitions are extremely rare: on average, less than 1% of all repetitions). Words that
have been marked as disfluent by the POS tagger are ignored when the repeated
sequences are considered, so we can correctly detect repetitions such as [ he said uh to
+ he said to ] him. . . .
We are evaluating the precision, recall, and F1-scores for this component at the
level of individual words when the POS tagger and the sentence boundary detection
component are used. Table 9 shows the results. We see that for the informal subcor-
pora, we get very good precision (only a few repetitions detected are incorrect), and
recall is in the 25?45% range (since we cannot detect substitution or insertion type of
repairs). The results for the formal subcorpora are considerably worse, so this filter
should probably not be used for corpora with as few repetitions as NEWSHOUR or
CROSSFIRE. We checked all of the 95 false positives of this evaluation and observed that
in the majority of cases (41%), the repetition was correctly detected but was not marked
by the human annotator, since it might be considered a case of emphasis. We believe
that although some nuances of the sentence(s) might be lost, for the purpose of sum-
marization it makes perfect sense to reduce this information. Sometimes, individual
words are repeated for emphasis, sometimes whole sentences (e.g., ?Good./ Good./?).
In the following example from English CALLHOME, the emphasis is rather extreme:
203 B: [...] How is the new person doing? q/
204 A: Very very very very very well. / [...]
466
Computational Linguistics Volume 28, Number 4
Further, about 19% of false positives were correct but not annotated because they span
multiple turns, and about 14% were erroneously missed by the human annotator. Only
the remaining cases (26%) were actual false positives, caused by incorrect POS tags
(5%, typically an incorrectly tagged ?that/WDT that/DT? sequence at the beginning
of a relative clause) or incorrect sentence boundaries (21%).
There have been attempts to get a more complete coverage of detection and cor-
rection of all types of speech repairs (Heeman and Allen 1999). We decided, however,
to use a simple method here that works well for a large subset of cases and is very
efficient at the same time.
5.3.9 Disfluency Correction in DIASUMM. After detection, the correction of disfluen-
cies is straightforward. When DIASUMM generates its output from the ranked list of
sentences, it skips the false starts, the repetitions, and the words that were tagged with
CO, DM, ET, or UH by the POS tagger.
5.4 Sentence Boundary Detection
5.4.1 Introduction. The purpose of the sentence boundary detection component is to
insert linguistically meaningful sentence boundaries in the text, given a POS-tagged
input. We consider all intraturn and interturn boundary positions for every speaker
in a conversation. We use the abbreviations EOS for end of complete sentence (E S in the
SWITCHBOARD corpus) and NEOS for end of noncomplete sentence (N S in the SWITCHBOARD
corpus). The frequency of sentence boundaries (with respect to the total number of
words) is about 13.3%, most of the boundaries (almost 90%) being end markers of
completed sentences (SWITCHBOARD).
5.4.2 Training and Testing. We trained a C4.5 decision tree and computed its input
features from a context of four words before and after a potential sentence boundary,
motivated by the results of Gavalda`, Zechner, and Aist (1997). Also following Gavalda`,
Zechner, and Aist (1997), we used 60 trigger words with high predictive potential,
employing the score computation method described in this article.
The decision tree input features for every word position are as follows:
? POS tag (42 different tags)
? trigger word (60 different trigger words)
? turn boundary before this word?
? if turn boundary: length of pause after last turn of same speaker
Since NEOS boundaries occur very infrequently (only about 10% of all boundaries,
which is only about 1% of all potential boundaries), we decided to merge this class
with the EOS class and report results for this combined class only (CEOS). (We relied on
the false-start detection module described above to identify the NEOS sentences within
this merged class of sentences after the sentence boundary classification.)
For training, we used 25,000 words from the Treebank-3 corpus; the test set size
was 1,000 words. Table 10 shows the results in detail for the various parameter com-
binations. We see that for good performance we need to know about one of these two
features: ?is there a turn boundary before this word?? or ?pause duration after last
turn from same speaker.?
467
Zechner Automatic Summarization of Dialogues
Table 10
Sentence boundary detection accuracy (F1-score).
With Interturn Pause Duration? Yes No
With Turn Boundary Info? Yes No Yes No
Training set .904 .903 .900 .884
Test set .887 .884 .884 .825
Table 11
Inter- and intraturn boundary detection (BD) results on 1,000-word test set.
Occurrence (%) Detection Accuracy (F1)
Interturn non-BD 12 (1.2) .56
Interturn BD 112 (11.3) .95
Intraturn non-BD 809 (81.4) .99
Intraturn BD 61 (6.1) .77
5.4.3 Effect of Imperfect POS Tagging. To see how much influence an imperfect POS
tagging might have on these results, we POS-tagged the test set data using the POS
tagger described above. For this and the following experiments, we increased the
training corpus for the decision tree to 40,000 words. The POS tagger accuracy for this
test set was about 95.3%, and the F1-score for CEOS was .882, which is 98.9% of .892 on
perfect POS-tagged input. This is encouraging, since it shows that the decision tree is
not very sensitive to the majority of POS errors.
5.4.4 Interturn and Intraturn Boundaries. In this analysis, we are interested in com-
paring the detection of sentence boundaries between turns (interturn) to the detection
of boundaries within a turn (intraturn). Table 11 shows the results of this analysis (same
test set as above). As might be expected, the performance is very good for the two
frequent classes: sentence boundaries at the end of turns and nonboundaries within
turns (F1 > .95), but considerably worse for the two more infrequent cases. The very
rare cases (around 1% only) of non?sentence boundaries at the end of turns (i.e. turn?
continuations) show the lowest performance (F1 = .56).
5.4.5 Sentence Boundary Detection on Dialogue Corpus. To get a picture of the realis-
tic performance of the sentence boundary detection component, using the (imperfect)
POS tagger and a faster, but slightly less accurate, decision tree,13 we evaluate the
sentence boundary detection accuracy for all five subcorpora of our dialogue corpus.
Table 12 provides the results of these experiments. The results reflect a trend very
similar to that for the SWITCHBOARD corpus, in that the two more frequent classes (in-
terturn boundaries and intraturn nonboundaries) have high detection scores, whereas
the two more infrequent classes are less well detected. Furthermore, we observe that
in cases in which the relative frequency of rare classes is further reduced, the classi-
fication accuracy declines overproportionally (particularly for the rarest class of the
interturn nonboundaries). Also, overall boundary detection is better for the two more
informal corpora, CALLHOME and GROUP MEETINGS (F1 > .72).
13 This decision tree uses a different type of encoding, but the same input features.
468
Computational Linguistics Volume 28, Number 4
Table 12
Boundary detection (BD) accuracy (F1) for five subcorpora (in parentheses: relative frequency
of class in percent).
8E-CH 4E-CH NHOUR XFIRE G-MTG
Interturn non-BD .51 (2.9) .31 (1.4) [0] (0.0) .10 (0.1) .06 (0.1)
Interturn BD .84 (9.9) .89 (12.3) .93 (2.0) .89 (2.9) .93 (5.4)
Intraturn non-BD .97 (80.7) .97 (79.5) .97 (91.8) .97 (91.2) .97 (87.6)
Intraturn BD .60 (6.5) .65 (6.8) .56 (6.2) .42 (5.8) .56 (6.9)
Overall BD .75 (16.4) .80 (19.1) .66 (8.2) .58 (8.7) .72 (12.4)
Overall non-BD .95 (83.6) .96 (80.9) .97 (91.8) .97 (91.3) .96 (87.6)
5.5 Cross-Speaker Information Linking
5.5.1 Introduction. One of the properties of multiparty dialogues is that shared infor-
mation is created between dialogue participants. The most obvious interactions of this
kind are question-answer (Q-A) pairs. The purpose of this component is to create au-
tomatically such coherent pieces of relevant information, which can then be extracted
together while generating the summary. The effects of such linkings on actual sum-
maries can be seen in two dimensions: (1) increased local coherence in the summary
and (2) a potentially higher informativeness of the summary. Since Q-A linking has a
side effect in that other information will be lost with respect to a summary of the same
length without Q-A linking, the second claim is much less certain to hold than the
first. We investigated these questions in related work (Zechner and Lavie 2001) and
found that although Q-A linking does not significantly change the informativeness
of summaries on average, it does increase summary coherence (fluency) significantly.
In this section, we will be concerned with the following two intuitive subtasks of
Q-A linking: (1) identifying questions (Qs) and (2) finding their corresponding an-
swers.
5.5.2 Related Work. Detecting a question and its corresponding answer can be seen
as a subtask of the speech act detection and classification task. Recently, Stolcke et al
(2000) presented a comprehensive approach to dialogue act modeling with statistical
techniques. A good overview and comparison of recent related work can also be found
in Stolcke et al?s article. Results from their evaluations on SWITCHBOARD data show
that word-based speech act classifiers usually perform better than prosody-based clas-
sifiers, but that a model combination of the two approaches can yield an improvement
in classification accuracy.
5.5.3 Corpus Statistics. For training of the question detection module, we used the
manually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for train-
ing of the answer detection component, we used the eight English CALLHOME dia-
logues (8E-CH), which were manually annotated for Q-A pairs. Although we were
aiming to detect all questions in the question detection module, the answer detection
module focuses on Q-A pairs only: We exclude all questions from consideration that
are not Yes-No- (YN) or Wh-questions (such as rhetorical or back-channel questions),
14 In this work, the notions of speech acts and sentences can be considered equivalent.
15 From the Johns Hopkins University Large Vocabulary Continuous Speech Recognition (LVCSR)
Summer Workshop 1997. Thanks to Klaus Ries for providing the data, which are also available from
http://www.colorado.edu/ling/jurafsky/ws97/.
469
Zechner Automatic Summarization of Dialogues
Table 13
Frequency of different types of questions in the 8E-CH data set.
Sentences 2,211
Wh-questions total 20
. . . With immediate answers 15 (75%)
YN-questions total 48
. . . With immediate answers 38 (79%)
Qs excluded for Q-A detection 15
Questions total 83 (3.75%)
as well as those that do not have an answer in the dialogue. Thus we employ only
68 pf the 83 questions marked in the 8E-CH data set for these evaluations. Table 13
provides the statistics concerning questions and answers for the 8E-CH subcorpus
and shows that for a small but significant number of questions, the answer does not
immediately follow the question speech act (delayed answers).
5.5.4 Automatic Question Detection. We used two different methods, both trained on
SWITCHBOARD data: (1) a speech act tagger16 and (2) a decision tree based on trigger
word and part-of-speech information.
Speech act tagger. The speech act tagger tags one speech act at a time and hence can
make use only of speech act unigram information. Within a speech act, it uses a lan-
guage model based on POS and the 500 most frequent word/POS pairs. It was trained
on the aforementioned SWITCHBOARD speech act training set. It was not optimized for
the task of question detection. Its typical run time for speech act classification is about
10 speech acts per second.
Decision tree question classifier. The decision tree classifier (C4.5) uses the following
set of features: (1) POS and trigger word information for the first and last five tokens
of each speech act;17 (2) speech act length, and (3) occurrence of POS bigrams. The set
of trigger words is the same as for the sentence boundary detection module. The POS
bigrams were designed to be most discriminative between question speech acts (q-SAs)
and non?question speech acts (non-q-SAs). The bigrams were obtained as follows:
1. For a balanced set of q-SAs and non-q-SAs (about 9,000 SAs each):
Count all the POS bigrams in positions 1 . . . 5 and (n ? 4) . . .n (using
START and END for the first and last bigrams, respectively) and memorize
position (beginning or end of SA) and type (q-SA vs. non-q-SA).
2. For all bigrams:
(a) Add one to the count (to prevent division by zero).
(b) Divide the q-SA count by the non-q-SA count.
(c) If the ratio is smaller than one, invert it (ratio := 1/ratio).
(d) Multiply the result of (c) by the sum of q-SA count and
non-q-SA count.18
3. Extract the 100 bigrams with the highest value.
16 Thanks to Klaus Ries for providing us with the software.
17 Shorter speech acts are padded with dummies.
18 Leaving out this step favors low-frequency, high-discriminative bigrams too much and causes a slight
reduction in overall Q-detection performance.
470
Computational Linguistics Volume 28, Number 4
Table 14
Question detection on the 8E-CH corpus using two different classifiers.
SA Tagger Decision Tree
Overall error 3.2% 4.7%
Precision .57 .63
Recall .61 .51
F1 .59 .56
Typical classification time (SAs/sec) 10 1,000
Experiments and results. The question detection decision tree was trained on a set of
about 20,000 speech acts from the SWITCHBOARD corpus. We first evaluated the speech
act tagger and the decision tree classifier on the 8E-CH data set. Whereas in the later
stage of answer detection, questions without answers and nonpropositional questions
are ignored, at this point we are interested in the detection of all annotated questions
in the corpus. This also reflects the fact that the training set contains all possible types
of questions.
Table 14 reports the results of the question detection experiments with the two
classifiers used on the 8E-CH subcorpus. We note that whereas the decision tree is
performing only slightly worse than the speech act tagger, its typical classification
time is two orders of magnitude faster. Based on these observations, we decided to
use the question detection decision tree in the Q-A linking component of the DIASUMM
system.
5.5.5 Detecting the Answers. After identifying which sentences are questions, the
next step is to identify the answers to them. From the 8E-CH statistics of Table 13
we observe that for more than 75% of the YN- and Wh-questions, the answer is to
be found in the first sentence of the speaker talking after the speaker uttering the
question. In the remainder of cases, the majority of answers are in the second (instead
of the first) sentence of the responding speaker. Further, the speaker who has posed a
question usually utters no (or only very few) sentences after the question is asked and
before the next speaker starts talking.
In addition to detecting sequential Q-A pairs, we also want to be able to detect
simple embedded questions, as shown in this example of a brief clarification dialogue:
Q 1 A: When are we meeting then?
Q 2 B: You mean tomorrow?
3 A: Yes.
4 B: At 4pm.
We devise the following heuristics to detect answers to question speech acts which
have been previously identified:
? If the first speaker change after the question occurs more than maxChg
sentences after the question, the search is stopped and no Q-A pair is
returned.
? Answer hypotheses are sought for maximally maxSeek sentences after the
first speaker change following the question, but not over interruptions
by any other speaker; that is, we check within a single speaker region
471
Zechner Automatic Summarization of Dialogues
(this is the stopping criterion for the following two heuristics). An
exception occurs if there is an embedded question in the first single
speaker region: In that case, we look at the next region where a speaker
different from the initial Q-speaker is active.19
? Answers have to be minimally minAns words long; if they are shorter,
we add the next sentence to the current answer hypothesis.
? Even if the minimum answer length is reached, the answer can be
optionally extended if at least one word in the answer matches a word
from the question (one of two different stop lists (StopShort, StopLong) or
no stop list is used to remove function words from consideration).20
We have these further restrictions for the case of embedded questions:
1. If we detect a potential embedded Q-A pair, the answer to the
surrounding question must immediately follow the answer to the
embedded question (i.e., the region following the potential answer
region of the embedded question?sentence 4 in our above
example?must (1) not contain a question itself and (2) be from a
different speaker than the surrounding question).
2. A crossover is prohibited; that is, we eliminate all pairs ?Qj, Al? when a
pair ?Qi, Ak? was already detected, with i < j < k < l (k, l being start
indices of answer spans).
The output of the algorithm is a list of triples ?Q, Astart , Aend?, where Q is the
sentence ID of the question and Astart the first and Aend the last sentence of the
answer. As mentioned above, we use only 68 of the 83 questions marked in the 8E-
CH data set for these evaluations, since only these are YN- or Wh-questions that
actually have answers in the dialogue. There are four possible outcomes for each triple:
(1) irrelevant: a Q-A pair with an incorrectly hypothesized question (this is the fault
of the question detection module, not of this heuristic); (2) missed: the answer was
missed entirely; (3) completely correct: Aend coincides with the correct answer sentence
ID; and (4) correct range: the answer is contained in the interval [Astart , Aend ] but
does not coincide with Aend . For the calculation of precision, recall, and F1-score, we
count classes (3) and (4) as correct and use the sum of all classes for the denominator
of precision and the total number of Q-A pairs (68 in this development set) as the
denominator of recall.
To determine the best parameters, we varied them across a reasonable set of values
and ran the answer detection script for all combinations of parameters. The best results
(with respect to F1-score) using questions detected by the speech act tagger and the
decision tree are reported in Table 15. In the DIASUMM system, we use the following
optimal parameter settings for the answer detection heuristics: maxChg = 2, maxSeek =
4, minAns = 10, sim = on , stop = no.
Finally, we evaluated the performance of both the Q-detection module and the
combined Q-A detection on all five subcorpora, using the decision tree for question
detection; the results are reported in Table 16. Except for the rather small NEWSHOUR
19 This would be sentence 4 in the example above.
20 StopLong contains 571 words, StopShort only 89 words, most of which are auxiliary verbs and filler
words.
472
Computational Linguistics Volume 28, Number 4
Table 15
Q-A detection results using two different classifiers for question detection (68 Q-A pairs to be
detected).
SA Tagger Decision Tree
All hypothesized Q-A pairs 80 54
Correct [(3) and (4)] 42 31
maxChg (1?5) 4 2
maxSeek (2?4) 3?4 2?4
minAns (1?10) 5?10 2?10
Similarity extension (on/off) on on
Stop list (no/short/long) no/short no/short
Precision .53 .57
Recall .62 .46
F1-score .57 .51
Table 16
Performance comparison for Q- and Q-A detection (Q-detection with the decision tree
question classifier).
8E-CH 4E-CH NHOUR XFIRE G-MTG
Q to detect 83 94 19 110 49
Q-hypotheses 67 60 16 71 52
Q-detection (F1) .56 .58 .80 .60 .59
Q-A pairs to detect 68 69 18 79 32
Q-A pair hypotheses 54 54 14 54 33
Q-A detection (F1) .51 .60 .81 .51 .51
corpus (with fewer than 20 questions or Q-A pairs to identify), the typical Q-detection
F1-score is around .6 and the Q-A detection F1-score around .5. In two cases, the Q-A
detection performance is slightly better than the Q-detection performance. This can be
explained by the fact that the answer detection algorithm prunes away a number of
Q-hypotheses, reducing the space for potential Q-A hypotheses.
5.5.6 Q-A Detection within DIASUMM. When we use the Q-A detection module as
part of the DIASUMM system, we want to ensure that (1) there are no Q-A pairs con-
taining Q-sentences that are false starts and that (2) the initial part of an answer is
not lost in case the disfluency detection component marks some indicative words as
disfluencies. To satisfy the first constraint, we block Q-detection of sentences that have
been previously classified as false starts; as for the second constraint, we create a list
of indicative words (relevant for YN-questions) that are not to be removed by the
summary generator if they appear in the beginning (leading five words) of answers.21
5.6 Sentence Ranking and Selection
5.6.1 Introduction. The sentence ranking and selection component is an implementa-
tion of the MMR algorithm (Carbonell, Geng, and Goldstein 1997), applied to extract-
ing the most relevant sentences from a topical segment of a dialogue. The component?s
output in isolation serves as the MMR baseline for the global system evaluation. Its
21 The current list comprises the following words: no, yes, yeah, yep, sure, uh-huh, mhm, nope.
473
Zechner Automatic Summarization of Dialogues
purpose is to determine weights for terms and sentences, to rank the sentences ac-
cording to their relevance within each topical segment of the dialogue, and finally
to select the sentences for the summary output according to their rank, as well as to
other criteria, such as question-answer linkages, established by previous components.
The selected sentences are presented to the user in text order.
5.6.2 Tokenization. In addition to the tokenization rules for the global system (sec-
tion 5.2), we apply a simple six-character truncation for stemming and use a stop word
list to eliminate frequent noncontent words. In the experiments, we used the following
five different stop word lists:
? the original SMART list (Salton 1971) (SMART-O)
? a manually edited stop list based on SMART (SMART-M)
? a stop list with all closed-class words from the POS tagger?s lexicon
(POS-O)
? a manually edited stop list based on the POS tagger?s lexicon and
frequent closed-class words in the CALLHOME training corpus (POS-M)
? an empty stop list (EMPTY)
5.6.3 Term and Sentence Weighting. The basic idea for determining the most relevant
sentences within a topical segment is as follows: First, we compute a vector of word
weights for the segment tfq (including all stemmed non?stop words) and do the same
for each sentence (tft), then we compute the similarity between sentence and segment
vectors for each sentence. That way, sentences that have many words in common with
the segment vector are rewarded and receive a higher relevance weight.
Whereas we compose the sentence vectors tft using direct term frequency counts,
the weights for segment terms are determined according to one of the three formulae
in equation (1) (freq, smax, and log), inspired by Cornell University?s SMART system
(Salton 1971):
tfi,s = fi,s or 0.5 + 0.5
fi,s
fsmax
or 1 + log fi,s, (1)
where fi,s are the in-segment frequencies of a stem and fsmax are maximal segment
frequencies of any stem in the segment. Finally, we multiply an inverse document fre-
quency (IDF) weight to tfs to obtain the segment vectors tfq, as shown in equations (2)
and (3):
tfi,q = tfi,sIDFi,s (2)
IDFi,s = 1 + log
Nseg
iseg
or
Nseg
iseg
. (3)
IDF values are computed with respect to a collection of topical segments, either the
current dialogue (DIALOGUE) or a set of dialogues (CORPUS). Nseg is the total number
of topical segments in the IDF corpus, and iseg is the number of segments in which the
token i appears at least once. The effect of using IDF values is to boost those words
that are (relatively) unique to any given segment over those that are more evenly
distributed across the corpus.
As stated above, the main algorithm is a version of the MMR algorithm (Carbonell,
Geng, and Goldstein 1997; Carbonell and Goldstein 1998), which emphasizes sentences
474
Computational Linguistics Volume 28, Number 4
that contain many highly weighted terms for the current segment (salience) and are
sufficiently dissimilar to previously ranked sentences (diversity or antiredundancy).
The MMR formula is given in equation (4):
nextsentence = arg max
tnr,j
(?sim1(query , tnr,j) ? (1 ? ?) max
tr,k
sim2(tnr ,j , tr,k)). (4)
The MMR formula describes an iterative algorithm and states that the next sentence to
be put in the ranked list will be taken from the sentences that have not yet been ranked
(tnr ). This sentence is (1) maximally similar to a query and (2) maximally dissimilar
to the sentences that have already been ranked (tr). We use the topical segment word
vector tfq as query vector. The ? parameter (0.0 ? ? ? 1.0) is used to trade off the
influence of salience against that of redundancy.
Both similarity metrics (sim1, sim2) are inner vector products of stemmed-term
frequencies (equations (5) and (6)). sim1 can be normalized in different ways: (1) to
yield a cosine vector product (division by product of vector lengths), (2) division by
number of content words,22 and (3) no normalization:
sim1 =
tfq tft
| tfq||tft|
or
tfq tft
1 +
?
i tfi,t
or tfq tft (5)
sim2 =
tft1 tft2
| tft1|| tft2|
(6)
Emphasis factors. Every sentence?s similarity weight (sim1) can be (de)emphasized,
based on a number of its properties. We implemented optional emphasis factors for:
? Lead emphasis: for the leading n% of a segment?s sentences: sim ?1 = sim1l,
with l being the lead factor.
? Q-A emphasis: for all sentences that belong to a question-answer pair:
sim ?1 = sim1q, with q being the Q-A emphasis factor.
? False-start deemphasis: for all sentences that are false starts: sim ?1 = sim1f ,
with f being the false-start factor.
? Speaker emphasis: for each individual speaker s, an emphasis factor se can
be defined: sim ?1 = sim1se for all sentences of speaker s.
23
These parameters can serve to fine-tune the system for particular applications or user
preferences. For example, if the false starts are deemphasized, they are less likely
to trigger a question?s being linked to them in the linking process. If questions and
answers are emphasized, more of them will show up in the summary, increasing its
coherence and readability. In a situation in which a particular speaker?s statements are
of higher interest than those of other speakers, his sentences can be emphasized, as
well.
Since sim2 is a cosine vector product and hence in [0,1], we have to normalize
sim1 to [0,1] as well to enable a proper application of the MMR formula. For this
normalization of sim1, we divide each sim1 score by the maximum of all sim1 scores
in a segment after initial computation and application of the various emphasis factors
described here.
22 To avoid division by zero, we add one to every sentence length.
23 Speaker emphasis is not used in our evaluations.
475
Zechner Automatic Summarization of Dialogues
5.6.4 Q-A Linking. While generating the output summary from the MMR-ranked
list of sentences, whenever a question or an answer is encountered (detected previ-
ously by the Q-A detection module), the corresponding answer/question is linked to
it and moved up the relevance ranking list to immediately follow the current ques-
tion/answer. If the question-answer pair consists of more than two sentences, the link-
ages are repeated until no further questions or answers can be added to the current
linkage cluster.
5.6.5 Summary Types. DIASUMM can generate several different types of summaries,
the two main versions being (1) the CLEAN summary, which is based on the output
of all DIASUMM components (disfluency detection, sentence boundary detection, Q-A
linking), and (2) the TRANS summary, in which all dialogue specific components are
ignored (essentially, this is an MMR summary of the original dialogue transcript).
For the purpose of the global system evaluation, we use only these two versions of
summaries, as well as LEAD baseline summaries, where the summary is formed by
extracting the first n words from a topical segment.24
Furthermore, the system can generate phrasal summaries, which render the sen-
tences in the same ranking order as the CLEAN summary but reduce the output to
noun phrases and potentially other phrases, depending on the setting of parameters.25
In Figure 2 we show an example of a set of LEAD, TRANS, CLEAN, and PHRASAL
summaries. The set was generated from the CALLHOME transcript presented in sec-
tion 2.
5.6.6 System Tuning. This section describes how we arrive at an optimal parameter
setting for each subcorpus (CALLHOME, NEWSHOUR, CROSSFIRE, GROUP MEETINGS). We
want to establish an MMR baseline for the global system evaluations with which we
can then compare the results of the entire DIASUMM system. Note that for all the tuning
experiments described in this subsection, we did not make use of any other DIASUMM
components, namely, disfluency detection, sentence boundary detection, and question-
answer linking. All experiments were based on the human gold standard with respect
to topical segments. We used only the devtest set for the four subcorpora here (8E-
CH = CALLHOME, DT-NH = NEWSHOUR, DT-XF = CROSSFIRE, and DT-MTG = GROUP
MEETINGS).
Since the length of turns varies widely, one could argue that an easy way to in-
crease performance for the MMR baseline (which does not use automatic sentence
boundary detection) might be to split overly long turns evenly into shorter chunks.
This was done by Valenza et al (1999), who experimented with lengths of 10?30 words
per extract fragment. We add this option as an additional parameter to the MMR base-
line. If the parameter is set to n words, turns with a length l ? 1.5n get cut into pieces
of lengths n iteratively until the last remaining piece is l < 1.5n.
Evaluation metric. To evaluate the performance of this component, we use the
word-based evaluation metric described in section 6.2, which gives the highest scores
to summaries containing words with the highest average relevance scores, as marked
by human annotators. We then average these scores over all topical segment sum-
maries of a particular subcorpus.
24 Note that LEAD summaries are to be distinguished from summaries in which lead emphasis is used, as
described above. In the latter case, the segment-initial sentence weights are increased, whereas in the
former case, we strictly extract the leading n words from a given segment.
25 To determine these constituents, we use the output of the chunk parser employed by the false start
detection component.
476
Computational Linguistics Volume 28, Number 4
LEAD:
1 a: Oh
2 b: They didn?t know he was going to get shot but
it was at a peace rally so I mean it just worked out
3 b: I mean it was a good place for the poor guy to die I mean
because it was you know right after the rally and
everything was on film and everything [...]
TRANS:
2 b: They didn?t know he was going to get shot but
it was at a peace rally so I mean it just worked out
3 b: I mean it was a good place for the poor guy to die
I mean because it was you know right after the
rally and everything was on film and everything
11 b: Him [...]
CLEAN:
7 b: We just finished the thirty days mourning for him now
it?s everybody?s still in shock it?s terrible what?s
going on over here
31 b: What?s the reaction in america really do people care [...]
34 a: Most I don?t know what I mean like the jewish community
a lot all of us were very upset
PHRASAL:
4 b: it just worked ... it was a good
place for the poor guy to die ... it was [...]
7 b: we just finished the thirty days mourning
for him ... it?s ... everybody?s ...
in shock it?s ... going ...
31 b: ?s the reaction in america ... do people care ...
34 a: i don?t know ... mean like the jewish
community a lot ... of us were
Note: The turn IDs are just indicating the relative position of the turns within the original text and do not
always correspond to the turn numbers of the original or to the turn numbers of the other summaries. The
. . . marks the position in those sentences where the length threshold for a summary was reached.
Figure 2
Example summaries of 20% length: LEAD, TRANS, CLEAN and PHRASAL.
Parameter tuning. The system tuning proceeded in three phases, in which we held
the summary size constant to 15% and optimized the following set of parameters:
1. Term weight type: freq, smax, log
2. Normalization: cos, length, none
3. IDF type: corpus, dialogue, none
4. IDF method: log, mult
5. Extract span: 10?30 or original turn (orig)
477
Zechner Automatic Summarization of Dialogues
Table 17
Optimally tuned parameters for MMR baseline system (tuning on devtest set subcorpora).
8E-CH DT-NH DT-XF DT-Mtg
Term weight type smax smax smax smax
Normalization cos none cos none
IDF type corpus corpus corpus corpus
IDF method log log mult log
Extract span 20 orig 25 orig
MMR-? 0.85 0.8 1.0 0.8
Stop list SMART-M POS-M POS-M POS-M
Lead factor 1.0 1.0 1.0 2.0
6. MMR-?: 0.8?1.0
7. Stop lists: SMART-O, SMART-M, POS-O, POS-M, EMPTY
8. Lead factor: 1.0?5.0 (applied to first 20% of sentences)
Table 17 shows the parameter settings that were determined to be optimal for the
MMR baseline system (TRANS summaries).
5.7 System Performance
The majority of the system components are implemented in Perl5, except for the
C4.5 decision tree (Quinlan 1992), the chunk parser (Ward 1991), and the POS tagger
(Brill 1994), which were implemented in C by the respective authors. We measured the
system runtime on a 300 MHz Sun Ultra60 dual-processor workstation with 1 GB main
memory, summarizing all 23 dialogue excerpts from our corpus. The average runtime
for the whole system, including all of its components except for the topic segmentation
module, was 17.8 seconds, and for the sentence selection component alone 7.0 seconds
(per-dialogue average). The average ratio of system runtime to dialogue duration was
0.029 (2.9% of real speaking time).
6. Evaluation
6.1 Introduction
Traditionally, summarization systems have been evaluated in two major ways: (1) in-
trinsically, measuring the amount of the core information preserved from the original
text (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997), and (2) extrinsically,
measuring how much the summary can benefit in accomplishing another task (e.g.,
finding a document relevant to a query or classifying a document into a topical cate-
gory) (Mani et al 1998).
In this work, we focus on intrinsic evaluation exclusively. That is, we want to
assess how well the summaries preserve the essential information contained in the
original texts. As other studies have shown (Rath, Resnick, and Savage 1961; Marcu
1999), the level of agreement between human annotators about which passages to
choose to form a good summary is usually quite low. Our own findings, reported in
section 4.2.4, support this in that the intercoder agreement, here measured on a word
level, is rather low.
We decided to minimize the bias that would result from selecting either a par-
ticular human annotator, or even the manually created gold standard, as a reference
478
Computational Linguistics Volume 28, Number 4
for automatic evaluation; instead, we weigh all annotations from all human coders
equally. Intuitively, we want to reward summaries that contain a high number of
words considered to be relevant by most annotators. We formalize this notion in the
following subsection.
6.2 Evaluation Metric
All evaluations are based on topically coherent segments from the dialogue excerpts
of our corpus. As mentioned before, the segment boundaries were chosen from the
human gold standard for the purpose of the global system evaluation.
For each segment s, for each annotator a, and for each word position wi, we define
a boolean word vector of annotations ws,a, each component ws,a,i being 1 if the word
wi is part of a nucleus IU or a satellite IU for that annotator and segment, and 0
otherwise. We then sum over all annotators? annotation vectors and normalize them
by the number of annotators per segment (A) to obtain the average relevance vector
for segment s, rs:
rs,i =
?A
a=1 ws,a,i
A
. (7)
To obtain the summary accuracy score sas,n for any segment summary with length n,
we multiply the boolean summary vector summs26 by the average relevance vector rs,
and then divide this product by the sum of the n highest scores within rs (maximum
achievable score), rsorts being the vector rs sorted by relevance weight in descending
order:
sas,n =
summsrs
?n
i=1 rsorts,i
(8)
It is easy to see that the summary accuracy score always is in the interval [0.0, 1.0].
6.3 Global System Evaluation
Whereas section 5 was concerned with the design and evaluation of the individual
system components, the goal here is to describe and analyze the quality of the global
system, with all its components combined. In this section, we compare our DIASUMM
system with the MMR baseline system, which operates without any dialogue-specific
components, and with the LEAD baseline. We described the optimization and fine-
tuning of the MMR system in subsection 5.6.6. The second column of Table 18 presents
the average relevance scores for this MMR baseline, averaged over the five summary
sizes of 5%, 10%, 15%, 20%, and 25% length, for the four devtest set and the four eval
set subcorpora; the first column of this table shows the results for the LEAD baseline.
We used the optimized baseline MMR parameters and varied only the emphasis
parameters for (1) false starts, (2) lead factor, and (3) Q-A sentences, to optimize the
CLEAN summaries further. (Again, for this step, we used only the devtest subcorpora.)
For each corpus in the devtest set, we determined the optimal parameter settings and
report the corresponding results also for the eval set subcorpora. Column 3 in Table 18
provides the results for this optimized DIASUMM system. Further, in column 4, we pro-
vide the summary accuracy averages for the human gold standard (nucleus IUs only,
fixed-length summaries). Table 19 shows the best emphasis parameter combinations
for the DIASUMM summaries used in these evaluations.
We determined the statistical differences between the DIASUMM system and the
two baselines for the eval set, using the Wilcoxon rank sum test for each of the four
26 Definition: 1 if summs,i is contained in the summary, 0 otherwise.
479
Zechner Automatic Summarization of Dialogues
Table 18
Average summary accuracy scores: devtest set and eval set subcorpora on optimized
parameters, comparing LEAD, MMR baseline, DIASUMM, and the human gold standard.
Subcorpus LEAD MMR DIASUMM Gold [Nucleus IUs] (Size in %)
8E-CH 0.463 0.545 0.597 0.709 (13.1)
DT-NH 0.386 0.637 0.554 0.791 (20.9)
DT-XF 0.516 0.595 0.541 0.764 (11.4)
DT-MTG 0.488 0.594 0.606 0.705 (14.9)
4E-CH 0.438 0.526 0.614 0.793 (12.9)
EVAL-NH 0.692 0.526 0.506 0.850 (14.4)
EVAL-XF 0.378 0.564 0.566 0.790 (13.9)
EVAL-MTG 0.324 0.449 0.583 0.704 (16.0)
Table 19
Best emphasis parameters for the DIASUMM system, trained on the devtest set.
Corpus False Start Q-A Lead Factor
CALLHOME 0.5 1.0 2.0
NEWSHOUR 0.5 2.0 1.0
CROSSFIRE 0.5 1.0 1.0
GROUP MEETINGS 0.5 1.0 3.0
Table 20
Average summary accuracy scores for different system configurations for the four different
subcorpora.
Corpus LEAD MMR DFF-ONLY SB-ONLY NO-QA DIASUMM
4E-CH .438 .526 .599 .547 .603 .614
EVAL-NH .692 .526 .551 .608 .619 .506
EVAL-XF .378 .564 .528 .525 .537 .566
EVAL-GMTG .324 .449 .488 .513 .584 .583
subcorpora. Comparisons were made for each of the five summary sizes within each
topical segment. For the CALLHOME and GROUP MEETINGS subcorpora, our DIASUMM
system is significantly better than the MMR baseline (p < 0.01); for the two more formal
subcorpora, NEWSHOUR and CROSSFIRE, the differences between the performance of
the two systems are not significant. Except for on the NEWSHOUR subcorpus, both the
MMR baseline and the DIASUMM system perform significantly better than the LEAD
baseline.
6.4 Discussion
Table 20 shows the average performance of the following six system configurations,
averaged over all topical segments and all summary sizes (5?25% length summaries;
in configurations 3?5 below, components used are in addition to the core MMR sum-
marizer):
1. LEAD: using the first n% of the words in a segment
2. MMR: the MMR baseline (tuned; see above)
480
Computational Linguistics Volume 28, Number 4
3. DFF-ONLY: using the disfluency detection components (POS tagger,
false-start detection, repetition detection), but no sentence boundary
detection or question-answer linking
4. SB-ONLY: using the sentence boundary detection module, but no other
dialogue-specific modules
5. NO-QA: a combination of DFF-ONLY and SB-ONLY (all preprocessing
components used except for question-answer linking)
6. DIASUMM: complete system with all components (all disfluency detection
components, sentence boundary detection, and Q-A linking)
We observe that in all subcorpora, except for CROSSFIRE, the addition of either
the disfluency components or the sentence boundary component improves the sum-
mary accuracy over that of the MMR baseline. As we would expect, given the much
higher frequency of disfluencies in the two informal subcorpora (CALLHOME, GROUP
MEETINGS), the relative performance increase of DFF-ONLY over the MMR baseline is
much higher here (about 10?15%) than for the two more formal subcorpora (5% and
below). Looking at the performance increase of SB-ONLY, we find marked improve-
ments over the MMR baseline for those two subcorpora that use the true original turn
boundaries in the MMR baseline: GROUP MEETINGS and NEWSHOUR (>10%); for the
two other subcorpora, the improvement is below 5%. Furthermore, the combination
of the disfluency detection and sentence boundary detection components (NO-QA)
improves the results over the configurations DFF-ONLY and SB-ONLY.
The situation is much less uniform when we add the question-answer detection
component (this then corresponds to the full DIASUMM system): In the CROSSFIRE
corpus, we have the largest performance increase (we also have the highest relative
frequency of question speech acts here). For the two informal corpora, the change
is only minor; for NEWSHOUR, the performance decreases substantially. We showed
in Zechner and Lavie (2001), however, that in general, for dialogues with relatively
frequent Q-A exchanges, the accuracy of a summary (informativeness) does not change
significantly when the Q-A detection component is applied. On the other hand, the
(local) coherence of the summary does increase significantly, but we cannot measure
this increase with the evaluation criterion of summary accuracy used here.
To conclude, we have shown that using dialogue-specific components, with the
possible exception of the Q-A detection module, can help in creating more accurate
summaries for more informal, casual, spontaneous dialogues. When more formal con-
versations (which may even be partially scripted), containing relatively few disfluen-
cies, are involved, either a simple LEAD method or a standard MMR summarizer will
be much harder to improve upon.
7. Discussion and Directions for Future Work
The problem of how to generate readable and concise summaries automatically for
spoken dialogues of unrestricted domains involves many challenges that need to be
addressed. Some of the research issues are similar or identical to those faced in summa-
rizing written texts (such as topic segmentation, determining the most salient/relevant
information, anaphora resolution, summary evaluation), but other additional dimen-
sions are added on top of this list, including speech disfluency detection, sentence
boundary detection, cross-speaker information linking, and coping with imperfect
speech recognition. The line of argument of this article has been that whereas using a
481
Zechner Automatic Summarization of Dialogues
traditional approach for written text summarization (such as the MMR-based sentence
selection component within DIASUMM) may be a good starting point, addressing the
dialogue-specific issues is key for obtaining better summaries for informal genres.
We decided to focus on the three problems of (1) speech disfluency detection,
(2) sentence boundary detection, and (3) cross-speaker information linking and im-
plemented trainable system components to address each of these issues. Both the
evaluations of the individual components of our spoken-dialogue summarization sys-
tem and the global evaluations as well have shown that we can successfully make use
of the SWITCHBOARD corpus (LDC 1999b) to train a system that works well on two
other genres of informal dialogues, CALLHOME and GROUP MEETINGS. We conjecture
that the reasons why the DIASUMM system was not able to improve over the MMR
baseline for the two other corpora, which are more formal, lies in their very nature of
being of a quite different genre: the NEWSHOUR and CROSSFIRE corpora have longer
turns and sentences, as well as fewer disfluencies. We would also conjecture that their
sentence structures are more complex than what we typically find in the other corpora
of more colloquial, spontaneous conversations.
Future work will have to address the issue of whether the availability of train-
ing data for more formal dialogues (in size and annotation style comparable to the
SWITCHBOARD corpus, though) could lead to an improvement in performance on those
data sets, as well, or if even then a standard written-text-based summarizer would be
hard to improve upon.
Given the complexity of the task, we had to make a number of simplifying assump-
tions, most notably about the input data for our system: We use perfect transcripts by
humans instead of ASR transcripts, which, for these genres, typically show word error
rates (WERs) ranging from 15% to 35%. Previous related work (Valenza et al 1999;
Zechner and Waibel 2000b) demonstrated that the actual WERs in summaries gen-
erated from ASR output are usually substantially lower than the full-ASR-transcript
WER and can further be reduced by taking acoustically derived confidence scores into
account.
We further did not explore the potential improvements of components as well as
of the system overall when prosodic information such as stress and pitch is added as
an input feature. Past work in related fields (Shriberg et al 1998; Shriberg et al 2000)
suggests that particularly for ASR input, noticeable improvements might be achievable
when such input is provided.
Although presegmentation of the input into topically coherent segments certainly
is a useful step in summarization for any kind of texts (written or spoken), we have
not addressed and discussed this issue in this article.
Finally, we think that there is more work needed in the area of automatically
deriving discourse structures for spoken dialogues in unrestricted domains, even if
the text spans covered might be only local (because of a lack of global discourse
plans). We believe that a summarizer, in addition to knowing about the interactively
constructed and coherent pieces of information (such as in question-answer pairs),
could make good use of such structured information and be better guided in making
its selections for summary generation. In addition, this discourse structure might aid
modules that perform automatic anaphora detection and resolution.
8. Conclusions
We have motivated, implemented, and evaluated an approach for automatically cre-
ating extract summaries for open-domain spoken dialogues in informal and formal
genres of multiparty conversations. Our dialogue summarization system DIASUMM
482
Computational Linguistics Volume 28, Number 4
uses trainable components to detect and remove speech disfluencies (making the out-
put more readable and less noisy), to determine sentence boundaries (creating suitable
text spans for summary generation), and to link cross-speaker information units (al-
lowing for increased summary coherence).
We used a corpus of 23 dialogue excerpts from four different genres (80 topical seg-
ments, about 47,000 words) for system development and evaluation and the disfluency-
annotated SWITCHBOARD corpus (LDC 1999b) for training of the dialogue-specific com-
ponents. Our corpus was annotated by six human coders for topical boundaries and
relevant text spans for summaries. Additionally, we had annotations made for disflu-
encies, sentence boundaries, question speech acts, and the corresponding answers to
those question speech acts.
In a global system evaluation we compared the MMR-based sentence selection
component with the DIASUMM system using all of its components discussed in this arti-
cle. The results showed that (1) both a baseline MMR system as well as DIASUMM create
better summaries than a LEAD baseline (except for NEWSHOUR) and that (2) DIASUMM
performs significantly better than the baseline MMR system for the informal dialogue
corpora (CALLHOME and GROUP MEETINGS).
Acknowledgments
We are grateful to Alex Waibel, Alon Lavie,
Jaime Carbonell, Vibhu Mittal, Jade
Goldstein, Klaus Ries, Lori Levin, and
Marsal Gavalda` for many discussions,
suggestions, and comments regarding this
work. We also want to commend the corpus
annotators for their efforts. Finally, we want
to thank the four anonymous reviewers for
their detailed feedback on a preliminary
draft, which greatly helped improve this
article. This work was performed while the
author was affiliated with the Language
Technologies Institute at Carnegie Mellon
University and was supported in part by
grants from the U.S. Department of Defense.
References
Alexandersson, Jan and Peter Poller. 1998.
Towards multilingual protocol generation
for spontaneous speech dialogues. In
Proceedings of INLG-98,
Niagara-on-the-Lake, Canada, August.
Aone, Chinatsu, Mary Ellen Okurowski,
and James Gorlinsky. 1997. Trainable,
scalable summarization using robust NLP
and machine learning. In ACL/EACL-97
Workshop on Intelligent and Scalable Text
Summarization, Madrid.
Arons, Barry. 1994. Pitch-based emphasis
detection for segmenting speech. In
Proceedings of ICSLP-94, pages 1931?1934.
Berger, Adam L. and Vibhu O. Mittal. 2000.
OCELOT: A system for summarizing Web
pages. In Proceedings of the 23rd
ACM-SIGIR Conference.
Bett, Michael, Ralph Gross, Hua Yu, Xiaojin
Zhu, Yue Pan, Jie Yang, and Alex Waibel.
2000. Multimodal meeting tracker. In
Proceedings of the Conference on
Content-Based Multimedia Information Access
(RIAO-2000), Paris, April.
Brill, Eric. 1994. Some advances in
transformation-based part of speech
tagging. In Proceedings of AAAI-94.
Carbonell, Jaime, Yibing Geng, and Jade
Goldstein. 1997. Automated
query-relevant summarization and
diversity-based reranking. In Proceedings
of the IJCAI-97 Workshop on AI and Digital
Libraries, Nagoya, Japan.
Carbonell, Jaime and Jade Goldstein. 1998.
The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Proceedings of the
21st ACM-SIGIR International Conference on
Research and Development in Information
Retrieval, Melbourne, Australia.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Linguistics, 23(1):13?31.
Chen, Francine R. and Margaret Withgott.
1992. The use of emphasis to
automatically summarize a spoken
discourse. In Proceedings of ICASSP-92,
pages 229?332.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37?46.
Davies, Mark and Joseph L. Fleiss. 1982.
Measuring agreement for multinomial
data. Biometrics, 38:1047?1051, December.
Garofolo, John S., Ellen M. Voorhees, Cedric
G. P. Auzanne, and Vincent M. Stanford.
483
Zechner Automatic Summarization of Dialogues
1999. Spoken document retrieval: 1998
evaluation and investigation of new
metrics. In Proceedings of the ESCA
Workshop: Accessing Information in Spoken
Audio, pages 1?7, Cambridge, UK, April.
Garofolo, John S., Ellen M. Voorhees,
Vincent M. Stanford, and Karen Sparck
Jones. 1997. TREC-6 1997 spoken
document retrieval track overview and
results. In Proceedings of the 1997 TREC-6
Conference, pages 83?91, Gaithersburg,
MD, November.
Gavalda`, Marsal, Klaus Zechner, and
Gregory Aist. 1997. High performance
segmentation of spontaneous speech
using part of speech and trigger word
information. In Proceedings of the fifth ANLP
Conference, Washington, DC, pages 12?15.
Godfrey, J. J., E. C. Holliman, and
J. McDaniel. 1992. SWITCHBOARD:
Telephone speech corpus for research and
development. In Proceedings of ICASSP-92,
volume 1, pages 517?520.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intentions, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Hearst, Marti A. 1997. TextTiling:
Segmenting text into multi-paragraph
subtopic passages. Computational
Linguistics, 23(1):33?64.
Heeman, Peter A. and James F. Allen. 1999.
Speech repairs, intonational phrases, and
discourse markers: Modeling speakers?
utterances in spoken dialogue.
Computational Linguistics, 25(4):527?571.
Hirschberg, Julia, Steve Whittaker, Don
Hindle, Fernando Pereira, and Amit
Singhal. 1999. Finding information in
audio: A new paradigm for audio
browsing/retrieval. In Proceedings of the
ESCA Workshop: Accessing Information in
Spoken Audio, pages 117?122, Cambridge,
UK, April.
Hori, Chiori and Sadaoki Furui. 2000.
Automatic speech summarization based
on word significance and linguistic
likelihood. In Proceedings of ICASSP-00,
pages 1579?1582, Istanbul, Turkey, June.
Jurafsky, Daniel, Rebecca Bates, Noah
Coccaro, Rachel Martin, Marie Meteer,
Klaus Ries, Elizabeth Shriberg, Andreas
Stolcke, Paul Taylor, and Carol Van
Ess-Dykema. 1998. SwitchBoard discourse
language modeling project: Final report.
Research Note 30, Center for Language
and Speech Processing, Johns Hopkins
University, Baltimore, MD.
Kameyama, Megumi, and I. Arima. 1994.
Coping with aboutness complexity in
information extraction from spoken
dialogues. In Proceedings of ICSLP 94,
pages 87?90, Yokohama, Japan.
Kameyama, Megumi, Goh Kawai, and Isao
Arima. 1996. A real-time system for
summarizing human-human spontaneous
spoken dialogues. In Proceedings of
ICSLP-96, pages 681?684.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the AAAI.
Koumpis, Konstantinos and Steve Renals.
2000. Transcription and summarization of
voicemail speech. In Proceedings of
ICSLP-00, pages 688?691, Beijing, China,
October.
Krippendorff, Klaus. 1980. Content Analysis.
Sage, Beverly Hills, CA.
Kupiec, J., J. Pedersen, and F. Chen. 1995. A
trainable document summarizer. In
Proceedings of the 18th ACM-SIGIR
Conference, pages 68?73.
Lavie, Alon, Alex Waibel, Lori Levin,
Michael Finke, Donna Gates, Marsal
Gavalda`, Torsten Zeppenfeld, and
Puming Zhan. 1997. Janus III:
Speech-to-speech translation in multiple
languages. In IEEE International Conference
on Acoustics, Speech and Signal Processing,
Munich.
Levin, Lori, Klaus Ries, Ann Thyme?-Gobbel,
and Alon Lavie. 1999. Tagging of speech
acts and dialogue games in Spanish call
home. In Proceedings of the ACL-99
Workshop on Discourse Tagging, College
Park, MD.
Linguistic Data Consortium (LDC). 1996.
CallHome and CallFriend LVCSR
databases.
Linguistic Data Consortium (LDC). 1999a.
Addendum to the part-of-speech tagging
guidelines for the Penn Treebank project
(Modifications for the SwitchBoard
corpus). LDC CD-ROM LDC99T42.
Linguistic Data Consortium (LDC). 1999b.
Treebank-3: Databases of disfluency
annotated Switchboard transcripts. LDC
CD-ROM LDC99T42.
Mani, Inderjeet, David House, Gary Klein,
Lynette Hirschman, Leo Obrst, Therese
Firmin, Michael Chrzanowski, and Beth
Sundheim. 1998. The TIPSTER SUMMAC
text summarization evaluation. Technical
Report MTR 98W0000138, Mitre
Corporation, October 1998.
Mani, Inderjeet and Mark T. Maybury,
editors. 1999. Advances in Automatic Text
Summarization. MIT Press, Cambridge.
Marcu, Daniel. 1999. Discourse trees are
good indicators of importance in text. In
I. Mani and M. T. Maybury, editors,
484
Computational Linguistics Volume 28, Number 4
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 123?136.
Meteer, Marie, Ann Taylor, Robert
MacIntyre, and Rukmini Iyer. 1995.
Dysfluency annotation stylebook for the
Switchboard corpus. Linguistic Data
Consortium (LDC) CD-ROM LDC99T42.
Miike, Seiji, Etuso Itoh, Kenji Onon, and
Kazuo Sumita. 1994. A full-text retrieval
system with a dynamic abstract
generation function. In Proceedings of the
17th ACM-SIGIR Conference, pages 318?
327.
Nakatani, Christine H. and Julia Hirschberg.
1994. A corpus-based study of repair cues
in spontaneous speech. Journal of the
Acoustic Society of America, 95(3):1603?1616.
Passonneau, Rebecca J. and Diane J. Litman.
1997. Discourse segmentation by human
and automated means. Computational
Linguistics, 23(1):103?139.
Quinlan, J. Ross. 1992. C4.5: Programs for
Machine Learning. Morgan Kaufmann, San
Mateo, CA.
Rath, G. J., A. Resnick, and T. R. Savage.
1961. The formation of abstracts by the
selection of sentences. American
Documentation, 12(2):139?143.
Reimer, U. and U. Hahn. 1988. Text
condensation as knowledge base
abstraction. In Proceedings of the fourth
Conference on Artificial Intelligence
Applications, pages 338?344, San Diego.
Reithinger, Norbert, Michael Kipp, Ralf
Engel, and Jan Alexandersson. 2000.
Summarizing multilingual spoken
negotiation dialogues. In Proceedings of the
38th Conference of the Association for
Computational Linguistics, pages 310?317,
Hong Kong, China, October.
Ries, Klaus, Lori Levin, Liza Valle, Alon
Lavie, and Alex Waibel. 2000. Shallow
discourse genre annotation in
CALLHOME Spanish. In Proceedings of the
Second Conference on Language Resources and
Evaluation (LREC-2000), Athens,
May/June.
Rose, Ralph Leon. 1998. The Communicative
Value of Filled Pauses in Spontaneous Speech.
Ph.D. thesis, University of Birmingham,
Birmingham, UK.
Salton, Gerard, editor. 1971. The SMART
Retrieval System?Experiments in Automatic
Text Processing. Prentice Hall, Englewood
Cliffs, NJ.
Santorini, Beatrice. 1990. Part-of-Speech
Tagging guidelines for the Penn Treebank
project. Linguistic Data Consortium
(LDC) CD-ROM LDC99T42.
Shriberg, Elizabeth E. 1994. Preliminaries to a
Theory of Speech Disfluencies. Ph.D. thesis,
University of Berkeley, Berkeley.
Shriberg, Elizabeth, Rebecca Bates, Andreas
Stolcke, Paul Taylor, Daniel Jurafsky,
Klaus Ries, Noah Coccaro, Rachel Martin,
Marie Meteer, and Carol Van
Ess-Dykema. 1998. Can prosody aid the
automatic classification of dialog acts in
conversational speech? Language and
Speech, 41(3?4):439?487.
Shriberg, Elizabeth, Andreas Stolcke, Dilek
Hakkani-Tu?r, and Go?khan Tu?r. 2000.
Prosody-based automatic segmentation of
speech into sentences and topics. Speech
Communication, 32(1?2):127?154.
Stifelman, Lisa J. 1995. A discourse analysis
approach to structured speech. In
AAAI-95 Spring Symposium on Empirical
Methods in Discourse Interpretation and
Generation, Stanford, March.
Stolcke, Andreas, Klaus Ries, Noah Coccaro,
Elizabeth Shriberg, Rebecca Bates, Daniel
Jurafsky, Paul Taylor, Rachel Martin,
Carol Van Ess-Dykema, and Marie
Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of
conversational speech. Computational
Linguistics, 26(3):339?373.
Stolcke, Andreas and Elizabeth Shriberg.
1996. Automatic linguistic segmentation
of conversational speech. In Proceedings of
ICSLP-96, pages 1005?1008.
Stolcke, Andreas, Elizabeth Shriberg,
Rebecca Bates, Mari Ostendorf, Dilek
Hakkani, Madeleine Plauche, Go?khan
Tu?r, and Yu Lu. 1998. Automatic
detection of sentence boundaries and
disfluencies based on recognized words.
In Proceedings of ICSLP-98, volume 5,
pages 2247?2250, Sydney, December.
Teufel, Simone and Marc Moens. 1997.
Sentence extraction as a classification task.
In ACL/EACL-97 Workshop on Intelligent
and Scalable Text Summarization, Madrid.
Valenza, Robin, Tony Robinson, Marianne
Hickey, and Roger Tucker. 1999.
Summarisation of spoken audio through
information extraction. In Proceedings of
the ESCA Workshop: Accessing Information in
Spoken Audio, pages 111?116, Cambridge,
UK, April.
Wahlster, Wolfgang. 1993.
Verbmobil?Translation of face-to-face
dialogs. In Proceedings of MT Summit IV,
Kobe, Japan.
Waibel, Alex, Michael Bett, and Michael
Finke. 1998. Meeting browser: Tracking
and summarizing meetings. In Proceedings
of the DARPA Broadcast News Workshop.
Ward, Wayne. 1991. Understanding
spontaneous speech: The PHOENIX
system. In Proceedings of ICASSP-91,
485
Zechner Automatic Summarization of Dialogues
pages 365?367.
Whittaker, Steve, Julia Hirschberg, John
Choi, Don Hindle, Fernando Pereira, and
Amit Singhal. 1999. SCAN: Designing and
evaluating user interfaces to support
retrieval from speech archives. In
Proceedings of the 22nd ACM-SIGIR
International Conference on Research and
Development in Information Retrieval,
pages 26?33, Berkeley, August.
Zechner, Klaus and Alon Lavie. 2001.
Increasing the coherence of spoken
dialogue summaries by cross-speaker
information linking. In Proceedings of the
NAACL-01 Workshop on Automatic
Summarization, pages 22?31, Pittsburgh,
June.
Zechner, Klaus and Alex Waibel. 2000a.
DIASUMM: Flexible summarization of
spontaneous dialogues in unrestricted
domains. In Proceedings of COLING-2000,
pages 968?974, Saarbru?cken, Germany,
July/August.
Zechner, Klaus and Alex Waibel. 2000b.
Minimizing word error rate in textual
summaries of spoken language. In
Proceedings of the First Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL-2000),
pages 186?193, Seattle, April/May.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 216?223,
New York, June 2006. c?2006 Association for Computational Linguistics
Towards Automatic Scoring of Non-Native Spontaneous Speech 
Klaus Zechner and Isaac I. Bejar 
Educational Testing Service 
Princeton, NJ, USA 
(kzechner,ibejar)@ets.org 
 
 
Abstract 
This paper investigates the feasibility of 
automated scoring of spoken English 
proficiency of non-native speakers. 
Unlike existing automated assessments 
of spoken English, our data consists of 
spontaneous spoken responses to 
complex test items. We perform both a 
quantitative and a qualitative analysis of 
these features using two different 
machine learning approaches. (1)  We 
use support vector machines to produce 
a score and evaluate it with respect to a 
mode  baseline and to human rater 
agreement. We find that scoring based 
on support vector machines yields 
accuracies approaching inter-rater 
agreement in some cases. (2) We use 
classification and regression trees  to 
understand the role of different features 
and feature classes in the 
characterization of speaking proficiency 
by human scorers. Our analysis shows 
that across all the test items most or all 
the feature classes are used in the nodes 
of the trees suggesting that the scores 
are, appropriately, a combination of 
multiple components of speaking 
proficiency. Future research will 
concentrate on extending the set of 
features and introducing new feature 
classes to arrive at a scoring model that 
comprises additional relevant aspects of 
speaking proficiency. 
1 Introduction 
While automated scoring of open-ended written 
discourse has been approached by several 
groups recently (Rudner & Gagne, 2001; Sher-
mis & Burstein, 2003), automated scoring of 
spontaneous spoken language  has proven to be 
more challenging and complex. Spoken lan-
guage tests are still mostly scored by human rat-
ers.  However, several systems exist that score 
different aspects of spoken language; (Bernstein, 
1999; C. Cucchiarini, H. Strik, & L. Boves, 
1997a; Franco et al, 2000).  Our work departs 
from previous research in that our goal is to 
study the feasibility of automating scoring for 
spontaneous speech, that is, when the spoken 
text is not known in advance.  
We approach scoring here as the characteri-
zation of a speaker?s oral proficiency based on 
features that can be extracted from a spoken re-
sponse to a well defined test question by means 
of automatic speech recognition (ASR).   We 
further approach scoring as the construction of a 
mapping from a set of features to a score scale, 
in our case five discrete scores from 1 (least pro-
ficient) to 5 (most proficient).  The set of fea-
tures and the specific mapping are motivated by 
the concept of communicative competence 
(Bachman, 1990; Canale & Swain, 1980; 
Hymes, 1972). This means that the features in 
the scoring system we are developing are meant 
to characterize specific components of commu-
nicative competence, such as mastery of pronun-
ciation, fluency, prosodic, lexical, grammatical 
and pragmatical subskills. The selection of fea-
tures is guided by an understanding of the nature 
of speaking proficiency.   We rely on the scoring 
behavior of judges to evaluate the features (sec-
tion 8) as well as a convenient criterion for 
evaluating the feasibility of automated scoring 
based on those features (section 7).  That is, the 
role of human scorers in this context is to pro-
vide a standard for system evaluations (see sec-
tion 7), as well as to validate specific features 
and feature classes chosen by the authors (sec-
tion 8). We use support vector machines (SVMs) 
 1
216
to determine how well the features recover hu-
man scores. We collect performance data under 
three different conditions, where features are 
either based on actual recognizer output or on 
forced alignment. (Forced alignment describes a 
procedure in speech recognition where the rec-
ognizer is looking for the most likely path 
through the Hidden Markov Models given a 
transcription of the speech file by an experi-
enced transcriber. This helps, e.g., in finding 
start and end times of words or phonemes.)  We 
then use classification and regression trees 
(CART) as a means to evaluate the relative im-
portance and salience of our features. When the 
classification criterion is a human score, as is the 
case in this study, an inspection of the CART 
tree can give us insights into the feature prefer-
ences a human judge might have in deciding on 
a score.  
The organization of this paper is as follows: 
first, we discuss related work in spoken lan-
guage scoring. Next, we introduce the data of 
our study and the speech recognizer used. In 
section 5 we describe features we used for this 
study. Section 6 describes the agreement among 
raters for this data. Section 7 describes the SVM 
analysis, section 8 the CART analysis.  This is 
followed by a discussion and then finally by 
conclusions and an outlook on future work. 
2 Related work 
There has been previous work to characterize 
aspects of communicative competence such as 
fluency, pronunciation, and prosody.   (Franco et 
al., 2000) present a system for automatic evalua-
tion of pronunciation performance on a phone 
level and a sentence level of native and non-
native speakers of English and other languages 
(EduSpeak). Candidates read English text and a 
forced alignment between the speech signal and 
the ideal path through the Hidden Markov 
Model (HMM) was computed. Next, the log 
posterior probabilities for pronouncing a certain 
phone at a certain position in the signal were 
computed to achieve a local pronunciation score. 
These scores are then combined with other 
automatically derived measures such as the rate 
of speech (number of words per second) or the 
duration of phonemes to yield global scores. 
(C. Cucchiarini, S. Strik, & L. Boves, 
1997b)) and (Cucchiarini et al, 1997a)) describe 
a system for Dutch pronunciation scoring along 
similar lines. Their feature set, however, is more 
extensive and contains, in addition to log likeli-
hood Hidden Markov Model scores, various du-
ration scores, and information on pauses, word 
stress, syllable structure, and intonation. In an 
evaluation, they find good agreement between 
human scores and machine scores. 
(Bernstein, 1999)) presents a test for spo-
ken English (SET-10) that has the following 
types of items: reading, repetition, fill-in-the-
blank, opposites and open-ended answers. All 
types except for the last are scored automatically 
and a score is reported that can be interpreted as 
an indicator of how native-like a speaker?s 
speech is. In (Bernstein, DeJong, Pisoni, & 
Townshend, 2000), an experiment is performed 
to establish the generalizability of the SET-10 
test.  It is shown that this test?s output can suc-
cessfully be mapped to the Council of Europe?s 
Framework for describing second language pro-
ficiency (North, 2000). This paper further re-
ports on studies done to correlate the SET-10 
with two other tests of English proficiency, 
which are scored by humans and where commu-
nicative competence is tested for. Correlations 
were found to be between 0.73 and 0.88.  
3 Data 
The data we are using for the experiments 
in this paper comes from a 2002 trial administra-
tion of TOEFLiBT? (Test Of English as a For-
eign Language?internet-Based Test) for non-
native speakers (LanguEdge ?).  Item responses 
were transcribed from the digital recording of 
each response. In all there are 927 responses 
from 171 speakers.  Of these, 798 recordings 
were from one of five main test items, identified 
as P-A, P-C, P-T, P-E and P-W.  The remaining 
129 responses were from other questions.  As 
reported below, we use all 927 responses in the 
adaptation of the speech recognizer but the SVM 
and CART analyses are based on the 798 re-
sponses to the five test items. Of the five test 
items, three are independent tasks (P-A, P-C, P-
T) where candidates have to talk freely about a 
certain topic for 60 seconds. An example might 
be ?Tell me about your favorite teacher.? Two of 
 2
217
the test items are integrated tasks (P-E, P-W) 
where candidates first read or listen to some ma-
terial to which they then have to relate in their 
responses (90 seconds speaking time). An ex-
ample might be that the candidates listen to a 
conversational argument about studying at home 
vs. studying abroad and then are asked to sum-
marize the advantages and disadvantages of both 
points of view. 
The textual transcription of our data set con-
tains about 123,000 words and the audio files are 
in WAV format and recorded with a sampling 
rate of 11025Hz and a resolution of 8 bit. 
For the purpose of adaptation of the speech 
recognizer, we split the full data (927 re-
cordings) into a training (596) and a test set (331 
recordings).   For the CART and SVM analyses 
we have 511 files in the train and 287 files in 
the eval set, summing up to 798. (Both data sets 
are subsets from the ASR adaptation training 
and test sets, respectively.)  The transcriptions of 
the audio files were done according to a tran-
scription manual derived from the German 
VerbMobil project (Burger, 1995). A wide vari-
ety of disfluencies are accounted for, such as, 
e.g., false starts, repetitions, fillers, or incom-
plete words.  One single annotator transcribed 
the complete corpus; for the purpose of testing 
inter-coder agreement, a second annotator tran-
scribed about 100 audio files, which were ran-
domly selected from the complete set of 927 
files. The disagreement between annotators, 
measured as word error rate (WER = (substitu-
tions + deletions + insertions) / (substitutions + 
deletions + correct)) was slightly above 20% 
(only lexical entries were measured here). This 
is markedly more disagreement than in other 
corpora, e.g., in SwitchBoard (Meteer & al., 
1995) where disagreements in the order of 5% 
are reported, but we have non-native speech 
from speakers at different levels of proficiency 
which is more challenging to transcribe. 
4 Speech recognition system 
Our speech recognizer is a gender-independent 
Hidden Markov Model system that was trained 
on 200 hours of dictation data by native speakers 
of English. 32 cepstral coefficients are used; the 
dictionary has about 30,000 entries. The sam-
pling rate of the recognizer is 16000Hz as op-
posed to 11025Hz for the LanguEdge? corpus. 
The recognizer can accommodate this difference 
internally by up-sampling the input data stream.  
As our speech recognition system was 
trained on data quite different from our applica-
tion (dictation vs. spontaneous speech and native 
vs. non-native speakers) we adapted the system 
to the LanguEdge ? corpus.  We were able to 
increase word accuracy on the unseen test set 
from 15% before adaptation to 33% in the fully 
adapted model (both acoustic and language 
model adaptation).  
5 Features 
Our feature set, partly inspired by (Cucchiarini 
et al, 1997a), focuses on low-level fluency fea-
tures, but also includes some features related to 
lexical sophistication and to content. The feature 
set alo stems, in part, from the written guide-
lines used by human raters for scoring this data. 
The features can be categorized as follows: (1) 
Length measures, (2) lexical sophistication 
measures, (3) fluency measures, (4) rate meas-
ures, and (5) content measures. Table 1 renders a 
complete list of the features we computed, along 
with a brief explanation.  We do not claim these 
features to provide a full characterization of 
communicative competence; they should be seen 
as a first step in this direction.  The goal of the 
research is to gradually build such a set of fea-
tures to eventually achieve as large a coverage 
of communicative competence as possible.  The 
features are computed based on the output of the 
recognition engine based on either forced align-
ment or on actual recognition. The output con-
sists of (a) start and end time of every token and 
hence potential silence in between (used for 
most features); (b) identity of filler words (for 
disfluency-related features); and (c) word iden-
tity (for content features). 
 
 
 
 
 3
218
  
Lexical counts and length measures 
Segdur Total duration in seconds of all the utterances 
Numutt Number of utterances in the response 
Numwds Total number of word forms in the speech sample  
Numdff Number of disfluencies (fillers) 
Numtok Number of tokens = Numwds+Numdff 
Lexical sophistication  
Types Number of unique word forms in the speech sample 
Ttratio Ratio Types/Numtok (type-token ratio, TTR)  
Fluency measures 
(based on pause information) 
Numsil Number of silences, excluding silences between utterances  
Silpwd Ratio Numsil/Numwds 
Silmean  Mean duration in seconds of all silences in a response to a test item 
Silstddv Standard deviation of silence duration 
Rate measures  
Wpsec Number of words per second 
Dpsec. Number of disfluencies per second 
Tpsec Number of types per second 
Silpsec. Number of silences per second 
Content measures We first compute test-item-specific word vectors with the frequency 
counts of all words occurring in the train set for each test item 
(wvec_testitem). Then we generate for every item response a word 
vector in kind (wvec_response) and finally compute the inner prod-
uct to yield a similarity score:  
sim = wvec_testitem*wvec_response 
Cvfull  wvec_testitem*wvec_response 
6 other Cv*-features As Cvfull but measure similarity to a subset of wvec_testitem, based 
on the scores in the train set (e.g., ?all responses with score 1?) 
Cvlennorm Length-normalized Cvfull: Cvfull/Numwds  
Table 1: List of features with definitions. 
 
6 Inter-rater agreement 
The training and scoring procedures followed 
standard practices in large scale testing.  Scorers 
are trained to apply the scoring standards that 
have been previously agreed upon by the devel-
opers of the test.  The training takes the form of 
discussing multiple instances of responses at 
each score level. The scoring of the responses 
used for training other raters is done by more 
experienced scorers working closely with the 
designers of the test.   
All the 927 speaking samples (see section 3) 
were rated once by one of several expert raters, 
which we call Rater1. A second rating was ob-
tained for approximately one half (454) of the 
speaking samples, which we call Rater2.  We 
computed the exact agreement for all Rater1-
Rater2 pairs for all five test items and report the 
results in the last column of Table 2. Overall, the 
exact agreement was about 49% and the kappa 
coefficient 0.34.  These are rather low numbers 
and certainly demonstrate the difficulty of the 
rating task for humans. Inter-rater agreement for 
integrated tasks is lower than for independent 
tasks.  We conjecture that this is related to the 
dual nature of scoring integrated tasks: for one, 
the communicative competence per se needs to 
be assessed, but on the other hand so does the 
correct interpretation of the written or auditory 
stimulus material. The low agreement in general 
is also understandable since the number of fea-
ture dimensions that have to be mentally inte-
 4
219
grated pose a significant cognitive load for 
judges.1   
7 SVM models 
As we have mentioned earlier, the rationale be-
hind using support vector machines for score 
prediction is to yield a quantitative analysis of 
how well our features would work in an actual 
scoring system, measured against human expert 
raters. The choice of the particular classifier be-
ing SVMs was due to their superior performance 
in many machine learning tasks. 
7.1 Support vector machines 
Support vector machines (SVMs) were in-
troduced by (Vapnik, 1995) as an instantiation 
of his approach to model regularization. They 
attempt to solve a multivariate discrete classifi-
cation problem where an n-dimensional hyper-
plane separates the input vectors into, in the 
simplest case, two distinct classes. The optimal 
hyperplane is selected to minimize the classifi-
cation error on the training data, while maintain-
ing a maximally large margin (the distance of 
any point from the separating hyperplane). 
 
                                                          
1 Inter-human agreement rates for written language, such as 
essays, are significantly higher, around 70-80% with a 5-
point scale (Y.Attali, personal communication). More re-
cently we observed agreement rates of about 60% for spo-
ken test items, but here a 4-point scale was used. 
7.2 Experiments 
We built five SVM models based on the 
train data, one for each of the five test items. 
Each model has two versions: (a) based on 
forced alignment with the true reference, repre-
senting the case with 100% word accuracy 
(align), and (b) based on the actual recognition 
output hypotheses (hypo). The SVM models 
were tested on the eval data set and there were 
three test conditions: (1) both training and test 
conditions derived from forced alignment (align-
align); (2) models trained on forced alignment 
and evaluated based on actual recognition hy-
potheses (align-hypo; this represented the realis-
tic situation that while human transcriptions are 
made for the training set, they would turn out to 
be too costly when the system is running con-
tinuously); and (3) both training and evaluation 
are based on ASR output in recognition mode 
(hypo-hypo). 
We identified the best models by running a 
set of SVMs with varying cost factors, ranging 
from 0.01 to 15, and three different kernels: ra-
dial basis function, and polynomial, of second 
degree and of third degree. We selected the best 
performing models measured on the train set 
and report results with these models on the eval 
set. The cost factor for all three configurations 
varied between 5 and 15 among the five test 
items, and as best kernel we found the radial 
basis function in almost all cases, except for 
some polynomial kernels in the hypo-hypo con-
figuration
 
 
Mode 
(% of 
eval 
set) 
Train : align 
Eval : align 
 
Train : align 
Eval : hypo 
Train : hypo 
Eval : hypo 
Human Rater 
Agreement (% 
of all pairs) 
P-A (ind) 34 40.7 33.9 35.9 53 
P-C (ind) 53 50.0 55.0 56.7 57 
P-T (ind) 38 43.4 18.9 37.7 54 
P-E (int) 25 42.1 26.3 47.4 43 
P-W (int) 29 34.5 20.7 39.7 42 
Table 2: Speech scoring:  Mode baseline, SVM performance on forced alignment and standard recogni-
tion data, and human agreement for all five test items (ind=independent task; int=integrated task). 
 
5
220
7.3 Results 
Table 2 shows the results for the SVM analysis 
as well as a baseline measure of agreement and 
the inter rater agreement.    The baseline refers 
to the expected level of agreement with Rater1 
by simply assigning the mode of the distribution 
of scores for a given question, i.e., to always 
assign the most frequently occurring score on 
the train set.  Table 2 also reports the agreement 
between trained raters. As can be seen the hu-
man agreement is consistently higher than the 
mode agreement but the difference is less for the 
integrated questions suggesting that humans 
scorers found those questions more challenging 
to score consistently.   
The other 3 columns of Table 2 report the 
results for the perfect agreement between a score 
assigned by the SVM developed for that test 
question and Rater1 on the eval corpus, which 
was not used in the development of the SVM. 
We observe that for the align-align configura-
tion, accuracies are all clearly better than the 
mode baseline, except for P-C, which has an 
unusually skewed score distribution and there-
fore a rather high mode baseline.  In the align-
hypo case, where SVM models were built based 
on features derived from ASR forced alignment 
and where these models were tested using ASR 
output in recognition mode, we see a general 
drop in performance ? again except for P-C ? 
which is to be expected as the training and test 
data were derived in different ways.  Finally, in 
the hypo-hypo configuration, using ASR recog-
nition output for both training and testing, SVM 
models are, in comparison to the align-align 
models, improved for the two integrated tasks 
but not for the independent tasks, again except 
for P-C. The SVM classification accuracies for 
the integrated tasks are in the range of human 
scorer agreement, which indicates that a per-
formance ceiling may have been reached al-
ready. These results suggest that the recovery of 
scores is more feasible for integrated rather than 
independent tasks.  However, it is also the case 
that human scorers had more difficulty with the 
integrated tasks, as discussed in the previous 
section. 
The fact that the classification performance of 
the hypo-hypo models is not greatly lower than 
that of the align-align models, and in some cases 
even higher ---and that with the relatively low 
word accuracy of 33%---, leads to our conjecture 
that this could be due to the majority of features 
being based on measures which do not require a 
correct word identity such as measures of rate or 
pauses. 
In a recent study (Xi, Zechner, & Bejar, 2006) 
with a similar speech corpus we found that while 
the hypo-hypo models are better than the align-
align models when using features related to flu-
ency, the converse is true when using word-
based vocabulary features.  
8 CART models 
 
8.1 Classification and regression trees 
 
Classification and regression trees (CART trees)  
were introduced by (Breiman, Friedman, Ol-
shen, & Stone, 1984). The goal of a classifica-
tion tree is to classify the data such that the data 
in the terminal or classification nodes is as pure 
as possible meaning all the cases have the same 
true classification, in the present case a score 
provided by a human rater, the variable Rater1 
above.  At the top of the tree all the data is avail-
able and is split into two groups based on a split 
of one of the features available.  Each split is 
treated in the same manner until no further splits 
are possible, in which case a terminal node has 
been reached.  
8.2 Tree analysis 
For each of the five test items described above 
we estimated a classification tree using as inde-
pendent variables the features described in Table 
1 and as the dependent variable a human score.  
The trees were built on the train set. Table 3 
shows the distribution of features in the CART 
tree nodes of the five test items (rows) based on 
feature classes (columns).  For P-A, for exam-
ple, it can be seen that three of the feature 
classes have a count greater than 0.   The last 
column shows the number of classes appearing 
in the tree and the number of total features, in 
parentheses.  The P-A tree, for example has six 
features from three classes.  The last row sum-
marizes the number of test items that relied on a 
feature class and the number of features from 
 6
221
that class across all five test items, in parenthe-
sis.  For example, Rate and Length were present 
in every test item and lexical sophistication was 
present in all but one test item.   The table sug-
gests that across all test items there was good 
coverage of feature classes but length was espe-
cially well represented.  This is to be expected 
with a group heterogeneous in speaking profi-
ciency.  The length features often were used to 
classify students in the lower scores, that is, stu-
dents who could not manage to speak suffi-
ciently to be responsive to the test item.  
9 Discussion 
9.1 Speech recognition 
We successfully adapted an off-the-shelf speech 
recognition engine for the purpose of assessing 
spontaneous speaking proficiency.  By acoustic 
and language model adaptation, we were able to 
markedly increase our speech recognition en-
gine?s word accuracy, from initially 15% to 
eventually 33%.  Although a 33% recognition 
rate is not high by current standards, the hurdles 
to higher recognition are significant, including 
the fact that the recognizer?s acoustic model was 
originally trained on quite different data, and the 
fact that our data is based on highly accented 
speech from non-native speakers of English of a 
range of proficiencies, which are harder to rec-
ognize than native speakers. 
9.2 SVM and CART models 
Our goal in this research has been to develop 
models for automatically scoring communicative 
competence in non-native speakers of English. 
The approach we took is to compute features 
from ASR output that may eventually serve as 
indicators of communicative competence.  We 
evaluated those features (a) in quantitative re-
spect by using SVM models for score prediction 
and (b) in qualitative respect in terms of their 
roles in assigning scores based on a human crite-
rion by means of CART analyses. 
We found in the analysis of the SVM mod-
els that despite low word accuracy, with ASR 
recognition as a basis for training and testing, 
scores near inter-rater agreement levels can be 
reached for those items that include a listening 
or reading passage. When simulating perfect 
word accuracy (in the align-align configuration), 
4 of 5 test items achieve scoring accuracies 
above the mode baseline. These results are very 
encouraging in the light that we are continuing 
to add features to the models on various levels of 
speech proficiency. 
 
 
Test item Length Lexical  
sophistication 
Fluency Rate Content Total: 
# classes  
(# features) 
P-A 4 1 0 1 0 3 (6) 
P-C 4 0 1 1 1 4 (7) 
P-T 2 1 0 1 1 4 (5) 
P-E 1 1 2 1 1 5 (6) 
P-W 1 2 0 1 0 3 (4) 
Total # 
classes (# 
features) 
5 (12) 4 (5) 2 (3) 5 (5) 3 (3) 19 (28) 
Table 3: Distribution of features from the nodes of five CART trees (rows) into feature classes (columns). The ?to-
tals? in the last colunmn and row count first the number of classes with at least one feature and then sums the fea-
tures (in parentheses).   
 
 
 
 
 
 
 
 
 
 
 
 7
222
CART trees have the advantage of being in-
spectable and interpretable (unlike, e.g., neural 
nets or support vector machines with non-linear 
kernels). It is easy to trace a path from the root 
of the tree to any leaf node and record the final 
decisions made along the way. We looked at the 
distribution of features in these CART tree 
nodes (Table 3) and
found that all the different categories of features 
were used by the set of trees. For all 5 test items, 
most classes occurred in the nodes of the respec-
tive CART trees (with a minimum of 3 out of 5 
classes).     
10 Conclusions and future work 
This paper is concerned with explorations into 
scoring spoken language test items of non-native 
speakers of English. We demonstrated that an ex-
tended feature set comprising features related to 
length, lexical sophistication, fluency, rate and 
content could be used to predict human scores in 
SVM models and to illuminate their distribution 
into five different classes by means of a CART 
analysis.  
An important step for future work will be to 
train the acoustic and language models of the 
speech recognizer directly from our corpus; we are 
additionally planning to use automatic speaker ad-
aptation and to evaluate its benefits. Furthermore 
we are aware that, maybe with the exception of the 
classes related to fluency, rate and length, our fea-
ture set is as of yet quite rudimentary and will need 
significant expansion in order to obtain a broader 
coverage of communicative competence.  
In summary, future work will focus on im-
proving speech recognition, and on significantly 
extending the feature sets in different categories. 
The eventual goal is to have a well-balanced multi-
component scoring system which can both rate 
non-native speech as closely as possible according 
to communicative criteria, as well as provide use-
ful feedback for the language learner. 
 
References 
 
Bachman, L. F. (1990). Fundamental considerations in 
language testing. Oxford: Oxford University Press. 
Bernstein, J. (1999). PhonePass Testing: Structure and 
Construct. Menlo Park, CA: Ordinate Corporation. 
Bernstein, J., DeJong, J., Pisoni, D., & Townshend, B. 
(2000). Two experiments in automatic scoring of 
spoken language proficiency. Paper presented at the 
InSTIL2000, Dundee, Scotland. 
Breiman, L., Friedman, J., Olshen, R., & Stone, C. 
(1984). Classification and Regression Trees. Bel-
mont, California: Wadsworth Int. Group. 
Burger, S. (1995). Konventionslexikon zur Translitera-
tion von Spontansprache. Munich, Germany. 
Canale, M., & Swain, M. (1980). Theoretical bases of 
communicative approaches to second language 
teaching and testing. Applied Linguistics, 1(1), 1-47. 
Cucchiarini, C., Strik, H., & Boves, L. (1997a, Septem-
ber). Using speech recognition technology to assess 
foreign speakers' pronunciation of Dutch. Paper pre-
sented at the Third international symposium on the 
acquisition of second language speech: NEW 
SOUNDS 97, Klagenfurt, Austria. 
Cucchiarini, C., Strik, S., & Boves, L. (1997b). Auto-
matic evaluation of Dutch pronunciation by using 
speech recognition technology. Paper presented at 
the IEEE Automatic Speech Recognition and Under-
standing Workshop, Santa Barbara, CA. 
Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R., 
& Butzberger, J. (2000). The SRI EduSpeak system: 
Recognition and pronunciation scoring for language 
learning. Paper presented at the InSTiLL-2000 (In-
telligent Speech Technology in Language Learning), 
Dundee, Scotland. 
Hymes, D. H. (1972). On communicative competence. 
In J. B. Pride & J. Holmes (Eds.), Sociolinguistics: 
selected readings (pp. 269-293). Harmondsworth, 
Middlesex: Penguin. 
Meteer, M., & al., e. (1995). Dysfluency Annotation 
Stylebook for the Switchboard Corpus.Unpublished 
manuscript. 
North, B. (2000). The Development of a Common 
Framework Scale of Language Proficiency. New 
York, NY: Peter Lang. 
Rudner, L., & Gagne, P. (2001). An overview of three 
approaches to scoring written essays by computer. 
Practical Assessment, Research & Development, 
7(26). 
Shermis, M. D., & Burstein, J. (2003). Automated essay 
scoring: A cross-disciplinary perspective. Hillsdale, 
NJ: Lawrence Erlbaum Associates, Inc. 
Vapnik, V. N. (1995). The Nature of Statistical Learn-
ing Theory: Springer. 
Xi, X., Zechner, K., & Bejar, I. (2006, April). Extract-
ing meaningful speech features to support diagnostic 
feedback: an ECD approach to automated scoring. 
Paper presented at the NCME, San Francisco, CA. 
 
 8
223
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 442?449,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improved Pronunciation Features for Construct-driven Assessment of
Non-native Spontaneous Speech
Lei Chen, Klaus Zechner, Xiaoming Xi
Educational Testing Service
Princeton, NJ, USA
{LChen,KZechner,XXi}@ets.org
Abstract
This paper describes research on automatic as-
sessment of the pronunciation quality of spon-
taneous non-native adult speech. Since the
speaking content is not known prior to the
assessment, a two-stage method is developed
to first recognize the speaking content based
on non-native speech acoustic properties and
then forced-align the recognition results with
a reference acoustic model reflecting native
and near-native speech properties. Features
related to Hidden Markov Model likelihoods
and vowel durations are extracted. Words with
low recognition confidence can be excluded
in the extraction of likelihood-related fea-
tures to minimize erroneous alignments due
to speech recognition errors. Our experiments
on the TOEFL R?Practice Online test, an En-
glish language assessment, suggest that the
recognition/forced-alignment method can pro-
vide useful pronunciation features. Our new
pronunciation features are more meaningful
than an utterance-based normalized acoustic
model score used in previous research from a
construct point of view.
1 Introduction
Automated systems for evaluating highly pre-
dictable speech (e.g. read speech or speech that
is quite constrained in the use of vocabulary and
syntactic structures) have emerged in the past
decade (Bernstein, 1999; Witt, 1999; Franco et al,
2000; Hacker et al, 2005) due to the growing matu-
rity of speech recognition and processing technolo-
gies. However, endeavors into automated scoring
for spontaneous speech have been sparse given the
challenge of both recognizing and assessing spon-
taneous speech. This paper addresses the develop-
ment and evaluation of pronunciation features for an
automated system for scoring spontaneous speech.
This system was deployed for the TOEFL R?Practice
Online (TPO) assessment used by prospective test
takers to prepare for the official TOEFL R?test.
A construct is a set of knowledge, skills, and abil-
ities measured by a test. The construct of the speak-
ing test is embodied in the rubrics that human raters
use to score the test. It consists of three key cat-
egories: delivery, language use, and topic devel-
opment. Delivery refers to the pace and the clar-
ity of the speech, including performance on into-
nation, rhythm, rate of speech, and degree of hesi-
tancy. Language use refers to the range, complex-
ity, and precision of vocabulary and grammar use.
Topic development refers to the coherence and full-
ness of the response. Most of the research on spon-
taneous speech assessment focuses on the delivery
aspect given the low recognition accuracy on non-
native spontaneous speech.
The delivery aspect can be measured on four di-
mensions: fluency, intonation, rhythm, and pronun-
ciation. For the TPO assessment, we have defined
pronunciation as the quality of vowels, consonants
and word-level stress (segmentals). Intonation and
sentence-level stress patterns (supra-segmentals) are
not defined as part of pronunciation. Pronuncia-
tion is one of the key factors that impact the intelli-
gibility and perceived comprehensibility of speech.
Because pronunciation plays an important role in
speech perception, features measuring pronuncia-
442
tion using speech technologies have been explored
in many previous studies. However, the bulk of the
research on automatic pronunciation evaluation con-
cerns read speech or highly predictable speech (Witt,
1999; Franco et al, 2000; Hacker et al, 2005),
where there is a high possibility of success in speech
recognition. Automatic pronunciation evaluation is
challenging for spontaneous speech and has been
under-explored.
In this paper, we will describe a method for
extracting pronunciation features based on sponta-
neous speech that is well motivated by theories and
supported by empirical evaluations of feature per-
formance. In conceptualizing and computing these
features, we draw on the literature on automatic pro-
nunciation evaluation for constrained speech. As de-
scribed in the related work in Section 2, the widely
used features for measuring pronunciation are (1)
likelihood (posterior probability) of a phoneme be-
ing spoken given the observed audio sample that
is computed in a Viterbi decoding process, and (2)
phoneme length measurements that are compared to
standard references based on native speech.
However, we have also come up with unique solu-
tions to address the issue of relatively low accuracy
in recognizing spontaneous speech. Our methods of
feature extraction are designed with considerations
of how to best capture the quality of pronunciation
given technological constraints.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related research; Sec-
tion 3 describes our method to extract a set of fea-
tures for measuring pronunciation; Section 4 de-
scribes the design of the experiments, including the
questions investigated, the data, the speech process-
ing technologies, and the measurement metrics; Sec-
tion 5 reports on the experimental results; Section 6
discusses the experimental results; and Section 7
summaries the findings and future research planned.
2 Related work
There is previous research on utilizing speech recog-
nition technology to automatically assess non-native
speakers? communicative competence (e.g., fluency,
intonation, and pronunciation). Witt (Witt, 1999)
developed the Goodness of Pronunciation (GOP)
measurement for measuring pronunciation based on
Hidden Markov Model (HMM) log likelihood. Us-
ing a similar method, Neumeyer et al (Neumeyer et
al., 2000) designed a series of likelihood related pro-
nunciation features, e.g., the local average likelihood
and global average likelihood. Hacker et al (Hacker
et al, 2005) utilized a relatively large feature vector
for scoring pronunciation.
Pronunciation has been the focus of assessment in
several automatic speech scoring systems. Franco et
al. (Franco et al, 2000) presented a system for au-
tomatic evaluation of pronunciation quality on the
phoneme level and the sentence level of speech by
native and non-native speakers of English and other
languages (e.g., French). A forced alignment be-
tween the speech read by subjects and the ideal path
through the HMM was computed. Then, the log
posterior probabilities for a certain position in the
signal were computed to achieve a local pronunci-
ation score. Cucchiarini et al (Cucchiarini et al,
1997a; Cucchiarini et al, 1997b) designed a system
for scoring Dutch pronunciation along a similar line.
Their pronunciation feature set was more extensive,
including various log likelihood HMM scores and
phoneme duration scores. In these two systems, the
speaking skill scores computed on features by ma-
chine are found to have good agreement with scores
provided by humans.
A limited number of studies have been conducted
on assessing speaking proficiency based on sponta-
neous speech. Moustroufas and Digalakis (Mous-
troufas and Digalakis, 2007) designed a system to
automatically evaluate the pronunciation of foreign
speakers using unknown text. The difference in the
recognition results between a recognizer trained on
speakers? native languages (L1) and another recog-
nizer trained on their learned languages (L2) was
used for pronunciation scoring. Zechner and Be-
jar (Zechner and Bejar, 2006) presented a system
to score non-native spontaneous speech using fea-
tures derived from the recognition results. Follow-
ing their work, an operational assessment system,
SpeechRaterTM , was implemented with further im-
provements (Zechner et al, 2007).
There are some issues with the method to extract
pronunciation features in the previous research on
automated assessment of spontaneous speech (Zech-
ner and Bejar, 2006; Zechner et al, 2007). For ex-
443
ample, the acoustic model (AM) that was used to es-
timate a likelihood of a phoneme being spoken was
well-fitted to non-native speech acoustic properties.
Further, other important aspects of pronunciation,
e.g., vowel duration, have not been utilized as a fea-
ture in the current SpeechRaterTMsystem. Likeli-
hoods estimated on non-words (such as silences and
fillers) that were not central to the measurement of
pronunciation were used in the feature extraction. In
addition, mis-recognized words lead to wrong like-
lihood estimation. Our paper attempts to address all
of these limitations described above.
3 Extraction of Pronunciation Features
Figure 1 depicts our new method for extracting an
expanded set of pronunciation features in a more
meaning way.
Figure 1: Two-stage pronunciation feature extraction
We used two different AMs for pronunciation fea-
ture extraction. First, we used an AM optimized
for speech recognition (typically an AM adapted
on non-native speech to better fit non-native speak-
ers? acoustics patterns) to generate word hypotheses;
then we used the other AM optimized for pronun-
ciation scoring (typically trained on native or near-
native speech to be a good reference model reflect-
ing expected speech characteristics) to force align
the speech signals to the word hypotheses and to
compute the likelihoods of individual words being
spoken and durations of phonemes; finally new pro-
nunciation features were extracted based on these
measurements.
Some notations used for computing the pronunci-
ation features are listed in Table 1. Based on these
notations, the proposed new pronunciation features
are described in Table 2. To address the limita-
tions of previous research on automated assessment
of pronunciation, which was described in Section 2,
our proposed method has achieved improvements on
(1) using the two-stage method to compute HMM
likelihoods using a reference acoustic model trained
on native and near-native speech, (2) expanding the
coverage of pronunciation features by using vowel
duration shifts that are compared to standard norms
of native speech, (3) and using likelihoods on the
audio portions that are recognized as words and ap-
plying various normalizations.
Table 1: Notations used for pronunciation feature extrac-
tion
Variable Meaning
L(xi) the likelihood of word xi being spo-
ken given the observed audio signal
ti the duration of word i in a response
Ts the duration of the entire response
T
n?
i=1
ti, the summation of the duration
of all words, where T ? Ts
n the number of words in a response
m the number of letters in a response
R mTs , the frequency of letters (as the rateof speech)
vi vowel i
Nv the total number of vowels
Pvi the duration of vowel vi
P? the average vowel duration (across all
vowels in the response being scored)
Dvi the standard average duration of
vowel vi (estimated on a native
speech corpus)
D? the averaged vowel duration (on all
vowels in a native speech corpus)
Svi |Pvi ? Dvi |, duration shift of vowel
vi (measured as the absolute value of
the difference between the duration of
vowel vi and its standard value)
Snvi |
Pvi
P? ?
Dvi
D? |, normalized duration shiftof vowel vi (measured as the absolute
value of the normalized difference be-
tween the duration of vowel vi and its
standard value)
4 Experiment design
We first raise three questions that we try to answer
with our experiments. Then, we describe the data
sets and the speech recognizers, especially the two
444
Table 2: A list of proposed pronunciation features
Feature Formula Meaning
L1
n?
i=1
L(xi) summation of likeli-
hoods of all the indi-
vidual words
L2 L1/n average likelihood
across all words
L3 L1/m average likelihood
across all letters
L4 L1/T average likelihood
per second
L5
n?
i=1
L(xi)
ti
n average likelihooddensity across all
words
L6 L4/R L4 normalized by the
rate of speech
L7 L5/R L5 normalized by the
rate of speech
S?
Nv?
i=1
Svi
Nv average vowel dura-tion shifts
S?n
Nv?
i=1
Snvi
Nv average normalizedvowel duration shifts
different acoustic models fitted to non-native and ex-
pected speech respectively. Finally, we describe the
evaluation criterion used in the experiment.
4.1 Research questions
In order to justify that the two-stage method for ex-
tracting pronunciation features is a valid method that
provides useful features for assessing pronunciation,
the following questions need to be answered:
Q1: Can the words hypothesized be used to approx-
imate the human transcripts in the forced align-
ment step?
Q2: Are the new pronunciation features effective
for assessment?
Q3: Can the likelihood-related features be im-
proved when using only words correctly recog-
nized?
4.2 Data
Table 3 lists the data sets used in the experiment.
Non-native speech collected in the TPO was used in
training a non-native AM. For feature evaluations,
we selected 1, 257 responses from the TPO data col-
lected in 2006. Within this set, 645 responses were
transcribed. Holistic scores were assigned by human
raters based on a score scale of 1 (the lowest profi-
ciency) to 4 (the highest proficiency).
In the TOEFL R?Native Speaker Study, native
speakers of primarily North American English
(NaE) took the TOEFL R?test and their speech files
were collected. This TOEFL R?native speech data
and some high-scored TPO responses were used
in the adaptation of an AM representing expected
speech properties. In addition, 1, 602 responses of
native speech, which had the highest speech profi-
ciency scores in NaE, were used to estimate standard
average vowel durations.
Type Function Source Size
non-
native
speech
AM training TPO ? 30 hrs
feature evalua-
tion
TPO col-
lected in
2006
1, 257
responses
(645 with
tran-
scripts)
native
or
near-
native
speech
AM adaptation TPO and
TOEFL
Native
? 2, 000
responses
estimation of
standard vowel
durations
TOEFL
Native
1, 602 re-
sponses
Table 3: Data sets used in the experiment
4.3 Speech technologies
For speech recognition and forced alignment, we
used a gender-independent fully continuous HMM
speech recognizer. Two different AMs were used in
the recognition and forced alignment steps respec-
tively.
The AM used in the recognition was trained
on about 30 hours of non-native speech from the
TPO. For language model training, a large corpus
of non-native speech (about 100 hours) was used
445
and mixed with a large general-domain language
model (trained from the Broadcast News (BN) cor-
pus (Graff et al, 1997) of the Linguistic Data Con-
sortium (LDC)). In the pronunciation feature extrac-
tion process depicted in Figure 1, this AM was used
to recognize non-native speech to generate the word
hypotheses.
The AM used in the forced alignment was trained
on native speech and high-scored non-native speech.
It was trained as follows: starting from a generic
recognizer, which was trained on a large and var-
ied native speech corpus, we adapted the AM using
batch-mode MAP adaptation. The adaptation corpus
contained about 2, 000 responses with high scores in
previous TPO tests and the TOEFL R?Native Speaker
Study. In addition, this AM was used to estimate
standard norms of vowels as described in Table 1.
4.4 Measurement metric
To measure the quality of the developed features,
a widely used metric is the Pearson correlation (r)
computed between the features and human scores.
In previous studies, human holistic scores of per-
ceived proficiency have been widely used in esti-
mating the correlations. In our experiment, we will
use the absolute value of Pearson correlation with
human holistic scores (|r|) to evaluate the features.
Given the close relationship between pronunciation
quality and overall speech proficiency, |r| is ex-
pected to approximate the strength of its relationship
with the human pronunciation scores.
5 Experimental Results
5.1 Results for Q1
When assessing read speech, the transcription of
the spoken content is known prior to the assess-
ment and used to forced-align the speech for fea-
ture extraction. However, when assessing sponta-
neous speech, we do not know the spoken content
and cannot provide a correct word transcription for
the forced alignment with imperfect speech recogni-
tion. A practical solution is to use the recognition
hypothesis to approximate the human transcript in
the forced alignment. Since the recognition word ac-
curacy on non-native spontaneous speech is not very
high (for example, a word accuracy of about 50% on
the TPO data was reported in (Zechner et al, 2007)),
it is critical to verify that the approximation can pro-
vide good enough pronunciation features compared
to the ones computed in an ideal scenario (by using
the human transcript in the forced alignment step).
We ran forced alignment on 645 TPO responses
with human transcriptions, using both the manual
transcription and the word hypotheses from the rec-
ognizer described in Section 4.3. Then, based on
these two forced alignment outputs, we extracted the
pronunciation features as described in Section 3.
Table 4 reports the |r|s between the proposed
pronunciation features and human holistic scores
when using the forced alignment results from ei-
ther transcriptions or recognition hypotheses. The
relative |r| reduction (defined as (|r|transcriptions ?
|r|hypotheses)/|r|transcriptions ? 100) is reported to
measure the magnitude reduction.
Based on the results shown in Table 4, we find that
the pronunciation features computed based on the
forced alignment results using transcriptions have
higher |r|s with the human holistic scores than the
corresponding features computed based on the FA
results using the recognition hypotheses. This is not
surprising given that only 50% ? 60% word accu-
racy can be achieved when recognizing non-native
spontaneous speech. However, the pronunciation
features computed using the recognition hypothe-
ses that is feasible in practice show some promising
correlations to human holistic scores. For example,
L3, L6, and L7 have |r|s larger than 0.45 and S?n
has an |r| larger than 0.35. Compared to the cor-
responding features computed using the FA results
based on transcriptions, these promising pronuncia-
tion features that can be obtained practically, show
some reduction in quality (from 13.4% to 21.1%)
but are still usable. Therefore, our proposed two-
stage method for pronunciation feature extraction is
proven to be a practical way for the computation of
features that have acceptable performance.
5.2 Result for Q2
Although our proposed modifications described in
Section 3 have improved the meaningfulness of the
features, an empirical study is needed to examine the
actual utility of these features for the assessment of
pronunciation.
In the experiment described in Section 5.1, four
pronunciation features (including L3, L6, L7, and
446
Feature |r| using
transcrip-
tion
|r| using
recog-
nition
hypothesis
relative |r|
reduction
(%)
L1 0.216 0.107 50.5
L2 0.443 0.416 6.1
L3 0.506 0.473 6.5
L4 0.363 0.294 19
L5 0.333 0.287 13.8
L6 0.549 0.475 13.5
L7 0.546 0.473 13.4
S? 0.396 0.296 25.3
S?n 0.451 0.356 21.1
Table 4: |r| between the pronunciation features and hu-
man holistic scores under two forced alignment input
conditions (using transcriptions vs. using recognition hy-
potheses) and relative |r| reduction
S?n) show promising correlations to human holistic
scores. To check the quality of the newly developed
pronunciation features, we compared these four fea-
tures with the amscore feature used in (Zechner et
al., 2007) on the TPO data set collected in 2006
(with 1, 257 responses). We first ran speech recog-
nition using the recognizer designed for non-native
speech. The recognition results were used to com-
pute the amscore, which is calculated by dividing
the likelihood over an entire response by the number
of letters. Then, we used the recognition hypothe-
ses to do the forced alignment using the other AM
trained on the native and near-native speech to ex-
tract those four pronunciation features. Finally, we
calculated the correlation coefficients between fea-
tures and the human holistic scores. The results are
reported in Table 5.
feature |r| to human holistic scores
amscore 0.434
L3 0.369
L6 0.444
L7 0.443
S?n 0.363
Table 5: A comparison of new pronunciation features to
amscore, the one used in SpeechRaterTM
Compared to the feature amscore, L6 and L7
have slightly higher |r|s with the human holistic
scores. This suggests that our construct-driven ap-
proach yields pronunciation features that are empiri-
cally comparable or even better than the amscore. In
addition, S?n, a new feature representing the vowel
production aspect of pronunciation, shows a rela-
tively high correlation with human holistic scores.
This suggests that our new pronunciation feature set
has an expanded coverage of pronunciation.
It is interesting to note that L3 has a lower |r|with
human holistic scores than the amscore does. Al-
though the computation of L3 is quite similar to that
of amscore, the major difference is that likelihoods
of non-word portions (such as silences and fillers)
are used to compute amscore but not L3. This sug-
gests that likelihood-related pronunciation features
that involve information related to non-words may
perform better in predicting human holistic scores.
For example, for amscore, the likelihoods measured
on those non-word units were involved in the feature
calculation; for L6 and L7, the temporal information
of those non-word units (e.g., duration of units) was
involved in the feature calculation 1.
5.3 Result for Q3
In the feature extraction, we used the words hy-
pothesized by the speech recognizer as the input for
the forced alignment. Since a considerable num-
ber of words are recognized incorrectly (especially
for non-native spontaneous speech), a natural way
to further improve the likelihood related features is
to only consider words which are correctly recog-
nized. A useful metric associated with the recog-
nition performance is the confidence score (CS) out-
put by the recognizer, which reflects the recognizer?s
estimation about the probability that a hypothesized
word is correctly recognized. The recognized words
with high confidence scores tend to be correctly rec-
ognized. Therefore, focusing on words recognized
with high confidence scores may reduce the negative
impact caused by recognition errors on the quality of
the likelihood related features.
On the TPO data with human transcripts, we used
the NIST?s sclite scoring tool (Fiscus, 2009) to mea-
sure the percentage of correct words (correct%),
which is defined as the ratio of the number of words
1L6 and L7 use R, which is computed as mTs , where Ts con-tains durations of non-words.
447
correctly recognized given the number of words in
the reference transcript. On all words (correspond-
ing to confidence scores ranging from 0.0 to 1.0), the
correct% is 53.3%. Figure 2 depicts the correct%
corresponding to ten confidence score bins ranging
from 0.0 to 1.0. Clearly, with the increase of the con-
fidence score, more words tend to be accurately rec-
ognized. Therefore, it is reasonable to only use like-
lihoods estimated on the hypothesized words with
high confidence scores for extracting likelihood re-
lated features.
 
0
 
10
 
20
 
30
 
40
 
50
 
60  0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Correct% of words hypothesized
Confid
ence s
core (C
S) bin
Figure 2: Correct% of words recognized across 10 confi-
dence score bins
On the TPO data set collected in 2006, we com-
puted three likelihood related features (including L3,
L6, and L7) only on words whose SC is equal to
or higher than a threshold (i.e., 0.5, 0.6, 0.7, 0.8,
and 0.9) and measured the |r| of a feature with the
human holistic scores. Table 6 lists the confidence
score cutting thresholds, the percentage of words
whose confidence scores are not lower than the cut-
ting threshold selected, and |r| between each like-
lihood feature to human holistic scores. In the Ta-
ble 6, we observe that only using words recognized
with high confidence improves the correlations be-
tween the features and the human holistic scores.
One issue about only using words recognized with
high confidence scores is that the number of words
used in the feature extraction has been reduced and
may reduce the robustness of the feature calculation.
Tc percentage
of words
whose CS
? Tc (%)
L3
|r|
L6
|r|
L7
|r|
0.0 100 0.369 0.444 0.443
0.5 84.21 0.38 0.462 0.461
0.6 77.07 0.377 0.465 0.464
0.7 69.31 0.363 0.461 0.461
0.8 60.86 0.371 0.466 0.466
0.9 50.76 0.426 0.477 0.475
Table 6: |r| between L3, L6, and L7 and human holistic
scores using only words recognized whose CSs are not
lower than a threshold (Tc)
6 Discussion
To assess the pronunciation of spontaneous speech,
we proposed a method for extracting a set of pro-
nunciation features. The method consists of two
stages: (1) recognizing speech using an AM well fit-
ted to non-native speech properties and (2) forced-
aligning the hypothesized words using the other
AM, which was trained on native and near-native
speech, and extracting features related to spectral
properties (HMM likelihood) and vowel production.
This method of using one AM optimized for speech
recognition and another AM optimized for pronun-
ciation evaluation is well motivated theoretically.
The derived pronunciation features have also been
found to have reasonably high correlations with hu-
man holistic scores. The results support the link-
age of the features to the construct of pronunciation
and their utility of being used in a scoring model to
predict human holistic judgments. Several contribu-
tions of this paper are described as below.
First, the two-stage method allows us to utilize
an AM trained on native and near-native speech as
a reference model when computing pronunciation
features. The decision to include high-scored non-
native speech was driven by the scoring rubrics de-
rived from the construct, where the pronunciation
quality of the highest level performance does not
necessarily require native-like accent, but highly in-
telligible speech. The way the reference model was
trained is consistent with the scoring rubrics, and
makes it an appropriate standard based on which the
pronunciation quality of non-native speech can be
448
evaluated. By using the recognition hypotheses from
the recognition step as input in the forced alignment
step, our experiments show a relatively small reduc-
tion in correlations with human holistic scores in
comparison to the features based on the human tran-
scriptions. This suggests that our method has po-
tential to be implemented in a real-time operational
setting.
Second, a few decisions we have made in com-
puting the pronunciation features are driven by
considerations of how these features are meaning-
fully linked to the construct of pronunciation as-
sessment. For example, we have excluded the
HMM likelihoods on non-words (such as pauses
and fillers) in the computations of likelihood-related
features. In addition, only using words recognized
with high confidence scores yields more informative
likelihood-related features for assessing the quality
of speech. The inclusion of vowel duration measures
in the feature set expanded the coverage of the qual-
ity of pronunciation.
7 Summary and future work
This paper presents a method for computing features
for assessing the pronunciation quality of non-native
spontaneous speech, guided by construct considera-
tions. We were able to show that using a two-stage
method of first recognizing speech with a non-native
AM and then forced aligning of the hypothesis using
a native or near-native speech AM we can generate
pronunciation features with promising correlations
with holistic scores assigned by human raters.
We plan to continue our research in the follow-
ing directions: (1) we will improve the native speech
norms for vowel durations, such as using the distri-
bution of vowel durations rather than just the mean
of durations in our feature computations; (2) we
will investigate other aspects of pronunciation, e.g.,
consonant quality and word stress; (3) we will add
other standard varieties of English (such as British,
Canadian, Australian, etc) to the training corpus for
the reference pronunciation model as the current
model is trained on primarily North American En-
glish (NaE).
References
J. Bernstein. 1999. PhonePass testing: Structure and
construct. Technical report, Ordinate Corporation.
C. Cucchiarini, H. Strik, and L. Boves. 1997a. Au-
tomatic evaluation of Dutch Pronunciation by us-
ing Speech Recognition Technology. In IEEE Auto-
matic Speech Recognition and Understanding Work-
shop (ASRU), Santa Barbara, CA.
C. Cucchiarini, H. Strik, and L. Boves. 1997b. Us-
ing Speech Recognition Technology to Assess Foreign
Speakers? Pronunciation of Dutch. In 3rd interna-
tional symosium on the acquision of second language
speech, Klagenfurt, Austria.
J. Fiscus. 2009. Speech Recognition Scoring Toolkit
(SCTK) Version 2.3.10.
H. Franco, V. Abrash, K. Precoda, H. Bratt, R. Rao, and
J. Butzberger. 2000. The SRI EduSpeak system:
Recognition and pronunciation scoring for language
learning. In InSTiLL (Intelligent Speech Technology
in Language Learning), Dundee, Stotland.
D. Graff, J. Garofolo, J. Fiscus, W. Fisher, and D. Pallett.
1997. 1996 English Broadcast News Speech (HUB4).
C. Hacker, T. Cincarek, R. Grubn, S. Steidl, E. Noth, and
H. Niemann. 2005. Pronunciation Feature Extraction.
In Proceedings of DAGM 2005.
N. Moustroufas and V. Digalakis. 2007. Automatic
pronunciation evaluation of foreign speakers using
unknown text. Computer Speech and Language,
21(6):219?230.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 6.
S. M. Witt. 1999. Use of Speech Recognition in
Computer-assisted Language Learning. Ph.D. thesis,
University of Cambridge.
K. Zechner and I. Bejar. 2006. Towards Automatic Scor-
ing of Non-Native Spontaneous Speech. In NAACL-
HLT, NewYork NY.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
449
Efficient Optimization for Bilingual Sentence Alignment  
Based on Linear Regression 
 
Bing Zhao 
 
Language Technologies 
Institute 
Carnegie Mellon University 
bzhao@cs.cmu.edu 
Klaus Zechner 
 
Educational Testing Service 
Rosedale Road, Princeton, 
NJ 08541 
kzechner@ets.org 
Stephan Vogel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
vogel+@cs.cmu.edu 
Alex Waibel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
ahw@cs.cmu.edu 
 
Abstract 
This paper presents a study on optimizing sen-
tence pair alignment scores of a bilingual sen-
tence alignment module. Five candidate 
scores based on perplexity and sentence 
length are introduced and tested. Then a linear 
regression model based on those candidates is 
proposed and trained to predict sentence pairs? 
alignment quality scores solicited from human 
subjects. Experiments are carried out on data 
automatically collected from Internet. The 
correlation between the scores generated by 
the linear regression model and the scores 
from human subjects is in the range of the in-
ter-subject agreement score correlations. Pear-
son's correlation ranges from 0.53 up to 0.72 
in our experiments.  
1 Introduction 
In many instances, multilingual natural language 
systems like machine translation systems are developed 
and trained on parallel corpora.  When faced with a dif-
ferent, unseen text genre, however, translation perform-
ance usually drops noticeably.  One way to remedy this 
situation is to adapt and retrain the system parameters 
based on bilingual data from the same source or at least 
a closely related source.  A bilingual sentence alignment 
program (Gale and Church, 1991, and Brown et al, 
1991) is the crucial part in this adaptation procedure, in 
that it collects bilingual document pairs from the Inter-
net, and identifies sentence pairs, which should have a 
high likelihood of being correct translations of each 
other.  The set of identified bilingual parallel sentence 
pairs is then added to the training set for parameter re-
estimation. 
As is well known, text mined from the Internet is 
very noisy.  Even after careful html parsing and filtering 
for text size and language, the text from comparable 
html-page pairs still contains mismatches of content or 
non-parallel junk text, and the sentence order can be too 
different to be aligned.  Together with a large mismatch 
of vocabulary, the aligned sentence pairs, which are 
extracted from these collected comparable html-page 
pairs, contain a number of low translation quality 
alignments.  These need to be removed before the re-
training of the MT system. 
In this paper, we present an approach to automati-
cally optimizing the alignment scores of such a bilingual 
sentence alignment program.  The alignment score is a 
combination (by linear regression) of two word transla-
tion lexicon scores and three sentence length scores and 
predicts the translation quality scores from a set of hu-
man annotators.  We also present experiments analyzing 
how many different human scorers are needed for good 
prediction and also how many sentence pairs should be 
scored per human annotator. 
The paper is structured as follows: in section 2, the 
text mining system is briefly described.  In section 3, 
five sentence alignment models based on lexical infor-
mation and sentence length are explained. In section 4, a 
regression model is proposed to combine the five mod-
els to get further improvement in predicting alignment 
quality.  We describe alignment experiments in section 
5, focusing on the correlation between the alignment 
scores predicted by the sentence alignment models and 
by humans.  Conclusions are given in section 6. 
2 System of Mining Parallel Text 
One crucial component of statistical machine trans-
lation (SMT) system is the parallel text mining from 
Internet. Several processing modules are applied to col-
lect, extract, convert, and clean the text from Internet.  
The components in our system include: 
? A web crawler, which collects potential parallel 
html documents based on link information follow-
ing (Philip Resnik 1999); 
? A bilingual html parser (based on flex for effi-
ciency), which is designed for both Chinese and 
English html documents.  The paragraphs? bounda-
ries within the html structure are kept.  
? A character encoding detector, which judges if the 
Chinese html document is GB2312 encoding or 
BIG5 encoding.  
? An encoding converter, which converts the BIG5 
documents to GB2312 encoding.  
? A language identifier to ensure that source and tar-
get documents are both of the proper language. 
(Noord?s Implementation).  
? A Chinese word segmenter, which parses the Chi-
nese strings into Chinese words.   
? A document alignment program, which judges if 
the document pair is close translation candidates, 
and filters out those non-translation pairs. 
? A sentence boundary detector, which is based on 
punctuation and capitalized characters; 
? And the key component, a sentence alignment pro-
gram, which aligns and extracts potential parallel 
sentence pairs from the candidate document pairs. 
   
After sentence alignment, each candidate of a par-
allel sentence pair is then re-scored by the regression 
models (to be described in section 5). These scores are 
used to judge the quality of the aligned sentences.  Thus 
one can select the aligned sentence pairs, which have 
high alignment quality scores, to re-estimate the sys-
tem?s parameters.  
2.1 Sentence Alignment 
Our sentence alignment program uses IBM Model-1 
based perplexity (section 2.2) to calculate the similarity 
of each sentence pair. Dynamic programming is applied 
to find Viterbi path for sentence alignments of the bilin-
gual comparable document pair. In our dynamic pro-
gramming implementation, we allow for seven 
alignment types between English and Chinese sentences: 
 
? 1:1 ? exact match, where one sentence is the trans-
lation of the other one; 
? 2:2 ? the break point between two sentences in the 
source document is different from the segmentation 
in the target document.  E.g. part of sentence one in 
the source might be translated as part of the second 
sentence in the target; 
? 2:1, 1:2, and 3:1 ? these cases are similar to the 
case before: they handle differences in how a text is 
split into sentences. The case 1:3 has not been used 
in the final configuration of the system, as this type 
did not occur in any significant number; 
? 1:0 (deletion) and (0:1) insertion ? a sentence in the 
source document is missing in the translation or 
vice versa. 
 
The deletion and insertion types are discarded, and 
the remaining types are extracted to be used as potential 
parallel data. In general, one Chinese sentence corre-
sponds to several English sentences. In (Bing and 
Stephan, 2002), experiments on a 10-year XinHua news 
story collection from the Linguistic Data Consortium 
(LDC) show that alignment types like (2:1) and (3:1) 
are common, and this 7-type alignment is shown to be 
reliable for English-Chinese sentence alignment.  How-
ever, only a small part of the whole 10-year collection 
was pre-aligned (Xiaoyi, 1999) and extracted for sen-
tence alignment.   
The picture can be very different when directly min-
ing the data from Internet. Due to the mismatch between 
the training data and the data collected from Internet, 
the vocabulary coverage can be very low; the data is 
very noisy; and the data aligned is not strictly parallel. 
The percentage of alignment types of insertion (0:1) and 
deletion (1:0) become very high as shown in section 5. 
The aligned sentence pairs are subject to many align-
ment errors. The alignment errors are not desired in the 
re-training of the system, and need to be removed.  
Though the sentence alignment outputs a score from 
Viterbi path for each of the aligned sentence pairs, this 
score is only a rough estimation of the alignment quality. 
A more reliable re-scoring of the data is desirable to 
estimate the alignment quality as a post processing step 
to filter out the errors and noise from the aligned data.  
2.2 Statistical Translation Lexicon 
We use a statistical translation lexicon known as IBM 
Model-1 in (Brown et al, 1993) for both efficiency and 
simplicity.  
In our approach, Model-1 is the conditional probabil-
ity that a word f in the source language is translated 
given word e in the target language, t(f|e). This prob-
ability can be reliably estimated using the expectation-
maximization (EM) algorithm (Cavnar, W. B. and J. M. 
Trenkle, 1994). 
Given training data consisting of parallel sen-
tences: }..1),,{( )()( Sief ii = , our Model-1 training for 
t(f|e) is as follows: 
?
=
?
=
S
s
ss
e efefceft
1
)()(1 ),;|()|( ?  
Where 1?
e? is a normalization factor such that 
0.1)|( =
?
j
j eft  
),;|( )()( ss efefc denotes the expected number of times 
that word e connects to word f.  
??
?
==
=
=
l
i
i
m
j
jl
k
k
ss eeff
eft
eft
efefc
11
1
)()( ),(),(
)|(
)|(),;|( ??  
With the conditional probability t(f|e), the probability 
for an alignment of foreign string F given English string 
E is in (1): 
??
= =
+
=
m
j
n
i
ijm eftlEFP 1 0
)|()1(
1)|(  (1) 
The probability of alignment F given E: )|( EFP is 
shown to achieve the global maximum under this EM 
framework as stated in (Brown et al,1993).  
In our approach, equation (1) is further normalized 
so that the probability for different lengths of F is com-
parable at the word level: 
m
m
j
n
i
ijm eftlEFP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (2) 
 
The alignment models described in (Brown et al, 
1993) are all based on the notion that an alignment 
aligns each source word to exactly one target word.  
This makes this type of alignment models asymmetric.  
Thus by using the conditional probability t(e|f) trans-
lation lexicon trained from English (source) to Chinese 
(target), different aspects of the bilingual lexical 
information can be captured. A similar probability to (2) 
can be defined based on this reverse translation lexicon: 
n
m
i
n
j
jim fetlFEP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (3) 
 
Starting from the Hong Kong news corpora provided 
by LDC, we trained the translation lexicons to be used 
in the parallel sentence alignment.  Each sentence pair 
has a perplexity, which is calculated using the minus log 
of the probability eg. equation (2).  
3 Alignment Models 
The alignment model is aimed at automatically pre-
dicting the alignment scores of a bilingual sentence 
alignment program. By scoring the alignment quality of 
the sentence pairs, we can filter out those mis-aligned 
sentence pairs, and save our SMT system from being 
corrupted by mis-aligned data. 
3.1 Lexicon Based Models 
It is necessary to include lexical features in the 
aligned quality evaluation. One way is to use the trans-
lation lexicon based perplexity as in our sentence 
alignment program.  
For each of the aligned sentence pairs, the sentence 
alignment generated a score, which is solely based on 
equation (2). Using this score only, we can do a simple 
filtering by setting a threshold of perplexity. The sen-
tence pairs which have a higher perplexity than the 
threshold will be removed. However the perplexity 
based on (2) is definitely not discriminative enough to 
evaluate the quality of aligned sentence pairs.  
In our experiment, it showed that perplexity (3) has 
more discriminative power in judging the quality of the 
aligned sentence pairs for Chinese-English sentence 
alignment. It is also possible that equation (2) is more 
suitable for other language pairs.  Both (2) and (3) are 
applied in our sentence alignment quality judgment, 
which is to be explained in section 4.  
3.2 Sentence Length Models 
As was shown in the sentence alignment literature 
(Church, K.W. 1993), the sentence length ratio is also a 
very good indication of the alignment of a sentence pair 
for languages from a similar family such as French and 
English.  For language pairs from very different families 
such as Chinese and English, the sentence length ratio is 
also a good indication of alignment quality as shown in 
our experiments.  
For the language pair of Chinese and English, the 
sentence length can be defined in several different ways.  
3.2.1 Sentence Length 
In general, a Chinese sentence does not have word 
boundary information; so one way to define Chinese 
sentence length is to count the number of bytes of the 
sentence. Another way is to first segment the Chinese 
sentence into words (section 3.2.2) and count how many 
words are in the sentence. For English sentences, we 
can similarly define the length in bytes and in words.  
The length ratio is assumed to be a Gaussian distri-
bution. The mean and variance are calculated from the 
parallel training corpus, which, in our case, is the Hong 
Kong parallel corpus with 290K parallel sentence pairs.  
3.2.2 A Chinese Word Segmenter 
The word segmenter for Chinese is to parse the Chi-
nese string into words. Different word segmenters can 
generate different numbers of words for the same Chi-
nese sentence.  
There are many word segmenters publicly available. 
In our experiments, we applied a two-pass strategy to 
segment the word according to the dictionary of the 
LDC bilingual dictionary of Chinese-English. The two-
pass started first from left to right, and then from right 
back to left, to calculate the maximum word frequency 
and select one best path to segment the words.  
In general, the sentence length is not sensitive to the 
segmenters used. But for reliability, we want each seg-
mented word can have an English translation, thus we 
used the LDC bilingual dictionary as a reference word 
list for segmentation.  
3.2.3 Sentence Length Model 
Assume the alignment probability of ),|( tsAP  is 
only related to the length of source sentence s and target 
sentence t: 
|))||,(|(~
|)||(|~
||)||,|||||(|~
),||||(|),|(
tsP
tsP
tstsP
tstsPtsAP
?=
?=
?=
?=
 
where || s and || t are the sentence lengths of s and t.  
The difference of the length |)||,(| ts? is assumed 
to be a Gaussian distribution (Church, K.W. 1993) and 
can be normalized as follows: 
)1,0(~
)1|(|
||||
2
N
s
cst
?
?
+
?
=  (4) 
where c is a constant indicating the mean length ratios 
between source and target sentences and 2? is the vari-
ance of the length ratios.  
In our case, we applied three length models de-
scribed in the following Table 1: 
 
Table 1. Three Length Models description 
L-1 Both English and Chinese sentence are meas-
ured in bytes 
L-2 Both English and Chinese sentence are meas-
ured in words 
L-3 English sentence is measured in words and 
Chinese sentence is measured in bytes 
 
The means and 2? of the length ratios for each of the 
length models are calculated from Hong Kong news 
parallel corpus. The statistics of the three sentence 
length models are shown in Table 2. 
 
Table 2. Sentence length ratio statistics 
  L-1  L-2 L-3: 
Mean 1.59 1.01 0.33 
Var 3.82 0.79 0.71 
 
In general, the smaller the variance, the better the 
sentence length model can be. From Table 2 we observe 
that the bytes based length ratio model has significantly 
larger variance (3.82) than the other two models (L-2: 
0.79, L-3: 0.71).  This means L1 is not as reliable as L2 
and L3. Both L2 and L3 have similar variance, which 
indicates measuring English sentences in words will 
entail smaller variance in length model; measuring Chi-
nese sentences in bytes or words entails only a slight 
difference in variance. This also indicates that the length 
model is not so sensitive to the Chinese word segmenter 
applied. L-1, L-2 and L-3 capture the length relationship 
of parallel sentence in different views. Their modeling 
power has overlap, but they also compensate each other 
in capturing the parallel characteristics of good transla-
tion quality. A combination of these models can poten-
tially bring further improvement, which is shown in our 
experiment in section 6.  
4 Regression Model 
Rather than doing a binary decision (classification) that 
the aligned sentence pair is either good or not, the re-
gression can give a confidence score indicating how 
good the alignment can be, thus offering more flexibil-
ity in decisions.   Predicting the alignment quality using 
the candidate models is considered as a regression prob-
lem in that different scores are combined together.   
There are many ways such as genetic programming, 
to combine the candidate models, and regression is one 
of the straight forward and efficient ones.  So in this 
work, we explored linear regression. 
4.1 Candidate Models 
We have five candidate models described in section 
3. They are: PP1, the perplexity based on the word pair 
conditional probability p(f|e) in equation (2); PP2, the 
perplexity based on the reverse word pair conditional 
probability p(e|f) in equation (3); L-1, Length ratio 
model measured in bytes (mean=1.59, var=3.82); L-2, 
length ratio model measured in words (mean=1.01, 
var=0.79); L-3, length ratio model, where the English 
sentence is measured in words and the Chinese sentence 
is measured in bytes (mean=0.33, var=0.71).  These five 
models capture different aspects of the aligned quality 
of the sentence pair. The idea is to combine these five 
models together to get better prediction of the aligned 
quality. 
Linear regression is applied to combine these five 
models. It is trained from the observation of the five 
models together with the label of human judgment on a 
training set. 
4.2 Regression Model Training 
The linear regression model tries to discover the 
equation for a line that most nearly fits the given data 
(Trevor Hastie et al 2001). That linear equation is then 
used to predict values for the data.  
Now given human subject judgment of the aligned 
translation quality of sentence pairs, we can train a re-
gression model based on the five models we described 
in section 4.1 under the objective of least square errors.  
The human evaluation is measures translation qual-
ity of aligned pairs on a discrete 6-point scale between 1 
(very bad) and 5 (perfect translation). The score 0 was 
used for alignments that were not genuine translation 
e.g., both sentences were from the same language. We 
will use n for the number of total sentence pairs labeled 
by humans and used in training.  
Let A= [PP1, PP2, L-1, L-2, L-3] be the machine-
generated scores for each of the sentence pairs. In our 
case, A is a 5?n  matrix.  
Let H= [Human-Judgment-Score] be the human 
evaluation of the sentence pairs on a 6-point scale. In 
our case, H is a 1?n  matrix. 
In linear regression modeling, a linear transforma-
tion matrix W should satisfy the least square error crite-
rion: 
||}{||min* HAWW
w
?=  (5) 
where W is in fact a 5x1 weight matrix. The equation 
can be solved as:  
HAAAW TT 1* )( ?=  (6) 
The inverse of matrix AAT  is usually calculated using 
singular vector decomposition (SVD). After W is calcu-
lated, the predicted score from the regression model is: 
*
' AWH =  (7) 
where 'H  is the final predicted alignment quality score 
of the regression model. We can also view 'H  as a 
weighted sum of the five models shown in section 4.1. 
The calculation of 'H  reduces to a linear weighted 
summation, which is very efficient to compute.  
5 Experiments 
1500 pairs of comparable html document pairs were 
obtained from bilingual web pages crawled from Inter-
net. After preprocessing, filtering, and sentence align-
ment, the alignment types were distributed as shown in 
Table 3. Ignoring the alignment type of insertion (0:1) 
and deletion (1:0), we extracted around 5941 parallel 
sentences.  
 
Table 3. Alignment types? distribution of mined 
data from noisy web data crawled 
 1:0 0:1 1:1 2:1 1:2 2:2 3:1 
% 23.7 41.9 29.4 1.99 0.01 0.02 2.79 
 
From Table 3, we see the data is very noisy, con-
taining a large portion of insertions (23.7%) and dele-
tions (41.9%).  This is very different from the LDC 
XinHua pre-aligned collection provided by LDC, which 
is relatively clean.  
For this set of English-Chinese bilingual sentences, 
we randomly selected 200 sentence pairs, focusing on 
Viterbi alignment scores below 12.0 from sentence 
alignment, which was an empirically determined 
threshold (The alignment scores here were purely re-
flecting the Model-1 parameters using equation (2)).  
Three human subjects then had to score the 'translation 
quality' of every sentence pair, using a 6 point scale 
described in section 4.2. We further excluded very short 
sentences from consideration and evaluated 168 remain-
ing sentences. 
Pearson R correlation is applied to calculate the mag-
nitude of the association between two variables (human-
human or human-machine in our case) that are on an 
interval or ratio scale. The correlation coefficients 
(Pearson R) between human subjects were in Table 4 
(all are statistically significant): 
 
Table 4. Correlation between Human Subjects 
 H2 H3 
H1 0.786 0.615 
H2 ---- 0.568 
 
Overall, more than 2/3 of the human scores are identical 
or differ by only 1 (between subjects). 
For the automatic score prediction, the five compo-
nent scores described in section 4.1 are used, which are 
then combined using a standard Linear Regression as 
described in section 4.2. Table 5 shows the correlation 
between alignment scores based on Model X and human 
subjects' predicted quality scores: 
 
Table 5. Correlation between optimization models 
and human subjects 
Model human-1 human -2 human -3 
PP-1 .57 .53 .32 
PP-2 .60 .58 .46 
L-1 .42 .41 .30 
L-2 .46 .41 .40 
L-3 .40 .38 .29 
Na?ve .58 .56 .38 
Regression  .72 .68 .53 
 
The data we used in our training of the lexicon is Hong 
Kong news parallel data from LDC. There are 290K 
parallel sentence pairs, with 7 million words of English 
and 7.3 million Chinese words after segmentation. The 
IBM Model-1 for PP-1 and PP-2 are both trained using 
5 EM iterations. The other three length models are also 
calculated from the same 290K sentence pairs. Punctua-
tion is removed before the calculation of all automatic 
score prediction models. 
The regression model here is the standard linear re-
gression using the observations from three human sub-
jects as described in section 4.1. The average 
performance of the regression model is shown in the 
bottom line of the above Table 5. The average correla-
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Average of 3-human judgement 5-scale score (5-best, 0-non)
Pearson r Correlation
tion varies from 0.53 upto 0.72, which shows that the 
regression model has a very strong positive correlation 
with the human judgment.  
Also from Table 5, we see both lexicon based mod-
els: PP-1 and PP-2 are better than the length models in 
term of correlation with human scorer. Model PP-2 has 
the largest correlation, and is slightly better than PP-1. 
PP-2 is based on the conditional probability of p(e|f), 
which models the generation of an English word from a 
Chinese word. The vocabulary size of Chinese is usu-
ally smaller than English vocabulary size, so this model 
can be more reliably estimated than the reverse direction 
of p(f|e). This explains why PP-2 is slightly better than 
PP-1.  
For sentence length models, we see L-2, for which 
the lengths of both the English sentence and the Chinese 
sentence are measured in words, has the best perform-
ance among the three settings of a sentence length 
model. This indicates that the length model measured in 
words is more reliable.  
Also shown in Table 5, the na?ve interpolation of 
these different models, i.e. just using each model with 
equal weight, resulted in lower correlation than the best 
single alignment model. 
We also performed correlation experiments with 
varied numbers of training sentences from either Hu-
man-1/Human-2/Human-3 or from all of the three hu-
man subjects.  We picked the first 30/60/90/120 labeled 
sentence pairs for training and saved the last 48 sen-
tence pairs for testing.  The average performance of the 
regression model is as follows: 
 
Table 6. Correlation between different training set 
sizes and human scorers. 
Training  
set size 
Human-1 Human -2 Human ?3 
30 .686 .639 .447 
60 .750 .707 .452 
90 .765 .721 .456 
120 .760 .721 .464 
 
The average correlation of the regression models 
showed here increased noticeably when the training set 
was increased from 30 sentence pairs to 90 sentence 
pairs. More sentence pairs caused no or only marginal 
improvements (esp. for the third human subject).  
Figure 1 shows a scatter plot, which illustrates a 
good correlation (here: Pearson R=0.74) between our 
regression model predictors and the human scorers. 
6 Conclusion 
In this paper, we have demonstrated ways to effi-
ciently optimize a sentence alignment module, such that 
it is able to select aligned sentence pairs of high transla-
tion quality automatically. This procedure of alignment 
score optimization requires (a) a small number of hu-
man subjects who annotate a set of about 100 sentence 
pairs each for translation quality; and (b) a set of align-
ment scores, based on perplexity and sentence length 
ratio, to be able to learn to predict the human scores. 
Based on the learned predictions, by means of linear 
regression, the alignment program can choose the best  
sentence pair candidates to be included in the training 
data for the SMT system re-estimation. 
Our experiments showed that, for Chinese-English 
language pair, perplexity based on the reverse word pair 
conditional probability p(e|f) (PP-2) gives the most reli-
able prediction among the five models proposed in this 
paper; the regression model, which combines those five 
models, give the best correlation between human score 
and automatic predictions. Our approach needs only a 
fairly limited number of human labeled sentences pairs, 
and is an efficient optimization of the sentence 
alignment system. 
 
Figure 1. Correlation between regression model and 
human scorers, Pearson R=0.74. 
 
References 
Bing Zhao, Stephan Vogel. 2002. Adaptive Parallel 
Sentences Mining from Web Bilingual News Collec-
tion. IEEE International Conference on Data Mining 
(ICDM 02) , pp. 745-748. Japan. 
Brown, P., Lai, J. C., and Mercer, R. 1991. Aligning 
Sentences in Parallel Corpora. In Proceedings of 
ACL-91, Berkeley CA. 1991 
Cavnar, W. B. and J. M. Trenkle. 1994.  N-Gram-Based 
Text Categorization.  Proceedings of Third Annual 
Symposium on Document Analysis and Information 
Retrieval, Las Vegas, NV, UNLV Publica-
tions/Reprographics, pp. 161-175, 11-13. 
Stanley Chen. 1993. Aligning sentences in Bilingual 
corpora using lexical information. In proceedings of 
the 31st Annual Conference of the Association for 
computational linguistics, pages 9-16, Columbus, 
Ohio, June 1993 
Church, K. W. 1993. Char_align: A Program for Align-
ing Parallel Texts at the Character Level. Proceed-
ings of ACL-93, Columbus OH. 
Gale, W. A. and Church, K. W.  1991. A Program for 
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-91, Berkeley CA. 1991. 
Melamed, I.D. 1996. A Geometric Approach to Map-
ping Bitext Correspondence. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA. 1996 
Noord?s Implementation of Textcat: http://odur.let. 
rug.nl/~vannoord/TextCat/index.html 
Peter F. Brown, Stephan A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer.  1993. The 
Mathmatics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics, vol 
19, no.2 , pp.263-311. 
Philip Resnik. 1999. Mining the Web for Bilingual Text. 
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL'99), University of Maryland, 
College Park, Maryland. 
Trevor Hastie, Robert Tibshirani, Jerome Friedman. 
2001. The Elements of Statistical Learning: Data 
Mining, Inference and Prediction. Springer Publisher. 
Xiaoyi Ma, Mark Y. Liberman, ?BITS: A Method for 
Bilingual Text Search over the Web?. Machine 
Translation Summit VII, 1999 
 
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 98?106,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
         Towards Automatic Scoring of a Test of Spoken Language with 
Heterogeneous Task Types 
 
 
 
Klaus Zechner     and     Xiaoming Xi 
Educational Testing Service 
Rosedale Road, Princeton, NJ 08541, USA 
{kzechner,xxi}@ets.org 
Abstract 
This paper describes a system aimed at auto-
matically scoring two task types of high and 
medium-high linguistic entropy from a spoken 
English test with a total of six widely differing 
task types. 
We describe the speech recognizer used for 
this system and its acoustic model and lan-
guage model adaptation; the speech features 
computed based on the recognition output; 
and finally the scoring models based on mul-
tiple regression and classification trees. 
For both tasks, agreement measures between 
machine and human scores (correlation, 
kappa) are close to or reach inter-human 
agreements. 
1 Introduction 
As demand for spoken language testing and cost of 
human scoring have increased in recent years, 
there is a growing interest in building both research 
and industrial systems for automatically scoring 
non-native speech (Bernstein, 1999, Zechner and 
Bejar, 2006, Zechner et al 2007).  
However, past approaches have focused typi-
cally only on one type of spoken language, or on a 
range of types similar in linguistic entropy. En-
tropy in this context can be seen as a measure for 
how predictable the language in the expected spo-
ken response is: Some tests, such as SET-10 (Bern-
stein 1999), are focused mostly on the lower 
entropy aspects of language, using tasks such as 
?reading? or ?repetition?, where the expected se-
quence of words is highly predictable. Other as-
sessments, such as the TOEFL? Practice Online 
Speaking test, on the other hand, focus on more 
spontaneous, high-entropy responses (Zechner et 
al., 2007). 
In this paper, we describe a spoken language test 
with heterogeneous task types, ranging from read 
speech to tasks that require candidates to give their 
opinions on an issue, whose goal is to assess com-
municative competence (Bachman, 1990; Bach-
man & Palmer, 1996); we call this test THT (Test 
with Heterogeneous Tasks). Communicative com-
petence, in this context, refers to a speaker's ability 
to use the language for communicative purposes.  
The effectiveness of the communication typically 
consists of a few aspects including comprehensibil-
ity, accuracy, clarity, coherence and appropriate-
ness, and is evident in a speaker's pronunciation, 
fluency, use of grammar and vocabulary, develop-
ment  of ideas, and sensitivity to the context of the 
communication.  
This test has the advantage of being able to as-
sess a wide range of non-native speakers? profi-
ciencies by using tasks of varying difficulty levels 
to allow even low proficiency speakers some de-
gree of success on easier task types. 
We select two tasks from this test, one of higher 
and one of medium to high entropy, and first adapt 
a non-native English speech recognizer (trained on 
TOEFL? Practice Online data) to transcribed THT 
task responses, then compute a set of relevant 
speech features based on the recognition output, 
and finally build a scoring model using a subset of 
these features to predict trained human rater scores. 
In this paper, we will demonstrate that the ma-
chine-human score agreements on these two task 
types come close to or even exceed the level of 
inter-human agreement. 
This paper is organized as follows: Section 2 
discusses related work, Section 3 describes the test 
and the challenges for automatic scoring involved, 
Section 4 discusses the speech recognizer and the 
acoustic and language model adaptations per-
98
formed, and Section 5 describes the speech fea-
tures selected for use in the scoring model. In Sec-
tion 6, we report the construction of the scoring 
model and its results, Section 7 contains a general 
discussion and Section 8 concludes the paper with 
a brief discussion of future research. 
2 Related work  
There has been previous work to automatically 
characterize aspects of communicative competence 
such as fluency, pronunciation, and prosody. 
Franco et al (2000) present a system for automatic 
evaluation of the pronunciation quality of both na-
tive and non-native speakers of English on a phone 
level and a sentence level (EduSpeak). Candidates 
read English texts and a forced alignment between 
the speech signal and the ideal path through the 
Hidden Markov Model (HMM) is computed. Next, 
the log posterior probabilities for pronouncing a 
certain phone at a certain position in the signal are 
computed to achieve a local pronunciation score. 
These scores are then combined with other auto-
matically derived measures such as the rate of 
speech (number of words per second) or the dura-
tion of phonemes to yield global pronunciation 
scores. 
Cucchiarini et al (1997a, 1997b) describe a sys-
tem for Dutch pronunciation scoring along similar 
lines. Their feature set, however, is more extensive 
and contains, in addition to log likelihood Hidden 
Markov Model scores, various duration scores, and 
information on pauses, word stress, syllable struc-
ture, and intonation. In an evaluation, correlations 
between four human scores and five machine 
scores range from 0.67 to 0.92. 
Bernstein (1999) presents a test for spoken Eng-
lish (SET-10) that uses the following types of task-
s: reading, sentence repetition, sentence building, 
opposites, short questions, and open-ended ques-
tions. All types except for the last are scored auto-
matically and a score is reported that can be 
interpreted as an indicator of how native-like a 
speaker?s speech is. In Bernstein et al (2000), an 
experiment is performed to investigate the per-
formance of the SET-10 test in predicting speak-
ers? oral proficiency.  It is shown that the SET-10 
test scores can predict different levels on the Oral 
Interaction Scale of the Council of Europe?s 
Framework (North, 2000) for describing oral pro-
ficiency of second/foreign language speakers with 
reasonable accuracy. This paper further reports on 
studies done to correlate the SET-10 automated 
scores with the human scores from two other tests 
of oral English communication skills. Correlations 
are found to be between 0.73 and 0.88.  
Zechner and Bejar (2006) investigate the auto-
mated scoring of unrestricted, spontaneous speech 
of non-native speakers. They focus on exploring a 
number of different fluency features for the auto-
mated scoring of short (one minute) responses to 
test questions in a TOEFL-related program. They 
explore scoring models based on classification and 
regression trees (CART) as well as support vector 
machines (SVM). Their findings are that the SVM 
models are more useful for a quantitative analysis, 
whereas the CART models allow for a more trans-
parent summary of the patterns underlying the 
data.  
In this paper, we use CART to build the scoring 
model for one task type. We also adopt multiple 
regression for another task type which has the ad-
vantage of being more easily interpreted than, for 
example, SVMs. Another major difference be-
tween previous work and the work reported in this 
paper is that we use feature normalization and 
transformation to obtain statistically more mean-
ingful input variables for the scoring model. In ad-
dition, we do not use the whole set of features in an 
exploratory fashion. Instead, we have carefully 
selected a subset of features that are both good pre-
dictors of human scores and maximize the repre-
sentation of the concept of communicative 
competence. 
3 The THT test 
3.1 Task types and scoring rubrics of the THT 
Speaking test 
There are six task types in the THT Speaking test, 
ranging from reading-aloud tasks to tasks that re-
quire short answers and tasks that require extended 
responses of one minute. The rubrics differ in both 
the dimensions of speaking skills measured and the 
possible score points. (Rubrics are characteriza-
tions of candidates? competence at given score lev-
els and are used by human raters to determine the 
appropriate score for a response.) Below is a brief 
description of the task types and the rubrics.  
 
99
Task type 1: Reading-aloud (Planning time: 45 
seconds; Response time: 45 seconds; zero/very-
low entropy) 
There are two read-aloud tasks. Each task requires 
the test-taker to read a short paragraph of 40-60 
words aloud. The reading materials include an-
nouncements, advertisements, introductions, etc. 
These two tasks are rated analytically on pronun-
ciation and intonation and stress on a 3-point scale. 
That is to say, two separate scores are given on 
each task ? one for pronunciation and one for into-
nation and stress.  
 
Task type 2: Picture description (Planning time: 
30 seconds; Response time: 45 seconds; me-
dium-high entropy) 
This task requires the test-taker to describe a pic-
ture in as much detail as possible.  
This task is rated holistically on the combined 
impact of delivery (fluency, pronunciation etc.), 
use of structures, vocabulary, content relevance 
and fullness on a 3-point scale.  
 
Task type 3: Open-ended short-answer ques-
tions (Planning time: none; Response time: 15-
30 seconds; low/low-medium entropy) 
The test-taker responds, without preparation, to 
three questions about familiar and accessible topics 
that draw on immediate personal experience. The 
first two questions each elicit a 15-second response 
that covers one or two pieces of information re-
lated to the specified topic. The third question re-
quires a 30-second response that expresses an 
opinion or gives an explanation related to the topic. 
This task is rated holistically on the combined im-
pact of delivery, use of structures, vocabulary, and 
task appropriateness on a 3-point scale.  
 
Task type 4: Constrained short-answer ques-
tions (Planning time: none; Response time: 15-
30 seconds; low/low-medium entropy) 
The test-taker responds to three questions about a 
schedule/agenda that is provided in written form. 
All the information needed to answer the questions 
should be included on or easily inferred from the 
schedule. The test-taker has 15 seconds to respond 
to each of the first two questions. These questions 
ask for specific information on the schedule or eas-
ily inferred information about the schedule. The 
test-taker has 30 seconds to respond to the last 
question which requires a summary of multiple 
events or multiple pieces of information on the 
schedule. This task is rated holistically on the 
combined impact of delivery, use of structures, 
vocabulary, task appropriateness and content accu-
racy on a 3-point scale.  
 
Task type 5: Respond to a voice mail (Planning 
time: 30 seconds; Response time: 60 seconds; 
high entropy) 
In this task, the test-taker listens to a voicemail that 
describes a problem, question or situation and then 
assumes a particular role (bank teller, office assis-
tant, etc.) to respond with a proposed solution or 
answer. This task is rated holistically on the com-
bined impact of fluency, pronunciation, intonation 
and stress, grammar, vocabulary, register, content 
relevance, and cohesion and idea progression on a 
5-point scale.  
 
Task type 6: Opinion task (Planning time: 15 
seconds; Response time: 60 seconds; high en-
tropy)  
In this task, the test-taker is expected to state an 
opinion or position on an issue that is familiar and 
accessible and to express support for the opinion or 
position with reasons, examples, arguments, etc. 
This task is rated holistically on the combined im-
pact of fluency, pronunciation, intonation and 
stress, grammar, vocabulary, content relevance, 
and cohesion and idea progression on a 5-point 
scale. 
3.2 Challenges of the THT test design to auto-
matic scoring 
1. Some of the tasks require responses that are ex-
pected to vary very little in vocabulary and content 
across examinees (e.g., Reading-aloud and Con-
strained short-answer questions) whereas others 
allow much more flexibility and variation in the 
use of vocabulary and grammatical structure and 
topical content (e.g. Respond to a voicemail and 
Opinion task). The predictability of the expected 
response will dictate what type of language model-
ing technique is preferable to optimize speech rec-
ognition results. Therefore, unlike in other systems 
focusing either on high or low entropy speech 
(e.g., Zechner and Bejar, 2006; Bernstein, 1999), 
in which a single speech recognizer is employed, it 
is anticipated that different types of speech recog-
nizers are needed to suit different THT task types. 
This may increase both the amount of development 
100
work and the complexity in integrating different 
types of recognizers into the real-time automated 
scoring system.   
2. Furthermore, the scoring criteria of these six 
different task types are somewhat different. This 
suggests that different scoring models may need to 
be developed for different task types since the 
relevant speech features to be included in the scor-
ing model for each task type may differ.   
3. THT speaking tasks use two kinds of score 
scales: 0-3 and 0-5. Classification techniques, such 
as classification trees or cumulative logit models 
(Agresti, 2002; Menard, 2001), may be more ap-
propriate for task types that use a 3-point scale. 
Prediction techniques such as multiple regression 
may be better suited for task types that are on a 5-
point scale. Training different types of scoring 
models will certainly increase the complexity and 
the amount of scoring model development and 
evaluation work.  
 In summary, the complexity of the design 
of the THT Speaking test is expected to have a ma-
jor impact on our efforts to develop an automated 
scoring system. Given these challenges and the 
research resources available, we decided on a strat-
egy of starting with high entropy task types and 
proceeding to low entropy task types. For this pa-
per, we selected the high entropy Opinion task and 
the medium-high entropy Picture tasks for system 
development.  
4 Adaptation of the speech recognizer  
For this work, we are using a state-of-the-art gen-
der-independent Hidden Markov Model speech 
recognizer whose acoustic model was trained on 
about 30 hours of non-native speech and whose 
language model was built on several hundred hours 
of both native and non-native speech. The non-
native data came from the TOEFL? Practice 
Online system, a web-based practice program for 
prospective takers of the Test Of English as a For-
eign Language (TOEFL) (Zechner et al, 2007). 
This data is somewhat different from the THT, as 
there are only high-entropy tasks in TOEFL Speak-
ing and as the speakers are generally more profi-
cient. Due to this difference, the baseline word 
accuracy was fairly low (see Table 1). 
Therefore, as a first step, we needed to adapt the 
automatic speech recognition engine to the THT 
speech data.  
We had approximately 1,000 responses each 
from the Picture and Opinion tasks transcribed. As 
mentioned above, while the Opinion task responses 
are generally more spontaneous, the Picture task 
requires the candidate to accurately describe a pic-
ture and thus restricts the possible answer space 
considerably. Still, there is more room for individ-
ual choice and variation in the vocabulary, gram-
mar and content produced than there is in the more 
restricted low-medium and low entropy task types 
in the THT Speaking test.  
When using our baseline automatic speech rec-
ognition (ASR)  engine without any adaptation to 
the THT speech data, we only obtained word accu-
racies between 25% and 33%, which was clearly 
inadequate, and far below a word accuracy where, 
at least for some speakers, meaningful information 
can be drawn from the ASR hypothesis. 
Therefore, we undertook a series of adaptation 
and optimization steps with the goal of maximizing 
the word accuracy on the two task types for the 
THT Speaking test. We first adapted the acoustic 
model in batch mode with supervised maximum a-
posteriori (MAP) adaptation using the combined 
data from both tasks, then the language model, op-
timized the filler cost parameter and finally con-
ducted unsupervised maximum likelihood linear 
regression (MLLR) acoustic model adaptation 
based on individual speakers. 
4.1 Acoustic model batch adaptation 
We randomly selected about 90% of Picture and 
Opinion task response data for acoustic model 
(AM) adaptation, which contained 1,800 response 
files (over 25 hours of speech, adult speakers with 
typically low to intermediate English proficiency). 
Results are always reported on the held-out evalua-
tion data containing 100 files for the Picture task 
and 80 files for the Opinion task. 
We performed supervised maximum a posteriori 
(MAP) adaptation which is the method of choice 
for larger amounts of data and is typically per-
formed in batch mode (Tomokiyo and Waibel, 
2001; Wang et al, 2003). After one cycle of adap-
tation, word accuracy improved by about 8%, as is 
shown in Table 1. We also performed unsupervised 
maximum likelihood linear regression (MLLR) 
adaptation, which is discussed in Section 4.4 be-
low. 
             
 
101
 Picture task word 
accuracy 
Opinion task word 
accuracy 
Method Absolute Increase 
from 
previous 
step 
Absolute Increase 
from 
previous 
step 
Baseline 
recognizer 
25.8% NA 
 
32.2% 
 
NA 
AM MAP 
adaptation 
33.6% 7.8% 
 
40.0% 
 
7.8% 
LM adap-
tation 
50.4% 16.8% 
 
51.0% 
 
11.0% 
Filler 
optimiza-
tion 
57.0% 6.6% 
 
56.3% 
 
5.3% 
 
Ignoring 
fillers 
60.5% 3.5% 
 
59.2% 
 
2.9% 
 
MLLR 
Speaker 
adaptation 
62.4% 1.9% 
 
61.2% 
 
2.0% 
 
Table 1.  Word accuracies after each incremental 
step of adaptation or optimization and performance 
improvement within each step for Picture and Opin-
ion task types.               
4.2 Language model adaptation 
The second step was language model (LM) adapta-
tion. The Picture and Opinion tasks were adapted 
separately using the same training sets as above. 
We built interpolated models between the task-
specific LM and the baseline LM (from the origi-
nal recognizer). 
We obtained the best results using only the task-
specific LM trained on the THT data set (given in 
Table 1). This indicates that the domain of each of 
the tasks is narrow enough that it can be suffi-
ciently described with a set of about 900 tran-
scribed examples each and it does not benefit from 
a larger LM such as our baseline LM.  
4.3 Filler cost optimization 
?Filler cost? is a recognizer-internal parameter that 
determines the likelihood of filler and noise words 
to be inserted into the hypothesis before or after 
?real? words. The higher the parameter?s value, the 
less likely fillers will be inserted. 
The experiments with the filler cost parameter 
grew out of an observation that the baseline recog-
nizer has a tendency to hypothesize too many 
words when faced with different kinds of ?uncer-
tain? audio, such as mumbled words, noises or fill-
ers. Therefore we conjectured that having the 
recognizer hypothesize more filler and noise words 
in these cases and be more restrictive with actual 
word hypotheses might increase the word accuracy 
overall. 
We varied the filler cost parameter from its de-
fault, 3, down to its lowest meaningful value, 0. 
Our experiments show that for fillercost=0, a 
maximum word accuracy was achieved (given in 
Table 1), albeit at the cost of more than doubling 
the length of the recognizer?s hypothesis by intro-
ducing a large amount of fillers (such as ?um? or 
?uh?, noises, mumbles etc.). We observe that using 
such a low filler cost parameter setting can nega-
tively affect some speech features which are can-
didates for being used in a scoring model, such as 
?language model score?. Therefore we have to 
carefully assess whether achieving a higher word 
accuracy is more beneficial to the overall perform-
ance of the feature set or whether it has too many 
negative effects on some important speech fea-
tures. In future work we will attempt to tune the 
recognizer in such a way that it is not only opti-
mized for a high word accuracy, but also for high 
accuracy in filler (and noise) prediction. 
Word accuracy was computed with the fillers 
included or excluded. Since fillers are not real 
words, and in this round of scoring model devel-
opment we did not use any features based on fill-
ers, it was reasonable to compute the overall word 
accuracy with the fillers removed from the human 
and recognizer transcriptions, resulting in a moder-
ate performance gain (see Table 1). 
4.4 Unsupervised speaker adaptation  
We used unsupervised maximum likelihood lin-
ear regression (MLLR) AM adaptation on top of 
the previous adaptation and optimization steps (To-
mokiyo and Waibel, 2001; Wang et al, 2003). In 
this step, all words whose confidence score was 
higher than a pre-set threshold were collected and 
their acoustic information was used to adapt the 
acoustic model. All adaptations were done based 
on the utterances of a single speaker and pertained 
to that speaker only, i.e., it was not incremental or 
cumulative. Since a second decoding run is needed 
after the actual MLLR adaptations, the recog-
nizer?s response time more than doubles when this 
method is employed. The unsupervised speaker 
adaptation led to an additional increase of  
 
102
Feature 
Number 
Feature  
Name 
Feature 
Class 
Description  Used in  
1 hmmscore Pronuncia-
tion  
Acoustic Model score: sum of the log probabilities of 
every frame, normalized for length 
Opinion & Picture  
2 typesper-
second 
Fluency & 
Vocabulary  
diversity  
Number of unique words in response (?types?) di-
vided by length of response 
Opinion & Picture 
3 
silences-
persecond Fluency  Number of silences per second Opinion & Picture 
4 repetitions Fluency  Number of repetitions divided by number of words Opinion  
5 relevance-
cos5 
Vocabulary 
& Content 
Cosine word vector product between a response and 
all responses in the training set that have the highest 
score (5 for the Opinion task) 
Opinion  
6 relevance-
cos3 
Vocabulary 
& Content 
Cosine word vector product between a response and 
all responses in the training set that have the highest 
score (3 for the Picture task) 
Picture 
Table 2. Final features used for the scoring models for the Opinion and Picture tasks 
 
 
approximately 2% for the Picture and Opinion 
tasks (see Table 1). There were large differences 
between different speakers in terms of the per-
formance gain of MLLR adaptation on our data 
set, however. There was also a large variation of 
word accuracies between speakers (13-100%). The 
variation in accuracy across speakers can be due to 
many different factors, including the degree of ac-
cent, the grammaticality of the response, the voice 
quality and the recording quality.  
5 Speech features  
Based on the output of the ASR engine, a feature 
computation module computes a set of about 40 
features for each response, mostly in the fluency 
domain (e.g.  ?average silence duration?), but also 
some features related to pronunciation, vocabulary 
diversity and content. 
Instead of using all of these features in a scoring 
model, we used a process of iterative refinement 
and selection to narrow down the feature set, based 
on both the coverage of the concept of communica-
tive competence and empirical performance (corre-
lations with human scores) of the features. 
Following this process, five features were selected 
to be included in developing the scoring models for 
the Opinion task type and four for the Picture task 
type (see Table 2). 
When we look at the correlations of these fea-
tures to the human scores, we find that hmmscore, 
after being transformed to improve normality, was 
the strongest predictor of human scores for both 
the Opinion and Picture tasks with typespersecond 
as the second strongest (0.5 <= Pearson r <= 0.7). 
6 Scoring models 
All the responses were double scored by a ran-
domly selected pair of raters who were trained for 
scoring this test. The agreements between the two 
ratings (both kappa and Pearson r correlation) were 
around 0.50 for the Picture and 0.72 for the Opin-
ion task. (Note that the fewer points a scale has, the 
lower correlation we can expect due to less score 
variability, everything else being equal.) 
While we use the same training sets for the scor-
ing model experiments as for the above ASR ex-
periments (sm-train), we add about 600 responses 
each to the evaluation sets (these responses were 
untranscribed) to yield a scoring model evaluation 
set size of about 700 responses each (sm-eval). 
Scoring models were developed and evaluated 
for the Opinion and Picture task types separately. 
The Opinion tasks are on a 0-5 point scale whereas 
the Picture tasks are on a 0-3 point scale. There 
were only a handful of 0s on each task and they 
were excluded in building the scoring models.  
For the Opinion tasks, multiple regression mod-
els employing different weights for the features 
were developed, namely an Equal Weights model, 
an Expert Weights model and an Optimal Weights 
model. In the Equal Weights model, each feature 
was assigned the same weight, indicating that all 
features are equally important in the prediction. In 
the Expert Weights model, different weights were 
assigned to different features that reflected our un-
derstanding of the different roles features play in 
indicating the overall speech quality. In the Opti-
mal Weights model, weights were determined by 
103
the least squares optimization procedure using the 
sm-train data.  All features were normalized to 
have a mean of 0 and a standard deviation of 1, 
such that their respective baseline influence on the 
model is comparable across features. 
For the Picture task type, CART was used to 
predict the score class each response should be 
assigned to. CART 5.0 (Steinberg & Colla, 1997) 
was used to build the classification trees. 
In addition, generic and task-specific models 
were developed for both task types. The task-
specific models made use of task-specific vocabu-
lary features (Features 5 and 6 in Table 2) which 
required using previous response data to each of 
the tasks within a particular task type. (Both task 
types had 4 different tasks each). The generic 
models, in contrast, used features that were the 
same across all tasks for a particular task type and 
did not use any task-specific vocabulary features.  
As it would be much more time-consuming and 
costly to build task-specific models, it is worth-
while to investigate how much more predictive 
power the task-specific vocabulary features could 
add over and beyond the features in the generic 
models. 
6.1    Opinion task type  
For the Opinion tasks, four features were used in 
building the generic models and five in developing 
the task-specific models. The following features  
Were used: hmmscore, typespersecond, silences-
persecond, repetitions and relevancecos5 (the latter 
only in the task-specific model). 
Table 3 shows the results on the sm-eval set. 
The Expert Weights model and the Optimal 
Weights models yielded very similar results 
(weighted kappa and correlation = 0.61-0.63) if we 
look at predicted scores that were rounded to the 
nearest integer. The agreements between regres-
sion model predicted scores and scores of human 
rater 1 were just a little below the agreements be-
tween two human raters (weighted kappa and cor-
relation = 0.72). However, the results for the Equal 
Weights model were inferior.  
The results for the task-specific models showed 
no improvement over the generic models, suggest-
ing that the task-specific vocabulary feature did not 
contribute more predictive power beyond the four 
features already in the generic models.  
 
 
Model 
Multiple 
Regres-
sion 
(Equal 
Weights)
Multiple 
Regression 
(Expert 
Weights) 
Multiple 
Regres-
sion (Op-
timal 
Weights)
 
Weighted ? 0.53 0.62 0.61 
Pearson r 
Correlation 
(unrounded)
0.62 0.68 0.69 
Pearson r 
Correlation 
(rounded) 
0.56 0.63 0.63 
Table 3. Performance of different weighting schemes 
on THT scoring model evaluation set for Opinion 
tasks (generic model)  
6.2    Picture task type  
As mentioned earlier, the Picture tasks are on a 0-3 
point scale and we removed a small number of 0-
scores from the analyses, making it a 3-point scale. 
Given this particular score scale, multiple regres-
sion may not be appropriate for this data as it re-
quires a continuous or a quasi-continuous 
dependent variable (i.e. a variable that has at least 
5 or more data points). Some classification tech-
niques such as CART (Brieman et al, 1984) or 
logistic regression, which can take ordered score 
categories as the outcome variable, are better 
suited for this data. In this study, we analyzed the 
data with CART models.  
CART 5.0 (Steinberg and Colla, 1997) was used 
to build the classification trees.  We built two sets 
of CART models, one set with the task-specific 
vocabulary feature (relevancecos3) and one set 
without it. We explored different model configura-
tions, i.e., different combinations of priors and 
splitting rules.  For each combination, a 10-fold 
cross-validation was conducted.  Subsequently, the 
optimal sub tree that was a relatively small tree 
with the highest or near-highest agreement with the 
human scores (weighted kappa) on the cross-
validation sample was identified. Then the cases in 
the sm-eval data set were dropped down the opti-
mal tree to obtain the evaluation results on the 
held-out data.  
The results for the generic model vs. task-
specific models are compared in Table 4. For both 
104
models, CART trees built using the Twoing1 split-
ting rule combined with mixed priors (average of 
equal priors for different score classes and sm-train 
sample priors) yielded the best kappa values on the 
cross-validation data and were selected as the op-
timal trees. The agreements between the CART 
model predicted scores and first rater scores 
slightly exceeded that between two human raters 
on the sm-eval data set. Another observation from 
Table 4 was that for this task type, the task-specific 
CART model did not demonstrate an advantage 
over the generic model; actually, its performance 
was slightly worse than that of the generic model, a 
finding in line with the Opinion task. 
 
 Generic Task-specific 
Inter-human 
agreement 
Weighted ? 0.51 0.50 0.49 
Pearson r 
Correlation  0.52 0.50 0.50 
Table 4. Performance of CART models on THT 
scoring model evaluation set for Picture tasks (ge-
neric model vs. task-specific model)  
7 Discussion 
This paper investigates the feasibility of develop-
ing an automatic scoring system for the THT 
Speaking test, focusing on the particular challenges 
posed by the design of the test. The main challenge 
posed by the test design is the high variability in 
task types -- ranging from low-entropy Reading-
aloud tasks to high-entropy Opinion tasks. While 
previous tests of spoken language have focused 
mainly on either high or low entropy tasks (Bern-
stein, 1999; Zechner and Bejar, 2006), we have 
made an attempt at starting to address the whole 
scale of entropy within a single test. 
In this paper, we selected one high entropy task 
(Opinion) and one medium-high entropy task (Pic-
ture) to start our explorations. While we found that 
we could, for the most part, use a similar set of 
features for both tasks, we had to address the dif-
ference in score scales between these two task 
types. While we could use multiple regression for 
scoring the 5-point-scale Opinion task, we had to 
                                                          
1 The Twoing rule divides the cases into two 
groups, gathers similar classes together, and at-
tempts to separate the two groups in descendant 
nodes.  
 
employ CART trees for the 3-point-scale Picture 
task, demonstrating that one can not necessarily 
use one type of scoring model for all tasks. 
When moving to low and low-medium entropy 
tasks, we expect further adaptations, both in terms 
of the feature set (e.g., the higher importance of 
pronunciation features in Reading-aloud tasks), 
and in speech recognition, where more restrictive 
language models will be needed. 
We have reported findings associated with the 
performance of the scoring models for the Opinion 
and Picture task types. Overall, the preliminary 
findings are quite promising: with a few key 
speech features, we were able to achieve prediction 
accuracies that could almost emulate or slightly 
exceed the agreements between two human raters 
at task level. Once we have developed scoring 
models for all task types, it is conceivable to ag-
gregate the task level scores to produce a total 
summary score at the test level and it is very likely 
we would see a much stronger association between 
human scores and automated scores for the whole 
test.  
The findings also suggest that task-specific 
modeling efforts did not seem to be necessary for 
the two task types investigated. This does not pre-
clude the possibility, though, that task-specific 
scoring models are superior for other task types in 
which the expected content is much more restricted 
(such as the Constrained short-answer questions). 
8 Conclusions and future work 
We have demonstrated that by using a three-stage 
architecture of automatic speech recognition, fea-
ture computation, and scoring models, we are able 
to achieve some degree of success in generating 
automated scores for two task types of a spoken 
language test with a wide variation in entropy in its 
tasks. The agreement between machine scores and 
human scores comes close to or reaches the inter-
human agreement levels for these two tasks. 
In future work, we will switch our focus to task 
types that elicit more constrained speech (such as 
the Reading-aloud tasks and Constrained short-
answer questions). In the meantime, we will con-
tinue to refine and evaluate the preliminary scoring 
models developed in this paper. In particular, we 
will explore cumulative logit models for tasks that 
are on a 0-3 point scale and compare the results to 
those of CART models. 
105
References  
Agresti, A. (2002). Categorial data analysis (2nd 
ed.).  New York: Wiley.  
Bachman, L.F. (1990). Fundamental Considera-
tions in Language Testing.  New York: Oxford 
University Press. 
Bachman, L. F., and Palmer, A. S. (1996). Lan-
guage testing in practice. Ox-
ford:OxfordUniversity Press.  
Bernstein, J. (1999). PhonePass testing: Structure 
and construct. Menlo Park, CA: Ordinate Corpo-
ration. 
Bernstein, J., DeJong, J., Pisoni, D., and Town-
shend, B. (2000). Two experiments in automatic 
scoring of spoken language proficiency. In-
STILL2000, Dundee, Scotland. 
Brieman, L., Jerome F., Olshen, R., and Stone, C. 
(1984). Classification and Regression Trees. Pa-
cific Grove: Wadsworth.  
Cucchiarini, C., Strik, H., & Boves, L. (1997a). 
Using speech recognition technology to assess 
foreign speakers' pronunciation of Dutch. Third 
international symposium on the acquisition of 
second language speech: NEW SOUNDS 97, 
Klagenfurt, Austria. 
Cucchiarini, C., Strik, S., and Boves, L. (1997b). 
Automatic evaluation of Dutch pronunciation by 
using speech recognition technology. IEEE 
Automatic Speech Recognition and Understand-
ing Workshop, Santa Barbara, CA. 
Franco, H., Abrash, V., Precoda, K., Bratt, H., 
Rao, R., and Butzberger, J. (2000). The SRI 
EduSpeak system: Recognition and pronuncia-
tion scoring for language learning. InSTiLL-
2000 (Intelligent Speech Technology in Lan-
guage Learning), Dundee, Scotland. 
Menard, S. (2001). Applied logistic regression 
analysis. Sage University Paper Series on Quan-
titative Applications in the Social Sciences 07-
106, Thousand Oaks, CA: Sage.  
North, B. (2000). The Development of a Common 
Framework Scale of Language Proficiency. 
New York, NY: Peter Lang. 
 
Steinberg, D., and Colla, P. (1997). CART -- Clas-
sification and Regression Trees. San Diego, CA: 
Salford Systems.  
Tomokiyo, L. M., and Waibel, A. (2001). Adapta-
tion methods for non-native speech. Multilin-
guality in Spoken Language Processing, 
Aalborg. 
Wang, Z., Schultz, T., and Waibel, A. (2003). 
Comparison of acoustic model adaptation tech-
niques on non-native speech. IEEE International 
Conference on Acoustics, Speech, and Signal Proc-
essing (ICASSP-2003), Hong Kong, China. 
Zechner, K., and Bejar, I. (2006). Towards Auto-
matic Scoring of Non-Native Spontaneous 
Speech. HLT-NAACL-06, New York, NY. 
Zechner, K., Higgins, D., and Xi, X. (2007). 
SpeechRater?: A Construct-Driven Approach to 
Score Spontaneous Non-Native Speech. Pro-
ceedings of the 2007 Workshop of the Interna-
tional Speech Communication Association 
(ISCA) Special Interest Group on Speech and 
Language Technology in Education (SLaTE), 
Farmington, PA, October. 
 
 
106
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
        Automatic Scoring of Children's Read-Aloud Text Passages and 
Word Lists 
 
Klaus Zechner and John Sabatini and Lei Chen 
Educational Testing Service 
Rosedale Road 
Princeton, NJ 08541, USA 
{kzechner,jsabatini,lchen}@ets.org 
 
 
 
 
Abstract 
Assessment of reading proficiency is typically 
done by asking subjects to read a text passage 
silently and then answer questions related to 
the text. An alternate approach, measuring 
reading-aloud proficiency, has been shown to 
correlate well with the aforementioned com-
mon method and is used as a paradigm in this 
paper.  
We describe a system that is able to automati-
cally score two types of children?s read speech 
samples (text passages and word lists), using 
automatic speech recognition and the target 
criterion ?correctly read words per minute?. 
Its performance is dependent on the data type 
(passages vs. word lists) as well as on the rela-
tive difficulty of passages or words for indi-
vidual readers. Pearson correlations with 
human assigned scores are around 0.86 for 
passages and around 0.80 for word lists. 
1 Introduction 
It has long been noted that a substantial number of 
U.S. students in the 10-14 years age group have 
deficiencies in their reading competence (National 
Center of Educational Statistics, 2006). With the 
enactment of the No Child Left Behind Act (2002), 
interest and focus on objectively assessing and im-
proving this unsatisfactory situation has come to 
the forefront. 
While assessment of reading is usually done post-
hoc with measures of reading comprehension, di-
rect reading assessment is also often performed 
using a different method, oral (read-aloud) reading. 
In this paradigm, students read texts aloud and 
their proficiency in terms of speed, fluency, pro-
nunciation, intonation etc. can be monitored di-
rectly while reading is in progress. In the reading 
research literature, oral reading has been one of the 
best diagnostic and predictive measures of founda-
tional reading weaknesses and of overall reading 
ability (e.g., Deno et al, 2001; Wayman et al, 
2007).  An association between low reading com-
prehension and slow, inaccurate reading rate has 
been confirmed repeatedly in middle school popu-
lations (e.g., Deno & Marsten, 2006).  Correlations 
consistently fall in the 0.65-0.7 range for predict-
ing untimed passage reading comprehension test 
outcomes (Wayman et al, 2007). 
 
In this paper, we investigate the feasibility of 
large-scale, automatic assessment of read-aloud 
speech of middle school students with a reasonable 
degree of accuracy (these students typically attend 
grades 6-8 and their age is in the 10-14 years 
range).  If possible, this would improve the utility 
of oral reading as a large-scale, school-based as-
sessment technique, making it more efficient by 
saving costs and time of human annotations and 
grading of reading errors. 
The most widely used measure of oral reading pro-
ficiency is ?correctly read words per minute? 
(cwpm) (Wayman et al, 2007). To obtain this 
measure, students? read speech samples are first 
10
recorded, then the reading time is determined, and 
finally a human rater has to listen to the recording 
and note all reading errors and sum them up. Read-
ing errors are categorized into word substitutions, 
deletions etc.  
We have several sets of digitally recorded read-
aloud samples from middle school students avail-
able which were not collected for use with auto-
matic speech recognition (ASR) but which were 
scored by hand. 
Our approach here is to pass the children?s speech 
samples through an automatic speech recognizer 
and then to align its output word hypotheses with 
the original text that was read by the student. From 
this alignment and from the reading time, an esti-
mate for the above mentioned measure of cwpm 
can then be computed. If the automatically com-
puted cwpm measures are close enough to those 
obtained by human hand-scoring, this process may 
be employed in real world settings eventually to 
save much time and money. 
 
Recognizing children?s speech, however, has been 
shown to be substantially harder than adult speech 
(Lee et al, 1999; Li and Russell, 2002), which is 
partly due to children?s higher degree of variability 
in different dimensions of language such as pro-
nunciation or grammar. In our data, there was also 
a substantial number of non-native speakers of 
English, presenting additional challenges. We used 
targeted training and adaptation of our ASR sys-
tems to achieve reasonable word accuracies. While 
for text passages, the word accuracy on unseen 
speakers was about 72%, it was only about 50% 
for word lists, which was due in part to a higher 
percentage of non-native speakers in this data set, 
to the fact that various sources of noise often pre-
vented the recognizer from correctly locating the 
spoken words in the signal, and also due to our 
choice of a uniform language model since conven-
tional n-gram models did not work on this data 
with many silences and noises between words. 
 
The remainder of this paper is organized as fol-
lows: in Section 2 we review related work, fol-
lowed by a description of our data in Section 3. 
Section 4 provides a brief description of our speech 
recognizer as well as the experimental setup. Sec-
tion 5 provides the results of our experiments, fol-
lowed by a discussion in Section 6 and conclusions 
and future work in Section 7. 
 
2 Related work 
Following the seminal paper about the LISTEN 
project (Mostow et al 1994), a number of studies 
have been conducted on using automatic speech 
recognition technology to score children?s read 
speech. 
 
Similar to automated assessment of adults? speech 
(Neumeyer, Franco et al 2000; Witt, 1999), the 
likelihood computed in the Hidden Markov Model 
(HMM) decoding and some measurements of  flu-
ency, e.g., speaking rate, are widely used as fea-
tures for predicting children?s speaking 
proficiency. Children?s speech is different than 
adults?. For example, children?s speech exhibits 
higher fundamental frequencies (F0) than adults on 
average. Also, children?s more limited knowledge 
of vocabulary and grammar results in more errors 
when reading printed text. Therefore, to achieve 
high-quality recognition on children?s speech, 
modifications have to be made on recognizers that 
otherwise work well for adults. 
In the LISTEN project (Mostow et al, 1994), the 
basic technology is to use speech recognition to 
classify each word of text as correctly read or not. 
Such a classification task is hard in that the chil-
dren?s speaking deviations from the text may in-
clude arbitrary words and non-words. In a study, 
they modeled variations by the modification of the 
lexicon and the language model of the Sphinx1 
speech recognizer.  
 
Recently, the Technology Based Assessment  of 
Language and Literacy project (TBALL,  (Alwan, 
2007)) has been attempting to assess and evaluate 
the language and literacy skills of young children 
automatically. In the TBALL project, a variety of 
tests including word verification, syllable blending, 
letter naming, and reading comprehension, are 
jointly used. Word verification is an assessment 
that measures the child?s pronunciation of read-
aloud target words. A traditional pronunciation 
verification method based on log-likelihoods from 
HMM models is used initially (Tepperman et al, 
2006). Then an improvement based on a Bayesian 
network classifier (Tepperman et al, 2007) is em-
                                                          
1 See http://cmusphinx.sourceforge.net/html/cmusphinx.php 
11
ployed to handle complicated errors such as pro-
nunciation variations and other reading mistakes. 
 
Many other approaches have been developed to 
further improve recognition performance on chil-
dren?s speech. For example, one highly accurate 
recognizer of children?s speech has been developed 
by Hagen et al (2007). Vocal tract length normali-
zation (VTLN) has been utilized to cope with the 
children?s different acoustic properties. Some spe-
cial processing techniques, e.g., using a general 
garbage model to model all miscues in speaking, 
have been devised to improve the language model  
used in the recognition of children?s speech (Li et 
al., 2007). 
 
3 Data 
For both system training and evaluation, we use a 
data set containing 3 passages read by the same 
265 speakers (Set1) and a fourth passage (a longer 
version of Passage 1), read by a different set of 55 
speakers (Set2). Further, we have word lists read 
by about 500 different speakers (Set3). All speak-
ers from Set12 and most (84%) from the third set 
were U. S. middle school students in grades 6-8 
(age 10-14). A smaller number of older students in 
grades 10-12 (age 15-18) was also included in the 
third set (16%).3 4  
In terms of native language, about 15% of Set1 and 
about 76% of Set35 are non-native speakers of 
English or list a language different from English as 
their preferred language. 
Table 1 provides the details of these data sets. In 
the word lists data set, there are 178 different word 
lists containing 212 different word types in total 
(some word lists were read by several different 
students). 
 
All data was manually transcribed using a spread-
sheet where each word is presented in one line and 
the annotator, who listens to the audio file, has to 
                                                          
2 For Set1, we have demographics for 254 of 265 speakers 
(both for grade level and native language). 
3 Grade demographics are available for 477 speakers of Set3. 
4 We do not have demographic data for the small Set2 (55 
speakers). 
5 This set (Set 3) has information on native language for 165 
speakers. 
mark-up any insertions, substitutions or deletions 
by the student.  
 
Name Recordings Length in 
words 
Passage 1 
(?Bed?, Set1-A) 
265 158 
Passage 2 
(?Girls?, Set1-B) 
265 74 
Passage 3 
(?Keen?, Set1-C) 
265 100 
Passage 4 
(?Bed*?) (Set2) 
55 197 
Word lists (Set3) 590 62 (average) 
Table 1. Text passages and word lists data sets. 
 
For ASR system training only, we additionally 
used parts of the OGI (Oregon Graduate Institute) 
and CMU (Carnegie Mellon University) Kids data 
sets as well (CSLU, 2008; LDC, 1997). 
 
4 ASR system and experiments 
The ASR system?s acoustic model (AM) was 
trained using portions of the OGI and CMU Kids? 
corpora as well as a randomly selected sub-set of 
our own passage and word list data sets described 
in the previous section. About 90% of each data set 
(Set1, Set2, Set3) was used for that purpose. Since 
the size of our own data set was too small for AM 
training, we had to augment it with the two men-
tioned corpora (OGI, CMU Kids), although they 
were not a perfect match in age range and accent. 
All recordings were first converted and down-
sampled to 11 kHz, mono, 16 bit resolution, PCM 
format. There was no speaker overlap between 
training and test sets. 
 
For the language model (LM), two different mod-
els were created: for passages, we built an interpo-
lated trigram LM where 90% of the weight is 
assigned to a LM trained only on the 4 passages 
from the training set (Set1, Set2) and 10% to a ge-
neric LM using the Linguistic Data Consortium 
(LDC) Broadcast News corpus (LDC, 1997). The 
dictionary contains all words from the transcribed 
passages in the training set, augmented with the 
1,000 most frequent words from the Broadcast 
News corpus. That way, the LM is not too restric-
tive and allows the recognizer to hypothesize some 
12
reading mistakes not already encountered in the 
human transcriptions of the training set. 
 
For the word lists, a trigram LM was found to be 
not working well since the words were spoken in 
isolation with sometimes significant pauses in be-
tween and automatic removal of these silences 
proved too hard given other confounding factors 
such as microphone, speaker, or background noise. 
Therefore it was decided to implement a grammar 
LM for the word list decoder where all possible 
words are present in a network that allows them to 
occur at any time and in any sequence, allowing 
for silence and/or noises in between words. This 
model with uniform priors, however, has the dis-
advantage of not including any words not present 
in the word list training set, such as common mis-
pronunciations and is therefore more restrictive 
than the LM for text passages. 
 
One could make the argument of using forced 
alignment instead of a statistical LM to determine 
reading errors. In fact, this approach is typically 
used when assessing the pronunciation of read 
speech. However, in our case, the interest is more 
in determining how many words were read cor-
rectly in the sequence of the text (and how fast 
they were read) as opposed to details in pronuncia-
tion. Further, even if we had confidence scores 
attached to words in forced alignment, deciding on 
which of the words obtained low confidence due to 
poor pronunciation or due to substitution would 
not be an easy decision. Finally, word deletions 
and insertions, if too frequent, might prevent the 
forced alignment algorithm from terminating. 
 
After training was complete, we tested the recog-
nizer on the held-out passage and word list data. 
After recognizing, we computed our target meas-
ure of ?correct words per minute? (cwpm) accord-
ing to the following formula (W= all words in a 
text, S= substitutions, D= deletions, T= reading 
time in minutes), performing a string alignment 
between the recognizer hypothesis and the passage 
or word list to be read: 
 
(1) 
W S D
cwpm
T
? ?=   
The reason that insertions are not considered here 
is that they contribute to an increase in reading 
time and therefore can be considered to be ac-
counted for already in the formula.  
 
Next, we performed an experiment that looks at 
whether automatic scoring of read-aloud speech 
allows for accurate predictions of student place-
ments in broad cohorts of reading proficiency. 
 
We then also look more closely at typical errors 
made by human readers and the speech recognizer. 
All these experiments are described and discussed 
in the following section. 
 
Table 2 describes the set-up of the experiments. 
Note that Passage4 (Set2) was included only in the 
training but not in the evaluation set since this set 
was very small. As mentioned in the previous sec-
tion, most speakers from the passage sets read 
more than one passage and a few speakers from the 
word lists set read more than one word list. 
 
Data set Recordings Speakers Language 
model 
type 
Passages1-
3 
101 37 Trigram  
Word lists 42 38 Grammar 
Table 2. Experiment set-up (evaluation sets). 
 
5 Results 
5.1 Overall results 
Table 3 depicts the results of our evaluation run 
with the ASR system described above. Word accu-
racy is measured against the transcribed speaker 
reference (not against the true text that was read). 
Word accuracy is computed according to Equation 
(2), giving equal weight to reference and ASR hy-
pothesis (c=correct, s=substitutions, d=deletions, 
i=insertions). This way, the formula is unbiased 
with respect to insertions or deletions: 
 
(2)  
0.5 100.0
c c
wacc
c s d c s i
? ?= ? ? +? ?+ + + +? ?  
 
 
13
 
 
Data set Recordings Speakers Average word 
Accuracy over all 
speech sample 
Minimum word 
accuracy on a 
speech sample 
Maximum word  
accuracy on a speech 
sample 
All Passages  
(1-3) 
101 37 72.2 20.4 93.8 
Passage1 
(?Bed?) 
28 28 70.8 20.4 83.6 
Passage2 
(?Girls?) 
36 36 64.1 25.4 85.7 
Passage3 
(?Keen?) 
37 37 77.7 27.4 93.8 
Word lists 42 38 49.6 10.8 78.9 
Table 3. ASR experiment results (word accuracies in percent) 
 
 
The typical run-time on a 3.2GHz Pentium proces-
sor was less than 30 seconds for a recording (faster 
than real time). 
 
We next compute cwpm measures for both human 
annotations (transcripts, ?gold standard?) and ma-
chine (ASR) hypotheses  
Human annotators went over each read passage 
and word list and marked all reading errors of the 
speakers (here, only deletions and substitutions are 
relevant). The reading time is computed directly 
from the speech sample, so machine and human 
cwpm scores only differ in error counts of dele-
tions and substitutions. Currently we only have one 
human annotation available per speech sample, but 
we aim to obtain a second annotation for the pur-
pose of determining inter-annotator agreement. 
 
Table 4 presents the overall results of comparing 
machine and human cwpm scoring. We performed 
both Pearson correlation as well as Spearman rank 
correlation. While the former provides a more ge-
neric measure of cwpm correlation, the latter fo-
cuses more on the question of the relative 
performance of different speakers compared to 
their peers which is usually the more interesting 
question in practical applications of reading as-
sessment. Note that unlike for Table 3, the ASR 
hypotheses are now aligned with the text to be read 
since in a real-world application, no human tran-
scriptions would be available. 
We can see that despite the less than perfect recog-
nition rate of the ASR system which causes a much 
lower average estimate for cwpm or cw (for word-
lists), both Pearson and Spearman correlation coef-
ficients are quite high, all above 0.7 for Spearman 
rank correlation and equal to 0.8 or higher for the 
Pearson product moment correlation. This is en-
couraging as it indicates that while current ASR 
technology is not yet able to exactly transcribe 
children?s read speech, it is 
 
Data set Gold 
cwpm
ASR-
based 
cwpm 
Pearson 
r corre-
lation 
Spearman 
rank cor-
relation 
All Pas-
sages  
(1-3) 
152.0 109.8 0.86 NA 
Passage1 
(Bed) 
174.3 123.5 0.87 0.72 
Passage2 
(Girls) 
133.1 86.5 0.86 0.73 
Passage3 
(Keen) 
153.4 122.2 0.86 0.77 
Word 
lists* 
 
48.0 29.4 0.80 0.81 
Table 4. CWPM results for passages and word 
lists. All correlations are significant at p<0.01. 
*For word lists, we use ?cw? (correct words, nu-
merator of Equation (1)) as the measure, since stu-
dents were not told to be rewarded for faster 
reading time here. 
 
possible to use its output to compute reasonable 
read-aloud performance measures such as cwpm 
14
which can help to quickly and automatically assess 
reading proficiencies of students. 
5.2 Cohort assignment experiment 
To follow up on the encouraging results with basic 
and rank correlation, we conducted an experiment 
to explore the question of practical importance 
whether the automatic system can assign students 
to reading proficiency cohorts automatically. 
For better comparison, we selected those 27 stu-
dents from 37 total who read all 3 passages (Set 1) 
and grouped them into three cohorts of 9 students 
each, based on their human generated cwpm score 
for all passages combined: (a) proficient 
(cwpm>190), (b) intermediate (135<cwpm<190), 
and (c) low proficient (cwpm<135). 
We then had the automatic system predict each 
student?s cohort based on the cwpm computed 
from ASR. Since ASR-based cwpm values are co-
nsistently lower than human annotator based cwpm 
values, the automatic cohort assignment is not 
based on the cwpm values but rather on their rank-
ing. 
The outcome of this experiment is very encourag-
ing in that there were no cohort prediction errors 
by the automatic system. While the precise ranking 
differs, the system is very well able to predict 
overall cohort placement of students based on 
cwpm. 
5.3 Overall comparison of students? reading er-
rors and ASR recognition errors 
To look into more detail of what types of reading 
errors children make and to what extent they are 
reflected by the ASR system output, we used the 
sclite-tool by the National Institute for Standards 
and Technology (NIST, 2008) and performed two 
alignments on the evaluation set: 
1. TRANS-TRUE: Alignment between human 
transcription and true passage or word list text to 
be read: this alignment informs us about the kinds 
of reading errors made by the students. 
2. HYPO-TRANS: Alignment between the ASR 
hypotheses and the human transcriptions; this 
alignment informs us of ASR errors. (Note that this 
is different from the experiments reported in Table 
4 above where we aligned the ASR hypotheses 
with the true reference texts to compute cwpm.) 
 
Table 5 provides general statistics on these two 
alignments. 
 
Data set Alignment SUB DEL INS 
Passages
1-3 
TRANS-
TRUE 
2.0% 6.1% 1.8% 
Pas-
sages1-3 
HYPO-
TRANS 
18.7% 9.6% 8.1% 
Word 
lists 
TRANS-
TRUE 
5.6% 6.2% 0.6% 
Word 
lists 
HYPO-
TRANS 
42.0%  8.9% 6.4% 
Table 5. Word error statistics on TRANS-TRUE 
and HYPO-TRANS alignments for both evaluation 
data sets. 
 
From Table 5 we can see that while for students, 
deletions occur more frequently than substitutions 
and, in particular, insertions, the ASR system, due 
to its imperfect recognition, generates mostly sub-
stitutions, in particular for the word lists where the 
word accuracy is only around 50%. 
Further, we observe that the students? average 
reading word error rate (only taking into account 
substitutions and deletions as we did above for the 
cwpm and cw measures) lies around 8% for pas-
sages and 12% for wordlists (all measured on the 
held-out evaluation data). 
5.4 Specific examples 
Next, we look at some examples of frequent confu-
sion pairs for those 4 combinations of data sets and 
alignments. Table 6 lists the top 5 most frequent 
confusion pairs (i.e., substitutions).  
 
For passages, all of the most frequent reading er-
rors by students are morphological variants of the 
target words, whereas this is only true for some of 
the ASR errors, while other ASR errors can be far 
off the target words. For word lists, student errors 
are sometimes just orthographically related to the 
target word (e.g., ?liner? instead of ?linear?), and 
sometimes of different part-of-speech (e.g., 
?equally? instead of ?equality?). ASR errors are 
typically related to the target word by some pho-
netic similarity (e.g., ?example? instead of ?sim-
ple?). 
 
15
 
Finally, we look at a comparison between errors 
made by the students and the fraction of those cor-
rectly identified by the ASR system in the recogni-
tion hypotheses. Table 7 provides the statistics on 
these matched errors for text passages and word 
lists.  
 
Data 
set 
Align-
ment 
Refer-
ence 
Spoken/ 
recog-
nized 
Count 
Pas-
sages
1-3 
TRANS
-TRUE 
asks 
savings 
projects 
teacher?s 
time 
ask 
saving 
project 
teacher 
times 
6 
5 
4 
4 
4 
Pas-
sages
1-3 
HYPO-
TRANS 
storm 
lee?s 
lee?s 
observer 
thousand 
storms 
be 
we 
and 
the 
11 
6 
6 
6 
6 
Word 
lists 
TRANS
-TRUE 
nature 
over-
sleep 
equality 
linear 
ware-
housed 
Natural 
overslept 
 
equally 
liner 
ware-
house 
6 
5 
 
4 
4 
3 
Word 
lists 
HYPO-
TRANS 
plan      
see  
simple 
unoffi-
cial   
loud 
planned 
season 
example 
competi-
tion 
through-
out 
8 
6 
6 
5 
 
4 
Table 6. Top 5 most frequent confusion pairs for 
passages and word list evaluation sets in two dif-
ferent alignments. For passages, substitutions 
among closed class words such as determiners or 
prepositions are omitted. 
 
Table 7 shows that while for text passages, almost 
half of the relevant errors (substitutions and dele-
tions) were correctly identified by the recognizer, 
for word lists, this percentage is substantially 
smaller. 
 
 
 
 
6 Discussion 
The goal of this paper is to evaluate the possibility 
of creating a system for automatic oral reading as-
sessment for middle school children, based on text 
passages and word lists. 
We decided to use the common reading profi-
ciency measure of ?correct words per minute? 
which enables us to align ASR word hypotheses 
with the correct texts, estimate cwpm based on this 
alignment and the reading time, and then compare 
the automatically estimated cwpm with human an-
notations of the same texts. 
 
 
Data set / error type Percentage of correctly 
identified errors 
Passages 1-3 ? SUB 20.6 
Passages 1-3 ? DEL 56.4 
Passages 1-3 ? 
SUB+DEL 
47.7 
Word lists ? SUB 2.7 
Word lists ? DEL 29.4 
Word lists ? 
SUB+DEL 
16.8 
Table 7. Statistics on matched errors: percentage of  
students? reading errors (substitutions and dele-
tions) that were also correctly identified by the 
ASR system. 
 
We built a recognizer with an acoustic model 
based on CMU and OGI kids? corpora as well as 
about 90% of our own text passages and word list 
data (Sets 1-3). For the in-context reading (text 
passages) we trained a trigram model focused 
mostly on transcriptions of the passages. For the 
out-of-context isolated word reading, we used a 
grammar language model where every possible 
word of the word lists in the training set can follow 
any other word at any time, with silence and/or 
noise between words. (While this was not our pre-
ferred choice, standard n-gram language models 
performed very poorly given the difficulty of re-
moving inter-word silences or noise automati-
cally.) 
Given how hard ASR for children?s speech is and 
given our small matched data sets, the word accu-
racy of 72% for text passages was not unreason-
able and was acceptable, particularly in a first 
development cycle. The word accuracy of only 
about 50% for word lists, however, is more prob-
16
lematic and we conjecture that the two main rea-
sons for the worse performance were (a) the ab-
sence of time stamps for the location of words 
which made it sometimes hard for the recognizer to 
locate the correct segment in the signal for word 
decoding (given noises in between), and (b) the 
sometimes poor recording conditions where vol-
umes were set too high or too low, too much back-
ground or speaker noise was present etc. Further, 
the high relative number of non-native speakers in 
that data set may also have contributed to the lower 
word accuracy of the word lists. 
While the current data collection had not been 
done with speech recognition in mind, in future 
data collection efforts, we will make sure that the 
sound quality of recordings is better monitored, 
with some initial calibration, and that we store time 
stamps when words are presented on the screen to 
facilitate the recognition task and to allow the rec-
ognizer to expect one particular word at one par-
ticular point in time. 
Despite imperfect word accuracies, however, for 
both passages and word lists we found encourag-
ingly high correlations between human and auto-
matic cwpm measures (cw measures for word 
lists). Obviously, the absolute values of cwpm dif-
fer greatly as the ASR system generates many 
more errors on average than the readers, but both 
Pearson correlation as well as Spearman rank cor-
relation measures are all above 0.7. This means 
that if we would use our automatic scoring results 
to rank students? reading proficiency, the ranking 
order would be overall quite similar to an order 
produced by human annotators. This observation 
about the rank, rather than the absolute value of 
cwpm, is important in so far as it is often the case 
that educators are interested in separating ?co-
horts? of readers with similar proficiency and in 
particular to identify the lowest performing cohort 
for additional reading practice and tutoring. 
An experiment testing the ability of the system to 
place students into three reading proficiency co-
horts based on cwpm was very encouraging in that 
all 27 students of the test set were placed in the 
correct cohort by the system. 
When we compare frequent student errors with 
those made by the machine (Table 6), we see that 
often times, students just substitute slight morpho-
logical variants (e.g., ?ask? for ?asks?), whereas in 
the ASR system, errors are typically more complex 
than just simple substitutions of morphological 
variants. However, in the case of word lists, we do 
find substitutions with related phonological content 
in the ASR output (e.g., ?example? for ?simple?). 
Finally, we observed that, only for the text pas-
sages, the ASR system could correctly identify a 
substantial percentage of readers? substitutions and 
deletions (about 48%, see Table 7). This is also 
encouraging as it is a first step towards meaningful 
feedback in a potential interactive setting. How-
ever, we here only look at recall ? because of the 
much larger number of ASR substitutions, preci-
sion is much lower and therefore the risk of over-
correction (false alarms) is still quite high. 
Despite all of the current shortcomings, we feel 
that we were able to demonstrate a ?proof-of-
concept? with our initial system in that we can use 
our trained ASR system to make reliable estimates 
on students? reading proficiency as measured with 
?correct words per minute?, where correlations 
between human and machine scores are in the 
0.80-0.86 range for text passages and word lists. 
 
7 Conclusions and future work 
This paper demonstrates the feasibility of building 
an automatic scoring system for middle school stu-
dents? reading proficiency, using a targeted trained 
speech recognition system and the widely used 
measure of ?correctly read words per minute? 
(cwpm). 
The speech recognizer was trained both on external 
data (OGI and CMU kids? corpora) and internal 
data (text passages and word lists), yielding two 
different modes for text passages (trigram language 
model) and word lists (grammar language model). 
Automatically estimated cwpm measures agreed 
closely with human cwpm measures, achieving 0.8 
and higher correlation with Pearson and 0.7 and 
higher correlation with Spearman rank correlation 
measures. 
Future work includes an improved set-up for re-
cordings such as initial calibration and on-line 
sound quality monitoring, adding time stamps to 
recordings of word lists, adding more data for 
training/adaptation of the ASR system, and explor-
ing other features (such as fluency features) and 
their potential role in cwpm prediction. 
 
 
17
Acknowledgements 
The authors would like to acknowledge the contri-
butions of Kathy Sheehan, Tenaha O?Reilly and 
Kelly Bruce to this work. We further are grateful 
for the useful feedback and suggestions from our 
colleagues at ETS and the anonymous reviewers 
that greatly helped improve our paper. 
 
References 
Alwan, A. (2007). A System for Technology Based 
Assessment of Language and Literacy in Young 
Children: the Role of Multiple Information 
Sources. Proceedings of MMSP, Greece. 
Center for Spoken Language Understanding 
(CSLU), 2008. Kids? Speech Corpus, 
http://www.cslu.ogi.edu/corpora/kids/.LDC, BN. 
Deno, S. L., Fuchs, L. S., Marston, D., & Shin, J. 
(2001). Using curriculum-based measurements 
to establish growth standards for students with 
learning disabilities. School Psychology Re-
view, 30(4), 507-524. 
Deno, S. L. and D. Marsten (2006). Curriculum-
based measurement of oral reading: An indicator 
of growth in fluency. What Research Has to Say 
about Fluency Instruction. S. J. Samuels and A. 
E. Farstrup. Newark, DE, International Reading 
Association: 179-203. 
Hagen, A., B. Pellom, & R. Cole. (2007). "Highly 
accurate children?s speech recognition for inter-
active reading tutors using subword units." 
Speech Communication 49(6): 861-873. 
Lee, S., A. Potamianos, & S. Narayanan. (1999). 
"Acoustics of children's speech: developmental 
changes of temporal and spectral parameters." 
Journal of Acoustics Society of American 
(JASA) 105: 1455-1468. 
Li, X., Y. C. Ju, L. Deng & A. Acero. (2007). Effi-
cient and Robust Language Modeling in an 
Automatic Children's Reading Tutor System. 
Proc. IEEE International Conference on Acous-
tics, Speech and Signal Processing ICASSP 
2007. 
Li, Q. and M. Russell (2002). An analysis of the 
causes of increased error rates in children's 
speech recognition. ICSLP. Denver, CO. 
Linguistic Data Consortium (LDC), 1997. 1996 
English Broadcast News Speech (HUB4), 
LDC97S44. 
Linguistic Data Consortium (LDC), 1997. The 
CMU Kids Corpus, LDC97S63. 
Mostow, J., S. F. Roth, G. Hauptmann & M. Kane. 
(1994). A prototype reading coach that listens. 
AAAI '94: Proceedings of the twelfth national 
conference on Artificial intelligence, Menlo 
Park, CA, USA, American Association for Arti-
ficial Intelligence. 
National Center of Educational Statistics. (2006). 
National Assessment of Educational Progress. 
Washington DC: U.S. Government Printing Of-
fice. 
National Institute for Standards and Technology 
(NIST), 2008. Sclite software package. 
http://www.nist.gov/speech/tools/ 
Neumeyer, L., H. Franco, V. Digalakis & M. 
Weintraub. (2000). "Automatic Scoring of Pro-
nunciation Quality." Speech Communication 6. 
No Child Left Behind Act of 2001, Pub. L. No. 
107-110, 115 Stat. 1425 (2002). 
Tepperman, J., J. Silva, A. Kazemzadeh, H. You, 
S. Lee, A. Alwan & S. Narayanan. (2006). Pro-
nunciation verification of children's speech for 
automatic literacy assessment. INTERSPEECH-
2006. Pittsburg, PA. 
Tepperman, J., M. Black, P. Price, S. Lee, A. Ka-
zemzadeh, M. Gerosa, M. Heritage, A. Alwan & 
S. Narayanan.(2007). A bayesian network clas-
sifier for word-level reading assessment. Pro-
ceedings of ICSLP, Antwerp, Belgium. 
Wayman, M. M., Wallace, T., Wiley, H. I., Ticha, 
R., & Espin, C. A. (2007). Literature synthesis 
on curriculum-based measurement in reading. 
The Journal of Special Education, 41(2), 85-120. 
Witt, S. M. (1999). Use of Speech Recognition in 
Computer-assisted Language Learning, Univer-
sity of Cambridge. 
 
18
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 103?111,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Content Features for Automated Speech Scoring
Shasha Xie, Keelan Evanini, Klaus Zechner
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{sxie,kevanini,kzechner}@ets.org
Abstract
Most previous research on automated speech
scoring has focused on restricted, predictable
speech. For automated scoring of unrestricted
spontaneous speech, speech proficiency has
been evaluated primarily on aspects of pro-
nunciation, fluency, vocabulary and language
usage but not on aspects of content and topi-
cality. In this paper, we explore features repre-
senting the accuracy of the content of a spoken
response. Content features are generated us-
ing three similarity measures, including a lex-
ical matching method (Vector Space Model)
and two semantic similarity measures (Latent
Semantic Analysis and Pointwise Mutual In-
formation). All of the features exhibit moder-
ately high correlations with human proficiency
scores on human speech transcriptions. The
correlations decrease somewhat due to recog-
nition errors when evaluated on the output of
an automatic speech recognition system; how-
ever, the additional use of word confidence
scores can achieve correlations at a similar
level as for human transcriptions.
1 Introduction
Automated assessment of a non-native speaker?s
proficiency in a given language is an attractive ap-
plication of automatic speech recognition (ASR) and
natural language processing (NLP) technology; the
technology can be used by language learners for
individual practice and by assessment providers to
reduce the cost of human scoring. While much
research has been done about the scoring of re-
stricted speech, such as reading aloud or repeating
sentences verbatim (Cucchiarini et al, 1997; Bern-
stein et al, 2000; Cucchiarini et al, 2000; Witt and
Young, 2000; Franco et al, 2000; Bernstein et al,
2010b), much less has been done about the scor-
ing of spontaneous speech. For automated scor-
ing of unrestricted, spontaneous speech, most auto-
mated systems have estimated the non-native speak-
ers? speaking proficiency primarily based on low-
level speaking-related features, such as pronuncia-
tion, intonation, rhythm, rate of speech, and fluency
(Cucchiarini et al, 2002; Zechner et al, 2007; Chen
et al, 2009; Chen and Zechner, 2011a), although a
few recent studies have explored features based on
vocabulary and grammatical complexity (Zechner et
al., 2007; Bernstein et al, 2010a; Bernstein et al,
2010b; Chen and Zechner, 2011b).
To date, little work has been conducted on au-
tomatically assessing the relatively higher-level as-
pects of spontaneous speech, such as the content
and topicality, the structure, and the discourse in-
formation. Automated assessment of these aspects
of a non-native speaker?s speech is very challeng-
ing for a number of reasons, such as the short length
of typical responses (approximately 100 words for
a typical 1 minute response, compared to over 300
words in a typical essay/written response), the spon-
taneous nature of the speech, and the presence of
disfluencies and possible grammatical errors. More-
over, the assessment system needs text transcripts
of the speech to evaluate the high level aspects, and
these are normally obtained from ASR systems. The
recognition accuracy of state-of-the-art ASR sys-
tems on non-native spontaneous speech is still rel-
atively low, which will sequentially impact the re-
103
liability and accuracy of automatic scoring systems
using these noisy transcripts. However, despite these
difficulties, it is necessary for an automated assess-
ment system to address the high level information
of a spoken response in order to fully cover all as-
pects that are considered by human raters. Thus, in
this paper we focus on exploring features to repre-
sent the high-level aspect of speech mainly on the
accuracy of the content.
As a starting point, we consider approaches that
have been used for the automated assessment of con-
tent in essays. However, due to the qualitative dif-
ferences between written essays and spontaneous
speech, the techniques developed for written texts
may not perform as well on spoken responses. Still,
as a baseline, we will evaluate the content features
used for essay scoring on spontaneous speech. In
addition to a straightforward lexical Vector Space
Model (VSM), we investigate approaches using two
other similarity measures, Latent Semantic Analysis
(LSA) and Pointwise Mutual Information (PMI), in
order to represent the semantic-level proficiency of
a speaker. All of the content features are analyzed
using both human transcripts and speech recognizer
output, so we can have a better understanding of the
impact of ASR errors on the performance of the fea-
tures. As expected, the results show that the per-
formance on ASR output is lower than when hu-
man transcripts are used. Therefore, we propose im-
proved content features that take into account ASR
confidence scores to emphasize responses whose es-
timated word accuracy is comparatively higher than
others. These improved features can obtain similar
performance when compared to the results using hu-
man transcripts.
This paper is organized as follows. In the next
section we introduce previous research on auto-
mated assessment of content in essays and spoken
responses. The content features we generated and
the model we used to build the final speaking scores
are described in Sections 3 and Section 4, respec-
tively. In Section 5 we show the performance of
all our proposed features. Finally, we conclude our
work and discuss potential future work in Section 6.
2 Related Work
Most previous research on assessment of non-native
speech has focused on restricted, predictable speech;
see, for example, the collection of articles in (Es-
kenazi et al, 2009). When assessing spontaneous
speech, due to relatively high word error rates of
current state-of-the-art ASR systems, predominantly
features related to low-level information have been
used, such as features related to fluency, pronuncia-
tion or prosody (Zechner et al, 2009).
For scoring of written language (automated essay
scoring), on the other hand, several features related
to the high level aspects have been used previously,
such as the content and the discourse information.
In one approach, the lexical content of an essay was
evaluated by using a VSM to compare the words
contained in each essay to the words found in a sam-
ple of essays from each score category (Attali and
Burstein, 2006). In addition, this system also used
an organization feature measuring the difference be-
tween the ideal structure of an essay and the actual
discourse elements found in the essay. The features
designed for measuring the overall organization of
an essay assumed a writing strategy that included
an introductory paragraph, at least a three-paragraph
body with each paragraph in the body consisting of
a pair of main point, supporting idea elements, and
a concluding paragraph. In another approach, the
content of written essays were evaluated using LSA
by comparing the test essays with essays of known
quality in regard of their degree of conceptual rele-
vance and the amount of relevant content (Foltz et
al., 1999).
There has been less work measuring spoken re-
sponses in terms of the higher level aspects. In
(Zechner and Xi, 2008), the authors used a content
feature together with other features related to vocab-
ulary, pronunciation and fluency to build an auto-
mated scoring system for spontaneous high-entropy
responses. This content feature was the cosine word
vector product between a test response and the train-
ing responses which have the highest human score.
The experimental results showed that this feature
did not provide any further contribution above a
baseline of only using non-content features, and
for some tasks the system performance was even
slightly worse after including this feature. However,
104
we think the observations about the content features
used in this paper were not reliable for the following
two reasons: the number of training responses was
limited (1000 responses), and the ASR system had a
relatively high Word Error Rate (39%).
In this paper, we provide further analysis on the
performance of several types of content features.
Additionally, we used a larger amount of training
data and a better ASR system in an attempt to extract
more meaningful and accurate content features.
3 Automatic Content Scoring
In automatic essay scoring systems, the content of an
essay is typically evaluated by comparing the words
it contains to the words found in a sample of es-
says from each score category (1-4 in our experi-
ments), where the scores are assigned by trained hu-
man raters. The basic idea is that good essays will
resemble each other in their word choice, as will
poor essays. We follow this basic idea when extract-
ing content features for spoken responses.
3.1 Scoring Features
For each test spoken response, we calculate its simi-
larity scores to the sample responses from each score
category. These scores indicate the degree of simi-
larity between the words used in the test response
and the words used in responses from different score
points. Using these similarity scores, 3 content fea-
tures are generated in this paper:
? Simmax: the score point which has the high-
est similarity score between test response and
score vector
? Sim4: the similarity score to the responses
with the highest score category (4 in our ex-
periments).
? Simcmb: the linear combination of the similar-
ity scores to each score category.
4?
i=1
wi ? Simi (1)
where wi is scaled to [-1, 1] to imply its positive
or negative impact.
3.2 Similarity Measures
There are many ways to calculate the similarity be-
tween responses. A simple and commonly used
method is the Vector Space Model, which is also
used in automated essay scoring systems. Under this
approach, all the responses are converted to vectors,
whose elements are weighted using TF*IDF (term
frequency, inverse document frequency). Then, the
cosine similarity score between vectors can be used
to estimate the similarity between the responses the
vectors originally represent.
Other than this lexical matching method, we also
try two additional similarity measures to better cap-
ture the semantic level information: Latent Semantic
Analysis (Landauer et al, 1998) and a corpus-based
semantic similarity measure based on pointwise mu-
tual information (Mihalcea et al, 2006). LSA has
been widely used for computing document similar-
ity and other information retrieval tasks. Under this
approach, Singular Value Decomposition (SVD) is
used to analyze the statistical relationship between a
set of documents and the words they contain. A m?n
word-document matrix X is first built, in which each
element Xij represents the weighted term frequency
of word i in document j. The matrix is decomposed
into a product of three matrices as follows:
X = U?V T (2)
where U is an m?m matrix of left-singular vectors,
? is an m?n diagonal matrix of singular values, and
V is the n? n matrix of right-singular vectors.
The top ranked k singular values in ? are kept,
and the left is set to be 0. So ? is reformulated as ?k.
The original matrix X is recalculated accordingly,
Xk = U?kV
T (3)
This new matrix Xk can be considered as a
smoothed or compressed version of the original ma-
trix. LSA measures the similarity of two documents
by calculating the cosine between the corresponding
compressed column vectors.
PMI was introduced to calculate the semantic
similarity between words in (Turney, 2001). It is
based on the word co-occurrence on a large corpus.
Given two words, their PMI is computed using:
PMI(w1, w2) = log2
p(w1&w2)
p(w1) ? p(w2)
(4)
105
This indicates the statistical dependency between w1
and w2, and can be used as a measure of the semantic
similarity of two words.
Given the word-to-word similarity, we can calcu-
late the similarity between two documents using the
following function,
sim(D1, D2) =
1
2
(
?
w?{D1} (maxSim(w,D2) ? idf(w))?
w?{D1} idf(w)
+
?
w?{D2}(maxSim(w,D1) ? idf(w)?
w?{D2} idf(w))
)
(5)
maxSim(w,Di) = maxwj?{Di}PMI(w,wj)
(6)
For each word w in document D1, we find the word
in document D2 which has the highest similarity
to w. Similarly, for each word in D2, we iden-
tify the most similar words in D1. The similarity
score between two documents is then calculated by
combining the similarity of the words they contain,
weighted by their word specificity (i.e., IDF values).
In this paper, we use these three similarity mea-
sures to calculate the similarity between the test re-
sponse and the training responses for each score cat-
egory. Using the VSM method, we convert all the
training responses in one score category into one big
vector, and for a given test response we calculate its
cosine similarity to this vector as its similarity to that
corresponding score point vector. For the other simi-
larity measures, we calculate the test response?s sim-
ilarity to each of the training responses in one score
category, and report the average score as its similar-
ity to this score point. We also tried using this av-
erage similarity score for the VSM method, but our
experimental results showed that this average score
obtained lower performance than using one big vec-
tor generated from all the training samples due to
data sparsity. After the similarity scores to each of
the four score categories are computed, the content
features introduced in Section 3.1 are then extracted
and are used to evaluate the speaking proficiency of
the speaker.
4 System Architecture
This section describes the architecture of our auto-
mated content scoring system, which is shown in
Figure 1. First, the test taker?s voice is recorded,
and sent to the automatic speech recognition system.
Second, the feature computation module takes the
output hypotheses from the speech recognizer and
generates the content features. The last component
considers all the scoring features, and produces the
final score for each spoken response.
Featu
re?
Comp
utatio
n
Reco
gnize
d?Wo
rds?
Scori
ng?
Comp
utatio
n?
Mod
ule
and?U
ttera
nces
Featu
res
Spee
ch
Scori
ngM
odel
Spee
ch?
Reco
gnize
r
Scori
ng?M
odel
Audio
Files
Spea
king?S
cores
Figure 1: Architecture of the automated content scoring
system.
While we are using human transcripts of spoken
responses as a baseline in this paper, we want to note
that in an operational system as depicted in this fig-
ure, the scoring features are computed and extracted
using the hypotheses from the ASR system, which
exhibits a relatively high word error rate. These
recognition errors will sequentially impact the pro-
cess of calculating the similarity and computing the
content scores, and decrease the performance of the
final speaking scores. In order to improve the system
performance in this ASR condition, we explore the
use of word confidence scores from the ASR system
during feature generation. In particular, the similar-
ity scores between the test response and each score
category are weighted using the recognition confi-
dence score of the response, so that the scores can
also contain information related to its acoustic accu-
racy. The confidence score for one response is the
average value of all the confidence scores for each
word contained in the response. In Section 5, we
will evaluate the performance of our proposed con-
tent features using both human transcripts and ASR
outputs, as well as the enhanced content features us-
106
ing ASR confidence scores.
5 Experimental Results
5.1 Data
The data we use for our experiments are from the
Test of English as a Foreign Language R? internet-
based test (TOEFL iBT) in which test takers respond
to several stimuli using spontaneous speech. This
data set contains 24 topics, of which 8 are opinion-
based tasks, and 16 are contextual-based tasks. The
opinion-based tasks ask the test takers to provide
information or opinions on familiar topics based
on their personal experience or background knowl-
edge. The purpose of these tasks is to measure the
speaking ability of examinees independent of their
ability to read or listen to English language. The
contextual-based tasks engage reading, listening and
speaking skills in combination to mimic the kinds
of communication expected of students in campus-
based situations and in academic courses. Test tak-
ers read and/or listen to some stimulus materials and
then respond to a question based on them. For each
of the tasks, after task stimulus materials and/or test
questions are delivered, the examinees are allowed a
short time to consider their response and then pro-
vide their responses in a spontaneous manner within
either 45 seconds (for the opinion-based tasks) or 60
seconds (for the contextual-based tasks).
For each topic, we randomly select 1800 re-
sponses for training, and 200 responses as develop-
ment set for parameter tuning. Our evaluation data
contains 1500 responses from the same English pro-
ficiency test, which contain the same 24 topics. All
of these data are scored on a 0-4 scale by expert hu-
man raters. In our automated scoring system, we use
a filtering model to identify responses which should
have a score of 0, such as responses with a technical
difficulty (e.g., equipment problems, high ambient
noise), responses containing uncooperative behavior
from the speakers (e.g., non-English speech, whis-
pered speech). So in this paper we only focused on
the responses with scores of 1-4. Statistics for this
data set are shown in Table 1. As the table shows,
the score distributions are similar across the train-
ing, development, and evaluation data sets.
5.2 Speech recognizer
We use an ASR system containing a cross-word
triphone acoustic model trained on approximately
800 hours of spoken responses from the same En-
glish proficiency test mentioned above and a lan-
guage model trained on the corresponding tran-
scripts, which contain a total of over 5 million
words. The Word Error Rate (WER) of this system
on the evaluation data set is 33%.
5.3 Evaluation metric
To measure the quality of the developed features, we
employ a widely used metric, the Pearson correla-
tion coefficient (r). In our experiments, we use the
value of the Pearson correlation between the feature
values and the human proficiency scores for each
spoken response.
5.4 Feature performance on transcripts
In Section 3.1, we introduced three features derived
from the similarity between the test responses and
the training responses for each score point. We first
build the training samples for each topic, and then
compare the test responses with their corresponding
models. Three similarity measures are used for cal-
culating the similarity scores, VSM, LSA, and the
PMI-based method. In order to avoid the impact of
recognition errors, we first evaluate these similarity
methods and content features using the human tran-
scripts. The Pearson correlation coefficients on the
evaluation data set for this experiment are shown in
Table 2. The parameters used during model build-
ing, such as the weights for each score category in
the feature Simcmb and the number of topics k in
LSA, are all tuned on the development set, and ap-
plied directly on the evaluation set.
The correlations show that even the simple vec-
tor space model can obtain a good correlation of
0.48 with the human rater scores. The feature
Simcmb performs the best across almost all the test
setups, since it combines the information from all
score categories. The PMI-based features outper-
form the other two similarity methods when evalu-
ated both on all responses or only on the contextual-
based topics. We also observe that the correlations
on contextual-based tasks are much higher than on
opinion-based tasks. The reason for this is that
107
Table 1: Summary statistics of training, development and evaluation data set.
Data sets Responses Speakers score avg score sd
Score distribution (percentage %)
1 2 3 4
Train 43200 8000 2.63 0.79 1750 (4.1) 15128 (35.0) 20828 (48.2) 4837 (11.2)
Dev 4800 3760 2.61 0.79 215 (4.5) 1719 (35.8) 2295 (47.8) 499 (10.4)
Eval 1500 250 2.57 0.81 95 (6.3) 549 (36.6) 685 (45.7) 152 (10.1)
Table 2: Pearson correlations of the content features using human transcripts.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.46 0.32 0.48 0.32 0.38 0.45 0.18 0.51 0.53
Contextual 0.50 0.51 0.58 0.36 0.55 0.57 0.21 0.57 0.62
Opinion 0.37 0.03 0.25 0.29 0.14 0.22 0.06 0.42 0.51
the contextual-based tasks are more constrained to
the materials provided with the test item, whereas
the opinion-based tasks are relatively open-ended.
Therefore, it is easier for the similarity measures to
track the content, the topics, or the vocabulary usage
of the contextual-based topics. Overall, the best cor-
relations are obtained using the feature combining
the similarity scores to each score category and the
PMI-based methods to calculate the similarity. Here,
the Pearson correlations are 0.53 for all responses,
and 0.62 for the contextual-based tasks only.
We also investigated whether additional perfor-
mance gains could be achieved by combining infor-
mation from the three different content features to
build a single overall content score, since the three
features may measure disparate aspects of the re-
sponse. The combination model we use is mul-
tiple regression, in which the score assigned to a
test response is estimated as a weighted linear com-
bination of a selected set of features. The fea-
tures are the similarity values to each score category
(Simi, i ? {1, 2, 3, 4}), calcuated using the three
similairty measures. In total we have 12 content
features. The regression model is also built on the
development set, and tested on the evaluation set.
The correlation for the final model is 0.60 on all
responses, which is significantly better than the in-
dividual models (0.48 for VSM, 0.45 for LSA, and
0.53 for PMI). Compared to results reported in pre-
vious work on similar speech scoring tasks but mea-
suring other aspects of speech, our correlation re-
sults are very competitive (Zechner and Xi, 2008;
Zechner et al, 2009).
5.5 Feature Performance on ASR output
The results shown in the previous section were ob-
tained using human transcripts of test responses, and
were reported in order to demonstrate the meaning-
fulness of the proposed features. However, in prac-
tical automated speech scoring systems, the only
available text is the output of the ASR system, which
may contain a large number of recognition errors.
Therefore, in this section we show the performance
of the content features extracted using ASR hy-
potheses. Note that we still use the human tran-
scripts of the training samples to train the models,
the parameter values and the regression weights;
however, we only use ASR output of the evaluation
data for testing the feature performance. These cor-
relations are shown in Table 3.
Compared to the results in Table 2, we find that
the VSM and LSA methods are very robust to recog-
nition errors, and we only observe slight correlation
decreases on these features. However, the decrease
for the PMI-based method is quite large. A possi-
ble reason for this is that this method is based on
word-to-word similarity computed on the training
data, so the mismatch between training and evalu-
ation set likely has a great impact on the computa-
tion of the similarity scores, since we train on human
transcripts, but test using ASR hypotheses. Likely
for the same reason, the regression model combining
all the features does not provide any further contri-
bution to the correlation result (0.44 when evaluated
108
Table 3: Pearson correlations of the content features using ASR output.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.43 0.34 0.48 0.30 0.37 0.43 0.11 0.24 0.42
Contextual 0.49 0.53 0.58 0.34 0.54 0.57 0.16 0.31 0.53
Opinion 0.30 0.05 0.07 0.25 0.12 0.15 0.05 0.17 0.27
Table 4: Pearson correlations of the content features using ASR output with confidence scores.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.43 0.36 0.48 0.30 0.40 0.45 0.11 0.39 0.51
Contextual 0.49 0.55 0.58 0.34 0.57 0.59 0.16 0.46 0.59
Opinion 0.30 0.24 0.25 0.25 0.18 0.20 0.05 0.32 0.40
on all responses).
In Section 4, we proposed using ASR confidence
scores during feature extraction to introduce acous-
tic level information and, thus, penalize responses
for which the ASR output is less likely to be correct.
Under this approach, all similarity scores are mul-
tiplied by the average word confidence score con-
tained in the test response. The performance of these
enhanced features is provided in Table 4. Compared
to the scores in Table 3, the enhanced features per-
form better than the basic features that do not take
the confidence scores into consideration. Using this
approach, we can improve the correlation scores for
most of the features, especially for the PMI-based
features. These features had lower correlations be-
cause of the recognition errors, but with the con-
fidence scores, they outperform the other features
when evaluated both on all responses or only on
contextual-based responses. Note that the correla-
tions for feature Simmax remains the same because
the same average confidence scores for each test re-
sponse is multiplied by the similarity scores to each
of the score points, so the score point obtaining the
highest similarity score is the same whether the con-
fidence scores are considered or not. The correlation
of the regression model also improves from 0.44 to
0.51 when the confidence scores are included. Over-
all, the best correlations for the individual similarity
features with the confidence scores are very close to
those obtained using human transcripts, as shown in
Tables 2 and 4: the difference is 0.53 vs. 0.51 for
all responses, and 0.62 vs. 0.59 for contextual-based
tasks only.
Because all models and parameter values are
trained on human transcripts, this experimental
setup might not be optimal for using ASR outputs.
For instance, the regression model does not outper-
form the results of individual features using ASR
outputs, although the confidence scores help im-
prove the overall correlation scores. We expect that
we can obtain better performance by using a regres-
sion model trained on ASR transcripts, which can
better model the impact of noisy data on the features.
In our future work, we will build sample responses
for each score category, tune the parameter values,
and train the regression model all on ASR hypothe-
ses. We hope this can solve the mismatch problem
during training and evaluation, and can provide us
even better correlation results.
6 Conclusion and Future Work
Most previous work on automated scoring of spon-
taneous speech used features mainly related to low-
level information, such as fluency, pronunciation,
prosody, as well as a few features measuring aspects
such as vocabulary diversity and grammatical accu-
racy. In this paper, we focused on extracting con-
tent features to measure the speech proficiency in
relatively higher-level aspect of spontaneous speech.
Three features were computed to measure the sim-
ilarity between a test response and a set of sam-
ple responses representing different levels of speak-
ing proficiency. The similarity was calculated using
different methods, including the lexical matching
109
method VSM, and two corpus-based semantic simi-
larity measures, LSA and PMI. Our experimental re-
sults showed that all the features obtained good cor-
relations with human proficiency scores if there are
no recognition errors in the text transcripts, with the
PMI-based method performing the best over three
similarity measures. However, if we used ASR tran-
scripts, we observed a marked performance drop for
the PMI-based method. Although we found that
VSM and LSA were very robust to ASR errors, the
overall correlations for the ASR condition were not
as good as using human transcripts. To solve this
problem, we proposed to use ASR confidence scores
to improve the feature performance, and achieved
similar results as when using human transcripts.
As we discussed in Section 5, all models were
trained using human transcripts, which might de-
crease the performance when these models are ap-
plied directly to the ASR outputs. In our future
work, we will compare models trained on human
transcripts and on ASR outputs, and investigate
whether we should use matching data for training
and evaluation, or whether we should not introduce
noise during training in order to maintain the validity
of the models. We will also investigate whether the
content features can provide additional information
for automated speech scoring, and help build better
scoring systems when they are combined with other
non-content features, such as the features represent-
ing fluency, pronunciation, prosody, vocabulary di-
versity information. We will also explore generating
other features measuring the higher-level aspects of
the spoken responses. For example, we can extract
features assessing the responses? relatedness to the
stimulus of an opinion-based task. For contextual-
based tasks, the test takers are asked to read or lis-
ten to some stimulus material, and answer a ques-
tion based on this information. We can build models
using these materials to check the correctness and
relatedness of the spoken responses.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R? v.2. The Journal of Technology,
Learning, and Assessment, 4(3):3?30.
Jared Bernstein, John De Jong, David Pisoni, and Brent
Townshend. 2000. Two experiments on automatic
scoring of spoken language proficiency. In Proceed-
ings of Integrating Speech Tech. in Learning (InSTIL).
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as predic-
tors of L2 oral proficiency. In Proceedings of Inter-
speech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Lei Chen and Klaus Zechner. 2011a. Applying rhythm
features to automatically assess non-native speech. In
Proceedings of Interspeech.
Miao Chen and Klaus Zechner. 2011b. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of ACL-HLT.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven as-
sessment of non-native spontaneous speech. In Pro-
ceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by using
speech recognition technology. In IEEE Workshop on
Auotmatic Speech Recognition and Understanding.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency by means of automatic speech recognition
technology. Journal of the Acoustical Society of Amer-
ica, 107:989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: comparisons between read and spontaneous
speech. Journal of the Acoustical Society of America,
111(6):2862?2873.
Maxine Eskenazi, Abeer Alwan, and Helmer Strik. 2009.
Spoken language technology for education. Speech
Communication, 51(10):831?1038.
Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.
1999. The Intelligent Essay Assessor: Applications to
educational technology. Interactive multimedia Elec-
tronic Journal of Computer-Enhanced Learning, 1(2).
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of ma-
chine scores for automatic grading of pronunciation
quality. Speech Communication, 30(1-2):121?130.
Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:259?284.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the American Association for Artificial Intelligence,
September.
110
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
ECML.
Silke M. Witt and Steve J. Young. 2000. Phone-
level pronunciation scoring and assessment for interac-
tive language learning. Speech Communication, 30(1-
2):95?108.
Klaus Zechner and Xiaoming Xi. 2008. Towards auto-
matic scoring of a test of spoken language with het-
erogeneous task types. In Proceedings of the Third
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications.
Klaus Zechner, Derrick Higgins, and Xiaoming Xi.
2007. SpeechraterTM: A construct-driven approach
to score spontaneous non-native speech. In Proceed-
ings of the 2007 Workshop of the International Speech
Communication Association (ISCA) Special Interest
Group on Speech and Language Technology in Edu-
cation (SLaTE).
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
111
Proceedings of NAACL-HLT 2013, pages 814?819,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Coherence Modeling for the Automated Assessment of  
Spontaneous Spoken Responses 
 
Xinhao Wang, Keelan Evanini, Klaus Zechner 
Educational Testing Service 
660 Rosedale Road 
Princeton, NJ 08541, USA 
xwang002,kevanini,kzechner@ets.org 
 
 
 
 
Abstract 
This study focuses on modeling discourse co-
herence in the context of automated assess-
ment of spontaneous speech from non-native 
speakers. Discourse coherence has always 
been used as a key metric in human scoring 
rubrics for various assessments of spoken lan-
guage. However, very little research has been 
done to assess a speaker's coherence in auto-
mated speech scoring systems. To address 
this, we present a corpus of spoken responses 
that has been annotated for discourse coher-
ence quality. Then, we investigate the use of 
several features originally developed for es-
says to model coherence in spoken responses. 
An analysis on the annotated corpus shows 
that the prediction accuracy for human holistic 
scores of an automated speech scoring system 
can be improved by around 10% relative after 
the addition of the coherence features.  Fur-
ther experiments indicate that a weighted F-
Measure of 73% can be achieved for the au-
tomated prediction of the coherence scores. 
1 Introduction 
In recent years, much research has been conducted 
into developing automated assessment systems to 
automatically score spontaneous speech from non-
native speakers with the goals of reducing the bur-
den on human raters, improving reliability, and 
generating feedback that can be used by language 
learners. Various features related to different as-
pects of speaking proficiency have been exploited, 
such as delivery features for pronunciation, proso-
dy, and fluency (Strik and Cucchiarini, 1999; Chen 
et al, 2009; Cheng, 2011; Higgins et al, 2011), as 
well as language use features for vocabulary and 
grammar, and content features (Chen and Zechner, 
2011; Xie et al, 2012). However, discourse-level 
features related to topic development have rarely 
been investigated in the context of automated 
speech scoring. This is despite the fact that an im-
portant criterion in the human scoring rubrics for 
speaking assessments is the evaluation of coher-
ence, which refers to the conceptual relations be-
tween different units within a response. 
Methods for automatically assessing discourse 
coherence in text documents have been widely 
studied in the context of applications such as natu-
ral language generation, document summarization, 
and assessment of text readability. For example, 
Foltz et al (1998) measured the overall coherence 
of a text by utilizing Latent Semantic Analysis 
(LSA) to calculate the semantic relatedness be-
tween adjacent sentences. Barzilay and Lee (2004) 
introduced an HMM-based model for the docu-
ment-level analysis of topics and topic transitions. 
Barzilay and Lapata (2005; 2008) presented an 
approach to coherence modeling which focused on 
the entities in the text and their grammatical transi-
tions between adjacent sentences, and calculated 
the entity transition probabilities on the document 
level. Pitler et al (2010) provided a summary of 
the performance of several different types of 
features for automated coherence evaluation, such 
as cohesive devices, adjacent sentence similarity, 
Coh-Metrix (Graesser et al, 2004), word co-
occurrence patterns, and entity-grid. 
In addition to studies on well-formed text, re-
searchers have also addressed coherence modeling 
on text produced by language learners, which may 
contain many spelling and grammar errors.  
Utilizing LSA and Random Indexing methods, 
Higgins et al (2004) measured the global 
814
coherence of students? essays by calculating the 
semantic relatedness between sentences and the 
corresponding prompts. In addition, Burstein et. al 
(2010) combined entity-grid features with writing 
quality features produced by an automated assess-
ment system of essays to predict the coherence 
scores of student essays. Recently, Yannakoudakis 
and Briscoe (2012) systematically analyzed a vari-
ety of coherence modeling methods within the 
framework of an automated assessment system for 
non-native free text responses and indicated that 
features based on Incremental Semantic Analysis 
(ISA), local histograms of words, the part-of-
speech IBM model, and word length were the most 
effective.   
In contrast to these previous studies involving 
well-formed text or learner text containing errors, 
this paper focuses on modeling coherence in spon-
taneous spoken responses as well as investigating 
discourse features in an attempt to extend the con-
struct coverage of an automated speech scoring 
system. In a related study, Hassanali et al (2012) 
investigated coherence modeling for spoken lan-
guage in the context of a story retelling task for the 
automated diagnosis of children with language im-
pairment. They annotated transcriptions of chil-
dren's narratives with coherence scores as well as 
markers of narrative structure and narrative quali-
ty; furthermore they built models to predict the 
coherence scores based on Coh-Metrix features 
and the manually annotated narrative features. The 
current study differs from this one in that it deals 
with free spontaneous spoken responses provided 
by students at a university level; these responses 
therefore contain more varied and more complicat-
ed information than the child narratives. 
The main contributions of this paper can be 
summarized as follows: First, we obtained coher-
ence annotations on a corpus of spontaneous spo-
ken responses drawn from a university-level 
English language proficiency assessment, and 
demonstrated an improvement of around 10% rela-
tive in the accuracy of the automated prediction of 
human holistic scores with the addition of the co-
herence annotations. Second, we applied the entity-
grid features and writing quality features from an 
automated essay scoring system to predict the co-
herence scores; the experimental results have 
shown promising correlations between some of 
these features and the coherence scores.  
2 Data and Annotation 
2.1 Data 
For this study, we collected 600 spoken responses 
from the international TOEFL? iBT assessment of 
English proficiency for non-native speakers. 100 
responses were drawn from each of 6 different test 
questions comprising two different speaking tasks: 
1) providing an opinion based on personal experi-
ence (N = 200) and 2) summarizing or discussing 
material provided in a reading and/or listening pas-
sage (N = 400). The spoken responses were all 
transcribed by humans with punctuation and capi-
talization. The average number of words contained 
in the responses was 104.4 (st. dev. = 34.4) and the 
average number of sentences was 5.5 (st. dev. = 
2.1).  
The spoken responses were all provided with 
holistic English proficiency scores on a scale of 1 - 
4 by expert human raters in the context of opera-
tional, high-stakes scoring for the spoken language 
assessment. The scoring rubrics address the fol-
lowing three main aspects of speaking proficiency: 
delivery (pronunciation, fluency, prosody), lan-
guage use (grammar and lexical choice), and topic 
development (content and coherence). In order to 
ensure a sufficient quantity of responses from each 
proficiency level for training and evaluating the 
coherence prediction features, the spoken respons-
es selected for this study were balanced based on 
the human scores as follows: 25 responses were 
selected randomly from each of the 4 score points 
(1 - 4) for each of the 6 test questions. In some 
cases, more than one response was selected from a 
given test-taker; in total, 471 distinct test-takers are 
represented in the data set. 
2.2 Annotation and Analysis 
The coherence annotation guidelines used for the 
spoken responses in this study were modified 
based on the annotation guidelines developed for 
written essays described in Burstein et al (2010). 
According to these guidelines, expert annotators 
provided each response with a score on a scale of 1 
- 3. The three score points were defined as follows: 
3 = highly coherent (contains no instances of con-
fusing arguments or examples), 2 = somewhat co-
herent (contains some awkward points in which the 
speaker's line of argument is unclear), 1 = barely 
815
coherent (the entire response was confusing and 
hard to follow; it was intuitively incoherent as a 
whole and the annotators had difficulties in identi-
fying specific weak points). For responses receiv-
ing a coherence score of 2, the annotators were 
required to highlight the specific awkward points 
in the response. In addition, the annotators were 
specifically required to ignore disfluencies and 
grammatical errors as much as possible; thus, they 
were instructed to not label sentences or clauses as 
awkward points solely because of the presence of 
disfluent or ungrammatical speech.  
Two annotators (not drawn from the pool of ex-
pert human raters who provided the holistic scores) 
made independent coherence annotations for all 
600 spoken responses. The distribution of annota-
tions across the three score points is presented in 
Table 1. The two annotators achieved a moderate 
inter-annotator agreement (Landis and Koch, 1977) 
of ? = 0.68 on the 3-point scale. The average of the 
two coherence scores provided by the two annota-
tors correlates with the holistic speaking proficien-
cy scores at r = 0.66, indicating that the overall 
proficiency scores of spoken responses can benefit 
from the discourse coherence annotations. 
 
 1 2 3 
# 1 160 (27%) 278 (46%) 162 (27%) 
# 2 125 (21%) 251 (42%) 224 (37%) 
Table 1. Distribution of coherence annotations from two 
annotators 
 
Furthermore, coherence features based on the 
human annotations were examined within the con-
text of an automated spoken language assessment 
system, SpeechRaterSM (Zechner et al, 2007; 
2009). We extracted 96 features related to pronun-
ciation, prosody, fluency, language use, and con-
tent development using SpeechRater. These 
features were either extracted directly from the 
speech signal or were based on the output of an 
automatic speech recognition system (with a word 
error rate of around 28%1
                                                          
1 Both the training and evaluation sets used to develop the 
speech recognizer consist of similar spoken responses drawn 
from the same assessment. However, there is no response 
overlap between these sets and the corpus used for discourse 
coherence annotation in this study. 
). By utilizing a decision 
tree classifier (the J48 implementation from Weka 
(Hall et al, 2009)), 4-fold cross validation was 
conducted on the 600 responses to train and evalu-
ate a scoring model for predicting the holistic pro-
ficiency scores. The resulting correlation between 
the predicted scores (based on the 96 baseline 
SpeechRater features) and the human holistic pro-
ficiency scores was r = 0.667.  
In order to model a spoken response's coher-
ence, three different features were extracted from 
the human annotations. Firstly, the average of the 
two annotators? coherence scores was directly used 
as a feature with a 5-point scale (henceforth 
Coh_5). Secondly, following the work in Burstein 
et al (2010), we collapsed the average coherence 
scores into a 2-point scale to deal with the 
difficulty in distinguishing somewhat and highly 
coherent responses. For this second feature 
(henceforth Coh_2), scores 1 and 1.5 were mapped 
to score 1, and scores 2, 2.5, and 3 were mapped to 
score 2. Finally, the number of awkward points 
was also counted as a feature (henceforth Awk). 
As shown in Table 2, when these three coherence 
features were combined separately with the 
SpeechRater features, the correlations could be 
improved from r = 0.667 to r > 0.7. Meanwhile, 
the accuracy (i.e., the percentage of correctly pre-
dicted holistic scores) could be improved from 
0.487 to a range between 0.535 and 0.543.  
 
Features r Accuracy 
SpeechRater 0.667 0.487 
SpeechRater+Coh_5 0.714 0.540 
SpeechRater+Coh_2 0.705 0.543 
SpeechRater+Awk 0.702 0.535 
SpeechRater+Coh_5+Awk 0.703 0.537 
SpeechRater+Coh_2+Awk 0.701 0.542 
Table 2. Improvement to an automated speech scoring 
system after the addition of human-assigned coherence 
scores and measures, showing both Pearson r correla-
tions and the ratio of correctly matched holistic scores 
between the system and human experts 
 
These experimental results demonstrate that the 
automatic scoring system can benefit from coher-
ence modeling either by directly using a human-
assigned coherence score or the identified awk-
ward points. However, the use of both kinds of 
annotations does not provide further improvement. 
When collapsing the average scores into a 2-point 
scale, there was a 0.009 correlation drop (not sta-
tistically significant), but the accuracy was slightly 
improved. In addition, due to the relatively small 
816
size of the set of available coherence annotations, 
we adopted the collapsed 2-point scale instead of 
the 5-point scale for the coherence prediction ex-
periments in the next section.  
2.3 Experimental Design 
As demonstrated in Section 2.2, the collapsed av-
erage coherence score can be used to improve the 
performance of an automated speech scoring sys-
tem. Therefore, this study treats coherence predic-
tion as a binary classification task: low-coherent 
vs. high-coherent, where the low-coherent re-
sponses are those with average scores 1 and 1.5, 
and the high-coherent responses are those with av-
erage scores 2, 2.5, and 3.  
For coherence modeling, we again use the J48 
decision tree from the Weka machine learning 
toolkit (Hall et al, 2009) and run 4-fold cross-
validation on the 600 annotated responses. The 
correlation coefficient (r) and the weighted aver-
age F-Measure2
In this experiment, we examine the performance 
of the entity-grid features and a set of features pro-
duced by the e-rater? system (an automated writ-
ing assessment system for learner essays) (Attali 
and Burstein, 2006) to predict the coherence scores 
of the spontaneous spoken responses, where all the 
features are extracted from human transcriptions of 
the responses.  
 are used as evaluation metrics.  
2.4 Entity Grid and e-rater Features 
First, we applied the algorithm from Barzilay and 
Lapata (2008) to extract entity-grid features, which 
calculated the vector of entity transition probabili-
ties across adjacent sentences.  Several different 
methods of representing the entities can be used 
before generating the entity-grid. First, all the enti-
ties can be described by their syntactic roles in-
cluding S (Subject), O (Object), and X (Other). 
Alternatively, these roles can also be reduced to P 
(Present) or N (Absent). Furthermore, entities can 
be defined as salient, when they appear two or 
more times, otherwise as non-salient. In this study, 
                                                          
2 The data distribution in the experimental corpus is unbal-
anced:  71% of the responses are high-coherent and 29% are 
low-coherent. Therefore, we adopt the weighted average F-
Measure to evaluate the performance of coherence prediction: 
first, the F1-Measure of each category is calculated, and then 
the percentages of responses in each category are used as 
weights to obtain the final weighted average F-Measure. 
we generated there basic entity grids: EG_SOX 
(entity grid with the syntactic roles S, O, and X), 
EG_REDUCED (entity grid with the reduced rep-
resentations P and N), and EG_SALIENT (entity 
grid with salient and non-salient entities). In addi-
tion to these entity-grid features, we also used 130 
writing quality features related to grammar, usage, 
mechanics, and style from e-rater to model the co-
herence. 
A baseline system for this task would simply as-
sign the majority class (high-coherent) to all of the 
responses; this baseline achieves an F-Measure of 
0.587. Table 3 shows that the EG_REDUCED and 
e-rater features can obtain F-Measures of 0.677 
and 0.726 as well as correlations with human 
scores of 0.20 and 0.33, respectively. However, the 
combination of the two sets of features only brings 
a very small improvement (from 0.33 to 0.34). In 
addition, our experiments show that by introducing 
the component of co-reference resolution for entity 
grid building, we can only get a very slight im-
provement on EG_SALIENT, but no improvement 
on EG_SOX and EG_REDUCED. That may be 
because it is generally more difficult to parse the 
transcriptions of spoken language than well-
formed text, and more errors are introduced during 
the process of co-reference resolution. 
 
 r F-Measure 
Baseline 0.0 0.587 
EG_SOX 0.16 0.664 
EG_REDUCED 0.2 0.677 
EG_SALIENT 0.2 0.678 
e-rater 0.33 0.726 
EG_SOX +e-rater 0.30 0.714 
EG_REDUCED +e-rater 0.34 0.73 
EG_SALIENT + e-rater 0.26 0.695 
Table 3. Performance of entity grid and e-rater features 
on the coherence modeling task  
2.5 Discussion and Future Work  
In order to further analyze these features, the  cor-
relation coefficients between various features and 
the average coherence scores (on a five-point 
scale) were calculated; Figure 1 shows the histo-
gram of these correlation values. As the figure 
shows, there are a total of approximately 50 fea-
tures with correlations larger than 0.1. Four of the 
entity-grid features have correlations between 0.15 
and 0.29. As for the writing quality features, some 
817
of them show high correlations with the average 
coherence scores, despite the fact that they are not 
explicitly related to discourse coherence, such as 
the number of good lexical collocations.  
Based on the above analysis, we plan to investi-
gate additional superficial features explicitly relat-
ed to discourse coherence, such as the distribution 
of conjunctions, pronouns, and discourse connec-
tives. Moreover, based on the research on well-
formed texts and learner essays, we will attempt to 
examine more effective features and models to bet-
ter cover the discourse aspects of spontaneous 
speech. For example, local semantic features relat-
ed to inter-sentential coherence and the ISA feature 
will be investigated on spoken responses. In addi-
tion, we will apply the features and build coher-
ence models using the output of automatic speech 
recognition in addition to human transcriptions. 
Finally, various coherence features or models will 
be integrated into a practical automated scoring 
system, and further experiments will be performed 
to measure their effect on the performance of au-
tomated assessment of spontaneous spoken re-
sponses.  
 
 
Figure1. Histogram of entity-grid and writing quality 
features based on their correlations with coherence 
scores 
 
3 Conclusion  
In this paper, we present a corpus of coherence 
annotations for spontaneous spoken responses pro-
vided in the context of an English speaking profi-
ciency assessment. Entity-grid features and fea-
tures from an automated essay scoring system were 
examined for coherence modeling of spoken re-
sponses. The analysis on the annotated corpus 
showed promising results for improving the per-
formance of an automated scoring system by 
means of modeling the coherence of spoken re-
sponses.  
Acknowledgments 
The authors wish to express our thanks to the dis-
course annotators Melissa Lopez and Matt Mulhol-
land for their dedicated work and our colleagues 
Jill Burstein and Slava Andreyev for their support 
in generating entity-grid features. 
References   
Yigal Attali and Jill Burstein. 2006. Automated essay 
scoring with e-rater? V.2.0. Journal of Technology, 
Learning, and Assessment. 4(3): 159-174. 
Regina Barzilay and Lillian Lee. 2004. Catching the 
drift: Probabilistic content models, with applications 
to generation and summarization. Proceedings of 
NAACL-HLT, 113-120. 
Regina Barzilay and Mirella Lapata. 2005. Modeling 
local coherence: An entity-based approach. 
Proceedings of ACL, 141-148. 
Regina Barzilay and Mirella Lapata. 2008. Modeling 
local coherence: An entity-based approach. 
Computational Linguistics, 34(1):1-34. 
Jill Burstein, Joel Tetreault and Slava Andreyev. 2010. 
Using entity-based features to model coherence in 
student essays. Proceedings of NAACL-HLT, 681-
684. Los Angeles, California. 
Lei Chen, Klaus Zechner and Xiaoming Xi. 2009. 
Improved pronunciation features for construct-
driven assessment of non-native spontaneous 
speech. Proceedings of NAACL-HLT, 442-449. 
Miao Chen and Klaus Zechner. 2011. Computing and 
evaluating syntactic complexity features for 
automated scoring of spontaneous non-native 
speech. Proceedings of ACL, 722-731. 
Jian Cheng. 2011. Automatic assessment of prosody in 
high-stakes English tests. Proceedings of 
Interspeech , 27-31. 
Peter W. Foltz, Walter Kintsch and Thomas K. 
Landauer. 1998. The measurement of textual 
coherence with Latent Semantic Analysis. Discourse 
Processes, 25(2&3):285-307. 
Arthur C. Graesser,  Danielle S. McNamara,  Max M. 
Louwerse and Zhiqiang Cai. 2004. Coh-Metrix: 
Analysis of text on cohesion and language. Behavior 
818
Research Methods, Instruments, & Computers, 
36(2):193-202. 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann and Ian H. Witten. 
2009. The WEKA data mining software: An update. 
SIGKDD Explorations, 11(1):10-18. 
Khairun-nisa Hassanali, Yang Liu and Thamar Solorio. 
2012. Coherence in child language narratives: A 
case study of annotation and automatic prediction of 
coherence. Proceedings of the Interspeech 
Workshop on Child, Computer and Interaction.  
Derrick Higgins, Jill Burstein, Daniel Marcu and 
Claudia Gentile. 2004. Evaluating multiple aspects 
of coherence in student essays. Proceedings of 
NAACL-HLT, 185-192. 
Derrick Higgins, Xiaoming Xi, Klaus Zechner and 
David Williamson.  2011. A three-stage approach to 
the automated scoring of spontaneous. Computer 
Speech and Language, 25:282-306. 
J. Richard Landis and Gary G. Koch. 1977. The 
measurement of observer agreement for categorical 
data. Biometrics, 33(1):159-174. 
Emily Pitler, Annie Louis and Ani Nenkova. 2010. 
Automatic evaluation of linguistic quality in multi-
document summarization. Proceedings of ACL. 
544?554. Uppsala. 
Helmer Strik and Catia Cucchiarini. 1999. Automatic 
assessment of second language learners' fluency. 
Proceedings of the 14th International Congress of 
Phonetic Sciences, 759-762. Berkeley, CA. 
Shasha Xie, Keelan Evanini and Klaus Zechner. 2012. 
Exploring content features for automated speech 
scoring. Proceedings of NAACL-HLT, 103-111. 
Helen Yannakoudakis and Ted Briscoe. 2012. Modeling 
coherence in ESOL learner texts. Proceedings of the 
7th Workshop on the Innovative Use of NLP for 
Building Educational Applications, 33-43. Montreal. 
Klaus Zechner, Derrick Higgins and Xiaoming Xi. 
2007.   SpeechRaterSM
Klaus Zechner, Derrick Higgins, Xiaoming Xi and 
David M. Williamson. 2009. Automatic scoring of 
non-native spontaneous speech in tests of spoken 
English. Speech Communication, 51(10):883-895. 
: A construct-driven approach 
to scoring spontaneous non-native speech. 
Proceedings of the International Speech 
Communication Association Special Interest Group 
on Speech and Language Technology in Education, 
128-131. 
 
819
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722?731,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
    Computing and Evaluating Syntactic Complexity Features for  
              Automated Scoring of Spontaneous Non-Native Speech  
    
 
Miao Chen Klaus Zechner 
School of Information Studies NLP & Speech Group 
Syracuse University Educational Testing Service 
Syracuse, NY, USA Princeton, NJ, USA 
mchen14@syr.edu kzechner@ets.org 
 
 
 
 
Abstract 
This paper focuses on identifying, extracting 
and evaluating features related to syntactic 
complexity of spontaneous spoken responses as 
part of an effort to expand the current feature 
set of an automated speech scoring system in 
order to cover additional aspects considered 
important in the construct of communicative 
competence. 
Our goal is to find effective features, se-
lected from a large set of features proposed 
previously and some new features designed in 
analogous ways from a syntactic complexity 
perspective that correlate well with human rat-
ings of the same spoken responses, and to build 
automatic scoring models based on the most 
promising features by using machine learning 
methods. 
On human transcriptions with manually 
annotated clause and sentence boundaries, our 
best scoring model achieves an overall Pearson 
correlation with human rater scores of r=0.49 
on an unseen test set, whereas correlations of 
models using sentence or clause boundaries 
from automated classifiers are around r=0.2. 
1 Introduction 
Past efforts directed at automated scoring of 
speech have used mainly features related to fluen 
cy (e.g., speaking rate, length and distribution of 
pauses), pronunciation (e.g., using log-likelihood 
scores from the acoustic model of an Automatic 
Speech Recognition (ASR) system), or prosody 
(e.g., information related to pitch  contours or syl-
lable stress)  (e.g., Bernstein, 1999; Bernstein et 
al., 2000; Bernstein et al, 2010; Cucchiarini et al, 
1997; Cucchiarini et al, 2000; Franco et al, 2000a; 
Franco et al, 2000b; Zechner et al, 2007, Zechner 
et al, 2009). 
While this approach is a good match to most of 
the important properties related to low entropy 
speech (i.e., speech which is highly predictable), 
such as reading a passage aloud, it lacks many im-
portant aspects of spontaneous speech which are 
relevant to be evaluated both by a human rater and 
an automated scoring system. Examples of such 
aspects of speech, which are considered part of the 
construct1 of ?communicative competence (Bach-
man, 1990), include grammatical accuracy, syntac-
tic complexity, vocabulary diversity, and aspects of 
spoken discourse structure, e.g., coherence and 
cohesion. These different aspects of speaking pro-
ficiency are often highly correlated in a non-native 
speaker (Xi and Mollaun, 2006; Bernstein et al, 
2010), and so scoring models built solely on fea-
tures of fluency and pronunciation may achieve 
reasonably high correlations with holistic human 
rater scores. However, it is important to point out 
that such systems would still be unable to assess 
many important aspects of the speaking construct 
and therefore cannot be seen as ideal from a validi-
ty point of view.2
The purpose of this paper is to address one of 
these important aspects of spoken language in 
more detail, namely syntactic complexity. This 
paper can be seen as a first step toward including 
 
                                                          
1  A construct is a set of knowledge, skills, and abilities 
measured by a test. 
2 ?Construct validity? refers to the extent that a test measures 
what it is designed to measure, in this case, communicative 
competence via speaking. 
722
features related to this part of the speaking con-
struct into an already existing automated speech 
scoring system for spontaneous speech which so 
far mostly uses features related to fluency and pro-
nunciation (Zechner et al, 2009). 
We use data from the speaking section of the 
TOEFL? Practice Online (TPO) test, which is a 
low stakes practice test for non-native speakers 
where they are asked to provide six spontaneous 
speech samples of about one minute in length each 
in response to a variety of prompts. Some prompts 
may be simple questions, and others may involve 
reading or listening to passages first and then ans-
wering related questions. All responses were 
scored holistically by human raters according to 
pre-defined scoring rubrics (i.e., specific scoring 
guidelines) on a scale of 1 to 4, 4 being the highest 
proficiency level. 
In our automated scoring system, the first com-
ponent is an ASR system that decodes the digitized 
speech sample, generating a time-annotated hypo-
thesis for every response. Next, fluency and pro-
nunciation features are computed based on the 
ASR output hypotheses, and finally a multiple re-
gression scoring model, trained on human rater 
scores, computes the score for a given spoken re-
sponse (see Zechner et al (2009) for more details). 
We conducted the study in three steps: (1) finding 
important measures of syntactic complexity from 
second language acquisition (SLA) and English 
language learning (ELL) literature, and extending 
this feature set based on our observations of the 
TPO data in analogous ways; (2) computing fea-
tures based on transcribed speech responses and 
selecting features with highest correlations to hu-
man rater scores, also considering their compara-
tive values for native speakers taking the same test;  
and (3) building scoring models for the selected 
sub-set of the features to generate a proficiency 
score for each speaker, using all six responses of 
that speaker. 
In the remainder of the paper, we will address 
related work in syntactic complexity (Section 2), 
introduce the speech data sets of our study (Section 
3), describe the methods we used for feature ex-
traction (Section 4), provide the experiment design 
and results (Section 5), analyze and discuss the 
results in Section 6, before concluding the paper 
(Section 7). 
2 Related Work 
2.1 Literature on Syntactic Complexity 
Syntactic complexity is defined as ?the range of 
forms that surface in language production and the 
degree of sophistication of such forms? (Ortega, 
2003). It is an important factor in the second lan-
guage assessment construct as described  in Bach-
man?s (1990) conceptual model of language 
ability, and therefore is often used as an index of 
language proficiency and development status of L2 
learners. Various studies have proposed and inves-
tigated measures of syntactic complexity as well as 
examined its predictiveness for language profi-
ciency, in both L2 writing and speaking settings, 
which will be reviewed respectively. 
Writing 
Wolfe-Quintero et al (1998) reviewed a number of 
grammatical complexity measures in L2 writing 
from thirty-nine studies, and their usage for pre-
dicting language proficiency was discussed. Some 
examples of syntactic complexity measures are: 
mean number of clauses per T-unit3
                                                          
3 T-units are defined as ?shortest grammatically allowable 
sentences into which (writing can be split) or minimally 
terminable units? (Hunt, 1965:20). 
, mean length 
of clauses, mean number of verbs per sentence, etc. 
The various measures can be grouped into two cat-
egories: (1) clauses, sentences, and T-units in 
terms of each other; and (2) specific grammatical 
structures (e.g., passives, nominals) in relation to 
clauses, sentences, or T-units (Wolfe-Quintero et 
al., 1998). Three primary methods of calculating 
syntactic complexity measures are frequency, ratio, 
and index, where frequency is the count of occur-
rences of a specific grammatical structure, ratio is 
the number of one type of unit divided by the total 
number of another unit, and index is computing 
numeric scores by specific formulae (Wolfe-
Quintero et al, 1998). For example, the measure 
?mean number of clauses per T-unit? is obtained 
by using the ratio calculation method and the 
clause and T-unit grammatical structures. Some 
structures such as clauses and T-units only need 
shallow linguistic processing to acquire, while 
some require parsing. There are numerous combi-
nations for measures and we need empirical evi-
723
dence to select measures with the highest perfor-
mance. 
There have been a series of empirical studies 
examining the relationship of syntactic complexity 
measures to L2 proficiency using real-world data 
(Cooper, 1976; Larsen-Freeman, 1978; Perkins, 
1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; 
Lu, 2010). The studies investigate measures that 
highly correlate with proficiency levels or distin-
guish between different proficiency levels. Many 
T-unit related measures were identified as statisti-
cally significant indicators to L2 proficiency, such 
as mean length of T-unit (Henry, 1996; Lu, 2010), 
mean number of clauses per T-unit (Cooper, 1976; 
Lu, 2010), mean number of complex nominals per 
T-unit (Lu, 2010), or the mean number of error-
free T-units per sentence (Ho-Peng, 1983). Other 
significant measures are mean length of clause (Lu, 
2010), or frequency of passives in composition 
(Kameen, 1979).   
Speaking 
Syntactic complexity analysis in speech mainly 
inherits measures from the writing domain, and the 
abovementioned measures can be employed in the 
same way on speech transcripts for complexity 
computation. A series of studies have examined 
relations between the syntactic complexity of 
speech and the speakers? holistic speaking profi-
ciency levels (Halleck, 1995; Bernstein et al, 
2010; Iwashita, 2006). Three objective measures of 
syntactic complexity, including mean T-unit 
length, mean error-free T-unit length, and percent 
of error-free T-units were found to correlate with 
holistic evaluations of speakers in Halleck (1995). 
Iwashita?s (2006) study on Japanese L2 speakers 
found that length-based complexity features (i.e., 
number of T-units and number of clauses per T-
unit) are good predictors for oral proficiency. In 
studies directly employing syntactic complexity 
measures in other contexts, ratio-based measures 
are frequently used. Examples are mean length of 
utterance (Condouris et al, 2003), word count or 
tree depth (Roll et al, 2007), or mean length of T-
units and mean number of clauses per T-unit 
(Bernstein et al, 2010). Frequency-based measures 
were used less, such as number of full phrases in 
Roll et al (2007). 
The speaking output is usually less clean than 
writing data (e.g., considering disfluencies such as 
false starts, repetitions, filled pauses etc.). There-
fore we may need to remove these disfluencies first 
before computing syntactic complexity features. 
Also, importantly, ASR output does not contain 
interpunctuation but both for sentential-based fea-
tures as well as for parser-based features, the 
boundaries of clauses and sentences need to be 
known. For this purpose, we will use automated 
classifiers that are trained to predict clause and 
sentence boundaries, as described in Chen et al 
(2010). With previous studies providing us a rich 
pool of complexity features, additionally we also 
develop features analogous to the ones from the 
literature, mostly by using different calculation 
methods. For instance, the frequency of Preposi-
tional Phrases (PPs) is a feature from the literature, 
and we add some variants such as number of PPs 
per clause as a new feature to our extended feature 
set. 
2.2 Devising the Initial Feature Set 
Through this literature review, we identified some 
important features that were frequently used in 
previous studies in both L2 speaking and writing, 
such as length of sentences and number of clauses 
per sentence. In addition, we also collected candi-
date features that were less frequently mentioned 
in the literature, in order to start with a larger field 
of potential candidate features. We further ex-
tended the feature set by inspecting our data, de-
scribed in the following section, and created 
suitable additional features by means of analogy. 
This process resulted in a set of 91 features, 11 of 
which are related to clausal and sentential unit 
measurements (frequency-based) and 80 to mea-
surements within such units (ratio-based). From 
the perspective of extracting measures, in our study, 
some measures can be computed using only clause 
and sentence boundary information, and some can 
be derived only if the spoken responses are syntac-
tically parsed. In our feature set, there are two 
types of features: clause and sentence boundary 
based (26 in total) and parsing based (65). The fea-
tures will be described in detail in Section 4. 
3 Data 
Our data set contains (1) 1,060 non-native speech 
responses of 189 speakers from the TPO test (NN 
set), and (2) 100 responses from 48 native speakers 
that took the same test (Nat set). All responses 
were verbatim transcribed manually and scored 
724
holistically by human raters. (We only made use of 
the scores for the non-native data set in this study, 
since we purposefully selected speakers with per-
fect or near perfect scores for the Nat set from a 
larger native speech data set.) As mentioned above, 
there are four proficiency levels for human scoring, 
levels 1 to 4, with higher levels indicating better 
speaking proficiency. 
The NN set was randomly partitioned into a 
training (NN-train) and a test set with 760 and 300 
responses, respectively, and no speaker overlap.  
 
Data 
Set 
Res-
ponses 
Speakers Responses per 
Speaker  
(average) 
NN-
train 
760 137 5.55 
Description: used to train sentence and 
clause boundary detectors, evaluate fea-
tures and train scoring models 
1: 
NN-
test-1-
Hum 
300 52 5.77 
Description: human transcriptions and 
annotations of sentence and clause boun-
daries 
2: 
NN-
test-2-
CB 
300 52 5.77 
Description: human transcriptions, au-
tomatically predicted clause boundaries 
3: 
NN-
test-3-
SB 
300 52 5.77 
Description: human transcriptions, au-
tomatically predicted sentence bounda-
ries 
4: 
NN-
test-4-
ASR-
CB 
300 52 5.77 
Description: ASR hypotheses, automati-
cally predicted clause boundaries 
5: 
NN-
test-5-
ASR-
SB 
300 52 5.77 
Description: ASR hypotheses, automati-
cally predicted sentence boundaries 
Table 1. Overview of non-native data sets. 
 
A second version of the test set contains ASR 
hypotheses instead of human transcriptions. The 
word error rate (WER4
                                                          
4 Word error rate (WER) is the ratio of errors from a string 
between the ASR hypothesis and the reference transcript, 
where the sum of substitutions, insertions, and deletions is 
) on this data set is 50.5%. 
We used a total of five variants of the test sets, as 
described in Table 1. Sets 1-3 are based on human 
transcriptions, whereas sets 4 and 5 are based on 
ASR output. Further, set 1 contains human anno-
tated clause and sentence boundaries, whereas the 
other 4 sets have clause or sentence boundaries 
predicted by a classifier. 
All human transcribed files from the NN data 
set were annotated for clause boundaries, clause 
types, and disfluencies by human annotators (see 
Chen et al (2010)). 
For the Nat data set, all of the 100 transcribed 
responses were annotated in the same manner by a 
human annotator. They are not used for any train-
ing purposes but serve as a comparative reference 
for syntactic complexity features derived from the 
non-native corpus. 
The NN-train set was used both for training 
clause and sentence boundary classifiers, as well as 
for feature selection and training of the scoring 
models. The two boundary detectors were machine 
learning based Hidden Markov Models, trained by 
using a language model derived from the 760 train-
ing files which had sentence and clause boundary 
labels (NN-train; see also Chen et al (2010)).  
Since a speaker?s response to a single test item 
can be quite short (fewer than 100 words in many 
cases), it may contain only very few syntactic 
complexity features we are looking for. (Note that 
much of the previous work focused on written lan-
guage with much longer texts to be considered.) 
However, if we aggregate responses of a single 
speaker, we have a better chance of finding a larger 
number of syntactic complexity features in the ag-
gregated file. Therefore we joined files from the 
same speaker to one file for the training set and the 
five test sets, resulting in 52 aggregated files in 
each test set. Accordingly, we averaged the re-
sponse scores of a single speaker to obtain the total 
speaker score to be used later in scoring model 
training and evaluation (Section 5).5
While disfluencies were used for the training of 
the boundary detectors, they were removed after-
wards from the annotated data sets to obtain a tran-
 
                                                                                           
divided by the length of the reference. To obtain WER in 
percent, this ratio is multiplied by 100.0. 
5 Although in most operational settings, features are derived 
from single responses, this may not be true in all cases. 
Furthermore, scores of multiple responses are often combined 
for score reporting, which would make such an approach 
easier to implement and argue for operationally. 
725
scription which is ?cleaner? and lends itself better 
to most of the feature extraction methods we use.  
4 Feature Extraction 
4.1 Feature Set 
As mentioned in Section 2, we gathered 91 candi-
date syntactic complexity features based on our 
literature review as initial feature set, which is 
grouped into two categories: (1) Clause and sen-
tence Boundary based features (CB features); and 
(2) Parse Tree based features (PT features). Clause 
based features are based on both clause boundaries 
and clause types and can be generated from human 
clause annotations, e.g., ?frequency of adjective 
clauses6
We first selected features showing high correla-
tion to human assigned scores. In this process the 
CB features were computed from human labeled 
clause boundaries in transcripts for best accuracy, 
and PT features were calculated from using parsing 
and other tools because we did not have human 
parse tree annotations for our data.  
 per one thousand words?, ?mean number 
of dependent clauses per clause?, etc. Parse tree 
based features refer to features that are generated 
from parse trees and cannot be extracted from hu-
man annotated clauses directly.  
We used the Stanford Parser (Klein and Man-
ning, 2003) in conjunction with the Stanford Tre-
gex package (Levy and Andrew, 2006) which 
supports using rules to extract specific configura-
tions from parse trees, in a package put together by 
Lu (Lu, 2011). When given a sentence, the Stan-
ford Parser outputs its grammatical structure by 
grouping words (and phrases) in a tree structure 
and identifies grammatical roles of words and 
phrases.  
Tregex is a tree query tool that takes Stanford 
parser trees as input and queries the trees to find 
subtrees that meet specific rules written in Tregex 
syntax (Levy and Andrew, 2006). It uses relational 
operators regulated by Tregex, for example, ?A << 
B? stands for ?subtree A dominates subtree B?. 
The operators primarily function in subtree prece-
dence, dominance, negation, regular expression, 
tree node identity, headship, or variable groups, 
among others (Levy and Andrew, 2006).   
                                                          
6 An adjective clause is a clause that functions as an adjective 
in modifying a noun. E.g., ?This cat is a cat that is difficult to 
deal with.? 
Lu?s tool (Lu, 2011), built upon the Stanford 
Parser and Tregex, does syntactic complexity anal-
ysis given textual data. Lu?s tool contributed 8 of 
the initial CB features and 6 of the initial PT fea-
tures, and we computed the remaining CB and PT 
features using Perl scripts, the Stanford Parser, and 
Tregex.  
Table 2 lists the sub-set of 17 features (out of 91 
features total) that were used for building the scor-
ing models described later (Section 5). 
4.2 Feature Selection 
We determined the importance of the features by 
computing each feature?s correlation with human 
raters? proficiency scores based on the training set 
NN-train. We also used criteria related to the 
speaking construct, comparisons with native 
speaker data, and feature inter-correlations. While 
approaches coming from a pure machine learning 
perspective would likely use the entire feature pool 
as input for a classifier, our goal here is to obtain 
an initial feature set by judicious and careful fea-
ture selection that can withstand the scrutiny of 
construct validity in assessment development. 
 
As noted earlier, the disfluencies in the training set 
had been removed to obtain a ?cleaner? text that 
looks somewhat more akin to a written passage and 
is easier to process by NLP modules such as pars-
ers and part-of-speech (POS) taggers. 7
                                                          
7 We are aware that disfluencies can provide valuable clues 
about spoken proficiency in and of themselves; however, this 
study is focused exclusively on syntactic complexity analysis, 
and in this context, disfluencies would distort the picture 
considerably due to the introduction of parsing errors, e.g. 
  The ex-
tracted features partly were taken directly from 
proposals in the literature and partly were slightly 
modified to fit our clause annotation scheme. In 
order to have a unified framework for computing 
syntactic complexity features, we used a combina-
tion of the Stanford Parser and Tregex for compu-
ting both clause- and sentence-based features as 
well as parse-tree-based features, i.e., we did not 
make use of the human clause boundary label an-
notations here. The only exception to this
726
  
  
is that we are using human clause and sentence 
labels to create a candidate set for the clause boun-
dary features evaluated by the Stanford Parser and 
Tregex, as explained in the following subsection. 
                                                          
8  Feature type: CB=Clause boundary based feature type, 
PT=Parse tree based feature type 
9A ?linguistically meaningful PP? (PP_ling) is defined as a PP 
immediately dominated by another PP in cases where a 
preposition contains a noun such as ?in spite of? or ?in front 
of?. An example would be ?she stood in front of a house? 
where ?in front of a house? would be parsed as two embedded 
PPs but only the top PP would be counted in this case.  
10 A ?linguistically meaningful VP? (VP_ling) is defined  as a 
verb phrase immediately dominated by a clausal phrase, in 
order to avoid VPs embedded in another VP, e.g., "should go 
to work" is identified as one VP instead of two embedded 
VPs. 
11 The ?P-based Sampson? is a raw production-based measure 
(Sampson, 1997), defined as "proportion of the daughters of a 
nonterminal node which are themselves nonterminal and 
nonrightmost, averaged over the nonterminals of a sentence". 
 
 
 
Clause and Sentence based Features (CB fea-
tures) 
 
Firstly, we extracted all 26 initial CB features di-
rectly from human annotated data of NN-train, us-
ing information from the clause and sentence type 
labels. The reasoning behind this was to create an 
initial pool of clause-based features that reflects 
the distribution of clauses and sentences as accu-
rately as possible, even though we did not plan to 
use this extraction method operationally, where the 
parser decides on clause and sentence types. After 
computing the values of each CB feature, we cal-
culated correlations between each feature and hu-
man-rated scores. Then we created an initial CB   
feature pool by selecting features that met two cri-
teria: (1) the absolute Pearson correlation coeffi-
cient with human scores was larger than 0.2; and 
(2) the mean value of the feature on non-native 
speakers was at least 20% lower than that for na-
Name Type8 Meaning  Correlation Regression 
MLS CB Mean length of sentences 0.329 0.101 
MLT CB Mean length of T-units 0.300 -0.059 
DC/C CB Mean number of dependent clauses per clause 0.291 2.873 
SSfreq CB Frequency of simple sentences per 1000 words -.0242 0.001 
MLSS CB Mean length of simple sentences 0.255 0.040 
ADJCfreq CB Frequency of adjective clauses per 1000 words 0.253 0.004 
Ffreq CB Frequency of fragments per 1000 words -0.386 -0.057 
MLCC CB Mean length of coordinate clauses 0.224 0.017 
CT/T PT Mean number of complex T-units per T-unit 0.248 0.908 
PP_ling/S PT Mean number of linguistically meaningful prepositional phrases (PP) per sentence9 0.310  0.423 
NP/S PT Mean number of noun phrases (NP) per sentence 0.244 -0.411 
CN/S PT Mean number of complex nominal per sentence 0.325 0.653 
VB _ling/T PT Mean number of linguistically meaningful10 0.273   verb phrases per T-unit -0.780 
PAS/S PT Mean number of passives per sentence 0.260 1.520 
DI/T PT Mean number of dependent infinitives per T-unit 0.325 1.550 
MLev PT Mean number of parsing tree levels per sentence 0.306 -0.134 
MPSam PT Mean P-based Sampson11 0.254  per sentence 0.234 
Table 2. List of syntactic complexity features selected to be included in building the scoring models. 
727
tive speakers in case of positive correlation and at 
least by 20% higher than for native speakers in 
case of negative correlation, using the Nat data set 
for the latter criterion. Note that all of these fea-
tures were computed without using a parser. This 
resulted in 13 important features. 
Secondly, Tregex rules were developed based 
on Lu?s tool to extract these 13 CB features from 
parsing results where the parser is provided with 
one sentence at a time. By applying the same selec-
tion criteria as before, except for allowing for cor-
relations above 0.1 and giving preference to 
linguistically more meaningful features, we found 
8 features that matched our criteria:  
MLS, MLT, DC/C, SSfreq, MLSS, ADJCfreq, 
Ffreq, MLCC 
All 28 pairwise inter-correlations between these 
8 features were computed and inspected to avoid 
including features with high inter-correlations in 
the scoring model. Since we did not find any inter-
correlations larger than 0.9, the features were con-
sidered moderately independent and none of them 
were removed from this set so it also maintains 
linguistic richness for the feature set.  
Due to the importance of T-units in complexity 
analysis, we briefly introduce how we obtain them 
from annotations. Three types of clauses labeled in 
our transcript can serve as T-units, including sim-
ple sentences, independent clauses, and conjunct 
(coordination) clauses. These clauses were identi-
fied in the human-annotated text and extracted as 
T-units in this phase. T-units in parse trees are 
identified using rules in Lu?s tool. 
 
Parse Tree based Features (PT features) 
 
We evaluated 65 features in total and selected fea-
tures with highest importance using the following 
two criteria (which are very similar as before): (1) 
the absolute Pearson correlation coefficient with 
human scores is larger than 0.2; and (2) the feature 
mean value on native speakers (Nat) is higher than 
on score 4 for non-native speakers in case of posi-
tive correlation, or lower for negative correlation. 
20 of 65 features were found to meet the require-
ments. 
Next, we examined inter-correlations between 
these features and found some correlations larger 
than 0.85.12
CT/T, PP_ling/S, NP/S, CN/S, VP_ling/T, PAS/S, 
DI/T, MLev, MPSam  
 For each feature pair exhibiting high 
inter-correlation, we removed one feature accord-
ing to the criterion that the removed feature should 
be linguistically less meaningful than the remain-
ing one. After this filtering, the 9 remaining PT 
features are: 
In summary, as a result of the feature selection 
process, a total of 17 features were identified as 
important features to be used in scoring models for 
predicting speakers? proficiency scores. Among 
them 8 are clause boundary based and the other 9 
are parse tree based. 
5 Experiments and Results 
In the previous section, we identified 17 syntactic 
features that show promising correlations with hu-
man rater speaking proficiency scores. These fea-
tures as well as the human-rated scores will be 
used to build scoring models by using machine 
learning methods. As introduced in Section 3, we 
have one training set (N=137 speakers with all of 
their responses combined) for model building and 
five testing sets (N=52 for each of them) for evalu-
ation.  
The publicly available machine learning pack-
age Weka was used in our experiments (Hall et al 
2009). We experimented with two algorithms in 
Weka: multiple regression (called ?LinearRegres-
sion? in Weka) and decision tree (called ?M5P?in 
Weka). The score values to be predicted are real 
numbers (i.e., non-integer), because we have to 
compute the average score of one speaker?s res-
ponses. Our initial runs showed that decision tree 
models were consistently outperformed by mul-
tiple regression (MR) models and thus decided to 
only focus on MR models henceforth. 
We set the ?AttributeSelectionMethod? parame-
ter in Weka?s LinearRegression algorithm to all 3 
of its possible values in turn: (Model-1) M5 me-
thod; (Model-2) no attribute selection; and (Model-
3) greedy method. The resulting three multiple re-
gression models were then tested against the five 
testing sets. Overall, correlations for all models for 
the NN-test-1-Hum set were between 0.45 and 
0.49, correlations for sets NN-test-2-CB and NN- 
                                                          
12 The reason for using a lower threshold than above was to 
obtain a roughly equal number of CB and PT features in the 
end. 
728
test-3-SB (human transcript based, and using au-
tomated boundaries) around 0.2, and for sets NN-
test-4-ASR-CB  and NN-test-5-ASR-SB (ASR hy-
potheses, and using automated boundaries), the 
correlations were not significant. Model-2 (using 
all 17 features) had the highest correlation on NN-
test-1-Hum and we provide correlation results of 
this model in Table 3. 
 
Test set 
Correlation 
coefficient 
Correlation significance 
(p < 0.05) 
NN-test-1-Hum 0.488 Significant 
NN-test-2-CB 0.220 Significant 
NN-test-3-SB 0.170 Significant 
NN-test-4-ASR-CB -0.025 Not significant 
NN-test-5-ASR-SB -0.013 Not significant 
Table 3. Multiple regression model testing results for 
Model-2. 
6 Discussion 
As we can see from the result table (Table 3) in the 
previous section, using only syntactic complexity 
features, based on clausal or parse tree information 
derived from human transcriptions of spoken test 
responses, can predict holistic human rater scores 
for combined speaker responses over a whole test 
with an overall correlation of r=0.49. While this is 
a promising result for this study with a focus on a 
broad spectrum of syntactic complexity features, 
the results also show significant limitations for an 
immediate operational use of such features. First, 
the imperfect prediction of clause and sentence 
boundaries by the two automatic classifiers causes 
a substantial degradation of scoring model perfor-
mance to about r=0.2, and secondly, the rather high 
error rate of the ASR system (50.5%) does not al-
low for the computation of features that would re-
sult in any significant correlation with human 
scores. We want to note here that while ASR sys-
tems can be found that exhibit WERs below 10% 
for certain tasks, such as restricted dictation in 
low-noise environments by native speakers, our 
ASR task is significantly harder in several ways: 
(1) we have to recognize non-native speak-
ers?rresponses where speakers have a number of 
different native language backgrounds; (2) the pro-
ficiency level of the test takers varies widely; and 
(3) the responses are spontaneous and uncon-
strained in terms of vocabulary. 
As for the automatic clause and sentence boun-
dary classifiers, we can observe (in Table 4) that 
although the sentence boundary classifier has a 
slightly higher F-score than the clause boundary 
classifier, errors in sentence boundary detection 
have more negative effects on the accuracy of 
score prediction than those made by the clause 
boundary classifier. In fact, the lower F-score of 
the latter is mainly due to its lower precision which 
indicates that there are more spurious clause boun-
daries in its output which apparently cause little 
harm to the feature extraction processes. 
Among the 17 final features, 3 of them are fre-
quency-based and the remaining 14 are ratio-
based, which mirrors our findings from previous 
work that frequency features have been used less 
successfully than ratio features. As for ratio fea-
tures, 5 of them are grammatical structure counts 
against sentence units, 4 are counts against T-units, 
and only 1 is based on counts against clause units. 
The feature set covers a wide range of grammatical 
structures, such as T-units, verb phrases, noun 
phrases, complex nominals, adjective clauses, 
coordinate clauses, prepositional phrases, etc. 
While this wide coverage provides for richness of 
the construct of syntactic complexity, some of the 
features exhibit relatively high correlation with 
each other which reduces their overall contribu-
tions to the scoring model?s performance. 
Going through the workflow of our system, we 
find at least five major stages that can generate 
errors which in turn can adversely affect feature 
computation and scoring model building. Errors 
may appear in each stage of our workflow, passing 
or even enlarging their effects from previous stages 
to later stages: 
1) grammatical errors by the speakers (test takers); 
2) errors by the ASR system; 
3) sentence/clause boundary detection errors; 
4) parser errors; and 
5) rule extraction errors. 
 
In future work we will need to address each er-
ror source to obtain a higher overall system per-
formance. 
 
729
Table 4. Performance of clause and sentence boundary 
detectors. 
7 Conclusion and Future Work 
In this paper, we investigated associations between 
speakers? syntactic complexity features and their 
speaking proficiency scores provided by human 
raters. By exploring empirical evidence from non-
native and native speakers? data sets of spontane-
ous speech test responses, we identified 17 features 
related to clause types and parse trees as effective 
predictors of human speaking scores. The features 
were implemented based on Lu?s L2 Syntactic 
Complexity Analyzer toolkit (Lu, 2011) to be au-
tomatically extracted from human or ASR tran-
scripts. Three multiple regression models were 
built from non-native speech training data with 
different parameter setup and were tested against 
five testing sets with different preprocessing steps. 
The best model used the complete set of 17 fea-
tures and exhibited a correlation with human 
scores of r=0.49 on human transcripts with boun-
dary annotations. 
When using automated classifiers to predict 
clause or sentence boundaries, correlations with 
human scores are around r=0.2. Our experiments 
indicate that by enhancing the accuracy of the two 
main automated preprocessing components, name-
ly ASR and automatic sentence and clause boun-
dary detectors, scoring model performance will 
increase substantially, as well. Furthermore, this 
result demonstrates clearly that syntactic complexi-
ty features can be devised that are able to predict 
human speaking proficiency scores. 
Since this is a preliminary study, there is ample 
space to improve all major stages in the feature 
extraction process. The errors listed in the previous 
section are potential working directions for prepro-
cessing enhancements prior to machine learning. 
Among the five types of errors, we can work on 
improving the accuracy of the speech recognizer, 
sentence and clause boundary detectors, parser, 
and feature extraction rules; as for the grammatical 
errors produced by test takers, we are envisioning 
to automatically identify and correct such errors. 
We will further experiment with syntactic com-
plexity measures to balance construct richness and 
model simplicity. Furthermore, we can also expe-
riment with additional types of machine learning 
models and tune parameters to derive scoring 
models with better performance. 
 
Acknowledgements 
The authors wish to thank Lei Chen and Su-Youn 
Yoon for their help with the sentence and clause 
boundary classifiers. We also would like to thank 
our colleagues Jill Burstein, Keelan Evanini, Yoko 
Futagi, Derrick Higgins, Nitin Madnani, and Joel 
Tetreault, as well as the four anonymous ACL re-
viewers for their valuable and helpful feedback and 
comments on our paper. 
References  
Bachman, L.F. (1990). Fundamental considerations in 
language testing. Oxford: Oxford University Press. 
Bernstein, J. (1999). PhonePass testing: Structure and 
construct. Menlo Park, CA: Ordinate Corporation. 
Bernstein, J., DeJong, J., Pisoni, D. & Townshend, B. 
(2000). Two experiments in automatic scoring of 
spoken language proficiency. Proceedings of In-
STILL 2000, Dundee, Scotland. 
Bernstein, J., Cheng, J., & Suzuki, M. (2010). Fluency 
and structural complexity as predictors of L2 oral 
proficiency. Proceedings of Interspeech 2010, Tokyo, 
Japan, September. 
Chen, L., Tetreault, J. & Xi, X. (2010). Towards using 
structural events to assess non-native speech. 
NAACL-HLT 2010. 5th Workshop on Innovative 
Use of NLP for Building Educational Applications, 
Los Angeles, CA, June. 
Condouris, K., Meyer, E. & Tagger-Flusberg, H. 
(2003). The relationship between standardized meas-
ures of language and measures of spontaneous speech 
in children with autism. American Journal of Speech-
Language Pathology, 12(3), 349-358. 
Cooper, T.C. (1976). Measuring written syntactic pat-
terns of second language learners of German. The 
Journal of Educational Research, 69(5), 176-183. 
Cucchiarini, C., Strik, H. & Boves, L. (1997). Automat-
ic evaluation of Dutch pronunciation by using speech 
recognition technology. IEEE Automatic Speech 
Recognition and Understanding Workshop, Santa 
Barbara, CA. 
Classifier Accu-
racy 
Preci-
sion 
Re-
call 
F score 
Clause boundary 0.954 0.721 0.748 0.734 
Sentence boundary 0.975    0.811 0.755 0.782    
730
Cucchiarini, C., Strik, H. & Boves, L. (2000). Quantita-
tive assessment of second language learners' fluency 
by means of automatic speech recognition technolo-
gy. Journal of the Acoustical Society of America, 
107, 989-999.  
Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R. 
& Butzberger, J. (2000a). The SRI EduSpeak system: 
Recognition and pronunciation scoring for language 
learning. Proceedings of InSTiLL-2000 (Intelligent 
Speech Technology in Language Learning), Dundee, 
Scotland.  
Franco, H., Neumeyer, L., Digalakis, V. & Ronen, O. 
(2000b). Combination of machine scores for auto-
matic grading of pronunciation quality. Speech 
Communication, 30, 121-130.  
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P. &  Witten, I.H. (2009). The WEKA 
Data Mining Software: An Update. SIGKDD Explo-
rations, 11(1). 
Halleck, G.B. (1995). Assessing oral proficiency: A 
comparison of holistic and objective measures. The 
Modern Language Journal, 79(2), 223-234. 
Henry, K. (1996). Early L2 writing development: A 
study of autobiographical essays by university-level 
students on Russian. The Modern Language Journal, 
80(3), 309-326. 
Ho-Peng, L. (1983). Using T-unit measures to assess 
writing proficiency of university ESL students. 
RELC Journal, 14(2), 35-43. 
Hunt, K. (1965). Grammatical structures written at three 
grade levels. NCTE Research report No.3. Cham-
paign, IL: NCTE. 
Iwashita, N. (2006). Syntactic complexity measures and 
their relations to oral proficiency in Japanese as a 
foreign language. Language Assessment Quarterly, 
3(20), 151-169. 
Kameen, P.T. (1979). Syntactic skill and ESL writing 
quality. In C. Yorio, K. Perkins, & J. Schachter 
(Eds.), On TESOL ?79: The learner in focus (pp.343-
364). Washington, D.C.: TESOL. 
Klein, D. & Manning, C.D. (2003). Fast exact inference 
with a factored model for a natural language parsing. 
In S.Becker, S. Thrun & K. Obermayer (Eds.), Ad-
vances in Neural Information Processing Systems 15 
(pp.3-10). Cambridge, MA: MIT Press. 
Larsen-Freeman, D. (1978). An ESL index of develop-
ment. Teachers of English to Speakers of Other Lan-
guages Quarterly, 12(4), 439-448. 
Levy, R. & Andrew, G. (2006). Tregex and Tsurgeon: 
Tools for querying and manipulating tree data struc-
tures. Proceedings of the Fifth International Confe-
rence on Language Resources and Evaluation.  
Lu, X. (2010). Automatic analysis of syntactic complex-
ity in second language writing. International Journal 
of Corpus Linguistics, 15(4), 474-496. 
Lu, X. (2011). L2 Syntactic Complexity Analyzer. Re-
trieved from 
http://www.personal.psu.edu/xxl13/downloads/l2sca.
html 
Ortega, L. (2003). Syntactic complexity measures and 
their relationship to L2 proficiency: A research syn-
thesis of college-level L2 writing. Applied Linguis-
tics, 24(4), 492-518. 
Perkins, K. (1980). Using objective methods of attained 
writing proficiency to discriminate among holistic 
evaluations. Teachers of English to Speakers of Oth-
er Languages Quarterly, 14(1), 61-69. 
Roll, M., Frid, J. & Horne, M. (2007). Measuring syn-
tactic complexity in spontaneous spoken Swedish. 
Language and Speech, 50(2), 227-245. 
Sampson, G. (1997). Depth in English grammar. Journal 
of Linguistics, 33, 131-151. 
Wolfe-Quintero, K., Inagaki, S. & Kim, H. Y. (1998). 
Second language development in writing: Measures 
of fluency, accuracy, & complexity. Honolulu, HI: 
University of Hawaii Press. 
Xi, X., & Mollaun, P. (2006).  Investigating the utility 
of analytic scoring for the TOEFL? Academic 
Speaking Test (TAST). TOEFL iBT Research Re-
port No. TOEFLiBT-01. 
Zechner, K., Higgins, D. & Xi, X. (2007). SpeechRa-
ter(SM): A construct-driven approach to score spon-
taneous non-native speech. Proceedings of the 2007 
Workshop of the International Speech Communica-
tion Association (ISCA) Special Interest Group on 
Speech and Language Technology in Education 
(SLaTE), Farmington, PA, October. 
Zechner, K., Higgins, D., Xi, X, & Williamson, D.M. 
(2009). Automatic scoring of non-native spontaneous 
speech in tests of spoken English. Speech Communi-
cation, 51 (10), October.  
 
731
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 53?56,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Amazon Mechanical Turk for Transcription of Non-Native Speech
Keelan Evanini, Derrick Higgins, and Klaus Zechner
Educational Testing Service
{KEvanini, DHiggins, KZechner}@ets.org
Abstract
This study investigates the use of Amazon
Mechanical Turk for the transcription of non-
native speech. Multiple transcriptions were
obtained from several distinct MTurk workers
and were combined to produce merged tran-
scriptions that had higher levels of agreement
with a gold standard transcription than the in-
dividual transcriptions. Three different meth-
ods for merging transcriptions were compared
across two types of responses (spontaneous
and read-aloud). The results show that the
merged MTurk transcriptions are as accurate
as an individual expert transcriber for the read-
aloud responses, and are only slightly less ac-
curate for the spontaneous responses.
1 Introduction
Orthographic transcription of large amounts of
speech is necessary for improving speech recogni-
tion results. Transcription, however, is a time con-
suming and costly procedure. Typical transcription
speeds for spontaneous, conversational speech are
around 7 to 10 times real-time (Glenn and Strassel,
2008). The transcription of non-native speech is an
even more difficult task?one study reports an aver-
age transcription time of 12 times real-time for spon-
taneous non-native speech (Zechner, 2009).
In addition to being more costly and time consum-
ing, transcription of non-native speech results in a
higher level of disagreement among transcribers in
comparison to native speech. This is especially true
when the speaker?s proficiency is low and the speech
contains large numbers of grammatical errors, in-
correct collocations, and disfluencies. For exam-
ple, one study involving highly predictable speech
shows a decline in transcriber agreement (measured
using Word Error Rate, WER) from 3.6% for na-
tive speech to 6.4% for non-native speech (Marge et
al., to appear). Another study involving spontaneous
non-native speech showed a range of WER between
15% and 20% (Zechner, 2009).
This study uses the Amazon Mechanical Turk
(MTurk) resource to obtain multiple transcriptions
for non-native speech. We then investigate several
methods for combining these multiple sources of in-
formation from individual MTurk workers (turkers)
in an attempt to obtain a final merged transcription
that is more accurate than the individual transcrip-
tions. This methodology results in transcriptions
that approach the level of expert transcribers on this
difficult task. Furthermore, a substantial savings in
cost can be achieved.
2 Previous Work
Due to its ability to provide multiple sources of
information for a given task in a cost-effective
way, several recent studies have combined multi-
ple MTurk outputs for NLP annotation tasks. For
example, one study involving annotation of emo-
tions in text used average scores from up to 10 turk-
ers to show the minimum number of MTurk anno-
tations required to achieve performance compara-
ble to experts (Snow et al, 2008). Another study
used preference voting to combine up to 5 MTurk
rankings of machine translation quality and showed
that the resulting judgments approached expert inter-
annotator agreement (Callison-Burch, 2009). These
53
tasks, however, are much simpler than transcription.
MTurk has been used extensively as a transcrip-
tion provider, as is apparent from the success of a
middleman site that act as an interface to MTurk
for transcription tasks.1 However, to our knowledge,
only one previous study has systematically evaluated
the quality of MTurk transcriptions (Marge et al,
to appear). This recent study also combined multi-
ple MTurk transcriptions using the ROVER method
(Fiscus, 1997) to produce merged transcriptions that
approached the accuracy of expert transcribers. Our
study is similar to that study, except that the speech
data used in our study is much more difficult to
transcribe?the utterances used in that study were rel-
atively predictable (providing route instructions for
robots), and contained speech from native speak-
ers and high-proficiency non-native speakers. Fur-
thermore, we investigate two additional merging al-
gorithms in an attempt to improve over the perfor-
mance of ROVER.
3 Experimental Design
3.1 Audio
The audio files used in this experiment consist of
responses to an assessment of English proficiency
for non-native speakers. Two different types of re-
sponses are examined: spontaneous and read-aloud.
In the spontaneous task, the speakers were asked to
respond with their opinion about a topic described
in the prompt. The speech in these responses is thus
highly unpredictable. In the read-aloud task, on the
other hand, the speakers were asked to read a para-
graph out loud. For these responses, the speech is
highly predictable; any deviations from the target
script are due to reading errors or disfluencies.
For this experiment, one set of 10 spontaneous
(SP) responses (30 seconds in duration) and two sets
of 10 read-aloud (RA) responses (60 seconds in du-
ration) were used. Table 1 displays the characteris-
tics of the responses in the three batches.
3.2 Transcription Procedure
The tasks were submitted to the MTurk interface in
batches of 10, and a turker was required to complete
the entire batch in order to receive payment. Turkers
1http://castingwords.com/
Batch Duration # of Words
(Mean)
# of Words
(Std. Dev.)
SP 30 sec. 33 14
RA1 60 sec. 97 4
RA2 60 sec. 93 10
Table 1: Characteristics of the responses used in the study
received $3 for a complete batch of transcriptions
($0.30 per transcription).
Different interfaces were used for transcribing the
two types of responses. For the spontaneous re-
sponses, the task was a standard transcription task:
the turkers were instructed to enter the words that
they heard in the audio file into a text box. For the
read-aloud responses, on the other hand, they were
provided with the target text of the prompt, one word
per line. They were instructed to make annotations
next to words in cases where the speaker deviated
from the target text (indicating substitutions, dele-
tions, and insertions). For both types of transcription
task, the turkers were required to successfully com-
plete a short training task before proceeding onto the
batch of 10 responses.
4 Methods for Merging Transcriptions
4.1 ROVER
The ROVER method was originally developed for
combining the results from multiple ASR systems to
produce a more accurate hypothesis (Fiscus, 1997).
This method iteratively aligns pairs of transcriptions
to produce a word transition network. A voting pro-
cedure is then used to produce the merged transcrip-
tion by selecting the most frequent word (including
NULL) in each correspondence set; ties are broken
by a random choice.
4.2 Longest Common Subsequence
In this method, the Longest Common Subsequence
(LCS) among the set of transcriptions is found by
first finding the LCS between two transcriptions,
comparing this output with the next transcription to
find their LCS, and iterating over all transcriptions in
this manner. Then, each transcription is compared to
the LCS, and any portions of the transcription that
are missing between words of the LCS are tallied.
Finally, words are interpolated into the LCS by se-
54
lecting the most frequent missing sequence from the
set of transcriptions (including the empty sequence);
as with the ROVER method, ties are broken by a ran-
dom choice among the most frequent candidates.
4.3 Lattice
In this method, a word lattice is formed from the
individual transcriptions by iteratively adding tran-
scriptions into the lattice to optimize the match be-
tween the transcription and the lattice. New nodes
are only added to the graph when necessary. Then,
to produce the merged transcription, the optimal
path through the lattice is determined. Three dif-
ferent configurations for computing the optimal path
through the lattice method were compared. In the
first configuration, ?Lattice (TW),? the weight of
a path through the lattice is determined simply by
adding up the total of the weights of each edge
in the path. Note that this method tends to fa-
vor longer paths over shorter ones, assuming equal
edge weights. In the next configuration, ?Lattice
(AEW),? a cost for each node based on the aver-
age edge weight is subtracted as each edge of the
lattice is traversed, in order to ameliorate the prefer-
ence for longer paths. Finally, in the third configura-
tion, ?Lattice (TWPN),? the weight of a path through
the lattice is defined as the total path weight in the
?Lattice (TW)? method, normalized by the number
of nodes in the path (again, to offset the preference
for longer paths).
4.4 WER calculation
All three of the methods for merging transcriptions
are sensitive to the order in which the individual
transcriptions are considered. Thus, in order to accu-
rately evaluate the methods, for each number of tran-
scriptions used to create the merged transcription,
N ? {3, 4, 5}, all possible permutations of all pos-
sible combinations were considered. This resulted
in a total of 5!(5?N)! merged transcriptions to be eval-
uated. For each N, the overall WER was computed
from this set of merged transcriptions.
5 Results
Tables 2 - 4 present the WER results for differ-
ent merging algorithms for the two batches of read-
aloud responses and the batch of spontaneous re-
sponses. In each table, the merging methods are or-
Method N=3 N=4 N=5
Individual Turkers 7.0%
Lattice (TWPN) 6.4% 6.4% 6.4%
Lattice (TW) 6.4% 6.4% 6.4%
LCS 6.0% 5.6% 5.6%
Lattice (AEW) 6.1% 6.0% 5.5%
ROVER 5.5% 5.2% 5.1%
Expert 4.7%
Table 2: WER results 10 read-aloud responses (RA1)
Method N=3 N=4 N=5
Individual Turkers 9.7%
Lattice (TW) 9.5% 9.5% 9.4%
Lattice (TWPN) 8.3% 8.0% 8.0%
Lattice (AEW) 8.2% 7.4% 7.8%
ROVER 7.9% 7.9% 7.6%
LCS 8.3% 8.0% 7.5%
Expert 8.1%
Table 3: WER results for 10 read-aloud responses (RA2)
dered according to their performance when all tran-
scriptions were used (N=5). In addition, the overall
WER results for the individual turkers and an expert
transcriber are provided for each set of responses.
In each case, the WER is computed by comparison
with a gold standard transcription that was created
by having an expert transcriber edit the transcription
of a different expert transcriber.
In all cases, the merged transcriptions have a
lower WER than the overall WER for the individual
turkers. Furthermore, for all methods, the merged
output using all 5 transcriptions has a lower (or
equal) WER to the output using 3 transcriptions. For
the first batch of read-aloud responses, the ROVER
method performed best, and reduced the WER in
the set of individual transcriptions by 27.1% (rela-
tive) to 5.1%. For the second batch of read-aloud
responses, the LCS method performed best, and re-
duced the WER by 22.6% to 7.5%. Finally, for the
batch of spontaneous responses, the Lattice (TW)
method performed best, and reduced the WER by
25.6% to 22.1%.
55
Method N=3 N=4 N=5
Individual Turkers 29.7%
Lattice (TWPN) 29.1% 28.9% 28.3%
LCS 29.2% 28.4% 27.0%
Lattice (AEW) 28.1% 25.8% 25.1%
ROVER 25.4% 24.5% 24.9%
Lattice (TW) 25.5% 23.5% 22.1%
Expert 18.3%
Table 4: WER results for 10 spontaneous responses
6 Conclusions
As is clear from the levels of disagreement be-
tween the expert transcriber and the gold standard
transcription for all three tasks, these responses are
much more difficult to transcribe accurately than
native spontaneous speech. For native speech, ex-
pert transcribers can usually reach agreement lev-
els over 95% (Deshmukh et al, 1996). For these
responses, however, the WER for the expert tran-
scriber was worse than this even for the read-aloud
speech. These low levels of agreement can be at-
tributed to the fact that the speech is drawn from a
wide range of English proficiency levels among test-
takers. Most of the responses contain disfluencies,
grammatical errors, and mispronunciations, leading
to increased transcriber uncertainty.
The results of merging multiple MTurk transcrip-
tions of this non-native speech showed an improve-
ment over the performance of the individual tran-
scribers for all methods considered. For the read-
aloud speech, the agreement level of the merged
transcriptions approached that of the expert tran-
scription when only three MTurk transcriptions were
used. For the spontaneous responses, the perfor-
mance of the best methods still lagged behind the ex-
pert transcription, even when five MTurk transcrip-
tions were used. Due to the consistent increase in
performance, and the low cost of adding additional
transcribers (in this study the cost was $0.30 per au-
dio minute for read-aloud speech and $0.60 per au-
dio minute for spontaneous speech), the approach of
combining multiple transcriptions should always be
considered when MTurk is used for transcription. It
is also possible that lower payments per task could
be provided without a decrease in transcription qual-
ity, as demonstrated by Marge et al (to appear). Ad-
ditional experiments will address the practicality of
producing more accurate merged transcriptions for
an ASR system?simply collecting larger amounts
of non-expert transcriptions may be a better invest-
ment than producing higher quality data (Novotney
and Callison-Burch, 2010).
It is interesting that the Lattice (TW) method
of merging transcriptions clearly outperformed all
other methods for the spontaneous responses, but
was less beneficial than the LCS and ROVER meth-
ods for read-aloud speech. It is likely that this is
caused by the preference of the Lattice (TW) method
for longer paths through the word lattice, since indi-
vidual transcribers of spontaneous speech may mark
different words as unitelligible, even though these
words exist in the gold standard transcription. Fur-
ther studies with a larger number of responses will
be needed to test this hypothesis.
References
Chris Callison-Burch. 2009. Fast, cheap and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. EMNLP.
Neeraj Deshmukh, Richard Jennings Duncan, Aravind
Ganapathiraju, and Joseph Picone. 1996. Benchmark-
ing human performance for continuous speech recog-
nition. In Proc. ICSLP.
Jonathan G. Fiscus. 1997. A post-processing system to
yield word error rates: Recognizer Ooutput Voting Er-
ror Reduction (ROVER). In Proc. ASRU.
Meghan Lammie Glenn and Stephanie Strassel. 2008.
Shared linguistic resources for the meeting domain.
In Lecture Notes in Computer Science, volume 4625,
pages 401?413. Springer.
Matthew Marge, Satanjeev Banerjee, and Alexander I.
Rudnicky. to appear. Using the Amazon Mechanical
Turk for transcription of spoken language. In Proc.
ICASSP.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast, and good enough: Automatic speech recognition
with non-expert transcription. In Proc. NAACL.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? But is it
good? Evaluating non-expert annotations for natural
language tasks. In Proc. EMNLP.
Klaus Zechner. 2009. What did they actually say?
Agreement and disagreement among transcribers of
non-native spontaneous speech responses in an En-
glish proficiency test. In Proc. ISCA-SLaTE.
56
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152?160,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Non-scorable Response Detection for Automated Speaking Proficiency
Assessment
Su-Youn Yoon, Keelan Evanini, Klaus Zechner
Educational Testing Service
660 Rosedale Road, Princeton, NJ, USA
{syoon,kevanini,kzechner}@ets.org
Abstract
We present a method that filters out non-
scorable (NS) responses, such as responses
with a technical difficulty, in an automated
speaking proficiency assessment system. The
assessment system described in this study first
filters out the non-scorable responses and then
predicts a proficiency score using a scoring
model for the remaining responses.
The data were collected from non-native
speakers in two different countries, using two
different item types in the proficiency assess-
ment: items that elicit spontaneous speech and
items that elicit recited speech. Since the pro-
portion of NS responses and the features avail-
able to the model differ according to the item
type, an item type specific model was trained
for each item type. The accuracy of the mod-
els ranged between 75% and 79% in spon-
taneous speech items and between 95% and
97% in recited speech items.
Two different groups of features, signal pro-
cessing based features and automatic speech
recognition (ASR) based features, were im-
plemented. The ASR based models achieved
higher accuracy than the non-ASR based mod-
els.
1 Introduction
We developed a method that filters out non-scorable
(NS) responses as a supplementary module to an
automated speech proficiency assessment system.
In this study, the method was developed for a
telephony-based assessment of English proficiency
for non-native speakers. The examinees? responses
were collected from several different environmen-
tal conditions, and many of the utterances contain
background noise from diverse sources. In ad-
dition to the presence of noise, many responses
have other sub-optimal characteristics. For exam-
ple, some responses contain uncooperative behav-
ior from the speakers, such as non-English speech,
whispered speech, and non-responses. These types
of responses make it difficult to provide a valid as-
sessment of a speaker?s English proficiency. There-
fore, in order to address the diverse types of causes
for these problematic responses, we used a two step
approach: first, these problematic responses were
filtered out by a ?filtering model,? and only the re-
maining responses were scored using the automated
scoring model.
The overall architecture of our method, includ-
ing the automated speech proficiency scoring sys-
tem, is as follows: for a given spoken response,
the system performs speech recognition, yielding a
word hypothesis and time stamps. In addition to
word recognition, the system computes pitch and
power to generate prosodic features; the system cal-
culates descriptive statistics such as the mean and
standard deviation of pitch and power at both the
word level and response level. Given the word hy-
potheses and pitch/power features, it derives features
for automated proficiency scoring. Next, the non-
ASR based features are calculated separately using
signal processing techniques. Finally, given both
sets of features, the filtering model identifies NS re-
sponses.
This paper will proceed as follows: we will re-
view previous studies (Section 2), present the data
152
(Section 3), and then describe the structure of the
filtering model (Section 4). Next, the results will
be presented (Section 5), followed by a discussion
(Section 6), and we will conclude with a summary
of the importance of the findings (Section 7).
2 Previous Work
Higgins et al (2011) developed a ?filtering model?
that is conceptually similar to the one in this pa-
per. The model was trained and tested on a corpus
containing responses from non-native speakers to an
English proficiency assessment. This system used
a regression model based on four features which
were originally designed for automated speech pro-
ficiency scoring: the number of distinct words in the
speech recognition output, the average speech rec-
ognizer confidence score, the average power of the
speech signal, and the mean absolute deviation of
the speech signal power. This model was able to
identify responses which were also identified as NS
responses by human raters with an approximately
98% accuracy when a false positive rate (the propor-
tion of responses without technical difficulties that
were incorrectly flagged as problematic) was lower
than 1%.
Although there are few other studies which are di-
rectly related to the task of filtering out non-scorable
responses in the domain of automated speech profi-
ciency assessment, several signal processing studies
are related to this work. Traditionally, the Signal to
Noise Ratio (SNR) has been used to detect speech
with a large amount of background noise. This
method measures the ratio between the total energy
of the speech signal and the total energy of the noise;
if the SNR is low, then the speech contains loud
background noise. A low SNR results in lower in-
telligibility and increases the difficulty for both hu-
man and automated scoring. Furthermore, spectral
characteristics can be also applied to detect speech
with loud background noise, since noise has differ-
ent spectral characteristics than speech (noise tends
to have no or few peaks in the spectral domain).
If a response contains loud background noise, then
the spectral characteristics of the speech may be ob-
scured by noise and it may have similar character-
istics with the noise. These differences in spectral
characteristics have been used in audio information
retrieval Lu and Hankinson (1998).
Secondly, responses without valid speech can be
identified using Voice Activity Detection (VAD).
VAD is a technique which distinguishes human
speech from non-speech. When speech is clean,
VAD can be calculated by simply computing the
zero-crossing rate which signals the existence of
cyclic waves such as vowels. However, if the re-
sponse also contains loud background noises, more
sophisticated methods are required. In order to re-
move the influence of noise, Chang and Kim (2003),
Chang et al (2006), Shin et al (2005) and Sohn et
al. (1999) estimated the characteristics of the noise
spectrum and the distribution of noise, and compen-
sated for them when speech is identified. The perfor-
mance of these systems is heavily-influenced by the
accuracy of estimating characteristics of the back-
ground noise.
In this study, we used a set of ASR based fea-
tures and non-ASR based features. ASR based fea-
tures were similar to the ones used by Zechner et al
(2009). In addition to the features based on ASR
hypotheses, the ASR based feature set contained ba-
sic pitch and power related features since the ASR
system in this study also produced pitch and power
measurements in order to generate prosodic features.
The non-ASR based features were comprised of four
groups of features based on signal processing tech-
niques such as SNR, VAD, and pitch and power.
Features related to pitch and power were included in
both the ASR based features and the non-ASR based
features. Since the non-ASR based features were
originally implemented as an independent module
from the ASR-based system (it was implemented for
the case where the appropriate recognizer is unavail-
able), there is some degree of overlap between the
two feature sets.
3 Data
The data for this experiment were drawn from a pro-
totype of a telephony-based English language as-
sessment. Non-native speakers of English each re-
sponded to 40 test items designed to evaluate their
level of English proficiency. The test was composed
of items that elicited both spontaneous speech (here-
after SS) and recited speech (hereafter RS). In this
study, 8 items (four SS and four RS) were used for
153
each speaker.
Participants used either a cell phone or a land
line to complete the assessment, and the participants
were compensated for their time. The motivation
level of the participants was thus lower than in the
case of an actual high stakes assessment, where a
participant?s performance could have a substantial
impact on their future. In addition, the data collec-
tion procedure was less controlled than in an op-
erational testing environment; for example, some
recordings exhibited higher levels of ambient noise
than others. These two facts led to the quality of
some of the responses being lower than would be
expected in an operational assessment.
The data for this study were collected from partic-
ipants in two countries: India and China. For India,
4900 responses from 638 speakers were collected.
For China, 5565 responses from 702 speakers were
collected (some of the participants did not provide
responses to all 8 test items). Each response is ap-
proximately 45 sec in duration.
After the data was collected, all of the responses
were given scores on a three-point scale by trained
raters. The raters also labeled responses as ?non-
scorable? (NS), when appropriate. NS responses are
ones that could not be given a score according to the
rubrics of the three-point scale. These were due to
either a technical difficulty obscuring the content of
the response or an inappropriate response from the
participant.
The proportion of NS responses differs markedly
between the two countries. 852 of the responses in
the India data set (17% of the total) were labeled as
NS, compared to 1548 responses (28%) in the China
data set.
Table 1 provides the different types of NS re-
sponses that were annotated by the raters, along with
the relative frequency of each NS category com-
pared to the others.
Excluding the category ?Other?, background
noise, non-responses, and unrelated topic were the
most frequent types of NS response for both data
sets. However, the relative proportions of each type
differed somewhat between the two countries. For
example, the most frequent NS type in India was
background noise; 33% of NS responses were of this
type, 1.7 times higher than in China.
The proportion of unrelated topic responses was
NS Type India (%) China (%)
Background noise 33.2 19.6
Other 25.0 15.4
Unrelated response 18.9 40.1
Non-response 10.6 8.8
Non-English speech 4.9 6.4
Too soft 2.8 1.0
Background speech 2.0 1.9
Missing samples 1.5 4.0
Too loud 0.8 0.1
Cheating 0.3 2.7
Table 1: Different types of NS responses and their relative
frequency, in % of all NS for each country (ranked by
frequency of occurrence in India)
Data Partition
India China
# of re-
sponses
NS
(%)
# of re-
sponses
NS
(%)
SS-train 1114 31.6 1382 32.2
SS-eval 1271 27.5 1391 33.8
RS-train 1253 8.0 1392 22.4
RS-eval 1275 4.8 1400 22.9
Table 2: Item-type specific training and evaluation data
also high in both countries, but it was much higher
in the China data set: it was 19% in the responses
from India and 40% for China (more than twice as
high as in India). All responses which were not di-
rectly related to the prompt fell into this category.
For SS items, the majority included responses about
a different topic. For RS items, responses in which
the speakers read different prompts were classified
into this category.
The responses were divided into training and test-
ing for NS response detection. Due to the significant
difference in the proportion of NS responses and rel-
ative frequencies of NS types in the two data sets, fil-
tering models were trained separately for each coun-
try. In addition, since the proportions of NS re-
sponses and the available features varied according
to the item type, training and testing data were fur-
ther classified by item types. The proportions of NS
responses and the sizes of the partitions, along with
the percent of NS responses in each item type, are
shown in Table 2.
154
The partitions for testing the filtering model were
selected to maximize the number of speakers with
complete sets of responses; however, this constraint
was not able to be met for the training partitions in
the India data set (due to insufficient data). This ex-
plains the lower proportion of NS responses in the
India test partitions, since speakers with complete
sets of responses were less likely to provide bad re-
sponses. As Table 2 shows, NS responses were more
frequent among SS items than RS items: the pro-
portion of NS responses in SS items was four times
higher than in RS items in India and 1.5 times in
China.
4 Method
4.1 Overview
In this study, two different sets of features were used
in the model training process; ASR-based features
and non-ASR based features. For each item-type,
an item-type-specific filtering model was developed
using these two sets of features.
4.2 Feature generation
4.2.1 ASR based features
For this feature set, we used the features from
an automated speech proficiency scoring system.
This scoring system used an ASR engine containing
word-internal triphone acoustic models and item-
type-specific language models. Separate acoustic
models were trained for the data sets from the two
countries. The acoustic training data for the two
models consisted of 45.5 hours of speech from In-
dia and 123.1 hours of speech from China. In addi-
tion, separate language models were trained for the
SS and RS items for each country; for the RS items,
the language models also incorporated the texts of
the prompts.
A total of 61 features were available. Among
these features, many features were conceptually
similar but based on different normalization meth-
ods. These features showed a strong intercorrela-
tion. For this study, 30 features were selected and
classified into four groups according to their char-
acteristics: basic features, fluency features, ASR-
confidence features, and Word Error Rate (WER)
features.
The basic features are related to power and pitch,
and they capture the overall distribution of pitch and
power values in a speaker?s response using mean and
variance calculations. These features are relevant
since NS responses may have an abnormal distribu-
tion in energy. For instance, non-responses contain
very low energy. In order to detect these abnormal-
ities in speech signal, pitch and power related fea-
tures were calculated.
The fluency features measure the length of a re-
sponse in terms of duration and number of words.
In addition, this group contains features related to
speaking rate and silences, such as mean duration
and number of silence. In particular, these features
are effective in identifying Non-responses which
contain zero or only a few words.
The ASR-confidence group contains features pre-
dicting the performance of the speech recognizer.
Low speech recognition accuracy may be indicated
by low confidence scores.
Finally, the WER group provides features esti-
mating the similarity between the prompts and the
recognition output. In addition to the conventional
word error rate (WER), term error rate (TER) was
also implemented for the filtering model. TER is
a metric commonly used in spoken information re-
trieval, and it only accounts for errors in content
words. This measure may be more effective in iden-
tifying NS responses than conventional WER; for in-
stance, the overlap in function words between off-
topic responses and prompts can be correctly ig-
nored. TER was calculated according to the follow-
ing formula:
dif(Wc) =
{
0 ifCref (Wc) < Chyp(Wc)
Cref (Wc)? Chyp(Wc) otherwise
TER =
?
c?WC
dif(Wc)
?
c?WC
Cref (Wc)
(1)
where Cref (Wc) is the number of occurrences of
the word Wc in reference, Chyp(Wc) is the number
of occurrences of the word Wc in hypothesis, and
WC is the set of content words in reference.
Formula 1 differs from the conventional method
155
Group List of features
Basic mean/standard deviation/minimum/maximum of power, difference between maxi-
mum and minimum in power, mean/standard deviation/minimum/maximum of pitch,
difference between maximum and minimum in pitch
Fluency duration of whole speech part, number of words, speaking rate (word per sec),
mean/standard deviation of silence duration, number of silences, silences per sec and
silences per word
ASR score mean of confidence score, normalized Acoustic Model score by word length, normal-
ized Language Model score by number of words
Word Error Rate the word accuracy between prompt and ASR word hypothesis, correct words per
minute, term error rate
Table 3: List of ASR based features
of calculating TER in two ways. Firstly, content
words which occurred only in the word hypothesis
are ignored in the formula. Secondly, if a word oc-
curred in the word hypothesis more frequently than
in the reference, the difference is ignored. These
modifications were made to address characteristics
of the responses in the data. On the one hand, speak-
ers occasionally inserted a few words such as ?too
difficult? at the end of a response. In addition, a few
speakers repeated words contained in the prompt
multiple times. The two modifications to TER ad-
dress both of these issues.
All features from the four groups are summarized
in Table 3.
4.2.2 Non-ASR based features
A total of 12 features from four different groups
were implemented using non-ASR based methods
such as VAD and SNR. These features are listed in
Table 4.
Feature Category Feature
VAD proportion of voiced
frames in response, num-
ber and total duration of
voiced regions
Syllable number of syllables
Amplitude maximum, minimum,
mean, standard deviation
SNR SNR, speech peak
Table 4: List of non-ASR based features
VAD related features were implemented using the
ESPS speech analysis program. For every 10 mil-
lisecond interval, the voice frame detector deter-
mined whether the interval was voiced or not. Three
features were implemented using this voiced interval
information: the number of voiced intervals, ratio of
voiced intervals in the entire response, and the total
duration of voiced intervals.
In addition, the number of syllables was estimated
based on the flow of energy. The energy of the syl-
lable tends to reach its peak in the nucleus and the
dip in the boundaries. By counting the number of
such fluctuations in energy measurements, the num-
ber of syllables can be estimated. The Praat script
from De Jong and Wempe (2009) was used for this
purpose.
In order to detect the abnormalities in energy, am-
plitude based features were calculated. These fea-
tures were similar to the basic features in ASR based
features.
Finally, if a response contains loud background
noise, the ratio of speech to noise is low. SNR, the
mean noise level, and the peak speech level were
computed using the NIST audio quality assurance
package (NIST, 2009).
The VAD and syllable feature groups were de-
signed to estimate the number of syllables, the pro-
portion of speech to non-speech, and the total dura-
tion of speech intervals. These features were similar
to the number of words and duration of speech fea-
tures in the ASR-based feature set. Despite the con-
ceptual similarity, these features were implemented
since the two types of features were calculated us-
ing different characteristics of the spoken response.
156
The VAD and syllable features are based on the flow
of energy and the zero crossing rate and the ASR-
based features are based on the speech recognition.
In particular, the speech recognizer tends to gener-
ate word hypotheses even for responses that contain
no speech input, but VAD does not have such a ten-
dency. Due to this difference, VAD based features
may be more robust in the responses with no valid
speech.
4.3 Model building
For each response, both ASR features and non-ASR
features were calculated. In contrast to non-ASR
features, which were available for all responses,
ASR features (except the Basic group) were un-
available for some responses, namely, responses for
which the ASR system did not generate any word
hypotheses because no tokens received scores above
the rejection threshold. This causes a missing value
problem; about 7% of the responses did not have a
complete set of attributes.
Missing values are a common problem in machine
learning. One of the popular approaches is to replace
a missing value with a unique value such as the at-
tribute?s mean. Ding and Simonoff (2008) proposed
a method that replaces a missing value with an arbi-
trary unique value. This method is preferable when
missing of a value depends on the target value and
this relationship holds in both training and test data.
In this study, the missing values were replaced
with unique values due to the relationship between
the missing values and the target label; if the speech
recognizer did not produce any word hypotheses, the
response was highly likely to be a NS response. 63%
of the responses where the speech recognizer failed
to generate word hypotheses were NS responses.
Since all ASR-based features were continuous val-
ues, we used two real values: 0.0 for fluency features
and ASR features and 100.0 for word error rate fea-
tures. The fluency features and ASR features tend to
be 0.0 while the word error rate features tend to be
100.0 when the responses are NS responses.
A total of 42 features were used in the model
building. The only exception was WER; since WER
features were only available for the model based
on recited speech, they were calculated only for RS
items. Decision tree models were trained using the
J48 algorithm (WEKA implementation of C4.5) of
WEKA machine learning toolkit (Hall et al, 2009).
5 Results
For each item-type, three models were built to in-
vestigate the impact of each feature group: a model
using non-ASR features, a model using ASR fea-
tures, and a model using both features (the ?Com-
bined? model). Tables 5 and 6 present the accuracy
of the SS models and Tables 7 and 8 present the ac-
curacy of the RS models. In all tables, the base-
line was calculated using majority voting, and rep-
resented a system in which no responses were clas-
sified as NS; since the majority class was scorable,
the baseline using the majority voting did not predict
any response as non-scorable response. Therefore,
precision, recall, F-score are all 0 in this case.
Model Acc. Pre. Rec. F-score
Baseline 72.5 0 0 0
Non-ASR 77.0 0.645 0.364 0.465
ASR 79.0 0.683 0.444 0.538
Combined 78.6 0.657 0.461 0.542
Table 5: Performance of the SS model in India
Model Acc. Pre. Rec. F-score
Baseline 66.2 0 0 0
Non-ASR 68.9 0.601 0.240 0.343
ASR 72.9 0.718 0.326 0.448
Combined 72.9 0.720 0.323 0.446
Table 6: Performance of the SS model in China
Model Acc. Pre. Rec. F-score
Baseline 94.8 0 0 0
Non-ASR 95.7 0.684 0.210 0.321
ASR 97.2 0.882 0.484 0.625
Combined 96.8 0.769 0.484 0.594
Table 7: Performance of the RS model in India
In both item-types, the models using ASR-based
features achieved the best performance. The SS
model achieved 79% accuracy in India and 73% ac-
curacy in China, representing improvements of ap-
proximately 7% over the baseline. In both data sets,
the RS model achieved high accuracies: 97% accu-
racy in India and 96% accuracy in China. In India,
157
Model Acc. Pre. Rec. F-score
Baseline 77.1 0 0 0
Non-ASR 78.3 0.555 0.268 0.361
ASR 95.6 0.942 0.860 0.899
Combined 95.1 0.912 0.872 0.892
Table 8: Performance of the RS model in China
this represents a 2.4% improvement over the base-
line. Although the absolute value of this error re-
duction is not very large, the relative error reduc-
tion is 46%. In China, the improvement was more
salient; there was 18% improvement over baseline,
corresponding to a relative error reduction of 78%.
Additional experiments were conducted to deter-
mine the robustness of the filtering models to evalu-
ation data from a country not included in the train-
ing data. The evaluation sets from both item types
(SS and RS) in both countries (India and China)
were processed using three different models: 1) a
model trained using the ASR-based features for the
responses from the same country (the ?Same? con-
dition, whose results are identical to the ?ASR? re-
sults in Tables 5 - 8), 2) a model trained using the
ASR-based features for the responses from the other
country (the ?Different? condition), and 3) a model
trained using the ASR-based features for the re-
sponses from both countries (the ?Both? condition).
Table 9 presents the accuracy results for these four
sets of experiments.
Model
India China
SS RS SS RS
Same 79.0 97.2 72.9 95.6
Different 80.1 95.4 73.5 93.8
Both 80.0 96.5 74.0 95.9
Table 9: Accuracy results using training and evaluation
data from different countries
These results show that the models are quite ro-
bust to evaluation data from a different country. In
all cases, there is at most a small decline in perfor-
mance when training data from the other country is
used (in the case of the SS responses, there is even a
slight increase in performance). Table 9 also shows
that the RS models performed worse in the Different
Country condition (compared to the Same Country
condition) than the SS models. This difference is
likely due to the difference in the number of NS re-
sponses among the RS data in the two countries (as
shown in Table 2). However, the decline is still rel-
atively small, suggesting that it would be reasonable
to extend the filtering models to responses from ad-
ditional countries that were not seen in the training
data.
6 Discussion
Approximately 40 features were available for the
model building, but not all features had a signifi-
cant impact on the detection of NS responses. For
each item-type, the importance of features were fur-
ther investigated using a logistic regression analysis.
The training data of India and China were combined,
and a stepwise logistic regression analysis was per-
formed using the SPSS statistical analysis program.
For each item-type, the top 3 features are pre-
sented in Table 10; the features are presented in the
same order selected in the models.
Model RS SS
ASR TER,
speaking
rate, s.d. of
pitch
mean of confi-
dence scores,
speaking rate,
s.d. of power
Non-ASR number of
syllables,
number
and du-
ration of
voiced
regions
number of sylla-
bles, s.d. and
mean of ampli-
tude
Combined TER,
speaking
rate, s.s.dd.
pitch
mean of confi-
dence scores,
speaking rate,
number of
voiced regions
Table 10: Top 3 features in stepwise logistic regression
model
For the RS items, TER was the best feature and it
was the top feature for both the ASR feature based
model and the combined model. The top 3 features
in the combined model were the same as the ASR
feature based model, and non-ASR features were not
158
selected. In non-ASR based features, the number of
syllables was the best feature, followed by the VAD
based features.
For the SS items, the top 2 features were the same
in both the ASR feature based model and the com-
bined model. The combined model selected one
non-ASR based feature, namely, a VAD based fea-
ture. As with the RS items, the number of syllables
was the best feature, followed by the energy related
feature.
These results show the importance of WER fea-
tures. Most of the current features are designed for
signal level abnormalities such as responses with
large background noise or non-responses. For in-
stance, fluency features and VAD features are effec-
tive for non-response detection, since they can deter-
mine whether the responses contain valid speech or
not. SNR and pitch/power related features are use-
ful for identifying responses with large background
noise. However, no features except the WER group
can identify content-level abnormalities such as un-
related topic and non-English responses. The high
proportion of these two types of responses (24%
in India and 46% in China) may be the major ex-
planation for the lower accuracy of the model for
SS responses than for RS responses. In the future,
content-related features should also be developed for
spontaneous speech.
The features selected the first time in the logis-
tic model differed according to item-types. The re-
sults support the item-type-specific model approach
adopted in this paper; item-type-specific models can
assign strong weights to the item-type-specific fea-
tures that are most important.
As shown in Tables 5 - 8, the combination of non-
ASR and ASR features could not achieve any fur-
ther improvement over the model consisting only of
ASR based features. However, in all cases, the non-
ASR based model did lead to some improvement
over the baseline. The magnitude of this improve-
ment was greater in SS items than RS items; in par-
ticular, it was greatest among the SS items in the
India data set. This difference may be due to the dif-
ferent distributions of the NS types among the data
sets. The non-ASR based features can cover only
limited types of NS responses such as non-responses
and responses with background noise, and the pro-
portion of these types is much higher among the SS
responses from India.
In addition, in RS items, the poor performance of
the combined model may be related to the high per-
formance of TER. The stepwise regression analysis
showed that the combined model did not select any
of non-ASR based features.
7 Conclusion
In this study, filtering models were implemented as a
supplementary module to an automated proficiency
scoring system. Due to the difference in the avail-
able features and proportion of NS responses, item-
type specific models were trained.
The item-types heavily influenced the overall
characteristics of the filtering models. First, the pro-
portion of NS responses was significantly different
according to item-type; it was much higher in spon-
taneous speech items than recited speech items. Sec-
ondly, the word error rate feature group was only
available for recited speech. Although the word er-
ror rate feature group contained three features, they
improved the performance of the filtering model sig-
nificantly.
ASR feature based models outperformed non-
ASR feature based models, but non-ASR based fea-
tures may be useful for new tests. Finally, experi-
ments demonstrated that the country-specific mod-
els using the ASR-based features are relatively ro-
bust to responses from a different country. This re-
sult suggests that this approach can generalize well
to speakers from different countries.
In this study, large numbers of features (42 for RS
items and 39 for SS items) were used in the model
training, but some features were conceptually simi-
lar and not all of them were significantly important;
the logistic regression analsysis using traning data
showed that there was no significant improvement
after selecting 5 features for RS items and 13 fea-
tures for SS items. Use of non-significant features
in the model training may result in the overfitting
problems. In future research, the features will be
classified into subgroups based on their conceptual
similarities; groups of features with high intercorre-
lations will be reduced to include only the best per-
forming feature in each group. Thus, based on care-
ful pre-selection procedures, only high performing
features will be selected, and the model will be re-
159
trained.
In addition, many different types of NS responses
were lumped into one big category (NS); this may
increase the confusion between scorable and non-
scorable responses and decrease the model?s perfor-
mance. Some of NS types have very different char-
acteristics compared to other NS types and this fact
caused critical differences in the feature values. For
instance, non-responses contained zero or close to
zero words, whereas non-English responses and off-
topic responses typically had a word count similar to
scorable responses. This difference may reduce the
effectiveness of this feature. In order to avoid this
type of problem, we will classify NS types into small
numbers of subgroups and build a seperate model for
each subgroup.
References
Joon-Hyuk Chang and Nam Soo Kim. 2003. Voice ac-
tivity detection based on complex Laplacian model.
Electronics Letters, 39(7):632?634.
Joon-Hyuk Chang, Nam Soo Kim, and Sanjit K. Mitra.
2006. Voice activity detection based on multiple sta-
tistical models. IEEE Transactions on Signal Process-
ing, 54(6):1965?1976.
Nivja H. De Jong and Ton Wempe. 2009. Praat script
to detect syllable nuclei and measure speech rate au-
tomatically. Behavior research methods, 41(2):385?
390.
Yufeng Ding and Jeffrey S. Simonoff. 2008. An investi-
gation of missing data methods for classification trees.
Statistics Working Papers Series.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
In SIGKDD Explorations, volume 11.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech and Language, 25:282?
306, April.
Guojun Lu and Templar Hankinson. 1998. A technique
towards automatic audio classification and retrieval.
In Proceedings of the 4th International Conference on
Signal Processing, volume 2, pages 1142?1145.
NIST. 2009. The NIST SPeech Quality Assurance
(SPQA) Package Version 2.3. from http://www.
nist.gov/speech/tools/index.htm.
Jong Won Shin, Hyuk Jin Kwon, Suk Ho Jin, and
Nam Soo Kim. 2005. Voice activity detection based
on generalized gamma distribution. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 781?784.
Jongseo Sohn, Nam Soo Kim, and Wonyong Sung.
1999. A statistical model-based voice activity detec-
tion. IEEE Signal Processing Letter, 6(1):1?3.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
160
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 86?94,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
     Using an Ontology for Improved Automated Content Scoring of Spontaneous Non-Native Speech   Miao Chen Klaus Zechner School of Information Studies Educational Testing Service Syracuse University 660 Rosedale Road Syracuse, NY 13244, USA Princeton, NJ 08541, USA mchen14@syr.edu kzechner@ets.org   Abstract 
This paper presents an exploration into auto-mated content scoring of non-native sponta-neous speech using ontology-based information to enhance a vector space ap-proach. We use content vector analysis as a baseline and evaluate the correlations between human rater proficiency scores and two co-sine-similarity-based features, previously used in the context of automated essay scoring. We use two ontology-facilitated approaches to improve feature correlations by exploiting the semantic knowledge encoded in WordNet: (1) extending word vectors with semantic con-cepts from the WordNet ontology (synsets); and (2) using a reasoning approach for esti-mating the concept weights of concepts not present in the set of training responses by ex-ploiting the hierarchical structure of WordNet. Furthermore, we compare features computed from human transcriptions of spoken respons-es with features based on output from an au-tomatic speech recognizer. We find that (1) for one of the two features, both ontologically based approaches improve average feature correlations with human scores, and that (2) the correlations for both features decrease on-ly marginally when moving from human speech transcriptions to speech recognizer output. 1 Introduction Currently, automated speech scoring systems mainly utilize features related to the acoustic as-pects of a spoken response of a test taker, for ex-ample, fluency, pronunciation, and prosody features (Cucchiarini et al, 2000, 2002; Franco et al, 2010; Zechner et al, 2009). In terms of the 
content aspect of speech, for highly predictable speech, such as reading a passage aloud, scoring of content reduces to measuring the reading accuracy of the read passage which is typically achieved by computing the string edit distance between the tar-get passage and the actual text read by the test tak-er, using the speech recognizer hypothesis as a proxy (Alwan et al, 2007; Balogh et al, 2007). For high entropy speech whose content is difficult to predict such as spontaneous speech in this study, on the other hand, content scoring has not been investigated much so far, mostly due to the diffi-culty of obtaining accurate word hypotheses for spontaneous non-native speech by Automated Speech Recognition (ASR) systems. In this paper, we use spoken responses from an English language spoken proficiency test where candidates, all non-native speakers of English, re-spond to four different prompts1 with a speaking time of one minute per response. For this study, we decide to use a baseline ap-proach for content scoring of spontaneous speech that was previously employed for a similar task in the context of automated essay scoring (Attali & Burstein, 2006), namely Content Vector Analysis (CVA) where every document is represented as a vector of word weights, based on their frequencies in a document or document collection. However, there are two issues with the CVA vector of words representation that we want to address with this study: (1) Similar words are treated in isolation and not grouped together. Words with similar meaning should be treated in the same way in an automated scoring system, so grouping word synonyms into semantic concepts can help with this issue. (2) The vector of word representation is based on an exist-                                                            1 Prompts are test tasks assigned to test takers to elicit spoken responses. 
86
ing corpus of training documents. When encoun-tering a word or concept in a test document that is not contained in the training set, it is difficult to decide the relevance of that word or concept.  We propose to use ontology-facilitated ap-proaches as solutions to these two issues, aiming at enriching speech content representations to im-prove speech content scoring.  Specifically, to ad-dress issue (1), we represent speech content by concept-level vectors, using the synsets (lists of synonymous words) of the WordNet ontology (Fellbaum, 1998; WordNet 3.0, 2010). As for issue (2), we expand the vector representation by infer-ring the importance (weight) of concepts not pre-sent in the training vectors based on their path distance to known concepts or words in the hierar-chical structure of the WordNet ontology. Since we only look at the content aspect of speech without considering the acoustic features in this study, we work on speech transcripts exclu-sively, both from human transcribers as well as from a state-of-the-art automated speech recogni-tion system, and compare results between the ideal human transcripts and the imperfect transcripts generated by the speech recognizer. For the pur-pose of simplified illustration, speech transcripts are often referred to as ?documents? in the paper as they are a special type of textual documents. The remainder of this paper is organized as fol-lows: in Section 2, we review related research in content scoring of texts, particularly student es-says; Section 3 describes the data set we use for this study and the ASR system; and Section 4 pre-sents the ontologically-facilitated methods we are using in detail. In Section 5, we present our exper-iments along with their results, followed by a dis-cussion in Section 6, and we conclude the paper with a summary and outlook in Section 7. 2 Related Work  There have been some effective approaches for test takers? written responses in language tests, namely in the area of Automated Essay Scoring (AES). AES has employed content vector analysis, i.e., vectors of words to represent text, for example, the e-rater system (Burstein, 2003; Attali & Burstein, 2006) and the experimental system in Larkey and Croft (2003). Representations in the BETSY sys-tem (Bayesian Essay Test Scoring System) also involve words, such as the frequency of content 
words, and also include specific phrases as well (Dikli, 2006). AES has also used latent concepts for text representation, such as the Intelligent Es-say Assessor system (Landauer et al, 2003). The latent concepts are generated by a statistical ap-proach called Latent Semantic Analysis (LSA), which constructs a semantic vector space and pro-jects essays to the new space.  Representing texts by vectors of words has also been a common practice in many research areas beyond AES, including information retrieval (Sal-ton et al, 1975; Croft et al, 2010). One of its weaknesses, however, is its difficulty in addressing issues such as synonyms and related terms. Differ-ent words, such as lawyer, attorney, counsel etc. can share similar meaning, while in a word vector representation they are treated as different dimen-sions; however, because they are conceptually sim-ilar, it makes more sense to group them into the same vector dimension. Ontologies are in a good position to resolve this issue because they organize words and terms under structured concepts, group terms with similar meaning together and also maintain various semantic relations between con-cepts. Therefore, text can be represented on a con-cept level by using ontology concepts as features. Recognizing concepts in documents can further reveal semantic relations between documents (Hotho et al, 2003a), thus can facilitate further text-related tasks such as clustering, information retrieval, as well as our speech scoring task. This type of representation has been tried in several studies (e.g., Hotho et al, 2003a; Hotho et al, 2003b; Bloehdorn & Hotho, 2004).  Hotho et al (2003a; 2003b) use ontology con-cepts to represent text and use the representation for document clustering. The studies employ the WordNet ontology, a general domain ontology. The experiments test three parameters of using an ontology for text representation: (1) whether con-cept features should be used alone or replace word features or be used together with word features; (2) word sense disambiguation strategies when using concepts; and (3) investigating the optimal level of word generalization in terms of the hierarchical structure of the ontology, i.e., how general the con-cepts should be. Some options of the first two pa-rameters will be implemented and tested in our experiment design below.  The vector representation approach of text doc-uments, either using words or concepts, can be 
87
used to measure the content similarity between essays. E-rater, for example, measures the similari-ty between test essays and training essays by com-puting the cosine similarity of their word vectors and by generating two content features based on this similarity metric. It uses multiple regression as its final scoring model, using both content features, as well as features related to other aspects of the essay, such as grammar and vocabulary usage (Burstein, 2003; Attali & Burstein, 2006). Intelli-gent Essay Assessor also employs cosine similarity between to-be-scored essays and training essays as basis of one content feature, and models the scor-ing process by normalization and regression analy-sis (Landauer et al, 2003). The IntelliMetric system uses a nonlinear and multidimensional modeling approach to reflect the complexity of the writing process as opposed to the general linear model (Dikli, 2006). Larkey and Croft (2003) em-ploy Bayesian classifiers for modeling, which is a type of text categorization technique. It treats essay scoring as a text categorization task, the purpose of which is to classify essays into score categories based on content features (i.e., if the scores range from 1-4, then there are four score categories).     Zechner and Xi (2008) report on experiments related to scoring of spontaneous speech responses where content vector analysis was used as one of several features in scoring models for two different item types. They found that while these content features performed reasonably well by themselves, they were not able to increase the overall scoring model performance over a baseline that did not use content features.      This paper will use CVA as a baseline for our experiment and investigate two ontology-based approaches to enhance the content representation and improve content feature performance. 3 Data  We use data from a test for English proficiency for non-native speakers of English. Candidates are asked to provide spontaneous speech responses to four prompts, with each of the responses being one minute in length. The four prompts are all integrat-ed prompts, meaning candidates are first given some materials to read or listen and then are asked to respond with their opinions or arguments to-wards the materials. The responses are scored ho-listically by human raters on a scale of 1 to 4, 4 
being the highest score. For holistic scoring, the human raters use a speech scoring rubric as the guideline of expected performance on aspects such as fluency, pronunciation, and content for each score level. Our data set contains 1243 speech samples in to-tal as responses to four different prompts, obtained from 327 speakers (note that not all speakers re-sponded to all prompts). Each response is verbatim transcribed by a human transcriber. The responses are grouped by their prompts since our experi-ments are prompt-specific. For responses of each prompt, we randomly split the responses into a training set (44%) and a test set (56%), making sure that response scores are distributed in a simi-lar proportion in both training and test sets. Each response is considered as a single document here. Table 1 shows the size of the two data sets. Prompt Training Set Test Set Total A 143 176 319 (4/79/158/78) B 140  168 308 (7/86/146/69) C 139  172 311 (4/74/154/79) D 137  168 305 (8/75/141/81) Table 1. Size of training and test data sets. The numbers in parentheses are the number of documents on score levels 1-4.  The training set is used for generating repre-sentative vectors of a prompt on different score levels, which are to be compared with test docu-ments. The test set is primarily used to compute content features for test documents and examine performance of approaches under different exper-iment setups. Besides human transcriptions of the speech files, we also obtained ASR output of the files, in order to examine performance of the proposed approach-es on imperfect output, in a fully automated opera-tional scenario where no human transcribers would be in the loop. Since the training set is used for deriving representative vectors for the four differ-ent prompts and we would like to generate accurate vectors based on human transcriptions, we do not use a separate training set for ASR data. Thus, we only obtain corresponding ASR output for the test set of each prompt. The ASR system we use for our experiments in this paper is a state-of-the-art gender-independent continuous density Hidden Markov Model speech recognizer, trained on about 30 hours of non-native 
88
spontaneous speech. Its word error rate on the test set used here is about 12.8%. 4 Method  We employ one baseline approach for word-level features and two experimental approaches for con-cept-level features to examine the effect of the WordNet ontology and concept-level features on content feature correlations. 4.1 Baseline Approach: Content Vector Analysis (CVA) We decide to use the two content features used by e-rater based on CVA analysis, called ?max.cos? and ?cos.w4? here (Attali & Burstein, 2006). The assumption behind this approach is that essays with similar human scores contain similar words; thus, they should share similar vector representa-tions in CVA. For our data, this assumption is held for the spoken test documents in the same way. Moreover, we conjecture this assumption is mostly true for high score responses as opposed to low score responses, because we expect high vocabu-lary uniformity in high score responses and more irrelevant and more diverse vocabulary in low score responses.  Before feature computation, some preprocessing is conducted on the speech transcripts. For each prompt, we group its training set into four groups according to their score levels (?score-level docu-ments?). Then we use the score-level documents of each prompt to generate a super vector as a repre-sentation for documents on this score level of this specific prompt. As a result, we have four score-level vectors under each prompt, generated from their training sets. While the score-level training vectors are produced using multiple documents of the same score level, vectors of test documents are generated on an individual document level. Given a test document that needs to be scored, we first convert it into the vector representation. Then we are ready to compute the two content features. Equation 1 provides the exact formula for the co-sine similarity measure used in all of our methods. 
(1)	 ? 	 ?
where n is the number of words and/or concepts in the score-level vector (from the training set docu-ments),  ?w ? ,?  are the word or concept weights of a score-level vector and w?,? are the word or concept weights of a test document (response transcrip-tion). ??,? are computed by term frequency and ?? ,? are computed in the same way after concatenating documents of the same score level as one large document. The max.cos feature. This feature measures which score level of documents the test document is most similar to in vector space by computing the cosine similarity with each score-level vector and then selecting the score level which has the largest cosine similarity to the test vector as feature value. Thus, this feature assumes integer values from 1 to 4 only. The cos.w4 feature2.  This feature measures con-tent similarity between the test document and the best quality documents in vector space. Since score 4 is the highest level in our data set of spoken re-sponses, we compute the cosine similarity between the test vector and the score level 4 vector as an indicator of how similar the test document is to the speech content of the test takers with highest profi-ciency.  The two features are evaluated based on their Pearson r correlation to human assigned scores. We evaluate the features in all experiments, as a way to observe how the two features? predictive-ness varies among different experiment setups. Note that since the max.cos feature assumes inte-ger values but the cos.w4 feature is real valued, we expect correlations to be higher for cos.w4 due to this difference, all other things being equal. 4.2 Ontology-facilitated Approaches We use two ontology-facilitated document repre-sentation approaches, which represent documents based on the WordNet ontology. The first approach matches words in a document to concepts and rep-resents documents by vectors of concepts, whereas the second one addresses the unknown word issue by inferring their weight based on the structure of the WordNet ontology. 
                                                            2 The feature is referred to as ?cos.w/6? in Attali and Burstein (2006) because there are usually 6 score levels, while here our data has 4 score levels therefore it is written as ?cos.w4?. ??? == = ni islni itni islit ww ww 1 2 ,1 2,1 ,,**
89
4.2.1 Ontology-facilitated representation ap-proach This representation uses concepts instead of the words as elements in the document vectors. Given a document, we map words in the document to concepts, using the synsets in WordNet. For exam-ple, chance and opportunity are different words, however they belong to the same WordNet synset (?opportunity.n.01?). This concept-level representa-tion groups words of similar meaning in the same vector dimension, thus making the vector space more succinct and semantically meaningful. The weighting scheme of concepts follows the one in the CVA approach. In this study, we focus on sin-gle words and match them to WordNet synsets; in future work, we consider matching multi-word ex-pressions to ontologies like Wikipedia (Wikipedia, 2011). Experiments show that including words and their corresponding WordNet synsets as vector dimensions has better performance than only in-cluding WordNet synsets for text clustering tasks (Hotho et al, 2003a) and the same result also oc-curs in our preliminary experiments. Therefore, we include both WordNet synsets and words in the vector representation. 4.2.2 Ontology-facilitated reasoning approach This approach is based on the ontology-facilitated representation and goes further to resolve the un-known word issue, i.e., handling words in test doc-uments that have not been seen in the training documents. First, test documents are converted to vectors of concepts plus words. If a concept in the test vector does not appear in the score level vector, its weight therefore is unknown, as well. We then estimate its weight based on structural information contained in the WordNet ontology. More specifically, given an unknown concept in the test document, we find the N most similar concepts to that unknown con-cept from the set of all concepts contained in the score level vector. We use a WordNet-based simi-larity estimate to measure similarity between con-cepts, namely the edge-based Path Similarity, which measures the length of a path from one con-cept to another concept in WordNet by computing the inverse of the shortest path between the two concepts (Pedersen et al, 2004). We submit that the estimated weight of the unknown concept in 
the test document vector should be close to the weights of its most similar concepts in the score level vector derived from the training documents. From this assumption, we propose estimating the unknown concept?s weight by averaging the weights of the N most similar concepts: (2) ??? ?( ??)/?????   with N denoting the number of similar concepts in a score level vector,   ?w? denoting the weights of these similar concepts, and w ??  standing for the resulting concept weight for the unknown concept in a test document. For example, a test document may be ?so radio also create a great impact on this uh people com-munication?. The words are matched to WordNet concepts, and we find that the concept synset ?im-pact.n.01? is an unknown concept to the score level 4 vector. From the dimensions of the score level 4 vector we find these three most similar concepts to the unknown concept: ?happening.n.01?, ?event.n.01?, and ?change.n.01?. We now can aver-age the weights of these three concepts in the score-level vector to use it as a weight estimate for the unknown concept ?impact.n.01?. We want to note that while this approach can es-timate weights for test document words or con-cepts contained in WordNet (but not in the training vectors), it cannot handle words that are not in-cluded in WordNet at all, such as many proper names, foreign words, etc. To address the latter as well, we would have to use a much larger and more comprehensive ontology, e.g., the online en-cyclopedia Wikipedia. 5 Experiments and Results We design experiments according to the above ap-proaches. The first experiment group is the base-line system using two features employed by e-rater, max.cos and cos.w4. The second and third experiment groups implement the two ontology-facilitated approaches, respectively. We first run CVA and compare several different parameter set-ups to optimize them for further experiments. 5.1 Parameter Optimization in CVA Experi-ments For the CVA method, we need to decide (1) which term weighting scheme to use, and (2) whether or not to use a list of stopwords to exclude common 
90
non-content words such as determiners or preposi-tions from consideration. We compare five com-monly used term weighting schemes, each one with or without using a stoplist, based on averaged correlations with human scores across all four prompts. The best results are obtained for the weighting scheme (TF/EDL)*IDF, where TF is the frequency of a term in a document, EDL is the Eu-clidean document length3, and IDF is the inverse document frequency of a term based on a collec-tion of documents. For this scheme, as for most others, there is almost no difference between using vs. not using a stoplist and we decide to use a stoplist for our experiments based on the tradition in the field. The selected term weighting scheme is applied in the same way for both the score-level vectors as well as the test document vectors. 5.2 Experiment Groups 5.2.1 Group 1: CVA As described above, we first convert the training sets to score level vectors and the test documents into test vectors with the TF/EDL*IDF weighting, and compute the max.cos and cos.w4 features for each test document.  5.2.2 Group 2: Ontology-facilitated Representa-tion We first match words in documents to WordNet concepts. There are several ways to achieve this (Hotho et al, 2003a). Given a word, it may corre-spond to multiple concepts in WordNet, in which each possibility is called a ?sense? in WordNet, and we need to decide which sense to use.  WordNet-Sense-1. In this study we employ a simple word sense disambiguation method by us-ing the first sense returned by WordNet. We send a word to WordNet synset search function, which returns all synstes of the word, and we select to use the first result because it is also the most frequently used sense for the word. After obtaining the senses and concepts for the words, the training sets and test documents are                                                             3 Given a vector of raw term frequencies (rtf?, rtf?,? , rtf?), its Euclidean length is computed in this way:  ?
?? ??????  
converted to vectors of WordNet concepts plus words, using TF/EDL*IDF weighting, the same one used by the CVA approach. We compute the max.cos and cos.w4 features in the same way as for the baseline CVA method.  5.2.3 Group 3: Ontology-facilitated Reasoning This approach, called here ?WordNet-Reasoning?, also extracts vectors of WordNet concepts plus words with the same term weighting scheme as before. For matching words to concepts, we still employ the WordNet?Sense-1 sense selection method. For unknown concepts, which appear in a test vector but not in any score level vectors, we infer their weights by using the reasoning approach proposed in section 4.2.2 with N=5 as the number of most similar concepts to the unknown concept4, located in the WordNet hierarchy. The score level vectors are expanded by the inferred unknown concepts. When we obtain the expanded score lev-el vectors, we compute the two content features from the vectors in the same way as before, and finally calculate feature correlations with human scores. 5.3 Results We run the three experiment groups on human and ASR transcriptions respectively and obtain the max.cos and cos.w4 feature values of test docu-ments in the experiments. As stated in 4.1, we compute the correlations between the two features and the human assigned scores for evaluating the approaches. Tables 2 and 3 (next page) list correlations of the two content features with human scores under different experiment setups. Significant differences on individual prompts between correlations of the two WordNet-based methods WordNet-Sense-1 and WordNet-Reasoning and the CVA baseline are denoted with * (p<0.05) and ** (p<0.01).  
                                                            4 We manually inspected some of the similar concepts of the unknown concepts and found the first 5 similar concepts were relevant to the unknown concepts, and thus made the decision of N=5. 
91
Prompt Hum, CVA Hum, WordNet-Sense-1 Hum, Word-Net-Reasoning ASR, CVA ASR, Word-Net-Sense-1 ASR, Word-Net-Reasoning A 0.320 0.333 0.038** 0.293 0.286 0.014** B 0.348 0.352 0.350 0.308 0.338 0.339 C 0.366 0.373 0.074** 0.396 0.386 0.106** D 0.343 0.323 0.265 0.309 0.309 0.265 Average 0.344 0.345 0.182 0.327 0.330 0.181 Table 2. Correlations between the max.cos feature and human scores (Hum=using human transcriptions; ASR=using ASR hypotheses).  Prompt Hum, CVA Hum, WordNet-Sense-1 Hum, Word-Net-Reasoning ASR, CVA ASR, Word-Net-Sense-1 ASR, Word-Net-Reasoning A 0.427 0.429 0.434 0.409 0.416 0.411 B 0.295 0.303 0.327* 0.259 0.278 0.292* C 0.352 0.385* 0.402** 0.338 0.366 0.380** D 0.368 0.385 0.389 0.360 0.379 0.374 Average 0.360 0.376 0.388 0.342 0.360 0.364 Table 3. Correlations between the cos.w4 feature and human scores (Hum=using human transcriptions; ASR=using ASR hypotheses) 6 Discussion 6.1 Results on Human Transcriptions On human transcriptions, Table 2 shows that the max.cos feature correlations increase, albeit not significantly, when using the method WordNet?Sense-1 on all prompts except for prompt D but decrease sometimes significantly when using the WordNet-Reasoning approach. The cos.w4 feature correlations, on the other hand, exhibit constant increases on all four prompts when using WordNet-Sense-1 and the increase on prompt C is significant. The average correlations further increase for all prompts when using WordNet-Reasoning and the increase is sig-nificant on prompts B and C (Table 3).  6.2 Results on ASR Output On the ASR output, for the max.cos feature, the average correlation barely changes when using the WordNet-Sense-1 method but decreases when us-ing WordNet-Reasoning with significant decrease on prompts A and C (Table 2).  For the cos.w4 feature, however, WordNet-Sense-1 improves correlations on all four prompts with 0.018 correlation increase on average but in-creases are not statistically significant on a prompt level. WordNet-Reasoning does not further im-prove correlations much beyond the correlations of WordNet-Sense-1, with a further 0.004 increase in 
average correlation. Compared to CVA, though, correlations for WordNet-Reasoning are signifi-cantly higher on prompts B and C (Table 3). 6.3 Overall Discussion Based on these observations, we find that for cos.w4, the WordNet-Sense-1 approach can im-prove average correlations compared to the CVA baseline on both ASR and human transcriptions. Hence, the extension of the document vectors by WordNet synsets has a positive impact on the ac-curacy of content scoring of the spoken responses by non-native speakers. Again looking at the cos.w4 feature, while the WordNet Reasoning approach works well on hu-man transcriptions to further improve correlations compared to WordNet-Sense-1, it does not consist-ently improve correlations on ASR output. This may indicate that WordNet-Reasoning is more sen-sitive to ASR errors than WordNet-Sense-1. For the max.cos feature, the correlation of WordNet-Reasoning decreases significantly from WordNet-Sense-1 on prompts A and C for both human and ASR transcriptions; moreover, in the WordNet-Reasoning approach the max.cos correla-tions vary greatly on the four prompts (Table 2). We conjecture that one reason for this finding may lie in the rather small sample size of the data set, as this is an exploratory study, and the differences across prompts may be smaller when using a sub-stantially larger data set. 
92
Comparing the average reduction in correlation between human and ASR transcriptions, we find an absolute drop in correlations of 0.017 between the CVA baseline for the max.cos and of 0.019 for the cos.w4 feature. Looking at the WordNet-Sense-1 approach for the cos.w4 feature, the average corre-lation of 0.376 for human transcriptions is reduced by 0.016 to 0.360 for ASR hypotheses. Hence, we observe that the imperfect speech recognition out-put does not cause a major degradation for this content feature; the degradations observed are all in the range of 5% relative (the ASR word error rate on the test set is about 13%.) Overall, the ontology-facilitated approaches are effective for the cos.w4 feature and seem to be less appropriate for the max.cos feature. We conjecture that the characteristics of the max.cos feature may be the reason for the poor performance of the on-tology-facilitated approaches on this feature. To compute this feature, we need to compare a test vector with vectors for each score level, and it is assumed that these vectors are representative vec-tors for documents at these score levels. In reality though, while the score level 4 vector is quite a good representative for the prompt topic (highest proficiency speakers), score level vectors of less proficient speakers are less uniform and more di-verse. The reason is that there are only a few ways to appropriately represent the correct topic in a good quality spoken response but there can be many different ways of generating responses that are not on topic. For example, the score level 1 vector contains vectors generated from score 1 documents, whose words are considered mostly irrelevant for the prompt. Then, given a test docu-ment, which also contains irrelevant words for the prompt but with little overlap to the level 1 score vector, the similarity between them would be very small. Thus, any ontological approach has to face this heterogeneous distribution of words in the score level vectors for responses with lower scores; any semantic generalizations are inherently more difficult compared to those on higher scoring re-sponses. For the cos.w4 feature, in contrast, only score level 4 vectors are used, and this problem does not surface here. Finally, we observe that average correlations of both features based on ASR hypotheses (except for WordNet-Reasoning for the max.cos feature) fall in the range of 0.32-0.37. This range is well in line with our better performing features in other dimen-
sions of spontaneous speech responses, e.g., fluen-cy, pronunciation, and prosody. 7 Conclusion and Future Work In this paper, we propose using ontology-facilitated approaches for content scoring of non-native spontaneous speech due to specific merits of ontologies. Two ontology-facilitated approaches are proposed and evaluated, and their results are compared against a CVA baseline. The results in-dicate that the ontology approaches can improve content feature correlations in some circumstances. As a summary, concept-level features and reason-ing-based approaches work well on the cos.w4 content feature where test documents are compared against a vector representing all training set docu-ments with the highest human score. For future work, we plan to investigate more so-phisticated reasoning approaches. For this study, we use a simple averaging method to infer the con-cept importance based on hierarchy-inferred simi-larity metrics. As a next step, we plan to infer weights according to different similarity metrics and differential weighting of the N closest terms. Another avenue for future research is to employ different ontologies, for example, Wikipedia, which contains more concepts and entities than WordNet and has a structure that has grown more organically and less from first principles.  Wikipe-dia also has a larger pool of multi-word expres-sions and we would like to explore how representations based on the Wikipedia ontology affects automated speech scoring performance. References  Alwan, A., Bai, Y., Black, M., Casey, L., Gerosa, M., Heritage, M., & Wang, S. (2007). A system for tech-nology based assessment of language and literacy in young children: The role of multiple information sources. Proceedings of the IEEE International Workshop on Multimedia signal Processing, Greece. Attali, Y., & Burstein, J. (2006). Automated essay scor-ing with e-rater? V. 2. The Journal of Technology, Learning and Assessment, 4(3).  Balogh, J., Bernstein, J., Cheng, J., & Townshend, B.  (2007). Automatic evaluation of reading accuracy: Assessing machine scores. Proceedings of the ISCA-SLaTE-2007 Workshop, Farmington, PA, October. Bloehdorn, S., & Hotho, A. (2004). Boosting for text classification with semantic features. Workshop on mining for and from the semantic web at the 10th 
93
ACM SIGKDD conference on knowledge discovery and data mining (KDD 2004).  Burstein, J. (2003). The E-rater? scoring engine: Au-tomated essay scoring with natural language pro-cessing. In M. D. Shermis, Burstein, J.C. (Ed.), Automated essay scoring: A cross-disciplinary per-spective (pp. 113-121). Mahwah, NJ: Lawrence Erl-baum Associates, Inc. Croft, W. B., Metzler, D., & Strohman, T. (2010). Search engines: Information retrieval in practice. Boston, MA: Addison-Wesley. Cucchiarini, C., Strik, H., & Boves, L. (2000). Quantita-tive assessment of second language learners? fluency by means of automatic speech recognition technolo-gy. Journal of the Acoustical Society of America, 107(2), 989-999. Cucchiarini, C., Strik, H., & Boves, L. (2002). Quantita-tive assessment of second language learners' fluen-cy: Comparisons between read and spontaneous speech. Journal of the Acoustical Society of Ameri-ca, 111(6), 2862-2873. Dikli, S. (2006). An overview of automated scoring of essays. The Journal of Technology, Learning and As-sessment, 5(1), 1-35. Fellbaum, C. (Ed.). (1998). WordNet: An electronic lexical database. Cambridge, MA: The MIT press. Franco, H., Bratt, H., Rossier, R., Gadde, V. R., Shriberg, E., Abrash, V., & Precoda, K. (2010). EduSpeak: A speech recognition and pronunciation scoring toolkit for computer-aided language  learn-ing applications. Language Testing, 27(3), 401-418. Hotho, A., Staab, S., & Stumme, G. (2003a). Ontologies improve text document clustering. Proceedings of the Third IEEE International Conference on Data Min-ing (ICDM?03).  Hotho, A., Staab, S., & Stumme, G. (2003b). Text clus-tering based on background knowledge (Technical report, no.425.): Institute of Applied Informatics and Formal Description Methods AIFB, University of Karlsruche. Landauer, T. K., Laham, D., & Foltz, P. W. (2003). Au-tomated scoring and annotation of essays with the In-telligent Essay Assessor. In M. D. Shermis, Burstein, J.C. (Ed.), Automated essay scoring: A cross-disciplinary perspective (pp. 87?112). Mahwah, NJ: Lawrence Erlbaum Associates, Inc. Larkey, L. S., & Croft, W. B. (2003). A Text Categori-zation Approach to Automated Essay Grading. In M. D. Shermis & J. C. Burstein (Eds.), Automated Essay Scoring: A Cross-discipline Perspective: Mahwah, NJ, Lawrence Erlbaum. Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet:: Similarity: measuring the relatedness of concepts. Proceedings of the Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04).  
Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic indexing. Communica-tions of the ACM, 18(11), 613-620. Wikipedia: The free encyclopedia (2011). FL: Wiki-media Foundation, Inc. Retrieved Apr 26, 2012, from http://www.wikipedia.org WordNet 3.0 Reference Manual. (2010). Retrieved Apr 26, 2012 from http://wordnet.princeton.edu/wordnet/documentation/ Zechner, K., Higgins, D., Xi, X, & D. M. Williamson (2009). Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communi-cation, 51(10), 883-895. Zechner, K., & X. Xi (2008). Towards Automatic Scor-ing of a Test of Spoken Language with Heterogene-ous Task Types. Proceedings of the ACL Workshop on Innovative Use of NLP for Building Educational Applications, Columbus, OH, June.   
94
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 180?189,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Vocabulary Profile as a Measure of Vocabulary Sophistication
Su-Youn Yoon, Suma Bhat*, Klaus Zechner
Educational Testing Service, 660 Rosedale Road, Princeton, NJ, USA
{syoon,kzechner}@ets.org
* University of Illinois, Urbana-Champaign, IL, USA
sumapramod@gmail.com
Abstract
This study presents a method that assesses
ESL learners? vocabulary usage to improve
an automated scoring system of sponta-
neous speech responses by non-native English
speakers. Focusing on vocabulary sophistica-
tion, we estimate the difficulty of each word
in the vocabulary based on its frequency in
a reference corpus and assess the mean diffi-
culty level of the vocabulary usage across the
responses (vocabulary profile).
Three different classes of features were gen-
erated based on the words in a spoken re-
sponse: coverage-related, average word rank
and the average word frequency and the extent
to which they influence human-assigned lan-
guage proficiency scores was studied. Among
these three types of features, the average word
frequency showed the most predictive power.
We then explored the impact of vocabulary
profile features in an automated speech scor-
ing context, with particular focus on the im-
pact of two factors: genre of reference corpora
and the characteristics of item-types.
The contribution of the current study lies in
the use of vocabulary profile as a measure of
lexical sophistication for spoken language as-
sessment, an aspect heretofore unexplored in
the context of automated speech scoring.
1 Introduction
This study provides a method that measures ESL
(English as a second language) learners? compe-
tence in vocabulary usage.
Spoken language assessments typically measure
multiple dimensions of language ability. Overall
proficiency in the target language can be assessed
by testing the abilities in various areas including flu-
ency, pronunciation, and intonation; grammar and
vocabulary; and discourse structure. With the recent
move toward the objective assessment of language
ability (spoken and written), it is imperative that we
develop methods for quantifying these abilities and
measuring them automatically.
A majority of the studies in automated speech
scoring have focused on fluency (Cucchiarini et al,
2000; Cucchiarini et al, 2002), pronunciation (Witt
and Young, 1997; Witt, 1999; Franco et al, 1997;
Neumeyer et al, 2000), and intonation (Zechner et
al., 2011). More recently, Chen and Yoon (2011)
and Chen and Zechner (2011) have measured syn-
tactic competence in speech scoring. However, only
a few have explored features related to vocabulary
usage and they have been limited to type-token ratio
(TTR) related features (e.g., Lu (2011)). In addi-
tion, Bernstein et al (2010) developed vocabulary
features that measure the similarity between the vo-
cabulary in the test responses and the vocabulary in
the pre-collected texts in the same topic. However,
their features assessed content and topicality, not vo-
cabulary usage.
The speaking construct of vocabulary usage com-
prises two sub-constructs: sophistication and preci-
sion. The aspect of vocabulary that we intend to
measure in this paper is that of lexical sophistication,
also termed lexical diversity and lexical richness in
second language studies. Measures of lexical so-
phistication attempt to quantify the degree to which
a varied and large vocabulary is used (Laufer and
Nation, 1995). In order to assess the degree of lex-
180
ical sophistication, we employ a vocabulary profile-
based approach (partly motivated from the results of
a previous study, as will be explained in Section 2).
By a vocabulary profile, it is meant that the fre-
quency of each vocabulary item is calculated from
a reference corpus covering the language variety of
the target situation. The degree of lexical sophisti-
cation is captured by the word frequency - low fre-
quency words are considered to be more difficult,
and therefore more sophisticated. We then design
features that capture the difficulty level of vocabu-
lary items in test takers? responses. Finally, we per-
form correlation analyses between these new fea-
tures and human proficiency scores and assess the
feature?s importance with respect to the other fea-
tures in an automatic scoring module. The novelty
of this study lies in the use of vocabulary profile in
an automatic scoring set-up to assess lexical sophis-
tication.
This paper will proceed as follows: we will re-
view related work in Section 2. Data and experiment
setup will be explained in Section 3 and Section 4.
Next, we will present the results in Section 5, discuss
them in Section 6, and conclude with a summary of
the importance of our findings in Section 7.
2 Related Work
Measures of lexical richness have been the focus of
several studies involving assessment of L1 and L2
language abilities (Laufer and Nation, 1995; Ver-
meer, 2000; Daller et al, 2003; Kormos and Denes,
2004). The types of measures considered in these
studies can be grouped into quantitative and qualita-
tive measures.
The quantitative measures give insight into the
number of words known, but do not distinguish them
from one another based on their category or fre-
quency in language use. They have evolved to make
up for the widely applied measure type-token-ratio
(TTR). However, owing to its sensitivity to the num-
ber of tokens, TTR has been considered as an un-
stable measure in differing proficiency levels of lan-
guage learners. The Guiraud index, Uber index, and
Herdan index (Vermeer, 2000; Daller et al, 2003;
Lu, 2011) are some measures in this category mostly
derived from TTR as either simpler transformations
of the TTR or its scaled versions to ameliorate the
effect of differing token cardinalities.
Qualitative measures, on the other hand, dis-
tinguish themselves from those derived from TTR
since they take into account distinctions between
words such as their parts of speech or difficulty lev-
els. Adding a qualitative dimension gives more in-
sight into lexical aspects of language ability than
the purely quantitative measures such as TTR-based
measures. Some measures in this category in-
clude a derived form of the limiting relative diver-
sity (LRD) given by
?
D(verbs)/D(nouns) using
the D-measure proposed in (Malvern and Richards,
1997), Lexical frequency profile (LFP) (Laufer and
Nation, 1995) and P-Lex (Meara and Bell, 2003).
LFP uses a vocabulary profile (VP) for a given
body of written text or spoken utterance and gives
the percentage of words used at different frequency
levels (such as from the one-thousand most com-
mon words, the next thousand most common words)
where the words themselves come from a pre-
compiled vocabulary list, such as the Academic
Word List (AWL) with its associated frequency dis-
tribution on words by Coxhead(1998). Frequency
level refers to a class of words (or appropriately cho-
sen word units) that are grouped based on their fre-
quencies of actual usage in corpora. P-Lex is an-
other approach that uses the frequency level of the
words to assess lexical richness. These measures are
based on the differing frequencies of lexical items
and hence rely on the availability of frequency lists
for the language being considered.
These two different types of measures have been
used in the analysis of essays written by second lan-
guage learners of English (ESL). Laufer and Nation
(1995) have shown that LFP correlates well with an
independent measure of vocabulary knowledge and
that it is possible to categorize learners according to
different proficiency levels using this measure. In
another study seeking to understand the extent to
which VP based on students? essays predicted their
academic performance (Morris and Cobb, 2004), it
was observed that students? vocabulary profile re-
sults correlated significantly with their grades. Ad-
ditionally, VP was found to be indicative of finer dis-
tinctions in the language skills of high proficiency
nonnative speakers than oral interviews can cover.
Furthermore, these measures have been employed
in automated essay scoring. Attali and Burstein
181
(2006) used average word frequency and average
word length in characters across the words in the
essay. In addition to the average word frequency
measure, the average word length measure was im-
plemented to assess the average difficulty of the
word used in the essay under the assumption that
the words with more characters were more difficult
than the words with fewer characters. These fea-
tures showed promising performance in estimating
test takers? proficiency levels.
In contrast to qualitative measures, quantitative
measures did not achieve promising performance.
Vermeer (2000) showed that quantitative measures
achieve neither the validity nor the reliability of the
measures, regardless of the transformations and cor-
rections.
More recently, the relationship of lexical rich-
ness to ESL learners? speaking task performance
has been studied by Lu (2011). The comprehensive
study was aimed at measuring lexical richness along
the three dimensions of lexical density, sophistica-
tion, and variation, using 25 different metrics (be-
longing to both the qualitative and quantitative cate-
gories above) available in the language acquisition
literature. His results, based on the manual tran-
scription of a spoken corpus of English learners, in-
dicate that a) lexical variation (the number of word
types) correlated most strongly with the raters? judg-
ments of the quality of ESL learners? oral narratives,
b) lexical sophistication only had a very small ef-
fect, and c) lexical density (indicative of proportion
of lexical words) in an oral narrative did not appear
to relate to its quality.
In this study, we seek to quantify vocabulary us-
age in terms of measures of lexical sophistication:
VP based on a set of reference word lists. The nov-
elty of the current study lies in the use of VP as
a measure of lexical sophistication for spoken lan-
guage assessment. It derives support from other
studies (Morris and Cobb, 2004; Laufer and Nation,
1995) but is carried out in a completely different
context, that of automatic scoring of proficiency lev-
els in spontaneous speech, an area not explored thus
far in existing literature.
Furthermore, we investigate the impact of the
genre of the reference corpus on the performance of
these lexical measures. For this purpose, three dif-
ferent corpora will be used to generate reference fre-
quency levels. Finally, we will investigate how the
characteristics of the item types influence the perfor-
mance of these measures.
3 Data
The AEST balanced data set, a collection of re-
sponses from the AEST, is used in this study.
AEST is a high-stakes test of English proficiency,
and it consists of 6 items in which speakers are
prompted to provide responses lasting between 45
and 60 seconds per item, yielding approximately 5
minutes of spoken content per speaker.
Among the 6 items, two items elicit information
or opinions on familiar topics based on the exam-
inees? personal experience or background knowl-
edge. These constitute the independent (IND) items.
The four remaining items are integrated tasks that
include other language skills such as listening and
reading. These constitute the integrated (INT)
items. Both sets of items extract spontaneous and
unconstrained natural speech. The primary dif-
ference between the two elicitation types is that
IND items only provide a prompt whereas INT items
provide a prompt, a reading passage, and a listening
stimulus. The size, purpose, and speakers? native
language information for each dataset are summa-
rized in Table 1. All items extract spontaneous, un-
constrained natural speech.
Each response was rated by a trained human rater
using a 4-point scoring scale, where 1 indicates
a low speaking proficiency and 4 indicates a high
speaking proficiency. The scoring guideline is sum-
marized in the AEST rubrics.
Since none of the AEST balanced data was
double-scored, we estimate the inter-rater agreement
ratio of the corpus by using a large double-scored
dataset which used the same scoring guidelines and
scoring process; using the 41K double-scored re-
sponses collected from AEST, we calculate the Pear-
son correlation coefficient to be 0.63, suggesting a
reasonable agreement. The distribution of scores for
this data can be found in Table 2.
4 Experiments
4.1 Overview
In this study, we developed vocabulary profile fea-
tures. From a reference corpus, we pre-compiled
182
Corpus
name
Purpose # of
speakers
# of re-
sponses
Native languages Size
(Hrs)
AEST bal-
anced data
Feature evaluation, Scor-
ing model training and
evaluation
480 2880 Korean (15%), Chinese (14%),
Japanese (7%), Spanish (9%),
Others (55%)
44
Table 1: Data size and speakers? native languages
Size Score1 Score2 Score3 Score4
Number
of files
141 1133 1266 340
(%) 5 40 45 12
Table 2: Distribution of proficiency scores in the dataset
multiple sets of vocabulary lists (e.g., a list of the
100 most frequent words in a reference corpus).
Next, for each test response, a transcription was gen-
erated using the speech recognizer. For each re-
sponse with respect to each reference word list, vo-
cabulary profile features were calculated. In addi-
tion to vocabulary profile features, type-token ratio
(TTR) was calculated as a baseline feature. Despite
its instability, TTR has been employed in the auto-
mated speech scoring systems such as (Zechner et
al., 2009), and its use here allows a direct compar-
ison of the performance of the features with the re-
sults of previous studies.
4.2 Vocabulary list generation
The three reference corpora we used in this study
are presented in Table 3: The General Service
List (GSL), the TOEFL 2000 Spoken and Written
Academic Language Corpus (T2K-SWAL) and the
AEST data.
Corpus Genre Tokens Types
GSL Written - 2,284
T2K-SWAL Spoken 1,869,346 28,855
AEST data Spoken 5,520,375 23,165
Table 3: Three reference corpora used in this study
GSL (West, 1953) comprises 2,284 words se-
lected to be of ?general service? to learners of En-
glish. In this study, we used the version with fre-
quency information from (Bauman, 1995). The orig-
inal version did not include word frequency and
was ?enhanced? by John Bauman and Brent Culli-
gan with the frequency information obtained from
the Brown Corpus, a collection of written texts.
T2K-SWAL (Biber et al, 2002) is a collection of
spoken and written texts covering a broad language
variety and use in the academic setting. In this study,
only its spoken texts were used. The spoken corpus
included manual transcriptions of discussions, con-
versations, and lectures that occurred in class ses-
sions, study-group meetings, office hours, and ser-
vice encounters.
Finally, AEST data is a collection of manual tran-
scriptions of spoken responses from the AEST for
non-native English speakers. Although there was no
overlap between AEST data and the evaluation data
(AEST balanced data), the vocabulary lists in AEST
data might be a closer match to the vocabulary lists
in the evaluation data since both of them come from
the same test products. From a content perspective,
this dataset is likely to better reflect characteristics
of non-native English speakers than the other two
reference corpora.
For T2K-SWAL and AEST, all transcriptions
were normalized; all the tokens were further de-
capitalized and removed of all non-alphanumeric
characters except for dash and quote. The morpho-
logical variants were considered as different words.
All words were sorted by the word occurrences in
the corpus, and a set of 6 lists were generated:
top-100 words (TOP1), word frequency ranks 101-
300 (TOP2), ranks 301-700 (TOP3), ranks 701-1500
(TOP4), ranks 1501-3000 (TOP5), and all other
words with ranks of 3001 and above (TOP6). For
GSL, a set of 5 lists was generated; TOP6 was
not generated since GSL only included about 2200
words.
Compared to written texts, speakers tended to use
a much smaller vocabulary in speech. For instance,
the percentage of words within the top-1000 words
on the total word types of AEST data responses was
over 90% on average, and they were similar across
183
proficiency levels. This is the reason why we sub-
classified the top 1000 words into three lists, unlike
the vocabulary profile features using top-1000 words
as one list like (Morris and Cobb, 2004), which did
not have any power to differentiate between profi-
ciency levels.
4.3 Transcription generation for evaluation
data
A Hidden Markov Model (HMM) speech recognizer
was trained on the AEST dataset, approximately
733 hours of non-native speech collected from 7872
speakers. A gender independent triphone acoustic
model and a combination of bigram, trigram, and
four-gram language models was used. The word
error rate (WER) on the held-out test dataset was
27%. For each response in the evaluation partition,
an ASR-based transcription was generated using the
speech recognizer.
4.4 Feature generation
Each response comprised less than 60 seconds of
speech with an average of 113 word tokens. Due
to the short response length, there was wide varia-
tion in the proportion of low-frequency word types
for the same speaker. In order to address this issue,
for each speaker, two responses from the same item-
type (IND/INT) were concatenated and used as one
large response. As a result, three concatenated re-
sponses (one IND response and two INT responses)
were generated for each speaker, yielding a total of
480 concatenated responses for IND items and 960
concatenated responses for INT items for our exper-
iment.
First, a list of word types was generated from
the ASR hypothesis of each concatenated response.
IND items provide only a one-sentence prompt,
while INT items provide stimuli including a prompt,
a reading passage, and a listening stimulus. In order
to minimize the influence of the vocabulary in the
stimuli on that of the speakers, we excluded the con-
tent words that occurred in the prompts or stimuli
from the word type list1.
1This process prevents to measure the content relevance;
whether the response is off-topic or not. However, this is not
problematic since the features in this study will be used in the
conjunction with the features that measure the accuracy of the
aspects of content and topicality such as (Xie et al, 2012)?s fea-
Table 4: List of features.
Feature # of Feature Description
features type
TTR 1 Ratio Type-token ratio
TOPn 5 or 6a Listrel Proportion of types
that occurred both
the response and
TOPn list in the to-
tal types of the re-
sponse.
aRank 1 Rank Avg. word rankb
aFreq 1 Freq Avg. word freq.c
lFreq 1 Freq Avg. log(word
freq)d
a For GSL, five different features were created using
TOP1-TOP5 lists, but TOP6 was not created. For
T2K-SWAL and AEST data, six different features were
created using TOP1-TOP6 lists separately.
b ?rank? is the ordinal number of words in a list that is sorted in
descending order of word frequency; words not present in the
reference corpus get the default rank of RefMaxRank+1.
c Avg. word frequency is the sum of the word-frequencies of
word types in the reference corpus divided by the total
number of words in the reference corpus; words not in the
reference corpus get assigned a default frequency of 1.
d Same as feature aFreq, but the logarithm of the word
frequency is taken here
Next, we generated five types of features using
three reference vocabulary lists. A maximum of 10
features were generated for each reference list. The
feature-types are tabulated in Table 4.
All features above were generated from word
types, not word tokens, i.e., multiple occurrences of
the same word in a response were only counted once.
Below we delineate the step-by-step process with
a sample response that leads to the feature genera-
tion outlined in Table 5.
? Step 1: Generate ASR hypothesis for the given
speech response. e.g: Every student has dif-
ferent perspective about how to relax. Playing
xbox.
? Step 2: Generate type list from ASR hypoth-
esis. For the response above we get the list
- about, how, different, xbox, to, relax, every,
perspective, student, has, playing.
tures.
184
word
freq. in
reference
corpus
word rank in
the reference
corpus
TOPn
about 25672 30 TOP1
how 8944 96 TOP1
has 18105 53 TOP1
to 218976 2 TOP1
different 5088 153 TOP2
every 2961 236 TOP2
playing 798 735 TOP4
perspec-
tive
139 1886 TOP5
xbox 1 20000 No
Table 5: An example of feature calculation.
? Step 3: Generate type list excluding words that
occurred in the prompt - about, how, different,
xbox, to, every, perspective, has, playing.
From the ASR hypotheses (result of Step 1), the
corresponding type list was generated (Step 2) and
two words (?student?, ?relax?) were excluded from
the final list due to overlap with the prompt. The
final word list used in the feature generation has 9
types (Step 3).
Next, for each word in the above type list, if it oc-
curs in the reference corpus (a list of words sorted
by frequency), its word frequency, word rank and
the TOPn information (whether the word belonged
to the TOPn list or not) are obtained. If it did not oc-
cur in the reference corpus, the default frequency (1)
and the default word rank (20000) were assigned. In
5, the default values were assigned for ?xbox? since
it was not in the reference corpus.
Finally, the average of the word frequencies and
the average of the the word ranks were calculated
(aFreq and aRank). For lFreq, the log value of each
frequency was calculated and then averaged. For
TOPn features, we obtain the proportion of the word
types that belong to the TOPn category. For the
above sample, the TOP1 feature value was 0.444
since 4 words belong to TOP1 and the total number
of word types was 9 (4/9=0.444).
5 Results
5.1 Correlation
We analyzed the relationship between the proposed
features and human proficiency scores to assess their
influence on predicting the proficiency score. The
reference proficiency score for a concatenated re-
sponse was estimated by summing up the two scores
of the constituent responses. Thus, the new score
scale was 2-8. Table 6 presents Pearson correlation
coefficients (r).
The best performing feature was aFreq followed
by TOP1. Both features showed statistically signif-
icant negative correlations with human proficiency
scores. TOP6 also showed statistically significant
correlation with human scores, but it was 10-20%
lower than TOP1. This suggests that a human rater
more likely assigned high scores when the vocabu-
lary of the response was not limited to a few most
frequent words. However, the use of difficult words
(low-frequency) shows a weaker relationship with
the proficiency scores.
Features based on AEST data outperformed fea-
tures based on T2K-SWAL or GSL. The correlation
of the AEST data-based aFreq feature was ?0.61
for the IND items and?0.51 for the INT items; they
were approximately 0.1 higher than the correlations
of T2K-SWAL or GSL-based features. A similar
tendency was found for the TOP1-TOP6 features,
although differences between AEST data-based fea-
tures and other reference-based features were less
salient overall.
For top-performing vocabulary profile features
including aFreq and TOP1, the correlations of
INT items were weaker than those of the IND items.
In general, the correlations of INT items were 10-
20% lower than those of the IND items in absolute
value.
aFreq and TOP1 consistently achieved better
performance than TTR across all item-types.
5.2 Scoring model building
To arrive at an automatic scoring model, we included
the new vocabulary profile features with other fea-
tures previously found to be useful in a multiple lin-
ear regression (MLR) framework. A total of 80 fea-
tures were generated by the automated speech pro-
ficiency scoring system from Zechner et al (2009),
185
Reference TTR TOP1 TOP2 TOP3 TOP4 TOP5 TOP6 aRank aFreq lFreq
IND GSL -.147 -.347 .027 .078 .000 .053 - .266 -.501 -.260
T2K-SWAL -.147 -.338 .085 .207 .055 .020 .168 .142 -.509 -.159
ATEST -.147 -.470 .014 .275 .172 .187 .218 .236 -.613 -.232
INT GSL -.245 -.255 -.086 -.019 -.068 -.031 - .316 -.404 -.318
T2K-SWAL -.245 -.225 .010 .094 .047 .079 .124 .087 -.405 -.198
ATEST -.245 -.345 -.092 .156 .135 .188 .194 .214 -.507 -.251
Table 6: Correlations between features and human proficiency scores
and they were classified into 5 sub-groups: fluency,
pronunciation, prosody, vocabulary complexity, and
grammar usage. For each sub-group, at least one
feature that correlated well with human scores but
had a low inter-correlation with other features was
selected. A total of following 6 features were se-
lected and used in the base model (base):
? wdpchk (fluency): Average chunk length in words;
a chunk is a segment whose boundaries are set by
long silences
? tpsecutt (fluency): Number of types per sec.
? normAM (pronunciation): Average acoustic model
score normalized by the speaking rate
? phn shift (pronunciation): Average absolute dis-
tance of the normalized vowel durations compared
to standard normalized vowel durations estimated
on a native speech corpus
? stretimdev (prosody): Mean deviation of distance
between stressed syllables in sec.
? lmscore (grammar): Average language model score
normalized by number of words
We first calculated correlations between these fea-
tures and human proficiency scores and compared
them with the most predictive vocabulary profile
features. Table 7 presents Pearson correlation co-
efficients (r) of these features.
In both item-types, the most correlated features
represented the aspect of fluency in production.
While tpsecutt was the best feature in IND items
and the correlation with human scores was approx-
imately 0.66, in INT items, wdpchk was the best
feature and the correlation was even higher, 0.73.
The performance of aFreq was particularly high
in IND items; it was the second best feature and only
marginally lower than the best feature (by 0.04).
aFreq also achieved promising performance in INT;
Features IND INT
wdpchk .538 .612
tpsecutt .659 .729
normAM .467 .429
phn shift -.503 -.535
stretimemdev -.442 -.397
lmscore .257 .312
aFreq -.613 -.507
TOP1 -.470 -.345
TTR -.147 -.245
Table 7: Comparison of feature-correlations with human-
assigned proficiency scores.
it was the fourth best feature. However, the perfor-
mance was considerably lower than the the best fea-
ture, and the difference between the best feature and
aFreq was approximately 22%.
We compared the performances of this base
model with an augmented model (base + TTR + all
vocabulary profile features) whose feature set was
the base augmented with our proposed measures of
vocabulary sophistication. Item-type specific multi-
ple linear regression models were trained using five-
fold cross validation. The 480 IND responses 960
INT responses were partitioned into five sets, sepa-
rately. In each fold, an item-type specific regression
model was trained using four of these partitions and
tested on the remaining one.
The averages of the five-fold models are sum-
marized in Table 8, showing weighted kappa to
indicate agreement between automatic scores and
human-assigned scores and also the Pearson?s cor-
relation (r) of the unrounded (un-rnd) and rounded
(rnd) scores with the human-assigned scores. We
used the correlation and weighted kappa as perfor-
mance evaluation measures to maintain the consis-
tency with the previous studies such as (Zechner
et al, 2009). In addition, the correlation metric
186
matches better with our goal to investigate the rela-
tionship between the predicted scores and the actual
scores rather than the difference between the pre-
dicted scores and the actual scores.
Features un-
rnd
corr.
rnd
corr.
weighted
kappa
IND base 0.66 0.62 0.55
base + TTR 0.66 0.63 0.56
base + TTR +
all
0.66 0.64 0.57
INT base 0.76 0.73 0.69
base + TTR 0.76 0.74 0.70
base + TTR +
all
0.77 0.74 0.70
Table 8: Performance of item-type specific multiple lin-
ear regression based scoring models.
The new scores show slightly better agreement
with human-assigned scores, but the improvement
was small in both item-types, approximately 1%.
6 Discussion
In general, we found that the test takers used a fairly
small number of vocabulary items in the spoken re-
sponses. On average, the total types used in the
responses was 87.21 for IND items and 98.52 for
INT items. Furthermore, the proportions of high
frequency words on test takers? spoken responses
were markedly high. The proportion of top-100
words was almost 50% and the proportion of top-
1500 words (summation of TOP1-TOP4) was over
89% on average. This means that only 1500 words
represent almost 90% of the active vocabulary of
the test takers in their spontaneous speech. Figure
1 presents the average TOP1-TOP6 features across
all proficiency levels.
The values of INT items were similar to IND
items, but the TOP3-TOP6 values were slightly
higher than IND items; INT items tended to include
more low frequency words. In order to investigate
the impact of the higher proportion of low frequency
words in INT items, we selected two features (aFreq
and TOP1) and further analyzed them.
Table 9 provides the mean of aFreq and TOP1 for
each score level. The features were generated using
AEST as a reference.
Figure 1: Proportion of top-N frequent words on average
Score aFreq TOP1
IND INT IND INT
2 43623 36175 .60 .52
3 38165 32493 .55 .49
4 33861 28884 .51 .48
5 30599 27118 .49 .46
6 28485 26327 .46 .45
7 27358 25093 .45 .43
8 26065 24711 .43 .43
Table 9: Mean of vocabulary profile features for each
score level
On average, the differences between adjacent
score levels in INT items were smaller than those
in IND items. The weaker distinction between score
levels may result in the lower performance of vo-
cabulary profile features in INT items. Particularly,
the differences were smaller in lower score levels (2-
4) than in higher score levels (5-8). The relatively
high proportion of low frequency words in the low
score level reduced the predictive power of vocabu-
lary profile features.
This difference between the item-types strongly
supports item-type-specific modeling. We combined
the IND and INT item responses and computed
a correlation between the features and the profi-
ciency scores over the entirety of data sets. De-
spite increase in sample sizes, the correlations were
lower than both the corresponding correlations of
the IND items and the INT items. For instance, the
correlation of the T2K-SWAL-based aFreq feature
was?0.393, and that of the AEST data-based aFreq
was?0.50, which was approximately 3% lower than
the INT items and 10% lower than the IND items.
The difference in the vocabulary distributions be-
tween the two item-types decreased the performance
187
of the features.
In this study, AEST data-based features outper-
formed T2K-SWAL-based features. Although no
items in the evaluation data overlapped with items
in AEST data, the similarity in the speakers? profi-
ciency levels and task types might have resulted in
a better match between the vocabulary and its dis-
tributions of AEST data with AEST balanced data,
finally the AEST data-based features achieved the
best performance.
In order to explore the degree to which AEST bal-
anced data (test responses) and the reference cor-
pora matched, we calculated the proportion of word
types that occurred in test responses and reference
corpora (the coverage of reference list). The ASR
hypotheses of AEST balanced data comprised 6,024
word types. GSL covered 73%, T2K-SWAL cov-
ered 99%, and AEST data covered close to 100%.
Considering the fact that, a) despite high coverage
of both T2K-SWAL and AEST data, T2K-SWAL-
based features achieved much lower performance
than AEST data, and, b) despite huge differences
in the coverage between T2K-SWAL and GSL, the
performance of features based on these reference
corpora were comparable, coverage was not likely
to have been a factor having a strong impact on the
performance. The large differences in the perfor-
mance of TOP1 across reference lists support the
possibility of the strong influence of high frequency
word types on proficiency; the kinds of word types
that were in the TOP1 bins were an important factor
that influenced the performance of vocabulary pro-
file features. Finally, genre differences (spoken texts
vs. written texts) in reference corpora did not have
strong impact on the predictive ability of the fea-
tures; the performance of features based on written
reference corpus (GSL) were comparable to those
based on a spoken reference corpus (T2K-SWAL).
Despite the high correlation shown by the indi-
vidual features (such as aFreq), we do not see a cor-
responding increase in the performance of the scor-
ing model with all the best performing features. The
most likely explanation to this is the small training
data size; in each fold, only about 380 responses for
IND and about 760 responses for INT were used
in the scoring model training. Another possibility
is overlap with the existing features; the aspect that
vocabulary profile features are modeling may be al-
ready covered to some extent in existing feature set.
In future research, we will further investigate this as-
pect in details.
7 Conclusions
In this study, we presented features that measure
ESL learners? vocabulary usage. In particular, we
focused on vocabulary sophistication, and explored
the suitability of vocabulary profile features to cap-
ture sophistication. From three different reference
corpora, the frequency of vocabulary items was cal-
culated which was then used to estimate the sophis-
tication of test takers? vocabulary. Among the three
different reference corpora, features based on AEST
data, a collections of responses similar to that of the
test set, showed the best performance. A total of 29
features were generated, and the average word fre-
quency (aFreq) achieved the best correlation with
human proficiency scores. In general, vocabulary
profile features showed strong correlations with hu-
man proficiency scores, but when used in an auto-
matic scoring model in combination with an existing
set of predictors of language proficiency, the aug-
mented feature set showed marginal improvement in
predicting human-assigned scores of proficiency.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e?rater R v.2. The Journal of Technology,
Learning, and Assessment, 4(3).
John Bauman. 1995. About the GSL. Retrieved
March 17, 2012 from http://jbauman.com/
gsl.html.
Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010.
Fluency and structural complexity as predictors of L2
oral proficiency. In Proceedings of InterSpeech 2010,
Tokyo, Japan, September.
Douglas Biber, Susan Conrad, Randi Reppen, Pat Byrd,
and Marie Helt. 2002. Speaking and writing in the
university: A multidimensional comparison. TESOL
Quarterly, 36:9?48.
Lei Chen and Su-Youn Yoon. 2011. Detecting structural
events for assessing non-native speech. In Proceed-
ings of the 6th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 38?45.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
188
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics 2011, pages 722?731.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 111(6):2862?2873.
Helmut Daller, Roeland van Hout, and Jeanine Treffers-
Daller. 2003. Lexical richness in the spontaneous
speech of bilinguals. Applied Linguistics, 24(2):197?
222.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scoring
for language instruction. In Proceedings of ICASSP
97, pages 1471?1474.
Judit Kormos and Mariann Denes. 2004. Exploring mea-
sures and perceptions of fluency in the speech of sec-
ond language learners. System, 32:145?164.
Batia Laufer and Paul Nation. 1995. Vocabulary size and
use: lexical richness in L2 written production. Applied
Linguistics, 16:307?322.
Xiaofei Lu. 2011. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Language Journal.
David D. Malvern and Brian J. Richards. 1997. A
new measure of lexical diversity. In Evolving mod-
els of language: Papers from the Annual Meeting of
the British Association of Applied Linguists held at the
University of Wales, Swansea, September, pages 58?
71.
Paul Meara and Huw Bell. 2003. P lex: A simple and
effective way of describing the lexical characteristics
of short L2 texts. Applied Linguistics, 24(2):197?222.
Lori Morris and Tom Cobb. 2004. Vocabulary profiles
as predictors of the academic performance of teaching
english as a second language trainees. System, 32:75?
87.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Anne Vermeer. 2000. Coming to grips with lexical rich-
ness in spontaneous speech data. Language Testing,
17(1):65?83.
Michael West. 1953. A General Service List of English
Words. Longman, London.
Silke Witt and Steve Young. 1997. Performance mea-
sures for phone-level pronunciation teaching in CALL.
In Proceedings of the Workshop on Speech Technology
in Language Learning, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of the NAACL-HLT, Montreal,
July.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51:883?895, October.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011. Eval-
uating prosodic features for automated scoring of non-
native read speech. In IEEE Workshop on Automatic
Speech Recognition and Understanding 2011, Hawaii,
December.
189
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 73?81,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Automated Content Scoring of Spoken Responses in an Assessment for 
Teachers of English 
 
Klaus Zechner, Xinhao Wang  
Educational Testing Service 
660 Rosedale Road 
Princeton, NJ 08541, USA 
kzechner@ets.org, xwang002@ets.org 
 
 
 
 
Abstract 
This paper presents and evaluates approaches 
to automatically score the content correctness 
of spoken responses in a new language test for 
teachers of English as a foreign language who 
are non-native speakers of English. Most ex-
isting tests of English spoken proficiency elic-
it responses that are either very constrained 
(e.g., reading a passage aloud) or are of a pre-
dominantly spontaneous nature (e.g., stating 
an opinion on an issue). However, the assess-
ment discussed in this paper focuses on essen-
tial speaking skills that English teachers need 
in order to be effective communicators in their 
classrooms and elicits mostly responses that 
fall in between these extremes and are moder-
ately predictable. In order to automatically 
score the content accuracy of these spoken re-
sponses, we propose three categories of robust 
features, inspired from flexible text matching, 
n-grams, as well as string edit distance met-
rics. The experimental results indicate that 
even based on speech recognizer output, most 
of the feature correlations with human expert 
rater scores are in the range of r = 0.4 to r = 
0.5, and further, that a scoring model for pre-
dicting human rater proficiency scores that in-
cludes our content features can significantly 
outperform a baseline without these features 
(r = 0.56 vs. r = 0.33).  
1 Introduction 
With the increased need for instruction of interna-
tional learners of English as a foreign language 
(EFL), there is a concomitant rise in demand to 
assess the language competence of English teach-
ers who are non-native speakers of English. This 
situation arises because it is neither possible nor 
affordable for countries where English is not spo-
ken as a native language to employ only or even 
mostly native speakers of English as EFL teachers. 
Moreover, as the language of instruction increas-
ingly becomes English in most classrooms, teach-
ers? competence in the productive language 
modality of speaking becomes substantially more 
important than in the past. In order to meet this 
demand for assessing the English language profi-
ciency of teachers of English, a new test, English 
Teachers Language Assessment (ETLA), was de-
veloped recently and piloted in 2012. The test 
comprises items for all four main language modali-
ties: reading, listening, writing and speaking. 
While reading and listening items use a multi-
ple-choice paradigm, test items for speaking and 
writing elicit open responses. For cost and effi-
ciency reasons, we aim to employ automated scor-
ing of written and spoken responses in this test. 
This paper is concerned in particular with the con-
ceptualization, implementation and evaluation of 
features that can assess one aspect of English 
speaking proficiency: the content correctness of a 
test taker?s response. Our automated speech scor-
ing system, SpeechRaterSM
The speaking items in ETLA range in complexi-
ty from reading a text passage aloud to more chal-
lenging tasks requiring multi-sentence responses 
related to typical teaching situations. The items, 
therefore, elicit speech in which predictability 
ranges from high (e.g., reading aloud) to medium 
(e.g., open responses based on teaching material). 
 (Zechner et al, 2009), 
also has features addressing other aspects of speak-
ing proficiency, such as fluency or pronunciation, 
but the details of these features will not be dis-
cussed as part of this paper. 
73
While approaches to capture the content of mostly 
predictable speech have been widely used in the 
past (see, e.g., Alwan et al, 2007; Franco et al, 
2010), this is not the case for responses that exhibit 
considerable variation but are still much shorter 
and more constrained than spontaneous items from 
other language tests, such as TOEFL iBT?
Therefore, the goal of the study reported in this 
paper is to conceptualize, implement and evaluate 
features that can address the subset of ETLA 
speaking items where responses are not strongly 
predictable but are still fairly short and constrained 
by the context of the item stimulus and prompt.
. 
1
To illustrate what an ETLA speaking item may 
look like, we provide a relatively simple example 
here. Suppose the test taker (i.e., an English lan-
guage teacher) is asked to request that the class 
open their textbooks on page 55. We could see a 
range of responses, from ?perfect? (score level 3, 
e.g., ?Please open your textbooks on page 55.? or 
?Please open your textbooks and turn to page 
55.?), to ?good? (score level 2, e.g., ?Please open 
the books on the page 55.?) and to ?poor? (score 
level 1, e.g., ?Open book page 55.?). Again, note 
that for this paper we are not interested in potential 
issues with fluency, such as long pauses or speak-
ing rate, nor with pronunciation or prosody. We 
just look at the content of the test takers? respons-
es, either in idealized form by means of a human 
transcription of what a test taker actually said, or in 
a realistic operational scenario, where we look at 
the output of an ASR system. In both cases, we 
consider the sequence of words only (i.e., a textual 
representation of the test takers? spoken respons-
es). 
 
One important aspect of any features used for con-
tent scoring is that they have to be robust with re-
spect to speech recognition errors. Robustness is 
necessary because we are using an automatic 
speech recognition (ASR) system as a front end, 
and the average word error rate of the system is 
around 27% for moderately predictable item re-
sponses. 
In order to investigate the effectiveness of can-
didate content features in a short-term development 
cycle before a larger amount of pilot data would be 
available, we first conducted a small scale in-house 
                                                          
1 A test item is a basic element of a test, consisting of stimulus 
material, such as text and/or visuals, and a prompt (test ques-
tion) that elicits a response from the test taker. 
data collection effort focusing on the moderately 
predictable spoken items in ETLA. Based on the 
analysis of this mini-corpus, several different cate-
gories of promising features were selected for po-
tential operational use and then evaluated on the 
pilot data. 
The paper is organized as follows: Section 2 
provides an overview on related work; Section 3 
describes the in-house data set, the pilot data and 
the ASR system; the developed features are pre-
sented in Section 4; Section 5 presents our experi-
ments; we then discuss our findings in Section 6 
and we conclude the paper in Section 7. 
2 Related Work  
Related to the automated assessment of writing 
free-text, research to date has concentrated mainly 
on two tasks: (1) scoring of short answers (Mitch-
ell et al, 2002; Leacock and Chodorow, 2003; 
Mohler and Mihalcea, 2009) and (2) scoring of 
essays (Foltz et al, 1999; Kanejiya et al, 2003; 
Attali and Burstein, 2006). For example, Leacock 
and Chodorow (2003) built an automated scoring 
system, c-rater?, to evaluate the short constructed 
or free-text responses, where the concepts given in 
test items were modeled, and the presence of these 
expected concepts in students? answers would be 
detected.  
As for the evaluation of free-text essays, Attali 
and Burstein (2006) used a selected set of mean-
ingful features to measure different constructed 
aspects of writing essays, such as grammar, usage, 
mechanics, style, organization, development, lexi-
cal complexity and prompt-specific vocabulary 
usage. In addition, the Intelligent Essay Assessor 
(Foltz et al, 1999) used Latent Semantic Analysis 
(LSA) to score students? answers by comparing 
them to domain-representative texts. Since LSA is 
based on the bag-of-words model, researchers have 
also tried to expand it by introducing additional 
information, such as part-of-speech (POS) tags 
(Kanejiya et al, 2003).  
In addition, research efforts have also been 
made to evaluate the content relatedness and cor-
rectness for spoken responses. For example, Xie et 
al. (2012) used LSA and Pairwise Mutual Infor-
mation approaches to evaluate the content correct-
ness of unrestricted spontaneous spoken responses. 
Moreover, Chen and Zechner (2011) explored fea-
74
tures related to grammatical complexity in an au-
tomated speech scoring system.  
In order to address the moderately predictable 
speaking test items in the new ETLA, this paper 
presents several different types of features to score 
the content correctness of the elicited spoken re-
sponses. Following a series of experiments and 
comparisons, seven features from three content 
feature categories are selected and evaluated. 
3 Data Sets and ASR System 
This study conducts experiments and evaluations 
based on two different data sets: (1) a small scale 
in-house data collection effort, which was used for 
the design and development of content features; 
and (2) a larger-scale pilot data collection, which 
was used to further evaluate the features selected 
according to the in-house data and to build scoring 
models for the prediction of human proficiency 
scores. 
3.1 In-house Data Collection 
Twenty-two items from ETLA with moderately 
predictable responses were selected for the in-
house data collection.2
                                                          
2 We decided to focus our efforts only on the moderately pre-
dictable items since scoring of highly predictable item types 
has been extensively studied in previous research already. 
 Firstly, 1,053 text responses 
in total for all three score levels (3 = high profi-
ciency, 2 = medium proficiency, 1 = low profi-
ciency) were drafted and collected by human 
experts. In order to simulate the operational scenar-
io with an ASR system in place, a subset of re-
sponses was recorded by a small set of 
predominantly non-native speakers of English. For 
each test item, four responses were randomly se-
lected from each score level, which resulted in 22 
? 3 ? 4 = 264 responses for voice recording. The 
remainder of 789 text responses comprised the set 
for feature development and training. In addition, 
about two thirds of the 264 text responses were 
randomly double-recorded by a second speaker, 
resulting in a speech corpus with 444 spoken re-
sponses in total, used as the evaluation set. Fur-
thermore, all these spoken responses were 
manually transcribed to accommodate the errors 
introduced by reading, such as insertions of various 
speech disfluencies.  
3.2 Pilot Data Collection 
This study uses data from a 2012 pilot administra-
tion of the ETLA assessment. In particular, we fo-
cus on 14 moderately predictable items from the 
pilot, covering 2,308 test takers. In order to build 
the automatic speech recognizer and the scoring 
models, the pilot data were partitioned into five 
different subsets without any speaker and response 
overlaps. The first three data partitions were used 
for training, development and evaluation of the 
speech recognition system (hereafter, ?asrTrain?, 
?asrDev? and ?asrEval?), which included spoken 
responses from both the moderately and highly 
predictable items. The asrTrain partition was fur-
ther used to develop and train the content features 
described below. The remaining two partitions 
were used for training and evaluation of scoring 
models that predicted item scores based on a set of 
features (hereafter, ?smTrain? and ?smEval?), 
where only the spoken responses from 14 moder-
ately predictable items from one pilot form were 
included.   
The detailed partition information is listed in 
Table 1. All these spoken responses have been 
manually transcribed and scored with holistic 
scores from 1 to 3 by trained human expert raters. 
For the smTrain and smEval partitions, there were 
6,367 responses receiving double annotation, and 
the inter-rater correlation was 0.73. Furthermore, 
the average length of responses from smTrain and 
smEval sets was 10.5 words, and the correspond-
ing vocabulary size was 855 (not including partial 
words).  
 
Partitions # Speakers # Responses 
asrTrain 1,658 27,604 
asrDev 25  700 
asrEval 25  700 
smTrain 300  3,452 
smEval 300  3,466 
Table 1. Number of speakers and number of responses 
included within each data partition. 
3.3 System Architecture 
Our automated speech scoring system, 
SpeechRater (Zechner et al, 2009), consists of an 
ASR system described below which generates a 
word hypothesis for every response by a test taker, 
including information about timing, energy and 
pitch, and other information from the input audio 
75
file. Next, the feature computation modules take 
the outputs of the ASR system and compute a set 
of features, related to fluency, pronunciation, pros-
ody, as well as content, the focus of this paper. Fi-
nally, a scoring model (linear regression model) is 
trained based on the smTrain set to predict scores 
and then evaluated on unseen data (smEval set). 
3.4 ASR System 
In this study, a state-of-the-art gender-independent 
Hidden Markov Model speech recognition system 
trained on about 800 hours of non-native speech is 
taken as the baseline recognizer, and its language 
model (LM) is then further adapted using the tran-
scriptions from the asrTrain data partition. The 
language model adaptation weights are tuned on 
the asrDev set, and the resulting word error rate 
(WER) on the asrEval set (with both moderately 
and highly predictable responses) is 11.7%, and its 
WER on the subset of 264 moderately predictable 
responses is 19.7%. This speech recognizer is fur-
ther evaluated on both smTrain and smEval sets as 
shown in Table 2, only including moderately pre-
dictable responses.  
 
Partition WER (%) 
smTrain 26.7 
smEval 26.9 
Table 2. Word error rates (WER) of the speech recog-
nizer on smTrain and smEval3
4 Content Features 
 data sets.  
Following a careful inspection and analysis of the 
collected in-house data (described in Section 3.1 
above), several different categories of content fea-
tures were designed and developed. The initial data 
analysis showed that features need to be able to 
capture very narrow ranges of expressions with 
minor variations, but also should be able to capture 
something like the ?overall accuracy? of expres-
sion, where local word sequences or phrases 
should conform to the expectations of the item de-
sign without requiring that a response follows a 
confined pattern in its entirety. For the former situ-
ation, features like regular expression matches 
                                                          
3 The calculation of WER is based on only the recognized 
outputs with more than one word. Thus, the number of actual-
ly recognized responses is less than that in Table 1, i.e., 3,264 
responses for smTrain and 3,255 responses for smEval. 
 
seem appropriate to be a good match, whereas for 
the latter, more flexible approaches such as n-gram 
models or string edit distance metrics may be more 
appropriate. We list and describe our proposed 
content features in the following section. 
A. Flexible String Matching Metrics 
AI. Regular Expressions 
Since many responses in ETLA are expected to 
follow certain patterns, it is intuitive to construct 
limited regular expressions (RegEx) to match gold 
standard responses for candidates with high profi-
ciency score levels. Accordingly, one type of regu-
lar expression related features, re_match, can be 
extracted to detect whether the test response can be 
matched by any of the pre-built regular expres-
sions. This feature can obtain the values of 0 (does 
not match), 1 (partially matches) and 2 (exactly 
matches). Here, a partial match indicates that a 
RegEx can be matched within a test response that 
also has other spoken material, which is useful 
when the speaker repeats or corrects the answer 
multiple times in a single item response, and the 
compiled RegEx can still be used to match parts of 
the test response. 
This content feature has the advantage of high 
precision, as it can precisely examine the content 
correctness of the test responses. Thus, the RegEx 
should be compiled to match all the example re-
sponses at the highest score level 3 from the train-
ing set. For some test items with relatively short 
and fixed answer patterns, this feature is quite use-
ful; however, it is very time-consuming and diffi-
cult to manually build regular expressions for 
items with longer and more flexible expressions. 
Meanwhile, the mechanism of exact matching can 
make this feature fail in very small variations of 
expression. Especially when applying this feature 
on ASR output, it is difficult to successfully match 
some content-correct responses that have 
disfluencies or recognition errors. 
Therefore, in order to improve the robustness of 
RegEx, another regular expression related feature 
is proposed. In general, for each item in ETLA, 
some pieces of specific expressions are required in 
a test response to represent its content correctness. 
Accordingly, we can segment the reference re-
sponses into several fragments and identify some 
pieces as key fragments. For example, when look-
ing at the reference response ?Please open your 
76
text books and turn to page 55.? two key fragments 
can be extracted with ?Please open your text 
books? and ?turn to page 55.? We group versions 
of these key fragments from the training corpus 
together and construct regular expressions to match 
each group. Afterwards, a feature can be defined to 
count how many key fragments can be matched by 
a test response, namely num_fragments.  
 
AII. Keyword Detection 
For moderately predictable items on ETLA, key-
word lists can be extracted from the stimulus mate-
rial and the item prompt, containing the words that 
need to be included in a test response by test tak-
ers. Then a feature, num_keywords, can be used to 
examine how many keywords appear in a test re-
sponse, which can be further normalized by the 
number of predefined keywords for each item, i.e., 
percent_keywords. In addition, as some keywords 
may be a phrase with multiple words, such as 
?page 55,? we can split all the keywords into sin-
gle words and get another sub-keywords list. Then 
two corresponding features can be extracted as 
num_sub_keywords and percent_sub_keywords. 
B. N-grams 
BI. Word N-grams 
The word n-gram model is introduced here to cap-
ture the similarity of word usage between the test 
and the reference responses. Based on the collected 
training samples, trigrams are trained using the text 
responses from the highest score level 3. Then, the 
LM can be used to score a test response, and the 
resulting probability can be taken as feature, called 
lm_3.  
 
BII. POS Similarity 
This feature measures the syntactic complexity of 
test responses based on the distribution of POS 
tags. First, all the responses from the training data 
set are assigned with POS tag sequences via an 
automatic POS tagger. Then, a POS vector accord-
ing to each score level can be obtained by gather-
ing the POS unigram, bigram or trigram statistics 
from the same score level. 
Given a test response, its corresponding POS 
sequence can be determined by the same POS tag-
ger, and the cosine similarities between the test 
POS n-gram vector and the POS vectors from three 
different score levels can be calculated as pos_1, 
pos_2 and pos_3, where pos_3 is used as a feature 
in our experiments below. Furthermore, by com-
paring these three cosine similarities, the score cat-
egory with the highest similarity can be extracted 
as another feature, i.e., pos_score. 
 
BIII. Machine Translation Evaluation Metric 
(BLEU) 
BLEU (Papineni et al, 2002) is one of the most 
popular metrics for automatic evaluation of ma-
chine translation, where the score is calculated 
based on the modified n-gram precision. In this 
study, the BLEU score is introduced to evaluate 
the content quality of a test response, where three 
different gold standard reference corpora are ex-
tracted from the training set according to each 
score level. Similar to the edit distance and WER 
features described below, three BLEU scores are 
calculated by comparing them with reference re-
sponses from each score level (i.e., bleu_1, bleu_2 
and bleu_3). We decide to use the following two 
features for our experiments below: bleu_3 and 
bleu_score, the score level which receives the 
maximum BLEU score.  
C. String Edit Distance Metrics 
CI. String Edit Distance 
As the edit distance is an effective string metric for 
measuring the amount of difference between two 
word sequences, including insertions, deletions and 
substitutions, we use it to capture the sequence dis-
tance between the test and reference responses.  
Given a test response, we can separately calcu-
late the edit distance by comparing it with training 
responses from each score level. Afterwards, the 
minimum edit distance from each score level can 
be extracted as ed_1, ed_2 and ed_3, where ed_3 is 
selected as feature for our experiments. Further-
more, by comparing these three edit distances, the 
score category with the minimum value is taken as 
another feature, ed_score.  
 
CII. Word Error Rate (WER) 
By dividing the edit distance by the length of the 
reference response, we obtain the word error rate 
(WER) metrics, commonly used in speech recogni-
tion, and two additional features, wer_3 and 
wer_score, similarly as above, can be calculated.  
Compared to the above category of n-gram re-
lated features, which capture the n-gram fragment 
77
matching between the test and reference samples, 
the category of edit distance features try to find the 
most similar reference sample to the test sample at 
the whole-response level.  
Finally, all the proposed features are implement-
ed and then examined based on both the ideal hu-
man transcription and the realistic ASR output. 
The speech recognizer used with the small in-
house data is the same as the ASR system de-
scribed in Section 3.4, but its language model is 
adapted with the much smaller set of 789 training 
text responses. The WER of this system is 17.8%, 
evaluated on 444 spoken responses. 
In addition, in order to increase the robustness of 
the extracted features, a preprocessing stage is in-
troduced to remove all the disfluencies from the 
ASR output, such as filler words, recognized par-
tial words and repeated words. Afterwards, each 
feature is evaluated on both the transcription and 
the ASR output of the 444 collected spoken re-
sponses, and its corresponding Pearson correlation 
coefficient with human scores is presented in Table 
3.  
Based on overall correlation, inter-correlation 
analyses, as well as on construct4
5 Experiments and Results 
 considerations, 
seven content features from three categories are 
selected and will be evaluated on a larger scale on 
ETLA pilot data in the next section: re_match 
(A1), num_fragments (A2), percent_sub_keywords 
(A3), bleu_3 (B1), ed_score (C1), wer_3 (C2) and 
wer_score (C3). 
This section first describes experiments related to 
the performance of the seven selected content fea-
tures on a larger corpus from an ETLA pilot ad-
ministration (described above in Section 3.2). 
Then, a similar analysis is conducted based on hu-
man rater analytic content scores on a subset of 
this data. Finally, the selected content features are 
combined with other features related to pronuncia-
tion, prosody and fluency to build a scoring model 
for the prediction of human scores. 
 
 
                                                          
4 A construct is the set of knowledge, skills and abilities 
measured by a test. The term ?construct considerations? in the 
context of feature selection refers to the process of ensuring 
that the selected feature set obtains a high coverage of all as-
pects of the relevant construct. 
 Feature Trans ASR  
A 
re_match 0.789 0.537 
num_fragments 0.629 0.523 
num_keywords 0.269 0.254 
percent_keywords 0.419 0.375 
num_sub_keywords 0.249 0.239 
percent_sub_keywords 0.482 0.417 
B 
lm_3 0.482 0.461 
pos_3 0.270 0.270 
pos_score 0.315 0.339 
bleu_3 0.531 0.458 
bleu_score 0.144 0.194 
C 
ed_3 -0.362 -0.337 
ed_score 0.642 0.614 
wer_3 -0.573 -0.513 
wer_score 0.585 0.557 
Table 3. Pearson correlation coefficients (r) of content 
features with human holistic scores. 
5.1 Feature Evaluation on Pilot Data 
In the following experiments, we use the asrTrain 
set to train the content features. Then these features 
are examined on the smTrain and smEval data sets. 
In order to extract the edit distance, WER- and 
BLEU-related features for each item, three text 
reference corpora according to different score lev-
els, are needed. Duplicate reference responses with 
the same content are removed within each score 
level.  
Furthermore, we improve two RegEx features 
using the reference responses from the highest 
score level 3 in the asrTrain set. (1) Since the pre-
viously obtained re_match feature based on the in-
house data may not be able to match multiple con-
tent-correct responses in the pilot data, we need to 
augment the set of RegEx for this feature based on 
correct responses from score level 3 in the asrTrain 
set. (2) Since the maximum number of candidate 
fragments varies across different ETLA items, the 
num_fragments feature values are not comparable 
across items. Therefore, we redesign this feature 
by assigning a list of manually selected keywords 
for each fragment. During feature extraction, we 
count the number of distinct keywords associated 
with all the matched fragments and divide this 
number by the number of predefined keywords for 
each item (as in AII. Keyword Detection), which 
results in another feature: perc_fragment_kw (A2).  
Based on the ASR output of smTrain and 
smEval data sets, seven content features are ex-
tracted and their Pearson correlation coefficients 
with the holistic human scores are calculated and 
shown in Table 4. 
78
 Feature 
smTrain (r) smEval (r) 
Trans ASR Trans ASR 
A1 0.53 0.415 0.534 0.441 
A2 0.576 0.458 0.583 0.48 
A3 0.42 0.286 0.419 0.297 
B1 0.597 0.478 0.564 0.452 
C1 0.535 0.412 0.52 0.39 
C2 -0.588 -0.469 -0.564 -0.446 
C3 0.554 0.433 0.51 0.428 
Table 4. Pearson correlation coefficients between con-
tent features and human holistic scores, based on both 
the transcription and the ASR output of smTrain and 
smEval.5
5.2 Evaluations Using Human Rater Analyt-
ic Content Scores 
 Features include A1 (re_match), A2 
(perc_fragment_kw), A3 (percent_sub_keywords), B1 
(bleu_3), C1 (ed_score), C2 (wer_3) and C3 
(wer_score) 
In addition to the human rating of all spoken re-
sponses of the ETLA pilot data set with holistic 
scores that take into account both the dimensions 
of ?delivery? (fluency, pronunciation, prosody) 
and ?content,? a subset of the data was further 
scored by human expert raters in these two dimen-
sions separately, resulting in so-called analytic 
scores for delivery and content. The inter-
correlation for content analytic scores was 0.79. 
1,410 responses from the smTrain set and 1,402 
responses from the smEval set received such ana-
lytic content scores. On this subset, table 5 shows 
the Pearson correlation coefficients between the 
content features and the analytic content scores, as 
well as the holistic scores, for comparison. 
5.3 Scoring Model Comparison 
We further examine these content features by in-
troducing them in a scoring model to predict hu-
man rater holistic proficiency scores, using 
smTrain for training of the models and smEval for 
their evaluation. The baseline system employs 14 
features related to the construct dimension of de-
livery, such as pronunciation, prosody and fluency.  
                                                          
5 The evaluation is conducted on recognition output with more 
than one word. In addition, due to technical problems, such as 
high background noise, some responses are non-scorable for 
human raters, and these responses are removed from the eval-
uation sets. Finally, there are 3176 responses included in 
smTrain, and 3084 responses in smEval.  
 
Feature 
smTrain (r) 
Holistic Content 
Trans ASR Trans ASR 
A1 0.529 0.415 0.563 0.434 
A2 0.564 0.46 0.646 0.525 
A3 0.422 0.283 0.452 0.277 
B1 0.6 0.499 0.654 0.504 
C1 0.527 0.43 0.555 0.46 
C2 -0.588 -0.473 -0.627 -0.488 
C3 0.542 0.434 0.563 0.462 
Feature 
smEval (r) 
Holistic Content 
Trans ASR Trans ASR 
A1 0.525 0.424 0.538 0.436 
A2 0.579 0.472 0.621 0.512 
A3 0.423 0.308 0.454 0.321 
B1 0.563 0.442 0.606 0.471 
C1 0.521 0.4 0.539 0.422 
C2 -0.543 -0.42 -0.584 -0.457 
C3 0.514 0.417 0.529 0.439 
Table 5. Pearson correlation coefficients between con-
tent features and human analytic content scores as well 
as human holistic scores.  
 
Furthermore, an extended scoring model is built by 
adding the selected seven content features to the 
model. Table 6 provides the comparison between 
these two scoring models, reporting both quadratic 
weighted kappa and Pearson correlation coeffi-
cients between automatically predicted scores and 
human holistic scores on the smEval data set. 
 
Scoring Model Kappa r 
Baseline (Delivery only) 0.30 0.33 
Extended (Delivery+Content) 0.53 0.56 
Table 6. Scoring model comparison: quadratic weighted 
kappa and Pearson correlation coefficients between pre-
dicted scores (unrounded) and human holistic scores.  
6 Discussion 
The goal of this paper was to conceptualize, im-
plement and evaluate features that can determine 
the content correctness of spoken item responses in 
an English language test for teachers of English 
who are not native speakers of English. 
Based on observations from a small in-house da-
ta collection, where human test developers and 
content experts created example responses to 22 
test items for three different score levels, we de-
cided to implement a range of features that can 
capture the content correctness of test takers? re-
sponses in varying degree of precision. Our fea-
79
tures belong to three classes: features related to 
fixed expressions, with potential small variations, 
such as regular expressions or keywords; features 
based on n-grams of words or POS tags, including 
the BLEU metrics frequently used for evaluations 
of machine translation output; and features related 
to measures of string edit distance, including the 
WER metrics commonly used in speech recogni-
tion evaluations.  
It should be noted that we use the term ?content? 
in a fairly broad way in this paper, namely, every-
thing in a spoken response that is not related to 
lower-level aspects of speech production such as 
fluency or pronunciation. Since the scoring rubrics 
for ETLA place a high emphasis both on the 
grammatical accuracy, as well as on the correct 
content (in a more narrow sense), this situation is 
reflected by our choice of features that focus both 
on elements traditionally associated with content 
(such as matching of keywords), as well as on ele-
ments more related to correct grammatical expres-
sions (e.g., sequences of POS tags). 
Our initial evaluations on the small in-house da-
ta collection showed that most of these features 
correlate well with human expert scores, both 
when using transcribed speech as well as when 
using ASR output. The absolute correlations for 
human transcriptions of speech range from r = 
0.144 (bleu_score) to r = 0.789 (re_match), and for 
ASR output from r = 0.194 (bleu_score) to r = 
0.614 (ed_score). The relative drop in correlation 
between these two conditions varies across fea-
tures, but is generally around 5%-15%, with 
re_match having a much larger performance drop 
from r = 0.789 for transcribed speech to r = 0.537 
for ASR output (32% relative decrease in perfor-
mance). 6
From this initial set of 15 features, we selected 
seven features based on feature performance, inter-
correlation analyses (i.e., avoiding features that 
have a high inter-correlation and measure a similar 
aspect of content), and considerations of construct, 
i.e., which features are representing content in a 
way that is consistent with what human experts 
would consider important in determining the con-
tent correctness of a response. This subset of seven 
 
                                                          
6 The correlation of one feature, pos_3, remained unchanged 
between the two conditions, and two features, pos_score and 
bleu_score, showed higher correlations for ASR output than 
for human transcriptions. 
features includes three features each from the clas-
ses of flexible string matching and string edit dis-
tance, and one feature (bleu_3) from the n-gram 
class. 
When evaluating these seven features on a larger 
data set, the smTrain and smEval sets of the 2012 
ETLA pilot data, we find absolute correlations be-
tween features and human holistic scores ranging 
from r = 0.286 to r = 0.480 for ASR output, and 
from r = 0.419 to r = 0.597 for transcriptions. The 
relative decrease in correlation between transcrip-
tions and ASR outputs ranges from 16% to 32% in 
these data sets (smTrain and smEval). The magni-
tude of content feature correlations observed in this 
study is similar to that of features related to fluen-
cy and pronunciation computed on spontaneous 
speech, as reported in Zechner et al (2009). In 
fact, due to the brevity of the moderately predicta-
ble responses in ETLA, features related to fluency 
and pronunciation achieve correlations of less than 
0.3 on this data set, making content features crucial 
for the assessment of speech here. 
When comparing the six content features that 
are identical between the original feature set of 15 
features (in-house data collection) and the final 
feature set, we observe a relative drop in feature 
correlation between the in-house data set and the 
smEval pilot data set between 1% (blue_3) and 
36% (ed_score), with an average decrease of 20%. 
This performance decrease can be explained by (1) 
the more challenging data set of the pilot, as indi-
cated, e.g., by a much higher word error rate of the 
ASR system (27% vs. 18%); and (2) the fact that 
the in-house data collection was much more con-
strained in terms of test taker response variation 
compared to the real-world pilot data. 
Since a subset of the ETLA responses was also 
scored analytically by human raters, we could fur-
ther compare the feature correlations between ho-
listic vs. analytic content scores (Section 5.2). We 
find that on smEval, for all features, absolute cor-
relations increase on human analytic content scores 
compared to human holistic scores. Although these 
differences are rather small (0.01 to 0.04), this is 
an indicator that our features are measuring what 
they are supposed to measure, since the holistic 
scores also take other dimensions of speech, such 
as fluency and pronunciation, into account. 
 
 
80
7 Conclusion and Future Work 
This paper presented a study whose aim was to 
conceptualize, implement and evaluate features to 
measure the content correctness of test takers? re-
sponses in a new assessment for EFL teachers 
whose native language is not English. 
We implemented and evaluated an initial set of 
15 content features from three feature classes: flex-
ible string matching, n-grams and string edit dis-
tance metrics. A subset of these features was then 
evaluated on a 2012 ETLA pilot administration, 
and we found correlations between features and 
human holistic scores in the range of r = 0.29 to r 
= 0.48 on ASR output. Correlations increased 
when comparing features with human analytic con-
tent scores. 
Finally, we compared a baseline regression scor-
ing model for prediction of human holistic scores 
without any content features to an extended model 
using seven content features and found that the 
model correlation substantially improved from r = 
0.33 (baseline) to r = 0.56 (extended model). 
Future work will include devising strategies on 
how to obtain RegEx features more quickly in a 
semi-automated way in order to reduce human la-
bor. Further, we plan more in-depth analysis of the 
feature performance across different test items and 
item types which potentially could lead to further 
improvements and refinements of our content fea-
tures. 
References  
Abeer Alwan, Yijian Bai, Matt Black, Larry Casey, 
Matteo Gerosa, Margaret Heritage, Markus Iseli, 
Barbara Jones, Abe Kazemzadeh, Sungbok Lee, 
Shrikanth Narayanan, Patti Price, Joseph Tepperman 
and Shizhen Wang. 2007. A system for technology 
based assessment of language and literacy in young 
children: the role of multiple information sources. 
Proceedings of IEEE International Workshop on 
Multimedia Signal Processing, 26-30. 
Yigal Attali and Jill Burstein. 2006. Automated essay 
scoring with e-rater? V.2.0. Journal of Technology, 
Learning, and Assessment, 4(3): 159-174. 
Miao Chen and Klaus Zechner. 2011. Computing and 
evaluating syntactic complexity features for 
automated scoring of spontaneous non-native 
speech. Proceedings of ACL, 722-731. 
Peter W. Foltz, Darrell Laham and Thomas K. 
Landauer. 1999. The intelligent essay assessor: 
Applications to educational technology. Interactive 
Multimedia Electronic Journal of Computer-
Enhanced Learning, 1(2). 
Horacio Franco, Harry Bratt, Romain Rossier, Venkata 
Rao Gadde, Elizabeth Shriberg, Victor Abrash and 
Kristin Precoda. 2010. EduSpeak?: A speech 
recognition and pronunciation scoring toolkit for 
computer-aided language learning applications. 
Language Testing, 27(3): 401-418. 
Dharmendra Kanejiya, Arun Kumary and Surendra 
Prasad. 2003. Automatic evaluation of students' 
answers using syntactically enhanced LSA. 
Proceedings of Workshop on Building Educational 
Applications Using Natural Language Processing, 
53-60. 
Claudia Leacock and Martin Chodorow. 2003. C-rater: 
Automated scoring of short-answer questions. 
Computers and Humanities,37: 389?405. 
Tom Mitchell, Terry Russell, Peter Broomhead and 
Nicola Aldridge. 2002. Towards robust 
computerised marking of free-text responses. 
Proceedings of International Computer Assisted 
Assessment Conference, 233-249. 
Michael Mohler and Rada Mihalcea. 2009. Text-to-text 
semantic similarity for automatic short answer 
grading. Proceedings of EACL, 567-575. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: A method for automatic 
evaluation of machine translation. Proceedings of 
ACL, 311-318. 
Shasha Xie, Keelan Evanini and Klaus Zechner. 2012. 
Exploring content features for automated speech 
scoring. Proceedings of NAACL-HLT, 103-111. 
Klaus Zechner, Derrick Higgins, Xiaoming Xi and 
David M. Williamson. 2009. Automatic scoring of 
non-mative spontaneous speech in tests of spoken 
English. Speech Communication, 51: 883-895. 
 
81
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 157?162,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Prompt-based Content Scoring for Automated Spoken Language Assessment
Keelan Evanini
Educational Testing Service
Princeton, NJ 08541, USA
kevanini@ets.org
Shasha Xie
Microsoft
Sunnyvale, CA 94089
shxie@microsoft.com
Klaus Zechner
Educational Testing Service
Princeton, NJ 08541, USA
kzechner@ets.org
Abstract
This paper investigates the use of prompt-
based content features for the automated as-
sessment of spontaneous speech in a spoken
language proficiency assessment. The results
show that single highest performing prompt-
based content feature measures the number
of unique lexical types that overlap with the
listening materials and are not contained in
either the reading materials or a sample re-
sponse, with a correlation of r = 0.450 with
holistic proficiency scores provided by hu-
mans. Furthermore, linear regression scor-
ing models that combine the proposed prompt-
based content features with additional spoken
language proficiency features are shown to
achieve competitive performance with scoring
models using content features based on pre-
scored responses.
1 Introduction
A spoken language proficiency assessment should
provide information about how well the non-native
speaker will be able to perform a wide range of tasks
in the target language. Therefore, in order to provide
a full evaluation of the non-native speaker?s speak-
ing proficiency, the assessment should include some
tasks eliciting unscripted, spontaneous speech. This
goal, however, is hard to achieve in the context of
a spoken language assessment which employs auto-
mated scoring, due to the difficulties in developing
accurate automatic speech recognition (ASR) tech-
nology for non-native speech and in extracting valid
and reliable features. Because of this, most spo-
ken language proficiency assessments which use au-
tomated scoring have focused on restricted speech,
and have included tasks such as reading a word / sen-
tence / paragraph out loud, answering single-word
factual questions, etc. (Chandel et al, 2007; Bern-
stein et al, 2010).
In order to address this need, some automated
spoken language assessment systems have also in-
cluded tasks which elicit spontaneous speech. How-
ever, these systems have focused primarily on a non-
native speaker?s pronunciation, prosody, and fluency
in their scoring models (Zechner et al, 2009), since
these types of features are relatively robust to ASR
errors. Some recent studies have investigated the
use of features related to a spoken response?s con-
tent, such as (Xie et al, 2012). However, the ap-
proach to content scoring taken in that study requires
a large amount of responses for each prompt to be
provided with human scores in order to train the
content models. This approach is not practical for a
large-scale, high-stakes assessment which regularly
introduces many new prompts into the assessment?
obtaining the required number of scored training re-
sponses for each prompt would be quite expensive
and could lead to potential security concerns for the
assessment. Therefore, it would be desirable to de-
velop an approach to content scoring which does not
require a large amount of actual responses to train
the models. In this paper, we propose such a method
which uses the stimulus materials for each prompt
contained in the assessment to evaluate the content
in a spoken response.
157
2 Related Work
There has been little prior work concerning auto-
mated content scoring for spontaneous spoken re-
sponses (a few recent studies include (Xie et al,
2012) and (Chen and Zechner, 2012)); however, sev-
eral approaches have been investigated for written
responses. A standard approach for extended writ-
ten responses (e.g., essays) is to compare the con-
tent in a given essay to the content in essays that
have been provided with scores by human raters us-
ing similarity methods such as Content Vector Anal-
ysis (Attali and Burstein, 2006) and Latent Semantic
Analysis (Foltz et al, 1999). This method thus re-
quires a relatively large set of pre-scored responses
for each test question in order to train the content
models. For shorter written responses (e.g., short an-
swer questions targeting factual content) approaches
have been developed that compare the similarity be-
tween the content in a given response and a model
correct answer, and thus do not necessarily require
the collection of pre-scored responses. These ap-
proaches range from fully unsupervised text-to-text
similarity measures (Mohler and Mihalcea, 2009) to
systems that incorporate hand-crafted patterns iden-
tifying specific key concepts (Sukkarieh et al, 2004;
Mitchell et al, 2002).
For extended written responses, it is less practical
to make comparisons with model responses, due to
the greater length and variability of the responses.
However, another approach that does not require
pre-scored responses is possible for test questions
that have prompts with substantial amounts of in-
formation that should be included in the answer. In
these cases, the similarity between the response and
the prompt materials can be calculated, with the hy-
pothesis that higher scoring responses will incorpo-
rate certain prompt materials more than lower scor-
ing responses. This approach was taken by (Gure-
vich and Deane, 2007) which demonstrated that
lower proficiency non-native essay writers tend to
use more content from the reading passage, which is
visually accessible and thus easier to comprehend,
than the listening passage. The current study inves-
tigates a similar approach for spoken responses.
3 Data
The data used in this study was drawn from TOEFL
iBT, an international assessment of academic En-
glish proficiency for non-native speakers. For this
study, we focus on a task from the assessment which
elicits a 60 second spoken response from the test
takers. In their response, the test takers are asked
to use information provided in reading and listen-
ing stimulus materials to answer a question concern-
ing specific details in the materials. The responses
are then scored by expert human raters on a 4-point
scale using a scoring rubric that takes into account
the following three aspects of spoken English pro-
ficiency: delivery (e.g., pronunciation, prosody, flu-
ency), language use (e.g., grammar, lexical choice),
and topic development (e.g., content, discourse co-
herence). For this study, we used a total of 1189
responses provided by 299 unique speakers to four
different prompts1 (794 responses from 199 speak-
ers were used for training and 395 responses from
100 speakers were used for evaluation).
4 Methodology
We investigated several variations of simple features
that compare the lexical content of a spoken re-
sponse to following three types of prompt materials:
1) listening passage: a recorded lecture or dialogue
containing information relevant to the test question
(the number of words contained in each of the four
listening passages used in this study were 213, 223,
234, and 318), 2) reading passage: an article or es-
say containing additional information relevant to the
test question (the number of words contained in the
two reading passages were 94 and 111), and 3) sam-
ple response: a sample response provided by the test
designers containing the main ideas expected in a
model answer (the number of words contained in the
four sample responses were 41, 74, 102, and 133).
The following types of features were investi-
gated for each of the materials: 1) stimulus cosine:
the cosine similarity between the spoken response
and the various materials, 2) tokens/response,
types/response: the number of word tokens / types
that occur in both the spoken response and each of
1Two out of the four tasks in this study had only listening
materials; responses to these tasks are not included in the results
for the features which require reading materials.
158
the materials, divided by the number of word to-
kens / types in the response,2 and 3) unique tokens,
unique types: the number of word tokens / types that
occur in both the spoken response and one or two
of the materials, but do not occur in the remaining
material(s).
As a baseline, we also compare the proposed
content features based on the prompt materials to
content features based on collections of scored re-
sponses to the same prompts. This type of feature
has been shown to be effective for content scoring
both in non-native essays (Attali and Burstein, 2006)
and spoken responses (Xie et al, 2012), and is com-
puted by comparing the content in a test response to
content models trained using responses from each of
the score points. It is defined as follows:
? Simi: the similarity score between the words
in the spoken response and a content model
trained from responses receiving score i (i ?
1, 2, 3, 4 in this study)
The Simi features were trained on a corpus of
7820 scored responses (1955 for each of the four
prompts), and we investigated two different meth-
ods for computing the similarity between the test
responses and the content models: Content Vector
Analysis using the cosine similarity metric (CVA)
and Pointwise Mutual Information (PMI).
The spoken responses were processed using an
HMM-based triphone ASR system trained on 800
hours of non-native speech (approximately 15% of
the training data consisted of responses to the four
test questions in this study), and the ASR hypothe-
ses were used to compute the content features.3
5 Results
We first examine the performance of each of the
individual features by calculating their correlations
with the holistic English speaking proficiency scores
provided by expert human raters. These results for
2Dividing the number of matching word tokens / types by
the number of word tokens in the response factors out the over-
all length of the response from the calculation of the feature.
3Transcriptions were not available for the spoken responses
used in this study, so the exact WER of the ASR system is un-
known. However, the WER of the ASR system on a comparable
set of spoken responses is 28%.
the training partition are presented in Table 1.4
Feature Set Feature r
stimulus cosine
listening 0.384
reading 0.176
sample 0.384
tokens/response
listening 0.022
reading 0.096
sample 0.121
types/response
listening 0.426
reading 0.142
sample 0.128
unique tokens
L?RS 0.116
L?RS? 0.162
LR?S 0.219
LR?S? 0.337
unique types
L?RS 0.140
L?RS? 0.166
LR?S 0.259
LR?S? 0.450
CVA
Sim1 0.091
Sim2 0.186
Sim3 0.261
Sim4 0.311
PMI
Sim1 0.191
Sim2 0.261
Sim3 0.320
Sim4 0.361
Table 1: Correlations of individual content features with
holistic human scores on the training partition
As Table 1 shows, some of the individual content
features based on the prompt materials obtain higher
correlations with human scores than the baseline
CVA and PMI features based on scored responses.
Next, we investigated the overall contribution of the
content features to a scoring model that takes into
account features from various aspects of speaking
proficiency. To show this, we built a baseline lin-
ear regression model to predict the human scores us-
ing 9 features from 4 different aspects of speaking
4For the unique tokens and unique types features, each row
lists how the prompt materials were used in the similarity com-
parison as follows: R = reading, L = listening, S = sample,
and ? indicates no lexical overlap between the spoken response
and the material. For example, L?RS indicates content from the
test response that overlapped with both the reading passage and
sample response but was not contained in the listening material.
159
proficiency (fluency, pronunciation, prosody, and
grammar) produced by SpeechRater, an automated
speech scoring system (Zechner et al, 2009), as
shown in Table 2.
Category Features
Fluency normalized number of silences
> 0.15 sec, normalized number
of silences > 0.495 sec, average
chunk length, speaking rate, nor-
malized number of disfluencies
Pronunciation normalzied Acoustic Model
score from forced alignment
using a native speaker AM,
average normalized phone du-
ration differnce compared to a
reference corpus
Prosody mean deviation of distance be-
tween stressed syllables
Grammar Language Model score
Table 2: Baseline speaking proficiency features used in
the scoring model
In order to investigate the contribution of the vari-
ous types of content features to the scoring model,
linear regression models were built by adding the
features from each of the feature sets in Table 1 to
the baseline features. The models were trained using
the 794 responses in the training set and evaluated
on the 395 responses in the evaluation set. Table 3
presents the resulting correlations both for the indi-
vidual responses (N=395) as well as the sum of all
four responses from each speaker (N=97).5
As Table 3 shows, all of the scoring models us-
ing feature sets with the proposed content features
based on the prompt materials outperform the base-
line model. While none of the models incorporat-
ing features from a single feature set outperforms
the baseline CVA model using features based on
scored responses, a model incorporating all of the
proposed prompt-based content features, all prompt-
based, does outperform this baseline. Furthermore,
a model incorporating all of the content features
(both the proposed features and the baseline CVA /
PMI features), all content, outperforms a model us-
5Three speakers were removed from the evaluation set for
this analysis since they provided fewer than four responses.
Feature Set response r speaker r
Baseline 0.607 0.687
+ types/response 0.612 0.701
+ tokens/response 0.615 0.700
+ unique tokens 0.616 0.695
+ stimulus cosine 0.630 0.716
+ unique types 0.658 0.761
+ CVA 0.665 0.762
+ all prompt-based 0.677 0.779
+ PMI 0.723 0.818
+ CVA and PMI 0.723 0.818
+ all content 0.742 0.838
Table 3: Performance of scoring models with the addition
of content features
ing only the baseline CVA and PMI features.6
6 Discussion and Conclusion
This paper has demonstrated that the use of content
scoring features based solely on the prompt stimu-
lus materials and a sample response is a viable al-
ternative to using features based on content mod-
els trained on large sets of pre-scored responses for
the automated assessment of spoken language profi-
ciency. Under this approach, automated scoring sys-
tems for large-scale spoken language assessments
involving spontaneous speech can begin to address
an area of spoken language proficiency (content ap-
propriateness) which has mostly been neglected in
systems that have been developed to date. Com-
pared to an approach using pre-scored responses for
training the content models, the proposed approach
is much more cost effective and reduces the risk
that test materials will be seen by test takers prior
to the assessment; both of these attributes are cru-
cial benefits for large-scale, high-stakes language as-
sessments. Furthermore, the proposed prompt-based
content features, when combined in a linear regres-
sion model with other speaking proficiency features,
outperform a baseline set of CVA content features
which use models trained on pre-scored responses,
6While the prompt-based content features do result in im-
provements, neither of these two differences are statistically sig-
nificant at ? = 0.05 using the Hotelling-Williams Test, since
both the magnitude of the increase and the size of the data set
are relatively small.
160
and they add further improvement to a model incor-
porating the higher performing baseline with PMI
content features.
The results in Table 1 indicate that the indi-
vidual features based on overlapping lexical types
(types/response and unique types) perform slightly
better than the ones based on overlapping lexical to-
kens (tokens/response and unique tokens). This sug-
gests that it is important for test takers to use a range
of concepts that are contained in the stimulus mate-
rials in their responses. Similarly to the result from
(Gurevich and Deane, 2007), Table 1 also shows that
the features measuring overlap between the response
and the listening materials typically perform better
than the features measuring overlap between the re-
sponse and the reading materials; the best individ-
ual feature, LR?S? for unique types, measures the
amount of overlap with lexical types that are con-
tained in the listening stimulus, but absent from the
reading stimulus and sample response. This indi-
cates that the use of content from the listening ma-
terials is a better differentiator among students of
differing language proficiency levels than reading
materials, likely because test takers generally have
more difficulty understanding the content from lis-
tening materials.
Table 1 also shows the somewhat counterintu-
itive result that features based on no lexical over-
lap with the sample response produce higher corre-
lations than features based on lexical overlap with
the sample response, when there is lexical overlap
with the listening materials and no overlap with the
reading materials. That is, the LR?S? feature out-
performs the LR?S feature for both the unique types
and unique tokens features sets. However, as shown
in Section 4, the sample responses varied widely
in length (ranging from 41 to 133 words), and all
were substantially shorter than the listening materi-
als, which ranged from 213 to 318 words. Therefore,
it is likely that many of the important lexical items
from the sample response are also contained in the
listening materials. Thus, the LR?S feature provided
less information than the LR?S? feature.
The features used in this study are all based on
simple lexical overlap statistics, and are thus triv-
ial to implement. Future research will investigate
more sophisticated methods of text-to-text similar-
ity for prompt-based content scoring, such as those
used in (Mohler and Mihalcea, 2009). Furthermore,
future research will address the validity of the pro-
posed features by ensuring that there are ways to fil-
ter out responses that are too similar to the stimulus
materials, and thus indicate that the test taker simply
repeated the source verbatim.
7 Acknowledgments
The authors would like to thank Yigal Attali for shar-
ing his ideas about prompt-based content scoring.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R? V.2. The Journal of Technol-
ogy, Learning, and Assessment, 4(3):3?30.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010. Validating automated speaking tests. Language
Testing, 27(3):355?377.
Abhishek Chandel, Abhinav Parate, Maymon Madathin-
gal, Himanshu Pant, Nitendra Rajput, Shajith Ikbal,
Om Deshmuck, and Ashish Verma. 2007. Sensei:
Spoken language assessment for call center agents. In
Proceedings of ASRU.
Miao Chen and Klaus Zechner. 2012. Using an ontol-
ogy for improved automated content scoring of spon-
taneous non-native speech. In Proceedings of the
7th Workshop on Innovative Use of NLP for Build-
ing Educational Applications, NAACL-HLT, Montre?al,
Canada. Association for Computational Linguistics.
Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.
1999. The intelligent essay assessor: Applications to
educational technology. Interactive Multimedia Elec-
tronic Journal of Computer-Enhanced Learning, 1(2).
Olga Gurevich and Paul Deane. 2007. Document
similarity measures to distinguish native vs. non-
native essay writers. In Proceedings of NAACL HLT,
Rochester, NY.
Tom Mitchell, Terry Russell, Peter Broomhead, and
Nicola Aldridge. 2002. Towards robust computerised
marking of free-text responses. In Proceedings of
the 6th International Computer Assisted Assessment
(CAA) Conference, Loughborough.
Michael Mohler and Rada Mihalcea. 2009. Text-to-
text semantic similarity for automatic short answer
grading. In Proceedings of the European Chapter of
the Association for Computational Linguistics (EACL
2009), Athens, Greece.
Jana Sukkarieh, Stephen Pulman, and Nicholas Raikes.
2004. Auto-marking 2: An update on the UCLES-
Oxford University research into using computational
linguistics to score short, free text responses. In
161
International Association of Educational Assessment,
Philadelphia.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 103?111, Montre?al, Canada. Association
for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
162
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 68?78,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automatic evaluation of spoken summaries: the case of language
assessment
Anastassia Loukina, Klaus Zechner, Lei Chen
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
aloukina@ets.org, kzechner@ets.org, lchen@ets.org
Abstract
This paper investigates whether ROUGE, a
popular metric for the evaluation of au-
tomated written summaries, can be ap-
plied to the assessment of spoken sum-
maries produced by non-native speakers
of English. We demonstrate that ROUGE,
with its emphasis on the recall of infor-
mation, is particularly suited to the as-
sessment of the summarization quality of
non-native speakers? responses. A stan-
dard baseline implementation of ROUGE-
1 computed over the output of the au-
tomated speech recognizer has a Spear-
man correlation of ? = 0.55 with experts?
scores of speakers? proficiency (? = 0.51
for a content-vector baseline). Further in-
creases in agreement with experts? scores
can be achieved by using types instead of
tokens for the computation of word fre-
quencies for both candidate and reference
summaries, as well as by using multiple
reference summaries instead of a single
one. These modifications increase the cor-
relation with experts? scores to a Spear-
man correlation of ? = 0.65. Furthermore,
we found that the choice of reference sum-
maries does not have any impact on per-
formance, and that the adjusted metric is
also robust to errors introduced by auto-
mated speech recognition (? = 0.67 for hu-
man transcriptions vs. ? = 0.65 for speech
recognition output).
1 Introduction
In this paper we explore whether metrics com-
monly used for the automated evaluation of writ-
ten summaries can be used to evaluate spoken
summaries in the context of language assessment.
The performance of automatic summarization
systems is routinely evaluated using content met-
rics such as ROUGE (Lin and Rey, 2004), which
measures the n-gram overlap between the candi-
date summary and a set of reference summaries
(see also Rankel et al. (2013) for historical back-
ground). ROUGE is a recall-oriented metric in-
spired by its precision-oriented counterpart BLEU,
developed to evaluate machine translations (Pap-
ineni et al., 2002). Recent research in this area has
been focused on identifying the most reliable vari-
ants of ROUGE and best practices in the application
of the metric (Owczarzak et al., 2012; Rankel et
al., 2013). These studies (reviewed in more detail
in Section 2.1) showed that less commonly used
variants of ROUGE may in fact be more consistent
with human judgments, at least in the context of
automatic summary evaluation.
Beyond the research in automatic summariza-
tion systems, ROUGE has also been used to eval-
uate written summaries in the context of educa-
tional assessment. Madnani et al. (2013) showed
that one of the variants of ROUGE, in combination
with other metrics, performed consistently well
for the automated scoring of written responses to
summary tasks produced by middle- and high-
school students. They did not investigate the effect
of using other variants of ROUGE.
In this paper, we explore whether ROUGE can be
used to automatically evaluate the content cover-
age of spoken summaries produced by non-native
speakers in the context of language assessment.
As in case of automatic text summaries, the hu-
man raters who score these responses are asked
to assess whether the summary accurately con-
veys the information contained in the stimulus.
While the length of the spoken responses is more
loosely constrained than in case of automatic text
summaries, human raters do not penalize for ex-
traneously irrelevant language. Therefore recall-
oriented ROUGE is an attractive evaluation metric
for this task.
At the same time, unlike automatic text sum-
68
maries, spoken summaries are abstractive and of-
ten contain ungrammatical sequences, repetitions,
repairs, and other disfluencies. Further ?noise?
is introduced by transcription errors generated by
the automated speech recognition system. In this
study, we assess whether (a) ROUGE is robust
against this type of noise; (b) how many refer-
ence summaries are necessary to obtain reliable
evaluation; and (c) how the choice of specific ref-
erence summaries affects the performance of the
metric (Section 4.1). We also assess which vari-
ants of ROUGE have the most agreement with hu-
man judgments on this type of summary and what
adjustments can be made to mitigate the effects
of disfluencies and errors introduced by automated
speech recognition (Section 4.2). Finally, we test
how well our adjusted variant of ROUGE can pre-
dict the human scores on unseen data (Section
4.3).
2 Related work
2.1 The application of ROUGE to evaluation
of automatic text summarization
There exist various versions of ROUGE which dif-
fer in terms of the length of their n-grams, the use
of skip-bigrams, the application of stemming, and
the exclusion of stop-words. Several studies have
compared these variants to identify those most
consistent with human judgments. In earlier work,
Lin (2004) reported that variants based on uni-
grams and skip-bigrams (ROUGE-SU4) or bigrams
alone (ROUGE-2) performed best. ROUGE-2 was
also identified as the best variant more recently
by Owczarzak et al. (2012). Rankel et al. (2013)
found that linear combinations of these metrics
with ROUGE based on longer n-grams are more ac-
curate in finding significantly different systems.
Previous work also explored various methods
of text pre-processing prior to the computation of
ROUGE, including stemming and the removal of
stop-words, neither of which had any substantial
effect on the performance of ROUGE (Lin and Rey,
2004; Owczarzak et al., 2012). Owczarzak et al.
(2012) reported that the agreement with human
judgments was, in fact, higher if the stop-words
were retained.
All applications discussed so far used ROUGE
to evaluate the textual summarization of written
texts. There have also been attempts to apply
this metric to text summaries of speech data with
mixed results (see Nenkova and McKeown (2011)
for a review). ROUGE performed reasonably well
for the evaluation of text summaries of spoken pre-
sentations (Hirohata et al., 2005), but was not cor-
related with the summary accuracy of summaries
of meetings or conversations (although see (Penn
and Zhu, 2008)).
Most of this work was performed on extractive
summaries produced by summarization systems
that used multiple summaries to evaluate each sys-
tem. In this study, we explore the application of
ROUGE to the evaluation of abstractive summaries
produced by students in a language assessment
context with an aim of producing a separate evalu-
ation for each summary. Furthermore, the fact that
these are spoken responses adds an extra layer of
complexity to the analysis, therefore the results of
previous studies cannot directly be applied to this
new context.
2.2 Previous approaches to the content
evaluation of spoken summaries for
assessment purposes
The research on the automated scoring of con-
tent accuracy in a language assessment has pri-
marily focused on the evaluation of written essays.
Most previous approaches in this area have used
so-called ?bag-of-words?-based models, gleaned
from the discipline of information retrieval. The
basic idea is that an essay is considered to be
highly content relevant to a given topic when it
contains words that are similar to those seen in
previously collected essays with high human-rater
scores. For instance, Attali and Burstein (2006)
used a vector-space model to compute the co-
sine similarities between word vectors found in
an essay to be automatically scored and word vec-
tors comprising previously scored essays with the
same human-rater score. In a similar vein, Foltz
et al. (1999) computed a compressed vector space
based on singular value decomposition for a set
of document-word vectors, called latent semantic
analysis, and then computed similarity scores for
essays based on this more compact representation.
It should be noted, though, that since all of these
models do not take word sequences into account,
they must be considered knowledge-poor in that
they cannot distinguish between syntactic roles
or a list of random words versus a well-formed
sentence. In operational systems, such bag-of-
words similarity features are combined with fea-
tures which evaluate grammar and other aspects
69
of language use; therefore a random list of con-
tent words is unlikely to lead to a high overall
score. However, finer-grained distinctions such as
negations or subject-object relationships between
words are often lost.
Applications of these methods to spontaneous
speech in spoken-language assessments have been
conducted much more recently as this domain of
language assessment relies on the output of Au-
tomatic Speech Recognition systems (ASR) that
typically have a fairly high word-error rate. These
errors can negatively affect the accuracy of the
methods developed for written responses. Fur-
thermore, spoken responses differ in many proper-
ties from written ones (Biber et al., 2004) and the
validity of existing methods for assessing speech
needs to be established before they can be used
for operational scoring.
Xie et al. (2012) presented experiments using
content features on spontaneous-speech data based
on vector-space models, latent semantic analysis,
as well as point-wise mutual information. Some
of these content features showed higher correla-
tions with human scores than features measuring
other aspects of speaking proficiency, such as flu-
ency or pronunciation. Chen and Zechner (2012)
also used a vector space model for the scoring of
spontaneous speech, but extended it by using the
ontological information contained in WordNet. Fi-
nally, Xiong et al. (2013) used a variety of ap-
proaches to capture the content of spontaneous re-
sponses from the same corpus that we are investi-
gating in this paper. Approaches varied from com-
puting the overlap between key words in the stim-
uli and responses to a more traditional vector space
model based on content vector analysis.
While these approaches have good correlations
with human scores, they have a number of short-
comings. The best performing method suggested
by Xiong et al. (2013) requires the manual annota-
tion of the relevant key words for each prompt be-
fore the computation of the metric. Vector space
models do not have this limitation, but they require
a substantial number of reference summaries to
achieve consistent results. Supporting this point,
Chen (2013) showed that at least 50 reference re-
sponses were necessary to obtain moderate agree-
ment between the cosine similarity measure and
human judgments, with further improvement in
agreement as the number of reference responses
is increased to 200. These limitations pose prac-
tical difficulties when new items are added to the
tests: the computation of content metrics for each
new item requires either a manual annotation or a
relatively large number of reference responses.
ROUGE appears promising in this context since
it does not have either of these limitations. First,
the computation of ROUGE does not require man-
ual annotation. Second, research on the evalua-
tion of written summaries suggests that relatively
few reference summaries may be necessary to ob-
tain reliable results, e.g., only four references were
used for the summary evaluation at the Text Anal-
ysis Conference (Rankel et al., 2013). In addition,
the recall-based nature of ROUGE is well-aligned
with the evaluation criteria for these responses.
Therefore in this paper, we explore whether any of
the variants of ROUGE can be successfully applied
to the content scoring of spoken summaries and
what modifications may be necessary to achieve
optimal performance.
3 Data and methodology
3.1 Description of the corpus
The study is based on a corpus of responses
collected during the pilot administration of the
TOEFL
R
?Junior
TM
Comprehensive test, an inter-
national assessment of English proficiency tar-
geted at middle-school students aged from 11 to
15 (see also Xiong et al. (2013) who used a subset
of this corpus).
The corpus used in this study included 5,934
spoken responses produced by 1,611 speakers; all
learners of English as a foreign language residing
in different countries. In addition to a read-aloud
task that was not relevant for this paper, the speak-
ers were presented with four other tasks. First, the
speakers were asked to describe a sequence of six
pictures. For the remaining three taks, the speak-
ers listened to one announcement and two frag-
ments from a lecture and were then asked to sum-
marize the content of what they heard. The stu-
dents were provided with a list of concepts that test
takers were expected to cover in their responses.
For example, a student may have listened to
a teacher giving an assignment in history class.
1
This assignment required the class to go to the li-
brary, look up information about the water supply
in old and modern cities, answer the questions on
their worksheet, and write a short paragraph about
1
http://toefljr.caltesting.org/sampletest/s-
historylesson.html
70
their findings. The students were then asked to re-
spond to the following prompt:
Imagine that your classmate was not
in class today. Tell your classmate
about what the history teacher asked
the students to do. Be sure to talk about
the following:
- the library
- the worksheet
- the homework
The corpus contained responses to 24 different
prompts with 6 different sets of prompts. Each
speaker only answered one set of prompts giving
4 responses per speaker. The recording time for
each response was limited to 60 seconds. The ac-
tual number of words varied between participants
with an average 72 words per response (? = 29).
From the originally recorded 6,444 responses,
we excluded from further analysis 510 responses
(about 8%), which contained either no speech or
where the quality of the recording was too low for
further analysis. All remaining 5,934 responses
were scored on a scale of 1-4 by two expert human
raters on a holistic scale that reflects all aspects
of speaking proficiency, including pronunciation,
grammar, and content coverage.
2
For content cov-
erage, the raters were asked to consider whether
the key information contained in the prompt was
conveyed accurately or, in case of the picture de-
scription prompt, whether the story was complete.
When the difference in the scores assigned by the
two raters was greater than 1, the final score was
assigned by an adjudicator.
The corpus was divided into non-overlapping
training and testing partitions. The training par-
tition contained 3,337 responses from 915 speak-
ers and the test partition contained 2,597 spoken
responses from 696 speakers. Both partitions in-
cluded responses for the same prompts but there
was no speaker overlap.
All responses were converted to text using
a state-of-the-art automatic speech recognizer
(ASR) with constrained vocabulary (see Evanini
and Wang (2013) for further details). To evalu-
ate the effect of the errors that may have been in-
troduced by the ASR system, all responses were
2
see http://www.ets.org/s/toefl junior/pdf/toefl junior
comprehensive speaking scoring guides.pdf for the scoring
rubrics
transcribed manually by professional human tran-
scribers. Comparison with the human transcrip-
tion showed that the ASR word error rate for this
corpus was 26.5% for picture narration tasks and
29.4% for the summarization tasks.
3.2 Computation of the metrics
Evaluation metrics. ROUGE was computed using
equation (1) as an n-gram (gr
n
) overlap between
candidate summary and each summary (S) from
the set of reference summaries (RS).
ROUGE
N
=
?
S?RS
?
gr
n
?S
Count
overlap
(gr
n
)
?
S?RS
?
gr
n
?S
Count(gr
n
)
(1)
We used n-grams whereby n was in a range
from 1 to 4 (ROUGE 1-4) and a combination
of unigrams with skip-bigrams with maximum
step of four words (ROUGE-SU1-4). Finally,
we also computed a combined measure ROUGE-
ALL which is the geometrical mean of ROUGE-1?
ROUGE-4, computed by using the same smoothing
procedure as for BLEU (Papineni et al., 2002).
We used the cosine distance (CVA) between the
response and reference summaries as a baseline
metric as this metric is commonly used for eval-
uating document similarity in the context of lan-
guage assessment. CVA was computed as the co-
sine distance between candidate responses and the
same reference responses as used for the com-
putation of ROUGE. All term frequencies were
weighted using tf-idf where tf is the frequency of
a term in a given response and idf is the inverse
document frequency. idf frequencies were com-
puted based on all of the responses in the corpus.
Reference summaries. The reference sum-
maries were selected from responses with the
highest human rater final score (4). This approach
is similar to using system outputs as pseudo-
models for the evaluation of machine-translation
or automatic-summarization systems (cf. Louis
and Nenkova (2013)). It has also been success-
fully applied to the content assessment of written
answers by Madnani et al. (2013) who used one
randomly selected highly scored summary as a ref-
erence summary.
Since previous work on summarization eval-
uation showed that multiple summaries increase
the reliability of evaluations (Louis and Nenkova,
2013; Nenkova and McKeown, 2011), we tested
71
how many summaries were necessary to achieve
consistent results. We therefore computed ROUGE
for each response using up to 10 randomly se-
lected responses with final score of 4. To inves-
tigate the effect that different choices of reference
summaries may have on the metrics, we repeated
the analysis for 20 randomly selected sets of refer-
ence responses.
The corpus did not contain a sufficient num-
ber of responses with the maximum score for each
prompt. Therefore, this part of the analysis was
based on a subset of 1,784 responses selected from
the training partition. This set included only 12
prompts for which human raters assigned a score
of 4 to more than 11 responses.
Text preprocessing. For the evaluation of writ-
ten summaries, ROUGE is usually computed using
the raw counts of all of the terms. In addition to us-
ing this classical approach using unstemmed terms
(?all?), we also computed ROUGE using three other
approaches: (1) excluding all stop-words (?Non-
stop?); (2) setting the frequency of all n-grams
within each summary to 1, that is, counting types
instead of tokens (?Types?); (3) excluding all stop-
words and counting types only (?Non-stop types?).
Finally, we computed all of these ROUGE variants
using raw text as well as lemmatized text. As a re-
sult, we computed 72 different variants of ROUGE
for each response and each combination of refer-
ence summaries: nine different types of ROUGE
(eight different n-gram lengths and ROUGE-ALL)
computed using four different methods of text pro-
cessing and two possible approaches to lemmati-
zation. All of the computations were done both on
ASR and manual transcriptions.
3.3 Evaluation
We computed the Spearman?s rank correlation be-
tween the metric and the holistic score assigned
by the first rater to identify the best method of
computing ROUGE and the optimal number of ref-
erences. Performance of the metric may be af-
fected by properties of the prompt (cf. (Nenkova
and Louis, 2008)), therefore we first analyzed each
prompt separately and then selected the variants
that achieved the highest performance across all
of the prompts. Since correlation coefficients are
not normally distributed, we used several non-
parametric methods to identify significant differ-
ences including non-parametric bootstrapping and
non-parametric ANOVAs. These analyses were
done using the data from the training partition of
the corpus.
We then evaluated how well the selected vari-
ants of ROUGE predicted human scores using a lin-
ear regression model trained on all of the data from
the training partition using pooled data from all of
the prompts. The model was tested on an unseen
test partition that had not been used for any of the
analyses.
Finally, we tested whether the new metrics im-
proved the performance of the automated scoring
engine for spoken responses. The current system
assigns scores based on the linear combination of
features with empirical weights obtained by train-
ing scoring models on scores assigned by expert
raters (Zechner et al., 2009; Higgins et al., 2011).
Current features measure various aspects of speak-
ing proficiency such as fluency, pronunciation, and
grammar usage. The performance of the system is
evaluated with correlations and quadratic kappas
between the scores assigned by the human raters
and rounded predicted scores.
4 Results
All analyses were performed twice: each for met-
rics computed using ASR and manual transcrip-
tions. We found that although the exact values
of the correlation coefficients differed across these
two transcriptions, the overall pattern of results
remained the same. There was also a high cor-
relation in metric values between the two types
of transcription (Pearson?s r for different types of
ROUGE varied between 0.81 for ROUGE-4 and 0.9
for ROUGE-1). Since automated scoring relies on
the output of automatic speech recognition, all nu-
merical results reported in the main text of this
section are based on ASR output. The tables re-
port the numbers for both ASR and manual tran-
scriptions.
4.1 Number and choice of reference
responses
Number of references. To identify the optimal
number of references for each prompt and met-
rics, we first found N
best
, which had the high-
est correlation with human scores and then iden-
tified the lowest number of reference summaries
for which the correlation coefficient was not sig-
nificantly lower than the correlation coefficient for
N
best
.
Comparisons between different correlations
72
were performed using the general method sug-
gested by Zou (2007) for comparing overlapping
correlations as implemented by Baguley (2012,
p.224) but we used bootstrapped confidence inter-
vals (Wilcox, 2009). Confidence intervals for each
correlation coefficient were constructed using pi-
geonhole bootstrapping (Owen, 2007) with 1,000
samples. For each N reference, we pooled the
values computed for 20 randomly selected sets of
different reference summaries. We then indepen-
dently sampled responses and sets of references
and selected values at each bootstrap repetition
at the intersection of the two samples. The con-
fidence intervals were constructed using the ad-
justed percentile method (Davison and Hinkley,
1997, p. 203-213). Since this analysis is more sen-
sitive to Type II errors (?false negatives?), we set
the significance threshold at ? = 0.15.
The optimal number of references varied be-
tween prompts, metrics, and methods of compu-
tation, but never exceeded 8. On average, opti-
mal performance was achieved with 3 references.
More references were required to achieve optimal
performance for ROUGE based on longer n-grams
(using the Kruskal-Wallis test, a non-parametric
analysis of variance, p < 2.2 ? 10
?16
). For ex-
ample, two references on average were required
to achieve reliable results for ROUGE-1, but for
ROUGE-4 this number was four references. The
required number of references was also signifi-
cantly dependent on the prompt (Kruskal-Wallis
test, p < 2.2 ? 10
?16
) with averages varying be-
tween two and four. When the number of ref-
erences was equal to or greater than the optimal
number, there were no significant differences in
the correlation coefficients across the different ref-
erence models.
For the analysis in the following section each
of the 72 variants of ROUGE for each prompt was
computed using the optimal N references identi-
fied for this variant and prompt.
4.2 Types of ROUGE and different methods of
computation
The correlation coefficients between the summa-
rization metrics and human ratings depended on
the length of n-grams (Kruskal-Wallis test p <
2.2 ? 10
?16
). While all types of ROUGE were pos-
itively correlated with human ratings, the corre-
lation coefficients were the highest for ROUGE-1
and ROUGE-SU2-4, which performed significantly
better than ROUGE-3-4 and the combined mea-
sures ROUGE-ALL (post-hoc Tukey HSD test on
ranked observations, p varied from p < 1 ? 10
?10
to 2.804 ? 10
?4
). The average correlations across
the different types of text pre-processing for ASR
and manual transcriptions are shown in Table 1.
Metrics ASR output Manual
ROUGE-1 0.616 0.637
ROUGE-SU4 0.592 0.608
ROUGE-SU3 0.595 0.609
ROUGE-SU2 0.594 0.613
ROUGE-SU1 0.598 0.619
ROUGE-ALL 0.523 0.527
ROUGE-2 0.553 0.560
ROUGE-3 0.468 0.461
ROUGE-4 0.366 0.357
Table 1: Average correlation coefficient with hu-
man scores (Spearman?s ?) across different meth-
ods of computation for ROUGE based on n-grams
of different lengths. The table shows the results
for metrics computed based on ASR and manual
transcriptions.
The effect of text pre-processing differed
across the metrics: for metrics that relied on
consecutive n-grams with n>2, the removal of
stop-words led to further drops in performance
(Kruskal-Wallis test p = 4.4 ? 10
?5
). For ROUGE
based on unigrams and skip-bigrams, counting
only type frequencies led to a significant im-
provement in performance (Kruskal-Wallis test,
p = 0.00017). Correlations for the different types
of pre-processing for the measures that performed
the best are given in Table 2. Lemmatization did
not make a significant difference to metric perfor-
mance.
Pre-processing ASR ouput Manual
All 0.573 0.606
Non-stop 0.585 0.600
Non-stop types 0.601 0.617
Types 0.622 0.634
Table 2: Average correlation coefficient with
human proficiency score (Spearman?s ?) across
ROUGE-1 and ROUGE-SU1-4 for different meth-
ods of text processing. The table shows the results
for metrics computed based on ASR output and
manual transcriptions.
73
Finally, a summarization metric performed bet-
ter on tasks that required the test takers to
summarize an announcement or lecture (average
?? = 0.653 for ROUGE-1 and ROUGE-SU1-4) rather
than on tasks that required them to describe a pic-
ture sequence (average ?? = 0.437, Mann-Whitney-
Wilcox test, a non-parametric test for comparing
two independent samples, p < 2.2 ? 10
?16
).
4.3 Evaluation of the final model
Analysis by prompt showed that the variants of
ROUGE that included unigram counts (ROUGE and
ROUGE-SU1-4) had the best correlations with hu-
man scores across all prompts. Further improve-
ment in their performance was obtained by count-
ing type frequencies only and by using several ref-
erence summaries. The optimal N references for
these variants of ROUGE varied between prompts,
but never exceeded four which was therefore se-
lected as the optimal N references for this corpus.
Based on these results we computed ROUGE-
1 metrics for all responses in the original train-
ing partition using four randomly selected, highly
scored responses for each prompt and ?types?
method of pre-processing. We then compared
it with two baselines: (1) cosine distance (CVA)
computed using type frequencies only and the
same four references, and (2) na??ve implementa-
tion of ROUGE-1 computed using one randomly
selected reference summary and raw frequencies
(tokens). The newly adjusted version of ROUGE-
1 metrics performed significantly above the base-
lines (using Zou?s method for the comparison of
overlapping correlations with confidence intervals
constructed at ? = 0.001). The correlation coeffi-
cients are shown in Table 3.
Metric ASR output Manual
New ROUGE-1 0.652 0.673
Base ROUGE-1 0.55 0.589
CVA 0.508 0.451
Table 3: Correlation coefficients with human
scores (Spearman?s ?) for the entire training parti-
tion for the newly adjusted version of ROUGE and
the baseline metrics. The table shows the results
for metrics computed based on ASR and manual
transcriptions.
We then trained a standard linear regression
model using the human scores as the dependent
variables and summarization metrics as indepen-
dent variables. The accuracy of prediction was
evaluated using two metrics as suggested, for ex-
ample, by Williamson et al. (2012): quadratic
weighted kappa (?) and Pearson?s correlation co-
efficient (r) between the observed and predicted
scores. For computation of ?, the predicted scores
were trimmed to the range of human scores and
rounded to the nearest integer.
Repeated 10-fold cross-validation on the train-
ing partition showed that a model based on
ROUGE-1 produced averages of r? = 0.65
(? = 0.031) and ?? = 0.54 (? = 0.036). The model
based on a linear combination of several ROUGE
variants using longer n-grams and a recursive fea-
ture elimination (Kuhn and Johnson, 2013, p. 480)
did not show any improvement in the performance
as compared to a model based on a single ROUGE-
1.
Finally, we tested the performance of the met-
rics on an unseen test set that had not been used
for any previous analyses. We tested both the
model based solely on the content metric as well
as on the performance of the content metrics in
combination with 11 other features used for the
automated scoring of spoken responses that mea-
sure pronunciation accuracy, prosody, fluency, and
grammar. These results are presented in Table
4. Note that the performance of the content-only
model based on the new ROUGE-1 was in line
with the estimates obtained on the training set.
Zou?s method for comparing overlapping correla-
tions showed that in all cases, the difference be-
tween the model based on an adjusted ROUGE and
the baselines was significant at ? = 0.001. In line
with previous results, the models based on manual
transcriptions showed better agreement with hu-
man scores than the models based on ASR output.
Table 4 shows that the addition of content met-
rics lead to relatively small increase in the perfor-
mance of the integrated models. This is due to the
fact that for most speakers different aspects of pro-
ficiency tend to be correlated. For example, more
fluent speakers also achieve higher ROUGE scores
(the correlation between ROUGE and pronuncia-
tion accuracy (Chen et al., 2009) is r = 0.62). As
a result, a model which measures only one as-
pect of performance such as fluency may some-
times reach near optimal performance and adding
further predictors leads to a relatively small gain.
When interpreting these results, it is important to
bear in mind that empirical performance is only
74
Model
ASR Manual
r ? r ?
Content only
CVA 0.492 0.340 0.469 0.303
Base ROUGE 0.587 0.440 0.632 0.489
New ROUGE 0.655 0.540 0.700 0.590
Integrated model
No content 0.678 0.565 0.678 0.565
CVA 0.691 0.600 0.698 0.602
Base ROUGE 0.700 0.597 0.719 0.610
New ROUGE 0.715 0.617 0.738 0.652
Table 4: Performance of the linear regression
model based on one content metric and an ?inte-
grated? model based on 11 features that measure
pronunciation, fluency, and grammar before and
after the addition of ?Base ROUGE,? ?CVA? or ?New
ROUGE.? The table shows the correlation coeffi-
cients (Pearson?s r) and quadratic weighted kappa
kappas (?) between the predicted scores and hu-
man ratings for the unseen test set. The agree-
ment between the two expert raters on this dataset
is ? = 0.69.
one aspect of evaluation of automated scoring sys-
tems. In addition to high agreement with hu-
man scores, operational automatic scoring systems
also need to show good construct representation
by covering different aspects of speaker perfor-
mance (Williamson et al., 2012). This requirement
ensures the validity of automated scores and pre-
vents future test-takers from fine-tuning their per-
formance to one particular feature measured by the
scoring system. Therefore the addition of ROUGE
to the automated scoring model serves both goals:
it improves the agreement with human raters and
also expands the construct coverage of the model.
5 Discussion
Summarization metrics can be successfully used
to evaluate spoken summaries in the context of
language assessment. Although the na??ve imple-
mentation of ROUGE had good agreement with the
scores assigned by human raters, several modifica-
tions led to a further increase in the performance.
Some of our findings show common patterns
with what has previously been reported for written
summaries. ROUGE-1, ROUGE-SU4 and ROUGE-
2 are the three variants of ROUGE most com-
monly used for the evaluation of automatic text
summaries. Our results showed that the first two
of these measures (ROUGE-1 and ROUGE-SU4)
were also most suitable for content assessment
of spoken responses. We note that both of these
measures include unigram counts. More recently,
Rankel et al. (2013) and Owczarzak et al. (2012)
reported that metrics based on longer consecutive
n-grams or linear combinations of different vari-
ants are more accurate. We did not find this for our
data. Since our data represents abstractive sum-
maries, poor performances of longer n-grams is
not surprising. Finally, as in the case of written
summaries, there was no effect of lemmatization
while the removal of stop-words sometimes led to
a decrease in performance.
Similar to written summaries, the use of more
than one reference summary improved the perfor-
mance. We found that the optimal number of refer-
ence summaries varied between prompts and met-
rics. For ROUGE-1, this number never exceeded
four across all prompts in our corpus. Further-
more, we found that the choice of reference sum-
maries from the pool of highly scored responses
had no significant effect on the performance of the
metric.
In addition to good agreement with human
scores, metrics used for automated scoring also
need to match the construct of interest, as defined
by the assessment program (Williamson et al.,
2012). The scoring guidelines for the tasks used
in this paper ask raters to judge whether the key
information contained in the prompt has been con-
veyed accurately. A notable difference between
ROUGE and previously used metrics is that as a
recall measure, ROUGE does not penalize for the
lack of precision. Our results suggest that a recall-
oriented approach has better agreement with hu-
man judgments than cosine distance which com-
bines both precision and recall.
Recall-based approaches are sensitive to the
length of candidate responses. In the case of auto-
matic summary evaluation, the length of the sum-
maries is limited to a predefined number of words.
In this data, the length of the responses is limited
more loosely by the time available to record the
response and the actual number of words varied
between the responses. Therefore, a recall-based
approach may produce inflated scores by assign-
ing higher metric values to a response which con-
tains multiple repetitions of the same n-gram as
long as the n-gram occurs several times in the ref-
erence response. The common occurrence of re-
75
pairs and repetitions in spoken speech further ag-
gravates this problem further. We addressed this
issue by only counting type frequencies, which
also improved agreement with human judgments.
The adjusted metric had better agreement with
human judgments than other ?bag-of-words? ap-
proaches such as the cosine-based measure com-
monly used for content scoring that requires a
much larger set of model responses than ROUGE.
It also performed equally well on human and ASR
transcriptions and did not require any manual an-
notation of the data. We also found that the per-
formance of ROUGE depended on the task: we
obtained better agreement for tasks that required
the student to summarize a stimulus rather than
tasks that required the student to describe a se-
quence of pictures. While in both cases the stu-
dents produced short summary-like texts, the pic-
ture description task allowed for greater variabil-
ity between the responses than the summarization
task and, therefore, recall-oriented comparisons
with highly-scored responses showed less agree-
ment with human scores.
As a ?bag-of-words? approach, ROUGE-1 has
the same shortcomings as other methods discussed
in Section 2.2 in that it doesn?t distinguish be-
tween syntactic roles. While variants based on
longer n-grams could in theory address this, our
results showed that neither a linear nor a geomet-
ric combination of these variants with ROUGE-1
improved agreement with human scores. This is-
sue has also been acknowledged in the context of
non-extractive text summarization and new met-
rics such as AutoSummEng (Giannakopoulos and
Karkaletsis, 2011) have been developed to address
it. Future research will include the conceptualiza-
tion and development of metrics that can address
the content accuracy of spoken summaries beyond
the ?bag-of-words? approach.
6 Conclusion
In this paper we applied ROUGE, a recall-based
metrics for evaluation of written summaries to the
automatic assessment of spoken summaries pro-
duced by non-native speakers of English. We per-
formed a thorough evaluation of different types of
ROUGE by varying the length of n-grams, vari-
ous methods of frequency computation, and text-
preprocessing. We also explored the effect of the
number of reference summaries. We found that
the standard baseline implementation of ROUGE-1
computed over the output of the automated speech
recognizer showed good agreement with expert
ratings and performed better than the cosine sim-
ilarity measure commonly used for the evaluation
content of spoken responses. A further increase in
agreement with human ratings could be achieved
by using types instead of tokens for the frequency
computation of both candidate and reference sum-
maries. We also found that the use of several refer-
ence summaries improves the performance of the
metric, but only four reference summaries were
necessary to achieve reliable results.
Acknowledgments
We would like to thank Keelan Evanini, Nitin
Madnani, Xinhao Wang, Derrick Higgins and
three anonymous reviewers for their helpful com-
ments and suggestions and Ren?e Lawless for edit-
ing assistance.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater V. 2. The Journal of Technology,
Learning and Assessment, 4(3):1?30.
Thomas Baguley. 2012. Serious Stats: A guide to ad-
vanced statistics for the behavioral sciences. Pal-
grave Macmillan.
Douglas Biber, Susan M. Conrad, Randi Reppen, Pat
Byrd, Marie Helt, Victoria Clark, Viviana Cortes,
Eniko Csomay, and Alfredo Urzua. 2004. Rep-
resenting language use in the university: analysis
of TOEFL 2000 Spoken and Written academic lan-
guage corpus. Educational Testing Service, Prince-
ton.
Miao Chen and Klaus Zechner. 2012. Using an on-
tology for improved automated content scoring of
spontaneous non-native speech. In Proceedings of
the 7th Workshop on Building Educational Applica-
tions Using NLP, pages 86?94, Stroudsburg, PA. As-
sociation for Computational Linguistics.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 442?449, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lei Chen. 2013. Applying unsupervised learning to
support vector space model based speaking assess-
ment. Proceedings of the 8th Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, Atlanta, Georgia, pages 58?62.
76
Anthony C. Davison and David V. Hinkley. 1997.
Bootstrap Methods and their Application (Cam-
bridge Series in Statistical and Probabilistic Mathe-
matics). Cambridge University Press.
Keelan Evanini and Xinhao Wang. 2013. Automated
speech scoring for non-native middle school stu-
dents with multiple task types. Proceedings of In-
terspeech 2013, Lyon, France, pages 2435?2439.
Peter W. Foltz, Darrell Laham, and Thomas K. Lan-
dauer. 1999. Automated essay scoring: applica-
tions to educational technology. In B. Collis and
R. Oliver, editors, Proceedings of World Confer-
ence on Educational Multimedia, Hypermedia and
Telecommunications 1999, pages 939?944.
George Giannakopoulos and Vangelis Karkaletsis.
2011. AutoSummENG and MeMoG in Evaluat-
ing Guided Summaries. In TAC 2011 Workshop,
Gaithersburg, MD, USA. NIST.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech & Language, 25(2):282?
306, April.
Makoto Hirohata, Yousuke Shinnaka, Koji Iwano, and
Sadaoke Furui. 2005. Sentence extraction-based
presentation summarization techniques and evalua-
tion metrics. In Acoustics, Speech, and Signal Pro-
cessing, 2005. Proceedings. (ICASSP ?05). IEEE In-
ternational Conference on, volume 1, pages 1065?
1068.
Max Kuhn and Kjell Johnson. 2013. Applied Predic-
tive Modeling. Springer.
Chin-Yew Lin and Marina Rey. 2004. ROUGE:
A package for automatic evaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, edi-
tor, Text Summarization Branches Out: Proceedings
of the ACL-04 Workshop, pages 74?81, Barcelona,
Spain. Association for Computational Linguistics.
Chin-Yew Lin. 2004. Looking for a few good metrics:
Automatic summarization evaluation - how many
samples are enough. In Proceedings of the NTCIR
Workshop, pages 1765?1776, Tokyo.
Annie Louis and A Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39(2):267?
300.
Nitin Madnani, Jill Burstein, John Sabatini, and Tenaha
O?Reilly. 2013. Automated scoring of a summary-
writing task designed to measure reading compre-
hension. In Proceedings of the 8th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 163?168, Atlanta, Georgia. Asso-
ciation for Computational Linguistics.
Ani Nenkova and Annie Louis. 2008. Can you sum-
marize this? Identifying correlates of input difficulty
for generic multi-document summarization. In Pro-
ceedings of the ACL-08: HLT, pages 825?833. As-
sociation for Computational Linguistics.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2-3):103?233.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of workshop on evaluation met-
rics and system comparison for automatic summa-
rization., pages 1?9, Stroudsburg, PA. Association
for Computational Linguistics.
Art B. Owen. 2007. The pigeonhole bootstrap. The
Annals of Applied Statistics, 1(2):386?411.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU : a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages
311?318, Philadelphia, PA. Association for Compu-
tational Linguistics.
Gerald Penn and Xiaodan Zhu. 2008. A Critical
Reassessment of Evaluation Baselines for Speech
Summarization. In in Proceedings of RANLP work-
shop on Crossing Barriers in Text Summarization
Research, number June, pages 470?478, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Peter A. Rankel, John. M. Conroy, Hoa Trang Dang,
and Ani Nenkova. 2013. A decade of automatic
content evaluation of news summaries: reassessing
the state of the art. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, Sofia, Bulgaria, August 4-9, 2013, pages
131?136, Sofia. Association for Computational Lin-
guistics.
Rand R. Wilcox. 2009. Comparing Pearson correla-
tions: dealing with heteroscedasticity and nonnor-
mality. Communications in Statistics - Simulation
and Computation, 38(10):2220?2234.
David M. Williamson, Xiaoming Xi, and F. Jay Breyer.
2012. A framework for evaluation and use of au-
tomated scoring. Educational measurement: issues
and practice, 31(1):2?13.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In NAACL HLT ?12 Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 103?111.
Wenting Xiong, Keelan Evanini, Klaus Zechner, and
Lei Chen. 2013. Automated content scoring of
77
spoken responses containing multiple parts with fac-
tual information. In Pierre Badin, Thomas Hue-
ber, G?erard Bailly, Didier Demolin, and Franc?oise
Raby, editors, Proceedings of SLaTE 2013, Greno-
ble, France, pages 137?142, Grenoble.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Guang Yong Zou. 2007. Toward using confidence
intervals to compare correlations. Psychological
methods, 12(4):399?413.
78
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
Abstract
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
1 Introduction
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers? spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
? The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
? The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
? Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
134
features that are robust to speech recognition
errors.
? A significant amount of responses (more than
7%) exhibit issues that make them hard or
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
2 Related Work
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children?s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker?s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone?s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone?s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker?s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker?s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker?s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
3 Data
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
135
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker?s response is:
? Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker?s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
? Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
Sets of 7 Speaking items are presented to the
test taker in thematic units, called ?lessons?, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker?s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
Restricted Speech
Type Description
Multiple
Choice
(MC)
The test taker selects the correct
option and reads it aloud
Read Aloud
(RA)
The test taker reads aloud a set
of classroom instructions
Repeat
Aloud (RP)
The test taker listens to a student
utterance twice and then repeats
it
Semi-restricted Speech
Type Description
Incomplete
Sentence
(IS)
The test taker is given a sentence
fragment and completes the sen-
tence according to the instruc-
tions
Key Words
(KW)
The test taker uses the key words
provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword
Chart (KC)
The test taker constructs a sen-
tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
Table 1: Types of speaking items included in the
assessment
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
136
human score distributions in each of these parti-
tions are displayed in Table 2.
4 System Architecture
The automated scoring system used for the teach-
ers? spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRater
SM
, (Zechner et al., 2009; Higgins et
al., 2011)):
? an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers? responses
? a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
? a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
? linear regression scoring models that predict
the score for each response based on a set of
selected features
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
5 Methodology
5.1 Speech features
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker?s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations
1
(Zechner
et al., 2009; Xi et al., 2008):
? empirical performance, i.e., feature correla-
tion with human scores
? construct
2
relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
? overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
? feature independence, i.e., the inter-
correlation between any two features of the
set should be low
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
1
While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2
A construct is the set of knowledge, skills, and abilities
measured by a test.
137
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
5.2 Filtering model
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
5.3 Scoring models
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
Table 4: Correlations between system and first hu-
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
6 Results
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
138
Feature Sub-construct Description
Content Ed1 Read/repeat accu-
racy / Fluency
Correctly read words per minute
Content Ed2 Read/repeat accu-
racy
Read/repeat word error rate
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu-
ency
Number of word types divided by utterance duration
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
139
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
Sub-construct Restricted Semi-restricted
Content 0.33?0.67 0.34?0.61
Fluency 0.19?0.33 0.20?0.33
Pronunciation 0.20?0.22 0.13?0.31
Prosody 0.18?0.24 0.12?0.27
Grammar ? 0.23?0.49
Vocabulary ? 0.21?0.32
Table 6: Range of Pearson r correlations for dif-
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.
3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
7 Discussion
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67?0.80), compared to the 3 re-
stricted item types (0.51?0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
3
In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34?0.67)
than for the 5 semi-restricted item types (0.43?
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas?in the case of
semi-restricted item types?may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33?0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ? 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6?
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
140
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
8 Conclusion
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system?s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
Acknowledgments
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
References
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862?2873.
141
Maxine Eskenazi. 2009. An overview of spoken lan-
guage technology for education. Speech Communi-
cation, 51(10):832?844.
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121?
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282?306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody ? Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21?30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83?93.
Pearson Education, Inc. 2011. Versant
TM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111, Montr?eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 600?608, Jeju Island, Korea.
Association for Computational Linguistics.
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr?eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
142
