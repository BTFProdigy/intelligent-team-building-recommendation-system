Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201?204,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Mining Association Language Patterns for  
Negative Life Event Classification 
 
 
Liang-Chih Yu1, Chien-Lung Chan1, Chung-Hsien Wu2 and Chao-Cheng Lin3 
1Department of Information Management, Yuan Ze University, Taiwan, R.O.C. 
2Department of CSIE, National Cheng Kung University, Taiwan, R.O.C. 
3Department of Psychiatry, National Taiwan University Hospital, Taiwan, R.O.C. 
{lcyu, clchan}@saturn.yzu.edu.tw, chwu@csie.ncku.edu.tw, linchri@gmail.com 
 
  
 
Abstract 
 
Negative life events, such as death of a family 
member, argument with a spouse and loss of a 
job, play an important role in triggering de-
pressive episodes. Therefore, it is worth to de-
velop psychiatric services that can automati-
cally identify such events. In this paper, we 
propose the use of association language pat-
terns, i.e., meaningful combinations of words 
(e.g., <loss, job>), as features to classify sen-
tences with negative life events into prede-
fined categories (e.g., Family, Love, Work). 
The language patterns are discovered using a 
data mining algorithm, called association pat-
tern mining, by incrementally associating fre-
quently co-occurred words in the sentences 
annotated with negative life events. The dis-
covered patterns are then combined with sin-
gle words to train classifiers. Experimental re-
sults show that association language patterns 
are significant features, thus yielding better 
performance than the baseline system using 
single words alone. 
1 Introduction 
With the increased incidence of depressive dis-
orders, many psychiatric websites have devel-
oped community-based services such as message 
boards, web forums and blogs for public access. 
Through these services, individuals can describe 
their stressful or negative life events such as 
death of a family member, argument with a 
spouse and loss of a job, along with depressive 
symptoms, such as depressive mood, suicidal 
tendencies and anxiety. Such psychiatric texts 
(e.g., forum posts) contain large amounts of natu-
ral language expressions related to negative life 
events, making them useful resources for build-
ing more effective psychiatric services. For in-
stance, a psychiatric retrieval service can retrieve 
relevant forum or blog posts according to the 
negative life events experienced by users so that 
they can be aware that they are not alone because 
many people have suffered from the same or 
similar problems. The users can then create a 
community discussion to share their experiences 
with each other. Additionally, a dialog system 
can generate supportive responses like ?Don?t 
worry?, ?That?s really sad? and ?Cheer up? if it 
can understand the negative life events embed-
ded in the example sentences shown in Table 1. 
Therefore, this study proposes a framework for 
negative life event classification. We formulate 
this problem as a sentence classification task; 
that is, classify sentences according to the type of 
negative life events within them. The class labels 
used herein are presented in Table 1, which are 
derived from Brostedt and Pedersen (2003). 
Traditional approaches to sentence classifica-
tion (Khoo et al, 2006; Naughton et al, 2008) or 
text categorization (Sebastiani 2002) usually 
adopt bag-of-words as baseline features to train 
classifiers. Since the bag-of-words approach 
treats each word independently without consider-
ing the relationships of words in sentences, some 
researchers have investigated the use of n-grams 
to capture sequential relations between words to 
boost classification performance (Chitturi and 
Hansen, 2008; Li and Zong, 2008). The use of n-
grams is effective in capturing local dependen-
cies of words, but tends to suffer from data 
sparseness problem in capturing long-distance 
dependencies since higher-order n-grams require 
large training data to obtain reliable estimation. 
For our task, the expressions of negative life 
events can be characterized by association lan-
guage patterns, i.e., meaningful combinations of 
words, such as <worry, children, health>, <break 
up, boyfriend>, <argue, friend>, <loss, job>, and 
201
<school, teacher, blame> in the example sen-
tences in Table 1. Such language patterns are not 
necessarily composed of continuous words. In-
stead, they are usually composed of the words 
with long-distance dependencies, which cannot 
be easily captured by n-grams. 
Therefore, the aim of this study is two-fold: (1) 
to automatically discover association language 
patterns from the sentences annotated with nega-
tive life events; and (2) to classify sentences with 
negative life events using the discovered patterns. 
To discover association language patterns, we 
incorporate the measure mutual information (MI) 
into a data mining algorithm, called association 
pattern mining, to incrementally derive fre-
quently co-occurred words in sentences (Section 
2). The discovered patterns are then combined 
with single words as features to train classifiers 
for negative life event classification (Section 3). 
Experimental results are presented in Section 4. 
Conclusions are finally drawn in Section 5. 
2 Association Language Pattern Mining 
The problem of language pattern acquisition can 
be converted into the problem of association pat-
tern mining, where each sales transaction in a 
database can be considered as a sentence in the 
corpora, and each item in a transaction denotes a 
word in a sentence. An association language pat-
tern is defined herein as a combination of multi-
ple associated words, denoted by 1,..., kw w< > . 
Thus, the task of association pattern mining is to 
mine the language patterns of frequently associ-
ated words from the training sentences. For this 
purpose, we adopt the Apriori algorithm 
(Agrawal and Srikant, 1994) and modified it 
slightly to fit our application. Its basic concept is 
to identify frequent word sets recursively, and 
then generate association language patterns from 
the frequent word sets. For simplicity, only the 
combinations of nouns and verbs are considered, 
and the length is restricted to at most 4 words, 
i.e., 2-word, 3-word and 4-word combinations. 
The detailed procedure is described as follows. 
2.1 Find frequent word sets 
A word set is frequent if it possesses a minimum 
support. The support of a word set is defined as 
the number of training sentences containing the 
word set. For instance, the support of a two-word 
set { iw , jw } denotes the number of training sen-
tences containing the word pair ( iw , jw ). The 
frequent k-word sets are discovered from (k-1)-
word sets. First, the support of each word, i.e., 
word frequency, in the training corpus is counted. 
The set of frequent one-word sets, denoted as 1L , 
is then generated by choosing the words with a 
minimum support level. To calculate kL , the fol-
lowing two-step process is performed iteratively 
until no more frequent k-word sets are found. 
z Join step: A set of candidate k-word sets, 
denoted as kC , is first generated by merg-
ing frequent word sets of 1kL ? , in which 
only the word sets whose first (k-2) words 
are identical can be merged. 
z Prune step: The support of each candidate 
word set in kC  is then counted to determine 
which candidate word sets are frequent. Fi-
nally, the candidate word sets with a sup-
port count greater than or equal to the 
minimum support are considered to form 
kL . The candidate word sets with a subset 
that is not frequent are eliminated. Figure 1 
shows an example of generating kL . 
Label Description Example Sentence 
Family  Serious illness of a family member; 
Son or daughter leaving home 
I am very worried about my children?s health. 
Love Spouse/mate engaged in infidelity; 
Broke up with a boyfriend or girlfriend 
I broke up with my dear but cruel boyfriend 
recently. 
School Examination failed or grade dropped; 
Unable to enter/stay in school 
I hate to go to school because my teacher al-
ways blames me. 
Work Laid off or fired from a job; 
Demotion and salary reduction 
I lost my job in this economic recession a few 
months ago. 
Social Substantial conflicts with a friend; 
Difficulties in social activities 
I argued with my best friend and was upset. 
Table 1.  Classification of negative life events.  
202
2.2 Generate association patterns from fre-
quent word sets 
Association language patterns can be generated 
via a confidence measure once the frequent word 
sets have been identified. The confidence of an 
association language pattern of k words is de-
fined as the mutual information of the k words, 
as shown below. 
1 1
1
1
1
( ,... ) ( ,... )
( ,... )
                              ( ,... ) log
( )
k k
k
k k
i
i
Conf w w MI w w
P w w
P w w
P w
=
< > =
=
?
      (1) 
where 1( ,... )kP w w  denotes the probability of the 
k words co-occurring in a sentence in the training 
set, and ( )iP w  denotes the probability of a sin-
gle word occurring in the training set. Accord-
ingly, each frequent word set in kL  is assigned a 
mutual information score. In order to generate a 
set of association language patterns, all frequent 
word sets are sorted in the descending order of 
the mutual information scores. The minimum 
confidence (a threshold at percentage) is then 
applied to select top N percent frequent word sets 
as the resulting language patterns. This threshold 
is determined empirically by maximizing classi-
fication performance (Section 4). Figure 1 (right-
hand side) shows an example of generating the 
association language patterns from kL . 
3 Sentence Classification  
The classifiers used in this study include Support 
Vector Machine (SVM), C4.5, and Na?ve Bayes 
(NB) classifier, which is provided by Weka 
Package (Witten and Frank, 2005). The feature 
set includes: 
Bag-of-Words (BOW): Each single word in 
sentences. 
Association language patterns (ALP): The top 
N percent association language patterns acquired 
in the previous section. 
Ontology expansion (Onto): The top N percent 
association language patterns are expanded by 
mapping the constituent words into their syno-
nyms. For example, the pattern <boss, conflict> 
can be expanded as <chief, conflict> since the 
words boss and chief are synonyms. Here we use 
the HowNet (http://www.keenage.com), a Chi-
nese lexical ontology, for pattern expansion. 
4 Experimental Results 
Data set: A total of 2,856 sentences were col-
lected from the Internet-based Self-assessment 
Program for Depression (ISP-D) database of the 
PsychPark (http://www.psychpark.org), a virtual 
psychiatric clinic, maintained by a group of vol-
unteer professionals of Taiwan Association of 
Mental Health Informatics (Bai et al, 2001). 
Each sentence was then annotated by trained an-
notators with one of the five types of negative 
life events. Table 2 shows the break-down of the 
distribution of sentence types.  
The data set was randomly split into a training 
set, a development set, and a test set with an 
8:1:1 ratio. The training set was used for lan-
guage pattern generation. The development set 
was used to optimize the threshold (Section 2.2) 
for the classifiers (SVM, C4.5 and NB). Each 
classifier was implemented using three different 
levels of features, namely BOW, BOW+ALP, 
Prune Step
(min. support)
Sorting
and
Thresholding
<Boyfriend, Conflict>
<Boyfriend, Break up>
<Boss, Conflict>
<Conflict, Break up>
Find Frequent Word Sets Generate Association Language Patterns
Join
Step
Prune Step
(min. support)
Prune Step
(min. support)
<Boyfriend, Conflict, Break up>Join Step
Figure 1.  Example of generating association language patterns. 
Sentence Type % in Corpus 
Family 28.8 
Love 22.8 
School 13.3 
Work 14.3 
Social 20.8 
Table 2. Distribution of sentence types. 
203
and BOW+ALP+Onto, to examine the effective-
ness of association language patterns. The classi-
fication performance is measured by accuracy, 
i.e., the number of correctly classified sentences 
divided by the total number of test sentences. 
4.1 Evaluation on threshold selection 
Since not all discovered association language 
patterns contribute to the classification task, the 
threshold described in Section 2.2 is used to se-
lect top N percent patterns for classification. This 
experiment is to determine an optimal threshold 
for each involved classifier by maximizing its 
classification accuracy on the development set. 
Figure 2 shows the classification accuracy of NB 
against different threshold values.  
When using association language patterns as 
features (BOW+ALP), the accuracy increased 
with increasing the threshold value up to 0.6, 
indicating that the top 60% discovered patterns 
contained more useful patterns for classification. 
By contrast, the accuracy decreased when the 
threshold value was above 0.6, indicating that the 
remaining 40% contained more noisy patterns 
that may increase the ambiguity in classification. 
When using the ontology expansion approach 
(BOW+ALP+Onto), both the number and diver-
sity of discovered patterns are increased. There-
fore, the accuracy was improved and the optimal 
accuracy was achieved at 0.5. However, the ac-
curacy dropped significantly when the threshold 
value was above 0.5. This finding indicates that 
expansion on noisy patterns may produce more 
noisy patterns and thus decrease performance. 
4.2 Results of classification performance 
The results of each classifier were obtained from 
the test set using its own threshold optimized in 
the previous section. Table 3 shows the compara-
tive results of different classifiers with different 
levels of features. The incorporation of associa-
tion language patterns improved the accuracy of 
NB, C4.5, and SVM by 3.9%, 1.9%, and 2.2%, 
respectively, and achieved an average improve-
ment of 2.7%. Additionally, the use of ontology 
expansion can further improve the performance 
by 1.6% in average. This finding indicates that 
association language patterns are significant fea-
tures for negative life event classification. 
5 Conclusion  
This work has presented a framework that uses a 
data mining algorithm and ontology expansion 
method to acquire association language patterns 
for negative life event classification. The asso-
ciation language patterns can capture word rela-
tionships in sentences, thus yielding higher per-
formance than the baseline system using single 
words alone. Future work will focus on devising 
a semi-supervised or unsupervised method for 
language pattern acquisition from web resources 
so as to reduce reliance on annotated corpora. 
References  
R. Agrawal and R. Srikant. 1994. Fast Algorithms for Min-
ing Association Rules. In Proc. Int?l Conf. Very Large 
Data Bases (VLDB), pages 487-499. 
Y. M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu. 2001. Vir-
tual Psychiatric Clinics. American Journal of Psychiatry, 
vol. 158, no. 7, pp. 1160-1161. 
E. M. Brostedt and N. L. Pedersen. 2003. Stressful Life 
Events and Affective Illness. Acta Psychiatrica Scandi-
navica, vol. 107, pp. 208-215. 
R. Chitturi and J. H.L. Hansen. 2008. Dialect Classification 
for online podcasts fusing Acoustic and Language based 
Structural and Semantic Information. In Proc. of ACL-08, 
pages 21-24. 
A. Khoo, Y. Marom and D. Albrecht. 2006. Experiments 
with Sentence Classification. In Proc. of Australasian 
Language Technology Workshop, pages 18-25. 
S. Li and C. Zong. 2008. Multi-domain Sentiment Classifi-
cation. In Proc. of ACL-08, pages 257-260. 
M. Naughton, N. Stokes, and J. Carthy. 2008. Investigating 
Statistical Techniques for Sentence-Level Event Classi-
fication. In Proc. of COLING-08, pages 617-624. 
F. Sebastiani. 2002. Machine Learning in Automated Text 
Categorization. ACM Computing Surveys, vol. 34, no. 1, 
pp. 1-47. 
I. H. Witten and E. Frank. 2005. Data Mining: Practical 
Machine Learning Tools and Techniques, 2nd Edition, 
Morgan Kaufmann, San Francisco. 
 NB C4.5 SVM
BOW 0.717 0.741 0.787
BOW+ALP 0.745 0.755 0.804
BOW+ALP+Onto 0.759 0.766 0.815
Table 3. Accuracy of classifiers on testing data. 
0.62
0.64
0.66
0.68
0.70
0.72
0.74
0.76
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
Ac
cu
rac
y
BOW+ALP
BOW+ALP+Onto
 
Figure 2. Threshold selection. 
204
Automated Alignment and Extraction of Bilingual Ontology for                   
Cross-Language Domain-Specific Applications  
Jui-Feng Yeh, Chung-Hsien Wu, Ming-Jun Chen and Liang-Chih Yu 
Department of Computer Science and Information Engineering 
National Cheng Kung University, Tainan, Taiwan, R.O.C. 
{jfyeh, chwu, mjchen,lcyu}@csie.ncku.edu.tw 
 
Abstract 
In this paper we propose a novel approach for 
ontology alignment and domain ontology extraction 
from the existing knowledge bases, WordNet and 
HowNet. These two knowledge bases are aligned to 
construct a bilingual ontology based on the co-
occurrence of the words in the sentence pairs of a 
parallel corpus. The bilingual ontology has the merit 
that it contains more structural and semantic 
information coverage from these two 
complementary knowledge bases. For domain-
specific applications, the domain specific ontology 
is further extracted from the bilingual ontology by 
the island-driven algorithm and the domain-specific 
corpus.  Finally, the domain-dependent 
terminologies and some axioms between domain 
terminologies are integrated into the ontology. For 
ontology evaluation, experiments were conducted 
by comparing the benchmark constructed by the 
ontology engineers or experts. The experimental 
results show that the proposed approach can extract 
an aligned bilingual domain-specific ontology. 
1 Introduction 
In recent years, considerable progress has been 
invested in developing the conceptual bases for 
building technology that allows knowledge reuse 
and sharing. As information exchangeability and 
communication becomes increasingly global, 
multilingual lexical resources that can provide 
transnational services are becoming increasingly 
important. On the other hand, multi-lingual 
ontology is very important for natural language 
processing, such as machine translation (MT), web 
mining (Oyama et al 2004) and cross language 
information retrieval (CLIR). Generally, a multi-
lingual ontology maps the keyword set of one 
language to another language, or compute the co-
occurrence of the words among languages. In 
addition, a key merit for multilingual ontology is 
that it can increase the relation and structural 
information coverage by aligning two or more 
language-dependent ontologies with different 
semantic features.  
 Over the last few years, significant effort has 
been made to construct the ontology manually 
according to the domain expert?s knowledge. 
Manual ontology merging using conventional 
editing tools without intelligent support is difficult, 
labor intensive and error prone. Therefore, several 
systems and frameworks for supporting the 
knowledge engineer in the ontology merging task 
have recently been proposed (Noy and Musen 
2000). To avoid the reiteration in ontology 
construction, the algorithm of ontology merging 
(UMLS http://umlsks.nlm.nih.gov/) (Langkilde and 
Knight 1998) and ontology alignment (Vossen and 
Peters 1997) (Weigard and Hoppenbrouwers 1998) 
(Asanoma 2001) were invested. The final ontology 
is a merged version of the original ontologies. The 
two original ontologies persist, with aligned links 
between them. Alignment usually is performed 
when the ontologies cover domains that are 
complementary to each other. In the past, domain 
ontology was usually constructed manually 
according to the knowledge or experience of the 
experts or ontology engineers. Recently, automatic 
and semi-automatic methods have been developed. 
OntoExtract (Fensel et al 2002) (Missikoff et al 
2002) provided an ontology engineering chain to 
construct the domain ontology from WordNet and 
SemCor. 
Nowadays vast investment is made in ontology 
construction for domain application. Finding the 
authoritative evaluation for ontology is becoming a 
critical issue. Some evaluations are integrated into 
the ontology tools to detect and prevent the 
mistakes. The mistakes that might be made in 
developing taxonomies with frames are described in 
(G?mez-P?rez 2001). They defined three mainly 
types of mistakes: Inconsistency, Incompleteness, 
and redundancy. To deal with these mistakes and 
carry out the validation and verification of ontology, 
some ontology checkers, validators and parsers 
were developed. These tools provide the efficacious 
appraisal of correctness when developing the new 
ontology. However, they are disappointing in 
ontology integration, especial when the original 
ontologies are well defined. For other approaches 
(Maedche and Staab 2002), the similarity measures 
are proposed in the earlier stage of the evaluation. 
The evaluation consists two layers: lexical layer and 
conceptual layer. In lexical layer, the edit distance 
is integrated into the lexical similarity measure. The 
measure is defined as: 
( ) ( ) ( )( ) [ ]
min , ,
, max 0, 0,1
min ,
i j i j
i j
i j
L L ed L L
SM L L
L L
? ??? ?? ?? ?? ?
 (1) 
where ( )SM    denotes the lexicon similarity 
function, ( )ed    is the Levensthein edit distance 
function defined in (Levensthein. 1966). iL  and jL  
are the words within the lexicons of the ontologies. 
The conceptual layer focuses on the conceptual 
structures of the ontologiesm namely taxonomic 
and nontaxonomic relations. 
In this paper, WordNet and HowNet knowledge 
bases are aligned to construct a bilingual universal 
ontology based on the co-occurrence of the words 
in a parallel corpus. For domain-specific 
applications, the medical domain ontology is further 
extracted from the universal ontology using the 
island-driven algorithm and a medical domain 
corpus. Finally, the axioms between medical 
terminologies are derived. The benchmark 
constructed by the ontology engineers and experts 
is introduced to evaluate the bilingual ontology 
constructed using the methods proposed in this 
paper. This paper defines two measures, taxonomic 
relation and non-taxonomic relation, as the 
quantitative metrics to evaluate the ontology.  
The rest of the paper is organized as follows. 
Section 2 describes ontology construction process 
and the web search system framework. Section 3 
presents the experimental results for the evaluation 
of our approach. Section 4 gives some concluding 
remarks. 
2 Methodologies 
Figure 1 shows the block diagram for ontology 
construction. There are two major processes in the 
proposed system: bilingual ontology alignment and 
domain ontology extraction. 
2.1 Bilingual Ontology Alignment 
In this approach, the cross-lingual ontology is 
constructed by aligning the words in WordNet to 
their corresponding words in HowNet. 
The hierarchical taxonomy is actually a 
conversion of HowNet. One of the important 
portions of HowNet is the methodology of defining 
the lexical entries. In HowNet, each lexical entry is 
defined as a combination of one or more primary 
features and a sequence of secondary features. The 
primary features indicate the entry?s category, 
namely, the relation: ?is-a? which is in a 
hierarchical taxonomy. Based on the category, the 
secondary features make the entry?s sense more 
explicit, but they are non-taxonomic. Totally 1,521 
primary features are divided into 6 upper categories: 
Event, Entity, Attribute Value, Quantity, and 
Quantity Value. These primary features are 
organized into a hierarchical taxonomy. 
First, the Sinorama (Sinorama 2001) database is 
adopted as the bilingual language parallel corpus to 
compute the conditional probability of the words in 
WordNet, given the words in HowNet. Second, a 
bottom up algorithm is used for relation mapping. 
In WordNet a word may be associated with many 
synsets, each corresponding to a different sense of 
the word. For finding a relation between two 
different words, all the synsets associated with each 
word are considered (Fellbaum 1998). In HowNet, 
each word is composed of primary features and 
secondary features. The primary features indicate 
the word?s category. The purpose of this approach 
is to increase the relation and structural information 
coverage by aligning the above two language-
dependent ontologies, WordNet and HowNet, with 
their semantic features. 
 
 
Figure 1 Ontology construction framework 
The relation ?is-a? defined in WordNet 
corresponds to the primary feature defined in 
HowNet. Equation (2) shows the mapping between 
the words in HowNet and the synsets in WordNet. 
Given a Chinese word, iCW  , the probability of the 
word related to synset, ksynset  , can be obtained 
via its corresponding English synonyms, 
,  1,...,kjEW j m=  , which are the elements in 
ksynset  . The probability is estimated as follows.  
1
1
Pr( | )
Pr( , | )
(Pr( | , ) Pr( | ))
k
i
m k k
j i
j
m k k k
j i j i
j
synset CW
synset EW CW
synset EW CW EW CW
=
=
= ?
= ??
     (2) 
where 
( )( )
( )
Pr | ,
, ,
, ,
k k
j i
k k
j j i
l k
j j i
l
synset EW CW
N synset EW CW
N synset EW CW
=
?
     (3) 
In the above equation, ( ), ,k kj j iN synset EW CW   
represents the number of co-occurrences of iCW  , 
k
jEW  and 
k
jsynset . The probability ( )Pr |kj iEW CW   is 
set to one when at least one of the primary features, 
( )li iPF CW , of the Chinese word defined in the 
HowNet matches one of the ancestor nodes of 
synset , ( )kj jsynset EW  except the root nodes in the 
hierarchical structures of the noun and verb; 
Otherwise the probability ( )Pr |kj iEW CW   is set to zero. 
( )
( )( )( )
Pr |
{ , , , }
1 
( ( )) { , , , }
0
j i
l
i i
l
k
j j
k
EW CW
PF CW entity event act play
if
ancestor synset EW entity event act play
otherwise
? ???= ? ? ? ????
U I
U U
                                                                               (4) 
where {enitity,event,act,play} is the concept set in 
the root nodes of HowNet and WordNet. 
Finally, the Chinese concept, iCW  , has been 
integrated into the synset , kjsynset   , in WordNet 
as long as the probability, Pr k i(synset |CW )  , is not 
zero. Figure 2(a) shows the concept tree generated 
by aligning WordNet and HowNet. 
2.2 Domain ontology extraction 
There are two phases to construct the domain 
ontology: 1) extract the ontology from the cross-
language ontology by the island-driven algorithm, 
and 2) integrate the terms and axioms defined in a 
medical encyclopaedia into the domain ontology.  
2.2.1 Extraction by island-driven algorithm 
Ontology provides consistent concepts and world 
representations necessary for clear communication 
within the knowledge domain. Even in domain-
specific applications, the number of words can be 
expected to be numerous. Synonym pruning is an 
effective alternative to word sense disambiguation. 
This paper proposes a corpus-based statistical 
approach to extracting the domain ontology. The 
steps are listed as follows: 
Step 1 Linearization: This step decomposes the 
tree structure in the universal ontology shown in 
Figure 2(a) into the vertex list that is an ordered 
node sequence starting at the leaf nodes and ending 
at the root node.  
Step 2 Concept extraction from the corpus: The 
node is defined as an operative node when the Tf-
idf value of word iW   in the domain corpus is 
higher than that in its corresponding contrastive 
(out-of-domain) corpus. That is, 
_ ( )
1,     ( ) ( )
0,   
i
Domain i Contrastive i
operative node W
if Tf idf W Tf idf W
Otherwise
? > ??= ??
                                                                 (5) 
where 
, ,
,
,
, ,
,
,
( )
log
( )
log
Domain i
i Domain i Contrastive
i Domain
i Domain
Contrastive i
i Domain i Contrastive
i Contrastive
i Contrastive
Tf idf W
n n
freq
n
Tf idf W
n n
freq
n
?
+= ?
?
+= ?
 
In the above equations, 
Domainifreq ,   and eContrastivifreq ,   
are the frequencies of word iW   in the domain 
documents and its contrastive (out-of-domain) 
documents, respectively. Domainin ,   and  ,i Contrastiven  
are the numbers of the documents containing word 
iW   in the domain documents and its contrastive 
documents, respectively. The nodes with bold circle 
in Figure 2(a) represent the operative nodes.  
Step 3 Relation expansion using the island-
driven algorithm: There are some domain concepts 
not operative after the previous steps due to the 
problem of sparse data. From the observation in 
ontology construction, most of the inoperative 
concept nodes have operative hypernym nodes and 
hyponym nodes. Therefore, the island-driven 
algorithm is adopted to activate these inoperative 
concept nodes if their ancestors and descendants are 
all operative. The nodes with gray background 
shown in Figure 2(a) are the activated operative 
nodes.  
Step 4 Domain ontology extraction: The final 
step is to merge the linear vertex list sequence into 
a hierarchical tree. However, some noisy concepts 
not belonging to this domain ontology are operative. 
These nodes with inoperative noisy concepts should 
be filtered out. Finally, the domain ontology is 
extracted and the final result is shown in Figure 
2(b).  
After the above steps, a dummy node is added as 
the root node of the domain concept tree. 
 
 
Figure 2(a) Concept tree generated by aligning 
WordNet and HowNet. The nodes with bold circle 
represent the operative nodes after concept 
extraction. The nodes with gray background 
represent the operative nodes after relation 
expansion. 
 
Figure 2(b) The domain ontology after filtering out 
the isolated concepts 
2.2.2 Axiom and terminology integration 
In practice, specific domain terminologies and 
axioms should be derived and introduced into the 
ontology for domain-specific applications. There 
are two approaches to add the terminologies and 
axioms: the first one is manual editing by the 
ontology engineers, and the other is to obtain from 
the domain encyclopaedia.  
For medical domain, we obtain 1213 axioms 
derived from a medical encyclopaedia about the 
terminologies related to diseases, syndromes, and 
the clinic information. Figure 3 shows an example 
of the axiom. In this example, the disease 
?diabetes? is tagged as level ?A? which represents 
that this disease is frequent in occurrence. And the 
degrees for the corresponding syndromes represent 
the causality between the disease and the 
syndromes. The axioms also provide two fields 
?department of the clinical care? and ?the category 
of the disease? for medical information retrieval or 
other medical applications. 
 
 
Figure 3   One example of the axioms 
3 Evaluation 
For evaluation, a medical domain ontology is 
constructed. A medical web mining system is also 
implemented to evaluate the practicability of the 
bilingual ontology. 
3.1 Conceptual Evaluation of Ontology 
The benchmark ontologies are created to be the 
test-suites of reusable data which can be employed 
by ontology engineers or constructer for 
benchmarking purposes. The benchmark ontology 
was constructed by the domain experts including 
two doctors and one pharmacologist based on 
UMLS. The domain experts have integrated the 
Chinese concepts without changing the contents of 
UMLS  
Evaluation of ontology construction adopted the 
two layer measures: Lexical and Conceptual layers 
(Eichmann et al 1998). The evaluation in the 
conceptual layer seems to be more important than 
that in the lexical layer when the ontology is 
constructed by aligning or merging several well 
defined source ontologies. There are two conceptual 
relation categories for evaluation: Taxonomic and 
non-Taxonomic evaluations. 
3.1.1 Evaluation of the taxonomic relation 
Step1 Linearization: This step decomposes the tree 
structure into the vertex list as described in Section 
2.2. The ontology, TO , and the benchmark, BO are 
shown in the Figure 4(a) and 4(b), respectively. 
After this linearization, the vertex list sets: 
TVLS and BVLS  are obtained as shown in Figure 
4(c), where { }1 2 3 4, , ,T T T TTVLS VL VL VL VL=  
and { }1 2 3, ,B B BBVLS VL VL VL= . 
 (a) The taxonomic hierarchical representation of  
target ontology TO  
 
(b) The taxonomic hierarchical representation of  
benchmark ontology BO  
 
TVLS  BVLS  
(c) The taxonomic vertex list set representation of  
target ontology and benchmark ontology 
Figure 4  Linearization of ontologies 
Step 2 Normalization: Since the frequencies of 
concepts in the vertex list set are not equal, the 
normalization factors are introduced to address this 
problem. For the target ontology, the factor vectors 
for normalization is { }1 2 3 4 5 6 7 8, , , , , , ,T T T T T T T T TNF nf nf nf nf nf nf nf nf= , 
and for the benchmark ontology is { }1 2 3 4 5 6 7 8 9, , , , , , , ,B B B B B B B B B BNF nf nf nf nf nf nf nf nf nf=
where oinf   is the normalization factor for the i-th 
concept of the ontology O. It is defined as the 
reciprocal of the frequency in the vertex list set.   
i
1
the vertex lists contain the concept  in ontology O
O
inf =  
Step 3 Estimation of the vertex list similarity: 
Therefore, the pairwise similarity of these two 
vertex lists of the target ontology and benchmark 
ontology can be obtained using the 
Needleman/Wunsch techniques shown in the 
following steps: 
Initialization: Create a matrix with m+1 columns 
and n+1 rows. m and n are the numbers of the 
concepts in the vertex lists of the target ontology 
and the bench mark ontology, respectively. The first 
row and first column of the matrix can be initially 
set to 0. That is, 
     ( , ) 0,   m 0  n 0 Sim m n if or= = =              (6) 
Matrix filling: Assign the values to the remnant 
elements in the matrix as the following equation: 
  
( )
( )
( )
1 1 1 1
1 1
1 1
( , )
1( 1, 1) ( , ),
2
1max ( 1, )) ( , ),
2
1( , 1) ( , )
2
ji
j ji i
j ji i
j ji i
BT
m n
B BT T
m n lexicon m n
B BT T
m n lexicon m n
B BT T
m n lexicon m n
Sim V V
Sim m n nf nf Sim V V
Sim m n nf nf Sim V V
Sim m n nf nf Sim V V
? ? ? ?
? ?
? ?
? ? ? + + ????= ? + + ???? ? + + ???
 
(7) 
There are some synonyms belonging to the same 
concept represented in one vertex. So the lexicon 
similarity can be described as   
1
1
1
( , )
Synonyms defined in the  and 
 Synonyms defined in the  or 
ji
ji
ji
BT
lexicon m n
BT
m n
BT
m n
Sim V V
V V
V V
?
?
?
=   (8) 
Traceback: Determine the actual alignment with 
the maximum score, , ji BTm nSim(V V ) , and therefore 
the pairwise similarity will be defined as the 
following equation: 
( ), arg max , ji BTT Bi j m nSim VL VL Sim(V V )?  (9) 
Step 4 Pairwise similarity matrix estimation: 
The pairwise similarity matrix is obtained after 
p q? times for Step3. p ,q are the numbers of the 
vertex list of target ontology and benchmark 
ontology. Each element of the pairwise similarity 
matrix as Equation (10) is obtained using Equation 
(9).  
1
TVL TpVL
1
BVL BqVL
T
iVL
B
jVL
( )1 1,T BSim VL VL ( ),T Bp qSim VL VL
( ),T Bi jSim VL VL
 
Figure 5 Pairwise similarity between the target 
ontolgy and benchmark ontology 
 
( )
( ) ( )
( ) ( )
1 1 1
1
,
, ... ,
: :
, ... ,
T B
T B T B
q
T B T B
p p q p q
PSM O O
Sim VL VL Sim VL VL
Sim VL VL Sim VL VL
?
? ?? ?? ? ?? ?? ?? ?
O
           (10) 
Step 5 Evaluation of the taxonomic hierarchy: 
The whole similarity between target ontology and 
benchmark ontology can be represented as: 
( )
( )
11
,
1 argmax ,
taxonomic T B
p
T B
i j
j qi
Sim O O
Sim VL VL
p ? ?=
= ?             (11) 
3.1.2 Evaluation of the non-taxonomic relation 
Some relations defined in the ontology are non-
taxonomic set such as synonym. In fact, the lexicon 
similarity is applied to measure the conceptual 
similarity. The lexicon similarity of set can be 
defined as the following equation: 
( , )
Words defined in the  and 
 Words defined in the  or 
ji
ji
ji
BT
lexicon s t
BT
s t
BT
s t
Sim V V
V V
V V
=           (12) 
Therefore, the evaluation of the non-taxonomic 
relation is defined as 
( )
1 1
,
1 ( , )ji
non taxonomic T B
p q
BT
lexicon s t
i j s t
Sim O O
Sim V V
p q
?
= =
= ? ????
           (13) 
3.1.3 Evaluation Results 
Using the benchmark ontology and evaluation 
metrics described in previous sections, the 
evaluation results are shown in Table 1. 
Table1 the similarity measure between the target 
ontology and benchmark ontology 
Taxonomic relation similarity 0.57 
Non-Taxonomic relation similarity 0.68 
According to the experimental results, some 
phenomena are discovered as follows: first, the 
number of words mapped to the same concept in the 
upper layer of ontology is larger than that in the 
lower layer because the terminologies usually 
appear in the lower layer.  
3.2 Evaluation of domain application  
To assess the ontology performance, a medical 
web-mining system to search the desired page has 
been implemented. In this system the web pages 
were collected from several Websites and totally 
2322 web pages for medical domain and 8133 web 
pages for contrastive domain were collected. The 
training and test queries for training and evaluating 
the system performance were also collected. Forty 
users, who do not take part in the system 
development, were asked to provide a set of queries 
given the collected web pages. After post-
processing, the duplicate queries and the queries out 
of the medical domain are removed. Finally, 3207 
test queries using natural language were obtained. 
The baseline system is based on the Vector-Space 
Model (VSM) and synonym expansion. The 
conceptual relations and axioms defined in the 
medical ontology are integrated into the baseline as 
the ontology-based system. The result is shown in 
Table 2. The results show that ontology-based 
system outperforms the baseline system with 
synonym expansion, especially in recall rate. 
4 Conclusion 
A novel approach to automated ontology 
alignment and domain ontology extraction from 
two knowledge bases is presented in this paper. In 
this approach, a bilingual ontology is developed 
from two well established language-dependent 
knowledge bases, WordNet and HowNet according 
to the co-occurrence of the words in the parallel 
bilingual corpus. A domain-dependent ontology is 
further extracted from the universal ontology using 
the island-driven algorithm and a domain and its 
contrastive corpus. In addition, domain-specific 
terms and axioms are also added to the domain 
ontology. This paper also proposed an evaluation 
method, benchmark and metrics, for ontology 
construction. Besides, we also applied the domain-
specific ontology to the web page search in medical 
domain. The experimental results show that the 
proposed approach outperformed the synonym 
expansion approach. The overall performance of the 
information retrieval system is directly related to 
the ontology.  
 
 
Table 2 Precision rate (%) at the 11 points recall level 
Recall Level 0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1 
Baseline system 78 73 68 65 60 52 38 30 21 15 11 
Ontology based 87 86 82 77 73 71 68 62 51 40 32 
References  
N. Asanoma, 2001. Alignment of Ontologies: 
WordNet and Goi-Taikei. WordNet and Other 
Lexical Resources Workshop Program, 
NAACL2001. 89-94 
D. Eichmann, M.  Ruiz, and P. Srinivasan, 1998. 
Cross-language information retrieval with the 
UMLS Metathesaurus, Proceeding of ACM 
Special Interest Group on Information Retreival 
(SIGIR), ACM Press, NY (1998), 72-80. 
D. Fensel, C. Bussler, Y. Ding, v. Kartseva1, M. 
Klein, M. Korotkiy, B. Omelayenko and R. 
Siebes, 2002. Semantic Web Application Areas, 
the 7th International Workshop on Applications 
of Natural Language to Information Systems 
(NLDB02). 
F. C. Fellbaum, 1998. WordNet an electronic 
Lexical Database, The MIT Press 1998. pp307-
308 
A. G?mez-P?rez, 2001. Evaluating ontologies: 
Cases of Study IEEE Intelligent Systems and 
their Applications: Special Issue on Verification 
and Validation of ontologies. Vol. 16, Number 3. 
March 2001. Pags: 391-409. 
I. Langkilde and K. Knight, 1998. Generation that 
Exploits Corpus-Based Statistical Knowledge. In 
Proceedings of COLING-ACL 1998. 
V. Levensthein, 1966. Binary codes capable of 
correcting deletions, insertions, and reversals. 
Soviet Physics?Doklady, 10:707?710. 
A. Maedche, and S. Staab, 2002. Measuring 
Similarities between Ontologies. In Proceedings 
of the 13th European Conference on Knowledge 
Engineering and Knowledge Management 
EKAW, Madrid, Spain 2002/10/04 
M. Missikoff,, R. Navigli, and P. Velardi, 2002. 
Integrated approach to Web ontology learning 
and engineering, Computer, Volume: 35 Issue: 
11 . 60 ?63 
N. F. Noy, and M. Musen, 2000. PROMPT: 
Algorithm and Tool for Automated Ontology 
Merging and Alignment, Proceedings of the 
National Conference on Artificial Intelligence. 
AAAI2000. 450-455 
S. Oyama, T. Kokubo, and T. Ishida, 2004. 
Domain-Specific Web Search with Keyword 
Spice. IEEE Transactions on Knowledge and 
Data Engineering, Vol 16,NO. 1, 17-27.  
Sinorama Magazine and Wordpedia.com Co., 2001. 
Multimedia CD-ROMs of Sinorama from 1976 to 
2000, Taipei. 
 
P. Vossen, and W. Peters, 1997. Multilingual 
design of EuroWordNet, Proceedings of the 
Delos workshop on Cross-language Information 
Retrieval. 
H. Weigard, and S. Hoppenbrouwers, 1998. 
Experiences with a multilingual ontology-based 
lexicon for news filtering, Proceedings in the 9th 
International Workshop on Database and Expert 
Systems Applications. 160-165 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1057?1064
Manchester, August 2008
OntoNotes: Corpus Cleanup of Mistaken Agreement Using  
Word Sense Disambiguation 
Liang-Chih Yu and Chung-Hsien Wu 
Dept. of Computer Science and Information Engineering
National Cheng Kung University 
Tainan, Taiwan, R.O.C. 
{lcyu,chwu}@csie.ncku.edu.tw 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA 90292, USA 
hovy@isi.edu 
 
 
 
 
Abstract 
Annotated corpora are only useful if their 
annotations are consistent.  Most large-scale 
annotation efforts take special measures to 
reconcile inter-annotator disagreement. To 
date, however, no-one has investigated how 
to automatically determine exemplars in 
which the annotators agree but are wrong. In 
this paper, we use OntoNotes, a large-scale 
corpus of semantic annotations, including 
word senses, predicate-argument structure, 
ontology linking, and coreference. To de-
termine the mistaken agreements in word 
sense annotation, we employ word sense 
disambiguation (WSD) to select a set of 
suspicious candidates for human evaluation. 
Experiments are conducted from three as-
pects (precision, cost-effectiveness ratio, and 
entropy) to examine the performance of 
WSD. The experimental results show that 
WSD is most effective on identifying erro-
neous annotations for highly-ambiguous 
words, while a baseline is better for other 
cases. The two methods can be combined to 
improve the cleanup process. This procedure 
allows us to find approximately 2% remain-
ing erroneous agreements in the OntoNotes 
corpus. A similar procedure can be easily 
defined to check other annotated corpora. 
1 Introduction 
Word sense annotated corpora are useful re-
sources for many natural language applications. 
Various machine learning algorithms can then be 
trained on these corpora to improve the applica-
tions? effectiveness. Lately, many such corpora 
have been developed in different languages, in-
cluding SemCor (Miller et al, 1993), LDC-DSO 
(Ng and Lee, 1996), Hinoki (Kasahara et al, 
2004), and the sense annotated corpora with the 
help of Web users (Chklovski and Mihalcea, 
2002). The SENSEVAL1 (Kilgarriff and Palmer, 
2000; Kilgarriff, 2001; Mihalcea and Edmonds, 
2004) and SemEval-20072 evaluations have also 
created large amounts of sense tagged data for 
word sense disambiguation (WSD) competitions.  
The OntoNotes (Pradhan et al, 2007a; Hovy et 
al., 2006) project has created a multilingual cor-
pus of large-scale semantic annotations, includ-
ing word senses, predicate-argument structure, 
ontology linking, and coreference3. In word sense 
creation, sense creators generate sense definitions 
by grouping fine-grained sense distinctions ob-
tained from WordNet and dictionaries into more 
coarse-grained senses. There are two reasons for 
this grouping instead of using WordNet senses 
directly. First, people have trouble distinguishing 
many of the WordNet-level distinctions in real 
text, and make inconsistent choices; thus the use 
of coarse-grained senses can improve inter-
annotator agreement (ITA) (Palmer et al, 2004; 
2006). Second, improved ITA enables machines 
to more accurately learn to perform sense tagging 
automatically. Sense grouping in OntoNotes has 
been calibrated to ensure that ITA averages at 
least 90%. Table 1 shows the OntoNotes sense 
                                                          
1 http://www.senseval.org 
2 http://nlp.cs.swarthmore.edu/semeval 
3 Year 1 of the OntoNotes corpus has been re-
leased by Linguistic Data Consortium (LDC) 
(http://www.ldc.upenn.edu) in early 2007. The 
Year 2 corpus will be released in early 2008. 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Un-ported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1057
tags and definitions for the word arm (noun 
sense). The OntoNotes sense tags have been used 
for many applications, including the SemEval-
2007 evaluation (Pradhan et al, 2007b), sense 
merging (Snow et al, 2007), sense pool verifica-
tion (Yu et al, 2007), and class imbalance prob-
lems (Zhu and Hovy, 2007). 
In creating OntoNotes, each word sense anno-
tation involves two annotators and an adjudicator. 
First, all sentences containing the target word 
along with its sense distinctions are presented 
independently to two annotators for sense annota-
tion. If the two annotators agree on the same 
sense for the target word in a given sentence, 
then their selection is stored in the corpus. Oth-
erwise, this sentence is double-checked by the 
adjudicator for the final decision. The major 
problem of the above annotation scheme is that 
only the instances where the two annotators dis-
agreed are double-checked, while those showing 
agreement are stored directly without any adjudi-
cation. Therefore, if the annotators happen to 
agree but are both wrong, then the corpus be-
comes polluted by the erroneous annotations. 
Table 2 shows an actual occurrence of an errone-
ous instance (sentence) for the target word man-
agement. In this example sentence, the actual 
sense of the target word is management.01, but 
both of our annotators made a decision of man-
agement.02. (Note that there is no difficulty in 
making this decision; the joint error might have 
occurred due to annotator fatigue, habituation 
after a long sequence of management.02 deci-
sions, etc.)   
Although most annotations in OntoNotes are 
correct, there is still a small (but unknown) frac-
tion of erroneous annotations in the corpus. 
Therefore, some cleanup procedure is necessary 
to produce a high-quality corpus. However, it is 
impractical for human experts to evaluate the 
whole corpus for cleanup. Given that we are fo-
cusing on word senses, this study proposes the 
use of WSD to facilitate the corpus cleanup proc-
ess. WSD has shown promising accuracy in re-
cent SENSEVAL and SemEval-2007 evaluations.  
The rest of this work is organized as follows. 
Section 2 describes the corpus cleanup procedure. 
Section 3 presents the features for WSD. Section 
4 summarizes the experimental results. Conclu-
sions are drawn in Section 5. 
2 Corpus Cleanup Procedure 
Figure 1 shows the cleanup procedure (dashed 
lines) for the OntoNotes corpus. As mentioned 
earlier, each word along with its sentence in-
stances is annotated by two annotators. The anno-
Sense Tag Sense Definition WordNet sense 
arm.01 The forelimb of an animal WN.1 
arm.02 A weapon WN.2 
arm.03 A subdivision or branch of an organization WN.3 
arm.04 A projection, a narrow extension of a structure WN.4 WN.5 
Table 1. OntoNotes sense tags and definitions. The WordNet version is 2.1. 
Example sentence: 
The 45-year-old Mr. Kuehn, who has a background in crisis management, succeeds Alan D. Rubendall, 45.
management.01: Overseeing or directing. Refers to the act of managing something. 
 
He was given overall management of the program. 
I'm a specialist in risk management. 
The economy crashed because of poor management. 
management.02: The people in charge. The ones actually doing the managing. 
 
Management wants to start downsizing. 
John was promoted to Management. 
I spoke to their management, and they're ready to make a deal. 
Table 2. Example sentence for the target word management along with its sense definitions. 
1058
tated corpus can thus be divided into two parts 
according to the annotation results. The first part 
includes the annotation with disagreement among 
the two annotators, which is then double-checked 
by the adjudicator. The final decisions made by 
the adjudicator are stored into the corpus. Since 
this part is double-checked by the adjudicator, it 
will not be evaluated by the cleanup procedure.  
The second part of the corpus is the focus of 
the cleanup procedure. The WSD system evalu-
ates each instance in the second part. If the output 
of the WSD system disagrees with the two anno-
tators, the instance is considered to be a suspi-
cious candidate, otherwise it is considered to be 
clean and stored into the corpus.  The set of sus-
picious candidates is collected and subsequently 
evaluated by the adjudicator to identify erroneous 
annotations. 
3 Word Sense Disambiguation 
This study takes a supervised learning approach 
to build a WSD system from the OntoNotes cor-
pus. The feature set used herein is similar to sev-
eral state-of-the-art WSD systems (Lee and Ng., 
2002; Ando, 2006; Tratz et al, 2007; Cai et al, 
2007; Agirre and Lopez de Lacalle, 2007; Specia 
et al, 2007), which is further integrated into a 
Na?ve Bayes classifier (Lee and Ng., 2002; Mi-
halcea, 2007). In addition, a new feature, predi-
cate-argument structure, provided by the 
OntoNotes corpus is also integrated. The feature 
set includes: 
Part-of-Speech (POS) tags: This feature in-
cludes the POS tags in the positions (P-3, P-2, P-1, 
P0, P1, P2, P3), relative to the POS tag of the tar-
get word.  
Local Collocations: This feature includes single 
words and multi-word n-grams. The single words 
include (W-3, W-2, W-1, W0, W1, W2, W3), relative 
to the target word W0. Similarly, the multi-word 
n-grams include (W-2,-1, W-1,1, W1,2, W-3,-2,-1, W-2,-1,1, 
W-1,1,2, W1,2,3). 
Bag-of-Words: This feature can be considered as 
a global feature, consisting of 5 words prior to 
and after the target word, without regard to posi-
tion. 
Predicate-Argument Structure: The predicate-
argument structure captures the semantic rela-
tions between the predicates and their arguments 
within a sentence, as shown in Figure 2. These 
relations can be either direct or indirect. A direct 
relation is used to model a verb-noun (VN) or 
noun-verb (NV) relation, whereas an indirect re-
lation is used to model a noun-noun (NN) rela-
tion. Additionally, an NN-relation can be built 
from the combination of an NV-relation and VN-
relation. For instance, in Figure 2, the NN-
relation (R3) can be built by combining the NV-
relation (R1) the VN-relation (R2). Therefore, the 
two features, R1 and R3, can be used to disam-
biguate the noun arm 4. 
4 Experimental Results 
4.1 Experiment setup  
The experiment data used herein was the 35 
nouns from the SemEval-2007 English Lexical 
Sample Task (Pradhan et al, 2007b). All sen-
tences containing the 35 nouns were selected 
from the OntoNotes corpus, resulting in a set of 
16,329 sentences. This data set was randomly 
split into training and test sets using different 
proportions (1:9 to 9:1, 10% increments). The 
WSD systems (described in Section 3) were then 
                                                          
4 Our WSD system does not include the sense 
identifier (except for the target word) for word-
level training and testing. 
The New York arm.03  ...  auctioned.01 off the estate.01
ARG0-INV ARG1
ARG0-INV-ARG1
NV-relation: (arm.03, ARG0-INV, auction.01)
VN-relation: (auction.01, ARG1, estate.01)
NN-relation: (arm.03, ARG0-INV-ARG1, estate.01)
Figure 2. Example of predicate-argument struc-
ture. The label ?-INV? denotes an inverse direc-
tion (i.e.,  from a noun to a verb). 
final decision
Annotation with 
agreement
Annotation with 
disagreement
AdjudicatorWSD
agree with 
annotators
disagree with annotators
Annotated
Corpus
 
Figure 1. Corpus cleanup procedure. 
1059
built from the different portions of the training 
set, called WSD_1 to WSD_9, respectively, and 
applied to their corresponding test sets. In each 
test set, the instances with disagreement among 
the annotators were excluded, since they have 
already been double-checked by the adjudicator. 
A baseline system was also implemented using 
the principle of most frequent sense (MFS), 
where each word sense distribution was retrieved 
from the OntoNotes corpus. Table 3 shows the 
accuracy of the baseline and WSD systems. 
The output of WSD may agree or disagree 
with the annotators. The instances with dis-
agreement were selected from each WSD system 
as suspicious candidates. This experiment ran-
domly selected at most 20 suspicious instances 
for each noun to form a suspicious set of 687 in-
stances. An adjudicator who is a linguistic expert 
then evaluated the suspicious set, and agreed in 
42 instances with the WSD systems, indicating 
about 6% (42/687) truly erroneous annotations. 
This corresponds to 2.6% (42/16329) erroneous 
annotations in the corpus as a whole, which we 
verified by an independent random spot check.  
In the following sections, we examine the per-
formance of WSD from three aspects: precision, 
cost-effectiveness ratio, and entropy, and finally 
summarize a general cleanup procedure for other 
sense annotated corpora. 
4.2 Cleanup precision analysis 
The cleanup precision for a single WSD system 
can be defined as the number of erroneous in-
stances identified by the WSD system, divided by 
the number of suspicious candidates selected by 
the WSD system. An erroneous instance refers to 
an instance where the annotators agree with each 
other but disagree with the adjudicator. Table 4 
lists the cleanup precision of the baseline and 
WSD systems. The experimental results show 
that WSD_7 (trained on 70% training data) iden-
tified 17 erroneous instances, out of 120 selected 
suspicious candidates, thus yielding the highest 
precision of 0.142. Another observation is that 
the upper bound of WSD_7 was 0.35 (42/120) 
under the assumption that it identified all errone-
ous instances. This low precision discourages the 
use of WSD to automatically correct erroneous 
annotations.  
4.3 Cleanup cost-effectiveness analysis 
The cleanup procedure used herein is a semi-
automatic process; that is, WSD is applied in the 
first stage to select suspicious candidates for hu-
man evaluation in the later stage. Obviously, we 
would like to minimize the number of candidates 
the adjudicator has to examine.  Thus we define a 
metric, the cost-effectiveness (CE) ratio, to 
measure the performance of WSD. The cost rate 
is defined as the number of suspicious instances 
selected by a single WSD system, divided by the 
total number of suspicious instances in the suspi-
cious set. The effectiveness rate is defined as the 
number of erroneous instances identified by a 
single WSD system, divided by the total number 
of erroneous instances in the suspicious set. In 
this experiment, the baseline value of the cost-
effectiveness ratio is 1, which means that human 
expert needs to evaluate all 687 instances in the 
suspicious set to identify 42 erroneous instances. 
Figure 3 illustrates the CE ratio of the WSD sys-
tems. The most cost-effective WSD system was 
WSD_7. The CE ratios of the baseline and 
WSD_7 are listed in Table 5. The experimental 
results indicate that 17.5% of suspicious in-
stances were required to be evaluated to identify 
about 40% erroneous annotations when using 
WSD_7. 
WSD  Baseline
(MFS) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
Accuracy 0.696 0.751 0.798 0.809 0.819 0.822 0.824 0.831 0.836 0.832
Table 3. Accuracy of the baseline and WSD systems with different training portions. 
WSD  Baseline (MFS) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
Prec 0.090 (17/188) 
0.113 
(20/177)
0.112 
(16/143) 
0.113 
(17/150)
0.124 
(16/129)
0.123 
(15/122)
0.127 
(16/126)
0.142 
(17/120) 
0.130 
(14/108) 
0.125 
(14/112)
Table 4. Cleanup precision of the baseline and WSD systems with different training portions. 
1060
4.4 Entropy analysis 
So far, the experimental results show that the best 
WSD system can help human experts identify 
about 40% erroneous annotations, but it still 
missed the other 60%. To improve performance, 
we conducted experiments to analyze the effect 
of word entropy with respect to WSD perform-
ance on identifying erroneous annotations. 
For the SemEval 35 nouns used in this experi-
ment, some words are very ambiguous and some 
words are not. This property of ambiguity may 
affect the performance of WSD systems on iden-
tifying erroneous annotation. To this end, this 
experiment used entropy to measure the ambigu-
ity of words (Melamed, 1997). The entropy of 
word can be computed by the word sense distri-
bution, defined as 
2( ) ( )log ( ),
i
i i
ws W
H W P ws P ws
?
= ? ?             (1) 
where ( )H W  denotes the entropy of a word W,  
and iws  denotes a word sense. A high entropy 
value indicates a high ambiguity level. For in-
stance, the noun defense has 7 senses (see Table 
7) in the OntoNotes corpus, occurring with the 
distribution {.14, .18, .19, .08, .04, .28, .09}, thus 
yielding a relative high entropy value (2.599). 
Conversely, the entropy of the noun rate is low 
(0.388), since it has only two senses with very 
skewed distribution {.92, .08}.   
Consider the two groups of the SemEval nouns: 
the nouns for which at least one (Group 1) or 
none (Group 2) of their erroneous instances can 
be identified by the machine. The average en-
tropy of these two groups of nouns was computed, 
as shown in Table 6. An independent t-test was 
then used to determine whether or not the differ-
ence of the average entropy among these two 
groups was statistically significant. The experi-
mental results show that WSD_7 was more effec-
tive on identifying erroneous annotations 
occurring in highly-ambiguous words (p<0.05), 
while the baseline system has no such tendency 
(p=0.368). 
Table 7 shows the detail analysis of WSD per-
formance on different words. As indicated, 
WSD_7 identified the erroneous instances (7/7) 
occurring in the two top-ranked highly-
ambiguous nouns, i.e., defense and position, but 
missed all those (0/12) occurring in the two most 
unambiguous words, i.e., move and rate. The ma-
jor reason is that the sense distribution of unam-
biguous words is often skew, thus WSD systems 
built from such imbalanced data tend to suffer 
from the over-fitting problem; that is, tend to 
over-fit the predominant sense class and ignore 
small sense classes (Zhu and Hovy, 2007). Fortu-
nately, the over-fitting problem can be greatly 
reduced when the entropy of words exceeds a 
certain threshold (e.g., the dashed line in Table 7), 
since the word sense has become more evenly 
distributed.  
4.5 Combination of WSD and MFS 
Another observation from Table 7 is that WSD_7 
identified more erroneous instances when the 
word entropy exceeded the cut-point, since the 
over-fitting problem was reduced. Conversely, 
MFS identified more ones when the word entropy 
is below the cut-point. This finding encourages 
the use of a combination of WSD_7 and MFS for 
corpus cleanup; that is, different strategies can be 
used with different entropy intervals. For this 
experiment data, MFS and WSD_7 can be ap-
plied below and above the cut-point, respectively, 
to select the suspicious instances for human 
evaluation. As illustrated in Figure 4, when the 
entropy of words increased, the accumulated ef-
fectiveness rates of both WSD_7 and MFS in-
creased accordingly, since more erroneous 
instances were identified. Additionally, the dif-
ference of the accumulated effect rate of MFS 
 Cost Effect CE Ratio
Baseline
(MFS) 
0.274 
(188/687)
0.405 
(17/42) 1.48 
WSD_7 0.175 (120/687)
0.405 
(17/42) 2.31 
Table 5. CE ratio of the baseline and WSD_7. 
 
Figure 3. CE ratio of WSD systems with differ-
ent training portions. 
1061
and WSD_7 increased gradually from the begin-
ning until the cut-point, since MFS identified 
more erroneous instances than WSD_7 did in this 
stage. When the entropy exceeded the cut-point, 
WSD_7 was more effective and thus its effec-
tiveness rate kept increasing, while that of MFS 
increased slowly, thus their difference was de-
creased with the rise of the entropy. For the com-
bination of MFS and WSD_7, its effectiveness 
rate before the cut-point was the same as that of 
MFS, since MFS was used in this stage to select 
the suspicious set. When WSD was used after the 
cut-point, the effectiveness rate of the combina-
tion system increased continuously, and finally 
reached 0.5 (21/42).  
Based on the above experimental results, the 
most cost-effective way for corpus cleanup is to 
use the combination method and begin with the 
most ambiguous words, since the WSD system in 
the combination method is more effective on 
identifying erroneous instances occurring in 
highly-ambiguous words and these words are 
also more important for many applications. Fig-
ure 5 shows the curve of the CE ratios of the 
combination method by starting with the most 
ambiguous word. The results indicate that the CE 
ratio of the combination method decreased 
gradually after more words with lower entropy 
were involved in the cleanup procedure. Addi-
tionally, the CE ratio of the combination method 
was improved by using MFS after the cut-point 
and finally reached 2.50, indicating that 50% 
(21/42) erroneous instances can be identified by 
double-checking 20% (137/687) of the suspicious 
set. This CE ratio was better than 2.31 and 1.48, 
reached by WSD_7 and MFS respectively. 
The proposed cleanup procedure can be ap-
plied to other sense annotated corpora by the fol-
lowing steps: 
Noun #sense Major Sense Entropy
#err. 
instances WSD_7 MFS 
WSD_7+ 
MFS 
defense 7 0.28 2.599 5 5 4 5 
position 7 0.30 2.264 2 2 2 2 
base 6 0.35 2.023 1 1 0 1 
system 6 0.54 1.525 2 1 0 1 
chance 4 0.49 1.361 1 1 1 1 
order 8 0.72 1.348 4 1 0 1 
part 5 0.70 1.288 1 1 1 1 
power 3 0.51 1.233 3 1 3 3 
area 3 0.72 1.008 2 1 2 2 
management 2 0.62 0.959 2 1 0 0 
condition 3 0.71 0.906 1 0 1 1 
job 3 0.78 0.888 1 0 0 0 
state 4 0.83 0.822 1 0 0 0 
hour 4 0.85 0.652 1 1 1 1 
value 3 0.90 0.571 2 1 1 1 
plant 3 0.88 0.556 1 0 0 0 
move 4 0.93 0.447 6 0 0 0 
rate 2 0.92 0.388 6 0 1 1 
Total ? ? ? 42 17 17 21 
Nouns without erroneous instances: authority, bill, capital, carrier, development, drug, 
effect, exchange, future, network, people, point, policy, president, share, source, space 
Table 7. Entropy of words versus WSD performance. The dashed line denotes a cut-point for the com-
bination of the baseline and WSD_7. 
 Group 1 Group 2 Difference p-value 
Baseline (MFS) 1.226 1.040 0.186 0.368 
WSD_7 1.401 0.932 0.469* 0.013 
  *p<0.05 
Table 6. Average entropy of two groups of nouns for the baseline and WSD_7. 
1062
z Build the baseline (MFS) and WSD systems 
from the corpus. 
z Create a suspicious set from the WSD systems. 
z Calculate the entropy for each word in terms 
of it sense distribution in the corpus. 
z Choose a cut-point value. Select a small por-
tion of words with entropy within a certain in-
terval (e.g., 1.0 ~ 1.5 in Table 7) for human 
evaluation to decide an appropriate cut-point 
value. The cut-point value should not be too 
low or too high, since WSD systems may suf-
fer from the over-fitting problem if it is too 
low, and the performance would be dominated 
by the baseline system if it is too high.  
z Combine the baseline and best single WSD 
system through the cut-point. 
z Start the cleanup procedure in the descending 
order of word entropy until the CE ratio is be-
low a predefined threshold. 
5 Conclusion 
This study has presented a cleanup procedure to 
identify incorrect sense annotation in a corpus. 
The cleanup procedure incorporates WSD sys-
tems to select a set of suspicious instances for 
human evaluation. The experiments are con-
ducted from three aspects: precision, cost-
effectiveness ratio, and entropy, to examine the 
performance of WSD. The experimental results 
show that the WSD systems are more effective 
on highly-ambiguous words. Additionally, the 
most cost-effective cleanup strategy is to use the 
combination method and begin with the most 
ambiguous words. The incorrect sense annota-
tions found in this study can be used for SemE-
val-2007 to improve the accuracy of WSD 
evaluation.  
The absence of related work on (semi-) auto-
matically determining cases of erroneous agree-
ment among annotators in a corpus is rather 
surprising. Variants of the method described here, 
replacing WSD for whatever procedure is appro-
priate for the phenomenon annotated in the cor-
pus (sentiment recognition for a sentiment corpus, 
etc.), are easy to implement and may produce 
useful results for corpora in current use. Future 
work will focus on devising an algorithm to per-
form the cleanup procedure iteratively on the 
whole corpus. 
References  
E. Agirre and O. Lopez de Lacalle. 2007. UBC-ALM: 
Combining k-NN with SVD for WSD. In Proc. of 
the 4th International Workshop on Semantic 
Evaluations (SemEval-2007) at ACL-07, pages 342-
345. 
R.K. Ando. 2006. Applying Alternating Structure Op-
timization to Word Sense Disambiguation. In Proc. 
of CoNLL, pages 77-84. 
J.F. Cai, W.S. Lee, and Y.W. Teh. 2007. Improving 
Word Sense Disambiguation Using Topic Features. 
In Proc. of EMNLP-CoNLL, pages 1015-1023. 
T. Chklovski and R. Mihalcea. 2002. Building a Sense 
Tagged Corpus with Open Mind Word Expert. In 
Proc. of the Workshop on Word Sense Disambigua-
tion: Recent Successes and Future Directions at 
ACL-02, pages 116-122. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 
E.H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and 
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proc. of HLT/NAACL-06, pages 57-60. 
K. Kasahara, H. Sato, F. Bond, T. Tanaka, S. Fujita, T. 
Kanasugi, and S. Amano. 2004. Construction of a 
Figure 4. Effectiveness rate against word entropy.
 
Figure 5. CE ratio against word entropy. 
1063
apanese Semantic Lexicon: Lexeed. In IPSG SIG: 
2004-NLC-159, Tokyo, pages 75-82. 
A. Kilgarriff. 2001. English Lexical Sample Task De-
scription. In Proc. of the SENSEVAL-2 Workshop, 
pages 17-20. 
A. Kilgarriff and M. Palmer, editors. 2000. 
SENSEVAL: Evaluating Word Sense Disambigua-
tion Programs, Computer and the Humanities, 
34(1-2):1-13. 
Y.K. Lee and H.T. Ng. 2002. An Empirical Evaluation 
of Knowledge Sources and Learning Algorithms 
for Word Sense Disambiguation. In Proc. of 
EMNLP, pages 41-48. 
I.D. Melamed. 1997. Measuring Semantic Entropy. In 
Proc. of ACL-SIGLEX Workshop, pages 41-46. 
R. Mihalcea. 2007. Using Wikipedia for Automatic-
Word Sense Disambiguation. In Proc. of 
NAACL/HLT-07, pages 196-203. 
R. Mihalcea and P. Edmonds, editors. 2004. In Proc. 
of SENSEVAL-3. 
G. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993. 
A Semantic Concordance. In Proc. of the 3rd 
DARPA Workshop on Human Language Technol-
ogy, pages 303?308. 
H.T. Ng and H.B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-based Approach. In Proc. of the 34th 
Meeting of the Association for Computational Lin-
guistics (ACL-96), pages 40-47. 
M. Palmer, O. Babko-Malaya, and H.T. Dang. 2004. 
Different Sense Granularities for Different Applica-
tions. In Proc. of the 2nd International Workshop 
on Scalable Natural Language Understanding at 
HLT/NAACL-04. 
M. Palmer, H.T. Dang, and C. Fellbaum. 2006. Mak-
ing Fine-grained and Coarse-grained Sense Distinc-
tions, Both Manually and Automatically. Journal of 
Natural Language Engineering, 13:137?163. 
S. Pradhan, E.H. Hovy, M. Marcus, M. Palmer, L. 
Ramshaw, and R. Weischedel. 2007a. OntoNotes: 
A Unified Relational Semantic Representation. In 
Proc. of the First IEEE International Conference 
on Semantic Computing (ICSC-07), pages 517-524.   
S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 
2007b. SemEval-2007 Task 17: English Lexical 
Sample, SRL and All Words. In Proc. of the 4th In-
ternational Workshop on Semantic Evaluations 
(SemEval-2007) at ACL-07, pages 87-92. 
R. Snow, S. Prakash, D. Jurafsky, and A.Y. Ng. 2007. 
Learning to Merge Word Senses. In Proc. of 
EMNLP-CoNLL, pages 1005-1014. 
L. Specia, M. Stevenson, and M. das Gracas V. Nunes. 
2007. Learning Expressive Models for Word Sense 
Disambiguation. In Proc. of the 45th Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-07), pages 41?48. 
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell, C. 
Posse, and P. Whitney. 2007. PNNL: A Supervised 
Maximum Entropy Approach to Word Sense Dis-
ambiguation. In Proc. of the 4th International 
Workshop on Semantic Evaluations (SemEval-2007) 
at ACL-07, pages 264-267. 
L.C. Yu, C.H. Wu, A. Philpot, and E.H. Hovy. 2007. 
OntoNotes: Sense Pool Verification Using Google 
N-gram and Statistical Tests. In Proc. of the On-
toLex Workshop at the 6th International Semantic 
Web Conference (ISWC 2007). 
J. Zhu and E.H. Hovy. 2007. Active Learning for 
Word Sense Disambiguation with Methods for Ad-
dressing the Class Imbalance Problem, In Proc. of 
EMNLP-CoNLL, pages 783-790. 
 
1064
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 945?952,
Sydney, July 2006. c?2006 Association for Computational Linguistics
HAL-based Cascaded Model for Variable-Length 
Semantic Pattern Induction from Psychiatry Web Resources 
 
Liang-Chih Yu and Chung-Hsien Wu 
Department of Computer Science and Information Engineering
National Cheng Kung University 
Tainan, Taiwan, R.O.C. 
{lcyu, chwu}@csie.ncku.edu.tw 
Fong-Lin Jang 
Department of Psychiatry 
Chi-Mei Medical Center 
Tainan, Taiwan, R.O.C. 
jcj0429@seed.net.tw
  
 
Abstract 
Negative life events play an important 
role in triggering depressive episodes. 
Developing psychiatric services that can 
automatically identify such events is 
beneficial for mental health care and pre-
vention. Before these services can be 
provided, some meaningful semantic pat-
terns, such as <lost, parents>, have to be 
extracted. In this work, we present a text 
mining framework capable of inducing 
variable-length semantic patterns from 
unannotated psychiatry web resources. 
This framework integrates a cognitive 
motivated model, Hyperspace Analog to 
Language (HAL), to represent words as 
well as combinations of words. Then, a 
cascaded induction process (CIP) boot-
straps with a small set of seed patterns 
and incorporates relevance feedback to 
iteratively induce more relevant patterns. 
The experimental results show that by 
combining the HAL model and relevance 
feedback, the CIP can induce semantic 
patterns from the unannotated web cor-
pora so as to reduce the reliance on anno-
tated corpora. 
1 Introduction 
Depressive disorders have become a major threat 
to mental health. People in their daily life may 
suffer from some negative or stressful life events, 
such as death of a family member, arguments 
with a spouse, loss of a job, and so forth. Such 
life events play an important role in triggering 
depressive symptoms, such as depressed mood, 
suicide attempts, and anxiety. Therefore, it is 
desired to develop a system capable of identify-
ing negative life events to provide more effective 
psychiatric services. For example, through the 
negative life events, the health professionals can 
know the background information about subjects 
so as to make more correct decisions and sugges-
tions. Negative life events are often expressed in 
natural language segments (e.g., sentences). To 
identify them, the critical step is to transform the 
segments into machine-interpretable semantic 
representation. This involves the extraction of 
key semantic patterns from the segments. Con-
sider the following example. 
Two years ago, I lost my parents.     (Event) 
Since that, I have attempted to kill myself 
several times.   (Suicide) 
In this example, the semantic pattern <lost, par-
ents> is constituted by two words, which indi-
cates that the subject suffered from a negative 
life event that triggered the symptom ?Suicide?. 
A semantic pattern can be considered as a se-
mantically plausible combination of k words, 
where k is the length of the pattern. Accordingly, 
a semantic pattern may have variable length. In 
Wu et al?s study (2005), they have presented a 
methodology to identify depressive symptoms. In 
this work, we go a further step to devise a text 
mining framework for variable-length semantic 
pattern induction from psychiatry web resources. 
Traditional approaches to semantic pattern in-
duction can be generally divided into two 
streams: knowledge-based approaches and cor-
pus-based approaches (Lehnert et al, 1992; 
Muslea, 1999). Knowledge-based approaches 
rely on exploiting expert knowledge to design 
handcrafted semantic patterns. The major limita-
tions of such approaches include the requirement 
of significant time and effort on designing the 
handcrafted patterns. Besides, when applying to 
a new domain, these patterns have to be redes-
igned. Such limitations form a knowledge acqui-
sition bottleneck. A possible solution to reducing 
the problem is to use a general-purpose ontology 
945
such as WordNet (Fellbaum, 1998), or a domain-
specific ontology constructed using automatic 
approaches (Yeh et al, 2004). These ontologies 
contain rich concepts and inter-concept relations 
such as hypernymy-hyponymy relations. How-
ever, an ontology is a static knowledge resource, 
which may not reflect the dynamic characteris-
tics of language. For this consideration, we in-
stead refer to the web resources, or more restrict-
edly, the psychiatry web resources as our knowl-
edge resource. 
Corpus-based approaches can automatically 
learn semantic patterns from domain corpora by 
applying statistical methods. The corpora have to 
be annotated with domain-specific knowledge 
(e.g., events). Then, various statistical methods 
can be applied to induce variable-length semantic 
patterns from all possible combinations of words 
in the corpora. However, statistical methods may 
suffer from data sparseness problem, thus they 
require large corpora with annotated information 
to obtain more reliable parameters. For some ap-
plication domains, such annotated corpora may 
be unavailable. Therefore, we propose the use of 
web resources as the corpora. When facing with 
the web corpora, traditional corpus-based ap-
proaches may be infeasible. For example, it is 
impractical for health professionals to annotate 
the whole web corpora. Besides, it is also im-
practical to enumerate all possible combinations 
of words from the web corpora, and then search 
for the semantic patterns.  
To address the problems, we take the notion of 
weakly supervised (Stevenson and Greenwood, 
2005) or unsupervised learning (Hasegawa, 2004; 
Grenager et al, 2005) to develop a framework 
able to bootstrap with a small set of seed patterns, 
and then induce more relevant patterns form the 
unannotated psychiatry web corpora. By this 
way, the reliance on annotated corpora can be 
significantly reduced. The proposed framework 
is divided into two parts: Hyperspace Analog to 
Language (HAL) model (Burgess et al, 1998; 
Bai et al, 2005), and a cascaded induction proc-
ess (CIP). The HAL model, which is a cognitive 
motivated model, provides an informative infra-
structure to make the CIP capable of learning 
from unannotated corpora. The CIP treats the 
variable-length induction task as a cascaded 
process. That is, it first induces the semantic pat-
terns of length two, then length three, and so on. 
In each stage, the CIP initializes the set of se-
mantic patterns to be induced based on the better 
results of the previous stage, rather than enumer-
ating all possible combinations of words. This 
would be helpful to avoid noisy patterns propa-
gating to the next stage, and the search space can 
also be reduced. 
A crucial step for semantic pattern induction is 
the representation of words as well as combina-
tions of words. The HAL model constructs a 
high-dimensional context space for the psychia-
try web corpora. Each word in the HAL space is 
represented as a vector of its context words, 
which means that the sense of a word can be in-
ferred through its contexts. Such notion is de-
rived from the observation of human behavior. 
That is, when an unknown word occurs, human 
beings may determine its sense by referring to 
the words appearing in the contexts. Based on 
the cognitive behavior, if two words share more 
common contexts, they are more semantically 
similar. To further represent a semantic pattern, 
the HAL model provides a mechanism to com-
bine its constituent words over the HAL space. 
Once the HAL space is constructed, the CIP 
takes as input a seed pattern per run, and in turn 
induces the semantic patterns of different lengths. 
For each length, the CIP first creates the initial 
set based on the results of the previous stage. 
Then, the induction process is iteratively per-
formed to induce more patterns relevant to the 
given seed pattern by comparing their context 
distributions. In addition, we also incorporate 
expert knowledge to guide the induction process 
by using relevance feedback (Baeza-Yates and 
Ribeiro-Neto, 1999), the most popular query re-
formulation strategy in the information retrieval 
(IR) community. The induction process is termi-
nated until the termination criteria are satisfied. 
In the remainder of this paper, Section 2 pre-
sents the overall framework for variable-length 
semantic pattern induction. Section 3 describes 
the process of constructing the HAL space. Sec-
tion 4 details the cascaded induction process. 
Section 5 summarizes the experiment results. 
Finally, Section 6 draws some conclusions and 
suggests directions for future work. 
2 Framework for Variable-Length Se-
mantic Pattern Induction 
The overall framework, as illustrated in Figure 1, 
is divided into two parts: the HAL model and the 
cascaded induction process. First of all, the HAL 
space is constructed for the psychiatry web 
corpora after word segmentation. Then, each 
word in HAL space is evaluated by computing its 
distance to a given seed pattern. A smaller 
distance   represents   that    the   word   is   more  
946
Distance
Evaluation
Stop Induced 
Patterns
Psychiatry
Web Corpora
HAL Space
Construction
Seed
Patterns
Word
Segmentation
HAL model
Iteration +1
Quality
Concepts
length 2 length 3 length k...
No
Relevance 
Feedback
Iteration=0
Initial Set
(length k)
Yes
k +1
Cascaded Induction Process
Induced
Patterns
Relevant 
Patterns
 
Figure 1. Framework for variable-length seman-
tic pattern induction. 
semantically related to the seed pattern. 
According to the distance measure, the CIP 
generates quality concepts, i.e., a set of 
semantically related words to the seed pattern. 
The quality concepts and the better semantic 
patterns induced in the previous stage are 
combined to generate the initial set for each 
length. For example, in the beginning stage, i.e., 
length two, the initial set is the all possible 
combinations of two quality concepts. In the later 
stages, each initial set is generated by adding a 
quality concept to each of the better semantic 
patterns. After the initial set for a particular 
length is created, each semantic pattern and the 
seed pattern are represented in the HAL space for 
further computing their distance. The more 
similar the context distributions between two 
patterns, the closer they are. Once all the 
semantic patterns are evaluated, the relevance 
feedback is applied to provide a set of relevant 
patterns judged by the health professionals. 
According to the relevant information, the seed 
pattern can be refined to be more similar to the 
relevant set. The refined seed pattern will be 
taken as the reference basis in the next iteration. 
The induction process for each stage is 
performed iteratively until no more patterns are 
judged as relevant or a maximum number of 
iteration is reached. The relevant set produced at 
the last iteration is considered as the result of the 
semantic patterns. 
3 HAL Space Construction 
The HAL model represents each word in the vo-
cabulary  using   a   vector  representation.  Each  
w1 w2 wl-2 wl-1 wl
AObservation window of length 
weight =1
2
 
Figure 2. Weighting scheme of the HAL model. 
 two years ago I lost my parents
two 0 0 0 0 0 0 0 
years 5 0 0 0 0 0 0 
ago 4 5 0 0 0 0 0 
I 3 4 5 0 0 0 0 
lost 2 3 4 5 0 0 0 
my 1 2 3 4 5 0 0 
parents 0 1 2 3 4 5 0 
Table 1. Example of HAL Space (window size=5) 
dimension of the vector is a weight representing 
the strength of association between the target 
word and its context word. The weights are com-
puted by applying an observation window of 
length l over the corpus. All words within the 
window are considered as co-occurring with each 
other. Thus, for any two words of distance d 
within the window, the weight between them is 
computed as 1l d? + . Figure 2 shows an exam-
ple. The HAL space views the corpus as a se-
quence of words. Thus, after moving the window 
by one word increment over the whole corpus, 
the HAL space is constructed. The resultant HAL 
space is an N N?  matrix, where N is the vo-
cabulary size. In addition, each word in the HAL 
space is called a concept. Table 1 presents the 
HAL space for the example text ?Two years ago, 
I lost my parents.?  
3.1 Representation of a Single Concept 
For each concept in Table 1, the correspond-
ing row vector represents its left context infor-
mation, i.e., the weights of the words preceding it. 
Similarly, the corresponding column vector 
represents its right context information. Accord-
ingly, each concept can be represented by a pair 
of vectors. That is, 
( )1 2 1 2
( , )
 ,  ,  . . . ,  , ,  ,  . . . ,  ,
i i
i i i N i i i N
left right
i c c
left left left right right right
c t c t c t c t c t c t
c v v
w w w w w w
=
=
               (1) 
where 
i
left
cv and i
right
cv represent the vectors of the 
left context information and right context infor-
mation of a concept ic , respectively, i jc tw denotes  
947
1 1 1
 
          ...          
N
Left Context
left left
c t c tw w

1c
Nc
.
.
.
1 1 1
 
         ...          
N
Right Context
right right
c t c tw w

 
Figure 3. Conceptual representation of the HAL 
space. 
the weight of the j-th dimension ( jt ) of a vector, 
and N is the dimensionality of a vector, i.e., vo-
cabulary size. The conceptual representation is 
depicted in Figure 3. 
The weighting scheme of the HAL model is 
frequency-based. For some extremely infrequent 
words, we consider them as noises and remove 
them from the vocabulary. On the other hand, a 
high frequent word tends to get a higher weight, 
but this does not mean the word is informative, 
because it may also appear in many other vectors. 
Thus, to measure the informativeness of a word, 
the number of the vectors the word appears in 
should be taken into account. In principle, the 
more vectors the word appears in, the less infor-
mation it carries to discriminate the vectors. Here 
we use a weighting scheme analogous to TF-IDF 
(Baeza-Yates and Ribeiro-Neto, 1999) to re-
weight the dimensions of each vector, as de-
scribed in Equation (2). 
* log ,
( )i j i j
vector
c t c t
j
N
w w
vf t
=            (2) 
where vectorN  denotes the total number of vectors, 
and ( )jvf t  denotes the number of vectors with jt  
as the dimension. After each dimension is re-
weighted, the HAL space is transformed into a 
probabilistic framework. Accordingly, each 
weight can be redefined as 
( | ) ,i j
i j
i j
c t
c t j i
c t
j
w
w P t c
w
? = ?            (3) 
where ( | )j iP t c  is the probability that jt  appears 
in the vector of ic . 
3.2 Concept Combination 
A semantic pattern is constituted by a set of con-
cepts, thus it can be represented through concept 
combination over the HAL space. This forms a 
new concept in the HAL space. Let 
1( ,..., )Ssp c c=  be a semantic pattern with S con-
stituent concepts, i.e., length S. The concept 
combination is defined as 
1 2 3((...( ) ) ... ),s Sc c c c c? ? ? ? ? ?            (4) 
where ?  denotes the symbol representing the 
combination operator over the HAL space, sc?  
denotes a new concept generated by the concept 
combination. The new concept is the representa-
tion of a semantic pattern, also a vector represen-
tation. That is, 
( )1 1( ) ( ) ( ) ( )
( , )
     ,  . . . ,  , ,  . . . ,  ,
s s
s s N s s N
left right
s c c
left left right right
c t c t c t c t
c v v
w w w w
? ?
? ? ? ?
? =
=
               (5) 
The combination operator, ? , is implemented 
by the product of the weights of the constituent 
concepts, described as follows. 
( )
1
1
         ( | ),
s j s j
S
c t c t
s
S
j s
s
w w
P t c
?
=
=
=
=
?
?
            (6) 
where ( )s jc tw ? denotes the weight of the j-th di-
mension of the new concept sc? . 
4 Cascaded Induction Process 
Given a seed pattern, the CIP is to induce a set of 
relevant semantic patterns with variable lengths 
(from 2 to k). Let 1( ,..., )seed Rsp c c=  be a seed 
pattern of length R, and 1( ,..., )Ssp c c=  be a 
semantic pattern of length S. The formal 
description of the CIP is presented as 
{ }
{ } ( )1 1
  |      
( ,..., )  |  ( ,..., )     iff   , ,
seed
R S r s
sp sp
c c c c Dist c c ?
?
? ? ? ? ? ?
                (7) 
where |? denotes the symbol representing the 
cascaded induction, rc?  and sc?  are the two 
new concepts representing seedsp  and sp , respec-
tively, and (  ,   )Dist i i  represents the distance 
between two semantic patterns. The main steps 
in the CIP include the initial set generation, dis-
tance measure, and relevance feedback. 
4.1 Initial Set Generation 
The initial set for a particular length contains a 
set of semantic patterns to be induced, i.e., the 
search space. Reducing the search space would 
be helpful for speeding up the induction process, 
948
especially for inducing those patterns with a lar-
ger length. For this purpose, we consider that the 
words and the semantic patterns similar to a 
given seed pattern are the better candidates for 
creating the initial sets. Therefore, we generate 
quality concepts, a set of semantically related 
words to a seed pattern, as the basis to create the 
initial set for each length. Thus, each seed pattern 
will be associated with a set of quality concepts. 
In addition, the better semantic patterns induced 
in the previous stage are also considered. The 
goodness of words and semantic patterns is 
measured by their distance to a seed pattern. 
Here, a word is considered as a quality concept if 
its distance is smaller than the average distance 
of the vocabulary. Similarly, only the semantic 
patterns with a distance smaller than the average 
distance of all semantic patterns in the previous 
stage are preserved to the next stage. By the way, 
the semantically unrelated patterns, possibly 
noisy patterns, will not be propagated to the next 
stage, and the search space can also be reduced. 
The principles of creating the initial sets of se-
mantic patterns are summarized as follows.  
? In the beginning stage, the aim is to cre-
ate the initial set for the semantic pat-
terns with length two. Thus, the initial 
set is the all possible combinations of 
two quality concepts. 
? In the latter stages, each initial set is cre-
ated by adding a quality concept to each 
of the better semantic patterns induced in 
the previous stage. 
4.2 Distance Measure 
The distance measure is to measure the distance 
between the seed patterns and semantic patterns 
to be induced. Let 1( ,..., )Ssp c c=  be a semantic 
pattern and 1( ,..., )seed Rsp c c=  be a given seed 
pattern, their distance is defined as 
( ), ( , ),seed s rDist sp sp Dist c c= ? ?            (8) 
where ( , )s rDist c c? ?  denotes the distance be-
tween two semantic patterns in the HAL space. 
As mentioned earlier, after concept combination, 
a semantic pattern becomes a new concept in the 
HAL space, which means the semantic pattern 
can be represented by its left and right contexts. 
Thus, the distance between two semantic patterns 
can be computed through their context distance. 
Equation (8) thereby can be written as 
( ), ( , ) ( , ).
s r s r
left left Right Right
seed c c c cDist sp sp Dist v v Dist v v? ? ? ?= +  (9) 
Because the weights of the vectors are repre-
sented using a probabilistic framework, each 
vector of a concept can be considered as a prob-
abilistic distribution of the context words. Ac-
cordingly, we use the Kullback-Liebler (KL) dis-
tance (Manning and Sch?tze, 1999) to compute 
the distance between two probabilistic distribu-
tions, as shown in the following. 
1
( )
( ) ( ) log ,
( )s r
N
j s
c c j s
j j r
P t c
D v v P t c
P t c? ? =
?= ? ??           (10) 
where (    )D i i  denotes the KL distance be-
tween two probabilistic distributions. When 
Equation (10) is ill-conditioned, i.e., zero de-
nominator, the denominator will be set to a small 
value (10-6). For the consideration of a symmet-
ric distance, we use the divergence measure, 
shown as follows. 
( , ) ( ) ( ).
s r s r r sc c c c c c
Div v v D v v D v v? ? ? ? ? ?= +        (11) 
By this way, the distance between two probabil-
istic distributions can be computed by their KL 
divergence. Thus, Equation (9) becomes 
( , ) ( , ) ( , ).
s r s r s r
left left Right Right
c c c c c cDist v v Div v v Div v v? ? ? ? ? ?= + (12) 
After each semantic pattern is evaluated, a 
ranked list is produced for relevance judgment. 
4.3 Relevance Feedback 
In the induction process, some non-relevant se-
mantic patterns may have smaller distance to a 
seed pattern, which may decrease the precision 
of the final results. To overcome the problem, 
one possible solution is to incorporate expert 
knowledge to guide the induction process. For 
this purpose, we use the technique of relevance 
feedback. In the IR community, the relevance 
feedback is to enhance the original query from 
the users by indicating which retrieved docu-
ments are relevant. For our task, the relevance 
feedback is applied after each semantic pattern is 
evaluated. Then, the health professionals judge 
which semantic patterns are relevant to the seed 
pattern. In practice, only the top n semantic pat-
terns are presented for relevance judgment. Fi-
nally, the semantic patterns judged as relevant 
are considered to form the relevant set, and the 
others form the non-relevant set. According to 
the relevant and non-relevant information, the 
seed pattern can be refined to be more similar to 
the relevant set, such that the induction process 
can induce more relevant patterns and move 
away from noisy patterns in the future iterations. 
949
The refinement of the seed pattern is to adjust 
its context distributions (left and right). Such ad-
justment is based on re-weighting the dimensions 
of the context vectors of the seed pattern. The 
dimensions more frequently regarded as relevant 
patterns are more significant for identifying rele-
vant patterns. Hence, such dimensions of the 
seed pattern should be emphasized. The signifi-
cance of a dimension is measured as follows. 
( )
( )
( ) ,
i k
i
j k
j
c t
c R
k
c t
c R
w
Sig t
w
?
? ?
?
? ?
=
?
?           (13) 
where ( )kSig t  denotes the significance of the di-
mension kt , ic?  and jc?  denote the semantic 
patterns of the relevant set and non-relevant set, 
respectively, and ( )i kc tw ?  and ( )j kc tw ?  denote the 
weights of kt  of ic?  and jc? , respectively. The 
higher the ratio, the more significant the dimen-
sion is. In order to smooth ( )kSig t  to the range 
from zero to one, the following formula is used: 
1
( ) ( )
1
( ) .
1
i k j k
i j
k
c t c t
c R c R
Sig t
w w
?
? ?
? ? ? ?
=
? ?? ?+ ? ?? ?
? ?
    (14) 
The corresponding dimension of the seed pattern 
seed rsp c= ?  is then re-weighted by 
( ) ( ) ( ).r k r kc t c t kw w Sig t? ?= +           (15) 
Once the context vectors of the seed pattern 
are re-weighted, they are also transformed into a 
probabilistic form using Equation (3). The re-
fined seed pattern will be taken as the reference 
basis in the next iteration. The relevance feed-
back is performed iteratively until no more se-
mantic patterns are judged as relevant or a 
maximum number of iteration is reached. At the 
same time, the induction process for a particular 
length is also stopped. The whole CIP process is 
stopped until the seed patterns are exhausted 
5 Experimental Results 
To evaluate the performance of the CIP, we built 
a prototype system and provided a set of seed 
patterns. The seed patterns were collected by re-
ferring to the well-defined instruments for as-
sessing negative life events (Brostedt and Peder-
sen, 2003; Pagano et al, 2004). A total of 20 
seed patterns were selected by the health profes-
sionals. Then, the CIP randomly selects one seed 
pattern per run without replacement from the 
seed set, and iteratively induces relevant patterns 
from the psychiatry web corpora. The psychiatry 
web corpora used here include some professional 
mental health web sites, such as PsychPark 
(http://www.psychpark.org) (Bai, 2001) and John 
Tung Foundation (http://www.jtf.org.tw). 
In the following sections, we describe some 
experiments to in turn examine the effect of us-
ing relevance feedback or not, and the coverage 
on real data using the semantic patterns induced 
by different approaches. Because the semantic 
patterns with a length larger than 4 are very rare 
to express a negative life event, we limit the 
length k to the range of 2 to 4. 
5.1 Evaluation on Relevance Feedback 
The relevance feedback employed in this study 
provides the relevant and non-relevant informa-
tion for the CIP so that it can refine the seed pat-
tern to induce more relevant patterns. The rele-
vance judgment is carried out by three experi-
enced psychiatric physicians. For practical con-
sideration, only the top 30 semantic patterns are 
presented to the physicians. During relevance 
judgment, a majority vote mechanism is used to 
handle the disagreements among the physicians. 
That is, a semantic pattern is considered as rele-
vant if any two or more physicians judged it as 
relevant. Finally, the semantic patterns with ma-
jority votes are obtained to form the relevant set. 
To evaluate the effectiveness of the relevance 
feedback, we construct three variants of the CIP, 
RF(5), RF(10), and RF(20), implemented by ap-
plying the relevance feedback for 5, 10, and 20 
iterations, respectively. These three CIP variants 
are then compared to the one without using the 
relevance feedback, denoted as RF(?). We use 
the evaluation metric, precision at 30 (prec@30), 
over all seed patterns to examine if the relevance 
feedback can help the CIP induce more relevant 
patterns. For a particular seed pattern, prec@n is 
computed as the number of relevant semantic 
patterns ranked in the top n of the ranked list, 
divided by n. Table 2 presents the results for k=2. 
The results reveal that the relevance feedback 
can help the CIP induce more relevant semantic 
patterns. Another observation indicates that ap-
plying the relevance feedback for more iterations  
 RF(?) RF(5) RF(10) RF(20)
prec@30 0.203 0.263 0.318 0.387
Table 2. Effect of applying relevance feedback 
for different number of iterations or not. 
950
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0 5 10 15 20 25 30 35 40 45 50
Num. of Iterations
pr
ec
@
30
RF(10)+pseudo
RF(20)
RF(?)
 
Figure 4. Effect of using the combination of rele-
vance feedback and pseudo-relevance feedback. 
can further improve the precision. However, it is 
usually impractical for experts to involve in the 
guiding process for too many iterations. Conse-
quently, we further consider pseudo-relevance 
feedback to automate the guiding process. The 
pseudo-relevance feedback carries out the rele-
vance judgment based on the assumption that the 
top ranked semantic patterns are more likely to 
be the relevant ones. Thus, this approach usually 
relies on setting a threshold or selecting only the 
top n semantic patterns to form the relevant set. 
However, determining the threshold is not trivial, 
and the threshold may be different with different 
seed patterns. Therefore, we apply the pseudo-
relevance feedback only after certain expert-
guided iterations, rather than applying it 
throughout the induction process. The notion is 
that we can get a more reliable threshold value 
by observing the behavior of the relevant seman-
tic patterns in the ranked list for a few iterations. 
 To further examine the effectiveness of the 
combined approach, we additionally construct a 
CIP variant, RF(10)+pseudo, by applying the 
pseudo-relevance feedback after 10 expert-
guided iterations. The threshold is determined by 
the physicians during their judgments in the 10-
th iteration. The results are presented in Figure 4. 
The precision of RF(10)+pseudo is inferior to 
that of RF(20) before the 25-th iteration. Mean-
while, after the 30-th iteration, RF(10)+pseudo 
achieves higher precision than the other methods. 
This indicates that the pseudo-relevance feed-
back can also contribute to semantic pattern in-
duction in the stage without expert intervention.  
5.2 Coverage on Real Data 
The final results of the semantic patterns are the 
relevant sets of the last iteration produced by 
RF(10)+pseudo, denoted as CIPSP . Parts of them 
are shown in Table 3.  
Seed 
Pattern < boyfriend, argue > 
Induced 
Patterns <girlfriend, break up>; <friend, fight>
Table 3. Parts of induced semantic patterns. 
We compare CIPSP    to    those    created    by   a   
corpus-based approach. The corpus-based ap-
proach relies on an annotated domain corpus and 
a learning mechanism to induce the semantic 
patterns. Thus, we collected 300 consultation 
records from the PsychPark as the domain corpus, 
and each sentence in the corpus is annotated with 
a negative life event or not by the three physi-
cians. After the annotation process, the sentences 
with negative life events are together to form the 
training set. Then, we adopt Mutual Information 
(Manning and Sch?tze, 1999) to learn variable-
length semantic patterns. The mutual information 
between k words is defined as 
1
1 1
1
( ,..., )
( ,..., ) ( ,..., ) log
( )
k
k k k
i
i
P w w
MI w w P w w
P w
=
=
?
    (16) 
where 1( ,... )kP w w  is the probability of the k 
words co-occurring in a sentence in the training 
set, and ( )iP w  is the probability of a single word 
occurring in the training set. Higher mutual in-
formation indicates that the k words are more 
likely to form a semantic pattern of length k. 
Here the length k also ranges from 2 to 4. For 
each k, we compute the mutual information for 
all possible combinations of words in the training 
set, and those with their mutual information 
above a threshold are selected to be the final re-
sults of the semantic patterns, denoted as MISP . 
In order to obtain reliable mutual information 
values, only words with at least the minimum 
number of occurrences (>5) are considered. 
To examine the coverage of CIPSP  and MISP  on 
real data, 15 human subjects are involved in cre-
ating a test set. The subjects provide their experi-
enced negative life events in the form of natural 
language sentences. A total of 69 sentences are 
collected to be the test set, of which 39 sentences 
contain a semantic pattern of length two, 21 sen-
tences contain a semantic pattern of length three, 
and 9 sentences contain a semantic pattern of 
length four. The evaluation metric used is out-of-
pattern (OOP) rate, a ratio of unseen patterns 
occurring in the test set. Thus, the OOP can be 
defined as the number of test sentences contain-
ing the semantic patterns not occurring in the 
training set, divided by the total number of sen-
tences in the test set. Table 4 presents the results. 
951
 k=2 k=3 k=4 
CIPSP  0.36 (14/39) 0.48 (10/21) 0.44 (4/9)
MISP  0.51 (20/39) 0.62 (13/21) 0.67 (6/9)
Table 4. OOP rate of the CIP and a corpus-based 
approach. 
The results show that the OOP of MISP  is 
higher than that of CIPSP . The main reason is the 
lack of a large enough domain corpus with anno-
tated life events. In this circumstance, many se-
mantic patterns, especially for those with a larger 
length, could not be learned, because the number 
of their occurrences would be very rare in the 
training set. With no doubt, one could collect a 
large amount of domain corpus to reduce the 
OOP rate. However, increasing the amount of 
domain corpus also increases the amount of an-
notation and computation complexity. Our ap-
proach, instead, exploits the quality concepts to 
reduce the search space, also applies the rele-
vance feedback to guide the induction process, 
thus it can achieve better results with time-
limited constraints. 
6 Conclusion 
This study has presented an HAL-based cascaded 
model for variable-length semantic pattern in-
duction. The HAL model provides an informa-
tive infrastructure for the CIP to induce semantic 
patterns from the unannotated psychiatry web 
corpora. Using the quality concepts and preserv-
ing the better results from the previous stage, the 
search space can be reduced to speed up the in-
duction process. In addition, combining the rele-
vance feedback and pseudo-relevance feedback, 
the induction process can be guided to induce 
more relevant semantic patterns. The experimen-
tal results demonstrated that our approach can 
not only reduce the reliance on annotated corpora 
but also obtain acceptable results with time-
limited constraints. Future work will be devoted 
to investigating the detection of negative life 
events using the induced patterns so as to make 
the psychiatric services more effective. 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. Addison-Wesley, Reading, 
MA. 
Y. M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu. 2001. 
Virtual Psychiatric Clinics. American Journal of 
Psychiatry, 158(7):1160-1161.  
J. Bai, D. Song, P. Bruza, J. Y. Nie, and G. Cao. 2005. 
Query Expansion Using Term Relationships in 
Language Models for Information Retrieval. In 
Proc. of the 14th ACM International Conference 
on Information and Knowledge Management, 
pages 688-695. 
E. M. Brostedt and N. L. Pedersen. 2003. Stressful 
Life Events and Affective Illness. Acta Psychiat-
rica Scandinavica, 107:208-215. 
C. Burgess, K. Livesay, and K. Lund. 1998. Explora-
tions in Context Space: Words, Sentences, Dis-
course. Discourse Processes. 25(2&3):211-257. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 
T. Grenager, D. Klein, and C. D. Manning. 2005. Un-
supervised Learning of Field Segmentation Models 
for Information Extraction. In Proc. of the 43th 
Annual Meeting of the ACL, pages 371-378. 
T. Hasegawa, S. Sekine, R. Grishman. 2004. Discov-
ering Relations among Named Entities from Large 
Corpora. In Proc. of the 42th Annual Meeting of 
the ACL,  pages 415-422. 
W.Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. 
Riloff, and S. Soderland. 1992. University of Mas-
sachusetts: Description of the CIRCUS System 
used for MUC-4. In Proc. of the Fourth Message 
Understanding Conference, pages 282-288. 
C. Manning and H. Sch?tze. 1999. Foundations of 
Statistical Natural Language Processing. MIT 
Press. Cambridge, MA. 
I. Muslea. 1999. Extraction Patterns for Information 
Extraction Tasks: A Survey. In Proc. of the AAAI-
99 Workshop on Machine Learning for Information 
Extraction, pages 1-6. 
M. E. Pagano, A. E. Skodol, R. L. Stout, M. T. Shea, 
S. Yen, C. M. Grilo, C.A. Sanislow, D. S. Bender, 
T. H. McGlashan, M. C. Zanarini, and J. G. Gun-
derson. 2004. Stressful Life Events as Predictors of 
Functioning: Findings from the Collaborative Lon-
gitudinal Personality Disorders Study. Acta Psy-
chiatrica Scandinavica, 110:421-429. 
M. Stevenson and M. A. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proc. of 
the 43th Annual Meeting of the ACL, pages 379-
386. 
C. H. Wu, L. C. Yu, and F. L. Jang. 2005. Using Se-
mantic Dependencies to Mine Depressive Symp-
toms from Consultation Records. IEEE Intelligent 
System, 20(6):50-58.  
J. F. Yeh, C. H. Wu, M. J. Chen, and L. C. Yu. 2004. 
Automated Alignment and Extraction of Bilingual 
Domain Ontology for Cross-Language Domain-
Specific Applications. In Proc. of the 20th COL-
ING, pages 1140-1146. 
952
Automated Alignment and Extraction of Bilingual Domain Ontology for 
Medical Domain Web Search 
Jui-Feng Yeh*?, Chung-Hsien Wu*, Ming-Jun Chen* and Liang-Chih Yu* 
*Department of Computer Science and Information Engineering 
National Cheng Kung University, Taiwan, R.O.C. 
{chwu, jfyeh, mjchen,lcyu}@csie.ncku.edu.tw 
?Department of Computer Application Engineering 
Far East College, Taiwan, R.O.C. 
 
Abstract 
This paper proposes an approach to automated 
ontology alignment and domain ontology 
extraction from two knowledge bases. First, 
WordNet and HowNet knowledge bases are 
aligned to construct a bilingual universal 
ontology based on the co-occurrence of the 
words in a parallel corpus. The bilingual 
universal ontology has the merit that it 
contains more structural and semantic 
information coverage from two 
complementary knowledge bases, WordNet 
and HowNet. For domain-specific applications, 
a medical domain ontology is further extracted 
from the universal ontology using the island-
driven algorithm and a medical domain corpus.  
Finally, the domain-dependent terms and some 
axioms between medical terms based on a 
medical encyclopaedia are added into the 
ontology. For ontology evaluation, 
experiments on web search were conducted 
using the constructed ontology. The 
experimental results show that the proposed 
approach can automatically align and extract 
the domain-specific ontology. In addition, the 
extracted ontology also shows its promising 
ability for medical web search. 
1 Introduction 
In intelligent mining, in order to obviate the 
unnecessary keyword expansion, some knowledge 
base should be involved in the intelligent 
information system. In recent years, considerable 
progress has been invested in developing the 
conceptual bases for building technology that 
allows knowledge reuse and sharing. As 
information exchangeability and communication 
becomes increasingly global, multiple-language 
lexical resources that can provide transnational 
services are becoming increasingly important. 
Over the last few years, significant effort has been 
made to construct the ontology manually according 
to the domain expert?s knowledge. Manual 
ontology merging using conventional editing tools 
without intelligent support is difficult, labor 
intensive and error prone. Therefore, several 
systems and frameworks for supporting the 
knowledge engineer in the ontology merging task 
have recently been proposed.  To avoid the 
reiteration in ontology construction, the algorithm 
of ontology merge (UMLS 
http://umlsks.nlm.nih.gov/) (Langkilde and Knight 
1998) and ontology alignment (Vossen and Peters 
1997) (Weigard and Hoppenbrouwers 1998) 
(Asanoma 2001) were invested. The final ontology 
is a merged version of the original ontologies. The 
two original ontologies persist, with links 
established between them in alignment. Alignment 
usually is performed when the ontologies cover 
domains that are complementary to each other. In 
the past, domain ontology was usually constructed 
manually according to the knowledge or 
experience of the experts or ontology engineers. 
Recently, automatic and semi-automatic methods 
have been developed. OntoExtract (Fensel et al 
2002) (Missikoff et al 2002) provided an ontology 
engineering chain to construct the domain ontology 
from WordNet and SemCor. 
On the other hand, multi-lingual ontology is very 
important for natural language processing, such as 
machine translation (MT), web mining and cross 
language information retrieval (CLIR). Generally, 
a multi-lingual ontology maps the keyword set of 
one language to another language, or compute the 
co-occurrence of the words among languages. In 
addition, a key merit for multilingual ontology is 
that it can increase the relation and structural 
information coverage by aligning two or more 
language-dependent ontologies with different 
semantic features.  
Nowadays large collections of information in 
various styles are available on the Internet. And 
finding desired information on the World Wide 
Web is becoming a critical issue. Some general-
purpose search engine like Google 
(http://www.google.com) and Altavista 
(http://www.altavista.com/) provide the facility to 
mine the web. There are three major research areas 
about web mining: web content mining, web 
structure mining and web usage mining. This paper 
proposes a novel method to web content mining 
with unstructured web pages. There are many 
approaches in the view of natural language 
processing. According to the representation of web 
pages, there are three kinds of the content: bag of 
words (with order or not) (Kargupta et al 1997) 
(Nahm and Mooney, 2000), phrases (Ahonen et al 
1998) (Frank et al 1999)(Yang et al 1999), 
relational terms (Cohen, 1998) (Junker 1999) and  
concept categories. We proposed an ontology-
based web search approach. Unfortunately, there 
are some irrelevant pages obtained and these pages 
result in low precision rate and recall rate due to 
the problem of polysemy. To solve this problem, 
domain knowledge becomes necessary. The 
domain-specific web miners like SPIRAL, Cora 
(Cohen, 1998), WebKB (Martin and  Eklund 2000) 
and HelpfulMed (Chen et al 2003) are employed 
as the special search engine for the interesting 
topic. These ones dedicated to recipes are less 
likely to return irrelevant web pages when the 
query is entered.  
In this paper, WordNet and HowNet knowledge 
bases are aligned to construct a bilingual universal 
ontology based on the co-occurrence of the words 
in a parallel corpus. For domain-specific 
applications, a medical domain ontology is further 
extracted from the universal ontology using the 
island-driven algorithm (Lee et al 1995) and a 
medical domain corpus.  Finally, the axioms 
between medical terms are derived based on 
semantic relations. A web search system for 
medical domain based on the extracted domain 
ontology is realized to demonstrate the feasibility 
of the methods proposed in this paper. 
The rest of the paper is organized as follows. 
Section 2 describes ontology construction process 
and the web searching system framework. Section 
3 presents the experimental results for the 
evaluation of our approach. Section 4 gives some 
concluding remarks. 
2 Methodologies 
Figure 1 shows the block diagram for ontology 
construction and the framework of the domain-
specific web search system. There are four major 
processes in the proposed system: bilingual 
ontology alignment, domain ontology extraction, 
knowledge representation and domain-specific web 
search. 
2.1 Bilingual Ontology Alignment 
In this approach, the cross-lingual ontology is 
constructed by aligning the words in WordNet with 
their corresponding words in HowNet. First, the 
Sinorama (Sinorama 2001) database is adopted as 
the bilingual language parallel corpus to compute 
the conditional probability of the words in 
WordNet, given the words in HowNet. Second, a 
bottom up algorithm is used for relation mapping. 
 
Figure 1 Ontology construction framework and the domain-specific web search system 
In WordNet a word may be associated with many 
synsets, each corresponding to a different sense of 
the word. When we look for a relation between two 
different words we consider all the synsets 
associated with each word (Christiane 1998). In 
HowNet, each word is composed of primary 
features and secondary features. The primary 
features indicate the word?s category. The purpose 
of this approach is to increase the relation and 
structural information coverage by aligning the 
above two language-dependent ontologies, 
WordNet and HowNet, with different semantic 
features. 
The relation ?is-a? defined in WordNet 
corresponds to the primary feature defined in 
HowNet. Equation (1) shows the mapping between 
the words in HowNet and the synsets in WordNet. 
Given a Chinese word, iCW  , the probability of the 
word related to synset, ksynset  , can be obtained 
via its corresponding English synonyms, 
,  1,...,kjEW j m=  , which are the elements in 
ksynset  . The probability is estimated as follows.  
 
1
1
Pr( | )
Pr( , | )
(Pr( | , ) Pr( | ))
k
i
m k k
j i
j
m k k k
j i j i
j
synset CW
synset EW CW
synset EW CW EW CW
=
=
= ?
= ??
     (1) 
where         
( ) ( )( )
, ,
Pr | ,
, ,
k k
j j ik k
j i
l k
j j i
l
N synset EW CW
synset EW CW
N synset EW CW
=
?
(2) 
In the above equation, ( ), ,k kj j iN synset EW CW   
represents the number of co-occurrences of iCW  , 
k
jEW  and kjsynset . The probability ( )Pr |kj iEW CW   is 
set to one when at least one of the primary features, 
( )li iPF CW , of the Chinese word defined in the 
HowNet matches one of the ancestor nodes of 
synset , ( )kj jsynset EW  except the root nodes in 
the hierarchical structures of the noun and verb; 
Otherwise set to zero. That is, 
 
( )
( )( )( )
Pr |
{ , , , }
1     
( ( )) { , , , }
0
j i
l
i i
l
k
j j
k
EW CW
PF CW entity event act play
if
ancestor synset EW entity event act play
otherwise
? ???= ? ? ? ????
?
? ? ?
                                                                             (3) 
where {enitity,event,act,play} is the concept set in 
the root nodes of HowNet and WordNet. 
Finally, the Chinese concept, 
iCW  , has been 
integrated into the synset , k
jsynset   , in WordNet as 
long as the probability, Pr k i(synset |CW )  , is not 
zero. Figure 2 shows the concept tree generated by 
aligning WordNet and HowNet. 
 
 
 
Figure 2. Concept tree generated by aligning 
WordNet and HowNet. The nodes with bold circle 
represent the operative nodes after concept 
extraction. The nodes with gray background 
represent the operative nodes after relation 
expansion. 
2.2 Domain ontology extraction 
There are two phases to construct the domain 
ontology: 1) extract the ontology from the cross-
language ontology by island-driven algorithm, and 
2) integrate the terms and axioms defined in a 
medical encyclopaedia into the domain ontology.  
2.2.1 Extraction by island-driven algorithm 
Ontology provides consistent concepts and world 
representations necessary for clear communication 
within the knowledge domain. Even in domain-
specific applications, the number of words can be 
expected to be numerous. Synonym pruning is an 
effective alternative to word sense disambiguation. 
This paper proposes a corpus-based statistical 
approach to extracting the domain ontology. The 
steps are listed as follows: 
Step 1: Linearization: This step decomposed the 
tree structure in the universal ontology shown in 
Figure 2 into the vertex list that is an ordered node 
sequence starting at the leaf nodes and ending at the 
root node.  
Step 2: Concept extraction from the corpus: The 
node is defined as an operative node when the Tf-
idf value of word iW   in the domain corpus is 
higher than that in its corresponding contrastive 
(out-of-domain) corpus. That is, 
_ ( )
1,     ( ) ( )
0,   
i
Domain i Contrastive i
operative node W
if Tf idf W Tf idf W
Otherwise
? > ??= ??
 (4) 
where 
, ,
,
,
, ,
,
,
( ) log
( ) log
i Domain i Contrastive
Domain i i Domain
i Domain
i Domain i Contrastive
Contrastive i i Contrastive
i Contrastive
n n
Tf idf W freq
n
n n
Tf idf W freq
n
+? = ?
+? = ?
 
In the above equations, Domainifreq ,   and 
eContrastivifreq ,   are the frequencies of word iW   in 
the domain documents and its contrastive (out-of-
domain) documents, respectively. Domainin ,   and  
,i Contrastiven  are the numbers of the documents 
containing word iW   in the domain documents and 
its contrastive documents, respectively. The nodes 
with bold circle in Figure 2 represent the operative 
nodes.  
Step 3: Relational expansion using the island-
driven algorithm: There are some domain concepts 
not operative after the previous steps due to the 
problem of insufficient data. From the observation 
in ontology construction, most of the inoperative 
concept nodes have operative hypernym nodes and 
hyponym nodes. Therefore, the island-driven 
algorithm is adopted to activate these inoperative 
concept nodes if their ancestors and descendants are 
all operative. The nodes with gray background 
shown in Figure 2 are the activated operative nodes.  
Step 4: Domain ontology extraction: The final 
step is to merge the linear vertex list sequence into 
a hierarchical tree. However, some noisy concepts 
not belonging to this domain ontology are operative 
after step 3. These noisy nodes with inoperative 
noisy concepts should be filtered out automatically. 
Finally, the domain ontology is extracted and the 
final result is shown in Figure 3.  
After the above steps, a dummy node is added as 
the root node of the domain concept tree. 
 
 
Figure 3 The domain ontology after filtering out the 
isolated concepts 
2.2.2 Axiom and terminology integration 
In practice, specific domain terminologies and 
axioms should be derived and introduced into the 
ontology for domain-specific applications. In our 
approach, 1213 axioms derived from a medical 
encyclopaedia have been integrated into the domain 
ontology. Figure 4 shows an example of the axiom. 
In this example, the disease ?diabetes? is tagged as 
level ?A? which represents that this disease is 
frequent in occurrence. And the degrees for the 
corresponding syndromes represent the causality 
between the disease and the syndromes. The axioms 
also provide two fields ?department of the clinical 
care? and ?the category of the disease? for medical 
information retrieval. 
 
 
 
Figure 4   axiom example 
2.3 Domain-specific web search  
This paper proposed a medical web search engine 
based on the constructed medical domain ontology. 
The engine consists of natural language interface, 
web crawler and indexer, relation inference module 
and axiom inference module. The functions and 
techniques of these modules are described as 
follows. 
2.3.1 Natural language interface and web 
crawler and indexer 
Natural language interface is generally 
considered as an enticing prospect because it offers 
many advantages: it would be easy to learn and 
easy to remember, because its structure and 
vocabulary are already familiar to the user; it is 
particularly powerful because of the multitude of 
ways in which to accomplish a search action by 
using the natural language input. A natural 
language query is transformed to obtain the desired 
representation after the word segmentation, 
removing the stop words, stemming and tagging 
process. 
The web crawler and indexer are designed to seek 
medical web pages from Internet, extract the 
content and establish the indices automatically. 
 
 
2.3.2 Concept inference module 
For semantic representation, traditionally, the 
keyword-based systems will introduce two 
problems. First, ambiguity usually results from the 
polysemy of words. The domain ontology will give 
a clear description of the concepts. In addition, not 
all the synonyms of the word should be expanded 
without constraints. Secondly, relations between the 
concepts should be expanded and weighted in order 
to include more semantic information for semantic 
inference. We treat each of the user?s input and the 
content of web pages as a sequence of words. This 
means that the sequence of words is treated as a bag 
of words regardless of the word order. For the word 
sequence of the user?s input,  
q=Wq=wq1, wq2 ,?, wqK ,  
and the word sequence of the web page,                   
A=WA=wA1, wA2, ?, wAL,  
The similarity between input query and the page is 
defined as the similarity between the two bags of 
words. The similarity measure based on key 
concepts in the ontology is defined as follows. 
( )
( )
1 2 1 2
,
1, 1
,
       ,
       ( , ,..., , , ,..., )
       
i
i i i
relation i
relation A q
relation A A A L q q qK
K L
kl
k l
Sim A q
Sim W W
Sim w w w w w w
H
= =
=
=
= ?
                                                                  (5) 
where Hkl  is concept similarity of wAl and wqk. Most 
of the keyword expansion approaches use the 
extension of scope by the synonyms. In this paper 
the similarity, Hkl, is defined as 
, 
2 , 
1  and  are identical
 and  are hypernyms1
 is the number of levels in between2
 and  are synonyms11
 is the number of their common concepts 2
0 others
Al qk
Al qk
r
kl
Al qk
r
w w
w w
r
H
w w
r
?????= ??? ??? ??? ????
(6) 
2.3.3 Axiom inference module 
Some axioms, such as ?result in? and ?result 
from,? that are expected to affect the performance 
of a web search system in a technical domain are 
defined to describe the relationship between 
syndromes and diseases. This aspect is the use of 
specific terms used in the medical domain. We 
collected the data about syndromes and diseases 
from a medical encyclopedia and tagged the 
diseases with three levels according to its 
occurrence and syndromes with four levels 
according to its significance to the specific disease. 
The ?result in? relation score is defined as ( , )iRI A q  
if a disease occurs in the input query and its 
corresponding syndromes appear in the web page. 
Similarly, if syndrome occurs in the input query and 
its corresponding disease appears in the web page, 
the ?result from? relation score is defined as 
( , )iRF A q . The relation score is estimated as follows. 
( )
1 2 1 2
1 2 1 2
, ,
1, 1 1, 1
, max{ ( , ), ( , )}
max{ ( , ,..., , , ,..., ),
        ( , ,..., , , ,..., )}
max{ , },
i i i
i i i
i i i
A A A P q q qR
A A A P q q qR
P R P RRI RF
pr pr
p r p r
Axiom A q RI A q RF A q
RI w w w w w w
RF w w w w w w
d d
= = = =
=
=
= ? ?
                                                                   (7) 
where 11/ 2RI nprd
?=  if disease Apw  results in 
syndrome qrw  and qrw  is the top-n feature of Apw . 
Similarly, 11/ 2RF nprd
?=  if syndrome Apw  results 
from disease qrw  and Apw  is the top-n feature of 
qrw . The conditional probability of the i-th web 
pages with respect to aspect ,2As  and query q is 
defined as  
( ) ( )( )
,
,
,
i
axiom i
i
i
Axiom A q
Sim A q
Axiom A q
= ?
. 
3 Evaluation 
To evaluate the proposed approach, a medical 
web search system was constructed. The web pages 
were collected from several Websites and totally 
2322 web pages for medical domain and 8133 web 
pages for contrastive domain were collected. 
On the other hand, the training and test queries 
for training and evaluating the system performance 
were also collected. Forty users, who do not take 
part in the system development, were asked to 
provide a set of queries given the collected web 
pages. After post-processing, the duplicate queries 
and the queries out of the medical domain are 
removed. Finally, 3207 test queries mixed Chinese 
with English words using natural language were 
obtained.  
3.1 Keyword-Based VSM Approach: A 
baseline system for comparison 
In recent years, most of the information retrieval 
approaches were based on the Vector-Space Model 
(VSM). Assuming that the query is denoted as a 
vector 1 2q ( , ,..., )nq q q=  and the web page is 
represented as a vector 1 2( , ,..., )nA a a a= . The 
Tf-idf measure is employed and the similarity can 
be measured by the cosine function defined as 
follows.  
1
2 2
1 1
( , ) cos(a,q)
n
i ii
keyword based i n n
i ii i
a q
Sim A q
a q
=
?
= =
= =
?
?
? ?
 
(8) 
where a 1= . This approach for key term 
expansion based on synonym set is also adopted in 
the baseline system. The results and discussions are 
described in the following sections. 
3.2 Weight determination using 11-avgP score 
The medical domain web search system is 
modeled by the linear combination of relational 
inference model and axiom inference model. The 
normalized weight factor, ? , is employed for 
concept expansion as follows. 
( , ) (1 ) ( , ) ( , )i relation i axiom iSim A q Sim A q Sim A q? ?= ? + ?  (9) 
This experiment is conducted on the estimation of 
the combination weights for each model. The 
results are shown in Figure 5. The performance 
measure called 11-AvgP [Eichmann and Srinivasan 
1998] was used to summarize the precision and 
recall rates. The best 11-AvgP score will be 
obtained when the weight 0.428? = .  
 
0.54
0.58
0.62
0.66
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
?
11
-a
vg
P
 s
co
re
 
Figure 5 The 11-avgP score with different values of ?  
3.3 Evaluation on different inference modules 
In the following experiments, web pages were 
separately evaluated by focusing on one inference 
module based on the domain-specific ontology at a 
time. That is, the mixture weight is set to 1 for one 
inference module and the other is set to 0 in each 
evaluation. For comparison, the keyword-based 
VSM approach and the ontology-based system are 
also evaluated and shown in Figure 6. The precision 
and recall rates are used as the evaluation measures. 
And the ontology based approach means the 
combination of concept inference and axiom 
inference described in the section 3.2. 
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100
Recall rate (%)
P
re
ci
si
on
 ra
te
 (%
)
Ontology based
Baseline+concept infernece
module
Baseline+axiom inference
module
Baseline
Figure 6 The precision rates and recall rates of the 
proposed method and  the baseline system 
4 Conclusion 
This paper has presented an approach to 
automated ontology alignment and domain 
ontology extraction from two knowledge bases. In 
this approach, a bilingual ontology is developed 
using a corpus-based statistical approach from two 
well established language-dependent knowledge 
bases, WordNet and HowNet. A domain-dependent 
ontology is further extracted from the universal 
ontology using the island-driven algorithm and a 
domain corpus. In addition, domain-specific terms 
and axioms are also added into the domain ontology. 
We have applied the domain-specific ontology to 
the web page search in medical domain. The 
experimental results show that the proposed 
approach outperformed the keyword-based  and  
synonym expansion approaches. 
References  
N. Asanoma. 2001 Alignment of Ontologies: 
WordNet and Goi-Taikei. WordNet and Other 
Lexical Resources Workshop Program, 
NAACL2001. 89-94 
Christiane Fellbaum, 1998 WordNet an electronic 
Lexical Database, The MIT Press 1998. pp307-
308 
Fensel, D., Bussler, C., Ding, Y., Kartseva1, V., 
Klein, M., Korotkiy, M., Omelayenko, B. and 
Siebes R. 2002 Semantic Web Application Areas, 
the 7th International Workshop on Applications 
of Natural Language to Information Systems 
(NLDB02). 
M. Missikoff,, R. Navigli, and P. Velardi. 2002 
Integrated approach to Web ontology learning 
and engineering, Computer , Volume: 35 Issue: 
11 . 60 ?63 
N.F. Noy, and M. Musen,. 2000 PROMPT: 
Algorithm and Tool for Automated Ontology 
Merging and Alignment, Proceedings of the 
National Conference on Artificial Intelligence. 
AAAI2000. 450-455 
Sinorama Magazine and Wordpedia.com Co. 2001 
Multimedia CD-ROMs of Sinorama from 1976 to 
2000, Taipei. 
P. Vossen, and W. Peters, 1997 Multilingual design 
of EuroWordNet, Proceedings of the Delos 
workshop on Cross-language Information 
Retrieval. 
H. Weigard, and S. Hoppenbrouwers, 1998, 
Experiences with a multilingual ontology-based 
lexicon for news filtering, Proceedings in the 9th 
International Workshop on Database and Expert 
Systems Applications. 160-165 
H. Kargupta, I. Hamzaoglu, and B. Stafford. 1997. 
Distributed data mining using an agent based 
architecture. In Proceedings of Knowledge 
Discovery and Data Mining, 211-214.  
U. Y. Nahm and R. J. Mooney. 2000. A mutually 
beneficial integration of data mining and 
information extraction. In Proceeding of the 
AAAI-00.  
H. Ahonen, O. Heinoen, M. Klemettinen, and A. 
Verkamo. 1998. Applying data mining techniques 
for descriptive phrase extraction in digital 
document collections. In Advence in Digital 
Libraries. 
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, 
and C. G. Nevill-Manning. 1999. Domain-
specific keyphrase extraction. In proceding of 
IJCAI-99, 668-673. 
Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. 
Archibald, and X. Liu. 1999. Learning 
approaches for detecting and tracking news 
events. IEEE Intelligent Systems, 14(4):32-43.  
W. W. Cohen. 1998. A web-based information 
system that reasons with structured collocations 
of text. In Proceedings of 2nd Agent'98  
M. Junker, M. Sintek, and M. Rinck. 1999 Learning 
for text categorization and information extraction 
with ilp. In Proceedings of the Workshop on 
Learning Language in Logic, Bled, Slovenia 
S. Oyama, T. Kokubo, and T. Ishida. 2004 Domain-
Specific Web Search with Keyword Spice. IEEE 
Transactions on Knowledge and Data 
Engineering, Vol 16,NO. 1, 17-27.  
Sankar K. Pal, Varun Talwar, and Pabitra Mitra. 
2002. Web Minging in Soft Computing 
Framework: Relevance, State of the Art and 
Future Directions. IEEE Transactions on Neural 
Networks, Vol. 13, NO. 5.  
T. Hofmann. 1999. The cluster-abstraction model: 
Unsupervised learning of topic hierarchies from 
text data. In Proceedings of 16th IJCAI, 682-687,  
P. Martin and P. Eklund. 2000. Knowledge 
Indexation and Retrieval and the Word Wide 
Web. IEEE Intelligent Systems, special issue 
"Knowledge Management and Knowledge 
Distribution over the Internet" 
H. Chen, A. M. Lally, B. Zhu, and M. Chau. ,2003 
HelpfulMed: Intelligent Searching for Medical 
Information over the internet. Journal od the 
American Society for Information Science and 
Technology, 54(7):683-694. 
D. Eichmann, , Ruiz, M., Srinivasan, P., 1998 
Cross-language information retrieval with the 
UMLS Metathesaurus, Proceeding of ACM 
Special Interest Group on Information Retreival 
(SIGIR), ACM Press, NY (1998), 72-80. 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024?1031,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Topic Analysis for Psychiatric Document Retrieval 
Liang-Chih Yu*?, Chung-Hsien Wu*, Chin-Yew Lin?, Eduard Hovy? and Chia-Ling Lin*
*Department of CSIE, National Cheng Kung University, Tainan, Taiwan, R.O.C. 
?Microsoft Research Asia, Beijing, China 
?Information Sciences Institute, University of Southern California, Marina del Rey, CA, USA 
liangchi@isi.edu,{chwu,totalwhite}@csie.ncku.edu.tw,cyl@microsoft.com,hovy@isi.edu 
 
 
Abstract 
Psychiatric document retrieval attempts to 
help people to efficiently and effectively 
locate the consultation documents relevant 
to their depressive problems. Individuals 
can understand how to alleviate their symp-
toms according to recommendations in the 
relevant documents. This work proposes 
the use of high-level topic information ex-
tracted from consultation documents to im-
prove the precision of retrieval results. The 
topic information adopted herein includes 
negative life events, depressive symptoms 
and semantic relations between symptoms, 
which are beneficial for better understand-
ing of users' queries. Experimental results 
show that the proposed approach achieves 
higher precision than the word-based re-
trieval models, namely the vector space 
model (VSM) and Okapi model, adopting 
word-level information alone. 
1 Introduction 
Individuals may suffer from negative or stressful 
life events, such as death of a family member, ar-
gument with a spouse and loss of a job. Such 
events play an important role in triggering depres-
sive symptoms, such as depressed moods, suicide 
attempts and anxiety. Individuals under these cir-
cumstances can consult health professionals using 
message boards and other services. Health profes-
sionals respond with suggestions as soon as possi-
ble. However, the response time is generally sev-
eral days, depending on both the processing time 
required by health professionals and the number of 
problems to be processed. Such a long response 
time is unacceptable, especially for patients suffer-
ing from psychiatric emergencies such as suicide 
attempts. A potential solution considers the prob-
lems that have been processed and the correspond-
ing suggestions, called consultation documents, as 
the psychiatry web resources. These resources gen-
erally contain thousands of consultation documents 
(problem-response pairs), making them a useful 
information resource for mental health care and 
prevention. By referring to the relevant documents, 
individuals can become aware that they are not 
alone because many people have suffered from the 
same or similar problems. Additionally, they can 
understand how to alleviate their symptoms ac-
cording to recommendations. However, browsing 
and searching all consultation documents to iden-
tify the relevant documents is time consuming and 
tends to become overwhelming. Individuals need 
to be able to retrieve the relevant consultation 
documents efficiently and effectively. Therefore, 
this work presents a novel mechanism to automati-
cally retrieve the relevant consultation documents 
with respect to users' problems. 
Traditional information retrieval systems repre-
sent queries and documents using a bag-of-words 
approach. Retrieval models, such as the vector 
space model (VSM) (Baeza-Yates and Ribeiro-
Neto, 1999) and Okapi model (Robertson et al, 
1995; Robertson et al, 1996; Okabe et al, 2005), 
are then adopted to estimate the relevance between 
queries and documents. The VSM represents each 
query and document as a vector of words, and 
adopts the cosine measure to estimate their rele-
vance. The Okapi model, which has been used on 
the Text REtrieval Conference (TREC) collections, 
developed a family of word-weighting functions 
1024
for relevance estimation. These functions consider 
word frequencies and document lengths for word 
weighting. Both the VSM and Okapi models esti-
mate the relevance by matching the words in a 
query with the words in a document. Additionally, 
query words can further be expanded by the con-
cept hierarchy within general-purpose ontologies 
such as WordNet (Fellbaum, 1998), or automati-
cally constructed ontologies (Yeh et al, 2004). 
However, such word-based approaches only 
consider the word-level information in queries and 
documents, ignoring the high-level topic informa-
tion that can help improve understanding of users' 
queries. Consider the example consultation docu-
ment in Figure 1. A consultation document com-
prises two parts: the query part and recommenda-
tion part. The query part is a natural language text, 
containing rich topic information related to users' 
depressive problems. The topic information in-
cludes negative life events, depressive symptoms, 
and semantic relations between symptoms. As in-
dicated in Figure 1, the subject suffered from a 
love-related event, and several depressive symp-
toms, such as <Depressed>, <Suicide>, <Insom-
nia> and <Anxiety>. Moreover, there is a cause-
effect relation holding between <Depressed> and 
<Suicide>, and a temporal relation holding be-
tween <Depressed> and <Insomnia>. Different 
topics may lead to different suggestions decided by 
experts. Therefore, an ideal retrieval system for 
consultation documents should consider such topic 
information so as to improve the retrieval precision. 
Natural language processing (NLP) techniques 
can be used to extract more precise information 
from natural language texts (Wu et al, 2005a; Wu 
et al, 2005b; Wu et al, 2006; Yu et al, 2007). 
This work adopts the methodology presented in 
(Wu et al 2005a) to extract depressive symptoms 
and their relations, and adopts the pattern-based 
method presented in (Yu et al, 2007) to extract 
negative life events from both queries and consul-
tation documents. This work also proposes a re-
trieval model to calculate the similarity between a 
query and a document by combining the similari-
ties of the extracted topic information. 
The rest of this work is organized as follows. 
Section 2 briefly describes the extraction of topic 
information. Section 3 presents the retrieval model. 
Section 4 summarizes the experimental results. 
Conclusions are finally drawn in Section 5. 
2 Framework of Consultation Document 
Retrieval 
Figure 2 shows the framework of consultation 
document retrieval. The retrieval process begins 
with receiving a user?s query about his depressive 
problems in natural language. The example query 
is shown in Figure 1. The topic information is then 
extracted from the query, as shown in the center of 
Figure 2. The extracted topic information is repre-
Consultation DocumentQuery:
It's normal to feel this way when going through these kinds of struggles, but over 
time your emotions should level out. Suicide doesn't solve anything; think about 
how it would affect your family........ There are a few things you can try to help 
you get to sleep at night, like drinking warm milk, listening to relaxing music....... 
Recommendation:
After that, it took me a long time to fall asleep at night.  
<Depressed>
<Suicide>
<Insomnia>
<Anxiety>
cause-effect temporal
I broke up with my boyfriend.
I often felt like crying and felt pain every day. 
So, I tried to kill myself several times. 
In recent months, I often lose my temper for no reason.
 
Figure 1.  Example of a consultation document. The bold arrowed lines denote cause-effect relations; ar-
rowed lines denote temporal relations; dashed lines denote temporal boundaries, and angle brackets de-
note depressive symptoms 
1025
sented by the sets of negative life events, depres-
sive symptoms, and semantic relations. Each ele-
ment in the event set and symptom set denotes an 
individual event and symptom, respectively, while 
each element in the relation set denotes a symptom 
chain to retain the order of symptoms. Similarly, 
the query parts of consultation documents are rep-
resented in the same manner. The relevance esti-
mation then calculates the similarity between the 
input query and the query part of each consultation 
document by combining the similarities of the sets 
of events, symptoms, and relations within them. 
Finally, a list of consultation documents ranked in 
the descending order of similarities is returned to 
the user. 
In the following, the extraction of topic informa-
tion is described briefly. The detailed process is 
described in (Wu et al 2005a) for symptom and 
relation identification, and in (Yu et al, 2007) for 
event identification. 
1) Symptom identification: A total of 17 symp-
toms are defined based on the Hamilton De-
pression Rating Scale (HDRS) (Hamilton, 
1960). The identification of symptoms is sen-
tence-based. For each sentence, its structure is 
first analyzed by a probabilistic context free 
grammar (PCFG), built from the Sinica Tree-
bank corpus developed by Academia Sinica, 
Taiwan (http://treebank.sinica.edu.tw), to gen-
erate a set of dependencies between word to-
kens. Each dependency has the format (modi-
fier, head, relmodifier,head). For instance, the de-
pendency (matters, worry about, goal) means 
that "matters" is the goal to the head of the sen-
tence "worry about". Each sentence can then 
be associated with a symptom based on the 
probabilities that dependencies occur in all 
symptoms, which are obtained from a set of 
training sentences. 
2) Relation Identification: The semantic rela-
tions of interest include cause-effect and tem-
poral relations. After the symptoms are ob-
tained, the relations holding between symp-
toms (sentences) are identified by a set of dis-
course markers. For instance, the discourse 
markers "because" and "therefore" may signal 
cause-effect relations, and "before" and "after" 
may signal temporal relations. 
3) Negative life event identification: A total of 5 
types of events, namely <Family>, <Love>, 
<School>, <Work> and <Social> are defined 
based on Pagano et als (2004) research. The 
identification of events is a pattern-based ap-
proach. A pattern denotes a semantically plau-
sible combination of words, such as <parents, 
divorce> and <exam, fail>. First, a set of pat-
terns is acquired from psychiatry web corpora 
by using an evolutionary inference algorithm. 
The event of each sentence is then identified 
by using an SVM classifier with the acquired 
patterns as features. 
3 Retrieval Model 
The similarity between a query and a document, 
( , )Sim q d , is calculated by combining the similari-
ties of the sets of events, symptoms and relations 
within them, as shown in (1). 
Consultation
Documents
Ranking
Relevance
Estimation
Query
(Figure 1)
Topic Information
Symptom 
Identification
Negative Life Event
Identification
Relation
Identification
D S A
D S Cause-Effect
D I A
Temporal
I
S I A
<Love>
Topic Analysis
 
Figure 2.  Framework of consultation document retrieval. The rectangle denotes a negative life event re-
lated to love relation. Each circle denotes a symptom. D: Depressed, S: Suicide, I: Insomnia, A: Anxiety. 
1026
( , )
( , ) ( , ) (1 ) ( , ),Evn Sym Rel
Sim q d
Sim q d Sim q d Sim q d? ? ? ?
=
+ + ? ?   (1) 
where ( , )EvnSim q d , ( , )SymSim q d  and ( , )RelSim q d , 
denote the similarities of the sets of events, symp-
toms and relations, respectively, between a query 
and a document, and ?  and ? denote the combi-
nation factors. 
3.1 Similarity of events and symptoms 
The similarities of the sets of events and symptoms 
are calculated in the same method. The similarity 
of the event set (or symptom set) is calculated by 
comparing the events (or symptoms) in a query 
with those in a document. Additionally, only the 
events (or symptoms) with the same type are 
considered. The events (or symptoms) with 
different types are considered as irrelevant, i.e., no 
similarity. For instance, the event <Love> is 
considered as irrelevant to <Work>. The similarity 
of the event set is calculated by 
( , )
1
( , ) cos( , ) .,
( )
Evn
q d q d
q d e q d
Sim q d
Type e e e e const
N Evn Evn ? ?
= +? ?  (2) 
where qEvn  and dEvn  denote the event set in a 
query and a document, respectively; qe  and de  
denote the events; ( )q dN Evn Evn?  denotes the 
cardinality of the union of qEvn  and dEvn  as a 
normalization factor, and ( , )q dType e e  denotes an 
identity function to check whether two events have 
the same type, defined as 
1     ( ) ( )
( , ) .
0    otherwise
q d
q d
Type e Type e
Type e e
=??= ???
  (3) 
The cos( , )q de e  denotes the cosine angle between 
two vectors of words representing qe  and de , as 
shown below. 
( ) ( )
1
2 2
1 1
cos( , ) ,q d
q d
T i i
e ei
q d
T Ti i
e ei i
w w
e e
w w
=
= =
= ?
? ?
  (4) 
where w denotes a word in a vector, and T denotes 
the dimensionality of vectors. Accordingly, when 
two events have the same type, their similarity is 
given as cos( , )q de e  plus a constant, const.. Addi-
tionally, cos( , )q de e  and const. can be considered 
as the word-level and topic-level similarities, re-
spectively. The optimal setting of const. is deter-
mined empirically. 
3.2 Similarity of relations 
When calculating the similarity of relations, only 
the relations with the same type are considered. 
That is, the cause-effect (or temporal) relations in a 
query are only compared with the cause-effect (or 
temporal) relations in a document. Therefore, the 
similarity of relation sets can be calculated as 
,
1
( , ) ( , ) ( , ),
q d
Rel q d q d
r r
Sim q d Type r r Sim r r
Z
= ?  (5) 
( ) ( ) ( ) ( ),C q C d T q T dZ N r N r N r N r= +   (6) 
where qr and dr denote the relations in a query and 
a document, respectively; Z denotes the normaliza-
tion factor for the number of relations; ( , )q dType e e  
denotes an identity function similar to (3), and 
( )CN i   and ( )TN i  denote the numbers of cause-
effect and temporal relations. 
Both cause-effect and temporal relations are rep-
resented by symptom chains. Hence, the similarity 
of relations is measured by the similarity of symp-
tom chains. The main characteristic of a symptom 
chain is that it retains the cause-effect or temporal 
order of the symptoms within it. Therefore, the 
order of the symptoms must be considered when 
calculating the similarity of two symptom chains. 
Accordingly, a sequence kernel function (Lodhi et 
al., 2002; Cancedda et al, 2003) is adopted to cal-
culate the similarity of two symptom chains. A 
sequence kernel compares two sequences of sym-
bols (e.g., characters, words) based on the subse-
quences within them, but not individual symbols. 
Thereby, the order of the symptoms can be incor-
porated into the comparison process. 
The sequence kernel calculates the similarity of 
two symptom chains by comparing their sub-
symptom chains at different lengths. An increasing 
number of common sub-symptom chains indicates 
a greater similarity between two symptom chains. 
For instance, both the two symptom chains 
1 2 3 4s s s s  and 3 2 1s s s  contain the same symptoms 1s , 
2s  and 3s , but in different orders. To calculate the 
similarity between these two symptom chains, the 
sequence kernel first calculates their similarities at 
length 2 and 3, and then averages the similarities at 
the two lengths. To calculate the similarity at 
1027
length 2, the sequence kernel compares their sub-
symptom chains of length 2, i.e., 
1 2 1 3 1 4 2 3 2 4 3 4{ , , , , , }s s s s s s s s s s s s  and 3 2 3 1 2 1{ , , }s s s s s s . 
Similarly, their similarity at length 3 is calculated 
by comparing their sub-symptom chains of length 
3, i.e., 1 2 3 1 2 4 1 3 4 2 3 4{ ,  ,  ,  }s s s s s s s s s s s s  and 3 2 1{ }s s s . 
Obviously, no similarity exists between 1 2 3 4s s s s  
and 3 2 1s s s , since no sub-symptom chains are 
matched at both lengths. In this example, the sub-
symptom chains of length 1, i.e., individual symp-
toms, do not have to be compared because they 
contain no information about the order of symp-
toms. Additionally, the sub-symptom chains of 
length 4 do not have to be compared, because the 
two symptom chains share no sub-symptom chains 
at this length. Hence, for any two symptom chains, 
the length of the sub-symptom chains to be com-
pared ranges from two to the minimum length of 
the two symptom chains. The similarity of two 
symptom chains can be formally denoted as 
1 2
1 2
1 2
2
( , ) ( , )
                ( , )
1
                ( , ),
1
N N
q d q d
N N
q d
N
N N
n q d
n
Sim r r Sim sc sc
K sc sc
K sc sc
N =
?
=
= ? ?
  (7) 
where 1Nqsc  and 2
N
dsc  denote the symptom chains 
corresponding to qr  and dr , respectively; 1N  and 
2N  denote the length of 1
N
qsc  and 2
N
dsc , respec-
tively; (  ,   )K i i  denotes the sequence kernel for 
calculating the similarity between two symptom 
chains; (  ,   )nK i i  denotes the sequence kernel for 
calculating the similarity between two symptom 
chains at length n, and N is the minimum length of 
the two symptom chains, i.e., 1 2min( , )N N N= . 
The sequence kernel 1 2( , )N Nn i jK sc sc  is defined as 
21
1 2
1 2
1 2
1 1 2 2
( )( )
( , )   
( ) ( )
( ) ( )
         ,
( ) ( ) ( ) ( )
n
n n
NN
n jN N n i
n i j N N
n i n j
N N
u i u j
u SC
N N N N
u i u j u i u j
u SC u SC
scsc
K sc sc
sc sc
sc sc
sc sc sc sc
? ?
? ? ? ?
?
? ?
??=
? ?
=
?
? ?
i
(8) 
where 1 2( , )N Nn i jK sc sc  is the normalized inner 
product of vectors 1( )Nn isc?  and 2( )Nn jsc? ; ( )n? i  
denotes a mapping that transforms a given symp-
tom chain into a vector of the sub-symptom chains 
of length n; ( )u? i  denotes an element of the vector, 
representing the weight of a sub-symptom chain u , 
and nSC  denotes the set of all possible sub-
symptom chains of length n. The weight of a sub-
symptom chain, i.e., ( )u? i , is defined as 
1
1
1
1       is a contiguous sub-symptom chain of 
    is a non-contiguous sub-symptom chain
( )  
        with  skipped symptoms
0       does not appear in ,
N
i
N
u i
N
i
u sc
u
sc
u sc
??? ?
???= ????
(9) 
where [0,1]??  denotes a decay factor that is 
adopted to penalize the non-contiguous sub-
symptom chains occurred in a symptom chain 
based on the skipped symptoms. For instance, 
1 2 2 31 2 3 1 2 3
( ) ( ) 1s s s ss s s s s s? ?= =  since 1 2s s  and 2 3s s  
are considered as contiguous in 1 2 3s s s , and 
1 3
1
1 2 3( )s s s s s? ?=  since 1 3s s  is a non-contiguous 
sub-symptom chain with one skipped symptom. 
The decay factor is adopted because a contiguous 
sub-symptom chain is preferable to a non-
contiguous chain when comparing two symptom 
chains. The setting of the decay factor is domain 
dependent. If 1? = , then no penalty is applied for 
skipping symptoms, and the cause-effect and tem-
poral relations are transitive. The optimal setting of 
Figure 3.  Illustrative example of relevance com-
putation using the sequence kernel function. 
1028
?  is determined empirically. Figure 3 presents an 
example to summarize the computation of the 
similarity between two symptom chains. 
4 Experimental Results 
4.1 Experiment setup 
1) Corpus: The consultation documents were 
collected from the mental health website of the 
John Tung Foundation (http://www.jtf.org.tw) 
and the PsychPark (http://www.psychpark.org), 
a virtual psychiatric clinic, maintained by a 
group of volunteer professionals of Taiwan 
Association of Mental Health Informatics (Bai 
et al 2001). Both of the web sites provide 
various kinds of free psychiatric services and 
update the consultation documents periodically. 
For privacy consideration, all personal infor-
mation has been removed. A total of 3,650 
consultation documents were collected for 
evaluating the retrieval model, of which 20 
documents were randomly selected as the test 
query set, 100 documents were randomly se-
lected as the tuning set to obtain the optimal 
parameter settings of involved retrieval models, 
and the remaining 3,530 documents were the 
reference set to be retrieved. Table 1 shows the 
average number of events, symptoms and rela-
tions in the test query set. 
2) Baselines: The proposed method, denoted as 
Topic, was compared to two word-based re-
trieval models: the VSM and Okapi BM25 
models. The VSM was implemented in terms 
of the standard TF-IDF weight. The Okapi 
BM25 model is defined as 
(1) 31
2
3
( 1)( 1)
| | ,
t Q
k qtfk tf avdl dl
w k Q
K tf k qtf avdl dl?
++ ?++ + +?  (10) 
where t denotes a word in a query Q; qtf and tf 
denote the word frequencies occurring in a 
query and a document, respectively, and  (1)w  
denotes the Robertson-Sparck Jones weight of 
t (without relevance feedback), defined as 
(1) 0.5log ,
0.5
N n
w
n
? += +             (11) 
where N denotes the total number of docu-
ments, and n denotes the number of documents 
containing t. In (10), K is defined as 
1((1 ) / ),K k b b dl avdl= ? + ?             (12) 
where dl and avdl denote the length and aver-
age length of a document, respectively. The 
default values of 1k , 2k , 3k  and b are describe 
in (Robertson et al, 1996), where 1k  ranges 
from 1.0 to 2.0; 2k  is set to 0; 3k  is set to 8, 
and b ranges from 0.6 to 0.75. Additionally, 
BM25 can be considered as BM15 and BM11 
when b is set to 1 and 0, respectively. 
3) Evaluation metric: To evaluate the retrieval 
models, a multi-level relevance criterion was 
adopted. The relevance criterion was divided 
into four levels, as described below. 
z Level 0: No topics are matched between a 
query and a document. 
z Level 1: At least one topic is partially 
matched between a query and a document. 
z Level 2: All of the three topics are partially 
matched between a query and a document. 
z Level 3: All of the three topics are partially 
matched, and at least one topic is exactly 
matched between a query and a document. 
To deal with the multi-level relevance, the dis-
counted cumulative gain (DCG) (Jarvelin and 
Kekalainen, 2000) was adopted as the evalua-
tion metric, defined as 
[1],                                   1     [ ]
[ 1] [ ]/ log , otherwisec
G if i
DCG i
DCG i G i i
=??= ? ? +??
(13) 
where i denotes the i-th document in the re-
trieved list; G[i] denotes the gain value, i.e., 
relevance levels, of the i-th document, and c 
denotes the parameter to penalize a retrieved 
document in a lower rank. That is, the DCG 
simultaneously considers the relevance levels, 
and the ranks in the retrieved list to measure 
the retrieval precision. For instance, let 
<3,2,3,0,0> denotes the retrieved list of five 
documents with their relevance levels. If no 
penalization is used, then the DCG values for 
Topic Avg. Number
Negative Life Event 1.45 
Depressive Symptom 4.40 
Semantic Relation 3.35 
Table 1. Characteristics of the test query set. 
1029
the retrieved list are <3,5,8,8,8>, and thus 
DCG[5]=8. Conversely, if c=2, then the docu-
ments retrieved at ranks lower than two are pe-
nalized. Hence, the DCG values for the re-
trieved list are <3,5,6.89,6.89,6.89>, and 
DCG[5]=6.89. 
The relevance judgment was performed by 
three experienced physicians. First, the pooling 
method (Voorhees, 2000) was adopted to gen-
erate the candidate relevant documents for 
each test query by taking the top 50 ranked 
documents retrieved by each of the involved 
retrieval models, namely the VSM, BM25 and 
Topic. Two physicians then judged each can-
didate document based on the multilevel rele-
vance criterion. Finally, the documents with 
disagreements between the two physicians 
were judged by the third physician. Table 2 
shows the average number of relevant docu-
ments for the test query set. 
4) Optimal parameter setting: The parameter 
settings of BM25 and Topic were evaluated us-
ing the tuning set. The optimal setting of 
BM25 were k1 =1 and b=0.6. The other two pa-
rameters were set to the default values, i.e., 
2 0k =  and 3 8k = . For the Topic model, the 
parameters required to be evaluated include the 
combination factors, ?  and ? , described in 
(1); the constant const. described in (2), and 
the decay factor, ? , described in (9). The op-
timal settings were 0.3? = ; 0.5? = ; 
const.=0.6 and 0.8? = . 
4.2 Retrieval results 
The results are divided into two groups: the preci-
sion and efficiency. The retrieval precision was 
measured by DCG values. Additionally, a paired, 
two-tailed t-test was used to determine whether the 
performance difference was statistically significant. 
The retrieval efficiency was measure by the query 
processing time, i.e., the time for processing all the 
queries in the test query set. 
Table 3 shows the comparative results of re-
trieval precision. The two variants of BM25, 
namely BM11 and BM15, are also considered in 
comparison. For the word-based retrieval models, 
both BM25 and BM11 outperformed the VSM, and 
BM15 performed worst. The Topic model 
achieved higher DCG values than both the BM-
series models and VSM. The reasons are three-fold. 
First, a negative life event and a symptom can each 
be expressed by different words with the same or 
similar meaning. Therefore, the word-based mod-
els often failed to retrieve the relevant documents 
when different words were used in the input query. 
Second, a word may relate to different events and 
symptoms. For instance, the term "worry about" is 
Relevance Level Avg. Number
Level 1 18.50 
Level 2 9.15 
Level 3 2.20 
Table 2. Average number of relevant documents 
for the test query set. 
 DCG(5) DCG(10) DCG(20) DCG(50) DCG(100) 
Topic 4.7516* 6.9298 7.6040* 8.3606* 9.3974* 
BM25 4.4624 6.7023 7.1156 7.8129 8.6597 
BM11 3.8877 4.9328 5.9589 6.9703 7.7057 
VSM 2.3454 3.3195 4.4609 5.8179 6.6945 
BM15 2.1362 2.6120 3.4487 4.5452 5.7020 
Table 3. DCG values of different retrieval models.  * Topic vs BM25 significantly different (p<0.05) 
Retrieval Model Avg. Time (seconds)
Topic 17.13 
VSM 0.68 
BM25 0.48 
Table 4. Average query processing time of differ-
ent retrieval models. 
1030
a good indicator for both the symptoms <Anxiety> 
and <Hypochondriasis>. This may result in ambi-
guity for the word-based models. Third, the word-
based models cannot capture semantic relations 
between symptoms. The Topic model incorporates 
not only the word-level information, but also more 
useful topic information about depressive problems, 
thus improving the retrieval results. 
The query processing time was measured using 
a personal computer with Windows XP operating 
system, a 2.4GHz Pentium IV processor and 
512MB RAM. Table 4 shows the results. The topic 
model required more processing time than both 
VSM and BM25, since identification of topics in-
volves more detailed analysis, such as semantic 
parsing of sentences and symptom chain construc-
tion. This finding indicates that although the topic 
information can improve the retrieval precision, 
incorporating such high-precision features reduces 
the retrieval efficiency. 
5 Conclusion 
This work has presented the use of topic informa-
tion for retrieving psychiatric consultation docu-
ments. The topic information can provide more 
precise information about users' depressive prob-
lems, thus improving the retrieval precision. The 
proposed framework can also be applied to differ-
ent domains as long as the domain-specific topic 
information is identified. Future work will focus on 
more detailed experiments, including the contribu-
tion of each topic to retrieval precision, the effect 
of using different methods to combine topic infor-
mation, and the evaluation on real users. 
References 
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. Addison-Wesley, Reading, 
MA. 
Cancedda, N., E. Gaussier, C. Goutte, and J. M. Renders. 
2003. Word-Sequence Kernels. Journal of Machine 
Learning Research, 3(6):1059-1082. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, MA. 
Hamilton, M. 1960. A Rating Scale for Depression. 
Journal of Neurology, Neurosurgery and Psychiatry, 
23:56-62 
Jarvelin, K. and J. Kekalainen. 2000. IR Evaluation 
Methods for Retrieving Highly Relevant Documents. 
In Proc. of the 23rd Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, pages 41-48. 
Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini, 
and C. Watkins. 2002. Text Classification Using 
String Kernels. Journal of Machine Learning Re-
search, 2(3):419-444. 
Okabe, M., K. Umemura and S. Yamada. 2005. Query 
Expansion with the Minimum User Feedback by 
Transductive Learning. In Proc. of HLT/EMNLP, 
Vancouver, Canada, pages 963-970. 
Pagano, M.E., A.E. Skodol, R.L. Stout, M.T. Shea, S. 
Yen, C.M. Grilo, C.A. Sanislow, D.S. Bender, T.H. 
McGlashan, M.C. Zanarini, and J.G. Gunderson. 
2004. Stressful Life Events as Predictors of Function-
ing: Findings from the Collaborative Longitudinal 
Personality Disorders Study. Acta Psychiatrica Scan-
dinavica, 110: 421-429. 
Robertson, S. E., S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M.Gatford. 1995. Okapi at TREC-3. In 
Proc. of the Third Text REtrieval Conference (TREC-
3), NIST. 
Robertson, S. E., S. Walker, M. M. Beaulieu, and 
M.Gatford. 1996. Okapi at TREC-4. In Proc. of the 
fourth Text REtrieval Conference (TREC-4), NIST. 
Voorhees, E. M. and D. K. Harman. 2000. Overview of 
the Sixth Text REtrieval Conference (TREC-6). In-
formation Processing and Management, 36(1):3-35. 
Wu, C. H., L. C. Yu, and F. L. Jang. 2005a. Using Se-
mantic Dependencies to Mine Depressive Symptoms 
from Consultation Records. IEEE Intelligent System, 
20(6):50-58. 
Wu, C. H., J. F. Yeh, and M. J. Chen. 2005b. Domain-
Specific FAQ Retrieval Using Independent Aspects. 
ACM Trans. Asian Language Information Processing, 
4(1):1-17. 
Wu, C. H., J. F. Yeh, and Y. S. Lai. 2006. Semantic 
Segment Extraction and Matching for Internet FAQ 
Retrieval. IEEE Trans. Knowledge and Data Engi-
neering, 18(7):930-940. 
Yeh, J. F., C. H. Wu, M. J. Chen, and L. C. Yu. 2004. 
Automated Alignment and Extraction of Bilingual 
Domain Ontology for Cross-Language Domain-
Specific Applications. In Proc. of the 20th COLING, 
Geneva, Switzerland, pages 1140-1146. 
Yu, L. C., C. H. Wu, Yeh, J. F., and F. L. Jang. 2007. 
HAL-based Evolutionary Inference for Pattern Induc-
tion from Psychiatry Web Resources. Accepted by 
IEEE Trans. Evolutionary Computation. 
1031
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1254?1262,
Beijing, August 2010
Discriminative Training for Near-Synonym Substitution 
 
Liang-Chih Yu1, Hsiu-Min Shih2, Yu-Ling Lai2, Jui-Feng Yeh3 and Chung-Hsien Wu4
1Department of Information Management, Yuan Ze University 
2Department of Mathematics, National Chung Cheng University 
3Department of CSIE, National Chia-Yi University 
4Department of CSIE, National Cheng Kung University 
Contact: lcyu@saturn.yzu.edu.tw 
 
  
 
Abstract 
Near-synonyms are useful knowledge re-
sources for many natural language applica-
tions such as query expansion for information 
retrieval (IR) and paraphrasing for text gen-
eration. However, near-synonyms are not nec-
essarily interchangeable in contexts due to 
their specific usage and syntactic constraints. 
Accordingly, it is worth to develop algorithms 
to verify whether near-synonyms do match the 
given contexts. In this paper, we consider the 
near-synonym substitution task as a classifica-
tion task, where a classifier is trained for each 
near-synonym set to classify test examples 
into one of the near-synonyms in the set. We 
also propose the use of discriminative training 
to improve classifiers by distinguishing posi-
tive and negative features for each near-
synonym. Experimental results show that the 
proposed method achieves higher accuracy 
than both pointwise mutual information (PMI) 
and n-gram-based methods that have been 
used in previous studies. 
1 Introduction 
Near-synonym sets represent groups of words 
with similar meaning, which are useful knowl-
edge resources for many natural language appli-
cations. For instance, they can be used for query 
expansion in information retrieval (IR) (Moldo-
van and Mihalcea, 2000; Bhogal et al, 2007), 
where a query term can be expanded by its near-
synonyms to improve the recall rate. They can 
also be used in an intelligent thesaurus that can 
automatically suggest alternative words to avoid 
repeating the same word in the composing of 
text when there are suitable alternatives in its 
synonym set (Inkpen and Hirst, 2006; Inkpen, 
2007). These near-synonym sets can be derived 
from manually constructed dictionaries such as 
WordNet (called synsets) (Fellbaum, 1998), Eu-
roWordNet (Rodr?guez et al, 1998), or clusters 
derived using statistical approaches (Lin, 1998). 
Although the words in a near-synonym set 
have similar meaning, they are not necessarily 
interchangeable in practical use due to their spe-
cific usage and collocational constraints. Pearce 
(2001) presented an example of collocational 
constraints for the context ?         coffee?. In the 
given near-synonym set {strong, powerful}, the 
word ?strong? is more suitable than ?powerful? 
to fill the gap, since ?powerful coffee? is an anti-
collocation. Inkpen (2007) also presented several 
examples of collocations (e.g. ghastly mistake) 
and anti-collocations (e.g. ghastly error). Yu et 
al. (2007) described an example of the context 
mismatch problem for the context ?        under 
the bay? and the near-synonym set {bridge, 
overpass, viaduct, tunnel} that represents the 
meaning of a physical structure that connects 
separate places by traversing an obstacle. The 
original word (target word) in the given context 
is ?tunnel?, and cannot be substituted by the 
other words in the same set since all the substitu-
tions are semantically implausible. Accordingly, 
it is worth to develop algorithms to verify 
whether near-synonyms do match the given con-
texts. Applications can benefit from this ability 
to provide more effective services. For instance, 
a writing support system can assist users to se-
lect an alternative word that best fits a given 
context from a list of near-synonyms. 
In measuring the substitutability of words, the 
co-occurrence information between a target word 
1254
(the gap) and its context words is commonly 
used in statistical approaches. Edmonds (1997) 
built a lexical co-occurrence network from 1989 
Wall Street Journal to determine the near-
synonym that is most typical or expected in a 
given context. Inkpen (2007) used the pointwise 
mutual information (PMI) formula to select the 
best near-synonym that can fill the gap in a 
given context. The PMI scores for each candi-
date near-synonym are computed using a larger 
web corpus, the Waterloo terabyte corpus, which 
can alleviate the data sparseness problem en-
countered in Edmonds? approach. Following 
Inkpen?s approach, Gardiner and Dras (2007) 
also used the PMI formula with a different cor-
pus (the Web 1T 5-gram corpus) to explore 
whether near-synonyms differ in attitude. 
Yu et al (2007) presented a method to com-
pute the substitution scores for each near-
synonym based on n-gram frequencies obtained 
by querying Google. A statistical test is then ap-
plied to determine whether or not a target word 
can be substituted by its near-synonyms. The 
dataset used in their experiments are derived 
from the OntoNotes copus (Hovy et al, 2006; 
Pradhan et al, 2007), where each near-synonym 
set corresponds to a sense pool in OntoNotes. 
Another direction to the task of near-synonym 
substitution is to identify the senses of a target 
word and its near-synonyms using word sense 
disambiguation (WSD), comparing whether they 
were of the same sense (McCarthy, 2002; Dagan 
et al, 2006). Dagan et al (2006) described that 
the use of WSD is an indirect approach since it 
requires the intermediate sense identification 
step, and thus presented a sense matching tech-
nique to address the task directly. 
In this paper, we consider the near-synonym 
substitution task as a classification task, where a 
classifier is trained for each near-synonym set to 
classify test examples into one of the near-
synonyms in the set. However, near-synonyms 
share more common context words (features) 
than semantically dissimilar words in nature. 
Such similar contexts may decrease classifiers? 
ability to discriminate among near-synonyms. 
Therefore, we propose the use of a supervised 
discriminative training technique (Ohler et al, 
1999; Kuo and Lee, 2003; Zhou and He, 2009) 
to improve classifiers by distinguishing positive 
and negative features for each near-synonym. To 
our best knowledge, this is the first study that 
uses discriminative training for near-synonym or 
lexical substitution. The basic idea of discrimi-
native training herein is to adjust feature weights 
according to the minimum classification error 
(MCE) criterion. The features that contribute to 
discriminating among near-synonyms will re-
ceive a greater positive weight, whereas the 
noisy features will be penalized and might re-
ceive a negative weight. This re-weighting 
scheme helps increase the separation of the cor-
rect class against its competing classes, thus im-
proves the classification performance.  
The proposed supervised discriminative train-
ing is compared with two unsupervised methods, 
the PMI-based (Inkpen, 2007) and n-gram-based 
(Yu et al, 2007) methods. The goal of the 
evaluation is described as follows. Given a near-
synonym set and a sentence with one of the near-
synonyms in it, the near-synonym is deleted to 
form a gap in this sentence. Figure 1 shows an 
example. Each method is then applied to predict 
an answer (best near-synonym) that can fill the 
gap. The possible candidates are all the near-
synonyms (including the original word) in the 
given set. Ideally, the correct answers should be 
provided by human experts. However, such data 
is usually unavailable, especially for a large set 
of test examples. Therefore, we follow Inkpen?s 
experiments to consider the original word as the 
correct answer. The proposed methods can then 
be evaluated by examining whether they can re-
store the original word by filling the gap with the 
best near-synonym.  
The rest of this work is organized as follows. 
Section 2 describes the PMI and n-gram-based 
methods for near-synonym substitution. Section 
3 presents the discriminative training technique. 
Section 4 summarizes comparative results. Con-
clusions are finally drawn in Section 5. 
Sentence: This will make the           message 
easier to interpret. 
Original word: error 
Near-synonym set: {error, mistake, oversight}
Figure 1. Example of a near-synonym set and a 
sentence to be evaluated. 
1255
2 Unsupervised Methods 
2.1 PMI-based method 
The mutual information can measure the co-
occurrence strength between a near-synonym 
and the words in a given context. A higher mu-
tual information score indicates that the near-
synonym fits well in the given context, thus is 
more likely to be the correct answer. The point-
wise mutual information (Church and Hanks, 
1991) between two words x and y is defined as  
2
( , )
( , ) log ,
( ) ( )
P x y
PMI x y
P x P y
=            (1) 
where ( , ) ( , )P x y C x y N=  denotes the prob-
ability that x and y co-occur; ( , )C x y  is the 
number of times x and y co-occur in the corpus, 
and N is the total number of words in the corpus. 
Similarly, ( ) ( )P x C x N= , where C(x) is the 
number of times x occurs in the corpus, and 
( ) ( )P y C y N= , where C(y) is the number of 
times y occurs in the corpus. Therefore, (1) can 
be re-written as  
2
( , )
( , ) log .
( ) ( )
C x y N
PMI x y
C x C y
?= ?           (2) 
Inkpen (2007) computed the PMI scores for each 
near-synonym using the Waterloo terabyte cor-
pus and a context window of size 2k (k=2). 
Given a sentence s with a gap, 
1 1 2... ...      ... ...k k ks w w w w+= , the PMI score for 
a near-synonym NSi to fill the gap is defined as  
1
2
1
( , ) ( , )
                        ( , ).
=
= +
= +?
?
k
j j ii
k
j ii k
PMI NS s PMI NS w
PMI NS w
          (3) 
The near-synonym with the highest score is con-
sidered as the answer. In this paper, we use the 
Web 1T 5-gram corpus to compute PMI scores, 
the same as in (Gardiner and Dras, 2007). The 
frequency counts C(?) are retrieved from this 
corpus in the same manner within the 5-gram 
boundary.  
2.2 N-gram-based method 
The n-grams can capture contiguous word asso-
ciations in given contexts. Given a sentence with 
a gap, the n-gram scores for each near-synonym 
are computed as follows. First, all possible n-
grams containing the gap are extracted from the 
sentence. Each near-synonym then fills the gap 
to compute a normalized frequency according to  
( )log ( ) 1
( ) ,
log ( )
j
j
i
NSi
NS
j
C ngram
Z ngram
C NS
+=         (4) 
where ( )
j
i
NSC ngram  denotes the frequency of an 
n-gram containing a near-synonym, ( )jC NS  
denotes the frequency of a near-synonym, and 
( )
j
i
NSZ ngram  denotes the normalized frequency 
of an n-gram, which is used to reduce the effect 
of high frequencies on measuring n-gram scores. 
All of the above frequencies are retrieved from 
the Web 1T 5-gram corpus.  
The n-gram score for a near-synonym to fill 
the gap is computed as  
1
1
( , ) ( ),
=
= ? jR ij NS
i
NGRAM NS s Z ngram
R
        (5) 
where ( , )jNGRAM NS s  denotes the n-gram 
score of a near-synonym, which is computed by 
averaging the normalized frequencies of all the 
n-grams containing the near-synonym, and R is 
the total number of n-grams containing the near-
synonym. Again, the near-synonym with the 
highest score is the proposed answer. We herein 
use the 4-gram frequencies to compute n-gram 
scores as Yu et al (2007) reported that the use of 
4-grams achieved higher performance than the 
use of bigrams and trigrams.  
3 Discriminative Training 
3.1 Classifier 
The supervised classification technique can also 
be applied to for near-synonym substitution. 
Each near-synonym in a set corresponds to a 
class. The classifiers for each near-synonym set 
are built from the labeled training data, i.e., a 
collection of sentences containing the near-
synonyms. Such training data is easy to obtain 
since it requires no human annotation. The train-
ing data used herein is collected by extracting 
the 5-grams containing the near-synonyms from 
the Web 1T 5-gram corpus. The features used 
for training are the words occurring in the con-
text of the near-synonyms in the 5-grams.  
1256
For each near-synonym set, an F K?  feature-
class matrix, denoted as M, is created for classi-
fication. The rows represent the F distinct words 
(features) and the columns represent the K near-
synonyms (classes) in the same set. Each entry 
in the matrix, mij, represents a weight of word i 
respect to near-synonym j, which is computed as 
the number of times word i appears in the con-
texts of near-synonym j divided by the total 
number of context words of near-synonym j. 
This frequency-based weight can then be trans-
formed into a probabilistic form, i.e., divided by 
the sum of the weights of word i respect to all 
near-synonyms. Each test sentence is also trans-
formed into an F-dimensional feature vector. Let 
1[ ,..., ,..., ]= i Fx x x x  denote the feature vector of 
an input sentence. The classification is per-
formed by computing the cosine similarity be-
tween x and the column vectors (near-synonyms) 
in the matrix, defined as 
1
2 2
1 1
 arg max cos( , )
    
        arg max                             (6)
        arg max ,
j
j j
j
j j
F
i iji
F Fj
i iji i
NS x m
x m
x m
x m
x m
?
=
= =
=
=
= ?? ?
i
 
where jm  is the j-th column vector in the matrix 
M. The near-synonym with the highest cosine 
similarity score, ?
j
NS , is the predicted class of 
the classifier. 
3.2 Minimum classification error criterion 
According to the decision rule of the classifier, a 
classification error will occur when the near-
synonym with the highest cosine score is not the 
correct class. Table 1 shows some examples, 
where Example 3 is an example of misclassifica-
tion. On the other hand, although Example 2 is a 
correct classification, it might be an ambiguous 
case to classifiers since the scores are close 
among classes. Therefore, if we can increase the 
separation of the correct class from its compet-
ing ones, then the classification performance can 
be improved accordingly. This can be accom-
plished by adjusting the feature weights of the 
matrix M that have direct influence on the com-
putation of cosine scores. The discriminative 
training performs the adjustment in the training 
phase according to the minimum classification 
error criterion. The detailed steps are as follows. 
Given an input vector x, the classifier com-
putes the cosine scores between x and each class 
using (6). The discriminant function for a class 
can thus be defined as the cosine measure; that is, 
( , ) cos( , ).=j jg x M x m             (7) 
where j denotes a class in K. Since the correct 
class of each input vector is known in the train-
ing phase, we can determine whether or not the 
input vector is misclassified by comparing the 
discriminant function (cosine score) of the cor-
rect class against its competing classes. In the 
case of misclassification, the cosine score of the 
correct class will be smaller than the competing 
cosine scores. Let k be the correct class of x, the 
misclassification function can be defined as  
 ( , )  ( , )  ( , ),k k kd x M g x M G x M= ? +            (8) 
where ( , )kg x M  is the discriminant function for 
the correct class k, and ( , )kG x M  is the anti-
discriminant function that represents the other 
1K ?  competing classes, defined as 
1
1
( , ) ( , ) ,
1k jj k
G x M g x M
K
??
?
? ?= ? ??? ??              (9) 
When 1? = , the anti-discriminant function 
( , )kG x M  is the average of all the competing 
cosine scores. With the increase of ? , 
( , )kG x M is gradually dominated by the biggest 
 Example 
 1 2 3 
1 1( , ) cos( , )=g x M x m  0.9* 0.6* 0.8 
2 2( , ) cos( , )=g x M x m  0.3 0.5 0.3* 
3 3( , ) cos( , )=g x M x m  0.2 0.4 0.1 
max ( , )? =j k ig x M  0.3 0.5 0.8 
( , ) =kd x M  -0.6 -0.1 0.5 
( , ) =kl x M  
                (?=5) 
0.047 0.378 0.924
Table 1. Examples of correct classification 
and misclassification. * denotes the scores of the 
correct class.  
1257
competing class. In the extreme case, i.e., 
? ?? , the anti-discriminant function becomes 
( , ) max  ( , ).k jj kG x M g x M?=          (10) 
The misclassification function in (8) can thus be 
rewritten as 
( , )  ( , ) max  ( , ),k k jj kd x M g x M g x M?= ? +    (11) 
In this case, the classification error is determined 
by comparing the discriminant function of the 
correct class against that of the biggest compet-
ing class. Obviously, ( , ) 0kd x M >  implies a 
classification error. For instance, in Example 3, 
the discriminant function for the correct class is 
2 ( , ) 0.3g x M = , and that of the biggest compet-
ing class is 1 3max( ( , ), ( , )) 0.8=g x M g x M , thus 
the classification error is ( , ) 0.5=kd x M . On the 
other hand, the classification error will be a 
negative value for correct classifications, as 
shown in Example 1 and 2. 
Intuitively, a greater classification error also 
results in a greater loss. We herein use the sig-
moid function as the class loss function; that is, 
1
( , ) ( ) ,
1 exp kk k d
l x M l d ??= = +          (12) 
where ? is a constant that controls the slope of 
the sigmoid function. The sigmoid function 
maps the values of classification error within the 
range of 0 to 1. For correct classifications, a 
greater separation of the correct class from the 
biggest competing class leads to a greater nega-
tive value of dk, i.e., a smaller classification error, 
resulting in a smaller loss tends asymptotically 
to 0 (Example 1), whereas a moderate loss is 
yielded if the separation was close (Example 2). 
For the cases of misclassification, a greater sepa-
ration leads to a greater positive value of dk, i.e., 
a greater classification error, resulting in a 
greater loss tends asymptotically to 1 (Example 
3). The overall loss of the entire training set can 
then be estimated as 
1
1
( ) ( , ),
= ?
= ??
k
K
k
k x C
L M l x M
Q
                      (13) 
where Ck denotes the set of training vectors of 
class k, and Q is the total number of vectors in 
the training set. The goal now is to minimize the 
loss. According to the above discussions on the 
three examples, to minimize the loss is to mini-
mize the classification error, and to improve the 
separation of the correct class against its compet-
ing classes. This can be accomplished by adjust-
ing the feature weights of the matrix M to distin-
guish positive and negative features for each 
class. We herein adopt a gradient descent 
method such as the generalized probabilistic de-
scent (GPD) algorithm (Katagiri et al, 1998) to 
update the matrix M. The detailed steps are as 
follows. 
Let the feature weights of the matrix M be the 
parameter set to be adjusted. The adjustment is 
performed iteratively according to the following 
update formula. 
( 1) ( ) ( ) ( )( , ),?+ = ? ?t t t tt kM M l x M         (14) 
where t denotes the t-th iteration, ? t  is an ad-
justment step of a small positive real number, 
and ( ) ( )( , )? t tkl x M is the gradient of the loss 
function, which is computed by the following 
two parts 
( ) ( )
( ) ( ) ( , )( , ) ,
t t
t t k k
k
k ij
l d x M
l x M
d m
? ?? = ? ?          (15) 
where  
( )(1 ( )),k k k k k
k
l
l d l d
d
?? = ??          (16) 
and from (7), (8), and (9),  
( ) ( )
( ) ( ) 1
( )
,                                           if  
( , ) ( , ) ( , ) ,
  ,  if  
( , )
?
?
?
?
? =??? = ? ?? ?? ?
i
t t
t tk
k j
i tij
jj k
x j k
d x M G x M g x M
x j km
g x M
                                                                         (17) 
where xi is an element of the input vector x. By 
replacing ( , )k t tl x M?  in (14) with the two parts 
in (15), at each iteration each feature weight mij 
in M is adjusted by 
  
( )
( 1)
( ) ( ) 1
( )
( )
,                                          if  
.
( , ) ( , )
, if  
( , )
?
?
?
?
+
?
?
?? + =? ??= ? ?? ? ?? ?? ?
t k
ij t i
kt
t tij
k jt k
ij t i t
k jj k
l
m x j k
d
m
G x M g x Ml
m x j k
d g x M
 
                         (18) 
The weight xi represents whether or not a dimen-
sion word occurs in an input sentence. A zero 
1258
weight indicates that the dimension word does 
not occur in the input sentence, thus the corre-
sponding dimension of each column vector will 
not be adjusted. On the contrary, the correspond-
ing dimension of the column vector of the cor-
rect class ( j k= ) is adjusted by adding a value, 
while those of the competing classes ( j k? ) are 
adjusted by subtracting a value from them. After 
a sequence of adjustments over the training set, 
the positive and negative features can be distin-
guished by adjusting their weights that result in a 
greater positive or negative value for each of 
them. The separation of the correct class against 
its competing ones can thus be increased.  
The weight adjustment in (18) is in proportion 
to the adjustment step ? t  and the slope of the 
sigmoid function k kl d? ? . The adjustment step 
? t  can be determined empirically. As (16) shows, 
the slope k kl d? ?  converges asymptotically to 
zero as the classification error dk approaches to a 
very large (or small) value. This leads to a small 
weight adjustment. For instance, the weight ad-
justment in Example 1 is small due to a small 
value of dk, or, say, due to a large separation be-
tween the correct class and its competing ones. 
This is reasonable because classifiers often per-
form well in such cases. Similarly, the weight 
adjustment in Example 3 (misclassification) is 
also small due to a large value of dk. A greater 
adjustment is not employed because such a large 
separation is difficult to be reduced significantly. 
Additionally, over-adjusting some features may 
introduce negative effects on other useful fea-
tures in the matrix. Therefore, discriminative 
training is more effective on the cases with a 
moderate value of dk, like Example 2. Such cases 
usually fall within the decision boundary and 
tend to be confusing to classifiers. Hence, im-
proving the separation on such cases helps sig-
nificantly improve the classification performance. 
4 Experimental Results 
4.1 Experiment setup 
1) Data: The near-synonym sets used for ex-
periments included the seven sets (Exp1) and the 
eleven sets (Exp2) used in the previous studies 
(Edmonds, 1997; Inkpen, 2007; Gardiner and 
Dras, 2007), as shown in Table 2. The Web 1T 
5-gram corpus was used to build classifiers, 
where the corpus was randomly split into a train-
ing set, a development set, and a test set with an 
8:1:1 ratio. For efficiency consideration, we ran-
domly sampled up to 100 5-grams from the test 
set for each near-synonym. This sampling pro-
cedure was repeated five times for evaluation of 
the classifiers. 
2) Classifiers: The classifiers were imple-
mented using PMI, n-grams, and discriminative 
training (DT) methods, respectively.  
PMI: Given a near-synonym set and a test 5-
gram with a gap, the PMI scores for each near-
synonym were calculated using (3), where the 
size of the context window k was set to 2. The 
frequency counts between each near-synonym 
and its context words were retrieved from the 
training set. 
NGRAM: For each test 5-gram with a gap, all 
possible 4-grams containing the gap were first 
extracted (excluding punctuation marks). The 
averaged 4-gram scores for each near-synonym 
were then calculated using (5). Again, the fre-
quency counts of the 4-grams were retrieved 
from the training set. 
DT: For each near-synonym set, the matrix M 
was built from the training set. Each 5-gram in 
the development set was taken as input to itera-
tively compute the cosine score, loss, classifica-
tion error, respectively, and finally to adjust the 
feature weights of M. The parameters of DT in-
cluded ?  for the anti-discriminative function, ? 
0 20 40 60 80 100
Iteration
0.68
0.7
0.72
0.74
0.76
A
cc
ur
ac
y
Test set
Development set
Figure 2. The change of classification accuracy 
during discriminative training.  
1259
for the sigmoid function, and t?  for the adjust-
ment step. The settings, 25? = , 35? = , and 
310? ?=t , were determined by performing DT 
for several iterations through the training set. 
These setting were used for the following ex-
periments. 
3) Evaluation metric: The answers proposed 
by each classifier are the near-synonyms with 
the highest score. The correct answers are the 
near-synonyms originally in the gap of the test 5-
grams. The performance is measure by the accu-
racy, which is defined as the number of correct 
answers made by each classifier, divided by the 
total number of test 5-grams. 
In the following sections, we first demonstrate 
the effect of DT on classification performance, 
followed by the comparison of the classifiers. 
4.2 Evaluation on discriminative training 
This experiment is to investigate the perform-
ance change during discriminative training. Fig-
ure 2 shows the accuracy at the first 100 itera-
tions for both development set and test set, with 
the 8th set in Exp2 as an example. The accuracy 
increased rapidly in the first 20 iterations, and 
stabilized after the 40th iteration. The discrimi-
native training is stopped until the accuracy has 
not been changed over 30 iterations or the 300th 
iteration has been reached. 
Accuracy (%) 
No. Near-synonym set No. ofcases NGRAM PMI COS DT 
1. difficult, hard, tough 300 58.60 61.67 60.13 63.13 
2. error, mistake, oversight 300 68.47 78.33 77.20 79.20 
3. job, task, duty 300 68.93 70.40 74.00 75.67 
4. responsibility, burden, obligation, commitment 400 69.80 66.95 68.75 69.55 
5. material, stuff, substance 300 70.20 67.93 71.07 75.13 
6. give, provide, offer 300 58.87 66.47 64.13 68.27 
7. settle, resolve 200 69.30 68.10 77.10 84.10 
Exp1 2,100 66.33 68.50 69.94 72.89 
1. benefit, advantage, favor, gain, profit 500 71.44 69.88 69.44 71.36 
2. low, gush, pour, run, spout, spurt, squirt, stream 800 65.45 65.00 68.68 71.08 
3. deficient, inadequate, poor, unsatisfactory 400 65.65 69.40 70.35 74.35 
4. 
afraid, aghast, alarmed, anxious, apprehensive, 
fearful, frightened, scared, terror-stricken* 789 49.84 44.74 47.00 49.33 
5. 
disapproval, animadversion*, aspersion*, blame, 
criticism, reprehension* 300 72.80 79.47 80.00 82.53 
6. mistake, blooper, blunder, boner, contretemps*,  error, faux pas*, goof, slip, solecism* 618 62.27 59.61 68.41 71.65 
7. alcoholic, boozer*, drunk, drunkard, lush, sot 433 64.90 80.65 77.88 84.34 
8. leave, abandon, desert, forsake 400 65.85 66.05 69.35 74.35 
9. 
opponent, adversary, antagonist, competitor, 
enemy, foe, rival 700 58.51 59.51 63.31 67.14 
10. 
thin, lean, scrawny, skinny, slender, slim, spare, 
svelte, willowy*, wiry 734 57.74 61.99 55.72 64.58 
11. lie, falsehood, fib, prevarication*,  rationalization, untruth 425 57.55 63.58 69.46 74.21 
Exp2 6,099 61.69 63.32 65.15 69.26 
Table 2. Accuracy of classifiers on Exp1 (7 sets) and Exp2 (11 sets). The words marked with * are 
excluded from the experiments because their 5-grams are very rare in the corpus. 
1260
4.3 Comparative results 
Table 2 shows the comparative results of the 
classification accuracy for the 18 near-synonym 
sets (Exp1 + Exp2). The accuracies for each 
near-synonym set were the average accuracies of 
the five randomly sampled test sets. The cosine 
measure without discrimination training (COS) 
was also considered for comparison. The results 
show that NGRAM performed worst among the 
four classifiers. The major reason is that not all 
4-grams of the test examples can be found in the 
corpus. Instead of contiguous word associations 
used by NGRAM, PMI considers the words in 
the contexts independently to select the best 
synonyms. The results show that PMI achieved 
better performance than NGRAM. The two su-
pervised methods, COS and DT, outperformed 
the two unsupervised methods, NGRAM and 
PMI. As indicated in the bold numbers, using the 
supervised method alone (without DT), COS 
yielded higher average accuracy by 5% and 2% 
over NGRAM and PMI, respectively, on Exp1, 
and by 6% and 3%, respectively, on Exp2. When 
DT was employed, the average accuracy was 
further improved by 4% and 6% on Exp1 and 
Exp2, respectively, compared with COS. 
The use of DT can improve the classification 
accuracy mainly because it can adjust the feature 
weights iteratively to improve the separation be-
tween the correct class and its competing ones, 
which helps tackle the ambiguous test examples 
that fall within the decision boundary. Table 3 
presents several positive and negative features 
for the near-synonym set {mistake, error, over-
sight}. The feature weights were adjusted ac-
cording to their contributions to discriminating 
among the near-synonyms in the set. For in-
stance, the features ?made? and ?biggest? both 
received a positive value for the class ?mistake?, 
and a negative value for the competing classes 
?error? and ?oversight?. These positive and 
negative weights help distinguish useful features 
from noisy ones for classifier training. On the 
other hand, if the feature weights were evenly 
distributed among the classes, these features 
would not be unlikely to contribute to the classi-
fication performance.  
4.4 Accuracy of Rank 1 and Rank 2 
The accuracy presented in Table 2 was com-
puted based on the classification results at Rank 
1, i.e., a test sample was considered correctly 
classified only if the near-synonym with the 
highest score was the word originally in the gap 
of the test sample. Similarly, the accuracy at 
Rank 2 can be computed by considering the top 
two near-synonyms proposed by each classifier. 
That is, if either the near-synonym with the 
highest or the second highest score was the cor-
rect answer, the test sample was considered cor-
rectly classified. Table 4 shows the accuracy of 
Rank 1 and Rank 2 for each classifier. The re-
sults show that the improvement of Rank 1 accu-
racy on Exp1 was about 20 to 30 percentage 
points, and was 25.76 in average. For Exp2, the 
average improvement was 19.80 percentage 
points 
Near-synonym set 
Features 
mistake error oversight
made 0.076 -0.004 -0.005 
biggest 0.074 -0.001 -0.004 
message -0.004 0.039 -0.010 
internal 0.001 0.026 -0.001 
supervision -0.001 -0.006 0.031 
audit -0.002 -0.003 0.028 
Table 3. Example of feature weights after dis-
criminative training.  
Exp1 Rank 1 Rank 2 Diff. 
NGRAM 66.33% 79.35% +19.63% 
PMI 68.50% 88.99% +29.91% 
COS 69.94% 89.93% +28.58% 
DT 72.89% 91.06% +24.93% 
Exp2 Rank 1 Rank 2 Diff. 
NGRAM 61.69% 68.48% +11.01% 
PMI 63.32% 79.11% +24.94% 
COS 65.15% 80.52% +23.59% 
DT 69.26% 82.86% +19.64% 
Table 4. Accuracy of Rank 1 and Rank 2 for 
each classifier.  
1261
5 Conclusion  
This work has presented the use of discrimina-
tive training for near-synonym substitution. The 
discriminative training can improve classifica-
tion performance by iteratively re-weighting the 
positive and negative features for each class. 
This helps improve the separation of the correct 
class against its competing ones, making classi-
fiers more effective on ambiguous cases close to 
the decision boundary. Experimental results 
show that the supervised discriminative training 
technique achieves higher accuracy than the two 
unsupervised methods, the PMI-based and n-
gram-based methods. The availability of a large 
labeled training set alo encourages the use of 
the proposed supervised method.  
Future work will investigate on the use of 
multiple features for discriminating among near-
synonyms. For instance, the predicate-argument 
structure, which can capture long-distance in-
formation, will be combined with currently used 
local contextual features to boost the classifica-
tion performance. More experiments will also be 
conducted to evaluate classifiers? ability to rank 
multiple answers. 
References  
J. Bhogal, A. Macfarlane, and P. Smith. 2007. A Re-
view of Ontology based Query Expansion. Infor-
mation Processing & Management, 43(4):866-886. 
K. Church and P. Hanks. 1991. Word Association 
Norms, Mutual Information and Lexicography. 
Computational Linguistics, 16(1):22-29. 
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein, 
and C. Strapparava. 2006. Direct Word Sense 
Matching for Lexical Substitution. In Proc. of 
COLING/ACL-06, pages 449-456. 
P. Edmonds. 1997. Choosing the Word Most Typical 
in Context Using a Lexical Co-occurrence Net-
work. In Proc. of ACL-97, pages 507-509. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, MA. 
M. Gardiner and M. Dras. 2007. Exploring Ap-
proaches to Discriminating among Near-Synonyms, 
In Proc. of the Australasian Technology Workshop, 
pages 31-39. 
E. H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and 
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proc. of HLT/NAACL-06, pages 57?60. 
D. Inkpen. 2007. Near-Synonym Choice in an Intelli-
gent Thesaurus. In Proc. of NAACL/HLT-07, pages 
356-363. 
D. Inkpen and G. Hirst. 2006. Building and Using a 
Lexical Knowledge-base of Near-Synonym Differ-
ences. Computational Linguistics, 32(2):1-39. 
S. Katagiri, B. H. Juang, and C. H. Lee. 1998. Pattern 
Recognition Using a Family of Design Algorithms 
based upon the Generalized Probabilistic Descent 
Method, Proc. of the IEEE, 86(11):2345-2373. 
H. K. J. Kuo and C. H. Lee. 2003. Discriminative 
Training of Natural Language Call Routers, IEEE 
Trans. Speech and Audio Processing, 11(1):24-35. 
D. Lin. 1998. Automatic Retrieval and Clustering of 
Similar Words. In Proc. of ACL/COLING-98, 
pages 768-774. 
D. McCarthy. 2002. Lexical Substitution as a Task 
for WSD Evaluation. In Proc. of the 
SIGLEX/SENSEVAL Workshop on Word Sense 
Disambiguation at ACL-02, pages 109-115. 
D. Moldovan and R. Mihalcea. 2000. Using Wordnet 
and Lexical Operators to Improve Internet 
Searches. IEEE Internet Computing, 4(1):34-43. 
U. Ohler, S. Harbeck, and H. Niemann. 1999. Dis-
criminative Training of Language Model Classifi-
ers, In Proc. of Eurospeech-99, pages 1607-1610. 
D. Pearce. 2001. Synonymy in Collocation Extraction. 
In Proc. of the Workshop on WordNet and Other 
Lexical Resources at NAACL-01. 
S. Pradhan, E. H. Hovy, M. Marcus, M. Palmer, L. 
Ramshaw, and R. Weischedel. 2007. OntoNotes: A 
Unified Relational Semantic Representation. In 
Proc. of the First IEEE International Conference 
on Semantic Computing (ICSC-07), pages 517-524. 
H. Rodr?guez, S. Climent, P. Vossen, L. Bloksma, W. 
Peters, A. Alonge, F. Bertagna, and A. Roventint. 
1998. The Top-Down Strategy for Building Eeu-
roWordNet: Vocabulary Coverage, Base Concepts 
and Top Ontology, Computers and the Humanities, 
32:117-159. 
L. C. Yu, C. H. Wu, A. Philpot, and E. H. Hovy. 2007. 
OntoNotes: Sense Pool Verification Using Google 
N-gram and Statistical Tests, In Proc. of the On-
toLex Workshop at the 6th International Semantic 
Web Conference (ISWC-07). 
D. Zhou and Y. He. 2009. Discriminative Training of 
the Hidden Vector State Model for Semantic Pars-
ing, IEEE Trans. Knowledge and Data Engineer-
ing, 21(1):66-77. 
1262
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 837?847, Dublin, Ireland, August 23-29 2014.
Identifying Emotion Labels from Psychiatric Social Texts Using  
Independent Component Analysis  
 
 
Liang-Chih Yu1,2  and  Chun-Yuan Ho1 
1Department of Information Management 
2Innovation Center for Big Data and Digital Convergence 
Yuan Ze University, Chung-Li, Taiwan 
 lcyu@saturn.yzu.edu.tw, s986304@mail.yzu.edu.tw 
 
? 
Abstract 
Accessing the web has been an efficient and effective means to acquire self-help knowledge when suf-
fering from depressive problems. Many mental health websites have developed community-based ser-
vices such as web forums and blogs for Internet users to share their depressive problems with other us-
ers and health professionals. Other users or health professionals can then make recommendations in re-
sponse to these problems. Such communications produce a large number of documents called psychiat-
ric social texts containing rich emotion labels representing different depressive problems. Automatically 
identify such emotion labels can make online psychiatric services more effective. This study proposes a 
framework combining latent semantic analysis (LSA) and independent component analysis (ICA) to ex-
tract concept-level features for emotion label identification. LSA is used to discover latent concepts that 
do not frequently occur in psychiatric social texts, and ICA is used to extract independent components 
by minimizing the term dependence among the concepts. By combining LSA and ICA, more useful la-
tent concepts can be discovered for different emotion labels, and the dependence between them can also 
be minimized. The discriminant power of classifiers can thus be improved by training them on the inde-
pendent components with minimized term overlap. Experimental results show that the use of concept-
level features yielded better performance than the use of word-level features. Additionally, combining 
LSA and ICA improved the performance of using each LSA and ICA alone.  
1 Introduction 
Sentiment analysis has been successfully applied for many applications (Picard, 1997; Pang and Lee, 
2008; Calvo and D'Mello, 2010; Liu, 2012; Johansson and Moschitti, 2013; Balahur et al., 2014). 
Analysis of online psychiatric or mental health texts (Wu et al., 2005; Yu et al., 2009) is also an 
emerging field that could benefit from sentiment analysis techniques because more and more people 
search for help from the web when they suffered from depressive problems, which boost the develop-
ment of online community-based services for Internet users to share their depressive problems with 
other users and health professionals. Through these services, individuals can describe their depressive 
symptoms via web forums and blogs. Other users or health professionals can then make recommenda-
tions in response to these problems. Figure 1 shows an example psychiatric social text collected from 
PsychPark (http://www.psychpark.org), a virtual psychiatric clinic, maintained by a group of volunteer 
professionals belonging to the Taiwan Association of Mental Health Informatics (Bai et al., 2001; Lin 
et al., 2003). 
This example shows a subject?s depressive problems and the responses recommended by the experts. 
Some meaningful tags called emotion labels herein are also annotated by the experts to indicate which 
categories the text belongs to. These emotion labels are useful information and can make online psy-
chiatric services more effective. For instance, psychiatric retrieval systems are able to retrieve relevant 
documents according to the depressive problems (emotion labels) described in user queries so that the 
                                                 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
837
users can learn self-help knowledge from the responses. Therefore, this study aims to identify emotion 
labels from psychiatric social texts. We cast this problem into a multi-label text classification task be-
cause a psychiatric social text may contain multiple emotion labels. Additionally, we propose the use 
of concept-level features to build classifiers instead of using surface-level features such as words, n-
grams and dependency structure commonly used in the previous studies (Naughton et al., 2008; Chit-
turi and Hansen, 2008; Li and Zong, 2008; Kessler and Sch?tze, 2012; Post and Bergsma, 2013; Yu et 
al., 2011). 
In extraction of concept-level features, latent semantic analysis (LSA) (Landauer et al., 1998) has 
been demonstrated its effectiveness in exploring the latent structure from a collection of documents. It 
uses singular value decomposition (SVD) (Golub and Van Loan, 1996) to discover latent features that 
do not frequently occur in the documents through the indirect associations between words and docu-
ments. Figure 2 shows an example. The original matrix, as shown in Figure 2(a), is built using five 
documents with two different emotion labels Ei and Ej. Suppose that the words w1, w2 are the useful 
features for Ei, and w3, w4 are useful for Ej, but w4 is a latent feature because it does not frequently oc-
cur in the documents of Ej. After applying SVD, the latent features can be identified by replacing the 
zero entries in the original matrix with non-zero real values through the indirect associations between 
words and documents. For instance, w4 originally does not occur in d3 and d4, but it does co-occur with 
w3 in the matrix (e.g., in d5), which means that w4 might also occur in the documents where w3 occurs 
User Problem: 
I broke up with my dear but cruel boyfriend recently. 
Since then, I have often felt like crying out of nowhere, and I feel pain every day.      <Depression>
Also, it takes me a long time to fall asleep at night.                                  <Insomnia> 
So, I think that continuing to live like this is meaningless                                              <Suicide> 
Recommendation: 
Feeling this way is normal when going through these kinds of struggles, but over time your emo-
tions should level out. Suicide doesn't solve anything; think about how it would affect your family. 
It's only when we learn to face our despair that we can learn the value of life, and also how to help 
other people. There are a few things you can try to help you get to sleep at night, like doing some 
light exercise in the evening, drinking warm milk, and listening to relaxing music; all of these can 
be conducive to sleep. If you still have trouble dealing with the pain, and you feel as if your mood 
is getting worse, it wouldn't hurt to get seek help from a healthcare professional, who can help you 
work through your emotions. 
Emotion Label: <Depression>, <Insomnia>, <Suicide> 
Figure 1. Example of a psychiatric social text. 
 
1w
2w
3w
3 4 5      
jE
d d d
?????
1 2   
iE
d d
???
4w
3 4 5     
jE
d d d
?????
1 2   
iE
d d
???
 
Figure 2. Comparison of LSA and ICA for feature representation. 
838
(e.g., d3 and d4). Therefore, the zero entries (w4, d3) and (w4, d4) are replaced with a non-zero value 
through the indirect associations between of w3 and w4 in d5, as shown in Figure 2(b). This helps iden-
tify a useful latent feature w4 for Ej. However, identifying latent features through the indirect associa-
tions cannot avoid feature overlap when different emotion labels share common words. For instance, 
in Figure 2(a), w1, which is useful for Ei, still occurs in the document of Ej (e.g., d4). Through the indi-
rect associations between of w1 and w3 in d4, the frequency of w1 increases in the document of Ej be-
cause it may also occur in the documents where w3 occurs (e.g., d3 and d5), as shown in Figure 2(b). 
Therefore, when all word features are to be accommodated in a low-dimensional space reduced by 
SVD, term overlap may occur between the latent concepts. As indicated in Figure 2(c), the two sample 
latent concepts which contribute to two different emotion labels share a common feature w1. Classifi-
ers trained on such latent vectors with term overlap may decrease classification performance. 
To reduce the term overlap among concepts, we used the independent component analysis (ICA) 
(Lee, 1998; Hyv?rinen et al., 2001; Naik and Kumar, 2011) because it can extract independent com-
ponents from a mixture of signals and has been used in various text applications (Kolenda and Hansen, 
2000; Rapp, 2004; Honkela et al., 2010; Yu and Chien, 2013). For our task, the psychiatric social texts 
are a mixture of emotion labels, which can be separated by ICA to obtain a set of independent compo-
nents (concepts) with minimized term dependency for different emotion labels. Instead of using ICA 
alone, we propose a framework combining LSA and ICA for emotion label identification. The LSA is 
used to discover latent features that do not frequently occur in psychiatric texts, and ICA is used to 
further minimize the dependence of the latent features such that overlapped features can be removed, 
as presented in Figure 2(d). Based on this combination, the proposed framework can discover more 
useful latent features for different emotion labels, and the dependence between them can also be min-
imized. The discriminant power of classifiers can thus be improved by training them on the independ-
ent components with minimized term overlap. In experiments, we evaluate the proposed method to 
determine whether the use of concept-level features could improve the classification performance, and 
determine whether the combination method could improve the performance of using each LSA and 
ICA alone. 
 The rest of this paper is organized as follows. Section 2 describes the overall framework including 
LSA and ICA for emotion label identification. Section 3 summarizes comparative results. Conclusions 
are finally drawn in Section 4. 
LSA ICA
Psychiatric social texts
Concept Analysis
topic 1
topic 2
topic 3...
topic n
...
demixing matrix W
X
SVM testing
WX
Wd
test document d
demxing
demixing
topic 1
topic 2
SVM training
model
 
Figure 3. Framework of emotion label identification. 
839
2 Framework of Emotion Label Identification 
Figure 3 shows the overall framework for emotion label identification. A corpus of psychiatric social 
texts with annotation of emotion labels are first collected from the web. This corpus which is a mixture 
of different emotion labels is then sequentially analyzed by LSA and ICA to generate a demixing ma-
trix composed of a set of concepts with minimized term dependency for different emotion labels. The 
demixing matrix is used to separate the psychiatric social texts with mixed emotion labels into inde-
pendent components for building a support vector machine (SVM) classifier. The classifier can then 
be benefit from the independent components to identity multiple emotion labels contained in each test 
example. 
2.1 Latent Semantic Analysis (LSA)  
LSA is a technique for analyzing the relationships between words and documents. For our task, 
LSA is used to identify useful latent concepts for emotion labels through indirect associations between 
words and documents. The first step in LSA is to build a word-by-document matrix from a corpus of 
psychiatric texts with different emotion labels, as shown in the sample matrix X in Figure 4. 
The columns in Q D?X  represent D psychiatric texts in the corpus, and the rows represent Q distinct 
words occurring in the corpus. Singular value decomposition (SVD) is then used to decompose the 
matrix Q D?X  into three matrices as follows: 
,TQ D Q n n n n D? ? ? ?? ?X U V             (1) 
where U and V respectively consist of a set of latent vectors of words and documents, ?  is a diagonal 
matrix of singular values, and min( , )n Q D?  denotes the dimensionality of the latent semantic space. 
Each element in U represents the weight of a word, and the higher-weighted words are the useful fea-
tures for the emotion labels. By selecting the largest k1 ( n? ) singular values together with the first k1 
columns of U and V, the word and documents can be represented in a low-dimensional latent semantic 
space. The matrix Tn D?V  can then represented with the reduced dimensions, as shown in Eq. (2). 
1 1 1 1
1 ,T Tk D k k k Q Q D?? ? ? ?? ?V U X                         (2) 
In SVM training and testing, each input psychiatric text first transformed into the latent semantic rep-
resentation as follows: 
1 1 11
1
11 ,
? ?
? ? ?? ? ? Tk k k Q Qkt U t                          (3) 
XQ D? ? UQ n? ? n n?? ? VTn D?
1( )Q k?
1 1( )k k?
( )n D?
1d Dd
1w
Qw
wo
rds ?
( )n n?( )Q n?( )Q D?
1( )k D?
 
Figure 4. Illustrative example of singular value decomposition for latent semantic analysis. 
840
where 1Q?t  denotes the vector representation of an input instance, and 1 1k
?
?t  denotes the transformed 
vector in the latent semantic space. An SVM classifier is then trained with the transformed training 
vectors. 
2.2 Independent Component Analysis (ICA)  
ICA is a technique for extracting independent components from a mixture of signals and has been suc-
cessfully applied to solve the blind source separation problem (Saruwatari et al., 2006; Chien and 
Hsieh, 2012). The ICA model can be formally described as 
X AS?                                (4) 
where X denotes the observed mixture signals, A denotes a mixing matrix, and S denotes the inde-
pendent components. The goal of ICA is to estimate both A and S. Once the mixing matrix A is esti-
mated, the demixing matrix can be obtained by 1W A?? , and Eq. (4) can be re-written as 
S WX?                 (5) 
That is, the observed mixture signals can be separated into independent components using the demix-
ing matrix. For our problem, psychiatric texts can be considered as a mixture of signals because each 
of them may contain multiple emotion labels. Therefore, ICA used herein is to estimate the demixing 
matrix so that it can separate the psychiatric texts with mixed emotion labels to derive the independent 
components for each emotion label. Figure 5 shows the diagram of the proposed method. 
2.1.1  LSA decomposition and transformation 
In the training phase, the original matrix Q D?X  is first processed by SVD using Eq. (1) and (2) Useful 
latent features that do not frequently occur in the original matrix can thus be discovered in this step. 
2.1.2  ICA decomposition and demixing 
The matrix 1Tk D?V  decomposed by SVD is then passed to ICA to estimate the demixing matrix. ICA 
accomplishes this by decomposing 1Tk D?V  using Eq. (6). Figure 6 shows an example of the decomposi-
tion. 
LSA
T
Q D Q n n n n D1) decomposition: X U V? ? ? ?? ?
1 1 1 1
1T T
k D k k k Q Q D2) transformation: V U X?? ? ? ???
TV
1 1 2 2
T
k D k k k D1) decomposition: V A S? ? ??
2 2 1 1
T
k D k k k D2) demixing: S W V? ? ??
S
SVM training
1 TU??
transformation matrix
W
demixing matrix
training
1 1 1
1
1
T
k k k Q QU t? ? ? ??
Q DX ?
testing
1Qt ?
2 1 1 1 1
1
1
T
k k k k k Q QW U t?? ? ? ??
ICA
LSA transformation
ICA demixing
Model
emotion labels  Figure 5. ICA-based method for emotion label identification. 
841
1 1 2 2 .Tk D k k k D? ? ??V A S                           (6) 
Based on this decomposition, the demixing matrix can be obtained by 2 1 1 21k k k kW A?? ?? , where k2 
( 1k? ) is the number of independent components. The demixing matrix is then used to separate 1Tk D?V  
to derive the independent components as follows: 
2 2 1 1 ,Tk D k k k D? ? ??S W V                      (7) 
An SVM classifier is then trained with the independent components 2k D?S , as shown in Figure 5. In 
testing, each test instance 1Q?t  is transformed using both LSA and ICA, and then predicted with the 
trained SVM model. 
3 Experimental Results  
3.1 Experiment Setup 
3.1.1  Data 
The data set used for experiments included 1,711 Chinese psychiatric social texts collected from the 
PsychPark. Each psychiatric social text was manually annotated with an emotion label by a group of 
volunteer mental health professionals. Table 1 shows the proportions of the emotion labels in the cor-
pus. In calculating the proportion of each emotion label, a psychiatric social text was counted for mul-
tiple emotion labels depending on the number of emotion labels contained in it. In evaluation, 20% of 
psychiatric social texts in the corpus were randomly selected as a test set, and the remaining 80% were 
used for training. 
 
 
No. Emotion Label Proportion 
1 Depression 35.26% 
2 Drug 13.38% 
3 Insomnia 5.79% 
4 Mood 30.04% 
5 OCD (Obsessive compulsive disorder) 4.51% 
6 Schizophrenia 5.36% 
7 Social Anxiety 5.65% 
Table 1. Distribution of emotion labels in experimental data 
? 1 2Ak k? ? 2Sk D?
1 2( )k k?
( )n D?
1( )k n?
2( )k D?
1
VTk D?
1( )k D?
1d Dd
con
cep
ts ?
 
Figure 6. Illustrative example of ICA decomposition. 
842
3.1.2 Classifiers 
The classifiers involved in this experiment included PureSVM, LSA, ICA, and LSA+ICA. The 
PureSVM was trained on word-level features, and the others were trained on concept-level features 
derived using LSA, ICA, and combination of them, respectively. The implementation details for each 
classifier are as follows: 
? PureSVM: An SVM classifier trained with bag-of-words features. 
? LSA: An SVM classifier trained with the latent vectors obtained from the word-by-document 
matrix built from the training corpus. 
? ICA: An SVM classifier trained with the independent components obtained by demixing the 
word-by-document matrix built from the training corpus. 
? LSA+ICA: An SVM classifier trained with the independent components obtained by demixing 
the word-by-document matrix produced by LSA. 
To identify multiple emotion labels contained in test examples, each emotion label presented in Ta-
ble 1 was trained a binary classifier in the training phase. That is, for each method presented above, we 
built seven binary classifiers so that they can output multiple positive results to indicate that a test ex-
ample contained multiple emotion labels.  
3.1.3 Evaluation Metrics 
The metrics used for performance evaluation included recall, precision, and F-measure, respectively. 
Recall was defined as the number of emotion labels correctly identified by the method divided by the 
total number of emotion labels in the test set. Precision was defined as the number of emotion labels 
correctly identified by the method divided by the number of emotion labels identified by the method. 
The F-measure (F1) was defined as 2 recall precision? ? / (recall + precision). 
3.2 Evaluation of LSA and ICA 
This experiment compared the performance of LSA and ICA using different settings for the parame-
ters k1 and k2, which respectively represent the dimensionality of the latent semantic space and the 
 
200 400 600 800100 300 500 700 900
k
0.4
0.45
0.5
0.55
0.6
0.65
F-m
ea
su
re
LSA+ICA
ICA
LSA
 
Figure 7. Performance of the LSA, ICA and LSA+ICA, as a function of k. 
843
number of independent components. Figure 7 shows the F-measure of LSA, ICA, and combination of 
them with the setting 1 2k k k? ? . The F-measure is the average F-measure over the seven emotion la-
bels. The results show that the optimal settings of LSA was k=200. The performance of LSA dropped 
dramatically as k>200, indicating that most useful latent features were discovered within the first 200 
concepts and the remaining concepts may contain noisy features, thus reducing performance. The re-
spective optimal settings for ICA and LSA+ICA were k=900 and k=800. In addition, both ICA and 
LSA+ICA outperformed LSA for most settings of k. The best settings of the parameters were used in 
the following experiments. 
3.3 Comparative Results 
This section reports the classification performance of PureSVM, LSA, ICA, and LSA+ICA. Table 2 
shows the comparative results. Compared to the use of word-level features (i.e., PureSVM), LSA, ICA, 
and LSA+ICA achieved a higher F-measure. Additionally, LSA yielded a much greater recall than did 
PureSVM, whereas ICA yielded much greater precision. These findings indicate that the concept-level 
features are useful for emotion label identification. Among the three concept-based methods, LSA can 
discover latent concepts for emotion labels, whereas ICA can extract independent components that can 
minimize the term dependence within them. The results show that ICA yielded higher recall and F-
measure but lower precision than did LSA. By combining LSA and ICA, the performance was im-
proved on all measures because LSA+ICA can not only discover latent concepts but also minimize 
term overlap among the concepts. 
 Another observation is that the emotion label Depression yielded the highest F-measure while both 
OCD and Schizophrenia yielded the lowest. One possible reason for these results is the distribution of 
emotion labels in the test set (e.g., Depression and Mood are the major classes). However, the skewed 
distribution was just a minor factor. For example, the test set included four small classes (Insomnia, 
OCD, Schizophrenia and Social Anxiety) with similar proportions (5.79%, 4.51%, 5.36% and 5.65%), 
but their F-measures were quite different (70%, 57%, 57% and 64%). Terms overlap emotion labels 
could have a significant impact on classification performance. For example, Insomnia had a much 
higher classification performance than the other three minor classes because the words used in this 
class were quite distinct from those used for other classes. Conversely, the words used for OCD and 
Social Anxiety overlapped significantly, thus yielding lower performance. Table 3 shows some repre-
sentative words (with higher weights) in the independent components for the emotion labels. 
 
 
Class PureSVM LSA ICA LSA+ICA R P F R P F R P F R P F 
Depression 58 59 59 68 74 71 72 75 73 73 78 75
Drug 60 38 47 57 71 63 51 69 59 55 72 62
Insomnia 53 66 59 49 76 60 65 76 70 66 75 70
Mood 63 48 54 61 56 58 65 59 62 67 61 64
OCD 58 39 47 53 53 53 53 53 53 56 59 57
Schizophrenia 63 23 34 56 64 60 56 47 51 58 57 57
Social Anxiety 34 40 37 24 78 37 52 71 60 56 74 64
Avg. 56 45 48 53 67 57 59 64 61 62 68 64
Table 2. Performance for different classifiers. The columns R, P, and F represent recall, precision, and 
f-measure, respectively. (in %) 
844
3.4 Term Overlap Analysis 
In order to investigate the term overlap in LSA and LSA+ICA, we analyze their respective corre-
sponding matrices Q kU ?  and TQ kW ?  where TQ kW ?  is the transpose of the demixing matrix obtained with 
the input of Q D?X  reconstructed using LSA. Each column of Q kU ?  and TQ kW ?  represents a latent vec-
tor/independent component of Q words, and each element in the vector is a word weight representing 
its relevance to the corresponding latent vector/independent component. Figure 8 shows two sample 
latent vectors for LSA and two independent components for LSA+ICA, where the weights shown in 
this figure are the absolute values. 
The upper part of Fig. 8 shows parts of the words and their weights in the two latent vectors, where 
latent vector #1 can be characterized by depressed, depression, and sad which are the useful features 
for identifying the emotion label <Depression>, and latent vector #2 can be characterized by depressed, 
sad, and cry which are useful for identifying <Mood>. Although the two latent vectors contained use-
ful features for the respective emotion labels, these features still had some overlap between the latent 
vectors, as marked by the dashed rectangles. The overlapped features, especially those with higher 
weights, may reduce the classifier?s ability to distinguish between the emotion labels. The lower part 
of Fig. 8 also shows two independent components for the emotion labels <Depression> and<Mood>. 
As indicated, the term overlap between the two independent components was relatively low. Table 3 
shows some representative words (with higher weights) in the independent components for the emo-
tion labels. 
Figure 8. Examples of latent vectors, selected from Q kU ? , and independent components, selected 
from TQ kW ? , for the emotion labels <Depression> and <Mood>. 
845
 
Emotion Label Representative Words 
Depression depression, depressed, sad, down, suicide 
Drug Medication, drug, dose, sedative, withdrawal,  
Insomnia sleep, insomnia, dream, nightmare, awake 
Mood cry, upset, anxious, energy, obstacle 
OCD OCD, compulsion, weight, overeating, behavior  
Schizophrenia paranoia, fantasy, memory, split, genetic 
Social Anxiety crowd, tense, friend, stiffness, ridicule, shivering   
Table 3. Representative words for the emotion labels. 
 
4 Conclusions 
This work has presented a framework combining LSA and ICA for emotion label identification. Both 
LSA and ICA are used to analyze concept-level features, where LSA is used to discover latent con-
cepts that do not frequently occur in psychiatric texts, and ICA is used to further minimize the term 
dependence among the concepts. The experimental results show that the use of concept-level features 
yielded better performance than the use of word-level features. Additionally, ICA can reduce the de-
gree of term overlap of LSA so that combining LSA and ICA can discover more useful latent concepts 
with minimized term dependence for different emotion labels, thus improving classification perfor-
mance. Future work will focus on investigating the use of the machine-labeled emotion labels as meta-
information to improve online psychiatric services such as information retrieval for self-help 
knowledge recommendation. 
Acknowledgments 
This work was supported by the Ministry of Science and Technology, Taiwan, ROC, under Grant No. 
NSC102-2221-E-155-029-MY3. 
Reference 
Y. M. Bai, C. C. Lin, J. Y. Chen, and W.C. Liu. 2001. Virtual psychiatric clinics. American Journal of Psychia-
try, 158(7): 1160-1161. 
A. Balahur, R. Mihalcea, and A. Montoyo. 2014. Computational approaches to subjectivity and sentiment analy-
sis: Present and envisaged methods and applications. Computer Speech & Language 28(1): 1-6. 
R. A. Calvo and S. D'Mello. 2010. Affect Detection: An interdisciplinary review of models, methods, and their 
applications. IEEE Trans. Affective Computing, 1(1): 18-37. 
J. T. Chien and H. L.Hsieh. 2012. Convex divergence ICA for blind source separation. IEEE Trans. Audio, 
Speech, and Language Processing, 20(1): 302-313. 
R. Chitturi and J. H. L. Hansen. 2008. Dialect classification for online podcasts fusing acoustic and language 
based structural and semantic information. Proceedings of the 46th Annual Meeting of the Association for 
Computational Linguistics (ACL-08), pages 21-24. 
G. H. Golub and C. F. Van Loan. 1996. Matrix Computations, Third Edition, Johns Hopkins University Press, 
Baltimore, MD. 
T. Honkela, A. Hyv?rinen, and J. J. V?yrynen. 2010. WordICA ? emergence of linguistic representations for 
words by independent component analysis. Natural Language Engineering, 16(3): 277-308. 
846
A. Hyv?rinen, J. Karhunen, and E. Oja. 2001. Independent Component Analysis. Wiley, New York. 
R. Johansson and A. Moschitti. 2013. Relational features in fine-grained opinion analysis. Computational Lin-
guistics, 39(3): 473-509. 
W. Kessler and H. Sch?tze. 2012. Classification of inconsistent sentiment words using syntactic constructions. 
Proceedings of the 24th International Conference on Computational Linguistics (COLING-12), pages 569-
578. 
T. Kolenda and L. K. Hansen. 2000. Independent components in text. Advances in Neural Information Pro-
cessing Systems 13: 235-256. 
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An introduction to latent semantic analysis. Discourse Pro-
cesses, 25(2&3): 259-284. 
T. W. Lee. 1998. Independent Component Analysis?Theory and Applications. Kluwer, Norwell, MA. 
S. Li and C. Zong. 2008. Multi-domain Sentiment Classification. Proceedings of the 46th Annual Meeting of the 
Association for Computational Linguistics (ACL-08), pages 257-260. 
C. C. Lin, Y. M. Bai, and J. Y. Chen. 2003. Reliability of information provided by patients of a virtual psychiat-
ric clinic, Psychiatric Services, 54(8): 1167-1168. 
B. Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan & Claypool, Chicago, IL. 
G. R. Naik and D. K. Kumar. 2011. An overview of independent component analysis and its applications. Infor-
matica 35(1): 63-81. 
M. Naughton, N. Stokes, and J. Carthy. 2008. Investigating statistical techniques for sentence-level event classi-
fication. Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08), pag-
es 617-624. 
B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Re-
trieval, 2: 1-135. 
R. W. Picard. 1997. Affective Computing, MIT Press, Cambridge, MA. 
M. Post and S. Bergsma. 2013. Explicit and implicit syntactic features for text classification. Proceedings of the 
51st Annual Meeting of the Association for Computational Linguistics (ACL-13), pages 866?872. 
R. Rapp. 2004. Mining text for word senses using independent component analysis. Proceedings of the 4th SIAM 
International Conference on Data Mining (SDM-04), pages 422-426. 
H. Saruwatari, T. Kawamura, T. Nishikawa, A. Lee, and K. Shikano. 2006. Blind source separation based on a 
fast-convergence algorithm combining ICA and beamforming. IEEE Trans. Audio, Speech, and Language 
Processing, 14(2): 666-678. 
C. H. Wu, L. C. Yu, and F. L. Jang. 2005. Using semantic dependencies to mine depressive symptoms from con-
sultation records. IEEE Intelligent System, 20(6): 50-58. 
L. C. Yu, C. L. Chan, C. C. Lin, and I. C. Lin. 2011. Mining association language patterns using a distributional 
semantic model for negative life event classification. Journal of Biomedical Informatics, 44(4): 509-518. 
L. C. Yu and W N. Chien. 2013. Independent component analysis for near-synonym choice. Decision Support 
Systems, 55(1): 146-155. 
L. C. Yu, C. H. Wu, and F. L. Jang. 2009. Psychiatric document retrieval using a discourse-aware model. Artifi-
cial Intelligence, 173(7-8): 817-829. 
 
847
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 67?70, Dublin, Ireland, August 23-29 2014.
 A Sentence Judgment System for Grammatical Error Detection 
 
Lung-Hao Lee 1,2, Liang-Chih Yu3,4, Kuei-Ching Lee1,2,  
Yuen-Hsien Tseng1, Li-Ping Chang5, Hsin-Hsi Chen2 
1Information Technology Center, National Taiwan Normal University 
2Dept. of Computer Science and Information Engineering, National Taiwan University 
3Dept. of Information Management, Yuen Ze University 
4Innovation Center for Big Data and Digital Convergence, Yuen Ze University 
5Mandarin Training Center, National Taiwan Normal University 
lcyu@saturn.yzu.edu.tw, {lhlee, johnlee, lchang, 
samtseng}@ntnu.edu.tw, hhchen@ntu.edu.tw 
  
 
Abstract 
This study develops a sentence judgment system using both rule-based and n-gram statistical 
methods to detect grammatical errors in Chinese sentences. The rule-based method provides 
142 rules developed by linguistic experts to identify potential rule violations in input sentences. 
The n-gram statistical method relies on the n-gram scores of both correct and incorrect training 
sentences to determine the correctness of the input sentences, providing learners with im-
proved understanding of linguistic rules and n-gram frequencies. 
1 Introduction 
China?s growing global influence has prompted a surge of interest in learning Chinese as a foreign 
language (CFL), and this trend is expected to continue. This has driven an increase in demand for au-
tomated IT-based tools designed to assist CFL learners in mastering the language, including so-called 
MOOCs (Massive Open Online Courses) which allows huge numbers of learners to simultaneously 
access instructional opportunities and resources. This, in turn, has driven demand for automatic proof-
reading techniques to help instructors review and respond to the large volume of assignments and tests 
submitted by enrolled learners. 
However, whereas many computer-assisted learning tools have been developed for use by students 
of English as a Foreign Language (EFL), support for CFL learners is relatively sparse, especially in 
terms of tools designed to automatically detect and correct Chinese grammatical errors. For example, 
while Microsoft Word has integrated robust English spelling and grammar checking functions for 
years, such tools for Chinese are still quite primitive. In contrast to the plethora of research related to 
EFL learning, relatively few studies have focused on grammar checking for CFL learners. Wu et al. 
(2010) proposed relative position and parse template language models to detect Chinese errors written 
by US learner. Yu and Chen (2012) proposed a classifier to detect word-ordering errors in Chinese 
sentences from the HSK dynamic composition corpus. Chang et al. (2012) proposed a penalized prob-
abilistic First-Order Inductive Learning (pFOIL) algorithm for error diagnosis. In summary, although 
there are many approaches and tools to help EFL learners, the research problem described above for 
CFL learning is still under-explored. In addition, no common platform is available to compare differ-
ent approaches and to promote the study of this important issue. 
This study develops a sentence judgment system using both rule-based and n-gram statistical meth-
ods to detect grammatical errors in sentences written by CFL learners. Learners can input Chinese sen-
tences into the proposed system to check for possible grammatical errors. The rule-based method uses 
a set of rules developed by linguistic experts to identify potential rule violations in input sentences. 
 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
67
The n-gr
tences to
proved u
can also 
assignme
2 A S
Figure 1
http://sjf
shown in
part-of-s
grammat
ods dete
informat
ence, as 
?
( I
The ru
(towards
frequenc
detail the
2.1 Pr
Chinese 
Languag
nese wo
usually s
corpus-b
Ma, 200
ed word
????
?POS:W
words, t
lexicon a
POS tag 
 
am statistica
 determine 
nderstandin
be incorpora
nts and test
entence Ju
 shows the
.itc.ntnu.edu
 the upper 
peech taggi
ical error de
ct grammatic
ion, the exp
shown in the
     ?     ??
      from   he
le-based me
)) cannot be
y of the big
 pre-process
e-processin
is written w
e Processing
rd segmente
uffers from 
ased learnin
2). This is fo
s with parts-
??? (Ob
ord? sequen
he translatio
nd therefore
?SHI? is a ta
l method re
the correctn
g of both lin
ted into onl
s. 
dgement S
 user interf
.tw/demo/. L
part of Fig. 
ng, and then
tection. Fina
al errors. O
lanation of t
 bottom par
    ?     ?
re    go   tow
thod shows
 used after a
ram ?? ?
ing, rule-ba
g 
ithout word 
 (NLP) task
rs are gener
the unknow
g method is 
llowed by a
of-speech (T
ama is the p
ce  shown 
n of a forei
 is extracted
g to represen
Figure 1.
lies on the n
ess of the in
guistic rules 
ine CFL MO
ystem 
ace of the 
earners can
1. Each inp
 passed to 
lly, an inpu
therwise, it w
he matched 
t of Fig. 1. F
         ? 
ads  north. )
a rule violat
 verb (e.g., ?
? (go toward
sed method, 
boundaries.
s, texts mus
ally trained 
n word (i.e.,
used to merg
 reliable and
sai and Che
resident of 
as follows: 
gn proper na
 by the unk
t the be-ver
Screenshot 
-gram scor
put sentence
and n-gram 
OC platform
sentence ju
 submit sing
ut sentence 
both the ru
t sentence w
ill be mark
rules and n-g
or instance, 
 
ion is detec
?? (go)). T
s) is relativ
and n-gram 
 As a result,
t undergo au
by an input 
 the out-of-v
e unknown
 cost-effecti
n, 2004). F
the USA). 
 Nb:???
me ????
nown word 
b ???. 
of the senten
es of both c
s. The syste
frequencies
s to help as
dgment sys
le or multip
is pre-proce
le-based an
ill be marke
ed as correc
ram frequen
the followin
ted and expl
he n-gram fr
e low. The f
statistical m
 prior to the
tomatic wo
lexicon and
ocabulary, o
 words to tac
ve POS-tagg
or example, 
It was segm
  SHI:?  N
? (Obama) 
detection me
ce judgemen
orrect and in
m helps lea
. In addition,
sess and/or 
tem, which 
le sentences
ssed for wo
d n-gram st
d as incorre
t ( ). In ad
cies are als
g sentence i
ains that a p
equencies al
ollowing su
ethod. 
 implementa
rd segmenta
 probability 
r OOV) pro
kle the OOV
ing method 
take the Ch
ented and ta
c:??  Na
is not likely
chanism. In
t system. 
correct train
rners develo
 the propose
score the nu
can be acc
 through th
rd segmenta
atistical met
ct ( ) if bo
dition to the
o presented 
s marked as 
reposition (e
so shows th
bsections de
tion of mos
tion. Autom
models. Ho
blem. In this
 problem (C
to label the 
inese senten
gged in the
:??. Amo
 to be inclu
 this case, th
ing sen-
p an im-
d system 
mbers of 
essed at 
e textbox 
tion and 
hods for 
th meth-
 decision 
for refer-
incorrect: 
.g., ??? 
at the the 
scribe in 
t Natural 
atic Chi-
wever, it 
 study, a 
hen and 
segment-
ce ???
 form of  
ng these 
ded in a 
e special 
 
68
2.2 Rule-based Linguistic Analysis 
Several symbols are used to represent the syntactic rules to facilitate the detection of errors embedded 
in Chinese sentences written by CFL learners: (1) ?*? is a wild card, with ?Nh*? denoting all subordi-
nate tags of ?Nh?, e.g., ?Nhaa,? ?Nhab,? ?Nhac,? ?Nhb,? and ?Nhc?. (2) ?-? means an exclusion from 
the previous representation, with ?N*-Nab-Nbc? indicating that the corresponding word should be any 
noun (N*) excluding countable entity nouns (Nab) and surnames (Nbc). (3) ?/? means an alternative 
(i.e., ?or?), where the expression ???/??/??? (some/these/those) indicates that one of these 
three words satisfies the rule. (4) The rule mx{W1 W2} denotes the mutual exclusivity of the two 
words W1 and W2. (5) ?<? denotes the follow-by condition, where the expression ?Nhb  <  Nep? 
means the POS-tag ?Nep? follows the tag ?Nhb? that can exist several words ahead of the ?Nep?. 
Using such rule symbols, we manually constructed syntactic rules to cover errors that frequently oc-
cur in sentences written by CFL learners. We adopted the ?Analysis of 900 Common Erroneous Sam-
ples of Chinese Sentences? (Cheng, 1997) as the development set to handcraft the linguistic rules with 
syntactic information. If an input sentence satisfies any syntactic rule, the system will report the input 
as suspected of containing grammatical errors, creating a useful tool for autonomous CFL learners.  
2.3 N-gram Statistical Analysis 
Language modeling approaches to grammatical error detection are usually based on a score (log prob-
ability) output by an n-gram model trained on a large corpus. A sentence with grammatical errors usu-
ally has a low n-gram score. However, choosing an appropriate threshold to determine whether a sen-
tence is correct is still a nontrivial task. Therefore, this study proposes the use of n-gram scores of cor-
rect and incorrect sentences to build the respective correct and incorrect statistical models for gram-
matical error detection. That is, a given sentence is denoted as incorrect (i.e., having grammatical er-
rors) if its probability score output by the statistical model of incorrect sentences (i.e., the incorrect 
model) is greater than that of correct sentences (i.e., the correct model).  
To build the incorrect and correct statistical models, a total of 19,080 sentences with grammatical 
errors were extracted from the HSK dynamic composition corpus. These sentences were then manual-
ly corrected. An n-gram (n= 2 and 3) language model was then built from the Sinica corpus released 
by the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) using 
the SRILM toolkit (Stolcke, 2002). The trained language model was used to assign an n?gram score 
for each correct and incorrect sentence, which were then used to build the respective correct and incor-
rect models based on a normal probability density function (Manning and Sch?tze, 1999). Both mod-
els can then be used to evaluate each test sentence by transforming its n-gram score into a probability 
score to determine whether the sentence is correct or not. 
3 Performance Evaluation 
The test set included 880 sentences with grammatical errors generated by CSL learners in the NCKU 
Chinese Language Center, and the corresponding 880 manually corrected sentences. For the rule-
based approach, a total of 142 rules were developed to identify incorrect sentences. For the n-gram 
statistical approach, both bi-gram and tri-gram language models were used for the correct and incor-
rect statistical models. In addition to precision, recall, and F1, the false positive rate (FPR) was defined 
as the number of correct sentences incorrectly identified as incorrect sentences divided by the total 
number of correct sentences in the test set. 
Table 1 shows the comparative results of the rule-based and n-gram statistical approaches to gram-
matical error detection. The results show that the rule-based approach achieved high precision, low 
recall and low FPR. Conversely, the n-gram-based approach yielded low precision, high recall and 
high FPR. In addition, the tri-gram model outperformed the bi-gram model for all metrics. Given the 
different results yielded by the rule-based and n-gram statistical approaches, we present different com-
binations of these two methods for comparison. The ?OR? combination means that a given sentence is 
identified as incorrect by only one of the methods, while the ?AND? combination means that a given 
sentence is identified as incorrect by both methods. The results show that the ?OR? combination yield-
ed better recall than the individual methods, and the ?AND? combination yielded better precision and 
FPR than the individual methods. Thus, the choice of methods may depend on application require-
ments or preferences 
69
Method Precision Recall F1 False Positive Rate 
Rule 0.857 0.224 0.356 0.038 
2-gram 0.555 0.751 0.638 0.603 
3-gram 0.585 0.838 0.689 0.595 
Rule OR 2-gram 0.500 1.000 0.667 1.000 
Rule OR 3-gram 0.502 1.000 0.668 0.993 
Rule AND 2-gram 0.924 0.083 0.153 0.007 
Rule AND 3-gram 0.924 0.083 0.153 0.007 
Table 1. Comparative results of the rule-based and n-gram statistical approaches. 
 
Many learner corpora exist for EFL for use in machine learning, including the International Corpus 
of Learner English (ICLE) and Cambridge Learner Corpus (CLC). But collecting a representative 
sample of authentic errors from CFL learners poses a challenge. In addition, English and Chinese 
grammars are markedly different. In contrast to syntax-oriented English language, Chinese is dis-
course-oriented, with meaning often expressed in several clauses to make a complete sentence. These 
characteristics make syntactic parsing difficult, due to long dependency between words in a clause or 
across clauses in a sentence. These difficulties constrain system performance.  
4 Conclusions  
This study presents a sentence judgment system developed using both rule-based and n-gram statisti-
cal methods to detect grammatical errors in sentences written by CFL learners. The system not only 
alerts learners to potential grammatical errors in their input sentences, but also helps them learn about 
linguistic rules and n-gram frequencies. The major contributions of this work include: (a) demonstrat-
ingg the feasibility of detecting grammatical errors in sentences written by CFL learners, (b) develop-
ing a system to facilitate autonomous learning among CFL learners and (c) collecting real grammatical 
errors  from CFL learners for the construction of a Chinese learner corpus. 
Acknowledgments 
This research was partially supported by Ministry of Science and Technology, Taiwan under the grant 
NSC102-2221-E-155-029-MY3, NSC 102-2221-E-002-103-MY3, and the "Aim for the Top Universi-
ty Project" sponsored by the Ministry of Education, Taiwan.  
Reference 
Andreas Stolcke. 2002. SRILM ? An extensible language modeling toolkit. Proceedings of ICSLP?02, pages 
901-904. 
Chi-Hsin Yu and Hsin-Hsi Chen. 2012. Detecting word ordering errors in Chinese sentences for learning Chi-
nese as a foreign language. Proceedings of COLING?12, pages 3003-3018. 
Christopher D. Manning and Hinrich Sch?tze. 1999. Foundations of Statistical Natural Language Processing. 
MIT Press. Cambridge, MA.  
Chung-Hsien Wu, Chao-Hung Liu, Matthew Harris and Liang-Chih Yu. 2010. Sentence correction incorporating 
relative position and parse template language model. IEEE Transactions on Audio, Speech, and Language 
Processing, 18(6):1170-1181. 
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word extraction for Chinese documents. Proceedings of 
COLING?02, pages 169-175. 
M. Cheng. 1997. Analysis of 900 Common Erroneous Samples of Chinese Sentences - for Chinese Learners 
from English Speaking Countries (in Chinese). Beijing, CN: Sinolingua. 
Ru-Ying Chang, Chung-Hsien Wu, and Philips K. Prasetyo. 2012. Error diagnosis of Chinese sentences using 
inductive learning algorithm and decomposition-based testing mechanism. ACM Transactions on Asian Lan-
guage Information Processing, 11(1):Article 3. 
Yu-Fang Tsai and Keh-Jiann Chen. 2004. Reliable and cost-effective pos-tagging. International Journal of 
Computational Linguistics and Chinese Language Processing, 9(1):83-96. 
70
