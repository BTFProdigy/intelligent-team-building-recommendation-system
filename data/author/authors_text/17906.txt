Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 139?144,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linggle: a Web-scale Linguistic Search Engine for Words in Context    Joanne Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu, Taiwan, R.O.C. 30013 {joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang} @gmail.com     Abstract 
In this paper, we introduce a Web-scale lin-guistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and ad-ditional regular expression (RE) operators. In our approach, we incorporate inverted file in-dexing, PoS information from BNC, and se-mantic indexing based on Latent Dirichlet Al-location with Google Web 1T. The method in-volves parsing the query to transforming it in-to several keyword retrieval commands. Word chunks are retrieved with counts, further filter-ing the chunks with the query as a RE, and fi-nally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided.  In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehen-sive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major lan-guages in the world. 1 Introduction As a non-native speaker writing in English, one encounters many problems. Doubts concerning the usage of a preposition, the mandatory presen-ce of a determiner, the correctness of the associa-tion of a verb with an object, or the need for syn-onyms of a term in a given context are issues that arise frequently. Printed collocation dictionaries and reference tools based on compiled corpora offer limited coverage of word usage while knowledge of collocations is vital to acquire a 
good level of linguistic competency. We propose to address these limitations with a comprehen-sive system aimed at helping the learners ?know a word by the company it keeps? (Firth, 1957). Linggle (linggle.com). The system based on Web-scaled datasets is designed to be a broad coverage language reference tool for English Second Language learners (ESL). It is conceived to search information related to word usage in context under various conditions. First, we build an inverted file index for the Google Web 1T n-grams to support queries with RE-like patterns including PoS and synonym matches. For example, for the query ?$V $D +important role?, Linggle retrieves 4-grams that start with a verb and a determiner followed by a synonym of important and the keyword role (e.g., play a significant role 202,800). A natural lan-guage interface is also available for users who are less familiar with pattern-based searches. For example, the question ?How can I describe a beach?? would retrieve two word chunks such as ?sandy beach 413,300? and ?rocky beach 16,800?. The n-gram search implementation is achieved through filtering, re-indexing, populat-ing an HBase database with the Web 1T n-grams and augmenting them with the most frequent PoS for words (without disambiguation) derived from the British National Corpus (BNC).   The n-grams returned for a query can then be linked to examples extracted from the New York Times Corpus (Sandhaus, 2008) in order to provide full sentential context for more effective learning.  In some situations, the user might need to search for words in a specific syntactic relation (e.g., Verb-Object collocation). The query absorb $N in n-grams display mode returns all the nouns that follow the verb ordered by decreasing n-gram counts. Some of these nouns might not be objects of the verb absorb. In contrast, the same 
139
query in cluster display mode will control that two words have been labeled verb-object by a parser. Moreover, n-grams grouped by object topic/domain give the learner an overview of the usage of the verb. For example the verb absorb takes clusters of objects related to the topics liq-uid, energy, money, knowledge, and population.  
  Figure 1. An example Linggle search for the que-ry ?absorb $N.?  This tendency of predicates to prefer certain classes of arguments is defined by Wilks (1978) as selectional preferences and widely reported in the literature. Erk and Pad? (2010) extend exper-iments on selectional preference induction to in-verse selectional preference, considering the re-striction imposed on predicates. Inverse sectional preference is also implemented in linggle (e.g. ?$V apple?). Linggle presents clusters of synonymous col-locates (adjectives, nouns and verbs) of a query keyword. We obtained the clusters by building on Lin and Pantel?s (2002) large-scale repository of dependencies and word similarity scores. Us-ing the method proposed by Ritter and Etzioni (2010) we induce selectional preference with a Latent Dirichlet Allocation (LDA) model to seed the clusters. The rest of the paper is organized as follows. We review the related work in the next section. Then we present the syntax of the queries and the functionalities of the system (Section 3). We de-scribe the details of implementation including the indexing of the n-grams and the clustering algo-rithm (Section 4) and draw perspective of devel-opment of Web scale search engines (Section 5). 2 Related work Web-scale Linguistic Search Engine (LSE) has been an area of active research. Recently, the state-of-the-art in LSE research has been re-
viewed in Fletcher (2012). We present in this paper a linguistic search engine that provides a more comprehensive and powerful set of query features.  Kilgarriff et al (2001) describe the implemen-tation of the linguistic search engine Word Sketch (2001) that displays collocations and de-pendencies acquired from a large corpus such as the BNC. Word Sketch is not as flexible as typi-cal search engines, only supporting a fixed set of queries.  Recently, researchers have been attempting to go one step further and work with Web scale da-tasets, but it is difficult for an academic institute to crawl a dataset that is on par with the datasets built by search engine companies. In 2006, Google released the Web 1T for several major languages of the world (trillion-word n-gram da-tasets for English, Japanese, Chinese, and ten European languages), to stimulate NLP research in many areas.  In 2008, Chang described a pro-totype that enhances Google Web 1T bigrams with PoS tags and supports search in the dataset by wildcards (wild-PoS), to identify recurring collocations. Wu, Witten and Franken (2010) describe a more comprehensive system (FLAX) that combines filtered Google data with text ex-amples from the BNC for several learning activi-ties.  In a way similar to Chang (2008) and Wu, Witten and Franken (2010), Stein, Potthast, and Trenkmann (2010) describe the implementation and application of NetSpeak, a system that pro-vides quick access to the Google Web 1T n-gram with RE-like queries (alternator ?|?, one arbitrary word ?*?, arbitrary number of words between two specified words ???). In contrast to Linggle, NetSpeak does not support PoS wildcard or con-ceptual clustering. An important function in both Linggle and NetSpeak is synonym query. NetSpeak uses WordNet (Fellbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synonyms, leading to poor coverage. Alternatively, one can use the distribu-tional approach to similarity based on a very large corpus. Lin and Pantel (2002) report efforts to build a large repository of dependencies ex-tracted from large corpora such as Wikipedia, and provide similarity between words (demo.patrickpantel.com). We use these results both for handling synonym queries and to or-ganize the n-grams into semantic classes.  More recently, Ritter and Etzioni (2010) pro-pose to apply an LDA model (Blei et al 2003) to 
140
the problem of inducing selectional preference. The idea is to consider the verbs in a corpus as the documents of a traditional LDA model. The arguments of the verb that are encountered in the corpus are treated as the words composing a document in the traditional model. The model seems to successfully infer the semantic classes that correspond to the preferred arguments of a verb. The topics are semi-automatically labeled with WordNet classes to produce a repository of human interpretable class-based selectional pref-erence. This choice might be due to the fact that if most LDA topic heads are usually reasonable upon human inspection, some topics are also in-coherent (Newman 2010) and lower frequency words are not handled as successfully. We con-trol the coherence of the topics and rearrange them into human interpretable clusters using a distributional similarity measure.  Microsoft Sempute Project (Sempute Team 2013) also explores core technologies and appli-cations of semantic computing. As part of Sempute project, NeedleSeek is aimed at auto-matically extracting data to support general se-mantic Web searches. While Linggle focuses on n-gram information for language learning, NeedleSeek also uses LDA to support question answering (e.g., What were the Capitals of an-cient China?) . In contrast to the previous research in Web scale linguistic search engines, we present a sys-tem that supports queries with keywords, wild-card words, POS, synonyms, and additional regular expression (RE) operators and displays the results according the count, similarity, and topic with clusters of synonyms or conceptually related words. We exploit and combine the power of both LDA analysis and distributional similarity to provide meaningful semantic classes that are constrained with members of high simi-larity. Distributional similarity (Lin 1998) and LDA topics become two angles of attack to view language usage and corpus patterns. 3 Linggle Functionalities The syntax of Linggle queries involves basic regular expression of keywords enriched with wildcard PoS and synonyms. Linggle queries can be either pattern-based commands or natural lan-guage questions. The natural language queries are currently handled by simple string matching based on a limited set of questions and command pairs provided by a native speaker informant.  
3.1 Natural language queries The handling of queries formulated in natural language has been implemented with handcrafted patterns refined from a corpus of questions found on various websites. Additionally, we asked both native and non-native speakers to use the system for text edition and to write down all the ques-tions that arise during the exercise.  Linggle transforms a question into commands for further processing based on a set of canned texts (e.g., ?How to describe a beach?? will be converted to ?$A beach?). We are in the process of gathering more examples of language-related question and answer pairs from Answers.com to improve the precision, versatility, and coverage. 3.2 Syntax of queries The syntax of the patterns for n-grams is shown in Table 1. The syntax supports two types of que-ry functions: basic keyword search with regular expression capability and semantic search.  Basic search operators enable the users to que-ry zero, one or more arbitrary words up to five words. For example, the query ?set off ? $N? is intended to search for all nouns in the right con-text of set off, within a maximum distance of three words.  In addition, the ??? operator in front of a word represents a search for n-grams with or without the word. For example, a user wanting to deter-mine whether to use the word to between listen and music can formulate the query ?listen ?to music.? Yet another operation ?|? is provided to search for information related to word choice. For ex-ample the query ?build | construct ... dream? can be used to reveal that people build a dream much more often than they construct a dream. A set of PoS symbols (shown in Table 2) is defined to support queries that need more preci-sion than the symbol *. More work might be needed to resolve PoS ambiguity for n-grams. Currently, any word that has been labeled with the requested PoS in the BNC more than 5% of the time is displayed.  The ?+? operator is provided to support se-mantic queries. Placed in front of a word, it is intended to search for synonyms in the context. For example the query ?+sandy beach? would generate rocky beach, stony beach, barren beach in the top three results. The query ?+abandoned beach? generates deserted, destroyed and empty beach at the top of the list. To support conceptual clustering of collocational n-grams, we need to 
141
identify synonyms related to different senses of a given word. Table 3 shows an example of the result obtained for the ambiguous word bank as a unigram query. We can see the two main senses of the word (river bank and institution) as clus-ters.  Operators  Description * Any Word ? With/without the word ? Zero or more words | Alternator $ Part of speech + Synonyms Table 1: Operators in the Linggle queries   Part of speech  Description N Noun V Verb A Adjective R Adverb PP Preposition NP Proper Noun PR Pronoun D Determiner Table 2: Part-of-speech in the Linggle queries   A cluster button on the interface activates or cancels conceptual clustering. When Linggle is switched into a cluster display mode, adjective-nouns, verb-objects and subject-verb relations can be browsed based on the induced conceptual clusters (see Figure 1). The New York Times Example Base In order to display complete sentence examples for users, the New York Times Corpus sentences are indexed by word. When the user searches for words in a specific syntactic relation, morpho-logical query expansion is performed and pat-terns are used to increase both the coverage and the precision of the provided examples. For ex-ample, the bi-gram kill bacteria will be associat-ed with the example sentence ?The bacteria are killed by high temperatures.?. 3.3 Semantic Clusters Two types of semantic clusters are provided in Linggle: selectional preference and clusters of synonyms. Selectional preference expresses for example that an apple is more likely to be eaten or cooked than to be killed or hanged. Different classes of arguments for a predicate (or of predi-cates for an argument) can be found automatical-ly. The favorite class of objects for the verb drink 
is LIQUID with the noun water ranked at the top. Less frequent objects belonging to the same class include liquor in the tail of the list. We aim at grouping arguments and predicates into semantic clusters for better readability.  valley mountain river lake hill bay plain north ridge coast city district town area community municipality country village land region route highway road railway bridge crossing canal railroad junction stream creek tributary  organization business institution company industry organisation agency school department university government court board channel network affiliate outlet supplier manufacturer distributor vendor retailer in-vestor broker provider lender owner creditor share-holder customer employer Table 3: First two level-one clusters of synonyms for the word ?bank? We produce clusters with a two-layer structure. Level one represents loose topical relatedness roughly corresponding to broad domains, while level two is aimed at grouping together closely similar words. For example, among the objects of the verb cultivate, the nouns tie and contact belong to the same level-two cluster. Attitude and spirit belong to another level-two cluster but both pairs are in the same level-one cluster. The nouns fruit and vegetable are clustered together in another level-one cluster. This double-layer representation is a solution to express at once close synonymy and topic relatedness. The clus-ters of symonyms displayed in Table 3 follow the same representation. 4 Implementation of the system In this section, we describe the implementation of Linggle, including how to index and store n-grams for a fast access (Section 4.1) and construction of the LDA models (Section 4.2). We will describe the clustering method in more details in section 5.  4.1 N-grams preprocessing The n-grams are first filtered keeping only the words that are in WordNet and in the British Na-tional Corpus, and then indexed by word and position in the n-gram, in a way similar to the rotated n-gram approach proposed by Lin et. al. (2010). The files are then stored in an Apache 
142
HBase NoSQL base. The major advantages of using a NoSQL database is the excellent perfor-mance in querying the ability of storing large amounts of data across several servers and the capability to scale up when we have additional entries in the dataset, or additional datasets to add to the system. 4.2 LDA models computations Two types of LDA models are calculated for Linggle. The first type is a selectional preference model between heads and modifiers. Six models are calculated in total for the subject-verb, the verb-object and the adjective-noun relations done in a similar way to Ritter and Etzioni?s (2010) model with binary relations instead of triples. The second is a word/synonyms model in which a word is considered as a document in LDA and its synonyms as the words of the document. This second model has the effect of splitting the syno-nyms of a word into different topics, as shown in Table 3.  Seeds                                             parameter: s1 1. Consider the m first topics for a verb v ac-cording to the LDA per document-topic dis-tribution (?) 2. Consider S = o1,?,on, a set of n objects of v.  3. Split S into m classes C1,..,Cm according to their LDA per topic-word probability: oi  is assigned to the topic in which it has the highest probability. 4. For each class Ci, move every object oj that is not similar to any other ok of Ci , according to a similarity threshold s1 into a new created class. Level 2                                           parameter: s2  While (Argmaxci ,cj Sim( ci , cj ) > s2):            Merge Argmaxci ,cj Sim( ci , cj ) into one class. Level 1                                           parameter: s3  While (Argmaxci ,cj Sim(ci , cj ) > s3):            Group Argmaxci ,cj Sim( ci ,cj ) under the             same level 1 cluster. Table 4:  Clustering Algorithm for the object of a giv-en verb  The hyperparameters alpha, eta, that affect the sparsity of the document-topic (theta) and the topic-word (lambda) distributions are both set to 0.5 and the number of topics is set to 300. More research would be necessary to optimize the val-ue for the parameters in the perspective of the clustering algorithm, as quickly discussed in the next section.  
 Sim (ci, cj): 1. Build the Cartesian product C = ci ? cj 2. Get P the set of the similarity between all word pairs in C 3. Return Sim(ci,cj) the mean of the scores in P  Table 5:  Similarity between two classes ti and tj 5 Clustering algorithm The clustering algorithm combines topic model-ing results and a semantic similarity measure. We use Pantel?s dependencies repository to compute LDA models for subject-verbs, verbs-objects and adjective-nouns relations in both di-rections. Currently, we also use Pantel?s similari-ty measure. It has a reasonable precision partly because it relies on parser information instead of bag of words windows. However the coverage of the available scores is lower than what would be needed for Linggle. We will address this issue in the near future by extending it with similarity scores computed from the n-grams. We combine the two distributional semantics approaches in a simple manner inspired by clus-tering by committee algorithm  (CBC). The simi-larity measure is used to refine the LDA topics and to generate finer grain clusters. Conversely, LDA topics can also be seen as the seeds of our clustering algorithm. This algorithm intends to constrain the words that belong to a final cluster more strictly than LDA does in order to obtain clearly interpretable clusters. The exact same algorithm is applied to synonym models, for synonyms of nouns, adjec-tives and verbs (shown in Table 3). Table 4 shows the algorithm for constructing double layer clusters for a set S of objects of a verb v. The objects are first roughly split into classes, attributing a single topic to every object oi. The topic of a word oi is determined accord-ing to its per topic-word probability. More exper-iments could be done using the product of the per document-topic and the per topic-word LDA probabilities instead, in order to take into account the specific verb when assigning a topic to the object. Such a way of assigning topics should also be more sensitive to the LDA hyperparame-ters.  At this stage, some classes are incoherent and that low frequency words that do not appear in the head of any topic are often misclassified. Words are rearranged between the classes and create new classes if necessary using the simi-larity measure. If any word of a class is not simi-
143
lar to any other word in this class (the threshold is set to s1 = 0.09), a new class is created for it. Any two classes are then merged if their simi-larity (computer accordingly to Table 5) is above s2=0.06, forming the level 2 clusters. Classes are then grouped together if the similarity between them is above s3 = 0.02 forming the level 1 clus-ters. Finally, the classes that contain less than three words are not displayed in Linggle and the predi-cate-arguments counts in the Web 1T are re-trieved using a few hand crafted RE and morpho-logical expansion of the nouns and the verbs.  This algorithm appears to generate interpreta-ble semantic classes and to be quite robust re-garding the threshold parameters. More tests and rigorous evaluation are left to future work.   6 Conclusion There are many different directions in which Linggle will be improved. The first one is to al-low users to work with word forms and with multiword expressions. The second one concerns the extension of the coverage of the example base with several large corpora such as Wikipe-dia and the extension of the coverage of the simi-larity measure. The third direction concerns the development of automatic suggestions for text edition, such as suggesting a better adjective or a different preposition in the context of a sentence. Finally, Linggle is currently being extended to Chinese. We presented a prototype that gives access to Web Scale collocations. Linggle displays both word usage and word similarity information. Depending on the type of the input query, the results are displayed under the form of lists or clusters of n-grams. The system is designed to become a multilingual platform for text edition and can also become a valuable resource for natural language processing research. References  David Blei, A. Ng, and M. Jordan. 2003. Latent Di-richlet alocation. Journal of Machine Learning Research, 3:993?1022, January 2003. Jason S. Chang, 2008. Linggle: a web-scale language reference search engine. Unpublished manuscript. Katrin Erk and Sebastian Pad?. 2010. A Flexible, Corpus-Driven Model of Regular and Inverse Se-lectional Preferences. In Proceedings of ACL 2010.  Christiane Fellbaum. 2010. WordNet. MIT Press, Cambridge, MA. 
John Rupert Firth. 1957. The Semantics of Linguistics Science. Papers in linguistics 1934-1951. London: Oxford University Press. William H Fletcher. 2012. Corpus analysis of the world wide web." In The Encyclopedia of Applied Linguistics. Adam Kilgarriff , and David Tugwell. 2001. Word sketch: Extraction and display of significant collo-cations for lexicography. In Proceedings of COL-LOCTION: Computational Extraction, Analysis and Exploitation workshop, 39th ACL and 10th EACL, pp. 32-38. Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th inter-national conference on Computational linguistics, volume 2. Association for Computational Linguis-tics, pp. 768-774.  Dekang Lin, and Patrick Pantel. 2002. Concept Dis-covery from Text. In Proceedings of Conference on Computational Linguistics (COLING-02). pp. 577-583. Taipei, Taiwan. Dekang Lin, Kenneth Ward Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, Sushant Narsale. 2010. New tools for web-scale n-grams. In Proceedings of LREC. David Newman, Jey Han Lau, Karl Grieser and Timo-thy Baldwin (2010). Automatic Evaluation of Topic Coherence. In Proceedings of Human Lan-guage Technologies, 11th NAACL HLT, Los Ange-les, USA, pp. 100?108. Evan Sandhaus. 2008. "New york times corpus: Cor-pus overview." LDC catalogue LDC2008T19.  Sempute Team. 2013. What is NeedleSeek? http://needleseek.msra.cn/readme.htm Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving customary Web language to assist writers. Advances in Information Retrieval. Springer Berlin Heidelberg, pp. 631-635.  Martin Potthast, Martin Trenkmann, and Benno Stein. Using Web N-Grams to Help Second-Language Speakers .2010. SIGIR 10 Web N-Gram Workshop, pages 49-49. Alan Ritter, Mausam, and Oren Etzioni. 2010. A Latent Dirichlet Allocation method for Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Lin-guistics (July 2010), pp. 424-434. Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence 11(3), pp. 197-223.  Shaoqun Wu, Ian H. Witten and Margaret Franken (2010). Utilizing lexical data from a web-derived corpus to expand productive collocation knowledge. ReCALL, 22(1), 83?102. 
144
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 20?25,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
CoNLL-2013 Shared Task: Grammatical Error Correction NTHU System Description  Ting-Hui Kao+, Yu-Wei Chang*, Hsun-Wen Chiu*, Tzu-Hsi Yen+,  Joanne Boisson*, Jian-Cheng Wu+, Jason S. Chang+
* Institute of Information Systems and Applications + Department of Computer Science National Tsing Hua University  HsinChu, Taiwan, R.O.C. 30013 { maxis1718, teer1990, chiuhsunwen, joseph.yen,  Joanne.boisson, wujc86, jason.jschang} @gmail.com    Abstract 
Grammatical error correction has been an active research area in the field of Natural Language Processing. This paper describes the grammatical error correction system developed at NTHU in participation of the CoNLL-2013 Shared Task. The system consists of four modules in a pipeline to correct errors related to determiners, prepositions, verb forms and noun number. Although more types of errors are involved that than last year?s Shared Task, leading to more complicated problem this year, our system still obtain higher F-score as compared to last year. We received an overall F-measure score of 0.325, which put our system in second place among 17 systems evaluated. 1 Introduction Grammatical error correction is a task involving automatically detecting and correcting grammatical errors and improper choices. Grammatical error correction in writing of English as a second language (L2) or foreign language (EFL) is an important issue, for there are 375 million L2 speakers and 750 million EFL speakers around the world (Graddol, 2006). Most of these non-native speakers tend to make many kinds of error in their writing. An error correction system has the short-term benefit of helping writers improve the quality of writing. In the long run, non-native writers might learn from the corrections and thus gradually gain better command of grammar and word choice. The HOO shared task of 2012 is aimed at detecting and correcting misuse of determiners and prepositions, two types of errors accounting 
for only 38% of all errors. Therefore, there are a lot more errors related to other parts of speech that we have to address in this year?s shared task. In this paper, we describe the system submission from NTHU. The system reads and processes a given sentence through a pipeline of four distinct modules dealing with determiners, prepositions, verb forms and noun plurality. The output of one module feeds into the next module as input. The system finally produces possibly corrected sentences. The rest of the article is organized as follows. Section 2 describes detection and correction approach of each module in detail. Section 3 describes experiment setting and results. Then in Section 4, we discuss strengths and limitations of the proposed system and directions of future work. We conclude in Section 5.  2 System Description The system is designed to read a sentence and process each type of errors in terms and finally produce a corrected sentence. In Section 2.1, we give an overview of the system. Then, in Sections 2.2-2.5, we describe how to correct errors related to noun number, determiner, verb tense, and preposition.   
  Figure 1. System Architecture 
20
Table 1. Moving windows of ?location? Moving Window n-grams MW5 track based on the location based on the location of  on the location of cell the location of cell phone location of cell phone by MW4 based on the location on the location of the location of cell location of cell phone MW3 on the location the location of location of cell MW2 the location location of 2.1 Overview In this section, we give an overview of our system. Figure 1 shows the architecture of the error correction system. In this study, we focus on five different grammatical error types, including the improper usage of Determiner (ArtOrDet), Noun Number (Nn), Verb-Tenses (Vform), Subject-Verb Agreement (SVA), and Preposition (Prep).  In order to deal with these different types of errors systematically, we propose a back-off model based on the moving window approach.  Moving Window  A moving window MW of certain word wi is defined as below. (Leacock et al, 2010; Rozovskaya et al, 2010)   ???,?(?) = {???? ,? ,????? ???  ?, ? = 0, ? ? 1 ?}  (1)  where i denotes the position of word, k the window size, and w the original or replacement word at position i. In our approach, the window size is set to 2 to 5 words.  For example, consider the target word ?location? in the sentence, ?Children can easily be track based on the location of cell phone by parents.? The n-grams in moving windows of related to ?location? of sizes 2 to 5 are shown in Table 1.  Back-off Model To determine whether the target word needs to be changed to a different form (e.g, from ?location? to ?locations?), we first replace the target word with its variant forms (e.g., ?locations? for ?location?) in all MW n-grams and 
Table 2. Trigram information of ?location? and ?locations? in back-off model MW3 n-gram Freq. S3 location on the location the location of location of cell 304,400 3,794,400 1,400 4 M locations on the locations the locations of locations of cell 18,200 374,000 200 0.04 M  then measure the ratio of the counts of the original and replaced n-grams in a corpus. The frequency counts are obtained by querying a linguistic search engine Linggle (Joanne Boisson et al 2013), a web-scale linguistic search engine based on Google Web1T (Brants and Franz, 2006). The sum of n-gram counts, Sk with  the word w (original or replacement) in the ith position is defined as        ??,?(?)  ?=  ? ?????(?????)????  ?? ? ? ?(?)      (2)  With MW and S, we design a Replace function to determine whether is necessary to replace wi with its variant form, w' :  
Figure 2. The function Replace for determining whether to replace a word in location i using moving windows of size k.  The parameters ? and ? in Replace are set empirically.  For instance, in the given sentence ?Children can easily be track based on the location of cell phone by parents?, the target word wi is ?location? and the candidate is ?locations? for the Nn type error. According to Equation 2, the sums S9,3(?location?) of the original trigrams is about 4 million, whereas S9,3(?locations?) of the replaced trigrams is only 0.4 million (see Table 2 for more details). The value of r is 0.096, and depending on the threshold, Replace either returns False or back off to consider again the ratio r of S9,2(?location?) of the original bigrams and S9,2(?locations?) of the replacement bigrams for confidence in replacing the word ?location.?  
function Replace(i, k, w?) r = Si,k(w?)/Si,k(wi) if r > ? return True else if k > 2 and r > ?: return Replace (i, k-1, w?) else: return False 
21
2.2 The number module The number module is designed to correct error related to noun number (i.e., Nn). Two types of error are included, errors of singular noun and plural noun. To correct errors, we identify heads of base noun phrase (i.e., NP consisting of maximal contiguous sequence of tokens without containing another noun phrase or clause) in the given sentence by using part-of-speech tags and GeniaTagger (Tsuruoka et al, 2006), then use the Replace function to replace the original nouns (either singular or plural) to a different form (i.e., singular to plural, or plural to singular). We use two methods in the number module: combining voting with back-off, and using dependency relations.   Combining voting with back-off   Each n-gram in a moving window of various sizes described in Section 2.1 gets to cast a vote. When the sum of frequency counts related to the original noun is higher than that related to the replacement noun, the original noun gets one vote and vise versa. Voting method determines whether to replace the noun based on majority of the votes. For example, all of the 14 replacement n-grams (MWi, k , k = 2, 5) in Table 1 get a vote, because the n-gram with ?location? has higher frequency count that the same n-gram replaced with ?locations?. Intuitively, we should be confident enough to decide to stay with the original noun, i.e., ?location.? Back-off model described in Section 2.1 make a decision to permit the Replace module to change the original noun depend on threshold ?. Both of voting and back-off model need to show that alternative noun number is better. For the scheme of voting and back-off model, we also require the top count ratio and absolute count of 0.95 and 60,000 based on empirical evidence.  Using dependency relations  In some cases, the noun number depends on subject-verb agreement. We use part-of-speech information of subject and governing verb obtained from a tagger to handle such cases. For that, we use 3rd person singular present (i.e., VBZ) and other verb forms (e.g., VBP) to detect noun number mistakes.  Consider the sentence, ?In the society today, there are many ideas or concept that are 
currently in the stages of research and development.?, where ?concept? is a singular noun, but should be plural according to syntactic dependency information. The dependency parser typically produces nsubj(are-7, concept-11) among other relations and the word ?are? is tagged as VBP. Accordingly, we can replace the original noun, ?concept? to its plural form, ?concepts.?  2.3 Determiners module  The determiner is aimed at correcting determiner errors (i.e., errors annotated as ArtOrDet ). Given a sentence, we first identify the base noun phrases and their determiners (or lack of determiner) and using the moving window approach to decide whether there is an error and which alternative form to use. For determiner errors, the variant form of a base NP with a determiner is simple the same NP with determiner removed, while the variant form of a base NP without a determiner is simple the same NP with a determiner added. In addition to the moving window and back-off model, we also use dependency relations to check if a determiner is required for a base noun phrase.  Frequency of n-grams  We adopt the moving window approach and combine it with the back-off model mentioned in Section 2.1 with slight modification for the cases specific to determiner errors. When the head of given Base-NP is the last word of the n-gram, (as in ?Prepare meals for the elderly is my duty.?), the head can often be used as an modifier (as in ?for elderly people? leading to higher counts unrelated to the our case of the word being used as the head.   Therefore, while we adopting the moving window approach, the count of such n-gram is not counted. We set the threshold in the Replace function empirically: ?=5 and ?=0.35.   Dependency  In some cases, the frequency information of n-grams provides limited evidence for identifying mistakes. Therefore, we use more effective rules based on dependency relations to recognize the determiner errors in a way similar to the number module. 
22
Table 3. Verb form n-grams with PMIs. 
Verb Form n-grams PMI Sum happening crash happening happening at 21.5 38.2 59.7 happen crash happen happen at 24.0 35.7 59.7 happened crash happened happened at air crash happened happened at Miami crash happened at 
30.5 43.0 36.2 31.8 43.2 
184.7 
happens crash happens happens at crash happens at 
27.9 42.4 37.0 
107.3 
 We remove a determiner from a noun phrase with a plural head and an existing determiner. Otherwise, this module adds an appropriate determiner before the current noun phrase. For a conjunction (i.e., X and/or Y) of two base NPs, the rules favor adding a determiner such that both NPs have the same kind of determiner. 2.4 The verb-tense module In this section, we mainly concentrate on providing more proper verb tenses. Besides moving window, we introduce accumulated point-wise mutual information (PMI) (Church and Hanks, 1990) to improve the performance of this module. Applying PMI to this topic is based on the hypothesis that an appropriate verb form has a higher PMI measure with the context. To achieve more flexibility than the standard PMI, we use the modified PMI, which is an extension of standard PMI allowing an n-gram s of arbitrary length as input   ???(?) = log ?(?|?)?(??)????                               (3) where wi denotes the i-th word in s, k = | s |, and P(wi) the probability of wi estimated using a very large corpus. P(s|k) is the probability based on maximal likelihood estimation:   ?(?|?) = ???? (?)???? (?)???                               (4)  where S denotes all n-grams of length k. The PMI value of n-grams related to the original and alternative tense forms of a give verb are then calculated to attempt to correct the verb in question with a decision in favor of highest PMI. 
Table 4. Sample search results of ?being ?$PP a dangerous situation? * 
N-gram Count being in a dangerous situation 161 being a dangerous situation 0 being at a dangerous situation 0 being on a dangerous situation 0 ? 0 being about a dangerous situation 0 * Note:? denotes option word and $PP denotes wildcard prepositions  With this extended notion of PMI, we proceed as follows. First, we select each verb in a sentence and extract n-grams in moving window method as described in Section 2.2. Next, we generate more alternative n-grams by substituting all the related verb forms for the selected verb. After that, for all these n-grams, we calculate PMIs and accumulate the measures for each group of verb forms. Finally, if the accumulated PMI of the original verb is lower than the mean value of PMI of all verb forms, the verb in question will be replaced with the verb form associated the highest PMI value. Consider the sentence, ?In late nineteenth century, there was a severe air crash happening at Miami international airport.? We attempt to correct the verbs ?was? and ?happening? in the sentence. Table 3 shows n-grams and corresponding PMIs of each verb form. The accumulated PMI of ?happened? has the maximum value. So, the module changes ?happening? to ?happened.?  2.5 The prepositions module For preposition, we attempt to handle the two types of error: DELETE and REPLACE, and leave the INSERT errors for future work. For DELETE errors, the preposition in question should be deleted from the given sentence, whereas for REPLACE errors the preposition should be replaced with a more appropriate alternative. The third error type of preposition, INSERT, is left for future study. The proposed solution is based on the hypothesis that the usage of preposition often depends on the collocation relation of verb or noun. Therefore, we propose a back-off model, which utilizes the dependency relations to identify the related words of the preposition in question. We proceed as follows: For a target preposition in a given sentence, we extract the n-gram containing the preposition, its prepositional object, and the content word before the 
23
preposition. For example, the n-gram ?being in a dangerous situation? is extracted from the sentence ?This can protect the students from being in a dangerous situation in particularly for the small children who are studying in nursery.? The n-gram ?being in a dangerous situation? is then transformed into a query for a linguistic search engine (e.g., Linggle as described in Joanne et al 2013) to obtain the counts of all preposition variant forms, including NULL (for DELETE) or other prepositions (for REPLACE). The transformation process is very simple involving changing the proposition to a wild part of speech symbol. For example, ?being in a dangerous situation? is transformed to ?being ?$PP a dangerous situation.? The sample search results are shown in Table 4. From the results, we could confirm that the preposition ?in? is used correctly.  Although we use the web-scale n-gram for validation of usage of preposition, however, data sparseness still poses a problem. Furthermore, we cannot obtain information for n-grams with length more than 5, since the Web 1T we used only contains 1 to 5-grams. In order to cope with the data sparseness problem, we transform a query into a more general form, if no result could be obtained in the first round of search. To generalize the query, we remove the modifiers of the prepositional object one after another. Additionally, we also attempt to change the modifiers with the most frequent modifier of the object. Consider the n-gram ?in modern digit world.? The generalized n-grams ?in digit world? and ?in new world? will then be transformed into queries in turns until the results are sufficient for the model to make a decision. To avoid false alarm, empirically determined thresholds are used to measure the ratio of count of a preposition variant form to the original preposition. 3 Experiment To assess the effectiveness of the proposed method, we used the official training and testing data of CoNLL-2013 Shared Task. We also exploited several tools including Linggle, Stanford Parser and Geniatagger in the proposed system. Linggle supports flexible linguistic queries with wild part of speech and returns matching n-grams counts in Google Web 1T 5gram. Stanford Parser and Geniatagger produce syntactical information including dependency relations, 
part-of-speech tags, and phrase boundary. The evaluation scorer, which computes precision, recall, and F-score, is provided by National University of Singapore, the organizer of CoNLL-2013 Shared Task. On the test data, our system obtained the precision, recall and F-score of .3057, 0.346, and .3246, which put us in first place in term of recall and second place in term of F-score. 4 Discussion In this section, we discuss the strengths and limitations of our system and propose approaches to overcome current limitations. The module of noun numbers, moving window and syntactic dependency for correcting errors cannot handle well some ambiguous cases. For example, in this case "In conclusion, what I have mentioned above, we have to agree, tracking system has many benefits?.", according to the gold-standard annotations, ?system has? is corrected to ?systems have?.  However, this module keep the original word because of the 3rd person singular present verb, ?has?. Before ?has? being corrected to ?have?, there was no sufficient evidence to support that ?systems? is a good replacement. In cases like this, it is often difficult to suggest a correction using only the sentential context and n-gram frequency and dependency relations. To correct such an error, we may need to consider the context of the discourse or combine the module of different error types such as noun numbers and verb tense, which is beyond the scope of the current system. We handle the determiner errors with threshold ?  and ?  empirically derived, but it would be more effective if we could use some form of minimal error rate tuning (MERT) to set the parameters. Besides, we found that applying the dependency criteria and moving window method in parallel leads to high recall but low precision. However, the moving window method often fails because of insufficient evidence. In such case, the system can perform better in both precision and recall by favoring the dependency model output. For our system, the performance of correcting verb form errors is severely limited by the lengths of n-gram. The failure related to verb forms correction are mostly caused by the limitation of n-gram length of Web 1T. There is a large portion of sentences where the subject (or the adverbs) and the verb are so far apart, that 
24
they are not within windows of five words. So, it is difficult to use the noun number of the subject to select the correct verb form. Another major area of limitations of handling verb form errors has to do with rare words which lead to unseen n-grams even in a very large dataset like Web 1T. These rare words are mostly name entities that have insufficient coverage when combined other words in n-grams. Intuitively, we can generalize the n-gram matching process as in the case of handling preposition errors. In this study, we use the preposition and object relation (POBJ) to determine whether the use of the preposition is correct. The relation is useful for generalizing the queries and in correcting preposition errors. However, many preposition errors are unrelated to POBJ. For example, in the sentence ?Surveillance technology will help to prevent the family to loss their member...?, the two words ?to loss? should be replace with ?from losing.? Unfortunately, the current system cannot correct such an error in the absence of POBJ relation. In order to correct this kind of error, we have to consider composed relations such as noun-preposition-verb, which is crucial to the capability of correcting such multiple consecutive errors (i.e., preposition plus verb). 5 Conclusion In this paper, we build four modules in determiner, noun number, verb form, and preposition for error detection and correction. For different types of errors, we have developed modules independently in accordance with their features. The constructed modules rely on both moving windows and back-off model to improve grammatical error correction. Additionally, for verb form errors, we introduce point-wise mutual information for higher precision and recall.  We plan to integrate all the modules in a more flexible way than the current pipeline scheme. Yet another direction for future research is to consider the discourse context. 6 Acknowledgements We would like to acknowledge the funding supports from Delta Electronic Corp and National Science Council, Taiwan (contract no: NSC 100-2511-S-007-005-MY3). We are also thankful for helpful comments from the anonymous reviewers.  
References  Joanne Boisson, Ting-Hui Kao, Jian-Cheng Wu, Tzu-Hsi Yen and Jason S. Chang. 2013. Linggle: a Web-scale Linguistic Search Engine for Words in Context. In proceedings of Association for Computational Linguistics demonstrations. (ACL 2013) Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram corpus version 1.1.LDC2006T13 Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics 16(1) (1990) 22?29 Leacock Claudia et al 2010. Automated grammatical error detection for language learners. Synthesis Lectures on Human Language Technologies, 3(1) 1?134. Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013). Daniel Dahlmeier and Hwee Tou Ng. 2012. Better Evaluation for Grammatical Error Correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012). pp. 568 ? 572 David Graddol. 2006. English next: Why global English may mean the end of ?English as a Foreign Language.? UK: British Council. John Lee and Stephanie Seneff. 2006. Automatic Grammar Correction for Second-Language Learners. In INTERSPEECH ICSLP. Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto and Joel Tetreault. 2013. The CoNLL-2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Alla Rozovskaya and Dan Roth. 2010. Generating confusion sets for context-sensitive error correction. In Proceedings of EMNLP, pp. 961?970. Yoshimasa Tsuruoka et al Developing a Robust Part-of-Speech Tagger for Biomedical Text. In Advances in Informatics - 10th Panhellenic Conference on Informatics, pp 382?392. 
25
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 91?95,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
NTHU at the CoNLL-2014 Shared Task
Jian-Cheng Wu*, Tzu-Hsi Yen*, Jim Chang*, Guan-Cheng Huang*,
Jimmy Chang*, Hsiang-Ling Hsu+, Yu-Wei Chang+, Jason S. Chang*
* Department of Computer Science
+ Institute of Information Systems and Applications
National Tsing Hua University
HsinChu, Taiwan, R.O.C. 30013
{wujc86, joseph.yen, cwebb.tw, a2961353,
rocmewtwo, ilovecat6717, teer1990, jason.jschang}@gmail.com
Abstract
In this paper, we describe a system for cor-
recting grammatical errors in texts written
by non-native learners. In our approach, a
given sentence with syntactic features are
sent to a number of modules, each focuses
on a specific error type. A main program
integrates corrections from these modules
and outputs the corrected sentence. We
evaluated our system on the official test
data of the CoNLL-2014 shared task and
obtained 0.30 in F-measure.
1 Introduction
Millions of non-native learners are using English
as their second language (ESL) or foreign lan-
guage (EFL). These learners often make different
kinds of grammatical errors and are not aware of
it. With a grammatical error corrector applies rules
or statistical learning methods, learners can use the
system to improve the quality of writing, and be-
come more aware of the common errors. It may
also help learners improve their writing skills.
The CoNLL-2014 shared task is aimed at pro-
moting research on correcting grammatical errors.
Types of errors handled in the shared task are ex-
tended from the five types in the previous shared
task to include all common errors present in an es-
say.
In this paper, we focus on the following errors
made by ESL writers:
? Spelling and comma
? Article and determiner
? Preposition
? Preposition + verb (interactive)
? Noun number
? Word form
? Subject-verb-agreement
For each error type, we developed and tuned a
module based on the official development data. A
main program combines the correction hypotheses
from these modules and produces the final correc-
tion. If multiple modules propose different cor-
rections to the same word/phrase, the correction
proposed by the module with the highest precision
will be chosen.
2 Method
2.1 Spelling and Comma module
In this section, we correct comma errors and
spelling errors, including missing/extraneous hy-
phens. For simplicity, we adopt Aspell
1
and
GingerIt
2
to detect spelling errors and generate
possible replacements, considered as confusable
words, which might contain the word with cor-
rect spelling. Then, we replace the word being
checked with confusable words to generate sen-
tences. Language models trained on well-formed
texts are used to measure the probability of these
1
http://aspell.net/
2
https://pypi.python.org/pypi/gingerit
We use GingerIt only for correcting missing/extraneous
hyphens
91
candidates. Candidate with the highest probability
is chosen as correction.
Omitted commas form a large proportion of
punctuation errors. We apply the CRF model pro-
posed by Israel, et. al. (2012) with some mod-
ification. We replace distance features with syn-
tactic features. More specifically, we do not use
features such as distances to the start of sentence
or last comma. And we add two features, one in-
dicates whether a word is at the end of a clause,
and the other indicates whether the current clause
starts with a prepositional phrase.
2.2 Subject-verb-agreement module
This module corrects subject-verb-agreement er-
rors in a given sentence. Consider the sentence
?The boy in blue pants are my brother?. The cor-
rect sentence should be ?The boy in blue pants is
my brother?. Since a verb could be far from it?s
subject, using ngram counts may fail to detect and
correct such an error.
We use a rule-based method in this module.
In the first stage, we identify the subject of each
clause by using information from the parser. Both
the dependency relation and syntactic structure are
used in this stage. Dependency relations such as
nsubj and rcmod indicate subjects of subject-
verb relation. If there is a verb that has not been
assigned a subject via dependency relations, head
of noun phrase in the same clause will be used in-
stead. And in the second stage, we check whether
subject and verbs agree, for each clause in the sen-
tence.
Here we explain our correction process in more
detail. For each clause, the singular and plural
forms of verbs in the clause must be consistent
with the subject of the clause unless the subject
is a quantifier. Consider the following sentences:
The number of cats is ten.
A number of cats are playing.
Since our judgment only depends on the subject
number, it?s hard to tell whether should we use
a plural verb or not in this case. The quantifiers
we do not handle are listed as follow: number, lot,
quantity, variety, type, amount, neither.
2.3 Number module and Forms module
We correct noun number error in two stages. In
the first stage, we generate a confusion set for each
word. While constructing confusion set for noun
number, both of the singular form and plural form
are included in the set. While constructing con-
fusion set for word forms, we use the word fam-
ilies in Academic Word List (AWL)
3
and British
National Corpus (BNC4000)
4
. Given a content
word, all the words in the same family except
antonyms are entered into the confusion set. How-
ever, comparative form and superlative form of an
adjective are eliminated from the confusion set,
since replacing an adjective with these forms is a
semantic rather than syntactic decision. The fol-
lowing examples illustrate what kinds of alterna-
tives are eliminated:
antonyms: misleading for the word lead
semantics: higher for the word high
Additionally, in the forms module, a correc-
tion is ignored, if it is actually correcting a verb
tense, subject-verb-agreement, or noun number er-
ror. We use part-of-speech (POS) tag to check this.
More specifically, any corrections that changes a
word with a VBZ tag to a word with a VBP or
VBD tag is ignored, and vice versa. And any cor-
rections that switches a noun between it?s singular
form and plural form is also ignored.
With the confusion sets, we proceed to the
second stage. In this stage, we use words in
the confusion set to attempt to replace potential
errors. Language models trained on well-formed
text are used to validate the replacement decisions.
Given a word w, If there is an alternative w? that
fits in the context better, w is flagged as an error
and w? is returned as a correction.
Here is our formula for correcting errors
P (O) =
P
ngram
(O) + P
rnn
(O)
2
P (R) =
P
ngram
(R) + P
rnn
(R)
2
Promotion =
P (R)? P (O)
|O|
While checking a content word w, we replace
w in the original sentence O with alternatives and
generate candidates C. We then generate the can-
didate R with the highest probability among all
3
http://www.victoria.ac.nz/lals/
resources/academicwordlist/sublists
4
http://simple.wiktionary.org/wiki/
Wiktionary:BNC_spoken_freq
92
candidates. We use an interpolated probability
5
of
ngram language model P
ngram
and recurrent neu-
ral network language model P
rnn
. Promotion in-
dicates the increase in probability per word after
we replace sentence O with the candidate R. We
use word number to normalize Promotion follow-
ing Dahlmeier, et al. (2012). Corrections are made
only if Promotion is higher than a empirically de-
termined threshold.
6
2.4 Article and Determiner module
In this subsection, we describe how we correct er-
rors of omitting a determiner or adding a spuri-
ous determiner. The language models mentioned
in the last subsection are also used in this module.
We tune our thresholds for making corrections on
development data
7
, and found that deleting a de-
terminer should have a lower threshold while in-
serting one should have a higher one, so we set
different thresholds accordingly.
8
To cope with the situation where a deter-
miner/article is far ahead of the head of the noun
phrase, we apply another constraint while making
correction decision.
First, we calculate statistics on the head of noun
phrases. We extract the most frequent 100,000
terms in Web-1T 5-gram corpus. These terms are
used to search their definitions in Wikipedia (usu-
ally at the first paragraph). The characteristic of a
definition is that it has no prior context and most
of the noun phrases with a determiner are unique
or known to the general public. Heads of these
nouns phrases are likely to always appear with a
determiner.
Heads that tend to appear with a determiner
the help us to decide whether a determiner should
be added to a noun phrase. We add a determiner
using two constraints. We only insert a determiner
or an article, if the statistics indicate that head of
a noun phrase tends to have a determiner, or the
promotion of log probability is much higher than
the threshold. A similar constraint is also applied,
for deleting a determiner or an article.
5
the probabilities present in the formula are log probabil-
ities
6
the threshold for noun number module is 0.035 and 0.050
for word form module. These threshold were set empirically
after testing on development data
7
test data of the CoNLL-2013 shared task
8
the threshold for insertion is 0.035 and 0.040 for deletion
2.5 Preposition module
For preposition errors, we focus on handling two
types of errors: REPLACE and DELETE. A
preposition, which should be deleted from the
given sentence, is regarded as a DELETE error,
whereas for a preposition, which should be re-
placed with a more appropriate alternative, is re-
garded as a REPLACE error. In this work, we
correct the two types of errors based on the as-
sumption that the usage of preposition often de-
pends on the collocation relations with a verb or
noun. Therefore, we use the dependency relations
such as dobj and pobj, and prep to identify
the related words, and then we validate the usage
of prepositions, and correct the preposition errors.
A dependency-based model is proposed in this
paper to handle the preposition errors. The model
consists of two stages: detecting the possible
preposition errors and correcting the errors.
In the first stage, we use the Stanford depen-
dency parser (Klein and Manning, 2003) to extract
the dependency relations for each preposition. The
relation tuples, which contain the preposition, verb
or noun, and prepositional object. For example,
the tuple of verb-prep-object (listen, to, music),
or the tuple of noun-prep-object (point, of, view)
are extracted for validation. We identify a preposi-
tion containing as an error, if the tuple containing
the preposition does not occur in a reference list
built using a reference corpus. In order to resolve
the data sparseness and false alarm problems, we
need a sufficiently large list of validated tuples.
In this study, the reference tuple lists are gener-
ated from the Google Books Syntactic N-grams
(Goldberg and Orwant, 2013)
9
. For example, we
can find (come, to, end, 236864) and (lead, to, re-
sult, 57632) in the verb-preposition-object refer-
ence list. We have generated 21,773,752 different
dependency tuples for our purpose.
In the second stage, we attempt to correct all
potential preposition errors. At first, a list of can-
didate tuples is generated by substituting the orig-
inal preposition in the error tuple with alterna-
tive prepositions. For example, the generated can-
didate tuples for the error tuple (join, at, party)
will include (join, in, party), (join, on, party), etc.
On the other hand, the tuple, (join, party), which
9
Data sets available from http://
commondatastorage.googleapis.com/books/
syntactic-ngrams/index.html
93
deletes the preposition, is also taken into consid-
eration. All candidates are ranked according to
the frequency provided by the reference lists. The
preposition in the tuple with the highest frequency
is returned as the correction suggestion.
Figure 1: Sample annotated trigrams
Figure 2: Sample trigram group
Figure 3: Sample phrase translation for a trigram
group
2.6 Interactive errors module
This module uses a new method for correcting
serial grammatical errors in a given sentence in
learners writing. A statistical translation model is
generated to attempt to translate the input with se-
rial and interactive errors into a grammatical sen-
tence. The method involves automatically learn-
ing translation models based on Web-scale n-
gram. The model corrects trigrams containing se-
rial preposition-verb errors via translation. Eval-
uation on a set of sentences in a learner corpus
shows that the method corrects serial errors rea-
sonably well.
Consider an error sentence ?I have difficulty to
understand English.? The correct sentence for this
should be ?I have difficulty in understanding En-
glish.? It is hard to correct these two errors one by
one, since the errors are dependent on each other.
Intuitively, by identifying difficulty to understand
as containing serial errors and correct it to diffi-
culty in understanding, we can handle this kind of
problem more effectively.
First, we generate translation phrase table as
follows. We begin by selecting trigrams related to
serial errors and correction from Web 1T 5-gram.
Figure 1 shows some sample annotation trigrams.
Then, we group the selected trigrams by the first
and last word in the trigrams. See Figure 2 for a
sample VPV group of trigrams. Finally, we gener-
ate translation phrase table for each group. Figure
3 shows a sample translation phrase table.
At run time, we tag the input sentence with part
of speech information in order to find trigrams
that fit the type of serial errors. Then, we search
phrase table and generate translations for the
input phrases in a machine translation decoder to
produce a corrected sentence.
3 Experiment
Two types of trigram language models, ngram
model and recurrent neural network (RNN) model,
are used in correcting spelling, noun number, word
form, and determiner errors. We trained the ngram
language model on English Gigaword and BNC
corpus, using the SRILM tool (Stolcke, 2002).
We train the RNN model with RNNLM toolkit
(Mikolov et al., 2011). Complexity of training the
RNN language model is much higher, so we train
it on a smaller corpus, the British National Corpus
(BNC).
We used the Stanford Parser (Klein and Christo-
pher D. Manning, 2003) to obtain dependency re-
lations in the preposition module, and to obtain
POS tags for the word form module. The subject-
verb-agreement module also uses dependency re-
lations contained in test data. Dependency rela-
tions in Google Books Syntactic N-grams (Gold-
94
berg and Orwant, 2013) were also used to develop
our dendepency-based model in the preposition
module.
To assess the effectiveness of the proposed
method, we used the official training, develop-
ment, and test data of the CoNLL-2014 shared
task. On the test data, our system obtained the pre-
cision, recall and F
0.5
score of 0.351, 0.189, and
0.299. The following table shows the performance
breakdown by module.
Figure 4: The performance breakdown by module.
(Displayed in %)
In the spelling and hyphen module, candidates
from Aspell include words that only differ from
the original word in one character, s. Language
models are then used to choose the candidate with
highest probability as our correction. The module
therefore gives some corrections about noun num-
bers or subject-verb-agreement. As a result, some
corrections made by this module overlap with cor-
rections made by the noun numbers module and
the subject-verb-agreement module, which makes
the recall of correcting spelling and hyphen errors,
4.11%, overestimated.
4 Conclusion
In this work, we have built several modules for er-
ror detection and correction. For different types
of errors, we developed modules independently
using different features and thresholds. If mul-
tiple modules propose different corrections to a
word/phrase, the one proposed by the module with
higher precision will be chosen. Many avenues
for future work present themselves. We plan to
integrate modules in a more flexible way. When
faced with different corrections made by different
modules, the decision would better be based on
the confidence of each correction with a uniform
standard, but not on the confidence of modules.
Acknowledgments
We would like to acknowledge the funding sup-
ports from Delta Electronic Corp and National
Science Council (contract no: NSC 100-2511-S-
007-005-MY3), Taiwan. We are also thankful to
anonymous reviewers and the organizers of the
shared task.
References
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng
2012. NUS at the HOO 2012 Shared Task. In Pro-
ceedings of the Seventh Workshop on Building Ed-
ucational Applications Using NLP, Association for
Computational Linguistics, June 7.
Daniel Dahlmeier and Hwee Tou Ng, 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2012).,568-672
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013.
Building a Large Annotated Corpus of Learner En-
glish: The NUS Corpus of Learner English. In
Proceedings of the 8th Workshop on Innovative Use
of NLP for Building Educational Applications(BEA
2013)
Yoav Goldberg and Jon Orwant 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Proceedings of the Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, GA, 2013.
Ross Israel, Joel Tetreault, and Martin Chodorow
2012. Correcting Comma Errors in Learner Essays,
and Restoring Commas in Newswire Text. In Pro-
ceeding of the 2012 Conference of the North Amer-
ica Chapter of the Association for Computational
Linguistics: Human Language Technologies,284-
294, Montreal Canada, June. Association for Com-
putational Linguistics
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, 423-430.
Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar
Burget, and Jan Honza Cernocky 2011. Strategies
for Training Large Scale Neural Network Language
Models Proceedings of ASRU 2011
Andreas Stolcke 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, vol 2, 901-904
95
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 34?37,
Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational Linguistics
GLANCE Visualizes Lexical Phenomena for Language Learning  
 
Mei-Hua Chen*, Shih-Ting Huang+, Ting-Hui Kao+, Sun-Wen Chiu+, Tzu-His Yen+ 
* Department of Foreign Languages and Literature, Hua Fan University, Taipei, Taiwan, 
R.O.C. 22301 
+ Department of Computer Science, National Tsing Hua University, HsinChu, Taiwan, 
R.O.C. 30013 
{chen.meihua,koromiko1104,maxis1718,chiuhsunwen, joseph.yen}@gmail.com 
 
 
Abstract 
Facilitating vocabulary knowledge is a 
challenging aspect for language learners. 
Although current corpus-based reference 
tools provide authentic contextual clues, the 
plain text format is not conducive to fully 
illustrating some lexical phenomena. Thus, 
this paper proposes GLANCE 1 , a text 
visualization tool, to present a large amount 
of lexical phenomena using charts and graphs, 
aimed at helping language learners 
understand a word quickly and intuitively. To 
evaluate the effectiveness of the system, we 
designed interfaces to allow comparison 
between text and graphics presentation, and 
conducted a preliminary user study with ESL 
students. The results show that the visualized 
display is of greater benefit to the 
understanding of word characteristics than 
textual display. 
1 Introduction 
Vocabulary is a challenging aspect for language 
learners to master. Extended word knowledge, 
such as word polarity and position, is not widely 
available in traditional dictionaries. Thus, for 
most language learners, it is very difficult to 
have a good command of such lexical phenome-
na.  
Current linguistics software programs use 
large corpus data to advance language learning. 
The use of corpora exposes learners to authentic 
contextual clues and lets them discover patterns 
or collocations of words from contextual clues 
(Partington, 1998). However, a huge amount of 
data can be overwhelming and time-consuming 
(Yeh et al., 2007) for language learners to induce 
rules or patterns. On the other hand, some lexical 
phenomena seem unable to be comprehended 
                                                 
1 http://glance-it.herokuapp.com/ 
fast and directly in plain text format (Koo, 2006). 
For example, in the British National Corpus 
(2007), ?however? seems more negative than 
?but?. Also, compared with ?but?, ?however? 
appears more frequently at the beginning of a 
sentence. 
With this in mind, we proposed GLANCE1, a 
text visualization tool, which presents corpus 
data using charts and graphs to help language 
learners understand the lexical phenomena of a 
word quickly and intuitively. In this paper, we 
focused on five types of lexical phenomena: po-
larity, position, POS, form and discipline, which 
will be detailed in the Section 3. Given a single 
query word, the GLANCE system shows graph-
ical representations of its lexical phenomena se-
quentially within a single web page.  
Additionally we believe that the use of 
graphics also facilitates the understanding of the 
differences between two words. Taking this into 
consideration, we introduce a comparison mode 
to help learners differentiate two words at a 
glance. Allowing two word input, GLANCE 
draws the individual representative graphs for 
both words and presents these graphs in a two-
column view. The display of parallel graphs de-
picts the distinctions between the two words 
clearly. 
 
2 Related Work 
Corpus-based language learning has widened the 
perspectives in second and foreign language edu-
cation, such as vocabulary learning (Wood, 
2001). In past decades, various corpus-based ref-
erence tools have been developed. For example, 
WordSmith (Scott, 2000), Compleat Lexical Tu-
tor (Cobb, 2007), GRASP (Huang et al., 2011), 
PREFER (Chen et al, 2012). 
Recently, some interactive visualization tools 
have been developed for the purpose of illustrat-
ing various linguistic phenomena. Three exam-
34
ples are Word Tree, a visual concordance (Wat-
tenberg and  i gas, 2008), WORDGRAPH, a 
visual tool for context-sensitive word choice 
(Riehmann et al., 2012) and Visual Thesaurus, a 
3D interactive reference tool (ThinkMap Inc., 
2005). 
 
3 Design of the GLANCE System 
The GLANCE system consists of several com-
ponents of corpus data visualization. We design 
and implement these visualization modules sepa-
rately to ensure all graphs are simple and clear 
enough for users to capture and understand the 
lexical phenomena quickly. 
In this paper, we use the d3.js (Data-Driven 
Documents) (Bostock et al., 2011) to visualize 
the data. The d3.js enables direct inspection and 
manipulation of a standard document object 
model (DOM) so that we are able to transform 
numeric data into various types of graphs when 
fitting these data to other visualization tools. In 
this section, we describe the ways we extract the 
data from the corpus and how we translate these 
data into informative graphs. 
 
3.1 Data Preprocessing 
We use the well-formed corpus, the BNC, to ex-
tract the data. In order to obtain the Part-of-
speech tags for each text, we use the GENIA 
tagger (Tsuruoka et al., 2005) to analyze the sen-
tences of the BNC and build a list of <POS-tag, 
frequency> pairs for each word in the BNC. Also 
the BNC contains the classification code as-
signed to the text in a genre-based analysis car-
ried out at Lancaster University by Lee (2001). 
For each word, the classification codes are ag-
gregated to a list of <code, frequency> pairs.  
  
3.2 Visualization of Lexical Phenomena  
Polarity 
A word may carry different sentiment polarities 
(i.e., positive, negative and objective). To help 
users quickly determine the proper sentiment 
polarity of a word, we introduce the sentiment 
polarity information of SentiWordNet 
(Baccianella et al., 2010) into our system. For 
each synset of a word, GLANCE displays the 
polarity in a bar with three different colors. The 
individual length of the three parts in the bar cor-
responds to the polarity scores of a synset (Fig-
ure 1). 
 
 
Figure 1. Representation of sentiment polarity  
 
Position 
The word position in a sentence is also an im-
portant lexical phenomenon. By calculating the 
word position in each sentence, we then obtain 
the location distribution. GLANCE visualizes the 
distribution information of a word using a bar 
chart. Figure 2 shows a plot of distribution of 
word position on the x-axis against the word fre-
quency on the y-axis. 
 
 
Figure 2. Distribution of word position 
 
Part Of Speech (POS) 
A lexical item may have more than one part of 
speech. Knowing the distribution of POS helps 
users quickly understand the general usage of a 
word.  
GLANCE displays a pie chart for each word 
to differentiate between its parts of speech. We 
use the maximum likelihood probability of a 
POS tag for a word as the arc length of the pie 
chart (Figure 3). 
 
 
35
Figure 3. POS representation 
 
Form 
The levels of formality of written and spoken 
language are different, which also confuse lan-
guage learners. Pie charts are used to illustrate 
the proportion of written and spoken English of 
individual words as shown in Figure 4. 
We derive the frequencies of both forms from 
the BNC classification code for each word. The 
arc length of each sector is proportional to the 
maximum likelihood probability of forms. 
 
 
Figure 4. Form representation 
 
Discipline 
Similar to language form, the discipline infor-
mation (e.g., newspaper or fiction) was gathered 
from the BNC classification code. The relations 
of the disciplines of a word are presented using a 
sunburst graph, a radial space-filling tree layout 
implemented with prefuse (Heer et al., 2005). In 
the sunburst graph (Figure 5.), each level corre-
sponds to the relation of the disciplines of a cer-
tain word. The farther the level is away from the 
center, the more specific the discipline is. Each 
level is given equal width, but the circular angle 
swept out by a discipline corresponds to the fre-
quency of the disciplines. 
 
  
Figure 5. Discipline relations 
 
4 Results 
4.1 Experimental Setting 
We performed a preliminary user study to assess 
the efficiency of our system in assisting language 
learners in grasping lexical phenomena. To ex-
amine the effectiveness of visualization, we built 
a textual interface for comparison with the 
graphical interface. 
Ten pre-intermediate ESL college students 
participated in the study. A total of six pairs of 
similar words were listed on the worksheet. After 
being introduced to GLANCE, all students were 
randomly divided into two groups. One group 
was required to consult the first three pairs using 
the graphical interface and the second three pairs 
the textual interface, and vice versa. The partici-
pants were allowed a maximum of one minute 
per pair, which meets the goal of this study of 
quickly glancing at the graphics and grasping the 
concepts of words. Then a test sheet containing 
the same six similar word pairs was used to ex-
amine the extent of students? word understanding. 
Note that during the test, no tool supports were 
provided. The student scored one point if he gave 
the correct answers to each question. In other 
words he would be awarded 6 points (the highest 
number of points) if he provided all the correct 
answers. They also completed a questionnaire, 
described below, evaluating the system. 
 
4.2 Experimental Results 
To determine the effectiveness of visualization of 
lexical phenomena, the students? average scores 
were used as performance indicators. Students 
achieved the average score 61.9 and 45.0 out of 
100.00 after consulting the graphic interface and 
textual interface respectively. Overall, the visual-
ized display of word characteristics outper-
formed the textual version. 
The questionnaire revealed that all the partici-
pants showed a positive attitude to visualized 
word information. Further analyses showed that 
all ten participants appreciated the position dis-
play and nine of them the polarity and form dis-
plays. In short, the graphical display of lexical 
phenomena in GLANCE results in faster assimi-
lation and understanding of word information. 
Moreover, the participants suggested several in-
teresting aspects for improving the GLANCE 
system. For example, they preferred bilingual 
environment, further information concerning an-
tonyms, more example sentences, and increased 
36
detail in the sunburst representation of disci-
plines. 
 
5 Conclusion and Future Work 
In this paper, we proposed GLANCE, a text vis-
ualization tool, which provides graphical display 
of corpus data. Our goal is to assist language 
learners in glancing at the graphics and grasping 
the lexical knowledge quickly and intuitively. To 
evaluate the efficiency and effectiveness of 
GLANCE, we conducted a preliminary user 
study with ten non-native ESL learners. The re-
sults revealed that visualization format outper-
formed plain text format. 
Many avenues exist for future research and 
improvement. We attempt to expand the single 
word to phrase level. For example, the colloca-
tion behaviors are expected to be deduced and 
displayed. Moreover, we are interested in sup-
porting more lexical phenomena, such as hypo-
nyms, to provide learners with more lexical rela-
tions of the word with other words. 
 
Reference 
Baccianella, S., Esuli, A., & Sebastiani, F. (2010, 
May). SentiWordNet 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion Min-
ing. In LREC (Vol. 10, pp. 2200-2204). 
Bostock, M., Ogievetsky, V., & Heer, J. (2011). D? 
data-driven documents. Visualization and Comput-
er Graphics, IEEE Transactions on, 17(12), 2301-
2309. 
Chen, M. H., Huang, S. T., Huang, C. C., Liou, H. C., 
& Chang, J. S. (2012, June). PREFER: using a 
graph-based approach to generate paraphrases for 
language learning. In Proceedings of the Seventh 
Workshop on Building Educational Applications 
Using NLP (pp. 80-85). Association for Computa-
tional Linguistics. 
Cobb, T. (2007). The compleat lexical tutor. Retrieved 
September, 22, 2009. 
Heer, J., Card, S. K., & Landay, J. A. (2005, April). 
Prefuse: a toolkit for interactive information visual-
ization. In Proceedings of the SIGCHI conference 
on Human factors in computing systems (pp. 421-
430). ACM. 
Huang, C. C., Chen, M. H., Huang, S. T., Liou, H. C., 
& Chang, J. S. (2011, June). GRASP: grammar-
and syntax-based pattern-finder in CALL. 
In Proceedings of the 6th Workshop on Innovative 
Use of NLP for Building Educational Applications 
(pp. 96-104). Association for Computational Lin-
guistics. 
Kyosung Koo (2006). Effects of using corpora and 
online reference tools on foreign language writing: 
a study of Korean learners of English as a second 
language. PhD. dissertation, University of Iowa. 
Lee, D. Y. (2001). Genres, registers, text types, do-
mains and styles: Clarifying the concepts and 
nevigating a path through the BNC jungle. 
Partington, A. (1998). Patterns and meanings: Using 
corpora for English language research and teach-
ing (Vol. 2). John Benjamins Publishing. 
Riehmann, P., Gruendl, H., Froehlich, B., Potthast, M., 
Trenkmann, M., & Stein, B. (2011, March). The 
NETSPEAK WORDGRAPH: Visualizing key-
words in context. In Pacific Visualization Sympo-
sium (PacificVis), 2011 IEEE (pp. 123-130). IEEE. 
Scott, M. (2004). WordSmith tools version 4. 
The British National Corpus, version 3 (BNC XML 
Edition). 2007. Distributed by Oxford University 
Computing Services on behalf of the BNC Consor-
tium. URL: http://www.natcorp.ox.ac.uk/ 
ThinkMap Inc. (2005). Thinkmap Visual Thesaurus. 
Available from http:// www.visualthesaurus.com 
Tsuruoka, Y., Tateishi, Y., Kim, J. D., Ohta, T., 
McNaught, J., Ananiadou, S., & Tsujii, J. I. (2005). 
Developing a robust part-of-speech tagger for bio-
medical text. In Advances in informatics (pp. 382-
392). Springer Berlin Heidelberg. 
Wattenberg, M., & Vi?gas, F. B. (2008). The word 
tree, an interactive visual concord-
ance. Visualization and Computer Graphics, IEEE 
Transactions on, 14(6), 1221-1228. 
Wood, J. (2001). Can software support children?s 
vocabulary development.Language Learning & 
Technology, 5(1), 166-201. 
Yeh, Y., Liou, H. C., & Li, Y. H. (2007). Online syn-
onym materials and concordancing for EFL college 
writing. Computer Assisted Language Learning, 
20(2), 131-152. 
 
37
