Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 71?79,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
A generalized method for iterative error mining in parsing results
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Jianqiang Ma
University of Groningen
j.ma@student.rug.nl
Gertjan van Noord
University of Groningen
g.j.m.van.noord@rug.nl
Abstract
Error mining is a useful technique for
identifying forms that cause incomplete
parses of sentences. We extend the iter-
ative method of Sagot and de la Clerg-
erie (2006) to treat n-grams of an arbi-
trary length. An inherent problem of in-
corporating longer n-grams is data sparse-
ness. Our new method takes sparseness
into account, producing n-grams that are
as long as necessary to identify problem-
atic forms, but not longer.
Not every cause for parsing errors can be
captured effectively by looking at word
n-grams. We report on an algorithm for
building more general patterns for min-
ing, consisting of words and part of speech
tags.
It is not easy to evaluate the various er-
ror mining techniques. We propose a new
evaluation metric which will enable us to
compare different error miners.
1 Introduction
In the past decade wide-coverage grammars and
parsers have been developed for various lan-
guages, such as the Alpino parser and grammar
(Bouma et al, 2001) for Dutch and the English
Resource Grammar (Copestake and Flickinger,
2000). Such grammars account for a large num-
ber of grammatical and lexical phenomena, and
achieve high accuracies. Still, they are usually
tailored to general domain texts and fail to reach
the same accuracy for domain-specific texts, due
to missing lexicon entries, fixed expressions, and
grammatical constructs. When parsing new texts
there are usually two types of parsing errors:
? The parser returns an incorrect parse. While
the parser may have constructed the correct
parse, the disambiguation model chose an in-
correct parse.
? The parser can not find an analysis that spans
the full sentence. If that sentence is allowed
in the language, the grammar or lexicon is in-
complete.
While the first type of errors can be alleviated
by improving the disambiguation model, the sec-
ond type of problems requires extension of the
grammar or lexicon. Finding incomplete descrip-
tions by hand can become a tedious task once a
grammar has wide coverage. Error mining tech-
niques aim to find problematic words or n-grams
automatically, allowing the grammar developer to
focus on frequent and highly suspicious forms
first.
2 Previous work
In the past, two major error mining techniques
have been developed by Van Noord (2004) and
Sagot and de la Clergerie (2006). In this paper we
propose a generalized error miner that combines
the strengths of these methods. Both methods fol-
low the same basic principle: first, a large (unan-
notated) corpus is parsed. After parsing, the sen-
tences can be split up in a list of parsable and a list
of unparsable sentences. Words or n-grams that
occur in the list of unparsable sentences, but that
do not occur in the list of parsable sentences have
a high suspicion of being the cause of the parsing
error.
2.1 Suspicion as a ratio
Van Noord (2004) defines the suspicion of a word
as a ratio:
S(w) =
C(w|error)
C(w)
(1)
where C(w) is the number of occurrences of
word w in all sentences, and C(w|error) is the
71
number of occurrences of w in unparsable sen-
tences. Of course, it is often useful to look at n-
grams as well. For instance, Van Noord (2004)
gives an example where the word via had a low
suspicion after parsing a corpus with the Dutch
Alpino parser, while the Dutch expression via via
(via a complex route) was unparsable.
To account for such phenomena, the notion of
suspicion is extended to n-grams:
S(wi..wj) =
C(wi..wj |error)
C(wi..wj)
(2)
Where a longer sequence wh...wi...wj ...wk is
only considered if its suspicion is higher than each
of its substrings:
S(wh...wi...wj ...wk) > S(wi...wj) (3)
While this method works well for forms that are
unambiguously suspicious, it also gives forms that
just happened to occur often in unparsable sen-
tences by ?bad luck? a high suspicion. If the occur-
rences in unparsable sentences were accompanied
by unambiguously suspicious forms, there is even
more reason to believe that the form is not prob-
lematic. However, in such cases this error mining
method will still assign a high suspicion to such
forms.
2.2 Iterative error mining
The error mining method described by Sagot and
de la Clergerie (2006) alleviates the problem of
?accidentally suspicious? forms. It does so by
taking the following characteristics of suspicious
forms into account:
? If a form occurs within parsable sentences, it
becomes less likely that the form is the cause
of a parsing error.
? The suspicion of a form should depend on the
suspicions of other forms in the unparsable
sentences in which it occurs.
? A form observed in a shorter sentence is ini-
tially more suspicious than a form observed
in a longer sentence.
To be able to handle the suspicion of a form
within its context, this method introduces the no-
tion of observation suspicion, which is the suspi-
cion of a form within a given sentence. The suspi-
cion of a form, outside the context of a sentence,
is then defined to be the average of all observation
suspicions:
Sf =
1
|Of |
?
oi,j?Of
Si,j (4)
HereOf is the set of all observations of the form
f , oi,j is the jth form of the ith sentence, and Si,j
is the observation suspicion of oi,j . The observa-
tion suspicions themselves are dependent on the
form suspicions, making the method an iterative
process. The suspicion of an observation is the
suspicion of its form, normalized by suspicions of
other forms occurring within the same sentence:
S(n+1)i,j = error(si)
S(n+1)F (oi,j)
?
1?j?|Si| S
(n+1)
F (oi,j)
(5)
Here error(si) is the sentence error rate, which
is normally set to 0 for parsable sentences and 1
for unparsable sentences. SF (oi,j) is the suspicion
of the form of observation oi,j .
To accommodate the iterative process, we will
have to redefine the form suspicion to be depen-
dent on the observation suspicions of the previous
cycle:
S(n+1)f =
1
|Of |
?
oi,j?Of
S(n)i,j (6)
Since there is a recursive dependence between
the suspicions and the observation suspicions,
starting and stopping conditions need to be defined
for this cyclic process. The observation suspicions
are initialized by uniformly distributing suspicion
over observed forms within a sentence:
S(0)i,j =
error(si)
|Si|
(7)
The mining is stopped when the process reaches
a fixed point where suspicions have stabilized.
This method solves the ?suspicion by accident?
problem of ratio-based error mining. However, the
authors of the paper have only used this method to
mine on unigrams and bigrams. They note that
they have tried mining with longer n-grams, but
encountered data sparseness problems. Their pa-
per does not describe criteria to determine when to
use unigrams and when to use bigrams to represent
forms within a sentence.
3 N-gram expansion
3.1 Inclusion of n-grams
While the iterative miner described by Sagot and
de la Clergerie (2006) only mines on unigrams and
72
bigrams, our prior experience with the miner de-
scribed by Van Noord (2004) has shown that in-
cluding longer n-grams in the mining process can
capture many additional phenomena. To give one
example: the words de (the), eerste (first), and
beste (best) had very low suspicions during er-
ror mining, while the trigram eerste de beste had
a very high suspicion. This trigram occurred in
the expression de eerste de beste (the first you can
find). While the individual words within this ex-
pression were described by the lexicon, this multi-
word expression was not.
3.2 Suspicion sharing
It may seem to be attractive to include all n-grams
within a sentence in the mining process. However,
this is problematic due to suspicion sharing. For
instance, consider the trigram w1, w2, w3 in which
w2 is the cause of a parsing error. In this case,
the bigrams w1, w2 and w2, w3 will become sus-
picious, as well as the trigram w1, w2, w3. Since
there will be multiple very suspicious forms within
the same sentence the unigramw2 will have no op-
portunity to manifest itself.
A more practical consideration is that the num-
ber of forms within a sentence grows at such a rate
(n + (n ? 1)... + 1) that error mining becomes
unfeasible for large corpora, both in time and in
space.
3.3 Expansion method
To avoid suspicion sharing we have devised a
method for adding and expanding n-grams when
it is deemed useful. This method iterates through
a sentence of unigrams, and expands unigrams to
longer n-grams when there is evidence that it is
useful. This expansion step is a preprocessor to
the iterative miner, that uses the same iterative al-
gorithm as described by Sagot and De la Clergerie.
Within this preprocessor, suspicion is defined in
the same manner as in Van Noord (2004), as a ra-
tio of occurrences in unparsable sentences and the
total number of occurrences.
The motivation behind this method is that there
can be two expansion scenarios. When we have
the bigram w1, w2, either one of the unigrams can
be problematic or the bigram w1, w2. In the for-
mer case, the bigram w1, w2 will also inherit the
high suspicion of the problematic unigram. In the
latter case, the bigram will have a higher suspicion
than both of its unigrams. Consequently, we want
to expand the unigram w1 to the bigram w1, w2 if
the bigram is more suspicious than both of its un-
igrams. If w1, w2 is equally suspicious as one of
its unigrams, it is not useful to expand to a bigram
since we want to isolate the cause of the parsing
error as much as possible.
The same methodology is followed when we
expand to longer n-grams. Expansion of w1, w2
to the trigram w1, w2, w3 will only be permitted
if w1, w2, w3 is more suspicious than its bigrams.
Since the suspicion of w3 aggregates to w2, w3,
we account for both w3 and w2, w3 in this com-
parison.
The general algorithm is that the expansion to
an n-gram i..j is allowed when S(i..j) > S(i..j?
1) and S(i..j) > S(i + 1..j). This gives us a sen-
tence that is represented by the n-grams n0..nx,
n1..ny, ... n|si|?1..n|si|?1.
3.4 Data sparseness
While initial experiments with the expansion al-
gorithm provided promising results, the expansion
algorithm was too eager. This eagerness is caused
by data sparseness. Since longer n-grams occur
less frequently, the suspicion of an n-gram oc-
curring in unparsable sentences goes up with the
length of the n-gram until it reaches its maximum
value. The expansion conditions do not take this
effect into account.
To counter this problem, we have introduced an
expansion factor. This factor depends on the fre-
quency of an n-gram within unparsable sentences
and asymptotically approaches one for higher fre-
quencies. As a result more burden of proof
is inflicted upon the expansion: the longer n-
gram either needs to be relatively frequent, or it
needs to be much more suspicious than its (n-1)-
grams. The expansion conditions are changed to
S(i..j) > S(i..j ? 1) ? extFactor and S(i..j) >
S(i + 1..j) ? extFactor, where
extFactor = 1 + e??|Of,unparsable| (8)
In our experiments ? = 1.0 proved to be a good
setting.
3.5 Pattern expansion
Previous work on error mining was primarily fo-
cused on the extraction of interesting word n-
grams. However, it could also prove useful to al-
low for patterns consisting of other information
than words, such as part of speech tags or lemmas.
We have done preliminary work on the integra-
tion of part of speech tags during the n-gram ex-
73
pansion. We use the same methodology as word-
based n-gram expansion, however we also con-
sider expansion with a part of speech tag.
Since we are interested in building patterns that
are as general as possible, we expand the pat-
tern with a part of speech tag if that creates a
more suspicious pattern. Expansion with a word
is attempted if expansion with a part of speech
tag is unsuccessful. E.g., if we attempt to ex-
pand the word bigram w1w2, we first try the tag
expansion w1w2t3. This expansion is allowed
when S(w1, w2, t3) > S(w1, w2) ? extFactor
and S(w1, w2, t3) > S(w2, t3) ? extFactor. If
the expansion is not allowed, then expansion to
S(w1, w2, w3) is attempted. As a result, mixed
patterns emerge that are as general as possible.
4 Implementation
4.1 Compact representation of data
To be able to mine large corpora some precau-
tions need to be made. During the n-gram expan-
sion stage, we need quick access to the frequen-
cies of arbitrary length n-grams. Additionally, all
unparsable sentences have to be kept in memory,
since we have to traverse them for n-gram expan-
sion. Ordinary methods for storing n-gram fre-
quencies (such as hash tables) and data will not
suffice for large corpora.
As Van Noord (2004) we used perfect hashing
to restrict memory use, since hash codes are gen-
erally shorter than the average token length. Addi-
tionally, comparisons of numbers are much faster
than comparisons of strings, which speeds up the
n-gram expansion step considerably.
During the n-gram expansion step the miner
calculates ratio-based suspicions of n-grams us-
ing frequencies of an n-gram in parsable and un-
parsable sentences. The n-gram can potentially
have the length of a whole sentence, so it is not
practical to store n-gram ratios in a hash table.
Instead, we compute a suffix array (Manber and
Myers, 1990) for the parsable and unparsable sen-
tences1. A suffix array is an array that contains in-
dices pointing to sequences in the data array, that
are ordered by suffix.
We use suffix arrays differently than Van No-
ord (2004), because our expansion algorithm re-
quires the parsable and unparsable frequencies of
the (n-1)-grams, and the second (n-1)-gram is not
1We use the suffix sorting algorithm by Peter M. McIlroy
and M. Douglas McIlroy.
(necessarily) adjacent to the n-gram in the suffix
array. As such, we require random access to fre-
quencies of n-grams occurring in the corpus. We
can compute the frequency of any n-gram by look-
ing up its upper and lower bounds in the suffix ar-
ray2, where the difference is the frequency.
4.2 Determining ratios for pattern expansion
While suffix arrays provide a compact and rela-
tively fast data structure for looking up n-gram fre-
quencies, they are not usable for pattern expansion
(see section 3.5). Since we need to look up fre-
quencies of every possible combination of repre-
sentations that are used, we would have to create
dl suffix arrays to be (theoretically) able to look
up pattern frequencies with the same time com-
plexity, where d is the number of dimensions and
l is the corpus length.
For this reason, we use a different method for
calculating pattern frequencies. First, we build a
hash table for each type of information that can
be used in patterns. A hash table contains an in-
stance of such information as a key (e.g. a specific
word or part of speech tag) and a set of corpus in-
dices where the instance occurred in the corpus as
the value associated with that key. Now we can
look up the frequency of a sequence i..j by calcu-
lating the set intersection of the indices of j and
the indices found for the sequence i..j ? 1, after
incrementing the indices of i..j ? 1 by one.
The complexity of calculating frequencies fol-
lowing this method is linear, since the set of in-
dices for a given instance can be retrieved with
a O(1) time complexity, while both increment-
ing the set indices and set intersection can be per-
formed in O(n) time. However, n can be very
large: for instance, the start of sentence marker
forms a substantial part of the corpus and is looked
up once for every sentence. In our implementation
we limit the time spent on such patterns by caching
very frequent bigrams in a hash table.
4.3 Removing low-suspicion forms
Since normally only one form within a sentence
will be responsible for a parsing error, many forms
will have almost no suspicion at all. However, dur-
ing the mining process, their suspicions will be
recalculated during every cycle. Mining can be
sped up considerably by removing forms that have
a negligible suspicion.
2Since the suffix array is sorted, finding the upper and
lower bounds is a binary search in O(log n) time.
74
If we do not drop forms, mining of the Dutch
Wikipedia corpus described in section 5.3, with
n-gram expansion and the extension factor en-
abled, resulted in 4.8 million forms with 13.4 mil-
lion form observations in unparsable sentences. If
we mine the same material and drop forms with
a suspicion below 0.001 there were 3.5 million
forms and 4.0 million form observations within
unparsable sentences left at the end of the iterative
mining process.
5 Evaluation
5.1 Methodology
In previous articles, error mining methods have
primarily been evaluated manually. Both Van No-
ord (2004) and Sagot and de la Clergerie (2006)
make a qualitative analysis of highly suspicious
forms. But once one starts experimenting with var-
ious extensions, such as n-gram expansion and ex-
pansion factor functions, it is difficult to qualify
changes through small-scale qualitative analysis.
To be able to evaluate changes to the error
miner, we have supplemented qualitative analysis
with a automatic quantitative evaluation method.
Since error miners are used by grammar engineers
to correct a grammar or lexicon by hand, the eval-
uation metric should model this use case:
? We are interested in seeing problematic forms
that account for errors in a large number of
unparsable sentences first.
? We are only interested in forms that actually
caused the parsing errors. Analysis of forms
that do not, or do not accurately pinpoint ori-
gin of the parsing errors costs a lot of time.
These requirements map respectively to the re-
call and precision metrics from information re-
trieval:
P =
|{Sunparsable} ? {Sretrieved}|
|{Sretrieved}|
(9)
R =
|{Sunparsable} ? {Sretrieved}|
|{Sunparsable}|
(10)
Consequently, we can also calculate the f-score
(van Rijsbergen, 1979):
F ? score =
(1 + ?2) ? (P ? R)
(?2 ? P + R)
(11)
The f-score is often used with ? = 1.0 to give
as much weight to precision as recall. In evalu-
ating error mining, this can permit cheating. For
instance, consider an error mining that recalls the
start of sentence marker as the first problematic
form. Such a strategy would instantly give a re-
call of 1.0, and if the coverage of a parser for a
corpus is relatively low, a relatively good initial f-
score will be obtained. Since error mining is often
used in situations where coverage is still low, we
give more bias to precision by using ? = 0.5.
We hope to provide more evidence in the future
that this evaluation method indeed correlates with
human evaluation. But in our experience it has the
required characteristics for the evaluation of error
mining. For instance, it is resistant to recalling
of different or overlapping n-grams from the same
sentences, or recalling n-grams that occur often in
both parsable and unparsable sentences.
5.2 Scoring methods
After error mining, we can extract a list of forms
and suspicions, and order the forms by their sus-
picion. But normally we are not only interested in
forms that are the most suspicious, but forms that
are suspicious and frequent. Sagot and de la Clerg-
erie (2006) have proposed three scoring methods
that can be used to rank forms:
? Concentrating on suspicions: Mf = Sf
? Concentrating on most frequent potential er-
rors: Mf = Sf |Of |
? Balancing between these possibilities: Mf =
Sf ? ln|Of |
For our experiments, we have replaced the ob-
servation frequencies of the form (|Of |) by the
frequency of observations within unparsable sen-
tences (|{Of,unparsable}|). This avoids assigning a
high score to very frequent unsuspicious forms.
5.3 Material
In our experiments we have used two corpora that
were parsed with the wide-coverage Alpino parser
and grammar for Dutch:
? Quantitative evaluation was performed on the
Dutch Wikipedia of August 20083. This cor-
pus consists of 7 million sentences (109 mil-
lion words). For 8.4% of the sentences no full
analysis could be found.
3http://ilps.science.uva.nl/WikiXML/
75
? A qualitative evaluation of the extensions was
performed on the Flemish Mediargus news-
paper corpus (up to May 31, 2007)4. This
corpus consists of 67 million sentences (1.1
billion words). For 9.2% of the sentences no
full analysis could be found.
Flemish is a variation of Dutch written and spo-
ken in Belgium, with a grammar and lexicon that
deviates slightly from standard Dutch. Previously,
the Alpino grammar and lexicon was never specif-
ically modified for parsing Flemish.
6 Results
6.1 Iterative error mining
We have evaluated the different mining methods
with the three scoring functions discussed in sec-
tion 5.2. In the results presented in this section we
only list the results with the scoring function that
performed best for a given error mining method
(section 6.3 provides an overview of the best scor-
ing functions for different mining methods).
Our first interest was if, and how much itera-
tive error mining outperforms error mining with
suspicion as a ratio. To test this, we compared
the method described by Van Noord (2004) and
the iterative error miner of Sagot and de la Clerg-
erie (2006). For the iterative error miner we eval-
uated both on unigrams, and on unigrams and bi-
grams where all unigrams and bigrams are used
(without further selection). Figure 6.1 shows the
f-scores for these miners after N retrieved forms.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  2000  4000  6000  8000  10000
F 0.5-
Scor
e
N
ratioiter.unigramsiter.uni.bigrams
Figure 1: F-scores after retrieving N forms for
ratio-based mining, iterative mining on unigrams
and iterative mining on uni- and bigrams.
4http://www.mediargus.be/
The unigram iterative miner outperforms the
ratio-based miner during the retrieval of the first
8000 forms. The f-score graph of the iterative
miner on unigrams flattens after retrieving about
4000 forms. At that point unigrams are not spe-
cific enough anymore to pinpoint more sophisti-
cated problems. The iterative miner on uni- and bi-
grams performs better than the ratio-based miner,
even beyond 8000 forms. More importantly, the
curves of the iterative miners are steeper. This is
relevant if we consider that a grammar engineer
will only look at a few thousands of forms. For
instance, the ratio-based miner achieves an f-score
of 0.4 after retrieving 8448 forms, while the iter-
ative miner on uni- and bigrams attains the same
f-score after retrieving 5134 forms.
6.2 N-gram expansion
In our second experiment we have compared the
performance of iterative mining on uni- and bi-
grams with an iterative miner using the n-gram
expansion algorithm described in section 3. Fig-
ure 6.2 shows the result of n-gram expansion com-
pared to mining just uni- and bigrams. Both the
results for expansion with and without use of the
expansion factor are shown.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  2000  4000  6000  8000  10000
F 0.5-
Scor
e
N
iter.uni.bigramiter.expansioniter.expansion.ef
Figure 2: F-scores after retrieving N forms for it-
erative mining on uni- and bigrams, and iterative
mining using n-gram expansion with and without
using an expansion factor.
We can see that the expansion to longer n-grams
gives worse results than mining on uni- and bi-
grams when data sparseness is not accounted for.
The expansion stage will select forms that may be
accurate, but that are more specific than needed.
As such, the recall per retrieved form is lower on
76
average, as can be seen in figure 6.2. But if sparse-
ness is taken into account through the use of the
expansion factor, we achieve higher f-scores than
mining on uni- and bigrams up to the retrieval of
circa five thousand forms. Since a user of an error
mining tool will probably only look at the first few
thousands of forms, this is a welcome improve-
ment.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0  2000  4000  6000  8000  10000
Reca
ll
N
iter.uni.bigramiter.expansioniter.expansion.ef
Figure 3: Recall after retrieving N forms for it-
erative mining on uni- and bigrams, and iterative
mining using n-gram expansion with and without
using an expansion factor.
Among the longer n-grams in the mining results
for the Mediargus corpus, we found many Flemish
idiomatic expressions that were not described in
the Alpino lexicon. For example:
? had er (AMOUNT) voor veil [had
(AMOUNT) for sale]
? (om de muren) van op te lopen [to get terribly
annoyed by]
? Ik durf zeggen dat [I dare to say that]
? op punt stellen [to fix/correct something]
? de daver op het lijf [shocked]
? (op) de tippen (van zijn tenen) [being very
careful]
? ben fier dat [am proud of]
? Nog voor halfweg [still before halfway]
? (om duimen en vingers) van af te likken [de-
licious]
Since these expressions are longer than bi-
grams, they cannot be captured properly without
using n-gram expansion. We also found longer
n-grams describing valid Dutch phrases that were
not described by the grammar or lexicon.
? Het stond in de sterren geschreven dat [It was
written in the stars that]
? zowat de helft van de [about half of the]
? er zo goed als zeker van dat [almost sure of]
? laat ons hopen dat het/dit lukt [let us hope that
it/this works]
6.3 Scoring methods
The miners that use n-gram expansion perform
best with the Mf = Sf |Of | function, while the
other miners perform best with the Mf = Sf ?
ln|Of | function. This is not surprising ? the it-
erative miners that do not use n-gram expansion
can not make very specific forms and give rela-
tively high scores to forms that happen to occur in
unparsable sentences (since some forms in a sen-
tence will have to take blame, if no specific sus-
picious form is found). If such forms also hap-
pen to be frequent, they may be ranked higher
than some more suspicious infrequent forms. In
the case of the ratio-based miner, there are many
forms that are ?suspicious by accident? which may
become highly ranked when they are more fre-
quent than very suspicious, but infrequent forms.
Since the miners with n-gram expansion can find
specific suspicious forms and shift blame to them,
there is less chance of accidentally ranking a form
to highly by directly including the frequency of
observations of that form within unparsable sen-
tences in the scoring function.
6.4 Pattern expansion
We have done some preliminary experiments with
pattern expansion, allowing for patterns consisting
of words and part of speech tags. For this exper-
iment we trained a Hidden Markov Model part of
speech tagger on 90% of the Dutch Eindhoven cor-
pus using a small tag set. We then extracted 50000
unparsable and about 495000 parsable sentences
from the Flemish Mediargus corpus. The pattern
expansion preprocessor was then used to find in-
teresting patterns.
We give two patterns that were extracted to give
an impression how patterns can be useful. A fre-
quent pattern was doorheen N (through followed
77
by a (proper) noun). In Flemish a sentence such
as We reden met de auto doorheen Frankrijk (lit-
eral: We drove with the car through France) is al-
lowed, while in standard Dutch the particle heen
is separated from the preposition door. Conse-
quently, the same sentence in standard Dutch is We
reden met de auto door Frankrijk heen. Mining
on word n-grams provided hints for this difference
in Flemish through forms such as doorheen Krot-
tegem, doorheen Engeland, doorheen Hawai, and
doorheen Middelkerke, but the pattern provides a
more general description with a higher frequency.
Another pattern that was found is wegens Prep
Adj (because of followed by a preposition and
an adjective). This pattern captures prepositional
modifiers where wegens is the head, and the fol-
lowing words within the constituent form an ar-
gument, such as in the sentence Dat idee werd
snel opgeborgen wegens te duur (literal: That idea
became soon archived because of too expensive).
This pattern provided a more general description
of forms such as wegens te breed (because it is
too wide), wegens te deprimerend (because it is
too depressing), wegens niet rendabel (because it
is not profitable), and wegens te ondraaglijk (be-
cause it is too unbearable).
While instances of both patterns were found us-
ing the word n-gram based miner, patterns consol-
idate different instances. For example, there were
120 forms with a high suspicion containing the
word wegens. If such a form is corrected, the other
examples may still need to be checked to see if a
solution to the parsing problem is comprehensive.
The pattern gives a more general description of the
problem, and as such, most of these 120 forms can
be represented by the pattern wegens Prep Adj.
Since we are still optimizing the pattern ex-
pander to scale to large corpora, we have not per-
formed an automatic evaluation using the Dutch
Wikipedia yet.
7 Conclusions
We combined iterative error mining with expan-
sion of forms to n-grams of an arbitrary length,
that are long enough to capture interesting phe-
nomena, but not longer. We dealt with the prob-
lem of data sparseness by introducing an expan-
sion factor that softens when the expanded form is
very frequent.
In addition to the generalization of iterative er-
ror mining, we introduced a method for automatic
evaluation. This allows us to test modifications to
the error miner without going through the tedious
task of ranking and judging the results manually.
Using this automatic evaluation method, we
have shown that iterative error mining improves
upon ratio-based error mining. As expected,
adding bigrams improves performance. Allowing
expansion beyond bigrams can lead to data sparse-
ness problems, but if we correct for data sparse-
ness the performance of the miner improves over
mining on just unigrams and bigrams.
We have also described preliminary work on
a preprocessor that allows for more general pat-
terns that incorporate additional information, such
as part of speech tags and lemmas. We hope to
optimize and improve pattern-based mining in the
future and evaluate it automatically on larger cor-
pora.
The error mining methods described in this pa-
per are generic, and can be used for any grammar
or parser, as long as the sentences within the cor-
pus can be divided in a list of parsable and un-
parsable sentences. The error miner is freely avail-
able5, and is optimized to work on large corpora.
The source distribution includes a graphical user
interface for browsing mining results, showing the
associated sentences, and removing forms when
they have been corrected in the grammar or lex-
icon.
References
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage Computational Anal-
ysis of Dutch. In Computational Linguistics in The
Netherlands 2000.
Ann Copestake and Dan Flickinger. 2000. An
open source grammar development environment and
broad-coverage English grammar using HPSG. In
Proceedings of LREC 2000, pages 591?600.
Udi Manber and Gene Myers. 1990. Suffix arrays: a
new method for on-line string searches. In SODA
?90: Proceedings of the first annual ACM-SIAM
symposium on Discrete algorithms, pages 319?327.
Society for Industrial and Applied Mathematics.
Beno??t Sagot and E?ric de la Clergerie. 2006. Error
mining in parsing results. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 329?336, Morristown, NJ, USA. Association
for Computational Linguistics.
5http://www.let.rug.nl/?dekok/errormining/
78
Gertjan Van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL ?04: Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 446, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
C. J. van Rijsbergen. 1979. Information retrieval. But-
terworths, London, 2 edition.
79
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 194?199,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reversible Stochastic Attribute-Value Grammars
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Barbara Plank
University of Groningen
b.plank@rug.nl
Gertjan van Noord
University of Groningen
g.j.m.van.noord@rug.nl
Abstract
An attractive property of attribute-value gram-
mars is their reversibility. Attribute-value
grammars are usually coupled with sepa-
rate statistical components for parse selection
and fluency ranking. We propose reversible
stochastic attribute-value grammars, in which
a single statistical model is employed both for
parse selection and fluency ranking.
1 Introduction
Reversible grammars were introduced as early as
1975 by Martin Kay (1975). In the eighties, the
popularity of attribute-value grammars (AVG) was
in part motivated by their inherent reversible na-
ture. Later, AVG were enriched with a statistical
component (Abney, 1997): stochastic AVG (SAVG).
Training a SAVG is feasible if a stochastic model
is assumed which is conditioned on the input sen-
tences (Johnson et al, 1999). Various parsers based
on this approach now exist for various languages
(Toutanova et al, 2002; Riezler et al, 2002; van
Noord and Malouf, 2005; Miyao and Tsujii, 2005;
Clark and Curran, 2004; Forst, 2007). SAVG can be
applied for generation to select the most fluent real-
ization from the set of possible realizations (Velldal
et al, 2004). In this case, the stochastic model is
conditioned on the input logical forms. Such gener-
ators exist for various languages as well (Velldal and
Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et
al., 2007; de Kok and van Noord, 2010).
If an AVG is applied both to parsing and gen-
eration, two distinct stochastic components are re-
quired, one for parsing, and one for generation. To
some extent this is reasonable, because some fea-
tures are only relevant in a certain direction. For
instance, features that represent aspects of the sur-
face word order are important for generation, but ir-
relevant for parsing. Similarly, features which de-
scribe aspects of the logical form are important for
parsing, but irrelevant for generation. Yet, there are
also many features that are relevant in both direc-
tions. For instance, for Dutch, a very effective fea-
ture signals a direct object NP in fronted position in
main clauses. If a main clause is parsed which starts
with a NP, the disambiguation component will fa-
vor a subject reading of that NP. In generation, the
fluency component will favor subject fronting over
object fronting. Clearly, such shared preferences are
not accidental.
In this paper we propose reversible SAVG in
which a single stochastic component is applied both
in parsing and generation. We provide experimen-
tal evidence that such reversible SAVG achieve sim-
ilar performance as their directional counterparts.
A single, reversible model is to be preferred over
two distinct models because it explains why pref-
erences in a disambiguation component and a flu-
ency component, such as the preference for subject
fronting over object fronting, are shared. A single,
reversible model is furthermore of practical inter-
est for its simplicity, compactness, and maintainabil-
ity. As an important additional advantage, reversible
models are applicable for tasks which combine as-
pects of parsing and generation, such as word-graph
parsing and paraphrasing. In situations where only a
small amount of training data is available for parsing
or generation, cross-pollination improves the perfor-
194
mance of a model. If preferences are shared between
parsing and generation, it follows that a generator
could benefit from parsing data and vice versa. We
present experimental results indicating that in such a
bootstrap scenario a reversible model achieves better
performance.
2 Reversible SAVG
As Abney (1997) shows, we cannot use relatively
simple techniques such as relative frequencies to
obtain a model for estimating derivation probabili-
ties in attribute-value grammars. As an alternative,
he proposes a maximum entropy model, where the
probability of a derivation d is defined as:
p(d) =
1
Z
exp
?
i
?ifi(d) (1)
fi(d) is the frequency of feature fi in derivation
d. A weight ?i is associated with each feature fi.
In (1), Z is a normalizer which is defined as fol-
lows, where ? is the set of derivations defined by
the grammar:
Z =
?
d???
exp
?
i
?ifi(d
?) (2)
Training this model requires access to all derivations
? allowed by the grammar, which makes it hard to
implement the model in practice.
Johnson et al (1999) alleviate this problem by
proposing a model which conditions on the input
sentence s: p(d|s). Since the number of derivations
for a given sentence s is usually finite, the calcula-
tion of the normalizer is much more practical. Con-
versely, in generation the model is conditioned on
the input logical form l, p(d|l) (Velldal et al, 2004).
In such directional stochastic attribute-value gram-
mars, the probability of a derivation d given an input
x (a sentence or a logical form) is defined as:
p(d|x) =
1
Z(x)
exp
?
i
?ifi(x, d) (3)
with Z(x) as (?(x) are all derivations for input x):
Z(x) =
?
d???(x)
exp
?
i
?ifi(x, d
?) (4)
Consequently, the constraint put on feature values
during training only refers to derivations with the
same input. If X is the set of inputs (for parsing,
all sentences in the treebank; for generation, all log-
ical forms), then we have:
Ep(fi)? Ep?(fi) = 0 ? (5)
?
x?X
?
d??(x)
p?(x)p(d|x)fi(x, d)? p?(x, d)fi(x, d) = 0
Here we assume a uniform distribution for p?(x).
Let j(d) be a function which returns 0 if the deriva-
tion d is inconsistent with the treebank, and 1 in case
the derivation is correct. p?(x, d) is now defined in
such a way that it is 0 for incorrect derivations, and
uniform for correct derivations for a given input:
p?(x, d) = p?(x)
j(d)
?d???(x)j(d?)
(6)
Directional SAVG make parsing and generation
practically feasible, but require separate models for
parse disambiguation and fluency ranking.
Since parsing and generation both create deriva-
tions that are in agreement with the constraints im-
plied by the input, a single model can accompany
the attribute-value grammar. Such a model estimates
the probability of a derivation d given a set of con-
straints c, p(d|c). We use conditional maximum en-
tropy models to estimate p(d|c):
p(d|c) =
1
Z(c)
exp
?
i
?ifi(c, d) (7)
Z(c) =
?
d???(c)
exp
?
i
?ifi(c, d
?) (8)
We derive a reversible model by training on data
for parse disambiguation and fluency ranking simul-
taneously. In contrast to directional models, we im-
pose the two constraints per feature given in figure 1:
one on the feature value with respect to the sentences
S in the parse disambiguation treebank and the other
on the feature value with respect to logical forms L
in the fluency ranking treebank. As a result of the
constraints on training defined in figure 1, the fea-
ture weights in the reversible model distinguish, at
the same time, good parses from bad parses as well
as good realizations from bad realizations.
3 Experimental setup and evaluation
To evaluate reversible SAVG, we conduct experi-
ments in the context of the Alpino system for Dutch.
195
?s?S
?
d??(s)
p?(s)p(d|c = s)fi(s, d)? p?(c = s, d)fi(s, d) = 0
?
l?L
?
d??(l)
p?(l)p(d|c = l)fi(l, d)? p?(c = l, d)fi(l, d) = 0
Figure 1: Constraints imposed on feature values for training reversible models p(d|c).
Alpino provides a wide-coverage grammar, lexicon
and parser (van Noord, 2006). Recently, a sentence
realizer has been added that uses the same grammar
and lexicon (de Kok and van Noord, 2010).
In the experiments, the cdbl part of the Alpino
Treebank (van der Beek et al, 2002) is used as train-
ing data (7,154 sentences). The WR-P-P-H part
(2,267 sentences) of the LASSY corpus (van Noord
et al, 2010), which consists of text from the Trouw
2001 newspaper, is used for testing.
3.1 Features
The features that we use in the experiment are the
same features which are available in the Alpino
parser and generator. In the following section, these
features are described in some detail.
Word adjacency. Two word adjacency features
are used as auxiliary distributions (Johnson and Rie-
zler, 2000). The first feature is the probability of the
sentence according to a word trigram model. The
second feature is the probability of the sentence ac-
cording to a tag trigram model that uses the part-
of-speech tags assigned by the Alpino system. In
both models, linear interpolation smoothing for un-
known trigrams, and Laplacian smoothing for un-
known words and tags is applied. The trigram mod-
els have been trained on the Twente Nieuws Corpus
corpus (approximately 110 million words), exclud-
ing the Trouw 2001 corpus. In conventional pars-
ing tasks, the value of the word trigram model is the
same for all derivations of a given input sentence.
Lexical frames. Lexical analysis is applied dur-
ing parsing to find all possible subcategorization
frames for the tokens in the input sentence. Since
some frames occur more frequently in good parses
than others, we use feature templates that record the
frames that were used in a parse. An example of
such a feature is: ??to play? serves as an intransi-
tive verb?. We also use an auxiliary distribution of
word and frame combinations that was trained on
a large corpus of automatically annotated sentences
(436 million words). The values of lexical frame
features are constant for all derivations in sentence
realization, unless the frame is not specified in the
logical form.
Dependency relations. There are also feature
templates which describe aspects of the dependency
structure. For each dependency, three types of de-
pendency features are extracted. Examples of such
features are ?a pronoun is used as the subject of
a verb?, ?the pronoun ?she? is used as the sub-
ject of a verb?, ?the noun ?beer? is used as the
object of the verb ?drink??. In addition, features
are used which implement auxiliary distributions
for selectional preferences, as described in Van No-
ord (2007). In conventional realization tasks, the
values of these features are constant for all deriva-
tions for a given input representation.
Syntactic features. Syntactic features include fea-
tures which record the application of each grammar
rule, as well as features which record the application
of a rule in the context of another rule. An exam-
ple of the latter is ?rule 167 is used to construct the
second daughter of a derivation constructed by rule
233?. In addition, there are features describing more
complex syntactic patterns such as: fronting of sub-
jects and other noun phrases, orderings in the middle
field, long-distance dependencies, and parallelism of
conjuncts in coordination.
3.2 Parse disambiguation
Earlier we assumed that a treebank is a set of cor-
rect derivations. In practice, however, a treebank
only contains an abstraction of such derivations (in
196
our case sentences with corresponding dependency
structures), thus abstracting away from syntactic de-
tails needed in a parse disambiguation model. As in
Osborne (2000), the derivations for the parse disam-
biguation model are created by parsing the training
corpus. In the current setting, up to at most 3000
derivations are created for every sentence. These
derivations are then compared to the gold standard
dependency structure to judge the quality of the
parses. For a given sentence, the parses with the
highest concept accuracy (van Noord, 2006) are con-
sidered correct, the rest is treated as incorrect.
3.3 Fluency ranking
For fluency ranking we also need access to full
derivations. To ensure that the system is able to
generate from the dependency structures in the tree-
bank, we parse the corresponding sentence, and se-
lect the parse with the dependency structure that
corresponds most closely to the dependency struc-
ture in the treebank. The resulting dependency
structures are fed into the Alpino chart generator
to construct derivations for each dependency struc-
ture. The derivations for which the corresponding
sentences are closest to the original sentence in the
treebank are marked correct. Due to a limit on gen-
eration time, some longer sentences and correspond-
ing dependency structures were excluded from the
data. As a result, the average sentence length was
15.7 tokens, with a maximum of 26 tokens. To com-
pare a realization to the correct sentence, we use the
General Text Matcher (GTM) method (Melamed et
al., 2003; Cahill, 2009).
3.4 Training the models
Models are trained by taking an informative sam-
ple of ?(c) for each c in the training data (Osborne,
2000). This sample consists of at most 100 ran-
domly selected derivations. Frequency-based fea-
ture selection is applied (Ratnaparkhi, 1999). A fea-
ture f partitions ?(c), if there are derivations d and
d? in ?(c) such that f(c, d) 6= f(c, d?). A feature is
used if it partitions the informative sample of ?(c)
for at least two c. Table 1 lists the resulting charac-
teristics of the training data for each model.
We estimate the parameters of the conditional
Features Inputs Derivations
Generation 1727 3688 141808
Parse 25299 7133 376420
Reversible 25578 10811 518228
Table 1: Size of the training data for each model
maximum entropy models using TinyEst,1 with a
Gaussian (`2) prior distribution (? = 0, ?2 = 1000)
to reduce overfitting (Chen and Rosenfeld, 1999).
4 Results
4.1 Parse disambiguation
Table 2 shows the results for parse disambiguation.
The table also provides lower and upper bounds: the
baseline model selects an arbitrary parse per sen-
tence; the oracle chooses the best available parse.
Figure 2 shows the learning curves for the direc-
tional parsing model and the reversible model.
Model CA (%) f-score (%)
Baseline 75.88 76.28
Oracle 94.86 95.09
Parse model 90.93 91.28
Reversible 90.87 91.21
Table 2: Concept Accuracy scores and f-scores in terms
of named dependency relations for the parsing-specific
model versus the reversible model.
The results show that the general, reversible,
model comes very close to the accuracy obtained
by the dedicated, parsing specific, model. Indeed,
the tiny difference is not statistically significant. We
compute statistical significance using the Approxi-
mate Randomization Test (Noreen, 1989).
4.2 Fluency ranking
Table 3 compares the reversible model with a di-
rectional fluency ranking model. Figure 3 shows
the learning curves for the directional generation
model and the reversible model. The reversible
model achieves similar performance as the direc-
tional model (the difference is not significant).
To show that a reversible model can actually profit
from mutually shared features, we report on an ex-
periment where only a small amount of generation
1http://github.com/danieldk/tinyest
197
0.0 0.1 0.2 0.3 0.4 0.57
67
88
08
28
48
68
89
0
Proportion parse training data
CA (%
)
parse modelreversible model
Figure 2: Learning curve for directional and reversible
models for parsing. The reversible model uses all training
data for generation.
Model GTM
Random 55.72
Oracle 86.63
Fluency 71.82
Reversible 71.69
Table 3: General Text Matcher scores for fluency ranking
using various models.
training data is available. In this experiment, we
manually annotated 234 dependency structures from
the cdbl part of the Alpino Treebank, by adding cor-
rect realizations. In many instances, there is more
than one fluent realization. We then used this data to
train a directional fluency ranking model and a re-
versible model. The results for this experiment are
shown in Table 4. Since the reversible model outper-
forms the directional model we conclude that indeed
fluency ranking benefits from parse disambiguation
data.
Model GTM
Fluency 70.54
Reversible 71.20
Table 4: Fluency ranking using a small amount of anno-
tated fluency ranking training data (difference is signifi-
cant at p < 0.05).
0.0 0.1 0.2 0.3 0.4 0.5
60
65
70
Proportion generation training data
GTM
 scor
e
generation modelreversible model
Figure 3: Learning curves for directional and reversible
models for generation. The reversible models uses all
training data for parsing.
5 Conclusion
We proposed reversible SAVG as an alternative to
directional SAVG, based on the observation that
syntactic preferences are shared between parse dis-
ambiguation and fluency ranking. This framework
is not purely of theoretical interest, since the exper-
iments show that reversible models achieve accura-
cies that are similar to those of directional models.
Moreover, we showed that a fluency ranking model
trained on a small data set can be improved by com-
plementing it with parse disambiguation data.
The integration of knowledge from parse disam-
biguation and fluency ranking could be beneficial for
tasks which combine aspects of parsing and genera-
tion, such as word-graph parsing or paraphrasing.
198
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In ENLG ?07: Proceedings of the Eleventh
European Workshop on Natural Language Genera-
tion, pages 17?24, Morristown, NJ, USA.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference - Short Pa-
pers, pages 97?100.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University, Pittsburg.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of the 42nd Annual Meeting of the ACL, pages
103?110, Morristown, NJ, USA.
Danie?l de Kok and Gertjan van Noord. 2010. A sentence
generator for Dutch. In Proceedings of the 20th Com-
putational Linguistics in the Netherlands conference
(CLIN).
Martin Forst. 2007. Filling statistics with linguistics:
property design for the disambiguation of german lfg
parses. In DeepLP ?07: Proceedings of the Workshop
on Deep Linguistic Processing, pages 17?24, Morris-
town, NJ, USA.
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proceedings of the 1st Meeting of the
NAACL, pages 154?161, Seattle, Washington.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the ACL.
Martin Kay. 1975. Syntactic processing and functional
sentence perspective. In TINLAP ?75: Proceedings of
the 1975 workshop on Theoretical issues in natural
language processing, pages 12?15, Morristown, NJ,
USA.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In HLT-
NAACL.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 83?90, Morristown, NJ, USA.
Hiroko Nakanishi and Yusuke Miyao. 2005. Probabilis-
tic models for disambiguation of an hpsg-based chart
generator. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT), pages 93?
102.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Miles Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics (COLING), pages 586?592.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151?175.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the wall street journal using a
lexical-functional grammar and discriminative estima-
tion techniques. In Proceedings of the 40th Annual
Meeting of the ACL, pages 271?278, Morristown, NJ,
USA.
Kristina Toutanova, Christopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich hpsg grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT), pages 253?263, Sozopol.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
Gertjan van Noord and Robert Malouf. 2005. Wide
coverage parsing with stochastic attribute value gram-
mars. Draft available from the authors. A preliminary
version of this paper was published in the Proceedings
of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
Gertjan van Noord, Ineke Schuurman, and Gosse Bouma.
2010. Lassy syntactische annotatie, revision 19053.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement Au-
tomatique des Langues naturelles, pages 20?42, Leu-
ven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Parsing
Technology (IWPT), ACL 2007 Workshop, pages 1?
10, Prague.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 517?525, Sydney,
Australia, July. ACL.
Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004.
Paraphrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Treebanks
and Linguistic Theories (TLT), pages 149?160.
199
Feature Selection for Fluency Ranking
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Abstract
Fluency rankers are used in modern sentence
generation systems to pick sentences that are
not just grammatical, but also fluent. It has
been shown that feature-based models, such as
maximum entropy models, work well for this
task.
Since maximum entropy models allow for in-
corporation of arbitrary real-valued features,
it is often attractive to create very general
feature templates, that create a huge num-
ber of features. To select the most discrim-
inative features, feature selection can be ap-
plied. In this paper we compare three fea-
ture selection methods: frequency-based se-
lection, a generalization of maximum entropy
feature selection for ranking tasks with real-
valued features, and a new selection method
based on feature value correlation. We show
that the often-used frequency-based selection
performs badly compared to maximum en-
tropy feature selection, and that models with a
few hundred well-picked features are compet-
itive to models with no feature selection ap-
plied. In the experiments described in this pa-
per, we compressed a model of approximately
490.000 features to 1.000 features.
1 Introduction
As shown previously, maximum entropy models
have proven to be viable for fluency ranking (Nakan-
ishi et al, 2005; Velldal and Oepen, 2006; Velldal,
2008). The basic principle of maximum entropy
models is to minimize assumptions, while impos-
ing constraints such that the expected feature value
is equal to the observed feature value in the train-
ing data. In its canonical form, the probability of a
certain event (y) occurring in the context (x) is a log-
linear combination of features and feature weights,
whereZ(x) is a normalization over all events in con-
text x (Berger et al, 1996):
p(y|x) =
1
Z(x)
exp
n?
i=1
?ifi (1)
The training process estimates optimal feature
weights, given the constraints and the principle of
maximum entropy. In fluency ranking the input (e.g.
a dependency structure) is a context, and a realiza-
tion of that input is an event within that context.
Features can be hand-crafted or generated auto-
matically using very general feature templates. For
example, if we apply a template rule that enumer-
ates the rules used to construct a derivation tree to
the partial tree in figure 1 the rule(max xp(np)) and
rule(np det n) features will be created.
Figure 1: Partial derivation tree for the noun phrase de
adviezen (the advices).
To achieve high accuracy in fluency ranking
quickly, it is attractive to capture as much of the lan-
guage generation process as possible. For instance,
in sentence realization, one could extract nearly ev-
ery aspect of a derivation tree as a feature using very
general templates. This path is followed in recent
work, such as Velldal (2008). The advantage of this
approach is that it requires little human labor, and
generally gives good ranking performance. How-
ever, the generality of templates leads to huge mod-
els in terms of number of features. For instance, the
model that we will discuss contains about 490,000
features when no feature selection is applied. Such
models are very opaque, giving very little under-
standing of good discriminators for fluency ranking,
and the size of the models may also be inconvenient.
To make such models more compact and transpar-
ent, feature selection can be applied.
In this paper we make the following contribu-
tions: we modify a maximum entropy feature selec-
tion method for ranking tasks; we introduce a new
feature selection method based on statistical corre-
lation of features; we compare the performance of
the preceding feature selection methods, plus a com-
monly used frequency-based method; and we give
an analysis of the most effective features for fluency
ranking.
2 Feature Selection
2.1 Introduction
Feature selection is a process that tries to extract
S ? F from a set of features F , such that the model
using S performs comparably to the model using
F . Such a compression of a feature set can be ob-
tained if there are features: that occur sporadically;
that correlate strongly with other features (features
that show the same behavior within events and con-
texts); or have values with little or no correlation to
the classification or ranking.
Features that do have no correlation to the classi-
fication can be removed from the model. For a set
of highly-correlating features, one feature can be se-
lected to represent the whole group.
Initially it may seem attractive to perform fluency
selection by training a model on all features, select-
ing features with relatively high weights. However,
if features overlap, weight mass will usually be di-
vided over these features. For instance, suppose that
f1 alone has a weight of 0.5 in a given model. If
we retrain the model, after adding the features f2..f5
that behave identically to f1, the weight may be dis-
tributed evenly between f1..f5, giving each feature
the weight 0.1.
In the following sections, we will give a short
overview of previous research in feature selection,
and will then proceed to give a more detailed de-
scription of three feature selection methods.
2.2 Background
Feature selection can be seen as model selection,
where the best model of all models that can be
formed using a set of features should be selected.
Madigan and Raftery (1994) propose an method for
model selection aptly named Occam?s window. This
method excludes models that do not perform com-
petitively to other models or that do not perform bet-
ter than one of its submodels. Although this method
is conceptually firm, it is nearly infeasable to apply
it with the number of features used in fluency rank-
ing. Berger et al (1996) propose a selection method
that iteratively builds a maximum entropy model,
adding features that improve the model. We modify
this method for ranking tasks in section 2.5. Ratna-
parkhi (1999) uses a simple frequency-based cutoff,
where features that occur infrequently are excluded.
We discuss a variant of this selection criterium in
section 2.3. Perkins et al (2003) describe an ap-
proach where feature selection is applied as a part
of model parameter estimation. They rely on the
fact that `1 regularizers have a tendency to force a
subset of weights to zero. However, such integrated
approaches rely on parameter tuning to get the re-
quested number of features.
In the fluency ranking literature, the use of a fre-
quency cut-off (Velldal and Oepen, 2006) and `1
regularization (Cahill et al, 2007) is prevalent. We
are not aware of any detailed studies that compare
feature selection methods for fluency ranking.
2.3 Frequency-based Selection
In frequency-based selection we follow Malouf and
Van Noord (2004), and count for each feature f the
number of inputs where there are at least two realiza-
tions y1, y2, such that f(y1) 6= f(y2). We then use
the first N features with the most frequent changes
from the resulting feature frequency list.
Veldall (2008) also experiments with this selec-
tion method, and suggests to apply frequency-based
selection to fluency ranking models that will be dis-
tributed to the public (for compactness? sake). In
the variant he and Malouf and Van Noord (2004)
discuss, all features that change within more than n
contexts are included in the model.
2.4 Correlation-based Selection
While frequency-based selection helps selecting fea-
tures that are discriminative, it cannot account for
feature overlap. Discriminative features that have a
strong correlation to features that were selected pre-
viously may still be added.
To detect overlap, we calculate the correlation of a
candidate feature and exclude the feature if it shows
a high correlation with features selected previously.
To estimate Pearson?s correlation of two features, we
calculate the sample correlation coefficient,
rf1,f2 =
?
x?X,y?Y (f1(x, y)? f?1)(f2(x, y)? f?2)
(n? 1)sf1sf2
(2)
where f?x is the average feature value of fx, and
sfx is the sample standard deviation of fx.
Of course, correlation can only indicate overlap,
and is in itself not enough to find effective features.
In our experiments with correlation-based selection
we used frequency-based selection as described in
2.3, to make an initial ranking of feature effective-
ness.
2.5 Maximum Entropy Feature Selection
Correlation-based selection can detect overlap, how-
ever, there is yet another spurious type of feature
that may reduce its effectiveness. Features with rel-
atively noisy values may contribute less than their
frequency of change may seem to indicate. For in-
stance, consider a feature that returns a completely
random value for every context. Not only does this
feature change very often, its correlation with other
features will also be weak. Such a feature may seem
attractive from the point of view of a frequency or
correlation-based method, but is useless in practice.
To account for both problems, we have to measure
the effectiveness of features in terms of how much
their addition to the model can improve prediction
of the training sample. Or in other words: does the
log-likelihood of the training data increase?
We have modified the Selective Gain Com-
putation (SGC) algorithm described by Zhou et
al. (2003) for ranking tasks rather than classification
tasks. This method builds upon the maximum en-
tropy feature selection method described by Berger
et al (1996). In this method features are added iter-
atively to a model that is initially uniform. During
each step, the feature that provides the highest gain
as a result of being added to the model, is selected
and added to the model.
In maximum entropy modeling, the weights of the
features in a model are optimized simultaneously.
However, optimizing the weights of the features in
model pS,f for every candidate feature f is compu-
tationally intractable. As a simplification, it is as-
sumed that the weights of features that are already
in the model are not affected by the addition of a
feature f . As a result, the optimal weight ? of f can
be found using a simple line search method.
However, as Zhou et al (2003) note, there is still
an inefficiency in that the weight of every candidate
feature is recalculated during every selection step.
They observe that gains of remaining candidate fea-
tures rarely increase as the result of adding a fea-
ture. If it is assumed that this never happens, a list
of candidate features ordered by gain can be kept.
To account for the fact that the topmost feature in
that list may have lost its effectiveness as the result
of a previous addition of a feature to the model, the
gain of the topmost feature is recalculated and rein-
serted into the list according to its new gain. When
the topmost feature retains its position, it is selected
and added to the model.
Since we use feature selection with features that
are not binary, and for a ranking task, we modified
the recursive forms of the model to:
sum?S?f (y|x) = sumS(y|x) ? e
?f(y) (3)
Z?S?f (x) = ZS(x)?
?
y
sumS(y|x)
+
?
y
sumS?f (y|x) (4)
Another issue that needs to be dealt with is the
calculation of context and event probabilities. In the
literature two approaches are prevalent. The first ap-
proach divides the probability mass uniformly over
contexts, and the probability of events within a con-
text is proportional to the event score (Osborne,
2000):
p(x) =
1
|X|
(5)
p(y|x) =
p(x)
( score(x,y)P
y score(x,y)
)
(6)
where |X| is the number of contexts. The sec-
ond approach puts more emphasis on the contexts
that contain relatively many events with high scores,
by making the context probability dependent on the
scores of events within that context (Malouf and van
Noord, 2004):
p(x) =
?
y score(x, y)
?
y?X score(x, y)
(7)
In our experiments, the second definition of con-
text probability outperformed the first by such a
wide margin, that we only used the second defini-
tion in the experiments described in this paper.
2.6 A Note on Overlap Detection
Although maximum-entropy based feature-selection
may be worthwhile in itself, the technique can also
be used during feature engineering to find overlap-
ping features. In the selection method of Berger et
al. (1996), the weight and gain of each candidate fea-
ture is re-estimated during each selection step. We
can exploit the changes in gains to detect overlap be-
tween a selected feature fn, and the candidates for
fn+1. If the gain of a feature changed drastically in
the selection of fn+1 compared to that of fn, this
feature has overlap with fn.
To determine which features had a drastic change
in gain, we determine whether the change has a sig-
nificance with a confidence interval of 99% after
normalization. The normalized gain change is cal-
culated in the following manner as described in al-
gorithm 1.
Algorithm 1 Calculation of the normalized gain
delta
?Gf ? Gf,n ?Gf,n?1
if ?Gf ? 0.0 then
?Gf,norm ?
?Gf
Gf ,n
else
?Gf,norm ?
?Gf
Gf,n?1
end if
3 Experimental Setup
3.1 Task
We evaluated the feature selection methods in con-
junction with a sentence realizer for Dutch. Sen-
tences are realized with a chart generator for the
Alpino wide-coverage grammar and lexicon (Bouma
et al, 2001). As the input of the chart genera-
tor, we use abstract dependency structures, which
are dependency structures leaving out information
such as word order. During generation, we store the
compressed derivation trees and associated (HPSG-
inspired) attribute-value structures for every real-
ization of an abstract dependency structure. We
then use feature templates to extract features from
the derivation trees. Two classes of features (and
templates) can be distinguished output features that
model the output of a process and construction fea-
tures that model the process that constructs the out-
put.
3.1.1 Output Features
Currently, there are two output features, both rep-
resenting auxiliary distributions (Johnson and Rie-
zler, 2000): a word trigram model and a part-of-
speech trigram model. The part-of-speech tag set
consists of the Alpino part of speech tags. Both
models are trained on newspaper articles, consist-
ing of 110 million words, from the Twente Nieuws
Corpus1.
The probability of unknown trigrams is estimated
using linear interpolation smoothing (Brants, 2000).
Unknown word probabilities are determined with
Laplacian smoothing.
1http://wwwhome.cs.utwente.nl/druid/
TwNC/TwNC-main.html
3.1.2 Construction Features
The construction feature templates consist of tem-
plates that are used for parse disambiguation, and
templates that are specifically targeted at generation.
The parse disambiguation features are used in the
Alpino parser for Dutch, and model various linguis-
tic phenomena that can indicate preferred readings.
The following aspects of a realization are described
by parse disambiguation features:
? Topicalization of (non-)NPs and subjects.
? Use of long-distance/local dependencies.
? Orderings in the middle field.
? Identifiers of grammar rules used to build the
derivation tree.
? Parent-daughter combinations.
Output features for parse disambiguation, such
as features describing dependency triples, were not
used. Additionally, we use most of the templates de-
scribed by Velldal (2008):
? Local derivation subtrees with optional grand-
parenting, with a maximum of three parents.
? Local derivation subtrees with back-off and
optional grand-parenting, with a maximum of
three parents.
? Binned word domination frequencies of the
daughters of a node.
? Binned standard deviation of word domination
of node daughters.
3.2 Data
The training and evaluation data was constructed by
parsing 11764 sentences of 5-25 tokens, that were
randomly selected from the (unannotated) Dutch
Wikipedia of August 20082, with the wide-coverage
Alpino parser. For every sentence, the best parse ac-
cording to the disambiguation component was ex-
tracted and considered to be correct. The Alpino
system achieves a concept accuracy of around 90%
on common Dutch corpora (Van Noord, 2007). The
2http://ilps.science.uva.nl/WikiXML/
original sentence is considered to be the best realiza-
tion of the abstract dependency structure of the best
parse.
We then used the Alpino chart generator to con-
struct derivation trees that realize the abstract de-
pendency structure of the best parse. The result-
ing derivation trees, including attribute-value struc-
tures associated with each node, are compressed and
stored in a derivation treebank. Training and testing
data was then obtained by extracting features from
derivation trees stored in the derivation treebank.
At this time, the realizations are also scored using
the General Text Matcher method (GTM) (Melamed
et al, 2003), by comparing them to the original
sentence. We have previously experimented with
ROUGE-N scores, which gave rise to similar results.
However, it is shown that GTM shows the highest
correlation with human judgments (Cahill, 2009).
3.3 Methodology
To evaluate the feature selection methods, we first
train models for each selection method in three
steps: 1. For each abstract dependency structure in
the training data 100 realizations (and corresponding
features) are randomly selected. 2. Feature selection
is applied, and the N -best features according to the
selection method are extracted. 3. A maximum en-
tropy model is trained using the TADM3 software,
with a `2 prior of 0.001, and using the N -best fea-
tures.
We used 5884 training instances (abstract depen-
dency trees, and scored realizations) to train the
model. The maximum entropy selection method was
used with a weight convergence threshold of 1e?6.
Correlation is considered to be strong enough for
overlap in the correlation-based method when two
features have a correlation coefficient of rf1,f2 ?
0.9
Each model is then evaluated using 5880 held-
out evaluation instances, where we select only in-
stances with 5 or more realizations (4184 instances),
to avoid trivial ranking cases. For every instance,
we select the realization that is the closest to the
original sentence to be the correct realization4. We
then calculate the fraction of instances for which the
3http://tadm.sourceforge.net/
4We follow this approach, because the original sentence is
not always exactly reproduced by the generator.
model picked the correct sentence. Of course, this is
a fairly strict evaluation, since there may be multiple
equally fluent sentences.
4 Results
4.1 Comparing the Candidates
Since each feature selection method that we evalu-
ated gives us a ranked list of features, we can train
models for an increasing number of features. We
have followed this approach, and created models for
each method, using 100 to 5000 features with a step
size of 100 features. Figure 2 shows the accuracy
for all selection methods after N features. We have
also added the line that indicates the accuracy that
is obtained when a model is trained with all features
(490667 features).
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  1000  2000  3000  4000  5000
Bes
t ma
tch 
accu
racy
Features
allmaxentcutoffcorr
Figure 2: Accuracy of maximum entropy, correlation-
based, and frequency-based selection methods after se-
lecting N features (N ? 5000), with increments of 100
features.
In this graph we can see two interesting phenom-
ena. First of all, only a very small number of fea-
tures is required to perform this task almost as well
as a model with all extracted features. Secondly, the
maximum entropy feature selection model is able
to select the most effective features quickly - fewer
than 1000 features are necessary to achieve a rela-
tively high accuracy.
As expected, the frequency-based method fared
worse than maximum entropy selection. Initially
some very useful features, such as the n-gram
models are selected, but improvement of accuracy
quickly stagnates. We expect this to be caused by
overlap of newly selected features with features that
were initially selected. Even after selecting 5000
features, this method does not reach the same accu-
racy as the maximum entropy selection method had
after selecting only a few hundred features.
The correlation-based selection method fares bet-
ter than the frequency-based method without over-
lap detection. This clearly shows that feature over-
lap is a problem. However, the correlation-based
method does not achieve good accuracy as quickly
as the maximum entropy selection method. There
are three possible explanations for this. First, there
may be noisy features that are frequent, and since
they show no overlap with selected features they are
good candidates according to the correlation-based
method. Second, less frequent features that overlap
with a frequent feature in a subset of contexts may
show a low correlation. Third, some less frequent
features may still be very discriminative for the con-
texts where they appear, while more frequent fea-
tures may just be a small indicator for a sentence
to be fluent or non-fluent. It is possible to refine
the correlation-based method to deal with the second
class of problems. However, the lack of performance
of the correlation-based method makes this unattrac-
tive - during every selection step a candidate feature
needs to be compared with all previously selected
features, rather than some abstraction of them.
Table 1 shows the peak accuracies when select-
ing up to 5000 features with the feature selection
methods described. Accuracy scores of the random
selection baseline, the n-gram models, and a model
trained on all features are included for comparison.
The random selection baseline picks a realizations
randomly. The n-gram models are the very same
n-gram models that were used as auxiliary distribu-
tions in the feature-based models. The combined
word/tag n-gram model was created by training a
model with both n-gram models as the only fea-
tures. We also list a variation of the frequency-based
method often used in other work (such as Velldal
(2008) and Malouf and Van Noord (2004)), where
there is a fixed frequency threshold (here 4), rather
than using the first N most frequently changing fea-
tures.
Besides confirming the observation that feature
selection can compress models very well, this table
shows that the popular method of using a frequency
cutoff, still gives a lot of opportunity for compress-
ing the model further. In practice, it seems best to
plot a graph as shown in figure 2, choose an accept-
able accuracy, and to use the (number of) features
that can provide that accuracy.
Method Features Accuracy
Random 0 0.0778
Tag n-gram 1 0.2039
Word n-gram 1 0.2799
Word/tag n-gram 2 0.2908
All 490667 0.4220
Fixed cutoff (4) 90103 0.4181
Frequency 4600 0.4029
Correlation 4700 0.4172
Maxent 4300 0.4201
Table 1: Peak accuracies for the maximum entropy,
correlation-based, and frequency-based selection meth-
ods when selecting up to 5000 features. Accuracies for
random, n-gram and full models are included for com-
parison.
4.2 Overlap in Frequency-based Selection
As we argued in section 2.5, the primary disadvan-
tage of the frequency-based selection is that it cannot
account for correlation between features. In the ex-
treme case, we could have two very distinctive fea-
tures f1 and f2 that behave exactly the same in any
event. While adding f2 after adding f1 does not im-
prove the model, frequency-based selection cannot
detect this. To support this argumentation empiri-
cally, we analyzed the first 100 selected features to
find good examples of this overlap.
Initially, the frequency-based selection chooses
three distinctive features that are also selected by
the maximum entropy selection method: the two n-
gram language models, and a preference for topical-
ized NP subjects. After that, features that indicate
whether the vp arg v(np) rule was used change very
frequently within a context. However, this aspect
of the parse tree is embodied in 13 successively se-
lected features. Due to the generality of the feature
templates, there are multiple templates to capture the
use of this grammar rule: through local derivation
trees (with optional grandparenting), back-off for lo-
cal derivation trees, and the features that calculate
lexical node dominance.
Another example of such overlap in the first
100 features is in features modeling the use of the
non wh topicalization(np) rule. Features containing
this rule identifier are used 30 times in sequence,
where it occurs in local derivation subtrees (with
varying amounts of context), back-off local deriva-
tion subtrees, lexical node domination, or as a grand-
parent of another local derivation subtree.
In the first 100 features, there were many overlap-
ping features, and we expect that this also is the case
for more infrequent features.
4.3 Effective Features
The maximum entropy selection method shows that
only a small number of features is necessary to per-
form fluency ranking (section 4.1). The first fea-
tures that were selected in maximum entropy selec-
tion can give us good insight of what features are
important for fluency ranking. Table 2 shows the 10
topmost features as returned by the maximum en-
tropy selection. The weights shown in this table, are
those given by the selection method, and their sign
indicates whether the feature was characteristic of a
fluent sentence (+) or a non-fluent sentence (?).
As expected (see table 1) the n-gram models are
a very important predictor for fluency. The only
surprise here may be that the overlap between both
n-gram models is small enough to have both mod-
els as a prominent feature. While the tag n-gram
model is a worse predictor than the word n-gram
model, we expect that the tag n-gram model is espe-
cially useful for estimating fluency of phrases with
word sequences that are unknown to the word n-
gram model.
The next feature that was selected,
r2(vp arg v(pred),2,vproj vc), indicates that
the rule vp arg v(pred) was used with a vproj vc
node as its second daughter. This combination
occurs when the predicative complement is placed
after the copula, for instance as in Amsterdam is de
hoofdstad van Nederland (Amsterdam is the capital
of The Netherlands), rather than De hoofdstad
van Nederland is Amsterdam (The capital of The
Netherlands is Amsterdam).
The feature s1(non subj np topic) and its neg-
ative weight indicates that realizations with non-
topicalized NP subjects are dispreferred. In Dutch,
non-topicalized NP subjects arise in the OVS word-
order, such as in de soep eet Jan (the soup eats Jan).
While this is legal, SVO word-order is clearly pre-
ferred (Jan eet de soep).
The next selected feature (ldsb(vc vb,vb v,
[vproj vc,vp arg v(pp)])) is also related to top-
icalization: it usually indicates a preference for
prepositional complements that are not topicalized.
For instance, dit zorgde voor veel verdeeldheid
(this caused lots of discord) is preferred over the
PP-topicalized voor veel verdeeldheid zorgde dit
(lots of discord caused this).
ldsb(n n pps,pp p arg(np),[]) gives preference
PP-ordering in conjuncts where the PP modifier fol-
lows the head. For instance, the conjunct groepen
van bestaan of khandas (planes of existance or khan-
das) is preferred by this feature over van bestaan
groepen of khandas (of existence planes or khan-
das).
The next feature (lds dl(mod2,[pp p arg(np)],
[1],[non wh topicalization(modifier)])) forms an
exception to the dispreference of topicalization of
PPs. If we have a PP that modifies a copula in a
subject-predicate structure, topicalization of the PP
can make the realization more fluent. For instance,
volgens Williamson is dit de synthese (according to
Williamson is this the synthesis) is considered more
fluent than dit is de synthese volgens Williamson
(this is the synthesis according to Williamson).
The final three features deal with punctuation.
Since punctuation is very prevalent in Wikipedia
texts due to the amount of definitions and clarifi-
cations, punctuation-related features are common.
Note that the last two lds dl features may seem to
be overlapping, they are not: they use different fre-
quency bins for word domination.
5 Conclusions and Future Work
Our conclusion after performing experiments with
feature selection is twofold. First, fluency models
can be compressed enormously by applying feature
selection, without losing much in terms of accuracy.
Second, we only need a small number of targeted
features to perform fluency ranking.
The maximum entropy feature selection method
Weight Name
0.012 ngram lm
0.009 ngram tag
0.087 r2(vp arg v(pred),2,vproj vc)
-0.094 s1(non subj np topic)
0.090 ldsb(vc vb,vb v,
[vproj vc,vp arg v(pp)])
0.083 ldsb(n n pps,pp p arg(np),[])
0.067 lds dl(mod2,[pp p arg(np)],[1],
[non wh topicalization(modifier)])
0.251 lds dl(start start ligg streep,
[top start xp,punct(ligg streep),
top start xp],[0,0,1],[top start])
0.186 lds dl(start start ligg streep,
[top start xp,punct(ligg streep),
top start xp],[0,0,2],[top start])
0.132 r2(n n modroot(haak),5,l)
Table 2: The first 10 features returned by maximum en-
tropy feature selection, including the weights estimated
by this feature selection method.
shows a high accuracy after selecting just a few fea-
tures. The commonly used frequency-based selec-
tion method fares far worse, and requires addition
of many more features to achieve the same perfor-
mance as the maximum entropy method. By exper-
imenting with a correlation-based selection method
that uses the frequency method to make an initial
ordering of features, but skips features that show a
high correlation with previously selected features,
we have shown that the ineffectiveness of frequency-
based selection can be attributed partly to feature
overlap. However, the maximum entropy method
was still more effective in our experiments.
In the future, we hope to evaluate the same tech-
niques to parse disambiguation. We also plan to
compare the feature selection methods described in
this paper to selection by imposing a `1 prior.
The feature selection methods described in this
paper are usable for feature sets devised for ranking
and classification tasks, especially when huge sets of
automatically extracted features are used. An open
source implementation of the methods described in
this paper is available5, and is optimized to work on
large data and feature sets.
5http://danieldk.eu/Code/FeatureSqueeze/
References
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational linguistics, 22(1):71.
G. Bouma, G. Van Noord, and R. Malouf. 2001. Alpino:
Wide-coverage computational analysis of Dutch. In
Computational Linguistics in the Netherlands 2000.
Selected Papers from the 11th CLIN Meeting.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochas-
tic realisation ranking for a free word order language.
In ENLG ?07: Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
17?24, Morristown, NJ, USA. Association for Com-
putational Linguistics.
A. Cahill. 2009. Correlating Human and Automatic
Evaluation of a German Surface Realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distributions in stochastic unification-based grammars.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 154?161, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
D. Madigan and A.E. Raftery. 1994. Model selection and
accounting for model uncertainty in graphical models
using Occam?s window. Journal of the American Sta-
tistical Association, 89(428):1535?1546.
R. Malouf and G. van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars.
In IJCNLP-04 Workshop: Beyond shallow analyses -
Formalisms and statistical modeling for deep analy-
ses. JST CREST, March.
I. D. Melamed, R. Green, and J. P. Turian. 2003. Pre-
cision and recall of machine translation. In HLT-
NAACL.
H. Nakanishi, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic models for disambiguation of an hpsg-based chart
generator. In Parsing ?05: Proceedings of the Ninth
International Workshop on Parsing Technology, pages
93?102, Morristown, NJ, USA. Association for Com-
putational Linguistics.
M. Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics, pages 586?592, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
S. Perkins, K. Lacker, and J. Theiler. 2003. Grafting:
Fast, incremental feature selection by gradient descent
in function space. The Journal of Machine Learning
Research, 3:1333?1356.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151?175.
G. Van Noord. 2007. Using self-trained bilexical pref-
erences to improve disambiguation accuracy. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 1?10. Association for Compu-
tational Linguistics.
E. Velldal and S. Oepen. 2006. Statistical ranking in
tactical generation. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 517?525. Association for Compu-
tational Linguistics.
E. Velldal. 2008. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo, Department of Informatics.
Y. Zhou, F. Weng, L. Wu, and H. Schmidt. 2003. A
fast algorithm for feature selection in conditional max-
imum entropy modeling.
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 54?63,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Discriminative features in reversible stochastic attribute-value grammars
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Abstract
Reversible stochastic attribute-value gram-
mars (de Kok et al, 2011) use one model
for parse disambiguation and fluency rank-
ing. Such a model encodes preferences with
respect to syntax, fluency, and appropriate-
ness of logical forms, as weighted features.
Reversible models are built on the premise
that syntactic preferences are shared between
parse disambiguation and fluency ranking.
Given that reversible models also use fea-
tures that are specific to parsing or genera-
tion, there is the possibility that the model is
trained to rely on these directional features. If
this is true, the premise that preferences are
shared between parse disambiguation and flu-
ency ranking does not hold.
In this work, we compare and apply feature se-
lection techniques to extract the most discrim-
inative features from directional and reversible
models. We then analyse the contributions of
different classes of features, and show that re-
versible models do rely on task-independent
features.
1 Introduction
Reversible stochastic attribute-value grammars (de
Kok et al, 2011) provide an elegant framework that
fully integrates parsing and generation. The most
important contribution of this framework is that it
uses one conditional maximum entropy model for
fluency ranking and parse disambiguation. In such
a model, the probability of a derivation d is con-
ditioned on a set of input constraints c that restrict
the set of derivations allowed by a grammar to those
corresponding to a particular sentence (parsing) or
logical form (generation):
p(d|c) =
1
Z(c)
exp
?
i
wifi(c, d) (1)
Z(c) =
?
d???(c)
exp
?
i
wifi(c, d
?) (2)
Here, ?(c) is the set of derivations for input c,
fi(c, d) the value of feature fi in derivation d of c,
and wi is the weight of fi. Reversibility is opera-
tionalized during training by imposing a constraint
on a given feature fi with respect to the sentences
T in the parse disambiguation treebank and logical
forms L in the fluency ranking treebank. This con-
straint is:
?
c?C
?
d??(c)
p?(c)p(d|c)fi(c, d) ? (3)
p?(c, d)fi(c, d) = 0
Where C = T ? L, p?(c) is the empirical proba-
bility of a set of constraints c, and p?(c, d) the joint
probability of a set of constraints c and a derivation
d.
Reversible stochastic-attribute grammars rest on
the premise that preferences are shared between lan-
guage comprehension and production. For instance,
in Dutch, subject fronting is preferred over direct
object fronting. If models for parse disambiguation
and fluency ranking do not share preferences with
respect to fronting, it would be difficult for a parser
54
to recover the logical form that was the input to a
generator.
Reversible models incorporate features that are
specific to parse disambiguation and fluency rank-
ing, as well as features that are used for both
tasks. Previous work (Cahill et al, 2007; de Kok,
2010) has shown through feature analysis that task-
independent features are indeed useful in directional
models. However, since reversible models assign
just one weight to each feature regardless the task,
one particular concern is that much of their discrim-
inatory power is provided by task-specific features.
If this is true, the premise that similar preferences
are used in parsing and generation does not hold.
In this work, we will isolate the most discrimina-
tive features of reversible models through feature se-
lection, and make a quantitative and qualitative anal-
ysis of these features. Our aim is to to verify that
reversible models do rely on features used both in
parsing and generation.
To find the most effective features of a model, we
need an effective feature selection method. Section 2
describes three such methods: grafting, grafting-
light, and gain-informed selection. These methods
are compared empirically in Section 4 using the ex-
perimental setup described in Section 3. We then use
the best feature selection method to perform quanti-
tative and qualitative analyses of reversible models
in Sections 5 and 6.
2 Feature selection
Feature selection is a procedure that attempts to ex-
tract a subset of discriminative features S ? F
from a set of features F , such that a model using
S performs comparable to a model using F and
|S|  |F |.
As discussed in De Kok (2010), a good feature se-
lection method should handle three kinds of redun-
dancies in feature sets: features that rarely change
value; features that overlap; and noisy features.
Also, for a qualitative evaluation of fluency ranking,
it is necessary to have a ranking of features by dis-
criminative power.
De Kok (2010) compares frequency-based selec-
tion, correlation selection, and a gain-informed se-
lection method. In that work, it was found that
the gain-informed selection method outperforms
frequency-based and correlation selection. For this
reason we exclude the latter two methods from our
experiments. Other commonly used selection meth-
ods for maximum entropy models include `1 regu-
larization (Tibshirani, 1996), grafting (Perkins et al,
2003; Riezler and Vasserman, 2004), and grafting-
light (Zhu et al, 2010). In the following sections,
we will give a description of these selection meth-
ods.
2.1 `1 regularization
During the training of maximum entropy mod-
els, regularization is often applied to avoid uncon-
strained feature weights and overfitting. If L(w) is
the objective function that is minimized during train-
ing, a regularizer ?q(w) is added as a penalty for
extreme weights (Tibshirani, 1996):
C(w) = L(w) + ?q(w) (4)
Given that the maximum entropy training pro-
cedure attempts to minimize the negative log-
likelihood of the model, the penalized objective
function is:
C(w) = ?
?
c,d
p?(c, d)log(p(d|c)) + ?q(w) (5)
The regularizer has the following form:
?q(w) = ?
n?
i=1
|wi|
q
Setting q = 1 in the regularizer function gives a
so-called `1 regularizer and amounts to applying a
double-exponential prior distribution with ? = 0.
Since the double-exponential puts much of its prob-
ability mass near its mean, the `1 regularizer has a
tendency to force weights towards zero, providing
integral feature selection and avoiding unbounded
weights. Increasing ? strengthens the regularizer,
and forces more feature weights to be zero.
Given an appropriate value for ?, `1 regulariza-
tion can exclude features that change value infre-
quently, as well as noisy features. However, it does
not guarantee to exclude overlapping features, since
55
the weight mass can be distributed among overlap-
ping features. `1 regularization also does not fulfill a
necessary characteristic for the present task, in that
it does not provide a ranking based on the discrimi-
native power of features.
2.2 Grafting
Grafting (Perkins et al, 2003) adds incremental fea-
ture selection during the training of a maximum en-
tropy model. The selection process is a repetition of
two steps: 1. a gradient-based heuristic selects the
most promising feature from the set of unselected
features Z, adding it to the set of selected features
S, and 2. a full optimization of weights is performed
over all features in S. These steps are repeated until
a stopping condition is triggered.
During the first step, the gradient of each unse-
lected feature fi ? Z is calculated with respect to
the model pS , that was trained with the set of se-
lected features, S:
?
?
?
?
?L(wS)
?wi
?
?
?
? = pS(fi)? p?(fi) (6)
The feature with the largest gradient is removed
from Z and added to S.
The stopping condition for grafting integrates the
`1 regularizer in the grafting method. Note that
when `1 regularization is applied, a feature is only
included (has a non-zero weight) if its penalty is out-
weighted by its contribution to the reduction of the
objective function. Consequently, only features for
which
?
?
?
?L(wS)
?wi
?
?
? > ? holds are eligible for selection.
This is enforced by stopping selection if for all fi in
Z
?
?
?
?
?L(wS)
?wi
?
?
?
? ? ? (7)
Although grafting uses `1 regularization, its iter-
ative nature avoids selecting overlapping features.
For instance, if f1 and f2 are identical, and f1 is
added to the model pS ,
?
?
?
?L(wS)
?w2
?
?
?will amount to zero.
Performing a full optimization after each selected
feature is computationally expensive. Riezler and
Vasserman (2004) observe that during the feature
step selection a larger number of features can be
added to the model (n-best selection) without a loss
of accuracy in the resulting model. However, this
so-called n-best grafting may introduce overlapping
features.
2.3 Grafting-light
The grafting-light method (Zhu et al, 2010) oper-
ates using the same selection step as grafting, but
improves performance over grafting by applying one
iteration of gradient-descent during the optimization
step rather than performing a full gradient-descent.
As such, grafting-light gradually works towards the
optimal weights, while grafting always finds the op-
timal weights for the features in S during each iter-
ation.
Since grafting-light does not perform a full
gradient-descent, an additional stopping condition is
required, since the model may still not be optimal
even though no more features can be selected. This
additional condition requires that change in value of
the objective function incurred by the last gradient-
descent is smaller than a predefined threshold.
2.4 Gain-informed selection
Gain-informed feature selection methods calculate
the gain ?L(S, fi) of adding a feature fi ? Z to
the model. If L(wS) is the negative log-likelihood
of pS , ?L(S, fi) is defined as:
?L(S, fi) ? L(wS)? L(wS?fi) (8)
During each selection step, the feature that gives
the highest gain is selected. The calculation
of L(pS?fi) requires a full optimization over the
weights of the features in S ? fi. Since it is com-
putationally intractable to do this for every fi in Z,
Berger et al (1996) propose to estimate the weight
wi of the candidate feature fi, while assuming that
the weights of features in S stay constant. Under
this assumption, wi can be estimated using a simple
line search method.
However, Zhou et al (2003) observe that, de-
spite this simplification, the gain-informed selection
method proposed by Berger et al (1996) still recal-
culates the weights of all the candidate features dur-
ing every cycle. They observe that the gains of can-
didate features rarely increase. If it is assumed that
the gain of adding a feature does indeed never in-
crease as a result of adding another feature, the gains
of features during the previous iteration can be kept.
56
To account for features that become ineffective, the
gain of the highest ranked feature is recalculated.
The highest ranked feature is selected if it remains
the best feature after this recalculation. Otherwise,
the same procedure is repeated for the next best fea-
ture.
De Kok (2010) modifies the method of Zhou et
al. (2003) for ranking tasks. In the present work, we
also apply this method, but perform a full optimiza-
tion of feature weights in pS every n cycles.
Since this selection method uses the gain of a fea-
ture in its selection criterion, it excludes noisy and
redundant features. Overlapping features are also
excluded since their gain diminishes after selecting
one of the overlapping features.
3 Experimental setup and evaluation
3.1 Treebanks
We carry out our experiments using the Alpino de-
pendency parser and generator for Dutch (van No-
ord, 2006; de Kok and van Noord, 2010). Two
newspaper corpora are used in the experiments.
The training data consists of the cdbl part of the
Eindhoven corpus1 (7,154 sentences). Syntactic
annotations are available from the Alpino Tree-
bank2 (van der Beek et al, 2002). Part of the Trouw
newspaper of 2001 is used for evaluation3. Syntac-
tic annotations are part of LASSY4 (van Noord et
al., 2010), part WR-P-P-H (2,267 sentences).
3.2 Features
In our experiments, we use the features described in
De Kok et al (2011). In this section, we provide a
short summarization of the types of features that are
used.
Word adjacency. Word and Alpino part-of-
speech tag trigram models are used as auxiliary dis-
tributions (Johnson and Riezler, 2000). In both
models, linear interpolation smoothing is applied to
handle unknown trigrams, and Laplacian smoothing
for unknown unigrams. The trigram models have
1http://www.inl.nl/corpora/
eindhoven-corpus
2http://www.let.rug.nl/vannoord/trees/
3http://hmi.ewi.utwente.nl/TwNC
4http://www.inl.nl/corpora/lassy-corpus
been trained on the Twente Nieuws Corpus (approx-
imately 100 million words), excluding the Trouw
2001 corpus. In parsing, the value of the word tri-
gram model is constant across derivations of a given
input sentence.
Lexical frames. The parser applies lexical analy-
sis to find all possible subcategorization frames for
tokens in the input sentence. Since some frames oc-
cur more frequently in good parses than others, two
feature templates record the use of frames in deriva-
tions. An additional feature implements an auxil-
iary distribution of frames, trained on a large cor-
pus of automatically annotated sentences (436 mil-
lion words). The values of lexical frame features
are constant for all derivations in sentence realiza-
tion, unless the frame is underspecified in the logical
form.
Dependency relations. Several templates de-
scribe aspects of the dependency structure. For each
dependency relation multiple dependency features
are extracted. These features list the dependency
relation, and characteristics of the head and depen-
dent, such as their roots or part of speech tags. Ad-
ditionally, features are used to implement auxiliary
distributions for selectional preferences (van Noord,
2007). In generation, the values of these features are
constant across derivations corresponding to a given
logical form.
Syntactic features. Syntactic features include fea-
tures that record the application of grammar rules,
as well as the application of a rule in the context
of another rule. Additionally, there are features de-
scribing more complex syntactic patterns, such as
fronting of subjects and other noun phrases, order-
ings in the middle field, long-distance dependencies,
and parallelism of conjuncts in coordinations.
3.3 Parse disambiguation
To create training and evaluation data for parse dis-
ambiguation, the treebanks described in section 3.1
are parsed, extracting the first 3000 derivations. On
average, there are about 649 derivations for the sen-
tences in the training data, and 402 derivations for
the sentences in the test data.
Since the parser does not always yield the cor-
rect parse, the concept accuracy (CA) (van Noord,
57
2006) of each derivation is calculated to estimate its
quality. The highest scoring derivations for each in-
put are marked as correct, all other derivations are
marked as incorrect. Features are then extracted
from each derivation.
The concept accuracy is calculated based on the
named dependency relations of the candidate and
correct parses. If Dp(t) is the bag of dependen-
cies produced by the parser for sentence t andDg(t)
is the bag of dependencies of the correct (gold-
standard) parse, concept accuracy is defined as:
CA =
?
t?T |Dp(t) ?Dg(t)|?
t?T max(|Dp(t)|, |Dg(t)|)
(9)
The procedure outlined above gives examples of
correct and incorrect derivations to train the model,
and derivations to test the resulting model.
3.4 Fluency ranking
For training and evaluation of the fluency ranker, we
use the same treebanks as in parse disambiguation.
We assume that the sentence that corresponds to a
dependency structure in the treebank is the correct
realization of that dependency structure. We parse
each sentence in the treebank, extracting the depen-
dency structure that is the most similar to that in
the treebank. We perform this step to assure that it
is possible to generate from the given dependency
structure. We then use the Alpino chart genera-
tor to make all possible derivations and realizations
conforming to that dependency structure. Due to a
limit on generation time, some longer sentences and
corresponding dependency structures are excluded
from the data. The average sentence length was 15.7
tokens, with a maximum of 26 tokens.
Since the sentence in the treebank cannot always
be produced exactly, we estimate the quality of each
realization using the General Text Matcher (GTM)
method (Melamed et al, 2003). The best-scoring
derivations are marked as correct, the other deriva-
tions are marked as incorrect. Finally, features are
extracted from these derivations.
The General Text Matcher method marks all cor-
responding tokens of a candidate realization and the
correct realization in a grid, and finds the maximum
matching (the largest subset of marks, such that no
marks are in the same row or column). The size of
the matchingM is then determined using the lengths
of runs r in the matching (a run is a diagonal of
marks), rewarding longer runs:
size(M) =
?
?
r?M
length(r)2 (10)
This method has been shown to have the highest
correlation with human judgments in a related lan-
guage (German), using a comparable system (Cahill,
2009).
3.5 Training
Models are trained by extracting an informative
sample of ?(c) for each c in the training data (Os-
borne, 2000). This informative sample consists of at
most 100 randomly selected derivations.
We then apply feature selection on the training
data. We let each method select 1711 features. This
number is derived from the number of non-zero fea-
tures that training a model with a `1 norm coefficient
of 0.0002 gives. Grafting and grafting-light selec-
tion are applied using TinyEst5. For gain-informed
selection, we use FeatureSqueeze6. For all three
methods, we add 10 features to the model during
each selection step.
3.6 Evaluation
We evaluate each selection method stepwise. We
train and evaluate a model on the best-n features ac-
cording to each selection method, for n = [0..1711].
In each case, the feature weights are estimated with
TinyEst using a `1 norm coefficient of 0.0002. This
stepwise evaluation allows us to capture the effec-
tiveness of each method.
Parse disambiguation and fluency ranking models
are evaluated on the WR-P-P-H corpus that was de-
scribed in Section 3.1, using CA and GTM scores
respectively.
4 Evaluation of feature selection methods
4.1 Incremental feature selection
Figure 1 shows the performance of the feature selec-
tion methods for parse disambiguation. This graph
shows that that both grafting methods are far more
5http://github.com/danieldk/tinyest
6https://github.com/rug-compling/
featuresqueeze
58
effective than gain-informed selection. We can also
see that only a small number of features is required
to construct a competitive model. Selecting more
features improves the model only gradually.
Figure 2 shows the performance of the feature
selection methods in fluency ranking. Again, we
see the same trend as in parse disambiguation.
The grafting and grafting-light methods outperform
gain-informed selection, with the grafting method
coming out on top. In feature selection, even a
smaller number of features is required to train an
effective model. After selecting more than approx-
imately 50 features, adding features only improves
the model very gradually.
Figure 1: Application of feature selection methods to
parse disambiguation
4.2 Selection using an `1 prior
During our experiments, we also evaluated the effect
of using an `1 prior in Alpino to see if it is worthwile
to replace feature selection using a frequency cut-
off (Malouf and van Noord, 2004). Using Alpino?s
default configuration with a frequency cut-off of 2
and an `2 prior with ?2 = 1000 the system had a
CA-score of 90.94% using 25237 features. We then
trained a model, applying an `1 prior with a norm
coefficient of 0.0002. With this model, the system
had a CA-score of 90.90% using 2346 features.
In generation, Alpino uses a model with the same
frequency cut-off and `2 prior. This model has
1734 features features and achieves a GTM score of
0.7187. Applying the `1 prior reduces the number
Figure 2: Effectiveness of feature selection methods in
fluency ranking. Both grafting methods outperform gain-
based ranking
of features to 607, while mildly increasing the GTM
score to 0.7188.
These experiments show that the use of `1 priors
can compress models enormously, even compared
to frequency-based feature selection, while retaining
the same levels of accuracy.
5 Quantitative analysis of reversible
models
For a quantitative analysis of highly discriminative
features, we extract the 300 most effective features
of the fluency ranking, parse disambiguation, and re-
versible models using grafting. We then divide fea-
tures into five classes: dependency (enumeration of
dependency triples), lexical (readings of words), n-
gram (word and tag trigram auxiliary distributions),
rule (identifiers of grammar rules), and syntactic
(abstract syntactic features). Of these classes, rule
and syntactic features are active during both parse
disambiguation and fluency ranking.
In the quantitative analyses, we train a model for
each selection step. The models contain the 1 to 300
best features. Using these models, we can calculate
the contribution of feature fi to the improvement ac-
cording to some evaluation function e
c(fi) =
e(p0..i)? e(p0..i?1)
e(p0..n)? e(p0)
(11)
where p0..i is a model trained with the i most dis-
59
criminative features, p0 is the uniform model, and
n = 300.
5.1 Parse disambiguation
Table 1 provides class-based counts of the 300 most
discriminative features for the parse disambiguation
and reversible models. Since the n-gram features are
not active during parse disambiguation, they are not
selected for the parse disambiguation model. All
other classes of features are used in the parse dis-
ambiguation model. The reversible model uses all
classes of features.
Class Directional Reversible
Dependency 93 84
Lexical 24 24
N-gram 0 2
Rule 156 154
Syntactic 27 36
Table 1: Per-class counts of the best 300 features accord-
ing to the grafting method.
Contributions per feature class in parse disam-
biguation are shown in table 2. In the directional
parse disambiguation model, parsing-specific fea-
tures (dependency and lexical) account for 55% of
the improvement over the uniform model.
In the reversible model, there is a shift of con-
tribution towards task-independent features. When
applying this model, the contribution of parsing-
specific features to the improvement over the uni-
form model is reduced to 45.79%.
We can conclude from the per-class feature con-
tributions in the directional parse disambiguation
model and the reversible model, that the reversible
model does not put more emphasis on parsing-
specific features. Instead, the opposite is true: task-
independent features are more important in the re-
versible model than the directional model.
5.2 Fluency ranking
Table 3 provides class-based counts of the 300 most
discriminative features of the fluency ranking and
reversible models. During fluency ranking, depen-
dency features and lexical features are not active.
Table 4 shows the per-class contribution to the
improvement in accuracy for the directional and re-
versible models. Since the dependency and lexical
Class Directional Reversible
Dependency 21.53 13.35
Lexical 33.68 32.62
N-gram 0.00 0.00
Rule 37.61 47.35
Syntactic 7.04 6.26
Table 2: Per-class contribution to the improvement of the
model over the base baseline in parse disambiguation.
Class Directional Reversible
Dependency 0 84
Lexical 0 24
N-gram 2 2
Rule 181 154
Syntactic 117 36
Table 3: Per-class counts of the best 300 features accord-
ing to the grafting method.
features are not active during fluency ranking, it may
come as a surprise that their contribution is nega-
tive in the reversible model. Since they are used for
parse disambiguation, they have an effect on weights
of task-independent features. This phenomenon did
not occur when using the reversible model for parse
disambiguation, because the features specific to flu-
ency ranking (n-gram features) were selected as the
most discriminative features in the reversible model.
Consequently, the reversible models with one and
two features were uniform models from the perspec-
tive of parse disambiguation.
Class Directional Reversible
Dependency 0.00 -4.21
Lexical 0.00 -1.49
N-gram 81.39 83.41
Rule 14.15 16.45
Syntactic 3.66 4.59
Table 4: Per-class contribution to the improvement of the
model over the baseline in fluency ranking.
Since active features compensate for this loss in
the reversible model, we cannot directly compare
per-class contributions. To this end, we normalize
the contribution of all positively contributing fea-
tures, leading to table 5. Here, we can see that the
reversible model does not shift more weight towards
task-specific features. On the contrary, there is a
60
mild effect in the opposite direction here as well.
Class Directional Reversible
N-gram 81.39 79.89
Rule 14.15 15.75
Syntactic 3.66 4.39
Table 5: Classes giving a net positive distribution, with
normalized contributions.
6 Qualitative analysis of reversible models
While the quantitative evaluation shows that task-
independent features remain important in reversible
models, we also want to get an insight into the ac-
tual features that were used. Since it is unfeasible to
study the 300 best features in detail, we extract the
20 best features.
Grafting-10 is too course-grained for this task,
since it selects the first 10 features solely by their
gradients, while there may be overlap in those fea-
tures. To get the most accurate list possible, we per-
form grafting-1 selection to extract the 20 most ef-
fective features. We show these features in table 6
with their polarities. The polarity indicates whether
a feature is an indicator for a good (+) or bad (-)
derivation.
We now provide a description of these features by
category.
Word/tag trigrams. The most effective features
in fluency ranking are the n-gram auxiliary distribu-
tions (1, 3). The word n-gram model settles prefer-
ences with respect to fixed expressions and common
word orders. It also functions as a (probabilistic)
filter of archaic inflections and incorrect inflections
that are not known to the Alpino lexicon. The tag
n-gram model help picking a sequence of part-of-
speech tags that is plausible.
Frame selection. Various features assist in the
selection of proper subcategorization frames for
words. This currently affects parse disambiguation
mostly. There is virtually no ambiguity of frames
during generation, and a stem/frame combination
normally only selects one inflection. The most ef-
fective feature for frame selection is (2), which is
an auxiliary distribution of words and correspond-
ing frames based on a large automatically annotated
Rank Polarity Feature
1 + ngram lm
2 + z f2
3 + ngram tag
4 - r1(np n)
5 + r2(np det n,2,n n pps)
6 - p1(pardepth)
7 + r2(vp mod v,3,vproj vc)
8 - r2(vp arg v(np),2,vproj vc)
9 - f1(adj)
10 + r2(vp mod v,2,optpunct(e))
11 - s1(non subj np topic)
12 + r1(n adj n)
13 + dep23(prep,hd/pc,verb)
14 + r1(optpunct(e))
15 + dep34(van,prep,hd/mod,noun)
16 + dep23(noun,hd/su,verb)
17 + p1(par)
18 - r1(vp v mod)
19 + dep23(prep,hd/mod,verb)
20 - f1(verb(intransitive))
Table 6: The twenty most discriminative features of the
reversible model, and their polarities.
corpus. Other effective features indicate that read-
ings as an adjective (9) and as an intransitive verb
(20) are not preferred.
Modifiers. Feature 5 indicates the preference to
attach prepositional phrases to noun phrases. How-
ever, if a modifier is attached to a verb, we prefer
readings and realizations where the modifier is left-
adjoining rather than right-adjoining (7, 18, 19). For
instance, zij heeft met de hond gelopen (she has with
the dog walked) is more fluent than zij heeft gelopen
met de hond (she has walked with the dog). Finally,
feature 15 gives preference to analyses where the
preposition van is a modifier of a noun.
Conjunctions. Two of the twenty most discrimi-
native features involve conjunctions. The first (6)
is a dispreference for conjunctions where conjuncts
have a varying depth. In conjunctions, the model
prefers derivations where all conjuncts in a con-
junctions have an equal depth. The other feature
(17) gives a preferences to conjunctions with paral-
lel conjuncts ? conjunctions where every conjunct
is constructed using the same grammar rule.
61
Punctuation. The Alpino grammar is very gen-
erous in allowing optional punctuation. An empty
punctuation sign is used to fill grammar rule slots
when no punctuation is used or realized. Two fea-
tures indicate preferences with respect to optional
punctuation. The first (10) gives preference to filling
the second daughter slot of the vp mod v with the
empty punctuation sign. This implies that deriva-
tions are preferred where a modifier and a verb are
not separated by punctuation. The second feature
(14) indicates a general preference for the occur-
rence of empty optional punctuation in the deriva-
tion tree.
Subjects/objects. In Dutch, subject fronting is
preferred over object fronting. For instance, Spanje
won de wereldbeker (Spain won the World Cup)
is preferred over de wereldbeker won Spanje (the
World Cup won spain). Feature 8 will in many cases
contribute to the preference of having topicalized
noun phrase subjects. It disprefers having a noun
phrase left of the verb. For example, zij heeft met de
hond gelopen (she has with the dog walked) is pre-
ferred over met de hond heeft zij gelopen (with the
dog she has walked). Feature 11 encodes the prefer-
ence for subject fronting, by penalizing derivations
where the topic is a non-subject noun phrase.
Other syntactic preferences. The remaining
features are syntactic preferences that do not
belong to any of the previous categories. Feature
4 indicates a dispreference for derivations where
bare nouns occur. Feature 12 indicates a preference
for derivations where a noun occurs along with
an adjective. Finally, feature 13 gives preference
to the prepositional complement (pc) relation if a
preposition is a dependent of a verb and lexical
analysis shows that the verb can combine with that
prepositional complement.
We can conclude from this description of fea-
tures that many of the features that are paramount
to parse disambiguation and fluency ranking are
task-independent, modeling phenomena such as
subject/object fronting, modifier adjoining, paral-
lelism and depth in conjunctions, and the use of
punctuation.
7 Conclusion
In this work we have used feature selection tech-
niques for maximum entropy modeling to analyze
the hypothesis that the models in reversible stochas-
tic attribute-value grammars use task-independent
features. To this end, we have first compared
three feature selection techniques, namely gain-
informed selection, grafting, and grafting-light. In
this comparison we see that grafting outperforms
both grafting-light and gain-informed selection in
parse disambiguation and fluency ranking tasks.
We then used grafting to select the most effective
features for parse disambiguation, fluency ranking,
and reversible models. In the quantitative analysis
we have shown that the reversible model does not
put more emphasis on task-specific features. In fact,
the opposite is true: in the reversible model task-
independent features become more defining than in
the directional models.
We have also provided a qualitative analysis of the
twenty most effective features, showing that many of
these features are relevant to both parsing and gener-
ation. Effective task-independent features for Dutch
model phenomena such as subject/object fronting,
modifier adjoining, parallelism and depth in con-
junctions, and the use of punctuation.
8 Future work
An approach for testing the reversibility of mod-
els that we have not touched upon in this work, is
to evaluate such models using tasks that combine
parsing and generation. For instance, a good word
graph parser should choose a fluent sentence with a
syntactically plausible reading. If reversible models
integrate parsing-specific, generation-specific, and
task-independent features properly, they should be
competitive to models specifically trained for that
task. In the future, we hope to evaluate reversible
stochastic attribute-value grammars in the light of
such tasks.
9 Acknowledgments
This work was funded by the DAISY project of the
STEVIN program. The author would also like to
thank Yan Zhao, Barbara Plank, and Gertjan van No-
ord for the many valuable discussions on maximum
entropy modeling and feature selection.
62
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational linguis-
tics, 22(1):71.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Designing features for parse disambiguation and real-
isation ranking. In The Proceedings of the LFG ?07
Conference, pages 128?147. CSLI Publications.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference - Short Pa-
pers, pages 97?100.
Danie?l de Kok and Gertjan van Noord. 2010. A sentence
generator for Dutch. In Proceedings of the 20th Com-
putational Linguistics in the Netherlands conference
(CLIN), pages 75?90.
Danie?l de Kok, Barbara Plank, and Gertjan van Noord.
2011. Reversible stochastic attribute-value grammars.
In Proceedings of the ACL HLT 2011 Conference -
Short Papers.
Danie?l de Kok. 2010. Feature selection for fluency rank-
ing. In Proceedings of the 6th International Natural
Language Generation Conference (INLG), pages 155?
163.
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 154?161, Seattle, Wash-
ington.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In IJCNLP-04 Workshop: Beyond shallow anal-
yses - Formalisms and statistical modeling for deep
analyses. JST CREST, March.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In HLT-
NAACL.
Miles Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics (COLING), pages 586?592.
Simon Perkins, Kevin Lacker, and James Theiler. 2003.
Grafting: Fast, incremental feature selection by gradi-
ent descent in function space. The Journal of Machine
Learning Research, 3:1333?1356.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and l1 regularization for re-
laxed maximum-entropy modeling. In Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP04), Barcelona, Spain.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the Lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267?288.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
Gertjan van Noord, Ineke Schuurman, and Gosse
Bouma. 2010. Lassy syntactische annotatie,
revision 19053. http://www.let.rug.nl/
vannoord/Lassy/sa-man_lassy.pdf.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement Au-
tomatique des Langues naturelles, pages 20?42, Leu-
ven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Pars-
ing Technology (IWPT), ACL 2007 Workshop, pages
1?10, Prague. Association for Computational Linguis-
tics, ACL.
Yaqian Zhou, Lide Wu, Fuliang Weng, and Hauke
Schmidt. 2003. A fast algorithm for feature se-
lection in conditional maximum entropy modeling.
In Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP ?03,
pages 153?159, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun Zhu, Ni Lao, and Eric P. Xing. 2010. Grafting-
light: fast, incremental feature selection and structure
learning of Markov random fields. In Proceedings of
the 16th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 303?
312. ACM.
63
