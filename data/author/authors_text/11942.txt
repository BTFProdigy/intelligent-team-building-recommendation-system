Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 73?78,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Multilingual Semantic Role Labeling 
 
Baoli Li, Martin Emms, Saturnino Luz, Carl Vogel 
Department of Computer Science 
Trinity College Dublin 
Dublin 2, Ireland 
{baoli.li,mtemms,luzs,vogel}@cs.tcd.ie 
 
 
 
Abstract 
This paper describes the multilingual semantic 
role labeling system of Computational Lin-
guistics Group, Trinity College Dublin, for the 
CoNLL-2009 SRLonly closed shared task. 
The system consists of two cascaded compo-
nents: one for disambiguating predicate word 
sense, and the other for identifying and classi-
fying arguments. Supervised learning tech-
niques are utilized in these two components. 
As each language has its unique characteris-
tics, different parameters and strategies have 
to be taken for different languages, either for 
providing functions required by a language or 
for meeting the tight deadline. The system ob-
tained labeled F1 69.26 averaging over seven 
languages (Catalan, Chinese, Czech, English, 
German, Japanese, and Spanish), which ranks 
the system fourth among the seven systems 
participating the SRLonly closed track. 
1 Introduction 
Semantic role labeling, which aims at computa-
tionally identifying and labeling arguments of 
predicate words, has become a leading research 
problem in computational linguistics with the ad-
vent of various supporting resources (e.g. corpora 
and lexicons) (M?rquez et al, 2008). Word seman-
tic dependencies derived by semantic role labeling 
are assumed to facilitate automated interpretation 
of natural language texts. Moreover, techniques for 
automatic annotation of semantic dependencies can 
also play an important role in adding metadata to 
corpora for the purposes of machine translation 
and speech processing. We are currently investi-
gating such techniques as part of our research into 
integrated language technology in the Center for 
Next Generation Localization (CNGL, 
http://www.cngl.ie). The multilingual nature of the 
CoNLL-2009 shared task on syntactic and seman-
tic dependency analysis, which includes Catalan, 
Chinese, Czech, English, German, Japanese, and 
Spanish (Haji? et al, 2009), makes it a good test-
bed for our research. 
We decided to participate in the CoNLL-2009 
shared task at the beginning of March, signed the 
agreement for getting the training data on March 
2nd, 2009, and obtained all the training data (espe-
cially the part from LDC) on March 4th, 2009. Due 
to the tight time constraints of the task, we chose to 
use existing packages to implement our system. 
These time constraints also meant that we had to 
resort to less computationally intensive methods to 
meet the deadline, especially for some large data-
sets (such as the Czech data). In spite of these dif-
ficulties and resource limitations, we are proud to 
be among the 21 teams who successfully submitted 
the results1. 
As a new participant, our goals in attending the 
CoNLL-2009 SRLonly shared task were to gain 
more thorough knowledge of this line of research 
and its state-of-the-art, and to explore how well a 
system quickly assembled with existing packages 
can fare at this hard semantic analysis problem.  
Following the successful approaches taken by 
the participants of the CoNLL-2008 shared task 
(Surdeanu et al, 2008) on monolingual syntactic 
and semantic dependency analysis, we designed 
and implemented our CoNLL-2009 SRLonly sys-
tem with pipeline architecture. Two main compo-
nents are cascaded in this system: one is for 
disambiguating predicate word sense 2 , and the 
other for identifying and classifying arguments for 
                                                          
1
 According to our correspondence with Dr. Jan Haji?, totally 
31 teams among 60 registered ones signed and got the evalua-
tion data. 
2
 As predicate words are marked in the CoNLL-2009 datasets, 
we don?t need to identify predicate words. 
73
predicate words. Different supervised learning 
techniques are utilized in these two components. 
For predicate word sense disambiguation (WSD), 
we have experimented with three algorithms: SVM, 
kNN, and Na?ve Bayes. Based on experimental 
results on the development datasets, we chose 
SVM and kNN to produce our submitted official 
results. For argument identification and classifica-
tion, we used a maximum entropy classifier for all 
the seven datasets. As each language has its unique 
characteristics and peculiarities within the dataset, 
different parameters and strategies have to be taken 
for different languages (as detailed below), either 
for providing functions required by a language or 
for meeting the tight deadline. Our official submis-
sion obtained 69.26 labeled F1 averaging over the 
seven languages, which ranks our system fourth 
among the seven systems in the SRLonly closed 
track. 
The rest of this paper is organized as follows. 
Section 2 discusses the first component of our sys-
tem for predicate word sense disambiguation. Sec-
tion 3 explains how our system detects and 
classifies arguments with respect to a predicate 
word. We present experiments in Section 4, and 
conclude in Section 5. 
2 Predicate Word Sense Disambiguation 
This component tries to determine the sense of a 
predicate word in a specific context. As a sense of 
a predicate word is often associated with a unique 
set of possible semantic roles, this task is also 
called role set determination. Based on the charac-
teristics of different languages, we take different 
strategies in this step, but the same feature set is 
used for different languages. 
2.1 Methods 
Intuitively, each predicate word should be treated 
individually according to the list of its possible 
senses. We therefore designed an initial solution 
based on the traditional methods in WSD: repre-
sent each sense as a vector from its definition or 
examples; describe the predicate word for disam-
biguation as a vector derived from its context; and 
finally output the sense which has the highest simi-
larity with the current context. We also considered 
using singular value decomposition (SVD) to over-
come the data sparseness problem. Unfortunately, 
we found this solution didn?t work well in our pre-
liminary experiments. The main problem is that the 
definition of each sense of a predicate word is not 
available. What we have is just a few example con-
texts for one sense of a predicate word, and these 
contexts are often not informative enough for 
WSD. On the other hand, our limited computing 
resources could not afford SVD operation on a 
huge matrix. 
We finally decided to take each sense tag as a 
class tag across different words and transform the 
disambiguation problem into a normal multi-class 
categorization problem. For example, in the Eng-
lish datasets, all predicates with ?01? as a sense 
identifier were counted as examples for the class 
?01?. With this setting, a predicate word may be 
assigned an invalid sense tag. It is an indirect solu-
tion, but works well. We think there are at least 
two possible reasons: firstly, most predicate words 
take their popular sense in running text. For exam-
ple, in the English dataset (training and develop-
ment), 160,477 of 185,406 predicate occurrences 
(about 86.55%) take their default sense ?01?. Sec-
ondly, predicates may share some common role 
sets, even though their senses may not be exactly 
the same, e.g. ?tell? and ?inform?. 
Unlike the datasets in other languages, the Japa-
nese dataset doesn?t have specialized sense tags 
annotated for each predicate word, so we simply 
copy the predicted lemma of a predicate word to its 
PRED field. For other datasets, we derived a train-
ing sample for each predicate word, whose class 
tag is its sense tag. Then we trained a model from 
the generated training data with a supervised learn-
ing algorithm, and applied the learned model for 
predicting the sense of a predicate word. This is 
our base solution. 
When transforming the datasets, the Czech data 
needs some special processing because of its 
unique annotation format. The sense annotation for 
a predicate word in the Czech data does not take 
the form ?LEMMA.SENSE?. In most cases, no 
specialized sense tags are annotated. The PRED 
field of these words only contains ?LEMMA?. In 
other cases, the disambiguated senses are anno-
tated with an internal representation, which is 
given in a predicate word lexicon. We decomposed 
the internal representation of each predicate word 
into two parts: word index id and sense tag. For 
example, from ?zv??en? v-w10004f2? we know ?v-
w10004? is the index id of word ?zv??en??, and 
?f2? is its sense tag. We then use these derived 
74
sense tags as class tags and add a class tag ?=? for 
samples without specialized sense tag. 
For each predicate word, we derive a vector de-
scribing its context and attributes, each dimension 
of which corresponds to a feature. We list the fea-
ture types in the next subsection. Features appear-
ing only once are removed. The TF*IDF weighting 
schema is used to calculate the weight of a feature. 
Three different algorithms were tried during the 
development period: support vector machines 
(SVM), distance-weighted k-Nearest Neighbor 
(kNN) (Li et al, 2004), and Na?ve Bayes with mul-
tinomial model (Mccallum and Nigam, 1998). As 
to the SVM algorithm, we used the robust 
LIBSVM package (Chang and Lin, 2001), with a 
linear kernel and default values for other parame-
ters. The algorithms achieving the best results in 
our preliminary experiments are chosen for differ-
ent languages: SVM for Catalan, Chinese, and 
Spanish; kNN for German (k=20). 
We used kNN for English (k=20) and Czech 
(k=10) because we could not finish training with 
SVM on these two datasets in limited time. Even 
with kNN algorithm, we still had trouble with the 
English and Czech datasets, because thousands of 
training samples make the prediction for the 
evaluation data unacceptably slow. We therefore 
had to further constrain the search space for a new 
predicate word to those samples containing the 
same predicate word. If there are not samples con-
taining the same predicate word in the training data, 
we will assign it the most popular sense tag (e.g. 
?01? for English). 
How to use the provided predicate lexicons is a 
challenging issue. Lexicons for different languages 
take different formats and the information included 
in different lexicons is quite different. We derived 
a sense list lexicon from the original predicate 
lexicon for Chinese, Czech, English, and German. 
Each entry in a sense list lexicon contains a predi-
cate word, its internal representation (especially for 
Czech), and a list of sense tags that the predicate 
can have. Then we obtained a variant of our base 
solution, which uses the sense list of a predicate 
word to filter impossible senses. It works as fol-
lows: 
- Disambiguate a new predicate with the base 
solution; 
- Choose the most possible sense from all the 
candidate senses obtained in step 1: if the 
base classifier doesn?t output a vector of 
probabilities for classes, only check 
whether the predicted one is a valid sense 
for the predicate; 
- If there is not a valid sense for a new predi-
cate (including the cases where the predi-
cate does not have an entry in the sense list 
lexicon), output the most popular sense tag; 
Unfortunately, preliminary experiments on the 
German and Chinese datasets didn?t support to in-
clude such a post-processing stage. The perform-
ance with this filtering became a little worse. 
Therefore, we decided not to use it generally, but 
one exception is for the Czech data. 
With kNN algorithm, we can greatly reduce the 
time for training the Czech data, but we do have 
problem with prediction, as there are totally 
469,754 samples in the training dataset. It?s a time-
consuming task to calculate the similarities be-
tween a new sample and all the samples in the 
training dataset to find its k nearest neighbors, thus 
we have to limit the search space to those samples 
that contain the predicate word for disambiguation. 
To process unseen predicate words, we used the 
derived sense list lexicon: if a predicate word for 
disambiguation is out of the sense list lexicon, we 
simply copy its predicted lemma to the PRED field; 
if no sample in the training dataset has the same 
predicate word, we take its first possible sense in 
the sense list lexicon. With this strategy, our sys-
tem can process the huge Czech dataset in short 
time. 
2.2 Features 
The features we used in this step include3: 
 
a. [Lemma | (Lemma with POS)] of all words in the sen-
tence; 
b. Attributes of predicate word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
c. [Lemma | POS] bi-grams of predicate word and its 
[previous | following] one word; 
d. [Lemma | POS] tri-grams of predicate word and its 
[previous | following] two words; 
e. [Lemma | (Lemma with POS)] of its most [left | right] 
child; 
f. [(Lemma+Dependency_Relation+Lemma) | (POS 
+Dependency_Relation+POS)] of predicate word and 
its most [left | right] child; 
                                                          
3
 We referred to those CoNLL-2008 participants? reports, e.g. 
(Ciaramita et al, 2008), when we designed the feature sets for 
the two components. 
75
g. [Lemma | (Lemma with POS)] of the head of the pre-
dicate word; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+D-
ependency_Relation+POS)] of predicate word and its 
head; 
i. [Lemma | (Lemma with POS)] of its [previous | fol-
lowing] two brothers; 
j. [Lemma | POS | (Dependency relation)] bi-gram of 
predicate word and its [previous | following] one 
brother; 
k. [Lemma | POS | (Dependency relation)] tri-gram of 
predicate word and its [previous | following] two 
brothers. 
3 Argument Identification and Classifica-
tion  
The second component of our system is used to 
detect and classify arguments with respect to a 
predicate word. We take a joint solution rather than 
solve the problem in two consecutive steps: argu-
ment identification and argument classification. 
3.1 Methods  
By introducing an additional argument type tag ?_? 
for non-arguments, we transformed the two tasks 
(i.e. argument identification and argument classifi-
cation) into one multi-class classification problem. 
As a word can play different roles with respect to 
different predicate words and a predicate word can 
be an argument of itself, we generate a training set 
by deriving a training example from each word-
predicate pair. For example, if a sentence with two 
predicates has 7 words, we will derive 7*2=14 
training examples. Therefore, the number of train-
ing examples generated in this step will be around 
L times larger than that obtained in the previous 
step, where L is the average length of sentences. 
We chose to use maximum entropy algorithm in 
this step because of its success in the CoNLL-2008 
shared task (Surdeanu et al, 2008). Le Zhang?s 
maximum entropy package (Zhang, 2006) is inte-
grated in our system. 
The Czech data cause much trouble again for us, 
as the training data derived by the above strategy 
became even larger. We had to use a special strat-
egy for the Czech data: we selectively chose word-
predicate pairs for generating the training dataset. 
In other words, not all possible combinations are 
used. We chose the following words with respect 
to each predicate: the first and the last two words 
of a sentence; the words between the predicate and 
any argument of it; two words before the predicate 
or any argument; and two words after the predicate 
or any argument. 
In the Czech and Japanese data, some words 
may play multiple roles with respect to a predicate 
word. We thus have to consider multi-label classi-
fication problem (Tsoumakas and Katakis, 2007) 
for these two languages? data. We tried the follow-
ing two solutions: 
? Take each role type combination as a class 
and transform the multi-label problem to a 
single-label classification problem; 
? Classify a word with a set of binary classi-
fiers: consider each role type individually 
with a binary classifier; any possible role 
type will be output; if no role type is ob-
tained after considering all the role types, 
the role type with the highest confidence 
value will be output; and, if ?_? is output 
with any other role type, remove it. 
We used the second solution in our official 
submission, but we finally found these two solu-
tions perform almost the same. The performance 
difference is very small. We found the cases with 
multi-labels (actually at most two) in the training 
data are very limited: 690 of 414,326 in the Czech 
data and 113 of 46,663 in the Japanese data. 
3.2 Features 
The features we used in this step include: 
 
a. Whether the current word is a predicate; 
b. [Lemma | POS] of current word and its [previous | fol-
lowing] one word; 
c. [Lemma | POS] bi-grams of current word and its [pre-
vious | following] one word; 
d. POS tri-grams of current word, its previous word and 
its following word; 
e. Dependency relation of current word to its head; 
f. [Lemma | POS] of the head of current word; 
g. [Lemma | POS] bi-grams of current word and its head; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of current word and its 
head; 
i. [Lemma | POS] of its most [left | right] child; 
j. [Lemma | POS] bi-grams of current word and its most 
[left | right] child; 
k. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS) of current word and its 
most [left | right] child; 
l. The number of children of the current word and the 
predicate word; 
m. Attributes of the current word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
n. The sense tag of the predicate word; 
76
o. [Lemma | POS] of the predicate word and its head; 
p. Dependency relation of the predicate word to its head; 
q. [Lemma | POS] bi-grams of the predicate word and its 
head; 
r. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its head; 
s. [Lemma | POS] of the most [left | right] child of the 
predicate word; 
t. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of predicate word and its 
head; 
u. [Lemma | POS] bi-gram of the predicate word and its 
most [left | right] child; 
v. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its most [left | right] child; 
w. The relative position of the current word to the predi-
cate one: before, after, or on; 
x. The distance of the current word to the predicate one; 
y. The relative level (up, down, or same) and level dif-
ference on the syntactic dependency tree of the current 
word to the predicate one; 
z. The length of the shortest path between the current 
word and the predicate word. 
4 Experiments 
4.1 Datasets  
The datasets of the CoNLL-2009 shared task con-
tain seven languages: Catalan (CA), Chinese (CN), 
Czech (CZ), English (EG), German (GE), Japanese 
(JP), and Spanish (SP). The training and evaluation 
data of each language (Taul? et al, 2008; Xue et 
al., 2008; Haji? et al, 2006; Palmer et al, 2002; 
Burchardt et al, 2006; Kawahara et al, 2002) have 
been converted to a uniform CoNLL Shared Task 
format. Each participating team is required to 
process all seven language datasets.  
 
Lanuage CA CN CZ EN GE JP SP 
Size (KB) 48974 41340 94284 58155 41091 8948 52430 
# of Sen-
tences 14924 24039 43955 40613 38020 4643 15984 
# of Predi-
cate words 42536 110916 469754 185404 17988 27251 48900 
Avg. # of 
Predicates 
per sentence 
2.85 4.61 10.69 4.57 0.47 5.87 3.06 
popular 
sense tag 
a2 
(37%) 
01 
(90%) 
= 
(81%) 
01 
(87%) 
1 
(75%) 
= 
(100%) 
a2 
(39%) 
Table 1. Statistical information of the seven language 
datasets (training and development). 
 
Table 1 shows some statistical information of 
both training and development data for each lan-
guage. The total size of the uncompressed original 
data without lexicons is about 345MB. The Czech 
dataset is the largest one containing 43,955 sen-
tences and 469,754 predicate words, while the 
Japanese dataset the smallest one. On average, 
10.69 predicate words appear in a Czech sentence, 
while only 0.47 predicate words exist in a German 
sentence. The most popular sense tag in the Czech 
datasets is ?=?, which means the PRED field has 
the same value as the PLEMMA field or the 
FORM field. About 81% of Czech predicate words 
take this value. 
4.2 Experimental Results  
F1 is used as the main evaluation metric in the 
CoNLL-2009 shared task. As to the SRLonly track, 
a joint semantic labeled F1, which considers predi-
cate word sense disambiguation and argument la-
beling equally, is used to rank systems. 
 
Avg. CA CN CZ EG GE JP SP 
69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54 
Table 2. Official results of our system. 
 
Table 2 gives the official results of our system 
on the evaluation data. The system obtained the 
best result (74.06) on the Catalan data, but per-
formed very poor (57.46) on the Czech data. Ex-
cept the Czech data, our system performs quite 
stable on the other six language data with mean of 
71.23 and standard deviation of 2.42. 
 
 Avg. CA CN CZ EG GE JP SP 
Over-
all F1 69.47 74.12 70.52 57.57 70.24 67.97 72.17 73.68 
Pred. 
WSD 
F1 
86.9 84.42 94.54 72.23 92.98 81.09 99.07 83.96 
Arg 
I&C 
F1 
57.24 69.29 57.71 33.19 58.25 60.64 52.72 68.86 
Arg 
I&C 
PR 
69.77 73.43 72.48 62.14 70.14 66.63 69.37 74.23 
Arg 
I&C 
RE 
49.77 65.6 47.94 22.64 49.81 55.64 42.52 64.21 
Table 3. Results of our system after fixing a minor bug. 
 
After submitting the official results, we found 
and fixed a minor bug in the implementation of the 
second component. Table 3 presents the results of 
our system after fixing this bug. The overall per-
formance doesn?t change much. We further ana-
lyzed the bottlenecks by checking the performance 
of different components. 
At the predicate WSD part, our system works 
reasonable with labeled F1 86.9, but the perform-
ance on the Czech data is lower than that of a base-
line system that constantly chooses the most 
popular sense tag. If we use this baseline solution, 
77
we can get predicate WSD F1 78.66, which further 
increases the overall labeled F1 on the Czech data 
to 61.68 from 57.57 and the overall labeled F1 
over the seven languages to 70.05 from 69.47. 
From table 3, we can see our system performs 
relatively poorly for argument identification and 
classification (57.24 vs. 86.9). The system seems 
too conservative for argument identification, which 
makes the recall very lower. We explored some 
strategies for improving the performance of the 
second component, e.g. separating argument iden-
tification and argument classification, and using 
feature selection (with DF threshold) techniques, 
but none of them helps much. We are thinking the 
features currently used may not be effective 
enough, which deserves further study. 
5 Conclusion and Future Work  
In this paper, we describe our system for the 
CoNLL-2009 shared task -- SRLonly closed track. 
Our system was built on existing packages with a 
pipeline architecture, which integrated two cas-
caded components: predicate word sense disam-
biguation and argument identification and 
classification. Our system performs well at disam-
biguating the sense of predicate words, but poorly 
at identifying and classifying arguments. In the 
future, we plan to explore much effective features 
for argument identification and classification. 
Acknowledgments 
This research was funded by Science Foundation 
Ireland under the CNGL grant. We used the IITAC 
Cluster in our initial experiments. We thank IITAC, 
the HEA, the National Development Plan and the 
Trinity Centre for High Performance Computing 
for their support. We are also obliged to John 
Keeney for helping us running our system on the 
CNGL servers. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2006). Genoa, Italy. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Massimiliano Ciaramita, Giuseppe Attardi, Felice 
Dell?Orletta, and Mihai Surdeanu. 2008. DeSRL: A 
Linear-Time Semantic Role Labeling System. Pro-
ceedings of the CoNLL-2008. 
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th 
Conference on Computational Natural Language 
Learning (CoNLL-2009). Boulder, Colorado, USA. 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie 
Mikulov? and Zden?k ?abokrtsk?. 2006. The Prague 
Dependency Treebank 2.0. Linguistic Data 
Consortium, USA. ISBN 1-58563-370-4. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. 
Baoli Li, Qin Lu and Shiwen Yu. 2004. An Adaptive k-
Nearest Neighbor Text Categorization Strategy. ACM 
Transactions on Asian Language Information 
Processing, 3(4): 215-226. 
Llu?s M?rquez, Xavier Carreras, Kenneth C. Litkowski 
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145-159. 
Andrew Mccallum and Kamal Nigam. 1998. A Com-
parison of Event Models for Naive Bayes Text Clas-
sification. Proceedings of AAAI/ICML-98 Workshop 
on Learning for Text Categorization. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis Marquez and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and 
Semantic Dependencies. Proceedings of the 12th 
Conference on Computational Natural Language 
Learning (CoNLL-2008).  
Mariona Taul?, Maria Ant?nia Mart? and Marta 
Recasens. 2008. AnCora: Multilevel Annotated 
Corpora for Catalan and Spanish. Proceedings of the 
6th International Conference on Language Resources 
and Evaluation (LREC-2008). Marrakech, Morocco. 
Grigorios Tsoumakas and Ioannis Katakis. 2007. Multi-
Label Classification: An Overview. International 
Journal of Data Warehousing and Mining, 3(3):1-13. 
Nianwen Xue and Martha Palmer. 2009. Adding 
semantic roles to the Chinese Treebank.  Natural 
Language Engineering, 15(1):143-172.  
Le Zhang. 2006. Maximum Entropy Modeling Toolkit 
for Python and C++. Software available at 
http://homepages.inf.ed.ac.uk/s0450736/maxent_tool
kit.html. 
78
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 332?339,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Assessing the effectiveness of conversational features for dialogue
segmentation in medical team meetings and in the AMI corpus
Saturnino Luz
Department of Computer Science
Trinity College Dublinm Ireland
luzs@cs.tcd.ie
Jing Su
School of Computer Science and Statistics
Trinity College Dublin, Ireland
sujing@scss.tcd.ie
Abstract
This paper presents a comparison of two
similar dialogue analysis tasks: segment-
ing real-life medical team meetings into
patient case discussions, and segment-
ing scenario-based meetings into topics.
In contrast to other methods which use
transcribed content and prosodic features
(such as pitch, loudness etc), the method
used in this comparison employs only the
duration of the prosodic units themselves
as the basis for dialogue representation. A
concept of Vocalisation Horizon (VH) al-
lows us to treat segmentation as a clas-
sification task where each instance to be
classified is represented by the duration
of a talk spurt, pause or speech overlap
event in the dialogue. We report on the re-
sults this method yielded in segmentation
of medical meetings, and on the implica-
tions of the results of further experiments
on a larger corpus, the Augmented Multi-
party Meeting corpus, to our ongoing ef-
forts to support data collection and infor-
mation retrieval in medical team meetings.
1 Introduction
As computer mediated communication becomes
more widespread, and data gathering devices start
to make their way into the meeting rooms and the
workplace in general, the need arises for mod-
elling and analysis of dialogue and human com-
municative behaviour in general (Banerjee et al,
2005). The focus of our interest in this area is
the study of multi-party interaction at Multidis-
ciplinary Medical Team Meeting (MDTMs), and
the eventual recording of such meetings followed
by indexing and structuring for integration into
electronic health records. MDTMs share a num-
ber of characteristics with more conventional busi-
ness meetings, and with the meeting scenarios tar-
geted in recent research projects (Renals et al,
2007). However, MDTMs are better structured
than these meetings, and more strongly influenced
by the time pressures placed upon the medical pro-
fessionals who take part in them (Kane and Luz,
2006).
In order for meeting support and review systems
to be truly effective, they must allow users to effi-
ciently browse and retrieve information of interest
from the recorded data. Browsing in these media
can be tedious and time consuming because con-
tinuous media such as audio and video are difficult
to access since they lack natural reference points.
A good deal of research has been conducted on in-
dexing recorded meetings. From a user?s point of
view, an important aspect of indexing continuous
media, and audio in particular, is the task of struc-
turing the recorded content. Banerjee et al (2005),
for instance, showed that users took significantly
less time to retrieve answers when they had access
to discourse structure annotation than in a control
condition in which they had access only to unan-
notated recordings.
The most salient discourse structure in a meet-
ing is the topic of conversation. The content within
a given topic is cohesive and should therefore be
viewed as a whole. In MDTMs, the meeting con-
sists basically of successive patient case discus-
sions (PCDs) in which the patient?s condition is
discussed among different medical specialists with
the objective of agreeing diagnoses, making pa-
tient management decisions etc. Thus, the individ-
ual PCDs can be regarded as the different ?topics?
which make up an MDTM (Luz, 2009).
In this paper we explore the use of a content-
free approach to the representation of vocalisation
events for segmentation of MDTM dialogues. We
start by extending the work of Luz (2009) on a
small corpus of MDTM recordings, and then test
our approach on a larger dataset, the AMI (Aug-
332
mented Multi-Party Interaction) corpus (Carletta,
2007). Our ultimate goal is to analyse and apply
the insights gained on the AMI corpus to our work
on data gathering and representation in MDTMs.
2 Related work
Topic segmentation and detection, as an aid to
meeting information retrieval and meeting index-
ing, has attracted the interest of many researchers
in recent years. The objective of topic segmenta-
tion is to locate the beginning and end time of a
cohesive segment of dialogue which can be sin-
gled out as a ?topic?. Meeting topic segmentation
has been strongly influenced by techniques devel-
oped for topic segmentation in text (Hearst, 1997),
and more recently in broadcast news audio, even
though it is generally acknowledged that dialogue
segmentation differs from text and scripted speech
in important respects (Gruenstein et al, 2005).
In early work (Galley et al, 2003), meeting
annotation focused on changes that produce high
inter-annotator agreement, with no further specifi-
cation of topic label or discourse structure. Cur-
rent work has paid greater attention to discourse
structure, as reflected in two major meeting cor-
pus gathering and analysis projects: the AMI
project (Renals et al, 2007) and the ICSI meet-
ing project (Morgan et al, 2001). The AMI cor-
pus distinguishes top-level and functional topics
such as ?presentation?, ?discussion?, ?opening?,
?closing?, ?agenda? which are further specified
into sub-topics (Hsueh et al, 2006). Gruenstein
et al (2005) sought to annotated the ICSI cor-
pus hierarchically according to topic, identifying,
in addition, action items and decision points. In
contrast to these more general types of meetings,
MDTMs are segmented into better defined units
(i.e. PCDs) so that inter-annotator agreement on
topic (patient case discussion) boundaries is less of
an issue, since PCDs are collectively agreed parts
of the formal structure of the meetings.
Meeting transcripts (either done manually or
automatically) have formed the basis for a num-
ber of approaches to topic segmentation (Galley
et al, 2003; Hsueh et al, 2006; Sherman and Liu,
2008). The transcript-based meeting segmentation
described in (Galley et al, 2003) adapted the un-
supervised lexical cohesion method developed for
written text segmentation (Hearst, 1997). Other
approaches have employed supervised machine
learning methods with textual features (Hsueh et
al., 2006). Prosodic and conversational features
have also been integrated into text-based represen-
tations, often improving segmentation accuracy
(Galley et al, 2003; Hsueh and Moore, 2007).
However, approaches that rely on transcription,
and sometimes higher-level annotation on tran-
scripts, as is the case of (Sherman and Liu, 2008),
have two shortcomings which limit their applica-
bility to MDTM indexing. First, manual transcrip-
tion is unfeasible in a busy hospital setting, and
automatic speech recognition of unconstrained,
noisy dialogues falls short of the levels of accu-
racy required for good segmentation. Secondly,
the contents of MDTMs are subject to stringent
privacy and confidentiality constraints which limit
access to training data. Regardless of such appli-
cation constraints, some authors (Malioutov et al,
2007; Shriberg et al, 2000) argue for the use of
prosodic features and other acoustic patterns di-
rectly from the audio signal for segmentation. The
approach investigated in this paper goes a step fur-
ther by representing the data solely through what
is, arguably, the simplest form of content-free rep-
resentation, namely: duration of talk spurts, si-
lences and speech overlaps, optionally comple-
mented with speaker role information (e.g. medi-
cal speciality).
3 Content-free representations
There is more to the structure (and even the
semantics) of a dialogue than the textual con-
tent of the words exchanged by its participants.
The role of prosody in shaping the illocution-
ary force of vocalisations, for instance, is well
documented (Holmes, 1984), and prosodic fea-
tures have been used for automatic segmentation
of broadcast news data into sentences and topics
(Shriberg et al, 2000). Similarly, recurring audio
patterns have been employed in segmentation of
recorded lectures (Malioutov et al, 2007). Works
in the area of social psychology have used the sim-
ple conversational features of duration of vocalisa-
tions, pauses and overlaps to study the dynamics
of group interaction. Jaffe and Feldstein (1970)
characterise dialogues as Markov processes, and
Dabbs and Ruback (1987) suggest that a ?content-
free? method based on the amount and structure
of vocal interactions could complement group in-
teraction frameworks such as the one proposed
by Bales (1950). Pauses and overlap statistics
alone can be used, for instance, to characterise
333
Speakers
Vocalisationevents ...
Eventdurations ... ... ...
Gaps &Overlaps
Figure 1: Vocalisation Horizon for event v.
the differences between face-to-face and telephone
dialogue (ten Bosch et al, 2005), and a corre-
lation between the duration of pauses and topic
boundaries has been demonstrated for recordings
of spontaneous narratives (Oliveira, 2002).
These works provided the initial motivation for
our content-free representation scheme and the
topic segmentation method proposed in this paper.
It is easy to verify by inspection of both the corpus
of medical team meetings described in Section 4
and the AMI corpus that pauses and vocalisations
vary significantly in duration and position on and
around topic boundaries. Table 1 shows the mean
durations of vocalisations that initiate new topics
or PCDs in MDTMs and the scenario-based AMI
meetings, as well as the durations of pauses and
overlaps that surround it (within one vocalisation
event to the left and right). In all cases the dif-
ferences were statistically significant. These re-
sults agree with those obtained by Oliveira (2002)
for discourse topics, and suggest that an approach
based on representing the duration of vocalisa-
tions, pauses and overlaps in the immediate con-
text of a vocalisation might be effective for auto-
matic segmentation of meeting dialogues into top-
ics or PCDs.
Table 1: Mean durations in seconds (and standard
deviations) of vocalisation and pauses on and near
topic boundaries in MDTM and AMI meetings.
Boundary Non-boundary t-test
AMI vocal. 5.3 (8.2) 1.6 (3.5) p < .01
AMI pauses 2.6 (4.9) 1.2 (2.8) p < .01
AMI overlaps 0.4 (0.4) 0.3 (0.6) p < .01
MDTM vocal. 12.0 (15.5) 8.1 (14.7) p < .05
MDTM pauses 9.7 (12.7) 8.2 (14.8) p < .05
We thus conceptualise meeting topic segmenta-
tion as a classification task approachable through
supervised machine learning. A meeting can
be pre-segmented into separate vocalisations (i.e.
talk spurts uttered by meeting participants) and si-
lences, and such basic units (henceforth referred
to as vocalisation events) can then be classified
as to whether they signal a topic transition. The
basic defining features of a vocalisation event are
the identity of the speaker who uttered the vocali-
sation (or speakers, for events containing speech
overlap) and its duration, or the duration of a
pause, for silence events. However, identity la-
bels and interval durations by themselves are not
enough to enable segmentation. As we have seen
above, some approaches to meeting segmentation
complement these basic data with text (keywords
or full transcription) uttered during vocalisation
events, and with prosodic features. Our proposal
is to retain the content-free character of the ba-
sic representation by complementing the speaker
and duration information for an event with data de-
scribing its preceding and succeeding events. We
thus aim to capture an aspect of the dynamics of
the dialogue by representing snapshots of vocali-
sation sequences. We call this general representa-
tion strategy Vocalisation Horizon (VH).
Figure 1 illustrates the basic idea. Vocalisa-
tion events are placed on a time line and com-
bine utterances produced by the speakers who took
part in the meeting. These events can be labelled
with nominal attributes (s1, s2, . . .) denoting the
speaker (or some other symbolic attribute, such as
the speaker?s role in the meeting). Silences (gaps)
and group talk (overlap) can either be assigned re-
served descriptors (such as ?Floor? and ?Group?)
or regarded as separate annotation layers. The
general data representation scheme for, say, seg-
ment v would involve a data from its left context
(v1?, v2?, v3?, . . .) and its right context (v1, v2, v3, . . .)
in addition to the data for v itself. These can be a
combination of symbolic labels (in Figure 1, for
instance, s1 for the current speaker, s3, s2, s1, . . .
for the preceding events and s3, s2, s3, . . . for the
following events), durations (d, d1?, d2?, d3?, . . . etc)
334
SpeakersVoarocaloit
nnnir
virV.EedeElratcuropln G rtsce&Onscpeaker lipou.EedeEattilaloit Proceedings of the SIGDIAL 2013 Conference, pages 160?162,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
WebWOZ: A Platform for Designing and Conducting Web-based Wizard
of Oz Experiments
Stephan Schlo?gl
Institut Mines-Te?le?com
Te?le?com ParisTech, CNRS LTCI
Paris, France
schlogl@enst.fr
Saturnino Luz, Gavin Doherty
Trinity College
University of Dublin
Dublin, Ireland
{firstname.lastname}@scss.tcd.ie
Abstract
The Wizard of Oz (WOZ) method has
been used for a variety of purposes in
early-stage development of dialogue sys-
tems and language technology applica-
tions, from data collection, to experimen-
tation, prototyping and evaluation. How-
ever, software to support WOZ experimen-
tation is often developed ad hoc for spe-
cific application scenarios. In this demo
we present WebWOZ, a web-based WOZ
prototyping platform that aims at support-
ing a variety of experimental settings and
combinations of different language tech-
nology components. We argue that a
generic and distributed platform such as
WebWOZ can increase the usefulness of
the WOZ method.
1 Introduction
The use of language technologies such as Auto-
matic Speech Recognition (ASR), Machine Trans-
lation (MT) and Text-to-Speech Synthesis (TTS)
has significantly increased in recent years. Drivers
of adoption have been enhanced quality and in-
creasingly ubiquitous access to products and ser-
vices. However, the technology is still far from
perfect and typically substantial engineering effort
is needed before prototypes can deliver a user ex-
perience robust enough to allow potential applica-
tions to be evaluated with real users. For graph-
ical interfaces, well-known prototyping methods
like sketching and wire-framing support the de-
signer in obtaining early impressions and initial
user feedback. These low-fidelity prototyping
techniques do, however, not map well onto sys-
tems based around speech and natural language.
Wizard of Oz (WOZ) tries to fill this gap by using
a human ?wizard? to mimic some of the function-
ality of a system, which allows for evaluating po-
tential user experiences and interaction strategies
without the need for building a fully functional
product first (Gould et al, 1983).
2 The WebWOZ Platform
WebWOZ is an entirely web-based, open-source
Wizard of Oz prototyping platform1. It allows for
testing interaction scenarios that employ one or
more Language Technology Components (LTC).
The integration of these LTCs is done via web ser-
vices. Currently we have integrated ASR from
Google using HTML-based Speech Input2, on-
the-fly MT from Microsoft3 and TTS provided
by the Muse Speech Technology Research Plat-
form4. In addition we support pre-recorded audio
and video files that are accessible through a web
server. Table 1 shows the different components
currently integrated into WebWOZ. Depending on
the application scenario those components can be
turned on and off as well as be used in combina-
tion (Schlo?gl et al, 2010; Schlo?gl et al, 2011).
2.1 Software Requirements
WebWOZ is written in Java and therefore can be
hosted on a typical application server (e.g. Apache
Tomcat). In addition a relational database (e.g.
MySQL) is needed. In order to run experiments
we further recommend the use of an up-to-date
web browser that is able to adequately interpret
recent HTML5 commands. For the moment, the
Chrome browser is probably the best choice, since
it supports speech input without the need for in-
stalling an additional plug-in. However, we are
convinced that soon most web browsers will sup-
port the majority of HTML5 features required by
WebWOZ.
1https://github.com/stephanschloegl/WebWOZ/
2http://lists.w3.org/Archives/Public/public-xg-
htmlspeech/2011Feb/att-0020/api-draft.html
3http://msdn.microsoft.com/en-us/library/ff512419.aspx
4http://muster.ucd.ie/content/muse-speech-technology-
research-platform
160
Table 1: WebWOZ Component List
ASR HTML Speech Input
MT Microsoft Translate
TTS Muse Speech Technology
Pre-recorded Audio Files
2.2 Supported Scenarios
One of the main features of WebWOZ is its in-
tegrated CMS-like editing functionality. This
permits researchers/designers to create their own
WOZ experiments without requiring from them
any programming skills. They can add, edit, and
delete utterances and organize them in different
tabs (dialogue stages) using the wizard interface
(cf. demo video5). Corresponding client (i.e. non-
wizard) user/password combinations can be added
and distinct interaction modes for the experiment
can be set (e.g. ASR on/off, TTS on/off, MT
on/off, etc.). The client interface itself runs in
a separate browser window, which allows for an
easy integration into already existing web applica-
tions.
Following this architecture WebWOZ supports
the design of a variety of experimental settings.
Different scenarios from classic monolingual text-
to-text to multi-lingual speech-to-speech interac-
tions are possible. From a wizard?s perspective,
tasks can reach from pure dialogue management
to augmenting LTC output. That is, in WebWOZ
a wizard can act as the substitute for a working di-
alogue manager, linking a test persons? input with
an appropriate response by choosing from a set
of pre-defined answer possibilities. Alternatively,
however, one could be focusing on enhancing the
quality of a single LTC by augmenting its output.
Examples might include choosing from an n-best
list of recognition results or the post-editing of
output produced by an MT service.
3 Why a Web-based Solution?
The WOZ technique is usually used for four main
purposes related to the design and implementation
of dialogue systems: (1) it is used for dialogue
data collection, (2) for controlled experimentation
(including system evaluation), (3) for exploration
of design alternatives and (4) for teaching of sys-
tem design. Given this context, why should one
build a web-based WOZ platform? What are the
5http://youtu.be/VPqHfXHq4X0
benefits of such a solution? As it turns out, one can
identify benefits to each of the above mentioned
main uses of the WOZ method.
In terms of data collection, the gathering of mul-
timodal dialogue corpora is often a complex and
time consuming enterprise. It requires standard-
ization and uniformity with respect to data format,
timing and encoding, as well as collection settings
and procedures. WOZ techniques have been in-
creasingly used for this purpose, particularly in the
gathering of data for studying multimodal infor-
mation presentation and interaction e.g. (Rieser et
al., 2011). A Web-based platform such as Web-
WOZ can facilitate data collection by geographi-
cally distributed groups while guaranteeing adher-
ence to the requisite standards.
As regards experiments, a crucial requirement
from the perspective of scientific methodology is
reproducibility. Different research groups need to
be able to replicate experiments according to pre-
cisely prescribed procedures and settings. Wiz-
ard of OZ experiments, however, are usually con-
ducted using purpose built, ad hoc tools and soft-
ware. This makes replication difficult, if not im-
possible. WebWOZ provides a widely available,
standardized environment in which experimental
protocols can be precisely specified and shared
with interested research groups, thus supporting
reproducibility. These features are similarly im-
portant for extrinsic system components evalua-
tion e.g. (Schneider and Luz, 2011) where the
overall system functionality should be kept con-
stant while a specific component to be tested (say,
an MT module) is varied.
WOZ techniques are also employed for explo-
ration (through prototyping) of design ideas and
alternatives, particularly at the early design stages
of interactive systems that involve diverse lan-
guage technology components. In this case, repro-
ducibility and controlled conditions are less im-
portant. However, as distributed system develop-
ment becomes a common practice WebWOZ can
be used in such scenarios as a shared design arti-
fact to support the activities of geographically dis-
tributed design teams as well as the communica-
tion among them.
Finally, WebWOZ can be (and has been) used in
support of teaching the development of dialogue
systems. While students are usually introduced to
WOZ (i.e. written on a lecture slide) only a small
portion of them receives actual hands-on experi-
161
ence. One reason for this lack of practical usage
might be that in order to be applicable in a teaching
context, any approach would have to have a low
logistical and technical overhead to enable stu-
dents to quickly design and carry out evaluations.
Our experience with WebWOZ has shown that the
web-based approach significantly lowers this bar-
rier. To date more than 50 students were able to de-
sign experiments and hence improve their under-
standing of the complexity of dialogue systems.
4 Uses of WebWOZ in Research
WebWOZ has already been employed in two dif-
ferent research studies. The first study explored
the effects of MT when it is used in combination
with TTS (Schneider et al, 2010). The second
study aimed at building and evaluating a corpus of
feedback utterances sent to language learners who
try to improve their pronunciation (Cabral et al,
2012).
The experimental set-up of these two stud-
ies differed greatly, highlighting the flexibility of
WebWOZ. The first study tested the scenario of
an intelligent computer system recommending ap-
propriate Internet connection bundles to German
speaking customers. To support this scenario a
set of pre-defined dialogue utterances as well as
the relevant domain utterances (i.e. examples of
Internet connection bundles) were collected, auto-
matically translated and then added to WebWOZ.
On-the-fly translation was not used as the experi-
menters wanted to control for any possible incon-
sistencies. The TTS part of the experiment did
not utilize a synthesis directly, but rather used the
possibility of WebWOZ handling pre-synthesized
audio files. ASR was simulated by the wizard.
Voice-over-IP was used to transmit the partici-
pant?s voice to the wizard, who then selected an
appropriate response.
The second study was less restrictive. Here the
researcher?s goal was to built up and evaluate a
corpus of feedback utterances, for which the wiz-
ard could be more open in terms of responses.
Similarly to the first study a set of pre-defined
responses was added to WebWOZ. However, in
cases were those utterances were not sufficient, the
wizard could use a free-text field to reply. Again
Voice-over-IP was used to transfer speech input
from a test user to the wizard and TTS was turned
off, as the experiment design used textual feed-
back only.
5 Conclusion and Future Work
We presented WebWOZ a Wizard of Oz proto-
typing platform that is developed in our research
group. WebWOZ differs from existing WOZ tools
by being entirely web-based and through its goal
of supporting various types of application scenar-
ios. The different features of WebWOZ were high-
lighted and it was described how two independent
studies already made use of them. Future work
aims to optimize WebWOZ, to generalise it to fur-
ther experimental settings and to extend it by inte-
grating additional modalities. To do so the system
has been installed in our partner institutions where
it has currently been adapted to support additional
settings in at least two other research projects. Al-
though we are aware of the fact that the great
difference between the interests of individual re-
searchers pose challenges to the design of a truly
generic WOZ tool, we believe that our platform
can be a helpful starting point for a variety of re-
searchers and designers who may wish to use the
WOZ method.
References
J. P. Cabral, M. Kane, Z. Ahmed, M. Abou-Zleikha,
E?. Sze?kely, A. Zahra, K. U. Ogbureke, P. Cahill,
J. Carson-Berndsen, and S. Schlo?gl. 2012. Rapidly
Testing the Interaction Model of a Pronunciation
Training System via Wizard-of-Oz. In Proceedings
of LREC.
J. D. Gould, J. Conti, and T. Hovanyecz. 1983. Com-
posing letters with a simulated listening typewriter.
Communications of the ACM, 26:295?308.
V. Rieser, S. Keizer, X. Liu, and O. Lemon. 2011.
Adaptive Information Presentation for Spoken Dia-
logue Systems: Evaluation with human subjects. In
Proceedings of ENLG, pages 102?109.
S. Schlo?gl, G. Doherty, N. Karamanis, A. H. Schneider,
and S. Luz. 2010. Observing the wizard: In search
of a generic interface for wizard of oz studies. In
Proceedings of Irish HCI, pages 43?50.
S. Schlo?gl, A. H. Schneider, S. Luz, and G. Doherty.
2011. Supporting the wizard: Interface improve-
ments in wizard of oz studies. In Proceedings of
BCS HCI.
A. H. Schneider and S. Luz. 2011. Speaker alignment
in synthesised, machine translated communication.
In Proceedings of IWSLT, pages 254?260.
A. H. Schneider, I. Van der Sluis, and S. Luz. 2010.
Comparing intrinsic and extrinsic evaluation of mt
output in a dialogue system. In Proceedings of the
IWSLT, pages 329?336.
162
