Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1024?1033, Prague, June 2007. c?2007 Association for Computational Linguistics
A Topic Model for Word Sense Disambiguation
Jordan Boyd-Graber
Computer Science
Princeton University
Princeton, NJ 08540
jbg@princeton.edu
David Blei
Computer Science
Princeton University
Princeton, NJ 08540
blei@cs.princeton.edu
Xiaojin Zhu
Computer Science
University of Wisconsin
Madison, WI 53706
jerryzhu@cs.wisc.edu
Abstract
We develop latent Dirichlet alocation with
WORDNET (LDAWN), an unsupervised
probabilistic topic model that includes word
sense as a hidden variable. We develop a
probabilistic posterior inference algorithm
for simultaneously disambiguating a corpus
and learning the domains in which to con-
sider each word. Using the WORDNET hi-
erarchy, we embed the construction of Ab-
ney and Light (1999) in the topic model and
show that automatically learned domains
improve WSD accuracy compared to alter-
native contexts.
1 Introduction
Word sense disambiguation (WSD) is the task of
determining the meaning of an ambiguous word in
its context. It is an important problem in natural
language processing (NLP) because effective WSD
can improve systems for tasks such as information
retrieval, machine translation, and summarization.
In this paper, we develop latent Dirichlet aloca-
tion with WORDNET (LDAWN), a generative prob-
abilistic topic model for WSD where the sense of
the word is a hidden random variable that is inferred
from data.
There are two central advantages to this approach.
First, with LDAWN we automatically learn the con-
text in which a word is disambiguated. Rather
than disambiguating at the sentence-level or the
document-level, our model uses the other words that
share the same hidden topic across many documents.
Second, LDAWN is a fully-fledged generative
model. Generative models are modular and can be
easily combined and composed to form more com-
plicated models. (As a canonical example, the ubiq-
uitous hidden Markov model is a series of mixture
models chained together.) Thus, developing a gen-
erative model for WSD gives other generative NLP
algorithms a natural way to take advantage of the
hidden senses of words.
In general, topic models are statistical models of
text that posit a hidden space of topics in which the
corpus is embedded (Blei et al, 2003). Given a
corpus, posterior inference in topic models amounts
to automatically discovering the underlying themes
that permeate the collection. Topic models have re-
cently been applied to information retrieval (Wei and
Croft, 2006), text classification (Blei et al, 2003),
and dialogue segmentation (Purver et al, 2006).
While topic models capture the polysemous use
of words, they do not carry the explicit notion of
sense that is necessary for WSD. LDAWN extends
the topic modeling framework to include a hidden
meaning in the word generation process. In this
case, posterior inference discovers both the topics
of the corpus and the meanings assigned to each of
its words.
After introducing a disambiguation scheme based
on probabilistic walks over the WORDNET hierar-
chy (Section 2), we embed the WORDNET-WALK
in a topic model, where each topic is associated with
walks that prefer different neighborhoods of WORD-
NET (Section 2.1). Then, we describe a Gibbs sam-
pling algorithm for approximate posterior inference
that learns the senses and topics that best explain a
corpus (Section 3). Finally, we evaluate our system
on real-world WSD data, discuss the properties of
the topics and disambiguation accuracy results, and
draw connections to other WSD algorithms from the
research literature.
1024
1740
entity 1930
3122object
20846
15024
animal 1304946
1305277
artifact male
2354808 2354559
foalcolt
3042424
colt
4040311
revolver
Synset ID
Word
six-gun
six-shooter
0.00 0.25
0.58
0.00 0.04
0.02 0.010.16
0.05
0.04
0.690.00
0.00
0.381.000.42 0.00
0.000.57
1.00
0.38
0.07
Figure 1: The possible paths to reach the word ?colt?
in WORDNET. Dashed lines represent omitted links.
All words in the synset containing ?revolver? are
shown, but only one word from other synsets is
shown. Edge labels are probabilities of transitioning
from synset i to synset j. Note how this favors fre-
quent terms, such as ?revolver,? over ones like ?six-
shooter.?
2 Topic models and WordNet
The WORDNET-WALK is a probabilistic process of
word generation that is based on the hyponomy re-
lationship in WORDNET (Miller, 1990). WORD-
NET, a lexical resource designed by psychologists
and lexicographers to mimic the semantic organiza-
tion in the human mind, links ?synsets? (short for
synonym sets) with myriad connections. The spe-
cific relation we?re interested in, hyponomy, points
from general concepts to more specific ones and is
sometimes called the ?is-a? relationship.
As first described by Abney and Light (1999), we
imagine an agent who starts at synset [entity],
which points to every noun in WORDNET 2.1 by
some sequence of hyponomy relations, and then
chooses the next node in its random walk from the
hyponyms of its current position. The agent repeats
this process until it reaches a leaf node, which corre-
sponds to a single word (each of the synset?s words
are unique leaves of a synset in our construction).
For an example of all the paths that might gener-
ate the word ?colt? see Figure 1. The WORDNET-
WALK is parameterized by a set of distributions over
children for each synset s in WORDNET, ?s.
Symbol Meaning
K number of topics
?k,s multinomial probability vector over
the successors of synset s in topic k
S scalar that, when multiplied by ?s
gives the prior for ?k,s
?s normalized vector whose ith entry,
when multiplied by S, gives the prior
probability for going from s to i
?d multinomial probability vector over
the topics that generate document d
? prior for ?
z assignment of a word to a topic
? a path assignment through
WORDNET ending at a word.
?i,j one link in a path ? going from syn-
set i to synset j.
Table 1: A summary of the notation used in the pa-
per. Bold vectors correspond to collections of vari-
ables (i.e. zu refers to a topic of a single word, but
z1:D are the topics assignments of words in docu-
ment 1 through D).
2.1 A topic model for WSD
The WORDNET-WALK has two important proper-
ties. First, it describes a random process for word
generation. Thus, it is a distribution over words
and thus can be integrated into any generative model
of text, such as topic models. Second, the synset
that produces each word is a hidden random vari-
able. Given a word assumed to be generated by a
WORDNET-WALK, we can use posterior inference
to predict which synset produced the word.
These properties allow us to develop LDAWN,
which is a fusion of these WORDNET-WALKs and
latent Dirichlet alocation (LDA) (Blei et al, 2003),
a probabilistic model of documents that is an im-
provement to pLSI (Hofmann, 1999). LDA assumes
that there are K ?topics,? multinomial distributions
over words, which describe a collection. Each docu-
ment exhibits multiple topics, and each word in each
document is associated with one of them.
Although the term ?topic? evokes a collection of
ideas that share a common theme and although the
topics derived by LDA seem to possess semantic
coherence, there is no reason to believe this would
1025
be true of the most likely multinomial distributions
that could have created the corpus given the assumed
generative model. That semantically similar words
are likely to occur together is a byproduct of how
language is actually used.
In LDAWN, we replace the multinomial topic dis-
tributions with a WORDNET-WALK, as described
above. LDAWN assumes a corpus is generated by
the following process (for an overview of the nota-
tion used in this paper, see Table 1).
1. For each topic, k ? {1, . . . ,K}
(a) For each synset s, randomly choose transition prob-
abilities ?k,s ? Dir(S?s).
2. For each document d ? {1, . . . , D}
(a) Select a topic distribution ?d ? Dir(?)
(b) For each word n ? {1, . . . , Nd}
i. Select a topic z ? Mult(1, ?d)
ii. Create a path ?d,n starting with ?0 as the root
node.
iii. From children of ?i:
A. Choose the next node in the walk ?i+1 ?
Mult(1, ?z,?i)
B. If ?i+1 is a leaf node, generate the associ-
ated word. Otherwise, repeat.
Every element of this process, including the
synsets, is hidden except for the words of the doc-
uments. Thus, given a collection of documents, our
goal is to perform posterior inference, which is the
task of determining the conditional distribution of
the hidden variables given the observations. In the
case of LDAWN, the hidden variables are the param-
eters of the K WORDNET-WALKs, the topic assign-
ments of each word in the collection, and the synset
path of each word. In a sense, posterior inference
reverses the process described above.
Specifically, given a document collection w1:D,
the full posterior is
p(?1:K ,z1:D,?1:D,?1:D |w1:D, ?, S?) ?(?K
k=1 p(?k |S?)
?D
d=1 p(?d | ?)
?Nd
n=1 p(?d,n |?1:K)p(wd,n |?d,n)
)
, (1)
where the constant of proportionality is the marginal
likelihood of the observed data.
Note that by encoding the synset paths as a hid-
den variable, we have posed the WSD problem as
a question of posterior probabilistic inference. Fur-
ther note that we have developed an unsupervised
model. No labeled data is needed to disambiguate a
corpus. Learning the posterior distribution amounts
to simultaneously decomposing a corpus into topics
and its words into their synsets.
The intuition behind LDAWN is that the words
in a topic will have similar meanings and thus share
paths within WORDNET. For example, WORDNET
has two senses for the word ?colt;? one referring to a
young male horse and the other to a type of handgun
(see Figure 1).
Although we have no a priori way of know-
ing which of the two paths to favor for a
document, we assume that similar concepts
will also appear in the document. Documents
with unambiguous nouns such as ?six-shooter?
and ?smoothbore? would make paths that pass
through the synset [firearm, piece,
small-arm] more likely than those go-
ing through [animal, animate being,
beast, brute, creature, fauna]. In
practice, we hope to see a WORDNET-WALK that
looks like Figure 2, which points to the right sense
of cancer for a medical context.
LDAWN is a Bayesian framework, as each vari-
able has a prior distribution. In particular, the
Dirichlet prior for ?s, specified by a scaling factor
S and a normalized vector ?s fulfills two functions.
First, as the overall strength of S increases, we place
a greater emphasis on the prior. This is equivalent to
the need for balancing as noted by Abney and Light
(1999).
The other function that the Dirichlet prior serves
is to enable us to encode any information we have
about how we suspect the transitions to children
nodes will be distributed. For instance, we might ex-
pect that the words associated with a synset will be
produced in a way roughly similar to the token prob-
ability in a corpus. For example, even though ?meal?
might refer to both ground cereals or food eaten at
a single sitting and ?repast? exclusively to the lat-
ter, the synset [meal, repast, food eaten
at a single sitting] still prefers to transi-
tion to ?meal? over ?repast? given the overall corpus
counts (see Figure 1, which shows prior transition
probabilities for ?revolver?).
By setting ?s,i, the prior probability of transition-
ing from synset s to node i, proportional to the to-
tal number of observed tokens in the children of i,
1026
we introduce a probabilistic variation on informa-
tion content (Resnik, 1995). As in Resnik?s defini-
tion, this value for non-word nodes is equal to the
sum of all the frequencies of hyponym words. Un-
like Resnik, we do not divide frequency among all
senses of a word; each sense of a word contributes
its full frequency to ?.
3 Posterior Inference with Gibbs Sampling
As described above, the problem of WSD corre-
sponds to posterior inference: determining the prob-
ability distribution of the hidden variables given ob-
served words and then selecting the synsets of the
most likely paths as the correct sense. Directly com-
puting this posterior distribution, however, is not
tractable because of the difficulty of calculating the
normalizing constant in Equation 1.
To approximate the posterior, we use Gibbs sam-
pling, which has proven to be a successful approx-
imate inference technique for LDA (Griffiths and
Steyvers, 2004). In Gibbs sampling, like all Markov
chain Monte Carlo methods, we repeatedly sample
from aMarkov chain whose stationary distribution is
the posterior of interest (Robert and Casella, 2004).
Even though we don?t know the full posterior, the
samples can be used to form an empirical estimate
of the target distribution. In LDAWN, the samples
contain a configuration of the latent semantic states
of the system, revealing the hidden topics and paths
that likely led to the observed data.
Gibbs sampling reproduces the posterior distri-
bution by repeatedly sampling each hidden variable
conditioned on the current state of the other hidden
variables and observations. More precisely, the state
is given by a set of assignments where each word
is assigned to a path through one of K WORDNET-
WALK topics: uth word wu has a topic assignment
zu and a path assignment ?u. We use z?u and ??u
to represent the topic and path assignments of all
words except for u, respectively.
Sampling a new topic for the word wu requires
us to consider all of the paths that wu can take in
each topic and the topics of the other words in the
document u is in. The probability of wu taking on
topic i is proportional to
p(zu = i |z?u)
?
? p(? |??u)1[wu ? ?], (2)
which is the probability of selecting z from ?d times
the probability of a path generating wu from a path
in the ith WORDNET-WALK.
The first term, the topic probability of the uth
word, is based on the assignments to the K topics
for words other than u in this document,
p(zu = i|z?u) =
n(d)?u,i + ?i
?
j n
(d)
?u,j +
?K
j=1 ?j
, (3)
where n(d)?u,j is the number of words other than u in
topic j for the document d that u appears in.
The second term in Equation 2 is a sum over the
probabilities of every path that could have generated
the word wu. In practice, this sum can be com-
puted using a dynamic program for all nodes that
have unique parent (i.e. those that can?t be reached
by more than one path). Although the probability of
a path is specific to the topic, as the transition prob-
abilities for a synset are different across topics, we
will omit the topic index in the equation,
p(?u = ?|??u, ) =
?l?1
i=1 ?
?u
?i,?i+1
. (4)
3.1 Transition Probabilities
Computing the probability of a path requires us to
take a product over our estimate of the probability
from transitioning from i to j for all nodes i and j in
the path ?. The other path assignments within this
topic, however, play an important role in shaping the
transition probabilities.
From the perspective of a single node i, only paths
that pass through that node affect the probability of
u also passing through that node. It?s convenient to
have an explicit count of all of the paths that tran-
sition from i to j in this topic?s WORDNET-WALK,
so we use T?ui,j to represent all of the paths that go
from i to j in a topic other than the path currently
assigned to u.
Given the assignment of all other words to paths,
calculating the probability of transitioning from i to
j with word u requires us to consider the prior ? and
the observations Ti,j in our estimate of the expected
value of the probability of transitioning from i to j,
??ui,j =
T?ui,j + Si?i,j
Si +
?
k T
?u
i,k
. (5)
1027
As mentioned in Section 2.1, we paramaterize the
prior for synset i as a vector ?i, which sums to one,
and a scale parameter S.
The next step, once we?ve selected a topic, is to
select a path within that topic. This requires the
computation of the path probabilities as specified in
Equation 4 for all of the paths wu can take in the
sampled topic and then sampling from the path prob-
abilities.
The Gibbs sampler is essentially a randomized
hill climbing algorithm on the posterior likelihood as
a function of the configuration of hidden variables.
The numerator of Equation 1 is proportional to that
posterior and thus allows us to track the sampler?s
progress. We assess convergence to a local mode of
the posterior by monitoring this quantity.
4 Experiments
In this section, we describe the properties of the
topics induced by running the previously described
Gibbs sampling method on corpora and how these
topics improve WSD accuracy.
Of the two data sets used during the course of
our evaluation, the primary dataset was SEMCOR
(Miller et al, 1993), which is a subset of the Brown
corpus with many nouns manually labeled with the
correct WORDNET sense. The words in this dataset
are lemmatized, and multi-word expressions that are
present in WORDNET are identified. Only the words
in SEMCOR were used in the Gibbs sampling pro-
cedure; the synset assignments were only used for
assessing the accuracy of the final predictions.
We also used the British National Corpus, which
is not lemmatized and which does not have multi-
word expressions. The text was first run through
a lemmatizer, and then sequences of words which
matched a multi-word expression in WORDNET
were joined together into a single word. We took
nouns that appeared in SEMCOR twice or in the
BNC at least 25 times and used the BNC to com-
pute the information-content analog ? for individ-
ual nouns (For example, the probabilities in Figure 1
correspond to ?).
4.1 Topics
Like the topics created by structures such as LDA,
the topics in Table 2 coalesce around reasonable
themes. The word list was compiled by summing
over all of the possible leaves that could have gen-
erated each of the words and sorting the words by
decreasing probability. In the vast majority of cases,
a single synset?s high probability is responsible for
the words? positions on the list.
Reassuringly, many of the top senses for the
present words correspond to the most frequent sense
in SEMCOR. For example, in Topic 4, the senses for
?space? and ?function? correspond to the top senses
in SEMCOR, and while the top sense for ?set? corre-
sponds to ?an abstract collection of numbers or sym-
bols? rather than ?a group of the same kind that be-
long together and are so used,? it makes sense given
the math-based words in the topic. ?Point,? however,
corresponds to the sense used in the phrase ?I got to
the point of boiling the water,? which is neither the
top SEMCOR sense nor a sense which makes sense
given the other words in the topic.
While the topics presented in Table 2 resemble
the topics one would obtain through models like
LDA (Blei et al, 2003), they are not identical. Be-
cause of the lengthy process of Gibbs sampling, we
initially thought that using LDA assignments as an
initial state would converge faster than a random ini-
tial assignment. While this was the case, it con-
verged to a state that less probable than the randomly
initialized state and no better at sense disambigua-
tion (and sometimes worse). The topics presented
in 2 represent words both that co-occur together in
a corpus and co-occur on paths through WORDNET.
Because topics created through LDA only have the
first property, they usually do worse in terms of both
total probability and disambiguation accuracy (see
Figure 3).
Another interesting property of topics in LDAWN
is that, with higher levels of smoothing, words that
don?t appear in a corpus (or appear rarely) but are
in similar parts of WORDNET might have relatively
high probability in a topic. For example, ?maturity?
in topic two in Table 2 is sandwiched between ?foot?
and ?center,? both of which occur about five times
more than ?maturity.? This might improve LDA-
based information retrieval schemes (Wei and Croft,
2006) .
1028
1740
1930
0.23 0.76
3122 0.42
0.01
2236
0.10 0.00
0.00
0.00
0.00
7626
someone
0.00
9609711
0.00
9120316
1743824
0.00
cancer
7998922 genus0.04
0.04
8564599
star_sign
0.06
8565580
0.06
cancer
0.5
9100327
cancer
1
constellation
0.01
0.01
cancer
0.5
crab
0.5
13875408
0.58 0.19
14049094 14046733
tumor
0.97
14050958
0.00
malignancy
0.06
0.94
14051451
0.90
cancer
0.96
Synset ID
Transition Prob
Word
1957888
1.0
Figure 2: The possible paths to reach the word ?cancer? in WORDNET along with transition probabilities
from the medically-themed Topic 2 in Table 2, with the most probable path highlighted. The dashed lines
represent multiple links that have been consolidated, and synsets are represented by their offsets within
WORDNET 2.1. Some words for immediate hypernyms have also been included to give context. In all other
topics, the person, animal, or constellation senses were preferred.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7
president growth material point water plant music
party age object number house change film
city treatment color value road month work
election feed form function area worker life
administration day subject set city report time
official period part square land mercer world
office head self space home requirement group
bill portion picture polynomial farm bank audience
yesterday length artist operator spring farmer play
court level art component bridge production thing
meet foot patient corner pool medium style
police maturity communication direction site petitioner year
service center movement curve interest relationship show
Table 2: The most probable words from six randomly chosen WORDNET-walks from a thirty-two topic
model trained on the words in SEMCOR. These are summed over all of the possible synsets that generate
the words. However, the vast majority of the contributions come from a single synset.
1029
 
0.275 0.28
 
0.285 0.29
 
0.295 0.3
 
0.305
 
0
 
1000
 
2000
 
3000
 
4000
 
5000
 
6000
 
7000
 
8000
 
9000
 
10000
Accuracy
Iteratio
n
Unsee
ded
Seede
d with 
LDA
-
96000
-
94000
-
92000
-
90000
-
88000
-
86000
-
84000
-
82000
-
80000
 
0
 
1000
 
2000
 
3000
 
4000
 
5000
 
6000
 
7000
 
8000
 
9000
 
10000
Model Probability
Iteratio
n
Unsee
ded
Seede
d with 
LDA
Figure 3: Topics seeded with LDA initially have
a higher disambiguation accuracy, but are quickly
matched by unseeded topics. The probability for the
seeded topics starts lower and remains lower.
4.2 Topics and the Weight of the Prior
Because the Dirichlet smoothing factor in part
determines the topics, it also affects the disam-
biguation. Figure 4 shows the modal disambigua-
tion achieved for each of the settings of S =
{0.1, 1, 5, 10, 15, 20}. Each line is one setting of K
and each point on the line is a setting of S. Each
data point is a run for the Gibbs sampler for 10,000
iterations. The disambiguation, taken at the mode,
improved with moderate settings of S, which sug-
gests that the data are still sparse for many of the
walks, although the improvement vanishes if S dom-
inates with much larger values. This makes sense,
as each walk has over 100,000 parameters, there are
fewer than 100,000 words in SEMCOR, and each
 
0.24
 
0.26
 
0.28 0.3
 
0.32
 
0.34
 
0.36
 
0.38
S=20
S=15
S=10
S=5
S=1
S=0.1
Accuracy
Smoot
hing F
actor
64 top
ics
32 top
ics
16 top
ics 8 topic
s
4 topic
s
2 topic
s
1 topic Rando
m
Figure 4: Each line represents experiments with a set
number of topics and variable amounts of smooth-
ing on the SEMCOR corpus. The random baseline
is at the bottom of the graph, and adding topics im-
proves accuracy. As smoothing increases, the prior
(based on token frequency) becomes stronger. Ac-
curacy is the percentage of correctly disambiguated
polysemous words in SEMCOR at the mode.
word only serves as evidence to at most 19 parame-
ters (the length of the longest path in WORDNET).
Generally, a greater number of topics increased
the accuracy of the mode, but after around sixteen
topics, gains became much smaller. The effect of ?
is also related to the number of topics, as a value of S
for a very large number of topics might overwhelm
the observed data, while the same value of S might
be the perfect balance for a smaller number of topics.
For comparison, the method of using a WORDNET-
WALK applied to smaller contexts such as sentences
or documents achieves an accuracy of between 26%
and 30%, depending on the level of smoothing.
5 Error Analysis
This method works well in cases where the delin-
eation can be readily determined from the over-
all topic of the document. Words such as ?kid,?
?may,? ?shear,? ?coach,? ?incident,? ?fence,? ?bee,?
and (previously used as an example) ?colt? were
all perfectly disambiguated by this method. Figure
2 shows the WORDNET-WALK corresponding to a
medical topic that correctly disambiguates ?cancer.?
Problems arose, however, with highly frequent
1030
words, such as ?man? and ?time? that have many
senses and can occur in many types of documents.
For example, ?man? can be associated with many
possible meanings: island, game equipment, ser-
vant, husband, a specific mammal, etc.
Although we know that the ?adult male? sense
should be preferred, the alternative meanings will
also be likely if they can be assigned to a topic
that shares common paths in WORDNET; the doc-
uments contain, however, many other places, jobs,
and animals which are reasonable explanations (to
LDAWN) of how ?man? was generated. Unfortu-
nately, ?man? is such a ubiquitous term that top-
ics, which are derived from the frequency of words
within an entire document, are ultimately uninfor-
mative about its usage.
While mistakes on these highly frequent terms
significantly hurt our accuracy, errors associated
with less frequent terms reveal that WORDNET?s
structure is not easily transformed into a probabilis-
tic graph. For instance, there are two senses of
the word ?quarterback,? a player in American foot-
ball. One is position itself and the other is a per-
son playing that position. While one would expect
co-occurrence in sentences such as ?quarterback is a
easy position, so our quarterback is happy,? the paths
to both terms share only the root node, thus making
it highly unlikely a topic would cover both senses.
Because of WORDNET?s breadth, rare senses
also impact disambiguation. For example, the
metonymical use of ?door? to represent a whole
building as in the phrase ?girl next door? is un-
der the same parent as sixty other synsets contain-
ing ?bridge,? ?balcony,? ?body,? ?arch,? ?floor,? and
?corner.? Surrounded by such common terms that
are also likely to co-occur with the more conven-
tional meanings of door, this very rare sense be-
comes the preferred disambiguation of ?door.?
6 Related Work
Abney and Light?s initial probabilistic WSD ap-
proach (1999) was further developed into a Bayesian
network model by Ciaramita and Johnson (2000),
who likewise used the appearance of monosemous
terms close to ambiguous ones to ?explain away? the
usage of ambiguous terms in selectional restrictions.
We have adapted these approaches and put them into
the context of a topic model.
Recently, other approaches have created ad hoc
connections between synsets in WORDNET and then
considered walks through the newly created graph.
Given the difficulties of using existing connections
in WORDNET, Mihalcea (2005) proposed creating
links between adjacent synsets that might comprise
a sentence, initially setting weights to be equal to
the Lesk overlap between the pairs, and then using
the PageRank algorithm to determine the stationary
distribution over synsets.
6.1 Topics and Domains
Yarowsky was one of the first to contend that ?there
is one sense for discourse? (1992). This has lead
to the approaches like that of Magnini (Magnini et
al., 2001) that attempt to find the category of a text,
select the most appropriate synset, and then assign
the selected sense using domain annotation attached
to WORDNET.
LDAWN is different in that the categories are not
an a priori concept that must be painstakingly anno-
tated within WORDNET and require no augmenta-
tion of WORDNET. This technique could indeed be
used with any hierarchy. Our concepts are the ones
that best partition the space of documents and do the
best job of describing the distinctions of diction that
separate documents from different domains.
6.2 Similarity Measures
Our approach gives a probabilistic method of us-
ing information content (Resnik, 1995) as a start-
ing point that can be adjusted to cluster words in
a given topic together; this is similar to the Jiang-
Conrath similarity measure (1997), which has been
used in many applications in addition to disambigua-
tion. Patwardhan (2003) offers a broad evaluation of
similarity measures for WSD.
Our technique for combining the cues of topics
and distance in WORDNET is adjusted in a way sim-
ilar in spirit to Buitelaar and Sacaleanu (2001), but
we consider the appearance of a single term to be
evidence for not just that sense and its immediate
neighbors in the hyponomy tree but for all of the
sense?s children and ancestors.
Like McCarthy (2004), our unsupervised system
acquires a single predominant sense for a domain
based on a synthesis of information derived from a
1031
textual corpus, topics, and WORDNET-derived sim-
ilarity, a probabilistic information content measure.
By adding syntactic information from a thesaurus
derived from syntactic features (taken from Lin?s au-
tomatically generated thesaurus (1998)), McCarthy
achieved 48% accuracy in a similar evaluation on
SEMCOR; LDAWN is thus substantially less effec-
tive in disambiguation compared to state-of-the-art
methods. This suggests, however, that other meth-
ods might be improved by adding topics and that our
method might be improved by using more informa-
tion than word counts.
7 Conclusion and Future Work
The LDAWN model presented here makes two con-
tributions to research in automatic word sense dis-
ambiguation. First, we demonstrate a method for au-
tomatically partitioning a document into topics that
includes explicit semantic information. Second, we
show that, at least for one simple model of WSD,
embedding a document in probabilistic latent struc-
ture, i.e., a ?topic,? can improve WSD.
There are two avenues of research with LDAWN
that we will explore. First, the statistical nature of
this approach allows LDAWN to be used as a com-
ponent in larger models for other language tasks.
Other probabilistic models of language could in-
sert the ability to query synsets or paths of WORD-
NET. Similarly, any topic based information re-
trieval scheme could employ topics that include se-
mantically relevant (but perhaps unobserved) terms.
Incorporating this model in a larger syntactically-
aware model, which could benefit from the local
context as well as the document level context, is an
important component of future research.
Second, the results presented here show a marked
improvement in accuracy as more topics are added
to the baseline model, although the final result is not
comparable to state-of-the-art techniques. As most
errors were attributable to the hyponomy structure
of WORDNET, incorporating the novel use of topic
modeling presented here with a more mature unsu-
pervised WSD algorithm to replace the underlying
WORDNET-WALK could lead to advances in state-
of-the-art unsupervised WSD accuracy.
References
Steven Abney and Marc Light. 1999. Hiding a semantic
hierarchy in a markov model. In Proceedings of the
Workshop on Unsupervised Learning in Natural Lan-
guage Processing, pages 1?8.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Ranking
and selecting synsets by domain relevance. In Pro-
ceedings of WordNet and Other Lexical Resources:
Applications, Extensions and Customizations. NAACL
2001. Association for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away ambiguity: Learning verb selectional
preference with bayesian networks. In COLING-00,
pages 187?193.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In HLT
?91: Proceedings of the workshop on Speech and Nat-
ural Language, pages 233?237. Association for Com-
putational Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, pages 5228?5235.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. Proceedings of the Twenty-Second Annual
International SIGIR Conference.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2001. Using domain information
for word sense disambiguation. In In Proceedings of
2nd International Workshop on Evaluating Word Sense
Disambiguation Systems, Toulouse, France.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In In 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 280?287.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Human Language Technology and Empirical
Methods in Natural Language Processing Conference,
pages 411?418.
1032
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In 3rd
DARPA Workshop on Human Language Technology,
pages 303?308.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using Measures of Semantic Related-
ness for Word Sense Disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics, pages
241?257.
Matthew Purver, Konrad Ko?rding, Thomas Griffiths, and
Joshua Tenenbaum. 2006. Unsupervised topic mod-
elling for multi-party spoken discourse. In Proceed-
ings of COLING-ACL.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Inter-
national Joint Conferences on Artificial Intelligence,
pages 448?453.
Christian Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer Texts in Statistics.
Springer-Verlag, New York, NY.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In Proceedings of
the Twenty-Ninth Annual International SIGIR Confer-
ence.
1033
Proceedings of NAACL HLT 2007, pages 97?104,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Improving Diversity in Ranking using Absorbing Random Walks
Xiaojin Zhu Andrew B. Goldberg Jurgen Van Gael David Andrzejewski
Department of Computer Sciences
University of Wisconsin, Madison
Madison, WI 53705
{jerryzhu, goldberg, jvangael, andrzeje}@cs.wisc.edu
Abstract
We introduce a novel ranking algorithm
called GRASSHOPPER, which ranks items
with an emphasis on diversity. That is, the
top items should be different from each
other in order to have a broad coverage
of the whole item set. Many natural lan-
guage processing tasks can benefit from
such diversity ranking. Our algorithm is
based on random walks in an absorbing
Markov chain. We turn ranked items into
absorbing states, which effectively pre-
vents redundant items from receiving a
high rank. We demonstrate GRASSHOP-
PER?s effectiveness on extractive text sum-
marization: our algorithm ranks between
the 1st and 2nd systems on DUC 2004
Task 2; and on a social network analy-
sis task that identifies movie stars of the
world.
1 Introduction
Many natural language processing tasks involve
ranking a set of items. Sometimes we want the top
items to be not only good individually but also di-
verse collectively. For example, extractive text sum-
marization generates a summary by selecting a few
good sentences from one or more articles on the
same topic (Goldstein et al, 2000). This can be for-
mulated as ranking all the sentences, and taking the
top ones. A good sentence is one that is represen-
tative, i.e., similar to many other sentences, so that
it likely conveys the central meaning of the articles.
On the other hand, we do not want multiple near-
identical sentences. The top sentences should be di-
verse.
As another example, in information retrieval on
news events, an article is often published by multi-
ple newspapers with only minor changes. It is unde-
sirable to rank all copies of the same article highly,
even though it may be the most relevant. Instead,
the top results should be different and complemen-
tary. In other words, one wants ?subtopic diversity?
in retrieval results (Zhai et al, 2003).
The need for diversity in ranking is not unique to
natural language processing. In social network anal-
ysis, people are connected by their interactions, e.g.,
phone calls. Active groups of people have strong in-
teractions among them, but many groups may exist
with fewer interactions. If we want a list of people
that represent various groups, it is important to con-
sider both activity and diversity, and not to fill the
list with people from the same active groups.
Given the importance of diversity in ranking,
there has been significant research in this area. Per-
haps the most well-known method is maximum
marginal relevance (MMR) (Carbonell and Gold-
stein, 1998), as well as cross-sentence informational
subsumption (Radev, 2000), mixture models (Zhang
et al, 2002), subtopic diversity (Zhai et al, 2003),
diversity penalty (Zhang et al, 2005), and others.
The basic idea is to penalize redundancy by lowering
an item?s rank if it is similar to items already ranked.
However, these methods often treat centrality rank-
ing and diversity ranking separately, sometimes with
heuristic procedures.
97
We propose GRASSHOPPER (Graph Random-walk
with Absorbing StateS that HOPs among PEaks
for Ranking), a novel ranking algorithm that en-
courages diversity. GRASSHOPPER is an alternative
to MMR and variants, with a principled mathemat-
ical model and strong empirical performance. It
ranks a set of items such that: 1. A highly ranked
item is representative of a local group in the set,
i.e., it is similar to many other items (centrality);
2. The top items cover as many distinct groups as
possible (diversity); 3. It incorporates an arbitrary
pre-specified ranking as prior knowledge (prior).
Importantly GRASSHOPPER achieves these in a uni-
fied framework of absorbing Markov chain random
walks. The key idea is the following: We define
a random walk on a graph over the items. Items
which have been ranked so far become absorbing
states. These absorbing states ?drag down? the im-
portance of similar unranked states, thus encourag-
ing diversity. Our model naturally balances central-
ity, diversity, and prior. We discuss the algorithm
in Section 2. We present GRASSHOPPER?s empiri-
cal results on text summarization and social network
analysis in Section 3.
2 The GRASSHOPPER Algorithm
2.1 The Input
GRASSHOPPER requires three inputs: a graph W , a
probability distribution r that encodes the prior rank-
ing, and a weight ? ? [0, 1] that balances the two.
The user needs to supply a graph with n nodes,
one for each item. The graph is represented by an
n? n weight matrix W , where wij is the weight on
the edge from i to j. It can be either directed or undi-
rected. W is symmetric for undirected graphs. The
weights are non-negative. The graph does not need
to be fully connected: if there is no edge from item
i to j, then wij = 0. Self-edges are allowed. For ex-
ample, in text summarization one can create an undi-
rected, fully connected graph on the sentences. The
edge between sentences i, j has weight wij , their co-
sine similarity. In social network analysis one can
create a directed graph with wij being the number
of phone calls i made to j. The graph should be
constructed carefully to reflect domain knowledge.
For examples, see (Erkan and Radev, 2004; Mihal-
cea and Tarau, 2004; Pang and Lee, 2004).
The user can optionally supply an arbitrary rank-
ing on the items as prior knowledge. In this
case GRASSHOPPER can be viewed as a re-ranking
method. For example, in information retrieval,
the prior ranking can be the ranking by relevance
scores. In text summarization, it can be the po-
sition of sentences in the original article. (There
is evidence that the first few sentences in an ar-
ticle are likely good summaries.) Somewhat un-
conventionally, the prior ranking is represented as
a probability distribution r = (r1, ? ? ? , rn)? such
that ri ? 0,
?n
i=1 ri = 1. The highest-ranked item
has the largest probability, the next item has smaller
probability, and so on. A distribution gives the user
more control. For example ra = (0.1, 0.7, 0.2)?
and rb = (0.3, 0.37, 0.33)? both represent the same
ranking of items 2, 3, 1, but with different strengths.
When there is no prior ranking, one can let r =
(1/n, ? ? ? , 1/n)?, the uniform distribution.
2.2 Finding the First Item
We find the first item in GRASSHOPPER ranking by
teleporting random walks. Imagine a random walker
on the graph. At each step, the walker may do one of
two things: with probability ?, she moves to a neigh-
bor state1 according to the edge weights; otherwise
she is teleported to a random state according to the
distribution r. Under mild conditions (which are sat-
isfied in our setting, see below), the stationary distri-
bution of the random walk defines the visiting prob-
abilities of the nodes. The states with large probabil-
ities can be regarded as central items, an idea used
in Google PageRank (Page et al, 1998) and other in-
formation retrieval systems (Kurland and Lee, 2005;
Zhang et al, 2005), text summarization (Erkan and
Radev, 2004), keyword extraction (Mihalcea and Ta-
rau, 2004) and so on. Depending on ?, items high on
the user-supplied prior ranking r may also have large
stationary probabilities, which is a way to incorpo-
rate the prior ranking.
As an example, we created a toy data set with 300
points in Figure 1(a). There are roughly three groups
with different densities. We created a fully con-
nected graph on the data, with larger edge weights
if points are closer2. Figure 1(b) shows the station-
ary distribution of the random walk on the graph.
1We use state, node and item interchangeably.
2We use wij = exp(??xi ? xj?2/0.16), ? = 1.
98
0 5 100
2
4
6
8
0 5
10
05
100
0.005
0.01
0.015
g1
0 5
10
05
100
2
4
6
g2
0 5
10
05
100
0.5
1
1.5
g3
(a) (b) (c) (d)
Figure 1: (a) A toy data set. (b) The stationary distribution pi reflects centrality. The item with the largest
probability is selected as the first item g1. (c) The expected number of visits v to each node after g1 becomes
an absorbing state. (d) After both g1 and g2 become absorbing states. Note the diversity in g1, g2, g3 as they
come from different groups.
Items at group centers have higher probabilities, and
tighter groups have overall higher probabilities.
However, the stationary distribution does not ad-
dress diversity at all. If we were to rank the items
by their stationary distribution, the top list would be
dominated by items from the center group in Fig-
ure 1(b). Therefore we only use the stationary dis-
tribution to find the first item, and use a method
described in the next section to rank the remaining
items.
Formally we first define an n ? n raw transition
matrix P? by normalizing the rows of W : P?ij =
wij/
?n
k=1 wik, so that P?ij is the probability that the
walker moves to j from i. We then make the walk
a teleporting random walk P by interpolating each
row with the user-supplied initial distribution r:
P = ?P? + (1 ? ?)1r?, (1)
where 1 is an all-1 vector, and 1r? is the outer prod-
uct. If ? < 1 and r does not have zero elements,
our teleporting random walk P is irreducible (possi-
ble to go to any state from any state by teleporting),
aperiodic (the walk can return to a state after any
number of steps), all states are positive recurrent (the
expected return time to any state is finite) and thus
ergodic (Grimmett and Stirzaker, 2001). Therefore
P has a unique stationary distribution pi = P?pi.
We take the state with the largest stationary proba-
bility to be the first item g1 in GRASSHOPPER rank-
ing: g1 = argmaxni=1 pii.
2.3 Ranking the Remaining Items
As mentioned early, the key idea of GRASSHOPPER
is to turn ranked items into absorbing states. We
first turn g1 into an absorbing state. Once the ran-
dom walk reaches an absorbing state, the walk is ab-
sorbed and stays there. It is no longer informative to
compute the stationary distribution of an absorbing
Markov chain, because the walk will eventually be
absorbed. Nonetheless, it is useful to compute the
expected number of visits to each node before ab-
sorption. Intuitively, those nodes strongly connected
to g1 will have many fewer visits by the random
walk, because the walk tends to be absorbed soon
after visiting them. In contrast, groups of nodes far
away from g1 still allow the random walk to linger
among them, and thus have more visits. In Fig-
ure 1(c), once g1 becomes an absorbing node (rep-
resented by a circle ?on the floor?), the center group
is no longer the most prominent: nodes in this group
have fewer visits than the left group. Note now the
y-axis is the number of visits instead of probability.
GRASSHOPPER selects the second item g2 with the
largest expected number of visits in this absorbing
Markov chain. This naturally inhibits items similar
to g1 and encourages diversity. In Figure 1(c), the
item near the center of the left group is selected as
g2. Once g2 is selected, it is converted into an ab-
sorbing state, too. This is shown in Figure 1(d). The
right group now becomes the most prominent, since
both the left and center groups contain an absorbing
state. The next item g3 in ranking will come from the
right group. Also note the range of y-axis is smaller:
99
with more absorbing states, the random walk will be
absorbed sooner. The procedure is repeated until all
items are ranked. The name GRASSHOPPER reflects
the ?hopping? behavior on the peaks.
It is therefore important to compute the expected
number of visits in an absorbing Markov chain. Let
G be the set of items ranked so far. We turn the states
g ? G into absorbing states by setting Pgg = 1 and
Pgi = 0,?i 6= g. If we arrange items so that ranked
ones are listed before unranked ones, we can write
P as
P =
[
IG 0
R Q
]
. (2)
Here IG is the identity matrix on G. Submatrices R
and Q correspond to rows of unranked items, those
from (1). It is known that the fundamental matrix
N = (I ?Q)?1 (3)
gives the expected number of visits in the absorbing
random walk (Doyle and Snell, 1984). In particular
Nij is the expected number of visits to state j be-
fore absorption, if the random walk started at state i.
We then average over all starting states to obtain vj ,
the expected number of visits to state j. In matrix
notation,
v = N
?1
n? |G| , (4)
where |G| is the size of G. We select the state with
the largest expected number of visits as the next item
g|G|+1 in GRASSHOPPER ranking:
g|G|+1 = argmaxni=|G|+1 vi. (5)
The complete GRASSHOPPER algorithm is summa-
rized in Figure 2.
2.4 Some Discussions
To see how ? controls the tradeoff, note when ? = 1
we ignore the user-supplied prior ranking r, while
when ? = 0 one can show that GRASSHOPPER re-
turns the ranking specified by r.
Our data in Figure 1(a) has a cluster struc-
ture. Many methods have exploited such structure,
e.g., (Hearst and Pedersen, 1996; Leuski, 2001; Liu
and Croft, 2004). In fact, a heuristic algorithm is
to first cluster the items, then pick the central items
from each cluster in turn. But it can be difficult to
Input: W , r, ?
1. Create the initial Markov chain P from
W, r, ? (1).
2. Compute P ?s stationary distribution pi. Pick the
first item g1 = argmaxi pii.
3. Repeat until all items are ranked:
(a) Turn ranked items into absorbing
states (2).
(b) Compute the expected number of visits v
for all remaining items (4). Pick the next
item g|G|+1 = argmaxi vi
Figure 2: The GRASSHOPPER algorithm
determine the appropriate number and control the
shape of clusters. In contrast, GRASSHOPPER does
not involve clustering. However it is still able to
automatically take advantage of cluster structures in
the data.
In each iteration we need to compute the fun-
damental matrix (3). This involves inverting an
(n ? |G|) ? (n ? |G|) matrix, which is expensive.
However the Q matrix is reduced by one row and
one column in every iteration, but is otherwise un-
changed. This allows us to apply the matrix in-
version lemma (Sherman-Morrison-Woodbury for-
mula) (Press et al, 1992). Then we only need to
invert the matrix once in the first iteration, but not in
subsequent iterations. Space precludes a full discus-
sion, but we point out that it presents a significant
speed up. A Matlab implementation can be found
at http://www.cs.wisc.edu/?jerryzhu/
pub/grasshopper.m.
3 Experiments
3.1 Text Summarization
Multi-document extractive text summarization is a
prime application for GRASSHOPPER. In this task, we
must select and rank sentences originating from a
set of documents about a particular topic or event.
The goal is to produce a summary that includes all
the relevant facts, yet avoids repetition that may
result from using similar sentences from multiple
documents. In this section, we demonstrate that
100
GRASSHOPPER?s balance of centrality and diversity
makes it successful at this task. We present em-
pirical evidence that GRASSHOPPER achieves results
competitive with the top text summarizers in the
2004 Document Understanding Conference (http:
//duc.nist.gov). DUC is a yearly text summa-
rization community evaluation, with several tasks in
recent years concentrating on multi-document sum-
marization (described in more detail below).
Many successful text summarization systems
achieve a balance between sentence centrality and
diversity in a two-step process. Here we review the
LexRank system (Erkan and Radev, 2004), which
is most similar to our current approach. LexRank
works by placing sentences in a graph, with edges
based on the lexical similarity between the sentences
(as determined by a cosine measure). Each sen-
tence is then assigned a centrality score by finding
its probability under the stationary distribution of
a random walk on this graph. Unlike the similar
PageRank algorithm (Page et al, 1998), LexRank
uses an undirected graph of sentences rather than
Web pages, and the edge weights are either cosine
values or 0/1 with thresholding. The LexRank cen-
trality can be combined with other centrality mea-
sures, as well as sentence position information. Af-
ter this first step of computing centrality, a sec-
ond step performs re-ranking to avoid redundancy
in the highly ranked sentences. LexRank uses cross-
sentence informational subsumption (Radev, 2000)
to this end, but MMR (Carbonell and Goldstein,
1998) has also been widely used in the text sum-
marization community. These methods essentially
disqualify sentences that are too lexically similar to
sentences ranked higher by centrality. In short, sim-
ilar graph-based approaches to text summarization
rely on two distinct processes to measure each sen-
tence?s importance and ensure some degree of diver-
sity. GRASSHOPPER, on the other hand, achieves the
same goal in a unified procedure.
We apply GRASSHOPPER to text summarization in
the following manner. Our graph contains nodes
for all the sentences in a document set. We
used the Clair Library (http://tangra.si.
umich.edu/clair/clairlib) to split docu-
ments into sentences, apply stemming, and create
a cosine matrix for the stemmed sentences. Cosine
values are computed using TF-IDF vectors. As in
LexRank, edges in the graph correspond to text sim-
ilarity. To create a sparse graph, we use the cosine
threshold value of 0.1 obtained in (Erkan and Radev,
2004). Specifically, the edge weight between sen-
tence vectors si and sj is defined as
wij =
{
1 if s
?
i sj
?si???sj? > 0.1
0 otherwise
. (6)
The second input for GRASSHOPPER is an initial
ranking distribution, which we derive from the po-
sition of each sentence in its originating document.
Position forms the basis for lead-based summaries
(i.e., using the first N sentences as the summary)
and leads to very competitive summaries (Brandow
et al, 1995). We form an initial ranking for each
sentence by computing p??, where p is the position
of the sentence in its document, and ? is a posi-
tive parameter trained on a development dataset. We
then normalize over all sentences in all documents
to form a valid distribution r ? p?? that gives high
probability to sentences closer to the beginning of
documents. With a larger ?, the probability assigned
to later sentences decays more rapidly.
To evaluate GRASSHOPPER, we experimented with
DUC datasets. We train our parameters (? and ?)
using the DUC 2003 Task 2 data. This dataset con-
tains 30 document sets, each with an average of 10
documents about a news event. We test GRASSHOP-
PER?s performance on the DUC 2004 Task 2, Tasks
4a and 4b data. DUC 2004 Task 2 has 50 document
sets of 10 documents each. Tasks 4a and 4b explored
cross-lingual summarization. These datasets consist
of Arabic-to-English translations of news stories.
The documents in Task 4a are machine-translated,
while Task 4b?s are manually-translated. Note that
we handle the translated documents in exactly the
same manner as the English documents.
We evaluate our results using the standard text
summarization metric ROUGE (http://www.
isi.edu/?cyl/ROUGE/). This is a recall-based
measure of text co-occurrence between a machine-
generated summary and model summaries manually
created by judges. ROUGE metrics exist based on
bigram, trigram, and 4-gram overlap, but ROUGE-1
(based on unigram matching) has been found to cor-
relate best with human judgments (Lin and Hovy,
2003).
101
Using the DUC 2003 training data, we tuned ?
and ? on a small grid (? ? {0.125, 0.25, 0.5, 1.0};
? ? {0.0, 0.0625, 0.125, 0.25, 0.5, 0.95}). Specifi-
cally, for each of the 30 DUC 2003 Task 2 document
sets, we computed ROUGE-1 scores comparing our
generated summary to 4 model summaries. We av-
eraged the resulting ROUGE-1 scores across all 30
sets to produce a single average ROUGE-1 score to
assess a particular parameter configuration. After
examining the results for all 24 configurations, we
selected the best one: ? = 0.25 and ? = 0.5.
Table 1 presents our results using these parame-
ter values to generate summaries for the three DUC
2004 datasets. Note that the averages listed are ac-
tually averages over 4 model summaries per set, and
over all the sets. Following the standard DUC pro-
tocol, we list the confidence intervals calculated by
ROUGE using a bootstrapping technique. The fi-
nal column compares our results to the official sys-
tems that participated in the DUC 2004 evaluation.
GRASSHOPPER is highly competitive in these text
summarization tasks: in particular it ranks between
the 1st and 2nd automatic systems on 2004 Task 2.
The lower performance in Task 4a is potentially due
to the documents being machine-translated. If they
contain poorly translated sentences, graph edges
based on cosine similarity could be less meaning-
ful. For such a task, more advanced text processing
is probably required.
3.2 Social Network Analysis
As another application of GRASSHOPPER, we iden-
tify the nodes in a social network that are the most
prominent, and at the same time maximally cover
the network. A node?s prominence comes from its
intrinsic stature, as well as the prominence of the
nodes it touches. However, to ensure that the top-
ranked nodes are representative of the larger graph
structure, it is important to make sure the results are
not dominated by a small group of highly prominent
nodes who are closely linked to one another. This re-
quirement makes GRASSHOPPER a useful algorithm
for this task.
We created a dataset from the Internet Movie
Database (IMDb) that consists of all comedy movies
produced between 2000 and 2006, and have received
more than 500 votes by IMDb users. This results in
1027 movies. We form a social network of actors by
co-star relationship. Not surprisingly, actors from
the United States dominate our dataset, although a
total of 30 distinct countries are represented. We
seek an actor ranking such that the top actors are
prominent. However, we also want the top actors to
be diverse, so they represent comedians from around
the world.
This problem is framed as a GRASSHOPPER rank-
ing problem. For each movie, we considered only
the main stars, i.e., the first five cast members, who
tend to be the most important. The resulting list con-
tains 3452 unique actors. We formed a social net-
work where the nodes are the actors, and undirected
weighted edges connect actors who have appeared in
a movie together. The edge weights are equal to the
number of movies from our dataset in which both
actors were main stars. Actors are also given a self-
edge with weight 1. The co-star graph is given to
GRASSHOPPER as an input. For the prior actor rank-
ing, we simply let r be proportional to the number
of movies in our dataset in which an actor has ap-
peared. We set the weight ? = 0.95. It is important
to note that no country information is ever given to
GRASSHOPPER.
We use two measurements, ?country coverage?
and ?movie coverage?, to study the diversity and
prominence of the ranking produced by GRASSHOP-
PER. We compare GRASSHOPPER to two baselines:
ranking based solely on the number of movies an ac-
tor has appeared in, MOVIECOUNT, and a randomly
generated ranking, RANDOM.
First, we calculate ?country coverage? as the num-
ber of different countries represented by the top k ac-
tors, for all k values. Each actor represents a single
country?the country that the actor has appeared in
the most. We hypothesize that actors are more likely
to have co-star connections to actors within the same
country, so our social network may have, to some
extent, a clustering structure by country. ?Country
coverage? approximates the number of clusters rep-
resented at different ranks.
Figure 3(a) shows that country coverage grows
much more rapidly for GRASSHOPPER than for
MOVIECOUNT. That is, we see more comedians from
around the world ranked highly by GRASSHOPPER.
In contrast, the top ranks of MOVIECOUNT are dom-
inated by US actors, due to the relative abundance
of US movies on IMDb. Many other countries are
102
Number of Average GRASSHOPPER
Dataset Doc. Sets ROUGE-1 95% C.I. Unofficial Rank
DUC 2004 Task 2 50 0.3755 [0.3622, 0.3888] Between 1 & 2 of 34
DUC 2004 Task 4a 24 0.3785 [0.3613, 0.3958] Between 5 & 6 of 11
DUC 2004 Task 4b 24 0.4067 [0.3883, 0.4251] Between 2 & 3 of 11
Table 1: Text summarization results on DUC 2004 datasets. GRASSHOPPER was configured using parameters
tuned on the DUC 2003 Task 2 dataset. The rightmost column lists what our rank would have been if we
had participated in the DUC 2004 evaluation.
not represented until further down in the ranked
list. This demonstrates that GRASSHOPPER ranking is
successful in returning a more diverse ranking. Be-
cause of the absorbing states in GRASSHOPPER, the
first few highly ranked US actors encourage the se-
lection of actors from other regions of the co-star
graph, which roughly correspond to different coun-
tries. RANDOM achieves even higher country cover-
age initially, but is quickly surpassed by GRASSHOP-
PER. The initial high coverage comes from the ran-
dom selection of actors. However these randomly
selected actors are often not prominent, as we show
next.
Second, we calculate ?movie coverage? as the to-
tal number of unique movies the top k actors are
in. We expect that actors who have been in more
movies are more prominent. This is reasonable be-
cause we count an actor in a movie only if the actor
is among the top five actors from that movie. Our
counts thus exclude actors who had only small roles
in numerous movies. Therefore high movie cov-
erage roughly corresponds to ranking more promi-
nent actors highly. It is worth noting that this mea-
sure also partially accounts for diversity, since an
actor whose movies completely overlap with those
of higher-ranked actors contributes nothing to movie
coverage (i.e., his/her movies are already covered by
higher-ranked actors).
Figure 3(b) shows that the movie cover-
age of GRASSHOPPER grows more rapidly than
MOVIECOUNT, and much more rapidly than RAN-
DOM. The results show that, while the RANDOM
ranking is diverse, it is not of high quality be-
cause it fails to include many prominent actors in
its high ranks. This is to be expected of a ran-
dom ranking. Since the vast majority of the ac-
tors appear in only one movie, the movie cover-
age curve is roughly linear in the number of ac-
tors. By ranking more prominent actors highly, the
GRASSHOPPER and MOVIECOUNT movie coverage
curves grow faster. Many of the US actors highly
ranked by MOVIECOUNT are co-stars of one an-
other, so GRASSHOPPER outperforms MOVIECOUNT
in terms of movie coverage too.
We inspect the GRASSHOPPER ranking, and find
the top 5 actors to be Ben Stiller, Anthony Anderson,
Johnny Knoxville, Eddie Murphy and Adam San-
dler. GRASSHOPPER also brings many countries, and
major stars from those countries, into the high ranks.
Examples include Mads Mikkelsen (?synonym to
the great success the Danish film industry has had?),
Cem Yilmaz (?famous Turkish comedy actor, cari-
caturist and scenarist?), Jun Ji-Hyun (?face of South
Korean cinema?), Tadanobu Asano (?Japan?s an-
swer to Johnny Depp?), Aamir Khan (?prominent
Bollywood film actor?), and so on3. These actors
are ranked significantly lower by MOVIECOUNT.
These results indicate that GRASSHOPPER
achieves both prominence and diversity in ranking
actors in the IMDb co-star graph.
4 Conclusions
GRASSHOPPER ranking provides a unified approach
for achieving both diversity and centrality. We have
shown its effectiveness in text summarization and
social network analysis. As future work, one direc-
tion is ?partial absorption,? where at each absorbing
state the random walk has an escape probability to
continue the random walk instead of being absorbed.
Tuning the escape probability creates a continuum
between PageRank (if the walk always escapes) and
GRASSHOPPER (if always absorbed). In addition, we
will explore the issue of parameter learning, and
3Quotes from IMDb and Wikipedia.
103
0 100 200 300 400 500
0
5
10
15
20
25
30
k (number of actors)
N
um
be
r o
f c
ou
nt
rie
s 
co
ve
re
d
 
 
GRASSHOPPER
MOVIECOUNT
RANDOM
0 100 200 300 400 500
0
100
200
300
400
500
600
700
800
900
1000
k (number of actors)
N
um
be
r o
f m
ov
ie
s 
co
ve
re
d
 
 
GRASSHOPPER
MOVIECOUNT
RANDOM
(a) Country coverage (b) Movie coverage
Figure 3: (a) Country coverage at ranks up to 500, showing that GRASSHOPPER and RANDOM rankings are
more diverse than MOVIECOUNT. (b) Movie coverage at ranks up to 500, showing that GRASSHOPPER and
MOVIECOUNT have more prominent actors than RANDOM. Overall, GRASSHOPPER is the best.
user feedback (e.g., ?This item should be ranked
higher.?). We also plan to apply GRASSHOPPER to a
variety of tasks, including information retrieval (for
example ranking news articles on the same event as
in Google News, where many newspapers might use
the same report and thus result in a lack of diversity),
image collection summarization, and social network
analysis for national security and business intelli-
gence.
Acknowledgment We thank Mark Craven and the anony-
mous reviewers for helpful comments. This work is supported
in part by Wisconsin Alumni Research Foundation (WARF) and
NLM training grant 5T15LM07359.
References
R. Brandow, K. Mitze, and Lisa F. Rau. 1995. Automatic con-
densation of electronic publications by sentence selection.
Inf. Process. Manage., 31(5):675?685.
Jaime Carbonell and Jade Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents and pro-
ducing summaries. In SIGIR?98.
P.G. Doyle and J.L. Snell. 1984. Random Walks and Electric
Networks. Mathematical Assoc. of America.
Gu?nes? Erkan and Dragomir R. Radev. 2004. LexRank: Graph-
based centrality as salience in text summarization. Journal
of Artificial Intelligence Research.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by sen-
tence extraction. In NAACL-ANLP 2000 Workshop on Auto-
matic summarization, pages 40?48.
Geoffrey R. Grimmett and David R. Stirzaker. 2001. Proba-
bility and Random Processes. Oxford Science Publications,
third edition.
Marti A. Hearst and Jan O. Pedersen. 1996. Reexamining
the cluster hypothesis: Scatter/gather on retrieval results. In
SIGIR-96.
Oren Kurland and Lillian Lee. 2005. PageRank without hyper-
links: Structural re-ranking using links induced by language
models. In SIGIR?05.
Anton Leuski. 2001. Evaluating document clustering for inter-
active information retrieval. In CIKM?01.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence statistics. In
NAACL?03, pages 71?78.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-based re-
trieval using language models. In SIGIR?04.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing
order into texts. In EMNLP?04.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Wino-
grad. 1998. The PageRank citation ranking: Bringing order
to the web. Technical report, Stanford Digital Library Tech-
nologies Project.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In ACL, pages 271?278.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery.
1992. Numerical recipes in C: the art of scientific comput-
ing. Cambridge University Press New York, NY, USA.
Dragomir Radev. 2000. A common theory of information fu-
sion from multiple text sources, step one: Cross-document
structure. In Proceedings of the 1st ACL SIGDIAL Workshop
on Discourse and Dialogue.
ChengXiang Zhai, William W. Cohen, and John Lafferty. 2003.
Beyond independent relevance: Methods and evaluation
metrics for subtopic retrieval. In SIGIR?03.
Yi Zhang, Jamie Callan, and Thomas Minka. 2002. Novelty
and redundancy detection in adaptive filtering. In SIGIR?02.
Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Weiguo Fan,
Zheng Chen, and Wei-Ying Ma. 2005. Improving web
search results using affinity graph. In SIGIR?05.
104
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 263?271,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
May All Your Wishes Come True:
A Study of Wishes and How to Recognize Them
Andrew B. Goldberg, Nathanael Fillmore, David Andrzejewski
Zhiting Xu, Bryan Gibson, Xiaojin Zhu
Computer Sciences Department, University of Wisconsin-Madison, Madison, WI 53706, USA
{goldberg, nathanae, andrzeje, zhiting, bgibson, jerryzhu}@cs.wisc.edu
Abstract
A wish is ?a desire or hope for something
to happen.? In December 2007, people from
around the world offered up their wishes to
be printed on confetti and dropped from the
sky during the famous New Year?s Eve ?ball
drop? in New York City?s Times Square. We
present an in-depth analysis of this collection
of wishes. We then leverage this unique re-
source to conduct the first study on building
general ?wish detectors? for natural language
text. Wish detection complements traditional
sentiment analysis and is valuable for collect-
ing business intelligence and insights into the
world?s wants and desires. We demonstrate
the wish detectors? effectiveness on domains
as diverse as consumer product reviews and
online political discussions.
1 Introduction
Each year, New York City rings in the New Year
with the famous ?ball drop? in Times Square. In
December 2007, the Times Square Alliance, co-
producer of the Times Square New Year?s Eve Cele-
bration, launched a Web site called the Virtual Wish-
ing Wall1 that allowed people around the world to
submit their New Year?s wishes. These wishes were
then printed on confetti and dropped from the sky
at midnight on December 31, 2007 in sync with the
ball drop.
We obtained access to this set of nearly 100,000
New Year?s wishes, which we call the ?WISH cor-
pus.? Table 1 shows a selected sample of the WISH
1http://www.timessquarenyc.org/nye/nye interactive.html
corpus. Some are far-reaching fantasies and aspi-
rations, while others deal with everyday concerns
like economic and medical distress. We analyze this
first-of-its-kind corpus in Section 2.
The New Oxford American Dictionary defines
?wish? as ?a desire or hope for something to hap-
pen.? How wishes are expressed, and how such
wishful expressions can be automatically recog-
nized, are open questions in natural language pro-
cessing. Leveraging the WISH corpus, we conduct
the first study on building general ?wish detectors?
for natural language text, and demonstrate their ef-
fectiveness on domains as diverse as consumer prod-
uct reviews and online political discussions. Such
wish detectors have tremendous value in collecting
business intelligence and public opinions. We dis-
cuss the wish detectors in Section 3, and experimen-
tal results in Section 4.
1.1 Relation to Prior Work
Studying wishes is valuable in at least two aspects:
1. Being a special genre of subjective expression,
wishes add a novel dimension to sentiment analy-
sis. Sentiment analysis is often used as an auto-
matic market research tool to collect valuable busi-
ness intelligence from online text (Pang and Lee,
2008; Shanahan et al, 2005; Koppel and Shtrim-
berg, 2004; Mullen and Malouf, 2008). Wishes
differ from the recent focus of sentiment analysis,
namely opinion mining, by revealing what people
explicitly want to happen, not just what they like or
dislike (Ding et al, 2008; Hu and Liu, 2004). For ex-
ample, wishes in product reviews could contain new
feature requests. Consider the following (real) prod-
263
514 peace on earth
351 peace
331 world peace
244 happy new year
112 love
76 health and happiness
75 to be happy
51 i wish for world peace
21 i wish for health and happiness for my family
21 let there be peace on earth
16 i wish u to call me if you read this 555-1234
16 to find my true love
8 i wish for a puppy
7 for the war in iraq to end
6 peace on earth please
5 a free democratic venezuela
5 may the best of 2007 be the worst of 2008
5 to be financially stable
1 a little goodness for everyone would be nice
1 i hope i get accepted into a college that i like
1 i wish to get more sex in 2008
1 please let name be healthy and live all year
1 to be emotionally stable and happy
1 to take over the world
Table 1: Example wishes and their frequencies in the
WISH corpus.
uct review excerpt: ?Great camera. Indoor shots
with a flash are not quite as good as 35mm. I wish
the camera had a higher optical zoom so that I could
take even better wildlife photos.? The first sentence
contains positive opinion, the second negative opin-
ion. However, wishful statements like the third sen-
tence are often annotated as non-opinion-bearing in
sentiment analysis corpora (Hu and Liu, 2004; Ding
et al, 2008), even though they clearly contain im-
portant information. An automatic ?wish detector?
text-processing tool can be useful for product manu-
facturers, advertisers, politicians, and others looking
to discover what people want.
2. Wishes can tell us a lot about people: their in-
nermost feelings, perceptions of what they?re lack-
ing, and what they desire (Speer, 1939). Many
psychology researchers have attempted to quantify
the contents of wishes and how they vary with
factors such as location, gender, age, and per-
sonality type (Speer, 1939; Milgram and Riedel,
1969; Ehrlichman and Eichenstein, 1992; King and
Broyles, 1997). These studies have been small scale
with only dozens or hundreds of participants. The
WISH corpus provides the first large-scale collec-
tion of wishes as a window into the world?s desires.
Beyond sentiment analysis, classifying sentences
as wishes is an instance of non-topical classifica-
tion. Tasks under this heading include compu-
tational humor (Mihalcea and Strapparava, 2005),
genre classification (Boese and Howe, 2005), au-
thorship attribution (Argamon and Shimoni, 2003),
and metaphor detection (Krishnakumaran and Zhu,
2007), among others (Mishne et al, 2007; Mihal-
cea and Liu, 2006). We share the common goal of
classifying text into a unique set of target categories
(in our case, wishful and non-wishful), but use dif-
ferent techniques catered to our specific task. Our
feature-generation technique for wish detection re-
sembles template-based methods for information ex-
traction (Brin, 1999; Agichtein and Gravano, 2000).
2 Analyzing the WISH Corpus
We analyze the WISH corpus with a variety of sta-
tistical methods. Our analyses not only reveal what
people wished for on New Year?s Eve, but also pro-
vide insight for the development of wish detectors in
Section 3.
The complete WISH corpus contains nearly
100,000 wishes collected over a period of 10 days
in December 2007, most written in English, with the
remainder in Portuguese, Spanish, Chinese, French,
and other languages. For this paper, we consider
only the 89,574 English wishes. Most of these En-
glish wishes contain optional geographic meta data
provided by the wisher, indicating a variety of coun-
tries (not limited to English-speaking) around the
world. We perform minimal preprocessing, includ-
ing TreeBank-style tokenization, downcasing, and
punctuation removal. Each wish is treated as a sin-
gle entity, regardless of whether it contains multiple
sentences. After preprocessing, the average length
of a wish is 8 tokens.
2.1 The Topic and Scope of Wishes
As a first step in understanding the content of the
wishes, we asked five annotators to manually an-
notate a random subsample of 5,000 wishes. Sec-
tions 2.1 and 2.2 report results on this subsample.
The wishes were annotated in terms of two at-
264
(a) Topic of Wishes
(b) Scope of Wishes
Figure 1: Topic and scope distributions based on manual
annotations of a random sample of 5,000 wishes in the
WISH corpus.
tributes: topic and scope. We used 11 pre-defined
topic categories, and their distribution in this sub-
sample of the WISH corpus is shown in Figure 1(a).
The most frequent topic is love, while health,
happiness, and peace are also common themes.
Many wishes also fell into an other category, in-
cluding specific individual requests (?i wish for a
new puppy?), solicitations or advertisements (?call
me 555-1234?, ?visit website.com?), or sinister
thoughts (?to take over the world?).
The 5,000 wishes were also manually assigned
a scope. The scope of a wish refers to the range
of people that are targeted by the wish. We used
6 pre-defined scope categories: self (?I want to be
happy?), family (?For a cure for my husband?), spe-
cific person by name (?Prayers for name?), country
(?Bring our troops home!?), world (?Peace to every-
one in the world?), and other. In cases where mul-
tiple scope labels applied, the broadest scope was
selected. Figure 1(b) shows the scope distribution.
It is bimodal: over one third of the wishes are nar-
rowly directed at one?s self, while broad wishes at
the world level are also frequent. The in-between
scopes are less frequent.
2.2 Wishes Differ by Geographic Location
As mentioned earlier, wishers had the option to enter
a city/country when submitting wishes. Of the man-
ually annotated wishes, about 4,000 included valid
location information, covering all 50 states in the
U.S., and all continents except Antarctica.
We noticed a statistically significant difference
between wishes submitted from the United States
(about 3600) versus non-U.S. (about 400), both in
terms of their topic and scope distributions. For each
comparison, we performed a Pearson ?2-test using
location as the explanatory variable and either topic
or scope as the response variable.2 The null hypoth-
esis is that the variables are independent. For both
tests we reject the null hypothesis, with p < 0.001
for topic, and p = 0.006 for scope. This indicates a
dependence between location and topic/scope. As-
terisks in Figure 2 denote the labels that differ sig-
nificantly between U.S. and non-U.S. wishes.3
In particular, we observed that there are signif-
icantly more wishes about love, peace, and travel
from non-U.S. locales, and more about religion from
the U.S. There are significantly more world-scoped
wishes from non-U.S. locales, and more country-
and family-scoped wishes from the U.S.
We also compared wishes from ?red states? ver-
sus ?blue states? (U.S. states that voted a majority
for the Republican and Democratic presidential can-
didates in 2008, respectively), but found no signifi-
cant differences.
2The topic test examined a 2 ? 11 contingency table, while
the scope test used a 2 ? 6 contingency table. In both tests, all
of the cells in the tables had an expected frequency of at least 5,
so the ?2 approximation is valid.
3To identify the labels that differ significantly by location,
we computed the standardized residuals for the cells in the two
contingency tables. Standardized residuals are approximately
N (0, 1)-distributed and can be used to locate the major con-
tributors to a significant ?2-test statistic (Agresti, 2002). The
asterisks in Figure 2 indicate the surprisingly large residuals,
i.e., the difference between observed and expected frequencies
is outside a 95% confidence interval.
265
(a) Wish topics differ by Location
(b) Wish scopes differ by Location
Figure 2: Geographical breakdown of topic and scope
distributions based on approximately 4,000 location-
tagged wishes. Asterisks indicate statistically significant
differences.
2.3 Wishes Follow Zipf?s Law
We now move beyond the annotated subsample and
examine the full set of 89,574 English wishes. We
noticed that a small fraction (4%) of unique wishes
account for a relatively large portion (16%) of wish
occurrences, while there are also many wishes that
only occur once. The question naturally arises: do
wishes obey Zipf?s Law (Zipf, 1932; Manning and
Schu?tze, 1999)? If so, we should expect the fre-
quency of a unique wish to be inversely proportional
to its rank, when sorted by frequency. Figure 3
plots rank versus frequency on a log-log scale and
reveals an approximately linear negative slope, thus
suggesting that wishes do follow Zipf?s law. It also
shows that low-occurrence wishes dominate, hence
learning might be hindered by data sparseness.
2.4 Latent Topic Modeling for Wishes
The 11 topics in Section 2.1 were manually pre-
defined based on domain knowledge. In contrast,
in this section we applied Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) to identify the latent
topics in the full set of 89,574 English wishes in an
100 101 102 103 104 10510
0
101
102
103 peace
to find my true love
to take overthe world
log(rank)
log(f
requ
ency
)
Figure 3: The rank vs. frequency plot of wishes, approx-
imately obeying Zipf?s law. Note the log-log scale.
unsupervised fashion. The goal is to validate and
complement the study in Section 2.1.
To apply LDA to the wishes, we treated each indi-
vidual wish as a short document. We used 12 topics,
Collapsed Gibbs Sampling (Griffiths and Steyvers,
2004) for inference, hyperparameters ? = 0.5 and
? = 0.1, and ran Markov Chain Monte Carlo for
2000 iterations.
The resulting 12 LDA topics are shown in Ta-
ble 2, in the form of the highest probability words
p(word|topic) in each topic. We manually added
summary descriptors for readability. With LDA, it is
also possible to observe which words were assigned
to which topics in each wish. For example, LDA as-
signed most words in the wish ?world(8) peace(8)
and my friends(4) in iraq(1) to come(1) home(1)?
to two topics: peace and troops (topic numbers in
parentheses). Interestingly, these LDA topics largely
agree with the pre-defined topics in Section 2.1.
3 Building Wish Detectors
We now study the novel NLP task of wish detection,
i.e., classifying individual sentences as being wishes
or not. Importantly, we want our approach to trans-
fer to domains other than New Year?s wishes, in-
cluding consumer product reviews and online politi-
cal discussions. It should be pointed out that wishes
are highly domain dependent. For example, ?I wish
for world peace? is a common wish on New Year?s
Eve, but is exceedingly rare in product reviews; and
vice versa: ?I want to have instant access to the vol-
ume? may occur in product reviews, but is an un-
266
Topic Summary Top words in the topic, sorted by p(word|topic)
0 New Year year, new, happy, 2008, best, everyone, great, years, wishing, prosperous, may, hope
1 Troops all, god, home, come, may, safe, s, us, bless, troops, bring, iraq, return, 2008, true, dreams
2 Election wish, end, no, more, 2008, war, stop, president, paul, not, ron, up, free, less, bush, vote
3 Life more, better, life, one, live, time, make, people, than, everyone, day, wish, every, each
4 Prosperity health, happiness, good, family, friends, all, love, prosperity, wealth, success, wish, peace
5 Love love, me, find, wish, true, life, meet, want, man, marry, call, someone, boyfriend, fall, him
6 Career get, wish, job, out, t, hope, school, better, house, well, want, back, don, college, married
7 Lottery wish, win, 2008, money, want, make, become, lottery, more, great, lots, see, big, times
8 Peace peace, world, all, love, earth, happiness, everyone, joy, may, 2008, prosperity, around
9 Religion love, forever, jesus, know, loves, together, u, always, 2, 3, 4, much, best, mom, christ
10 Family healthy, happy, wish, 2008, family, baby, life, children, long, safe, husband, stay, marriage
11 Health com, wish, s, me, lose, please, let, cancer, weight, cure, mom, www, mother, visit, dad
Table 2: Wish topics learned from Latent Dirichlet Allocation. Words are sorted by p(word|topic).
likely New Year?s wish. For this initial study, we do
assume that there are some labeled training data in
the target domains of interest.
To transfer the knowledge learned from the out-
of-domain WISH corpus to other domains, our key
insight is the following: while the content of wishes
(e.g., ?world peace?) may not transfer across do-
mains, the ways wishes are expressed (e.g., ?I wish
for ?) may. We call these expressions wish tem-
plates. Our novel contribution is an unsupervised
method for discovering candidate templates from the
WISH corpus which, when applied to other target
domains, improve wish detection in those domains.
3.1 Two Simple Wish Detectors
Before describing our template discovery method,
we first describe two simple wish detectors, which
serve as baselines.
1. [Manual]: It may seem easy to locate
wishes. Perhaps looking for sentences containing
the phrases ?i wish,? ?i hope,? or some other sim-
ple patterns is sufficient for identifying the vast ma-
jority of wishes in a domain. To test this hypothe-
sis, we asked two native English speakers (not the
annotators, nor affiliated with the project; no expo-
sure to any of the wish datasets) to come up with
text patterns that might be used to express wishes.
They were shown three dictionary definitions of ?to
wish (v)? and ?wish (n)?. They produced a ranked
list of 13 templates; see Table 3. The underscore
matches any string. These templates can be turned
into a simple rule-based classifier: If part of a sen-
tence matches one of the templates, the sentence is
i wish
i hope
i want
hopefully
if only
would be better if
would like if
should
would that
can?t believe didn?t
don?t believe didn?t
do want
i can has
Table 3: Manual templates for identifying wishes.
classified as a wish. By varying the depth of the list,
one can produce different precision/recall behaviors.
Overall, we expect [Manual] to have relatively high
precision but low recall.
2. [Words]: Another simple method for detecting
wishes is to train a standard word-based text clas-
sifier using the labeled training set in the target do-
main. Specifically, we represent each sentence as
a binary word-indicator vector, normalized to sum
to 1. We then train a linear Support Vector Ma-
chine (SVM). This method may have higher recall,
but precision may suffer. For instance, the sentence
?Her wish was carried out by her husband? is not a
wish, but could be misclassified as one because of
the word ?wish.?
Note that neither of the two baseline methods uses
the WISH corpus.
267
3.2 Automatically Discovering Wish Templates
We now present our method to automatically dis-
cover high quality wish templates using the WISH
corpus. The key idea is to exploit redundancy in
how the same wish content is expressed. For ex-
ample, as we see in Table 1, both ?world peace? and
?i wish for world peace? are common wishes. Sim-
ilarly, both ?health and happiness? and ?i wish for
health and happiness? appear in the WISH corpus.
It is thus reasonable to speculate that ?i wish for ?
is a good wish template. Less obvious templates can
be discovered in this way, too, such as ?let there be
? from ?peace on earth? and ?let there be peace
on earth.?
We formalize this intuition as a bipartite graph, il-
lustrated in Figure 4. Let W = {w1, . . . , wn} be the
set of unique wishes in the WISH corpus. The bi-
partite graph has two types of nodes: content nodes
C and template nodes T , and they are generated as
follows. If a wish wj (e.g., ?i wish for world peace?)
contains another wish wi (e.g., ?world peace?), we
create a content node c1 = wi and a template node
t1 =?i wish for ?. We denote this relationship by
wj = c1+ t1. Note the order of c1 and t1 is insignif-
icant, as how the two combine is determined by the
underscore in t1, and wj = t1 + c1 is just fine. In
addition, we place a directed edge from c1 to t1 with
edge weight count(wj), the frequency of wish wj in
the WISH corpus. Then, a template node appears to
be a good one if many heavy edges point to it.
On the other hand, a template is less desirable
if it is part of a content node. For example, when
wj =?health and happiness? and wi =?health?, we
create the template t2 =? and happiness? and the
content node c3 = wi. If there is another wish
wk =?i wish for health and happiness?, then there
will be a content node c2 = wj . The template t2
thus contains some content words (since it matches
c2), and may not generalize well in a new domain.
We capture this by backward edges: if ?c? ? C, and
? string s (s not necessarily in C or W ) such that
c? = s+ t, we add a backward edge from t to c? with
edge weight count(c?).
Based on such considerations, we devised the fol-
lowing scheme for scoring templates:
score(t) = in(t)? out(t), (1)
health and happiness
c1
c2
c3
t1
t2
i wish for ___
___ and happiness
world peace
health
count(c1+t1)
count(c2)
Figure 4: The bipartite graph to create templates.
where in(t) is the in-degree of node t, defined as the
sum of edge weights coming into t; out(t) is the out-
degree of node t, defined similarly. In other words, a
template receives a high score if it is ?used? by many
frequent wishes but does not match many frequent
content-only wishes. To create the final set of tem-
plate features, we apply the threshold score(t) ? 5.
This produces a final list of 811 templates. Table 4
lists some of the top templates ranked by score(t).
While some of these templates still contain time- or
scope-related words (?for my family?), they are de-
void of specific topical content. Notice that we have
automatically identified several of the manually de-
rived templates in Table 3, and introduce many new
variations that a learning algorithm can leverage.
Top 10 Others in Top 200
in 2008 i want to
i wish for for everyone
i wish i hope
i want my wish is
this year please
i wish in 2008 wishing for
i wish to may you
for my family i wish i had
i wish this year to finally
in the new year for my family to have
Table 4: Top templates according to Equation 1.
3.3 Learning with Wish Template Features
After discovering wish templates as described
above, we use them as features for learning in a new
domain (e.g., product reviews). For each sentence in
the new domain, we assign binary features indicat-
ing which templates match the sentence. Two types
of matching are possible. Strict matching requires
that the template must match an entire sentence from
beginning to end, with at least one word filling in for
the underscore. (All matching during the template
generation process was strict.) Non-strict matching
268
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 
ManualWordsTemplatesWords + Templates
Figure 5: Politics domain precision-recall curves.
requires only that template match somewhere within
a sentence. Rather than choose one type of match-
ing, we create both strict and non-strict template fea-
tures (1622 binary features total) and let the machine
learning algorithm decide what is most useful.
Our third wish detector, [Templates], is a linear
SVM with the 1622 binary wish template features.
Our fourth wish detector, [Words + Templates], is
a linear SVM with both template and word features.
4 Experimental Results
4.1 Target Domains and Experimental Setup
We experimented with two domains, manually la-
beled at the sentence-level as wishes or non-wishes.4
Example wishes are listed in Table 6.
Products. Consumer product reviews: 1,235 sen-
tences selected from a collection of amazon.com and
cnet.com reviews (Hu and Liu, 2004; Ding et al,
2008). 12% of the sentences are labeled as wishes.
Politics. Political discussion board postings:
6,379 sentences selected from politics.com (Mullen
and Malouf, 2008). 34% are labeled as wishes.
We automatically split the corpora into sen-
tences using MxTerminator (Reynar and Ratna-
parkhi, 1997). As preprocessing before learning, we
tokenized the text in the Penn TreeBank style, down-
4These wish-annotated corpora are available for download
at http://pages.cs.wisc.edu/?goldberg/wish data.
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 
ManualWordsTemplatesWords + Templates
Figure 6: Products domain precision-recall curves.
cased, and removed all punctuation.
For all four wish detectors, we performed 10-fold
cross validation. We used the default parameter in
SVMlight for all trials (Joachims, 1999). As the
data sets are skewed, we compare the detectors us-
ing precision-recall curves and the area under the
curve (AUC). For the manual baseline, we produce
the curve by varying the number of templates ap-
plied (in rank order), which gradually predicts more
sentences as wishes (increasing recall at the expense
of precision). A final point is added at recall 1.0,
corresponding to applying an empty template that
matches all sentences. For the SVM-based meth-
ods, we vary the threshold applied to the real-valued
margin prediction to produce the curves. All curves
are interpolated, and AUC measures are computed,
using the techniques of (Davis and Goadrich, 2006).
4.2 Results
Figure 5 shows the precision-recall curves for the
Politics corpus. All curves are averages over 10
folds (i.e., for each of 100 evenly spaced, interpo-
lated recall points, the 10 precision values are aver-
aged). As expected, [Manual] can be very precise
with low recall?only the very top few templates
achieve high precision and pick out a small num-
ber of wishes with ?i wish? and ?i hope.? As we
introduce more templates to cover more true wishes,
precision drops off quickly. [Templates] is similar,
269
Corpus [Manual] [Words] [Templates] [Words + Templates]
Politics 0.67? 0.03 0.77? 0.03 0.73? 0.03 0.80? 0.03
Products 0.49? 0.13 0.52? 0.16 0.47? 0.16 0.56? 0.16
Table 5: AUC results (10-fold averages ? one standard deviation).
Products:
the only area i wish apple had improved upon would be the screen
i just want music to eminate from it when i want how i want
the dial on the original zen was perfect and i wish it was on this model
i would like album order for my live albums and was just wondering
Politics:
all children should be allowed healthcare
please call on your representatives in dc and ask them to please stop the waste in iraq
i hope that this is a new beginning for the middle east
may god bless and protect the brave men and that we will face these dangers in the future
Table 6: Example target-domain wishes correctly identified by [Words + Templates].
with slightly better precision in low recall regions.
[Words] is the opposite: bad in high recall but good
in low recall regions. [Words + Templates] is the
best, taking the best from both kinds of features to
dominate other curves. Table 5 shows the average
AUC across 10 folds. [Words + Templates] is sig-
nificantly better than all other detectors under paired
t-tests (p = 1 ? 10?7 vs. [Manual], p = 0.01 vs.
[Words], and p = 4 ? 10?7 vs. [Templates]). All
other differences are statistically significant, too.
Figure 6 shows the precision-recall curves for
the Products corpus. Again, [Words + Templates]
mostly dominates other detectors. In terms of av-
erage AUC across folds (Table 5), [Words + Tem-
plates] is also the best. However, due to the small
size of this corpus, the AUC values have high vari-
ance, and the difference between [Words + Tem-
plates] and [Words] is not statistically significant un-
der a paired t-test (p = 0.16).
Finally, to understand what is being learned in
more detail, we take a closer look at the SVM mod-
els? weights for one fold of the Products corpus
(Table 7). The most positive and negative features
make intuitive sense. Note that [Words + Templates]
seems to rely on templates for selecting wishes and
words for excluding non-wishes. This partially ex-
plains the synergy of combining the feature types.
Sign [Words] [Templates] [Words +Templates]
+ wish i hope hoping
+ hope i wish i hope
+ hopefully hoping i just want
+ hoping i just want i wish
+ want i would like i would like
- money family micro
- find forever about
- digital let me fix
- again d digital
- you for my dad you
Table 7: Features with the largest magnitude weights in
the SVM models for one fold of the Products corpus.
5 Conclusions and Future Work
We have presented a novel study of wishes from
an NLP perspective. Using the first-of-its-kind
WISH corpus, we generated domain-independent
wish templates that improve wish detection perfor-
mance across product reviews and political discus-
sion posts. Much work remains in this new research
area, including the creation of more types of fea-
tures. Also, due to the difficulty in obtaining wish-
annotated training data, we plan to explore semi-
supervised learning for wish detection.
Acknowledgements We thank the Times Square Al-
liance for providing the WISH corpus, and the Wisconsin
Alumni Research Foundation. AG is supported in part by
a Yahoo! Key Technical Challenges Grant.
270
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In In Proceedings of the 5th ACM International Con-
ference on Digital Libraries, pages 85?94.
Alan Agresti. 2002. Categorical Data Analysis. Wiley-
Interscience, second edition.
Shlomo Argamon and Anat Rachel Shimoni. 2003. Au-
tomatically categorizing written texts by author gen-
der. Literary and Linguistic Computing, 17:401?412.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elizabeth Sugar Boese and Adele Howe. 2005. Genre
classification of web documents. In Proceedings of
the 20th National Conference on Artificial Intelligence
(AAAI-05), Poster paper.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In WebDB ?98: Selected
papers from the International Workshop on The World
Wide Web and Databases, pages 172?183. Springer-
Verlag.
Jesse Davis and Mark Goadrich. 2006. The relationship
between precision-recall and roc curves. In ICML ?06:
Proceedings of the 23rd international conference on
Machine learning, New York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
WSDM ?08: Proceedings of the international confer-
ence on Web search and web data mining, pages 231?
240. ACM.
Howard Ehrlichman and Rosalind Eichenstein. 1992.
Private wishes: Gender similarities and difference.
Sex Roles, 26(9):399?422.
Thomas Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. Proceedings of the National Academy of
Sciences, 101(suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD ?04,
the ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177. ACM
Press.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Laura A. King and Sheri J. Broyles. 1997. Wishes, gen-
der, personality, and well-being. Journal of Personal-
ity, 65(1):49?76.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? let the market decide. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text,
pages 86?88.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Rada Mihalcea and Hugo Liu. 2006. A corpus-based ap-
proach to finding happiness. In Proceedings of AAAI-
CAAW-06, the Spring Symposia on Computational Ap-
proaches to Analyzing Weblogs.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Empirical Methods in Natural Lan-
guage Processing.
Norman A. Milgram and Wolfgang W. Riedel. 1969.
Developmental and experiential factors in making
wishes. Child Development, 40(3):763?771.
Gilad Mishne, Krisztian Balog, Maarten de Rijke, and
Breyten Ernsting. 2007. Moodviews: Tracking and
searching mood-annotated blog posts. In Proceed-
ings International Conf. on Weblogs and Social Media
(ICWSM-2007), pages 323?324.
Tony Mullen and Robert Malouf. 2008. Taking sides:
User classification for informal online political dis-
course. Internet Research, 18:177?190.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Fifth Conference on Applied Natural
Language Processing.
James Shanahan, Yan Qu, and Janyce Wiebe, editors.
2005. Computing attitude and affect in text. Springer,
Dordrecht, The Netherlands.
George S. Speer. 1939. Oral and written wishes of
rural and city school children. Child Development,
10(3):151?155.
G. K. Zipf. 1932. Selected Studies of the Principle of
Relative Frequency in Language. Harvard University
Press.
271
Proceedings of ACL-08: HLT, pages 656?664,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Bigrams from Unigrams
Xiaojin Zhu? and Andrew B. Goldberg? and Michael Rabbat? and Robert Nowak?
?Department of Computer Sciences, University of Wisconsin-Madison
?Department of Electrical and Computer Engineering, McGill University
?Department of Electrical and Computer Engineering, University of Wisconsin-Madison
{jerryzhu, goldberg}@cs.wisc.edu, michael.rabbat@mcgill.ca, nowak@ece.wisc.edu
Abstract
Traditional wisdom holds that once docu-
ments are turned into bag-of-words (unigram
count) vectors, word orders are completely
lost. We introduce an approach that, perhaps
surprisingly, is able to learn a bigram lan-
guage model from a set of bag-of-words docu-
ments. At its heart, our approach is an EM al-
gorithm that seeks a model which maximizes
the regularized marginal likelihood of the bag-
of-words documents. In experiments on seven
corpora, we observed that our learned bigram
language models: i) achieve better test set per-
plexity than unigram models trained on the
same bag-of-words documents, and are not far
behind ?oracle bigram models? trained on the
corresponding ordered documents; ii) assign
higher probabilities to sensible bigram word
pairs; iii) improve the accuracy of ordered-
document recovery from a bag-of-words. Our
approach opens the door to novel phenomena,
for example, privacy leakage from index files.
1 Introduction
A bag-of-words (BOW) is a basic document repre-
sentation in natural language processing. In this pa-
per, we consider a BOW in its simplest form, i.e.,
a unigram count vector or word histogram over the
vocabulary. When performing the counting, word
order is ignored. For example, the phrases ?really
neat? and ?neat really? contribute equally to a BOW.
Obviously, once a set of documents is turned into
a set of BOWs, the word order information within
them is completely lost?or is it?
In this paper, we show that one can in fact partly
recover the order information. Specifically, given a
set of documents in unigram-count BOW representa-
tion, one can recover a non-trivial bigram language
model (LM)1, which has part of the power of a bi-
gram LM trained on ordered documents. At first
glance this seems impossible: How can one learn
bigram information from unigram counts? However,
we will demonstrate that multiple BOW documents
enable us to recover some higher-order information.
Our results have implications in a wide range of
natural language problems, in particular document
privacy. With the wide adoption of natural language
applications like desktop search engines, software
programs are increasingly indexing computer users?
personal files for fast processing. Most index files
include some variant of the BOW. As we demon-
strate in this paper, if a malicious party gains access
to BOW index files, it can recover more than just
unigram frequencies: (i) the malicious party can re-
cover a higher-order LM; (ii) with the LM it may at-
tempt to recover the original ordered document from
a BOW by finding the most-likely word permuta-
tion2. Future research will quantify the extent to
which such a privacy breach is possible in theory,
and will find solutions to prevent it.
There is a vast literature on language modeling;
see, e.g., (Rosenfeld, 2000; Chen and Goodman,
1999; Brants et al, 2007; Roark et al, 2007). How-
1A trivial bigram LM is a unigram LM which ignores his-
tory: P (v|u) = P (v).
2It is possible to use a generic higher-order LM, e.g., a tri-
gram LM trained on standard English corpora, for this purpose.
However, incorporating a user-specific LM helps.
656
ever, to the best of our knowledge, none addresses
this reverse direction of learning higher-order LMs
from lower-order data. This work is inspired by re-
cent advances in inferring network structure from
co-occurrence data, for example, for computer net-
works and biological pathways (Rabbat et al, 2007).
2 Problem Formulation and Identifiability
We assume that a vocabulary of size W is given.
For notational convenience, we include in the vo-
cabulary a special ?begin-of-document? symbol ?d?
which appears only at the beginning of each docu-
ment. The training corpus consists of a collection of
n BOW documents {x1, . . . ,xn}. Each BOW xi is
a vector (xi1, . . . , xiW ) where xiu is the number of
times word u occurs in document i. Our goal is to
learn a bigram LM ?, represented as aW?W transi-
tion matrix with ?uv = P (v|u), from the BOW cor-
pus. Note P (v|?d?) corresponds to the initial state
probability for word v, and P (?d?|u) = 0,?u.
It is worth noting that traditionally one needs or-
dered documents to learn a bigram LM. A natural
question that arises in our problem is whether or not
a bigram LM can be recovered from the BOW cor-
pus with any guarantee. Let X denote the space
of all possible BOWs. As a toy example, consider
W = 3 with the vocabulary {?d?, A, B}. Assuming
all documents have equal length |x| = 4 (including
?d?), then X = {(?d?:1, A:3, B:0), (?d?:1, A:2, B:1),
(?d?:1, A:1, B:2), (?d?:1, A:0, B:3)}. Our training
BOW corpus, when sufficiently large, provides the
marginal distribution p?(x) for x ? X . Can we re-
cover a bigram LM from p?(x)?
To answer this question, we first need to introduce
a generative model for the BOWs. We assume that
the BOW corpus is generated from a bigram LM ?
in two steps: (i) An ordered document is generated
from the bigram LM ?; (ii) The document?s unigram
counts are collected to produce the BOW x. There-
fore, the probability of a BOW x being generated
by ? can be computed by marginalizing over unique
orderings z of x:
P (x|?) =
?
z??(x)
P (z|?) =
?
z??(x)
|x|
?
j=2
?zj?1,zj ,
where ?(x) is the set of unique orderings, and |x| is
the document length. For example, if x =(?d?:1,
A:2, B:1) then ?(x) = {z1, z2, z3} with z1 =
??d? A A B?, z2 = ??d? A B A?, z3 = ??d? B A A?.
Bigram LM recovery then amounts to finding a ?
that satisfies the system of marginal-matching equa-
tions
P (x|?) = p?(x) , ?x ? X . (1)
As a concrete example where one can exactly re-
cover a bigram LM from BOWs, consider our toy
example again. We know there are only three free
variables in our 3?3 bigram LM ?: r = ??d?A, p =
?AA, q = ?BB , since the rest are determined by
normalization. Suppose the documents are gener-
ated from a bigram LM with true parameters r =
0.25, p = 0.9, q = 0.5. If our BOW corpus is very
large, we will observe that 20.25% of the BOWs are
(?d?:1, A:3, B:0), 37.25% are (?d?:1, A:2, B:1), and
18.75% are (?d?:1, A:0, B:3). These numbers are
computed using the definition of P (x|?). We solve
the reverse problem of finding r, p, q from the sys-
tem of equations (1), now explicitly written as
?
?
?
?
?
?
?
rp2 = 0.2025
rp(1? p) + r(1? p)(1? q)
+(1? r)(1? q)p = 0.3725
(1? r)q2 = 0.1875.
The above system has only one valid solution,
which is the correct set of bigram LM parameters
(r, p, q) = (0.25, 0.9, 0.5).
However, if the true parameters were (r, p, q) =
(0.1, 0.2, 0.3) with proportions of BOWs being
0.4%, 19.8%, 8.1%, respectively, it is easy to ver-
ify that the system would have multiple valid solu-
tions: (0.1, 0.2, 0.3), (0.8819, 0.0673, 0.8283), and
(0.1180, 0.1841, 0.3030). In general, if p?(x) is
known from the training BOW corpus, when can
we guarantee to uniquely recover the bigram LM
?? This is the question of identifiability, which
means the transition matrix ? satisfying (1) exists
and is unique. Identifiability is related to finding
unique solutions of a system of polynomial equa-
tions since (1) is such a system in the elements of ?.
The details are beyond the scope of this paper, but
applying the technique in (Basu and Boston, 2000),
it is possible to show that for W = 3 (including ?d?)
we need longer documents (|x| ? 5) to ensure iden-
tifiability. The identifiability of more general cases
is still an open research question.
657
3 Bigram Recovery Algorithm
In practice, the documents are not truly generated
from a bigram LM, and the BOW corpus may be
small. We therefore seek a maximum likelihood es-
timate of ? or a regularized version of it. Equiva-
lently, we no longer require equality in (1), but in-
stead find ? that makes the distribution P (x|?) as
close to p?(x) as possible. We formalize this notion
below.
3.1 The Objective Function
Given a BOW corpus {x1, . . . ,xn}, its nor-
malized log likelihood under ? is ?(?) ?
1
C
?n
i=1 logP (xi|?), where C =
?n
i=1(|xi| ? 1)
is the corpus length excluding ?d??s. The idea is to
find ? that maximizes ?(?). This also brings P (x|?)
closest to p?(x) in the KL-divergence sense. How-
ever, to prevent overfitting, we regularize the prob-
lem so that ? prefers to be close to a ?prior? bi-
gram LM ?. The prior ? is also estimated from the
BOW corpus, and is discussed in Section 3.4. We
define the regularizer to be an asymmetric dissimi-
larity D(?,?) between the prior ? and the learned
model ?. The dissimilarity is 0 if ? = ?, and
increases as they diverge. Specifically, the KL-
divergence between two word distributions condi-
tioned on the same history u is KL(?u???u?) =
?W
v=1 ?uv log ?uv?uv . We define D(?,?) to be
the average KL-divergence over all histories:
D(?,?) ? 1W
?W
u=1 KL(?u???u?), which is con-
vex in ? (Cover and Thomas, 1991). We will use
the following derivative later: ?D(?,?)/??uv =
??uv/(W?uv).
We are now ready to define the regularized op-
timization problem for recovering a bigram LM ?
from the BOW corpus:
max
?
?(?)? ?D(?,?)
subject to ?1 = 1, ? ? 0. (2)
The weight ? controls the strength of the prior. The
constraints ensure that ? is a valid bigram matrix,
where 1 is an all-one vector, and the non-negativity
constraint is element-wise. Equivalently, (2) can be
viewed as themaximum a posteriori (MAP) estimate
of ?, with independent Dirichlet priors for each row
of ?: p(?u?) = Dir(?u?|?u?) and hyperparameters
?uv = ?CW ?uv + 1.
The summation over hidden ordered documents
z in P (x|?) couples the variables and makes (2) a
non-concave problem. We optimize ? using an EM
algorithm.
3.2 The EM Algorithm
We derive the EM algorithm for the optimization
problem (2). Let O(?) ? ?(?) ? ?D(?,?) be the
objective function. Let ?(t?1) be the bigram LM at
iteration t? 1. We can lower-bound O as follows:
O(?)
= 1C
n
?
i=1
log
?
z??(xi)
P (z|?(t?1),x) P (z|?)
P (z|?(t?1),x)
??D(?,?)
? 1C
n
?
i=1
?
z??(xi)
P (z|?(t?1),x) log P (z|?)
P (z|?(t?1),x)
??D(?,?)
? L(?,?(t?1)).
We used Jensen?s inequality above since log()
is concave. The lower bound L involves
P (z|?(t?1),x), the probability of hidden orderings
of the BOW under the previous iteration?s model.
In the E-step of EM we compute P (z|?(t?1),x),
which will be discussed in Section 3.3. One
can verify that L(?,?(t?1)) is concave in ?, un-
like the original objective O(?). In addition, the
lower bound ?touches? the objective at ?(t?1), i.e.,
L(?(t?1),?(t?1)) = O(?(t?1)).
The EM algorithm iteratively maximizes the
lower bound, which is now a concave optimization
problem: max? L(?,?
(t?1)), subject to ?1 = 1.
The non-negativity constraints turn out to be auto-
matically satisfied. Introducing Lagrange multipli-
ers ?u for each history u = 1 . . .W , we form the
Lagrangian ?:
? ? L(?,?(t?1))?
W
?
u=1
?u
( W
?
v=1
?uv ? 1
)
.
Taking the partial derivative with respect to ?uv and
setting it to zero: ??/??uv = 0, we arrive at the
following update:
?uv ?
n
?
i=1
?
z??(xi)
P (z|?(t?1),x)cuv(z) +
?C
W ?uv.
(3)
658
Input: BOW documents {x1, . . . ,xn}, a prior bi-
gram LM ?, weight ?.
1. t = 1. Initialize ?(0) = ?.
2. Repeat until the objective O(?) converges:
(a) (E-step) Compute P (z|?(t?1),x) for z ?
?(xi), i = 1, . . . , n.
(b) (M-step) Compute ?(t) using (3). Let t =
t + 1.
Output: The recovered bigram LM ?.
Table 1: The EM algorithm
The normalization is over v = 1 . . .W . We use
cuv(z) to denote the number of times the bigram
?uv? appears in the ordered document z. This is the
M-step of EM. Intuitively, the first term counts how
often the bigram ?uv? occurs, weighing each order-
ing by its probability under the previous model; the
second term pulls the parameter towards the prior.
If the weight of the prior ? ? ?, we would have
?uv = ?uv. The update is related to the MAP esti-
mate for a multinomial distribution with a Dirichlet
prior, where we use the expected counts.
We initialize the EM algorithm with ?(0) = ?.
The EM algorithm is summarized in Table 1.
3.3 Approximate E-step
The E-step needs to compute the expected bigram
counts of the form
?
z??(x)
P (z|?,x)cuv(z). (4)
However, this poses a computational problem. The
summation is over unique ordered documents. The
number of unique ordered documents can be on the
order of |x|!, i.e., all permutations of the BOW. For a
short document of length 15, this number is already
1012. Clearly, brute-force enumeration is only fea-
sible for very short documents. Approximation is
necessary to handle longer ones.
A simple Monte Carlo approximation to (4)
would involve sampling ordered documents
z1, z2, . . . , zL according to zi ? P (z|?,x), and
replacing (4) with ?Li=1 cuv(zi)/L. This estimate
is unbiased, and the variance decreases linearly
with the number of samples, L. However, sampling
directly from P is difficult.
Instead, we sample ordered documents zi ?
R(zi|?,x) from a distribution R which is easy
to generate, and construct an approximation us-
ing importance sampling (see, e.g., (Liu, 2001)).
With each sample, zi, we associate a weight
wi ? P (zi|?,x)/R(zi|?,x). The importance
sampling approximation to (4) is then given by
(?Li=1 wicuv(zi))/(
?L
i=1 wi). Re-weighting the
samples in this fashion accounts for the fact that we
are using a sampling distribution R which is differ-
ent the target distribution P , and guarantees that our
approximation is asymptotically unbiased.
The quality of an importance sampling approxi-
mation is closely related to how closelyR resembles
P ; the more similar they are, the better the approxi-
mation, in general. Given a BOW x and our current
bigram model estimate, ?, we generate one sample
(an ordered document zi) by sequentially drawing
words from the bag, with probabilities proportional
to ?, but properly normalized to form a distribution
based on which words remain in the bag. For exam-
ple, suppose x = (?d?:1, A:2, B:1, C:1). Then we
set zi1 = ?d?, and sample zi2 = A with probabil-
ity 2??d?A/(2??d?A + ??d?B + ??d?C). Similarly,
if zi(j?1) = u and if v is in the original BOW that
hasn?t been sampled yet, then we set the next word in
the ordered document zij equal to v with probability
proportional to cv?uv, where cv is the count of v in
the remaining BOW. For this scheme, one can ver-
ify (Rabbat et al, 2007) that the importance weight
corresponding to a sampled ordered document zi =
(zi1, . . . , zi|x|) is given by wi =
?|x|
t=2
?|x|
i=t ?zt?1zi .
In our implementation, the number of importance
samples used for a document x is 10|x|2 if the length
of the document |x| > 8; otherwise we enumerate
?(x) without importance sampling.
3.4 Prior Bigram LM ?
The quality of the EM solution ? can depend on the
prior bigram LM ?. To assess bigram recoverabil-
ity from a BOW corpus alone, we consider only pri-
ors estimated from the corpus itself3. Like ?, ? is a
W?W transition matrix with ?uv = P (v|u). When
3Priors based on general English text or domain-specific
knowledge could be used in specific applications.
659
appropriate, we set the initial probability ??d?v pro-
portional to the number of times word v appears in
the BOW corpus. We consider three prior models:
Prior 1: Unigram ?unigram. The most na??ve
? is a unigram LM which ignores word history.
The probability for word v is estimated from the
BOW corpus frequency of v, with add-1 smoothing:
?unigramuv ? 1 +
?n
i=1 xiv. We should point out
that the unigram prior is an asymmetric bigram, i.e.,
?unigramuv 6= ?unigramvu .
Prior 2: Frequency of Document Co-
occurrence (FDC) ?fdc. Let ?(u, v|x) = 1 if
words u 6= v co-occur (regardless of their counts)
in BOW x, and 0 otherwise. In the case u = v,
?(u, u|x) = 1 only if u appears at least twice in
x. Let cfdcuv =
?n
i=1 ?(u, v|xi) be the number of
BOWs in which u, v co-occur. The FDC prior is
?fdcuv ? cfdcuv + 1. The co-occurrence counts cfdc
are symmetric, but ?fdc is asymmetric because
of normalization. FDC captures some notion of
potential transitions from u to v. FDC is in spirit
similar to Kneser-Ney smoothing (Kneser and Ney,
1995) and other methods that accumulate indicators
of document membership.
Prior 3: Permutation-Based (Perm) ?perm. Re-
call that cuv(z) is the number of times the bigram
?uv? appears in an ordered document z. We define
cpermuv =
?n
i=1 Ez??(xi)[cuv(z)], where the expecta-
tion is with respect to all unique orderings of each
BOW. We make the zero-knowledge assumption of
uniform probability over these orderings, rather than
P (z|?) as in the EM algorithm described above. EM
will refine these estimates, though, so this is a natu-
ral starting point. Space precludes a full discussion,
but it can be proven that cpermuv =
?n
i=1 xiuxiv/|xi|
if u 6= v, and cpermuu =
?n
i=1 xiu(xiu ? 1)/|xi|. Fi-
nally, ?permuv ? cpermuv + 1.
3.5 Decoding Ordered Documents from BOWs
Given a BOW x and a bigram LM ?, we for-
mulate document recovery as the problem z? =
argmaxz??(x)P (z|?). In fact, we can generate
the top N candidate ordered documents in terms
of P (z|?). We use A? search to construct such
an N-best list (Russell and Norvig, 2003). Each
state is an ordered, partial document. Its succes-
sor states append one more unused word in x to
the partial document. The actual cost g from the
start (empty document) to a state is the log proba-
bility of the partial document under bigram ?. We
design a heuristic cost h from the state to the goal
(complete document) that is admissible: the idea is
to over-use the best bigram history for the remain-
ing words in x. Let the partial document end with
word we. Let the count vector for the remaining
BOW be (c1, . . . , cW ). One admissible heuristic
is h = log?Wu=1 P (u|bh(u);?)cu , where the ?best
history? for word type u is bh(u) = argmaxv?vu,
and v ranges over the word types with non-zero
counts in (c1, . . . , cW ), plus we. It is easy to see that
h is an upper bound on the bigram log probability
that the remaining words in x can achieve.
We use a memory-bounded A? search similar
to (Russell, 1992), because long BOWs would oth-
erwise quickly exhaust memory. When the priority
queue grows larger than the bound, the worst states
(in terms of g + h) in the queue are purged. This
necessitates a double-ended priority queue that can
pop either the maximum or minimum item. We use
an efficient implementation with Splay trees (Chong
and Sahni, 2000). We continue running A? after
popping the goal state from its priority queue. Re-
peating this N times gives the N-best list.
4 Experiments
We show experimentally that the proposed algo-
rithm is indeed able to recover reasonable bigram
LMs from BOW corpora. We observe:
1. Good test set perplexity: Using test (held-
out) set perplexity (PP) as an objective measure of
LM quality, we demonstrate that our recovered bi-
gram LMs are much better than na??ve unigram LMs
trained on the same BOW corpus. Furthermore, they
are not far behind the ?oracle? bigram LMs trained
on ordered documents that correspond to the BOWs.
2. Sensible bigram pairs: We inspect the recov-
ered bigram LMs and find that they assign higher
probabilities to sensible bigram pairs (e.g., ?i mean?,
?oh boy?, ?that?s funny?), and lower probabilities to
nonsense pairs (e.g., ?i yep?, ?you let?s?, ?right lot?).
3. Document recovery from BOW: With the bi-
gram LMs, we show improved accuracy in recover-
ing ordered documents from BOWs.
We describe these experiments in detail below.
660
Corpus |V | # Docs # Tokens |x|
SV10 10 6775 7792 1.2
SV25 25 9778 13324 1.4
SV50 50 12442 20914 1.7
SV100 100 14602 28611 2.0
SV250 250 18933 51950 2.7
SV500 500 23669 89413 3.8
SumTime 882 3341 68815 20.6
Table 2: Corpora statistics: vocabulary size, document
count, total token count, and mean document length.
4.1 Corpora and Protocols
We note that although in principle our algorithm
works on large corpora, the current implementa-
tion does not scale well (Table 3 last column). We
therefore experimented on seven corpora with rel-
atively small vocabulary sizes, and with short doc-
uments (mostly one sentence per document). Ta-
ble 2 lists statistics describing the corpora. The first
six contain text transcripts of conversational tele-
phone speech from the small vocabulary ?SVitch-
board 1? data set. King et al constructed each cor-
pus from the full Switchboard corpus, with the re-
striction that the sentences use only words in the cor-
responding vocabulary (King et al, 2005). We re-
fer to these corpora as SV10, SV25, SV50, SV100,
SV250, and SV500. The seventh corpus comes from
the SumTime-Meteo data set (Sripada et al, 2003),
which contains real weather forecasts for offshore
oil rigs in the North Sea. For the SumTime cor-
pus, we performed sentence segmentation to pro-
duce documents, removed punctuation, and replaced
numeric digits with a special token.
For each of the seven corpora, we perform 5-fold
cross validation. We use four folds other than the
k-th fold as the training set to train (recover) bigram
LMs, and the k-th fold as the test set for evaluation.
This is repeated for k = 1 . . . 5, and we report the
average cross validation results. We distinguish the
original ordered documents (training set z1, . . . zn,
test set zn+1, . . . , zm) and the corresponding BOWs
(training set x1 . . .xn, test set xn+1 . . .xm). In all
experiments, we simply set the weight ? = 1 in (2).
Given a training set and a test set, we perform the
following steps:
1. Build prior LMs ?X from the training BOW
corpus x1, . . .xn, for X = unigram, fdc, perm.
2. Recover the bigram LMs ?X with the EM al-
gorithm in Table 1, from the training BOW corpus
x1, . . .xn and using the prior from step 1.
3. Compute the MAP bigram LM from the or-
dered training documents z1, . . . zn. We call this the
?oracle? bigram LM because it uses order informa-
tion (not available to our algorithm), and we use it
as a lower-bound on perplexity.
4. Test all LMs on zn+1, . . . , zm by perplexity.
4.2 Good Test Set Perplexity
Table 3 reports the 5-fold cross validation mean-test-
set-PP values for all corpora, and the run time per
EM iteration. Because of the long running time, we
adopt the rule-of-thumb stopping criterion of ?two
EM iterations?. First, we observe that all bigram
LMs perform better than unigram LMs ?unigram
even though they are trained on the same BOW cor-
pus. Second, all recovered bigram LMs ?X im-
proved upon their corresponding baselines ?X . The
difference across every row is statistically significant
according to a two-tailed paired t-test with p < 0.05.
The differences among PP(?X ) for the same corpus
are also significant (except between ?unigram and
?perm for SV500). Finally, we observe that ?perm
tends to be best for the smaller vocabulary corpora,
whereas ?fdc dominates as the vocabulary grows.
To see how much better we could do if we had or-
dered training documents z1, . . . , zn, we present the
mean-test-set-PP of ?oracle? bigram LMs in Table 4.
We used three smoothing methods to obtain oracle
LMs: absolute discounting using a constant of 0.5
(we experimented with other values, but 0.5 worked
best), Good-Turing, and interpolated Witten-Bell as
implemented in the SRILM toolkit (Stolcke, 2002).
We see that our recovered LMs (trained on un-
ordered BOW documents), especially for small vo-
cabulary corpora, are close to the oracles (trained on
ordered documents). For the larger datasets, the re-
covery task is more difficult, and the gap between
the oracle LMs and the ? LMs widens. Note that the
oracle LMs do much better than the recovered LMs
on the SumTime corpus; we suspect the difference is
due to the larger vocabulary and significantly higher
average sentence length (see Table 2).
4.3 Sensible Bigram Pairs
The next set of experiments compares the recov-
ered bigram LMs to their corresponding prior LMs
661
Corpus X PP(?X ) PP(?X ) Time/
Iter
SV10
unigram 7.48 6.95 < 1s
fdc 6.52 6.47 < 1s
perm 6.50 6.45 < 1s
SV25
unigram 16.4 12.8 0.1s
fdc 12.3 11.8 0.1s
perm 12.2 11.7 0.1s
SV50
unigram 29.1 19.7 2s
fdc 19.6 17.8 4s
perm 19.5 17.7 5s
SV100
unigram 45.4 27.8 7s
fdc 29.5 25.3 11s
perm 30.0 25.6 11s
SV250
unigram 91.8 51.2 5m
fdc 60.0 47.3 8m
perm 65.4 49.7 8m
SV500
unigram 149.1 87.2 3h
fdc 104.8 80.1 3h
perm 123.9 87.4 3h
SumTime
unigram 129.7 81.8 4h
fdc 103.2 77.7 4h
perm 187.9 85.4 3h
Table 3: Mean test set perplexities of prior LMs and bi-
gram LMs recovered after 2 EM iterations.
in terms of how they assign probabilities to word
pairs. One naturally expects probabilities for fre-
quently occurring bigrams to increase, while rare
or nonsensical bigrams? probabilities should de-
crease. For a prior-bigram pair (?, ?), we evaluate
the change in probabilities by computing the ratio
?hw = P (w|h,?)P (w|h,?) =
?hw
?hw . For a given history h, we
sort words w by this ratio rather than by actual bi-
gram probability because the bigrams with the high-
est and lowest probabilities tend to stay the same,
while the changes accounting for differences in PP
scores are more noticeable by considering the ratio.
Due to space limitation, we present one specific
result (FDC prior, fold 1) for the SV500 corpus in
Table 5. Other results are similar. The table lists
a few most frequent unigrams as history words h
(left), and the words w with the smallest (center)
and largest (right) ?hw ratio. Overall we see that our
EM algorithm is forcing meaningless bigrams (e.g.,
?i goodness?, ?oh thing?) to have lower probabil-
ities, while assigning higher probabilities to sensi-
ble bigram pairs (e.g., ?really good?, ?that?s funny?).
Note that the reverse of some common expressions
(e.g., ?right that?s?) also rise in probability, suggest-
ing the algorithm detects that the two words are of-
Corpus Absolute
Discount
Good-
Turing
Witten-
Bell
??
SV10 6.27 6.28 6.27 6.45
SV25 10.5 10.6 10.5 11.7
SV50 14.8 14.9 14.8 17.7
SV100 20.0 20.1 20.0 25.3
SV250 33.7 33.7 33.8 47.3
SV500 50.9 50.9 51.3 80.1
SumTime 10.8 10.5 10.6 77.7
Table 4: Mean test set perplexities for oracle bigram LMs
trained on z1, . . . , zn and tested on zn+1, . . . , zm. For
reference, the rightmost column lists the best result using
a recovered bigram LM (?perm for the first three corpora,
?fdc for the latter four).
ten adjacent, but lacks sufficient information to nail
down the exact order.
4.4 Document Recovery from BOW
We now play the role of the malicious party men-
tioned in the introduction. We show that, com-
pared to their corresponding prior LMs, our recov-
ered bigram LMs are better able to reconstruct or-
dered documents out of test BOWs xn+1, . . . ,xm.
We perform document recovery using 1-best A? de-
coding. We use ?document accuracy? and ?n-gram
accuracy? (for n = 2, 3) as our evaluation criteria.
We define document accuracy (Accdoc) as the frac-
tion of documents4 for which the decoded document
matches the true ordered document exactly. Simi-
larly, n-gram accuracy (Accn) measures the fraction
of all n-grams in test documents (with n or more
words) that are recovered correctly.
For this evaluation, we compare models built for
the SV500 corpus. Table 6 presents 5-fold cross val-
idation average test-set accuracies. For each accu-
racy measure, we compare the prior LM with the
recovered bigram LM. It is interesting to note that
the FDC and Perm priors reconstruct documents sur-
prisingly well, but we can always improve them by
running our EM algorithm. The accuracies obtained
by ? are statistically significantly better (via two-
tailed paired t-tests with p < 0.05) than their cor-
responding priors ? in all cases except Accdoc for
?perm versus ?perm. Furthermore, ?fdc and ?perm
are significantly better than all other models in terms
of all three reconstruction accuracy measures.
4We omit single-word documents from these computations.
662
h w (smallest ?hw) w (largest ?hw)
i yep, bye-bye, ah, goodness, ahead mean, guess, think, bet, agree
you let?s, us, fact, such, deal thank, bet, know, can, do
right as, lot, going, years, were that?s, all, right, now, you?re
oh thing, here, could, were, doing boy, really, absolutely, gosh, great
that?s talking, home, haven?t, than, care funny, wonderful, true, interesting, amazing
really now, more, yep, work, you?re sad, neat, not, good, it?s
Table 5: The recovered bigram LM ?fdc decreases nonsense bigram probabilities (center column) and increases
sensible ones (right column) compared to the prior ?fdc on the SV500 corpus.
?perm reconstructions of test BOWs ?perm reconstructions of test BOWs
just it?s it?s it?s just going it?s just it?s just it?s going
it?s probably out there else something it?s probably something else out there
the the have but it doesn?t but it doesn?t have the the
you to talking nice was it yes yes it was nice talking to you
that?s well that?s what i?m saying well that?s that?s what i?m saying
a little more here home take a little more take home here
and they can very be nice too and they can be very nice too
i think well that?s great i?m well i think that?s great i?m
but was he because only always but only because he was always
that?s think i don?t i no no i don?t i think that?s
that in and it it?s interesting and it it?s interesting that in
that?s right that?s right that?s difficult right that?s that?s right that?s difficult
so just not quite a year so just not a quite year
well it is a big dog well it is big a dog
so do you have a car so you do have a car
Table 7: Subset of SV500 documents that only ?perm or ?perm (but not both) reconstructs correctly. The correct
reconstructions are in bold.
Accdoc Acc2 Acc3
X ?X ?X ?X ?X ?X ?X
unigram 11.1 26.8 17.7 32.8 2.7 11.8
fdc 30.2 31.0 33.0 35.1 11.4 13.3
perm 30.9 31.5 32.7 34.8 11.5 13.1
Table 6: Percentage of correctly reconstructed docu-
ments, 2-grams and 3-grams from test BOWs in SV500,
5-fold cross validation. The same trends continue for 4-
grams and 5-grams (not shown).
We conclude our experiments with a closer look
at some BOWs for which ? and ? reconstruct dif-
ferently. As a representative example, we compare
?perm to ?perm on one test set of the SV500 cor-
pus. There are 92 documents that are correctly re-
constructed by ?perm but not by ?perm. In con-
trast, only 65 documents are accurately reordered by
?perm but not by ?perm. Table 7 presents a subset
of these documents with six or more words. Over-
all, we conclude that the recovered bigram LMs do
a better job at reconstructing BOW documents.
5 Conclusions and Future Work
We presented an algorithm that learns bigram lan-
guage models from BOWs. We plan to: i) inves-
tigate ways to speed up our algorithm; ii) extend
it to trigram and higher-order models; iii) handle
the mixture of BOW documents and some ordered
documents (or phrases) when available; iv) adapt a
general English LM to a special domain using only
BOWs from that domain; and v) explore novel ap-
plications of our algorithm.
Acknowledgments
We thank Ben Liblit for tips on doubled-ended
priority queues, and the anonymous reviewers for
valuable comments. This work is supported in
part by the Wisconsin Alumni Research Founda-
tion, NSF CCF-0353079 and CCF-0728767, and the
Natural Sciences and Engineering Research Council
(NSERC) of Canada.
663
References
Samit Basu and Nigel Boston. 2000. Identifiability of
polynomial systems. Technical report, University of
Illinois at Urbana-Champaign.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language models
in machine translation. In Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Stanley F. Chen and Joshua T. Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
13(4):359?393.
Kyun-Rak Chong and Sartaj Sahni. 2000.
Correspondence-based data structures for double-
ended priority queues. The ACM Journal of
Experimental Algorithmics, 5(2).
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley & Sons, Inc.
Simon King, Chris Bartels, and Jeff Bilmes. 2005.
SVitchboard 1: Small vocabulary tasks from Switch-
board 1. In Interspeech 2005, Lisbon, Portugal.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off forM-gram language modeling. In
ICASSP.
Jun S. Liu. 2001. Monte Carlo Strategies in Scientific
Computing. Springer.
Michael Rabbat, Ma?rio Figueiredo, and Robert Nowak.
2007. Inferring network structure from co-
occurrences. In Advances in Neural Information Pro-
cessing Systems (NIPS) 20.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373?392.
Ronald Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88(8).
Stuart Russell and Peter Norvig. 2003. Artificial Intel-
ligence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, second edition.
Stuart Russell. 1992. Efficient memory-bounded search
methods. In The 10th European Conference on Artifi-
cial Intelligence.
Somayajulu G. Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Exploiting a parallel TEXT-DATA corpus.
In Proceedings of Corpus Linguistics, pages 734?743,
Lancaster, U.K.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
Denver, Colorado.
664
Tutorial Abstracts of ACL-08: HLT, page 3,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semi-supervised Learning for Natural Language Processing
John Blitzer
Natural Language Computing Group
Microsoft Research Asia
Beijing, China
blitzer@cis.upenn.edu
Xiaojin Jerry Zhu
Department of Computer Science
University of Wisconsin, Madison
Madison, WI, USA
jerryzhu@cs.wisc.edu
1 Introduction
The amount of unlabeled linguistic data available
to us is much larger and growing much faster than
the amount of labeled data. Semi-supervised learn-
ing algorithms combine unlabeled data with a small
labeled training set to train better models. This
tutorial emphasizes practical applications of semi-
supervised learning; we treat semi-supervised learn-
ing methods as tools for building effective models
from limited training data. An attendee will leave
our tutorial with
1. A basic knowledge of the most common classes
of semi-supervised learning algorithms and where
they have been used in NLP before.
2. The ability to decide which class will be useful
in her research.
3. Suggestions against potential pitfalls in semi-
supervised learning.
2 Content Overview
Self-training methods Self-training methods use
the labeled data to train an initial model and then
use that model to label the unlabeled data and re-
train a new model. We will examine in detail the co-
training method of Blum and Mitchell [2], includ-
ing the assumptions it makes, and two applications
of co-training to NLP data. Another popular self-
training method treats the labels of the unlabeled
data as hidden and estimates a single model from
labeled and unlabeled data. We explore new meth-
ods in this framework that make use of declarative
linguistic side information to constrain the solutions
found using unlabeled data [3].
Graph regularization methods Graph regulariza-
tion methods build models based on a graph on in-
stances, where edges in the graph indicate similarity.
The regularization constraint is one of smoothness
along this graph. We wish to find models that per-
form well on the training data, but we also regularize
so that unlabeled nodes which are similar according
to the graph have similar labels. For this section, we
focus in detail on the Gaussian fields method of Zhu
et al [4].
Structural learning Structural learning [1] uses un-
labeled data to find a new, reduced-complexity hy-
pothesis space by exploiting regularities in feature
space via unlabeled data. If this new hypothesis
space still contains good hypotheses for our super-
vised learning problem, we may achieve high accu-
racy with much less training data. The regularities
we use come in the form of lexical features that func-
tion similarly for prediction. This section will fo-
cus on the assumptions behind structural learning, as
well as applications to tagging and sentiment analy-
sis.
References
[1] Rie Ando and Tong Zhang. A Framework for Learn-
ing Predictive Structures from Multiple Tasks and Unla-
beled Data. JMLR 2005.
[2] Avrim Blum and Tom Mitchell. Combining Labeled
and Unlabeled Data with Co-training. COLT 1998.
[3] Aria Haghighi and Dan Klein. Prototype-driven
Learning for Sequence Models. HLT/NAACL 2006.
[4] Xiaojin Zhu, Zoubin Ghahramani, and John Laf-
ferty. Semi-supervised Learning using Gaussian Fields
and Harmonic Functions. ICML 2003.
3
Workshop on TextGraphs, at HLT-NAACL 2006, pages 45?52,
New York City, June 2006. c?2006 Association for Computational Linguistics
Seeing stars when there aren?t many stars:
Graph-based semi-supervised learning for sentiment categorization
Andrew B. Goldberg
Computer Sciences Department
University of Wisconsin-Madison
Madison, W.I. 53706
goldberg@cs.wisc.edu
Xiaojin Zhu
Computer Sciences Department
University of Wisconsin-Madison
Madison, W.I. 53706
jerryzhu@cs.wisc.edu
Abstract
We present a graph-based semi-supervised
learning algorithm to address the senti-
ment analysis task of rating inference.
Given a set of documents (e.g., movie
reviews) and accompanying ratings (e.g.,
?4 stars?), the task calls for inferring nu-
merical ratings for unlabeled documents
based on the perceived sentiment ex-
pressed by their text. In particular, we
are interested in the situation where la-
beled data is scarce. We place this task
in the semi-supervised setting and demon-
strate that considering unlabeled reviews
in the learning process can improve rating-
inference performance. We do so by creat-
ing a graph on both labeled and unlabeled
data to encode certain assumptions for this
task. We then solve an optimization prob-
lem to obtain a smooth rating function
over the whole graph. When only lim-
ited labeled data is available, this method
achieves significantly better predictive ac-
curacy over other methods that ignore the
unlabeled examples during training.
1 Introduction
Sentiment analysis of text documents has received
considerable attention recently (Shanahan et al,
2005; Turney, 2002; Dave et al, 2003; Hu and
Liu, 2004; Chaovalit and Zhou, 2005). Unlike tra-
ditional text categorization based on topics, senti-
ment analysis attempts to identify the subjective sen-
timent expressed (or implied) in documents, such as
consumer product or movie reviews. In particular
Pang and Lee proposed the rating-inference problem
(2005). Rating inference is harder than binary posi-
tive / negative opinion classification. The goal is to
infer a numerical rating from reviews, for example
the number of ?stars? that a critic gave to a movie.
Pang and Lee showed that supervised machine learn-
ing techniques (classification and regression) work
well for rating inference with large amounts of train-
ing data.
However, review documents often do not come
with numerical ratings. We call such documents un-
labeled data. Standard supervised machine learning
algorithms cannot learn from unlabeled data. As-
signing labels can be a slow and expensive process
because manual inspection and domain expertise are
needed. Often only a small portion of the documents
can be labeled within resource constraints, so most
documents remain unlabeled. Supervised learning
algorithms trained on small labeled sets suffer in
performance. Can one use the unlabeled reviews to
improve rating-inference? Pang and Lee (2005) sug-
gested that doing so should be useful.
We demonstrate that the answer is ?Yes.? Our
approach is graph-based semi-supervised learning.
Semi-supervised learning is an active research area
in machine learning. It builds better classifiers or
regressors using both labeled and unlabeled data,
under appropriate assumptions (Zhu, 2005; Seeger,
2001). This paper contains three contributions:
? We present a novel adaptation of graph-based
semi-supervised learning (Zhu et al, 2003)
45
to the sentiment analysis domain, extending
past supervised learning work by Pang and
Lee (2005);
? We design a special graph which encodes
our assumptions for rating-inference problems
(section 2), and present the associated opti-
mization problem in section 3;
? We show the benefit of semi-supervised learn-
ing for rating inference with extensive experi-
mental results in section 4.
2 A Graph for Sentiment Categorization
The semi-supervised rating-inference problem is
formalized as follows. There are n review docu-
ments x1 . . . xn, each represented by some standard
feature representation (e.g., word-presence vectors).
Without loss of generality, let the first l ? n doc-
uments be labeled with ratings y1 . . . yl ? C. The
remaining documents are unlabeled. In our exper-
iments, the unlabeled documents are also the test
documents, a setting known as transduction. The
set of numerical ratings are C = {c1, . . . , cC}, with
c1 < . . . < cC ? R. For example, a one-star to
four-star movie rating system has C = {0, 1, 2, 3}.
We seek a function f : x 7? R that gives a contin-
uous rating f(x) to a document x. Classification is
done by mapping f(x) to the nearest discrete rating
in C. Note this is ordinal classification, which dif-
fers from standard multi-class classification in that
C is endowed with an order. In the following we use
?review? and ?document,? ?rating? and ?label? inter-
changeably.
We make two assumptions:
1. We are given a similarity measure wij ? 0
between documents xi and xj . wij should
be computable from features, so that we can
measure similarities between any documents,
including unlabeled ones. A large wij im-
plies that the two documents tend to express
the same sentiment (i.e., rating). We experi-
ment with positive-sentence percentage (PSP)
based similarity which is proposed in (Pang and
Lee, 2005), and mutual-information modulated
word-vector cosine similarity. Details can be
found in section 4.
2. Optionally, we are given numerical rating pre-
dictions y?l+1, . . . , y?n on the unlabeled doc-
uments from a separate learner, for in-
stance ?-insensitive support vector regression
(Joachims, 1999; Smola and Scho?lkopf, 2004)
used by (Pang and Lee, 2005). This acts
as an extra knowledge source for our semi-
supervised learning framework to improve
upon. We note our framework is general and
works without the separate learner, too. (For
this to work in practice, a reliable similarity
measure is required.)
We now describe our graph for the semi-
supervised rating-inference problem. We do this
piece by piece with reference to Figure 1. Our undi-
rected graph G = (V,E) has 2n nodes V , and
weighted edges E among some of the nodes.
? Each document is a node in the graph (open cir-
cles, e.g., xi and xj). The true ratings of these
nodes f(x) are unobserved. This is true even
for the labeled documents because we allow for
noisy labels. Our goal is to infer f(x) for the
unlabeled documents.
? Each labeled document (e.g., xj) is connected
to an observed node (dark circle) whose value
is the given rating yj . The observed node is
a ?dongle? (Zhu et al, 2003) since it connects
only to xj . As we point out later, this serves
to pull f(xj) towards yj . The edge weight be-
tween a labeled document and its dongle is a
large number M . M represents the influence
of yj : if M ? ? then f(xj) = yj becomes a
hard constraint.
? Similarly each unlabeled document (e.g., xi) is
also connected to an observed dongle node y?i,
whose value is the prediction of the separate
learner. Therefore we also require that f(xi)
is close to y?i. This is a way to incorporate mul-
tiple learners in general. We set the weight be-
tween an unlabeled node and its dongle arbi-
trarily to 1 (the weights are scale-invariant oth-
erwise). As noted earlier, the separate learner
is optional: we can remove it and still carry out
graph-based semi-supervised learning.
46
yi^ xi
xj
yj
labeled
reviews
unlabeled
reviews
1
a wij
b wij
 neighborsk?
M
neighborsk
Figure 1: The graph for semi-supervised rating in-
ference.
? Each unlabeled document xi is connected to
kNNL(i), its k nearest labeled documents.
Distance is measured by the given similarity
measure w. We want f(xi) to be consistent
with its similar labeled documents. The weight
between xi and xj ? kNNL(i) is a ? wij .
? Each unlabeled document is also connected to
k?NNU (i), its k? nearest unlabeled documents
(excluding itself). The weight between xi and
xj ? k?NNU (i) is b ? wij . We also want
f(xi) to be consistent with its similar unla-
beled neighbors. We allow potentially different
numbers of neighbors (k and k?), and different
weight coefficients (a and b). These parameters
are set by cross validation in experiments.
The last two kinds of edges are the key to semi-
supervised learning: They connect unobserved
nodes and force ratings to be smooth throughout the
graph, as we discuss in the next section.
3 Graph-Based Semi-Supervised Learning
With the graph defined, there are several algorithms
one can use to carry out semi-supervised learning
(Zhu et al, 2003; Delalleau et al, 2005; Joachims,
2003; Blum and Chawla, 2001; Belkin et al, 2005).
The basic idea is the same and is what we use in this
paper. That is, our rating function f(x) should be
smooth with respect to the graph. f(x) is not smooth
if there is an edge with large weight w between
nodes xi and xj , and the difference between f(xi)
and f(xj) is large. The (un)smoothness over the par-
ticular edge can be defined as w
(
f(xi) ? f(xj)
)2
.
Summing over all edges in the graph, we obtain the
(un)smoothness L(f) over the whole graph. We call
L(f) the energy or loss, which should be minimized.
Let L = 1 . . . l and U = l + 1 . . . n be labeled
and unlabeled review indices, respectively. With the
graph in Figure 1, the loss L(f) can be written as
?
i?L
M(f(xi)? yi)
2 +
?
i?U
(f(xi)? y?i)
2
+
?
i?U
?
j?kNNL(i)
awij(f(xi)? f(xj))
2
+
?
i?U
?
j?k?NNU (i)
bwij(f(xi)? f(xj))
2. (1)
A small loss implies that the rating of an unlabeled
review is close to its labeled peers as well as its un-
labeled peers. This is how unlabeled data can par-
ticipate in learning. The optimization problem is
minf L(f). To understand the role of the parame-
ters, we define ? = ak + bk? and ? = ba , so that
L(f) can be written as
?
i?L
M(f(xi)? yi)
2 +
?
i?U
[
(f(xi)? y?i)
2
+
?
k + ?k?
( ?
j?kNNL(i)
wij(f(xi)? f(xj))
2
+
?
j?k?NNU (i)
?wij(f(xi)? f(xj))
2
)]
. (2)
Thus ? controls the relative weight between labeled
neighbors and unlabeled neighbors; ? is roughly
the relative weight given to semi-supervised (non-
dongle) edges.
We can find the closed-form solution to the opti-
mization problem. Defining an n? n matrix W? ,
W?ij =
?
?
?
0, i ? L
wij , j ? kNNL(i)
?wij , j ? k?NNU (i).
(3)
Let W = max(W? , W?>) be a symmetrized version
of this matrix. Let D be a diagonal degree matrix
with
Dii =
n?
j=1
Wij . (4)
Note that we define a node?s degree to be the sum of
its edge weights. Let ? = D ?W be the combina-
torial Laplacian matrix. Let C be a diagonal dongle
47
weight matrix with
Cii =
{
M, i ? L
1, i ? U
. (5)
Let f = (f(x1), . . . , f(xn))> and y =
(y1, . . . , yl, y?l+1, . . . , y?n)>. We can rewrite L(f) as
(f ? y)>C(f ? y) +
?
k + ?k?
f>?f . (6)
This is a quadratic function in f . Setting the gradient
to zero, ?L(f)/?f = 0 , we find the minimum loss
function
f =
(
C +
?
k + ?k?
?
)?1
Cy. (7)
Because C has strictly positive eigenvalues, the in-
verse is well defined. All our semi-supervised learn-
ing experiments use (7) in what follows.
Before moving on to experiments, we note an
interesting connection to the supervised learning
method in (Pang and Lee, 2005), which formulates
rating inference as a metric labeling problem (Klein-
berg and Tardos, 2002). Consider a special case of
our loss function (1) when b = 0 and M ? ?. It
is easy to show for labeled nodes j ? L, the opti-
mal value is the given label: f(xj) = yj . Then the
optimization problem decouples into a set of one-
dimensional problems, one for each unlabeled node
i ? U : Lb=0,M??(f(xi)) =
(f(xi)? y?i)
2 +
?
j?kNNL(i)
awij(f(xi)? yj)
2. (8)
The above problem is easy to solve. It corresponds
exactly to the supervised, non-transductive version
of metric labeling, except we use squared differ-
ence while (Pang and Lee, 2005) used absolute dif-
ference. Indeed in experiments comparing the two
(not reported here), their differences are not statis-
tically significant. From this perspective, our semi-
supervised learning method is an extension with in-
teracting terms among unlabeled data.
4 Experiments
We performed experiments using the movie re-
view documents and accompanying 4-class (C =
{0, 1, 2, 3}) labels found in the ?scale dataset v1.0?
available at http://www.cs.cornell.edu/people/pabo/
movie-review-data/ and first used in (Pang and Lee,
2005). We chose 4-class instead of 3-class labeling
because it is harder. The dataset is divided into four
author-specific corpora, containing 1770, 902, 1307,
and 1027 documents. We ran experiments individu-
ally for each author. Each document is represented
as a {0, 1} word-presence vector, normalized to sum
to 1.
We systematically vary labeled set size |L| ?
{0.9n, 800, 400, 200, 100, 50, 25, 12, 6} to observe
the effect of semi-supervised learning. |L| = 0.9n
is included to match 10-fold cross validation used
by (Pang and Lee, 2005). For each |L| we run 20
trials where we randomly split the corpus into la-
beled and test (unlabeled) sets. We ensure that all
four classes are represented in each labeled set. The
same random splits are used for all methods, allow-
ing paired t-tests for statistical significance. All re-
ported results are average test set accuracy.
We compare our graph-based semi-supervised
method with two previously studied methods: re-
gression and metric labeling as in (Pang and Lee,
2005).
4.1 Regression
We ran linear ?-insensitive support vector regression
using Joachims? SVMlight package (1999) with all
default parameters. The continuous prediction on a
test document is discretized for classification. Re-
gression results are reported under the heading ?reg.?
Note this method does not use unlabeled data for
training.
4.2 Metric labeling
We ran Pang and Lee?s method based on metric la-
beling, using SVM regression as the initial label
preference function. The method requires an item-
similarity function, which is equivalent to our simi-
larity measure wij . Among others, we experimented
with PSP-based similarity. For consistency with
(Pang and Lee, 2005), supervised metric labeling re-
sults with this measure are reported under ?reg+PSP.?
Note this method does not use unlabeled data for
training either.
PSPi is defined in (Pang and Lee, 2005) as the
percentage of positive sentences in review xi. The
similarity between reviews xi, xj is the cosine angle
48
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
fine?grain rating
me
an 
and 
stan
dard
 dev
iatio
n of 
PSP
Positive?sentence percentage (PSP) statistics
 
 Author (a)Author (b)Author (c)Author (d)
Figure 2: PSP for reviews expressing each fine-grain
rating. We identified positive sentences using SVM
instead of Na??ve Bayes, but the trend is qualitatively
the same as in (Pang and Lee, 2005).
between the vectors (PSPi, 1?PSPi) and (PSPj , 1?
PSPj). Positive sentences are identified using a bi-
nary classifier trained on a separate ?snippet data
set? located at the same URL as above. The snippet
data set contains 10662 short quotations taken from
movie reviews appearing on the rottentomatoes.com
Web site. Each snippet is labeled positive or neg-
ative based on the rating of the originating review.
Pang and Lee (2005) trained a Na??ve Bayes classi-
fier. They showed that PSP is a (noisy) measure for
comparing reviews?reviews with low ratings tend
to receive low PSP scores, and those with higher
ratings tend to get high PSP scores. Thus, two re-
views with a high PSP-based similarity are expected
to have similar ratings. For our experiments we de-
rived PSP measurements in a similar manner, but us-
ing a linear SVM classifier. We observed the same
relationship between PSP and ratings (Figure 2).
The metric labeling method has parameters
(the equivalent of k, ? in our model). Pang and
Lee tuned them on a per-author basis using cross
validation but did not report the optimal parameters.
We were interested in learning a single set of
parameters for use with all authors. In addition,
since we varied labeled set size, it is convenient
to tune c = k/|L|, the fraction of labeled reviews
used as neighbors, instead of k. We then used
the same c, ? for all authors at all labeled set
sizes in experiments involving PSP. Because c is
fixed, k varies directly with |L| (i.e., when less
labeled data is available, our algorithm considers
fewer nearby labeled examples). In an attempt to
reproduce the findings in (Pang and Lee, 2005),
we tuned c, ? with cross validation. Tuning ranges
are c ? {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and ? ?
{0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}.
The optimal parameters we found are c = 0.2 and
? = 1.5. (In section 4.4, we discuss an alternative
similarity measure, for which we re-tuned these
parameters.)
Note that we learned a single set of shared param-
eters for all authors, whereas (Pang and Lee, 2005)
tuned k and ? on a per-author basis. To demonstrate
that our implementation of metric labeling produces
comparable results, we also determined the optimal
author-specific parameters. Table 1 shows the ac-
curacy obtained over 20 trials with |L| = 0.9n for
each author, using SVM regression, reg+PSP using
shared c, ? parameters, and reg+PSP using author-
specific c, ? parameters (listed in parentheses). The
best result in each row of the table is highlighted in
bold. We also show in bold any results that cannot
be distinguished from the best result using a paired
t-test at the 0.05 level.
(Pang and Lee, 2005) found that their metric la-
beling method, when applied to the 4-class data we
are using, was not statistically better than regres-
sion, though they observed some improvement for
authors (c) and (d). Using author-specific parame-
ters, we obtained the same qualitative result, but the
improvement for (c) and (d) appears even less sig-
nificant in our results. Possible explanations for this
difference are the fact that we derived our PSP mea-
surements using an SVM classifier instead of an NB
classifier, and that we did not use the same range of
parameters for tuning. The optimal shared parame-
ters produced almost the same results as the optimal
author-specific parameters, and were used in subse-
quent experiments.
4.3 Semi-Supervised Learning
We used the same PSP-based similarity measure
and the same shared parameters c = 0.2, ? =
1.5 from our metric labeling experiments to per-
form graph-based semi-supervised learning. The
results are reported as ?SSL+PSP.? SSL has three
49
reg+PSP reg+PSP
Author reg (shared) (specific)
(a) 0.592 0.592 0.592 (0.05, 0.01)
(b) 0.501 0.498 0.496 (0.05, 3.50)
(c) 0.592 0.589 0.593 (0.15, 1.50)
(d) 0.496 0.498 0.500 (0.05, 3.00)
Table 1: Accuracy using shared (c = 0.2, ? = 1.5)
vs. author-specific parameters, with |L| = 0.9n.
additional parameters k?, ?, and M . Again
we tuned k?, ? with cross validation. Tuning
ranges are k? ? {2, 3, 5, 10, 20} and ? ?
{0.001, 0.01, 0.1, 1.0, 10.0}. The optimal parame-
ters are k? = 5 and ? = 1.0. These were used for all
authors and for all labeled set sizes. Note that unlike
k = c|L|, which decreases as the labeled set size de-
creases, we let k? remain fixed for all |L|. We set M
arbitrarily to a large number 108 to ensure that the
ratings of labeled reviews are respected.
4.4 Alternate Similarity Measures
In addition to using PSP as a similarity measure be-
tween reviews, we investigated several alternative
similarity measures based on the cosine of word
vectors. Among these options were the cosine be-
tween the word vectors used to train the SVM re-
gressor, and the cosine between word vectors con-
taining only words with high (top 1000 or top 5000)
mutual information values. The mutual information
is computed with respect to the positive and negative
classes in the 10662-document ?snippet data set.?
Finally, we experimented with using as a similarity
measure the cosine between word vectors containing
all words, each weighted by its mutual information.
We found this measure to be the best among the op-
tions tested in pilot trial runs using the metric label-
ing algorithm. Specifically, we scaled the mutual in-
formation values such that the maximum value was
one. Then, we used these values as weights for the
corresponding words in the word vectors. For words
in the movie review data set that did not appear in
the snippet data set, we used a default weight of zero
(i.e., we excluded them. We experimented with set-
ting the default weight to one, but found this led to
inferior performance.)
We repeated the experiments described in sec-
tions 4.2 and 4.3 with the only difference being
that we used the mutual-information weighted word
vector similarity instead of PSP whenever a simi-
larity measure was required. We repeated the tun-
ing procedures described in the previous sections.
Using this new similarity measure led to the opti-
mal parameters c = 0.1, ? = 1.5, k? = 5, and
? = 10.0. The results are reported under ?reg+WV?
and ?SSL+WV,? respectively.
4.5 Results
We tested the five algorithms for all four authors us-
ing each of the nine labeled set sizes. The results
are presented in table 2. Each entry in the table rep-
resents the average accuracy across 20 trials for an
author, a labeled set size, and an algorithm. The best
result in each row is highlighted in bold. Any results
on the same row that cannot be distinguished from
the best result using a paired t-test at the 0.05 level
are also bold.
The results indicate that the graph-based semi-
supervised learning algorithm based on PSP simi-
larity (SSL+PSP) achieved better performance than
all other methods in all four author corpora when
only 200, 100, 50, 25, or 12 labeled documents
were available. In 19 out of these 20 learning sce-
narios, the unlabeled set accuracy by the SSL+PSP
algorithm was significantly higher than all other
methods. While accuracy generally degraded as we
trained on less labeled data, the decrease for the SSL
approach was less severe through the mid-range la-
beled set sizes. SSL+PSP remains among the best
methods with only 6 labeled examples.
Note that the SSL algorithm appears to be quite
sensitive to the similarity measure used to form the
graph on which it is based. In the experiments where
we used mutual-information weighted word vector
similarity (reg+WV and SSL+WV), we notice that
reg+WV remained on par with reg+PSP at high la-
beled set sizes, whereas SSL+WV appears signif-
icantly worse in most of these cases. It is clear
that PSP is the more reliable similarity measure.
SSL uses the similarity measure in more ways than
the metric labeling approaches (i.e., SSL?s graph is
denser), so it is not surprising that SSL?s accuracy
would suffer more with an inferior similarity mea-
sure.
Unfortunately, our SSL approach did not do as
well with large labeled set sizes. We believe this
50
PSP word vector
|L| regression reg+PSP SSL+PSP reg+WV SSL+WV
A
ut
ho
r(
a)
1593 0.592 0.592 0.546 0.592 0.544
800 0.553 0.554 0.534 0.553 0.517
400 0.522 0.525 0.526 0.522 0.497
200 0.494 0.498 0.521 0.494 0.472
100 0.463 0.477 0.511 0.462 0.450
50 0.439 0.458 0.499 0.438 0.429
25 0.408 0.421 0.465 0.400 0.404
12 0.401 0.378 0.451 0.335 0.398
6 0.390 0.359 0.422 0.314 0.389
A
ut
ho
r(
b)
811 0.501 0.498 0.481 0.503 0.473
800 0.501 0.497 0.478 0.503 0.474
400 0.471 0.471 0.465 0.471 0.450
200 0.447 0.449 0.452 0.447 0.429
100 0.415 0.423 0.443 0.415 0.397
50 0.388 0.396 0.434 0.387 0.376
25 0.373 0.380 0.418 0.364 0.367
12 0.354 0.360 0.399 0.313 0.353
6 0.348 0.352 0.380 0.302 0.347
A
ut
ho
r(
c)
1176 0.592 0.589 0.566 0.594 0.514
800 0.579 0.585 0.559 0.579 0.509
400 0.550 0.556 0.544 0.551 0.491
200 0.513 0.519 0.532 0.513 0.479
100 0.484 0.495 0.521 0.484 0.466
50 0.462 0.476 0.504 0.461 0.456
25 0.459 0.472 0.484 0.439 0.454
12 0.420 0.405 0.477 0.356 0.414
6 0.320 0.382 0.366 0.334 0.322
A
ut
ho
r(
d)
924 0.496 0.498 0.495 0.499 0.490
800 0.500 0.501 0.495 0.504 0.483
400 0.474 0.478 0.486 0.477 0.463
200 0.459 0.459 0.468 0.459 0.445
100 0.444 0.445 0.460 0.444 0.437
50 0.429 0.431 0.445 0.429 0.428
25 0.411 0.411 0.425 0.400 0.409
12 0.393 0.362 0.405 0.335 0.391
6 0.393 0.357 0.403 0.312 0.393
Table 2: 20-trial average unlabeled set accuracy for each author across different labeled set sizes and meth-
ods. In each row, we list in bold the best result and any results that cannot be distinguished from it with a
paired t-test at the 0.05 level.
51
is due to two factors: a) the baseline SVM regres-
sor trained on a large labeled set can achieve fairly
high accuracy for this difficult task without consid-
ering pairwise relationships between examples; b)
PSP similarity is not accurate enough. Gain in vari-
ance reduction achieved by the SSL graph is offset
by its bias when labeled data is abundant.
5 Discussion
We have demonstrated the benefit of using unla-
beled data for rating inference. There are several
directions to improve the work: 1. We will inves-
tigate better document representations and similar-
ity measures based on parsing and other linguis-
tic knowledge, as well as reviews? sentiment pat-
terns. For example, several positive sentences fol-
lowed by a few concluding negative sentences could
indicate an overall negative review, as observed in
prior work (Pang and Lee, 2005). 2. Our method
is transductive: new reviews must be added to the
graph before they can be classified. We will extend
it to the inductive learning setting based on (Sind-
hwani et al, 2005). 3. We plan to experiment with
cross-reviewer and cross-domain analysis, such as
using a model learned on movie reviews to help clas-
sify product reviews.
Acknowledgment
We thank Bo Pang, Lillian Lee and anonymous re-
viewers for helpful comments.
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2005. On manifold regularization. In Proceedings of
the Tenth International Workshop on Artificial Intelli-
gence and Statistics (AISTAT 2005).
A. Blum and S. Chawla. 2001. Learning from labeled
and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning.
Pimwadee Chaovalit and Lina Zhou. 2005. Movie re-
view mining: a comparison between supervised and
unsupervised classification approaches. In HICSS.
IEEE Computer Society.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
WWW ?03: Proceedings of the 12th international con-
ference on World Wide Web, pages 519?528.
Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux.
2005. Efficient non-parametric function induction in
semi-supervised learning. In Proceedings of the Tenth
International Workshop on Artificial Intelligence and
Statistics (AISTAT 2005).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD ?04,
the ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177. ACM
Press.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML-03, 20th
International Conference on Machine Learning.
Jon M. Kleinberg and ?Eva Tardos. 2002. Approxima-
tion algorithms for classification problems with pair-
wise relationships: metric labeling and markov ran-
dom fields. J. ACM, 49(5):616?639.
Bo Pang and Lillian Lee. 2005. Seeing stars: exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Matthias Seeger. 2001. Learning with labeled and unla-
beled data. Technical report, University of Edinburgh.
James Shanahan, Yan Qu, and Janyce Wiebe, editors.
2005. Computing attitude and affect in text. Springer,
Dordrecht, The Netherlands.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. Beyond the point cloud: from transductive to
semi-supervised learning. In ICML05, 22nd Interna-
tional Conference on Machine Learning, Bonn, Ger-
many.
A. J. Smola and B. Scho?lkopf. 2004. A tutorial on
support vector regression. Statistics and Computing,
14:199?222.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of ACL-02, 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 417?424.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian fields
and harmonic functions. In ICML-03, 20th Interna-
tional Conference on Machine Learning.
Xiaojin Zhu. 2005. Semi-supervised learning lit-
erature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
52
Proceedings of the Workshop on Computational Approaches to Figurative Language, pages 13?20,
Rochester, NY, April 26, 2007. c?2007 Association for Computational Linguistics
Hunting Elusive Metaphors Using Lexical Resources
Saisuresh Krishnakumaran?
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706
ksai@cs.wisc.edu
Xiaojin Zhu
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706
jerryzhu@cs.wisc.edu
Abstract
In this paper we propose algorithms
to automatically classify sentences into
metaphoric or normal usages. Our algo-
rithms only need the WordNet and bigram
counts, and does not require training. We
present empirical results on a test set de-
rived from the Master Metaphor List. We
also discuss issues that make classification
of metaphors a tough problem in general.
1 Introduction
Metaphor is an interesting figure of speech which
expresses an analogy between two seemingly un-
related concepts. Metaphoric usages enhance the
attributes of the source concept by comparing it
with the attributes of the target concept. Abstrac-
tions and enormously complex situations are rou-
tinely understood via metaphors (Lakoff and John-
son, 1980). Metaphors begin their lives as Novel
Poetic Creations with marked rhetoric effects whose
comprehension requires special imaginative leap.
As time goes by, they become part of general use
and their comprehension becomes automatic and
idiomatic and rhetoric effect is dulled (Nunberg,
1987). We term such metaphors whose idiomatic ef-
fects are dulled because of common usage as dead
metaphors while metaphors with novel usages as
live metaphors. In this paper we are interested only
in identifying live metaphors.
? The first author is currently affiliated with Google Inc,
Mountain View, CA.
Metaphors have interesting applications in many
NLP problems like machine translation, text sum-
marization, information retrieval and question an-
swering. Consider the task of summarizing a parable
which is a metaphoric story with a moral. The best
summary of a parable is the moral. Paraphrasing a
metaphoric passage like a parable is difficult with-
out understanding the metaphoric uses. The per-
formance of the conventional summarizing systems
will be ineffective because they cannot identify such
metaphoric usages. Also it is easy to create novel
and interesting uses of metaphors as long as one con-
cept is explained in terms of another concept. The
performance of machine translation systems will be
affected in such cases especially if they have not en-
countered such metaphoric uses before.
Metaphor identification in text documents is,
however, complicated by issues including con-
text sensitiveness, emergence of novel metaphoric
forms, and the need for semantic knowledge about
the sentences. Metaphoric appeal differs across lan-
guage or people?s prior exposure to such usages. In
addition, as (Gibbs, 1984) points out, literal and fig-
urative expressions are end points of a single con-
tinuum along which metaphoricity and idiomaticity
are situated, thereby making clear demarcation of
metaphoric and normal usages fuzzy.
We discuss many such issues that make the task
of classifying sentences into metaphoric or non-
metaphoric difficult. We then focuses on a sub-
set of metaphoric usages involving the nouns in a
sentence. In particular, we identify the subject-
object, verb-noun and adjective-noun relationships
in sentences and classify them as metaphoric or
13
non-metaphoric. Extensions to other metaphoric
types will be part of future work. Our algorithms
use the hyponym relationship in WordNet (Fell-
baum, 1998), and word bigram counts, to predict the
metaphors. In doing so we circumvent two issues:
the absence of labeled training data, and the lack of
clear features that are indicative of metaphors.
The paper is organized as follows. Section 2
presents interesting observations that were made
during the initial survey, and presents examples
that makes metaphor identification hard. Sec-
tion 3 discusses our main techniques for identifying
metaphors in text documents. Section 4 analyzes the
effect of the techniques. Section 5 discusses relevant
prior work in the area of metaphor processing and
identification. Finally we conclude in Section 6.
2 Challenges in Metaphor Identification
In this section we present some issues that make
metaphor identification hard.
2.1 Context Sensitivity
Some metaphoric usages are sensitive to the context
in which they occur. For example, the following
sentence can act as a normal sentence as well as a
metaphoric sentence.
Men are animals.
It is a normal sentence in a biology lecture because
all human beings fall under the animal kingdom.
However this is a metaphoric sentence in a social
conversation when it refers to animal qualities. Also
the word ?Men? has two different senses in WordNet
and hence it is necessary to disambiguate the senses
based on the context. Sense disambiguation is be-
yond the scope of this paper.
2.2 Pronoun Resolution
Consider the following sentence,
This homework is a breeze. The previous
one was on calculus. It was a tornado.
The techniques we discuss in this paper can clas-
sify the reference to ?breeze? as metaphoric. In or-
der to correctly classify the reference to ?tornado?
as metaphoric, however, the system needs to resolve
the reference to the pronoun ?It?. Strictly speak-
ing, this example might be solved without resolu-
tion because any of the potential antecedents render
the sentence metaphoric, but in general resolution is
necessary.
2.3 Word Usages
Consider the following two sentences,
He is a Gandhi. vs. He is Gandhi.
The first sentence is a metaphor which attributes the
qualities of Gandhi to the actor, while the second
sentence is a normal one. Here the article ?a? dis-
tinguishes the first sentence from the second. Sim-
ilarly, in the following example, the phrase ?among
men? helps in making the second usage metaphoric.
He is a king. vs. He is a king among men.
A comprehensive list of such uses are not known and
incorporating all such grammatical features would
make the system quite complex.
2.4 Parser Issues
The techniques that we propose work on the parsed
sentences. Hence the accuracy of our technique is
highly dependent on the accuracy of the parser.
2.5 Metaphoric Usages in WordNet
Some metaphoric senses of nouns are already part of
the WordNet.
He is a wolf.
The metaphoric sense of ?wolf? is directly men-
tioned in the WordNet. We call such usages as ?dead
metaphors? because they are so common and are al-
ready part of the lexicon. In this paper we are inter-
ested in identifying only novel usages of metaphors.
3 Noun-Form Metaphors
We restrict ourselves to metaphoric usages involving
nouns. In particular, we study the effect of verbs and
adjectives on the nouns in a sentence. We categorize
the verb-noun relationship in sentences as Type I and
Type II based on the verb. We call the adjective-
noun relationship as Type III, see Table 1.
For Type I, the verb is one of the ?be? form verbs
like ?is?, ?are?, ?am?, ?was?, etc. An example of Type
I form metaphor is
14
Table 1: Terminology
Sentence Type Relationship
Type I Subject IS-A Object
Type II Verb acting on Noun
(verb not ?be?)
Type III Adjective acting on Noun
He is a brave lion.
An example of Type II form metaphor is
He planted good ideas in their minds.
An example for Type III form metaphor is
He has a fertile imagination.
We use two different approaches for Type I vs.
Types II, III. In Type I form we are interested in
the relationship between the subject and the object.
We use a hyponym heuristic. In Types II and III,
we are interested in the subject-verb, verb-object,
or adjective-noun relations. We use hyponym to-
gether with word co-occurrence information, in this
case bigrams from the Web 1T corpus (Brants and
Franz, 2006). Sections 3.1 and 3.2 discuss the two
algorithms, respectively. We use a parser (Klein and
Manning, 2003) to obtain the relationships between
nouns, verbs and adjectives in a sentence.
3.1 Identifying Type I metaphors
We identify the WordNet hyponym relationship (or
the lack thereof) between the subject and the object
in a Type I sentence. We classify the sentence as
metaphoric, if the subject and object does not have
a hyponym relation. A hyponym relation exists be-
tween a pair of words if and only if one word is a
subclass of another word. We motivate this idea us-
ing some examples. Let us consider a normal sen-
tence with a subject-object relationship governed by
a ?be? form verb, ?is?.
A lion is a wild animal.
The subject-verb-object relationship of this normal
sentence is shown in Figure 1.
The subject and the object in the above example is
governed by ?IS-A? relationship. Thus, Lion ?IS-A?
type of animal. The ?IS-A? relationship is captured
Figure 1: The Subject-Verb-Object relationship for
?A lion is a wild animal.?
as the ?hyponym? relationship in WordNet, where
?Lion? is the hyponym of ?animal?. Consider another
example,
He is a scientist.
Here the object ?scientist? is the occupation of the
subject ?He?, which we change to ?person?. ?Sci-
entist? is a hyponym of ?person? in WordNet. The
above two examples show that we expect a subject-
object hyponym relation for normal Type I relations.
On the other hand, consider a metaphoric example in
Type I form,
All the world?s a stage.
- William Shakespeare
The subject-verb-object relationship is represented
by Figure 2.
Figure 2: The Subject-Verb-Object relationship for
?All the world is a stage.?
There is a subject-object relation between ?World?
and ?Stage?, but they do not hold a hyponym re-
lation in WordNet. This is an important observa-
tion which we use in classifying relationships of this
15
form. Consider another example with complex sen-
tences,
Men are April when they woo, December
when they wed. Maids are May when
they are maids, but the sky changes
when they are wives.
-Shakespeare?s ?As You Like It?.
In this case, there are two explicit subject-object
relations, namely Men-April and Maids-May. The
WordNet hyponym relation does not exist between
either pair.
From the examples considered above, it is seems
that when a hyponym relation exists between the
subject and the object, the relationship is normal,
and metaphoric otherwise. The effectiveness of this
approach is analyzed in detail in Section 4. The
pseudo code for classifying Type I relations is given
below:
1. Parse the sentences and get al R ? {subject,
be, object} relations in those sentences.
2. for each relation Rsub,obj
if Hyponym(sub,obj) = true
then Rsub,obj is normal usage
else Rsub,obj is a metaphoric relation
3. All sentences with at least one metaphoric rela-
tion is classified as metaphoric.
3.2 Identifying Type II and Type III metaphors
We use a two dimensional V/A-N co-occurrence ma-
trix, in addition to WordNet, for detecting Type II
and Type III metaphors. V/A-N matrix stands for
Verb/Adjective-Noun matrix, which is a two dimen-
sional matrix with verbs or adjectives along one di-
mension, and nouns along the other. The entries
are co-occurrence frequency of the word pair, from
which we may estimate the conditional probabil-
ity p(wn|w) for a noun wn and a verb or adjec-
tive w. Ideally the matrix should be constructed
from a parsed corpus, so that we can identify V/A-
N pairs from their syntactic roles. However pars-
ing a large corpus would be prohibitively expensive.
As a practical approximation, we use bigram counts
from the Web 1T corpus (Brants and Franz, 2006).
Web 1T corpus consists of English word n-grams
(up to 5-grams) generated from approximately 1 tril-
lion word tokens of text from public Web pages. In
this paper we use the bigram data in which a noun
follows either a verb or an adjective. We note that
this approximation thus misses, for example, the pair
(plant, idea) in phrases like ?plant an idea?. Nonethe-
less, the hope is that the corpus makes it up by sheer
size.
3.2.1 Type II metaphors
We discuss the metaphoric relationship between
a verb-noun pair (wv, wn). The idea is that if nei-
ther wn nor its hyponyms or hypernyms co-occur
frequently with wv, then the pair is a novel usage,
and we classify the pair as metaphoric. To this end,
we estimate the conditional probability p(wh|wv) =
count(wv, wh)/count(wv) from the V/A-N matrix,
where wh is wn itself, or one of its hyponyms / hy-
pernyms. If at least one of these wh has high enough
conditional probability as determined by a thresh-
old, we classify it as normal usage, and metaphoric
otherwise. Consider the following example
He planted good ideas in their minds.
The verb ?planted? acts on the noun ?ideas? and
makes the sentence metaphoric. In our corpus the
objects that occur more frequently with the verb
?planted? are ?trees?, ?bomb? and ?wheat?, etc. Nei-
ther the noun ?ideas? nor its hyponyms / hypernyms
occurs frequently enough with ?planted?. Hence we
predict this verb-object relationship as metaphoric.
The pseudo code for classifying Type II metaphors
is given below:
1. Parse the sentences and obtain all R ? {verb,
noun} relations in those sentences.
2. for each relation Rverb,noun
Sort all nouns w in the vocabulary by de-
creasing p(w|verb). Take the smallest set of
top k nouns whose conditional probability sum
? threshold T .
if ?wh such that wh is related to noun by
the hyponym relation in WordNet, and wh ?
top k words above,
then Rverb,noun is normal usage
else Rverb,noun is a Type II metaphoric re-
lation
16
3. All sentences with at least one metaphoric rela-
tionship is classified as a metaphor.
3.2.2 Type III metaphors
The technique for detecting the Type III
metaphors is the same as the technique for detecting
the Type II metaphors except that it operates on dif-
ferent relationship. Here we compare the Adjective-
Noun relationship instead of the Verb-Noun rela-
tionship. For example,
He has a fertile imagination.
Here the adjective ?fertile? acts on the noun ?imagi-
nation? to make it metaphoric. The nouns that occur
frequently with the ?fertile? in our corpus are ?soil?,
?land?, ?territory?, and ?plains?, etc. Comparison of
the WordNet hierarchies of the noun ?imagination?
with each of these nouns will show that there does
not exist any hyponym relation between ?imagina-
tion? and any of these nouns. Hence we classify
them as metaphors. As another example,
TV is an idiot box.
The adjective ?idiot? qualifies nouns related to peo-
ple such as ?boy?, ?man?, etc. that are unrelated to
the noun ?box?. Thus we classify it as a Type III
metaphor.
4 Experimental Results
We experimented with the Berkeley Master
Metaphor List (Lakoff and Johnson, 1980) to
compute the performance of our techniques. The
Berkeley Master Metaphor List is a collection of
nearly 1728 unique sentences and phrases. We
corrected some typos and spelling errors in the
Master list and expanded phrases to complete
sentences. The list has many metaphoric uses
which has become very common usages in today?s
standards, and thus no longer have any rhetoric
effects. Therefore, we manually label the sentences
in the Master List into 789 ?live metaphors? and
the remaining ones ?dead metaphors? as the ground
truth1.
Table 2 shows the initial performance of the
Type I algorithm. There are 129 sentences in the
1Our processed and labeled dataset is available at
http://www.cs.wisc.edu/?ksai/publications/
2007/HLT_NAACL_metaphors/metaphors.html
Master List that contain subject-be-object form. Our
algorithm has a precision of 70% and a recall of 61%
with respect to the live/dead labels. Note that al-
though the accuracy is 58%, the algorithm is bet-
ter than a random classification in terms of precision
and recall. One thing to note is that our negative
examples are (subjectively labeled) dead metaphors.
We thus expect the task to be harder than with ran-
dom non-metaphoric sentences. Another point to
note here is that the live/dead labels are on sentences
and not on particular phrases with type I relations.
A sentence can contain more than one phrases with
various types. Therefore this result does not give a
complete picture of our algorithm.
Table 2: Type I Performance
Predicted as Predicted as
Metaphoric Normal
Annotated as live 50 32
Annotated as dead 22 25
A few interesting metaphors detected by our algo-
rithm are as follows:
Lawyers are real sharks.
Smog pollution is an environmental
malaise.
Some false negatives are due to phrases qualifying
the object of the sentence as in the following exam-
ple,
He is a budding artist.
There is a Type I relation in this sentence because
the subject ?He? and the object ?artist? are related
by the ?be? form verb ?is?. In this case, the Type I
algorithm compares the hyponyms relation between
?person? and ?artist? and declares it as a normal sen-
tence. However the adjective ?budding? adds Type
III figurative meaning to this sentence. Therefore al-
though the Type I relation is normal, there are other
features in the sentences that make it metaphoric.
We observed that most of false negatives that are
wrongly classified because of the above reason have
pronoun subject like ?he?, ?she? etc.
Another major source of issue is the occurrences
of pronoun ?it? which is hard to resolve. We replaced
it by ?entity?, which is the root of WordNet, when
17
comparing the hyponyms. ?Entity? matches the hy-
ponym relation with any other noun and hence all
these sentences with ?it? as the subject are classified
as normal sentences.
Table 3: Type I Performance for sentences with non-
pronoun subject
Predicted as Predicted as
Metaphoric Normal
Annotated as live 40 1
Annotated as dead 19 4
Table 3 shows the performance of our Type I al-
gorithm for sentences with non-pronoun subjects.
It clearly shows that the performance in Table 2 is
affected by sentences with pronoun subjects as ex-
plained in the earlier paragraphs.
In some cases, prepositional phrases affects the
performance of our algorithm. Consider the follow-
ing example,
He is the child of evil.
Here the phrase ?child of evil? is metaphoric. But the
parser identifies a subject-be-object relationship be-
tween ?He? and ?child? and our algorithm compares
the hyponym relation between ?person? and ?child?
and declares it as a normal sentence.
Our current algorithm does not deal with cases
like the following example
The customer is a scientist. vs. The cus-
tomer is king.
Since there is no direct hyponym relation between
scientist/king with customer we declare both these
sentences as metaphors although only the latter is.
Unlike the algorithm for Type I, there is a thresh-
old T to be set for Type II and III algorithm. By
changing T , we are able to plot a precision recall
curve. Figure 3 and figure 4 show the precision re-
call graph for Type II and Type III relations respec-
tively. Figure 5 shows the overall precision recall
graph for all three types put together.
False positives in Type II and Type III were due
to very general verbs and adjectives. These verbs
and adjectives can occur with a large number of
nouns, and tend to produce low conditional prob-
abilities even for normal nouns. Thereby they are
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Type II Precision  Recall graph
Type II
Figure 3: Precision Recall curve for Type II rela-
tions.
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Type III Precision  Recall graph
Type III
Figure 4: Precision Recall curve for Type III rela-
tions.
often mistakenly classified as metaphoric relations.
We expect the performance to improve if these gen-
eral verbs and adjectives are handled properly. Some
general verbs include ?gave?, ?made?, ?has?, etc., and
similarity general adjectives include ?new?, ?good?,
?many?, ?more?, etc. The plot for Type III is more
random.
Most errors can be attributed to some of the fol-
lowing reasons:
? As mentioned in the challenges section, the
parser is not very accurate. For example,
They battled each other over the
chess board every week.
Here the parser identifies the verb-object rela-
tion as ( battled , week ), which is not correct.
18
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Combined Precision  Recall graph
All types combined
Figure 5: Overall Precision Recall curve for all three
types combined.
? Pronoun resolution: As discussed earlier, the
pronoun ?it? is not resolved and hence they in-
troduce additional source of errors.
? Manual annotations could be wrong. In our ex-
periment we have used only two annotators, but
having more would have increased the confi-
dence in the labels.
? Many of the verb-noun forms are most natu-
rally captured by trigrams instead of bigram.
For example, (developed , attachment) most
likely occurs in a corpus as ?developed an at-
tachment? or ?developed the attachment?. Our
bigram approach can fail here.
? Sense disambiguation: We don?t disambiguate
senses while comparing the WordNet relations.
This increases our false negatives.
? Also as mentioned earlier, the labels are on
sentences and not on the typed relationships.
Therefore even though a sentence has one or
more of the noun form types, those may be
normal relationships while the whole sentence
may be metaphoric because of other types.
Note, however, that some of these mismatches
are corrected for the ?All types combined? re-
sult.
5 Related Work
There has been a long history of research in
metaphors. We briefly review some of them here.
One thing that sets our work apart is that most pre-
vious literatures in this area tend to give little em-
pirical evaluation of their approaches. In contrast, in
this study we provide detailed analysis of the effec-
tiveness of our approaches.
(Fass and Wilks, 1983) proposes the use of pref-
erence semantics for metaphor recognition. Tech-
niques for automatically detecting selections prefer-
ences have been discussed in (McCarthy and Car-
rol, 2003) and (Resnik, 1997). Type II and Type III
approaches discussed in this paper uses both these
ideas for detecting live metaphors. Fass (Fass, 1991)
uses selectional preference violation technique to
detect metaphors. However they rely on hand-coded
declarative knowledge bases. Our technique de-
pends only on WordNet and we use selection prefer-
ence violation based on the knowledge learned from
the bigram frequencies on the Web.
Markert and Nissim (Markert and Nissim, 2002)
presents a supervised classification algorithm for re-
solving metonymy. Metonymy is a closely related
figure of speech to metaphors where a word is sub-
stituted by another with which it is associated. Ex-
ample,
A pen is mightier than a sword.
Here sword is a metonymy for war and pen is a
metonymy for articles. They use collocation, co-
occurrence and grammatical features in their clas-
sification algorithm.
MetaBank (Martin, 1994) is a large knowledge
base of metaphors empirically collected. The de-
tection technique compares new sentences with this
knowledge base. The accuracy is dependent on the
correctness of the knowledge base and we expect
that some of these metaphors would be dead in the
present context. The techniques we discuss in this
work will drastically reduce the need for manually
constructing such a large collection.
Goatly (Goatly, 1997) proposes using analogy
markers such as ?like?, ?such as?, ?illustrated by?
and lexical markers like ?literally?, ?illustrating?,
?metaphorically? etc. These would be useful for
identifying simile and explicit metaphoric relations
but not metaphors where the relation between the
target concept and the source concept is not explicit.
The CorMet system (Mason, 2004) dynamically
mines domain specific corpora to find less frequent
19
usages and identifies conceptual metaphors. How-
ever the system is limited to extracting only selec-
tional preferences of verbs. Verbal selectional pref-
erence is the verb?s preference for the type of argu-
ment it takes.
Dolan (Dolan, 1995) uses the path and path length
between words in the knowledge base derived from
lexical resources for interpreting the interrelation-
ship between the component parts of a metaphor.
The effectiveness of this technique relies on whether
the metaphoric sense is encoded in the dictionar-
ies. This approach however will not be effective for
novel metaphoric usages that are not encoded in dic-
tionaries.
6 Conclusion
In this paper we show that we can use the hyponym
relation in WordNet and word co-occurrence infor-
mation for detecting metaphoric uses in subject-
object, verb-noun and adjective-noun relationships.
According to (Cameron and Deignan, 2006), non
literal expressions with relatively fixed forms and
highly specific semantics are over-represented in the
metaphor literature in comparison to corpora occur-
rences. Therefore as part of future work we would
be studying the effect of our algorithms for naturally
occurring text. We are also interested in increasing
the confidence of the labels using more and diverse
annotators and see how the techniques perform. The
study can then be extended to incorporate the role of
prepositions in metaphoric uses.
7 Acknowledgment
We would like to thank our anonymous reviewers for
their constructive suggestions that helped improve
this paper. We would also like to thank Mr. Kr-
ishna Kumaran Damodaran for annotating the Mas-
ter Metaphor List.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, Philadelphia.
Lynne Cameron and Alice Deignan. 2006. The emer-
gence of metaphor in discourse. Applied Linguistics,
27(4):671?690.
William B. Dolan. 1995. Metaphor as an emergent
property of machine-readable dictionaries. AAAI 1995
Spring Symposium, 95(1):27?32.
Dan Fass and Yorick Wilks. 1983. Preference semantics,
ill-formedness, and metaphor. American Journal of
Computational Linguistics, 9(3):178?187.
Dan Fass. 1991. Met: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Raymond Gibbs. 1984. Literal meaning and psychologi-
cal theory. Cognitive Science, 8:275?304.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge,London.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press, Chicago, Illi-
nois.
Katja Markert and Malvina Nissim. 2002. Metonymy
resolution as a classification task. In Proceedings of
ACL-02 conference on Empirical Methods in Natural
Language Processing, pages 204?213.
James H. Martin. 1994. Metabank: a knowledge-base
of metaphoric language conventions. Computational
Intelligence, 10(2):134?149.
Zachary J. Mason. 2004. Cormet: A computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Diana McCarthy and John Carrol. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4):639?654.
Geoffrey Nunberg. 1987. Poetic and prosaic metaphors.
In Proceedings of the 1987 workshop on Theoretical
issues in natural language processing, pages 198?201.
Philip Resnik. 1997. Selectional preferences and word
sense disambiguation. In Proceedings of ACL Siglex
Workshop on Tagging Text with Lexical Semantics,
Why, What and How?, Washington, D.C., pages 52?
57.
20
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 87?93,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
How Creative is Your Writing? A Linguistic Creativity Measure from
Computer Science and Cognitive Psychology Perspectives
Xiaojin Zhu, Zhiting Xu and Tushar Khot
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI, USA 53706
{jerryzhu, zhiting, tushar}@cs.wisc.edu
Abstract
We demonstrate that subjective creativity in
sentence-writing can in part be predicted us-
ing computable quantities studied in Com-
puter Science and Cognitive Psychology. We
introduce a task in which a writer is asked to
compose a sentence given a keyword. The
sentence is then assigned a subjective creativ-
ity score by human judges. We build a linear
regression model which, given the keyword
and the sentence, predicts the creativity score.
The model employs features on statistical lan-
guage models from a large corpus, psycholog-
ical word norms, and WordNet.
1 Introduction
One definition of creativity is ?the ability to tran-
scend traditional ideas, rules, patterns, relationships,
or the like, and to create meaningful new ideas,
forms, methods, interpretations, etc.? Therefore,
any computational measure of creativity needs to ad-
dress two aspects simultaneously:
1. The item to be measured has to be different
from other existing items. If one can model ex-
isting items with a statistical model, the new
item should be an ?outlier?.
2. The item has to be meaningful. An item con-
sists of random noise might well be an outlier,
but it is not of interest.
In this paper, we consider the task of measuring hu-
man creativity in composing a single sentence, when
the sentence is constrained by a given keyword. This
simple task is a first step towards automatically mea-
suring creativity in more complex natural language
text. To further simplify the task, we will focus on
the first aspect of creativity, i.e., quantifying how
novel the sentence is. The second aspect, how mean-
ingful the sentence is, requires the full power of Nat-
ural Language Processing, and is beyond the scope
of this initial work. This, of course, raises the con-
cern that we may regard a nonsense sentence as
highly creative. This is a valid concern. However,
in many applications where a creativity measure is
needed, the input sentences are indeed well-formed.
In such applications, our approach will be useful.
We will leave this issue to future work. The present
paper uses a data set (see the next section) in which
all sentences are well-formed.
A major difficulty in studying creativity is the
lack of an objective definition of creativity. Because
creative writing is highly subjective (?I don?t know
what is creativity, but I recognize it when I see one?),
we circumvent this problem by using human judg-
ment as the ground truth. Our experiment procedure
is the following. First, we give a keyword z to a
human writer, and ask her to compose a sentence
x about z. Then, the sentence x is evaluated by a
group of human judges who assign it a subjective
?creativity score? y. Finally, given a dataset con-
sisting of many such keyword-sentence-score triples
(z,x, y), we develop a statistical predictor f(x, z)
that predicts the score y from the sentence x and
keyword z.
There has been some prior attempts on charac-
terizing creativity from a computational perspec-
tive, for examples see (Ritchie, 2001; Ritchie, 2007;
87
Pease et al, 2001). The present work distinguishes
itself in the use of a statistical machine learning
framework, the design of candidate features, and its
empirical study.
2 The Creativity Data Set
We select 105 keywords from the English version of
the Leuven norms dataset (De Deyne and Storms,
2008b; De Deyne and Storms, 2008a). This ensures
that each keyword has their norms feature defined,
see Section 3.2. These are common English words.
The keywords are randomly distributed to 21 writ-
ers, each writer receives 5 keywords. Each writer
composes one sentence per keyword. These 5 key-
words are further randomly split into two groups:
1. The first group consists of 1 keyword. The
writers are instructed to ?write a not-so-creative
sentence? about the keyword. Two examples
are given: ?Iguana has legs? for ?Iguana?, and
?Anvil can get rusty? for ?Anvil.? The purpose
of this group is to establish a non-creative base-
line for the writers, so that they have a sense
what does not count as creative.
2. The second group consists of 4 keywords. The
writers are instructed to ?try to write a creative
sentence? about each keyword. They are also
told to write a sentence no matter what, even if
they cannot come up with a creative one. No
example is given to avoid biasing their creative
thinking.
In the next stage, all sentences are given to four
human judges, who are native English speakers. The
judges are not the writers nor the authors of this
paper. The order of the sentences are randomized.
The judges see the sentences and their correspond-
ing keywords, but not the identity of the writers,
nor which group the keywords are in. The judges
work independently. For each keyword-sentence
pair, each judge assigns a subjective creativity score
between 0 and 10, with 0 being not creative at all
(the judges are given the Iguana and Anvil exam-
ples for this), and 10 the most creative. The judges
are encouraged to use the full scale when scoring.
There is statistically significant (p < 10?8) linear
correlation among the four judges? scores, showing
their general agreement on subjective creativity. Ta-
ble 1 lists the pairwise linear correlation coefficient
between all four judges.
Table 1: The pairwise linear correlation coefficient be-
tween four judges? creativity scores given to the 105 sen-
tences. All correlations are statistically significant with
p < 10?8.
judge 2 judge 3 judge 4
judge 1 0.68 0.61 0.74
judge 2 0.55 0.74
judge 3 0.61
The scores from four judges on each sentence are
then averaged to produce a consensus score y. Ta-
ble 2 shows the top and bottom three sentences as
sorted by y.
As yet another sanity check, note that the judges
have no information which sentences are from group
1 (where the writers are instructed to be non-
creative), and which are from group 2. We would
expect that if both the writers and the judges share
some common notion of creativity, the mean score
of group 1 should be smaller than the mean score of
group 2. Figure 1 shows that this is indeed the case,
with the mean score of group 1 being 1.5? 0.6, and
that of group 2 being 5.1 ? 0.4 (95% confidence in-
terval). A t-test shows that this difference is signifi-
cant (p < 10?11).
1 20
2
4
6
group
sco
re
Figure 1: The mean creativity score for group 1 is signif-
icantly smaller than that for group 2. That is, the judges
feel that sentences in group 2 are more creative.
To summarize, in the end our dataset con-
sists of 105 keyword, sentence, creativity
score tuples {(zi,xi, yi)} for i = 1, . . . , 105.
The sentence group information is not in-
cluded. This ?Wisconsin Creative Writ-
ing? dataset is publicly available at http:
88
Table 2: Example sentences with the largest and smallest consensus creativity scores.
consensus score y keyword z sentence x
9.25 hamster She asked if I had any pets, so I told her I once did until I discovered
that I liked taste of hamster.
9.0 wasp The wasp is a dinosaur in the ant world.
8.5 dove Dove can still bring war by the information it carries.
...
0.25 guitar A Guitar has strings.
0.25 leech Leech lives in the water.
0.25 elephant Elephant is a mammal.
//pages.cs.wisc.edu/?jerryzhu/pub/
WisconsinCreativeWriting.txt.
3 Candidate Features for Predicting
Creativity
In this section, we discuss two families of candi-
date features we use in a statistical model to pre-
dict the creativity of a sentence. One family comes
from a Computer Science perspective, using large-
corpus statistics (how people write). The other fam-
ily comes from a Cognitive Psychology perspective,
specifically the word norms data and WordNet (how
people think).
3.1 The Computer Science Perspective:
Language Modeling
We start from the following hypothesis: if the words
in the sentence x frequently co-occur with the key-
word z, then x is probably not creative. This is of
course an over-simplification, as many creative sen-
tences are about novel usage of common words1.
Nonetheless, this hypothesis inspires some candi-
date features that can be computed from a large cor-
pus.
In this study, we use the Google Web 1T 5-
gram Corpus (Brants et al, 2007). This corpus
was generated from about 1012 word tokens from
Web pages. It consists of counts of N-gram for
N = 1, . . . , 5. We denote the words in a sentence
by x = x1, . . . , xn, where x1 = ?s? and xn = ?/s?
are special start- and end-of-sentence symbols. We
1For example, one might argue that Lincoln?s famous sen-
tence on government: ?of the people, by the people, for the
people? is creative, even though the keyword ?government? fre-
quently co-occurs with all the words in that sentence.
design the following candidate features:
[f1: Zero N-gram Fraction] Let c(xi+N?1i ) be
the count of the N-gram xi . . . xi+N?1 in the corpus.
Let ?(A) be the indicator function with value 1 if
the predicate A is true, and 0 otherwise. A ?Zero
N-gram Fraction? feature is the fraction of zero N-
gram counts in the sentence:
f1,N (x) =
?n?N+1
i=1 ?(c(xi+N?1i ) = 0)
n ? N + 1 . (1)
This provided us with 5 features, namely N-gram
zero count fractions for each value of N. These fea-
tures are a crude measure of how surprising the sen-
tence x is. A feature value of 1 indicates that none of
the N-grams in the sentence appeared in the Google
corpus, a rather surprising situation.
[f2: Per-Word Sentence Probability] This fea-
ture is the per-word log likelihood of the sentence,
to normalize for sentence length:
f2(x) = 1n log p(x). (2)
We use a 5-gram language model to estimate
p(x), with ?naive Jelinek-Mercer? smoothing. As
in Jelinek-Mercer smoothing (Jelinek and Mercer,
1980), it is a linear interpolation of N-gram language
models for N = 1 . . . 5. Let the Maximum Likeli-
hood (ML) estimate of a N-gram language model be
pNML(xi|xi?1i?N+1) =
c(xii?N+1)
c(xi?1i?N+1)
, (3)
which is the familiar frequency estimate of proba-
bility. The denominator is the count of the history
of length N ? 1, and the numerator is the count of
the history plus the word to be predicted. A 5-gram
89
Jelinek-Mercer smoothing language model on sen-
tence x is
p(x) =
n?
i=1
p(xi|xi?1i?5+1) (4)
p(xi|xi?1i?5+1) =
5?
N=1
?NPNML(xi|xi?1i?N+1),(5)
where the linear interpolation weights ?1 + . . . +
?5 = 1. The optimal values of ??s are a function of
history counts (binned into ?buckets?) c(xi?1i?N+1),
and should be optimized with convex optimiza-
tion from corpus. However, because our corpus is
large, and because we do not require precise lan-
guage modeling, we instead set the ??s in a heuris-
tic manner. Starting from N=5 to 1, ?N is set
to zero until the N where we have enough history
count for reliable estimate. Specifically, we require
c(xi?1i?N+1) > 1000. The first N that this happens
receives ?N = 0.9. The next lower order model
receives 0.9 fraction of the remaining weight, i.e.,
?N?1 = 0.9 ? (1 ? 0.9), and so on. Finally, ?1 re-
ceives all remaining weight to ensure ?1+. . .+?5 =
1. This heuristic captures the essence of Jelinek-
Mercer smoothing and is highly efficient, at the price
of suboptimal interpolation weights.
[f3: Per-Word Context Probability] The previ-
ous feature f2 ignores the fact that our sentence x
is composed around a given keyword z. Given that
the writer was prompted with the keyword z, we are
interested in the novelty of the sentence surround-
ing the keyword. Let xk be the first occurrence of
z in the sentence, and let x?k be the context of the
keyword, i.e., the sentence with the k-th word (the
keyword) removed. This notion of context novelty
can be captured by
p(x?k|xk = z) = p(x?k, xk = z)p(xk = z) =
p(x)
p(z) , (6)
where p(x) is estimated from the naive Jelinek-
Mercer 5-gram language model above, and p(z) is
estimated from a unigram language model. Our third
feature is the length-normalized log likelihood of the
context:
f3(x, z) = 1n ? 1 (log p(x) ? log p(z)) . (7)
3.2 The Cognitive Psychology Perspective:
Word Norms and WordNet
A text corpus like the one above captures how peo-
ple write sentences related to a keyword. However,
this can be different from how people think about re-
lated concepts in their head for the same keyword.
In fact, common sense knowledge is often under-
represented in a corpus ? for example, why bother
repeating ?A duck has a long neck? over and over
again? However, this lack of co-occurrence does not
necessarily make the duck sentence creative.
The way people think about concepts can in part
be captured by word norms experiments in psychol-
ogy. In such experiments, a human subject is pro-
vided with a keyword z, and is asked to write down
the first (or a few) word x that comes to mind.
When aggregated over multiple subjects on the same
keyword, the experiment provides an estimate of
the concept transition probability p(x|z). Given
enough keywords, one can construct a concept net-
work where the nodes are the keywords, and the
edges describe the transitions (Steyvers and Tenen-
baum, 2005). For our purpose, we posit that a sen-
tence x may not be creative with respect to a key-
word z, if many words in x can be readily retrieved
as the norms of keyword z. In a sense, the writer
was thinking the obvious.
[f4: Word Norms Fraction] We use the Leu-
ven dataset, which consists of norms for 1,424 key-
words (De Deyne and Storms, 2008b; De Deyne and
Storms, 2008a). The original Leuven dataset is in
Dutch, we use a version that is translated into En-
glish. For each sentence x, we first exclude the key-
word z from the sentence. We also remove punctu-
ations, and map all words to lower case. We further
remove all stopwords using the Snowball stopword
list (Porter, 2001), and stem all words in the sentence
and the norm word list using NLTK (Loper and Bird,
2002). We then count the number of words xi that
appear in the norm list of the keyword z in the Leu-
ven data. Let this count be cnorm(x, z). The feature
is the fraction of such norm words in the original
sentence:
f4(x, z) = cnorm(x, z)n . (8)
It is worth noting that the Leuven dataset is relatively
small, with less than two thousand keywords. This
90
is a common issue with psychology norms datasets,
as massive number of human subjects are difficult
to obtain. To scale our method up to handle large
vocabulary in the future, one possible method is to
automatically infer the norms of novel keywords us-
ing corpus statistics (e.g., distributional similarity).
[f5 ? f13: WordNet Similarity] WordNet is an-
other linguistic resource motivated by cognitive psy-
chology. For each sentence x, we compute Word-
Net 3.0 similarity between the keyword z and each
word xi in the sentence. Specifically, we use the
?path similarity? provided by NLTK (Loper and
Bird, 2002). Path similarity returns a score denot-
ing how similar two word senses are, based on the
shortest path that connects the senses in the hyper-
nym/hyponym taxonomy. The score is in the range
0 to 1, except in those cases where a path cannot be
found, in which case -1 is returned. A score of 1
represents identity, i.e., comparing a sense with it-
self. Let the similarities be s1 . . . sn. We experiment
with the following features: The mean, median, and
variance of similarities:
f5(x, z) = mean(s1 . . . sn) (9)
f6(x, z) = median(s1 . . . sn) (10)
f7(x, z) = var(s1 . . . sn). (11)
Features f8, . . . , f12 are the top five similarities.
When the length of the sentence is shorter than five,
we fill the remaining features with -1. Finally, fea-
ture f13 is the fraction of positive similarity:
f13(x, z) =
?n
i=1 ?(si > 0)
n . (12)
4 Regression Analysis on Creativity
With the candidate features introduced in Section 3,
we construct a linear regression model to predict the
creativity scores given a sentence and its keyword.
The first question one asks in regression analy-
sis is whether the features have a (linear) correlation
with the creativity score y. We compute the correla-
tion coefficient
?i = Cov(fi, y)?fi?y (13)
for each candidate feature fi separately on the first
row in Table 3. Some observations:
? The feature f4 (Word Norms Fraction) has the
largest correlation coefficient -0.48 in terms of
magnitude. That is, the more words in the sen-
tence that are also in the norms of the keyword,
the less creative the sentence is.
? The feature f12 (the 5-th WordNet similarity in
the sentence to the keyword) has a large posi-
tive coefficient 0.47. This is rather unexpected.
A closer inspection reveals that f12 equals -1
for about half of the sentences, and is around
0.05 for the other half. Furthermore, the second
half has on average higher creativity scores. Al-
though we hypothesized earlier that more simi-
lar words means lower creativity, this (together
with the positive ? for f10, f11) suggests the
other way around: more similar words are cor-
related with higher creativity.
? The feature f5 (mean WordNet similarity) has
a negative correlation with creativity. This fea-
ture is related to f12, but in a different direc-
tion. We speculate that this feature measures
the strength of similar words, while f12 indi-
rectly measures the number of similar words.
? The feature f3 (Per-Word Context Probability)
has a negative correlation with creativity. The
more predictable the sentence around the key-
word using a language model, the lower the
creativity.
Next, we build a linear regression model to pre-
dict creativity. We use stepwise regression, which
is a technique for feature selection by iteratively
including / excluding candidate features from the
regression model based on statistical significance
tests (Draper and Smith, 1998). The result is a lin-
ear regression model with a small number of salient
features. For the creativity dataset, the features (and
their regression coefficients) included by stepwise
regression are shown on the second row in Table 3.
The corresponding linear regression model is
y?(x, z) = ?5.06 ? f4 + 1.80 ? f12 ? 0.76 ? f3
?3.39 ? f5 + 0.92. (14)
A plot comparing y? and y is given in Figure 2. The
root mean squared error (RMSE) of this model is
91
Table 3: ?: The linear correlation coefficients between a candidate feature and the creativity score y. ?: The selected
features and their regression coefficients in stepwise linear regression.
f1,1 f1,2 f1,3 f1,4 f1,5 f2 f3 f4 f5
? 0.09 0.09 0.17 0.06 -0.04 -0.07 -0.32 -0.48 -0.41
? -0.76 -5.06 -3.39
f6 f7 f8 f9 f10 f11 f12 f13
? -0.19 -0.25 -0.02 0.06 0.23 0.30 0.47 -0.01
? 1.80
0 5 100
2
4
6
8
10
predicted score
tru
e s
cor
e
Figure 2: The creativity score y? as predicted by the linear
regression model in equation 14, compared to the true
score y. Each dot is a sentence.
1.51. In contrast, the constant predictor would have
RMSE 2.37 (i.e., the standard deviation of y).
We make two comments:
1. It is interesting to note that our intuitive fea-
tures are able to partially predict subjective cre-
ativity scores. On the other hand, we certainly
do not claim that our features or model solved
this difficult problem.
2. All three kinds of knowledge: corpus statistics
(f3), word norms (f4), and WordNet (f5, f12)
are included in the regression model. Coin-
cidentally, these features have the largest cor-
relation coefficients with the creativity score.
The fact that they are all included suggests that
these are not redundant features, and each cap-
tures some aspect of creativity.
5 Conclusions and Future Work
We presented a simplified creativity prediction task,
and showed that features derived from statistical
language modeling, word norms, and WordNet can
partially predict human judges? subjective creativity
scores.
Our problem setting is artificial, in that the cre-
ativity of the sentences are judged with respect to
their respective keywords, which are assumed to be
known beforehand. This allows us to design features
centered around the keywords. We hope our analysis
can be extended to the setting where the only input is
the sentence, without the keyword. This can poten-
tially be achieved by performing keyword extraction
on the sentence first, and apply our analysis on the
extracted keyword.
As discussed in the introduction, our analysis
is susceptible to nonsense input sentences, which
could be predicted as highly creative. Combining
our analysis with a ?sensibility analysis? is an im-
portant future direction.
Finally, our model might be adapted to explain
why a sentence is deemed creative, by analyzing the
contribution of individual features in the model.
6 Acknowledgments
We thank the anonymous reviewers for suggestions
on related work and other helpful comments, and
Chuck Dyer, Andrew Goldberg, Jake Rosin, and
Steve Yazicioglu for assisting the project. This work
is supported in part by the Wisconsin Alumni Re-
search Foundation.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In EMNLP.
S. De Deyne and G Storms. 2008a. Word associations:
Network and semantic properties. Behavior Research
Methods, 40:213?231.
S. De Deyne and G Storms. 2008b. Word associations:
Norms for 1,424 Dutch words in a continuous task.
Behavior Research Methods, 40:198?205.
92
Norman R. Draper and Harry Smith. 1998. Applied
Regression Analysis (Wiley Series in Probability and
Statistics). John Wiley & Sons Inc, third edition.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Workshop on Pattern Recognition in
Practice.
Edward Loper and Steven Bird. 2002. NLTK: The nat-
ural language toolkit. In The ACL Workshop on Ef-
fective Tools and Methodologies for Teaching Natural
Language Processing and Computational Linguistics,
pages 62?69.
Alison Pease, Daniel Winterstein, and Simon Colton.
2001. Evaluating machine creativity. In Workshop
on Creative Systems, 4th International Conference on
Case Based Reasoning, pages 129?137.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms. Published online.
Graeme Ritchie. 2001. Assessing creativity. In Pro-
ceedings of the AISB01 Symposium on Artificial Intel-
ligence and Creativity in Arts and Science, pages 3?11.
Graeme Ritchie. 2007. Some empirical criteria for at-
tributing creativity to a computer program. Minds and
Machines, 17(1):67?99.
Mark Steyvers and Joshua Tenenbaum. 2005. The large
scale structure of semantic networks: Statistical anal-
yses and a model of semantic growth. Cognitive Sci-
ence, 29(1):41?78.
93
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Keepin? It Real: Semi-Supervised Learning with Realistic Tuning
Andrew B. Goldberg
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
goldberg@cs.wisc.edu
Xiaojin Zhu
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
jerryzhu@cs.wisc.edu
Abstract
We address two critical issues involved in ap-
plying semi-supervised learning (SSL) to a
real-world task: parameter tuning and choos-
ing which (if any) SSL algorithm is best suited
for the task at hand. To gain a better un-
derstanding of these issues, we carry out a
medium-scale empirical study comparing su-
pervised learning (SL) to two popular SSL al-
gorithms on eight natural language processing
tasks under three performance metrics. We
simulate how a practitioner would go about
tackling a new problem, including parameter
tuning using cross validation (CV). We show
that, under such realistic conditions, each of
the SSL algorithms can be worse than SL on
some datasets. However, we also show that
CV can select SL/SSL to achieve ?agnostic
SSL,? whose performance is almost always no
worse than SL. While CV is often dismissed as
unreliable for SSL due to the small amount of
labeled data, we show that it is in fact effective
for accuracy even when the labeled dataset
size is as small as 10.
1 Introduction
Imagine you are a real-world practitioner working
on a machine learning problem in natural language
processing. If you have unlabeled data, should you
use semi-supervised learning (SSL)? Which SSL al-
gorithm should you use? How should you set its pa-
rameters? Or could it actually hurt performance, in
which case you might be better off with supervised
learning (SL)?
A large number of SSL algorithms have been de-
veloped in recent years that allow one to improve
performance with unlabeled data, in tasks such
as text classification, sequence labeling, and pars-
ing (Zhu, 2005; Chapelle et al, 2006; Brefeld and
Scheffer, 2006). However, many of them are tested
on ?SSL-friendly? datasets, such as ?two moons,?
USPS, and MNIST. Furthermore, the algorithms?
parameters are often chosen based on test set per-
formance or manually set based on heuristics and
researcher experience. These issues create practical
concerns for deploying SSL in the real world.
We note that (Chapelle et al, 2006)?s benchmark
chapter explores these issues to some extent by com-
paring several SSL methods on several real and ar-
tificial datasets. The authors reach the conclusions
that parameter tuning is difficult with little labeled
data and that no method is universally superior. We
reexamine these issues in the context of NLP tasks
and offer a simple attempt at overcoming these road-
blocks to practical application of SSL.
The contributions of this paper include:
? We present a medium-scale empirical study
comparing SL to two popular SSL algorithms
on eight less-familiar tasks using three per-
formance metrics. Importantly, we tune pa-
rameters realistically based on cross validation
(CV), as a practitioner would do in reality.
? We show that, under such realistic conditions,
each of the SSL algorithms can be worse than
SL on some datasets.
? However, this can be prevented. We show that
CV can be used to select SL/SSL to achieve
agnostic SSL, whose performance is almost al-
ways no worse than SL. Traditionally, CV is
19
often dismissed as unreliable for SSL because
of the small labeled dataset size. But we show
that CV is effective when using accuracy as an
optimization criterion, even when the labeled
dataset size is as small as 10.
? We show the power of cloud computing: we
were able to complete roughly 3 months worth
of experiments in less than a week.
2 SSL with Realistic Tuning
Given a particular labeled and unlabeled dataset,
how should you set parameters for a particular SSL
model? The most realistic approach for a practi-
tioner is to use CV to tune parameters on a grid. We
therefore argue that the model parameters obtained
in this way truly determine how SSL will perform
in practice. Algorithm 1 describes a particular in-
stance1 of CV in detail. We call it ?RealSSL,? as
this is all a real user can hope to do when applying
SSL (and SL too) in a realistic problem scenario.
3 Experimental Procedure
Given the RealSSL procedure in Algorithm 1, we
designed an experimental setup to simulate differ-
ent settings that a real-world practitioner might face
when given a new task and a set of algorithms to
choose from (some of which use unlabeled data).
This will allow us to compare algorithms across
datasets in a variety of situations. Algorithm 2
measures the performance of one algorithm on one
dataset for several different l and u combinations.
Specifically, we consider l ? {10, 100} and u ?
{100, 1000}. For each combination, we perform
multiple trials (T = 10 here) using different random
assignments of data to Dlabeled and Dunlabeled, to
obtain confidence intervals around our performance
measurements. All random selections of subsets of
data are the same across different algorithms? runs,
to permit paired t-tests for evaluation. Note that,
when l 6= max(L) or u 6= max(U), a portion of
Dpool is not used for training. Also, the RealSSL
procedure ensures that all parameters are tuned by
cross-validation without ever seeing the held-out
1The particular choice of 5-fold CV, the way to split labeled
and unlabeled data, and the parameter grid, is important too.
But we view them as secondary to the fact that we are tuning
SSL by CV.
test set Dtest. Lastly, we stress that the same grid
of algorithm-specific parameter values (discussed in
Section 5) is considered for all datasets.
4 Datasets
Table 1 summarizes the datasets used for the com-
parisons. In this study we consider only binary clas-
sification tasks. Note that d is the number of dimen-
sions, P (y = 1) is the proportion of instances in
the full dataset belonging to class y = 1, and |Dtest|
refers to the size of the test set (the instances remain-
ing after max(L) + max(U) = 1100 have been set
aside for training trials).
[MacWin] is the Mac versus Windows text clas-
sification data from the 20-newsgroups dataset, pre-
processed by the authors of (Sindhwani et al, 2005).
[Interest] is a binary version of the word sense
disambiguation data from (Bruce and Wiebe, 1994).
The task is to distinguish the sense of ?interest?
meaning ?money paid for the use of money? from
the other five senses (e.g., ?readiness to give atten-
tion,? ?a share in a company or business?). The
data comes from a corpus of part-of-speech (POS)
tagged sentences containing the word ?interest.?
Each instance is a bag-of-word/POS vector, exclud-
ing words containing the root ?interest? and those
that appeared in less than three sentences overall.
Datasets [aut-avn] and [real-sim] are the
auto/aviation and real/simulated text classification
datasets from the SRAA corpus of UseNet articles.
The [ccat] and [gcat] datasets involve identifying
corporate and government articles, respectively, in
the RCV1 corpus. We use the versions of these
datasets prepared by the authors of (Sindhwani et
al., 2006).
Finally, the two WISH datasets come from (Gold-
berg et al, 2009) and involve discriminating be-
tween sentences that contain wishful expressions
and those that do not. The instances in [WISH-
politics] correspond to sentences taken from a po-
litical discussion board, while [WISH-products] is
based on sentences from Amazon product reviews.
The features are a combination of word and template
features as described in (Goldberg et al, 2009).
20
Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj}uj=1, algorithm, performance metric
Randomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}.
Randomly partition Dunlabeled into 5 equally-sized disjoint subsets {Du1, Du2, Du3, Du4, Du5}.
Combine partitions: Let Dfold k = Dlk ? Duk for all k = 1, . . . , 5.
foreach parameter configuration in grid do
foreach fold k do
Train model using algorithm on ?i6=kDfold i.
Evaluate metric on Dfold k.
end
Compute the average metric value across the 5 folds.
end
Choose parameter configuration that optimizes average metric.
Train model using algorithm and the chosen parameters on Dlabeled and Dunlabeled.
Output: Optimal model; Average metric value achieved by optimal parameters during tuning.
Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on a
specific labeled and unlabeled dataset using cross-validation to tune parameters.
Input: dataset D = {xi, yi}ni=1, algorithm, performance metric, set L, set U , trials T
Randomly divide D into Dpool (of size max(L) + max(U)) and Dtest (the rest).
foreach l in L do
foreach u in U do
foreach trial 1 up to T do
Randomly select Dlabeled = {xj , yj}lj=l and Dunlabeled = {xk}uk=1 from Dpool.
Run RealSSL(Dlabeled, Dunlabeled, algorithm, metric) to obtain model and tuning
performance value (see Algorithm 1).
Use model to classify Dunlabeled and record transductive metric value.
Use model to classify Dtest and record test metric value.
end
end
end
Output: Tuning, transductive, and test performance for T runs of algorithm using all l and u
combinations.
Algorithm 2: Experimental procedure used for all comparisons.
21
Name d P (y = 1) |Dtest|
[MacWin] 7511 0.51 846
[Interest] 2687 0.53 1268
[aut-avn] 20707 0.65 70075
[real-sim] 20958 0.31 71209
[ccat] 47236 0.47 22019
[gcat] 47236 0.30 22019
[WISH-politics] 13610 0.34 4999
[WISH-products] 4823 0.12 129
Table 1: Datasets used in benchmark comparison. See
text for details.
5 Algorithms
We consider only linear classifiers for this study,
since they tend to work well for text problems. In
future work, we plan to explore a range of kernels
and other non-linear classifiers.
As a baseline supervised learning method, we use
a support vector machine (SVM), as implemented
by SVMlight (Joachims, 1999). This baseline simply
ignores all the unlabeled data (xl+1, . . . ,xn). Recall
this solves the following regularized risk minimiza-
tion problem
min
f
1
2 ||f ||
2
2 + C
l?
i=1
max(0, 1 ? yif(xi)), (1)
where f(x) = w>x + b, and C is a parame-
ter controlling the trade-off between training er-
rors and model complexity. Using the procedure
outlined above, we tune C over a grid of values
{10?6, 10?5, 10?4, 10?3, 10?2, 10?1, 1, 10, 100}.
We consider two popular SSL algorithms, which
make different assumptions about the link between
the marginal data distribution Px and the conditional
label distribution Py|x. If the assumption does not
hold in a particular dataset, the SSL algorithm could
use the unlabeled data ?incorrectly,? and perform
worse than SL.
The first SSL algorithm we use is a semi-
supervised support vector machine (S3VM), which
makes the cluster assumption: the classes are well-
separated clusters of data, such that the decision
boundary falls into a low density region in the fea-
ture space. While many implementations exist,
we chose the deterministic annealing (DA) algo-
rithm implemented in the SVMlin package (Sind-
hwani et al, 2006; Sindhwani and Keerthi, 2007).
This DA algorithm often achieved the best accu-
racy across several datasets in the empirical com-
parison in (Sindhwani and Keerthi, 2007), despite
being slower than the multi-switch algorithm pre-
sented in the same paper. Note that the transductive
SVM implemented in SVMlight would have been
prohibitively slow to carry out the range of exper-
iments conducted here. Recall that an S3VM seeks
an optimal classifier f? that cuts through a region of
low density between clusters of data. One way to
view this is that it tries to find the best possible la-
beling of the unlabeled data such the classifier maxi-
mizes the margin on both labeled and unlabeled data
points. This is achieved by solving the following
non-convex minimization problem
min
f,y??{?1,1}u
?
2 ||f ||
2
2
+ 1l
l?
i=1
V (yif(xi) + ?
?
u
n?
j=l+1
V (y?jf(xj)),
subject to a class-balance constraint. Note that
V is a loss function (typically the hinge loss
as in (1)), and the parameters ?, ?? control the
relative importance of model complexity versus
locating a low-density region within the unlabeled
data. We tune both parameters in a grid of values
{10?6, 10?5, 10?4, 10?3, 10?2, 10?1, 1, 10, 100}.
In past studies (Sindhwani et al, 2006), ? was set to
1, and ?? was tuned over a grid containing a subset
of these values.
Finally, as an example of a graph-based
SSL method, we consider manifold regularization
(MR) (Belkin et al, 2006), using the implementa-
tion provided on the authors? Web site.2 This algo-
rithm makes the manifold assumption: the labels are
?smooth? with respect to a graph connecting labeled
and unlabeled instances. In other words, if two in-
stances are connected by a strong edge (e.g., they
are highly similar to one another), their labels tend
to be the same. Manifold regularization represents a
family of methods; we specifically use the Laplacian
SVM, which extends the basic SVM optimization
2http://manifold.cs.uchicago.edu/
manifold regularization/software.html
22
problem with a graph-based regularization term.
min
f
?A||f ||22 + 1l
l?
i=1
max(0, 1 ? yif(xi))
+ ?I
n?
i=1
n?
j=1
wij(f(xi) ? f(xj))2,
where ?A and ?I are parameters that trade off am-
bient and intrinsic smoothness, and wij is a graph
weight between instances xi and xj . In this pa-
per, we consider kNN graphs with k ? {3, 10, 20}.
Edge weights are formed using a heat kernel wij =
exp(? ||xi?xj ||22?2 ), where ? is set to be the mean dis-
tance between nearest neighbors in the graph, as
in (Chapelle et al, 2006). The ? parameters are each
tuned over the grid {10?6, 10?4, 10?2, 1, 100}.
Of course, many other SSL algorithms exist, some
of which combine different assumptions (Chapelle
and Zien, 2005; Karlen et al, 2008), and others
which exploit multiple (real or artificial) views of
the data (Blum and Mitchell, 1998; Sindhwani and
Rosenberg, 2008). We plan to extend our study to
include many more diverse SSL algorithms in the
future.
6 Choosing an Algorithm for a Real-World
Task
Given the choice of several algorithms, how should
one choose the best one to apply to a particular learn-
ing setting? Traditionally, CV is used for model
selection in supervised learning settings. However,
with only a small amount of labeled data in semi-
supervised settings, model selection with CV is of-
ten viewed as unreliable. We explicitly tested this
hypothesis by using CV to not only choose the pa-
rameters of the model, but also choose the type of
model itself. The main goal is to automatically
choose between SVM, S3VM, and MR for a par-
ticular learning setting, in an attempt to ensure that
the final performance is never hurt by including un-
labeled data (which might be called agnostic SSL).
Given a set of algorithms (e.g., one SL, several
SSL), the procedure is the following:
1. Tune the parameters of each algorithm on the
labeled and unlabeled training set using Algo-
rithm 1.3
2. Compare the best tuning performance (5-fold
CV average) achieved by the optimal parame-
ters for each of the algorithms.
? If there are no ties, select the algorithm
with the highest tuning performance.
? If there is a tie, and SL is among the best,
select SL.
? If there is a tie between SSL algorithms,
select one of them at random.
3. Use the selected ?Best Tuning? algorithm (and
the tuned parameters) to build a model on all
the training data; then apply it to the test data.
Note that the procedure is conservative in that it
prefers SL in the case of ties. In this paper, we use
this simple ?best tuning performance? heuristic.
Finally, we stress the fact that, when applying
this procedure within the context of Algorithm 2, a
potentially different algorithm is chosen in each of
the 10 trials for a particular setting. This simulates
the real-world scenario where one only has a single
fixed training set of labeled and unlabeled data and
must choose a single algorithm to produce a model
for future predictions.
7 Performance Metrics
We compare different algorithms? performance us-
ing three metrics often used for evaluation in NLP
tasks: accuracy, maxF1, and AUROC. Accuracy
is simply the fraction of instances correctly classi-
fied. MaxF1 is the maximal F1 value (harmonic
mean of recall and precision) achieved over the en-
tire precision-recall curve (Cai and Hofmann, 2003).
AUROC is the area under the ROC curve (Fawcett,
2004). Throughout the paper, when we discuss a
result involving a particular metric, the algorithms
use this metric as the criterion for parameter tuning,
and we use it for the final evaluation. We are not
simply evaluating a single experiment using multi-
ple metrics?the experiments are fundamentally dif-
ferent and produce different learned models.
3We ensure each algorithm uses the same 5 partitions during
the tuning step.
23
8 Results
We now report the results of our empirical compar-
ison of SL and SSL on the eight NLP datasets. We
first consider each dataset separately and examine
how often each type of algorithm outperforms the
other. We then examine cross-dataset performance.
8.1 Detailed Results
Table 2 contains all results for SVM, S3VM, and
MR for all datasets and all metrics.4 Note that within
each l,u cell for a particular dataset and evaluation
metric, we show the maximum value in each row
(tune, transductive, or test) in boldface. Results that
are not statistically significantly different using a
paired t-test are also shown in boldface.
Several things are immediately obvious from Ta-
ble 2. First, no algorithm is superior in all datasets
or settings. In several cases, all algorithms are statis-
tically indistinguishable. Most importantly, though,
each of the SSL algorithms can be worse than SL on
some datasets using some metric. We used paired
t-tests to compare transductive and test performance
of each SSL algorithm with SVM for a particular l,u
combination and dataset (32 settings total per evalu-
ation metric). In terms of accuracy, MR transductive
performance is significantly worse than SVM in 5
settings, while MR test performance is significantly
worse in 7 settings. MR is also significantly worse in
4 settings based on transductive maxF1, in 3 settings
based on transductive AUROC, and 1 setting based
on test AUROC. S3VM is significantly worse than
SVM in 2 settings based on transductive maxF1, 2
settings based on transductive AUROC, and in 1 set-
ting based on test AUROC. While these numbers
may seem relatively low, it is important to realize
that each algorithm may be worse than SSL many
times on a trial-by-trial basis, which is the more real-
istic scenario: a practitioner has only a single dataset
to work with. Results based on individual trials are
discussed below shortly.
4Note that the results here for a particular dataset and algo-
rithm combination may be qualitatively and quantitatively dif-
ferent than in previous published work, due to differences in
parameter tuning, choices of parameter grids, l and u sizes, and
randomization. We are not trying to replicate or raise doubt
about past results: we simply intend to compare algorithms on
a wide array of datasets using the standardized procedures out-
lined above.
We also applied our ?Best Tuning? model selec-
tion procedure to automatically choose a single al-
gorithm for each trial in each setting. We compare
average SL test performance versus the average test
performance of the Best Tuning selections across the
10 trials (not shown in Table 2). Comparisons based
on transductive performance are similar. When the
performance metric is test accuracy, the Best Tuning
algorithm performs statistically significantly better
than SL in 24 settings and worse in only 6 settings.
In the remaining 2 settings, Best Tuning chose SL
in all 10 trials, so they are equivalent. These results
suggest that accuracy-based tuning is a valid method
for choosing a SSL algorithm to improve accuracy
on test data. To some extent, this holds for maxF1,
too: the Best Tuning selections perform better than
SL (on average) in 18 settings and worse in 14 set-
tings when tuning and test evaluation is based on
maxF1. However, when using AUROC as the per-
formance metric, cross validation seems to be unre-
liable: Best Tuning produces a better result in only
11 out of the 32 settings.
8.2 Results Aggregated Across Datasets
We now aggregate the detailed results to better un-
derstand the relative performance of the different
methods across all datasets. We perform this sum-
mary evaluation in two ways, based on test set
performance (transductive performance is similar).
First, we compare the SSL algorithms across all
datasets based on the numbers of times each is worse
than, the same as, or better than SL. For each of
the 80 trials of a particular l,u,metric combination,
we compare the performance of S3VM, MR, and
Best Tuning to SVM. Note that each of these com-
parisons is akin to a real-world scenario where a
practitioner would have to choose an algorithm to
use. Table 3 lists tuples of the form ?(#trials worse
than SVM, #trials equal to SVM, #trials better than
SVM).? Note that the numbers in each tuple sum to
80. The perfect SSL algorithm would have a tuple of
?(0, 0, 80),? meaning that it always outperforms SL.
In terms of accuracy (Table 3, top) and maxF1 (Ta-
ble 3, middle), the Best Tuning method turns out to
do worse than SVM less often than either S3VM or
MR does (i.e., the first number in the tuples for Best
Tuning is lower than the corresponding numbers for
the other algorithms). At the same time, Best Tuning
24
accuracy maxF1 AUROC
u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000
Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR
[MacWin]
10
0.60 0.72 0.83 0.60 0.72 0.86 0.66 0.67 0.67 0.66 0.67 0.67 0.63 0.69 0.67 0.63 0.69 0.69 Tune
0.51 0.51 0.70 0.51 0.50 0.69 0.74 0.77 0.80 0.74 0.74 0.75 0.72 0.75 0.82 0.72 0.71 0.80 Trans
0.53 0.50 0.71 0.53 0.50 0.68 0.74 0.75 0.79 0.74 0.75 0.74 0.73 0.72 0.83 0.73 0.71 0.76 Test
100
0.87 0.87 0.91 0.87 0.87 0.90 0.94 0.95 0.95 0.94 0.95 0.95 0.96 0.97 0.97 0.96 0.96 0.96 Tune
0.89 0.89 0.89 0.89 0.89 0.89 0.91 0.93 0.92 0.91 0.90 0.90 0.97 0.97 0.96 0.97 0.97 0.96 Trans
0.89 0.89 0.91 0.89 0.89 0.90 0.92 0.92 0.92 0.92 0.91 0.91 0.97 0.97 0.97 0.97 0.97 0.97 Test
[Interest]
10
0.68 0.75 0.78 0.68 0.75 0.79 0.73 0.77 0.77 0.73 0.78 0.77 0.52 0.66 0.66 0.52 0.68 0.64 Tune
0.52 0.56 0.56 0.52 0.56 0.56 0.72 0.72 0.72 0.72 0.71 0.71 0.55 0.54 0.54 0.55 0.56 0.61 Trans
0.52 0.57 0.57 0.52 0.57 0.58 0.68 0.69 0.69 0.68 0.69 0.69 0.58 0.56 0.61 0.58 0.58 0.62 Test
100
0.77 0.78 0.76 0.77 0.78 0.77 0.84 0.85 0.85 0.84 0.85 0.84 0.89 0.90 0.89 0.89 0.85 0.84 Tune
0.79 0.79 0.71 0.79 0.79 0.77 0.84 0.83 0.82 0.84 0.81 0.81 0.91 0.91 0.89 0.91 0.79 0.87 Trans
0.81 0.80 0.78 0.81 0.80 0.79 0.82 0.81 0.81 0.82 0.81 0.81 0.90 0.91 0.89 0.90 0.81 0.88 Test
[aut-avn]
10
0.72 0.76 0.82 0.72 0.76 0.79 0.89 0.92 0.91 0.89 0.92 0.91 0.58 0.67 0.65 0.58 0.67 0.65 Tune
0.65 0.63 0.67 0.65 0.61 0.69 0.83 0.83 0.84 0.83 0.81 0.82 0.71 0.67 0.73 0.71 0.65 0.72 Trans
0.62 0.61 0.67 0.62 0.61 0.67 0.80 0.81 0.82 0.80 0.81 0.81 0.71 0.70 0.73 0.71 0.65 0.69 Test
100
0.75 0.82 0.87 0.75 0.82 0.86 0.94 0.94 0.95 0.94 0.94 0.94 0.93 0.94 0.94 0.93 0.94 0.93 Tune
0.77 0.79 0.88 0.77 0.83 0.87 0.92 0.92 0.91 0.92 0.91 0.90 0.93 0.93 0.91 0.93 0.94 0.93 Trans
0.77 0.82 0.89 0.77 0.83 0.87 0.91 0.91 0.91 0.91 0.91 0.91 0.95 0.94 0.95 0.95 0.95 0.95 Test
[real-sim]
10
0.53 0.63 0.82 0.53 0.63 0.78 0.65 0.66 0.66 0.65 0.66 0.65 0.77 0.81 0.81 0.77 0.81 0.77 Tune
0.64 0.63 0.72 0.64 0.64 0.70 0.57 0.66 0.70 0.57 0.62 0.56 0.65 0.75 0.79 0.65 0.74 0.67 Trans
0.65 0.66 0.74 0.65 0.66 0.68 0.53 0.58 0.63 0.53 0.59 0.53 0.64 0.73 0.80 0.64 0.74 0.66 Test
100
0.74 0.73 0.86 0.74 0.73 0.84 0.88 0.90 0.90 0.88 0.91 0.89 0.93 0.94 0.94 0.93 0.94 0.93 Tune
0.78 0.76 0.84 0.78 0.78 0.85 0.81 0.83 0.79 0.81 0.81 0.81 0.94 0.93 0.91 0.94 0.94 0.94 Trans
0.79 0.78 0.85 0.79 0.78 0.85 0.78 0.79 0.78 0.78 0.79 0.79 0.93 0.93 0.93 0.93 0.94 0.93 Test
[ccat]
10
0.54 0.60 0.82 0.54 0.60 0.81 0.84 0.85 0.85 0.84 0.85 0.84 0.74 0.78 0.78 0.74 0.78 0.74 Tune
0.50 0.49 0.65 0.50 0.51 0.67 0.69 0.69 0.73 0.69 0.67 0.69 0.60 0.61 0.71 0.60 0.59 0.72 Trans
0.49 0.52 0.64 0.49 0.52 0.66 0.66 0.66 0.69 0.66 0.67 0.67 0.61 0.63 0.72 0.61 0.59 0.71 Test
100
0.80 0.80 0.84 0.80 0.80 0.84 0.89 0.89 0.90 0.89 0.89 0.89 0.91 0.92 0.92 0.91 0.92 0.91 Tune
0.80 0.79 0.80 0.80 0.81 0.83 0.83 0.85 0.84 0.83 0.82 0.82 0.91 0.91 0.89 0.91 0.90 0.91 Trans
0.81 0.80 0.81 0.81 0.80 0.82 0.80 0.81 0.81 0.80 0.81 0.81 0.90 0.90 0.90 0.90 0.90 0.90 Test
[gcat]
10
0.74 0.83 0.82 0.74 0.79 0.81 0.44 0.47 0.46 0.44 0.47 0.46 0.69 0.79 0.75 0.69 0.79 0.75 Tune
0.69 0.68 0.75 0.69 0.72 0.76 0.60 0.62 0.69 0.60 0.59 0.62 0.71 0.73 0.82 0.71 0.69 0.76 Trans
0.66 0.67 0.73 0.66 0.71 0.74 0.58 0.61 0.66 0.58 0.60 0.59 0.69 0.69 0.81 0.69 0.69 0.75 Test
100
0.77 0.77 0.90 0.77 0.77 0.91 0.92 0.92 0.93 0.92 0.92 0.92 0.97 0.96 0.97 0.97 0.96 0.96 Tune
0.81 0.80 0.89 0.81 0.81 0.90 0.88 0.88 0.84 0.88 0.86 0.85 0.96 0.97 0.95 0.96 0.96 0.96 Trans
0.80 0.80 0.89 0.80 0.80 0.90 0.86 0.86 0.85 0.86 0.86 0.86 0.96 0.96 0.96 0.96 0.96 0.96 Test
[WISH-politics]
10
0.70 0.77 0.79 0.70 0.77 0.82 0.61 0.62 0.61 0.61 0.62 0.61 0.74 0.78 0.74 0.74 0.78 0.76 Tune
0.50 0.56 0.63 0.50 0.62 0.56 0.58 0.58 0.61 0.58 0.55 0.53 0.62 0.62 0.69 0.62 0.62 0.61 Trans
0.52 0.56 0.60 0.52 0.62 0.53 0.52 0.53 0.53 0.52 0.54 0.52 0.57 0.58 0.61 0.57 0.62 0.60 Test
100
0.75 0.75 0.75 0.75 0.75 0.74 0.74 0.75 0.76 0.74 0.75 0.75 0.79 0.80 0.80 0.79 0.80 0.80 Tune
0.73 0.73 0.71 0.73 0.73 0.70 0.65 0.66 0.67 0.65 0.64 0.64 0.76 0.74 0.75 0.76 0.75 0.76 Trans
0.75 0.75 0.72 0.75 0.75 0.71 0.64 0.63 0.63 0.64 0.63 0.64 0.78 0.76 0.77 0.78 0.76 0.77 Test
[WISH-products]
10
0.89 0.89 0.67 0.89 0.89 0.67 0.19 0.22 0.16 0.19 0.22 0.16 0.76 0.80 0.74 0.76 0.80 0.74 Tune
0.87 0.87 0.66 0.87 0.87 0.61 0.31 0.29 0.32 0.31 0.24 0.25 0.56 0.52 0.58 0.56 0.54 0.56 Trans
0.90 0.90 0.67 0.90 0.90 0.61 0.22 0.23 0.30 0.22 0.24 0.27 0.50 0.53 0.62 0.50 0.54 0.59 Test
100
0.90 0.90 0.82 0.90 0.90 0.81 0.49 0.50 0.54 0.49 0.52 0.52 0.73 0.73 0.77 0.73 0.78 0.75 Tune
0.88 0.88 0.81 0.88 0.88 0.80 0.34 0.28 0.37 0.34 0.27 0.30 0.60 0.55 0.57 0.60 0.57 0.61 Trans
0.90 0.90 0.79 0.90 0.91 0.76 0.33 0.28 0.33 0.33 0.32 0.38 0.59 0.56 0.60 0.59 0.56 0.60 Test
Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,
the boldface indicates the maximum value in each row, as well as others in the row that are not statistically significantly
different based on a paired t-test.
u = 100 u = 1000
Metric l S3VM MR Best Tuning S3VM MR Best Tuning
accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test100 (27, 7, 46) (38, 0, 42) (20, 16, 44) (27, 6, 47) (37, 0, 43) (16, 19, 45) Test
Metric l S3VM MR Best Tuning S3VM MR Best Tuning
maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test100 (39, 0, 41) (34, 4, 42) (31, 15, 34) (39, 1, 40) (44, 4, 32) (26, 21, 33) Test
Metric l S3VM MR Best Tuning S3VM MR Best Tuning
AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test100 (43, 0, 37) (37, 0, 43) (38, 8, 34) (38, 0, 42) (46, 0, 34) (28, 24, 28) Test
Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of
the form ?(#trials worse than SVM, #trials equal to SVM, #trials better than SVM).?
25
u = 100 u = 1000
Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning
accuracy 10 0.61 0.62 0.67 0.68 0.61 0.63 0.64 0.67 Test100 0.81 0.82 0.83 0.85 0.81 0.82 0.83 0.85 Test
Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning
maxF1 10 0.59 0.61 0.64 0.59 0.59 0.61 0.61 0.59 Test100 0.76 0.75 0.76 0.75 0.76 0.76 0.76 0.76 Test
Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning
AUROC 10 0.63 0.64 0.72 0.61 0.63 0.64 0.67 0.61 Test100 0.87 0.87 0.87 0.87 0.87 0.86 0.87 0.86 Test
Table 4: Aggregate test results averaged over the 80 trials (8 datasets, 10 trials each) in a particular setting.
outperforms SVM in fewer trials than the other algo-
rithms in some settings for these two metrics. This
is because Best Tuning conservatively selects SVM
in many trials. The take home message is that tuning
using CV based on accuracy (and to a lesser extent
maxF1) appears to mitigate some risk involved in
applying SSL. AUROC, on the other hand, does not
appear as effective for this purpose. Table 3 (bottom)
shows that, for u = 1000, Best Tuning is worse than
SVM fewer times, but for u = 100, MR achieves
better performance overall.
We also compare overall average test performance
(across datasets) for each metric and l,u combina-
tion. Table 4 reports these results for accuracy,
maxF1, and AUROC. In terms of accuracy, we see
that the Best Tuning approach leads to better per-
formance than SVM, S3VM, or MR in all settings
when averaged over datasets. We appear to achieve
some synergy in dynamically choosing a different
algorithm in each trial. In terms of maxF1, Best
Tuning, S3VM, and MR are all at least as good as
SL in three of the four l,u settings, and nearly as
good in the fourth. Based on AUROC, though, the
results are mixed depending on the specific setting.
Notably, though, Best Tuning consistently leads to
worse performance than SL when using this metric.
8.3 A Note on Cloud Computing
The experiments were carried out using the Condor
High-Throughput Computing platform (Thain et al,
2005). We ran many trials per algorithm (using dif-
ferent datasets, l, u, and metrics). Each trial in-
volved training hundreds of models using different
parameter configurations repeated across five folds,
and then training once more using the selected pa-
rameters. In the end, we trained a grand total of
794,880 individual models to produce the results in
Table 2. Through distributed computing on approxi-
mately 50 machines in parallel, we were able to run
all these experiments in less than a week, while us-
ing roughly three months worth of CPU time.
9 Conclusions
We have explored ?realistic SSL,? where all parame-
ters are tuned via 5-fold cross validation, to simulate
a real-world experience of trying to use unlabeled
data in a novel NLP task. Our medium-scale empir-
ical study of SVM, S3VM, and MR revealed that no
algorithm is always superior, and furthermore that
there are cases in which each SSL algorithm we ex-
amined can perform worse than SVM (in some cases
significantly worse across 10 trials). To mitigate
such risks, we proposed a simple meta-level proce-
dure that selects one of the three models based on
tuning performance. While cross validation is often
dismissed for model selection in SSL due to a lack
of labeled data, this Best Tuning approach proves ef-
fective in helping to ensure that incorporating unla-
beled data does not hurt performance. Interestingly,
this works well only when optimizing accuracy dur-
ing tuning. For future work, we plan to extend this
study to include additional datasets, algorithms, and
tuning criteria. We also plan to develop more so-
phisticated techniques for choosing which SL/SSL
algorithm to use in practice.
Acknowledgments
A. Goldberg is supported in part by a Yahoo! Key
Technical Challenges Grant.
26
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled exam-
ples. Journal of Machine Learning Research, 7:2399?
2434, November.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT: Proceedings of the Workshop on Computa-
tional Learning Theory.
Ulf Brefeld and Tobias Scheffer. 2006. Semi-supervised
learning for structured output variables. In ICML06,
23rd International Conference on Machine Learning,
Pittsburgh, USA.
R. Bruce and J. Wiebe. 1994. Word-sense disambigua-
tion using decomposable models. In Proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 139?146.
Lijuan Cai and Thomas Hofmann. 2003. Text catego-
rization by boosting automatically extracted concepts.
In SIGIR ?03: Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in informaion retrieval.
Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classification by low density separation. In
Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics (AISTAT 2005).
Olivier Chapelle, Alexander Zien, and Bernhard
Scho?lkopf, editors. 2006. Semi-supervised learning.
MIT Press.
Tom Fawcett. 2004. ROC graphs: Notes and practical
considerations for researchers. Technical Report HPL-
2003-4.
Andrew B. Goldberg, Nathanael Fillmore, David Andrze-
jewski, Zhiting Xu, Bryan Gibson, and Xiaojin Zhu.
2009. May all your wishes come true: A study of
wishes and how to recognize them. In Proceedings
of NAACL HLT.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
M. Karlen, J. Weston, A. Erkan, and R. Collobert. 2008.
Large scale manifold transduction. In Andrew McCal-
lum and Sam Roweis, editors, Proceedings of the 25th
Annual International Conference on Machine Learn-
ing (ICML 2008), pages 448?455. Omnipress.
Vikas Sindhwani and S. Sathiya Keerthi. 2007. New-
ton methods for fast solution of semi-supervised linear
SVMs. In Leon Bottou, Olivier Chapelle, Dennis De-
Coste, and Jason Weston, editors, Large-Scale Kernel
Machines. MIT Press.
V. Sindhwani and D. Rosenberg. 2008. An rkhs for
multi-view learning and manifold co-regularization.
In Andrew McCallum and Sam Roweis, editors, Pro-
ceedings of the 25th Annual International Conference
on Machine Learning (ICML 2008), pages 976?983.
Omnipress.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. Beyond the point cloud: from transductive to
semi-supervised learning. In ICML05, 22nd Interna-
tional Conference on Machine Learning.
Vikas Sindhwani, Sathiya Keerthi, and Olivier Chapelle.
2006. Deterministic annealing for semi-supervised
kernel machines. In ICML06, 23rd International Con-
ference on Machine Learning, Pittsburgh, USA.
Douglas Thain, Todd Tannenbaum, and Miron Livny.
2005. Distributed computing in practice: the condor
experience. Concurrency - Practice and Experience,
17(2-4):323?356.
Xiaojin Zhu. 2005. Semi-supervised learning literature
survey. Technical Report 1530, Department of Com-
puter Sciences, University of Wisconsin, Madison.
27
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43?48,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Latent Dirichlet Allocation with Topic-in-Set Knowledge?
David Andrzejewski
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
andrzeje@cs.wisc.edu
Xiaojin Zhu
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
jerryzhu@cs.wisc.edu
Abstract
Latent Dirichlet Allocation is an unsupervised
graphical model which can discover latent top-
ics in unlabeled data. We propose a mech-
anism for adding partial supervision, called
topic-in-set knowledge, to latent topic mod-
eling. This type of supervision can be used
to encourage the recovery of topics which are
more relevant to user modeling goals than the
topics which would be recovered otherwise.
Preliminary experiments on text datasets are
presented to demonstrate the potential effec-
tiveness of this method.
1 Introduction
Latent topic models such as Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) have emerged as a use-
ful family of graphical models with many interesting
applications in natural language processing. One of
the key virtues of LDA is its status as a fully genera-
tive probabilistic model, allowing principled exten-
sions and variations capable of expressing rich prob-
lem domain structure (Newman et al, 2007; Rosen-
Zvi et al, 2004; Boyd-Graber et al, 2007; Griffiths
et al, 2005).
LDA is an unsupervised learning model. This
work aims to add supervised information in the form
of latent topic assignments to LDA. Traditionally,
topic assignments have been denoted by the variable
z in LDA, and we will call such supervised informa-
tion ?z-labels.? In particular, a z-label is the knowl-
? We would like to acknowledge the assistance of Brandi
Gancarz with the biological annotations. This work is supported
in part by the Wisconsin Alumni Research Foundation.
edge that the topic assignment for a given word po-
sition is within a subset of topics. As such, this work
is a combination of unsupervised model and super-
vised knowledge, and falls into the category simi-
lar to constrained clustering (Basu et al, 2008) and
semi-supervised dimensionality reduction (Yang et
al., 2006).
1.1 Related Work
A similar but simpler type of topic labeling infor-
mation has been applied to computer vision tasks.
Topic modeling approaches have been applied to
scene modeling (Sudderth et al, 2005), segmen-
tation, and classification or detection (Wang and
Grimson, 2008). In some of these vision applica-
tions, the latent topics themselves are assumed to
correspond to object labels. If labeled data is avail-
able, either all (Wang and Mori, 2009) or some (Cao
and Fei-Fei, 2007) of the z values can be treated as
observed, rather than latent, variables. Our model
extends z-labels from single values to subsets, thus
offer additional model expressiveness.
If the topic-based representations of documents
are to be used for document clustering or classi-
fication, providing z-labels for words can be seen
as similar to semi-supervised learning with labeled
features (Druck et al, 2008). Here the words are
features, and z-label guidance acts as a feature la-
bel. This differs from other supervised LDA vari-
ants (Blei and McAuliffe, 2008; Lacoste-Julien et
al., 2008) which use document label information.
The ?LDA model for statistical software debug-
ging (Andrzejewski et al, 2007) partitions the topics
into 2 sets: ?usage? topics which can appear in all
43
documents, and ?bug? topics which can only appear
in a special subset of documents. This effect was
achieved by using different ? hyperparameters for
the 2 subsets of documents. z-labels can achieve the
same effect by restricting the z?s in documents out-
side the special subset, so that the z?s cannot assume
the ?bug? topic values. Therefore, the present ap-
proach can be viewed as a generalization of ?LDA.
Another perspective is that our z-labels may
guide the topic model towards the discovery of sec-
ondary or non-dominant statistical patterns in the
data (Chechik and Tishby, 2002). These topics may
be more interesting or relevant to the goals of the
user, but standard LDA would ignore them in favor
of more prominent (and perhaps orthogonal) struc-
ture.
2 Our Model
2.1 Review of Latent Dirichlet Allocation
We briefly review LDA, following the notation
of (Griffiths and Steyvers, 2004) 1. Let there be
T topics. Let w = w1 . . . wn represent a cor-
pus of D documents, with a total of n words. We
use di to denote the document of word wi, and zi
the hidden topic from which wi is generated. Let
?(w)j = p(w|z = j), and ?(d)j = p(z = j) for
document d. LDA involves the following generative
model:
? ? Dirichlet(?) (1)
zi|?(di) ? Multinomial(?(di)) (2)
? ? Dirichlet(?) (3)
wi|zi, ? ? Multinomial(?zi), (4)
where ? and ? are hyperparameters for the
document-topic and topic-word Dirichlet distribu-
tions, respectively. Even though they can be vector
valued, for simplicity we assume ? and ? are scalars,
resulting in symmetric Dirichlet priors.
Given our observed words w, the key task is in-
ference of the hidden topics z. Unfortunately, this
posterior is intractable and we resort to a Markov
Chain Monte Carlo (MCMC) sampling scheme,
specifically Collapsed Gibbs Sampling (Griffiths
and Steyvers, 2004). The full conditional equation
1We enclose superscripts in parentheses in this paper.
used for sampling individual zi values from the pos-
terior is given by
P (zi = v|z?i,w, ?, ?) ?(
n(d)?i,v + ??T
u (n(d)?i,u + ?)
)(
n(wi)?i,v + ??W
w?(? + n(w
?)
?i,v)
)
(5)
where n(d)?i,v is the number of times topic v is used in
document d, and n(wi)?i,v is the number of times word
wi is generated by topic v. The ?i notation signifies
that the counts are taken omitting the value of zi.
2.2 Topic-in-Set Knowledge: z-labels
Let
qiv =
(
n(d)?i,v + ??T
u (n(d)?i,u + ?)
)(
n(wi)?i,v + ??W
w?(? + n(w
?)
?i,v)
)
.
We now define our z-labels. Let C(i) be the set of
possible z-labels for latent topic zi. We set a hard
constraint by modifying the Gibbs sampling equa-
tion with an indicator function ?(v ? C(i)), which
takes on value 1 if v ? C(i) and is 0 otherwise:
P (zi = v|z?i,w, ?, ?) ? qiv?(v ? C(i)) (6)
If we wish to restrict zi to a single value (e.g., zi =
5), this can now be accomplished by setting C(i) =
{5}. Likewise, we can restrict zi to a subset of val-
ues {1, 2, 3} by setting C(i) = {1, 2, 3}. Finally, for
unconstrained zi we simply set C(i) = {1, 2, ..., T},
in which case our modified sampling (6) reduces to
the standard Gibbs sampling (5).
This formulation gives us a flexible method for in-
serting prior domain knowledge into the inference of
latent topics. We can set C(i) independently for ev-
ery single word wi in the corpus. This allows us, for
example, to force two occurrences of the same word
(e.g., ?Apple pie? and ?Apple iPod?) to be explained
by different topics. This effect would be impossible
to achieve by using topic-specific asymmetric ? vec-
tors and setting some entries to zero.
This hard constraint model can be relaxed. Let
0 ? ? ? 1 be the strength of our constraint, where
? = 1 recovers the hard constraint (6) and ? = 0
recovers unconstrained sampling (5):
P (zi = v|z?i,w, ?, ?) ? qiv
(
??(v ? C(i)) + 1? ?
)
.
44
While we present the z-label constraints as a me-
chanical modification to the Gibbs sampling equa-
tions, it can be derived from an undirected extension
of LDA (omitted here) which encodes z-labels. The
soft constraint Gibbs sampling equation arises nat-
urally from this formulation, which is the basis for
the First-Order Logic constraints described later in
the future work section.
3 Experiments
We now present preliminary experimental results to
demonstrate some interesting applications for topic-
in-set knowledge. Unless otherwise specified, sym-
metric hyperparameters ? = .5 and ? = .1 were
used and all MCMC chains were run for 2000 sam-
ples before estimating ? and ? from the final sample,
as in (Griffiths and Steyvers, 2004).
3.1 Concept Expansion
We explore the use of topic-in-set for identifying
words related to a target concept, given a set of
seed words associated with that concept. For ex-
ample, a biological expert may be interested in the
concept ?translation?. The expert would then pro-
vide a set of seed words which are strongly related
to this concept, here we assume the seed word set
{translation,trna,anticodon,ribosome}. We add the
hard constraint that zi = 0 for all occurrences of
these four words in our corpus of approximately
9,000 yeast-related abstracts.
We ran LDA with the number of topics T = 100,
both with and without the z-label knowledge on the
seed words. Table 1 shows the most probable words
in selected topics from both runs. Table 1a shows
Topic 0 from the constrained run, while Table 1b
shows the topics which contained seed words among
the top 50 most probable words from the uncon-
strained run.
In order to better understand the results, these
top words were annotated for relevance to the tar-
get concept (translation) by an outside biological ex-
pert. The words in Table 1 were then colored blue
if they were one of the original seed words, red if
they were judged as relevant, and left black other-
wise. From a quick glance, we can see that Topic
0 from the constrained run contains more relevant
terms than Topic 43 from the standard LDA run.
Topic 31 has a similar number of relevant terms, but
taken together we can see that the emphasis of Topic
31 is slightly off-target, more focused on ?mRNA
turnover? than ?translation?. Likewise, Topic 73
seems more focused on the ribosome itself than the
process of translation. Overall, these results demon-
strate the potential effectiveness of z-label informa-
tion for guiding topic models towards a user-seeded
concept.
3.2 Concept Exploration
Suppose that a user has chosen a set of terms and
wishes to discover different topics related to these
terms. By constraining these terms to only appear
in a restricted set of topics, these terms will be con-
centrated in the set of topics. The split within those
set of topics may be different from what a standard
LDA will produce, thus revealing new information
within the data.
To make this concrete, say we are interested in
the location ?United Kingdom?. We seed this con-
cept with the following LOCATION-tagged terms
{britain, british, england, uk, u.k., wales, scotland,
london}. These terms are then restricted to ap-
pear only in the first 3 topics. Our corpus is an
entity-tagged Reuters newswire corpus used for the
CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003). In order to focus on our tar-
get location, we also restrict all other LOCATION-
tagged tokens to not appear in the first 3 topics. For
this experiment we set T = 12, arrived at by trial-
and-error in the baseline (standard LDA) case.
The 50 most probable words for each topic are
shown in Figure 2, and tagged entities are prefixed
with their tags for easy identification. Table 2a
shows the top words for the first 3 topics of our z-
label run. These three topics are all related to the
target LOCATION United Kingdom, but they also
split nicely into business, cricket, and soccer. Words
which are highly relevant to each of these 3 concepts
are colored blue, red, and green, respectively.
In contrast, in Table 2b we show topics from stan-
dard LDA which contain any of the ?United King-
dom? LOCATION terms (which are underlined)
among the 50 most probable words for that topic.
We make several observations about these topics.
First, standard LDA Topic 0 is mostly concerned
with political unrest in Russia, which is not particu-
45
Topic 0
translation, ribosomal, trna, rrna, initiation, ribosome, protein, ribosomes, is, factor, processing, translational
nucleolar, pre-rrna, synthesis, small, 60s, eukaryotic, biogenesis, subunit, trnas, subunits, large, nucleolus
factors, 40, synthetase, free, modification, rna, depletion, eif-2, initiator, 40s, ef-3, anticodon, maturation
18s, eif2, mature, eif4e, associated, synthetases, aminoacylation, snornas, assembly, eif4g, elongation
(a) Topic 0 with z-label
Topic 31
mrna, translation, initiation, mrnas, rna, transcripts, 3, transcript, polya, factor, 5, translational, decay, codon
decapping, factors, degradation, end, termination, eukaryotic, polyadenylation, cap, required, efficiency
synthesis, show, codons, abundance, rnas, aug, nmd, messenger, turnover, rna-binding, processing, eif2, eif4e
eif4g, cf, occurs, pab1p, cleavage, eif5, cerevisiae, major, primary, rapid, tail, efficient, upf1p, eif-2
Topic 43
type, is, wild, yeast, trna, synthetase, both, methionine, synthetases, class, trnas, enzyme, whereas, cytoplasmic
because, direct, efficiency, presence, modification, aminoacylation, anticodon, either, eukaryotic, between
different, specific, discussed, results, similar, some, met, compared, aminoacyl-trna, able, initiator, sam
not, free, however, recognition, several, arc1p, fully, same, forms, leads, identical, responsible, found, only, well
Topic 73
ribosomal, rrna, protein, is, processing, ribosome, ribosomes, rna, nucleolar, pre-rrna, rnase, small, biogenesis
depletion, subunits, 60s, subunit, large, synthesis, maturation, nucleolus, associated, essential, assembly
components, translation, involved, rnas, found, component, mature, rp, 40s, accumulation, 18s, 40, particles
snornas, factors, precursor, during, primary, rrnas, 35s, has, 21s, specifically, results, ribonucleoprotein, early
(b) Standard LDA Topics
Figure 1: Concept seed words are colored blue, other words judged relevant to the target concept are colored
red.
larly related to the target location. Second, Topic 2
is similar to our previous business topic, but with
a more US-oriented slant. Note that ?dollar? ap-
pears with high probability in standard LDA Topic
2, but not in our z-label LDA Topic 0. Standard
LDA Topic 8 appears to be a mix of both soccer and
cricket words. Therefore, it seems that our topic-in-
set knowledge helps in distilling topics related to the
seed words.
Given this promising result, we attempted to
repeat this experiment with some other nations
(United States, Germany, China), but without much
success. When we tried to restrict these LOCATION
words to the first few topics, these topics tended to
be used to explain other concepts unrelated to the
target location (often other sports). We are investi-
gating the possible causes of this problem.
4 Conclusions and Future Work
We have defined Topic-in-Set knowledge and
demonstrated its use within LDA. As shown in the
experiments, the partial supervision provided by z-
labels can encourage LDA to recover topics rele-
vant to user interests. This approach combines the
pattern-discovery power of LDA with user-provided
guidance, which we believe will be very attractive to
practical users of topic modeling.
Future work will deal with at least two impor-
tant issues. First, when will this form of partial
supervision be most effective or appropriate? Our
experimental results suggest that this approach will
struggle if the user?s target concepts are simply not
prevalent in the text. Second, can we modify this
approach to express richer forms of partial super-
vision? More sophisticated forms of knowledge
may allow users to specify their preferences or prior
knowledge more effectively. Towards this end, we
are investigating the use of First-Order Logic in
specifying prior knowledge. Note that the set z-
labels presented here can be expressed as simple log-
ical formulas. Extending our model to general log-
ical formulas would allow the expression of more
powerful relational preferences.
References
David Andrzejewski, Anne Mulhern, Ben Liblit, and Xi-
aojin Zhu. 2007. Statistical debugging using latent
topic models. In Stan Matwin and Dunja Mladenic,
editors, 18th European Conference on Machine Learn-
ing, Warsaw, Poland.
46
Topic 0
million, company, ?s, year, shares, net, profit, half, group, [I-ORG]corp, market, sales, share, percent
expected, business, loss, stock, results, forecast, companies, deal, earnings, statement, price, [I-LOC]london
billion, [I-ORG]newsroom, industry, newsroom, pay, pct, analysts, issue, services, analyst, profits, sale
added, firm, [I-ORG]london, chief, quarter, investors, contract, note, tax, financial, months, costs
Topic 1
[I-LOC]england, [I-LOC]london, [I-LOC]britain, cricket, [I-PER]m., overs, test, wickets, scores, [I-PER]ahmed
[I-PER]paul, [I-PER]wasim, innings, [I-PER]a., [I-PER]akram, [I-PER]mushtaq, day, one-day, [I-PER]mark, final
[I-LOC]scotland, [I-PER]waqar, [I-MISC]series, [I-PER]croft, [I-PER]david, [I-PER]younis, match, [I-PER]ian
total, [I-MISC]english, [I-PER]khan, [I-PER]mullally, bat, declared, fall, [I-PER]d., [I-PER]g., [I-PER]j.
bowling, [I-PER]r., [I-PER]robert, [I-PER]s., [I-PER]steve, [I-PER]c. captain, golf, tour, [I-PER]sohail, extras
[I-ORG]surrey
Topic 2
soccer, division, results, played, standings, league, matches, halftime, goals, attendance, points, won, [I-ORG]st
drawn, saturday, [I-MISC]english, lost, premier, [I-MISC]french, result, scorers, [I-MISC]dutch, [I-ORG]united
[I-MISC]scottish, sunday, match, [I-LOC]london, [I-ORG]psv, tabulate, [I-ORG]hapoel, [I-ORG]sydney, friday
summary, [I-ORG]ajax, [I-ORG]manchester, tabulated, [I-MISC]german, [I-ORG]munich, [I-ORG]city
[I-MISC]european, [I-ORG]rangers, summaries, weekend, [I-ORG]fc, [I-ORG]sheffield, wednesday, [I-ORG]borussia
[I-ORG]fortuna, [I-ORG]paris, tuesday
(a) Topics with set z-labels
Topic 0
police, ?s, people, killed, [I-MISC]russian, friday, spokesman, [I-LOC]moscow, told, rebels, group, officials
[I-PER]yeltsin, arrested, found, miles, km, [I-PER]lebed, capital, thursday, tuesday, [I-LOC]chechnya, news
saturday, town, authorities, airport, man, government, state, agency, plane, reported, security, forces
city, monday, air, quoted, students, region, area, local, [I-LOC]russia, [I-ORG]reuters, military, [I-LOC]london
held, southern, died
Topic 2
percent, ?s, market, thursday, july, tonnes, week, year, lower, [I-LOC]u.s., rate, prices, billion, cents, dollar
friday, trade, bank, closed, trading, higher, close, oil, bond, fell, markets, index, points, rose
demand, june, rates, september, traders, [I-ORG]newsroom, day, bonds, million, price, shares, budget, government
growth, interest, monday, [I-LOC]london, economic, august, expected, rise
Topic 5
?s, match, team, win, play, season, [I-MISC]french, lead, home, year, players, [I-MISC]cup, back, minutes
champion, victory, time, n?t, game, saturday, title, side, set, made, wednesday, [I-LOC]england
league, run, club, top, good, final, scored, coach, shot, world, left, [I-MISC]american, captain
[I-MISC]world, goal, start, won, champions, round, winner, end, years, defeat, lost
Topic 8
division, [I-LOC]england, soccer, results, [I-LOC]london, [I-LOC]pakistan, [I-MISC]english, matches, played
standings, league, points, [I-ORG]st, cricket, saturday, [I-PER]ahmed, won, [I-ORG]united, goals
[I-PER]wasim, [I-PER]akram, [I-PER]m., [I-MISC]scottish, [I-PER]mushtaq, drawn, innings, premier, lost
[I-PER]waqar, test, [I-PER]croft, [I-PER]a., [I-PER]younis, declared, wickets, [I-ORG]hapoel, [I-PER]mullally
[I-ORG]sydney, day, [I-ORG]manchester, [I-PER]khan, final, scores, [I-PER]d., [I-MISC]german, [I-ORG]munich
[I-PER]sohail, friday, total, [I-LOC]oval
Topic 10
[I-LOC]germany, ?s, [I-LOC]italy, [I-LOC]u.s., metres, seconds, [I-LOC]france, [I-LOC]britain, [I-LOC]russia
world, race, leading, [I-LOC]sweden, [I-LOC]australia, [I-LOC]spain, women, [I-MISC]world, [I-LOC]belgium
[I-LOC]netherlands, [I-PER]paul, [I-LOC]japan, [I-MISC]olympic, [I-LOC]austria, [I-LOC]kenya, men, time
results, [I-LOC]brussels, [I-MISC]cup, [I-LOC]canada, final, minutes, record, [I-PER]michael, meeting, round
[I-LOC]norway, friday, scores, [I-PER]mark, [I-PER]van, [I-LOC]ireland, [I-PER]peter, [I-MISC]grand
[I-MISC]prix, points, saturday, [I-LOC]finland, cycling, [I-ORG]honda
(b) Standard LDA Topics
Figure 2: Topics containing ?United Kingdom? location words. Words related to business are colored blue,
cricket red, and soccer green.
Sugato Basu, Ian Davidson, and Kiri Wagstaff, edi-
tors. 2008. Constrained Clustering: Advances in
Algorithms, Theory, and Applications. Chapman &
Hall/CRC Press.
David Blei and Jon McAuliffe. 2008. Supervised topic
models. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 121?128. MIT Press,
47
Cambridge, MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 1024?1033.
Liangliang Cao and Li Fei-Fei. 2007. Spatially coher-
ent latent topic model for concurrent segmentation and
classification of objects and scenes. In ICCV, pages 1?
8.
Gal Chechik and Naftali Tishby. 2002. Extracting rel-
evant structures with side information. In NIPS 15,
pages 857?864. MIT press.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In SIGIR 2008, pages 595?
602, New York, NY, USA. ACM.
Thomas Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. Proceedings of the National Academy of
Sciences, 101(suppl. 1):5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In NIPS 17.
S. Lacoste-Julien, F. Sha, and M. Jordan. 2008. Disclda:
Discriminative learning for dimensionality reduction
and classification. In Advances in Neural Information
Processing Systems 21 (NIPS08).
David Newman, Kat Hagedorn, Chaitanya
Chemudugunta, and Padhraic Smyth. 2007. Subject
metadata enrichment using statistical topic models.
In JCDL ?07: Proceedings of the 7th ACM/IEEE-CS
joint conference on Digital libraries, pages 366?375,
New York, NY, USA. ACM.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence (UAI),
pages 487?494, Arlington, Virginia, United States.
AUAI Press.
Erik B. Sudderth, Antonio B. Torralba, William T. Free-
man, and Alan S. Willsky. 2005. Learning hierar-
chical models of scenes, objects, and parts. In ICCV,
pages 1331?1338.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142?147, Edmonton, Canada.
Xiaogang Wang and Eric Grimson. 2008. Spatial latent
dirichlet alocation. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, NIPS 20, pages 1577?1584.
MIT Press, Cambridge, MA.
Yang Wang and Greg Mori. 2009. Human action recog-
nition by semi-latent topic models. In IEEE Transac-
tions on Pattern Analysis and Machine Intelligence.
Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Barlow.
2006. Semi-supervised nonlinear dimensionality re-
duction. In ICML-06, 23nd International Conference
on Machine Learning.
48
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 563?567,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Behavioral Factors in Interactive Training of Text Classifiers
Burr Settles
Machine Learning Department
Carnegie Mellon University
Pittsburgh PA 15213, USA
bsettles@cs.cmu.edu
Xiaojin Zhu
Computer Sciences Department
University of Wisconsin
Madison WI 53715, USA
jerryzhu@cs.wisc.edu
Abstract
This paper describes a user study where hu-
mans interactively train automatic text clas-
sifiers. We attempt to replicate previous re-
sults using multiple ?average? Internet users
instead of a few domain experts as annotators.
We also analyze user annotation behaviors to
find that certain labeling actions have an im-
pact on classifier accuracy, drawing attention
to the important role these behavioral factors
play in interactive learning systems.
1 Introduction
There is growing interest in methods that incorpo-
rate human domain knowledge in machine learning
algorithms, either as priors on model parameters or
as constraints in an objective function. Such ap-
proaches lend themselves well to natural language
tasks, where input features are often discrete vari-
ables that carry semantic meaning (e.g., words). A
feature label is a simple but expressive form of do-
main knowledge that has received considerable at-
tention recently (Druck et al, 2008; Melville et al,
2009). For example, a single feature (word) can be
used to indicate a particular label or set of labels,
such as ?excellent?? positive or ?terrible?? neg-
ative, which might be useful word-label rules for a
sentiment analysis task.
Contemporary work has also focused on mak-
ing such learning algorithms active, by enabling
them to pose ?queries? in the form of feature-based
rules to be labeled by annotators in addition to ?
and sometimes lieu of ? data instances such as
documents (Attenberg et al, 2010; Druck et al,
2009). These concepts were recently implemented
in a practical system for interactive training of text
classifiers called DUALIST1. Settles (2011) reports
that, in user experiments with real annotators, hu-
mans were able to train near state of the art classi-
fiers with only a few minutes of effort. However,
there were only five subjects, who were all com-
puter science researchers. It is possible that these
positive results can be attributed to the subjects? im-
plicit familiarity with machine learning and natural
language processing algorithms.
This short paper sheds more light on previous ex-
periments by replicating them with many more hu-
man subjects, and of a different type: non-experts
recruited through the Amazon Mechanical Turk ser-
vice2. We also analyze the impact of annotator be-
havior on the resulting classifiers, and suggest rela-
tionships to recent work in curriculum learning.
2 DUALIST
Figure 1 shows a screenshot of DUALIST, an inter-
active machine learning system for quickly build-
ing text classifiers. The annotator is allowed to
take three kinds of actions: ? label query docu-
ments (instances) by clicking class-label buttons in
the left panel, ? label query words (features) by
selecting them from the class-label columns in the
right panel, or ? ?volunteer? domain knowledge by
typing labeled words into a text box at the top of
each class column. The underlying classifier is a
na??ve Bayes variant combining informative priors,
1http://code.google.com/p/dualist/
2http://mturk.com
563
12
3
Figure 1: The DUALIST user interface.
maximum likelihood estimation, and the EM algo-
rithm for fast semi-supervised training. When a
user performs action ? or ?, she labels queries that
should help minimize the classifier?s uncertainty on
unlabeled documents (according to active learning
heuristics). For action ?, the user is free to volun-
teer any relevant word, whether or not it appears in
a document or word column. For example, the user
might volunteer the labeled word ?oscar? ? posi-
tive in a sentiment analysis task for movie reviews
(leveraging her knowledge of domain), even if the
word ?oscar? does not appear anywhere in the in-
terface. This flexibility goes beyond traditional ac-
tive learning, which restricts the user to feedback on
items queried by the learner (i.e., actions ? and ?).
After a few labeling actions, the user submits her
feedback and receives the next set of queries in real
time. For more details, see Settles (2011).
3 Experimental Setup
We recruited annotators through the crowdsourcing
marketplace Mechanical Turk. Subjects were shown
a tutorial page with a brief description of the clas-
sification task, as well as a cartoon of the interface
similar to Figure 1 explaining the various annotation
options. When they decided they were ready, users
followed a link to a web server running a customized
version of DUALIST, which is an open source web-
based application. At the end of each trial, subjects
were given a confirmation code to receive payment.
We conducted experiments using two corpora
from the original DUALIST study: Science (a subset
of the 20 Newsgroups benchmark: cryptography,
electronics, medicine, and space) and Movie Re-
views (a sentiment analysis collection). These are
not specialized domains, i.e., we could expect av-
erage Internet users to be knowledgable enough to
perform the annotations. While both are generally
accessible, these corpora represent different types of
tasks and vary both in number of categories (four
vs. two) and difficulty (Movie Reviews is known to
be harder for learning algorithms). We replicated
the same experimental conditions as previous work:
DUALIST (the full interface in Figure 1), active-doc
(the left-hand ? document panel only), and passive-
doc (the ? document panel only, but with texts se-
lected at random and not queried by active learning).
For each condition, we recruited 25 users for the
Science corpus (75 total) and 35 users for Movie Re-
views (105 total). We were careful to publish tasks
on MTurk in a way that no one user annotated more
than one condition. Some users experienced techni-
cal difficulties that nullified their work, and four ap-
peared to be spammers3. After removing these sub-
jects from the analysis, we were left with 23 users
for the Science DUALIST condition, 25 each for the
two document-only conditions (73 total), 32 users
for the Movie Reviews DUALIST condition, and
33 each for the document-only conditions (98 total).
DUALIST automatically logged data about user ac-
tions and model accuracies as training progressed,
although users could not see these statistics. Trials
lasted 6 minutes for the Science corpus and 10 min-
utes for Movie Reviews. We did advertise a ?bonus?
for the user who trained the best classifier to encour-
age correctness, but otherwise offered no guidance
on how subjects should prioritize their time.
4 Results
Figure 2(a) shows learning curves aggregated across
all users in each experimental condition. Curves are
LOESS fits to classifier accuracy over time: locally-
weighted polynomial regressions (Cleveland et al,
1992) ?1 standard error, with the actual user data
points omitted for clarity. For the Science task (top),
DUALIST users trained significantly better classi-
fiers after about four minutes of annotation time.
Document-only active learning also outperformed
3A spammer was ruled to be one whose document error rate
(vs. the gold standard) was more than double the chance error,
and whose feature labels appeared to be arbitrary clicks.
564
0.20
0.30
0.40
0.50
0.60
0.70
 0  60  120  180  240  300  360
Sc
ien
ce
 
DUALIST
active-doc
passive-doc
0.49
0.52
0.55
0.58
0.61
0.64
 0  120  240  360  480  600
M
ov
ie 
Re
vie
ws
annotation time (sec)
DUALIST
active-doc
passive-doc
(a) learning curves
DUALIST active-doc passive-doc
0.
3
0.
5
0.
7
DUALIST active-doc passive-doc
0.
50
0.
60
0.
70
(b) final classifier accuracies
0.20
0.30
0.40
0.50
0.60
0.70
 0  60  120  180  240  300  360
Sc
ien
ce
 
DV++ (5)
DV+ (9)
DV- (9)
0.49
0.52
0.55
0.58
0.61
0.64
 0  120  240  360  480  600
M
ov
ie 
Re
vie
ws
annotation time (sec)
DV++ (8)
DV+ (13)
DV- (11)
(c) behavioral subgroup curves
Figure 2: (a) Learning curves plotting accuracy vs. actual annotation time for the three conditions. Curves are LOESS
fits (?1 SE) to all classifier accuracies at that point in time. (b) Box plots showing the distribution of final accuracies
under each condition. (c) Learning curves for three behavioral subgroups found in the DUALIST condition. The
DV++ group volunteered many labeled words (action ?), DV+ volunteered some, and DV- volunteered none.
standard passive learning, which is consistent with
previous work. However, for Movie Reviews (bot-
tom), there is little difference among the three set-
tings, and in fact models trained with DUALIST ap-
pear to lag behind active learning with documents.
Figure 2(b) shows the distribution of final classi-
fier accuracies in each condition. For Science, the
DUALIST users are significantly better than either
of the baselines (two-sided KS test, p < 0.005).
While the differences in DUALIST accuracies are
not significantly different, we can see that the top
quartile does much better than the two baselines.
Clearly some DUALIST users are making better use
of the interface and training better classifiers. How?
It is important to note that users in the active-
doc and passive-doc conditions can only choose ac-
tion ? (label documents), whereas those in the DU-
ALIST condition must allocate their time among
three kinds of actions. It turns out that the anno-
tators exhibited very non-uniform behavior in this
respect. In particular, activity of action ? (volunteer
labeled words) follows a power law, and many sub-
jects volunteered no features at all. By inspecting
the distribution of these actions for natural break-
points, we identified three subgroups of DUALIST
users: DV++ (many volunteered words), DV+ (some
words), and DV- (none; labeled queries only). Note
Movie Reviews Science
Group # Words Users # Words Users
DV++ 21?62 8 24?42 5
DV+ 1?15 13 2?19 9
DV- 0 11 0 9
Table 1: The range of volunteered words and number of
users in each behavioral subgroup of DUALIST subjects.
that DV- is not functionally equivalent to the active-
doc condition, as users in the DV- group could still
view and label word queries. The three behavioral
subgroups are summarized in Table 1.
Figure 2(c) shows learning curves for these three
groups. We can see that the DV++ and DV+ groups
ultimately train better classifiers than the DV- group,
and DV++ also dominates both the active and pas-
sive baselines from Figure 2(a). The DV++ group is
particularly effective on the Movie Reviews corpus.
This suggests that a user?s choice to volunteer more
labeled features ? by occasionally side-stepping the
queries posed by the active learner and directly in-
jecting their domain knowledge ? is a good predic-
tor of classifier accuracy on this task.
To tease apart the relative impact of other behav-
iors, we conducted an ordinary least-squares regres-
sion to predict classifier accuracy at the end of a trial.
We included the number of user events for each ac-
565
tion as independent variables, plus two controls: the
subject?s document error rate in [0,1] with respect to
the gold standard, and class entropy in [0, logC] of
all labeled words (whereC is the number of classes).
The entropy variable is meant to capture how ?bal-
anced? a user?s word-labeling activity was for ac-
tions? and?, with the intuition that a skewed set of
words could confuse the learner, by biasing it away
from categories with fewer labeled words.
Table 2 summarizes these results. Surprisingly,
query-labeling actions (? and ?) have a relatively
small impact on accuracy. The number of volun-
teered words and entropy among word labels appear
to be the only two factors that are somewhat signif-
icant: the former is strongest in the Movie Reviews
corpus, the latter in Science4. Interestingly, there is a
strong positive correlation between these two factors
in the Movie Reviews corpus (Spearman?s ? = 0.51,
p = 0.02) but not in Science (? = 0.03). When we
consider change in word label entropy over time, the
Science DA++ group is balanced early on and be-
comes steadily more so on average , whereas
DA+ goes for several minutes before catching up
(and briefly overtaking) . This may account
for DA+?s early dip in accuracy in Figure 2(c). For
Movie Reviews, DA++ is more balanced than DA+
throughout the trial. DA++ labeled many words that
were also class-balanced, which may explain why
it is the best consistently-performing group. As is
common in behavior modeling with small samples,
the data are noisy and the regressions in Table 2 only
explain 33%?46% of the variance in accuracy.
5 Discussion
We were able to partially replicate the results from
Settles (2011). That is, for two of the same data sets,
some of the subjects using DUALIST significantly
outperformed those using traditional document-only
interfaces. However, our results show that the
gains come not merely from the interface itself, but
from which labeling actions the users chose to per-
form. As interactive learning systems continue to
expand the palette of interactive options (e.g., la-
4Science has four labels and a larger entropy range, which
might explain the importance of the entropy factor here. Also,
labels are more related to natural clusterings in this corpus
(Nigam et al, 2000), so class-balanced priors might be key for
DUALIST?s semi-supervised EM procedure to work well.
Movie Reviews Science
Action ? SE ? SE
(intercept) 0.505 0.038 *** 0.473 0.147 **
? label query docs 0.001 0.001 0.005 0.005
? label query words -0.001 0.001 0.000 0.001
? volunteer words 0.002 0.001 * 0.000 0.002
human error rate -0.036 0.109 -0.328 0.230
word label entropy 0.053 0.051 0.201 0.102 .
R2 = 0.4608 ** R2 = 0.3342
*** p < 0.001 ** p < 0.01 * p < 0.05 . p < 0.1
Table 2: Linear regressions estimating the accuracy of a
classifier as a function of annotator actions and behaviors.
beling and/or volunteering features), understanding
how these options impact learning becomes more
important. In particular, training a good classifier
in our experiments appears to be linked to (1) vol-
unteering more labeled words, and (2) maintaining
a class balance among them. Users who exhibited
both of these behaviors ? which are possibly arti-
facts of their good intuitions ? performed the best.
We posit that there is a conceptual connection be-
tween these insights and curriculum learning (Ben-
gio et al, 2009), the commonsense notion that learn-
ers perform better if they begin with clear and unam-
biguous examples before graduating to more com-
plex training data. A recent study found that some
humans use a curriculum strategy when teaching a
1D classification task to a robot (Khan et al, 2012).
About half of those subjects alternated between ex-
treme positive and negative instances in a relatively
class-balanced way. This behavior was explained by
showing that it is optimal under an assumption that,
in reality, the learning task has many input features
for which only one is relevant to the task.
Text classification exhibits similar properties:
there are many features (words), of which only a few
are relevant. We argue that labeling features can be
seen as a kind of training by curriculum. By volun-
teering labeled words in a class-balanced way (espe-
cially early on), a user provides clear, unambiguous
training signals that effectively perform feature se-
lection while biasing the classifier toward the user?s
hypothesis. Future research on mixed-initiative user
interfaces might try to detect and encourage these
kinds of annotator behaviors, and potentially im-
prove interactive machine learning outcomes.
566
Acknowledgments
This work was funded in part by DARPA, the
National Science Foundation (under grants IIS-
0953219 and IIS-0968487), and Google.
References
J. Attenberg, P. Melville, and F. Provost. 2010. A uni-
fied approach to active dual supervision for labeling
features and examples. In Proceedings of the Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD), pages 40?55. Springer.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In Proceedings of the In-
ternational Conference on Machine Learning (ICML),
pages 119?126. Omnipress.
W.S. Cleveland, E. Grosse, and W.M. Shyu. 1992. Lo-
cal regression models. In J.M. Chambers and T.J.
Hastie, editors, Statistical Models in S. Wadsworth &
Brooks/Cole.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 595?602. ACM Press.
G. Druck, B. Settles, and A. McCallum. 2009. Ac-
tive learning by labeling features. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 81?90. ACL Press.
F. Khan, X. Zhu, and B. Mutlu. 2012. How do humans
teach: On curriculum learning and teaching dimen-
sion. In Advances in Neural Information Processing
Systems (NIPS), volume 24, pages 1449?1457. Mor-
gan Kaufmann.
P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sen-
timent analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of the In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 1275?1284. ACM Press.
K. Nigam, A.K. Mccallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using em. Machine Learning, 39:103?134.
B. Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1467?1478. ACL Press.
567
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 656?666,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Learning from Bullying Traces in Social Media
Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706, USA
{xujm,deltakam,jerryzhu}@cs.wisc.edu
Amy Bellmore
Department of Educational Psychology
University of Wisconsin-Madison
Madison, WI 53706, USA
abellmore@wisc.edu
Abstract
We introduce the social study of bullying to
the NLP community. Bullying, in both physi-
cal and cyber worlds (the latter known as cy-
berbullying), has been recognized as a seri-
ous national health issue among adolescents.
However, previous social studies of bully-
ing are handicapped by data scarcity, while
the few computational studies narrowly re-
strict themselves to cyberbullying which ac-
counts for only a small fraction of all bullying
episodes. Our main contribution is to present
evidence that social media, with appropriate
natural language processing techniques, can
be a valuable and abundant data source for the
study of bullying in both worlds. We iden-
tify several key problems in using such data
sources and formulate them as NLP tasks, in-
cluding text classification, role labeling, senti-
ment analysis, and topic modeling. Since this
is an introductory paper, we present baseline
results on these tasks using off-the-shelf NLP
solutions, and encourage the NLP community
to contribute better models in the future.
1 Introduction to Bullying
Bullying, also called peer victimization, has been
recognized as a serious national health issue by
the White House (The White House, 2011), the
American Academy of Pediatrics (The American
Academy of Pediatrics, 2009), and the American
Psychological Association (American Psychological
Association, 2004). One is being bullied or victim-
ized when he or she is exposed repeatedly over time
to negative actions on the part of others (Olweus,
1993). Far-reaching and insidious sequelae of bul-
lying include intrapersonal problems (Juvonen and
Graham, 2001; Jimerson, Swearer, and Espelage,
2010) and lethal school violence in the most extreme
cases (Moore et al, 2003). Youth who experience
peer victimization report more symptoms of depres-
sion, anxiety, loneliness, and low self-worth com-
pared to their nonvictimized counterparts (Bellmore
et al, 2004; Biggs, Nelson, and Sampilo, 2010; Gra-
ham, Bellmore, and Juvonen, 2007; Hawker and
Boulton, 2000). Other research suggests that victim-
ized youth have more physical complaints (Fekkes
et al, 2006; Nishina and Juvonen, 2005; Gini and
Pozzoli, 2009). Victimized youth are absent from
school more often and get lower grades than nonvic-
timized youth (Ladd, Kochenderfer, and Coleman,
1997; Schwartz et al, 2005; Juvonen and Gross,
2008).
Bullying happens traditionally in the physical
world and, recently, online as well; the latter is
known as cyberbullying (Cassidy, Jackson, and
Brown, 2009; Fredstrom, Adams, and Gilman,
2011; Wang, Iannotti, and Nansel, 2009; Vande-
bosch and Cleemput, 2009). Bullying usually starts
in primary school, peaks in middle school, and lasts
well into high school and beyond (Nansel et al,
2001; Smith, Madsen, and Moody, 1999; Cook et
al., 2010). Across a national sample of students in
grades 4 through 12, 38% of students reported be-
ing bullied by others and 32% reported bullying oth-
ers (Vaillancourt et al, 2010).
656
reinforcer
bystander
bully victim
assistant defender reporter
accuser
Figure 1: The roles in a bullying episode. Solid circles
represent traditional roles in social science, while dotted
circles are new roles we augmented for social media. The
width of the edges represents interaction strength.
1.1 The Structure of a Bullying Episode
Bullying takes multiple forms, most noticeably face-
to-face physical (e.g., hitting), verbal (e.g., name-
calling), and relational (e.g., exclusion) (Archer and
Coyne, 2005; Little et al, 2003; Nylund et al,
2007). Cyberbullying reflects a venue (other than
face to face contact) through which verbal and rela-
tional forms can occur.
A main reason individuals are targeted with bul-
lying is perceived differences, i.e., any characteristic
that makes an individual stand out differently from
his or her peers. These include race, socio-economic
status, gender, sexuality, physical appearance, and
behaviors.
Participants in a bullying episode take well-
defined roles (see Figure 1). More than one person
can have the same role in a bullying episode. Roles
include the bully (or bullies), the victims, bystanders
(who saw the event but did not intervene), defend-
ers of the victim, assistants to the bully (who did
not initiate but went along with the bully), and rein-
forcers (who did not directly join in with the bully
but encouraged the bully by laughing, for exam-
ple) (Salmivalli, 1999). This recognition that bully-
ing involves multiple roles makes evident the broad-
ranging impact of bullying; any child or adolescent
is susceptible to participation in bullying, even those
who are not directly involved (Janosz et al, 2008;
Rivers et al, 2009).
1.2 Some Scientific Questions NLP can Answer
Like many complex social issues, effective solutions
to bullying go beyond technology alone and require
the concerted efforts of parents, educators, and law
enforcement. To guide these efforts it is paramount
to study the dynamics of bullying. Such study criti-
cally depends on text in the form of self-report social
study surveys and electronic communication among
participants. Such text is often fragmental, noisy,
and covers only part of a bullying episode from a
specific role?s perspective. As such, the NLP com-
munity can help answer a host of scientific ques-
tions: Which pieces of text refer to the same under-
lying bullying episode? What is the form, reason,
location, time, etc. of a bullying episode? Who are
the participants of each episode, and what are their
roles? How does a person?s role evolve over time?
This paper presents our initial investigation on some
of these questions, while leaving others to future re-
search by the NLP community.
1.3 Limitations of the State-of-the-Art
The social science study of bullying has a long his-
tory. However, a fundamental problem there is data
acquisition. The standard approach is to conduct
time-consuming personal surveys in schools. The
sample size is typically in the hundreds, and partici-
pants typically write 3 to 4 sentences about each bul-
lying episode (Nishina and Bellmore, 2010). Such a
small corpus fails to assess the true frequency of bul-
lying over the population, and cannot determine the
evolution of roles. The computational study of bul-
lying is largely unexplored, with the exception of a
few studies on cyberbullying (Lieberman, Dinakar,
and Jones, 2011; Dinakar, Reichart, and Lieber-
man, 2011; Ptaszynski et al, 2010; Kontostathis,
Edwards, and Leatherman, 2010; Bosse and Stam,
2011; Latham, Crockett, and Bandar, 2010). These
studies did not consider the much more frequent bul-
lying episodes in the physical world.
2 Bullying Traces in Social Media
The main contribution of the present paper is not
on novel algorithms, but rather on presenting evi-
dence that social media data and off-the-shelf NLP
tools can be an effective combination for the study
of bullying. Participants of a bullying episode (in ei-
ther physical or cyber venues) often post social me-
dia text about the experience. We collectively call
such social media posts bullying traces. Bullying
657
traces include but far exceed incidences of cyberbul-
lying. Most of them are in fact responses to a bul-
lying experience ? the actual attack is hidden from
view. Bullying traces are valuable, albeit fragmental
and noisy, data which we can use to piece together
the underlying episodes.
In the rest of the paper, we focus on publicly
available Twitter ?tweets,? though our methods
apply readily to other social media services, too.
Here are some examples of bullying traces:
? Reporting a bullying episode: ?some tweens
got violent on the n train, the one boy got off
after blows 2 the chest... Saw him cryin as he
walkd away :( bullying not cool?
? Accusing someone as a bully: ?@USERNAME
i didnt jump around and act like a monkey T T
which of your eye saw that i acted like a monkey
:( you?re a bully?
? Revealing self as a victim: ?People bullied me
for being fat. 7 years later, I was diagnosed
with bulimia. Are you happy now??
? Cyber-bullying direct attack: ?Lauren is a fat
cow MOO BITCH?
Bullying traces are abundant. From the publicly
available 2011 TREC Microblog track corpus (16
million tweets sampled between January 23rd and
February 8th, 2011), we uniformly sampled 990
tweets for manual inspection by five experienced an-
notators (not the authors of the present paper). Of
the 990 tweets, the annotators labeled 617 as non-
English, 371 as English but not bullying traces, and
2 as English bullying traces. The Maximum Likeli-
hood Estimate of the frequency of English bullying
traces, out of all tweets, is 2/990 ? 0.002. The
exact Binomial 95% confidence interval is (0.0002,
0.0073). This is a tiny fraction. Nonetheless, it rep-
resents an abundance of tweets: by some estimates,
Twitter produces 250 million tweets per day in late
2011. Even with the lower bound in the confidence
interval, it translates into 50,000 English bullying
traces per day. The actual number can be much
higher.
Bullying traces contain valuable information. For
example, Figure 2 shows the daily number of bully-
ing traces identified by our classifier, to be discussed
Figure 2: Temporal variation of bullying traces
in section 3. A weekly pattern was obvious in late
August. A small peak was caused by 14-year-old
bullying victim Jamey Rodemeyer?s suicide on Sept.
18. This was followed by a large peak after Lady
Gaga dedicated a song to him on Sept. 24.
In the following sections, we identify several key
problems in using social media for the study of bul-
lying. We formulate each key problem as an NLP
task. We then present standard off-the-shelf NLP ap-
proaches to establish baseline performances. Since
bullying traces account for only a tiny fraction of all
tweets, it posed a significant challenge for our an-
notators to find enough bullying traces without la-
beling an unreasonable amount of tweets. For this
reason, in the rest of the paper we restrict ourselves
to an ?enriched dataset.? This enriched dataset is ob-
tained by collecting tweets using the public Twitter
streaming API, such that each tweet contains at least
one of the following keywords: ?bully, bullied, bul-
lying.? We further removed re-tweets (the analogue
of forwarded emails) by excluding tweets containing
the acronym ?RT.? The enrichment process is meant
to retain many first-hand bullying traces at the cost
of a selection bias.
3 NLP Task A: Text Categorization
One important task is to distinguish bullying traces
from other social media posts. Our enriched dataset,
generated by simple keyword filtering, still contains
many irrelevant tweets. For example, ?Forced veg-
anism by removing a persons choice is just another
form of bullying? is not a bullying trace, since it does
658
not describe a bullying episode. Our task is to dis-
tinguish posts like this from true bullying traces such
as those mentioned in the previous section. We for-
mulate it as a binary text categorization task.
Methods. The same annotators who labeled the
TREC corpus labeled 1762 tweets sampled uni-
formly from the enriched dataset on August 6, 2011.
Among them, 684 (39%) were labeled as bullying
traces.
Following (Settles, 2011), these 1762 tweets were
case-folded but without any stemming or stop-
word removal. Any user mentions preceded by a
?@? were replaced by the anonymized user name
?@USERNAME?. Any URLs starting with ?http?
were replaced by the token ?HTTPLINK?. Hashtags
(compound words following ?#?) were not split and
were treated as a single token. Emoticons, such as
?:)? or ?:D?, were also included as tokens.
After these preprocessing procedures, we created
three different sets of feature representations: un-
igrams (1g), unigrams+bigrams (1g2g), and POS-
colored unigrams+bigrams (1g2gPOS). POS tag-
ging was done with the Stanford CoreNLP pack-
age (Toutanova et al, 2003). POS-coloring was
done by expanding each token into token:POS.
We chose four commonly used text classifiers,
namely, Naive Bayes, SVM with linear kernel
(SVM(linear)), SVM with RBF kernel (SVM(RBF))
and Logistic Regression (equivalent to MaxEnt). We
used the WEKA (Hall et al, 2009) implementation
for the first three (calling LibSVM (Chang and Lin,
2011) with WEKA?s interfaces for SVMs), and the
L1General package (Schmidt, Fung, and Rosales,
2007) for the fourth.
We held out 262 tweets for test, and systemat-
ically varied training set size among the remain-
ing tweets, from 100 to 1500 with the step-size
100. We tuned all parameters jointly by 5-fold
cross validation on the training set with the grid
{2?8, 2?6, . . . , 28}. All the four text classifiers were
trained on the training sets and tested on the test set.
The whole procedure was repeated 30 times for each
feature representation.
Results. Figure 3 reports the held-out set accu-
racy as the training set size increases. The error bars
are ?1 standard error. With the largest training set
size (1500), the combination of SVM(linear) + 1g
achieves an average accuracy 79.7%. SVM(linear)
+ 1g2g achieves 81.3%, which is significantly bet-
ter (t-test, p = 4 ? 10?6). It shows that in-
cluding bigrams can significantly improve the clas-
sification performance. SVM(linear) + 1g2gPOS
achieves 81.6%, though the improvement is not sta-
tistically significant (p = 0.088), which indicates
that POS coloring does not help too much on this
task. SVM(RBF) gives similar performance, Logis-
tic Regression is slightly worse and Naive Bayes is
much worse, for a large range of training set sizes.
In summary, SVM(linear) + 1g2g is the preferred
model because of its accuracy and simplicity. We
also note that these accuracies are much better than
the majority class baseline of 61%. On the held-
out set, SVM(linear) + 1g2g achieves precision
P=0.76, recall R=0.79, and F-measure 0.77.
Discussions. Note that the learning curves are
still increasing, suggesting that better accuracy can
be obtained if we annotate more training data. As to
why the best accuracy is not close to 1, one hypoth-
esis is noisy labels caused by intrinsic disagreement
among labelers. Tweets are short and some are am-
biguous. Without prior knowledge about the users
and their other tweets, labelers interpret the tweets
in their own ways. For example, for the very short
tweet feels like a bully..... our annotators disagreed
on whether it is a bullying trace. Labelers may have
different views on these ambiguous tweets and cre-
ated noisy bullying trace labels.
A future direction is to categorize bullying traces
at a finer granularity, e.g., by forms, reasons, etc.
This can be solved by multi-class classification
methods. Another direction is to extend the clas-
sifiers from the ?enriched data? to the full range of
tweets. Recall that the difference is whether we pre-
filter the tweets by keywords. Clearly, they have
different tweet distributions. Techniques used for
covariate shift may be adapted to solve this prob-
lem (Blitzer, 2008).
4 NLP Task B: Role Labeling
Identifying participants? bullying roles (Figure 1) is
another important task, which is also a prerequi-
site of studying how a person?s role evolves over
time. For bullying traces in social media, we aug-
ment the traditional role system with two new roles:
reporter (may not be present during the episode, un-
659
(a) 1g (b) 1g2g (c) 1g2gPOS
Figure 3: Learning Curves for different feature sets and classification algorithms
like a bystander) and accuser (accusing someone as
the bully). Both roles can be a victim, a defender,
or a bystander in the traditional sense ? there is just
not enough information in the tweet. Accuser (A),
bully (B), reporter (R) and victim (V) are the four
most frequent roles observed in social media. We
merged all remaining roles into a generic category
?other? (O) in the following study. Our task is to
classify the role (A, B, R, V, O) of the tweet author
and any person-mentions in a tweet. For example,
AUTHOR(R): ?We(R) visited my(V) cousin(V) today
& #Itreallymakesmemad that he(V) barely eats bec
he(V) was bullied . :( I(R) wanna kick the crap out
of those mean(B) kids(B).? Note that the special to-
ken ?AUTHOR? is introduced to hold the label of
the author?s role.
Labeling author?s role and other person-mention?s
role are two different sub-tasks. The former can be
formulated as a multi-class text classification task;
the latter is better formulated as a sequential tagging
task. We will discuss them separately below.
4.1 Author?s Roles
Methods. Our annotators labeled the author?s role
for each of the 684 positive bullying traces in Task A
(296 R, 162 V, 98 B, 86 A, 42 O). We used the same
classifiers and features in Section 3. We conducted
10-fold cross validation to evaluate all combinations
of classifiers and feature sets. Like before, we tuned
all parameters jointly by 5-fold cross validation on
the training set with the grid {2?8, 2?6, . . . , 28}.
Results. The best combination is SVM(linear)
+ 1g2g with cross validation accuracy 61%. Even
though it is far from perfect, it is significantly better
than the majority class (R) baseline of 43%. It shows
predicted as
A B R V O
A 33 3 39 10 1
B 5 25 57 11 0
R 15 5 249 27 0
V 1 4 48 109 0
O 1 1 37 3 0
Table 1: Confusion Matrix of Author Role Classification
that there is signal in the text to infer the authors?
roles.
Table 1 shows the confusion matrix of the best
model. Most R and V authors are correctly rec-
ognized, but not B and A. The model misclassified
many authors as R. It is possible that the tweets au-
thored by reporters are diverse in topic and style, and
overlap with other classes in the feature space.
Discussions. As tweets are short, our feature rep-
resentation may not be the best for predicting au-
thor?s role. Many authors mentioned themselves
in the tweets with first-person pronouns, making
it advantageous to consider joint classification by
merging sections 4.1 and 4.2. Furthermore, assum-
ing roles change infrequently, it may be helpful to
jointly classify many tweets authored by the same
person.
4.2 Person-Mention?s Roles
This sub-task labels each person-mention with a
bullying role. It uses Named Entity Recognition
(NER) (Finkel, Grenager, and Manning, 2005; Rati-
nov and Roth, 2009; Ritter et al, 2011) as a sub-
routine to identify named person entities, though we
are also interested in unnamed persons such as ?my
teacher? and pronouns. It is related to Semantic Role
660
Labeling (SRL) (Gildea and Jurafsky, 2002; Pun-
yakanok, Roth, and Yih, 2008) but differs critically
in that our roles are not tied to specific verb predi-
cates.
Methods. Our annotators labeled each token
in the 684 bullying traces with the tags A, B,
R, V, O and N for not-a-person. There are
11,751 tokens in total. Similar to the sequen-
tial tagging formulation (Ma`rquez et al, 2005; Liu
et al, 2010), we trained a linear CRF to label
each token in the tweet with the CRF++ package
(http://crfpp.sourceforge.net/).
As standard in linear CRFs, we used pairwise la-
bel features f(yi?1, yi) and input features f(yi,w),
where f ?s are binary indicator functions on the val-
ues of their arguments and w is the text. In the fol-
lowing, we introduce our input features using the ex-
ample tweet ?@USERNAME i?ll tell vinny you bul-
lied me.? with the current token wi =?vinny?:
(i) The token, lemma, and POS tag of the
five tokens around position i. For example,
fbully,wi?1=tell(yi,w) will be 1 if the current to-
ken has label yi = ?bully?? and wi?1 = ?tell??.
Similarly, fvictim,POSi+2=V BD(yi,w) will be 1 if
yi = ?victim?? and the POS of wi+2 is VBD.
(ii) The NER tag of wi.
(iii) Whether wi is a person mention. This is a
Boolean feature which is true if wi is tagged as PER-
SON by NER, or if POSi = pronoun (excluding
?it?), or if wi is @USERNAME. For example, this
feature is true on ?vinny? because it is tagged as
PERSON by NER.
(iv) The relevant verb vi of wi, vi?s lemma, POS,
and the combination of vi with the lemma/POS of
wi. The relevant verb vi of wi is defined by the
semantic dependency between wi and the verb, if
one exists. Otherwise, vi is the closest verb to wi.
For example, the relevant verb of wi = ?vinny?? is
vi = ?tell?? because ?vinny? is found as the object
of ?tell? by dependency parsing.
(v) The distance, relative position (left or right)
and dependency type between vi and wi. For ex-
ample, the distance between ?vinny? and its relevant
verb ?tell? is 1. ?vinny? is on the right and is the
object of ?tell?.
The lemma, POS tags, NER tags and dependency
relationship were obtained using Stanford CoreNLP.
As a baseline, we trained SVM(linear) with the
Accuracy Precision Recall F-1
CRF 0.87 0.53 0.42 0.47
SVM 0.85 0.42 0.31 0.36
Table 2: Cross Validation Result of Person-Mention
Roles
same input features as CRF. Classification is done
individually on each token. We randomly split the
684 tweets into 10 folds and conducted cross vali-
dation based on this split. For CRF, we trained on
the tweets in the training set with their labels, and
tested the model on those in the test set. For SVM,
we trained and tested at the token level in the corre-
sponding sets.
Results. Table 2 reports the cross validation ac-
curacy, precision, recall and F-1 measure. Accu-
racy measures the percentage of tokens correctly
assigned the groundtruth labels, including N (not-
a-person) tokens. Precision measures the fraction
of correctly labeled person-mention tokens over all
tokens that are not N according to the algorithm.
Recall measures the fraction of correctly labeled
person-mention tokens over all tokens that are not
N according to the groundtruth. F-1 is the har-
monic mean of precision and recall. Linear CRF
achieved an accuracy 0.87, which is higher than the
baseline of majority class predictor (N, 0.80) (t-
test, p = 10?10). However, the precision and re-
call is low potentially because the tweets are short
and noisy. CRF outperforms SVM in all measures,
showing the value of joint classification.
Discussions. Table 3 shows the confusion ma-
trix of person-mention role labeling by linear CRF.
There are several reasons for these mistakes. First,
words like ?teacher?, ?sister?, or ?girl? were missed
by our person mention feature (iii). Second, the
NER tagger was trained on formal English which is
a mismatch for the informal tweets, leading to NER
errors. Third, noisy labeling continues to affect ac-
curacy. For example, some annotators considered
?other people? as an entity and labeled both tokens
as person mentions; others labeled ?people? only.
In general, bullying role labeling may be im-
proved by jointly considering multiple tweets at the
episode level. Co-reference resolution should im-
prove the performance as well.
661
predicted as
A B R V O N
A 0 4 5 10 0 4
B 0 406 13 125 103 302
R 0 28 31 67 0 13
V 0 142 28 380 43 202
O 0 112 4 42 156 86
N 0 78 4 41 16 9306
Table 3: Confusion Matrix of Person-Mention Roles by
CRF
5 NLP Task C: Sentiment Analysis
Sentiment analysis on participants involved in a bul-
lying episode is of significant importance. As Fig-
ure 4 suggests, there are a wide range of emotions in
bullying traces. For example, victims usually expe-
rience negative emotions such as depression, anxiety
and loneliness; Some emotions are more violent or
even suicidal. Detecting at-risk individuals via sen-
timent analysis enables potential interventions. In
addition, social scientists are interested in sentiment
analysis of bullying participants to understand their
motivations.
In the present paper we investigate a special form
of sentiment in bullying traces, namely teasing. We
observed that many bullying traces were written jok-
ingly. One example of a teasing post is ?@USER-
NAME lol stop being a cyber bully lol :p.? Teas-
ing may indicate the lack of severity of a bullying
episode; It may also be a manifest of coping strate-
gies in bullying victims. Therefore, there is consid-
erable interest among social scientists to understand
teasing in bullying traces.
Methods. One first task is to identify teasing bul-
lying traces. We formulated it as a binary classifi-
cation problem, similar to classic positive/negative
sentiment classification (Pang and Lee, 2004). Our
annotators labeled each of the 684 bullying traces in
Task A as teasing (99) or not (585). We used the
same feature representations, classifiers and param-
eter tuning as in Section 3 and 10-fold cross valida-
tion procedure.
Results. The best cross validation accuracy of
89% is obtained by SVM(linear) + 1g2g. This
is significantly better than the majority class (not-
teasing) baseline of 86% (t-test, p = 10?33). It
shows that even simple features and off-the-shelf
predicted as
Tease Not
Tease 52 47
Not 26 559
Table 4: Confusion Matrix of Teasing Classification
classifier can detect some signal in the text. How-
ever, the accuracy is not high. Table 4 shows the
confusion matrix. About half of the tease examples
were misclassified. We found several possible ex-
planations. First, teasing is not always accompanied
by joking emoticons or tokens like ?LOL,? ?lmao,?
?haha.? For example, ?I may bully you but I love
you lots. Just like jelly tots!? and ?Been bullied into
watching a scary film, I love my friends!? Such teas-
ing sentiment requires deeper NLP or much larger
training sets. Second, tweets containing those jok-
ing emoticons and tokens are not necessarily teas-
ing. For example, ?This Year I?m Standing Up For
The Kids That Are Being Bullied All Over The Na-
tion :) .? Third, the joking tokens have diverse
spellings. For example, ?lol? was spelled as ?loll,?
?lolol,? ?lollll,? ?loool,? ?LOOOOOOOOOOOL?;
?haha? was spelled as ?HAHAHAHA,? ?Hahaha,?
?Bwahahaha,? ?ahahahah,? ?hahah.?
Discussions. Specialized word normalization for
social media text may significantly improve perfor-
mance. For example, word lengthening can be iden-
tified and used as cues for teasing (Brody and Di-
akopoulos, 2011). Teasing is diverse in its form
and content. Our training set is perhaps too small.
Borrowing training data from other corpora, such as
one-liner jokes (Mihalcea and Strapparava, 2005),
may be helpful.
6 NLP Task D: Latent Topic Modeling
Methods. Given the large volume of bullying traces,
methods for automatically analyzing what people
are talking about are needed. Latent topic models
allow us to extract the main topics in bullying traces
to facilitate understanding. We used latent Dirich-
let alocation (LDA) (Blei, Ng, and Jordan, 2003) as
our exploratory tool. Specifically, we ran a collapsed
Gibbs sampling implementation of LDA (Griffiths
and Steyvers, 2004).
The corpus consists of 188K enriched tweets from
Aug. 21 to Sept. 17, 2011 that are classified as
662
bullying traces by our classifier in Task A. We per-
formed stopword removal and further removed word
types occurring less than 7 times, resulting in a vo-
cabulary of size 12K. We set the number of topics
to 50, Dirichlet parameter for word multinomials to
? = 0.01, Dirichlet parameter for document topic
multinomial to ? = 1, and ran Gibbs sampling for
10K iterations.
Results. Space precludes a complete list of top-
ics. Figure 4 shows six selected topics discovered by
LDA. Recall that each topic in LDA is a multinomial
distribution over the vocabulary. The figure shows
each topic?s top 20 words with size proportional to
p(word | topic). The topic names are manually as-
signed.
These topics contain semantically coherent words
relevant to bullying: (feelings) how people feel
about bullying; (suicide) discussions of suicide
events; (family) sibling names probably used in a
good buddy sense; (school) the school environment
where bullying commonly occurs; (verbal bullying)
derogatory words such as fat and ugly; (physical bul-
lying) actions such as kicking and pushing.
We also ran a variational inference implementa-
tion of LDA (Blei, Ng, and Jordan, 2003). The re-
sults were similar, thus we omit discussion of them.
Discussions. Some recovered topics, including
the ones shown here, provide valuable insight into
bullying traces. However, not all topics are inter-
pretable to social scientists. It may be helpful to al-
low scientists the ability to combine their domain
knowledge with latent topic modeling, thus arriv-
ing at more useful topics. For example, the scien-
tists can formulate their knowledge in First-Order
Logic, which can then be combined with LDA with
stochastic optimization (Andrzejewski et al, 2011).
7 Conclusion and Future Work
We introduced social media as a large-scale, near
real-time, dynamic data source for the study of bul-
lying. Social media offers a broad range of bully-
ing traces that include but go beyond cyberbullying.
In the present paper, we have identified several key
problems in using social media to study bullying and
formulated them as familiar NLP tasks. Our baseline
performance with standard off-the-shelf approaches
shows that it is feasible to learn from bullying traces.
?feelings? ?suicide?
?family? ?school?
?verbal bullying? ?physical bullying?
Figure 4: Selected topics discovered by latent Dirichlet
allocation.
Much work remains in this new research direc-
tion. In the short term, we need to develop spe-
cialized NLP tools for processing bullying traces in
social media, similar to (Ritter et al, 2011; Liu et
al., 2010), to achieve better performance than mod-
els trained on formal English. In the long term, we
need to tackle the problem of piecing together the
underlying bullying episodes from fragmental bully-
ing traces. Consider two separate bullying episodes
with the following participants and roles:
E1: B: Buffy, V: Vivian & Virginia, O: Debra
E2: B: Burton, V: Buffy, O: Irene
The corresponding bullying traces can be three posts
in this order:
w1 Debra: Virginia, I heard Buffy call you and
Vivian fat?ignore her!
w2 Buffy to Irene: Burton picked on me again
because I?m only 5?1
w3 Vivian: Buffy I?m not fat! Stop calling me that.
Reconstructing E1, E2 fromw1,w2,w3 is challeng-
ing for a number of reasons: (1) There is no explicit
episode index in the posts. (2) Posts from a single
episode may be dispersed in time (e.g., w1,w3 be-
long to E1, but not w2), each containing only part
663
of an episode. (3) The number of episodes and peo-
ple can grow indefinitely as more posts arrive. (4)
People may switch roles in different episodes (e.g.,
Buffy was the bully in E1 but the victim in E2). Joint
probabilistic modeling over multiple posts using so-
cial network structures hold great promise in solving
this problem.
To facilitate bullying research in the NLP com-
munity, we make our annotations and software
publicly available at http://research.cs.
wisc.edu/bullying.
Acknowledgments
We thank Wei-Ting Chen, Rachael Hansen, Ting-
Lan Ma, Ji-in You and Bryan Gibson for their help
on the data and the paper.
References
[American Psychological Association2004] American
Psychological Association. 2004. APA reso-
lution on bullying among children and youth.
http://www.apa.org/about/governance/
council/policy/bullying.pdf.
[Andrzejewski et al2011] Andrzejewski, David, Xiaojin
Zhu, Mark Craven, and Ben Recht. 2011. A frame-
work for incorporating general domain knowledge into
Latent Dirichlet Allocation using First-Order Logic.
In the 22nd IJCAI, pages 1171?1177.
[Archer and Coyne2005] Archer, John and Sarah M.
Coyne. 2005. An integrated review of indirect, re-
lational, and social aggression. Personality and Social
Psychology Review, 9:212?230.
[Bellmore et al2004] Bellmore, Amy D., Melissa R.
Witkow, Sandra Graham, and Jaana Juvonen. 2004.
Beyond the individual: The impact of ethnic context
and classroom behavioral norms on victims? adjust-
ment. Developmental Psychology, 40:1159?1172.
[Biggs, Nelson, and Sampilo2010] Biggs, Bridget K.,
Jennifer Mize Nelson, and Marilyn L. Sampilo. 2010.
Peer relations in the anxiety-depression link: Test
of a mediation model. Anxiety, Stress & Coping,
23(4):431?447.
[Blei, Ng, and Jordan2003] Blei, David M., Andrew Y.
Ng, and Michael I. Jordan. 2003. Latent dirichlet al
location. JMLR, 3:993?1022.
[Blitzer2008] Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems. Ph.D. thesis,
University of Pennsylvania.
[Bosse and Stam2011] Bosse, Tibor and Sven Stam.
2011. A normative agent system to prevent cyberbul-
lying. In WI-IAT 2011, pages 425?430.
[Brody and Diakopoulos2011] Brody, Samuel and
Nicholas Diakopoulos. 2011. Cooooooooooooooolll-
lllllllllll!!!!!!!!!!!!!! using word lengthening to detect
sentiment in microblogs. In EMNLP 2011, pages
562?570.
[Cassidy, Jackson, and Brown2009] Cassidy, Wanda,
Margaret Jackson, and Karen N. Brown. 2009. Sticks
and stones can break my bones, but how can pixels
hurt me? students? experiences with cyber-bullying.
School Psychology Int?l, 30(4):383?402.
[Chang and Lin2011] Chang, Chih-Chung and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM Trans. on Intelligent Systems and Tech-
nology, 2:27:1?27:27.
[Cook et al2010] Cook, Clayton R., Kirk R. Williams,
Nancy G. Guerra, Tia E. Kim, and Shelly Sadek.
2010. Predictors of bullying and victimization in
childhood and adolescence: A meta-analytic investi-
gation. School Psychology Quarterly, 25(2):65?83.
[Dinakar, Reichart, and Lieberman2011] Dinakar, K.,
R. Reichart, and H. Lieberman. 2011. Modeling the
detection of textual cyberbullying. In International
Conference on Weblog and Social Media - Social
Mobile Web Workshop, Barcelona, Spain.
[Fekkes et al2006] Fekkes, Minne, Frans I.M. Pijpers,
A. Miranda Fredriks, Ton Vogels, and S. Pauline
Verloove-Vanhorick. 2006. Do bullied children get
ill, or do ill children get bullied? a prospective cohort
study on the relationship between bullying and health-
related symptoms. Pediatrics, 117:1568?1574.
[Finkel, Grenager, and Manning2005] Finkel, Jenny R.,
Trond Grenager, and Christopher Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In the 43rd ACL,
pages 363?370.
[Fredstrom, Adams, and Gilman2011] Fredstrom, Brid-
get K., Ryan E. Adams, and Rich Gilman. 2011. Elec-
tronic and school-based victimization: Unique con-
texts for adjustment difficulties during adolescence. J.
Youth and Adolescence, 40(4):405?415.
[Gildea and Jurafsky2002] Gildea, Daniel and Daniel Ju-
rafsky. 2002. Automatic labeling of semantic roles.
Comput. Linguist., 28(3):245?288.
[Gini and Pozzoli2009] Gini, Gianluca and Tiziana Poz-
zoli. 2009. Association between bullying and psy-
chosomatic problems: a meta-analysis. Pediatrics,
123(3):1059?1065.
[Graham, Bellmore, and Juvonen2007] Graham, Sandra,
Amy Bellmore, and Jaana Juvonen. 2007. Peer vic-
timization in middle school: When self- and peer
664
views diverge. In Joseph E. Zins, Maurice J. Elias, and
Charles A. Maher, editors, Bullying, victimization, and
peer harassment: A handbook of prevention and inter-
vention. Haworth Press, New York, NY, pages 121?
141.
[Griffiths and Steyvers2004] Griffiths, Thomas L. and
Mark Steyvers. 2004. Finding scientific topics.
PNAS, 101(suppl. 1):5228?5235.
[Hall et al2009] Hall, Mark, Eibe Frank, Geoffrey
Holmes, Bernhard Pfahringer, Peter Reutemann, and
Ian H. Witten. 2009. The weka data mining software:
an update. ACM SIGKDD Explorations Newsletter,
11:10?18.
[Hawker and Boulton2000] Hawker, David S. J. and
Michael J. Boulton. 2000. Twenty years? research on
peer victimization and psychosocial maladjustment: A
meta-analytic review of cross-sectional studies. J. of
Child Psychology And Psychiatry, 41(4):441?455.
[Janosz et al2008] Janosz, Michel, Isabelle Archambault,
Linda S. Pagani, Sophie Pascal, Alexandre J.S. Morin,
and Franc?ois Bowen. 2008. Are there detrimental
effects of witnessing school violence in early adoles-
cence? J. of Adolescent Health, 43(6):600?608.
[Jimerson, Swearer, and Espelage2010] Jimerson,
Shane R., Susan M. Swearer, and Dorothy L.
Espelage. 2010. Handbook of Bullying in Schools:
An international perspective. Routledge/Taylor &
Francis Group, New York, NY.
[Juvonen and Graham2001] Juvonen, Jaana and Sandra
Graham. 2001. Peer harassment in school: The plight
of the vulnerable and victimized. Guilford Press, New
York, NY.
[Juvonen and Gross2008] Juvonen, Jaana and Elisheva F.
Gross. 2008. Extending the school grounds? ? Bul-
lying experiences in cyberspace. J. of School Health,
78:496?505.
[Kontostathis, Edwards, and Leatherman2010]
Kontostathis, April, Lynne Edwards, and Amanda
Leatherman. 2010. Text mining and cybercrime.
In Michael W. Berry and Jacob Kogan, editors, Text
Mining: Applications and Theory. John Wiley &
Sons, Ltd, Chichester, UK.
[Ladd, Kochenderfer, and Coleman1997] Ladd, Gary W.,
Becky J. Kochenderfer, and Cynthia C. Coleman.
1997. Classroom peer acceptance, friendship, and vic-
timization: Distinct relational systems that contribute
uniquely to children?s school adjustment? Child De-
velopment, 68:1181?1197.
[Latham, Crockett, and Bandar2010] Latham, Annabel,
Keeley Crockett, and Zuhair Bandar. 2010. A
conversational expert system supporting bullying
and harassment policies. In the 2nd ICAART, pages
163?168.
[Lieberman, Dinakar, and Jones2011] Lieberman, Henry,
Karthik Dinakar, and Birago Jones. 2011. Let?s gang
up on cyberbullying. Computer, 44:93?96.
[Little et al2003] Little, Todd D., Christopher C. Hen-
rich, Stephanie M. Jones, and Patricia H. Hawley.
2003. Disentangling the ?whys? from the ?whats? of
aggressive behavior. Int?l J. of Behavioral Develop-
ment, 27:122?133.
[Liu et al2010] Liu, Xiaohua, Kuan Li, Bo Han, Ming
Zhou, Long Jiang, Zhongyang Xiong, and Changning
Huang. 2010. Semantic role labeling for news tweets.
In the 23rd COLING, pages 698?706.
[Ma`rquez et al2005] Ma`rquez, Llu??s, Pere Comas, Jesu?s
Gime?nez, and Neus Catala`. 2005. Semantic role la-
beling as sequential tagging. In the 9th CoNLL, pages
193?196.
[Mihalcea and Strapparava2005] Mihalcea, Rada and
Carlo Strapparava. 2005. Making computers laugh:
Investigations in automatic humor recognition. In
EMNLP 2005, pages 531?538.
[Moore et al2003] Moore, Mark H., Carol V. Petrie, An-
thony A. Braga, and Brenda L. McLaughlin. 2003.
Deadly lessons: Understanding lethal school violence.
The National Academies Press, Washington, DC.
[Nansel et al2001] Nansel, Tonja R., Mary Overpeck,
Ramani S. Pilla, W. June Ruan, Bruce Simons-
Morton, and Peter Scheidt. 2001. Bullying behav-
iors among US youth: prevalence and association with
psychosocial adjustment. J. Amer. Medical Assoc.,
285(16):2094?2100.
[Nishina and Bellmore2010] Nishina, Adrienne and
Amy D. Bellmore. 2010. When might aggression,
victimization, and conflict matter most?: Contextual
considerations. J. of Early Adolescence, pages 5?26.
[Nishina and Juvonen2005] Nishina, Adrienne and Jaana
Juvonen. 2005. Daily reports of witnessing and ex-
periencing peer harassment in middle school. Child
Development, 76:435?450.
[Nylund et al2007] Nylund, Karen, Amy Bellmore, Adri-
enne Nishina, and Sandra Graham. 2007. Subtypes,
severity, and structural stability of peer victimization:
What does latent class analysis say? Child Develop-
ment, 78:1706?1722.
[Olweus1993] Olweus, Dan. 1993. Bullying at school:
What we know and what we can do. Blackwell, Ox-
ford, UK.
[Pang and Lee2004] Pang, Bo and Lillian Lee. 2004. A
sentimental education: Sentiment analysis using sub-
jectivity summarization based on minimum cuts. In
the 42nd ACL, pages 271?278.
[Ptaszynski et al2010] Ptaszynski, Michal, Pawel Dy-
bala, Tatsuaki Matsuba, Fumito Masui, Rafal Rzepka,
665
and Kenji Araki. 2010. Machine learning and af-
fect analysis against cyber-bullying. In the 36th AISB,
pages 7?16.
[Punyakanok, Roth, and Yih2008] Punyakanok, Vasin,
Dan Roth, and Wen-tau Yih. 2008. The importance
of syntactic parsing and inference in semantic role
labeling. Comput. Linguist., 34(2):257?287.
[Ratinov and Roth2009] Ratinov, Lev and Dan Roth.
2009. Design challenges and misconceptions in
named entity recognition. In the 13th CoNLL, pages
147?155.
[Ritter et al2011] Ritter, Alan, Sam Clark, Mausam, and
Oren Etzioni. 2011. Named entity recognition in
tweets: An experimental study. In EMNLP 2011,
pages 1524?1534.
[Rivers et al2009] Rivers, Ian, V. Paul Poteat, Nathalie
Noret, and Nigel Ashurst. 2009. Observing bullying
at school: The mental health implications of witness
status. School Psychology Quarterly, 24(4):211?223.
[Salmivalli1999] Salmivalli, Christina. 1999. Participant
role approach to school bullying: Implications for in-
tervention. J. of Adolescence, 22(4):453?459.
[Schmidt, Fung, and Rosales2007] Schmidt, Mark W.,
Glenn Fung, and Ro?mer Rosales. 2007. Fast opti-
mization methods for l1 regularization: A comparative
study and two new approaches. In the 18th ECML,
pages 286?297.
[Schwartz et al2005] Schwartz, David, Andrea Hop-
meyer Gorman, Jonathan Nakamoto, and Robin L. To-
blin. 2005. Victimization in the peer group and chil-
dren?s academic functioning. J. of Educational Psy-
chology, 87:425?435.
[Settles2011] Settles, Burr. 2011. Closing the loop: Fast,
interactive semi-supervised annotation with queries on
features and instances. In the EMNLP 2011, pages
1467?1478.
[Smith, Madsen, and Moody1999] Smith, Peter K.,
Kirsten C. Madsen, and Janet C. Moody. 1999. What
causes the age decline in reports of being bullied at
school? Towards a developmental analysis of risks of
being bullied. Educational Research, 41(3):267?285.
[The American Academy of Pediatrics2009] The Ameri-
can Academy of Pediatrics. 2009. Policy statement?
role of the pediatrician in youth violence prevention.
Pediatrics, 124(1):393?402.
[The White House2011] The White House. 2011.
Background on White House conference on bul-
lying prevention. http://www.whitehouse.gov/the-
press-office/2011/03/10/background-white-house-
conference-bullying-prevention.
[Toutanova et al2003] Toutanova, Kristina, Dan Klein,
Christopher D. Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In NAACL-HLT 2003, pages 173?
180.
[Vaillancourt et al2010] Vaillancourt, Tracy, Vi Trinh,
Patricia McDougall, Eric Duku, Lesley Cunning-
ham, Charles Cunningham, Shelley Hymel, and Kathy
Short. 2010. Optimizing population screening of bul-
lying in school-aged children. J. of School Violence,
9:233?250.
[Vandebosch and Cleemput2009] Vandebosch, Heidi and
Katrien Van Cleemput. 2009. Cyberbullying among
youngsters: profiles of bullies and victims. New media
& society, 11(8):1349?1371.
[Wang, Iannotti, and Nansel2009] Wang, Jing, Ronald J.
Iannotti, and Tonja R. Nansel. 2009. School bully-
ing among adolescents in the united states: Physical,
verbal, relational, and cyber. J. Adolescent Health,
45(4):368?375.
666
Proceedings of NAACL-HLT 2013, pages 697?702,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
An Examination of Regret in Bullying Tweets
Jun-Ming Xu, Benjamin Burchfiel, Xiaojin Zhu
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706, USA
{xujm,burchfie,jerryzhu}@cs.wisc.edu
Amy Bellmore
Department of Educational Psychology
University of Wisconsin-Madison
Madison, WI 53706, USA
abellmore@wisc.edu
Abstract
Social media users who post bullying related
tweets may later experience regret, potentially
causing them to delete their posts. In this pa-
per, we construct a corpus of bullying tweets
and periodically check the existence of each
tweet in order to infer if and when it becomes
deleted. We then conduct exploratory analy-
sis in order to isolate factors associated with
deleted posts. Finally, we propose the con-
struction of a regrettable posts predictor to
warn users if a tweet might cause regret.
1 Introduction
A large body of literature suggests that participants
in bullying events, including victims, bullies, and
witnesses, are likely to report psychological adjust-
ment problems (Jimerson, Swearer, and Espelage,
2010). One potential source of therapy for these is-
sues can be self-disclosure of the experience to an
adult or friend (Mishna and Alaggia, 2005); exist-
ing research suggests that victims who seek advice
and help from others report less maladjustment than
victims who do not (Shelley and Craig, 2010).
Disclosure of bullying experiences through so-
cial media may be a particularly effective mecha-
nism for participants seeking support because so-
cial media has the potential to reach large audi-
ences and because participants may feel less inhi-
bition when sharing private information in an on-
line setting (Walther, 1996). Furthermore, there is
evidence that online communication stimulates self-
disclosure, which leads to higher quality social rela-
tionships and increased well-being (Valkenburg and
Peter, 2009).
Online disclosure may also present risks for
those involved in bullying however, such as re-
victimization, embarrassment, and social ostraciza-
tion. Evidence exists that some individuals may re-
act to these risks retroactively, by deleting their so-
cial media posts (Child et al, 2011; Christofides,
Muise, and Desmarais, 2009). Several relevant mo-
tives have been found to be associated with delet-
ing posted information, including conflict manage-
ment, safety, fear of retribution, impression manage-
ment, and emotional regulation (Child, Haridakis,
and Petronio, 2012).
Our previous work (Xu et al, 2012) demonstrates
that social media can be a valuable data source when
studying bullying, and proposes a text categorization
method to recognize social media posts describing
bullying episodes, bullying traces. To better under-
stand, and possibly prevent, user regret after posting
bullying related tweets, we collect bullying traces
using the same method and perform regular status
checks to determine if and when tweets become in-
accessible. While a tweet becoming inaccessible
does not guarantee it has been deleted, we attempt to
leverage http response codes to rule out other com-
mon causes of inaccessibility. Speculating that re-
gret may be a major cause of deletion, we first con-
duct exploratory analysis on this corpus and then re-
port the results of an off-the-shelf regret predictor.
2 Data Collection
We adopt the procedure used in (Xu et al, 2012) to
obtain bullying traces; each identified trace contains
697
at least one bullying related keyword and passes a
bullying-or-not text classifier.
Our data was collected in realtime using the
Twitter streaming API; once a tweet is collected,
we query its url (https://twitter.com/
USERID/status/TWEETID) at regular intervals
and infer its status from the resulting http response
code. We interpret an HTTP 200 response as an indi-
cation a tweet still exists and an HTTP 404 response,
which indicates the tweet is unavailable, as indicat-
ing deletion. A user changing their privacy settings
can also result in an HTTP 403 response; we do not
consider this to be a deletion. Other response codes,
which appear quite rarely, are treated as anomalies
and ignored. All non HTTP 200 responses are re-
tried twice to ensure they are not transient oddities.
To determine when a tweet is deleted, we at-
tempted to access each tweet at time points Ti =
5 ? 4i minutes for i = 0, 1 . . . 7 after the creation
time. These roughly correspond to periods of 5 min-
utes, 20 minutes, 1.5 hours, 6 hours, 1 day, 4 days,
2 weeks, and 2 months. While we assume that user
deletion is the main cause of a tweet becoming un-
available, other causes are possible such as the cen-
sorship of illegal contents by Twitter (Twitter, 2012).
Our sample data was collected from July 31
through October 31, 2012 and contains 522,984 bul-
lying traces. Because of intermittent network and
computer issues, several multiple day data gaps ex-
ist in the data. To combat this, we filter our data to
include only tweets of unambiguous status. If any
check within the 20480 minutes (about two weeks)
interval returns an HTTP 404 code, the tweet is
no longer accessible and we consider it deleted. If
the 20480 minute or 81920 minute check returns an
HTTP 200 response, that tweet is still accessible and
we consider it surviving. The union of the surviving
and deleted groups formed our cleaned dataset, con-
taining 311,237 tweets in total.
3 Exploratory Data Analysis
A user?s decision to delete a bullying trace may be
the result of many factors which we would like to
isolate and understand. In this section we will ex-
amine several such possible factors.
3.1 Word Usage
Our dataset contains 331,070 distinct words and we
are interested in isolating those with a significantly
higher presence among either deleted or surviving
tweets. We define the odds ratio of a word w
r(w) =
P (w | deleted)
P (w | surviving)
,
where P (w | deleted) is the probability of word w
occurring in a deleted tweet, and P (w | surviving) is
the probability of w appearing in a surviving tweet.
In order to ensure stability in the probability estima-
tion, we only considered words appearing at least 50
times in either the surviving or deleted corpora.
Following (Bamman, OConnor, and Smith,
2012), we qualitatively analyzed words with ex-
treme values of r(w), and found some interesting
trends. There was a significant tendency for ?jok-
ing? words to occur with r(w) < 0.5; examples in-
clude ?xd,? ?haha,? and ?hahaha.? Joking words oc-
cur less frequently in deleted tweets than surviving
ones. On the other end of the spectrum, there were
no joking words with r(w) > 2. What we found
instead were words such as ?rip,? ?fat,? ?kill,? and
?suicide.? While it is relatively clear that joking is
less likely to occur in deleted tweets, there was less
of a trend among words appearing more frequently
in deleted tweets.
3.2 Surviving Time
Let N be the total number of tweets in our cor-
pus, and D(Ti) be the number of tweets that were
first detected as deleted at minute Ti after creation.
Note that D(Ti) is not cumulative over time: it in-
cludes only deletions that occurred in the time inter-
val (Ti?1, Ti]. Then we may define the deletion rate
at time Ti as
RT (Ti) =
D(Ti)
N(Ti ? Ti?1)
.
In other words, RT (t) is the fraction of tweets that
are deleted during the one minute period (t, t+ 1).
We plot RT vs. t using logarithmic scales on both
axes in Figure 1 and the result is a quite strong linear
trend. Fitting the plot with a linear regression, we
derive an inverse relationship between RT and t of
the form
RT (t) ? 1/t.
698
5 minutes 1.5 hours 1 day 2 weeks1E?7
1E?6
1E?5
1E?4
1E?3
t
R T(
t)
Figure 1: Deletion rate decays over time.
This result makes sense; the social effects of a par-
ticular bullying tweet may decay over time, making
regret less of a factor. Furthermore, the author may
assume an older tweet has already been seen, render-
ing deletion ineffective. Additionally, because the
drop off in deletion rate is so extreme, we are able to
safely exclude deletions occurring after two weeks
from our filtered dataset without introducing a sig-
nificant amount of noise. Finally,
??
t=0RT (t) gives
the overall fraction of deletion, which in our case is
around 4%.
3.3 Location and Hour of Creations
Some bullying traces contain location meta-data in
the form of GPS coordinates or a user-created profile
string. We employed a reverse geocoding database
(http://www.datasciencetoolkit.org)
and a rule-based string matching method to map
these tweets to their origins (at the state level; only
for tweets within the USA). This also allowed us to
convert creation timestamps from UTC to local time
by mapping user location to timezone. Because
many users don?t share their location, we were only
able to successfully map 85,465 bullying traces to a
US state s, and local hour of day h. Among these
traces, 3,484 were deleted which translates to an
overall deletion rate of about 4%.
Let N(s, h) be the count of bullying traces cre-
ated in state s and hour h. Aggregating these counts
temporally yields NS(s) =
?
hN(s, h), while ag-
gregating spatially produces NH(h) =
?
sN(s, h).
Similarly, we can defineD(s, h),DS(s) andDH(h)
as the corresponding counts of deleted traces. We
can now compute the deletion rate
RH(h) =
DH(h)
NH(h)
, and RS(s) =
DS(s)
NS(s)
.
The top row of Figure 2 shows NH(h), DH(h),
and RH(h). We find that NH(h) and DH(h) peak
in the evening, indicating social media users are gen-
erally more active at that time. The peak of RH(h)
appears at late night and, while there are multiple
potential causes for this, we hypothesize that users
may fail to fully evaluate the consequences of their
posts when tired. The bottom row of Figure 2 shows
NS(s), DS(S), and RS(s). The plot of NS(s)
shows that bullying traces are more likely to origi-
nate in California, Texas or New York which is the
result of a population effect. Importantly however,
the deletion rate RS(s) is not affected by population
bias and we see, as expected, that spatial differences
in RS(s) are small. We performed ?2-test to see if
a state?s deletion rate is significantly different from
the national average. We chose the significance level
at 0.05 and used Bonferroni correction for multiple
testing. Only four states have significantly differ-
ent deletion rates from the average: Arizona (6.3%,
p = 5.9?10?5), California (5.2%, p = 2.7?10?7),
Maryland (1.9%, p = 2.3 ? 10?5), and Oklahoma
(7.1%, p = 3.5? 10?5).
3.4 Author?s Role
Participants in a bullying episode assume well-
defined roles which dramatically affect the view-
point of the author describing the event. We trained
a text classifier to determine author role (Xu et al,
2012), and used it to label each bullying trace in the
cleaned corpus by author role: Accuser, Bully, Re-
porter, Victim or Other.
Table 1 shows that compared to tweets produced
by bullies, victims create more bullying traces, pos-
sibly due to an increased need for social support on
the part of the victim. More importantly, P (deleted |
victim) is higher than P (deleted | bully), a statis-
tically significant difference in a two-proportion z-
test. Possibly, victims are more sensitive to their au-
dience?s reaction than bullies.
3.5 Teasing
Many bullying traces are written jokingly. We built a
text classifier to identify teasing bullying traces (Xu
et al, 2012) and applied it to the cleaned corpus.
Table 2 shows that P (deletion | Teasing) is much
lower than P (deletion | Not Teasing) and the differ-
ence is statistically significant in a two-proportion z-
699
NH(h) DH(h) RH(h)
NS(s) DS(s) RS(s)
Figure 2: Counts and deletion rates of geo-tagged bullying traces.
Deleted Total P (deleted | Role)
Accuser 2541 50088 5.07%
Bully 1792 30123 5.95%
Reporter 11370 147164 7.73%
Victim 6497 83412 7.79%
Other 41 450 9.11%
Table 1: Counts and deletion rate for different roles.
Deleted Total P (deleted | Teasing?)
Yes 858 22876 3.75%
Not 21383 288361 7.42%
Table 2: Counts and deletion rate for teasing or not.
test. It seems plausible that authors are less likely to
regret teasing posts because they are less controver-
sial and have less potential to generate negative au-
dience reactions. This also corroborates our findings
in word usage that joking words are less frequent in
deleted tweets.
4 Predicting Regrettable Tweets
Once a bullying tweet is published and seen by oth-
ers, the ensuing effects are often impossible to undo.
Since ill-thought-out posts may cause unexpectedly
negative consequences to an author?s reputation, re-
lationship, and career (Wang et al, 2011), it would
be helpful if a system could warn users before a po-
tentially regrettable tweet is posted. One straightfor-
ward approach is to formulate the task as a binary
text categorization problem.
We use the cleaned dataset, in which each tweet
is known to be surviving or deleted after 20480 min-
utes (about two weeks). Since this dataset contains
22,241 deleted tweets, we randomly sub-sampled
the surviving tweets down to 22,241 to force our
deleted and surviving datasets to be of equal size.
Consequentially, the baseline accuracy of the clas-
sifier is 0.5. While this does make the problem ar-
tificially easier, our initial goal was to test for the
presence of a signal in the data.
We then followed the preprocessing procedure
in (Xu et al, 2012), performing case-folding,
anonymization, and tokenization, treating URLs,
emoticons and hashtags specially. We also chose
the unigrams+bigrams feature representation, only
keeping tokens appearing at least 15 times in the cor-
pus.
We chose to employ a linear SVM implemented
in LIBLINEAR (Fan et al, 2008) due to its effi-
ciency on this large sparse text categorization task
and a 10-fold cross validation was conducted to eval-
700
uate its performance. Within the first fold, we use
an inner 5-fold cross validation on the training por-
tion to tune the regularization parameter on the grid
{2?10, 2?9, . . . , 1}; the selected parameter is then
fixed for all the remaining folds.
The resulting cross validation accuracy was 0.607
with a standard deviation of 0.012. While it is statis-
tically significantly better than the random-guessing
baseline accuracy of 0.5 with a p-value of 5.15 ?
10?10, this accuracy is nevertheless too low to be
useful in a practical system. One possibility is that
the tweet text contains very limited information for
predicting inaccessibility; a user?s decision to delete
a tweet potentially depends on many other factors,
such as the conversation context and the characteris-
tics of the author and audience.
In the spirit of exploring additional informative
features for deletion prediction, we also used the
teasing and author role classifiers in (Xu et al,
2012), and appended the predicted teasing, and au-
thor role labels to our feature vector. This aug-
mented feature representation achieved a cross val-
idation accuracy of 0.606, with standard deviation
0.007; not statistically significantly different from
the text-only feature representation. While it seems
that a signal does exist, leveraging it usefully in real
world scenarios may prove challenging due to the
highly-skewed nature of the data.
5 Discussion
There have been several recent works examin-
ing causes of deletion in social media. Wang
et al (2011) qualitatively investigated regret associ-
ated with users? posts on social networking sites and
identified several possible causes of regret. Bamman
et al (2012) focused on censorship-related deletion
of social media posts, identifying a set of sensitive
terms related to message deletion through a statisti-
cal analysis and spatial variation of deletion rate.
Assuming that deletion in social media is indica-
tive of regret, we studied regret in a bullying con-
text by analyzing deletion trends in bullying re-
lated tweets. Through our analysis, we were able
to isolate several factors related to deletion, includ-
ing word usage, surviving time, and author role. We
used these factors to build a regret predictor which
achieved statistically significant results on this very
noisy data. In the future, we plan to explore more
factors to better understand deletion behavior and re-
gret, including users? recent posts, historical behav-
ior, and other statistics related to their specific social
network.
Acknowledgments
We thank Kwang-Sung Jun, Angie Calvin, and
Charles Dyer for helpful discussions. This work
is supported by National Science Foundation grants
IIS-1216758 and IIS-1148012.
References
Bamman, David, Brendan OConnor, and Noah Smith.
2012. Censorship and deletion practices in chinese so-
cial media. First Monday, 17(3-5).
Child, Jeffrey T., Paul M. Haridakis, and Sandra Petro-
nio. 2012. Blogging privacy rule orientations, privacy
management, and content deletion practices: The vari-
ability of online privacy management activity at differ-
ent stages of social media use. Computers in Human
Behavior, 28(5):1859 ? 1872.
Child, Jeffrey T, Sandra Petronio, Esther A Agyeman-
Budu, and David A Westermann. 2011. Blog scrub-
bing: Exploring triggers that change privacy rules.
Computers in Human Behavior, 27(5):2017?2027.
Christofides, Emily, Amy Muise, and Serge Desmarais.
2009. Information disclosure and control on facebook:
are they two sides of the same coin or two different
processes? CyberPsychology & Behavior, 12(3):341?
345.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Jimerson, Shane R., Susan M. Swearer, and Dorothy L.
Espelage. 2010. Handbook of Bullying in Schools: An
international perspective. Routledge/Taylor & Francis
Group, New York, NY.
Mishna, Faye and Ramona Alaggia. 2005. Weighing the
risks: A child?s decision to disclose peer victimization.
Children & Schools, 27(4):217?226.
Shelley, Danielle and Wendy M Craig. 2010. Attri-
butions and coping styles in reducing victimization.
Canadian Journal of School Psychology, 25(1):84?
100.
Twitter. 2012. The twitter rules. http:
//support.twitter.com/articles/
18311-the-twitter-rules.
701
Valkenburg, Patti M and Jochen Peter. 2009. Social
consequences of the internet for adolescents a decade
of research. Current Directions in Psychological Sci-
ence, 18(1):1?5.
Walther, Joseph B. 1996. Computer-mediated commu-
nication impersonal, interpersonal, and hyperpersonal
interaction. Communication research, 23(1):3?43.
Wang, Yang, Gregory Norcie, Saranga Komanduri,
Alessandro Acquisti, Pedro Giovanni Leon, and Lor-
rie Faith Cranor. 2011. ?I regretted the minute I
pressed share?: a qualitative study of regrets on face-
book. In Proceedings of the Seventh Symposium on
Usable Privacy and Security, SOUPS ?11, pages 10:1?
10:16. ACM.
Xu, Jun-Ming, Kwang-Sung Jun, Xiaojin Zhu, and Amy
Bellmore. 2012. Learning from bullying traces in so-
cial media. In Proceedings of the 2012 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 656?666, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
702
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 119?126
Manchester, August 2008
Easy as ABC? Facilitating Pictorial Communication
via Semantically Enhanced Layout
Andrew B. Goldberg, Xiaojin Zhu, Charles R. Dyer, Mohamed Eldawy, Lijie Heng
Department of Computer Sciences
University of Wisconsin, Madison, WI 53706, USA
{goldberg, jerryzhu, dyer, eldawy, ljheng}@cs.wisc.edu
Abstract
Pictorial communication systems convert
natural language text into pictures to as-
sist people with limited literacy. We define
a novel and challenging problem: picture
layout optimization. Given an input sen-
tence, we seek the optimal way to lay out
word icons such that the resulting picture
best conveys the meaning of the input sen-
tence. To this end, we propose a family
of intuitive ?ABC? layouts, which organize
icons in three groups. We formalize layout
optimization as a sequence labeling prob-
lem, employing conditional random fields
as our machine learning method. Enabled
by novel applications of semantic role la-
beling and syntactic parsing, our trained
model makes layout predictions that agree
well with human annotators. In addition,
we conduct a user study to compare our
ABC layout versus the standard linear lay-
out. The study shows that our semantically
enhanced layout is preferred by non-native
speakers, suggesting it has the potential to
be useful for people with other forms of
limited literacy, too.
1 Introduction
A picture is worth a thousand words?especially
when you are someone with communicative dis-
orders, a foreign language speaker, or a young
child. Pictorial communication systems aim to au-
tomatically convert general natural language text
into meaningful pictures. A perfect pictorial
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
communication system can turn signs and opera-
tion instructions into easy-to-understand graphical
forms; combined with optical character recogni-
tion input, a personal assistant device could create
such visual translations on-the-fly without the help
of a caretaker. Pictorial communication may also
facilitate literacy development and rapid browsing
of documents through pictorial summaries.
Pictorial communication research is in its in-
fancy with a spectrum of experimental systems,
which we review in Section 2. At one end of
the spectrum, some systems render highly realis-
tic 3D scenes but require specific scene-descriptive
language. At the other end, some systems per-
form dictionary-based iconic transliteration (turn-
ing words into icons1 one by one) on arbitrary text
but the pictures can be hard to understand. We are
interested in using pictorial communication as an
assistive communication tool. Thus, our system
needs to be able to handle general text yet produce
easy-to-understand pictures, which is in the middle
of the spectrum. To this end, our system adopts
a ?collage? approach (Zhu et al, 2007). Given a
piece of text (e.g., a sentence), it first identifies im-
portant and easy-to-depict words (or phrases) with
natural language processing (NLP) techniques. It
then finds one good icon per word, either from a
manually created picture-dictionary, or via image
analysis on image search results. Finally, it lays
out the icons to create the picture. Each step in-
volves several interesting research problems.
This paper focuses exclusively on the picture
layout component and addresses the following
question: Can we use machine learning and NLP
techniques to learn a good picture layout that im-
1In this paper, an icon refers to a small thumbnail image
corresponding to a word or phrase. A picture refers to the
overall large image corresponding to the whole text.
119
proves picture comprehension for our target audi-
ences of limited literacy? We first propose a sim-
ple yet novel picture layout scheme called ?ABC.?
Next, we design a Conditional Random Field-
based semantic tagger for predicting the ABC lay-
out. Finally, we conduct a user study contrasting
our ABC layout to the linear layout used in iconic
transliteration. The main contribution of this paper
is to introduce the novel task of layout prediction,
learned using linguistic features including Prop-
Bank role labels, part-of-speech tags, and lexical
features.
2 Prior Pictorial Communication Work
At one extreme, there has been significant prior
work on ?text-to-scene? type systems, which were
often intended to aid graphic designers in placing
objects in a 3D environment. Example systems in-
clude NALIG (Adorni et al, 1983), SPRINT (Ya-
mada et al, 1992), Put (Clay and Wilhelms,
1996), and others (Brown and Chandrasekaran,
1981). Perhaps the best known system of this type,
WordsEye (Coyne and Sproat, 2001), uses a large
manually tagged collection of 3D polyhedral mod-
els to create photo-realistic scenes. Similarly, Car-
Sim (Johansson et al, 2005) can create animated
scenes, but operates exclusively in the limited do-
main of reconstructing road accidents from traffic
reports. These systems cater to detailed descriptive
text with visual and spatial elements. They are not
intended as assistive tools to communicate general
text, which is our goal.
Several systems (Zhu et al, 2007; Mihalcea and
Leong, 2006; Joshi et al, 2006) attempt to bal-
ance language coverage versus picture sophistica-
tion. They perform some form of keyword selec-
tion, and select corresponding icons automatically
from a 2D image database. The result is a pictorial
summary representing the main idea of the origi-
nal text, but precisely determining the original text
by looking at the picture can be difficult.
At the other extreme, augmentative and alterna-
tive communication software allows users to in-
put arbitrary text. The words, and sometimes
common phrases, are semi-automatically translit-
erated into icons, and displayed in sequential or-
der. Users must learn special icons, which corre-
spond to function words, before the resulting pic-
tures can be fully understood. Examples include
SymWriter (Widgit Software, 2007) and Blissym-
bols (Hehner, 1980).
Other than explicit scene-descriptive languages,
pictorial communication systems have not suffi-
ciently addressed the issue of picture layout for
general text. We believe a good layout can better
communicate the text a picture is trying to convey.
The present work studies the use of a semantically
inspired layout to enhance pictorial communica-
tion. For simplicity, we restrict our attention to the
layout of a single sentence. We anticipate the use
of text simplification (Chandrasekar et al, 1996;
Vickrey and Koller, 2008) to convert complex text
into a set of appropriate inputs for our system.
3 The ABC Layout
A good picture layout scheme must be intuitive to
humans and easy to generate by computers. To
design such a layout, we conducted a pilot study.
Five human annotators produced free-hand pic-
tures of many sentences. Analyzing these pictures,
we found a large amount of agreement in the use
of arrows to mark actions and to provide structure
to what would otherwise be a jumble of icons.
Motivated by the pilot study, we propose a sim-
ple layout scheme called ABC. It features three
positions, referred to as A, B, and C. In addition,
an arrow points from A through B to C (Figure 1).
These positions are meant to denote certain seman-
tic roles: roughly speaking, A denotes ?who,? B
denotes ?what action,? and C denotes ?to whom,
for what.? Each position can contain any number
of icons, each representing a word or phrase in the
text. Words that do not play a significant role in
the text will be omitted from the ABC layout.
There are two main advantages of the ABC lay-
out:
1. The ABC positioning of icons allows users to
infer the semantic role of the corresponding con-
cepts. In particular, we found that verbs can be dif-
ficult to depict and understand without such hints.
The B position serves as an action indicator to dis-
ambiguate between multiple senses of the same
icon. For example, in Figure 1, the school bus icon
clearly represents the verb phrase ?rides the bus,?
rather than just the noun ?bus.?
2. Such a layout is particularly amenable to ma-
chine learning. Specifically, we can turn the prob-
lem of finding the optimal layout for an input sen-
tence into a sequence tagging problem, which is
well-studied in NLP.
120
The girl rides the bus to school in the morning
O A B B B O C O O B
Figure 1: Example ABC picture layout, original
text, and tag sequence.
3.1 ABC Layout as Sequence Tagging
Given an input sentence, one can assign each word
a tag from the set {A, B, C, O}. The bottom row in
Figure 1 shows an example tag sequence. The tag
specifies the ABC layout position of the icon cor-
responding to that word. Tag O means ?other? and
marks words not included in the picture. Within
each position, icons appear in the word order in the
input sentence. Therefore, a tag sequence uniquely
determines an ABC layout of the picture.
Finding the optimal ABC layout of the input
sentence is thus equivalent to computing the most
likely tag sequence given the input sentence. We
adopt a machine learning approach by training a
sequence tagger for this task. To do so, we need
to collect labeled training data in the form of sen-
tences with manually annotated tag sequences. We
discuss our annotation effort next, and present our
machine learning models in Section 4.
3.2 Human Annotated Training Data
We asked the five annotators to manually label 571
sentences compiled from several online sources,
including grade school texts about history and sci-
ence, children?s books, and recent news headlines.
Some sentences were written by the annotators and
describe daily activities. The annotators tagged
each sentence using a Web-based tool to drag-and-
drop icons into the desired positions in the layout2.
To gauge the quality of the manually labeled
data, and to understand the difficulty of the ABC
2The manual tagging actually employs a more detailed tag
set to denote phrase structure: Each A, B, or C tag is com-
bined with a modifier of b (begin phrase) or i (inside phrase).
For example, the phrase ?rides the bus? in Figure 1 is tagged
with Bb Bi Bi, and shares one icon. The icons were also
manually selected by the annotator from a list of Web image
search results.
layout, we computed inter annotator agreement
among three of the five annotators on a common
set of 48 sentences. Considering all pair-wise com-
parisons of the three annotators, the overall aver-
age tag agreement was 77%. This measures the to-
tal number of matching tags (across all sentences)
divided by the total number of tags. Matching
strictly requires both the correct tag and the correct
modifier. We also computed Fleiss? kappa, which
measures the degree of inter-annotator agreement
beyond the amount expected by chance (Fleiss,
1971). The values range from 0 to 1, with 1 indi-
cating perfect agreement. The kappa statistic was
0.71, which is often considered moderate to high
agreement.
Further inspection revealed that most disagree-
ment was due to annotators reversing A and C
tags. This could arise from interpreting passive
sentences in different ways or trying to represent
physical movement. For example, some annotators
found it more natural to depict eating by placing a
food item in A and the eater in C, treating the ar-
row as the transfer of food. It was also common for
annotators to disagree on whether certain adverbs
and time modifiers belong in B or in C. These dif-
ferences all suggest the highly subjective nature of
conceptualizing pictures from text.
4 A Conditional Random Field Model for
ABC Layout Prediction
We now introduce our approach to automatically
predicting the ABC layout of an input sentence.
While it was most natural for human annotators to
annotate text at the word level, early experiments
quickly revealed that predicting tags at this level is
quite challenging. Most of this stems from the fact
that human annotators tend to fragment the text
into many small segments based on the availability
of good icons. For example, the phrase ?the white
pygmy elephant? may be tagged as ?O A O A? be-
cause it is difficult for the annotator to find an icon
of this exact phrase or the word ?pygmy,? but easy
to find icons of ?white? and ?elephant? separately.
Essentially, human annotation combines two tasks
in one: deciding where each phrase goes in the lay-
out, and deciding which words within a phrase can
be depicted with icons.
To rectify this situation, we make layout predic-
tions at the level of chunks (phrases); that is, we
automatically break the text into chunks, then pre-
dict one A, B, C, or O tag for each chunk. Since the
121
tag choices made for different chunks may depend
on each other, we employ Conditional Random
Fields (CRF) (Lafferty et al, 2001), which are fre-
quently used in sequential labeling tasks like infor-
mation extraction. Our choice of chunking is de-
scribed in Section 4.1, and the CRF models and in-
put features are described in Section 4.2. The task
of deciding which words within a chunk should ap-
pear in the picture is addressed by a ?word pictura-
bility? model, and is discussed in a separate paper.
For training, we automatically map the word-
level tags in our annotated data to chunk-level tags
based on the majority ABC tag within a chunk.
4.1 Chunking by Semantic Role Labeling
Ideally, we would like semantically coherent text
chunks to be represented pictorially in the same
layout position. To obtain such chunks, we lever-
age existing semantic role labeling (SRL) tech-
nology (Palmer et al, 2005; Gildea and Jurafsky,
2002). SRL is an active NLP task in which words
or phrases in a sentence are assigned a label indi-
cating the role they play with respect to a particu-
lar verb (also known as the target predicate). SRL
systems like FrameNet (Baker et al, 1998) and
PropBank (Palmer et al, 2005) aim to provide a
rich representation for applications requiring some
degree of natural language understanding, and are
thus perfectly suited for our needs. We shall fo-
cus on PropBank labels because they are easier to
use for our task. To obtain semantic role labels,
we use the automatic statistical semantic role la-
beler ASSERT (Pradhan et al, 2004), trained to
identify PropBank arguments through the use of
support vector machines and full syntactic parses.
To understand how SRL can be useful for deriv-
ing pictorial layouts, consider the sentence ?The
boy gave the ball to the girl.? PropBank marks
the semantic role labels of the ?arguments? of
verbs. The target verb ?give? is part of the frameset
?transfer,? with core arguments ?Arg0: giver? (the
boy), ?Arg1: thing given? (the ball), and ?Arg2:
entity given to? (the girl). Verbs can also in-
volve non-core modifier arguments, such as ArgM-
TMP (time), ArgM-LOC (location), ArgM-CAU
(cause), etc. The entities playing semantic roles
are likely to be entities we want to portray in a
picture. For PropBank, Arg0 often represents an
Agent, and Arg1 the Patient or Theme. If we could
map the different semantic role labels to ABC tags
with simple rules, then we would be done.
Unfortunately, it is not this simple, as Prop-
Bank roles are verb-specific. As Palmer et al
pointed out, ?No consistent generalizations can be
made across verbs for the higher-numbered argu-
ments? (Palmer et al, 2005). In the above exam-
ple, we might expect a layout rule of [Arg0]?A,
[Target, Arg1]?B, [Arg2]?C. However, this rule
does not generalize to other verbs, such as ?drive,?
as in the sentence ?The boy drives his parents
crazy,? which also has three core arguments ?Arg0:
driver,? ?Arg1: thing in motion,? and ?Arg2: sec-
ondary predication on Arg1.? However, here the
action is figurative, and we would expect a lay-
out rule that puts Arg1 in position C: [Arg0]?A,
[Target]?B, [Arg1,Arg2]?C.
In addition, while modifier arguments have the
same meaning across verbs, their pictorial repre-
sentation may differ based on context. Consider
the sentences ?Polar bears live in the Arctic.? and
?Yesterday at the zoo, the students saw a polar
bear.? In the former, a human annotator is likely
to place an icon for the ArgM-LOC ?in the Arc-
tic? in position C (e.g., following a polar bear icon
in A and a house icon in B). However, the ArgM-
LOC in the second sentence, ?at the zoo,? seems
more appropriately placed in position B since it de-
scribes where this particular action occurred.
Finally, the situation is further complicated
when a sentence contains multiple verbs. SRL
treats each verb in isolation, producing multiple
sets of role labels, yet our goal is to produce a sin-
gle picture. Clearly, the mapping from semantic
roles to layout positions is non-trivial. We describe
our statistical machine learning approach next.
4.2 Our CRF Models and Features
We use a linear-chain CRF as our sequence tag-
ging model. A CRF is a discriminative model of
the conditional probability p(y|x), where y is the
sequence of layout tags in Y ={A,B,C,O}, and x
is the sequence of SRL chunks produced by the
process described in Section 4.1. Our CRF has the
general form
p(y|x) =
1
Z(x)
exp
?
?
|x|
?
t=1
K
?
k=1
?kfk(yt, yt?1, x, t)
?
?
where the model parameters are {?k}. We
use binary features fk(yt, yt?1, x, t) detailed be-
low. Finally, we use an isotropic Gaussian prior
N(0,?2I) on parameters as regularization.
122
We explored three versions of the above model
by specializing the weighted feature function
?kfk(). Model 1 ignores the pairwise label poten-
tials and treats each labeling prediction indepen-
dently: ?jk1{yt=j}fk(x, t), where 1{z} is an indi-
cator function on z. This is equivalent to a multi-
class logistic regression classifier. Model 2 resem-
bles a Hidden Markov Model (HMM) by factoring
pairwise label potentials and emission potentials:
?ij1{yt?1=i}1{yt=j}+?jk1{yt=j}fk(x, t). Finally,
Model 3 has the most general linear-chain poten-
tial: ?ijk1{yt?1=i}1{yt=j}fk(x, t). Model 3 is the
most flexible, but has the most weights to learn.
We use the following binary predicate features
fk(x, t) in all our models, evaluated on each chunk
produced by the semantic role labeler:
1. PropBank role label(s) of the chunk (e.g., Tar-
get, Arg0, Arg1, ArgM-LOC). A chunk can have
multiple role labels if the sentence contains multi-
ple verbs; in this case, we merge the multiple SRL
results by taking their union.
2. Part-of-speech tags of all the words in the
chunk. All syntactic parsing results are obtained
from the Stanford Parser (Klein and Manning,
2003), using the default PCFG model.
3. Phrase type (e.g., NP, VP, PP) of the deepest
syntactic parse tree node covering the entire chunk.
We also include a feature indicating whether the
phrase is nested within an ancestor VP.
4. Lexical features: individual word identities in
the top 5000 most frequent words in the Google 1T
5gram corpus (Brants and Franz, 2006). For other
words, we use their automatically predicted Word-
Net supersenses (Ciaramita and Altun, 2006). Su-
persenses are 41 broad semantic categories (e.g.,
noun.location, verb.communication). By dividing
lexical features in this way, we hope to learn spe-
cific qualities of common words, but generalize
across rarer words.
We also experimented with features derived
from typed dependency relations, but these did not
improve our models. We suspect the PropBank
role labels capture much of the same information.
In addition, the Google 5000-word list was the best
among several word lists that we explored for split-
ting up the lexical features.
4.3 CRF Experimental Results
We trained our CRF models using the MAL-
LET toolkit (McCallum, 2002). Our complete
dataset consists of the 571 manually annotated sen-
10?1 100 101
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
Variance
Ac
cu
ra
cy
 a
nd
 F
1
 
 
Accuracy
F1
Model 1
Model 2
Model 3
Figure 2: 5-fold cross validation results for dif-
ferent values of the regularization parameter (vari-
ance ?2) and three CRF models predicting A, B,
C, or O layout tags.
tences (tags mapped to chunk-level). The only
tuning parameter is the Gaussian prior variance,
?2. We performed 5-fold cross validation, vary-
ing ?2 and comparing performance across models.
Figure 2 demonstrates that peak per-chunk accu-
racy (77.6%) and macro-averaged F1 scores are
achieved using the most general sequence labeling
model. As a result, the user study in the next sec-
tion is based on layouts predicted by Model 3 with
?2 = 1.0, trained on all the data.
To understand which features contribute most
to performance, we experimented with removing
each of the four types (individually). Peak accu-
racy drops the most when lexical features are re-
moved (76.4%), followed by PropBank features
(76.5%), phrase features (76.9%), and POS fea-
tures (77.1%).
The features in the final learned model make in-
tuitive sense. It prefers tag transitions A?B and
B?C, but not A?C or C?A. The model likes the
word ?I? and noun phrases (not nested in a verb
phrase) to have tag A. Verbs and ArgM-NEGs are
frequently tagged B, while noun.object?s, Arg4s,
and ArgM-CAUs are typically C. The model dis-
courages Arg0s and conjunctions in B, and dislikes
adverbial phrases and noun.time?s in C.
While 77.6% cross validation accuracy may
seem low, it is in fact close to the 81% inter an-
notator agreement3, and thus close to optimal. The
confusion matrix (not shown) reveals that most er-
3The 81% agreement is on mapped chunk-level tags with-
out modifiers (Fleiss? kappa 0.74), while the 77% agreement
in Section 3.2 is on word-level tags with modifiers.
123
rors probably arise from disagreements in the in-
dividual annotators. The most common errors are
predicting B for chunks labeled O and confusing
tags B and C. Manually inspecting the pictures in
our training set shows that annotators often omit-
ted the verb (such as ?is? or ?has?) and left the B
position empty, since it could be inferred by the
presence of the arrow and the images in A and C.
Also, annotators tended to disagree on the location
of adverbial expressions, dividing them between
positions B and C. Finally, only 3.3% of chunks
were incorrectly omitted from the pictures. There-
fore, we conclude that our CRFmodels are capable
of predicting the ABC layouts.
5 User Study
We have proposed the ABC layout, and showed
that we can learn to predict it reasonably well. But
an important question remains: Can the proposed
ABC layout help a target audience of limited lit-
eracy understand pictures better, compared to the
linear layout used in state-of-the-art augmentative
and alternative communication software? We de-
scribe a user study as our first attempt to answer
this question. This line of work has two main chal-
lenges: one is the practical difficulty of working
with human subjects of limited literacy; the other is
the lack of a quantitative measure of picture com-
prehension.
[Subjects]: To partially overcome the first chal-
lenge, we recruited two groups of subjects with
medium and high literacy respectively, in hopes
of extrapolating our findings towards the low lit-
eracy group. Specifically, the medium group con-
sisted of seven non-native English speakers who
speak some degree of English??medium literacy?
refers to their English fluency; twelve native En-
glish speakers comprised the high literacy group.
All subjects were adults and did not include the
authors of this paper or the five annotators. The
subjects had no prior exposure to pictorial com-
munication systems.
[Material]: We randomly chose 90 test sen-
tences from three sources4 representing our
target application domains: short narratives
written by and for individuals with commu-
nicative disorders (symbolworld.org);
one-sentence news synopses written in simple
English targeting foreign language learners
(simpleenglishnews.com); and the child
4Distinct from the sources of the 571 training sentences.
writing sections of the LUCY corpus (Sampson,
2003). We created two pictures for each test
sentence: one using a linear layout and one
using an ABC layout. For the linear layout,
we used SymWriter. Typing text in SymWriter
automatically produces a left-to-right sequence
of icons, chosen from an icon database. In cases
where SymWriter suggests several possible icons
for a word, we manually selected the best one. For
words not in the database, we found appropriate
thumbnail images using Web image search. This
is how a typical user would use SymWriter. To
produce the ABC layout, we applied the trained
CRF tagger Model 3 to the test sentence. After
obtaining A, B, C, and O tags for text chunks, we
placed the corresponding icons (from SymWriter?s
linear layout) in the correct layout positions. Icons
for words tagged O did not appear in the ABC
version of the picture. Aside from this difference,
both pictures of each test sentence contained
exactly the same icons?the only difference was
the layout.
[Protocol]: All 19 subjects observed each of
the 90 test sentences exactly once: 45 with the
linear layout and 45 with the ABC layout. The
layouts and the order of sentences were both ran-
domized throughout the sequence, and the subjects
were counter-balanced so each sentence?s linear
and ABC layouts were viewed by roughly equal
numbers of subjects. At the start of the study,
each subject read a brief introduction describing
the task and saw an example of each layout style.
Then for each test sentence, we displayed a pic-
ture, and the subject typed a guess of the underly-
ing sentence. Finally, the subject provided a confi-
dence rating (2=?almost sure,? 1=?maybe correct,?
or 0=?no idea?). We measured response time as
the time from image display until sentence/rating
submission. Figure 3 shows a test sentence in both
layouts, together with several subjects? guesses.
[Evaluation metrics]: As noted above, the
second main challenge is measuring picture
comprehension?we need a way to compare the
original sentences with the subjects? guesses. In
many ways, this is like machine translation (via
pictures), so we turned to two automatic eval-
uation metrics: BLEU-1 (Papineni et al, 2002)
and METEOR (Lavie and Agarwal, 2007). BLEU-1
computes unigram precision (i.e., fraction of re-
sponse words that exactly match words in the orig-
inal), multiplied by a brevity penalty for omit-
124
?we sing a song about a farm.?
?i sing about the farm and animals?
?we sang for the farmer and he gave us animals.?
?Someone went to his grandfather?s farm
and played with the animals?
?i can?t sing in the choir because i have to tend
to the animals.?
?twins sing old macdonald has a farm?
?they sang about a farm?
?they sing old mcdonald had a farm.?
?we have a farm with a sheep, a pig and a cow.?
?two people sing old mcdonald had a farm?
?we sang old mcdonald on the farm.?
?they both sing ?old macdonald had a farm?.?
Figure 3: The linear and ABC layout pictures for the test sentence ?We sang Old MacDonald had a
farm.? and some subjects? guesses. Note the predicted ABC layout omits the ambiguous ?had? icon.
ting words. In contrast, METEOR finds a one-to-
one word alignment between the texts that allows
partial matches (after stemming and by consider-
ing WordNet-based synonyms) and optionally ig-
nores stop words. Based on this alignment, uni-
gram precision, recall, and weighted F measure are
computed, and the final METEOR score is obtained
by scaling F to account for word-order preserva-
tion. We computed METEOR using its default pa-
rameters and the stop word list from the Snowball
project (Porter, 2001).
[Results]: We report average METEOR and BLEU
scores, confidence ratings, and response time for
the 4 conditions (native vs. non-native, ABC vs.
linear) in Table 1. The most striking observation
is that native speakers perform better (in terms of
METEOR and BLEU) with the linear layout, while
non-native speakers do better with ABC. 5
To explain this finding, it is worth noting that
SymWriter pictures include function words, whose
icons are abstract but distinct. We speculate that
even though none of our subjects were trained to
recognize these function-word icons, the native
speakers are more accustomed to the English syn-
tactic structure, so they may be able to transliter-
ate those icons back to words. In an ABC lay-
5Using a Mann-Whitney rank sum test, the difference in
native speakers? METEOR scores is statistically significant
(p = 0.003), though the other differences are not (native
BLEU, p = 0.085; non-native METEOR, p = 0.172; non-
native BLEU, p = 0.170). Nevertheless, we observe some
evidence to support our hypothesis that non-native speak-
ers benefit from the ABC layout, and we intend to conduct
follow-up experiments to test the claim further.
Non-native Native
ABC Linear ABC Linear
METEOR 0.1975 0.1800 0.2955 0.3335
BLEU 0.1497 0.1456 0.2710 0.3011
Conf. 0.50 0.47 0.90 0.89
Time 47.4s 47.8s 38.1s 38.6s
Table 1: User study results.
out, the sentence order is mostly removed, and
some phrases might be omitted due to the O tag.
Thus native speakers do not get as many syntactic
hints. On the other hand, non-native speakers do
not have the same degree of built-in English syn-
tactic knowledge. As such, they do not gain much
from seeing the whole sentence sequence includ-
ing function-word icons. Instead, they may have
benefited from the ABC layout?s added organiza-
tion and potential exclusion of irrelevant icons.
If this reasoning holds, it has interesting impli-
cations for viewers who have lower English liter-
acy: they might take away more meaning from a
semantically structured layout like ABC. Verifying
this is a direction for future work.
Finally, it is interesting that all subjects feel
more confident in their responses to ABC layouts
than linear layouts, and, despite their added com-
plexity, ABC layouts do not require more response
time than linear layouts.
125
6 Conclusions
We proposed a semantically enhanced picture lay-
out for pictorial communication. We formulated
our ABC layout prediction problem as sequence
tagging, and trained CRF models with linguistic
features including semantic role labels. A user
study indicated that our ABC layout has the poten-
tial to facilitate picture comprehension for people
with limited literacy. Future work includes incor-
porating ABC layouts into our pictorial communi-
cation system, improving other components, and
verifying our findings with additional user studies.
Acknowledgments
This work is supported by NSF IIS-0711887, and
by the Wisconsin Alumni Research Foundation.
References
Adorni, G., M. Di Manzo, and G. Ferrari. 1983. Natu-
ral language input for scene generation. In ACL.
Baker, C. F., C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In COLING.
Brants, T. and A. Franz. 2006. Web 1T 5-gram version
1.1. Linguistic Data Consortium, Philadelphia.
Brown, D. C. and B. Chandrasekaran. 1981. Design
considerations for picture production in a natural lan-
guage graphics system. SIGGRAPH, 15(2).
Chandrasekar, R., C. Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
COLING.
Ciaramita, M. and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In EMNLP.
Clay, S. R. and J. Wilhelms. 1996. Put: Language-
based interactive manipulation of objects. IEEE
Computer Graphics and Applications, 16(2).
Coyne, B. and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
Fleiss, J. L. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76(5).
Gildea, D. and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3).
Hehner, B. 1980. Blissymbols for use. Blissymbolics
Communication Institute.
Johansson, R., A. Berglund, M. Danielsson, and
P. Nugues. 2005. Automatic text-to-scene conver-
sion in the traffic accident domain. In IJCAI.
Joshi, D., J. Z. Wang, and J. Li. 2006. The story pictur-
ing engine?a system for automatic text illustration.
ACM Transactions on Multimedia Computing, Com-
munications, and Applications, 2(1).
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
Lavie, A. and A. Agarwal. 2007. METEOR: An au-
tomatic metric for MT evaluation with high levels of
correlation with human judgments. In Second Work-
shop on Statistical Machine Translation, June.
McCallum, A. K. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Mihalcea, R. and B. Leong. 2006. Toward commu-
nicating simple sentences using pictorial representa-
tions. In Association of Machine Translation in the
Americas.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
Porter, M. F. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/.
Pradhan, S., W. Ward, K. Hacioglu, J. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In HLT/NAACL.
Sampson, G. 2003. The structure of children?s writ-
ing: Moving from spoken to adult written norms. In
Granger, S. and S. Petch-Tyson, editors, Extending
the Scope of Corpus-Based Research. Rodopi.
Vickrey, D. and D. Koller. 2008. Sentence simplifica-
tion for semantic role labeling. In ACL. To appear.
Widgit Software. 2007. SymWriter.
http://www.mayer-johnson.com.
Yamada, A., T. Yamamoto, H. Ikeda, T. Nishida, and
S. Doshita. 1992. Reconstructing spatial image
from natural language texts. In COLING.
Zhu, X., A. B. Goldberg, M. Eldawy, C. Dyer, and
B. Strock. 2007. A Text-to-Picture synthesis system
for augmenting communication. In AAAI.
126
