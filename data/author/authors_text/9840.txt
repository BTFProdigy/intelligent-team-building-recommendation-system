R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 245 ? 256, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Method of Recognizing Entity and Relation 
Xinghua Fan1, 2 and Maosong Sun1 
1
 State Key Laboratory of Intelligent Technology and Systems,  
Tsinghua University, Beijing 100084, China 
fanxh@tsinghua.org.cn, sms@mail.tsinghua.edu.cn 
2
 State Intellectual Property Office of P.R. China, Beijing, 100088, China 
Abstract. The entity and relation recognition, i.e. (1) assigning semantic classes 
to entities in a sentence, and (2) determining the relations held between entities, 
is an important task in areas such as information extraction. Subtasks (1) and 
(2) are typically carried out sequentially, but this approach is problematic: the 
errors made in subtask (1) are propagated to subtask (2) with an accumulative 
effect; and, the information available only in subtask (2) cannot be used in sub-
task (1). To address this problem, we propose a method that allows subtasks (1) 
and (2) to be associated more closely with each other. The process is performed 
in three stages: firstly, employing two classifiers to do subtasks (1) and (2) in-
dependently; secondly, recognizing an entity by taking all the entities and rela-
tions into account, using a model called the Entity Relation Propagation Dia-
gram; thirdly, recognizing a relation based on the results of the preceding stage. 
The experiments show that the proposed method can improve the entity and re-
lation recognition in some degree. 
1   Introduction 
The entity and relation recognition, i.e. assigning semantic classes (e.g., person, or-
ganization and location) to entities in a sentence and determining the relations (e.g., 
born-in and employee-of) that hold between entities, is an important task in areas such 
as information extraction (IE) [1] [2] [3] [4], question answering (QA) [5] and story 
comprehension [6]. In a QA system, many questions concern the specific entities in 
some relations. For example, the question that ?Where was Poe born?? in TREC-9 
asks for the location entity in which Poe was born. In a typical IE task in constructing 
a job database from unstructured texts, the system are required to extract many mean-
ingful entities like titles and salary from the texts and to determine how these entities 
are associated with job positions. 
The task of recognizing entity and relation is usually treated as two separate sub-
tasks carried out sequentially: (1) to recognize entities using an entity recognizer, and 
(2) to determine the relations held between them. This approach has two shortcom-
ings. Firstly, the errors made in subtask (1) will be propagated to subtask (2) with an 
accumulative effect, leading to a loss in performance of relation recognition. For 
example, if ?Boston? is mislabeled as a person, it will never have chance to be classi-
fied as the location of Poe?s birthplace. Secondly, the information available only in 
246 X. Fan and M. Sun 
subtask (2) cannot be used for subtask (1). For example, if we feel difficult to  
determine whether the entity X is a person or not, but we can determine that there 
exists a relation born-in between X and China easily, it is obvious that we can claim 
that X must be a person. 
To address the problems described above, this paper presents a novel approach 
which allows subtasks (1) and (2) to be linked more closely together. The process is 
separated into three stages. Firstly, employing two classifiers to perform subtasks (1) 
and (2) independently. Secondly, recognizing an entity by taking all the entities and 
relations into account using a particularly designed model called the Entity Relation 
Propagation Diagram. And, thirdly, recognizing a relation based on the results of the 
preceding step. 
The rest of the paper is organized as follows. Section 2 defines the problem of en-
tity and relation recognition in a formal way. Section 3 describes the proposed method 
of recognizing entity and relation. Section 4 gives the experimental results. Section 5 
is the related work and comparison. Section 6 is conclusions. 
2   The Problem of Entity and Relation Recognition 
Conceptually, the entities and relations in a sentence can be viewed, while taking 
account of the mutual dependencies among them, as a labeled graph in Fig. 1. 
 
Fig. 1. Concept view of the entities and relations among them 
In Fig.1, a node represents an entity and a link denotes the relation held between 
two entities. The arrowhead of a link represents the direction of the relation. Each 
entity or relation has several attributes, which are structured as a list of the node or the 
edge. These attributes can be classified into two classes. Some of them that are easy to 
acquire, such as words in an entity and parts of speech of words in a context, are 
called local attributes; the others that are difficult to acquire, such as semantic classes 
of phrases and relations among them, are called decision attributes. The issue of entity 
and relation recognition is to determine a unique value for each decision attribute of 
all entities and relations, by considering the local attributes of them. To describe the 
problem in a formal way, we first give some basic definitions as follows. 
 A Method of Recognizing Entity and Relation 247 
Definition 1 (Entity). An entity can be a single word or a set of consecutive words 
with a predefined boundary. A sentence is a linked list, which consists of words and 
entities. Entities in a sentence are denoted as E1, E2 ? according to their order, with 
values ranging over a set of entity class CE. For example, the sentence in Fig. 2 has 
three entities: E1= ?Dole?, E2= ?Elizabeth? and E3= ?Salisbury, N.C.?. Note that it is 
not easy to determine the entity boundaries [7]. Here we assume that it has been 
solved and its output serves as the input to our model.  
 
Fig. 2. A sentence that have three entities 
Definition 2 (Relation). In this paper, we only consider the relation between two 
entities. An entity pair (Ei, Ej) represents a relation Rij from entity Ei and Ej, where Ei 
is the first argument and Ej is the second argument. Relation Rij takes its value that 
ranges over a set of relation class CR. Note that (Ei, Ej) is an ordered pair, and there 
exist two relations Rij =(Ei, Ej) and Rji =(Ej, Ei) between entities Ei and Ej. 
Definition 3 (Class). The class of an entity or relation is its decision attribute, which 
is one of the predefined class set and is unknown before being recognized. We denote 
the sets of predefined entity class and relation class as CE and CR respectively. CE has 
one special element other-ent, which represents any unlisted entity class. For algo-
rithmic reasons, we suppose all elements in CE are mutually exclusive. Similarly, CR 
also has one special element other-rel, which represents that the two involved entities 
are irrelevant or their relation class is undefined. For algorithmic reasons, we suppose 
all elements in CR are mutually exclusive. In fact, because the class of an entity or a 
relation is only a label that we want to predict, if an entity or a relation have more 
than one labels simultaneously, to satisfy the constraint that all elements in CE or CR 
are mutually exclusive, we can separate it into several cases and construct several 
predefined entity class sets and relation class sets.  
The classes of entities and relations in a sentence must satisfy some constraints. For 
example, if the class of entity E1, which is the first argument of relation R12, is a loca-
tion, then the class of relation R12 cannot be born-in because the class of the first ar-
gument in relation R12 has to be a person. 
Definition 4 (Constraint). A constraint is a 5-tuple ),,, R,( R21 ????? . The symbols 
are defined as follows. RCR ?  represents the class of relation R. E21 C, ??? repre-
sents the classes of the first argument Ei and the second argument Ej in the relation R 
respectively. ]1,0[?R?  is a real number that represents a joint conditional probability 
distribution }R|,Pr{ 21R ??? = . ]1,0[???  is a real number that represents a condi-
tional probability distribution },|RPr{ 21 ???? = . Note that R?  and ??  need not to 
be specified manually and can be learned from an annotated training dataset easily. 
248 X. Fan and M. Sun 
Definition 5 (Observation). We denote the observations of an entity and a relation in 
a sentence as OE and OR respectively. OE or OR represent all the ?known? local attrib-
utes of an entity or a relation, e.g., the spelling of a word, parts of speech, and  
semantic related attributes acquired from external resources such as WordNet. The  
observations OE and OR can be viewed as a random event, and 1}Pr{O}Pr{O RE ?=  
because OE and OR in a sentence are known.        
Based on the above definitions, the issue of entity and relation recognition can be 
described in a formal way as follows. Suppose in a sentence, the set of entity is {E1, 
E2 ? En}, the set of relation is {R12, R21, R13, R31, ?, R1n, Rn1, ?, Rn-1,n, Rn,n-1}, the 
predefined sets of entity class and relation class are CE ={e1, e2, ? em} and CR ={ r1, 
r2, ? rk} respectively, the observation of entity Ei is EiO , and the observation of rela-
tion Rij is RijO . n, m and k represent the number of entity, the number of the prede-
fined entity class and the number of the predefined relation class respectively. The 
problem is to search the most probable class assignment for each entity and each 
relation of interest, given the observations of all entities and relations. In other words, 
the problem is to solve the following two equations, using two kinds of constraint 
knowledge ???  ,R  and the interaction among entities and relations.  
} O ,O ,,O ,O , ,O ,O ,O, ,O ,O|ePr{E max arge R 1-nn,R n1,-nRn1R1nR21R12EnE2E1did LLL==  (1) 
} O ,O ,,O ,O , ,O ,O ,O, ,O ,O|rPr{R max argr R 1-nn,R n1,-nRn1R1nR21R12EnE2E1dijd LLL==  (2) 
In (1), d =1, 2, ?, m, and in (2), d=1, 2, ?, k. 
3   The Proposed Method 
Because the class assignment of a single entity or relation depends not only on local 
attributes itself, but also on those of all other entities and relations, the equations (1) 
and equation (2) cannot be solved directly. To simplify the problem, we present the 
following method consisting of three stages. Firstly, employ two classifiers to perform 
entity recognition and relation recognition independently. Their outputs are the condi-
tional probability distributions Pr{E| OE} and Pr{R|OR}, given the corresponding 
observations. Secondly, recognize an entity by taking account of all entities and rela-
tions, as computed in the previous step. This is achieved by using the model Entity 
Relation Propagation Diagram (ERPD). And, recognize a relation based on the results 
of the second step at last.       
In this paper, we concentrate on the processes at the second and the third stages, as-
suming that the process at the first stage is solved and its output are given to us as 
input. At the second stage, the aim of introducing ERPD is to estimate the conditional 
probability distribution 
 ERPD}|Pr{E  given the constraint R?  in Definition 5 and 
the sets {  }O|Pr{E Eii } and { }O|Pr{R R ijij } (i, j=1,?,n), as computed at the first 
stage. For the readability, suppose 
 ERPD}|Pr{E is given, the entity recognition 
equation (1) becomes the equation (3). 
 A Method of Recognizing Entity and Relation 249 
??
???
?=
>=
=
?
?
RV     ERPD}|ePr{E max arg
RV     }O|ePr{E max arg
e
dd
E
dd
i
i
i
 (3) 
where ?  is a threshold determined by the experiment. RV? [0, 1] is a real number, 
called the reliable value, representing the belief degree of the output of the entity 
recognizer at the first stage. Suppose the maximum value of the conditional probabil-
ity distribution  }O|Pr{E E is Vm and the second value is Vs, RV is defined as: 
sm
sm
VV
VVRV
+
?
=  (4) 
The reason of introducing RV is due to a fact that only for ambiguous entities, it is 
effective by taking the classes of all entities in a sentence into account. ?Reliable 
Value? measures whether an entity is ambiguous. 
At the third stage, the basic idea of recognizing a relation is to search the probable 
relation given its observation, under a condition of satisfying the constraints imposed 
by the results of entity recognition at the second stage. The relation recognition equa-
tion (2) becomes the equation (5). 
RR
k
k W}O|rPr{R max argr ?==  
??
???
=
>
=
 0}?,?|Pr{r if  0
0}?,?|Pr{r if   1
W
21
21
R  
(5) 
where 21,??  is the results of entity recognition at the second stage, },|Pr{r 21 ??  is 
constraint knowledge ??  in Definition 4, and WR is the weight of the constraint 
knowledge. 
In the following sections, we present ERPD and two algorithms to estimate the 
conditional probability distribution
 ERPD}|Pr{E . 
3.1   The Entity Relation Propagation Diagram 
To represent the mutual dependencies among entities and relations, a model named 
the Entity Relation Propagation Diagram that can deal with cycles, similar to the 
Causality Diagram [8][9] for the complex system fault diagnosis, is developed for 
entity and relation recognition. 
The classes of any two entities are dependent on each other through the relations 
between them, while taking account of the relations in between. For example, the 
class of entity Ei in Fig. 3 (a) depends on the classes of relations Rji between entities 
Ei and Ej, and the classes of relations Rij and Rji depend on the classes of entities  
Ei and Ej. This means that we can predict the class of a target entity according to the 
class of its neighboring entity, making use of the relations between them. We  
further introduce the relation reaction intensity to describe the prediction ability of 
this kind. 
250 X. Fan and M. Sun 
 
Fig. 3. Illustration of relation reaction 
Definition 6 (Relation Reaction Intensity). We denote the relation reaction intensity 
from entity Ei to entity Ej as Pij, which represents the ability that we guess the class of 
Ej if we know the class of its neighboring entity Ei and the relation Rij between them. 
The relation reaction intensity could be modeled using a condition probability distri-
bution Pij=Pr {Ej |Ei}. 
The element klijp of Pij represents the conditional probability Pr {Ej=el |Ei=ek}:  
?
=
=
====
====
N
1t ki
tijljkitij
kilj
kl
ij }ePr{E
}rR| eE ,e}Pr{ErPr{R}eE|eEPr{p  
according to Definition 5:  
}O|rPr{R}rPr{R Rijtijtij === , }O|ePr{E}ePr{E Eikiki ===  
Then, we have: 
 }O|ePr{E
}rR| eE ,e}Pr{EO|rPr{R
 
N
1t
E
iki
tijljki
R
ijtijkl
ij ?
=
=
====
=p  (6) 
where Rt Cr ? , N is the number of relations in relation class set. In equation (6), 
}rR| eE ,ePr{E tijljki === represents the constraint knowledge R?  among entities 
and relations. }O|rPr{R Rijtij =  and }O|ePr{E Eiki =  represent the outputs at the  
first stage.  
Definition 7 (Observation Reaction Intensity). We denote the observation reaction 
intensity as the conditional probability distribution }O|Pr{E E  of an entity class, 
given the observation, which is the output at the first stage. 
The Entity Relation Propagation Diagram (ERPD). is a directed diagram that 
allows cycles. As illustrated in Fig. 4, the symbols used in the ERPD are defined as 
follows. A circle node represents an event variable that can be any one from a set of 
mutually exclusive events, which all together cover the whole sample space. Here, an  
event variable represents an entity, an event represents a predefined entity class, and 
the whole sample space represents the set of predefined entity classes. Box node 
represents a basic event which is one of the independent sources of the associated 
event variable. Here, a basic event represents the observation of an entity. Directed 
arc represents a linkage event variable that may or may not enable an input event to 
cause the corresponding output event. The linkage event variable from an event  
 A Method of Recognizing Entity and Relation 251 
variable to another event variable represents the relation reaction intensity in Defini-
tion 6. And, the linkage event variable from a basic event to the corresponding event 
variable represents the observation reaction intensity in Definition 7. All arcs pointing 
to a node are in a logical OR relationship. 
 
Fig. 4. Illustration of the Entity Relation Propagation Diagram 
Now, we present two algorithms to compute the conditional probability distribu-
tion
 ERPD}|Pr{E , one is based on the entity relation propagation tree, and the other 
is the directed iteration algorithm on ERPD. 
3.2   The Entity Relation Propagation Tree 
The Entity Relation Propagation Tree (ERPT). is a tree decomposed from an 
ERPD, which represents the relation reaction propagation from all basic events to 
each event variable logically. Each event variable in the ERPD corresponds to an 
ERPT. For example, the ERPT of X1 in Fig. 4 is illustrated in Fig. 5. The symbols 
used in the ERPT are defined as follows. The root of the tree, denoted as Circle, is an 
event variable corresponding to the event variable in the ERPD. A leaf of the tree, 
denoted as Box, is a basic event corresponding to the basic event in the ERPD. The 
middle node of the tree, denoted as Diamond, is a logical OR gate variable, which is 
made from an event variable that has been expanded in the ERPD, and, the label in 
Diamond corresponds to the label of the expanded event variable. The directed arc of 
the tree corresponds to the linkage event variable in the ERPD. All arcs pointing to a 
node are in a logical OR relationship. The relation between the directed arc and the 
node linked to it is in logical AND relationship. 
To decompose an ERPD into entity relation propagation trees, firstly we decom-
pose the ERPD into mini node trees. Each event variables in the ERPD corresponds to 
a mini node tree, in which the root of the mini tree is the event variable in concern at 
present, and the leaves are composed of all neighboring basic events and event vari-
ables that are connected to the linkage event variables pointing to the top event vari-
ables. Secondly, expand a mini node tree into an entity relation propagation tree, i.e., 
the neighboring event variables in the mini node tree are replaced with their corre-
sponding mini trees. During expanding a node event variable, when there are loops, 
Rule BreakLoop is applied to break down the loops. 
252 X. Fan and M. Sun 
 
Fig. 5. Illustration of the entity relation propagation tree 
Rule BreakLoop. An event variable cannot propagate the relation reaction to itself. 
Rule 1 is derived from a law commonsense - one can attest that he is sinless. When such 
a loop is encountered, the descendant event variable, which is same as the head event 
variable of the loop, is treated as a null event variable, together with its connected link-
age event variable to be deleted.    
Compute the Conditional Probability Distribution in an ERPT. After an ERPD is 
decomposed into entity relation propagation trees, the conditional probability distribu-
tion  ERPD}|Pr{E becomes  ERPT}|Pr{E . When an event variable Xi has more than 
one input, these inputs will be in logic OR relationship, as defined in the ERPD. Since 
these inputs are independent, there exists such a case that one input causes Xi to be an 
instance kiX  while another input causes Xi to be an instance
l
iX , this would be impos-
sible because kiX  and 
l
iX  are exclusive. In the real world, the mechanism, in which 
iX  can response to more than one independent input properly, is very complicated 
and may vary from one case to another. To avoid this difficulty, a basic assumption is 
introduced.           
Assumption. When there is more than one input to Xi, each input will contribute a 
possibility to Xi. For each input, its contribution to this possibility equals to the prob-
ability that it causes Xi directly, as if the other inputs do not exist. The final possibility 
that Xi occurs is the sum of the possibilities from all inputs. 
Suppose an event variable X has m inputs, and the probability distributions of all 
linkage event variables, linked basic events or event variables are Pi and Pr {Xi} re-
spectively, i=1,2?m. Based on the above assumption, the formula for computing the 
probability distribution of X can be derived as: 
)
}Pr{X
}Pr{X
PNorm(
}Pr{X
}Pr{X
m
1i n
i
1
i
i
n
1
?
= ??
?
?
?
??
?
?
?
?=
??
?
?
?
??
?
?
?
MM  (7) 
 A Method of Recognizing Entity and Relation 253 
where, Norm () is a function that normalizes the vector in {}, and n is the state  
number of X. 
So, the probability distribution  ERPT}|Pr{E of the variable X in the correspond-
ing ERPT can be computed in the following steps. Firstly, to find the middle node 
sequence in the corresponding ERPT in the depth-first search; secondly, according to 
the sequence, for each middle node, equation (7) is applied to compute its probability 
distribution. In this procedure, the previous results can be used for the latter  
computation. 
3.3   The Directed Iteration Algorithm on ERPD 
The idea is to compute the probability distribution of the event variable on the ERPD 
directly, without decomposing the ERPD to some ERPTs. The aim is to avoid the 
computational complexity of using ERPT. This is achieved by adopting an iteration 
strategy, which is the same as that used in the loopy belief network [10]. 
The Directed Iteration Algorithm. is as follows: Firstly, only take the basic event as 
input, and initialize each event variable according to formula (7), i.e., assigning an 
initialized probability distribution to each event variable. Secondly, take the basic 
event and the probability distributions of all neighboring nodes computed in the pre-
vious step as input, and iterate to update the probability distributions of all nodes in 
ERPD in parallel according to formula (7). Thirdly, if none of the probability distribu-
tion of all nodes in ERPD in successive iterations changes larger than a small thresh-
old, the iteration is said to converge and then stops. 
4   Experiments 
Dataset. The dataset in our experiments is the same as the Roth?s dataset ?all? [11], 
which consists of 245 sentences that have the relation kill, 179 sentences that have the 
relation born-in and 502 sentences that have no relations. The predefined entity 
classes are other-ent, person and location, and the predefined relation classes are 
other-rel, kill and born-in. In fact, we use the results at the first stage in our method as 
the input, which are provided by W. Yih. 
Experiment Design. We compare five approaches in the experiments: Basic, Omnis-
cient, ERPD, ERPD* and BN. The Basic approach, which is a baseline, tests the per-
formance of the two classifiers at the first stage, which are learned from their local 
attributes independently. The Omniscient approach is similar to Basic, the only defer-
ence is that the classes of entities are exposed to relation classifier and vice versa. 
Note that it is certainly impossible to know the true classes of an entity and a relation 
in advance. The BN is the method based on the belief network, -- we follow the BN 
method according to the description in [11]. The ERPD is the proposed method based 
on ERPT, and the ERPD* is the proposed method based on the directed iteration 
algorithm. The threshold of RV is 0.4. 
Results. The experimental results are shown in Table 1. It can be seen from the table 
that 1) it is very difficult to improve the entity recognition because BN and Omnis-
cient almost do not improve the performance of Basic; 2) the proposed method can 
254 X. Fan and M. Sun 
improve the precision, which is thought of being more important than the recall for 
the task of recognizing entity; 3) the relation recognition can be improved if we can 
improve the entity recognition, as indicated by the comparisons of Basic, ERPD and 
Omniscient; 4) the proposed method can improve the relation recognition, and it per-
formance is almost equal to that of BN; 5) the performance of ERPD and ERPD* is 
almost equal, so the directly iteration algorithm is effective. 
Table 1. Experimental results 
 
5   Related Work and Comparison  
Targeting at the problems mentioned above, a method based on the belief network has 
been presented in [11], in which two subtasks are carried out simultaneously. Its pro-
cedure is as follows: firstly, two classifiers are trained for recognizing entities and 
relations independently and their outputs are treated as the conditional probability 
distributions for each entity and relation, given the observed data; secondly, this in-
formation together with the constraint knowledge among relations and entities are 
represented in a belief network [12] and are used to make global inferences for all 
entities and relations of interest. This method is denoted BN in our experiments.  
Although BN can block the error propagation from the entity recognizer to the rela-
tion classifier as well as improve the relation recognition, it cannot make use of the 
information, which is only available in relation recognition, to help entity recognition. 
Experiments show that BN cannot improve entity recognition. 
Comparing to BN, the proposed method in this paper can overcome the two short-
comings of it. Experiments show that it can not only improve the relation recognition, 
but also improve the precision of entity recognition. Moreover, the model ERPD 
could be more expressive enough than the belief network for the task of recognizing 
 A Method of Recognizing Entity and Relation 255 
entity and relation. It can represent the mutually dependences between entities and 
relations by introducing relation reaction intensity, and can deal with a loop without 
the limitation of directed acyclic diagram (DAG) in the belief network. At the same 
time, the proposed method can merge two kinds of constraint knowledge (i.e. 
???  and R  in Definition 4), but the method based on belief network can only use ?? . 
Finally, the proposed method has a high computation efficiency while using the di-
rected iteration algorithm. 
6   Conclusions 
The subtasks of entity recognition and relation recognition are typically carried out 
sequentially. This paper proposed an integrated approach that allows the two subtasks 
to be performed in a much closer way. Experimental results show that this method can 
improve the entity and relation recognition in some degree. 
In addition,  the Entity Relation Propagation Diagram (ERPD) is used to figure out 
the dependencies among entities and relations. It can also merge some constraint 
knowledge. Regarding to ERPD, two algorithms are further designed, one is based on 
the entity relation propagation tree, the other is the directed iteration algorithm on 
ERPD. The latter can be regarded as an approximation of the former with a higher 
computational efficiency. 
Acknowledgements 
We would like to express our deepest gratitude to Roth D. and Yih W. for making 
their dataset available for us. The research is supported in part by the National 863 
Project of China under grant number 2001AA114210-03, the National Natural Sci-
ence Foundation of China under grant number 60321002, and the Tsinghua-ALVIS 
Project co-sponsored by the National Natural Science Foundation of China under 
grant number 60520130299 and EU FP6. 
References 
1. Chinchor, N. MUC-7 Information Extraction Task Definition. In Proceeding of the Sev-
enth Message Understanding Conference (MUC-7), Appendices, 1998. 
2. Califf, M. and Mooney, R. Relational Learning of Pattern-match Rules for Information 
Extraction. In Proceedings of the Sixteenth National Conference on Artificial Intelligence 
and Eleventh Conference on Innovative Applications of Artificial Intelligence, 328-334, 
Orlando, Florida, USA, AAAI Press, 1999. 
3. Freitag, D. Machine Learning for Information Extraction in Informal Domains. Machine 
learning, 39(2/3): 169-202, 2000. 
4. Roth, D. and Yih, W. Relational Learning via Prepositional Algorithms: An Information 
Extraction Case Study. In Proceedings of the Seventeenth International Joint Conference on 
Artificial Intelligence, 1257-1263, Seattle, Washington, USA, Morgan Kaufmann, 2001. 
5. Voorhees, E. Overview of the Trec-9 Question Answering Track. In The Ninth Text Re-
trieval Conference (TREC-9), 71-80, 2000. 
256 X. Fan and M. Sun 
6. Hirschman, L., Light, M., Breck, E. and Burger, J. Deep Read: A Reading Comprehension 
System. In Proceedings of the 37th Annual Meeting of Association for Computational Lin-
guistics, 1999. 
7. Abney, S.P. Parsing by Chunks. In S. P. Abney, R. C. Berwick, and C. Tenny, editors, 
Principle-based parsing: Computation and Psycholinguistics, 257-278. Kluwer, Dordrecht, 
1991. 
8. Xinghua Fan. Causality Diagram Theory Research and Applying it to Fault Diagnosis of 
Complexity System, Ph.D. Dissertation of Chongqing University, P.R. China,  2002. 
9. Xinghua Fan, Zhang Qin, Sun Maosong, Huang Xiyue. Reasoning Algorithm in Multi-
Valued Causality Diagram, Chinese Journal of Computers, 26(3), 310-322, 2003. 
10. Murphy, K., Weiss, Y., and Jordan, M. Loopy Belief Propagation for Approximate Infer-
ence: An empirical study. In Proceeding of Uncertainty in AI, 467-475, 1999. 
11. Roth, D. and Yih, W. Probability Reasoning for Entity & Relation Recognition. In Pro-
ceedings of 20th International Conference on Computational Linguistics (COLING-02), 
835-841, 2002. 
12. Pearl, J. Probability Reasoning in Intelligence Systems. Morgan Kaufmann, 1988. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 302 ? 313, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Classifying Chinese Texts in Two Steps 
Xinghua Fan1, 2, 3, Maosong Sun1, Key-sun Choi3, and Qin Zhang2 
1
 State Key Laboratory of Intelligent Technology and Systems, Tsinghua University,  
Beijing 100084, China 
fanxh@tsinghua.org.cn, sms@tsinghua.edu.cn 
2
 State Intellectual Property Office of P.R. China, Beijing, 100088, China 
zhangqin@sipo.gov.cn 
3
 Computer Science Division, Korterm, KAIST, 373-1 Guseong-dong Yuseong-gu, 
Daejeon 305-701, Korea 
kschoi@cs.kaist.ac.kr 
Abstract. This paper  proposes a two-step method for Chinese text categoriza-
tion (TC). In the first step, a Na?ve Bayesian classifier is used to fix the fuzzy 
area between two categories, and, in the second step, the classifier with more 
subtle and powerful features is used to deal with documents in the fuzzy area, 
which are thought of being unreliable in the first step. The preliminary experi-
ment validated the soundness of this method. Then, the method is extended 
from two-class TC to multi-class TC. In this two-step framework, we try to fur-
ther improve the classifier by taking the dependences among features into con-
sideration in the second step, resulting in a Causality Na?ve Bayesian Classifier. 
1   Introduction 
Text categorization (TC) is a task of assigning one or multiple predefined category 
labels to natural language texts. To deal with this sophisticated task, a variety of sta-
tistical classification methods and machine learning techniques have been exploited 
intensively[1], including the Na?ve Bayesian (NB) classifier [2], the Vector Space 
Model (VSM)-based classifier [3], the example-based classifier [4], and the Support 
Vector Machine [5]. 
Text  filtering is a basic type of text categorization (two-class TC). It can find 
many real-life applications [6], a typical one is the ill information filtering, such as 
erotic information and garbage information filtering on the web, in e-mails and in 
short messages of mobile phone. It is obvious that this sort of information should be 
carefully controlled. On the other hand, the filtering performance using the existing 
methodologies is still not satisfactory in general. The reason lies in that there exist a 
number of documents with high degree of ambiguity, from the TC point of view, in a 
document collection, that is, there is a fuzzy area across the border of two classes (for 
the sake of expression, we call the class consisting of the ill information-related texts, 
or, the negative samples, the category of TARGET, and, the class consisting of the ill 
information-not-related texts, or, the positive samples, the category of Non-
TARGET). Some documents in one category may have great similarities with some 
other documents in the other category, for example, a lot of words concerning love 
 Classifying Chinese Texts in Two Steps 303 
story and sex are likely appear in both negative samples and positive samples if the 
filtering target is erotic information. We observe that most of the classification errors 
come from the documents falling into the fuzzy area between two categories. 
The idea of this paper is inspired by the fuzzy area between categories.  A two-step 
TC method is thus proposed: in the first step, a classifier is used to fix the fuzzy area 
between categories; in the second step, a classifier (probably the same as that in the 
first step) with more subtle and powerful features is used to deal with documents in 
the fuzzy area which are thought of being unreliable in the first step. Experimental 
results validate the soundness of this method. Then we extend it from two-class TC to 
multi-class TC. Furthermore, in this two-step framework, we try to improve the clas-
sifier by taking the dependences among features into consideration in the second step, 
resulting in a Causality Na?ve Bayesian Classifier. 
This paper is organized as follows: Section 2 describes the two-step method in the 
context of two-class Chinese TC; Section 3 extends it to multi-class TC; Section 4 
introduces the Causality Na?ve Bayesian Classifier; and Section 5 is conclusions.  
2   Basic Idea: A Two-Step Approach to Text Categorization 
2.1   Fix the Fuzzy Area Between Categories by the Na?ve Bayesian Classifier 
We use the Na?ve Bayesian Classifier to fix the fuzzy area in the first step. For a 
document represented by a binary-valued vector d = (W1, W2, ?, W|D|), the two-class 
Na?ve Bayesian Classifier is given as follows: 
???
===
?++=
=
||
1 2
2
||
1 1
1
||
1 2
1
2
1
2
1
1
log
1
log
1
1
log}Pr{
}Pr{
log
}Pr{
}Pr{
log)(
D
k k
k
k
D
k k
k
k
D
k k
k
-p
p
W
-p
p
W
-p
-p
c
c
|dc
|dcdf
 
 
(1) 
where Pr{
?
} is the probability that event {
?
} occurs, ci  is category i, and 
pki=Pr{Wk=1|ci} (i=1,2). If f(d) ?0, the document d will be assigned the category label 
c1, otherwise, c2.   
Let:  
?
=
+=
||
1 2
1
2
1
1
1log}Pr{
}Pr{log
D
k k
k
-p
-p
c
cCon  (2) 
?
=
=
||
1 1
1
1
log
D
k k
k
k
-p
p
WX  (3) 
?
=
=
||
1 2
2
1
log
D
k k
k
k
-p
pWY  (4) 
304 X. Fan et al 
where Con is a constant relevant only to the training set, X and Y are the measures 
that the document d belongs to categories c1 and c2 respectively.  
We rewrite (1) as: 
ConYXdf +?=)(  (5) 
Apparently,  f(d)=0 is the separate line in a two-dimensional space with X and Y 
being X-coordinate and Y-coordinate. In this space, a given document d can be 
viewed as a point (x, y), in which the values of x and y are calculated according to (3) 
and (4).  
As shown in Fig.1, the distance from the point (x, y) to the separate line will be: 
)(
2
1 ConyxDist +?=  (6) 
 
 
Fig. 1. Distance from point (x, y) to the separate line 
Fig. 2 illustrates the distribution of a training set (refer to Section 2.2) regarding 
Dist in the two-dimensional space, with the curve on the left for the negative samples, 
and the curve on the right for the positive samples. As can be seen in the figure, most 
of the misclassified documents, which unexpectedly across the separate line, are near 
the line. The error rate of the classifier is heavily influenced by this area, though the 
documents falling into this area only constitute a small portion of the training set.  
 
Fig. 2. Distribution of the training set in the two-dimensional space 
 Classifying Chinese Texts in Two Steps 305 
Thus, the space can be partitioned into reliable area and unreliable area: 
??
??
?
<
>
??
  reliable is   to  label  theAssigning                 
 reliable is    to  label  theAssigning        
 unreliable is for Decision          
22
11
12
dc,     DistDist
 dc     ,         DistDist
d, DistDistDist
 (7) 
where Dist1 and Dist2 are constants determined by experiments, Dist1 is positive real 
number and Dist2 is negative real number. 
In the second step, more subtle and powerful features will be designed in particular 
to tackle the unreliable area identified in the first step. 
2.2   Experiments on the Two-Class TC 
The dataset used here is composed of 12,600 documents with 1,800 negative samples 
of TARGET and 10,800 positive samples of Non-TARGET. It is split into 4 parts 
randomly, with three parts as training set and one part as test set. All experiments in 
this section are performed in 4-fold cross validation.  
CSeg&Tag3.0, a Chinese word segmentation and POS tagging system developed 
by Tsinghua University, is used to perform the morphological analysis for Chinese 
texts. In the first step, Chinese words with parts-of-speech verb, noun, adjective and 
adverb are considered as features. The original feature set is further reduced to a much 
smaller one according to formula (8) or (9). A Na?ve Bayesian Classifier is then ap-
plied to the test set. In the second step, only the documents that are identified unreli-
able in terms of (7) in the first step are concerned. This time, bigrams of Chinese 
words with parts-of-speech verb and noun are used as features, and the Na?ve Bayes-
ian Classifier is re-trained and applied again. 
?
=
=
n
i ik
ik
ikk
ct
,ct
,ct,ctMI
1
1 }Pr{}Pr{
}Pr{
log}Pr{)(  (8) 
?
=
=
n
i ik
ik
k
ct
,ct
,ctMI
1
2 }Pr{}Pr{
}Pr{
log)(  (9) 
where tk stands for the kth feature, which may be a Chinese word or a word bigram, 
and ci is the ith predefined category.  
We try five methods as follows.  
Method-1: Use Chinese words as features, reduce features with (9), and classify 
documents directly without exploring the two-step strategy.  
Method-2: same as Method-1 except feature reduction with (8).  
Method-3: same as Method-1 except Chinese word bigrams as features.  
Method-4: Use the mixture of Chinese words and Chinese word bigrams as fea-
tures, reduce features with (8), and classify documents directly.  
Method-5: (i.e., the proposed method): Use Chinese words as features in the first 
step and then use word bigrams as features in the second step, reduce features with 
(8), and classify the documents in two steps. 
306 X. Fan et al 
Note that the proportion of negative samples and positive samples is 1:6. Thus if 
all the documents in the test set is arbitrarily set to positive, the precision will reach 
85.7%. For this reason, only the experimental results for negative samples are consid-
ered in evaluation, as given in Table 1. For each method, the number of features is set 
by the highest point in the curve of the classifier performance with respect to the 
number of features (For the limitation of space, we omit all the curves here). The 
numbers of features set in five methods are 4000, 500, 15000, 800 and 500+3000 (the 
first step + the second step) respectively. 
Table 1.  Performance comparisons of the five methods in two-class TC 
 
Comparing Method-1 and Method-2, we can see that feature reduction formula (8) 
is superior to (9). Moreover, the number of features determined in the former is less 
than that in the latter (500 vs. 4000). Comparing Method-2, Method-3 and Method-4, 
we can see that Chinese word bigrams as features have better discriminating capabil-
ity meanwhile with more serious data sparseness: the performances of Method-3 and 
Method-4 are higher than that of Method-2, but the number of features used in 
Method-3 is more than those used in Method-2 and Method-4 (15000 vs. 500 and 
800). Table 1 shows that the proposed method (Methond-5) has the best performance 
(95.54% F1) and good efficiency. It integrates the merit of words and word bigrams. 
Using words as features in the first step aims at its better statistical coverage, -- the 
500 selected features in the first step can treat a majority of documents, constituting 
63.13% of the test set. On the other hand, using word bigrams as features in the sec-
ond step aims at its better discriminating capability, although the number of features 
becomes comparatively large (3000). Comparing Method-5 with Method-2, Method-3 
and Method-4, we find that the two-step approach is superior to either using only one 
kind of features (word or word bigram) in the classifier, or using the mixture of two 
kinds of features in one step. 
3   Extending the Two-Step Approach to the Multi-class TC 
We extend the two-step method presented in Section 2 to handle the multi-class TC 
now. The idea is to transfer the multi-class TC to the two-class TC. Similar to two-
class TC, the emphasis is still on the misclassified documents given by a classifier, 
though we use a modified multi-class Na?ve Bayesian Classifier here. 
 Classifying Chinese Texts in Two Steps 307 
3.1   Fix the Fuzzy Area Between Categories by the Multi-class Bayesian      
Classifier 
For a document represented by a binary-valued vector d = (W1, W2, ?, W|D|), the 
multi-class Na?ve Bayesian Classifier can be re-written as: 
??
==?
? ++=
||
1
||
1
)
1
log)1(log}{Prlog(maxarg
D
k ki
ki
k
D
k
kii
Cc -p
p
 W-p cc
i
 (10) 
where Pr{
?
} is the probability that event {
?
} occurs, pki=Pr{Wk=1|ci}, (i=1,2, ?, |C|), C is the number of predefined categories. Let: 
 ??
==
++=
||
1
||
1 1
log)1(log}{Prlog
D
k ki
ki
k
D
k
kiii
-p
p
 W-p cMV  (11) 
         )(maximummax_ iCcF MVMV i?=  (12) 
         
Cc
iS
i
MVMV
?
= )imum(second_maxmax_  (13) 
where MVi stands for the likelihood of assigning a label ci?C to the document d,  
MVmax_F and MVmax_S are the maximum and the second maximum over all MVi 
(i?|C|) respectively. We approximately rewrite (10) as: 
SF MVMVdf max_max_)( ?=           (14) 
We try to transfer the multi-class TC described by (10) into a two-class TC de-
scribed by (14). Formula (14) means that the binary-valued multi-class Na?ve Bayes-
ian Classifier can be approximately regarded as searching a separate line in a two-
dimensional space with MVmax_F being the X-coordinate and MVmax_S being the Y-
coordinate. The distance from a given document, represented as a point (x, y) with the 
values of x and y calculated according to (12) and (13) respectively, to the separate 
line in this two-dimensional space will be:  
      y)(xDist ?=
2
1
    (15) 
The value of Dist directly reflects the degree of confidence of assigning the label c* 
to the document d. 
The distribution of a training set (refer to Section 3.2) regarding Dist in this two-
dimensional space, and, consequently, the fuzzy area for the Na?ve Bayesian Classi-
fier, are observed and identified, similar to its counterpart in Section 2.2.  
3.2   Experiments on the Multi-class TC 
We construct a dataset, including 5 categories and the total of 17756 Chinese docu-
ments. The document numbers of five categories are 4192, 6968, 2080, 3175 and 
308 X. Fan et al 
1800 respectively, among which the last three categories have the high degree of 
ambiguity each other. The dataset is split into four parts randomly, one as the test set 
and the other three as the training set. We again run the five methods described in 
Section 2.2 on this dataset. The strategy of determining the number of features also 
follows that used in Section 2.2. The experimentally determined numbers of features 
regarding the five methods are 8000, 400, 5000, 800 and 400 + 9000 (the first step +  
the second step) respectively. 
The average precision, average recall and average F1 over the five categories are 
used to evaluate the experimental results, as shown in Table 2. 
Table 2.  Performance comparisons of the five methods in multi-class TC 
 
We can see from Table 2 that the very similar conclusions as that in the two-class 
TC in Section 2.2 can be obtained here: 
1) Formula (8) is superior to (9) in feature reduction. This comes from the per-
formance comparison between Method-2 and Method-1: the former has higher per-
formance and higher efficiency that the latter (the average F1, 97.20% vs. 91.48%, and 
the number of features used, 400 vs. 8000). 
2) Word bigrams as features have better discriminating capability than words as 
features, along with more serious data sparseness. The performances of Method-3 and 
Method-4, which use Chinese word bigrams and the mixture of words and word bi-
grams as features respectively, are higher than that of Method-2, which only uses 
Chinese words as features. But the number of features used in Method-3 is much 
more than those used in Method-2 and Method-4 (5000 vs. 400 and 800). 
3) The proposed method (Methond-5) has the best performances and acceptable ef-
ficiency. In term of the average F1, the performance is improved from the baseline 
91.48% (Method-1) to 98.56% (Method-5). In the first step in Method-5, the number 
of feature set is small (only 400), but a majority of documents can be treated by it. 
The number of features exploited in Method-5 is the highest among the five methods 
(9000), but it is still acceptable. 
4   Using Dependences Among Features in Two-Step 
Categorization  
In this section, a two-step text categorization method taking the dependences among 
features into account is presented. We do the same task with the Na?ve Bayesian Clas-
sifier in the first step, exactly same as what we did in Section 2 and Section 3. In the 
 Classifying Chinese Texts in Two Steps 309 
second step, each document identified unreliable in the first step are further processed 
by exploring the dependences among features. This is realized by a model named the 
Causality Na?ve Bayesian Classifier. 
4.1   The Causality Na?ve Bayesian Classifier (CNB) 
The Causality Na?ve Bayesian Classifier (CNB) is an improved Na?ve Bayesian Clas-
sifier. It contains two additional parts, i.e., the k-dependence feature list and the fea-
ture causality diagram. The former is used to represent the dependence relation among 
features, and the latter is used to estimate the probability distribution of a feature 
dynamically while taking its dependences into account.  
K-Dependence Feature List (K-DFL): CNB allows each feature node Y to have a 
maximum of k features nodes as parents that constitute the k-dependence feature list 
representing the dependences among features. In other words, ?(Y) = {Yd, C}, where 
Yd is the set of at most k features nodes, C is the category node, and ?(C) =?. 
Note that we can build a K-DFL for each feature under each class ct, which repre-
sents different dependence relations under different class.  
Obviously, there exists a 0-dependence feature list for every feature in the Na?ve 
Bayesian Classifier, from the definition of K-DFL. 
The algorithm of constructing K-DFL is as follows: Given the maximum depend-
ence number k, mutual information threshold ? and the class ct. For each feature Y, 
repeat the follow steps. 1) Compute class conditional mutual information MI(Yi, Yj| 
ct), for every pair of features Yi and Yj, where i?j. 2) Construct the set Si={ Yj | 
MI(Yi, Yj| ct) > ?}. 3) Let m= min (k, | Si|), select the top m features as K-DFL  
from Si. 
Feature Causality Diagram (FCD): CNB allows each feature Y, which occurs in a 
given document, to have a Feature Causality Diagram (FCD). FCD is a double-layer 
directed diagram, in which the first layer has only the feature node Y, and the second 
layer allows to have multiple nodes that include the class node C and the correspond-
ing dependence node set S of Y. Here, S=Sd?SF, Sd is the K-DFL node set of Y and 
SF={Xi| Xi is a feature node that occurs in the given document. There exists a directed 
arc from every node Xi at the second layer to the node Y at the first layer. The arc is 
called causality link event Li which represents the causality intensity between node Y 
and Xi, and the probability of Li is pi=Pr{Li}=Pr{Y=1|Xi=1}. The relation among all 
arcs is logical OR. The Feature Causality Diagram can be considered as a sort of 
simplified causality diagram [9][10]. 
Suppose feature Y?s FCD is G, and it parent node set S={X1, X2,?,Xm } (m?1) in 
G, we can estimate the conditional probability as follows while considering the de-
pendences among features: 
? ?
=
?
==
?+===?===
m
i
i
j
ji
i
i ppp
2
1
1
1
m
1
m1 )1(}LPr{ G}|1Pr{Y1}X,1,X|1Pr{Y UL  (16) 
Note that when m=1, C}|1Pr{YG}|1Pr{Y1}X|1Pr{Y 1 ====== . 
310 X. Fan et al 
Causality Na?ve Bayesian Classifier (CNB): For a document represented by a bi-
nary-valued vector d=(X1 ,X2 , ?,X|d|), divide the features into two sets X1 and X2, 
X1= {Xi| Xi=1} and X2= {Xj| Xj=0}. The Causality Na?ve Bayesian Classifier can be 
written as: 
}))c|{XPrlog(1}G|logPr{X}(logPr{cmax argc*
||
1
||
1
tj
ji
iit
Cct
??
==?
?++=
21 XX
 (17) 
4.2   Experiments on CNB 
As mentioned earlier, the first step remains unchanged as that in Section 2 and Sec-
tion 3. The difference is in the second step: for the documents identified unreliable in 
the first step, we apply the Causality Na?ve Bayesian Classifier to handle them. 
We use two datasets in the experiments. one is the two-class dataset described in 
Section 2.2, called Dataset-I, and the other one is the multi-class dataset described in 
Section 3.2, called Dataset-I. 
To evaluate CNB and compare all methods presented in this paper, we experiment 
the following methods:  
1) Na?ve Bayesian Classifier (NB), i.e., the method-2 in Section 2.2;  
2) CNB without exploring the two-step strategy;  
3) The two-step strategy: NB and CNB in the first and second step (TS-CNB);  
4) Limited Dependence Bayesian Classifier (DNB) [11];  
5) Method-5 in Section 2.2 and Section 3.2 (denoted TS-DF here).  
Experimental results for two-class Dataset-I and multi-class Dataset-II are listed in 
Table3 and Table 4. The data for NB and TS-DF are derived from the corresponding 
columns of Table 1 and Table 2. The parameters in CNB and TS-CNB are that the 
dependence number k=1 and 5, the threshold?= 0.0545 and 0.0045 for Dataset-I and 
Dataset-II respectively. The parameters in DNB are that dependence number k=1and 
3, the threshold?= 0.0545 and 0.0045 for Dataset-I and Dataset-II respectively. 
Table 3.  Performance comparisons in two-class Dataset-I 
 
Table 3 and Table 4 demonstrate that 1) The  performance of the Na?ve Bayesian 
Classifier can be improved by taking the dependences among features into account, as 
evidenced by the fact that CNB, TS-CNB and DNB outperform NB. By tracing the 
experiment, we find an interesting phenomenon, as expected: for the documents  
 Classifying Chinese Texts in Two Steps 311 
identified reliable by NB, CNB cannot improve it, but for those identified unreliable 
by NB, CNB can improve it. The reason should be even though NB and CNB use the 
same features, but CNB uses the dependences among features additionally. 2) CNB 
and TS-CNB have the same capability in effectiveness, but TS-CNB has a higher 
computational efficiency. As stated earlier, TS-CNB uses NB to classify documents in 
the reliable area and then uses CNB to classify documents in the unreliable area. At 
the first glance, the efficiency of TS-CNB seems lower than that of using CNB only 
because the former additionally uses NB in the first step, but in fact, a majority of 
documents (e.g., 63.13% of the total documents in dataset-I) fall into the reliable area 
and are then treated by NB successfully (obviously, NB is higher than CNB in effi-
ciency) in the first step, so they will never go to the second step, resulting in a higher 
computational efficiency of TS-CNB than CNB. 3) The performances of CNB, TS-
CNB and DNB are almost identical, among which, the efficiency of TS-CNB is the 
highest. And, the efficiency of CNB is higher than that of DNB, because CNB uses a 
simpler network structure than DNB, with the same learning and inference formalism. 
4) TS-DF has the highest performance among the all. Meanwhile, the ranking of 
computational efficiency (in descending order) is NB, TS-DF, TS-CNB, CNB,  
and DNB.  
Table 4.  Performance comparisons in multi-class Dataset-II 
 
5   Related Works 
Combining multiple methodologies or representations has been studied in several 
areas of information retrieval so far, for example, retrieval effectiveness can be im-
proved by using multiple representations [12]. In the area of text categorization in 
particular, many methods of combining different classifiers have been developed. For 
example, Yang et al [13] used simple equal weights for normalized score of each 
classifier output so as to integrate multiple classifiers linearly in the domain of Topic 
Detection and Tracking; Hull at al. [14] used linear combination for probabilities or 
log odds scores of multiple classifier output in the context of document filtering. Lar-
key et al [15] used weighted linear combination for system ranks and scores of multi-
ple classifier output in the medical document domain; Li and Jain [16] used voting 
and classifier selection technique including dynamic classifier selection and adaptive 
classifier. Lam and Lai [17] automatically selected a classifier for each category based 
on the category-specific statistical characteristics. Bennett et al [18] used voting, 
classifier-selection techniques and a hierarchical combination method with  
reliability indicators. 
312 X. Fan et al 
6   Conclusions 
The issue of how to classify Chinese documents characterized by high degree ambi-
guity from text categorization?s point of view is a challenge. For this issue, this paper 
presents two solutions in a uniform two-step framework, which makes use of the 
distributional characteristics of misclassified documents, that is, most of the misclas-
sified documents are near to the separate line between categories. The first solution is 
a two-step TC approach based on the Na?ve Bayesian Classifier. The second solution 
is to further introduce the dependences among features into the model, resulting in a 
two-step approach based on the so-called Causality Na?ve Bayesian Classifier. Ex-
periments show that the second solution is superior to the Na?ve Bayesian Classifier, 
and is equal to CNB without exploring two-step strategy in performance, but has a 
higher computational efficiency than the latter. The first solution has the best per-
formance in all the experiments, outperforming all other methods (including the sec-
ond solution): in the two-class experiments, its F1 increases from the baseline 82.67% 
to the final 95.54%, and in the multi-class experiments, its average F1 increases from 
the baseline 91.48% to the final 98.56%. 
In addition, the other two conclusions can be drawn from the experiments: 1) Us-
ing Chinese word bigrams as features has a better discriminating capability than using 
words as features, but more serious data sparseness will be faced; 2) formula (8) is 
superior to (9) in feature reduction in both the two-class and multi-class Chinese text 
categorization. 
It is worth point out that we believe the proposed method is in principle language 
independent, though all the experiments are performed on Chinese datasets. 
Acknowledgements 
The research is supported in part by the National 863 Project of China under grant 
number 2001AA114210-03, 2003 Korea-China Young Scientists Exchange Program, 
the Tsinghua-ALVIS Project co-sponsored by the National Natural Science Founda-
tion of China under grant number 60520130299 and EU FP6, and the National Natu-
ral Science Foundation of China under grant number 60321002.  
References 
1. Sebastiani, F. Machine Learning in Automated Text Categorization. ACM Computing Sur-
veys, 34(1):1-47, 2002. 
2. Lewis, D. Naive Bayes at Forty: The Independence Assumption in Information Retrieval. 
In Proceedings of ECML-98, 4-15, 1998. 
3. Salton, G. Automatic Text Processing: The Transformation, Analysis, and Retrieval of In-
formation by Computer. Addison-Wesley, Reading, MA, 1989. 
4. Mitchell, T.M. Machine Learning. McCraw Hill, New York, NY, 1996. 
5. Yang, Y., and Liu, X. A Re-examination of Text Categorization Methods. In Proceedings 
of SIGIR-99, 42-49,1999. 
6. Xinghua Fan. Causality Reasoning and Text Categorization, Postdoctoral Research Report 
of Tsinghua University, P.R. China, April 2004. (In Chinese) 
 Classifying Chinese Texts in Two Steps 313 
7. Dumais, S.T., Platt, J., Hecherman, D., and Sahami, M. Inductive Learning Algorithms 
and Representation for Text Categorization. In Proceedings of CIKM-98, Bethesda, MD, 
148-155, 1998. 
8. Sahami, M., Dumais, S., Hecherman, D., and Horvitz, E. A. Bayesian Approach to Filter-
ing Junk E-Mail. In Learning for Text Categorization: Papers from the AAAI Workshop, 
55-62, Madison Wisconsin. AAAI Technical Report WS-98-05, 1998. 
9. Xinghua Fan. Causality Diagram Theory Research and Applying It to Fault Diagnosis of 
Complexity System, Ph.D. Dissertation of Chongqing University, P.R. China, April 2002. 
(In Chinese) 
10. Xinghua Fan, Zhang Qin, Sun Maosong, and Huang Xiyue. Reasoning Algorithm in 
Multi-Valued Causality Diagram, Chinese Journal of Computers, 26(3), 310-322, 2003. 
(In Chinese) 
11. Sahami, M. Learning Limited Dependence Bayesian Classifiers. In Proceedings of the 
Second International Conference on Knowledge Discovery and Data Mining, Portland, 
335-338, 1996. 
12. Rajashekar, T. B. and Croft, W. B. Combining Automatic and Manual Index Representa-
tions in Probabilistic Retrieval. Journal of the American society for information science, 
6(4): 272-283,1995. 
13. Yang, Y., Ault, T. and Pierce, T. Combining Multiple Learning Strategies for Effective 
Cross Validation. In Proceedings of  ICML 2000, 1167?1174, 2000. 
14. Hull, D. A., Pedersen, J. O. and H. Schutze. Method Combination for Document Filtering. 
In Proceedings of SIGIR-96, 279?287, 1996. 
15. Larkey, L. S. and Croft, W. B. Combining Classifiers in Text Categorization. In Proceed-
ings of SIGIR-96, 289-297, 1996. 
16. Li, Y. H., and Jain, A. K. Classification of Text Documents. The Computer Journal, 41(8): 
537-546, 1998. 
17. Lam, W., and Lai, K.Y. A Meta-learning Approach for Text Categorization. In Proceed-
ings of SIGIR-2001, 303-309, 2001. 
18. Bennett, P. N., Dumais, S. T., and Horvitz, E. Probabilistic Combination of Text Classifi-
ers Using Reliability Indicators: Models and Results. In Proceedings of SIGIR-2002, 11-
15, 2002. 
