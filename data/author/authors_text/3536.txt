Unsupervised Discovery of Scenario-Level Patterns for 
Information Extraction 
Roman Yangarber  Ra lph  Gr i shman 
roman?as, nyu. edu grishman?cs, nyu. edu 
Courant Inst i tute of Courant Inst i tute of 
Mathematical  Sciences Mathematical  Sciences 
New York University New York University 
Pas i  Tapana inen  )~ Si l ja  Hut tunen $ 
tapanain?conexor, fi sihuttun~ling.helsinki, fi 
t Conexor Oy :~ University of Helsinki 
Helsinki, F inland F in land 
Abst rac t  
Information Extraction (IE) systems are com- 
monly based on pattern matching. Adapting 
an IE system to a new scenario entails the 
construction of a new pattern base---a time- 
consuming and expensive process. We have 
implemented a system for finding patterns au- 
tomatically from un-annotated text. Starting 
with a small initial set of seed patterns proposed 
by the user, the system applies an incremental 
discovery procedure to identify new patterns. 
We present experiments with evaluations which 
show that the resulting patterns exhibit high 
precision and recall. 
0 I n t roduct ion  
The task of Information Extraction (I-E) is 
the selective extraction of meaning from free 
natural language text. I "Meaning" is under- 
stood here in terms of a fixed set of semantic 
objects--entities, relationships among entities, 
and events in which entities participate. The 
semantic objects belong to a small number of 
types, all having fixed regular structure, within 
a fixed and closely circumscribed subject do- 
main. The extracted objects are then stored in 
a relational database. In this paper, we use the 
nomenclature accepted in current IE literature; 
the term subject domain denotes a class of tex- 
tual documents to be processed, e.g., "business 
news," and scenario denotes the specific topic 
of interest within the domain, i.e., the set of 
facts to be extracted. One example of a sce- 
nario is "management succession," the topic of 
MUC-6 (the Sixth Message Understanding Con- 
ference); in this scenario the system seeks to 
identify events in which corporate managers left 
1For general references on IE, cf., e.g., (Pazienza, 
1997; muc, 1995; muc, 1993). 
their posts or assumed new ones. We will con- 
sider this scenario in detail in a later section 
describing experiments. 
IE systems today are commonly based on pat- 
tern matching. The patterns are regular ex- 
pressions, stored in a "pattern base" containing 
a general-purpose component and a substantial 
domain- and scenario-specific component. 
Portability and performance are two major 
problem areas which are recognized as imped- 
ing widespread use of IE. This paper presents a
novel approach, which addresses both of these 
problems by automatically discovering good 
patterns for a new scenario. The viability of 
our approach is tested and evaluated with an 
actual IE system. 
In the next section we describe the problem in 
more detail in the context of our IE system; sec- 
tions 2 and 3 describe our algorithm for pattern 
discovery; section 4 describes our experimental 
results, followed by comparison with prior work 
and discussion, in section 5. 
1 The  IE  Sys tem 
Our IE system, among others, contains a a back- 
end core engine, at the heart of which is a 
regular-e~xpression pattern matcher. The engine 
draws on attendant knowledge bases (KBs) of 
varying degrees of domain-specificity. The KB 
components are commonly factored out to make 
the systems portable to new scenarios. There 
are four customizable knowledge bases in our IE 
system: the Lexicon contains general dictionar- 
ies and scenario-specific terms; the concept base 
groups terms into classes; the predicate base de- 
scribes the logical structure of events to be ex- 
tracted, and the pattern base contains patterns 
that catch the events in text. 
Each KB has a. substantial domain-specific 
component, which must be modified when mov-  
282 
ing to new domains and scenarios. The system 
allows the user (i.e. scenario developer) to start 
with example sentences in text which contain 
events of interest, the candidates, and general- 
ize them into patterns. However, the user is 
ultimately responsible for finding all the can- 
didates, which amounts to manually processing 
example sentences in a very large training cor- 
pus. Should s/he fail to provide an example 
of a particular class of syntactic/semantic con- 
struction, the system has no hope of recovering 
the corresponding events. Our experience has 
shown that (1) the process of discovering candi- 
dates is highly expensive, and (2) gaps in pat- 
terns directly translate into gaps in coverage. 
How can the system help automate the pro- 
cess of discovering new good candidates? The 
system should find examples of all common lin- 
guistic constructs relevant o a scenario. While 
there has been prior research on identifying the 
primary lexical patterns of a sub-language or 
corpus (Grishman et al, 1986; Riloff, 1996), the 
task here is more complex, since we are typi- 
cally not provided in advance with a sub-corpus 
of relevant passages; these passages must them- 
selves be found as part of the discovery process. 
The difficulty is that one of the best indications 
of the relevance of the passages i precisely the 
presence of these constructs. Because of this 
circularity, we propose to acquire the constructs 
and passages in tandem. 
2 So lu t ion  
We outline our procedure for automatic ac- 
quisition of patterns; details are elaborated in 
later sections. The procedure is unsupervised 
in that it does not require the training corpus 
to be manually annotated with events of inter- 
est, nor a pro-classified corpus with relevance 
judgements, nor any feedback or intervention 
from the user 2. The idea is to combine IR-style 
document selection with an iterative relaxation 
process; this is similar to techniques used else- 
where in NLP, and is inspired in large part, if 
remotely, by the work of (Kay and RSscheisen, 
1993) on automatic alignment of sentences and 
words in a bilingual corpus. There, the reason- 
ing was: sentences that are translations of each 
2however, it may be supervised after each iteration, 
where the user can answer yes/no questions to improve 
the quality of the results 
other are good indicators that words they con- 
tain are translation pairs; conversely, words that 
are translation pairs indicate that the sentences 
which contain them correspond to one another. 
In our context, we observe that documents 
that are relevant to the scenario will neces- 
sarily contain good patterns; conversely, good 
patterns are strong indicators of relevant docu- 
ments. The outline of our approach is as follows. 
. 
. 
Given: (1) a large corpus of un-annotated 
and un-classified documents in the domain; 
(2) an initial set of trusted scenario pat- 
terns, as chosen ad hoc by the user--the 
seed; as will be seen, the seed can be quite 
small--two or three patterns eem to suf- 
fice. (3) an initial (possibly empty) set of 
concept classes 
The pattern set induces a binary partition 
(a split) on the corpus: on any document, 
either zero or more than zero patterns will 
match. Thus the universe of documents, U, 
is partitioned into the relevant sub-corpus, 
R, vs. the non-relevant sub-corpus, R = 
U - R, with respect o the given pattern 
set. Actually, the documents are assigned 
weights which are 1 for documents matched 
by the trusted seed, and 0 otherwise. 3 
2. Search for new candidate patterns: 
(a) Automatically convert each sentence 
in the corpus,into a set of candidate 
patterns, 4 
(b) Generalize each pattern by replacing 
each lexical item which is a member of 
a concept class by the class name. 
(c) Working from the relevant documents, 
select those patterns whose distribu- 
tion is strongly correlated with other 
relevant documents (i.e., much more 
3R represents he trusted truth through the discovery 
iterations, since it was induced by the manually-selected 
seed. 
4Here, for each clause in the sentence we extract a 
tuple of its major roles: the head of the subject, the 
verb group, the object, object complement, asdescribed 
below. This tuple is considered to be a pattern for the 
present purposes of discovery; it is a skeleton for the 
rich, syntactically transformed patterns our system uses 
in the extraction phase. 
283 
densely distributed among the rele- 
vant documents than among the non- 
relevant ones). The idea is to consider 
those candidate patterns, p, which 
meet the density, criterion: 
IHnRI IRI - - > >  
IHnUI IUI 
where H = H(p) is the set of docu- 
ments where p hits. 
(d) Based on co-occurrence with the cho- 
sen patterns, extend the concept 
classes. 
3. Optional: Present he new candidates and 
classes to the user for review, retaining 
those relevant o the scenario. 
4. The new pattern set induces a new parti- 
tion on the corpus. With this pattern set, 
return to step 1. Repeat he procedure un- 
til no more patterns can be added. 
3 Methodo logy  
3.1 Pre-proeess ing:  Normal i za t ion  
Before applying the discovery procedure, we 
subject the corpus to several stages o f  pre- 
processing. First, we apply a name recognition 
module, and replace each name with a token 
describing its class, e.g. C-Person, C-Company, 
etc. We collapse together all numeric expres- 
sions, currency values, dates, etc., using a single 
token to designate ach of these classes. 
3.2 Syntact ic  Analys is  
We then apply a parser to perform syntactic 
normalization to transform each clause into a 
common predicate-argument structure. We use 
the general-purpose d pendency parser of En- 
glish, based on the FDG formalism (Tapanainen 
and J~rvinen, 1997) and developed by the Re- 
search Unit for Multilingual Language Technol- 
ogy at the University of Helsinki, and Conexor 
Oy. The parser (modified to understand the 
name labels attached in the previous step) is 
used for reducing such variants as passive and 
relative clauses to a tuple, consisting of several 
elements. 
1. For each claus, the first element is the sub- 
ject, a "semantic" subject of a non-finite 
sentence or agent of the passive. 5 
2. The second element is the verb. 
3. The third element is the object, certain 
object-like adverbs, subject of the passive 
or subject complement 6 
4. The fourth element is a phrase which 
refers to the object or the subject. A 
typical example of such an argument is 
an object complement, such as Com- 
pany named John Smith pres ident .  An- 
other instance is the so-called copredica- 
tire (Nichols, 1978), in the parsing system 
(J~irvinen and Tapanainen, 1997). A co- 
predicative refers to a subject or an object, 
though this distinction is typically difficult 
to resolve automatically/ 
Clausal tuples also contain a locative modifier, 
and a temporal modifier. We used a corpus of 
5,963 articles from the Wall Street Journal, ran- 
domly chosen. The parsed articles yielded a to- 
tal of 250,000 clausal tuples, of which 135,000 
were distinct. 
3.3 Genera l i za t ion  and  Concept  Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pairs: e.g., a verb- 
object pair, a subject-object pair, etc. Each 
pair is used as a generalized pattern during 
the candidate selection stage. Once we have 
identified pairs which are relevant o the sce- 
nario, we use them to construct or augment con- 
cept classes, by grouping together the missing 
roles, (for example, a class of verbs which oc- 
cur with a relevant subject-object pair: "com- 
pany (hire/fire/expel...} person"). This is sim- 
ilar to work by several other groups which 
aims to induce semantic lasses through syn- 
tactic co-occurrence analysis (Riloff and Jones, 
1999; Pereira et al, 1993; Dagan et al, 1993; 
Hirschman et al, 1975), although in .our case 
the contexts are limited to selected patterns, 
relevant o the scenario. 
SE.g., " John sleeps", "John is appointed by 
Company" ,  "I saw a dog which sleeps", "She asked 
John  to buy a car". 
6E.g., " John is appointed by Company", "John is the 
pres ident  of Company", "I saw a dog which sleeps", 
The dog  which I saw sleeps. 
7For example, "She gave us our coffee black",  "Com- 
pany appointed John Smith as pres ident" .  
284 
3.4 Pattern Discovery 
Here we present he results from experiments 
we conducted on the MUC-6 scenario, "man- 
agement succession". The discovery procedure 
was seeded with a small pattern set, namely: 
Subject Verb Direct Object 
C-Company C-Appoint C-Person 
C-Person C-Resign 
Documents are assigned relevance scores on 
a scale between 0 and 1. The seed patterns 
are accepted as ground truth; thus the docu- 
ments they match have relevance 1. On sub- 
sequent iterations, the newly accepted patterns 
are not trusted as absolutely. On iteration um- 
ber i q- 1, each pattern p is assigned a precision 
measure, based on the relevance of the docu- 
ments it matches: 
Here C-Company and C-Person denote se- 
mantic classes containing named entities of the 
corresponding semantic types. C-Appoirlt de- 
notes a class of verbs, containing four verbs 
{ appoint, elect, promote, name}; C-Resign = 
{ resign, depart, quit, step-down }. 
During a single iteration, we compute the 
score s, L(p), for each candidate pattern p: 
L(p) = Pc(P)" log {H A R\] (1) 
where R denotes the relevant subset, and H -- 
H(p) the documents matching p, as above, and 
\[gnR\[ Pc(P) -- Igl is the conditional probability of 
relevance. We further impose two support cri- 
teria: we distrust such frequent patterns where 
\[HA U{ > a\[U\[ as uninformative, and rare pat- 
terns for which \[H A R\[ </3  as noise. ? At the 
end of each iteration, the system selects the pat- 
tern with the highest score, L(p), and adds it to 
the seed set. The documents which the winning 
pattern hits are added to the relevant set. The 
pattern search is then restarted. 
3.5 Re-computat lon  of  Document  
Relevance 
The above is a simplification of the actual pro- 
cedure, in several important respects. 
Only generalized patterns are considered for 
candidacy, with one or more slots filled with 
wild-cards. In computing the score of the gen- 
eralized pattern, we do not take into considera- 
tion all possible values of the wild-card role. We 
instead constrain the wild-card to those values 
which themselves in turn produce patterns with 
high scores. These values then become members 
of a new class, which is output in tandem with 
the winning pattern 1? 
Ssimilarly to (Riloff, 1996) 
?U denotes the universe of documents. We used c~ = 
0.i and ~----- 2. 
1?The classes are currently unused by subsequent i er- 
ations; this important issue is considered in future work. 
Preci+l(p) = 1 {H(p){ ~ Reli(d) (2) 
dEH(p) 
where Reli(d) is the relevance of the document 
from the previous iteration, and H(p) is the set 
of documents where p matched. More generally, 
if K is a classifier consisting of a set of patterns, 
we can define H(K) as the set of documents 
where all of patterns p E K match, and the 
"cumulative" precision 11 of K as 
Preci+l(K) = 1 ~ Reli(d) (3) 
IH(K)\[ riCH(K) 
Once the new winning pattern is accepted, 
the relevance scores of the documents are re- 
adjusted as follows. For each document d which 
is matched by some (non-empty) subset of the 
currently accepted patterns, we can view that 
subset of patterns as a classifier K d = {py}. 
These patterns determine the new relevance 
score of the document 
Reli+l(d) = max (Rel~(d),Prec~+l(Kd)) (4) 
This ensures that the relevance score grows 
monotonically, and only when there is sufficient 
positive evidence, as the patterns in effect vote 
"conjunctively" on the documents. The results 
which follow use this measure. 
Thus in the formulas above, R is not sim- 
ply the count of the relevant documents, but 
is rather their cumulative relevance. The two 
formulas, (3) and (4), capture the mutual de- 
pendency of patterns and documents; this re- 
computation and growing of precision and rele- 
vance scores is at the heart of the procedure. 
11Of course, this measure is defined only when 
H(K) # 0. 
285 
4 Resu l ts  1 
An objective measure of goodness of a pattern o. 9 
is not trivial to establish since the patterns can- 
not be used for extraction directly, without be- o. s 
ing properly incorporated into the knowledge 
base. Thus, the discovery procedure does not o. v 
lend itself easily to MUC-style evaluations, ince 
0.6  a pattern lacks information about which events 
it induces and which slots its arguments should 0.5  
fill. 
However, it is possible to apply some objec- o. a 
tive measures of performance. One way we eval- 
uated the system is by noting that in addition o.  
to growing the pattern set, the procedure also 
grows the relevance of documents. The latter o. 2 
can be objectively evaluated. 
0.1  
We used a test corpus of 100 MUC-6 formal- 
training documents (which were included in the o 
main development corpus of about 6000 docu- 
ments) plus another 150 documents picked at 
random from the main corpus and judged by 
hand. These judgements constituted the ground 
truth and were used only for evaluation, (not in 
the discovery procedure). 
4.1 Text  F i l ter ing 
Figure 1 shows the recall/precision measures 
with respect to the test corpus of 250 docu- 
ments, over a span of 60 generations, tarting 
with the seed set in table 3.4. The Seed pat- 
terns matched 184 of the 5963 documents, yield- 
ing an initial recall of .11 and precision of .93; 
by the last generation it searched through 982 
documents with non-zero relevance, and ended 
with .80 precision and .78 recall. This facet of 
the discovery procedure is closely related to the 
MUC '%ext-filtering" sub-task, where the sys- 
tems are judged at the level of documents rather 
than event slots. It is interesting to compare the 
results with other MUC-6 participants, shown 
anonymously in figure 2. Considering recall and 
precision separately, the discovery procedure at- 
tains values comparable to those achieved by 
some of the participants, all of which were ei- 
ther heavily-supervised or manually coded sys- 
tems. It is important o bear in mind that the 
discovery procedure had no benefit of training 
material, or any information beyond the seed 
pattern set. 
I I I I I I 
......i"{+X'N+~v i P ~ e c i s i o n  ' i 
" "  ... .  i . .~ .  i Re~aa - - -?- - -  
. . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  ~ . . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  
...... iiiiiiiiiiiiiilEi   ........... ........ 
/... 
. . . . . . . . . .  ~ . . . . . . . . . .  '." . . . . . . . .  ": . . . . . . . . . .  ~ . . . . . . . . . . .  r . . . . . . . . . .  ~ .. . . . . . . . .  ! . . . . . . . . .  
2111111ji.. iii121122;1211111;ii122221ilSiiii12112121SiiiiSiii: . . . . . .
0 I0  20  30  40  50  60  70  
G e n e r a t i o n  # 
80 
Figure h Recall/Precision curves for Manage- 
ment Succession 
4.2 Cho ice  of  Test  Corpus  
Figure 2 shows two evaluations of our discovery 
procedure, tested against the original MUC-6 
corpus of 100 documents, and against our test 
corpus, which consists of an additional 150 doc- 
uments judged manually. The two plots in the 
figure show a slight difference in results, indi- 
cating that in some sense, the MUC corpus was 
more "random", or that our expanded corpus 
was somewhat skewed in favor of more common 
patterns that the system is able to find more 
easily. 
4.3 Cho ice  of  Eva luat ion  Met r i c  
The graphs shown in Figures 1 and 2 are based 
on an "objective" measure we adopted during 
the experiments. This is the same measure of 
relevance used internally by the discovery proce- 
dure on each iteration (relative to the "truth" of 
relevance scores of the previous iteration), and 
is not quite the standard measure used for text 
filtering in IR. According to this measure, the 
system gets a score for each document based on 
the relevance which it assigned to the document. 
Thus if the system .assigned relevance of X per- 
cent to a relevant document, it only received X 
286 
0 
( I )  
0 . 9  
0 . 8  
0 . 7  
I I  I I I I I I : : . , . : ? 
. . . . . . .  . . . . . . . .  . . . . . . . . . . . . . . .  - - -  ! . . . . . . . .  . . . . . . .  
i i i :: i i i i i ~ 
. . . . . .  i . . . . . . . .  i . . . . . . . .  \[ . . . . . . .  ? .. . . . . . .  f . . . . . . . . . . . .  T . . . . . . .  
. . . . . .  J . . . . . . . .  i . . . . . . . .  i .  . . . . . . .  i . . . . . . . .  ~ . . . . . . .  .; . . . . . . . . . . .  ..: . . . . . . . .  i . . . . :  
0 6 . . . . . . . . . . . . . . . . . . . . . . . .  .'7 . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ~ . . . . . . . . . . . . . . . . . . . . . . .  
i 
i 
i 
0 . 5 . . . . . . .  '. . . . . . . .  , . . . . . . . .  ~" . . . . . . .  , . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  * . . . . . . .  "=. . . . . . . .  , . . . . . . .  
i z 
0 . 4  i I 
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 2: Precision vs. Recall 
I 
i . i ~ iB  i 
. . . . . .  e . . . . . . . .  i . . . . . . . .  ! . . . . . . . .  ~ . . . . . . . .  ~ . . .  ' . . . . . . . .  ! . . . . . . . .  i . . . . . . .  0 . 9  
i i i i i im~ ! ! 
! i i D iE  c 
! i i i i i i i i 
0 . '7  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~" . . . . . . . . . . . . . . .  
C o n ~ i n ~ o u -  ? ~ut i - -o  f 
0 . 6 . . . . . .  ~ . . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ; . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  
0 . 5 ...... ~ ........ , ........ ~ ....... ~ ........ ~ ....... ~ ........ ! ........ : ........ 4 ....... 
0 . 4  
0 0 . i 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 3: Results on the MUC corpus 
percent on the recall score for classifying that 
document correctly. Similarly, if the system as- 
signed relevance Y to an irrelevant document, 
it was penalized only for the mis-classified Y
percent on the precision score. To make our re- 
sults more comparable to those of other MUC 
competitors, we chose a cut-off point and force 
the system to make a binary relevance decision 
on each document. The cut-off of 0.5 seemed 
optimal from empirical observations. Figure 3 
shows a noticeable improvement in scores, when 
using our continuous, "objective" measure, vs. 
the cut-off measure, with the entire graph essen- 
tially translated to the right for a gain of almost 
10 percentage points of recall. 
4.4 Eva luat ing  Pat terns  
Another effective, if simple, measure of perfor- 
mance is  how many of the patterns the pro- 
cedure found, and comparing them with those 
used by an extraction engine which was manu- 
ally constructed for the same task. Our MUC-6 
system used approximately 75 clause level pat- 
terns, with 30 distinct verbal heads. In one 
conservative experiment, we observed that the 
discovery procedure found 17 of these verbs, or 
57%. However, it also found at least 8 verbs the 
manual system lacked, which seemed relevant to 
the scenario: 
company-bring-person-\[as?officer\] 12 
person-come-\[to+eompanv\]-\[as+oZScer\] 
person-rejoin- company-\[as + o25cer\] 
person-{ ret  , conti,  e, remai, ,stay}-\[as + o25cer\] 
person-pursue-interest 
At the risk of igniting a philosophical de- 
bate over what is or is not relevant o a sce- 
nario, we note that the first four of these verbs 
are evidently essential to the scenario in the 
strictest definition, since they imply changes of 
post. The next three are "staying" verbs, and 
are actually also needed, since higher-level infer- 
ences required in tracking events for long-range 
merging over documents, require knowledge of 
persons occupying posts, rather than only as- 
suming or leaving them. The most curious one 
is "person-pursue-interesf'; urprisingly, it too 
is useful, even in the strictest MUC sense, cf., 
(muc, 1995). Systems are judged on filling a 
slot called "other-organization", i dicating from 
or to which company the person came or went. 
This pattern is consistently used in text to indi- 
nbracketed  const i tuents  a re  outs ide  o f  the  cent ra l  
SVO t r ip le t ,  inc luded here  fo r  c la r i ty .  
287 
cate that the person left to pursue other, undis- 
closed interests, the knowledge of which would 
relieve the system from seeking other informa- 
tion in order to fill this slot. This is to say that 
here strict evaluation is elusive. 
5 D iscuss ion  and  Cur rent  Work  
Some of the prior research as emphasized in- 
teractive tools to convert examples to extraction 
patterns, cf. (Yangarber and Grishman, 1997), 
while others have focused on methods for au- 
tomatically converting a corpus annotated with 
extraction examples into such patterns (Lehn- 
ert et al, 1992; Fisher et al, 1995; Miller et 
al., 1998). These methods, however, do not re- 
duce the burden of finding the examples to an- 
notate. With either approach, the portability 
bottleneck is shifted from the problem of build- 
ing patterns to that of finding good candidates. 
The prior work most closely related to this 
study is (Riloff, 1996), which, along with (Riloff, 
1993), seeks automatic methods for filling slots 
in event templates. However, the prior work 
differs from that presented here in several cru- 
cial respects; firstly, the prior work does not at- 
tempt to find entire events, after the fashion 
of MUC's highest-level scenario-template task. 
Rather the patterns produced by those systems 
identify NPs that fill individual slots, without 
specifying how these slots may be combined 
at a later stage into complete vent templates. 
The present work focuses on directly discovering 
event-level, multi-slot relational patterns. Sec- 
ondly, the prior work either relies on a set of 
documents with relevance judgements to find 
slot fillers where they are relevant o events, 
(Riloff, 1996), or utilizes an un-classified cor- 
pus containing a very high proportion of rele- 
vant documents o find all instances of a seman- 
tic class, (Riloff and Jones, 1999). By contrast, 
our procedure requires no relevance judgements, 
and works on the assumption that the corpus is 
balanced and the proportion of relevant docu- 
ments is small. Classifying documents by hand, 
although admittedly easier than tagging event 
instances in text for automatic training, is still 
a formidable task. When we prepared the test 
corpus, it took 5 hours to mark 150 short doc- 
uments. 
The presented results indicate that our 
method of corpus analysis can be used to rapidly 
identify a large number of relevant patterns 
without pre-classifying a large training corpus. 
We are at the early stages of understanding 
how to optimally tune these techniques, and 
there are number of areas that need refinement. 
We are working on capturing the rich informa- 
tion about concept classes which is currently re- 
turned as part of our pattern discovery proce- 
dure, to build up a concept dictionary in tandem 
with the pattern base. We are also consider- 
ing the proper selection of weights and thresh- 
olds for controlling the rankings of patterns and 
documents, criteria for terminating the itera- 
tion process, and for dynamic adjustments of 
these weights. We feel that the generalization 
technique in pattern discovery offers a great 
opportunity for combating sparseness of data, 
though this requires further research. Lastly, 
we are studying these algorithms under several 
unrelated scenarios to determine to what extent 
scenario-specific phenomena affect their perfor- 
mance. 
References 
Ido Dagan, Shaul Marcus, and Shaul 
Markovitch. 1993. Contextual word simi- 
larity and estimation from sparse data. In 
Proceedings of the 31st Annual Meeting of 
the Assn. for Computational Linguistics, 
pages 31-37, Columbus, OH, June. 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fang-fang Feng, and Wendy Lehnert. 
1995. Description of the UMass system as 
used for MUC-6. In Proc. Si;zth Message Un- 
derstanding Conf. (MUC-6), Columbia, MD, 
November. Morgan Kaufmann. 
R. Grishman, L. Hirschman, and N.T. Nhan. 
1986. Discovery procedures for sublanguage 
selectional patterns: Initial experiments. 
Computational Linguistics, 12(3):205-16. 
Lynette Hirschman, Ralph Grishman, and 
Naomi Sager. 1975. Grammatically-based 
automatic word class formation. Information 
Processing and Management, 11(1/2):39-57. 
Timo J/irvinen and Pasi Tapanainen. 1997. A 
dependency parser for English. Technical Re- 
port TR-1, Department of General Linguis- 
tics, University of Helsinki, Finland, Febru- 
ary. 
Martin Kay and Martin RSscheisen. 1993. 
288 
Text-translation alignment. Computational 
Linguistics, 19(1). 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of massachusetts: MUC-4 test results 
and analysis. In Proc. Fourth Message Un- 
derstanding Conf., McLean, VA, June. Mor- 
gan Kaufmann. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance Ramshaw, Richard Schwartz, Rebecca 
Stone, Ralph Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract information; BBN: Description of the 
SIFT system as used for MUC-7. In Proc. of 
the Seventh Message Understanding Confer- 
ence, Fairfax, VA. 
1993. Proceedings of the Fifth Message Un- 
derstanding Conference (MUC-5), Baltimore, 
MD, August. Morgan Kaufmann. 
1995. Proceedings of the Sixth Message Un- 
derstanding Conference (MUC-6), Columbia, 
M_D, November. Morgan Kaufmann. 
Johanna Nichols. 1978. Secondary predicates. 
Proceedings of the 4th Annual Meeting of 
Berkeley Linguistics Society, pages 114-127. 
Maria Teresa Pazienza, editor. 1997. Infor- 
mation Extraction. Springer-Verlag, Lecture 
Notes in Artificial Intelligence, Rome. 
Fernando Pereira, Naftali Tishby, and Lillian 
Lee. 1993. Distributional clustering of En- 
glish words. In Proceedings of the 31st An- 
nual Meeting of the Assn. for Computational 
Linguistics, pages 183-190, Columbus, OH, 
June. 
Ellen Riloff and Rosie Jones. 1999. Learn- 
ing dictionaries for information extraction by 
multi-level bootstrapping. In Proceedings of
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99), Orlando, Florida, 
Ellen Riloff. 1993. Automatically construct- 
ing a dictionary for information extraction 
tasks. In Proceedings of Eleventh National 
Conference on Artificial Intelligence (AAAI- 
93), pages 811-816. The AAAI Press/MIT 
Press. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from untagged text. In 
Proceedings of Thirteenth National Confer- 
ence on Artificial Intelligence (AAAL96), 
pages 1044-1049. The AAAI Press/MIT 
Press. 
Pasi Tapanainen and Timo J~rvinen. 1997. A 
non-projective dependency parser. In Pro- 
ceedings of the 5th Conference on Applied 
Natural Language Processing, pages 64-71, 
Washington, D.C., April. ACL. 
Roman Yangarber and Ralph Grishman. 1997. 
Customization of information extraction sys- 
tems. In Paola Velardi, editor, International 
Workshop on Lexically Driven Information 
Extraction, pages 1-11, Frascati, Italy, July. 
Universit?~ di Roma. 
289 
Automatic Acquisition of Domain Knowledge for Information 
Extraction 
Roman Yangarber, Ralph Grishman Past Tapanainen 
Courant  Inst i tute of Conexor oy 
Mathemat ica l  Sciences Helsinki, F in land 
New York University 
{roman \[ grishman}@cs, nyu. edu Pasi. Tapanainen@conexor. fi 
Si!ja Ituttunen 
University of Helsinki 
F inland 
sihuttun@ling.helsinki.fi 
Abstract  
In developing an Infbrmation Extraction tIE) 
system tbr a new class of events or relations, one 
of the major tasks is identifying the many ways 
in which these events or relations may be ex- 
pressed in text. This has generally involved the 
manual analysis and, in some cases, the anno- 
tation of large quantities of text involving these 
events. This paper presents an alternative ap- 
proach, based on an automatic discovery pro- 
cedure, ExDIsCO, which identifies a set; of rele- 
wmt documents and a set of event patterns from 
un-annotated text, starting from a small set of 
"seed patterns." We evaluate ExDIScO by com- 
paring the pertbrmance of discovered patterns 
against that of manually constructed systems 
on actual extraction tasks. 
0 Introduct ion 
Intbrmation Extraction is the selective xtrac- 
tion of specified types of intbrmation from nat- 
ural language text. The intbrmation to be 
extracted may consist of particular semantic 
classes of objects (entities), relationships among 
these entities, and events in which these entities 
participate. The extraction system places this 
intbrmation into a data base tbr retrieval and 
subsequent processing. 
In this paper we shall be concerned primar- 
ily with the extraction of intbrmation about 
events. In the terminology which has evolved 
ti'om the Message Understanding Conferences 
(muc, 1995; muc, 1993), we shall use the term 
subject domain to refer to a broad class of texts, 
such as business news, and tile term scenario to 
refer to tile specification of tile particular events 
to be extracted. For example, the "Manage- 
ment Succession" scenario for MUC-6, which we 
shall refer to throughout this paper, involves in- 
formation about corporate executives tarting 
and leaving positions. 
The fundamental problem we face in port- 
ing an extraction system to a new scenario is 
to identify the many ways in which intbrmation 
about a type of event may be expressed in the 
text;. Typically, there will be a few common 
tbrms of expression which will quickly come to 
nfind when a system is being developed. How- 
ever, the beauty of natural language (and the 
challenge tbr computational linguists) is that 
there are many variants which an imaginative 
writer cast use, and which the system needs to 
capture. Finding these variants may involve 
studying very large amounts of text; in the sub- 
ject domain. This has been a major impediment 
to the portability and performance of event ex- 
traction systems. 
We present; in this paper a new approach 
to finding these variants automatically fl'om a 
large corpus, without the need to read or amLo- 
tate the corpus. This approach as been evalu- 
ated on actual event extraction scenarios. 
In the next section we outline the strncture of 
our extraction system, and describe the discov- 
ery task in the context of this system. Sections 
2 and 3 describe our algorithm for pattern dis- 
covery; section 4 describes our experimental re- 
sults. This is tbllowed by comparison with prior 
work and discussion in section 5. 
1 The Extract ion System 
In the simplest terms, an extraction system 
identifies patterns within the text, and then 
mat)s some constituents of these patterns into 
data base entries. (This very simple descrip- 
lion ignores the problems of anaphora nd in- 
tersentential inference, which must be addressed 
by any general event extraction system.) AI- 
though these l)atterns could in principle be 
stated in terms of individual words, it is much 
940 
easier to state them in terms of larger SylltaC- 
tic constituents, uch as noun phrases and verb 
groups. Consequently, extraction ormally con- 
sists of an analysis of the l;e.xt in terms of general 
linguistic structures and dolnain-specifio con- 
structs, tbllowed by a search for the scenario- 
specific patterns. 
It is possible to build these constituent struc- 
tures through a flfll syntactic analysis of the 
text, and the discovery procedure we describe 
below woul(1 be applicable to such an architec- 
ture. Howe, ver, for re&sellS of slme,(t , coverage, 
and system rolmstness, the more (:ommon ap- 
t)roa(:h at present is to peribrni a t)artial syn- 
tactic analysis using a cascade of finite-state 
transducers. This is the at)t)roa(:h used by our 
e.xtraction system (Grishman, 1995; Yangarber 
and Grishman, 1998). 
At; the heart of our syslx'an is a regular ex- 
pression pattern matcher which is Cal)al)le of 
matching a set of regular exl)ressions against 
a partially-analyzed text and producing addi- 
tional annotations on the text. This core draws 
on a set of knowledge bases of w~rying degrees 
of domain- and task-specificity. The lexicon in- 
cludes both a general English dictionary and 
definitions of domain and scenario terms. The 
concept base arranges the domain terms into 
a semantic hierarchy. The predicate base. de- 
s('ribes the, logical structure of I;he events to be 
extracl;od. 'Fire pattern \])ase consists of sets of 
patterns (with associated actions), whi(;h make 
r(;ferollCO to information Kern the other knowl- 
e(lge bases. Some t)attorn sots, su(:h as those for 
n(mn and verb groups, are broadly apl)licable , 
wlfile other sets are spe(:ifio to the scenario. 
V~Ze, have previously (Yangarl)er and Grish- 
man, 1.997) (lescrit)ed a user interface which 
supt)orts the rapid cust;omization of the extrac- 
tion system to a new scenario. This interface 
allows the user to provide examples of role- 
wmt events, which are automatically converted 
into the appropriate patterns and generalized to 
cover syntactic variants (passive, relative clause, 
etc.). Through this internee, the user can also 
generalize l;he pattern semanti('ally (to (:over a 
broader class of words) and modify the concet)t 
base and lexicon as needed. Given an appro- 
priate set; of examples, thereibre, it; has become 
possible to adapt the extraction system quite 
ral)idly. 
However, the burden is still on the user to 
find the appropriate set of examples, which may 
require a painstaldng and expensive search of a 
large corpus. Reducing this cost is essential for 
enhanced system portability; this is the problem 
addressed by the current research. 
Ilow can we automatically discover a suitable 
set; of candidate patterns or examples (patterns 
which at least have a high likelihood of being 
relevant to the scenario)? The basic idea is to 
look for linguistic patterns which apt)ear with 
relatively high frequency in relevant documents. 
While there has been prior research oll idea|i- 
lying the primary lexical t)atterns of a sublan- 
guage or cortms (Orishman et al 1986; Riloff, 
1996), the task here is more complex, since we 
are tyt)ically not provided in advance with a 
sub-corpus of relevmlt passages; these passages 
must themselves be tbund as part of t;t1(; discov- 
ery i)rocedure. The difficulty is that one of the 
l)est imlic~tions of the relevance of the passages 
is t)recisely the t)resence of these constructs. Bo- 
(:ause of this (:ircularity, we l)ropose to a(:quire. 
the constructs and t)assagos in tandem. 
2 ExDISCO: the  D iscovery  P rocedure  
We tirst outline ExDIsco ,  our procedure for 
discovery of oxl,raction patterns; details of some 
of the stops arc l)rcse, nted in the section which 
follows, and an earlier t)~q)er on our at)l)roach 
(Yang~u:bcr ot al., 2000). ExDIscO is mi ml- 
supervised 1)rocedure: the training (:ortms does 
not need to t)e amlotated with the specific event 
intbrmatkm to be. e.xtracted, or oven with infor- 
mation as to whi(;h documents in the ('orpus are 
relevant o the scenario. 'i7tlo only intbrmation 
the user must provide, as described below, is a 
small set of seed patterns regarding the s(:enario. 
Starting with this seed, the system automati- 
(:ally pertbnns a repeated, automatic expansion 
of the pattern set. This is analogous to the pro- 
cess of automatic t;enn expansion used in s()me 
information retrieval systems, where, the terlns 
Dora the most relewmt doculncnts are added 
to the user query and then a new retriewfl is 
imrformed. However, by expanding in terms of 
1)atl;erns rather than individual terms, a more 
precise expansion is possit)le. This process pro- 
coeds as tbllows: 
0. We stm:t with a large, corlms of documents 
in the domain (which have not been anne- 
941 
tared or classified in any way) and an initial 
"seed" of scenario patterns selected by the 
user - -  a small set of patterns whose pres- 
ence reliably indicates thai; the document 
is relevant o the scenario. 
. The pattern set is used to divide the cor- 
tins U into a set of relewmt documents, R
(which contain at; least one instance of one 
of the patterns), and a set of non-relevant 
documents R = U - R. 
2. Search tbr new candidate patterns: 
? automatically convert each document 
in the eorIms into a set of candidate 
patterns, one for each clause 
? rank patterns by the degree to which 
their distribution is correlated with 
docmnent relevance (i.e., appears with 
higher frequency in relevant docu- 
ments than in non-relewmt ones). 
3. Add the highest ranking pattern to the pat- 
tern set. (Optionally, at this point, we may 
present he pattern to the user for review.) 
4. Use the new pattern set; to induce a new 
split of the corpus into relevant and non- 
relevant documents. More precisely, docu- 
ments will now be given a relevance confi- 
dence measure; documents containing one 
of the initial seed patterns will be given 
a score of 1, while documents which arc 
added to the relevant cortms through newly 
discovered patterns will be given a lower 
score. I/,epeat the procedure (from step 1) 
until some iteration limit is reached, or no 
more patterns can be added. 
3 Methodo logy  
3.1 Pre-processing: Syntact ic Analysis 
Before at)plying ExDIsco ,  we pre-proeessed 
the cortms using a general-purpose d pendency 
parser of English. The parser is based on 
the FDG tbrmalism (Tapanainen and Jgrvi- 
hen, 1997) and developed by the Research Unit 
for Multilingual Language Technology at the 
University of Helsinki, and Conexor Oy. The 
parser is used ibr reducing each clause or noun 
phrase to a tuple, consisting of the central ar- 
guments, ms described in detail in (Yangarber 
et al, 2000). We used a corlms of 9,224 articles 
from the Wall Street; Journal. The parsed arti- 
cles yielded a total of 440,000 clausal tuples, of 
which 215,000 were distinct. 
3.2 Normal izat ion 
We applied a name recognition module prior to 
parsing, and replaced each name with a token 
describing its (:lass, e.g. C-Person, C-Company, 
etc. We collapsed together all numeric expres- 
sions, currency wflues, dates, etc., using a single 
token to designate ach of these classes. Lastly, 
the parser performed syntactic normalization to 
transtbrm such variants ms the various passive 
and relative clauses into a common tbrm. 
3.3 General izat ion and Concept Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pints: e.g., a verb- 
object pair, a subject-object pair, etc. Each pair 
is used as a generalized pattern during the can- 
didate selection stage. Once we have identitied 
pairs which are relevant o the scenario, we use 
them to gather the set; of words for the miss- 
ing role(s) (tbr example, a class of verbs which 
occur with a relevant subject-ot@ct pair: "com- 
pany {hire/fire/expel...} person"). 
3.4 Pat tern  Discovery 
We (-onducte(1 exi)eriments in several scenarios 
within news domains such as changes in cor- 
porate ownership, and natural disasters. Itere 
we present results on the "Man~geme.nt Suc- 
cession" and "Mergers/Acquisitions" cenarios. 
ExDIsco  was seeded with lninimal pattern sets, 
namely: 
Subject Verb Direct Object 
C-Company C-At)point C-Person 
C-Person C-Resign 
ibr the Mmmgement task, and 
Subject Verb Direct Object 
* C-Buy C-Conlt)any 
C-Company merge * 
for Acquisitions. Here C-Company and C- 
Person denote semantic classes containing 
named entities of the corresponding types. C- 
Appoint denotes the list of verbs { appoint, elect, 
promote, name, nominate}, C-Resign = { re- 
sign, depart, quit }, and C-Buy = { buy , pur- 
chase }. 
942 
\ ])uring ~ single iter~tion, we conqmt(; the 
score, See're(p), for each cm~(lidate 1)attern p, 
using (;he fornmla~: 
S, :o ' , ' ,@)  = IH n l~l 
IHI - 1,,~ IHn  ~.1 (:t) 
where 12. (Icnotes (;h(', l'clewmt subsc(; of docu- 
ments, mid I t=  It(p) the, ( locmnents imttching 
p, as above; the Iirst (;erm a(:(:ounts for the con- 
(lition~fl t)robabil ity of relev;m('e oil p~ and |;11(; 
second tbr its support .  We further impose two 
support criteria: we distrust such frequent pat- 
(;,~.,-.~ w\]le,:e I1~ n UI > ,~IUI, ,~ uninforn,,~tive, 
mid rare patte.rns \['or which I1\] r-I \]~.1 < fl as 
noise. 2 At the end ot' (.aeh il;eratiol~, the sysl;em 
selects the pal;tern with the highest Sco'/'d(p)~ 
and adds it (;o (;lie seed scl;. The (to(:un~enl;s 
which t;he winning t)~(;t;ern hits are added (;o 
t;111(; relevant set. The  t)al;l;(;rn s(;areh is then 
r(;sl;m:l;(;d. 
3.5 Document  Re- rank ing  
Th(: above is a simt)lifi(';~l;ion of (;he a,(:tual pro- 
cedlll'(}~ in severa\] r(',st)e('(;s. 
Only generalized t)ntl;erns are (:onsidered fi)r 
(:audi(t~my, with one or mot(', slol;s fill(:(1 wi(;h 
wihl-cm'ds. In comput ing the score of th(', ge, n- 
(;raliz(:d \]);tttern, w(: do not take into ('onsi(h:r- 
;i,1;i()11 all possible va,hw, s of the, wil(1-('m:d role. 
\?e instea.d (:()llS(;raJll (;he wild-(:ar(l to thos(~ wd- 
u(:s wlli(:h l;ht',llls(;lv(;s ill (;llrH \]l;tV(: high scores. 
Th(:se v~du(:s l;lw, n |)e(:on~e lllClll\])(;l'S of }/. II(:W 
(:lass, whi(:h is l)rOdu(:ed in (;:tlldClll with the 
wimfing 1)att(:rn. 
\])o('umel~tS reh:wm('e is s(-ored (m ~ s(;ah: l)e- 
(;ween 0 and 1. Tlm seed t)atterns a.re a.(:cet)ted 
~,s trut\]~; the do('mlw, nts (;hey mat(:\]1 hnve rcle- 
vmme 1. On i(;er;~tion i + 1, e~mh t)a(;tern p is 
assigned a precision measure, t)ase(l on the rel- 
(':Vall(;e of |;11(; (locllnlelfl;s i|; 111a, l;(;ll(',,q: 
~ ".d~(d) (~) 
f f , , :d  +~ (v)  - -  IH(v) l ,~.(,,) 
where l~,eli(d) is the re, levmlce of' 1;11(: doeunmn(; 
fi'om t;t1(', previous iteration, ~md l I(p) is the set 
of documents where p matched, in general, if K 
is a classifier (:onsisting of ~ set of l)al;terns, w(', 
define H (K)  as the st:l; of documents  where all 
~similar to that used in (liiloff, 1996) 
~W(: used ,:-- 0.1 and fl = 2. 
of t)~d;terns p C K m~l;(:h, mid the "cunmlative" 
precision of K as 
1 ~ 1~4~(a,) (3) P~.~d +~(1() = IH U()I <.(K)  
Once the wimfing pa,l;l;ern is accepted, the rel- 
ewmee of the documents is re-adjusted. For 
(;~mh document  d which is matched by some 
subset of l;he currently accet)t('d pntterns, we 
can view thai; sul)s(',t; of  l)~tterns as ~ classitier 
Kd = {pj}. These  patterns (tel;ermilm the new 
reh;wmce score of the document  as 
J~, "~l,~ " ( ,0  : 111~x (:tc,,.1,*(,O,v,.,;, .~" (K , ) )  (~:) 
This ensures tha.(; l;he rclewmce score grows 
monotonical ly, and only when there is sufliei(mt 
positive evidence, as (;he i)ntterns in etl'e(:I; vote 
"conjmmtively" on the (loculncnl;s. 
We also tr ied an alternative, ::disjun(:tive" 
voting scheme, with weights wlfich accounts tbr 
vm:intion in support of the p~ttterns, 
J,.,.1, (d) . . . .  ~ "~ I I  (1 - ~',.~,.c~(p))"",' (5) 
~c K(d) 
where t;11(', weights ,wp arc (tetint;d using the tel- 
ewm(:(: of the (loeuments, a,s the total  SUl)l)or(; 
which the pa, I;I;ern p receives: 
% = log ~ l;.d,(d) 
dE 11 (p) 
and ;,7 is (;11(' largest weight. The  r(',cursive for- 
nmb~s ('apl;m:e (;he mul;u~fl dependency of t)~t- 
terns ~md documents;  this re-computat ion ~md 
growing of precision and relevmlce rmlks is the 
core of the t)rocedure. :~ 
4 Resu l ts  
4 .1  Event  Ext ract ion  
'l'he, most nal;m'a.l measm'e of efl'ecl;iveness of our 
discovery procedure is the performmme of ml ex- 
tract ion systmn using the, discovered t)~tterns. 
However, il; is not 1)ossil)le to apply this reel;- 
rio direei;ly because the discovered t)al;terns lack 
some of the information required tbr entries ill 
:{\V('. did not el)serve a significam; difl'erencc in 1)crfi)r- 
lIiHl\[CO, bet, ween the two tormulas 4 alt(t 5 in o111" experi- 
in(mrs; the results whit:h tbllow use 5. 
943 
the pattern base: information about the event 
type (predicate) associated with the pattern, 
and the mapping from pattern elements to pred- 
icate arguments. We have evaluated ExDIsco  
by manually incorporating the discovered pat- 
terns into the Proteus knowledge bases and run- 
ning a full MUC-style evaluation. 
We started with our extraction system, Pro- 
tens, which was used in MUC-6 in 1995, and 
has undergone continual improvements since 
the MUC evaluation. We removed all the 
scenario-specific clause and nominalization pat- 
terns. 4 We then reviewed all the patterns which 
were generated by the ExDIsco,  deleting those 
which were not relewmt to the task, or which 
did not correspond irectly to a predicate al- 
ready implemented tbr this task)  The remain- 
ing pat;terns were augmented with intbnnation 
about the corresponding predicate, and the re- 
lation between the pattern and the predicate 
al'guments, a The resulting variants of Proteus 
were applied to the formal training corpus and 
the (hidden) formal test corpus for MUC-6, and 
the output evaluated with the MUC scorer. 
The results on the training corpus are: 
Pattern Base Recall Precision 
Seed 38 83 
Ex I ) Isco 62 80 
Union 69 __79 
Manual-MUC ~ 71 L~1.9~ 
Manual-NOW 6(3~ 79 L7!~z\[)_t_j 
and on the test cortms: 
4There are also a few noun phrase patterns which can 
give rise to scenario events. For example, "Mr Smith, 
former president of IBM", may produce an event record 
where l%ed Smith left IBM. These patterns were left in 
Proteus for all the runs, and they make some contribu- 
tion to the relatively high baseline scores obtained using 
just the seed event patterns. 
~ExD~sco f und patterns which were relevant to the 
task lint could not be easily aceomodated in Proteus. 
For instance "X remained as president" could be rele- 
vant, particularly in the case of a merger creating anew 
corporate ntity, but Proteus was not equipped to trun- 
dle such iIfformation, and has not yet been extended to 
incorporate such patterns. 
6As with all clause-level patterns in Proteus, these 
patterns m-e automatically generalized tohandle syntac- 
tic wn'iants uch as passive, relative clause, etc. 
Pattern Base Recall Precision F 
Seed 27 74 39.58 
ExDIsco 52 72 60.16 
Union 57 73 63.56 
Manual-NOW -- 56 75 6404. 
The tables show the recall and precision mea- 
sures for the patterns, with F-measure being 
the harmonic mean of the two. The Seed pat- 
tern base consists of just the initial pattern set, 
given in the table on the previous page. ~ib this 
we added the patterns which the system discov- 
ered automatically after about 100 iterations, 
producing the pattern set called ExDIsco.  For 
comparison, M anual-MUC is the pattern base 
lnanually develot)ed on the MUC-6 training 
corpus-1)repared over the course of 1 month 
of full-time work by at least one computational 
linguist (during which the 100-document train- 
ing corpus was studied in detail). The last row, 
Manual-now, shows the current pertbrmance of
the Proteus system. The base called Ultiolt con- 
tains the union of ExDIScO and Manual-No'w. 
We find these results very encouraging: Pro- 
teus performs better with the patterns discov- 
ered by ExI)IscO than it did after one month 
of manual tinting and development; in fact, this 
perfi)rmance is close to current levels, which 
are the result of substantial additional devel- 
opmeut. These results umst be interpreted, 
however, with several caveats. First, Proteus 
performance depends on many fimtors besides 
the event patterns, such as the quality of name 
re, cognition, syntactic mmlysis, anaphora reso~ 
lution, inferencing, etc. Several of these were 
improved since the MUC formal evaluation, so 
some of the gain over the MUC formal evalua- 
tion score is attritmtable to these factors. How~ 
ever, all of the other scores are comparable in 
these regards. Second, as we noted above, the 
patterns were reviewed and augmented manu- 
ally, so the overall procedure is not entirely au- 
tomatic. However, the review and augmenta- 
tion process took little time, as compared to 
the manual corpus analysis and development of
the pattern base. 
4.2 Text  f i l ter ing 
We can obtain a second measure of pertbr- 
mance by noting that, in addition to growing 
the tmttern set, ExDIsco  also grows the rele- 
944 
0.9 
0.8 
0.7 
0.6 
0.5 
_ . r -~H . . . . . . . . . . . . . . . . .  r . . . . . . .  T ~ ~ : : ~  T 
;!\ >. g~t : 
- ' il 
%i 
\[!\] 
7 
\[!J 
Legend: 
Management/Test ? .-{~ ...... 
ManagemenVl-raie - :*: -- 
MUC-6 ? 
0.2 0.4 0.6 0.8 
Recall 
Figure l: Management Suc('cssion 
0.9 
0.8 
0.7 
0.6 
0.5 
L_~/r 
Legend: 
Acquisition 
0.2 0.4 0.6 
Recall 
0.8 
Figme 2: Mergers/A(:quisitions 
vance rankings of documents. The latter cnn be 
evahlated irectly, wil;hollt human intervention. 
We tested Exl)IsC, o ~tgainst wo cor\])orn: th(; 
100 documents from MUC-6 tbrmal training, 
a:nd the 100 documents from the MUC-6 for- 
mal test (both are contained anlong the 10,000 
ExDIsoO training set) r. Figure 1 shows recall 
t)\]otted against precision on the two corpora, 
over 100 iterations, starting with the seed pat- 
te, nls in section 3.d. This view on the discovery 
procedure is closely related to the MUC %ext- 
till;ering" task, in which the systems are jlulged 
at the \]evel of doc,wm, e,'nt.s rather thmt event slots. 
It; is interesting to (:omt)m:e Exl)IsCO's results 
with how other MUC-6 part\]tit)ants performed 
on the MUC-b '  test cortms , shown anonymously. 
ExDIscO attains values within the range of 
the MUC participald;S, all of which were either 
heavily-supervised or m~mually coded systems. 
II; is important to bear in mind that Ex I ) I sco  
had no benefit of training material, or any in- 
tbrmation beyond the seed pattern set. 
Figure 2 shows the 1)ertbrmance, of text fil- 
tering on the Acquisition task, again, given the 
seed in section 3.4. ExDisco  was trained on 
|;lie same WSJ eorlms, and tested against a set 
of 200 documents. We retrieved this set using 
keyword-based IR, search, and judged their rel- 
evance by halId. 
rThesc judgements constituted the truth which was 
used only for evaluation, not visible to ExDISCO 
5 Discuss ion  
The development of a w~riety of information 
extra(:tion systems over the last decade has 
demonstrated their feasibility but also the lim- 
itations on their portability and t)erformance. 
Prcl)aring good t)atterns tbr these syste, ms re- 
quires (:onsiderable skill, and achieving good 
(:overage requires |;lie analysis of a large amount 
of text. These t)rol)lems h~ve t)een impedinmnts 
to the -wide\].' use of extraction systenls. 
These dit\[iculties have stimulate.d resear('h on 
1)attel . 'n a ( : ( lu i s i t ion .  So lne  o f  th i s  work  has  en l -  
i)hasized il\]teractive tools to (:onvert examples 
to extractioi~ t)atterlls (Yangarber and Grish- 
man, 1997); nmch ot:' the re, search has focused on 
methods for automatically converting a cortms 
annotated with extraction examples into pat- 
terns (Lehnert et al, 1992; Fisher et al, 1995; 
Miller el; al., 1998). These techniques may re- 
duce the level of systeln expertise required to 
develop a new extraction N)plieation, but they 
do not lessen the lmrden of studying a large cor- 
lms in order to .find relevant candidates. 
The prior work most closely related to our 
own is that of (R.ilotf, 1996), who also seeks to 
lmild pattenls automatically without the need 
to annotate a corpus with the information to 
be extracted. Itowever, her work ditfers t'rom 
01217 own in several i lnportant respects. First, 
her patterns identit~y phrases that fill individual 
slots in the template, without specifying how 
these slots may be combined at a later stage 
into complete templates. In contrast, our pro- 
cedure discovers complete, multi-slot event pat- 
945 
terns. Second, her procedure relies on a cort)us 
in which |;tie documents have been classified for 
relevance by hand (it was applied to the MUC-3 
task, tbr which over 1500 classified documents 
are available), whereas ExDIsco requires no 
manual relevance judgements. While classify- 
ing documents tbr relevance is much easier than 
annotating docunlents with the information to 
be extracted, it; is still a significant ask, and 
places a limit on |:tie size of the training corpus 
that can be effectively used. 
Our research as demonstrated that for the 
studied scenarios automatic pattern discovery 
Call yield extraction perfi)rmance colnt)arabh~ to
that obtained through extensive corpus anal- 
ysis. There are many directions in which the 
work reported here needs to be extended: 
? nsing larger training corpora, in order to 
find less frequent exanlplcs, and in that way 
hopefully exceeding the i)erfornlancc of our 
best hand-trained system 
? cat)luring the word classes which are gen- 
erated as a by-product of our pattern dis- 
covery 1)rocedure (in a manner similar to 
(Riloff and ,Jones, 1999)) and using them 
to discover less frequent )atterns in subse- 
quent iterations 
- evaluating the effectiveness of the discov- 
cry procedure on other scenarios. In par- 
titular, we need to be able to identi\[y top- 
its which cast be most effbctively charac- 
terized by clause-level patterns (as was the 
case tbr the business domain), and topics 
which can be better characterized by other 
means. We. wouM also like to understand 
how the topic clusters (of documents and 
patterns) which are developed by our pro- 
cedure line up with pre-specified scenarios. 
References 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fangfang Feng, and Wendy Lelmert. 
1995. Description of the UMass system as 
used fbr MUC-6. In Prec. Sixth Message Un- 
dcrstandin9 Conf. (MUC-6), Columbia, MD, 
November. Morgan Kauflnann. 
R.alph Grishman. 1995. The NYU systenl tbr 
MUC-6, or where's the syntax? Ill Prec. 
Sixth Message Understanding Conf. (MUC- 
6), pages 167 176, Columl)ia, MD, Novem- 
ber. Morgan Kauflnann. 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of nlassachusetts: MUC-4 test results 
and analysis. Ill P,'oe. Fourth Message Un- 
der.standing Con.\[., McLean, VA, June. Mor- 
gan Kauflnaml. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance II,amshaw, R,ichard Schwartz, Rebecca 
Stone, Rall)h Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract intbrmation; BBN: Description of the 
SIFT systenl as used for MUC-7. In PTve. 7th 
Mc.ssagc Understanding Co~:f., FMrfax, VA. 
1993. Proceedings of the F'~ifth Message UTz.- 
derstanding Confer(race (MUC-5), Baltimore, 
MD, August. Morgan Kauflnann. 
1995. PTveeedings of the Sixth Message U~I,- 
derstav, ding Conference (MUC-6), Colmnt)ia, 
MD, November. Morgan Kauflnaml. 
Ellen Rilotf and Rosie Jones. 1999. Learn- 
ing dictionaries for infbrmation extraction by 
multi-level bootstrat)ping. In Prec. 16th Nat'l 
Cord'erenee on Art'~i\[icial Intelli9enee (AAA I 
99), Orlando, Florida. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from m~tagged text. In 
Prec. I3th Nat'l Co~~:f. on Art~ificial Intel- 
ligence (AAAI-96). The AAAI Press/MIT 
Press. 
l?asi '\])~panainen a d Time .J/h:vinen. 1997. A 
non-t)rojectivc dependency parser. In P'mc. 
5th Conf. on Applied Nat'aral Language P~v- 
cessiu9, pages 64-71, Washington, D.C. ACL. 
Roman Yangarber and RalI)h Grishman. 1997. 
Customization of intbrmation extraction sys- 
tems. In Paola Velardi, editor, I~tt'l Work- 
shop on Lexically Driven I~7:forrnation Extrac- 
tion, Frascati, Italy. Universith di Roma. 
Roman Yangarl)er and Ralph Grishman. 1998. 
NYU: Description of thc Protens/PET sys- 
tem as used tbr MUC-7 ST. In 7th Message 
Understanding Conference, Columbia, MD. 
Roman Yangarl)er, Ralph Grishman, Past 
Tapanainen, and Silja Huttunen. 2000. Un- 
supervised discovery of scenario-level pat- 
terns tbr information extraction. Ill PTve. 
Co~@ on Applied Nat'aral Lang'aage Pr'ocess- 
tug (ANLP-NAACL), Seattle, WA. 
946 
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 22?23,
Vancouver, October 2005.
Extracting Information about Outbreaks of Infectious Epidemics
Roman Yangarber Lauri Jokipii
Department of Computer Science
University of Helsinki, Finland
first.last@cs.helsinki.fi
Antti Rauramo
Index, Oy
Helsinki, Finland
Silja Huttunen
Department of Linguistics
University of Helsinki, Finland
Abstract
This work demonstrates the ProMED-
PLUS Epidemiological Fact Base. The
facts are automatically extracted from
plain-text reports about outbreaks of in-
fectious epidemics around the world. The
system collects new reports, extracts new
facts, and updates the database, in real
time. The extracted database is available
on-line through a Web server.
1 Introduction
Information Extraction (IE) is a technology for find-
ing facts in plain text, and coding them in a logical
representation, such as a relational database.
Much published work on IE reports on ?closed?
experiments; systems are built and evaluated based
on carefully annotated corpora, at most a few hun-
dred documents.1 The goal of the work presented
here is to explore the IE process in the large: the
system integrates a number of off-line and on-line
components around the core IE engine, and serves
as a base for research on a wide range of problems.
The system is applied to a large dynamic collec-
tion of documents in the epidemiological domain,
containing tens of thousands of documents. The
topic is outbreaks of infectious epidemics, affecting
humans, animals and plants. To our knowledge, this
is the first large-scale IE database in the epidemio-
logical domain publicly accessible on-line.2
1Cf., e.g., the MUC and ACE IE evaluation programmes.
2On-line IE databases do exist, e.g., CiteSeer, but none that
extract multi-argument events from plain natural-language text.
2 System Description
The architecture of the ProMED-PLUS system3 is
shown in Fig. 1. The core IE Engine (center) is im-
plemented as a sequence, or ?pipeline,? of stages:
  Layout analysis, tokenisation, lexical analysis;
  Name recognition and classification;
  Shallow syntactic analysis;
  Resolution of co-reference among entities;
  Pattern-based event matching and role mapping;
  Normalisation and output generation
The database (DB) contains facts extracted from
ProMED-Mail, a mailing list about epidemic out-
breaks.4
The IE engine is based in part on earlier work,
(Grishman et al, 2003). Novel components use ma-
chine learning at several stages to enhance the per-
formance of the system and the quality of the ex-
tracted data: acquisition of domain knowledge for
populating the knowledge bases (left side in Fig. 1),
and automatic post-validation of extracted facts for
detecting and reducing errors (upper right). Novel
features include the notion of confidence,5 and ag-
gregation of separate facts into outbreaks across
multiple reports, based on confidence.
Operating in the large is essential, because the
learning components in the system rely on the
availability of large amounts of data. Knowledge
3PLUS: Pattern-based Learning and Understanding System.
4ProMED, www.promedmail.org, is the Program for Mon-
itoring Emerging Diseases, of the International Society for In-
fectious Diseases. It is one of the most comprehensive sources
of reports about the spread of infectious epidemics around the
world, collected for over 10 years.
5Confidence for individual fields of extracted facts, and for
entire facts, is based on document-local and global information.
22
IE engine
Customization 
environment
Lexicon
Ontology
Patterns
Inference rules
Unsupervised 
learning
Extracted facts
Candidate knowledge
DB server
User query Response
publisher user
Data collection
Web server
customizer
Noise reduction/
Data correction/ 
Cross-validation
Other corpora
Text documents
Knowledge bases:
Figure 1: System architecture of ProMED-PLUS
acquisition, (Yangarber et al, 2002; Yangarber,
2003) requires a large corpus of domain-specific and
general-topic texts. On the other hand, automatic
error reduction requires a critical mass of extracted
facts. Tighter integration between IE and KDD com-
ponents, for mutual benefit, is advocated in recent
related research, e.g., (Nahm and Mooney, 2000;
McCallum and Jensen, 2003). In this system we
have demonstrated that redundancy in the extracted
data (despite the noise) can be leveraged to improve
quality, by analyzing global trends and correcting
erroneous fills which are due to local mis-analysis,
(Yangarber and Jokipii, 2005). For this kind of ap-
proach to work, it is necessary to aggregate over a
large body of extracted records.
The interface to the DB is accessible on-line
at doremi.cs.helsinki.fi/plus/ (lower-right
of Fig. 1). It allows the user to view, select and sort
the extracted outbreaks, as well as the individual in-
cidents that make up the aggregated outbreaks. All
facts in the database are linked back to the original
reports from which they were extracted. The dis-
tribution of the outbreaks may also be plotted and
queried through the Geographic Map view.
References
R. Grishman, S. Huttunen, and R. Yangarber. 2003. In-
formation extraction for enhanced access to disease
outbreak reports. J. of Biomed. Informatics, 35(4).
A. McCallum and D. Jensen. 2003. A note on the uni-
fication of information extraction and data mining us-
ing conditional-probability, relational models. In IJ-
CAI?03 Workshop on Learning Statistical Models from
Relational Data.
U. Y. Nahm and R. Mooney. 2000. A mutually beneficial
integration of data mining and information extraction.
In AAAI-2000, Austin, TX.
R. Yangarber and L. Jokipii. 2005. Redundancy-based
correction of automatically extracted facts. In Proc.
HLT-EMNLP 2005, Vancouver, Canada.
R. Yangarber, W. Lin, and R. Grishman. 2002. Un-
supervised learning of generalized names. In Proc.
COLING-2002, Taipei, Taiwan.
R. Yangarber. 2003. Counter-training in discovery of se-
mantic patterns. In Proc. ACL-2003, Sapporo, Japan.
23
Complexity of Event Structure in IE Scenarios
Silja Huttunen, Roman Yangarber, Ralph Grishman
Courant Institute of Mathematical Sciences
New York University
fsilja,roman,grishmang@cs.nyu.edu
Abstract
This paper presents new Information Extrac-
tion scenarios which are linguistically and struc-
turally more challenging than the traditional
MUC scenarios. Traditional views on event
structure and template design are not adequate
for the more complex scenarios.
The focus of this paper is to show the com-
plexity of the scenarios, and propose a way to
recover the structure of the event. First we
identify two structural factors that contribute
to the complexity of scenarios: the scattering
of events in text, and inclusion relationships
between events. These factors cause di?culty
in representing the facts in an unambiguous
way. Then we propose a modular, hierarchi-
cal representation where the information is split
in atomic units represented by templates, and
where the inclusion relationships between the
units are indicated by links. Lastly, we discuss
how we may recover this representation from
text, with the help of linguistic cues linking the
events.
1 Introduction
Information Extraction (IE) is a technology
used for locating and extracting specic pieces
of information from texts. The knowledge bases
are customized for each new topic or scenario,
as dened by ll rules that state which facts are
needed for constitution of an extractable event.
A scenario is a set of predened facts to be ex-
tracted from a large text corpus, such as news
articles, and organized in output templates.
Our experience with customizing our IE sys-
tem called Proteus (Grishman, 1997; Grishman
et al, 2002) to new scenarios suggests that the
lexical and structural properties of the scenario
aect the performance of the system. To make
an IE system exible for tasks of varying com-
plexity, it is essential to conduct a linguistic
analysis of the texts relating to dierent sce-
narios.
In this paper, we focus on the Infectious Dis-
ease Outbreak scenario (Grishman et al, 2002),
and the Natural Disaster scenario (Hirschman
et al, 1999) collectively called the \Nature" sce-
narios. During the customization of the IE sys-
tem to the Nature scenarios, we encountered
problems that did not arise in the traditional
scenarios of the Message Understanding Con-
ferences (MUCs). This included, in particular,
delimiting the scope of a single event and orga-
nizing the events into templates.
We identify two structural factors that con-
tribute to the complexity of a scenario: rst, the
scattering of events in text, and second, inclu-
sion relationships between events. These factors
cause di?culty in representing the facts in an
unambiguous way. We proposed that such event
relationships can be described with a modular,
hierarchical model (Huttunen et al, 2002).
The phenomenon of inclusion is widespread in
the Nature scenarios, and the types of inclusions
are numerous. In this paper we present prelim-
inary results obtained from our corpus analysis,
with a classication and distribution of inclu-
sion relationships. We discuss the potential for
recovery of these inclusions from text with the
help of the linguistic cues, of which we show
some examples.
This paper will argue that a thorough linguis-
tic analysis of the corpus is needed to help recov-
ery of the complex event structure in the text.
In the next section we give a brief description
of the scenarios we are investigating. In section
3 we review the problems of scattering, inclusion
and event denition, and propose a method for
representing template structure. In section 4
we present examples of the linguistic cues to
Disaster Date Location VictimDead Damage
tornado Sunday night Georgia one person motel
Disease Date Location VictimDead VictimSick
Ebola since September Uganda 156 people -
Table 1: Disaster Event and Disease Event
recover the complex event structure, followed
by discussion in section 5.
2 Background
2.1 Information Extraction
Our IE system has been previously customized
for several news topics, as part of the MUC
program, such as Terrorist Attacks (MUC,
1991; MUC, 1992) and Management Succession
(MUC, 1995; Grishman, 1995). Subsequently to
the MUCs, we customized Proteus to extract,
among other scenarios, Corporate Mergers and
Acquisitions, Natural Disasters and Infectious
Disease Outbreaks.
We contrasted the Nature scenarios with the
earlier MUC scenarios (Huttunen et al, 2002).
The \traditional" template structure is such
that all the information about the main event
can be presented within a single template. The
main events form separate instances, and there
are no links between them. Management Suc-
cession scenario presents a slightly more com-
plicated template structure, but it is still possi-
ble to present in one template. The traditional
representation is not adequate to represent the
complex structure of the Nature scenarios.
In the next section, we give a short descrip-
tion of the Nature scenarios.
2.2 Scenarios
For the Natural Disaster scenario, the task
is to nd occurrences of disasters (earthquakes,
storms, etc.) around the world, as reported in
newspaper articles. The information extracted
for each disaster should include the type of dis-
aster, date and location of the occurrence, and
the amount of human or material damage.
An example of a Natural Disaster template
is in table 1, extracted from the following news
fragment:
\[...] tornadoes that destroyed a Geor-
gia motel and killed one person in a
mobile home Sunday night."
For the Infectious Disease Outbreak sce-
nario, the task is to track the spread of epi-
demics of infectious diseases around the world.
The system has to nd the name of the disease,
the time and location of the outbreak, the num-
ber of victims (infected and dead), and type of
victims (e.g., human or animal). The next ex-
ample is a fragment of a disease outbreak report,
and the extracted facts are shown in table 1.
\Ebola fever has killed 156 people, [...],
in Uganda since September."
3 Structure of Events
The complex event structure in Nature scenar-
ios is partly due to the fact that the events are
reported in a scattered manner in the text.
By scattering of events we mean that their
components are not close to each other in the
text, and a typical text contains several related
events. This is partly because the articles are
often in a form of an update, where the latest
reported damages contribute to the total dam-
ages reported earlier, over several locations and
over dierent time spans.
The example in table 2 illustrates scattering
in the Disease scenario. It is a fragment of an
update about a cholera epidemic in Sudan, from
the World Health Organization's (WHO) web
report. The locations are highlighted in italics
and the victim counts are in boldface, to show
the scattering. In this example there are six
separate mentions|partial descriptions of the
event in text|giving the number of infected
and dead victims, in Sudan, and in two loca-
tions within Sudan. Paragraph (1) reports the
number of victims in Sudan, 2549 infected, and
186 dead. In paragraph (2), the focus is shifted
to another location in Sudan, and new numbers
are reported. Paragraph (3) gives the respective
(0) Meningococcal in Sudan
(1) A total of 2 549 cases of meningococcal disease, of which 186 were fatal, was reported to the
national health authorities between 1 January and 31 March 2000.
(2) Bahar aj Jabal State has been most aected to date, with 1 437 cases (including 99 deaths)
reported in the Juba city area.
(3) Other States aected include White Nile (197 cases, 15 deaths), [...]
Table 2: Example of a Disease Outbreak Report
Disease Location Infected Dead
Meningococcal Sudan 2549 186
Bahar aj Jabal State 1437 99
White Nile 197 15
Table 3: Facts from Disease Outbreak Report
numbers for yet another location in Sudan. The
mentions are summarized in table 3.
3.1 Inclusion Relationships
As we frequently observe in the Nature scenar-
ios, the information in the various mentions in
table 2 is overlapping, and the mentions par-
tially include each other.
For example, the numbers for infected victims
in paragraph (2) and (3), contribute to the total
number of infected cases in paragraph (1). The
extraction system should be able to extract all
the numbers for this text. The problem is how
to group these mentions into a template in an
unambiguous and coherent way. It is impossi-
ble to represent an event with overlapping in-
formation in a single template, since it consists
of multiple numbers of victims in several areas
and several time intervals.
For the purpose of handling this phenomenon,
we rst introduce a distinction between out-
breaks and incidents. An incident is a short de-
scription, or a mention, of one occurrence that
relates to an outbreak. It covers a single specic
span of time in a single specic area. An out-
break takes place over a longer period of time,
and possibly over wider geographical area: it
consists of multiple incidents.
In general, one incident may include others,
which give further detailed information.
Therefore, we analyze the news fragment in
table 2 as containing six incidents, with two
types of inclusions: rst, inclusion by status,
where the dead count contributes to the infected
count of the same area, and second, inclusion by
location, where the numbers of infected cases in
Bahar aj Jabal State, in paragraph (2), and in
White Nile, in (3), contribute to the infected
count in Sudan, in paragraph (1).
The Natural Disaster scenario poses further
complications for this schema. The scattering
is complicated by the relationship of causation:
the main disaster triggers derivative disasters
(sub-disasters), which in turn may cause dam-
ages that contribute to the overall damage. This
is illustrated by the news fragment in table 4,
from the New York Times. Names of disasters
are in bold, and the damages are italicized.
In table 4, paragraph (1), a disaster includes
rain and winds, which cause ooding. In para-
graph (3), the human damages caused by snow
are included in the total human damages caused
by the storm in (2). The derivative disasters
and their damages often take place in several lo-
cations, appearing relatively far in the text from
the rst mention of the main disaster. The -
nal logical representation of the event should be
such that the eects of the sub-disasters could
be traced back to the main event.
The following is a summary of the inclusion
relationships found in the two Nature scenarios:
 location: e.g, victim count in one city con-
tributes to the victim count in the whole
country.
 time: e.g. victim count for an update re-
port contributes to the overall victim count
since the beginning of the outbreak.
 status: dead or sick count is included in
(1) A brutal northeaster thrashed the Eastern Seaboard again Thursday with cold, slicing rain
and strong winds that caused ooding in coastal areas of New Jersey and Long Island. [...]
(2) Elsewhere along the East Coast, 19 deaths have been attributed to the storm since it began
on Monday.
(3) The 19 deaths include ve in accidents on snowy roads in Kentucky and two in Indiana. [...]
Table 4: Example of Disaster Reporting
the infected count, as in paragraph (2) of
table 2.
 victim type or descriptor: e.g., \people" in-
cludes \health workers", and \children".
 disease name (Disease scenario): e.g., the
number of Hepatitis C cases may be in-
cluded in the number of Hepatitis cases.
 disaster (Disaster scenario): e.g., damages
caused by rain may be included in the dam-
ages caused by rain and winds.
 causation (Disaster scenario): a disaster
can trigger derivative disasters.
3.2 Type and Distribution of Inclusions
To investigate the extent of inclusions and their
distribution by type, we analyzed 40 documents
related to Nature scenarios.
1
To conrm the feasibility and applicability of
this approach, we manually tagged the inclu-
sion relationships present in these documents.
Table 5 shows the number of incidents found in
the documents, as well as the number and the
types of inclusion. There are also multiple in-
clusions: e.g., infected health workers in a town
in Uganda are included in the total number of
infected people in the whole country: this is in-
clusion by both case-descriptor and location.
Multiple inheritance also occurs: in table 2,
the deaths in Bahar aj Jabal State contribute
to the infected count in that state, as well as to
the total number of deaths in Sudan. However,
in table 5, we show only the inclusion in the
immediately preceding parent.
3.3 Hierarchical Template Structure
Our proposed solution is to have a separate tem-
plate for each incident. Once we have broken
1
The training corpus was used to evaluate the per-
formance of our IE system on these tasks. For the Dis-
aster scenario we analyzed a total of 14 reports from
NYT, ABC, APW, CNN, VOA and WSJ. For Disease
Outbreaks, a total of 26 documents from NYT, Promed,
WHO, and ABC.
Scenario Disease Disaster
Documents 26 14
Words 9 500 6500
Incidents 125 112
Inclusions 57 81
time 6 6
location 19 20
status 19 1
case-descriptor 6 1
case-desc/location 3 {
disease 1 {
causation { 19
causation/location { 11
causation/time { 3
time/location { 7
disaster { 5
disaster/location { 2
damage { 4
others 3 2
Table 5: Type and Number of Inclusion
down the information into smaller incident tem-
plates, the inclusion relationship between them
is indicated by event pointers. This approach
makes it possible to represent the information
in a natural and intuitive way.
The nal template for the Infectious Disease
scenario is shown in table 6. Note that there is
a separate slot indicating the parent incident.
Disease Name
Date
Location
Victim Number
Victim Descriptor
Victim Status
Victim Type
Parent Event
Table 6: Infectious Disease Template
Figure 1: Infectious Disease Outbreak
Figure 2: Natural Disaster
Figure 1 is a graphical representation of the
inclusion relationships among the incidents ex-
tracted from the Disease report in table 2. The
gure shows the main incident with several sub-
incidents. Two of the sub-incidents have, in
turn, sub-incidents. The types of inclusions are
shown in the last row.
Figure 2 shows a graphical representation of
inclusion by causation in Natural Disaster sce-
nario. The incidents are extracted from ta-
ble 4.
2
There is a causation relationship be-
tween the incidents. It is important to recover
the long causation chains from the text.
As a result, the templates are simple, but
2
Note that the northeaster is not in causation rela-
tionship with storm, which began on Monday. The dam-
ages that the synonymous northeaster caused, are from
the following Thursday.
there are typically many templates per docu-
ment. The separation of incidents aects the
process of extraction, since we can now focus
on looking for smaller atomic pieces rst. Then
we must address the problem of linking together
related incidents as a separate problem in the
overall process of IE.
4 Linguistic cues
The process of tracking the inclusion relation-
ships between the incidents is not trivial. A
human reader uses the cohesive devices in the
text to construct the connections between parts
of text (see e.g., (Halliday and Hasan, 1976; Hal-
liday, 1985)). Finding the relationship between
incidents may be a less complex task than track-
ing cohesion through an entire text or discourse.
Our task is limited to nding the cohesive de-
vices connecting a small set of pre-dened facts,
that may occur nearby within one sentence, or
are separated by one or more sentence bound-
aries. Our goal is to locate the cues in the text,
and use them to automatically recover these re-
lationships.
An example of a linguistic cue is in the fol-
lowing fragment of an update from table 4:
Elsewhere along the East Coast, 19
deaths have been attributed to the
storm [...]
Elsewhere indicates a shift in the focus from
one location to another and there is probably
no inclusion between the following and immedi-
ately preceding mention of the damages.
We have identied several linguistic cues that
signal the presence or absence of an inclusion
relationship between two incidents. These cues
can be one of following types:
 Specic lexical items, which can be e.g.,
adverbs, verbs, prepositions, connectives.
Elsewhere in the previous example implies
that damages caused by the following dis-
aster do not contribute to the damages of
the immediately preceding disaster.
 Two expressions in separate incidents
which are related in the scenario-specic
concept hierarchy, may indicate the pres-
ence and also the direction of an inclusion,
e.g., health worker is included in people;
names of plants, animals and terms refer-
ring to human beings, are hyponyms of vic-
tim.
 Locative or temporal expressions that are
in a hierarchical relationship in a location
hierarchy or in the implicit time hierarchy,
often indicate presence or direction of in-
clusion.
 Elliptical elements create cohesion. Ellipsis
indicates the presence of a parent incident
earlier in the text. In paragraph (3) of table
4, in the parent incident we observe a case
descriptor, deaths, which is elided in the
two sub-incidents.
 Anaphora: anaphoric reference usually in-
dicates the absence of an inclusion between
two incidents, merging into one. For exam-
ple, in table 4, paragraph (3), the 19 deaths
is coreferential with 19 deaths caused by the
storm in paragraph (2).
 Coordination tends to indicate the absence
of inclusion relationship. For example,
when two incidents are conjoined by and
and do not share information about loca-
tion or time, there is typically no inclusion.
However, there are cases where other cues
override this general tendency.
These cues often do not appear in isolation,
and they may interact.
We give an example of three lexical items and
their role as an indicator of inclusion in the In-
fectious Disease Outbreak Scenario. Consider
the preposition with
3
, the participle including
and the nite verb include.
\More than 500 cases of dengue hem-
orrhagic fever were reported in Mexico
last year, with 30 deaths, Ruiz said."
The 30 deaths are included in the 500 cases.
The direction of the inclusion is reversed in the
following example:
"Disease has killed 10 persons, with
242 cases having already been re-
ported."
The latter incident includes the former. Here
additional cues are provided by the concept hi-
erarchy, and the numbers: a smaller number
cannot include a larger one.
The following illustrates the participle includ-
ing as cue:
Ebola fever has killed 156 people, in-
cluding 14 health workers, in Uganda
since September.
The incidents are connected by including,
which also indicates the direction explicitly. Ad-
ditional information is obtained from the case-
descriptors, related in the concept hierarchy.
The context for such \trigger" words as they
indicate inclusion, is that the trigger appears
between two incidents, preceding and preceded
3
In the case of with we look only at free prepositions,
that is, those not bound to a preceding verb (Biber et
al., 1999).
by a quantied NP
4
and optional phrases or
items from the concept hierarchy.
Q fcase-descriptor j statusg [reported
j get sick j time j location j disease] [,]
trigger Q fcase-descriptor j statusg
These triggers can indicate inclusion also in-
side a parenthetical phrase, preceding a quanti-
ed NP, as in table 2 in paragraph (2).
The trigger include (as a nite verb) functions
similarly, but can also occur between sentences:
[...] the Ugandan Ministry of Health
has reported [...] 370 cases and 140
deaths. This gure includes 16 new
conrmed cases in Gulu [...]
In our training corpus, when these cue words
occurred in this context, they consistently indi-
cated an event inclusion relation.
5 Discussion
Complexity of a scenario seems to depend of
multiple factors. The notion of complexity,
however, has not been investigated in great
depth. Some research on this was done by
(Bagga and Biermann, 1997; Bagga, 1997),
classifying scenarios according to di?culty by
counting distances between \components" of an
event in the text. In this way it attempts to ac-
count for variation in performance across the
MUC scenarios.
Our analysis suggests that the type and
amount of inclusion relationships depend on
the nature of the topic. In such scenarios as
Management Succession and Corporate Acqui-
sitions, an event usually occurs at one specic
point in time. By contrast, the Nature events
typically take place across a span of time and
space. As the event \travels" and evolves, its
manifestations are reported in a piecewise fash-
ion, sometimes on an hour-by-hour basis.
An extensive linguistic analysis of the cor-
pus is necessary to resolve these complex is-
sues. For evaluation and training, we are build-
ing test and training corpora, totaling 70 doc-
uments and annotated with inclusion relation-
ships.
4
Here the case descriptor or status can be elided:
however, one of quantiers should have a case descriptor
or a status.
Acknowledgments
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Sys-
tems Center San Diego, and by the National Science
Foundation under Grant IIS-0081962.
This paper does not necessarily reect the posi-
tion or the policy of the U.S. Government.
References
A. Bagga and A. W. Biermann. 1997. Analyzing
the complexity of a domain with respect to an
information extraction task. In Proc. 10th Intl.
Conf. on Research on Computational Linguistics
(ROCLING X).
A. Bagga. 1997. Analyzing the performance of
message understanding systems. In Proc. Natu-
ral Language Processing Pacic Rim Symposium
(NLPRS'97).
D. Biber, S. Johansson, G. Leech, S. Conrad, and
E. Finegan. 1999. Longman Grammar of Spoken
and Written English. Longman.
R. Grishman, S. Huttunen, and R. Yangarber. 2002.
Real-time event extraction for infectious disease
outbreaks. In Proc. HLT 2002: Human Language
Technology Conf., San Diego, CA.
R. Grishman. 1995. The NYU system for MUC-
6, or where's the syntax? In Proc. 6th Message
Understanding Conf. (MUC-6), Columbia, MD.
Morgan Kaufmann.
R. Grishman. 1997. Information extraction: Tech-
niques and challenges. In M. T. Pazienza, editor,
Information Extraction. Springer-Verlag, Lecture
Notes in Articial Intelligence, Rome.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman, London.
M.A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London.
L. Hirschman, E. Brown, N. Chinchor, A. Douthat,
L. Ferro, R. Grishman, P. Robinson, and B. Sund-
heim. 1999. Event99: A proposed event indexing
task for broadcast news. In Proc. DARPA Broad-
cast News Workshop, Herndon, VA.
S. Huttunen, R. Yangarber, and R. Grishman. 2002.
Diversity of scenarios in information extraction.
In Proc. 3rd Intl. Conf. of Language Resources
and Evaluation, LREC-2002, Las Palmas de Gran
Canaria, Spain.
1991. Proc. 3th Understanding Conf. (MUC-3).
Morgan Kaufmann.
1992. Proc. 4th Message Understanding Conf.
(MUC-4). Morgan Kaufmann.
1995. Proc. 6th Message Understanding Conf.
(MUC-6). Morgan Kaufmann.
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 29?37,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Assessment of Utility
in Web Mining for the Domain of Public Health
Peter von Etter, Silja Huttunen, Arto Vihavainen,
Matti Vuorinen and Roman Yangarber
Department of Computer Science
University of Helsinki, Finland
First.Last@cs.helsinki.fi
Abstract
This paper presents ongoing work on applica-
tion of Information Extraction (IE) technology
to domain of Public Health, in a real-world
scenario. A central issue in IE is the quality
of the results. We present two novel points.
First, we distinguish the criteria for quality:
the objective criteria that measure correctness
of the system?s analysis in traditional terms
(F-measure, recall and precision), and, on the
other hand, subjective criteria that measure the
utility of the results to the end-user.
Second, to obtain measures of utility, we build
an environment that allows users to interact
with the system by rating the analyzed con-
tent. We then build and compare several clas-
sifiers that learn from the user?s responses to
predict the relevance scores for new events.
We conduct experiments with learning to pre-
dict relevance, and discuss the results and their
implications for text mining in the domain of
Public Health.
1 Introduction
We describe an on-going project for text mining in
the domain of Public Health. The aim of the project
is to build a system for providing decision support
to Public Health (PH) professionals and officials, in
the task of Epidemic Surveillance.
Epidemic surveillance may be sub-divided into
indicator-based vs. event-based surveillance, (Hart-
ley et al, 2010). Whereas the former is based on
structured, quantitative data, which is collected, e.g.,
from national or international clinical laboratories
or databases, and is of reliable quality, the latter
is much more noisy, and relies on ?alert and ru-
mour scanning?, particularly from open-source me-
dia, such as on-line news sites. While the latter
kind of information sources are less reliable over-
all, they nonetheless constitute a crucial channel of
information in PH. This is because the media are ex-
tremely adept at picking up isolated cases and weak
signals?which may be indicative of emergence of
important events, such as an incipient epidemic or
critical change in a public-health situation?and in
many cases they can do so much more swiftly than
official channels. National and supra-national (e.g.,
European-level) Health Authorities require timely
information about threats posed to the public by
emerging infectious diseases and epidemics. There-
fore, these Agencies rely on media-monitoring as a
matter of routine, on a continual basis as part of their
day-to-day operations.
The system described in this paper, PULS, is de-
signed to support Epidemic Surveillance by moni-
toring open-source media for reports about events of
potential significance to Public Health (Yangarber
and Steinberger, 2009). We focus in this paper on
news articles mentioning incidents of infectious dis-
eases. The system does not make decisions, but pro-
vides decision support, by filtering massive volumes
of information and trying to identify those cases that
should be brought to the attention of epidemic intel-
ligence officers (EIO)?public health specialists en-
gaged in epidemic surveillance.
This is an inter-disciplinary effort. The system
builds on methods from text mining and computa-
tional linguistics to identify the items of potential
interest (Grishman et al, 2003). The EIOs, on the
other hand, are medical professionals, and are gen-
erally not trained in computational methods. There-
fore the tools that they use must be intuitive and must
29
not overwhelm the user with volume or complexity.
A convenient baseline for comparison is keyword-
based search, as provided by search engines and
news aggregators. Systems that rely on keyword-
matching to find articles related to infectious threats
and epidemics quickly overwhelm the user with a
vast amount of news items, much of which is noise.
We have tuned PULS, the ?Pattern-based Under-
standing and Learning System,? to support Epidemic
Surveillance in several phases. PULS is a collabo-
rative effort with MedISys, a system for gathering
epidemic intelligence built by the European Com-
mission (EC) at the Joint Research Centre (JRC)
in Ispra, Italy. First, MedISys finds news articles
from thousands of on-line sources around the world,
identifies articles potentially relevant to Epidemic
Surveillance, using a broad keyword-based Web
search, and sends them via an RSS feed to PULS
on a continual basis. Second, PULS employs ?fact-
finding? technology, Information Extraction (IE), to
determine exactly what happened in each article:
who was affected by what disease/condition, where
and when?creating a structured record that is stored
in the database. Articles that do not trigger cre-
ation of a database record are discarded. A third
component then determines the relevance of the se-
lected articles?and cases that they describe?to the
domain of Public Health, specifically to Epidemic
Surveillance.
Traditionally in IE research, performance has
been measured in terms of formal correctness?how
accurately the system is able to analyze the article
(Hirschman, 1998). In this paper we argue the need
for other measures of performance for text mining,
using as a case study our application of Web mining
to the domain of Public Health. In the next section,
we lay down criteria for judging quality, and present
the approach taken in our system. Section 3 out-
lines the organisation of the system, and Section 4
presents in detail our experiments with automatic as-
signment of relevance scores. In the final section we
discuss the results and outline next steps.
2 Criteria for quality
In this section we take a critical view at traditional
measures of quality, in text analysis in general, and
IE in particular. What defines quality most appropri-
ately for our application, and how should we mea-
sure quality? We propose the following taxonomy
of quality in our context:
? Objective: system?s perspective
? Correctness
? Confidence
? Subjective: user?s perspective
? Utility or relevance
? Reliability
At the top level, we distinguish objective vs. sub-
jective measures. Most IE research has focused on
correctness over the last two decades, e.g., in the
MUC and ACE initiatives (Hirschman, 1998; ACE,
2004). Correctness is a measure of how accurately
the system extracts the semantics from an article
of text, in terms of matching the system?s answers
to a set of answers pre-defined by human annota-
tors. In our context, a set of articles is annotated
with a ?gold-standard? set of database records, each
record containing fields like: the name of the dis-
ease/infectious agent, the location/country of the in-
cident, the date of the incident, the number of vic-
tims, whether they are human or animal, whether
they survived, etc. Then the system?s response can
be compared to the gold standard and correctness
can be computed in terms of recall and precision,
F-measure, accuracy, etc.?counting how many of
the fields in each record were correctly extracted.
This approach to quality is similar to the approach
taken in other areas of computational linguistics:
how many structures in the text were correctly iden-
tified, how many were missed, and how many spuri-
ous structures were introduced.
Confidence has been studied as well, to estimate
the probability of the correctness of the system?s an-
swer, e.g., in (Culotta and McCallum, 2004). Our
system computes confidence using discourse-level
cues, (Huttunen et al, 2002): e.g., confidence de-
creases as the distance between event trigger and
event attributes increases?the sentence that men-
tions that someone has fallen ill or died is far from
the mention of the disease. Confidence also de-
pends on uniqueness of attributes?e.g., if a doc-
ument mentions only one country, the system has
30
more confidence that an event referring to this coun-
try is correct.
On the subjective side, utility, or relevance, asks
how useful the result is to the user. There are several
points to note. First, it is clearly a highly subjective
measure, not easy to capture in exact terms. Sec-
ond, it is ?orthogonal? to correctness in the sense
that from the user?s perspective utility matters irre-
spective of correctness. For example, an extracted
case can be 100% correct, yet have very low utility
to the user, (for the task of epidemic surveillance)?
a perfectly extracted event that happened too long
ago would not matter in the current context. Con-
versely, every slot in the record may be extracted
erroneously, and yet the event may be of great im-
portance and value to the user. We focus specifically
on relevance vs. correctness.
Given the current performance ?ceilings? of 70-
80% F-measure in state-of-the-art IE, what does cor-
rectness of x% mean in practice? It likely means
that if x > y then a system achieving F-measure
x is better to have than one achieving y. But what
does it say about utility? In the best case, correct-
ness may be correlated with utility, in the worst case
it is independent of utility (e.g., if the system hap-
pens to achieve high correctness on events from the
past, which have low relevance). Since we are tar-
geting a specific user base, the user?s perspective
must be taken into account when estimating quality,
not (only) the system?s perspective. This implies the
need for automatic assignment of relevance scores
to analyzed events or documents.
Finally, reliability measures whether the reported
event is ?true?. The relevance of extracted fact may
be high, but is it credible? Can the information be
trusted? We list this criterion for quality for com-
pleteness, since it is the ultimate goal of any surveil-
lance process. However, answering this requires a
great deal of knowledge external to the system, that
can only be obtained by the human user through a
detailed down-stream verification process. The sys-
tem may provide some support for determining reli-
ability, e.g., by tracking the performance of different
information sources over time, since the reliability
of the facts extracted from an article is related to the
reliability of the source. It may be possible to clas-
sify Web-based sources according to their credibil-
ity; some sources may habitually withhold informa-
tion (for fear of impact to tourism, trade, etc.); other
sites may try to attract readership by exaggerated
claims (e.g., tabloids). On the other hand, clearly
disreputable sites may carry true information. This
measure of quality is beyond the scope of this paper.
3 The System: Background
PULS, the Pattern-based Understanding and Learn-
ing System, is developed at the University of
Helsinki to extract factual information from plain
text. PULS has been adapted to analyse texts for
Epidemic Surveillance.1
The components of PULS have been described
in detail previously, (Yangarber and Steinberger,
2009; Steinberger et al, 2008; Yangarber et al,
2007). In several respects, it is similar to other
existing systems for automated epidemic surveil-
lance, viz., BioCaster (Doan et al, 2008), MedISys
and PULS (Yangarber and Steinberger, 2009),
HealthMap (Freifeld et al, 2008), and others (Linge
et al, 2009).
PULS relies on EC-JRC?s MedISys for IR (in-
formation retrieval)?MedISys performs a broad
Web search, using a set of boolean keyword-based
queries, (Steinberger et al, 2008). The result is
a continuous stream of potentially relevant docu-
ments, updated every few minutes. Second, an IE
component, (Grishman et al, 2003; Yangarber and
Steinberger, 2009), analyzes each retrieved docu-
ment, to try to find events of potential relevance
to Public Health. The system stores the struc-
tured information about every detected event into a
database. The IE component uses a large set of lin-
guistic patterns, which in turn depend on a large-
scale public health ontology, similar to MeSH,2 that
contains concepts for diseases and infectious agents,
infectious vectors and animals, medical drugs, and
geographic locations.
From each article, PULS?s pattern matching en-
gine tries to extract a set of incidents, or ?facts??
detailed information related to instances of disease
outbreak. An incident is described by a set of fields,
or attributes: location and country of the incident,
disease name, the date of the incident, information
about the victims?their type (people, animals, etc.),
1puls.cs.helsinki.fi/medical
2www.nlm.nih.gov/mesh
31
number, whether they survived or died, etc.
The result of IE is a populated database of ex-
tracted items, that can be browsed and searched by
any attribute, according to the user?s interests. It is
crucial to note that the notion of a user?s focus or
interest is not the same as the notion of relevance,
introduced above. We take the view that the notion
of relevance is shared among the entire PH commu-
nity: an event is either relevant to PH or it is not.
Note also, that this view is upheld by several classic,
human-moderated PH surveillance systems, such as
ProMED-Mail3 or Canadian GPHIN. User?s inter-
est is individual, e.g., a user may have specific ge-
ographic, or medical focus (e.g., only viral or tropi-
cal illnesses), and given the structured database, s/he
can filter the content according to specific criteria.
But that is independent of the shared notion of rele-
vance to PH. User focus can be exploited for targeted
recommendation, using techniques such as collabo-
rative filtering; at present, this is beyond the scope
of our work.
The crawler and IE components have been in op-
eration and under refinement for some time. We next
build a classifier to assign relevance scores to each
extracted event and matched document.
4 Experimental Setup
We now present the work on automatic classification
of relevance scores. In collaboration with the end-
users, we defined guidelines for judging relevance
on a 6-point scale, summarized in Table 1.
Criteria Score
New information, highly relevant 5
Important updates, 4
on-going developments
Review of current events, 3
potential risk of disease
Historical/non-current events 2
Background information
Non-specific, non-factive events, 1
secondary topics, scientific studies
hypothetical risk
Unrelated to PH 0
Table 1: Guidelines for relevance scores in medical news
3www.promedmail.org
Note, the separation between the ?high-
relevance? scores, 4 and 5, vs. the rest; this
split is addressed in detail in Section 4.3.
4.1 Discourse features
It is clear that these guidelines are highly subjec-
tive, and cannot be encoded by rules directly. In
order to model the relevance judgements, we ex-
tracted features?the discourse features?from the
document that are indicative of, or mappable to,
the relevance scores. Discourse features try to cap-
ture higher-order information, including complex
and longer-range inter-dependencies and clues, in-
volving the physical layout of the document, and
deeper semantic and conceptual information found
in the document. Some examples of discourse fea-
tures are:
? Relative-position, which is represented by a
number from zero to 1 indicating the propor-
tion of the document one needs to read to reach
the event text;
? Disease-in-header is a binary value that indi-
cates whether the disease is mentioned in the
headline or the first two sentences;
? Disease-to-trigger-distance indicates how far
the disease is from the trigger sentence (same
as for confidence computation);
? Recency is the number of days between the re-
ported occurrence of the event and the publica-
tion date;
We compiled over two dozen discourse-level fea-
tures. It is clear that the discourse features do not
determine the relevance scores, but provide weak
indicators of relevance, so that probabilistic classi-
fication is appropriate. For example, a higher rel-
ative position of an event probably indicates lower
relevance, but there are often news summary arti-
cles that gather many unrelated news together, and
may contain very important items anywhere in the
article.4 A feature such as Victim-named, stating
whether the victim?s name is mentioned, often in-
dicates lower-relevance events (obituaries, stories
4Due to space limitations, we do not provide a detailed list
of the discourse features.
32
about public personalities, etc.). However, some-
times news articles about disease outbreaks deliber-
ately personify the victims, to give the reader a sense
of their background, lifestyle, to speculate about the
victims? common circumstances.
We describe two classifiers we have built for rel-
evance. A Naive Bayes classifier (NB) was used as
the baseline. We then tried to obtain improved per-
formance with Support Vector Machines (SVM).
4.2 Data
The dataset is the database of facts extracted by the
system. The system pre-assigns relevance to each
event, and users have the option to accept or cor-
rect the system?s relevance score, through the User
Interface, which also allows the users to correct er-
roneous fills, e.g., if a country, disease name, etc.,
was extracted incorrectly by the system.
Along with the users, members of the develop-
ment team also evaluated a sample of the extracted
events, and corrected relevance and erroneous fills.
The developers are computer scientists and linguists,
whereas the users are medics, and because they in-
terpreted the guidelines differently this had an im-
pact on the results, described in Tables 2 and 5.
?Cleaned data?: PULS?s user interface also per-
mits users to correct incorrect fills in the events (in
the two rightmost columns in the tables). This al-
lowed us to obtain two parallel sets of examples
with relevance labels: the raw examples, as they
were automatically extracted by the system, and the
?cleaned? examples, after users/developer correc-
tions. The raw set is more noisy, since it contains er-
rors introduced by the system. We used the cleaned
examples to train our classifiers, and tested them on
both the cleaned set and the raw set. Testing against
the cleaned set gives an ?idealized? performance, (as
if the IE system made no errors in analysis). True
performance is expected be closer to testing on the
raw set.
In total, there were just under 1000 examples la-
beled by the users and the developers (some exam-
ples were labeled by both, since the system allows
multiple users to attach different relevance judge-
ments to the same example. Most of the time
users agreed on the relevance judgements, but non-
developers were less likely to clean examples.)
4.3 Naive Bayes classifier
Initially, we planned to perform regression to the
complete [0?5] relevance scale. However, this
proved problematic, since the amount of labeled data
was not sufficient to cover the continuum between
highly relevant and not-so-relevant items. We there-
fore decided instead to build a binary classifier. This
decision is also justified in the context of our sys-
tem?s user interface, which provides the users with
two views:
? the Front Page View contains only high-
relevance items (rated 4 or 5), in case the user
wants to see only the most urgent items first;
? the Complete View shows the user all extracted
items, irrespective of relevance. (The user can
always filter the database by relevance value.)
Thus, the relevance score is also used to guide
a binary decision: whether to present a given
event/article to the user on the Front-Page View. The
NB classifier using the entire set of discourse fea-
tures did not perform well, because the discourse
features we have implemented are inherently not in-
dependent, which affects the performance of NB.
To try to reduce the mutual dependence among
the features, we added a simple, greedy feature-
selection phase during training. Feature selection
starts by training a classifier on the full set of fea-
tures, using leave-one-out (LOO) cross-validation to
estimate the classifier?s performance. In the next
phase, the algorithm in turn excludes the features
one by one, and runs the LOO cross-validation
again, once with each feature excluded. The feature
whose exclusion gives rise to the biggest increase in
performance is dropped out, and the selection step is
repeated with the reduced set of features. We con-
tinue to drop features until performance does not in-
crease for several iterations; in our experiments, we
used three steps beyond the top performance. We
then back up to the step that yielded peak perfor-
mance. The resulting subset of features is used to
train the final NB classifier.
The NB classifier is implemented in R Language.
Because relevance prediction is difficult for all
events, we also tried to predict the relevance of an
article, making the simplifying assumption that the
article is only as relevant as the first event found in
33
the article.5 The results are presented in Table 2.
The rows labeled Dev only refer to the data sets la-
beled by developers, and Users only to sets labeled
by (non-developer) users.
Testing on Number examples
Clean Raw Clean Raw
Event-level
Dev only 76.96 76.66 560 510
All 72.19 73.34 863 799
Users only 70.38 66.53 303 289
Document-level
Dev only 80.41 79.00 291 281
All 73.94 72.45 545 530
Users only 65.82 67.09 238 232
Table 2: Naive Bayes prediction accuracy
The event-level classification is shown in the top
portion of the table. Throughout, as expected, test-
ing on the cleaned data usually gives slightly bet-
ter (more idealized) performance estimates than test-
ing on the raw. Also, as expected, testing on
the first-only events (document-level) gives slightly
better performance, since it?s a simpler problem?
although there is less data to train/test on.
It is important to observe that using data la-
beled by developers gives significantly higher per-
formance. This is because coercing the users to fol-
low the guidelines strictly is not possible, and they
deviate from the rules that they themselves helped
articulate. The rows labeled ?all? show performance
when all combined available data was used?labeled
by both the developers and the users.
This performance is quite good for a baseline.6
The confusion matrices?for the developer-only
event-level raw data set?show the distribution of
true/false positives/negatives.
4.4 SVM Classifier
For comparison, we built two additional classifiers
using the SVMLight Toolkit.7 We first used a linear
5A manual check confirmed that there were no instances
where the first event in an article had lower relevance than a
subsequent event.
6Consider for comparison, that the correctness on a manu-
ally constructed, non-hidden set of articles used for system de-
velopment, is under 75% F-measure.
7http://svmlight.joachims.org/
True Labels
Predicted labels 4-5 0-3
High-relevance 4-5 125 77
Low-relevance 0-3 42 266
Table 3: NB confusion matrix
kernel as a baseline, and used a RBF kernel, which
is potentially more expressive. The conditions for
testing the SVM classifiers were same as the ones
for the NB classifiers, and same datasets were used
as for the NB.
As SVM with the RBF kernel can use non-linear
separating hyperplanes in the original feature space
by using the kernel trick (Aizerman et al, 1964),
we aimed to test whether it would provide an im-
provement over the linear kernel. (For more detailed
discussions of SVM and different kernel functions
for text classification, cf., for example, (Joachims,
1998).)
To regularize the input for SVM, all feature val-
ues were normalized to lie between 0 and 1 (for
continuous-valued features), and set to 0 or 1 for
binary features. Table 4 describes the accuracy
achieved with the linear kernel. Experiments labeled
All discourse features use the complete set of dis-
course features (over 20 features). Rows labeled Se-
lected discourse features show results from training
with exactly same features as resulted from the fea-
ture selection phase of NB.
Event-level Document-level
Clean Raw Clean Raw
All discourse features
Dev only 75.33 77.17 76.87 76.56
All 71.60 72.26 70.51 69.96
Selected discourse features only
Dev only 76.07 77.95 77.94 77.62
All 71.40 72.14 69.75 69.37
Table 4: SVM prediction accuracy using linear kernel
The difference when training with selected dis-
course features and all discourse features is not
large, since SVM is able to distinguish between rel-
evant and non-relevant features fairly well. The re-
sults from SVM using linear kernel appear compa-
34
rable with the results from the NB.
In addition to using the discourse features, we
also tried using lexical features. The lexical fea-
tures for a given example?extracted event?is sim-
ply the bag of words from the sentence containing
the event, plus the two surrounding sentences. To
reduce data sparsity, the sentences are pre-processed
by a lemmatizer, and passed through a named en-
tity (NE) recognizer (Grishman et al, 2003), which
replaces persons, organizations, locations and dis-
ease names with a special token indicating the NE?s
class. ?Stop-word? parts of speech were dropped?
prepositions, conjunctions, and articles.
Event-level Document-level
Clean Raw Clean Raw
All discourse features
Dev only 74.69 75.37 77.93 78.38
All 69.58 70.26 71.56 71.25
Selected discourse features only
Dev only 77.51 79.01 79.19 79.04
All 72.02 72.84 72.59 72.30
Lexical features only
Dev only 75.93 76.37 79.11 80.07
All 73.28 73.47 74.53 74.71
Lexical and selected discourse features
Dev only 78.87 79.24 82.66 81.83
All 76.48 76.58 76.52 76.19
Table 5: SVM prediction accuracy using RBF kernel
The performance of SVM with the RBF kernel
is strongly dependent on the values of SVM pa-
rameters C?the trade-off between training error
and margin? and ??the kernel width (Joachims,
1998). We tuned these parameters manually by
checking a grid of values against a development
dataset, and finding areas where the SVM performed
well. These areas were then further investigated. Af-
ter trying 40 combinations, we set C as 10000 and ?
to 0.001 for subsequent evaluations. The results for
SVM using RBF kernel are given in Table 5.
High accuracy of lexical features alone was some-
what surprising as lexical features consist only of the
bag of words in the event-bearing sentence, plus the
preceding and the following sentences. News arti-
cles often have various pieces of information related
to the event scattered around the document. For
example, the disease can appear only in the head-
line, the location/country in the middle of the doc-
ument, and the event-bearing sentence in a third lo-
cation, (Huttunen et al, 2002). Our lexical features,
as presented here, are not capable of capturing such
long-distance relationships.
The observed difference in performance on rele-
vance prediction between the data sets labeled by de-
velopers vs. non-developer users, likely arises from
the fact that developers follow the formal guidelines
more strictly (being computer scientists). Rows la-
beled all show performance against data sets la-
beled by real users, who work in different PH orga-
nizations in several different countries, each group
of users intuitively following their own, subjective
guidelines, despite the common guidelines agreed-
upon for this project. There may also be deviation
within organizations. For example, certain doctors
may find specific diseases or locations more inter-
esting, giving events containing them a high rele-
vance, thus injecting personal preference into docu-
ment relevance.
5 Discussion and Conclusions
The SVM performs somewhat better than the Naive
Bayes classifier, though there is still much to be ex-
plored and improved. One odd effect is that some-
times testing on the raw data gives slightly better
results than testing on the clean data, though this
is probably not significant, since the SVM classi-
fier is still not finely tuned (and the data contain
some noise). Using all discourse features performs
slightly worse than using a reduced set of features?
the same set of features that we obtained through
greedy feature selection for NB.
Although the lexical features alone seem to do
somewhat worse than the discourse features alone on
event-level classification, we still see that the lexical
features contain a great deal of information (which
the NB cannot use). As expected, adding the dis-
course features improves performance over lexical
features alone, since discourse features capture in-
formation about long-range dependencies that local
lexical features do not.
In forming splits for cross-validation or LOO, we
made sure not to split examples from the same doc-
35
ument across the training and test sets. That is, for
a given document, all events in it are either used for
training or for testing, to avoid biasing the testing.
To summarize, the points addressed in this paper:
? We have presented a language-technology-
based approach to a problem in Public Health,
specifically the problem of event-based epi-
demic surveillance through monitoring on-line
media.
? The user?s perspective needs to be taken into
account when estimating quality, not just the
system?s perspective. Utility to the user is at
least as important as (if not more important
than) correctness.
? We have presented an operational system that
suggests articles potentially relevant to the user,
and assigns relevance scores to each extracted
event.
? For now, we assume the users share same no-
tion of relevance of an event to Public Health.
? We have presented experiments and an initial
evaluation of assignment of relevance scores.
? Experiments indicate that relevance appears
to be a tractable measure of quality, at
least in principle. Marking document-level
relevance?only for the first event in the
document?appears to be easier. However,
making real users follow strict guidelines is dif-
ficult in practice.
On-going work includes refining the classification
approaches, especially, using Bayesian networks, re-
gression, using transductive SVMs to leverage unla-
beled data, and exploring collaborative filtering to
address users? individual interests.
Acknowledgments
This research was supported in part by: the Tech-
nology Development Agency of Finland (TEKES),
through the ContentFactory Project, and by the
Academy of Finland?s National Centre of Excel-
lence ?Algorithmic Data Analysis (ALGODAN).?
References
ACE. 2004. Automatic content extraction.
M. A. Aizerman, E. A. Braverman, and L. Rozonoer.
1964. Theoretical foundations of the potential func-
tion method in pattern recognition learning. In Au-
tomation and Remote Control, volume 25, pages 821?
837.
Aron Culotta and Andrew McCallum. 2004. Confi-
dence estimation for information extraction. In Pro-
ceedings of Human Language Technology Conference
and North American Chapter of the Association for
Computational Linguistics.
Son Doan, Quoc Hung-Ngo, Ai Kawazoe, and Nigel Col-
lier. 2008. Global Health Monitor?a web-based sys-
tem for detecting and mapping infectious diseases. In
Proceedings of the International Joint Conference on
Natural Language Processing (IJCNLP).
C.C. Freifeld, K.D. Mandl, B.Y. Reis, and J.S. Brown-
stein. 2008. HealthMap: Global infectious disease
monitoring through automated classification and visu-
alization of internet media reports. Journal of Ameri-
can Medical Informatics Association, 15:150?157.
Ralph Grishman, Silja Huttunen, and Roman Yangarber.
2003. Information extraction for enhanced access to
disease outbreak reports. Journal of Biomedical Infor-
matics, 35(4):236?246.
David Hartley, Noele Nelson, Ronald Walters, Ray
Arthur, Roman Yangarber, Larry Madoff, Jens Linge,
Abla Mawudeku, Nigel Collier, John Brownstein, Ger-
main Thinus, and Nigel Lightfoot. 2010. The land-
scape of international event-based biosurveillance.
Emerging Health Threats Journal, 3(e3).
Lynette Hirschman. 1998. Language understanding eval-
uations: Lessons learned from muc and atis. In Pro-
ceedings of the First International Conference on Lan-
guage Resources and Evaluation (LREC), pages 117?
122, Granada, Spain, May.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Complexity of event structure in information
extraction. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), Taipei, August.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML: European Conference on Machine
Learning, pages 137?142.
J.P. Linge, R. Steinberger, T.P. Weber, R. Yangarber,
E. van der Goot, D.H. Al Khudhairy, and N.I. Stil-
ianakis. 2009. Internet surveillance systems for early
alerting of health threats. Eurosurveillance Journal,
14(13).
Ralf Steinberger, Flavio Fuart, Erik van der Goot, Clive
Best, Peter von Etter, and Roman Yangarber. 2008.
36
Text mining from the web for medical intelligence. In
Domenico Perrotta, Jakub Piskorski, Franoise Soulie?-
Fogelman, and Ralf Steinberger, editors, Mining Mas-
sive Data Sets for Security. OIS Press, Amsterdam, the
Netherlands.
Roman Yangarber and Ralf Steinberger. 2009. Auto-
matic epidemiological surveillance from on-line news
in MedISys and PULS. In Proceedings of IMED-
2009: International Meeting on Emerging Diseases
and Surveillance, Vienna, Austria.
Roman Yangarber, Clive Best, Peter von Etter, Flavio
Fuart, David Horby, and Ralf Steinberger. 2007.
Combining information about epidemic threats from
multiple sources. In Proceedings of the MMIES
Workshop, International Conference on Recent Ad-
vances in Natural Language Processing (RANLP
2007), Borovets, Bulgaria, September.
37
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 29?37,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Event representation across genre
Lidia Pivovarova, Silja Huttunen and Roman Yangarber
University of Helsinki
Finland
Abstract
This paper describes an approach for investi-
gating the representation of events and their
distribution in a corpus. We collect and
analyze statistics about subject-verb-object
triplets and their content, which helps us com-
pare corpora belonging to the same domain
but to different genre/text type. We argue that
event structure is strongly related to the genre
of the corpus, and propose statistical proper-
ties that are able to capture these genre differ-
ences. The results obtained can be used for the
improvement of Information Extraction.
1 Introduction
The focus of this paper is collecting data about
certain characteristics of events found in text, in
order to improve the performance of an Infor-
mation Extraction (IE) system. IE is a tech-
nology used for locating and extracting specific
pieces of information?or ?facts??from unstruc-
tured natural-language text, by transforming the
facts into abstract, structured objects, called events.
In IE we assume that events represent real-world
facts and the main objective is to extract them from
plain text; the nature of the events themselves rarely
receives in-depth attention in current research.
Events may have various relationships to real-
world facts, and different sources may have contra-
dictory views on the facts, (Saur?? and Pustejovsky,
2012). Similarly to many other linguistic units, an
event is a combination of meaning and form; the
structure and content of an event is influenced by
both the structure of the corresponding real-world
fact and by the properties of the surrounding text.
We use the notion of scenario to denote a set
of structured events of interest in a real-world do-
main: e.g., the MUC Management Succession sce-
nario, (Grishman and Sundheim, 1996), within the
broader Business domain.
The representation and the structure of events in
text depends on the scenario. For example, Huttunen
et al (2002a; Huttunen et al (2002b) points out that
?classic? MUC scenarios, such as Management Suc-
cession or Terrorist Attacks, describe events that oc-
cur in a specific point in time, whereas other sce-
narios like Natural Disaster or Disease Outbreak
describe processes that are spread out across time
and space. As a consequence, events in the latter,
?nature?-related scenarios are more complex, may
have a hierarchical structure, and may overlap with
each other in text. Linguistic cues that have been
proposed in Huttunen et al (2002a) to identify the
overlapping or partial events include specific lexical
items, locative and temporal expressions, and usage
of ellipsis and anaphora.
Grishman (2012) has emphasized that for fully
unsupervised event extraction, extensive linguistic
analysis is essential; such analysis should be able
to capture ?modifiers on entities, including quan-
tity and measure phrases and locatives; modifiers on
predicates, including negation, aspect, quantity, and
temporal information; and higher-order predicates,
including sequence and causal relations and verbs of
belief and reporting.? It is clear that such sophisti-
cated linguistic analysis increases the importance of
text style and genre for Information Extraction.
29
The idea of statistical comparison between text
types goes back at least as far as (Biber, 1991). It
was subsequently used in a number of papers on au-
tomatic text categorization (Kessler et al, 1997; Sta-
matatos et al, 2000; Petrenz and Webber, 2011).
Szarvas et al (2012) studied the linguistic cues
indicating uncertainty of events in three genres:
news, scientific papers and Wikipedia articles. They
demonstrate significant differences in lexical usage
across the genres; for example, such words as fear
or worry may appear relatively often in news and
Wikipedia, but almost never in scientific text. They
also investigate differences in syntactic cues: for
example, the relation between a proposition and a
real-word fact is more likely to be expressed in the
passive voice in scientific papers (it is expected),
whereas in news the same words are more likely ap-
pear in the active.
Because events are not only representations of
facts but also linguistic units, an investigation of
events should take into account the particular lan-
guage, genre, scenario and medium of the text?i.e.,
events should be studied in the context of a particu-
lar corpus. Hence, the next question is how corpus-
driven study of events should be organized in prac-
tice, or, more concretely, what particular statistics
are needed to capture the scenario-specific charac-
teristics of event representation in a particular cor-
pus, and what kind of markup is necessary to solve
this task. We believe that answers to these questions
will likely depend on the ultimate goals of event de-
tection. We investigate IE in the business domain?
thus, we believe that preliminary study of the corpus
should use exactly the same depth of linguistic anal-
ysis as would be later utilized by the IE system.
2 Problem Statement
2.1 Events in the Business domain
We investigate event structure in the context of
PULS,1 an IE System, that discovers, aggregates,
verifies and visualizes events in various scenarios.
This paper focuses on the Business domain, in which
scenarios include investments, contracts, layoffs and
other business-related events, which are collected in
a database to be used for decision support. In the
Business domain, PULS currently handles two types
1More information is available at: http://puls.cs.helsinki.fi
Figure 1: Distributions of document length in the news
and business analysts? reports corpora
of documents: news reports and short summaries
written by professional business analysts. Thus,
events extracted from both corpora relate to approx-
imately the same real-world facts.
Both corpora are in English (though some of the
analysts? reports are based on news articles written
in other languages). We collected a corpus of re-
ports containing 740 thousand documents over three
years 2010-2012, and a news corpus containing 240
thousand documents over the same period.
The two corpora demonstrate significant linguis-
tic differences. First, the documents have different
length: the average length of an analyst reports is 5.5
sentences including the title, and 80% of the docu-
ments have length between 4 and 7 sentences, (see
Figure 1). News articles are on average 19 sentences
long?and much more varied in length.
The topical structure is also quite different for the
two corpora. Each analyst report is most typically
dedicated to a particular single real-world event.
Also, the reports tend to have a standardized, formu-
laic structure. The analysts who generate these re-
ports tend to follow a specific, strict style and struc-
ture over time.
By contrast, documents in the news corpus are
much more heterogeneous. These texts can follow
a wide variety of different styles?short messages,
surveys, interviews, etc. News documents can focus
not only strictly on business events but on related
topics as well. For example, political events have
complex interaction with and impact on business ac-
30
tivity, and therefore political news frequently appear
in business news feeds.
PULS aims to use the same processing chain for
various types of input documents. One key goal of
the current work is to investigate whether different
IE processing approaches are needed for documents
belonging to different text types, as exemplified by
analyst reports vs. articles from news feeds.
To summarize, the goals of the present work are:
? investigate how text genre influences event rep-
resentation;
? find formal markers able to capture and mea-
sure the differences in corpus style/genre;
? propose a methodology for adaptating an IE
system to a different text genre.
2.2 System Description
In this section we describe how the IE system is used
in a ?pattern-mining mode,? to address the afore-
mentioned problems.
PULS is a pipeline of components, including:
a shallow parser/chunker; domain ontologies and
lexicons; low-level patterns for capturing domain-
specific entities and other semantic units, such as
dates and currency expressions; higher-level pat-
terns for capturing domain-specific relations and
events; inference rules, which combine fragments of
an event that may be scattered in text?that a pattern
may not have picked up in the immediate context
(e.g., the date of the event); reference resolution for
merging co-referring entities and events.
The ontology and the lexicon for the Business do-
main encode the taxonomic relations and support
merging of synonyms: e.g., the ontology stores the
information that cellphone and mobile phone are
synonymous, and that a super-concept for both is
PRODUCT.
Low-level patterns are used to extract entities
from text, such as company names, dates, and lo-
cations. On a slightly higher level, there are pat-
terns that match contexts such as range (collection,
line, etc.) of X and assign them the type of X. For
instance, the phrase a collection of watches would
be assigned semantic type watch, etc. The top-level
patterns in all IE scenarios are responsible for find-
ing the target events in text.
In the pattern-mining mode we use the gen-
eral pattern SUBJECT?VERB?OBJECT, where the
components may have any semantic type and are
constrained only by their deep syntactic function?
the system attempts to normalize many syntactic
variants of the basic, active form: including passive
clauses, relative clauses, etc.2
The idea of using very simple, local patterns
for obtaining information from large corpora in
the context of event extraction is similar to work
reported previously, e.g., the bootstrapping ap-
proaches in (Thelen and Riloff, 2002; Yangarber et
al., 2000; Riloff and Shepherd, 1997). Here, we
do not use iterative learning, and focus instead on
collecting and analyzing interesting statistics from
a large number of S-V-O patterns. We collected
all such ?generalized? S-V-O triplets from the cor-
pus and stored them in a database. In addition to
the noun groups, we save the head nouns and their
semantic classes. This makes it easy to use sim-
ple SQL queries to count instances of a particular
pattern, e.g., all objects of a particular verb, or all
actions that can be applied to an object of seman-
tic class ?PRODUCT.? For each triplet the database
stores a pointer the original sentence, making it pos-
sible to analyze specific examples in their context.
In the next two sections we present the statis-
tics that we collected using the pattern-mining
mode. This information reflects significant differ-
ences among the corpora genres and can be used to
measure variety of genre. We believe that in the fu-
ture such data analysis will support the adaptation of
PULS to new text genres.
3 Statistical Properties of the Corpora
3.1 Personal pronouns
Pronouns play a key role in anaphoric relations; the
more pronouns are present in the corpus, the more
crucial anaphora resolution becomes. Analysis of
relationships between frequencies of personal pro-
nouns in text and the genre of the text is not new;
it has been observed and studied extensively, going
2By normalization of syntactic variants we mean, for in-
stance, that clauses like ?Nokia releases a new cellphone? (ac-
tive), ?a new cellphone is released by Nokia? (passive), ?a new
cellphone, released by Nokia,...? (relative), etc., are all reduced
to the same S-V-O form.
31
Reports News
Pronoun Object Subject Object Subject
I/me 0.003 0.007 0.2 1.0
we/us 0.001 0.001 0.4 1.7
you 0.002 0.003 0.3 0.8
he/him 0.05 0.4 0.6 2.2
she/her 0.007 0.05 0.1 0.5
they/them 0.3 0.6 0.8 1.3
it 1.1 2.6 1.5 2.3
Total 1.5 3.6 4.0 9.8
Table 1: Personal pronouns appearing in the subject or
object position in the corpora. The numerical values are
proportions of the total number of verbs.
back as far as, e.g., (Karlgren and Cutting, 1994).
The analysis of pronoun distribution in our corpora
is presented in Table 1, which shows the proportions
of personal pronouns, as they appear in subject or
object position with verbs in the collected triples.
The numbers are relative to the count of all verb to-
kens in the corpus, i.e., the total number of the S?V?
O triplets extracted from the corpus. The total num-
ber of triplets is approximately 5.7M in the report
corpus and 11M in the news corpus.
It can be seen from Table 1 that personal pro-
nouns are much more rare in the report corpus than
in the news corpus. Only 1.5% of verbs in the re-
ports corpus have a pronoun as an object, and 3.6%
as a subject. By contrast, in the news corpus 4%
of verbs have a personal pronoun as an object, and
9.8% as a subject. This corresponds to the observa-
tion in (Szarvas et al, 2012), that ?impersonal con-
structions are hardly used in news media.?
It is interesting to note the distribution of the par-
ticular pronouns in the two corpora. Table 1 shows
that it is the most frequent pronoun, they and he are
less frequent; the remaining pronouns are much less
frequent in the report corpus, whereas in the news
the remaining personal pronouns have a much more
even distribution. This clearly reflects a more re-
laxed style of the news that may use rhetorical de-
vices more freely, including citing direct speech and
use a direct addressing the reader (you). It is also
interesting to note that in the third-person singular,
the feminine pronoun is starkly more rare in both
corpora than the masculine, but roughly twice more
rare among the analyst reports.
Reports News
Subject Object Subject Object
All 21.8 6.6 14.6 6.5
Business 27.1 8.1 20.1 9.5
Table 2: Distribution of proper names as subjects and ob-
jects, as a proportion the total number of all verbs (top
row) vs. business-related verbs (bottom row).
3.2 Proper Names
Proper names play a crucial role in co-reference res-
olution, by designating anaphoric relations in text,
similarly to pronouns. In the Business domain, e.g.,
a common noun phrase (NP) may co-refer with a
proper name, as ?the company? may refer to the
name of a particular firm. A correctly extracted
event can be much less useful for the end-user if it
does not contain the specific name of the company
involved in the event.
A verbs is often the key element of a pattern that
indicates to the IE system the presence of an event
of interest in the text. When the subject or ob-
ject of the verb is a common NP, the corresponding
proper name must be found in the surrounding con-
text, using reference resolution or domain-specific
inference rules. Since reference resolution is itself
a phase that contributes some amount of error to
the overall IE process, it is natural to expect that if
proper-name subjects and objects are more frequent
in the corpus, then the analysis can be more precise,
since all necessary information can be extracted by
pattern without the need for additional extra infer-
ence. Huttunen et al (2012) suggests that the com-
pactness of the event representation may be used as
one of the discourse cues that determine the event
relevance.
Table 2 shows the percentage of proper name ob-
jects and subjects for the two corpora. Proper-name
objects have comparable frequency in both corpora,
though proper-name subjects appear much more fre-
quently in analyst reports than in news. Further-
more, for the business verbs, introduced below in
section 4.1?i.e., the specific set of verbs that are
used in event patterns in the Business scenarios?as
seen in the second row of the table?proper-name
objects and subjects are more frequent still. This
suggests that business events tend to mention proper
names.
32
Percentage of business verbs
Corpus Total Title 1st sentence
Reports 49.5 7.6 13.8
News 31.8 0.6 1.1
Table 3: Business verbs in analyst reports and news cor-
pora, as a proportion of the total number of verbs.
4 Business Verbs
4.1 Distribution of Business verbs
The set of business-related verbs is an important part
of the system?s domain-specific lexicon for the Busi-
ness domain. These verbs are quite diverse: some
are strongly associated with the Business domain,
e.g., invest; some are more general, e.g., pay, make;
many are ambiguous, e.g., launch, fire. Inside ana-
lyst reports these verbs always function as markers
of certain business events or relations. The verbs
are the key elements of the top-level patterns and it
is especially crucial to investigate their usage in the
corpora to understand how the pattern base should
be fine-tune for the task.
Since the majority of these verbs fall in the am-
biguous category, none of these verbs can by them-
selves serve a sufficient indicator of the document?s
topic. Even the more clear-cut business verbs, such
as invest, can be used metaphorically in the non-
business context. However, their distribution in the
particular document and in the corpus as a whole can
reflect the genre specificity of the corpus.
Table 3 shows the overall proportion of the busi-
ness verbs, and their proportion in titles and in the
first sentence of a documents. It suggests that almost
50% of the verbs in the report corpus are ?business?
verbs, and almost half of them are concentrated in
the beginning of a document. By contrast, the frac-
tion of business verbs in the news corpus is less than
one third and they are more scattered through the
text. This fact is illustrated by the plot in Figure 2.
The first sentence is often the most informa-
tive part of text, since it introduces the topic of
the document to the reader and the writer must do
his/her best to attract the reader?s attention. It was
shown in (Huttunen et al, 2012) that 65% of highly-
relevant events in the domain of medical epidemics
appear in the title or in the first two sentences of a
news article; Lin and Hovy (1997) demonstrated that
Figure 2: Percentage of business verbs in the text; sen-
tence 0 refers to the title of the document. The fraction of
verbs is presented as a percent of all verb instances in the
corpus. Logarithmic scale is used for the x axis.
about 50% of topical keywords are concentrated in
the titles. We have noticed that some documents in
the news corpus have relevance to the business sce-
nario, although relevant events still can be extracted
from the second or third paragraphs of the text, men-
tioned incidentally. By contrast, each analyst report
is devoted to a specific business event, and these
events are frequently mentioned as early as in the
title.
4.2 Case study: is ?launch? a business verb?
A set of verbs such as launch, introduce, release,
present,3 etc., are used in the Business scenarios to
extract events about bringing new products to mar-
ket. In the domain ontology they are grouped under
a concept called LAUNCH-PRODUCT. An example
of a pattern that uses this concept is following:
np(COMPANY) vg(LAUNCH-PRODUCT)
np(ANYTHING)
This pattern matches when a NP designating a com-
pany is followed by a verb from the ontology, fol-
lowed by any other NP. This pattern matches, for
example, such sentence as: The Real Juice Company
has launched Pomegranate Blueberry flavour to its line
of 100% juices. However, this pattern also over-
generates by matching sentences such as, e.g.: Cen-
3Note, the S-V-O triplet extraction also handles phrasal
verbs, such as roll out, correctly, i.e., identifies them as a single
linguistic unit, and treats them the same as single-word verbs.
33
tral bank unveils effort to manage household debt. Even
among analyst reports, approximately 14% of the
NEW-PRODUCT events found by the system are
false positives. It is not feasible to collect a list of
all possible products to restrict the semantic type
of the object of the verb, since new, unpredictable
types of products can appear on the market every
day. It seemed more feasible to try to discover all
non-products that can appear in the object slot, due
to the ambiguity of the verbs in patterns?a kind of a
black-list. We introduce an ontology concept NON-
PRODUCT that groups nouns that can be matched
by the LAUNCH verbs but are in fact not products,
e.g., budget, effort, plan, report, study. The ontology
supports multiple inheritance, so any of these words
can be attached to other parents as well, if necessary.
If the <PRODUCT> slot in of event is filled by
one of the black-listed concepts, the event is also
black-listed, and not visible to the end-user. They
are used as discourse features by learning algorithms
that predict the relevance of other events from the
same documents (Huttunen et al, 2012).
The NON-PRODUCT class is populated in an ad-
hoc manner over time. The content of such a list
depends on the particular corpus; the more diverse
the topical and stylistic structure of the corpus, the
more time-consuming and the less tractable such de-
velopment becomes. Thus, an important task is to
adjust the patterns and the class of NON-PRODUCT
nouns to work for the news corpus, and to develop
a feasible methodology to address the false-positive
problem. We next show how we can use the pattern-
mining mode to address these problems.
We extract all instances of the LAUNCH-
PRODUCT verbs appearing in the corpora from the
S?V?O database. In total 27.5% of all verb instances
in reports corpus are verbs from this semantic class,
in comparison to 0.7% in the news corpus. The num-
ber of distinct objects are approximately the same in
both corpora: 3520 for reports and 3062 for news,
see Table 4. In total 247 different objects from the
report corpus attached to the semantic class PROD-
UCT in PULS ontology, and 158 objects have this
semantic class in the news corpus.
For 21% of launch verbs in the report corpus, and
34% in the news corpus, the system is not able to ex-
tract the objects, which may be a consequence of the
more diverse and varied language of news. Recall,
LAUNCH- distinct PRODUCT
Corpus PRODUCT objects objects
Reports 204193 3520 247
News 77463 3062 158
Table 4: Distributions of LAUNCH-PRODUCT verbs in
the corpora
that the system extracts a deep-structure verbal argu-
ments, i.e., for a sentence like ?A new cell-phone has
been launched by company XYZ? it identifies cell-
phone as the (deep) object, and the agent company
XYZ as the (deep) subject.
It is interesting to examine the particular sets of
words that can appear in the object position. We col-
lected the 50 most frequent objects of the LAUNCH-
PRODUCT verbs for each corpus; they are shown in
Table 5 ranked by frequency (we show the top 30
objects to save space). The table shows the semantic
class according to our ontology.
Of the 50 most frequent objects, 24 belong to
the semantic class PRODUCT in the report corpus,
while only 8 objects do in the general news cor-
pus. By contrast, 20 objects belong to the NON-
PRODUCT class in the news corpus and only 9 ob-
jects in reports. Moreover, 8 objects in the news cor-
pus are not found in the ontology at all, in compari-
son to only one such case from the report corpus.
Some object classes may mean that the event is
still relevant for the business domain, though it does
not belong to the NEW-PRODUCT scenario. For
example, when object is an advertising campaign the
event is likely to belong to the MARKETING sce-
nario, when the object is a facility (factory, outlet,
etc.) it is likely INVESTMENT. Inference rules may
detect such dependencies and adjust the scenario of
these events in the Business domain.
The inference rules are supported by the same do-
main ontology, but can test domain- and scenario-
specific conditions explicitly, and thus can be more
accurate than the generic reference resolution mech-
anism. However, this also means that inference rules
are more sensitive to the corpus genre and may not
easily transfer from one corpus to another.
In some cases an object type cannot be interpreted
as belonging to any reasonable event type, e.g., if
it is an ORGANIZATION or PERSON. Such cases
may arise due to unusual syntax in the sentence that
34
Rank Reports News
Object Freq Class Object Freq Class
1 Proper Name unspecified 19987 Proper Name unspecified 5971
2 product 7331 PROD report 1078 NON
3 service 6510 PROD result 851 NON
4 campaign 3537 CAMP plan 805 NON
5 project 2870 PROD product 792 PROD
6 range 2536 COLL service 648 PROD
7 plan 2524 NON it 618 PRON
8 organization 2450 ORG data 552
9 system 2166 FAC campaign 510 CAMP
10 line 1938 COLL organization 495 ORG
11 model 1920 PROD statement 467 NON
12 application 1345 PROD Proper Name person 449 PER
13 website 1321 PROD program 439
14 flight 1315 PROD Proper Name company 432 ORG
15 Proper Name company 1232 ORG information 411 NON
16 brand 1200 COLL detail 398 NON
17 offer 1187 NON investigation 380 NON
18 production 1112 NON website 373 PROD
19 programme 998 NON measure 368 NON
20 store 993 PROD they 363 PRON
21 currency 958 CUR he 358 PRON
22 route 954 PROD device 352 PROD
23 drink 891 PROD system 340 FAC
24 solution 883 NON smartphone 337 PROD
25 smartphone 852 PROD attack 335
26 fragrance 824 PROD figure 318 NON
27 card 802 PROD opportunity 295 INV
28 fund 801 PROD fund 290 NON
29 scheme 773 NON currency 287 CUR
30 facility 756 FAC model 286 COLL
Table 5: The most frequent objects of LAUNCH verbs. Class labels: PROD: product, NON: non-product (black-
listed), CAMP: advertising campaign, INV: investment. Domain independent labels: COLL: collective; PRON: pro-
noun, FAC: facility, ORG: organization, PER: person, CUR: currency,
confuses the shallow parser.
In summary, the results obtained from the S-V-O
pattern-mining can be used to improve the perfor-
mance of IE. First, the most frequent subjects and
objects for the business verbs can be added to the
ontology; second, inference rules and patterns are
adjusted to handle the new concepts and words.
It is very interesting to investigate?and we plan
to pursue this in the future?how this can be done
fully automatically; the problem is challenging since
the semantic classes for these news concepts de-
pend on the domain and task; for example, some
objects are of type PRODUCT (e.g., ?video?), and
others are of type NON-PRODUCT (e.g., ?attack?,
?report?, etc.). Certain words can be ambiguous
even within a limited domain: e.g., player may des-
ignate a COMPANY (?a major player on the mar-
ket?), a PRODUCT (DVD-player, CD-player, etc.),
or a person (tennis player, football player, etc.); the
last meaning is relevant for the Business domain
since sports personalities participate in promotion
campaigns, and can launch their own brands. Au-
tomating the construction of the knowledge bases is
a challenging task.
In practice, we found that the semi-automated ap-
proach and the pattern-mining tool can be helpful for
analyzing genre-specific event patterns; it provides
the advantages of a corpus-based study.
35
5 Conclusion
We have described an approach for collecting use-
ful statistics about event representation and distribu-
tion of event arguments in corpora. The approach
was easily implemented using pattern-based extrac-
tion of S-V-O triplets with PULS; it can be equally
efficiently implemented on top of a syntactic parser,
or a shallow parser of reasonable quality. An ontol-
ogy and lexicons are necessary to perform domain-
specific analysis. We have discussed how the results
of such analysis can be exploited for fine-tuning of
a practical IE scenario.
The pattern-mining process collects deep-
structure S?V?O triplets from the corpus?which
are ?potential? events. The triplets are stored in
a database, to facilitate searching and grouping
by words or by semantic class appearing as the
arguments of the triplets. This helps us quickly
find all realizations of a particular pattern?for
example, all semantic classes that appear in the
corpus as objects of verbs that have semantic class
LAUNCH-PRODUCT. The subsequent analysis of
the frequency lists can help improve the perfor-
mance of the IE system by suggesting refinements
to the ontology and the lexicon, as well as patterns
and inference rules appropriate for the particular
genre of the corpus.
Our current work includes the adaptation of the
IE system developed for the analyst reports to the
general news corpus devoted to the same topics. We
also plan to develop a hybrid methodology, to com-
bine the presented corpus-driven analysis with open-
domain techniques for pattern acquisition, (Cham-
bers and Jurafsky, 2011; Huang and Riloff, 2012).
The approach outlined here for analyzing the dis-
tributions of features in documents is useful for
studying events within the context of a corpus. It
demonstrates that event structure depends on the text
genre, and that genre differences can be easily cap-
tured and measured. By analyzing document statis-
tics and the output of the pattern-mining, we can
demonstrate significant differences between the gen-
res of analyst reports and general news, such as: sen-
tence length, distribution of the domain vocabulary
in the text, selectional preference in domain-specific
verbs, word co-occurrences, usage of pronouns and
proper names.
The pattern mining collects other statistical fea-
tures, beyond those that have been discussed in de-
tail above. For example, it showed that active voice
is used in 95% of the cases in the news corpus in
comparison to 88% in the analyst report corpus. It
is also possible to count and compare the usage of
other grammatical cues, such as verb tense, modal-
ity, etc. Thus, we should investigate not only lexical
and semantic cues, but also broader syntactic prefer-
ences and selectional constrains in the corpora.
In further research we plan to study how the for-
mal representation of the genre differences can be
used in practice, that is, for obtaining directly mea-
surable improvements in the quality of event extrac-
tion. Taking into account the particular genre of the
corpora from which documents are drawn will also
have implications for the work on performance im-
provements via cross-document merging and infer-
ence, (Ji and Grishman, 2008; Yangarber, 2006).
The frequency-based analysis described in Sec-
tion 4.2 seems to be effective. Sharpening the results
of the analysis as well as putting it to use in practical
IE applications will be the subject of further study.
Acknowledgements
We wish to thank Matthew Pierce and Peter von Et-
ter for their help in implementation of the pattern
mining more described in this paper. The work was
supported in part by the ALGODAN: Algorithmic
Data Analysis Centre of Excellence of the Academy
of Finland.
References
Douglas Biber. 1991. Variation across speech and writ-
ing. Cambridge University Press.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL-HLT, pages 976?986.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: A brief history. In Pro-
ceedings of COLING, volume 96, pages 466?471.
Ralph Grishman. 2012. Structural linguistics and un-
supervised information extraction. Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction (AKBC-WEKEX 2012), pages 57?61.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. EACL 2012,
pages 286?295.
36
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002a. Complexity of event structure in IE scenarios.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING 2002), Taipei,
August.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002b. Diversity of scenarios in information extrac-
tion. In Proceedings of the Third International Confer-
ence on Language Resources and Evaluation (LREC
2002), Las Palmas de Gran Canaria, Spain, May.
Silja Huttunen, Arto Vihavainen, Mian Du, and Roman
Yangarber. 2012. Predicting relevance of event ex-
traction for the end user. In T. Poibeau et al, edi-
tor, Multi-source, Multilingual Information Extraction
and Summarization, pages 163?177. Springer-Verlag,
Berlin.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL-2008: HLT, pages 254?262, June.
Jussi Karlgren and Douglas Cutting. 1994. Recogniz-
ing text genres with simple metrics using discriminant
analysis. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1071?1075, Kyoto,
Japan, August.
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics and Eighth Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 32?38. Association for Com-
putational Linguistics.
Chin-Yew Lin and Eduard Hovy. 1997. Identifying top-
ics by position. In Proceedings of the fifth conference
on Applied natural language processing, pages 283?
290. Association for Computational Linguistics.
Philipp Petrenz and Bonnie Webber. 2011. Stable clas-
sification of text genres. Computational Linguistics,
37(2):385?393.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?124.
Association for Computational Linguistics, Somerset,
New Jersey.
Roser Saur?? and James Pustejovsky. 2012. Are you sure
that this happened? Assessing the factuality degree of
events in text. Computational Linguistics, 38(2):261?
299.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2000. Text genre detection using com-
mon word frequencies. In Proceedings of the 18th
conference on Computational linguistics - Volume 2,
COLING ?00, pages 808?814, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?fra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Mark Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2002).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisi-
tion of domain knowledge for information extrac-
tion. In Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING 2000),
Saarbru?cken, Germany, August.
Roman Yangarber. 2006. Verification of facts across
document boundaries. In Proceedings of the Inter-
national Workshop on Intelligent Information Access
(IIIA-2006), Helsinki, Finland, August.
37
