Chinese Word Segmentation based on Mixing Model 
Wei Jiang          Jian Zhao          Yi Guan          Zhiming Xu  
ITNLP, Harbin Institute of Technology,  Heilongjiang Province, 150001 China 
jiangwei@insun.hit.edu.cn
Abstract
This paper presents our recent work for 
participation in the Second Interna-
tional Chinese Word Segmentation 
Bakeoff. According to difficulties, we 
divide word segmentation into several 
sub-tasks, which are solved by mixed 
language models, so as to take advan-
tage of each approach in addressing 
special problems. The experiment indi-
cated that this system achieved 96.7% 
and 97.2% in F-measure in PKU and 
MSR open test respectively. 
1 Introduction 
Word is a logical semantic and syntactic unit in 
natural language. So word segmentation is the 
foundation of most Chinese NLP tasks. Though 
much progress has been made in the last two 
decades, there is no existing model that can 
solve all the problems perfectly at present. So 
we try to apply different language models to 
solve each special sub-task, due to ?No Free 
Lunch Theorem? and ?Ugly Duckling Theorem?. 
Our system participated in the Second Inter-
national Chinese Word Segmentation Bakeoff 
(henceforce, the bakeoff) held in 2005. Recently, 
we have done more work in dealing with three 
main sub-tasks: (1) Segmentation disambigua-
tion; (2) Named entities recognition; (3) New 
words1 detection. We apply different approachs 
to solve above three problems, and all the mod-
ules are integrated into a pragmatic system 
(ELUS). Due to the limitation of available re-
source, some kinds of features, e.g. POS, have 
been erased in our participation system. This 
segmenter will be briefly describled in this paper. 
                                                          
1 New words refer to this kind of out-of ?vocabulary words 
that are neither recognized named entities or factoid words 
nor morphological words. 
2 ELUS Segmenter 
All the words are categorized into five types: 
Lexicon words (LW), Factoid words (FT), Mor-
phologically derived words (MDW), Named 
entities (NE), and New words (NW). Accord-
ingly, four main modules are included to iden-
tify each kind of words, as shown in Figure 1.  
Class-based trigram model (Gao 2004) is 
adopted in the Basic Segmentation to convert 
the sentence into a word sequence. Let w = w1
w2 ?wn be a word class sequence, then the most 
likely word class sequence w* in trigram is: 
?
 
 
n
i
iii
www
wwwP
n 1
12 )|(maxarg*
21 
w ,
where let P(w0|w-2 w-1) be P(w0) and let 
P(w1|w-1 w0) be P(w1|w0). And wi represents LW 
or a type of FT or MDW. Viterbi algorithm is 
used to search the best candidate. Absolute 
smoothing algorithm is applied to overcome the 
data sparseness. Here, LW, FT and MDW are 
idendified (Zhao Yan 2005). All the Factoid 
words can be represented as regular expressions. 
As a result, the detection of factoid words can be 
archieved by Finite State Machines. 
Figure 1 ELUS Segmenter 
Factoid Detect
Morphology Word
Lexicon words
String
Result
NE Recognization
Sentence
Basic Segmentation
NW Detection
Disambiguation
180
Four kinds of Named entities are detected, i.e. 
Chinese person name, foreign person name, lo-
cation name and orgnization name. This is the 
most complicated module in ELUS.  
Three kinds of models are applied here. 
HMM model (one order) is described as: 
?
 
 
n
i
iiii
TTT
TTPTWP
n 1
1
# )|()|(maxarg
21 
T ,
where iT  represents the tag of current word,  
Viterbi algorithm is used to search the best path. 
Another model is Maximum Entropy (Zhao Jian 
2005, Hai Leong Chieu 2002). Take Chinese 
person name as example. Firstly, we combine 
HMM and Maximum Entropy (ME) model to 
lable the person name tag, e.g. ??/CPB ?/CPI  
?/CPI? (Tongmei Yao); Secondly, the tagged  
name is merged by combining ME Model and 
Support Vector Machine (SVM) and some aided 
rules, e.g. merged into ??/??? in PKU test.
Some complex features are added into ME 
model (described in Zhao Jian 2005), in addition, 
we also collect more than 110,000 person names, 
and acquire the statistic about common name 
characters, these kinds of features are also fused 
into the ME model to detect NE. The other kinds 
of NE recognition adopt similar method, except 
for individual features. 
New Words is another important kind of 
OOV words, especially in closed test. Take PKU 
test as example, we collect NW suffixes, such as 
???(city),???(lamp). Those usually construct 
new words, e.g. ?????(sighting lamp). 
A variance-based method is used to collect 
suffixes. And three points need to be consid-
ered:(1) It is tail of many words;(2) It has large 
variance in constructing word;(3) It is seldom 
used alone. We acquire about 25 common suf-
fixes in PKU training corpus by above method. 
We use Local Maximum Entropy model, e.g. 
??? /1 ? /1?(Huanggang city), i.e. only the 
nearer characters are judged before the suffix 
??? (city). By our approach, the training corpus 
can be generated via given PKU corpus in the 
bakeoff. The features come from the nearer con-
text, besides, common single words and punc-
tuations are not regarded as a part of New Word. 
The last module is Word Disambiugation. 
Word segmentation ambiguities are usually clas-
sified into two classes: overlapping ambiguity 
and combination ambiguity. By evaluating 
ELUS, the most segmentation errors are one 
segmentation errors (about 95%). i.e. the two 
words on both sides of current segmentation 
errors are right. These include LW ambiguities 
and FT ambiguities etc. Here, we adopt Maxi-
mum Entropy model. The same as other mod-
ules, it is defined over HhT in segmentation 
disambiguation, where H is the set of possible 
contexts around target word that will be tagged, 
and T is the set of allowable tags. Then the 
model?s conditional probability is defined as 
? ? Tt thp
thphtp
'
)',(
),()|(
, where 
?
 
 
k
j
thf
j
jthp
1
),(),( DSP
where h is current context and t is one of the 
possible tags. The ambiguous words are mainly 
collected by evaluating our system.  
In NE module and Word Disambiguation 
module, we introduce rough rule features, which 
are extracted by Rough Set (Wang Xiaolong 
2004), e.g. ???????(display ability), ??
???/??(only? can just), ???+person+?
?? (the reporter+person+report). Previous ex-
periment had indicated word disambiguation 
could achieve better performance by applying 
Rough Set. 
3 Performance and analysis 
The performance of ELUS in the bakeoff is pre-
sented in Table 1 and Table 2 respectively, in 
terms of recall(R), precision(P) and F score in 
percentages.
Table 1 Closed test, in percentages (%) 
Closed R P F OOV Roov Riv
PKU 95.4 92.7 94.1 5.8 51.8 98.1
MSR 97.3 94.5 95.9 2.6 32.3 99.1
CITYU 93.4 86.5 89.8 7.4 24.8 98.9
AS 94.3 89.5 91.8 4.3 13.7 97.9
Table 2 Open test, in percentages (%) 
Open R P F OOV Roov Riv
PKU 96.8 96.6 96.7 5.8 82.6 97.7
MSR 98.0 96.5 97.2 2.6 59.0 99.0
CITYU 94.6 89.8 92.2 7.4 41.7 98.9
AS 95.2 92.0 93.6 4.3 35.4 97.9
Our system has good performance in terms 
of F-measure in simplified Chinese open test, 
including PKU and MSR open test. In addition, 
181
its IV word identification performance is re-
markable, ranging from 97.7% to 99.1%, stands 
at the top or nearer the top in all the tests in 
which we have participated. This good perform-
ance owes to class-based trigram, absolute 
smoothing and word disambiguation module and 
rough rules. 
There is almost the same IV performance be-
tween open test and closed test in MSR, CITYU 
and AS respectively, because we adopt the same 
Lexicon between open test and closed test re-
spectively. While in open test of PKU, we adopt 
another Lexicon that comes from six-month 
corpora of Peoples? Daily (China) in 1998, 
which were also annotated by Peking University. 
The OOV word identification performance 
seems uneven, compared with PKU, the other 
tests seem lower, due to the following reasons: 
 (1) Because of our resource limitation, NE 
training resource is six-month corpora of Peo-
ples? Daily (China) in 1998, which came from 
Peking University, and some newspapers and 
web pages annotated by our laboratory;  
(2) We have no traditional Chinese corpus, 
so the NE training resource for CITYU and AS 
is acquired via converting above corpora. Since 
these corpora are converted from simplified 
Chinese, they are not well suitable to traditional 
Chinese corpora;
(3) The different corpora have different crite-
rions in NE detection, especially in location 
name and organization name, e.g. ????/??
/??? (Cuicun Town Xiangtang Hogpen) in 
PKU and ????????? in MSR criterion. 
Even if our system recognizes the ????/?/
?/??? as a orgnization name, we are not eas-
ily to regard ??? ? as one word in PKU, 
since ???? isn?t a lexical word. However in 
MSR, that is easy, because its criterion regard 
the whole Orgnization as a word;
(4) We need do more to comply with the 
segmentation criterion, e.g. ?????(outlier) in 
CITYU come from ???? + ???, while this 
kind of false segment is due to our bad under-
standing to CITYU criterion. 
Though there are above problems, our sys-
tem does well in regonization precision, since 
we adopt two steps in recognizing NE, especial 
in recognizing Chinese person name. And from 
the result of evalution in the bakeoff, we need to 
improve the NE recall in the future.  
In order to make our New words comply 
with the criterion, we conservatively use New 
Word Detection module, in order to avoid hav-
ing bad recognition result, since each corpus has 
its own New Word criterion.
4 Conclusion and Future work 
We have briefly describled our system based on 
mixed models. Different approachs are adopted 
to solve each special sub-task, since there is ?No 
Free Lunch Theorem?. And mixed models are 
used in NE detection. This sytem has a good 
performance in the simplified Chinese in the 
bakeoff.
The future work is mainly concentrating on 
two directions: finding effective features and 
delicately adjusting internal relations among 
different modules, in order to improve segmen-
tation performance. 
References
Fu Fuohong. 2000. Research on Statistical Methods 
of Chinese Syntactic Disambiguation. Ph.D. The-
sis. Harbin Institute of Technology, China. 
Hai Leong Chieu, Hwee Tou Ng. Named Entity. 
Recognition: A Maximum Entropy Approach Us-
ing Global. Information. Proceedings of the 19th 
International Conference. on Computational Lin-
guistics, 2002. 
Hua-Ping Zhang, Qun Liu etc. 2003. Chinese Lexical 
Analysis Using Hierarchical Hidden Markov 
Model, Second SIGHAN workshop affiliated with 
4th ACL, Sapporo Japan, pp.63-70, July 2003. 
Jianfeng Gao, Mu Li et al 2004. Chinese Word 
Segmentation: A Pragmatic Approach. MSR-TR-
2004-123, November 2004. 
Wang Xiaolong, Chen Qingcai, and Daniel S.Yeung. 
2004. Mining PinYin-to-Character Conversion 
Rules From Large-Scale Corpus: A Rough Set Ap-
proach, IEEE TRANSACTION ON SYSTEMS, 
MAN. AND CYBERNETICS-PART 
B:CYBERNETICS. VOL. 34, NO.2, APRIL. 
Zhao Jian, Wang Xiao-long et al 2005. Comparing 
Features Combination with Features Fusion in 
Chinese Named Entity Recognition. Computer 
Application. China. 
Zhao Yan. 2005. Research on Chinese Morpheme 
Analysis Based on Statistic Language Model. 
Ph.D. Thesis. Harbin Institute of Technology, 
China. 
182
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 720?727,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Probabilistic Approach to Syntax-based Reordering
for Statistical Machine Translation
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou
Microsoft Research Asia
Beijing, China
chl, dozhang@microsoft.com
muli, mingzhou@microsoft.com
Minghui Li, Yi Guan
Harbin Institute of Technology
Harbin, China
mhli@insun.hit.edu.cn
guanyi@insun.hit.edu.cn
Abstract
Inspired by previous preprocessing ap-
proaches to SMT, this paper proposes a
novel, probabilistic approach to reordering
which combines the merits of syntax and
phrase-based SMT. Given a source sentence
and its parse tree, our method generates,
by tree operations, an n-best list of re-
ordered inputs, which are then fed to stan-
dard phrase-based decoder to produce the
optimal translation. Experiments show that,
for the NIST MT-05 task of Chinese-to-
English translation, the proposal leads to
BLEU improvement of 1.56%.
1 Introduction
The phrase-based approach has been considered the
default strategy to Statistical Machine Translation
(SMT) in recent years. It is widely known that the
phrase-based approach is powerful in local lexical
choice and word reordering within short distance.
However, long-distance reordering is problematic
in phrase-based SMT. For example, the distance-
based reordering model (Koehn et al, 2003) al-
lows a decoder to translate in non-monotonous or-
der, under the constraint that the distance between
two phrases translated consecutively does not ex-
ceed a limit known as distortion limit. In theory the
distortion limit can be assigned a very large value
so that all possible reorderings are allowed, yet in
practise it is observed that too high a distortion limit
not only harms efficiency but also translation per-
formance (Koehn et al, 2005). In our own exper-
iment setting, the best distortion limit for Chinese-
English translation is 4. However, some ideal trans-
lations exhibit reorderings longer than such distor-
tion limit. Consider the sentence pair in NIST MT-
2005 test set shown in figure 1(a): after translating
the word ?V/mend?, the decoder should ?jump?
across six words and translate the last phrase ?
? ?_/fissures in the relationship?. Therefore,
while short-distance reordering is under the scope
of the distance-based model, long-distance reorder-
ing is simply out of the question.
A terminological remark: In the rest of the paper,
we will use the terms global reordering and local
reordering in place of long-distance reordering and
short-distance reordering respectively. The distinc-
tion between long and short distance reordering is
solely defined by distortion limit.
Syntax1 is certainly a potential solution to global
reordering. For example, for the last two Chinese
phrases in figure 1(a), simply swapping the two chil-
dren of the NP node will produce the correct word
order on the English side. However, there are also
reorderings which do not agree with syntactic anal-
ysis. Figure 1(b) shows how our phrase-based de-
coder2 obtains a good English translation by reorder-
ing two blocks. It should be noted that the second
Chinese block ??e? and its English counterpart
?at the end of? are not constituents at all.
In this paper, our interest is the value of syntax in
reordering, and the major statement is that syntactic
information is useful in handling global reordering
1Here by syntax it is meant linguistic syntax rather than for-
mal syntax.
2The decoder is introduced in section 6.
720
Figure 1: Examples on how syntax (a) helps and (b) harms reordering in Chinese-to-English translation
The lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom
half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks
found by our decoder.
and it achieves better MT performance on the ba-
sis of the standard phrase-based model. To prove it,
we developed a hybrid approach which preserves the
strength of phrase-based SMT in local reordering as
well as the strength of syntax in global reordering.
Our method is inspired by previous preprocessing
approaches like (Xia and McCord, 2004), (Collins
et al, 2005), and (Costa-jussa` and Fonollosa, 2006),
which split translation into two stages:
S ? S? ? T (1)
where a sentence of the source language (SL), S,
is first reordered with respect to the word order of
the target language (TL), and then the reordered SL
sentence S? is translated as a TL sentence T by
monotonous translation.
Our first contribution is a new translation model
as represented by formula 2:
S ? n? S? ? n? T ? T? (2)
where an n-best list of S?, instead of only one S?, is
generated. The reason of such change will be given
in section 2. Note also that the translation process
S??T is not monotonous, since the distance-based
model is needed for local reordering. Our second
contribution is our definition of the best translation:
argmax
T
exp(?rlogPr(S?S?)+
?
i
?iFi(S??T ))
where Fi are the features in the standard phrase-
based model and Pr(S ? S?) is our new feature,
viz. the probability of reordering S as S?. The de-
tails of this model are elaborated in sections 3 to 6.
The settings and results of experiments on this new
model are given in section 7.
2 Related Work
There have been various attempts to syntax-
based SMT, such as (Yamada and Knight, 2001)
and (Quirk et al, 2005). We do not adopt these
models since a lot of subtle issues would then be in-
troduced due to the complexity of syntax-based de-
coder, and the impact of syntax on reordering will
be difficult to single out.
There have been many reordering strategies un-
der the phrase-based camp. A notable approach is
lexicalized reordering (Koehn et al, 2005) and (Till-
mann, 2004). It should be noted that this approach
achieves the best result within certain distortion limit
and is therefore not a good model for global reorder-
ing.
There are a few attempts to the preprocessing
approach to reordering. The most notable ones
are (Xia and McCord, 2004) and (Collins et al,
2005), both of which make use of linguistic syntax
in the preprocessing stage. (Collins et al, 2005) an-
alyze German clause structure and propose six types
721
of rules for transforming German parse trees with
respect to English word order. Instead of relying
on manual rules, (Xia and McCord, 2004) propose
a method in learning patterns of rewriting SL sen-
tences. This method parses training data and uses
some heuristics to align SL phrases with TL ones.
From such alignment it can extract rewriting pat-
terns, of which the units are words and POSs. The
learned rewriting rules are then applied to rewrite SL
sentences before monotonous translation.
Despite the encouraging results reported in these
papers, the two attempts share the same shortcoming
that their reordering is deterministic. As pointed out
in (Al-Onaizan and Papineni, 2006), these strategies
make hard decisions in reordering which cannot be
undone during decoding. That is, the choice of re-
ordering is independent from other translation fac-
tors, and once a reordering mistake is made, it can-
not be corrected by the subsequent decoding.
To overcome this weakness, we suggest a method
to ?soften? the hard decisions in preprocessing. The
essence is that our preprocessing module generates
n-best S?s rather than merely one S?. A variety of
reordered SL sentences are fed to the decoder so
that the decoder can consider, to certain extent, the
interaction between reordering and other factors of
translation. The entire process can be depicted by
formula 2, recapitulated as follows:
S ? n? S? ? n? T ? T? .
Apart from their deterministic nature, the two
previous preprocessing approaches have their own
weaknesses. (Collins et al, 2005) count on man-
ual rules and it is suspicious if reordering rules for
other language pairs can be easily made. (Xia and
McCord, 2004) propose a way to learn rewriting
patterns, nevertheless the units of such patterns are
words and their POSs. Although there is no limit to
the length of rewriting patterns, due to data sparse-
ness most patterns being applied would be short
ones. Many instances of global reordering are there-
fore left unhandled.
3 The Acquisition of Reordering
Knowledge
To avoid this problem, we give up using rewriting
patterns and design a form of reordering knowledge
which can be directly applied to parse tree nodes.
Given a node N on the parse tree of an SL sentence,
the required reordering knowledge should enable the
preprocessing module to determine how probable
the children of N are reordered.3 For simplicity, let
us first consider the case of binary nodes only. Let
N1 and N2, which yield phrases p1 and p2 respec-
tively, be the child nodes of N . We want to deter-
mine the order of p1 and p2 with respect to their TL
counterparts, T (p1) and T (p2). The knowledge for
making such a decision can be learned from a word-
aligned parallel corpus. There are two questions in-
volved in obtaining training instances:
? How to define T (pi)?
? How to define the order of T (pi)s?
For the first question, we adopt a similar method
as in (Fox, 2002): given an SL phrase ps =
s1 . . . si . . . sn and a word alignment matrix A, we
can enumerate the set of TL words {ti : ti?A(si)},
and then arrange the words in the order as they ap-
pear in the TL sentence. Let first(t) be the first word
in this sorted set and last(t) be the last word. T (ps)
is defined as the phrase first(t) . . . last(t) in the TL
sentence. Note that T (ps) may contain words not in
the set {ti}.
The question of the order of two TL phrases is not
a trivial one. Since a word alignment matrix usu-
ally contains a lot of noises as well as one-to-many
and many-to-many alignments, two TL phrases may
overlap with each other. For the sake of the quality
of reordering knowledge, if T (p1) and T (p2) over-
lap, then the node N with children N1 and N2 is
not taken as a training instance. Obviously it will
greatly reduce the amount of training input. To rem-
edy data sparseness, less probable alignment points
are removed so as to minimize overlapping phrases,
since, after removing some alignment point, one of
the TL phrases may become shorter and the two
phrases may no longer overlap. The implementation
is similar to the idea of lexical weight in (Koehn et
al., 2003): all points in the alignment matrices of the
entire training corpus are collected to calculate the
probabilistic distribution, P (t|s), of some TL word
3Some readers may prefer the expression the subtree rooted
at node N to node N . The latter term is used in this paper for
simplicity.
722
t given some SL word s. Any pair of overlapping
T (pi)s will be redefined by iteratively removing less
probable word alignments until they no longer over-
lap. If they still overlap after all one/many-to-many
alignments have been removed, then the refinement
will stop and N , which covers pis, is no longer taken
as a training instance.
In sum, given a bilingual training corpus, a parser
for the SL, and a word alignment tool, we can collect
all binary parse tree nodes, each of which may be an
instance of the required reordering knowledge. The
next question is what kind of reordering knowledge
can be formed out of these training instances. Two
forms of reordering knowledge are investigated:
1. Reordering Rules, which have the form
Z : X Y ?
{
X Y Pr(IN-ORDER)
Y X Pr(INVERTED)
where Z is the phrase label of a binary node
and X and Y are the phrase labels of Z?s chil-
dren, and Pr(INVERTED) and Pr(IN-ORDER)
are the probability that X and Y are inverted on
TL side and that not inverted, respectively. The
probability figures are estimated by Maximum
Likelihood Estimation.
2. Maximum Entropy (ME) Model, which does
the binary classification whether a binary
node?s children are inverted or not, based on a
set of features over the SL phrases correspond-
ing to the two children nodes. The features that
we investigated include the leftmost, rightmost,
head, and context words4, and their POSs, of
the SL phrases, as well as the phrase labels of
the SL phrases and their parent.
4 The Application of Reordering
Knowledge
After learning reordering knowledge, the prepro-
cessing module can apply it to the parse tree, tS ,
of an SL sentence S and obtain the n-best list of
S?. Since a ranking of S? is needed, we need some
way to score each S?. Here probability is used as
the scoring metric. In this section it is explained
4The context words of the SL phrases are the word to the left
of the left phrase and the word to the right of the right phrase.
how the n-best reorderings of S and their associated
scores/probabilites are computed.
Let us first look into the scoring of a particular
reordering. Let Pr(p?p?) be the probability of re-
ordering a phrase p into p?. For a phrase q yielded by
a non-binary node, there is only one ?reordering? of
q, viz. q itself, thus Pr(q?q) = 1. For a phrase p
yielded by a binary node N , whose left child N1 has
reorderings pi1 and right child N2 has the reorder-
ings pj2 (1 ? i, j ? n), p? has the form pi1pj2 or pj2pi1.
Therefore, Pr(p?p?) =
{
Pr(IN-ORDER)? Pr(pi1?pi
?
1 )? Pr(pj2?pj
?
2 )
Pr(INVERTED)? Pr(pj2?pj
?
2 )? Pr(pi1?pi
?
1 )
The figures Pr(IN-ORDER) and Pr(INVERTED) are
obtained from the learned reordering knowledge. If
reordering knowledge is represented as rules, then
the required probability is the probability associated
with the rule that can apply to N . If reordering
knowledge is represented as an ME model, then the
required probability is:
P (r|N) = exp(
?
i ?ifi(N, r))?
r? exp(
?
i ?ifi(N, r?))
where r?{IN-ORDER, INVERTED}, and fi?s are fea-
tures used in the ME model.
Let us turn to the computation of the n-best re-
ordering list. Let R(N) be the number of reorder-
ings of the phrase yielded by N , then:
R(N) =
{
2R(N1)R(N2) if N has children N1, N2
1 otherwise
It is easily seen that the number of S?s increases ex-
ponentially. Fortunately, what we need is merely an
n-best list rather than a full list of reorderings. Start-
ing from the leaves of tS , for each node N covering
phrase p, we only keep track of the n p?s that have
the highest reordering probability. Thus R(N) ? n.
There are at most 2n2 reorderings for any node and
only the top-scored n reorderings are recorded. The
n-best reorderings of S, i.e. the n-best reorderings
of the yield of the root node of tS , can be obtained
by this efficient bottom-up method.
5 The Generalization of Reordering
Knowledge
In the last two sections reordering knowledge is
learned from and applied to binary parse tree nodes
723
only. It is not difficult to generalize the theory of
reordering knowledge to nodes of other branching
factors. The case of binary nodes is simple as there
are only two possible reorderings. The case of 3-ary
nodes is a bit more complicated as there are six.5 In
general, an n-ary node has n! possible reorderings
of its children. The maximum entropy model has the
same form as in the binary case, except that there are
more classes of reordering patterns as n increases.
The form of reordering rules, and the calculation of
reordering probability for a particular node, can also
be generalized easily.6 The only problem for the
generalized reordering knowledge is that, as there
are more classes, data sparseness becomes more se-
vere.
6 The Decoder
The last three sections explain how the S?n?S?
part of formula 2 is done. The S??T
part is simply done by our re-implementation
of PHARAOH (Koehn, 2004). Note that non-
monotonous translation is used here since the
distance-based model is needed for local reordering.
For the n?T? T? part, the factors in consideration
include the score of T returned by the decoder, and
the reordering probability Pr(S ? S?). In order
to conform to the log-linear model used in the de-
coder, we integrate the two factors by defining the
total score of T as formula 3:
exp(?r logPr(S?S?) +
?
i
?iFi(S??T )) (3)
The first term corresponds to the contribution of
syntax-based reordering, while the second term that
of the features Fi used in the decoder. All the fea-
ture weights (?s) were trained using our implemen-
tation of Minimum Error Rate Training (Och, 2003).
The final translation T? is the T with the highest total
score.
5Namely, N1N2N3, N1N3N2, N2N1N3, N2N3N1,
N3N1N2, and N3N2N1, if the child nodes in the original order
are N1, N2, and N3.
6For example, the reordering probability of a phrase p =
p1p2p3 generated by a 3-ary node N is
Pr(r)?Pr(pi1)?Pr(pj2)?Pr(pk3)
where r is one of the six reordering patterns for 3-ary nodes.
It is observed in pilot experiments that, for a lot of
long sentences containing several clauses, only one
of the clauses is reordered. That is, our greedy re-
ordering algorithm (c.f. section 4) has a tendency to
focus only on a particular clause of a long sentence.
The problem was remedied by modifying our de-
coder such that it no longer translates a sentence at
once; instead the new decoder does:
1. split an input sentence S into clauses {Ci};
2. obtain the reorderings among {Ci}, {Sj};
3. for each Sj , do
(a) for each clause Ci in Sj , do
i. reorder Ci into n-best C ?is,
ii. translate each C ?i into T (C
?
i),
iii. select T? (C ?i);
(b) concatenate {T? (C ?i)} into Tj ;
4. select T?j .
Step 1 is done by checking the parse tree if there
are any IP or CP nodes7 immediately under the root
node. If yes, then all these IPs, CPs, and the remain-
ing segments are treated as clauses. If no, then the
entire input is treated as one single clause. Step 2
and step 3(a)(i) still follow the algorithm in sec-
tion 4. Step 3(a)(ii) is trivial, but there is a subtle
point about the calculation of language model score:
the language model score of a translated clause is not
independent from other clauses; it should take into
account the last few words of the previous translated
clause. The best translated clause T? (C ?i) is selected
in step 3(a)(iii) by equation 3. In step 4 the best
translation T?j is
argmax
Tj
exp(?rlogPr(S?Sj)+
?
i
score(T (C ?i))).
7 Experiments
7.1 Corpora
Our experiments are about Chinese-to-English
translation. The NIST MT-2005 test data set is used
for evaluation. (Case-sensitive) BLEU-4 (Papineni
et al, 2002) is used as the evaluation metric. The
7 IP stands for inflectional phrase and CP for complementizer
phrase. These two types of phrases are clauses in terms of the
Government and Binding Theory.
724
Branching Factor 2 3 >3
Count 12294 3173 1280
Percentage 73.41 18.95 7.64
Table 1: Distribution of Parse Tree Nodes with Dif-
ferent Branching Factors Note that nodes with only one
child are excluded from the survey as reordering does not apply
to such nodes.
test set and development set of NIST MT-2002 are
merged to form our development set. The training
data for both reordering knowledge and translation
table is the one for NIST MT-2005. The GIGA-
WORD corpus is used for training language model.
The Chinese side of all corpora are segmented into
words by our implementation of (Gao et al, 2003).
7.2 The Preprocessing Module
As mentioned in section 3, the preprocessing mod-
ule for reordering needs a parser of the SL, a word
alignment tool, and a Maximum Entropy training
tool. We use the Stanford parser (Klein and Man-
ning, 2003) with its default Chinese grammar, the
GIZA++ (Och and Ney, 2000) alignment package
with its default settings, and the ME tool developed
by (Zhang, 2004).
Section 5 mentions that our reordering model can
apply to nodes of any branching factor. It is inter-
esting to know how many branching factors should
be included. The distribution of parse tree nodes
as shown in table 1 is based on the result of pars-
ing the Chinese side of NIST MT-2002 test set by
the Stanford parser. It is easily seen that the major-
ity of parse tree nodes are binary ones. Nodes with
more than 3 children seem to be negligible. The 3-
ary nodes occupy a certain proportion of the distri-
bution, and their impact on translation performance
will be shown in our experiments.
7.3 The decoder
The data needed by our Pharaoh-like decoder are
translation table and language model. Our 5-gram
language model is trained by the SRI language mod-
eling toolkit (Stolcke, 2002). The translation table
is obtained as described in (Koehn et al, 2003), i.e.
the alignment tool GIZA++ is run over the training
data in both translation directions, and the two align-
Test Setting BLEU
B1 standard phrase-based SMT 29.22
B2 (B1) + clause splitting 29.13
Table 2: Experiment Baseline
Test Setting BLEU BLEU
2-ary 2,3-ary
1 rule 29.77 30.31
2 ME (phrase label) 29.93 30.49
3 ME (left,right) 30.10 30.53
4 ME ((3)+head) 30.24 30.71
5 ME ((3)+phrase label) 30.12 30.30
6 ME ((4)+context) 30.24 30.76
Table 3: Tests on Various Reordering Models
The 3rd column comprises the BLEU scores obtained by re-
ordering binary nodes only, the 4th column the scores by re-
ordering both binary and 3-ary nodes. The features used in the
ME models are explained in section 3.
ment matrices are integrated by the GROW-DIAG-
FINAL method into one matrix, from which phrase
translation probabilities and lexical weights of both
directions are obtained.
The most important system parameter is, of
course, distortion limit. Pilot experiments using the
standard phrase-based model show that the optimal
distortion limit is 4, which was therefore selected for
all our experiments.
7.4 Experiment Results and Analysis
The baseline of our experiments is the standard
phrase-based model, which achieves, as shown by
table 2, the BLEU score of 29.22. From the same
table we can also see that the clause splitting mech-
anism introduced in section 6 does not significantly
affect translation performance.
Two sets of experiments were run. The first set,
of which the results are shown in table 3, tests the
effect of different forms of reordering knowledge.
In all these tests only the top 10 reorderings of
each clause are generated. The contrast between
tests 1 and 2 shows that ME modeling of reordering
outperforms reordering rules. Tests 3 and 4 show
that phrase labels can achieve as good performance
as the lexical features of mere leftmost and right-
most words. However, when more lexical features
725
Input 0 2005#?R????q?Z??/???=?
Reference Hainan province will continue to increase its investment in the public services and
social services infrastructures in 2005
Baseline Hainan Province in 2005 will continue to increase for the public service and social
infrastructure investment
Translation with
Preprocessing
Hainan Province in 2005 will continue to increase investment in public services
and social infrastructure
Table 4: Translation Example 1
Test Setting BLEU
a length constraint 30.52
b DL=0 30.48
c n=100 30.78
Table 5: Tests on Various Constraints
are added (tests 4 and 6), phrase labels can no longer
compete with lexical features. Surprisingly, test 5
shows that the combination of phrase labels and lex-
ical features is even worse than using either phrase
labels or lexical features only.
Apart from quantitative evaluation, let us con-
sider the translation example of test 6 shown in ta-
ble 4. To generate the correct translation, a phrase-
based decoder should, after translating the word
?? as ?increase?, jump to the last word ?=
?(investment)?. This is obviously out of the capa-
bility of the baseline model, and our approach can
accomplish the desired reordering as expected.
By and large, the experiment results show that no
matter what kind of reordering knowledge is used,
the preprocessing of syntax-based reordering does
greatly improve translation performance, and that
the reordering of 3-ary nodes is crucial.
The second set of experiments test the effect of
some constraints. The basic setting is the same as
that of test 6 in the first experiment set, and reorder-
ing is applied to both binary and 3-ary nodes. The
results are shown in table 5.
In test (a), the constraint is that the module does
not consider any reordering of a node if the yield
of this node contains not more than four words.
The underlying rationale is that reordering within
distortion limit should be left to the distance-based
model during decoding, and syntax-based reorder-
ing should focus on global reordering only. The
result shows that this hypothesis does not hold.
In practice syntax-based reordering also helps lo-
cal reordering. Consider the translation example
of test (a) shown in table 6. Both the baseline
model and our model translate in the same way up
to the word ??w? (which is incorrectly translated
as ?and?). From this point, the proposed preprocess-
ing model correctly jump to the last phrase ??q?
?X/discussed?, while the baseline model fail to do
so for the best translation. It should be noted, how-
ever, that there are only four words between ??w?
and the last phrase, and the desired order of decod-
ing is within the capability of the baseline system.
With the feature of syntax-based global reordering,
a phrase-based decoder performs better even with
respect to local reordering. It is because syntax-
based reordering adds more weight to a hypothesis
that moves words across longer distance, which is
penalized by the distance-based model.
In test (b) distortion limit is set as 0; i.e. reorder-
ing is done merely by syntax-based preprocessing.
The worse result is not surprising since, after all,
preprocessing discards many possibilities and thus
reduce the search space of the decoder. Some local
reordering model is still needed during decoding.
Finally, test (c) shows that translation perfor-
mance does not improve significantly by raising the
number of reorderings. This implies that our ap-
proach is very efficient in that only a small value of
n is capable of capturing the most important global
reordering patterns.
8 Conclusion and Future Work
This paper proposes a novel, probabilistic approach
to reordering which combines the merits of syntax
and phrase-based SMT. On the one hand, global
reordering, which cannot be accomplished by the
726
Input ?$3 ,?)Z?C?wOcu??q??X
Reference Meanwhile , Yushchenko and his assistants discussed issues concerning the estab-
lishment of a new government
Baseline The same time , Yushchenko assistants and a new Government on issues discussed
Translation with
Preprocessing
The same time , Yushchenko assistants and held discussions on the issue of a new
government
Table 6: Translation Example 2
phrase-based model, is enabled by the tree opera-
tions in preprocessing. On the other hand, local re-
ordering is preserved and even strengthened in our
approach. Experiments show that, for the NIST MT-
05 task of Chinese-to-English translation, the pro-
posal leads to BLEU improvement of 1.56%.
Despite the encouraging experiment results, it
is still not very clear how the syntax-based and
distance-based models complement each other in
improving word reordering. In future we need to
investigate their interaction and identify the contri-
bution of each component. Moreover, it is observed
that the parse trees returned by a full parser like
the Stanford parser contain too many nodes which
seem not be involved in desired reorderings. Shal-
low parsers should be tried to see if they improve
the quality of reordering knowledge.
References
Yaser Al-Onaizan, and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. Pro-
ceedings for ACL 2006.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. Proceedings for ACL 2005.
M.R. Costa-jussa`, and J.A.R. Fonollosa. 2006. Statis-
tical Machine Reordering. Proceedings for EMNLP
2006.
Heidi Fox. 2002. Phrase Cohesion and Statistical Ma-
chine Translation. Proceedings for EMNLP 2002.
Jianfeng Gao, Mu Li, and Chang-Ning Huang 2003.
Improved Source-Channel Models for Chinese Word
Segmentation. Proceedings for ACL 2003.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings for ACL 2003.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-based Translation. Proceedings for
HLT-NAACL 2003.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. Proceedings for AMTA 2004.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
Proceedings for IWSLT 2005.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. Proceedings for ACL
2003.
Franz J. Och, and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. Proceedings for ACL 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. Proceedings for ACL
2002.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. Proceedings for ACL 2005.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings for the Interna-
tional Conference on Spoken Language Understand-
ing 2002.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. Proceed-
ings for ACL 2004.
Fei Xia, and Michael McCord 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. Proceedings for COLING 2004.
Kenji Yamada, and Kevin Knight. 2001. A syntax-
based statistical translation model. Proceedings for
ACL 2001.
Le Zhang. 2004. Maximum Entropy
Modeling Toolkit for Python and C++.
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
727
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 189?192,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pragmatic Chinese Word Segmentation System 
Wei Jiang, Yi Guan, Xiao-Long Wang 
School of Computer Science and Technology, Harbin Institute of Technology,  
Heilongjiang Province, 150001, P.R.China 
jiangwei@insun.hit.edu.cn 
 
Abstract 
This paper presents our work for partici-
pation in the Third International Chinese 
Word Segmentation Bakeoff. We apply 
several processing approaches according 
to the corresponding sub-tasks, which are 
exhibited in real natural language. In our 
system, Trigram model with smoothing 
algorithm is the core module in word 
segmentation, and Maximum Entropy 
model is the basic model in Named En-
tity Recognition task. The experiment 
indicates that this system achieves F-
measure 96.8% in MSRA open test in the 
third SIGHAN-2006 bakeoff. 
1 Introduction 
Word is a logical semantic and syntactic unit in 
natural language. Unlike English, there is no de-
limiter to mark word boundaries in Chinese lan-
guage, so in most Chinese NLP tasks, word seg-
mentation is a foundation task, which transforms 
Chinese character string into word sequence. It is 
prerequisite to POS tagger, parser or further ap-
plications, such as Information Extraction, Ques-
tion Answer system. 
Our system participated in the Third Interna-
tional Chinese Word Segmentation Bakeoff, 
which held in 2006. Compared with our system 
in the last bakeoff (Jiang 2005A), the system in 
the third bakeoff is adjusted intending to have a 
better pragmatic performance. This paper mainly 
focuses on describing two sub-tasks: (1) The ba-
sic Word Segmentation; (2) Named entities rec-
ognition. We apply different approaches to solve 
above two tasks, and all the modules are inte-
grated into a pragmatic system (ELUS). 
2 System Description 
All the words in our system are categorized into 
five types: Lexicon words (LW), Factoid words 
(FT), Morphologically derived words (MDW), 
Named entities (NE), and New words (NW). 
Figure 1 demonstrates our system structure.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The input character sequence is converted into 
one or several sentences, which is the basic deal-
ing unit. The ?Basic Segmentation? is used to 
identify the LW, FT, MDW words, and ?Named 
Entity Recognition? is used to detect NW words. 
We don?t adopt the New Word detection algo-
rithm in our system in this bakeoff. The ?Disam-
biguation? module performs to classify compli-
cated ambiguous words, and all the above results 
are connected into the final result, which is de-
noted by XML format. 
2.1 Trigram and Smoothing Algorithm 
We apply the trigram model to the word segmen-
tation task (Jiang 2005A), and make use of Ab-
solute Smoothing algorithm to overcome the 
sparse data problem. 
Trigram model is used to convert the sentence 
into a word sequence. Let w = w1 w2 ?wn be a 
word sequence, then the most likely word se-
quence w* in trigram is: 
?
=
??=
n
i
iii
www
wwwP
n 1
12 )|(maxarg*
21 L
w                   (1) 
where let P(w0|w-2 w-1) be P(w0) and let P(w1|w-1 
w0) be P(w1|w0), and wi represents LW or a type 
of FT or MDW. In order to search the best seg-
mentation way, all the word candidates are filled 
in the word lattice (Zhao 2005). And the Viterbi 
Basic 
Segmentation
Disambiguation
Sentence
Named Entity 
Recognition
Word 
Sequence
Factoid Detect
Lexicon Words
Morphological 
Word
Input 
Sequence
Figure 1 ELUS Segmenter and NER 
189
algorithm is used to search the best word seg-
mentation path.  
FT and MDW need to be detected when con-
structing word lattice (detailed in section 2.2). 
The data structure of lexicon can affect the effi-
ciency of word segmentation, so we represent 
lexicon words as a set of TRIEs, which is a tree-
like structure. Words starting with the same 
character are represented as a TRIE, where the 
root represents the first Chinese character, and 
the children of the root represent the second 
characters, and so on (Gao 2004). 
When searching a word lattice, there is the 
zero-probability phenomenon, due to the sparse 
data problem. For instance, if there is no cooc-
curence pair ???/?/???(we eat bananas) in 
the training corpus, then P(??|????) = 0. 
According to formula (1), the probability of the 
whole candidate path, which includes ???/?/
??? is zero, as a result of the local zero prob-
ability. In order to overcome the sparse data 
problem, our system has applied Absolute Dis-
counting Smoothing algorithm (Chen, 1999). 
|}0)(:{|)( 1 1
1
11 >=? ? +?? +?+ ii niii ni wwcwwN             (2) 
The notation N1+ is meant to evoke the number 
of words that have one or more counts, and the ?  
is meant to evoke a free variable that is summed 
over. The function ()c  represents the count of 
one word or the cooccurence count of multi-
words. In this case, the smoothing probability 
? +?+?? +?
?=
iw
i
ni
i
nii
nii
wc
Dwc
wwp
)(
}0,)(max{
)|(
1
11
1  
)|()1( 1 2
? +??+ i nii wwp?          (3)    
where, 
??
?
?
?
??
?
?
?
?=? ? +?+
+?? )()(1 1 111 i niw i ni wNwc
D
i
?     (4) 
Because we use trigram model, so the maxi-
mum n may be 3. A fixed discount D (0 ?D ? 1) 
can be set through the deleted estimation on the 
training data. They arrive at the estimate 
21
1
2nn
nD +=                                              (5) 
where n1 and n2 are the total number of n-grams 
with exactly one and two counts, respectively. 
After the basic segmentation, some compli-
cated ambiguous segmentation can be further 
disambiguated. In trigram model, only the previ-
ous two words are considered as context features, 
while in disambiguation processing, we can use 
the Maximum Entropy model fused more fea-
tures (Jiang 2005B) or rule based method. 
2.2 Factoid and Morphological words 
All the Factoid words can be represented as regu-
lar expressions. So the detection of factoid words 
can be achieved by Finite State Automaton(FSA). 
In our system, the following categories of factoid 
words can be detected, as shown in table 1. 
Table 1 Factoid word categories 
FT type Factoid word Example 
Number
Integer, real, 
percent  etc. 
2910, 46.12%, ??
?, ?????? 
Date Date 2005? 5? 12? 
Time Time 8:00, ????? 
English English word, How, are, you 
www Website, IP address  
http://www.hit.edu.cn
192.168.140.133 
email Email elus@google.com 
phone Phone, fax 0451-86413322 
Deterministic FSA (DFA) is efficient because 
a unique ?next state? is determined, when given 
an input symbol and the current state. While it is 
common for a linguist to write rule, which can be 
represented directly as a non-deterministic FSA 
(NFA), i.e. which allows several ?next states? to 
follow a given input and state.  
Since every NFA has an equivalent DFA, we 
build a FT rule compiler to convert all the FT 
generative rules into a DFA. e.g.  
z ?< digit > -> [0..9]; 
z < year > ::= < digit >{< digit >+}??; 
z < integer > ::= {< digit >+}; 
where ?->? is a temporary generative rule, and 
?::=? is a real generative rule. 
As for the morphological words, we erase the 
dealing module, because the word segmentation 
definition of our system adopts the PKU standard. 
3 Named Entity Recognition 
We adopt Maximum Entropy model to perform 
the Named Entity Recognition. The extensive 
evaluation on NER systems in recent years (such 
as CoNLL-2002 and CoNLL-2003) indicates the 
best statistical systems are typically achieved by 
using a linear (or log-linear) classification algo-
rithm, such as Maximum Entropy model, to-
gether with a vast amount of carefully designed 
linguistic features. And this seems still true at 
present in terms of statistics based methods. 
Maximum Entropy model (ME) is defined 
over H? T in segmentation disambiguation, 
where H is the set of possible contexts around 
target word that will be tagged, and T is the set of 
allowable tags, such as B-PER, I-PER, B-LOC, 
I-LOC etc. in our NER task. Then the model?s 
conditional probability is defined as 
190
? ?= Tt thp
thphtp
'
)',(
),()|(                                (6) 
where ?
=
=
k
j
thf
j
jthp
1
),(),( ???                              (7) 
where h is the current context and t is one of the 
possible tags.  
The several typical kinds of features can be 
used in the NER system. They usually include 
the context feature, the entity feature, and the 
total resource or some additional resources.  
Table 2 shows the context feature templates. 
Table2 NER feature template1 
Type Feature Template 
One order feature wi-2, wi-1, wi, wi+1, wi+2 
Two order feature wi-1:i, wi:i+1 
NER tag feature t i-1 
While, we only point out the local feature 
template, some other feature templates, such as 
long distance dependency templates, are also 
helpful to NER performance. These trigger fea-
tures can be collected by Average Mutual Infor-
mation or Information Gain algorithm etc. 
Besides context features, entity features is an-
other important factor, such as the suffix of Lo-
cation or Organization. The following 8 kinds of 
dictionaries are usually useful (Zhao 2006): 
Table 3 NER resource dictionary2  
List Type Lexicon Example 
Place lexicon ??, ??, ???Word list 
Chinese surname ?, ?, ?, ?? 
Prefix of PER ?, ?, ? 
Suffix of PLA ?, ?, ?, ?, ? String list 
Suffix of ORG ?, ??, ??, ? 
Character for CPER ?,?, ?, ?, ? 
Character for FPER ?, ?, ?, ?, ? Character list 
Rare character ?, ?, ? 
In addition, some external resources may im-
prove the NER performance too, e.g. we collect a 
lot of entities for Chinese Daily Newspaper in 
2000, and total some entity features. 
However, our system is based on Peking Uni-
versity (PKU) word segmentation definition and 
PKU NER definition, so we only used the basic 
features in table 2 in this bakeoff.  Another effect 
is the corpus: our system is training by the Chi-
nese Peoples? Daily Newspaper corpora in 1998, 
which conforms to PKU NER definition. In the 
section 4, we will give our system performance 
with the basic features in Chinese Peoples? Daily 
Newspaper corpora. 
                                                 
1 wi ? current word, wi-1 ? previous word, ti ? current tag. 
2 Partial translation: ?? BeiJing,?? New york;? Zhang, 
?Wang; ? Old;? mountain,? lake;? bureau. 
4 Performance and analysis 
4.1 The Evaluation in Word Segmentation 
The performance of our system in the third bake-
off is presented in table 4 in terms of recall(R), 
precision(P) and F score in percentages. The 
score software is standard and open by SIGHAN. 
Table 4 MSRA test in SIGHAN2006 (%) 
MSRA R P F OOV Roov Riv
Close 96.3 91.8 94.0 3.4 17.5 99.1
Open 97.7 96.0 96.8 3.4 62.4 98.9
Our system has good performance in terms of 
Riv measure. The Riv measure in close test and in 
open test are 99.1% and 98.9% respectively. This 
good performance owes to class-based trigram 
with the absolute smoothing and word disam-
biguation algorithm. 
In our system, it is the following reasons that 
the open test has a better performance than the 
close test: 
(1) Named Entity Recognition module is 
added into the open test system. And Named En-
tities, including PER, LOC, ORG, occupy the 
most of the out-of-vocabulary words. 
(2) The system of close test can only use the 
dictionary that is collected from the given train-
ing corpus, while the system of open test can use 
a better dictionary, which includes the words that 
exist in MSRA training corpus in SIGHAN2005. 
And we know, the dictionary is the one of impor-
tant factors that affects the performance, because 
the LW candidates in the word lattice are gener-
ated from the dictionary. 
As for the dictionary, we compare the two col-
lections in SIGHAN2005 and SIGHAN2006, and 
evaluating in SIGHAN2005 MSRA close test. 
There are less training sentence in SIGHAN2006, 
as a result, there is at least 1.2% performance 
decrease. So this result indicates that the diction-
ary can bring an important impact in our system. 
Table 5 gives our system performance in the 
second bakeoff. We?ll make brief comparison. 
Table 5 MSRA test in SIGHAN 2005 (%) 
MSRA R P F OOV Roov Riv
Close 97.3 94.5 95.9 2.6 32.3 99.1
Open 98.0 96.5 97.2 2.6 59.0 99.0
Comparing table 4 with table 5, we find that 
the OOV is 3.4 in third bakeoff, which is higher 
than the value in the last bakeoff. Obviously, it is 
one of reasons that affect our performance. 
In addition, based on pragmatic consideration, 
our system has been made some simplifier, for 
instance, we erase the new word detection algo-
rithm and the is no morphological word detection. 
191
4.2 Named Entity Recognition 
In MSRA NER open test, our NER system is 
training in prior six-month corpora of Chinese 
Peoples? Daily Newspaper in 1998, which were 
annotated by Peking University. Table 6 shows 
the NER performance in the MSRA open test. 
Table 6 The NER performance in MSRA Open test 
MSRA NER Precision Recall  F Score 
PER 93.68% 86.37% 89.87 
LOC 85.50% 59.67% 70.29 
ORG 75.87% 47.48% 58.41 
Overall 86.97% 65.56% 74.76 
As a result of insufficiency in preparing bake-
off, our system is only trained in Chinese Peo-
ples? Daily Newspaper, in which the NER is de-
fined according to PKU standard. However, the 
NER definition of MSRA is different from that 
of PKU, e.g, ???/LOC ???, ??/PER ?
/PER??? in MSRA, are not entities in PKU. 
So the training corpus becomes a main handicap 
to decrease the performance of our system, and it 
also explains that there is much difference be-
tween the recall rate and the precision in table 6. 
Table 7 gives the evaluation of our NER sys-
tem in Chinese Peoples? Daily Newspaper, train-
ing in prior five-month corpora and testing in the 
sixth month corpus. We also use the feature tem-
plates in table 2, in order to make comparison 
with table 6. 
Table 7 The NER test in Chinese Peoples? Daily 
MSRA NER Precision Recall  F Score 
CPN 93.56 90.96 92.24 
FPN 90.42 86.47 88.40 
LOC 91.94 90.52 91.22 
ORG 88.38 84.52 86.40 
Overall 91.35 88.85 90.08 
This experiment indicates that our system can 
have a good performance, if the test corpus and 
the training corpora conform to the condition of 
independent identically distributed attribution. 
4.3 Analysis and Discussion 
Some points need to be further considered: 
(1) The dictionary can bring a big impact to 
the performance, as the LW candidates come 
from the dictionary. However a big dictionary 
can be easily acquired in the real application. 
(2) Due to our technical and insufficiently 
preparing problem, we use the PKU NER defini-
tion, however they seem not unified with the 
MSRA definition. 
(3) Our NER system is a word-based model, 
and we have find out that the word segmentation 
with two different dictionaries can bring a big 
impact to the NER performance. 
(4) We erase the new word recognition algo-
rithm in our system. While, we should explore 
the real annotated corpora, and add new word 
detection algorithm, if it has positive effect. e.g. 
???  ??(lotus prize) can be recognized as one 
word by the conditional random fields model. 
5 Conclusion 
We have briefly described our word segmenta-
tion system and NER system. We use word-
based features in the whole processing. Our sys-
tem has a good performance in terms of Riv 
measure, so this means that the trigram model 
with the smoothing algorithm can deal with the 
basic segmentation task well. However, the result 
in the bakeoff indicates that detecting out-of-
vocabulary word seems to be a harder task than 
dealing with the segmentation-ambiguity task. 
The work in the future will concentrate on two 
sides: improving the NER performance and add-
ing New Word Detection Algorithm. 
References 
HuaPing Zhang, Qun Liu etc. 2003. Chinese Lexical 
Analysis Using Hierarchical Hidden Markov 
Model, Second SIGHAN workshop affiliated with 
4th ACL, Sapporo Japan, pp.63-70. 
Jianfeng Gao, Mu Li et al 2004. Chinese Word Seg-
mentation: A Pragmatic Approach. MSR-TR-2004-
123, November 2004. 
Peng Fuchun, Fangfang Feng and Andrew McCallum. 
Chinese segmentation and new word detection us-
ing conditional random fields. In:COLING 2004. 
Stanley F.Chen and J. Goodman. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language. 13:369-394. 
Wei Jiang, Jian Zhao et al 2005A.Chinese Word 
Segmentation based on Mixing Model. 4th 
SIGHAN Workshop. pp. 180-182.  
Wei Jiang, Xiao-Long Wang, Yi Guan et al 2005B. 
applying rough sets in word segmentation disam-
biguation based on maximum entropy model. Jour-
nal of Harbin Institute of Technology (New Series). 
13(1): 94-98. 
Zhao Jian. 2006. Research on Conditional Probabilis-
tic Model and Its Application in Chinese Named 
Entity Recognition. Ph.D. Thesis. Harbin Institute 
of Technology, China. 
Zhao Yan. 2005. Research on Chinese Morpheme 
Analysis Based on Statistic Language Model. Ph.D. 
Thesis. Harbin Institute of Technology, China. 
192
 Selecting Optimal Feature Template Subset for CRFs 
Xingjun Xu1 and Guanglu Sun2 and Yi Guan1 and  
Xishuang Dong1 and Sheng Li1 
1: School of Computer Science and Technology,  
Harbin Institute of Technology,  
150001, Harbin, China 
2: School of Computer Science and Technology,  
Harbin University of Science and Technology 
150080, Harbin, China 
 xxjroom@163.com; guanglu.sun@gmail.com 
guanyi@hit.edu.cn; dongxishuang@gmail.com 
lisheng@hit.edu.cn 
 
 
 
Abstract 
Conditional Random Fields (CRFs) are the 
state-of-the-art models for sequential labe-
ling problems. A critical step is to select 
optimal feature template subset before em-
ploying CRFs, which is a tedious task. To 
improve the efficiency of this step, we pro-
pose a new method that adopts the maxi-
mum entropy (ME) model and maximum 
entropy Markov models (MEMMs) instead 
of CRFs considering the homology be-
tween ME, MEMMs, and CRFs. Moreover, 
empirical studies on the efficiency and ef-
fectiveness of the method are conducted in 
the field of Chinese text chunking, whose 
performance is ranked the first place in 
task two of CIPS-ParsEval-2009. 
1 Introduction 
Conditional Random Fields (CRFs) are the state-
of-the-art models for sequential labeling problem. 
In natural language processing, two aspects of 
CRFs have been investigated sufficiently: one is to 
apply it to new tasks, such as named entity recog-
nition (McCallum and Li, 2003; Li and McCallum, 
2003; Settles, 2004), part-of-speech tagging (Laf-
ferty et al, 2001), shallow parsing (Sha and Perei-
ra, 2003), and language modeling (Roark et al, 
2004); the other is to exploit new training methods 
for CRFs, such as improved iterative scaling (Laf-
ferty et al, 2001), L-BFGS (McCallum, 2003) and 
gradient tree boosting (Dietterich et al, 2004). 
One of the critical steps is to select optimal fea-
ture subset before employing CRFs. McCallum 
(2003) suggested an efficient method of feature 
induction by iteratively increasing conditional log-
likelihood for discrete features. However, since 
there are millions of features and feature selection 
is an NP problem, this is intractable when search-
ing optimal feature subset. Therefore, it is neces-
sary that selects feature at feature template level, 
which reduces input scale from millions of fea-
tures to tens or hundreds of candidate templates. 
In this paper, we propose a new method that 
adopts ME and MEMMs instead of CRFs to im-
prove the efficiency of selecting optimal feature 
template subset considering the homology between 
ME, MEMMs, and CRFs, which reduces the train-
ing time from hours to minutes without loss of 
performance. 
The rest of this paper is organized as follows. 
Section 2 presents an overview of previous work 
for feature template selection. We propose our op-
timal method for feature template selection in Sec-
tion 3. Section 4 presents our experiments and re-
sults. Finally, we end this paper with some con-
cluding remarks. 
2 Related Work 
Feature selection can be carried out from two le-
vels: feature level (feature selection, or FS), or 
feature template level (feature template selection, 
or FTS). FS has been sufficiently investigated and 
 share most concepts with FTS. For example, the 
target of FS is to select a subset from original fea-
ture set, whose optimality is measured by an eval-
uation criterion (Liu and Yu, 2005). Similarly, the 
target of FTS is to select a subset from original 
feature template set. To achieve optimal feature 
subset, two problems in original set must be elimi-
nated: irrelevance and redundancy (Yu and Liu, 
2004). The only difference between FS and FTS is 
that the number of elements in feature template set 
is much less than that in feature set. 
Liu and Yu (2005) classified FS models into 
three categories: the filter model, the wrapper 
model, and the hybrid model. The filter model 
(Hall 2000; Liu and Setiono, 1996; Yu and Liu, 
2004) relies on general characteristics of the data 
to evaluate and select feature subsets without any 
machine learning model. The wrapper model (Dy 
and Brodley, 2000; Kim et al, 2000; Kohavi and 
John, 1997) requires one predetermined machine 
learning model and uses its performance as the 
evaluation criterion. The hybrid model (Das, 2001) 
attempts to take advantage of the two models by 
exploiting their different evaluation criteria in dif-
ferent search stages. 
There are two reasons to employ the wrapper 
model to accomplish FTS: (1) The wrapper model 
tends to achieve better effectiveness than that of 
the filter model with respect of a more direct eval-
uation criterion; (2) The computational cost is trac-
table because it can reduce the number of subsets 
sharply by heuristic algorithm according to the 
human knowledge. And our method belongs to 
this type. 
Lafferty (2001) noticed the homology between 
MEMMs and CRFs, and chose optimal MEMMs 
parameter vector as a starting point for training the 
corresponding CRFs. And the training process of 
CRFs converges faster than that with all zero pa-
rameter vectors. 
On the other hand, the general framework that 
processes sequential labeling with CRFs has also 
been investigated well, which can be described as 
follows: 
1. Converting the new problem to sequential 
labeling problem; 
2. Selecting optimal feature template subset for 
CRFs; 
3. Parameter estimation for CRFs; 
4. Inference for new data. 
In the field of English text chunking (Sha and 
Pereira, 2003), the step 1, 3, and 4 have been stu-
died sufficiently, whereas the step 2, how to select 
optimal feature template subset efficiently, will be 
the main topic of this paper.  
3 Feature Template Selection 
3.1 The Wrapper Model for FTS 
The framework of FTS based on the wrapper 
model for CRFs can be described as: 
1. Generating the new feature template subset; 
2. Training a CRFs model; 
3. Updating optimal feature template subset if the 
new subset is better; 
4. Repeating step 1, 2, 3 until there are no new 
feature template subsets. 
Let N denote the number of feature templates, 
the number of non-empty feature template subsets 
will be (2N-1). And the wrapper model is unable to 
deal with such case without heuristic methods, 
which contains: 
1. Atomic feature templates are firstly added to 
feature template subset, which is carried out by: 
Given the position i, the current word Wi and the 
current part-of-speech Pi are firstly added to cur-
rent feature template subset, and then Wi-1 and Pi-1, 
or Wi+1 and Pi+1, and so on, until the effectiveness 
is of no improvement. Taking the Chinese text 
chunking as example, optimal atomic feature tem-
plate subset is {Wi-3~Wi+3, Pi-3~Pi+3}; 
2. Adding combined feature templates properly 
to feature template set will be helpful to improve 
the performance, however, too many combined 
feature templates will result in severe data sparse-
ness problem. Therefore, we present three restric-
tions for combined feature templates: (1) A com-
bined feature template that contains more than 
three atomic templates are not allowable; (2) If a 
combined feature template contains three atomic 
feature template, it can only contain at most one 
atomic word template; (3) In a combined template, 
at most one word is allowable between the two 
most adjacent atomic templates; For example, the 
combined feature templates, such as {Pi-1, Pi, Pi+1, 
Pi+2}, {Wi, Wi+1, Pi},  and {Pi-1, Pi+2}, are not al-
lowable, whereas the combined templates, such as 
{Pi, Pi+1, Pi+2}, {Pi-1, Wi, Pi+1}, and {Pi-1, Pi+1}, are 
allowable. 
3. After atomic templates have been added, {Wi-
1, Wi}, or {Wi, Wi+1}, or {Pi-1, Pi}, or {Pi, Pi+1} are 
firstly added to feature template subset. The tem-
plate window is moved forward, and then back-
ward. Such process will repeat with expanding 
template window, until the effectiveness is of no 
improvement. 
 Tens or hundreds of training processes are still 
needed even if the heuristic method is introduced. 
People usually employ CRFs model to estimate the 
effectiveness of template subset However, this is 
more tedious than that we use ME or MEMMs 
instead. The idea behind this lie in three aspects: 
first, in one iteration, the Forward-Backward Al-
gorithm adopted in CRFs training is time-
consuming; second, CRFs need more iterations 
than that of ME or MEMMs to converge because 
of larger parameter space; third, ME, MEMMs, 
and CRFs, are of the same type (log-linear models) 
and based on the same principle, as will be dis-
cussed in detail as follows. 
3.2 Homology of ME, MEMMs and CRFs 
ME, MEMMs, and CRFs are all based on the Prin-
ciple of Maximum Entropy (Jaynes, 1957). The 
mathematical expression for ME model is as for-
mula (1): 
1
1( | ) exp( ( , ))( )
m
i ii
P y x x yZ x f??? ?
    (1) 
, and Z(x) is the normalization factor. 
MEMMs can be considered as a sequential ex-
tension to the ME model. In MEMMs, the HMM 
transition and observation functions are replaced 
by a single function P(Yi|Yi-1, Xi). There are three 
kinds of implementations of MEMMs (McCallum 
et al, 2000) in which we realized the second type 
for its abundant expressiveness. In implementation 
two, which is denoted as MEMMs_2 in this paper, 
a distributed representation for the previous state 
Yi-1 is taken as a collection of features with 
weights set by maximum entropy, just as we have 
done for the observations Xi. However, label bias 
problem (Lafferty et al, 2001) exists in MEMMs, 
since it makes a local normalization of random 
field models. CRFs overcome the label bias prob-
lem by global normalization. 
Considering the homology between CRFs and 
MEMMs_2 (or ME), it is reasonable to suppose 
that a useful template for MEMMs_2 (or ME) is 
also useful for CRFs, and vice versa. And this is a 
necessary condition to replace CRFs with ME or 
MEMMs for FTS. 
3.3 A New Framework for FTS 
Besides the homology of these models, the other 
necessary condition to replace CRFs with ME or 
MEMMs for FTS is that all kinds of feature tem-
plates in CRFs can also be expressed by ME or 
MEMMs. There are two kinds of feature templates 
for CRFs: one is related to Yi-1, which is denoted 
as g(Yi-1, Yi, Xi); the other is not related to Yi-1, 
which is denoted as f(Yi, Xi). Both of them can be 
expressed by MEMMs_2. If there is only the 
second kind of feature templates in the subset, it 
can also be expressed by ME. For example, the 
feature function f(Yi, Pi) in CRFs can be expressed 
by feature template {Pi} in MEMMs_2 or ME; and 
g(Yi-1, Yi, Pi) can be expressed by feature template 
{Yi-1, Pi} in MEMM_2.  
Therefore, MEMMs_2 or ME can be employed 
to replace CRFs as machine learning model for 
improving the efficiency of   FTS. 
Then the new framework for FTS will be: 
1. Generating the new feature template subset; 
2. Training an MEMMs_2 or ME model; 
3. Updating optimal feature template subset 
if the new subset is better; 
4. Repeating step 1, 2, 3 until there are no 
new feature template subsets. 
The wrapper model evaluates the effectiveness 
of feature template subset by evaluating the model 
on testing data. However, there is a serious effi-
ciency problem when decoding a sequence by 
MEMMs_2. Given N as the length of a sentence, 
C as the number of candidate labels, the time 
complexity based on MEMMs_2 is O(NC2) when 
decoding by viterbi algorithm. Considering the C 
different Yi-1 for every word in a sentence, we 
need compute P(Yi|Yi-1, Xi) (N.C) times for 
MEMMs_2. 
Reducing the average number of candidate label 
C can help to improve the decoding efficiency. 
And in most cases, the Yi-1 in P(Yi|Yi-1, Xi) is not 
necessary (Koeling, 2000; Osbome, 2000). There-
fore, to reduce the average number of candidate 
labels C, it is reasonable to use an ME model to 
filter the candidate label. Given a threshold T (0 
<= T <= 1), the candidate label filtering algorithm 
is as follows: 
1. CP = 0; 
2. While CP <= T 
a) Add the most probable candidate label Y? 
to viterbi algorithm; 
b) Delete Y? from the candidate label set; 
c) CP = P(Y?|Xi) + CP. 
If the probability of the most probable candidate 
label has surpassed T, other labels are discarded. 
Otherwise, more labels need be added to viterbi 
algorithm. 
4 Evaluation and Result 
4.1 Evaluation 
We evaluate the effectiveness and efficiency of the 
new framework by the data set in the task two of 
 CIPS-ParsEval-2009 (Zhou and Li, 2010). The 
effectiveness is supported by high F-1 measure in 
the task two of CIPS-ParsEval-2009 (see Figure 1), 
which shows that optimal feature template subset 
driven by ME or MEMMs is also optimal for 
CRFs. The efficiency is shown by significant de-
cline in training time (see Figure 3), where the 
baseline is CRFs, and comparative methods are 
ME or MEMMs. 
We design six subsets of feature template set 
and six experiments to show the effectiveness and 
efficiency of the new framework. As shown in 
Table 1 and Table 2, the 1~3 experiments shows 
the influence of the feature templates, which are 
unrelated to Yi-1, for both ME and CRFs. And the 
4~6 experiments show the influence of the feature 
templates, which are related to Yi-1, for both 
MEMMs_2 and CRFs. In table 1, six template 
subsets can be divided into two sets by relevance 
of previous label: 1, 2, 3 and 4, 5, 6. Moreover, the 
first set can be divided into 1, 2, and 3 by distances 
between features with headwords;  the second set 
can be divided into 4, 5 and 6 by relevance of ob-
served value. In order to ensure the objectivity of 
comparative experiments, candidate label filtering 
algorithm is not adopted. 
 
Figure 1: the result in the task two of CIPS-
ParsEval-2009 
 
 
 
 
1 Wi, Wi-1, Wi-2, Wi+1, Wi+2, Pi, Pi-1, Pi-2, Pi+1, 
Pi+2, Wi-1_Wi, Wi_Wi+1, Wi-1_Wi+1, Pi-1_Pi, 
Pi-2_Pi-1, Pi_Pi+1, Pi-1_Pi+1, Pi-1_Pi_Pi+1, Pi-
2_Pi-1_Pi,     Pi_Pi+1_Pi+2, Wi_Pi+1, Wi_Pi+2, 
Pi_Wi-1, Wi-2_Pi-1_Pi, Pi_Wi+1_Pi+1, Pi-
1_Wi_Pi, Pi_Wi+1 
2 Wi-3, Wi+3, Pi-3, Pi+3, Wi-3_Wi-2, Wi+2_Wi+3, 
Pi-3_Pi-2, Pi+2_Pi+3 
3 Wi-4, Wi+4, Pi-4, Pi+4, Wi-4_Wi-3, Wi+3_Wi+4, 
Pi-4_Pi-3, Pi+3_Pi+4 
4 Yi-1 
5 Yi-1_Pi_Pi+1, Yi-1_Pi, Yi-1_Pi-1_Pi 
6 Yi-1_Pi-4, Yi-1_Pi+4 
Table 1: six subsets of feature template set 
 
id Model FT subset 
1 ME vs. CRFs 1 
2 ME vs. CRFs 1, 2 
3 ME vs. CRFs 1, 2, 3 
4 MEMMs vs. CRFs 1, 2, 4 
5 MEMMs vs. CRFs 1, 2, 4, 5 
6 MEMMs vs. CRFs 1, 2, 4, 5, 6 
Table 2: six experiments 
4.2 Empirical Results 
The F-measure curve is shown in Figure 2. For the 
same and optimal feature template subset, the F-1 
measure of CRFs is superior to that of ME because 
of global normalization; and it is superior to that of 
MEMMs since it overcomes the label bias. 
 
Figure 2: the F-measure curve 
 
 
Figure 3: the training time curve 
 
The significant decline in training time of the 
new framework is shown in Figure 3, while the 
testing time curve in Figure 4 and the total time 
curve in Figure 5. The testing time of ME is more 
 than that of CRFs because of local normalization; 
and the testing time of MEMMs_2 is much more 
than that of CRFs because of N.C times of P(Yi|Yi-
1, Xi) computation. 
 
 
Figure 4: the testing time curve 
 
Figure 5: the total time curve 
All results of ME and MEMMs in figures are 
represented by the same line because perfor-
mances of these two models are the same when 
features are only related to observed values. 
5 Conclusions 
In this paper, we propose a new optimal feature 
template selection method for CRFs, which is car-
ried out by replacing the CRFs with MEMM_2 
(ME) as the machine learning model to address the 
efficiency problem according to the homology of 
these models. Heuristic method and candidate la-
bel filtering algorithm, which can improve the ef-
ficiency of FTS further, are also introduced. The 
effectiveness and efficiency of the new method is 
confirmed by the experiments on Chinese text 
chunking.  
Two problems deserve further study: one is to 
prove the homology of ME, MEMMs, and CRFs 
theoretically; the other is to expand the method to 
other fields. 
For any statistical machine learning model, fea-
ture selection or feature template selection is a 
computation-intensive step. This work can be ade-
quately reduced by means of analyzing the homol-
ogy between models and using the model with less 
computation amount. Our research proves to be a 
successful attempt. 
References 
Das Sanmay. 2001. Filters, wrappers and a boosting-
based hybrid for feature selection. In Proceedings of 
the Eighteenth International Conference on Machine 
Learning, pages 74?81. 
Dietterich Thomas G., Adam Ashenfelter, Yaroslav 
Bulatov. 2004. Training Conditional Random Fields 
via Gradient Tree Boosting. In Proc. of the 21th In-
ternational Conference on Machine Learning 
(ICML). 
Dy Jennifer G., and Carla E. Brodley. 2000. Feature 
subset selection and order identification for unsuper-
vised learning. In Proceedings of the Seventeenth In-
ternational Conference on Machine Learning, pages 
247?254. 
Hall Mark A.. 2000. Correlation-based feature selection 
for discrete and numeric class machine learning. In 
Proceedings of the Seventeenth International Confe-
rence on Machine Learning, pages 359?366. 
Jaynes, Edwin T.. 1957. Information Theory and Statis-
tical Mechanics. Physical Review 106(1957), May. 
No.4, pp. 620-630. 
Kim YongSeog, W. Nick Street and Filippo Menczer. 
2000. Feature Selection in Unsupervised Learning 
via Evolutionary Search. In Proceedings of the Sixth 
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining, pages 365?369. 
Koeling Rob. 2000. Chunking with Maximum Entropy 
Models. In Proceeding of CoNLL-2000 and LLL-
2000, Lisbon, Portugal, 2000, pp. 139-141. 
Kohavi Ron, and George H. John. 1997. Wrappers for 
feature subset selection. Artificial Intelligence, 97(1-
2):273?324. 
Lafferty John, Andrew McCallum, and Fernando Perei-
ra. 2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
Proceedings of the Eighteenth International Confe-
rence on Machine Learning. 
Li Wei, and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition using 
Conditional Random Fields and Feature Induction. 
ACM Transactions on Asian Language Information 
Processing (TALIP).  
Liu Huan, and Lei Yu. 2005. Toward Integrating Fea-
ture Selection Algorithms for Classification and 
Clustering. IEEE Transactions on knowledge and 
Data Engineering, v.17 n.4, p.491-502. 
Liu Huan, and Rudy Setiono. 1996. A probabilistic ap-
proach to feature selection - a filter solution. In Pro-
 ceedings of the Thirteenth International Conference 
on Machine Learning, pages 319?327. 
McCallum Andrew. 2003. Efficiently Inducing Features 
of Conditional Random Fields. In Proceedings of the 
Nineteenth Conference on Uncertainty in Artificial 
Intelligence. 
McCallum Andrew, DAyne Freitag, Fernando Pereira. 
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proceedings 
of ICML'2000, Stanford, CA, USA, 2000, pp. 591-
598. 
McCallum Andrew, and Wei Li. 2003. Early Results for 
Named Entity Recognition with Conditional Random 
Fields, Feature Induction and Web-Enhanced Lex-
icons. In Proceedings of The Seventh Conference on 
Natural Language Learning (CoNLL-2003), Edmon-
ton, Canada.  
Osbome Miles. 2000. Shallow Parsing as Part-of-
speech Tagging. In Proceeding of CoNLL-2000 and 
LLL-2000, Lisbon, Portugal, 2000,pp. 145-147. 
Roark Brian, Murat Saraclar, Michael Collins, and 
Mark Johnson. 2004. Discriminative language mod-
eling with conditional random fields and the percep-
tron algorithm. Proceedings of the 42nd Annual 
Meeting of the Association for Computational Lin-
guistics. 
Settles Burr. 2004. Biomedical Named Entity Recogni-
tion Using Conditional Random Fields and Rich Fea-
ture Sets. COLING 2004 International Joint Work-
shop on Natural Language Processing in Biomedi-
cine and its Applications (NLPBA). 
Sha Fei, and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. Proceedings of the 
2003 conference of the North American Chapter of 
the Association for Computational Linguistics on 
Human Language Technology, Edmonton, Canada. 
Yu Lei, and Huan Liu. 2004. Feature selection for high-
dimensional data: a fast correlation-based filter solu-
tion. In Proceedings of the twentieth International 
Conference on Machine Learning, pages 856?863. 
Zhou Qiang, and Yumei Li. 2010. Chinese Chunk Pars-
ing Evaluation Tasks. Journal of Chinese Informa-
tion Processing. 
 Complete Syntactic Analysis Based on Multi-level Chunking 
ZhiPeng Jiang and Yu Zhao and Yi Guan and  
Chao Li and Sheng Li 
School of Computer Science and Technology,  
Harbin Institute of Technology,  
150001, Harbin, China 
 xyf-3456@163.com; woshizhaoy@gmail.com 
guanyi@hit.edu.cn; beyondlee2008@yahoo.cn 
lisheng@hit.edu.cn 
 
 
 
Abstract 
This paper describes a complete syntactic 
analysis system based on multi-level 
chunking. On the basis of the correct se-
quences of Chinese words provided by 
CLP2010, the system firstly has a Part-of-
speech (POS) tagging with Conditional 
Random Fields (CRFs), and then does the 
base chunking and complex chunking with 
Maximum Entropy (ME), and finally gene-
rates a complete syntactic analysis tree. 
The system took part in the Complete Sen-
tence Parsing Track of the Task 2 Chinese 
Parsing in CLP2010, achieved the F-1 
measure of 63.25% on the overall analysis, 
ranked the sixth; POS accuracy rate of 
89.62%, ranked the third. 
1 Introduction 
Chunk is a group of adjacent words which belong 
to the same s-projection set in a sentence, whose 
syntactic structure is actually a tree (Abney, 1991), 
but apart from the root node, all other nodes are 
leaf nodes. Complete syntactic analysis requires a 
series of analyzing processes, eventually to get a 
full parsing tree. Parsing by chunks is proved to be 
feasible (Abney, 1994). 
The concept of chunking was first proposed by 
Abney in 1991, who defined chunks in terms of 
major heads, and parsed by chunks in 1994 (Ab-
ney, 1994). An additional chunk tag set {B, I, O} 
was added to chunking (Ramshaw and  Marcus, 
1995), which limited dependencies between ele-
ments in a chunk, changed chunking into a ques-
tion of sequenced tags, to promote the develop-
ment of chunking. Chunking algorithm was ex-
tended to the bottom-up parser, which is trained 
and tested on the Wall Street Journal (WSJ) part of 
the Penn Treebank (Marcus, Santorini and Mar-
cinkiewicz 1993), and achieved a performance of 
80.49% F-measure, the results show that it per-
formed better than a standard probabilistic con-
text-free grammar, and can improve performance 
by adding the information of parent node (Sang, 
2000). 
On Chinese parsing, Maximum Entropy Model 
was first used to have a POS tagging and chunking, 
and then a full parsing tree was generated (Fung, 
2004), training and testing in the Penn Chinese 
Treebank, which achieved 79.56% F-measure. The 
parsing process was divided into POS tagging, 
base chunking and complex chunking, having a 
POS tagging and chunking on a given sentence, 
and then looping the process of complex chunking 
up to identify the root node (Li and Zhou, 2009). 
This parsing method is the basis of this paper. In 
addition, we have the existing Chinese chunking 
system in laboratory, which ranked first in Task 2: 
Chinese Base Chunking of CIPS-ParsEval-2009, 
so we try to apply chunking to complete syntactic 
analysis in CLP2010, to achieve better results. 
We will describe the POS tagging based on 
CRFs in Section 2, including CRFs, feature tem-
plate selection and empirical results. Multi-level 
chunking based on ME will be expounded in Sec-
tion 3, including ME, MEMM, base chunking and 
complex chunking. Finally, we will summarize our 
work in Section 4. 
 2 POS Tagging Based on CRFs 
2.1 Conditional Random Fields 
X is a random variable over data sequences to be 
labeled, and Y is a random variable over corres-
ponding label sequences. All components Yi of Y 
are assumed to range over a finite label alphabet. 
For example, X might range over natural language 
sentences and Y range over part-of-speech tags of 
those sentences, a finite label alphabet is the set of 
possible part-of-speech tags (Lafferty and McCal-
lum and Pereira, 2001). CRFs is represented by the 
local feature vector f and the corresponding weight 
vector, f is divided into the state feature s (y, x, i) 
and transfer feature t (y, y', x, i), where y and y' are 
possible POS tags, x is the current input sentence, i 
is the position of current term (Jiang and Guan and 
Wang, 2006). Formalized as follows: 
s (y, x, i) = s (yi, x, i)                      (1) 
? ?
? ?1, , , 1
, ,
0 1
i it y y x i i
t y x i
i
?? ??? ?
? ??
      (2) 
By the local feature of the formula (1) and (2), 
the global features of x and y: 
? ? ? ?, , ,iF y x f y x i??              (3) 
At this point of (X, Y), the conditional probabil-
ity distribution of CRFs: 
? ? ? ?? ?? ?
exp ,| F Y Xp Y X Z X? ?
?
?
?       (4) 
where ? ? ? ?? ?exp ,yZ x F y x? ?? ??  is a fac-
tor for normalizing. For the input sentence x, the 
best sequence of POS tagging: 
? ?arg max |
y
y p y x?? ?
 
2.2 Feature Template Selection 
We use the template as a baseline which is taken 
by Yang (2009) in CIPS-ParsEval-2009, directly 
testing the performance, whose accuracy was 
93.52%. On this basis, we adjust the feature tem-
plate through the experiment, and improve the 
tagging accuracy of unknown words by introduc-
ing rules, in the same corpus for training and test-
ing, accuracy is to 93.89%. Adjusted feature tem-
plate is shown in Table 1, in which the term pre is 
the first character in current word, suf is the last 
character of current word, num is the number of 
characters of current word, pos-1 is the tagging re-
sults of the previous word. 
 
Table 1: feature template 
feature template 
w2,w1,w0,w-1,w-2,w+1w0,w0w-1,pre0, pre0w0,suf0, 
w0suf0,num,pos-1 
2.3 Empirical Results and Analysis 
We divide the training data provided by CLP2010 
into five folds, the first four of which are train cor-
pus, the last one is test corpus, on which we use 
the CRF++ toolkit for training and testing. Tag-
ging results with different features are shown in 
table 2. 
Table 2: tagging results with different features 
Model Explain Accuracy 
CRF baseline 93.52% 
CRF1 add w-1, pos-1 93.58% 
CRF2 add num 93.66% 
CRF3 add num, w-1, pos-1 93.68% 
CRF4 add num, rules 93.80% 
CRF5 add num, w-1, pos-1, rules 93.89% 
Tagging results show that the number of charac-
ter and POS information can be added to improve 
the accuracy of tagging, but in CLP2010, the tag-
ging accuracy is only 89.62%, on the one hand it 
may be caused by differences of corpus, on the 
other hand it may be due to that we don?t use all 
the features of CRFs but remove the features 
which appear one time in order to reduce the train-
ing time. 
3 Multi-level Chunking Based on ME 
3.1 Maximum Entropy Models and Maxi-
mum Entropy Markov Models 
Maximum entropy model is mainly used to esti-
mate the unknown probability distribution whose 
entropy is the maximum under some existing con-
ditions. Suppose h is the observations of context, t 
is tag, the conditional probability p (t | h) can be 
expressed as: 
exp( ( , ))( | ) ( )
ii i t hP t h Z h
f?? ?
 
where fi is the feature of model,  
( ) exp( ( , ))i it iZ h t hf??? ? 
 is a factor for nor-
malizing. 
i? is weigh of feature fi, training is the 
process of seeking the value of 
i? .  
Maximum entropy Markov model is the seria-
lized form of Maximum entropy model (McCal-
lum and Freitag and Pereira, 2000), for example, 
transition probabilities and emission probabilities 
are merged into a single conditional probability 
 function 
1( | , )i iP t t h?  in binary Maximum entropy 
Markov model, 
1( | , )i iP t t h?  is turned to ( | )p t h  to 
be solved by adding features which can express 
previously tagging information (Li and Sun and 
Guan, 2009). 
3.2 Base Chunking 
Following the method of multi-level chunking, we 
first do the base chunking on the sentences which 
are through the POS tagging, then loop the process 
of complex chunking until they can?t be merged. 
We use the existing Chinese base chunking system 
to do base chunking in laboratory, which marks 
boundaries and composition information of chunk 
with MEMM, and achieved 93.196% F-measure in 
Task 2: Chinese Base Chunking of CIPS-ParsEval 
-2009. The input and output of base chunking are 
as follows: 
Input???/nS ??/a  ??/n ?/v ??/nR  ?
?/n ?/p ??/n ?/uJDE ??/n ?/wD ??/n 
??/vN ?/f ?/wP ??/d  ??/v ?/wP ??/d  
??/v ?/c ??/d  ??/v ?/uJDE ??/v ??
/a  ??/n ??/n ?/uJDE ??/n  ??/n ?/wE 
Output???/nS [np ??/a  ??/n ] ?/v [np ?
?/nR  ??/n ] ?/p ??/n ?/uJDE [np ??
/n  ?/wD ??/n ] ??/vN ?/f ?/wP [vp ??
/d  ??/v ] ?/wP [vp ??/d  ??/v ] ?/c [vp 
??/d  ??/v ] ?/uJDE ??/v [np ??/a  ??
/n ] ??/n ?/uJDE [np ??/n  ??/n ] ?/wE 
3.3 Complex Chunking 
We take the sentences which are through POS tag-
ging and base chunking as input, using Li?s tag-
ging method and feature template. Categories of 
complex chunk include xx_Start, xx_Middle, 
xx_End and Other, where xx is a category of arbi-
trary chunk. The process of complex chunking is 
shown as follows: 
Step 1: input the sentences which are through POS 
tagging and base chunking, for example: 
??/nS  [np ??/a  ??/n  ] ?/uJDE  [np ??
/vN  ??/vN  ] ?/c  [np ??/n  ??/n  ]  
Step 2: if there are some category tags in the sen-
tence, then turn a series of tags to brackets, for 
instance, if continuous cells are marked as 
xx_Start, xx_Middle, ..., xx_Middle, xx_End, then 
the combination of continuous cells is a complex 
chunk xx; 
Step 3: determine the head words with the set of 
rules, and compress the sentence: 
??/nS  [np ??/n  ] ?/uJDE  [np ??/vN  ] ?
/c  [np ??/n  ] 
Step 4: if the sentence can be merged, mark the 
sentence with ME, then return step 2, else the 
analysis process ends: 
?? /nS@np_Start  [np ?? /n  ]@np_End ?
/uJDE@Other  [np ?? /vN  ]@np_Start ?
/c@np_Middle  [np ??/n  ]@np_End 
At last, the output is: 
[np [np ??/nS  [np ??/a  ??/n  ] ] ?/uJDE  
[np [np ??/vN  ??/vN  ] ?/c  [np ??/n  ?
?/n  ] ] ] 
Following the above method, we first use the 
Viterbi decoding, but in the decoding process we 
encountered two problems:  
1. Similar to the label xx_Start, whose back is only 
xx_Middle or xx_End, so the probability of 
xx_Start label turning to Other is 0, But, if only 
using ME to predict, the probability may not be 0.  
2. Viterbi decoding can?t solve that all the labels of 
predicted results are Other, if all labels are Other, 
they can?t be merged, this result doesn?t make 
sense. 
Solution:  
For the first question, we add the initial transfer 
matrix and the end transfer matrix in decoding 
process, that is, the corresponding xx_Middle or 
xx_End of xx_Start is seted to 1 in the transfer 
matrix, the others are marked as 0, matrix multip-
lication is taken during the state transition. It can 
effectively avoid errors caused by probability to 
improve accuracy.  
To rule out the second question, we use heuris-
tic search approach to decode, and exclude all 
Other labels with the above matrix. In addition, we 
defined another ME classifier to do some pruning 
in the decoding process, the features of ME clas-
sifier are POS, the head word, the POS of head 
word. The pseudo-code of Heuristic search is: 
While searching priority queue is not empty 
Take the node with the greatest priority in the 
queue; 
If the node?s depth = length of the chunking 
results 
Searching is over, reverse the search-
ing path to get searching results; 
Else 
Compute the probability of all candi-
date children nodes according to 
the current probability; 
Record searching path; 
Press it into the priority queue; 
In addition, we found that some punctuation at 
the end of a sentence can?t be merged, probably 
due to sparseness of data, according to that the 
tone punctuation (period, exclamation mark, ques-
 tion mark) at the end of the sentence can be added 
to implement a complete sentence (zj) (Zhou, 
2004), we carried out a separate deal with this sit-
uation, directly add punctuation at the end of the 
sentence, to form a sentence. 
In training data provided by CLP2010 in sub-
task: Complete Sentence Parsing, the head words 
aren?t marked. We can?t use the statistical method 
to determine the head words, but only by rules. We 
take Li?s rule set as baseline, but the rule set was 
used to supplement the statistical methods, so 
some head words don?t appear in the rule set, re-
sulting in many head words are marked as NULL, 
for this situation, we add some rules through expe-
riment, Table 3 lists some additional rules. 
Table 3: increasing part of rules 
parent  head words 
vp vp, vB, vSB, vM, vJY, vC, v 
ap a, b, d 
mp qN, qV, qC, q 
dj vp, dj, ap, v, fj 
dlc vp 
mbar m, mp 
3.4 Empirical Results and Analysis 
We take the corpus which are through correct POS 
tagging and base chunking for training and testing, 
it is divided into five folds, the first four as train-
ing corpus, the last one as testing corpus, using the 
existing ME toolkit to train and test model in la-
boratory. Table 4 shows the results on Viterbi de-
coding and Heuristic Search method, where head 
words are determined by rules. 
Table 4: results with different decoding 
Decoding Accuracy Recall Fmeasure 
Viterbi  84.87% 84.47% 84.67% 
Heuristic 
Search 
85.62% 85.19% 85.40% 
The system participated in the Complete Sen-
tence Parsing of CLP2010, results are shown in 
Table 5 below. Because we can?t determine the 
head words by statistical method on the corpus 
provided by CLP2010, resulting in the accuracy 
decreasing, creating a great impact on results. 
Table 5: the track results 
Training 
mode 
Model use F-measure POS 
Accuracy 
Closed Single 63.25% 89.62% 
4 Conclusions 
In this paper, we use CRFs to have a POS tagging, 
and increase the tagging accuracy by adjusting the 
feature template; multi-level chunking is applied 
to complete syntactic analysis, we do the base 
chunking with MEMM to recognize boundaries 
and components, and make the complex chunking 
with ME to generate a full parsing tree; on decod-
ing, we add transfer matrix to improve perfor-
mance, and remove some features with a ME clas-
sifier to reduce training time.  
As the training data are temporarily changed, 
our system?s training on the Event Description 
Sub-sentence Analysis of CLP2010 isn?t com-
pleted, and head words are marked in the training 
corpus of this task, so our next step will be to 
complete training and testing of this task, compare 
the existing evaluation results, and use ME clas-
sifier to determine head words, analyze impact of 
head words on system. On the POS tagging, we 
will retain all features to train and compare tag-
ging results. 
Acknowledgement 
We would like to thank XingJun Xu and BenYang 
Li for their valuable advice to our work in Com-
plete Sentence Parsing of CLP2010. We also thank 
JunHui Li, XiaoRui Yang and HaiLong Cao for 
paving the way for our work. 
References 
S. Abney (1991) Parsing by Chunks. Kluwer Academic 
Publishers, Dordrecht, 257-278 
Lance A. Ramshaw, Mitchell P. Marcus (1995) Text 
Chunking Using Transformation-Based Learning. In 
Proceeding of the Third ACL Workshop on Very 
Large Corpora, USA, 87-88 
Erik F. Tjong Kim Sang (2001) Transforming a Chunk-
er to a Parser. Computational Linguistics in the 
Netherlands 2000, 6-8 
YongSheng Yang, BenFeng Chen (2004) A Maximum-
Entropy Chinese Parser Augmented by Transforma-
tion-Based Learning. ACM Transactions on Asian 
Language Information Processing, 4-8 
John Lafferty, Andrew McCallum, and Fernando Perei-
ra (2001) Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
Proceedings of the Eighteenth International Confe-
rence on Machine Learning, 282-289 
Junhui Li, Guodong Zhou (2009) Soochow University 
Report for the 1st China Workshop on Syntactic 
Parsing. CIPS-ParsEval-2009, 5-8 
Wei Jiang, Yi Guan, and Xiaolong Wang (2006) Condi-
tional Random Fields Based POS Tagging.Computer 
Engineering and Applications, 14-15 
 Xiaorui Yang, Bingquan Liu, Chengjie Sun, and Lei 
Lin (2009) InsunPOS: a CRF-based POS Tagging 
System. CIPS-ParsEval-2009, 4-6 
A. McCallum, D. Freitag, and F. Pereira (2000) Maxi-
mum Entropy Markov Models for Information Ex-
traction and Segmentation. Proceedings of ICML-
2000, Stanford University, USA, 591-598 
Chao Li, Jian Sun, Yi Guan, Xingjun Xu, Lei Hou, and 
Sheng Li (2009) Chinese Chunking With Maximum 
Entropy Models. CIPS-ParsEval-2009, 2-4 
Qiang Zhou (2004) Annotation Scheme for Chinese 
Treebank. Journal of Chinese Information Processing,  
4-5 
