Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 264?273,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
HotSpots: Visualizing Edits to a Text
Srinivas Bangalore
AT&T Labs ? Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
David Smith
AT&T Labs ? Research
180 Park Ave
Florham Park, NJ 07932
dsmith@research.att.com
Abstract
Compared to the telephone, email based cus-
tomer care is increasingly becoming the pre-
ferred channel of communication for corpora-
tions and customers. Most email-based cus-
tomer care management systems provide a
method to include template texts in order to re-
duce the handling time for a customer?s email.
The text in a template is suitably modified
into a response by a customer care agent. In
this paper, we present two techniques to im-
prove the effectiveness of a template by pro-
viding tools for the template authors. First,
we present a tool to track and visualize the ed-
its made by agents to a template which serves
as a vital feedback to the template authors.
Second, we present a novel method that au-
tomatically extracts potential templates from
responses authored by agents. These meth-
ods are investigated in the context of an email
customer care analysis tool that handles over a
million emails a year.
1 Introduction
Email based customer care is increasingly becom-
ing the preferred channel of communication for cor-
porations and customers compared to the conven-
tional telephone-based customer care. For cus-
tomers, email channel offers several advantages ?
there are no tedious menus to navigate, there is no
waiting time to reach an operator, the request can
be formulated at the customer?s pace and additional
material supporting the case can be attached to the
email. There is also a record of the service re-
quest for the customer unlike the telephone-based
customer care. However, there are also limitations
of the email channel. The most significant one is
that the customer-agent interaction could be drawn
out over successive emails spanning over several
days as opposed to being resolved in one or two
telephone calls. For corporations, the asynchronous
nature of email-based customer care offers signifi-
cant opportunities to reduce operations cost by ef-
fective load balancing compared to telephone-based
customer care. It is quite common for an email cus-
tomer care agent to work on several cases simulta-
neously over a period of a few hours. Email chan-
nel also offers higher bandwidth for corporations to
send additional information in the form of web links,
images and video or audio instructions.
The effectiveness of customer care in the email
channel is measured using two competing metrics:
Average Handling Time (AHT) and Customer Ex-
perience Evaluation (CEE). AHT measures the time
taken from when a customer email is opened to the
time when the response is sent out. This time is typ-
ically averaged over a period of a week or a month
for reporting purposes. CEE measures customer sat-
isfaction through a survey of a random subset of cus-
tomers who have interacted with the email customer
care center. These surveys typically involve qual-
itative and quantitative questions and measure the
quality of the interactions along a number of differ-
ent dimensions. As is the case in many surveys the
population responding to such questionnaires is typ-
ically small and very often quite biased. We do not
use the CEE metric for the work we report in this
paper.
As is evident from the definitions of AHT and
CEE, it is in the interest of a corporation to minimize
AHT while maximizing CEE. In order to reduce
AHT, most email customer care systems (Kana,
2008; Genesys, 2008) provide a mechanism for an
agent to respond to a customer?s email by selecting
a predefined template text that can be quickly cus-
tomized to serve as the response. The template text
is usually associated with a problem category it is in-
tended to address and might even be suggested to the
agent automatically using classification techniques
264
applied to the customer?s email. Once the template
is selected, the agent edits the template text to per-
sonalize as well as add case specific details as part
of composing a response. Each of the text edits con-
tributes to the handling time of the email. Hence,
it is in the interest of the template designer to mini-
mize the number of edits of the template in order to
lower AHT.
Although most email management systems pro-
vide a mechanism to author the template text, there
is typically no mechanism to monitor and track how
these templates are modified by the agents when
they compose a response. This information is vital
to the template authors when creating new versions
of the templates that reduce the number of edits and
consequently reduce AHT.
In this paper, we present two methods for improv-
ing the templates in a principled manner. After de-
scribing the related work in Section 2, we present a
brief description of the email tracking tool we have
developed in Section 3. In Section 4, we present
a tool called HotSpots that helps visualize the edits
being made by the customer care agents to the tem-
plates. This tool provides a visual feedback to the
template authors and suggests means of improving
the template text based on the edits made by agents.
In Section 5, we present a new approach to automat-
ically identify emerging templates ? texts that are
repeatedly created by agents and are similar to each
other but distinct from the current template text. We
use AHT as the metric to minimize for automatic
identification of emerging templates. We discuss
some of the issues concerning this work in Section 6
and conclude in Section 7.
2 Related Work
There are few threads of research that are relevant to
the work presented in this paper. First, the topic of
email response generation in the context of customer
care has been investigated by (Coch, 1996; Lapalme
and Kosseim, 2003; Zukerman and Marom, 2007).
In (Coch, 1996), the authors model multi-sentence
generation of response letters to customer com-
plaints in French. The generation model is carefully
crafted for the domain using domain-specific rules
for conceptual planning, rhetorical relations and sur-
face word order operators. They show that their
approach performs better than predefined templates
and slightly worse than human generated responses.
In (Lapalme and Kosseim, 2003), the authors ex-
plore three different approaches based on classifica-
tion, case-based reasoning and question-answering
to compose responses to queries in an email cus-
tomer care application for the telecommunication in-
dustry. The case-based reasoning approach is the
most similar to the template approach we follow. In
(Zukerman and Marom, 2007), the authors investi-
gate an approach to assembling a response by first
predicting the clusters of sentences to be included in
the response text and then applying multi-document
summarization techniques to collate the representa-
tive sentences into a single response. In contrast, in
this paper, due to constraints from the deployment
environment, we rely on a template-based approach
to response generation. We focus on providing tools
for investigating how the templates are modified and
suggest techniques for evolving more effective tem-
plates based on quantitative criteria.
Another thread of relevant research are methods
for visualizing texts. There are several methods that
have been proposed to provide a visual map of a set
of text documents with the focus of illustrating the
relatedness of these texts (Card et al, 1999). Us-
ing a metric for comparing texts (e.g. n-gram over-
lap) , the texts are clustered and the resulting clus-
ters are visualized as two or three dimensional color
maps. These approaches are useful to depict similar-
ities in a static repository of documents or the return
results of a search query. These maps are primar-
ily designed for exploration and navigation through
the document space. While the underlying algorithm
we use to illustrate the text edits is similar to the one
used in text map visualizations, our focus in this pa-
per is to provide a mechanism for template designers
to quickly identify the variants of a template sen-
tence created by the agents.
A third thread is in the context of human-
assisted machine translation, where a human trans-
lator post-edits the output of a machine translation
system (Foster et al, 1997; Foster et al, 2002; Och
et al, 2003). In order to improve the efficiency of
a human translator, the k-best output of a translation
system could be displayed as word or phrase choices
which are color coded based on the confidence value
assigned by the translation model. While the ap-
265
proach we follow is partly motivated by the post-
editing paradigm, there are significant differences in
the context we apply this approach. In the context
of this paper, the template designer is presented a
summary of the set of variants created by each agent
for each sentence of the template. The task of the
template designer is to use this tool to select (or con-
struct) a new variant for the template sentence with
the aim of minimizing the need for editing that sen-
tence in future uses of the template.
3 Email Customer Care
Typically, a large email customer care management
center receives over 100,000 emails a month. These
centers typically use a customer care management
system that offer not only logging and tracking of
emails but also tools for improving the efficiency
of agents responding to emails. Usually, an incom-
ing customer email is categorized into a set of few
topics/issues. The categorization might be done au-
tomatically based on regular expressions involving
keywords in the email or using weighted classifiers
that are trained on data. In order for an agent to re-
spond to an incoming email, these systems provide a
text box which allows the agent to author a response
from scratch. However, most email customer care
systems offer the ability to store a prefabricated re-
sponse (also called templates), instead of agents hav-
ing to author a response from scratch. These tem-
plates are typically associated with a problem cate-
gory or an issue that they are intended to address.
A template helps an agent compose a well-formed
response quickly. It contains hints for information
that the agent should enter as well as indications of
where that information should be entered in the tem-
plate. The template might also contain helpful infor-
mation to the customer in addition to legal verbiage
that the customer needs to be aware of.
An agent receives a customer email and after
comprehending the issues and consulting the cus-
tomer records in the database, selects one of the pre-
defined set of templates that best addresses the is-
sues raised in the email. Less frequently, she might
even select more than one template to compose the
response. She then proceeds to edit and personal-
ize the chosen templates to better suit the customer?s
email. An example of a ?generic? template ? not as-
sociated with a specific problem category is shown
in Figure 1.
Greetings Contact.FirstName,
Thank you for your email in regard to
XXXXXXXX.
I will be happy to assist you with your in-
quiry.
XXX BODY XXX
If I can be of any further assistance,
please reply directly to this email.
Thank you for using our company.
We appreciate your business and contin-
ued loyalty.
Regards,
Agent.FirstName
Figure 1: An example of a generic template
The process of selecting an appropriate template
that addresses the customer?s inquiries could be
quite tedious when there are hundreds of templates.
Email management systems offer tools that suggest
appropriate template to use based on the content of
the customer?s email. These tools are trained using
classification techniques on previous email interac-
tions.
As mentioned earlier, there are two metrics that
are typically used to measure the effectiveness and
efficiency of email responses. Customers are sur-
veyed after their email interaction to assess their
level of satisfaction for the service they received.
This is usually called the Customer Experience
Evaluation (CEE) and includes an evaluation of the
customer?s total interaction experience with the cor-
poration, not just the last email interaction. A small
subset of customers who had an interaction with the
email center is randomly chosen (typically in the or-
der of about 10% of customers) and are invited to
take part in the follow-up survey. Typically, only
a small percent (about 10%) of the customers who
receive these invitations respond to the survey; ef-
fectively about 1% of the total emails have customer
survey scores.
A second metric that is also used to measure the
efficiency of an operation is called the average han-
dling time (AHT) which measures the average of
266
times taken by agents to respond to emails. The
handling time includes the time to comprehend the
email, the time for database lookup and the time for
response composition. It is in the interest of the
email customer care operation to minimize AHT and
maximize CEE scores.
3.1 Email Customer Care Analysis Tool
We have designed and developed an Email Customer
Care Analysis Tool (ECAT) to help analyze the op-
erations of the email care center. It provides an end-
to-end view from the activities involved in answer-
ing emails to the results of subsequent customer care
surveys. In addition, ECAT also provides insights
into how the agents are editing templates as well as
guides template authors in designing more effective
templates.
ECAT is a web-based tool and offers a birds-eye
summary of the operations aggregated by region,
the template used, and the customer satisfaction sur-
vey results. Using this tool, analysts can drill down
through a series of views until they are eventually
presented with the results of a single survey or a sin-
gle email interaction.
One of the most useful functions of the tool is that
it shows the extent to which agents edit the tem-
plates in the process of creating responses to cus-
tomer emails. The degree to which a template is
edited is based on Levenshtein string edit distance
metric (Levenshtein, 1966). This metric measures
the number of edits (substitution, deletion and inser-
tions) of words that are needed to transform a tem-
plate into a response. The number of edits is normal-
ized by the number of words in the template. These
morphing scores can be viewed for a single email or
averaged per agent or per template used. The scores
range from 100 to 0, with 100 representing a tem-
plate which hadn?t been edited at all.
The tool also allows the morphing score to be
viewed alongside the handling time for an email, in
other words the amount of time that the agent spends
gathering data and actually composing a response.
Handling time is an important metric since it is di-
rectly related to cost of operating the email customer
care center. The more editing an agent does, the
more time they take to respond to a customer. So,
the number of templates, their precise wording and
the ease with which agents can distinguish them ob-
viously have significant influences on overall han-
dling time.
Beyond the confines of the email centers them-
selves, the CEE is the most important elements in
gauging the effectiveness of the agent. The survey
asks customers to rate their overall satisfaction with
the email reply to their question. Five is the high-
est score which equates with ?Extremely Satisfied?
while a one equals ?Extremely Dissatisfied.? Cus-
tomers are also asked to rank the email in terms of
it?s content, clarity, professionalism and the length
of time it took to receive a reply. The customer is
also allowed to enter some free text so that they can
say how satisfied they were, or not, with how an in-
quiry or problem was dealt with. Customers can also
say whether they called the company using a tele-
phone channel before turning to the email channel.
The survey files, all of which can be accessed in
their entirety from within the ECAT tool, also con-
tain information on what templates were used when
replying to the customer. They also tell the analyst
who the replying agent was and whether this was
the first or a subsequent email in communications
between the customer and the company.
The ECAT tool juxtaposes this CEE score with
the template morphing score to show correlations
between customer satisfaction and the degree to
which the template had been edited. This data is
graphed so that the analyst can immediately see if
heavy editing of a template is leading to higher CEE.
Heavy editing with a low customer rating could
mean that the template is not helping the agent to
respond correctly to the customer.
4 HotSpots
We designed the HotSpots tool that provides in-
sights to the template authors on how templates are
being edited by the agents when creating responses.
It suggests methods for improving the next version
of the template so as to reduce edits by agents and
hence reduce the handling time for an email. In this
section, we discuss the algorithm and the visualiza-
tion of the information that aids template authors in
improving the efficacy of the templates.
The HotSpots algorithm proceeds in two steps as
shown in Algorithm 1. The first step creates an
alignment between the template string and the re-
267
Algorithm 1 Compute HotSpots for a template T
given a response set R
1: EdEv = ?
2: T = s1s2 . . . sn
3: Ts = {si|1 ? i ? n}
4: Rs = {r
j
i |Rj ? R, Rj = r
j
1r
j
2 . . . r
j
mj , 1 ? i ?
mj}
5: Index : {Ts ?Rs} ? I
6: Tin = Index(s1)Index(s2) . . . Index(sn)
7: for all R ? R do
8: R = r1r2 . . . rnR
9: Rin = Index(r1)Index(r2) . . . Index(rnR)
// compute distance with sentences as tokens
and return the alignment and score
10: (alignment, score) = IndDist(Tin, Rin)
// for each of the sentences in T, update its
map
11: for all si ? T do
12: in = Index(si)
13: if (si, ) ? alignment then
14: EdEv[in].map = EdEv[in].map ?
{?delete?}
15: else // (si, rj) ? alignment
16: EdEv[in].map = EdEv[in].map ?
{rj}
17: end if
18: end for
19: end for
// Cluster the response sentences aligned for
each template sentence
20: for all si ? T do
21: in = Index(si)
22: Cl = KmedianCl(EdEv[in].map, ncl)
23: end for
Algorithm 2KmedianCl: Compute k centroids for a
set of strings S using k-median clustering algorithm
1: cs = ? // centroid of string s?s cluster
2: cei = ? // centroid of cluster i
3: numcl = 0 // number of cluster created so far
4: Cli = ? // members of cluster i
5: while (numcl ? k) ? (numcl ? |S|) do
6: if numcl = 0 then
7: ce0 = argmin
c?S
?
s?S
Dist(c, s)
8: else // select the string (s) that is farthest from its
centroid (cs)
9: cenumcl = argmax
s?S
Dist(cs, s)
10: end if
// Move strings to the closest cluster and compute
centroids until the set of centroids don?t change
11: repeat
12: for all s ? S do
13: i? = argmin
0?i?numcl
Dist(cei, s)
14: cs = cei?
15: Cli? = Cli? ? {s}
// Computed the closest cluster centroid cei
to s.
16: end for
// Recompute the cluster centroids cei
17: for all i such that 0 ? i ? numcl do
18: cei = argmin
c ? Cli
?
s?Cli
Dist(c, s)
19: end for
20: until set of centroids does not change
21: numcl = numcl + 1 // new cluster added
22: end while
268
sponse string. For the purposes of this paper, we
consider the alignments between the template text
and the response text with sentences as tokens in-
stead of a word-based alignment. The rationale
for this tokenization is that for template develop-
ers the visualization of the edits is expected to be
more meaningful when aggregated at the sentence
level rather than at the word level. In the second
step, using the sentence-level alignment we compute
the edit events (insertion, deletion and substitution)
of the template sentences in order to create the re-
sponse. All the edits events associated with a tem-
plate sentence are then clustered into k clusters and
the centroids of the k clusters are displayed as the
potential changes to that sentence. We next describe
these two steps in detail as illustrated in Algorithm
1 and Algorithm 2.
Given a set of responses R that agents create us-
ing a template T , Algorithm 1 proceeds as follows.
Each of the sentences in the template and the set of
responses are mapped into an integer index (Line
1). The template T and each of the responses in
R are split into sentences and mapped into index
sequences (Line 6 and Line 9). The alignment be-
tween the two index strings is computed in Line
10. This is a dynamic programming algorithm sim-
ilar to computing Levenshtein distance between two
strings, except the cost function used to compute the
match between tokens is as shown below.
From the alignment that maps si to rj , we collect
the set of response sentences associated with each
template sentence (Line 13-16). These sentences are
then clustered using k-median clustering method (il-
lustrated in Algorithm 2) in Line 22.
In Algorithm 2, we illustrate the method of clus-
tering we use to summarize the set of sentences we
have collected for each template sentence after the
alignment step. The algorithm is similar to the k-
means algorithm (Duda et al, 2001), however, given
that we are clustering strings instead of real num-
bers (as is typical in applications of k-means), we re-
strict the centroid of a cluster to be one of the mem-
bers of the set being clustered, hence the name k-
median algorithm (Martnez-Hinarejos et al, 2003).
The distance function to measure the closeness of
two strings is instantiated to be an n-gram overlap
between the two strings.1
The algorithm iterates over three steps until the
data is partitioned into k clusters (Line 5). The first
step (Lines 6-10) is the initialization of a centroid
for a new cluster. Initially when the data is not parti-
tioned into any cluster, the median string of the data
set is used as the initial centroid. For subsequent
iterations, the farthest point from all the centroids
computed thus far is used as the centroid for the new
cluster. In the second step (Lines 11-16), each mem-
ber of the data set is assigned to the nearest clus-
ter based on its distance to that cluster?s centroid.
Finally, in the third step (Lines 17-20), the cluster
centroids are recomputed based on the new cluster
memberships. Steps two and three are repeated until
there are no changes in the cluster memberships and
cluster centroids. This completes the introduction of
a new cluster for the data.
For the purposes of our task, we use up to a
four-gram overlap to measure distance between two
strings and use k = 5 for clustering the data.
4.1 Visualizing the HotSpots
The HotSpots page was created within the ECAT
tool to surgically dissect the way in which templates
were being morphed. For a given template, as shown
in Figure 2, the analyst is presented with a copy of
the texts from the current and previous versions of
that template. Each sentence in the two versions
of the template are color coded to show how fre-
quently the agents have changed that sentence. This
involved running the HotSpots algorithm against ap-
proximately 1,000 emails per template version. A
sentence that is colored red is one that was changed
in over 50% of the emails that were responded to
using that template. An orange sentence is one that
was edited in between 30% and 50%, green is be-
tween 10% to 30% and blue is between 0% and 10%.
The more often a sentence is edited the ?hotter? the
color.
The analyst can see the typical substitutions for a
sentence by hovering the mouse over that sentence.
The typical sentences computed as the centroids of
the clusters created using Algorithm 2 are them-
selves color coded using the same identification sys-
1We have also experimented with a symmetric version of
Levenshtein distance, but we prefer the n-gram overlap score
due to its linear run time complexity.
269
Figure 2: Example of two versions of a template and the edit score (Avg. Morph. Score) and centroids associated with
each sentence of the template.
tem. A typical sentence that occurred in over 50%
of the emails is colored red. A typical sentence that
occurred in 30% to 50% of the emails was orange
and so on.
In seeing the two versions side by side, the an-
alyst can visually inspect the agents? edits on the
current version of a template relative to the previ-
ous version. If the previous version of the template
is a ?hotter? document (with more red sentences), it
means that the changes made to the template by the
author had led to less editing by agents thus speeding
up the process of creating a customer response. If
the current template looks hotter, it suggests that the
changes made to the template were increasing the
agents? edits and probably the email handling time.
5 Automatic Extraction of Potential
Templates
The goal of the template author is to minimize the
number of edits done to a template and thus in-
directly lowering the handling time for an email.
In the preceding section, we discussed a tool that
aids the template authors to identify sentences where
changes are most often made by agents to a tem-
plate. This information could be used by the tem-
plate authors to create a new version of the template
that achieve the goal.
In this section, we investigate a technique that au-
tomatically identifies a possible template with the
potential of directly minimizing the average han-
dling time for an email. We use the set of responses
created by the agents using a given template and se-
lect one of the responses to be generalized and stored
as a new template. The response to be converted
into a template is chosen so as to directly minimize
the average handling time. In essence, we seek to
partition the set of responses R generated from tem-
plate T into two clusters R1 and R2. These clus-
ters have centroids T (current template) and T ? (new
template) such that constraint shown in 1 holds.
(?r?R1AHT (T, r) < AHT (T
?, r)) ?
(?r?R2AHT (T
?, r) < AHT (T, r)) (1)
Now, the quantity AHT (T, r) is logged as part
of the email management system and corresponds
to the time taken to respond to a customer?s email.2
2Although typically this time includes the time to look up a
270
Cluster Number of Centroid (Template/Response)
members
1 1799 GREETINGSPHR, Thank you for your recent email.
On behalf of the company, I would like to extend my sincere
apology for the problems you encountered when (XXX over key
with appropriate response XXX). It is our goal to provide
excellent customer service, and I am sorry that we did not
meet that objective. Your input is very valuable, and we will
take your feedback into consideration. Regards, Agent.FirstName
2 206 GREETINGSPHR, Thank you for letting me know that you?ve been
unable to send an online order to upgrade your NAMEDENTITY service.
Please accept my apologies for any problems this issue may have caused
you. You?re a highly valued customer. I understand your
concerns and I?ll be happy to address them. I am investigating this
issue. I have already made a personal commitment to email you
tomorrow, with the resolution. Thank you for your patience and for
choosing the company. We appreciate your business and continued
loyalty. Sincerely, Agent.FirstName
Table 1: Result of clustering responses using the AHT model as the distance metric.
However, we do not have access to AHT (T ?, r) for
any T ? 6= T . We propose a model to estimate this in
the next section.
5.1 Modeling Average Handling Time
We model AHT as a linear combination of sev-
eral factors which we believe would influence the
handling time for an email. These factors in-
clude the length in words of the customer?s input
email (inplen), the length in words of the template
(templatelen), the length in words of the response
(resplen), the total number of edits between the
template and the response (edit), the normalized edit
score (nedit), the number of individual events of
the edit distance ? substitution (sub), insertion (ins),
deletion (del) and identity (id), the number of block
(contiguous) substitution (blksub), block insertion
(blkins) and block deletion (blkdel). Using these in-
dependent variables, we fit a linear regression model
using the AHT values for 6175 responses created
from one particular template (say G). The result of
the regression fit is shown in Equation 2 and the data
and error statistics are shown in Table 2. It must be
noted that the coefficients for the variables are not
necessarily reflective of the importance of the vari-
ables, since they compensate for the different ranges
in variable values. We have also tried several differ-
the customer?s account etc., we assume that time is quite similar
for all responses created from the same template.
ent regression fits with fewer variables, but find that
this fit gives us the best correlation with the data.
?AHT = 0.5314 ? inplen? 2.7648 ? templatelen
+1.9982 ? resplen? 0.5822 ? edit
+2900.5242 ? nedit
+4.7499 ? id? 1.6647 ? del
?1.6021 ? ins + 26.6704 ? blksub
?15.239 ? blkins + 24.3931 ? blkdel
?261.6627 (2)
Mean AHT 675.74 seconds
Median AHT 543 seconds
Mode AHT 366 seconds
Standard Deviation 487.72 seconds
Correlation coefficient 0.3822
Mean absolute error 320.2 seconds
Root mean squared error 450.64 seconds
Total Number of Instances 6175
Table 2: Data statistics and the goodness of the regression
model for 6175 AHT data points.
Based on the goodness statistics of the regression
fit, it is clear the AHT model could be improved
further. However, we acknowledge that AHT does
not depend solely on the editing of a template to a
271
response but involves several other components in-
cluding the user interface, the complexity of cus-
tomer?s email, the database retrieval to access the
customer?s account and so forth.
Nevertheless, we use this model to cluster a new
set of 2005 responses originating from the same
template (G), as shown in Equation 1. Using the
k-median clustering as described earlier, we parti-
tion the responses into two clusters. We restrict the
first cluster centroid to be the template and search
for the best centroid for the second cluster. The re-
sults are shown in Table 1. The centroid for clus-
ter 1 with 1799 members is the template itself while
the centroid for cluster 2 with 206 members is a re-
sponse that could be suitably generalized to serve as
a template. The overall AHT for the 2005 responses
using the template was 989.2 seconds, while the av-
erage AHT for the members of cluster 1 and 2 was
971.9 seconds and 1140 seconds, indicating that the
template had to be edited considerably to create the
members of cluster 2.
6 Discussion
For the purposes of this paper, it is assumed that
AHT is the same as or correlates well with the time
to compose a response for an email. However, in
most cases the email care agent might have to per-
form several verification, validation, and problem
resolution phases by consulting the specifics of a
customer account before formulating and compos-
ing a response. The time taken for each of these
phases typically varies depending on the customer?s
account and the problem category. Nevertheless, we
assume that the times for these phases is mostly a
constant for a given problem category, and hence the
results presented in this paper need to be interpreted
on a per problem category basis.
A second limitation of the approach presented in
this paper is that the metric used to measure the sim-
ilarity between strings (n-gram overlap) is only a
crude approximation of an ideal semantic similarity
metric. There are however other similarity metrics
(e.g. BLEU (Papineni et al, 2002)) which could be
used equally well. The purpose of this paper is to il-
lustrate the possibility of analysis of responses using
one particular instantiation of the similarity metric.
In spite of the several directions that this work can
be improved, the system and algorithms described
in this paper have been deployed in an operational
customer care center. The qualitative feedback we
have received are extremely positive and analysts
have greatly improved the efficiency of the opera-
tion using this tool.
7 Conclusions
In this paper, we have presented two approaches that
help template authors in designing effective tem-
plates for email customer care agents. In the first ap-
proach, we have presented details of a graphical tool
that provides vital feedback to the template authors
on how their templates are being modified by agents
when creating responses. The template authors can
accommodate this information when designing the
next version of the template. We also presented a
novel technique for identifying responses that can
potentially serve as templates and reduce AHT. To-
wards this end, we discussed a method to model
AHT based on the characteristics of the customer?s
email, the template text and the response text.
8 Acknowledgments
Wewould like to thanks Mazin Gilbert, Junlan Feng,
Narendra Gupta and Wenling Hsu for the discus-
sions during the course of this work. We also thank
the members who generously offered to their sup-
port to provide us with data used in this study with-
out which this work would not have been possible.
We thank the anonymous reviewers for their useful
suggestions in improving the quality of this paper.
References
S.K. Card, J. Mackinlay, and B. Shneiderman. 1999.
Readings in Information Visualization: Using Vision
to Think. Morgan Kaufmann.
J. Coch. 1996. Evaluating and comparing three text-
production techniques. In Proceedings of Coling-96,
pages 249?254, Copenhagen, Denmark.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
Classification. Wiley, New York.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Tar-
get text mediated interactive machine translation. Ma-
chine Translation, 12(1):175?194.
G. Foster, P. Langlais, and G. Lampalme. 2002. User-
friendly text prediction for translators. In EMNLP-02,
pages 46?51, Philadelphia, USA.
Genesys. 2008. http://www.genesys.com. Genesys Cor-
poration.
272
Kana. 2008. http://www.kana.com. Kana Corporation.
G. Lapalme and L. Kosseim. 2003. Mercure: Towards
an automatic e-mail follow-up system. IEEE Compu-
tational Intelligence Bulletin, 2(1):14?18.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertion and reversals. Soviet Physics
Doklady, 10:707?710.
C.D. Martnez-Hinarejos, A. Juan, and F. Casacuberta.
2003. Generalized k-medians clustering for strings.
Lecture Notes in Computer Science, 2652/2003:502?
509.
F.J. Och, R. Zens, and H. Ney. 2003. Efficient search for
interactive statistical machine translation. In EACL-
03.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meeting
of the Association of Computational Linguistics, pages
313?318, Philadelphia, PA, July.
I. Zukerman and Y. Marom. 2007. Evaluation of a large-
scale email response system. In Proceedings of IJ-
CAI07, Hyderabad, India.
273
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 822?831,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Parser Adaptation and Projection
with Quasi-Synchronous Grammar Features
?
David A. Smith
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
dasmith@cs.umass.edu
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Abstract
We connect two scenarios in structured
learning: adapting a parser trained on
one corpus to another annotation style, and
projecting syntactic annotations from one
language to another. We propose quasi-
synchronous grammar (QG) features for
these structured learning tasks. That is, we
score a aligned pair of source and target
trees based on local features of the trees
and the alignment. Our quasi-synchronous
model assigns positive probability to any
alignment of any trees, in contrast to a syn-
chronous grammar, which would insist on
some form of structural parallelism.
In monolingual dependency parser adap-
tation, we achieve high accuracy in trans-
lating among multiple annotation styles
for the same sentence. On the more
difficult problem of cross-lingual parser
projection, we learn a dependency parser
for a target language by using bilin-
gual text, an English parser, and auto-
matic word alignments. Our experiments
show that unsupervised QG projection im-
proves on parses trained using only high-
precision projected annotations and far
outperforms, by more than 35% absolute
dependency accuracy, learning an unsu-
pervised parser from raw target-language
text alone. When a few target-language
parse trees are available, projection gives
a boost equivalent to doubling the number
of target-language trees.
?
The first author would like to thank the Center for Intel-
ligent Information Retrieval at UMass Amherst. We would
also like to thank Noah Smith and Rebecca Hwa for helpful
discussions and the anonymous reviewers for their sugges-
tions for improving the paper.
1 Introduction
1.1 Parser Adaptation
Consider the problem of learning a dependency
parser, which must produce a directed tree whose
vertices are the words of a given sentence. There
are many differing conventions for representing
syntactic relations in dependency trees. Say that
we wish to output parses in the Prague style and
so have annotated a small target corpus?e.g.,
100 sentences?with those conventions. A parser
trained on those hundred sentences will achieve
mediocre dependency accuracy (the proportion of
words that attach to their correct parent).
But what if we also had a large number of trees
in the CoNLL style (the source corpus)? Ide-
ally they should help train our parser. But unfor-
tunately, a parser that learned to produce perfect
CoNLL-style trees would, for example, get both
links ?wrong? when its coordination constructions
were evaluated against a Prague-style gold stan-
dard (Figure 1).
If it were just a matter of this one construction,
the obvious solution would be to write a few rules
by hand to transform the large source training cor-
pus into the target style. Suppose, however, that
there were many more ways that our corpora dif-
fered. Then we would like to learn a statistical
model to transform one style of tree into another.
We may not possess hand-annotated training
data for this tree-to-tree transformation task. That
would require the two corpora to annotate some of
the same sentences in different styles.
But fortunately, we can automatically obtain a
noisy form of the necessary paired-tree training
data. A parser trained on the source corpus can
parse the sentences in our target corpus, yielding
trees (or more generally, probability distributions
over trees) in the source style. We will then learn
a tree transformation model relating these noisy
source trees to our known trees in the target style.
822
now o

r

never now

or

never
Prague Mel??cuk
no

w

or never now or ne

ver

CoNLL MALT
Figure 1: Four of the five logically possible schemes for
annotating coordination show up in human-produced depen-
dency treebanks. (The other possibility is a reverse Mel??cuk
scheme.) These treebanks also differ on other conventions.
This model should enable us to convert the orig-
inal large source corpus to target style, giving us
additional training data in the target style.
1.2 Parser Projection
For many target languages, however, we do not
have the luxury of a large parsed ?source cor-
pus? in the language, even one in a different style
or domain as above. Thus, we may seek other
forms of data to augment our small target corpus.
One option would be to leverage unannotated text
(McClosky et al, 2006; Smith and Eisner, 2007).
But we can also try to transfer syntactic informa-
tion from a parsed source corpus in another lan-
guage. This is an extreme case of out-of-domain
data. This leads to the second task of this paper:
learning a statistical model to transform a syntac-
tic analysis of a sentence in one language into an
analysis of its translation.
Tree transformations are often modeled with
synchronous grammars. Suppose we are given a
sentence w
?
in the ?source? language and its trans-
lation w into the ?target? language. Their syn-
tactic parses t
?
and t are presumably not indepen-
dent, but will tend to have some parallel or at least
correlated structure. So we could jointly model
the parses t
?
, t and the alignment a between them,
with a model of the form p(t, a, t
?
| w,w
?
).
Such a joint model captures how t, a, t
?
mu-
tually constrain each other, so that even partial
knowledge of some of these three variables can
help us to recover the others when training or de-
coding on bilingual text. This idea underlies a
number of recent papers on syntax-based align-
ment (using t and t
?
to better recover a), grammar
induction from bitext (using a to better recover t
and t
?
), parser projection (using t
?
and a to better
Figure 2: With the English tree and alignment provided by
a parser and aligner at test time, the Chinese parser finds the
correct dependencies (see ?6). A monolingual parser?s incor-
rect edges are shown with dashed lines.
recover t), as well as full joint parsing (Smith and
Smith, 2004; Burkett and Klein, 2008).
In this paper, we condition on the 1-best source
tree t
?
. As for the alignment a, our models ei-
ther condition on the 1-best alignment or integrate
the alignment out. Our models are thus of the
form p(t | w,w
?
, t
?
, a) or, in the generative case,
p(w, t, a | w
?
, t
?
). We intend to consider other for-
mulations in future work.
So far, this is very similar to the monolingual
parser adaptation scenario, but there are a few key
differences. Since the source and target sentences
in the bitext are in different languages, there is
no longer a trivial alignment between the words
of the source and target trees. Given word align-
ments, we could simply try to project dependency
links in the source tree onto the target text. A
link-by-link projection, however, could result in
invalid trees on the target side, with cycles or dis-
connected words. Instead, our models learn the
necessary transformations that align and transform
a source tree into a target tree by means of quasi-
synchronous grammar (QG) features.
Figure 2 shows an example of bitext helping
disambiguation when a parser is trained with only
a small number of Chinese trees. With the help
of the English tree and alignment, the parser is
able to recover the correct Chinese dependen-
cies using QG features. Incorrect edges from
the monolingual parser are shown with dashed
lines. (The bilingual parser corrects additional er-
rors in the second half of this sentence, which has
been removed to improve legibility.) The parser
is able to recover the long-distance dependency
from the first Chinese word (China) to the last
(begun), while skipping over the intervening noun
823
phrase that confused the undertrained monolin-
gual parser. Although, due to the auxiliary verb,
?China? and ?begun? are siblings in English and
not in direct dependency, the QG features still
leverage this indirect projection.
1.3 Plan of the Paper
We start by describing the features we use to
augment conditional and generative parsers when
scoring pairs of trees (?2). Then we discuss in turn
monolingual (?3) and cross-lingual (?4) parser
adaptation. Finally, we present experiments on
cross-lingual parser projection in conditions when
no target language trees are available for training
(?5) and when some trees are available (?6).
2 Form of the Model
What should our model of source and target trees
look like? In our view, traditional approaches
based on synchronous grammar are problematic
both computationally and linguistically. Full in-
ference takes O(n
6
) time or worse (depending on
the grammar formalism). Yet synchronous mod-
els only consider a limited hypothesis space: e.g.,
parses must be projective, and alignments must de-
compose according to the recursive parse struc-
ture. (For example, two nodes can be aligned
only if their respective parents are also aligned.)
The synchronous model?s probability mass func-
tion is also restricted to decompose in this way,
so it makes certain conditional independence as-
sumptions; put another way, it can evaluate only
certain properties of the triple (t, a, t
?
).
We instead model (t, a, t
?
) as an arbitrary graph
that includes dependency links among the words
of each sentence as well as arbitrary alignment
links between the words of the two sentences.
This permits non-synchronous and many-to-many
alignments. The only hard constraint we impose
is that the dependency links within each sentence
must constitute a valid monolingual parse?a di-
rected projective spanning tree.
1
Given the two sentences w,w
?
, our probabil-
ity distribution over possible graphs considers lo-
cal features of the parses, the alignment, and both
jointly. Thus, we learn what local syntactic con-
figurations tend to occur in each language and how
they correspond across languages. As a result, we
might learn that parses are ?mostly synchronous,?
but that there are some systematic cross-linguistic
1
Non-projective parsing would also be possible.
divergences and some instances of sloppy (non-
parallel or inexact) translation. Our model is thus a
form of quasi-synchronous grammar (QG) (Smith
and Eisner, 2006a). In that paper, QG was applied
to word alignment and has since found applica-
tions in question answering (Wang et al, 2007),
paraphrase detection (Das and Smith, 2009), and
machine translation (Gimpel and Smith, 2009).
All the models in this paper are conditioned on
the source tree t
?
. Conditionally-trained models
of adaptation and projection also condition on the
target string w and its alignment a to w
?
and thus
have the form p(t | w,w
?
, t
?
, a); the unsupervised,
generative projection models in ?5 have the form
p(w, t, a | w
?
, t
?
).
The score s of a given tuple of trees, words, and
alignment can thus be written as a dot product of
weights w with features f and g:
s(t, t
?
, a, w,w
?
) =
?
i
w
i
f
i
(t, w)
+
?
j
w
j
g
j
(t, t
?
, a, w,w
?
)
The features f look only at target words and de-
pendencies. In the conditional models of ?3 and
?6, these features are those of an edge-factored
dependency parser (McDonald et al, 2005). In
the generative models of ?5, f has the form of a
dependency model with valence (Klein and Man-
ning, 2004). All models, for instance, have a fea-
ture template that considers the parts of speech of
a potential parent-child relation.
In order to benefit from the source language, we
also need to include bilingual features g. When
scoring a candidate target dependency link from
word x ? y, these features consider the relation-
ship of their corresponding source words x
?
and
y
?
. (The correspondences are determined by the
alignment a.) For instance, the source tree t
?
may
contain the link x
?
? y
?
, which would cause a fea-
ture for monotonic projection to fire for the x ? y
edge. If, on the other hand, y
?
? x
?
? t
?
, a
head-swapping feature fires. If x
?
= y
?
, i.e. x
and y align to the same word, the same-word fea-
ture fires. Similar features fire when x
?
and y
?
are
in grandparent-grandchild, sibling, c-command, or
none-of-the above relationships, or when y aligns
to NULL. These alignment classes are called con-
figurations (Smith and Eisner, 2006a, and follow-
ing). When training is conditioned on the target
words (see ?3 and ?6 below), we conjoin these
824
configuration features with the part of speech and
coarse part of speech of one or both of the source
and target words, i.e. the feature template has from
one to four tags.
In conditional training, the exponentiated
scores s are normalized by a constant: Z =
?
t
exp[s(t, t
?
, a, w,w
?
)]. For the generative
model, the locally normalized generative process
is explained in ?5.3.4.
Previous researchers have written fix-up rules
to massage the projected links after the fact and
learned a parser from the resulting trees (Hwa et
al., 2005). Instead, our models learn the necessary
transformations that align and transform a source
tree into a target tree. Other researchers have tack-
led the interesting task of learning parsers from
unparsed bitext alone (Kuhn, 2004; Snyder et al,
2009); our methods take advantage of investments
in high-resource languages such as English. In
work most closely related to this paper, Ganchev et
al. (2009) constrain the posterior distribution over
target-language dependencies to align to source
dependencies some ?reasonable? proportion of the
time (? 70%, cf. Table 2 in this paper). This
approach performs well but cannot directly learn
regular cross-language non-isomorphisms; for in-
stance, some fixup rules for auxiliary verbs need
to be introduced. Finally, Huang et al (2009)
use features, somewhat like QG configurations, on
the shift-reduce actions in a monolingual, target-
language parser.
3 Adaptation
As discussed in ?1, the adaptation scenario is a
special case of parser projection where the word
alignments are one-to-one and observed. To test
our handling of QG features, we performed ex-
periments in which training saw the correct parse
trees in both source and target domains, and the
mapping between them was simple and regular.
We also performed experiments where the source
trees were replaced by the noisy output of a trained
parser, making the mapping more complex and
harder to learn.
We used the subset of the Penn Treebank from
the CoNLL 2007 shared task and converted it to
dependency representation while varying two pa-
rameters: (1) CoNLL vs. Prague coordination
style (Figure 1), and (2) preposition the head vs.
the child of its nominal object.
We trained an edge-factored dependency parser
(McDonald et al, 2005) on ?source? domain data
that followed one set of dependency conventions.
We then trained an edge-factored parser with QG
features on a small amount of ?target? domain
data. The source parser outputs were produced for
all target data, both training and test, so that fea-
tures for the target parser could refer to them.
In this task, we know what the gold-standard
source language parses are for any given text,
since we can produce them from the original Penn
Treebank. We can thus measure the contribution
of adaptation loss alone, and the combined loss
of imperfect source-domain parsing with adapta-
tion (Table 1). When no target domain trees are
available, we simply have the performance of the
source domain parser on this out-of-domain data.
Training a target-domain parser on as few as 10
sentences shows substantial improvements in ac-
curacy. In the ?gold? conditions, where the target
parser starts with perfect source trees, accuracy
approaches 100%; in the realistic ?parse? condi-
tions, where the target-domain parser gets noisy
source-domain parses, the improvements are quite
significant but approach a lower ceiling imposed
by the performance of the source parser.
2
The adaptation problem in this section is a sim-
ple proof of concept of the QG approach; however,
more complex and realistic adaptation problems
exist. Monolingual adaptation is perhaps most ob-
viously useful when the source parser is a black-
box or rule-based system or is trained on unavail-
able data. One might still want to use such a parser
in some new context, which might require new
data or a new annotation standard.
We are also interested in scenarios where we
want to avoid expensive retraining on large rean-
notated treebanks. We would like a linguist to be
able to annotate a few trees according to a hy-
pothesized theory and then quickly use QG adap-
tation to get a parser for that theory. One example
would be adapting a constituency parser to pro-
duce dependency parses. We have concentrated
here on adapting between two dependency parse
styles, in order to line up with the cross-lingual
tasks to which we now turn.
2
In the diagonal cells, source and target styles match, so
training the QG parser amounts to a ?stacking? technique
(Martins et al, 2008). The small training size and overreg-
ularization of the QG parser mildly hurts in-domain parsing
performance.
825
% Dependency Accuracy on Target
CoNLL-PrepHead CoNLL-PrepChild Prague-PrepHead Prague-PrepChild
Source 0 10 100 0 10 100 0 10 100 0 10 100
Gold CoNLL-PrepHead 100 99.6 99.6 79.5 96.9 97.8 90.5 95.0 98.1 71.0 92.7 95.4
Parse CoNLL-PrepHead 89.5 88.9 89.0 71.4 85.9 87.9 82.5 84.3 87.8 65.2 82.2 86.1
Gold CoNLL-PrepChild 79.5 96.6 97.3 100 99.6 99.6 71.0 91.3 95.5 89.9 94.5 97.9
Parse CoNLL-PrepChild 71.0 84.2 86.8 88.1 87.5 88.0 64.9 80.7 84.9 80.9 83.5 86.1
Gold Prague-PrepHead 90.5 95.5 96.7 71.0 92.0 94.2 100 99.6 99.6 79.6 97.4 98.1
Parse Prague-PrepHead 83.0 87.1 87.4 65.6 84.2 85.9 88.5 88.3 88.0 70.7 86.4 86.8
Gold Prague-PrepChild 71.0 91.6 93.8 89.9 95.6 96.4 79.6 96.0 97.1 100 99.6 99.6
Parse Prague-PrepChild 65.3 81.7 84.6 81.2 84.5 86.1 70.4 83.2 85.3 86.9 86.1 86.8
Table 1: Adapting a parser to a new annotation style. We learn to parse in a ?target? style (wide column label) given some
number (narrow column label) of supervised target-style training sentences. As a font of additional features, all training and
test sentences have already been augmented with parses in some ?source? style (row label): either gold-standard parses (an
oracle experiment) or else the output of a parser trained on 18k source trees (more realistic). If we have 0 training sentences, we
simply output the source-style parse. But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt,
mostly closing the gap with the diagonal block in the same column. In the diagonal blocks, source and target styles match, and
the QG parser degrades performance when acting as a ?stacked? parser.
4 Cross-Lingual Projection: Background
As in the adaptation scenario above, many syn-
tactic structures can be transferred from one lan-
guage to another. In this section, we evaluate the
extent of this direct projection on a small hand-
annotated corpus. In ?5, we will use a QG genera-
tive model to learn dependency parsers from bitext
when there are no annotations in the target lan-
guage. Finally, in ?6,we show how QG features
can augment a target-language parser trained on a
small set of labeled trees.
For syntactic annotation projection to work at
all, we must hypothesize, or observe, that at least
some syntactic structures are preserved in transla-
tion. Hwa et al (2005) have called this intuition
the Direct Correspondence Assumption (DCA,
with slight notational changes):
Given a pair of sentences w and w
?
that
are translations of each other with syn-
tactic structure t and t
?
, if nodes x
?
and
y
?
of t
?
are aligned with nodes x and y of
t, respectively, and if syntactic relation-
ship R(x
?
, y
?
) holds in t
?
, then R(x, y)
holds in t.
The validity of this assumption clearly depends
on the node-to-node alignment of the two trees.
We again work in a dependency framework, where
syntactic nodes are simply lexical items. This al-
lows us to use existing work on word alignment.
Hwa et al (2005) tested the DCA under ide-
alized conditions by obtaining hand-corrected de-
pendency parse trees of a few hundred sentences
of Spanish-English and Chinese-English bitext.
They also used human-produced word alignments.
Corpus Prec.[%] Rec.[%]
Spanish 64.3 28.4
(no punc.) 72.0 30.8
Chinese 65.1 11.1
(no punc.) 68.2 11.5
Table 2: Precision and recall of direct dependency projection
via one-to-one links alone.
Since their word alignments could be many-to-
many, they gave a heuristic Direct Projection Al-
gorithm (DPA) for resolving them into component
dependency relations. It should be noted that this
process introduced empty words into the projected
target language tree and left words that are un-
aligned to English detached from the tree; as a re-
sult, they measured performance in dependency F-
score rather than accuracy. With manual English
parses and word alignments, this DPA achieved
36.8% F-score in Spanish and 38.1% in Chinese.
With Collins-model English parses and GIZA++
word alignments, F-score was 33.9% for Spanish
and 26.3% for Chinese. Compare this to the Span-
ish attach-left baseline of 31.0% and the Chinese
attach-right baselines of 35.9%. These discour-
agingly low numbers led them to write language-
specific transformation rules to fix up the projected
trees. After these rules were applied to the pro-
jections of automatic English parses, F-score was
65.7% for English and 52.4% for Chinese.
While these F-scores were low, it is useful to
look at a subset of the alignment: dependencies
projected across one-to-one alignments before the
heuristic fix-ups had a much higher precision, if
lower recall, than Hwa et al?s final results. Us-
826
ing Hwa et al?s data, we calculated that the preci-
sion of projection to Spanish and Chinese via these
one-to-one links was ? 65% (Table 2). There is
clearly more information in these direct links than
one would think from the F-scores. To exploit this
information, however, we need to overcome the
problems of (1) learning from partial trees, when
not all target words are attached, and (2) learning
in the presence of the still considerable noise in the
projected one-to-one dependencies?e.g., at least
28% error for Spanish non-punctuation dependen-
cies.
What does this noise consist of? Some errors
reflect fairly arbitrary annotation conventions in
treebanks, e.g. should the auxiliary verb gov-
ern the main verb or vice versa. (Examples like
this suggest that the projection problem contains
the adaptation problem above.) Other errors arise
from divergences in the complements required of
certain head words. In the German-English trans-
lation pair, with co-indexed words aligned,
[an [den Libanon
1
]] denken
2
? remember
2
Lebanon
1
we would prefer that the preposition an attach
to denken, even though the preposition?s object
Libanon aligns to a direct child of remember.
In other words, we would like the grandparent-
parent-child chain of denken ? an ? Libanon
to align to the parent-child pair of remember ?
Lebanon. Finally, naturally occurring bitexts con-
tain some number of free or erroneous transla-
tions. Machine translation researchers often seek
to strike these examples from their training cor-
pora; ?free? translations are not usually welcome
from an MT system.
5 Unsupervised Cross-Lingual Projection
First, we consider the problem of parser projection
when there are zero target-language trees avail-
able. As in much other work on unsupervised
parsing, we try to learn a generative model that
can predict target-language sentences. Our novel
contribution is to condition the probabilities of the
generative actions on the dependency parse of a
source-language translation. Thus, our generative
model is a quasi-synchronous grammar, exactly as
in (Smith and Eisner, 2006a).
3
When training on target sentences w, there-
fore, we tune the model parameters to maxi-
mize not
?
t
p(t, w) as in ordinary EM, but rather
3
Our task here is new; they used it for alignment.
?
t
p(t, w, a | t
?
, w
?
). We hope that this condi-
tional EM training will drive the model to posit ap-
propriate syntactic relationships in the latent vari-
able t, because?thanks to the structure of the QG
model?that is the easiest way for it to exploit the
extra information in t
?
, w
?
to help predict w.
4
At
test time, t
?
, w
?
are not made available, so we just
use the trained model to find argmax
t
p(t | w),
backing off from the conditioning on t
?
, w
?
and
summing over a.
Below, we present the specific generative model
(?5.1) and some details of training (?5.2). We will
then compare three approaches (?5.3):
?5.3.2 a straight EM baseline (which does not
condition on t
?
, w
?
at all)
?5.3.3 a ?hard? projection baseline (which naively
projects t
?
, w
?
to derive direct supervision in
the target language)
?5.3.4 our conditional EM approach above (which
makes t
?
, w
?
available to the learner for ?soft?
indirect supervision via QG)
5.1 Generative Models
Our base models of target-language syntax are
generative dependency models that have achieved
state-of-the art results in unsupervised dependency
structure induction. The simplest version, called
Dependency Model with Valence (DMV), has been
used in isolation and in combination with other
models (Klein and Manning, 2004; Smith and Eis-
ner, 2006b). The DMV generates the right chil-
dren, and then independently the left children, for
each node in the dependency tree. Nodes corre-
spond to words, which are represented by their
part-of-speech tags. At each step of generation,
the DMV stochastically chooses whether to stop
generating, conditioned on the currently generat-
ing head; whether it is generating to the right or
left; and whether it has yet generated any chil-
dren on that side. If it chooses to continue, it then
4
The contrastive estimation of Smith and Eisner (2005)
also used a form of conditional EM, with similar motiva-
tion. They suggested that EM grammar induction, which
learns to predictw, unfortunately learns mostly to predict lex-
ical topic or other properties of the training sentences that do
not strongly require syntactic latent variables. To focus EM
on modeling the syntactic relationships, they conditioned the
prediction of w on almost complete knowledge of the lexi-
cal items. Similarly, we condition on a source translation of
w. Furthermore, our QG model structure makes it easy for
EM to learn to exploit the (explicitly represented) syntactic
properties of that translation when predicting w.
827
stochastically generates the tag of a new child,
conditioned on the head. The parameters of the
model are thus of the form
p(stop | head, dir, adj) (1)
p(child | head, dir) (2)
where head and child are part-of-speech tags,
dir ? {left, right}, and adj, stop ? {true, false}.
ROOT is stipulated to generate a single right child.
Bilingual configurations that condition on t
?
, w
?
(?2) are incorporated into the generative process
as in Smith and Eisner (2006a). When the model
is generating a new child for word x, aligned to x
?
,
it first chooses a configuration and then chooses a
source word y
?
in that configuration. The child y is
then generated, conditioned on its parent x, most
recent sibling a, and its source analogue y
?
.
5.2 Details of EM Training
As in previous work on grammar induction, we
learn the DMV from part-of-speech-tagged target-
language text. We use expectation maximization
(EM) to maximize the likelihood of the data. Since
the likelihood function is nonconvex in the unsu-
pervised case, our choice of initial parameters can
have a significant effect on the outcome. Although
we could also try many random starting points, the
initializer in Klein and Manning (2004) performs
quite well.
The base dependency parser generates the right
dependents of a head separately from the left de-
pendents, which allows O(n
3
) dynamic program-
ming for an n-word target sentence. Since the QG
annotates nonterminals of the grammar with sin-
gle nodes of t
?
, and we consider two nodes of t
?
when evaluating the above dependency configura-
tions, QG parsing runs inO(n
3
m
2
) for anm-word
source sentence. If, however, we restrict candidate
senses for a target child c to come from links in
an IBM Model 4 Viterbi alignment, we achieve
O(n
3
k
2
), where k is the maximum number of
possible words aligned to a given target language
word. In practice, k  m, and parsing is not ap-
preciably slower than in the monolingual setting.
If all configurations were equiprobable, the
source sentence would provide no information to
the target. In our QG experiments, therefore,
we started with a bias towards direct parent?child
links and a very small probability for breakages
of locality. The values of other configuration pa-
rameters seem, experimentally, less important for
insuring accurate learning.
5.3 Experiments
Our experiments compare learning on target lan-
guage text to learning on parallel text. In the lat-
ter case, we compare learning from high-precision
one-to-one alignments alone, to learning from all
alignments using a QG.
5.3.1 Corpora
Our development and test data were drawn from
the German TIGER and Spanish Cast3LB tree-
banks as converted to projective dependencies for
the CoNLL 2007 Shared Task (Brants et al, 2002;
Civit Torruella and Mart?? Anton??n, 2002).
5
Our training data were subsets of the 2006
Statistical Machine Translation Workshop Shared
Task, in particular from the German-English
and Spanish-English Europarl parallel corpora
(Koehn, 2002). The Shared Task provided pre-
built automatic GIZA++ word alignments, which
we used to facilitate replicability. Since these
word alignments do not contain posterior proba-
bilities or null links, nor do they distinguish which
links are in the IBMModel intersection, we treated
all links as equally likely when learning the QG.
Target language words unaligned to any source
language words were the only nodes allowed to
align to NULL in QG derivations.
We parsed the English side of the bitext with the
projective dependency parser described by Mc-
Donald et al (2005) trained on the Penn Treebank
??2?20. Much previous work on unsupervised
grammar induction has used gold-standard part-
of-speech tags (Smith and Eisner, 2006b; Klein
and Manning, 2004; Klein and Manning, 2002).
While there are no gold-standard tags for the Eu-
roparl bitext, we did train a conditional Markov
5
We made one change to the annotation conventions in
German: in the dependencies provided, words in a noun
phrase governed by a preposition were all attached to that
preposition. This meant that in the phrase das Kind (?the
child?) in, say, subject position, das was the child of Kind;
but, in f?ur das Kind (?for the child?), das was the child of
f?ur. This seems to be a strange choice in converting from the
TIGER constituency format, which does in fact annotate NPs
inside PPs; we have standardized prepositions to govern only
the head of the noun phrase. We did not change any other
annotation conventions to make them more like English. In
the Spanish treebank, for instance, control verbs are the chil-
dren of their verbal complements: in quiero decir (?I want to
say?=?I mean?), quiero is the child of decir. In German co-
ordinations, the coordinands all attach to the first, but in En-
glish, they all attach to the last. These particular divergences
in annotation style hurt all of our models equally (since none
of them have access to labeled trees). These annotation diver-
gences are one motivation for experiments below that include
some target trees.
828
Dependency accuracy [%]
Baselines German Spanish
Modify prev. 18.2 28.5
Modify next 27.5 21.4
EM 30.2 25.6
Hard proj. 66.2 59.1
Hard proj. w/EM 58.6 53.0
QG w/EM 68.5 64.8
Table 3: Test accuracy with unsupervised training methods
model tagger on a few thousand tagged sentences.
This is the only supervised data we used in the tar-
get. We created versions of each training corpus
with the first thousand, ten thousand, and hundred
thousand sentence pairs, each a prefix of the next.
Since the target-language-only baseline converged
much more slowly, we used a version of the cor-
pora with sentences 15 target words or fewer.
5.3.2 Fully Unsupervised EM
Using the target side of the bitext as training data,
we initialized our model parameters as described
in ?5.2 and ran EM. We checked convergence on
a development set and measured unlabeled depen-
dency accuracy on held-out test data. We com-
pare performance to simple attach-right and at-
tach left baselines (Table 3). For mostly head-
final German, the ?modify next? baseline is bet-
ter; for mostly head-initial Spanish, ?modify pre-
vious? wins. Even after several hundred iterations,
performance was slightly, but not significantly bet-
ter than the baseline for German. EM training did
not beat the baseline for Spanish.
6
5.3.3 Hard Projection, Semi-Supervised EM
The simplest approach to using the high-precision
one-to-one word alignments is labeled ?hard pro-
jection? in the table. We filtered the training cor-
pus to find sentences where enough links were
projected to completely determine a target lan-
guage tree. Of course, we needed to filter more
than 1000 sentences of bitext to output 1000
training sentences in this way. We simply per-
form supervised training with this subset, which
is still quite noisy (?4), and performance quickly
6
While these results are worse than those obtained previ-
ously for this model, the experiments in Klein and Manning
(2004) and only used sentences of 10 words or fewer, without
punctuation, and with gold-standard tags. Punctuation in par-
ticular seems to trip up the initializer: since a sentence-final
periods appear in most sentences, EM often decides to make
it the head.
plateaus. Still, this method substantially improves
over the baselines and unsupervised EM.
Restricting ourselves to fully projected trees
seems a waste of information. We can also sim-
ply take all one-to-one projected links, impute ex-
pected counts for the remaining dependencies with
EM, and update our models. This approach (?hard
projection with EM?), however, performed worse
than using only the fully projected trees. In fact,
only the first iteration of EM with this method
made any improvement; afterwards, EM degraded
accuracy further from the numbers in Table 3.
5.3.4 Soft Projection: QG & Conditional EM
The quasi-synchronous model used all of the
alignments in re-estimating its parameters and per-
formed significantly better than hard projection.
Unlike EM on the target language alone, the QG?s
performance does not depend on a clever initial-
izer for initial model weights?all parameters of
the generative model except for the QG configura-
tion features were initialized to zero. Setting the
prior to prefer direct correspondence provides the
necessary bias to initialize learning.
Error analysis showed that certain types of de-
pendencies eluded the QG?s ability to learn from
bitext. The Spanish treebank treats some verbal
complements as the heads of main verbs and aux-
iliary verbs as the children of participles; the QG,
following the English, learned the opposite de-
pendency direction. Spanish treebank conventions
for punctuation were also a common source of er-
rors. In both German and Spanish, coordinations
(a common bugbear for dependency grammars)
were often mishandled: both treebanks attach the
later coordinands and any conjunctions to the first
coordinand; the reverse is true in English. Finally,
in both German and Spanish, preposition attach-
ments often led to errors, which is not surprising
given the unlexicalized target-language grammars.
Rather than trying to adjudicate which dependen-
cies are ?mere? annotation conventions, it would
be useful to test learned dependency models on
some extrinsic task such as relation extraction or
machine translation.
6 Supervised Cross-Lingual Projection
Finally, we consider the problem of parser projec-
tion when some target language trees are available.
As in the adaptation case (?3), we train a condi-
tional model (not a generative DMV) of the target
829
tree given the target sentence, using the monolin-
gual and bilingual QG features, including config-
urations conjoined with tags, outlined above (?2).
For these experiments, we used the LDC?s
English-Chinese Parallel Treebank (ECTB). Since
manual word alignments also exist for a part of
this corpus, we were able to measure the loss in
accuracy (if any) from the use of an automatic
English parser and word aligner. The source-
language English dependency parser was trained
on the Wall Street Journal, where it achieved 91%
dependency accuracy on development data. How-
ever, it was only 80.3% accurate when applied to
our task, the English side of the ECTB.
7
After parsing the source side of the bitext, we
train a parser on the annotated target side, using
QG features described above (?2). Both the mono-
lingual target-language parser and the projected
parsers are trained to optimize conditional likeli-
hood of the target trees t
?
with ten iterations of
stochastic gradient ascent.
In Figure 3, we plot the performance of the
target-language parser on held-out bitext. Al-
though projection performance is, not surprisingly,
better if we know the true source trees at training
and test time, even with the 1-best output of the
source parser, QG features help produce a parser
as accurate asq one trained on twice the amount
of monolingual data. In ablation experiments, we
included bilingual features only for directly pro-
jected links, with no features for head-swapping,
grandparents, etc. When using 1-best English
parses, parsers trained only with direct-projection
and monolingual features performed worse; when
using gold English parses, parsers with direct-
projection-only features performed better when
trained with more Chinese trees.
7 Discussion
The two related problems of parser adaptation and
projection are often approached in different ways.
Many adaptation methods operate by simple aug-
mentations of the target feature space, as we have
done here (Daume III, 2007). Parser projection, on
the other hand, often uses a multi-stage pipeline
7
It would be useful to explore whether the techniques of
?3 above could be used to improve English accuracy by do-
main adaptation. In theory a model with QG features trained
to perform well on Chinese should not suffer from an inaccu-
rate, but consistent, English parser, but the results in Figure 3
indicate a significant benefit to be had from better English
parsing or from joint Chinese-English inference.
10 20 50 100 200 500 1000 2000
0.6
0
0.6
5
0.7
0
0.7
5
0.8
0
0.8
5
Training examples
Un
lab
ele
d a
ccu
rac
y
Target only
+Gold alignments
+Source text
+Gold parses, alignments
+Gold parses
Figure 3: Parser projection with target trees. Using the true
or 1-best parse trees in the source language is equivalent to
having twice as much data in the target language. Note that
the penalty for using automatic alignments instead of gold
alignments is negligible; in fact, using Source text alone is
often higher than +Gold alignments. Using gold source trees,
however, significantly outperforms using 1-best source trees.
(Hwa et al, 2005). The methods presented here
move parser projection much closer in efficiency
and simplicity to monolingual parsing.
We showed that augmenting a target parser with
quasi-synchronous features can lead to significant
improvements?first in experiments with adapt-
ing to different dependency representations in En-
glish, and then in cross-language parser projec-
tion. As with many domain adaptation problems,
it is quite helpful to have some annotated tar-
get data, especially when annotation styles vary
(Dredze et al, 2007). Our experiments show that
unsupervised QG projection improves on parsers
trained using only high-precision projected anno-
tations and far outperforms, by more than 35%
absolute dependency accuracy, unsupervised EM.
When a small number of target-language parse
trees is available, projection gives a boost equiv-
alent to doubling the number of target trees.
The loss in performance from conditioning only
on noisy 1-best source parses points to some nat-
ural avenues for improvement. We are explor-
ing methods that incorporate a packed parse for-
est on the source side and similar representations
of uncertainty about alignments. Building on our
recent belief propagation work (Smith and Eis-
ner, 2008), we can jointly infer two dependency
trees and their alignment, under a joint distribu-
tion p(t, a, t
?
| w,w
?
) that evaluates the full graph
of dependency and alignment edges.
830
References
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In TLT.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
M. Civit Torruella and M. A. Mart?? Anton??n. 2002.
Design principles for a Spanish treebank. In TLT.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In ACL-IJCNLP.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL, pages 256?263.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Jo?ao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adap-
tation for dependency parsing. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 1051?1055.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL-IJCNLP.
Kevin Gimpel and Noah A. Smith. 2009. Feature-rich
translation by quasi-synchronous lattice parsing. In
EMNLP.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:311?325.
Dan Klein and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In ACL, pages 128?135.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In ACL, pages
479?486.
Philipp Koehn. 2002. Europarl: A multilingual
corpus for evaluation of machine translation.
http://www.iccs.informatics.ed.ac.uk/?pkoehn/-
publications/europarl.ps.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In ACL, pages 470?477.
Andr?e F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In EMNLP.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In ACL, pages 337?344.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL, pages 91?98.
Noah A. Smith and Jason Eisner. 2005. Guiding unsu-
pervised grammar induction using contrastive esti-
mation. In International Joint Conference on Artifi-
cial Intelligence (IJCAI) Workshop on Grammatical
Inference Applications, Edinburgh, July.
David A. Smith and Jason Eisner. 2006a. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proceedings of
the HLT-NAACL Workshop on Statistical Machine
Translation, pages 23?30.
Noah A. Smith and Jason Eisner. 2006b. Annealing
structural bias in multilingual weighted grammar in-
duction. In ACL-COLING, pages 569?576.
David A. Smith and Jason Eisner. 2007. Bootstrap-
ping feature-rich dependency parsers with entropic
priors. In EMNLP-CoNLL, pages 667?677.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages
145?156.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In EMNLP, pages 49?56.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL-IJCNLP.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In EMNLP-CoNLL,
pages 22?32.
831
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880?889,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Polylingual Topic Models
David Mimno Hanna M. Wallach Jason Naradowsky
University of Massachusetts, Amherst
Amherst, MA 01003
{mimno, wallach, narad, dasmith, mccallum}cs.umass.edu
David A. Smith Andrew McCallum
Abstract
Topic models are a useful tool for analyz-
ing large text collections, but have previ-
ously been applied in only monolingual,
or at most bilingual, contexts. Mean-
while, massive collections of interlinked
documents in dozens of languages, such
as Wikipedia, are now widely available,
calling for tools that can characterize con-
tent in many languages. We introduce a
polylingual topic model that discovers top-
ics aligned across multiple languages. We
explore the model?s characteristics using
two large corpora, each with over ten dif-
ferent languages, and demonstrate its use-
fulness in supporting machine translation
and tracking topic trends across languages.
1 Introduction
Statistical topic models have emerged as an in-
creasingly useful analysis tool for large text col-
lections. Topic models have been used for analyz-
ing topic trends in research literature (Mann et al,
2006; Hall et al, 2008), inferring captions for im-
ages (Blei and Jordan, 2003), social network anal-
ysis in email (McCallum et al, 2005), and expand-
ing queries with topically related words in infor-
mation retrieval (Wei and Croft, 2006). Much of
this work, however, has occurred in monolingual
contexts. In an increasingly connected world, the
ability to access documents in many languages has
become both a strategic asset and a personally en-
riching experience. In this paper, we present the
polylingual topic model (PLTM). We demonstrate
its utility and explore its characteristics using two
polylingual corpora: proceedings of the European
parliament (in eleven languages) and a collection
of Wikipedia articles (in twelve languages).
There are many potential applications for
polylingual topic models. Although research liter-
ature is typically written in English, bibliographic
databases often contain substantial quantities of
work in other languages. To perform topic-based
bibliometric analysis on these collections, it is
necessary to have topic models that are aligned
across languages. Such analysis could be sig-
nificant in tracking international research trends,
where language barriers slow the transfer of ideas.
Previous work on bilingual topic modeling
has focused on machine translation applications,
which rely on sentence-aligned parallel transla-
tions. However, the growth of the internet, and
in particular Wikipedia, has made vast corpora
of topically comparable texts?documents that are
topically similar but are not direct translations of
one another?considerably more abundant than
ever before. We argue that topic modeling is
both a useful and appropriate tool for leveraging
correspondences between semantically compara-
ble documents in multiple different languages.
In this paper, we use two polylingual corpora
to answer various critical questions related to
polylingual topic models. We employ a set of di-
rect translations, the EuroParl corpus, to evaluate
whether PLTM can accurately infer topics when
documents genuinely contain the same content.
We also explore how the characteristics of dif-
ferent languages affect topic model performance.
The second corpus, Wikipedia articles in twelve
languages, contains sets of documents that are not
translations of one another, but are very likely to
be about similar concepts. We use this corpus
to explore the ability of the model both to infer
similarities between vocabularies in different lan-
guages, and to detect differences in topic emphasis
between languages. The internet makes it possible
for people all over the world to access documents
from different cultures, but readers will not be flu-
ent in this wide variety of languages. By linking
topics across languages, polylingual topic mod-
els can increase cross-cultural understanding by
providing readers with the ability to characterize
880
the contents of collections in unfamiliar languages
and identify trends in topic prevalence.
2 Related Work
Bilingual topic models for parallel texts with
word-to-word alignments have been studied pre-
viously using the HM-bitam model (Zhao and
Xing, 2007). Tam, Lane and Schultz (Tam et
al., 2007) also show improvements in machine
translation using bilingual topic models. Both
of these translation-focused topic models infer
word-to-word alignments as part of their inference
procedures, which would become exponentially
more complex if additional languages were added.
We take a simpler approach that is more suit-
able for topically similar document tuples (where
documents are not direct translations of one an-
other) in more than two languages. A recent ex-
tended abstract, developed concurrently by Ni et
al. (Ni et al, 2009), discusses a multilingual topic
model similar to the one presented here. How-
ever, they evaluate their model on only two lan-
guages (English and Chinese), and do not use the
model to detect differences between languages.
They also provide little analysis of the differ-
ences between polylingual and single-language
topic models. Outside of the field of topic mod-
eling, Kawaba et al (Kawaba et al, 2008) use
a Wikipedia-based model to perform sentiment
analysis of blog posts. They find, for example,
that English blog posts about the Nintendo Wii of-
ten relate to a hack, which cannot be mentioned in
Japanese posts due to Japanese intellectual prop-
erty law. Similarly, posts about whaling often
use (positive) nationalist language in Japanese and
(negative) environmentalist language in English.
3 Polylingual Topic Model
The polylingual topic model (PLTM) is an exten-
sion of latent Dirichlet alocation (LDA) (Blei et
al., 2003) for modeling polylingual document tu-
ples. Each tuple is a set of documents that are
loosely equivalent to each other, but written in dif-
ferent languages, e.g., corresponding Wikipedia
articles in French, English and German. PLTM as-
sumes that the documents in a tuple share the same
tuple-specific distribution over topics. This is un-
like LDA, in which each document is assumed to
have its own document-specific distribution over
topics. Additionally, PLTM assumes that each
?topic? consists of a set of discrete distributions
D
N
1
T
N
L
...
w
? ?
wz
z
...
?
1
?
L
?
1
?
L
Figure 1: Graphical model for PLTM.
over words?one for each language l = 1, . . . , L.
In other words, rather than using a single set of
topics ? = {?
1
, . . . ,?
T
}, as in LDA, there are L
sets of language-specific topics, ?
1
, . . . ,?
L
, each
of which is drawn from a language-specific sym-
metric Dirichlet with concentration parameter ?
l
.
3.1 Generative Process
A new document tuplew = (w
1
, . . . ,w
L
) is gen-
erated by first drawing a tuple-specific topic dis-
tribution from an asymmetric Dirichlet prior with
concentration parameter ? and base measurem:
? ? Dir (?, ?m). (1)
Then, for each language l, a latent topic assign-
ment is drawn for each token in that language:
z
l
? P (z
l
|?) =
?
n
?
z
l
n
. (2)
Finally, the observed tokens are themselves drawn
using the language-specific topic parameters:
w
l
? P (w
l
| z
l
,?
l
) =
?
n
?
l
w
l
n
|z
l
n
. (3)
The graphical model is shown in figure 1.
3.2 Inference
Given a corpus of training and test document
tuples?W and W
?
, respectively?two possible
inference tasks of interest are: computing the
probability of the test tuples given the training
tuples and inferring latent topic assignments for
test documents. These tasks can either be accom-
plished by averaging over samples of ?
1
, . . . ,?
L
and ?m from P (?
1
, . . . ,?
L
, ?m |W
?
, ?) or by
evaluating a point estimate. We take the lat-
ter approach, and use the MAP estimate for ?m
and the predictive distributions over words for
?
1
, . . . ,?
L
. The probability of held-out docu-
ment tuples W
?
given training tuples W is then
approximated by P (W
?
|?
1
, . . . ,?
L
, ?m).
Topic assignments for a test document tuple
w = (w
1
, . . . ,w
L
) can be inferred using Gibbs
881
sampling. Gibbs sampling involves sequentially
resampling each z
l
n
from its conditional posterior:
P (z
l
n
= t |w, z
\l,n
,?
1
, . . . ,?
L
, ?m)
? ?
l
w
l
n
|t
(N
t
)
\l,n
+ ?m
t
?
t
N
t
? 1 + ?
, (4)
where z
\l,n
is the current set of topic assignments
for all other tokens in the tuple, while (N
t
)
\l,n
is
the number of occurrences of topic t in the tuple,
excluding z
l
n
, the variable being resampled.
4 Results on Parallel Text
Our first set of experiments focuses on document
tuples that are known to consist of direct transla-
tions. In this case, we can be confident that the
topic distribution is genuinely shared across all
languages. Although direct translations in multi-
ple languages are relatively rare (in contrast with
comparable documents), we use direct translations
to explore the characteristics of the model.
4.1 Data Set
The EuroParl corpus consists of parallel texts in
eleven western European languages: Danish, Ger-
man, Greek, English, Spanish, Finnish, French,
Italian, Dutch, Portuguese and Swedish. These
texts consist of roughly a decade of proceedings
of the European parliament. For our purposes we
use alignments at the speech level rather than the
sentence level, as in many translation tasks using
this corpus. We also remove the twenty-five most
frequent word types for efficiency reasons. The
remaining collection consists of over 121 million
words. Details by language are shown in Table 1.
Table 1: Average document length, # documents, and
unique word types per 10,000 tokens in the EuroParl corpus.
Lang. Avg. leng. # docs types/10k
DA 160.153 65245 121.4
DE 178.689 66497 124.5
EL 171.289 46317 124.2
EN 176.450 69522 43.1
ES 170.536 65929 59.5
FI 161.293 60822 336.2
FR 186.742 67430 54.8
IT 187.451 66035 69.5
NL 176.114 66952 80.8
PT 183.410 65718 68.2
SV 154.605 58011 136.1
Models are trained using 1000 iterations of
Gibbs sampling. Each language-specific topic?
word concentration parameter ?
l
is set to 0.01.
centralbank europ?iske ecb s l?n centralbanks 
zentralbank ezb bank europ?ischen investitionsbank darlehen 
??????? ???????? ???????? ??? ????????? ???????? 
bank central ecb banks european monetary 
banco central europeo bce bancos centrales 
keskuspankin ekp n euroopan keskuspankki eip 
banque centrale bce europ?enne banques mon?taire 
banca centrale bce europea banche prestiti 
bank centrale ecb europese banken leningen 
banco central europeu bce bancos empr?stimos 
centralbanken europeiska ecb centralbankens s l?n 
b?rn familie udnyttelse b?rns b?rnene seksuel 
kinder kindern familie ausbeutung familien eltern 
?????? ??????? ?????????? ??????????? ?????? ???????? 
children family child sexual families exploitation 
ni?os familia hijos sexual infantil menores 
lasten lapsia lapset perheen lapsen lapsiin 
enfants famille enfant parents exploitation familles 
bambini famiglia figli minori sessuale sfruttamento 
kinderen kind gezin seksuele ouders familie 
crian?as fam?lia filhos sexual crian?a infantil 
barn barnen familjen sexuellt familj utnyttjande 
m?l n? m?ls?tninger m?let m?ls?tning opn? 
ziel ziele erreichen zielen erreicht zielsetzungen 
??????? ????? ?????? ?????? ?????? ???????? 
objective objectives achieve aim ambitious set 
objetivo objetivos alcanzar conseguir lograr estos 
tavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteen 
objectif objectifs atteindre but cet ambitieux 
obiettivo obiettivi raggiungere degli scopo quello 
doelstellingen doel doelstelling bereiken bereikt doelen 
objectivo objectivos alcan?ar atingir ambicioso conseguir 
m?l m?let uppn? m?len m?ls?ttningar m?ls?ttning 
andre anden side ene andet ?vrige 
anderen andere einen wie andererseits anderer 
????? ???? ???? ????? ?????? ???? 
other one hand others another there 
otros otras otro otra parte dem?s 
muiden toisaalta muita muut muihin muun 
autres autre part c?t? ailleurs m?me 
altri altre altro altra dall parte 
andere anderzijds anderen ander als kant 
outros outras outro lado outra noutros 
andra sidan ? annat ena annan 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
Figure 2: EuroParl topics (T=400)
The concentration parameter ? for the prior over
document-specific topic distributions is initialized
to 0.01T , while the base measure m is initialized
to the uniform distribution. Hyperparameters ?m
are re-estimated every 10 Gibbs iterations.
4.2 Analysis of Trained Models
Figure 2 shows the most probable words in all lan-
guages for four example topics, from PLTM with
400 topics. The first topic contains words relating
to the European Central Bank. This topic provides
an illustration of the variation in technical ter-
minology captured by PLTM, including the wide
array of acronyms used by different languages.
The second topic, concerning children, demon-
strates the variability of everyday terminology: al-
though the four Romance languages are closely
882
related, they use etymologically unrelated words
for children. (Interestingly, all languages except
Greek and Finnish use closely related words for
?youth? or ?young? in a separate topic.) The third
topic demonstrates differences in inflectional vari-
ation. English and the Romance languages use
only singular and plural versions of ?objective.?
The other Germanic languages include compound
words, while Greek and Finnish are dominated by
inflected variants of the same lexical item. The fi-
nal topic demonstrates that PLTM effectively clus-
ters ?syntactic? words, as well as more semanti-
cally specific nouns, adjectives and verbs.
Although the topics in figure 2 seem highly fo-
cused, it is interesting to ask whether the model
is genuinely learning mixtures of topics or simply
assigning entire document tuples to single topics.
To answer this question, we compute the posterior
probability of each topic in each tuple under the
trained model. If the model assigns all tokens in
a tuple to a single topic, the maximum posterior
topic probability for that tuple will be near to 1.0.
If the model assigns topics uniformly, the maxi-
mum topic probability will be near 1/T . We com-
pute histograms of these maximum topic prob-
abilities for T ? {50, 100, 200, 400, 800}. For
clarity, rather than overlaying five histograms, fig-
ure 3 shows the histograms converted into smooth
curves using a kernel density estimator.
1
Although
there is a small bump around 1.0 (for extremely
short documents, e.g., ?Applause?), values are
generally closer to, but greater than, 1/T .
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
10
12
Smoothed histograms of max(P(t))
Maximum topic probability in document
Den
sity 800 topics400 topics200 topics100 topics50 topics
Figure 3: Smoothed histograms of the probability of the
most probable topic in a document tuple.
Although the posterior distribution over topics
for each tuple is not concentrated on one topic,
it is worth checking that this is not simply be-
cause the model is assigning a single topic to the
1
We use the R density function.
tokens in each of the languages. Although the
model does not distinguish between topic assign-
ment variables within a given document tuple (so
it is technically incorrect to speak of different pos-
terior distributions over topics for different docu-
ments in a given tuple), we can nevertheless divide
topic assignment variables between languages and
use them to estimate a Dirichlet-multinomial pos-
terior distribution for each language in each tuple.
For each tuple we can then calculate the Jensen-
Shannon divergence (the average of the KL di-
vergences between each distribution and a mean
distribution) between these distributions. Figure 4
shows the density of these divergences for differ-
ent numbers of topics. As with the previous fig-
ure, there are a small number of documents that
contain only one topic in all languages, and thus
have zero divergence. These tend to be very short,
formulaic parliamentary responses, however. The
vast majority of divergences are relatively low (1.0
indicates no overlap in topics between languages
in a given document tuple) indicating that, for each
tuple, the model is not simply assigning all tokens
in a particular language to a single topic. As the
number of topics increases, greater variability in
topic distributions causes divergence to increase.
0.0 0.1 0.2 0.3 0.4 0.5
0
5
10
15
20
Smoothed histograms of inter?language JS divergence
Jensen?Shannon Divergence
Den
sity
800 topics400 topics200 topics100 topics50 topics
Figure 4: Smoothed histograms of the Jensen-Shannon
divergences between the posterior probability of topics be-
tween languages.
4.3 Language Model Evaluation
A topic model specifies a probability distribution
over documents, or in the case of PLTM, docu-
ment tuples. Given a set of training document tu-
ples, PLTM can be used to obtain posterior esti-
mates of ?
1
, . . . ,?
L
and ?m. The probability of
previously unseen held-out document tuples given
these estimates can then be computed. The higher
the probability of the held-out document tuples,
the better the generalization ability of the model.
883
Analytically calculating the probability of a set
of held-out document tuples given ?
1
, . . . ,?
L
and
?m is intractable, due to the summation over an
exponential number of topic assignments for these
held-out documents. However, recently developed
methods provide efficient, accurate estimates of
this probability. We use the ?left-to-right? method
of (Wallach et al, 2009). We perform five esti-
mation runs for each document and then calculate
standard errors using a bootstrap method.
Table 2 shows the log probability of held-out
data in nats per word for PLTM and LDA, both
trained with 200 topics. There is substantial varia-
tion between languages. Additionally, the predic-
tive ability of PLTM is consistently slightly worse
than that of (monolingual) LDA. It is important to
note, however, that these results do not imply that
LDA should be preferred over PLTM?that choice
depends upon the needs of the modeler. Rather,
these results are intended as a quantitative analy-
sis of the difference between the two models.
Table 2: Held-out log probability in nats/word. (Smaller
magnitude implies better language modeling performance.)
PLTM does slightly worse than monolingual LDA models,
but the variation between languages is much larger.
Lang PLTM sd LDA sd
DA -8.11 0.00067 -8.02 0.00066
DE -8.17 0.00057 -8.08 0.00072
EL -8.44 0.00079 -8.36 0.00087
EN -7.51 0.00064 -7.42 0.00069
ES -7.98 0.00073 -7.87 0.00070
FI -9.25 0.00089 -9.21 0.00065
FR -8.26 0.00072 -8.19 0.00058
IT -8.11 0.00071 -8.02 0.00058
NL -7.84 0.00067 -7.75 0.00099
PT -7.87 0.00085 -7.80 0.00060
SV -8.25 0.00091 -8.16 0.00086
As the number of topics is increased, the word
counts per topic become very sparse in mono-
lingual LDA models, proportional to the size of
the vocabulary. Figure 5 shows the proportion
of all tokens in English and Finnish assigned to
each topic under LDA and PLTM with 800 topics.
More than 350 topics in the Finnish LDA model
have zero tokens assigned to them, and almost all
tokens are assigned to the largest 200 topics. En-
glish has a larger tail, with non-zero counts in all
but 16 topics. In contrast, PLTM assigns a sig-
nificant number of tokens to almost all 800 top-
ics, in very similar proportions in both languages.
PLTM topics therefore have a higher granularity ?
i.e., they are more specific. This result is impor-
tant: informally, we have found that increasing the
granularity of topics correlates strongly with user
perceptions of the utility of a topic model.
0 200 400 600 800
0.00
0.01
0.02
0.03
0.04
Sorted topic rank
Perc
enta
ge o
f tok
ens
Figure 5: Topics sorted by number of words assigned.
Finnish is in black, English is in red; LDA is solid, PLTM is
dashed. LDA in Finnish essentially learns a 200 topic model
when given 800 topics, while PLTM uses all 800 topics.
4.4 Partly Comparable Corpora
An important application for polylingual topic
modeling is to use small numbers of comparable
document tuples to link topics in larger collections
of distinct, non-comparable documents in multiple
languages. For example, a journal might publish
papers in English, French, German and Italian. No
paper is exactly comparable to any other paper, but
they are all roughly topically similar. If we wish
to perform topic-based bibliometric analysis, it is
vital to be able to track the same topics across all
languages. One simple way to achieve this topic
alignment is to add a small set of comparable doc-
ument tuples that provide sufficient ?glue? to bind
the topics together. Continuing with the exam-
ple above, one might extract a set of connected
Wikipedia articles related to the focus of the jour-
nal and then train PLTM on a joint corpus consist-
ing of journal papers and Wikipedia articles.
In order to simulate this scenario we create a
set of variations of the EuroParl corpus by treat-
ing some documents as if they have no paral-
lel/comparable texts ? i.e., we put each of these
documents in a single-document tuple. To do this,
we divide the corpusW into two sets of document
tuples: a ?glue? set G and a ?separate? set S such
that |G| / |W| = p. In other words, the proportion
of tuples in the corpus that are treated as ?glue?
(i.e., placed in G) is p. For every tuple in S, we
assign each document in that tuple to a new single-
document tuple. By doing this, every document in
S has its own distribution over topics, independent
of any other documents. Ideally, the ?glue? doc-
884
uments in G will be sufficient to align the topics
across languages, and will cause comparable doc-
uments in S to have similar distributions over top-
ics even though they are modeled independently.
Table 3: The effect of the proportion p of ?glue? tuples on
mean Jensen-Shannon divergence in estimated topic distribu-
tions for pairs of documents in S that were originally part of
a document tuple. Lower divergence means the topic distri-
butions distributions are more similar to each other.
p Mean JS # of pairs Std. Err.
0.01 0.83755 487670 0.00018
0.05 0.79144 467288 0.00021
0.1 0.70228 443753 0.00026
0.25 0.38480 369608 0.00029
0.5 0.29712 246380 0.00030
Table 4: Topics are meaningful within languages but di-
verge between languages when only 1% of tuples are treated
as ?glue? tuples. With 25% ?glue? tuples, topics are aligned.
lang Topics at p = 0.01
DE ru?land russland russischen tschetschenien sicherheit
EN china rights human country s burma
FR russie tch?etch?enie union avec russe r?egion
IT ho presidente mi perch?e relazione votato
lang Topics at p = 0.25
DE ru?land russland russischen tschetschenien ukraine
EN russia russian chechnya cooperation region belarus
FR russie tch?etch?enie avec russe russes situation
IT russia unione cooperazione cecenia regione russa
We train PLTM with 100 topics on corpora with
p ? {0.01, 0.05, 0.1, 0.25, 0.5}. We use 1000 it-
erations of Gibbs sampling with ? = 0.01. Hy-
perparameters ?m are re-estimated every 10 it-
erations. We calculate the Jensen-Shannon diver-
gence between the topic distributions for each pair
of individual documents in S that were originally
part of the same tuple prior to separation. The
lower the divergence, the more similar the distri-
butions are to each other. From the results in fig-
ure 4, we know that leaving all document tuples
intact should result in a mean JS divergence of
less than 0.1. Table 3 shows mean JS divergences
for each value of p. As expected, JS divergence is
greater than that obtained when all tuples are left
intact. Divergence drops significantly when the
proportion of ?glue? tuples increases from 0.01 to
0.25. Example topics for p = 0.01 and p = 0.25
are shown in table 4. At p = 0.01 (1% ?glue? doc-
uments), German and French both include words
relating to Russia, while the English and Italian
word distributions appear locally consistent but
unrelated to Russia. At p = 0.25, the top words
for all four languages are related to Russia.
These results demonstrate that PLTM is appro-
priate for aligning topics in corpora that have only
a small subset of comparable documents. One area
for future work is to explore whether initializa-
tion techniques or better representations of topic
co-occurrence might result in alignment of topics
with a smaller proportion of comparable texts.
4.5 Machine Translation
Although the PLTM is clearly not a substitute for
a machine translation system?it has no way to
represent syntax or even multi-word phrases?it is
clear from the examples in figure 2 that the sets of
high probability words in different languages for a
given topic are likely to include translations. We
therefore evaluate the ability of the PLTM to gen-
erate bilingual lexica, similar to other work in un-
supervised translation modeling (Haghighi et al,
2008). In the early statistical translation model
work at IBM, these representations were called
?cepts,? short for concepts (Brown et al, 1993).
We evaluate sets of high-probability words in
each topic and multilingual ?synsets? by compar-
ing them to entries in human-constructed bilingual
dictionaries, as done by Haghighi et al (2008).
Unlike previous work (Koehn and Knight, 2002),
we evaluate all words, not just nouns. We col-
lected bilingual lexica mapping English words to
German, Greek, Spanish, French, Italian, Dutch
and Swedish. Each lexicon is a set of pairs con-
sisting of an English word and a translated word,
{w
e
, w
`
}. We do not consider multi-word terms.
We expect that simple analysis of topic assign-
ments for sequential words would yield such col-
locations, but we leave this for future work.
For every topic t we select a small number K
of the most probable words in English (e) and
in each ?translation? language (`): W
te
and W
t`
,
respectively. We then add the Cartesian product
of these sets for every topic to a set of candidate
translations C. We report the number of elements
of C that appear in the reference lexica. Results
for K = 1, that is, considering only the single
most probable word for each language, are shown
in figure 6. Precision at this level is relatively
high, above 50% for Spanish, French and Italian
with T = 400 and 800. Many of the candidate
pairs that were not in the bilingual lexica were
valid translations (e.g. EN ?comitology? and IT
885
?comitalogia?) that simply were not in the lexica.
We also do not count morphological variants: the
model finds EN ?rules? and DE ?vorschriften,? but
the lexicon contains only ?rule? and ?vorschrift.?
Results remain strong as we increase K. With
K = 3, T = 800, 1349 of the 7200 candidate
pairs for Spanish appeared in the lexicon.
l l
l
l
l
200 400 600 800
0
100
200
300
400
500
Translation pairs at K=1
Topics
Corr
ect t
rans
lation
s
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ESFRITDESVEL
Figure 6: Are the single most probable words for a given
topic in different languages translations of each other? The
number of such pairs that appear in bilingual lexica is shown
on the y-axis. For T = 800, the top English and Spanish
words in 448 topics were exact translations of one another.
4.6 Finding Translations
In addition to enhancing lexicons by aligning
topic-specific vocabulary, PLTM may also be use-
ful for adapting machine translation systems to
new domains by finding translations or near trans-
lations in an unstructured corpus. These aligned
document pairs could then be fed into standard
machine translation systems as training data. To
evaluate this scenario, we train PLTM on a set of
document tuples from EuroParl, infer topic distri-
butions for a set of held-out documents, and then
measure our ability to align documents in one lan-
guage with their translations in another language.
It is not necessarily clear that PLTM will be ef-
fective at identifying translations. In finding a low-
dimensional semantic representation, topic mod-
els deliberately smooth over much of the varia-
tion present in language. We are therefore inter-
ested in determining whether the information in
the document-specific topic distributions is suffi-
cient to identify semantically identical documents.
We begin by dividing the data into a training
set of 69,550 document tuples and a test set of
17,435 document tuples. In order to make the task
more difficult, we train a relatively coarse-grained
PLTM with 50 topics on the training set. We then
use this model to infer topic distributions for each
40
50
60
70
80
90
100
Min query doc length
% o
f tra
nsl 
at r
ank
0 50 100 200
Rank 1
Rank 5
Rank 10
Rank 20
Figure 7: Percent of query language documents for which
the target language translation is ranked at or above 1, 5, 10
or 20 by JS divergence, averaged over all language pairs.
of the 11 documents in each of the held-out doc-
ument tuples using a method similar to that used
to calculate held-out probabilities (Wallach et al,
2009). Finally, for each pair of languages (?query?
and ?target?) we calculate the difference between
the topic distribution for each held-out document
in the query language and the topic distribution for
each held-out document in the target language. We
use both Jensen-Shannon divergence and cosine
distance. For each document in the query language
we rank all documents in the target language and
record the rank of the actual translation.
Results averaged over all query/target language
pairs are shown in figure 7 for Jensen-Shannon
divergence. Cosine-based rankings are signifi-
cantly worse. It is important to note that the
length of documents matters. As noted before,
many of the documents in the EuroParl collection
consist of short, formulaic sentences. Restrict-
ing the query/target pairs to only those with query
and target documents that are both longer than 50
words results in significant improvement and re-
duced variance: the average proportion of query
documents for which the true translation is ranked
highest goes from 53.9% to 72.7%. Performance
continues to improve with longer documents, most
likely due to better topic inference. Results vary
by language. Table 5 shows results for all tar-
get languages with English as a query language.
Again, English generally performs better with Ro-
mance languages than Germanic languages.
5 Results on Comparable Texts
Directly parallel translations are rare in many lan-
guages and can be extremely expensive to pro-
duce. However, the growth of the web, and in par-
ticular Wikipedia, has made comparable text cor-
886
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,
centers around Nordic counties. The center topic, actor role television actress, is relatively uniform. The right topic, ottoman
empire khan byzantine, is popular in all languages but especially in regions near Istanbul.
Table 5: Percent of English query documents for which the
translation was in the top n ? {1, 5, 10, 20} documents by JS
divergence between topic distributions. To reduce the effect
of short documents we consider only document pairs where
the query and target documents are longer than 100 words.
Lang 1 5 10 20
DA 78.0 90.7 93.8 95.8
DE 76.6 90.0 93.4 95.5
EL 77.1 90.4 93.3 95.2
ES 81.2 92.3 94.8 96.7
FI 76.7 91.0 94.0 96.3
FR 80.1 91.7 94.3 96.2
IT 79.1 91.2 94.1 96.2
NL 76.6 90.1 93.4 95.5
PT 80.8 92.0 94.7 96.5
SV 80.4 92.1 94.9 96.5
pora ? documents that are topically similar but are
not direct translations of one another ? consider-
ably more abundant than true parallel corpora.
In this section, we explore two questions re-
lating to comparable text corpora and polylingual
topic modeling. First, we explore whether com-
parable document tuples support the alignment of
fine-grained topics, as demonstrated earlier using
parallel documents. This property is useful for
building machine translation systems as well as
for human readers who are either learning new
languages or analyzing texts in languages they do
not know. Second, because comparable texts may
not use exactly the same topics, it becomes cru-
cially important to be able to characterize differ-
ences in topic prevalence at the document level (do
different languages have different perspectives on
the same article?) and at the language-wide level
(which topics do particular languages focus on?).
5.1 Data Set
We downloaded XML copies of all Wikipedia ar-
ticles in twelve different languages: Welsh, Ger-
man, Greek, English, Farsi, Finnish, French, He-
brew, Italian, Polish, Russian and Turkish. These
versions of Wikipedia were selected to provide a
diverse range of language families, geographic ar-
eas, and quantities of text. We preprocessed the
data by removing tables, references, images and
info-boxes. We dropped all articles in non-English
languages that did not link to an English article. In
the English version of Wikipedia we dropped all
articles that were not linked to by any other lan-
guage in our set. For efficiency, we truncated each
article to the nearest word after 1000 characters
and dropped the 50 most common word types in
each language. Even with these restrictions, the
size of the corpus is 148.5 million words.
We present results for a PLTM with 400 topics.
1000 Gibbs sampling iterations took roughly four
days on one CPU with current hardware.
5.2 Which Languages Have High Topic
Divergence?
As with EuroParl, we can calculate the Jensen-
Shannon divergence between pairs of documents
within a comparable document tuple. We can then
average over all such document-document diver-
gences for each pair of languages to get an over-
all ?disagreement? score between languages. In-
terestingly, we find that almost all languages in
our corpus, including several pairs that have his-
torically been in conflict, show average JS diver-
gences of between approximately 0.08 and 0.12
for T = 400, consistent with our findings for
EuroParl translations. Subtle differences of sen-
timent may be below the granularity of the model.
887
sadwrn blaned gallair at lloeren mytholeg 
space nasa sojus flug mission 
?????????? sts nasa ???? small 
space mission launch satellite nasa spacecraft 
??????? ??????? ???? ???? ??????? ?????  
sojuz nasa apollo ensimm?inen space lento 
spatiale mission orbite mars satellite spatial 
?????? ? ???? ??? ???? ????  
spaziale missione programma space sojuz stazione 
misja kosmicznej stacji misji space nasa 
??????????? ???? ???????????? ??????? ???????
uzay soyuz ay uzaya salyut sovyetler 
sbaen madrid el la jos? sbaeneg 
de spanischer spanischen spanien madrid la 
???????? ??????? de ??????? ??? ??????? 
de spanish spain la madrid y 
?????? ???? ????????? ???????  de ????  
espanja de espanjan madrid la real 
espagnol espagne madrid espagnole juan y 
???? ??????? ????? ?? ?????? ????  
de spagna spagnolo spagnola madrid el 
de hiszpa?ski hiszpanii la juan y 
?? ?????? ??????? ??????? ????????? de 
ispanya ispanyol madrid la k?ba real 
bardd gerddi iaith beirdd fardd gymraeg 
dichter schriftsteller literatur gedichte gedicht werk 
??????? ?????? ?????? ???? ??????? ???????? 
poet poetry literature literary poems poem 
???? ???? ????? ?????? ??? ????  
runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi 
po?te ?crivain litt?rature po?sie litt?raire ses 
?????? ????? ???? ???? ????? ?????
poeta letteratura poesia opere versi poema 
poeta literatury poezji pisarz in jego 
???? ??? ???????? ?????????? ?????? ????????? 
?air edebiyat ?iir yazar edebiyat? adl? 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 9: Wikipedia topics (T=400).
Overall, these scores indicate that although indi-
vidual pages may show disagreement, Wikipedia
is on average consistent between languages.
5.3 Are Topics Emphasized Differently
Between Languages?
Although we find that if Wikipedia contains an ar-
ticle on a particular subject in some language, the
article will tend to be topically similar to the arti-
cles about that subject in other languages, we also
find that across the whole collection different lan-
guages emphasize topics to different extents. To
demonstrate the wide variation in topics, we cal-
culated the proportion of tokens in each language
assigned to each topic. Figure 8 represents the es-
timated probabilities of topics given a specific lan-
guage. Competitive cross-country skiing (left) ac-
counts for a significant proportion of the text in
Finnish, but barely exists in Welsh and the lan-
guages in the Southeastern region. Meanwhile,
interest in actors and actresses (center) is consis-
tent across all languages. Finally, historical topics,
such as the Byzantine and Ottoman empires (right)
are strong in all languages, but show geographical
variation: interest centers around the empires.
6 Conclusions
We introduced a polylingual topic model (PLTM)
that discovers topics aligned across multiple lan-
guages. We analyzed the characteristics of PLTM
in comparison to monolingual LDA, and demon-
strated that it is possible to discover aligned top-
ics. We also demonstrated that relatively small
numbers of topically comparable document tu-
ples are sufficient to align topics between lan-
guages in non-comparable corpora. Additionally,
PLTM can support the creation of bilingual lexica
for low resource language pairs, providing candi-
date translations for more computationally intense
alignment processes without the sentence-aligned
translations typically used in such tasks. When
applied to comparable document collections such
as Wikipedia, PLTM supports data-driven analysis
of differences and similarities across all languages
for readers who understand any one language.
7 Acknowledgments
The authors thank Limin Yao, who was involved
in early stages of this project. This work was
supported in part by the Center for Intelligent In-
formation Retrieval, in part by The Central In-
telligence Agency, the National Security Agency
and National Science Foundation under NSF grant
number IIS-0326249, and in part by Army prime
contract number W911NF-07-1-0216 and Uni-
versity of Pennsylvania subaward number 103-
548106, and in part by National Science Founda-
tion under NSF grant #CNS-0619337. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this material are the authors?
and do not necessarily reflect those of the sponsor.
References
David Blei and Michael Jordan. 2003. Modeling an-
notated data. In SIGIR.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. CL, 19(2):263?311.
888
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In EMNLP.
Mariko Kawaba, Hiroyuki Nakasaki, Takehito Utsuro,
and Tomohiro Fukuhara. 2008. Cross-lingual blog
analysis based on multilingual blog distillation from
multilingual Wikipedia entries. In ICWSM.
Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.
Gideon Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leverag-
ing topic analysis. In JCDL.
Andrew McCallum, Andr?es Corrada-Emmanuel, and
Xuerui Wang. 2005. Topic and role discovery in
social networks. In IJCAI.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia.
In WWW.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 28:187?
207.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In SIGIR.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In NIPS.
889
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 132?140, Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Models of Nonprojective Dependency Trees
David A. Smith
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218 USA
dasmith@cs.jhu.edu
Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
nasmith@cs.cmu.edu
Abstract
A notable gap in research on statistical de-
pendency parsing is a proper conditional
probability distribution over nonprojective
dependency trees for a given sentence. We
exploit the Matrix Tree Theorem (Tutte,
1984) to derive an algorithm that efficiently
sums the scores of all nonprojective trees
in a sentence, permitting the definition of
a conditional log-linear model over trees.
While discriminative methods, such as those
presented in McDonald et al (2005b), ob-
tain very high accuracy on standard de-
pendency parsing tasks and can be trained
and applied without marginalization, ?sum-
ming trees? permits some alternative tech-
niques of interest. Using the summing al-
gorithm, we present competitive experimen-
tal results on four nonprojective languages,
for maximum conditional likelihood estima-
tion, minimum Bayes-risk parsing, and hid-
den variable training.
1 Introduction
Recently dependency parsing has received renewed
interest, both in the parsing literature (Buchholz
and Marsi, 2006) and in applications like translation
(Quirk et al, 2005) and information extraction (Cu-
lotta and Sorensen, 2004). Dependency parsing can
be used to provide a ?bare bones? syntactic struc-
ture that approximates semantics, and it has the addi-
tional advantage of admitting fast parsing algorithms
(Eisner, 1996; McDonald et al, 2005b) with a neg-
ligible grammar constant in many cases.
The latest state-of-the-art statistical dependency
parsers are discriminative, meaning that they are
based on classifiers trained to score trees, given a
sentence, either via factored whole-structure scores
(McDonald et al, 2005a) or local parsing decision
scores (Hall et al, 2006). In the works cited, these
scores are not intended to be interpreted as proba-
bilistic quantities.
Here we consider weighted dependency parsing
models that can be used to define well-formed con-
ditional distributions p(y | x), for dependency
trees y and a sentence x. Conditional distribu-
tions over outputs (here, trees) given inputs (here,
sentences) have certain advantages. They per-
mit marginalization over trees to compute poste-
riors of interesting sub-events (e.g., the probabil-
ity that two noun tokens bear a relation, regard-
less of which tree is correct). A probability model
permits alternative decoding procedures (Goodman,
1996). Well-motivated hidden variable training
procedures (such as EM and conditional EM) are
also readily available for probabilistic models. Fi-
nally, probability models can be chained together (as
in a noisy channel model), mixed, or combined in a
product-of-experts.
Sequence models, context-free models, and de-
pendency models have appeared in several guises;
a cross-model comparison clarifies the contribution
of this paper. First, there were generative, stochas-
tic models like HMMs, PCFGs, and Eisner?s (1996)
models. Local discriminative classifiers were pro-
posed by McCallum et al (2000) for sequence mod-
eling, by Ratnaparkhi et al (1994) for constituent
parsing, and by Hall et al (2006) (among others) for
132
dependencies. Large-margin whole-structure mod-
els were proposed for sequence labeling by Al-
tun et al (2003), for constituents by Taskar et al
(2004), and for dependency trees by McDonald et
al. (2005a). In this paper, we propose a model
most similar to the conditional random fields?
interpretable as log-linear models?of Lafferty et al
(2001), which are now widely used for sequence la-
beling. Log-linear models have been used in pars-
ing by Riezler et al (2000) (for constraint-based
grammars) and Johnson (2001) and Miyao and Tsu-
jii (2002) (for CFGs). Like McDonald et al, we use
an edge-factored model that permits nonprojective
trees; like Lafferty et al, we argue for an alternative
interpretation as a log-linear model over structures,
conditioned on the observed sentence.
In Section 2 we point out what would be required,
computationally, for conditional training of nonpro-
jective dependency models. The solution to the con-
ditionalization problem is given in Section 3, using a
widely-known but newly-applied Matrix Tree Theo-
rem due to Tutte (1984), and experimental results are
presented with a comparison to the MIRA learning
algorithm used by McDonald et al (2005a). We go
on to describe and experiment with two useful appli-
cations of conditional modeling: minimum Bayes-
risk decoding (Section 4) and hidden-variable train-
ing by conditional maximum likelihood estimation
(Section 5). Discussion in Section 6 considers the
implications of our experimental results.
Two indepedent papers, published concurrently
with this one, report closely related results to ours.
Koo et al (2007) and McDonald and Satta (2007)
both describe how the Matrix Tree Theorem can be
applied to computing the sum of scores of edge-
factored dependency trees and the edge marginals.
Koo et al compare conditional likelihood training
(as here) to the averaged perceptron and a max-
imum margin model trained using exponentiated-
gradient (Bartlett et al, 2004); the latter requires
the same marginalization calculations as conditional
log-linear estimation. McDonald and Satta discuss a
variety of applications (including minimum Bayes-
risk decoding) and give complexity results for non-
edge-factored models. Interested readers are re-
ferred to those papers for further discussion.
2 Conditional Training for Nonprojective
Dependency Models
Let x = ?x1, ..., xn? be a sequence of words (possi-
bly with POS tags, lemmas, and morphological in-
formation) that are the input to a parser. y will refer
to a directed, unlabeled dependency tree, which is a
map y : {1, ..., n} ? {0, ..., n} from child indices
to parent indices; x0 is the invisible ?wall? symbol.
Let Yx be the set of valid dependency trees for x. In
this paper, Yx is equivalent to the set of all directed
spanning trees over x.1
A conditional model defines a family of probabil-
ity distributions p(y | x), for all x and y ? Yx. We
propose that this model take a log-linear form:
p~?(y | x) =
e~??~f(x,y)
?
y??Yx
e
~??~f(x,y?)
=
e~??~f(x,y)
Z~?(x)
(1)
where ~f is a feature vector function on parsed sen-
tences and ~? ? Rm parameterizes the model. Fol-
lowing McDonald et al (2005a), we assume that the
features are edge-factored:
~f(x,y) =
n?
i=1
~f(x, xi, xy(i)) (2)
In other words, the dependencies between words in
the tree are all conditionally independent of each
other, given the sequence x and the fact that the
parse is a spanning tree. Despite the constraints they
impose on features, edge-factored models have the
advantage of tractable O(n3) inference algorithms
or, with some trickery, O(n2) maximum a posteriori
(?best parse tree?) inference algorithms in the non-
projective case. Exact nonprojective inference and
estimation become intractable if we break edge fac-
toring (McDonald and Pereira, 2006).
We wish to estimate the parameters ~? by maxi-
mizing the conditional likelihood (like a CRF) rather
1To be precise, every word has in-degree 1, with the sole
edge pointing from the word?s parent, xy(i) ? xi. x0 has in-
degree 0. By definition, trees are acyclic. The edges need not
be planar and may ?cross? in the plane, since we do not have a
projectivity constraint. In some formulations, exactly one node
in x can attach to x0; here we allow multiple nodes to attach
to x0, since this occurs with some frequency in many existing
datasets. Summation over trees where x0 has exactly one child
is addressed directly by Koo et al (2007).
133
than the margin (McDonald et al, 2005a). For an
empirical distribution p? given by a set of training ex-
amples, this means:
max
~?
?
x,y
p?(x,y)
(
~? ? ~f(x,y)
)
?
?
x
p?(x) logZ~?(x)
(3)
This optimization problem is typically solved us-
ing a quasi-Newton numerical optimization method
such as L-BFGS (Liu and Nocedal, 1989). Such a
method requires the gradient of the objective func-
tion, which for ?k is given by the following differ-
ence in expectations of the value of feature fk:
?
??k
= (4)
Ep?(X,Y) [fk(X,Y)] ?Ep?(X)p~?(Y|X) [fk(X,Y)]
The computation of Z~?(x) and the sufficient
statistics (second expectation in Equation 4) are typ-
ically the difficult parts. They require summing the
scores of all the spanning trees for a given sentence.
Note that, in large-margin training, and in standard
maximum a posteriori decoding, only a maximum
over spanning trees is called for?it is conditional
training that requires Z~?(x). In Section 3, we will
show how this can be done exactly in O(n3) time.
3 Exploiting the Matrix Tree Theorem for
Z~?(x)
We wish to apply conditional training to estimate
conditional models of nonprojective trees. This re-
quires computing Z~?(x) for each training example
(as an inner loop to training). In this section we show
how the summation can be computed and how con-
ditional training performs.
3.1 Kirchoff Matrix
Recall that we defined the unnormalized probability
(henceforth, score) of a dependency tree as a combi-
nation of edge-factored scores for the edges present
in the tree (Eq. 2):
exp ~??~f(x,y) =
n?
i=1
e
~??~f(x,xi,xy(i)) =
n?
i=1
sx,~?(i,y(i))
(5)
where y(i) denotes the parent of xi in y. sx,~?(i, j),
then, denotes the (multiplicative) contribution of the
edge from child i to parent j to the total score of
the tree, if the edge is present. Define the Kirchoff
matrixKx,~? ? R
n?n by
[
Kx,~?
]
mom,kid
= (6)
?
?
?
?sx,~?(kid ,mom) if mom 6= kid?
j?{0,...n}:j 6=mom
sx,~?(kid , j) if mom = kid .
where mom indexes a parent node and kid a child
node.
Kx ~? can be regarded as a special weighted adja-
cency matrix in which the ith diagonal entry is the
sum of edge-scores directed into vertex i (i.e., xi is
the child)?note that the sum includes the score of
attaching xi to the wall x0.
In our notation and in one specific form, the Ma-
trix Tree Theorem (Tutte, 1984) states:2
Theorem 1 The determinant of the Kirchoff matrix
Kx,~? is equal to the sum of scores of all directed
spanning trees in Yx rooted at x0. Formally:
?
?
?Kx,~?
?
?
? = Z~?(x).
A proof is omitted; see Tutte (1984).
To compute Z~?(x), we need only take the deter-
minant of Kx,~?, which can be done in O(n
3) time
using the standard LU factorization to compute the
matrix inverse. Since all of the edge weights used
to construct the Kirchoff matrix are positive, it is di-
agonally dominant and therefore non-singular (i.e.,
invertible).
3.2 Gradient
The gradient of Z~?(x) (required for numerical opti-
mization; see Eqs. 3?4) can be efficiently computed
from the same matrix inverse. While ? logZ~?(x)
equates to a vector of feature expectations (Eq. 4),
we exploit instead some facts from linear algebra
2There are proven generalizations of this theorem (Chen,
1965; Chaiken, 1982; Minoux, 1999); we give the most specific
form that applies to our case, originally proved by Tutte in 1948.
Strictly speaking, ourKx,~? is not the Kirchoff matrix, but rather
a submatrix of the Kirchoff matrix with a leftmost column of
zeroes and a topmost row [0,?sx,~?(1, 0), ...,?sx,~?(n, 0)] re-
moved. Farther afield, Jaakkola et al (1999) used an undirected
matrix tree theorem for learning tree structures for graphical
models.
134
Kx,~? =
?
?
?
?
?
?
?
?
?
?
?
?
?
j?{0,...,n}:j 6=1
sx,~?(1, j) ?sx,~?(2, 1) ? ? ? ?sx,~?(n, 1)
?sx,~?(1, 2)
?
j?{0,...,n}:j 6=2
sx,~?(2, j) ? ? ? ?sx,~?(n, 2)
...
...
. . .
...
?sx,~?(1, n) ?sx,~?(2, n) ? ? ?
?
j?{0,...,n}:j 6=n
sx,~?(n, j)
?
?
?
?
?
?
?
?
?
?
?
?
and the chain rule. First, note that, for any weight
?k,
? logZ~?(x)
??k
=
? log |Kx,~?|
??k
=
1
|Kx,~?|
?|Kx,~?|
??k
=
1
|Kx,~?|
n?
i=1
n?
j=0
?|Kx,~?|
?sx,~?(i, j)
?sx,~?(i, j)
??k
=
1
|Kx,~?|
n?
i=1
n?
j=0
sx,~?(i, j)fk(x, xi, xj)
?
?|Kx,~?|
?sx,~?(i, j)
(7)
(We assume sx,~?(i, i) = 0, for simplicity of nota-
tion.) The last line follows from the definition of
sx,~?(i, j) as exp
~?? ~f(x, xi, xj). Now, since sx,~?(i, j)
affects the Kirchoff matrix in at most two cells?
(i, i) and (j, i), the latter only when j > 0?we
know that
?|Kx,~?|
?sx,~?(i, j)
=
?|Kx,~?|
?[Kx,~?]i,i
?[Kx,~?]i,i
?sx,~?(i, i)
?
?|Kx,~?|
?[Kx,~?]j,i
?[Kx,~?]j,i
?sx,~?(i, j)
=
?|Kx,~?|
?[Kx,~?]i,i
?
?|Kx,~?|
?[Kx,~?]j,i
(8)
We have now reduced the problem of the gradient
to a linear function of ?|Kx,~?| with respect to the
cells of the matrix itself. At this point, we simplify
notation and consider an arbitrary matrixA.
The minor mj,i of a matrix A is the determi-
nant of the submatrix obtained by striking out row
j and column i of A; the cofactor cj,i of A is then
(?1)i+jmj,i. Laplace?s formula defines the deter-
minant as a linear combination of matrix cofactors
of an arbitrary row j:
|A| =
n?
i=1
[A]j,icj,i (9)
It should be clear that any cj,k is constant with re-
spect to the cell [A]j,i (since it is formed by remov-
ing row j of A) and that other entries of A are con-
stant with respect to the cell [A]j,i. Therefore:
?|A|
?[A]j,i
= cj,i (10)
The inverse matrixA?1 can also be defined in terms
of cofactors:
[A?1]i,j =
cj,i
|A|
(11)
Combining Eqs. 10 and 11, we have:
?|A|
?[A]j,i
= |A|[A?1]i,j (12)
Plugging back in through Eq. 8 to Eq. 7, we have:
? logZ~?(x)
??k
=
n?
i=1
n?
j=0
sx,~?(i, j)fk(x, xi, xj)
?
([
K?1
x,~?
]
i,i
?
[
K?1
x,~?
]
i,j
)
(13)
where [K?1]i,0 is taken to be 0. Note that the cofac-
tors do not need to be computed directly. We pro-
posed in Section 3.1 to get Z~?(x) by computing the
inverse of the Kirchoff matrix (which is known to
exist). Under that procedure, the marginalization is
a by-product of the gradient.
135
decode train Arabic Czech Danish Dutch
map MIRA 79.9 81.4 86.6 90.0
CE 80.4 80.2 87.5 90.0 (Section 3)
mBr MIRA 79.4 80.3 85.0 87.2 (Section 4)
CE 80.5 80.4 87.5 90.0 (Sections 3 & 4)
Table 1: Unlabeled dependency parsing accuracy (on test data) for two training methods (MIRA, as in
McDonald et al (2005b), and conditional estimation) and with maximum a posteriori (map) and minimum
Bayes-risk (mBr) decoding. Boldface scores are best in their column on a permutation test at the .05 level.
3.3 Experiment
We compare conditional training of a nonprojective
edge-factored parsing model to the online MIRA
training used by McDonald et al (2005b). Four lan-
guages with relatively common nonprojective phe-
nomena were tested: Arabic (Hajic? et al, 2004),
Czech (Bo?hmova? et al, 2003), Danish (Kromann,
2003), and Dutch (van der Beek et al, 2002). The
Danish and Dutch datasets were prepared for the
CoNLL 2006 shared task (Buchholz and Marsi,
2006); Arabic and Czech are from the 2007 shared
task. We used the same features, extracted by Mc-
Donald?s code, in both MIRA and conditional train-
ing. In this paper, we consider only unlabeled de-
pendency parsing.
Our conditional training used an online gradient-
based method known as stochastic gradient descent
(see, e.g., Bottou, 2003). Training with MIRA and
conditional estimation take about the same amount
of time: approximately 50 sentences per second.
Training proceeded as long as an improvement on
held-out data was evident. The accuracy of the hy-
pothesized parses for the two models, on each lan-
guage, are shown in the top two rows of Tab. 1 (la-
beled ?map? for maximum a posteriori, meaning
that the highest-weighted tree is hypothesized).
The two methods are, not surprisingly, close in
performance; conditional likelihood outperformed
MIRA on Arabic and Danish, underperformed
MIRA on Czech, and the two tied on Dutch. Results
are significant at the .05 level on a permutation test.
Conditional estimation is in practice more prone to
over-fitting than maximum margin methods, though
we did not see any improvement using zero-mean
Gaussian priors (variance 1 or 10).
These experiments serve to validate conditional
estimation as a competitive learning algorithm for
parsing models, and the key contribution of the sum-
ming algorithm that permits conditional estimation.
4 Minimum Bayes-Risk Decoding
A second application of probability distributions
over trees is the alternative decoding algorithm
known as minimum Bayes-risk (mBr) decoding.
The more commonly used maximum a posteriori
decoding (also known as ?Viterbi? decoding) that
we applied in Section 3.3 sought to minimize the ex-
pected whole-tree loss:
y? = argmax
y
p~?(y | x) = argmin
y
Ep~?(Y|x) [??(y,Y)]
(14)
Minimum Bayes-risk decoding generalizes this idea
to an arbitrary loss function ` on the proposed tree:
y? = argmin
y
Ep~?(Y|x) [`(y,Y)] (15)
This technique was originally applied in speech
recognition (Goel and Byrne, 2000) and translation
(Kumar and Byrne, 2004); Goodman (1996) pro-
posed a similar idea in probabilistic context-free
parsing, seeking to maximize expected recall. For
more applications in parsing, see Petrov and Klein
(2007).
The most common loss function used to evaluate
dependency parsers is the number of attachment er-
rors, so we seek to decode using:
y? = argmin
y
Ep~?(Y|x)
[
n?
i=1
??(y(i),Y(i))
]
= argmax
y
n?
i=1
p~?(Y(i) = y(i) | x) (16)
To apply this decoding method, we make use of
Eq. 13, which gives us the posterior probabilities
136
of edges under the model, and the same Chiu-
Liu-Edmonds maximum directed spanning tree al-
gorithm used for maximum a posteriori decoding.
Note that this decoding method can be applied re-
gardless of how the model is trained. It merely re-
quires assuming that the tree scores under the trained
model (probabilistic or not) can be treated as unnor-
malized log-probabilities over trees given the sen-
tence x.
We applied minimum Bayes-risk decoding to the
models trained using MIRA and using conditional
estimation (see Section 3.3). Table 1 shows that,
across languages, minimum Bayes-risk decoding
hurts slightly the performance of a MIRA-trained
model, but helps slightly or does not affect the per-
formance of a conditionally-trained model. Since
MIRA does not attempt to model the distribution
over trees, this result is not surprising; interpreting
weights as defining a conditional log-linear distribu-
tion is questionable under MIRA?s training criterion.
One option, which we do not test here, is to
use minimum Bayes-risk decoding inside of MIRA
training, to propose a hypothesis tree (or k-best
trees) at each training step. Doing this would more
closely match the training conditions with the test-
ing conditions; however, it is unclear whether there
is a formal interpretation of such a combination, for
example its relationship to McDonald et al?s ?fac-
tored MIRA.?
Minimum Bayes-risk decoding, we believe, will
become important in nonprojective parsing with
non-edge-factored models. Note that minimium
Bayes-risk decoding reduces any parsing problem to
the maximum directed spanning tree problem, even
if the original model is not edge-factored. All that
is required are the marginals p~?(Y(i) = y(i) | x),
which may be intractable to compute exactly, though
it may be possible to develop efficient approxima-
tions.
5 Hidden Variables
A third application of probability distributions over
trees is hidden-variable learning. The Expectation-
Maximization (EM) algorithm (Baum and Petrie,
1966; Dempster et al, 1977; Baker, 1979), for
example, is a way to maximum the likelihood of
training data, marginalizing out hidden variables.
This has been applied widely in unsupervised pars-
ing (Carroll and Charniak, 1992; Klein and Man-
ning, 2002). More recently, EM has been used to
learn hidden variables in parse trees; these can be
head-child annotations (Chiang and Bikel, 2002), la-
tent head features (Matsuzaki et al, 2005; Prescher,
2005; Dreyer and Eisner, 2006), or hierarchically-
split nonterminal states (Petrov et al, 2006).
To date, we know of no attempts to apply hid-
den variables to supervised dependency tree mod-
els. If the trees are constrained to be projective, EM
is easily applied using the inside-outside variant of
the parsing algorithm described by Eisner (1996) to
compute the marginal probability. Moving to the
nonprojective case, there are two difficulties: (a) we
must marginalize over nonprojective trees and (b)
we must define a generative model over (X,Y).
We have already shown in Section 3 how to solve
(a); here we avoid (b) by maximizing conditional
likelihood, marginalizing out the hidden variable,
denoted z:
max
~?
?
x,y
p?(x,y) log
?
z
p~?(y, z | x) (17)
This sort of conditional training with hidden vari-
ables was carried out by Koo and Collins (2005),
for example, in reranking; it is related to the infor-
mation bottleneck method (Tishby et al, 1999) and
contrastive estimation (Smith and Eisner, 2005).
5.1 Latent Dependency Labels
Noting that our model is edge-factored (Eq. 2), we
define our hidden variables to be edge-factored as
well. We can think of the hidden variables as clusters
on dependency tokens, and redefine the score of an
edge to be:
sx,~?(i, j) =
?
z?Z
e
~??~f(x,xi,xj ,z) (18)
where Z is a set of dependency clusters.
Note that keeping the model edge-factored means
that the cluster of each dependency in a tree is con-
ditionally independent of all the others, given the
words. This is computationally advantageous (we
can factor out the marginalization of the hidden vari-
able by edge), and it permits the use of any cluster-
ing method at all. For example, if an auxiliary clus-
tering model q(z | x,y)?perhaps one that did not
137
make such independence assumptions?were used,
the posterior probability q(Zi = z | x,y) could
be a feature in the proposed model. On the other
hand, we must consider carefully the role of the
dependency clusters in the model; if clusters are
learned extrinsic to estimation of the parsing model,
we should not expect them to be directly advanta-
geous to parsing accuracy.
5.2 Experiments
We tried two sets of experiments with clustering. In
one case, we simply augmented all of McDonald
et al?s edge features with a cluster label in hopes
of improved accuracy. Models were initialized near
zero, with Gaussian noise added to break symmetry
among clusters.
Under these conditions, performance stayed the
same or changed slightly (see Table 2); none of the
improvements are significant. Note that three de-
coders were applied: maximum a posteriori (map)
and minimum Bayes-risk (mBr) as described in Sec-
tion 4, and ?max-z,? in which each possible edge
was labeled and weighted only with its most likely
cluster (rather than the sum over all clusters), before
finding the most probable tree.3 For each of the three
languages tested, some number of clusters and some
decoding method gave small improvements over the
baseline.
More ambitiously, we hypothesized that many
lexicalized features on edges could be ?squeezed?
through clusters to reduce the size of the feature set.
We thus removed all word-word and lemma-lemma
features and all tag fourgrams. Although this re-
duced our feature set by a factor of 60 or more (prior
to taking a cross-product with the clusters), the dam-
age of breaking the features was tremendous, and
performance even with a thousand clusters barely
broke 25% accuracy.
6 Discussion
Noting that adding latent features to nonterminals
in unlexicalized context-free parsing has been very
successful (Chiang and Bikel, 2002; Matsuzaki et
al., 2005; Prescher, 2005; Dreyer and Eisner, 2006;
Petrov et al, 2006), we were surprised not to see a
3Czech experiments were not done, since the number of fea-
tures (more than 14 million) was too high to multiply out by
clusters.
# cl. decoding Arabic Danish Dutch
none map=max-z 80.4 87.5 90.0
mBr 80.5 87.5 90.0
2 map 80.4 87.5 89.5
mBr 80.6 87.3 89.7
max-z 80.4 86.3 89.4
16 map 80.4 87.6 90.1
mBr 80.4 87.6 90.1
max-z 80.4 87.6 90.2
32 map 80.0 87.6 ?
mBr 80.4 87.5 ?
max-z 80.0 87.5 ?
Table 2: Augmenting edge features with clusters re-
sults in similar performance to conditional training
with no clusters (top two lines). Scores are unla-
beled dependency accuracy on test data.
more substantial performance improvement through
latent features. We propose several interpretations.
First, it may simply be that many more clusters may
be required. Note that the label-set sizes for the la-
beled versions of these datasets are larger than 32
(e.g., 50 for Danish). This has the unfortunate effect
of blowing up the feature space beyond the mem-
ory capacity of our machines (hence our attempts
at squeezing high-dimensional features through the
clusters).
Of course, improved clustering methods may
also improve performance. In particular, a cluster-
learning algorithm that permits clusters to split
and/or merge, as in Petrov et al (2006) or in Pereira
et al (1993), may be appropriate.
Given the relative simplicity of clustering meth-
ods for context-free parsing to date (gains were
found just by using Expectation-Maximization), we
believe the fundamental reason clustering was not
particularly helpful here is a structural one. In
context-free parsing, the latent features are (in pub-
lished work to date) on nonterminal states, which are
the stuctural ?bridge? between context-free rules.
Adding features to those states is a way of pushing
information?encoded indirectly, perhaps?farther
around the tree, and therefore circumventing the
strict independence assumptions of probabilistic
CFGs.
In an edge-factored dependency model, on the
138
other hand, latent features on the edges seem to have
little effect. Given that they are locally ?summed
out? when we compute the scores of possible at-
tachments, it should be clear that the edge clusters
do not circumvent any independence assumptions.
Three options appear to present themselves. First,
we might attempt to learn clusters in tandem with
estimating a richer, non-edge-factored model which
would require approximations to Z~?(x), if condi-
tional training were to be used. Note that the approx-
imations to maximizing over spanning trees with
second-order features, proposed by McDonald and
Pereira (2006), do not permit estimating the clusters
as part of the same process as weight estimation (at
least not without modification). In the conditional
estimation case, a variational approach might be ap-
propriate. The second option is to learn clusters of-
fline, before estimating the parser. (We suggested
how to incorporate soft clusters into our model in
Section 5.1.) This option is computationally ad-
vantageous but loses sight of the aim of learning
the clusters specifically to improve parsing accuracy.
Third, noting that the structural ?bridge? between
two coincident edges is the shared vertex (word), we
might consider word token clustering.
We also believe this structural locality issue helps
explain the modesty of the gains using minimum
Bayes-risk decoding with conditional training (Sec-
tion 4). In other dependency parsing scenarios, min-
imum Bayes-risk decoding has been found to offer
significant advantages?why not here? Minimum
Bayes-risk makes use of global statistical dependen-
cies in the posterior when making local decisions.
But in an edge-factored model, the edges are all con-
ditionally independent, given that y is a spanning
tree.
As a post hoc experiment, we compared
purely greedy attachment (attach each word to its
maximum-weighted parent, without any tree con-
straints). Edge scores as defined in the model were
compared to minimum Bayes-risk posterior scores,
and the latter were consistently better (though this
always under-performed optimal spanning-tree de-
coding, unsurprisingly). This comparison serves
only to confirm that minimum Bayes-risk decoding
is a way to circumvent independence assumptions
(here made by a decoder), but only when the trained
model does not make those particular assumptions.
7 Conclusion
We have shown how to carry out exact marginaliza-
tion under an edge-factored, conditional log-linear
model over nonprojective dependency trees. The
method has cubic runtime in the length of the se-
quence, but is very fast in practice. It can be used
in conditional training of such a model, in minimum
Bayes-risk decoding (regardless of how the model is
trained), and in training with hidden variables. We
demonstrated how each of these techniques gives re-
sults competitive with state-of-the-art existing de-
pendency parsers.
Acknowledgments
The authors thank the anonymous reviewers, Jason
Eisner, Keith Hall, and Sanjeev Khudanpur for help-
ful comments, and Michael Collins and Ryan Mc-
Donald for sharing drafts of their related, concurrent
papers. This work was supported in part by NSF
ITR grant IIS-0313193.
References
Y. Altun, M. Johnson, and T. Hofmann. 2003. Inves-
tigating loss functions and optimization methods for
discriminative learning of label sequences. In Proc. of
EMNLP.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Proc. of the Acoustical Society of America,
pages 547?550.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester.
2004. Exponentiated gradient algorithms for large-
margin structured classification. In Advances in NIPS
17.
L. E. Baum and T. Petrie. 1966. Statistical inference for
probabilistic functions of finite state Markov chains.
Annals of Mathematical Statistics, 37:1554?1563.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The PDT: a 3-level annotation scenario.
In A. Abeille, editor, Building and Exploiting
Syntactically-Annotated Corpora. Kluwer.
L. Bottou. 2003. Stochastic learning. In Advanced Lec-
tures in Machine Learning, pages 146?168. Springer.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
S. Chaiken. 1982. A combinatorial proof of the all mi-
nors matrix tree theorem. SIAM Journal on Algebraic
and Discrete Methods, 3(3):319?329.
139
W.-K. Chen. 1965. Topological analysis for active
networks. IEEE Transactions on Circuit Theory,
12(1):85?91.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In Proc. of COLING.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. of ACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum
likelihood estimation from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society B,
39:1?38.
M. Dreyer and J. Eisner. 2006. Better informed training
of latent syntactic features. In Proc. of EMNLP.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
V. Goel and W. Byrne. 2000. Minimum Bayes risk auto-
matic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
J. Hajic?, O. Smrz?, P. Zema?nek J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic Dependency Treebank: Devel-
opment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
learning for data-driven dependency parsing. In Proc.
of COLING-ACL.
T. Jaakkola, M. Meila, and T. Jebara. 1999. Maximum
entropy discrimination. In Advances in NIPS 12.
M. Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. of ACL.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of ACL.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. of EMNLP.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the Matrix-Tree The-
orem. In Proc. of EMNLP-CoNLL.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of TLT.
S. Kumar and W. Byrne. 2004. Minimum Bayes risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proc. of ACL.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proc. of IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of HLT-EMNLP.
M. Minoux. 1999. A generalization of the all minors ma-
trix tree theorem to semirings. Discrete Mathematics,
199:139?150.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estima-
tion for feature forests. In Proc. of HLT.
F. C. N. Pereira, N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of English words. In Proc. of the 31st
ACL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of COLING-ACL.
D. Prescher. 2005. Head-driven PCFGs with latent-head
statistics. In Proc. of IWPT.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A
maximum entropy model for parsing. In Proc. of IC-
SLP.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000.
Lexicalized stochastic modeling of constraint-based
grammars using log-linear measures and EM training.
In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
N. Tishby, F. C. N. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the 37th
Allerton Conference on Communication, Control and
Computing, pages 368?377.
W. T. Tutte. 1984. Graph Theory. Addison-Wesley.
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In
CLIN.
140
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 667?677, Prague, June 2007. c?2007 Association for Computational Linguistics
Bootstrapping Feature-Rich Dependency Parsers with Entropic Priors
David A. Smith and Jason Eisner
Department of Computer Science
Johns Hopkins University
Balitmore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
One may need to build a statistical parser for a new language,
using only a very small labeled treebank together with raw
text. We argue that bootstrapping a parser is most promising
when the model uses a rich set of redundant features, as in re-
cent models for scoring dependency parses (McDonald et al,
2005). Drawing on Abney?s (2004) analysis of the Yarowsky
algorithm, we perform bootstrapping by entropy regulariza-
tion: we maximize a linear combination of conditional likeli-
hood on labeled data and confidence (negative Re?nyi entropy)
on unlabeled data. In initial experiments, this surpassed EM
for training a simple feature-poor generative model, and also
improved the performance of a feature-rich, conditionally esti-
mated model where EM could not easily have been applied. For
our models and training sets, more peaked measures of con-
fidence, measured by Re?nyi entropy, outperformed smoother
ones. We discuss how our feature set could be extended with
cross-lingual or cross-domain features, to incorporate knowl-
edge from parallel or comparable corpora during bootstrapping.
1 Motivation
In this paper, we address the problem of bootstrap-
ping new statistical parsers for new languages, gen-
res, or domains.
Why is this problem important? Many applica-
tions of multilingual NLP require parsing in order
to extract information, opinions, and answers from
text, and to produce improved translations. Yet
an adequate labeled training corpus?a large tree-
bank of manually constructed parse trees of typi-
cal sentences?is rarely available and would be pro-
hibitively expensive to develop.
We show how it is possible to train instead from
a small hand-labeled treebank in the target domain,
together with a large unannotated collection of in-
domain sentences. Additional resources such as
parsers for other domains or languages can be in-
tegrated naturally.
Dependency parsing is important as a key com-
ponent in leading systems for information extrac-
tion (Weischedel, 2004)1 and question answering
(Peng et al, 2005). These systems rely on edges
or paths in dependency parse trees to define their ex-
traction patterns and classification features. Parsing
is also key to the latest advances in machine transla-
tion, which translate syntactic phrases (Galley et al,
2006; Marcu et al, 2006; Cowan et al, 2006).
2 Our Approach
Our approach rests on three observations:
? Recent ?feature-based? parsing models are an
excellent fit for bootstrapping, because the
parse is often overdetermined by many redun-
dant features.
? The feature-based framework is flexible
enough to incorporate other sources of guid-
ance during training or testing?such as the
knowledge contained in a parser for another
language or domain.
? Maximizing a combination of likelihood on la-
beled data and confidence on unlabeled data is
a principled approach to bootstrapping.
2.1 Feature-Based Parsing
McDonald et al (2005) introduced a simple, flexi-
ble framework for scoring dependency parses. Each
directed edge e in the dependency tree is described
with a high-dimensional feature vector f(e). The
edge?s score is the dot product f(e) ? ?, where ? is a
learned weight vector. The overall score of a depen-
dency tree is the sum of the scores of all edges in the
tree.
1Ralph Weischedel (p.c.) reports that this system?s perfor-
mance degrades considerably when only phrase chunking is
available rather than full parsing.
667
Given an n-word input sentence, the parser begins
by scoring each of the O(n2) possible edges, and
then seeks the highest-scoring legal dependency tree
formed by any n? 1 of these edges, using an O(n3)
dynamic programming algorithm (Eisner, 1996) for
projective trees. For non-projective parsing, O(n3),
or with some trickery O(n2), greedy algorithms ex-
ist (Chu and Liu, 1965; Edmonds, 1967; Gabow et
al., 1986).
The feature function f may pay attention to many
properties of the directed edge e. Of course, features
may consider the parent and child words connected
by e, and their parts of speech.2 But some features
used by McDonald et al (2005) also consider the
parts of speech of words adjacent to the parent and
child, or between the parent and child, as well as the
number of words between the parent and child. In
general, these features are not available in a genera-
tive model such as a PCFG.
Although feature-based models are often trained
purely discriminatively, we will see in ?2.6 how to
train them to model conditional probabilities.
2.2 Feature-Based Parsing and Bootstrapping
The above parsing model is robust, thanks to its
many features. On the Penn Treebank WSJ sections
02?21, for example, McDonald?s parser extracts 5.5
million feature types from supervised edges alone,
with about 120 feature tokens firing per edge. The
highest-scoring parse tree represents a consensus
among all features on all prospective edges. Even if
a prospective edge has some discouraging features
(i.e., with negative or zero weights), it may still have
a relatively high score thanks to its other features.
Furthermore, even if the edge has a low total score,
it may still appear in the consensus parse if the al-
ternatives are even worse or are incompatible with
other high-scoring edges.
Put another way, the parser is not able to include
high-scoring features or edges independently of one
another. Selecting a good feature means accepting
all other features on that edge. It also means reject-
ing various other edges, because of the global con-
straints that a legal parse tree must give each word
only one parent and must be free of cycles and, in
2Note that since we are not trying to predict parts of speech,
we treat the output of one or more automatic taggers as yet more
inputs to edge feature functions.
the projective case, crossings.
Our observation is that this situation is ideal for
so-called ?bootstrapping,? ?co-training,? or ?min-
imally supervised? learning methods (Yarowsky,
1995; Blum and Mitchell, 1998; Yarowsky and Wi-
centowski, 2000). Such methods should thrive when
the right answer is overdetermined owing to redun-
dant features and/or global constraints.
Concretely, suppose we start by training a super-
vised parser on only 100 examples, using some reg-
ularization method to prevent overfitting to this set.
While many features might truly be relevant to the
task, only a few appear often enough in this small
training set to acquire significantly positive or nega-
tive weights.
Even this lightly trained parser may be quite sure
of itself on some test sentences in a large unanno-
tated corpus, when one parse scores far higher than
all others. More generally, the parser may be sure
about part of a sentence: it may be certain that a par-
ticular edge is present (or absent), because that edge
tends to be present (or absent) in all high-scoring
parses.
Retraining the feature weights ? on these high-
confidence edges can learn about additional features
that are correlated with an edge?s success or failure.
For example, it may now learn strong weights for
lexically specific features that were never observed
in the supervised training set. The retrained parser
may now be able to confidently parse even more of
the unannotated examples; so we can iterate the pro-
cess.
Our hope is that the model identifies new good
and bad edges at each step, and does so correctly.
The more features and global constraints the model
has,
? the more power it will have to discriminate
among edges even when ? is insufficiently
trained. (Some feature weights may be too
weak (i.e., too close to zero) because the initial
labeled set is small.)
? the more robust it will be against errors even
when ? is incorrectly trained. (Some feature
weights may be too strong or have the wrong
sign, because of overfitting or mistaken parses
during bootstrapping.)
668
In the former case, strong features lend their strength
to weak ones. In the latter case, a conflict among
strong features weakens the ones that depart from
the consensus, or discounts the example sentence if
there is no consensus.
Previous work on parser bootstrapping has not
been able to exploit this redundancy among features,
because it has used PCFG-like models with far fewer
features (Steedman et al, 2003).
2.3 Adaptation and Projection via Features
The previous section assumed that we had a small
supervised treebank in the target language and do-
main (plus a large unsupervised corpus). We now
consider other, more dubious, knowledge sources
that might supplement or replace this small tree-
bank. In each case, we can use these knowledge
sources to derive features that may?or may not?
prove trustworthy during bootstrapping.
Parses from a different domain. One might have
a treebank for a different domain or genre of the tar-
get language.
One could simply include these trees in the ini-
tial supervised training, and hope that bootstrapping
corrects any learned weights that are inappropriate
to the target domain, as discussed above. In fact,
McClosky et al (2006) found a similar technique to
be effective?though only in a model with a large
feature space (?PCFG + reranking?), as we would
predict.
However, another approach is to train a separate
out-of-domain parser, and use this to generate addi-
tional features on the supervised and unsupervised
in-domain data (Blitzer et al, 2006). Bootstrapping
now teaches us where to trust the out-of-domain
parser. If our basic model has 100 features, we could
add features 101 through 200, where for example
f123(e) = f23 ? log P?r(e) and P?r(e) is the poste-
rior edge probability according to the out-of-domain
parser. Learning that this feature has a high weight
means learning to trust the out-of-domain parser?s
decision on edges where in-domain feature 23 fires.
Even more sensibly, we could add features such as
f201(e) =
?10
i=1 f?i(e) ? ??i, where f? and ?? are the fea-
ture and weight vectors for the out-of-domain parser.
Learning that this feature has a high weight means
learning to trust the out-of-domain parser?s feature
weights for a particular class of features (those num-
bered 1 through 10). This addresses the intuition that
some linguistic phenomena remain stable across do-
mains.
Parses of translations. Suppose we have transla-
tions into English of some of our supervised or unsu-
pervised sentences. Good probabilistic dependency
parsers already exist for English, so we run one over
the English translation. We can now derive many
additional features on candidate edges on the tar-
get sentence. For example, dependency edges in the
target language of the form c
poss
?? p (this denotes
a child-to-parent dependency with label possessor)
might often correspond to dependency paths in the
English translation of the form p?
prep
?? of
pobj
?? c?. To
discover whether this is so, we define a feature i by
fi(c
poss
?? p) def= log
?
c?,p?
(Pr(c aligns with c?)
?Pr(p aligns with p?)
?Pr(p?
prep
?? of
pobj
?? c?))
(1)
where c?, p? range over word tokens in the English
translation, ?of? is a literal English word, and the
probabilities are posteriors provided by a probabilis-
tic aligner and a probabilistic English parser. Note
that this is a single feature (not a feature family pa-
rameterized by c, p). It scores any candidate edge on
whether it is a
poss
?? edge that seems to align to an
English
prep
?? of
pobj
?? path.
This method is inspired by Hwa et al (2005),
who bootstrapped parsers for Spanish and Chinese
by projecting dependencies from English transla-
tions and training a new parser on the resulting noisy
treebank. They used only 1-best translations, 1-best
alignments, dependency paths of length 1, and no
labeled data in Spanish or Chinese.
Hwa et al (2005) used a manually written post-
processor to correct some of the many incorrect pro-
jections. By contrast, our framework uses the pro-
jected dependencies only as one source of features.
They may be overridden by other features in particu-
lar cases, and will be given a high weight only if they
tend to agree with other features during bootstrap-
ping. A similar soft projection of dependencies was
used in supervised machine translation by Smith and
Eisner (2006), who used a source sentence?s depen-
dency paths to bias the generation of its translation.
669
Note that these bilingual features will only fire
on those supervised or unsupervised sentences for
which we have an English translation. In particu-
lar, they will usually be unavailable on the test set.
However, we hope that they will seed and facilitate
the bootstrapping process, by helping us confidently
parse some unsupervised sentences that we would
not be able to confidently parse without an English
translation.
Parses of comparable English sentences. World
knowledge can be useful in parsing. Suppose
you see a French sentence that contains mangeons
and pommes, and you know that manger=eat and
pomme=apple. You might reasonably guess that
pommes is the direct object of mangeons, because
you know that apple is a plausible direct object for
eat. We can discover this last bit of world knowl-
edge from comparable English text. Translation dic-
tionaries can themselves be induced from compara-
ble corpora (Schafer and Yarowsky, 2002; Schafer,
2006; Klementiev and Roth, 2006), or extracted
from bitext or digitized versions of human-readable
dictionaries if these are available.
The above inference pattern can be captured by
features similar to those in equation (1). For exam-
ple, one can define a feature j by
fi(c
poss
?? p) def= log Pr (p?
prep
?? of
pobj
?? c?
| p? translates p, c? translates c)
(2)
where each event in the event space is a pair (c?, p?)
of same-sentence tokens in comparable English text,
all pairs being equally likely. Thus, to estimate
Pr(? | ?), the denominator counts same-sentence
token pairs (c?, p?) in the comparable English cor-
pus that translate into the types (c, p), and the nu-
merator counts such pairs that are also related by
a
prep
?? of
pobj
?? path. Since the lexical transla-
tions and dependency paths are typically not labeled
in the English corpus, a given pair must be counted
fractionally according to its posterior probability of
satisfying these conditions, given models of contex-
tual translation and English parsing.3
3Similarly, Jansche (2005) imputes ?missing? trees by using
comparable corpora.
2.4 Bootstrapping as Optimization
Section 2.2 assumed a relatively conventional kind
of bootstrapping, where each iteration retrains the
model on the examples where it is currently most
confident. This kind of ?confidence thresholding?
has been popular in previous bootstrapping work (as
cited in ?2.2). It attempts to maintain high accu-
racy while gradually expanding coverage. The as-
sumption is that throughout the training procedure,
the parser?s confidence is a trustworthy guide to its
correctness. Different bootstrapping procedures use
different learners, smoothing methods, confidence
measures, and procedures for ?forgetting? the label-
ings from previous iterations.
In his analysis of Yarowsky (1995), Abney (2004)
formulates several variants of bootstrapping. These
are shown to increase either the likelihood of the
training data, or a lower bound on that likelihood. In
particular, Abney defines a function K that is an up-
per bound on the negative log-likelihood, and shows
his bootstrapping algorithms locally minimize K.
We now present a generalization of Abney?s K
function and relate it to another semi-supervised
learning technique, entropy regularization (Brand,
1999; Grandvalet and Bengio, 2005; Jiao et al,
2006). Our experiments will tune the feature weight
vector, ?, to minimize our function. We will do so
simply by applying a generic function minimization
method (stochastic gradient descent), rather than by
crafting a new Yarowsky-style or Abney-style itera-
tive procedure for our specific function.
Suppose we have examples xi and correspond-
ing possible labelings yi,k. We are trying to learn
a parametric model p?(yi,k | xi). If p?(yi,k | xi) is
a ?labeling distribution? that reflects our uncertainty
about the true labels, then our expected negative log-
likelihood of the model is
K def= ?
?
i
?
k
p?(yi,k | xi) log p?(yi,k | xi)
=
?
i
?
k
p?(yi,k|xi) log
p?(yi,k|xi)
p?(yi,k|xi)p?(yi,k|xi)
=
?
i
D(p?i?p?,i) + H(p?i) (3)
where p?i(?)
def= p?(? | xi) and p?,i(?)
def= p?(? | xi).
Note that K is a function not only of ? but also
670
of the labeling distribution p?; a learner might be al-
lowed to manipulate either in order to decrease K.
The summands of K in equation (3) can be di-
vided into two cases, according to whether xi is la-
beled or not. For the labeled examples {xi : i ? L},
the labeling distribution p?i is a point distribution that
assigns all probability to the true, known label y?i .
Then H(p?i) = 0. The total contribution of these ex-
amples to K simplifies to
?
i?L? log p?(y
?
i | xi),
i.e., just the negative log-likelihood on the labeled
data.
But what is the labeling distribution for the unla-
beled examples {xi : i 6? L}? Abney simply uses
a uniform distribution over labels (e.g., parses), to
reflect that the label is unknown. If his bootstrap-
ping algorithm ?labels? xi, then i moves into L and
H(p?i) is thereby reduced from maximal to 0. As a
result, a method that labels the most confident ex-
amples may reduce K, and Abney shows that his
method does so.
Our approach is different: we will take the label-
ing distribution p?i to be our actual current belief
p?,i, and manipulate it through changing ? rather
than L. L remains the original set of supervised ex-
amples. The total contribution of the unsupervised
examples to K then simplifies to
?
i6?L H(p?,i).
We have no reason to believe that these two con-
tributions (supervised and unsupervised) should be
weighted equally. We thus introduce a multiplier ?
to form the actual objective function that we mini-
mize with respect to ?:4
?
?
i?L
log p?,i(y
?
i ) + ?
N?
i6?L
H(p?,i) (4)
One may regard ? as a Lagrange multiplier that is
used to constrain the classifier?s uncertainty H to
be low, as presented in the work on entropy regular-
ization (Brand, 1999; Grandvalet and Bengio, 2005;
Jiao et al, 2006).
Conventional bootstrapping retrains on the most
confident unsupervised examples, making them
4This function is not necessarily convex in ?, because of the
addition of the entropy term (Jiao et al, 2006). One might try an
annealing strategy: start ? at zero (where the function is convex)
and gradually increase it, hoping to ?ride? the global maximum.
Although we could increase ? until the entropy term dominates
the minimizations and we approach a completely deterministic
classifier, it is preferable to use some labeled heldout data to
evaluate a stopping criterion.
more confident. Gradient descent on equation (4)
essentially does the same, since unsupervised exam-
ples contribute to (4) only through H , and the shape
of the H function means that it is most rapidly de-
creased by making the most confident unsupervised
examples more confident.
Besides favoring models that are self-confident on
the unlabeled data, the objective function (4) also ex-
plicitly asks the model to continue to get the correct
answers on the initial supervised corpus. 1/? con-
trols the strength of this request. One could obtain
a similar effect in conventional bootstrapping by up-
weighting the initial labeled corpus when retraining.
2.5 Online Learning
Minimizing equation (4) for parsing is more com-
putationally intensive than in many other applica-
tions of bootstrapping, such as word sense disam-
biguation or document classification. With millions
of features, our objective could take many iterations
to converge to a local optimum, if we were only to
update our parameter vector ? after each iteration
through a large unsupervised corpus.
For many machine learning problems over large
datasets, online learning methods such as stochas-
tic gradient descent (SGD) have been empirically
observed to converge in fewer iterations (Bottou,
2003). In SGD, instead of taking an optimiza-
tion step in the direction of the gradient calculated
over all unsupervised training examples, we parse
each example, calculate the gradient of the objective
function evaluated on that example alone, and then
take a small step downhill. The update rule is thus
?(t+1) ? ?(t) ? ? ? ?F (t)(?(t)) (5)
where ?(t) is the parameter vector at time t, F (t)(?)
is the objective function specialized to the time-t ex-
ample, and ? > 0 is a learning rate that we choose.
We check for convergence after each pass through
the example set.
2.6 Algorithms and Complexity
To evaluate equation (4), we need a conditional
model of trees given a sentence xi. We define one
by exponentiating and normalizing the tree scores:
p?,i(yi,k)
def= exp(
?
e?yi,k f(e) ? ?)/Zi.
With exponentially many parses of xi, does our
objective function (4) now have prohibitive com-
671
putational complexity? The complexity is actually
similar to that of the inside algorithm for parsing.
In fact, the first term of (4) for projective parsing
is found by running the O(n3) inside algorithm on
supervised data,5 and its gradient is found by the
corresponding O(n3) outside algorithm. For non-
projective parsing, the analogy to the inside algo-
rithm is the O(n3) ?matrix-tree algorithm,? which is
dominated asymptotically by a matrix determinant
(Smith and Smith, 2007; Koo et al, 2007; McDon-
ald and Satta, 2007). The gradient of a determinant
may be computed by matrix inversion, so evaluating
the gradient again has the same O(n3) complexity
as evaluating the function.
The second term of (4) is the Shannon entropy
of the posterior distribution over parses. Computing
this for projective parsing takes O(n3) time, using a
dynamic programming algorithm that is closely re-
lated to the inside algorithm (Hwa, 2000).6 For non-
projective parsing, unfortunately, the runtime rises
to O(n4), since it requires determinants of n distinct
matrices (each incorporating a log factor in a dif-
ferent column; we omit the details). The gradient
evaluation in both cases is again about as expensive
as the function evaluation.
A convenient speedup is to replace Shannon en-
tropy with Re?nyi entropy. The family of Re?nyi en-
tropy measures is parameterized by ?:
R?(p) =
1
1? ?
log
(
?
y
p(y)?
)
(6)
In our setting, where p = p?,i, the events y are the
possible parses yi,k of xi. Observe that under our
definition of p,
?
y p(y)
? = {
?
y exp[
?
e?y f(e) ?
(??)]}/Z?i . We already have Zi from running the
inside algorithm, and we can find the numerator by
running the inside algorithm again with ? scaled
by ?. Thus with Re?nyi entropy, all computations
and their gradients are O(n3)?even in the non-
projective case.
Re?nyi entropy is also a theoretically attractive
generalization. It can be shown that lim??1 R?(p)
5The numerator of p?,i(y?i ) (see definition above) is trivial
since y?i is a single known parse. But the denominator Zi is a
normalizing constant that sums over all parses; it is found by a
dependency-parsing variant of the inside algorithm, following
(Eisner, 1996).
6See also (Mann and McCallum, 2007) for similar results on
conditional random fields.
is in fact the Shannon entropy H(p) and that
lim???R?(p) = ? logmaxy p(y), i.e. the nega-
tive log probability of the modal or ?Viterbi? label
(Arndt, 2001; Karakos et al, 2007). The ? = 2
case, widely used as a measure of purity in decision
tree learning, is often called the ?Gini index.? Fi-
nally, when ? = 0, we get the log of the number
of labels, which equals the H(uniform distribution)
that Abney used in equation (3).
3 Evaluation
For this paper, we performed some initial bootstrap-
ping experiments on small corpora, using the fea-
tures from (McDonald et al, 2005). After discussing
experimental setup (?3.1), we look at the correlation
of confidence with accuracy and with oracle likeli-
hood, and at the fine-grained behaviour of models?
dependency edge posteriors (?3.2). We then com-
pare our confidence-maximizing bootstrapping to
EM, which has been widely used in semi-supervised
learning (?3.4). Section 3.3 presents overall boot-
strapping accuracy.
3.1 Experimental Design
We bootstrapped non-projective parsers for lan-
guages assembled for the CoNLL dependency pars-
ing competitions (Buchholz and Marsi, 2006). We
selected German, Spanish, and Czech (Brants et
al., 2002; Civit Torruella and Mart?? Anton??n, 2002;
Bo?hmova? et al, 2003). After removing sentences
more than 60 words long, we randomly divided each
corpus into small seed sets of 100 and 1000 trees;
development and test sets of 200 trees each; and an
unlabeled training set from the rest.
These treebanks contain strict dependency trees,
in the sense that their only nodes are the words and
a distinguished root node. In the Czech dataset,
more than one word can attach to the root; also, the
trees in German, Spanish, and Czech may be non-
projective. We use the MSTParser implementa-
tion described in McDonald et al (2005) for fea-
ture extraction. Since our seed sets are so small, we
extracted features from all edges in both the seed
and the unlabeled parts of our training data, not just
the edges annotated as correct. Since this produced
many more features, we pruned our features to those
with at least 10 occurrences over all edges.
672
Correlation of
100-tree model 1000-tree model
Re?nyi ? Acc. Xent. Acc. Xent.
(uniform, Abney) 0 -0.254 0.980 -0.180 0.937
.5 -0.256 0.981 -0.203 0.955
(Shannon) 1 -0.260 0.983 -0.220 0.964
(Gini) 2 -0.266 0.985 -0.250 0.977
5 -0.291 0.992 -0.304 0.990
7 -0.301 0.993 -0.341 0.991
(Viterbi) ? -0.317 0.995 -0.326 0.992
Xent. -0.391 1.000 -0.410 1.000
Table 1: Correlation, on development sentences, of Re?nyi en-
tropy with model accuracy and with cross-entropy (?Xent.?).
Since these are measures of uncertainty, we see a negative cor-
relation. As ? increases, we place more confidence in high-
probability parses and correlate better with accuracy.
We used stochastic gradient descent first to min-
imize equation (4) on the labeled seed sets. Then
we continued to optimize over the labeled and unla-
beled data together. We tested for convergence using
accuracy on development data.
3.2 Empirically Evaluating Entropy
Bootstrapping assumes that where the parser is con-
fident, it tends to be correct. Standard bootstrapping
methods retrain directly on confident links; simi-
larly, our approach tries to make the parser even
more confident on those links.
Is this assumption really true empirically? Yes:
not only does confidence on unlabeled data correlate
with cross-entropy, but both confidence and cross-
entropy correlate well with accuracy. As we will
see, some confidence measures correlate better than
others. In particular, measures that are more peaked
around the one-best prediction of the parser, as in
Viterbi re-estimation, perform well.
If we train a non-projective German parser on
small seed sets of 100 and 1000 trees, only, how well
does its own confidence predict its performance?
For 200 points?labeled development sentences?
we measured the linear correlation of various Re?nyi
entropies (6), normalized by sentence length, with
tree accuracy (Table 1). We also measured how these
normalized Re?nyi entropies correlate with the pos-
terior log-probability the model assigns to the true
parse (the cross-entropy).
Since Re?nyi entropy is a measure of uncertainty,
we see a negative correlation with accuracy. This
correlation strengthens as we raise ? to ?, so we
might expect Viterbi re-estimation, or a differen-
Figure 1: Posterior probability of correct and incorrect edges
in German test data under various models. We show the distri-
bution of posterior probabilities for correct edges, known from
an oracle, in black and incorrect edges in gray. In the upper
row, learning on an initial supervised set raises the posterior
probability of correct edges while dragging along some incor-
rect edges. In the lower row, we see that adding unlabeled data
with R2 entropy continues the pattern of the supervised learner.
R? (Viterbi) training induces a second mode in correct pos-
terior probabilities near 1 although it does shift more incorrect
edges closer to 1.
Figure 2: Precision-recall curves for selecting edges according
to their posterior probabilities: better bootstrapping puts more
area under the curve.
tiable objective function with a very high ?, to per-
form best on held-out data. Note also that the cross-
entropy, which looks at the true labels on the held-
out data, does not itself correlate very much bet-
ter with accuracy than the best unsupervised confi-
dence measures. Finally, we see that Re?nyi entropies
with higher ? are more stable: when calculated for a
model trained on more data, they improve their cor-
relation with accuracy.
From tree confidence, we now turn to edge confi-
dence: what is the posterior probability that a model
assigns to each of the n2 edges in the dependency
graph? Figure 1 shows smoothed histograms of true
edges (black) and false edges (gray) in held-out data,
according to the posterior probabilities we assign to
673
them. Since there are many more false edges, the
figures are cropped to zoom in on the distribution of
true edges. As we start training on the labeled seed
set, the posterior probabilities of true edges move to-
wards one; many false edges also get greater mass,
but not to the same extent. As we add unlabeled
data, we can see the different learning strategies of
different confidence measures. R2 gradually moves
a few true and many fewer false edges towards 1,
while R? (Viterbi) learning is so confident as to in-
duce a bimodal distribution in the posteriors of true
edges. Figure 2 visualizes the same data as four
precision-recall curves, which show how noisy the
highest-confidence edges are, across a range of con-
fidence thresholds. Although the very high precision
end stays stable after 10 iterations on the seed set,
the addition of unlabeled data puts more area under
the curve. Again, R? dominates R2.
3.3 Bootstrapping Results
We performed bootstrapping experiments on the full
CoNLL sets for Czech, German, and Spanish us-
ing the non-projective model from McDonald et al
(2005). Performance confirms the results of our
analysis above (Table 2). Adding unlabeled data im-
proves performance over that of the seed set, with
the exception of the Czech data with R2 bootstrap-
ping. As we saw in ?3.2, bootstrapping with R?
dominates bootstrapping with R2 confidence. For
comparison, we also show the results obtained by
supervised training on the combined seed and unla-
beled sets. Recall that we did not use the tree anno-
tations to perform feature selection; models trained
with only supported features ought to perform better.
Although we see statistically significant improve-
ments (at the .05 level on a paired permutation test),
the quality of the parsers is still quite poor, in con-
trast to other applications of bootstrapping which
?rival supervised methods? (Yarowsky, 1995). Al-
most certainly, the CoNLL datasets, comprising at
most some tens of thousands of sentences per lan-
guage, are too small to afford qualitative improve-
ments. Also, at these relatively small training sizes,
our preliminary attempts to leverage comparable En-
glish corpora did not improve performance.
What features were learned, and how dependent
is performance on the seed set? We analyzed the
performance of German bootstrapping on a develop-
% accuracy
Seed trees ? = 0 2 ?
Czech 100 56.1 54.8 58.3
1000 68.1 68.2 68.2
71468 77.9 ? ?
German 100 60.9 62.4 65.3
1000 74.6 74.5 75.0
37745 86.0 ? ?
Spanish 100 63.6 64.1 64.4
2786 76.6 ? ?
Table 2: Dependency accuracy of the McDonald model on 200
test sentences. When ? = 0, training only occurs on the super-
vised seed data. As ? increases, we train based on confidence
in our model?s analysis of the unlabeled data. Boldface results
are the best in their rows in a permutation test at the .05 level.
ment set (Table 3). Using the parameters at the last
iteration of supervised training on the seed set as a
baseline, we tried updating to their bootstrapped val-
ues the weights of only those features that occurred
in the seed set. This achieved nearly the same ac-
curacy as updating all the features. As one would
expect, using only the non-seed features? weights
performs abysmally. This might be the case sim-
ply because the seed set is likely to contain fre-
quently occurring features. If, however, we use only
the features occurring in an alternate training set of
the same size (100 sentences), we get much worse
performance. These results indicate that our boot-
strapped parser is still heavily dependent on the fea-
tures that happened to fire in the seed set; we have
not ?forgotten? our initial conditions. Similar exper-
iments show that unlexicalized features contribute
the most to bootstrapping performance. Since in
our log-linear models features have been trained to
work together, we must not put too much weight on
these ablation results. These experiments do, how-
ever, suggest that bootstrapping improved our results
by refining the values of known, non-lexicalized fea-
tures.
3.4 Comparison with EM
Perhaps the most popular statistical method for
learning from incomplete data is the EM algorithm
(Dempster et al, 1977). Since we cannot try EM on
McDonald?s conditional model, we ran some pilot
experiments using the generative dependency model
with valence (DMV) of Klein and Manning (2004).
As in their experiments, and unlike the other exper-
iments in the current paper, we restricted ourselves
674
Updated M feat. acc. Updated M feat. acc.
all 15.5 64.3 none 0 60.9
seed 1.4 64.1 non-seed 14.1 44.7
non-lexical 3.5 64.4 lexical 12.0 59.9
non-bilex. 12.6 64.4 bilexical 2.9 61.0
Table 3: Using all features, dependency accuracy on German
development data rose to 64.3% on bootstrapping. We show the
contribution of different feature splits to the performance of this
final model. For example, although this model was trained by
updating all 15.5M feature weights, it performs as well if we
then keep only the 1.4M features that appeared at least once in
the seed set, zeroing out the weights of the others. We do as well
as the full feature set if we keep only the 3.5M non-lexicalized
features.
% accuracy
train Bulg. German Spanish
supervised ML 74.2 80.0 71.3
CL 77.5 79.3 75.0
semi- EM 58.6 58.8 68.4
supervised Conf. 80.0 80.5 76.7
Table 4: Dependency accuracy of the DMV model (Klein and
Manning, 2004). Maximizing confidence using R1 (Shannon)
entropy improved performance over its own conditional like-
lihood (CL) baseline and over maximum likelihood (ML). EM
degraded its ML baseline. Since these models were only trained
and tested on sentences of 10 words or fewer, accuracy is much
higher than the full results in Table 2.
to sentences of ten words or fewer and to part-of-
speech sequences alone, without any lexical infor-
mation. Since the DMV models projective trees, we
ran experiments on three CoNLL corpora that had
augmented their primary non-projective parses with
alternate projective annotations: Bulgarian (Simov
et al, 2005), German, and Spanish.
We performed supervised maximum likelihood
and conditional likelihood estimation on a seed set
of 100 sentences for each language. These models
respectively initialized EM and confidence training
on unlabeled data. We see (Table 4) that EM de-
grades the performance of its ML baseline. Meri-
aldo (1994) saw a similar degradation over small
(and large) seed sets in HMM POS tagging. We
tried fixing and not fixing the feature expectations on
the seed set during EM and show the former, better
numbers. Confidence maximization improved over
both its own conditional likelihood initializer and
also over ML. We selected optimal smoothing pa-
rameters for all models and optimal ? (equation (6))
and ? (equation (4)) for the confidence model on la-
beled held-out data.
4 Future Work
We hypothesize that qualitatively better bootstrap-
ping results will require much larger unlabeled data
sets. In scaling up bootstrapping to larger unla-
beled training sets, we must carefully weight trade-
offs between expanding coverage and introducing
noise from out-of-domain data. We could also bet-
ter exploit the data we have with richer models of
syntax. In supervised dependency parsing, second-
order edge features provide improvements (McDon-
ald and Pereira, 2006; Riedel and Clarke, 2006);
moreover, the feature-based approach is not limited
to dependency parsing. Similar techniques could
score parses in other formalisms, such as CFG or
TAG. In this case, f extracts features from each
of the derivation tree?s rewrite rules (CFG) or ele-
mentary trees (TAG). In lexicalized formalisms, f
will still be able to score lexical dependencies that
are implicitly represented in the parse. Finally, we
want to investigate whether larger training sets will
provide traction for sparser cross-lingual and cross-
domain features.
5 Conclusions
Feature-rich dependency models promise to help
bootstrapping by providing many redundant features
for the learner, and they can also cleanly incorporate
cross-domain and cross-language information.
We explored bootstrapping feature-rich non-
projective dependency parsers for Czech, German,
and Spanish. Our bootstrapping method maximizes
a linear combination of likelihood and confidence.
In initial experiments on small datasets, this sur-
passed EM for training a simple feature-poor gener-
ative model, and also improved the performance of
a feature-rich, conditionally estimated model where
EM could not easily have been applied. For our
models and training sets, more peaked measures
of confidence, measured by Re?nyi entropy, outper-
formed smoother ones.
Acknowledgments
The authors thank the anonymous reviewers, Noah
A. Smith, and Keith Hall for helpful comments, and
Ryan McDonald for making his parsing code pub-
licly available. This work was supported in part by
NSF ITR grant IIS-0313193.
675
References
Steven Abney. 2004. Understanding the Yarowsky algo-
rithm. CL, 30(3):365?395.
Cristoph Arndt. 2001. Information Measures. Springer.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP, pages 120?128.
A. Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, volume 20 of Text, Speech and Language Tech-
nology, chapter 7. Kluwer.
Le?on Bottou. 2003. Stochastic learning. In Ad-
vanced Lectures in Machine Learning, pages 146?168.
Springer.
Matthew E. Brand. 1999. Structure learning in condi-
tional probability models via an entropic prior and pa-
rameter extinction. Neural Computation, 11(5):1155?
1182.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In TLT.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Civit Torruella and M. A. Mart?? Anton??n. 2002. De-
sign principles for a Spanish treebank. In TLT.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In EMNLP, pages 232?241.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum
likelihood estimation from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society B,
39:1?38.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In COLING,
pages 340?345.
H. N. Gabow, Z. Galil, T. H. Spencer, and R. E. Tarjan.
1986. Efficient algorithms for finding minimum span-
ning trees in undirected and directed graphs. Combi-
natorica, 6(2):109?122.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Yves Grandvalet and Yoshua Bengio. 2005. Semi-
supervised learning by entropy minimization. In
NIPS.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:311?325.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In EMNLP, pages 45?52.
Martin Jansche. 2005. Treebank transfer. In IWPT.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved se-
quence segmentation and labeling. In COLING/ACL,
pages 209?216.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Carey E. Priebe. 2007. Cross-instance tuning
of unsupervised document clustering algorithms. In
HLT-NAACL, pages 252?259.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In ACL, pages 479?486.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In COLING-
ACL, pages 817?824.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the Matrix-Tree The-
orem. In EMNLP-CoNLL.
Gideon S. Mann and Andrew McCallum. 2007. Efficient
computation of entropy gradient for semi-supervised
conditional random fields. In HLT-NAACL.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In EMNLP, pages 44?52, July.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL, pages 337?344.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In EACL.
676
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In IWPT.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL, pages 91?98.
Bernardo Merialdo. 1994. Tagging English text with a
probabilistic model. CL, 20(2):155?72.
Fuchun Peng, Ralph Weischedel, Ana Licuanan, and
Jinxi Xu. 2005. Combining deep linguistics analy-
sis and surface pattern learning: A hybrid approach
to Chinese definitional question answering. In HLT-
EMNLP, pages 307?314.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In EMNLP, pages 129?137.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In CoNLL.
Charles Schafer. 2006. Translation Discovery Using Di-
verse Smilarity Measures. Ph.D. thesis, Johns Hop-
kins University.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005. Design and implementation of the Bulgarian
HPSG-based treebank. In Journal of Research on Lan-
guage and Computation ? Special Issue. Kluwer.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 23?30.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees. In
EMNLP-CoNLL.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In EACL.
Ralph Weischedel. 2004. Extracting dynamic evidence
networks. Technical Report AFRL-IF-RS-TR-2004-
246, BBN Technologies, Cambridge, MA, December.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL, pages 207?216.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL, pages
189?196.
677
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 962?966,
Prague, June 2007. c?2007 Association for Computational Linguistics
Log-linear Models of Non-projective Trees, k-best MST Parsing and
Tree-ranking
Keith Hall1 and Jir??? Havelka2 and David A. Smith1
1Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD USA
keith hall@jhu.edu
dasmith@cs.jhu.edu
2Institute of Formal and Applied Linguistics
Charles University
Prague, Czech Republic
havelka@ufal.mff.cuni.cz
Abstract
We present our system used in the CoNLL
2007 shared task on multilingual parsing.
The system is composed of three compo-
nents: a k-best maximum spanning tree
(MST) parser, a tree labeler, and a reranker
that orders the k-best labeled trees. We
present two techniques for training the
MST parser: tree-normalized and graph-
normalized conditional training. The tree-
based reranking model allows us to explic-
itly model global syntactic phenomena. We
describe the reranker features which include
non-projective edge attributes. We provide
an analysis of the errors made by our system
and suggest changes to the models and fea-
tures that might rectify the current system.
1 Introduction
Reranking the output of a k-best parser has been
shown to improve upon the best results of a state-
of-the-art constituency parser (Charniak and John-
son, 2005). This is primarily due to the ability to
incorporate complex structural features that cannot
be modeled under a CFG. Recent work shows that
k-best maximum spanning tree (MST) parsing and
reranking is also viable (Hall, 2007). In the current
work, we explore the k-best MST parsing paradigm
along with a tree-based reranker. A system using
the parsing techniques presented in this paper was
entered in the CoNLL 2007 shared task competi-
tion (Nivre et al, 2007). This task evaluated pars-
ing performance on 10 languages: Arabic, Basque,
Catalan, Chinese, Czech, English, Greek, Hungar-
ian, Italian, and Turkish using data originating from
a wide variety of dependency treebanks, and trans-
formations of constituency-based treebanks (Hajic?
et al, 2004; Aduriz et al, 2003; Mart?? et al, 2007;
Chen et al, 2003; Bo?hmova? et al, 2003; Marcus et
al., 1993; Johansson and Nugues, 2007; Prokopidis
et al, 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003).
We show that oracle parse accuracy1 of the out-
put of our k-best parser is generally higher than the
best reported results. We also present the results
of a reranker based on a rich set of structural fea-
tures, including features explicitly targeted at mod-
eling non-projective configurations. Labeling of the
dependency edges is accomplished by an edge la-
beler based on the same feature set as used in train-
ing the k-best MST parser.
2 Parser Description
Our parser is composed of three components: a k-
best MST parser, a tree-labeler, and a tree-reranker.
Log-linear models are used for each of the com-
ponents independently. In this section we give an
overview of the models, the training techniques, and
the decoders.
2.1 MST Parsing, Reranking, and Labeling
The connection between the maximum spanning
tree problem and dependency parsing stems from
the observation that a dependency parse is simply an
oriented spanning tree on the graph of all possible
1The oracle accuracy for a set of hypotheses is the maximal
accuracy for any of the hypotheses.
962
dependency links (the fully connected dependency
graph). Unfortunately, by mapping the problem to
a graph, we assume that the scores associated with
edges are independent, and thus, are limited to edge-
factored models.
Edge-factored models are severely limited in their
capacity to predict structure. In fact, they can only
directly model parent-child links. In order to allevi-
ate this, we use a k-best MST parser to generate a
set of candidate hypotheses. Then, we rerank these
trees using a model based on rich structural features
that model features such as valency, subcategoriza-
tion, ancestry relationships, and sibling interactions,
as well as features capturing the global structure of
dependency trees, aimed primarily at modeling lan-
guage specific non-projective configurations.
We assign dependency labels to entire trees, rather
than predicting the labels during tree construction.
Given that we have a reranking process, we can la-
bel the k-best tree hypotheses output from our MST
parser, and rerank the labeled trees. We have ex-
plored both labeled and unlabeled reranking. In the
latter case, we simply label the maximal unlabeled
tree.
2.1.1 MST Training
McDonald et al (2005) present a technique for
training discriminative models for dependency pars-
ing. The edge-factored models we use for MST
parsing are closely related to those described in the
previous work, but allow for the efficient compu-
tation of normalization factors which are required
for first and second-order (gradient-based) training
techniques.
We consider two estimation procedures for
parent-prediction models. A parent-prediction
model assigns a conditional score s(g|d) for ev-
ery parent-child pair (we denote the parent/governor
g, and the child/dependent d), where s(g|d) =
s(g, d)/
?
g? s(g
?, d). In our work, we compute
probabilities p(g|d) based on conditional log-linear
models. This is an approximation to a generative
model that predicts each node once (i.e.,
?
d p(d|g)).
In the graph-normalized model, we assume that
the conditional distributions are independent of one
another. In particular, we find the model parameters
that maximize the likelihood of p(g?|d), where g?
is the correct parent in the training data. We per-
form the optimization over the entire training set,
tying the feature parameters. In particular, we per-
form maximum entropy (MaxEnt) estimation over
the conditional distribution using second-order gra-
dient descent optimization techniques.2 An advan-
tage of the parent-prediction model is that we can
frame the estimation problem as that of minimum-
error training with a zero-one loss term:
p(e, g|d) =
exp(
?
i ?ifi(e, g, d))
Zd
(1)
where e ? {0, 1} is the error term (e is 1 for
the correct parent and 0 for all other nodes) and
Zd =
?
j exp(
?
i ?ifi(ej , gj , d)) is the normaliza-
tion constant for node d. Note that the normaliza-
tion factor considers all graphs with in-degree zero
for the root node and in-degree one for other nodes.
At parsing time, of course, our parent predictions
are constrained to produce a (non-projective) tree
structure. We can sum over all non-projective span-
ning trees by taking the determinant of the Kirchhoff
matrix of the graph defined above, minus the row
and column corresponding to the root node (Smith
and Smith, 2007). Training graph-normalized and
tree-normalized models under identical conditions,
we find tree normalization wins by 0.5% to 1% ab-
solute dependency accuracy. Although tree normal-
ization also shows a (smaller) advantage in k-best
oracle accuracy, we do not believe it would have a
large effect on our reranking results.
2.1.2 Reranker Training
The reranker is based on a conditional log-linear
model subject to the MaxEnt constraints using the
same second-order optimization procedures as the
graph-normalized MST models. The primary dif-
ference here is that there is no single correct tree in
the set of k candidate parse trees. Instead, we have
k trees that are generated by our k-best parser, each
with a score assigned by the parser. If we are per-
forming labeled reranking, we label each of these
hypotheses with l possible labelings, each with a
score assigned by the labeler.
As with the parent-prediction, graph-normalized
model, we perform minimum-error training. The
2For the graph-normalized models, we use L-BFGS opti-
mization provided through the TAO/PETSC optimization li-
brary (Benson et al, 2005; Balay et al, 2004).
963
optimization is achieved by assuming the oracle-best
parse(s) are correct and the remaining hypotheses
are incorrect. Furthermore, the feature values are
scaled according to the relative difference between
the oracle-best score and the score assigned to the
non-oracle-best hypothesis.
Note that any reranker could be used in place of
our current model. We have chosen to keep the
reranker model closely related to the MST parsing
model so that we can share feature representations
and training procedures.
2.1.3 Labeler Training
We used the same edge features to train a sep-
arate log-linear labeling model. Each edge feature
was conjoined with a potential label, and we then
maximized the likelihood of the labeling in the train-
ing data. Since this model is also edge-factored, we
can store the labeler scores for each of the n2 po-
tential edges in the dependency tree. In the submit-
ted system, we simply extracted the Viterbi predic-
tions of the labeler for the unlabeled trees selected
by the reranker. We also (see below) ran experiments
where each entry in the k-best lists input as training
data to the reranker was augmented by its l-best la-
belings. We hoped thereby to inject more diversity
into the resulting structures.
2.1.4 Model Features
Our MST models are based on the features de-
scribed in (Hall, 2007); specifically, we use features
based on a dependency nodes? form, lemma, coarse
and fine part-of-speech tag, and morphological-
string attributes. Additionally, we use surface-string
distance between the parent and child, buckets of
features indicating if a particular form/lemma/tag
occurred between or next to the parent and child, and
a branching feature indicating whether the child is
to the left or right of the parent. Composite features,
combining the above features are also included (e.g.,
a single feature combining branching, parent & child
form, parent & child tag).
The tree-based reranker includes the features de-
scribed in (Hall, 2007) as well as features based on
non-projective edge attributes explored in (Havelka,
2007a; Havelka, 2007b). One set of features mod-
els relationships of nodes with their siblings, in-
cluding valency and subcategorization. A second
set of features models global tree structure and in-
cludes features based on a node?s ancestors and the
depth and size of its subtree. A third set of fea-
tures models the interaction of word order and tree
structure as manifested on individual edges, i.e., the
features model language specific projective and non-
projective configurations. They include edge-based
features corresponding to the global constraints of
projectivity, planarity and well-nestedness, and for
non-projective edges, they furthermore include level
type, level signature and ancestor-in-gap features.
All features allow for an arbitrary degree of lexical-
ization; in the reported results, the first two sets of
features use coarse and fine part-of-speech lexical-
izations, while the features in the third set are used
in their unlexicalized form due to time limitations.
3 Results and Analysis
Hall (2007) shows that the oracle parsing accuracy
of a k-best edge-factored MST parser is consid-
erably higher than the one-best score of the same
parser, even when k is small. We have verified that
this is true for the CoNLL shared-task data by evalu-
ating the oracle rates on a randomly sampled devel-
opment set for each language.
In order to select optimal model parameters for
the MST parser, the labeler, and reranker, we sam-
pled approximately 200 sentences from each train-
ing set to use as a development test set. Training the
reranker requires a jackknife n-fold training proce-
dure where n?1 partitions are used to train a model
that parses the remaining partition. This is done n
times to generate k-best parses for the entire training
set without using models trained on the data they are
run on.
For lack of space, we report only results on the
CoNLL evaluation data set here, but note that the
trends observed on the evaluation data are identical
to those observed on our development sets.
In Table 1 we present results for labeled (and un-
labeled) dependency accuracy on the CoNLL 2007
evaluation data set. We report the oracle accu-
racy for different sized k-best hypothesis sets. The
columns are labeled by the number of trees output
from the MST parser, k;3 and by the number of al-
3All results are reported for the graph-normalized training
technique.
964
Language Oracle Accuracy New CoNLL07 CoNLL07
k = 1, l = 1 k = 10, l = 5 k = 50, l = 1 k = 50, l = 2 Reranked Reported Best
Arabic (83.10) (85.56) (86.96) (83.67) 73.40 (83.45) 76.52 (86.09)
Basque 67.92 (76.88) 76.25 (82.19) 69.93 (84.99) 76.81 (77.76) 69.80 (78.52) 76.92 (82.80)
Catalan 82.28 (87.82) 85.11 (90.87) 86.82 (92.68) 86.82 (89.43) 82.38 (87.80) 88.70 (93.40)
Chinese 73.86 (85.58) 91.32 (93.39) 82.39 (95.80) 92.21 (87.87) 82.77 (87.91) 84.69 (88.94)
Czech 74.05 (80.21) 78.58 (85.08) 80.97 (87.60) 80.97 (82.20) 72.27 (78.47) 80.19 (86.28)
English 82.21 (83.63) 85.95 (87.59) 87.99 (89.75) 87.99 (85.31) 81.93 (83.21) 89.61 (90.63)
Greek 72.21 (81.16) 78.58 (84.89) 74.13 (86.95) 79.48 (81.81) 74.21 (82.04) 76.31 (84.08)
Hungarian 71.68 (78.57) 79.70 (83.03) 74.32 (85.12) 80.75 (80.05) 74.20 (79.34) 80.27 (83.55)
Italian 77.92 (83.16) 85.05 (87.54) 80.30 (89.66) 86.42 (84.71) 80.69 (84.81) 84.40 (87.91)
Turkish 75.34 (83.63) 83.96 (89.65) 77.78 (92.40) 84.98 (84.13) 77.42 (85.18) 79.81 (86.22)
Table 1: Labeled (unlabeled) attachment accuracy for k-best MST oracle results and reranked data on the evaluation set. The
1-best results (k = 1, l = 1) represent the performance of the MST parser without reranking. The New Reranked field shows recent
unlabeled reranking results of 50-best trees using a modified feature set. For arabic, we only report unlabeled accuracy for different
k and l.
ternative labelings for each tree, l. When k = 1,
the score is the best achievable by the edge-factored
MST parser using our models. As k increases, the
oracle parsing accuracy increases. The most ex-
treme difference between the one-best accuracy and
the 50-best oracle accuracy can be seen for Turkish
where there is a difference of 9.64 points of accu-
racy (8.77 for the unlabeled trees). This means that
the reranker need only select the correct tree from
a set of 50 to increase the score by 9.64%. As our
reranking results show, this is not as simple as it may
appear.
We report the results for our CoNLL submission
as well as recent results based on alternative param-
eters optimization on the development set. We re-
port the latest results only for unlabeled accuracy of
reranking 50-best MST output.
4 Conclusion
Our submission to the CoNLL 2007 shared task
on multilingual parsing supports the hypothesis that
edge-factored MST parsing is viable given an effec-
tive reranker. The reranker used in our submission
was unable to achieve the oracle rates. We believe
this is primarily related to a relatively impoverished
feature set. Due to time constraints, we have not
been able to train lexicalized reranking models. The
introduction of lexicalized features in the reranker
should influence the selection of better trees, which
we know exist in the k-best hypothesis sets.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. Diaz
de Ilarraza, A. Garmendia, and M. Oronoz. 2003. Con-
struction of a Basque dependency treebank. In Proc. of the
2nd Workshop on Treebanks and Linguistic Theories (TLT),
pages 201?204.
Satish Balay, Kris Buschelman, Victor Eijkhout, William D.
Gropp, Dinesh Kaushik, Matthew G. Knepley, Lois Curf-
man McInnes, Barry F. Smith, and Hong Zhang. 2004.
PETSc users manual. Technical Report ANL-95/11 - Re-
vision 2.1.5, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge More?, and
Jason Sarich. 2005. TAO user manual (revision 1.8).
Technical Report ANL/MCS-TM-242, Mathematics and
Computer Science Division, Argonne National Laboratory.
http://www.mcs.anl.gov/tao.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The
PDT: a 3-level annotation scenario. In Abeille? (Abeille?,
2003), chapter 7, pages 103?127.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and
Z. Gao. 2003. Sinica treebank: Design criteria, representa-
tional issues and implementation. In Abeille? (Abeille?, 2003),
chapter 13, pages 231?248.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005. The
Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic
Language Resources and Tools, pages 110?117.
Keith Hall. 2007. k-best spanning tree parsing. In (To Appear)
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics.
965
Jir??? Havelka. 2007a. Beyond projectivity: Multilingual eval-
uation of constraints and measures on non-projective struc-
tures. In (To Appear) Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics.
Jir??? Havelka. 2007b. Relationship between non-projective
edges, their level types, and well-nestedness. In Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages 61?64.
R. Johansson and P. Nugues. 2007. Extended constituent-to-
dependency conversion for English. In Proc. of the 16th
Nordic Conference on Computational Linguistics (NODAL-
IDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Online large-margin training of dependency parsers. In Pro-
ceedings of the 43nd Annual Meeting of the Association for
Computational Linguistics.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, O. Coraz-
zari, A. Lenci, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino, F. Zan-
zotto, N. Nana, F. Pianesi, and R. Delmonte. 2003. Build-
ing the Italian Syntactic-Semantic Treebank. In Abeille?
(Abeille?, 2003), chapter 11, pages 189?210.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of the Joint Conf. on Empiri-
cal Methods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In Abeille? (Abeille?, 2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papageor-
giou, and S. Piperidis. 2005. Theoretical and practical is-
sues in the construction of a Greek dependency treebank. In
Proc. of the 4th Workshop on Treebanks and Linguistic The-
ories (TLT), pages 149?160.
David A. Smith and Noah A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In (To Appear) Pro-
ceedings of the 2007 Conference on Empirical Methods in
Natural Language Processing.
966
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 145?156,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Dependency Parsing by Belief Propagation?
David A. Smith and Jason Eisner
Dept. of Computer Science, Johns Hopkins University
Balitmore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
We formulate dependency parsing as a graphical model
with the novel ingredient of global constraints. We show
how to apply loopy belief propagation (BP), a simple and
effective tool for approximate learning and inference. As
a parsing algorithm, BP is both asymptotically and em-
pirically efficient. Even with second-order features or la-
tent variables, which would make exact parsing consider-
ably slower or NP-hard, BP needs only O(n3) time with
a small constant factor. Furthermore, such features sig-
nificantly improve parse accuracy over exact first-order
methods. Incorporating additional features would in-
crease the runtime additively rather than multiplicatively.
1 Introduction
Computational linguists worry constantly about run-
time. Sometimes we oversimplify our models, trad-
ing linguistic nuance for fast dynamic programming.
Alternatively, we write down a better but intractable
model and then use approximations. The CL com-
munity has often approximated using heavy pruning
or reranking, but is beginning to adopt other meth-
ods from the machine learning community, such
as Gibbs sampling, rejection sampling, and certain
variational approximations.
We propose borrowing a different approximation
technique from machine learning, namely, loopy be-
lief propagation (BP). In this paper, we show that
BP can be used to train and decode complex pars-
ing models. Our approach calls a simpler parser as a
subroutine, so it still exploits the useful, well-studied
combinatorial structure of the parsing problem.1
2 Overview and Related Work
We wish to make a dependency parse?s score de-
pend on higher-order features, which consider ar-
?This work was supported by the Human Language Tech-
nology Center of Excellence.
1As do constraint relaxation (Tromble and Eisner, 2006) and
forest reranking (Huang, 2008). In contrast, generic NP-hard
solution techniques like Integer Linear Programming (Riedel
and Clarke, 2006) know nothing about optimal substructure.
bitrary interactions among two or more edges in the
parse (and perhaps also other latent variables such
as part-of-speech tags or edge labels). Such features
can help accuracy?as we show. Alas, they raise the
polynomial runtime of projective parsing, and ren-
der non-projective parsing NP-hard. Hence we seek
approximations.
We will show how BP?s ?message-passing? disci-
pline offers a principled way for higher-order fea-
tures to incrementally adjust the numerical edge
weights that are fed to a fast first-order parser. Thus
the first-order parser is influenced by higher-order
interactions among edges?but not asymptotically
slowed down by considering the interactions itself.
BP?s behavior in our setup can be understood intu-
itively as follows. Inasmuch as the first-order parser
finds that edge e is probable, the higher-order fea-
tures will kick in and discourage other edges e? to the
extent that they prefer not to coexist with e.2 Thus,
the next call to the first-order parser assigns lower
probabilities to parses that contain these e?. (The
method is approximate because a first-order parser
must equally penalize all parses containing e?, even
those that do not in fact contain e.)
This behavior is somewhat similar to parser stack-
ing (Nivre and McDonald, 2008; Martins et al,
2008), in which a first-order parser derives some of
its input features from the full 1-best output of an-
other parser. In our method, a first-order parser de-
rives such input features from its own previous full
output (but probabilistic output rather than just 1-
best). This circular process is iterated to conver-
gence. Our method also permits the parse to in-
teract cheaply with other variables. Thus first-order
parsing, part-of-speech tagging, and other tasks on a
common input could mutually influence one another.
Our method and its numerical details emerge nat-
urally as an instance of the well-studied loopy BP
algorithm, suggesting several potential future im-
2This may be reminiscent of adjusting a Lagrange multiplier
on e? until some (hard) constraint is satisfied.
145
provements to accuracy (Yedidia et al, 2004; Braun-
stein et al, 2005) and efficiency (Sutton and McCal-
lum, 2007).
Loopy BP has occasionally been used before in
NLP, with good results, to handle non-local fea-
tures (Sutton and McCallum, 2004) or joint decod-
ing (Sutton et al, 2004). However, our application
to parsing requires an innovation to BP that we ex-
plain in ?5?a global constraint to enforce that the
parse is a tree. The tractability of some such global
constraints points the way toward applying BP to
other computationally intensive NLP problems, such
as syntax-based alignment of parallel text.
3 Graphical Models of Dependency Trees
3.1 Observed and hidden variables
To apply BP, we must formulate dependency parsing
as a search for an optimal assignment to the vari-
ables of a graphical model. We encode a parse using
the following variables:
Sentence. The n-word input sentence W is fully
observed (not a lattice). Let W = W0W1 ? ? ?Wn,
where W0 is always the special symbol ROOT.
Tags. If desired, the variables T = T1T2 ? ? ?Tn may
specify tags on the nwords, drawn from some tagset
T (e.g., parts of speech). These variables are needed
iff the tags are to be inferred jointly with the parse.
Links. The O(n2) boolean variables {Lij : 0 ?
i ? n, 1 ? j ? n, i 6= j} correspond to the possible
links in the dependency parse.3 Lij = true is in-
terpreted as meaning that there exists a dependency
link from parent i? child j.4
Link roles, etc. It would be straightforward to add
other variables, such as a binary variable Lirj that is
true iff there is a link i
r
? j labeled with role r (e.g.,
AGENT, PATIENT, TEMPORAL ADJUNCT).
3.2 Markov random fields
We wish to define a probability distribution over all
configurations, i.e., all joint assignments A to these
3?Links? are conventionally called edges, but we reserve the
term ?edge? for describing the graphical model?s factor graph.
4We could have chosen a different representation withO(n)
integer variables {Pj : 1 ? j ? n}, writing Pj = i instead of
Lij = true. This representation can achieve the same asymp-
totic runtime for BP by using sparse messages, but some con-
straints and algorithms would be somewhat harder to explain.
variables. Our distribution is simply an undirected
graphical model, or Markov random field (MRF):5
p(A)
def
=
1
Z
?
m
Fm(A) (1)
specified by the collection of factors Fm : A 7?
R?0. Each factor is a function that consults only a
subset of A. We say that the factor has degree d
if it depends on the values of d variables in A, and
that it is unary, binary, ternary, or global if d is
respectively 1, 2, 3, or unbounded (grows with n).
A factor function Fm(A) may also depend freely
on the observed variables?the input sentence W
and a known (learned) parameter vector ?. For no-
tational simplicity, we suppress these extra argu-
ments when writing and drawing factor functions,
and when computing their degree. In this treatment,
these observed variables are not specified by A, but
instead are absorbed into the very definition of Fm.
In defining a factor Fm, we often define the cir-
cumstances under which it fires. These are the only
circumstances that allow Fm(A) 6= 1. When Fm
does not fire, Fm(A) = 1 and does not affect the
product in equation (1).
3.3 Hard constraints
A hard factor Fm fires only on parsesA that violate
some specified condition. It has value 0 on those
parses, acting as a hard constraint to rule them out.
TREE. A hard global constraint on all the Lij vari-
ables at once. It requires that exactly n of these vari-
ables be true, and that the corresponding links form
a directed tree rooted at position 0.
PTREE. This stronger version of TREE requires
further that the tree be projective. That is, it pro-
hibits Lij and Lk` from both being true if i ? j
crosses k ? `. (These links are said to cross if one
of k, ` is strictly between i and j while the other is
strictly outside that range.)
EXACTLY1. A family of O(n) hard global con-
straints, indexed by 1 ? j ? n. EXACTLY1j re-
quires that j have exactly one parent, i.e., exactly
one of the Lij variables must be true. Note that EX-
ACTLY1 is implied by TREE or PTREE.
5Our overall model is properly called a dynamic MRF, since
we must construct different-size MRFs for input sentences of
different lengths. Parameters are shared both across and within
these MRFs, so that only finitely many parameters are needed.
146
ATMOST1. A weaker version. ATMOST1j re-
quires j to have one or zero parents.
NAND. A family of hard binary constraints.
NANDij,k` requires that Lij and Lk` may not both be
true. We will be interested in certain subfamilies.
NOT2. Shorthand for the family of O(n3) bi-
nary constraints {NANDij,kj}. These are collectively
equivalent to ATMOST1, but expressed via a larger
number of simpler constraints, which can make the
BP approximation less effective (footnote 30).
NO2CYCLE. Shorthand for the family of O(n2)
binary constraints {NANDij,ji}.
3.4 Soft constraints
A soft factor Fm acts as a soft constraint that prefers
some parses to others. In our experiments, it is al-
ways a log-linear function returning positive values:
Fm(A)
def
= exp
?
h?features(Fm)
?hfh(A,W,m) (2)
where ? is a learned, finite collection of weights and
f is a corresponding collection of feature functions,
some of which are used by Fm. (Note that fh is
permitted to consult the observed input W . It also
sees which factor Fm it is scoring, to support reuse
of a single feature function fh and its weight ?h by
unboundedly many factors in a model.)
LINK. A family of unary soft factors that judge
the links in a parse A individually. LINKij fires iff
Lij = true, and then its value depends on (i, j),
W , and ?. Our experiments use the same features as
McDonald et al (2005).
A first-order (or ?edge-factored?) parsing model
(McDonald et al, 2005) contains only LINK factors,
along with a global TREE or PTREE factor. Though
there are O(n2) link factors (one per Lij), only n
of them fire on any particular parse, since the global
factor ensures that exactly n are true.
We?ll consider various higher-order soft factors:
PAIR. The binary factor PAIRij,k` fires with some
value iff Lij and Lk` are both true. Thus, it penal-
izes or rewards a pair of links for being simultane-
ously present. This is a soft version of NAND.
GRAND. Shorthand for the family of O(n3) binary
factors {PAIRij,jk}, which evaluate grandparent-
parent-child configurations, i ? j ? k. For exam-
ple, whether preposition j attaches to verb i might
depend on its object k. In non-projective parsing,
we might prefer (but not require) that a parent and
child be on the same side of the grandparent.
SIB. Shorthand for the family of O(n3) binary fac-
tors {PAIRij,ik}, which judge whether two children
of the same parent are compatible. E.g., a given verb
may not like to have two noun children both to its
left.6 The children do not need to be adjacent.
CHILDSEQ. A family of O(n) global factors.
CHILDSEQi scores i?s sequence of children; hence
it consults all variables of the form Lij . The scor-
ing follows the parametrization of a weighted split
head-automaton grammar (Eisner and Satta, 1999).
If 5 has children 2, 7, 9 under A, then CHILDSEQi
is a product of subfactors of the form PAIR5#,57,
PAIR57,59, PAIR59,5# (right child sequence) and
PAIR5#,52, PAIR52,5# (left child sequence).
NOCROSS. A family of O(n2) global constraints.
If the parent-to-j link crosses the parent-to-` link,
then NOCROSSj` fires with a value that depends
only on j and `. (If j and ` do not each have ex-
actly one parent, NOCROSSj` fires with value 0; i.e.,
it incorporates EXACTLY1j and EXACTLY1`.)7
TAGi is a unary factor that evaluates whether Ti?s
value is consistent with W (especially Wi).
TAGLINKij is a ternary version of the LINKij fac-
tor whose value depends on Lij , Ti and Tj (i.e., its
feature functions consult the tag variables to decide
whether a link is likely). One could similarly enrich
the other features above to depend on tags and/or
link roles; TAGLINK is just an illustrative example.
TRIGRAM is a global factor that evaluates the tag
sequence T according to a trigram model. It is a
product of subfactors, each of which scores a tri-
gram of adjacent tags Ti?2, Ti?1, Ti, possibly also
considering the word sequence W (as in CRFs).
4 A Sketch of Belief Propagation
MacKay (2003, chapters 16 and 26) provides an
excellent introduction to belief propagation, a gen-
6A similar binary factor could directly discourage giving the
verb two SUBJECTs, if the model has variables for link roles.
7In effect, we have combined the O(n4) binary factors
PAIRij,k` into O(n2) groups, and made them more precise
by multiplying in EXACTLYONE constraints (see footnote 30).
This will permit O(n3) total computation if we are willing to
sacrifice the ability of the PAIR weights to depend on i and k.
147
LINK
L[2,5]
GRAND
L[5,6]
LINK
TREE
Figure 1: A fragment of a factor graph, illustrating a few
of the unary, binary, and global factors that affect vari-
ables L25 and L56. The GRAND factor induces a loop.
eralization of the forward-backward algorithm that
is deeply studied in the graphical models literature
(Yedidia et al, 2004, for example). We briefly
sketch the method in terms of our parsing task.
4.1 Where BP comes from
The basic BP idea is simple. Variable L34 main-
tains a distribution over values true and false?a
?belief??that is periodically recalculated based on
the current distributions at other variables.8
Readers familiar with Gibbs sampling can regard
this as a kind of deterministic approximation. In
Gibbs sampling, L34?s value is periodically resam-
pled based on the current values of other variables.
Loopy BP works not with random samples but their
expectations. Hence it is approximate but tends to
converge much faster than Gibbs sampling will mix.
It is convenient to visualize an undirected factor
graph (Fig. 1), in which each factor is connected
to the variables it depends on. Many factors may
connect to?and hence influence?a given variable
such as L34. If X is a variable or a factor, N (X)
denotes its set of neighbors.
4.2 What BP accomplishes
Given an input sentence W and a parameter vector
?, the collection of factors Fm defines a probabil-
ity distribution (1). The parser should determine the
values of the individual variables. In other words,
we would like to marginalize equation (1) to obtain
the distribution p(L34) over L34 = true vs. false,
the distribution p(T4) over tags, etc.
If the factor graph is acyclic, then BP com-
putes these marginal distributions exactly. Given
8Or, more precisely?this is the tricky part?based on ver-
sions of those other distributions that do not factor in L34?s re-
ciprocal influence on them. This prevents (e.g.) L34 and T3
from mutually reinforcing each other?s existing beliefs.
an HMM, for example, BP reduces to the forward-
backward algorithm.
BP?s estimates of these distributions are called be-
liefs about the variables. BP also computes be-
liefs about the factors, which are useful in learn-
ing ? (see ?7). E.g., if the model includes the factor
TAGLINKij , which is connected to variables Lij , Ti,
Tj , then BP will estimate the marginal joint distribu-
tion p(Lij , Ti, Tj) over (boolean, tag, tag) triples.
When the factor graph has loops, BP?s beliefs are
usually not the true marginals of equation (1) (which
are in general intractable to compute). Indeed, BP?s
beliefs may not be the true marginals of any distribu-
tion p(A) over assignments, i.e., they may be glob-
ally inconsistent. All BP does is to incrementally
adjust the beliefs till they are at least locally con-
sistent: e.g., the beliefs at factors TAGLINKij and
TAGLINKik must both imply9 the same belief about
variable Ti, their common neighbor.
4.3 The BP algorithm
This iterated negotiation among the factors is han-
dled by message passing along the edges of the fac-
tor graph. A message to or from a variable is a (pos-
sibly unnormalized) probability distribution over the
values of that variable.
The variable V sends a message to factor F , say-
ing ?My other neighboring factors G jointly suggest
that I have posterior distribution qV?F (assuming
that they are sending me independent evidence).?
Meanwhile, factor F sends messages to V , saying,
?Based on my factor function and the messages re-
ceived from my other neighboring variables U about
their values (and assuming that those messages are
independent), I suggest you have posterior distribu-
tion rF?V over your values.?
To be more precise, BP at each iteration k (until
convergence) updates two kinds of messages:
q(k+1)V?F (v) = ?
?
G?N (V )\F
r(k)G?V (v) (3)
from variables to factors, and
r(k+1)F?V (v) = ?
?
A s.t. A[V ]=v
F (A)
?
U?N (F )\V
q(k)U?F (A[U ])
(4)
9In the sense that marginalizing the belief p(Lij , Ti, Tj) at
the factor yields the belief p(Ti) at the variable.
148
from factors to variables. Each message is a proba-
bility distribution over values v of V , normalized by
a scaling constant ?. Alternatively, messages may be
left as unnormalized distributions, choosing ? 6= 1
only as needed to prevent over- or underflow. Mes-
sages are initialized to uniform distributions.
Whenever we wish, we may compute the beliefs
at V and F :
b(k+1)V? (v)
def
= ?
?
G?N (V )
r(k)G?V (v) (5)
b(k+1)F? (A)
def
= ? F (A)
?
U?N (F )
q(k)U?F (A[U ]) (6)
These beliefs do not truly characterize the ex-
pected behavior of Gibbs sampling (?4.1), since the
products in (5)?(6) make conditional independence
assumptions that are valid only if the factor graph
is acyclic. Furthermore, on cyclic (?loopy?) graphs,
BP might only converge to a local optimum (Weiss
and Freedman, 2001), or it might not converge at all.
Still, BP often leads to good, fast approximations.
5 Achieving Low Asymptotic Runtime
One iteration of standard BP simply updates all the
messages as in equations (3)?(4): one message per
edge of the factor graph.
Therefore, adding new factors to the model in-
creases the runtime per iteration additively, by in-
creasing the number of messages to update. We
believe this is a compelling advantage over dy-
namic programming?in which new factors usually
increase the runtime and space multiplicatively by
exploding the number of distinct items.10
5.1 Propagators for local constraints
But how long does updating each message take? The
runtime of summing over all assignments
?
A in
10For example, with unknown tags T , a model with
PTREE+TAGLINK will take only O(n3 + n2g2) time for BP,
compared to O(n3g2) time for dynamic programming (Eisner
& Satta 1999). Adding TRIGRAM, which is string-local rather
than tree-local, will increase this only to O(n3 + n2g2 + ng3),
compared to O(n3g6) for dynamic programming.
Even more dramatic, adding the SIB family of O(n3)
PAIRij,ik factors will add only O(n3) to the runtime of BP
(Table 1). By contrast, the runtime of dynamic programming
becomes exponential, because each item must record its head-
word?s full set of current children.
equation (4) may appear prohibitive. Crucially, how-
ever, F (A) only depends on the values in A of F ?s
its neighboring variables N (F ). So this sum is pro-
portional to a sum over restricted assignments to just
those variables.11
For example, computing a message from
TAGLINKij ? Ti only requires iterating over all
(boolean, tag, tag) triples.12 The runtime to update
that message is therefore O(2 ? |T | ? |T |).
5.2 Propagators for global constraints
The above may be tolerable for a ternary factor. But
how about global factors? EXACTLY1j has n neigh-
boring boolean variables: surely we cannot iterate
over all 2n assignments to these! TREE is even
worse, with 2O(n
2) assignments to consider. We will
give specialized algorithms for handling these sum-
mations more efficiently.
A historical note is in order. Traditional constraint
satisfaction corresponds to the special case of (1)
where all factors Fm are hard constraints (with val-
ues in {0, 1}). In that case, loopy BP reduces to
an algorithm for generalized arc consistency (Mack-
worth, 1977; Bessie`re and Re?gin, 1997; Dechter,
2003), and updating a factor?s outgoing messages is
known as constraint propagation. Re?gin (1994)
famously introduced an efficient propagator for
a global constraint, ALLDIFFERENT, by adapting
combinatorial bipartite matching algorithms.
In the same spirit, we will demonstrate efficient
propagators for our global constraints, e.g. by adapt-
ing combinatorial algorithms for weighted parsing.
We are unaware of any previous work on global fac-
tors in sum-product BP, although for max-product
BP,13 Duchi et al (2007) independently showed
that a global 1-to-1 alignment constraint?a kind
of weighted ALLDIFFERENT?permits an efficient
propagator based on weighted bipartite matching.
5.3 Constraint propagators for parsing
Table 1 shows our asymptotic runtimes for all fac-
tors in ??3.3?3.4. Remember that if several of these
11The constant of proportionality may be folded into ?; it is
the number of assignments to the other variables.
12Separately for each value v of Ti, get v?s probability by
summing over assignments to (Lij , Ti, Tj) s.t. Ti = v.
13Max-product replaces the sums in equations (3)?(6) with
maximizations. This replaces the forward-backward algorithm
with its Viterbi approximation.
149
factor degree runtime count runtime
family (each) (each) (total)
TREE O(n2) O(n3) 1 O(n3)
PTREE O(n2) O(n3) 1 O(n3)
EXACTLY1 O(n) O(n) n O(n2)
ATMOST1 O(n) O(n) n O(n2)
NOT2 2 O(1) O(n3) O(n3)
NO2CYCLE 2 O(1) O(n2) O(n2)
LINK 1 O(1) O(n2) O(n2)
GRAND 2 O(1) O(n3) O(n3)
SIB 2 O(1) O(n3) O(n3)
CHILDSEQ O(n) O(n2) O(n) O(n3)
NOCROSS O(n) O(n) O(n2) O(n3)
TAG 1 O(g) O(n) O(ng)
TAGLINK 3 O(g2) O(n2) O(n2g2)
TRIGRAM O(n) O(ng3) 1 O(ng3)
Table 1: Asymptotic runtimes of the propagators for var-
ious factors (where n is the sentence length and g is the
size of the tag set T ). An iteration of standard BP propa-
gates through each factor once. Running a factor?s prop-
agator will update all of its outgoing messages, based on
its current incoming messages.
factors are included, the total runtime is additive.14
Propagating the local factors is straightforward
(?5.1). We now explain how to handle the global
factors. Our main trick is to work backwards from
marginal beliefs. Let F be a factor and V be one
of its neighboring variables. At any time, F has a
marginal belief about V (see footnote 9),
b(k+1)F? (V = v) =
?
A s.t. A[V ]=v
b(k+1)F? (A) (7)
a sum over (6)?s products of incoming messages. By
the definition of rF?V in (4), and distributivity, we
can also express the marginal belief (7) as a point-
wise product of outgoing and incoming messages15
b(k+1)F? (V = v) = r
(k+1)
F?V (v) ? q
(k)
V?F (v) (8)
up to a constant. If we can quickly sum up the
marginal belief (7), then (8) says we can divide out
each particular incoming message q(k)V?F to obtain
its corresponding outgoing message r(k+1)F?V .
14We may ignore the cost of propagators at the variables.
Each outgoing message from a variable can be computed in
time proportional to its size, which may be amortized against
the cost of generating the corresponding incoming message.
15E.g., the familiar product of forward and backward mes-
sages that is used to extract posterior marginals from an HMM.
Note that the marginal belief and both messages
are unnormalized distributions over values v of V .
F and k are clear from context below, so we simplify
the notation so that (7)?(8) become
b(V = v) =
?
A s.t. A[V ]=v
b(A) = rV (v) ? qV (v)
TRIGRAM must sum over assignments to the tag
sequence T . The belief (6) in a given assignment
is a product of trigram scores (which play the role
of transition weights) and incoming messages qTj
(playing the role of emission weights). The marginal
belief (7) needed above, b(Ti = t), is found by sum-
ming over assignments where Ti = t. All marginal
beliefs are computed together in O(ng3) total time
by the forward-backward algorithm.16
EXACTLY1j is a sparse hard constraint. Even
though there are 2n assignments to its n neighboring
variables {Lij}, the factor function returns 1 on only
n assignments and 0 on the rest. In fact, for a given i,
b(Lij = true) in (7) is defined by (6) to have exactly
one non-zero summand, in whichA puts Lij = true
and all other Li?j = false. We compute the marginal
beliefs for all i together in O(n) total time:
1. Pre-compute pi
def
=
?
i qLij (false).
17
2. For each i, compute the marginal belief
b(Lij = true) as pi ? q?Lij , where q?Lij ? R de-
notes the odds ratio qLij (true)/qLij (false).
18
3. The partition function b() denotes
?
A b(A);
compute it in this case as
?
i b(Lij = true).
4. For each i, compute b(Lij = false) by subtrac-
tion, as b()? b(Lij = true).
TREE and PTREE must sum over assignments to
the O(n2) neighboring variables {Lij}. There are
now exponentially many non-zero summands, those
in whichA corresponds to a valid tree. Nonetheless,
16Which is itself an exact BP algorithm, but on a different
graph?a junction tree formed from the graph of TRIGRAM sub-
factors. Each variable in the junction tree is a bigram. If we had
simply replaced the global TRIGRAM factor with its subfactors
in the full factor graph, we would have had to resort to General-
ized BP (Yedidia et al, 2004) to obtain the same exact results.
17But taking pi = 1 gives the same results, up to a constant.
18As a matter of implementation, this odds ratio q?Lij can be
used to represent the incoming message qLij everywhere.
150
we can follow the same approach as for EXACTLY1.
Steps 1 and 4 are modified to iterate over all i, j such
that Lij is a variable. In step 3, the partition function?
A b(A) is now pi times the total weight of all trees,
where the weight of a given tree is the product of the
q?Lij values of its n edges. In step 2, the marginal
belief b(Lij = true) is now pi times the total weight
of all trees having edge i? j.
We perform these combinatorial sums by calling a
first-order parsing algorithm, with edge weights q?ij .
Thus, as outlined in ?2, a first-order parser is called
each time we propagate through the global TREE or
PTREE constraint, using edge weights that include
the first-order LINK factors but also multiply in any
current messages from higher-order factors.
The parsing algorithm simultaneously computes
the partition function b(), and all O(n2) marginal
beliefs b(Lij = true). For PTREE (projective), it
is the inside-outside version of a dynamic program-
ming algorithm (Eisner, 1996). For TREE (non-
projective), Koo et al (2007) and Smith and Smith
(2007) show how to employ the matrix-tree theorem.
In both cases, the total time is O(n3).19
NOCROSSj` must sum over assignments to O(n)
neighboring variables {Lij} and {Lk`}. The non-
zero summands are assignments where j and `
each have exactly one parent. At step 1, pi
def
=
?
i qLij (false) ?
?
k qLk`(false). At step 2, the
marginal belief b(Lij = true) sums over the n non-
zero assignments containing i ? j. It is pi ? q?Lij ??
k q?Lk` ? PAIRij,k`, where PAIRij,k` is xj` if i ? j
crosses k ? ` and is 1 otherwise. xj` is some factor
value defined by equation (2) to penalize or reward
the crossing. Steps 3?4 are just as in EXACTLY1j .
The question is how to compute b(Lij = true) for
each i in only O(1) time,20 so that we can propagate
each of the O(n2) NOCROSSj` in O(n) time. This
is why we allowed xj` to depend only on j, `. We
can rewrite the sum b(Lij = true) as
pi ? q?Lij ? (xj` ?
?
crossing k
q?Lk` + 1 ?
?
noncrossing k
q?Lk`) (9)
19A dynamic algorithm could incrementally update the out-
going messages if only a few incoming messages have changed
(as in asynchronous BP). In the case of TREE, dynamic matrix
inverse allows us to update any row or column (i.e., messages
from all parents or children of a given word) and find the new
inverse in O(n2) time (Sherman and Morrison, 1950).
20Symmetrically, we compute b(Lk` = true) for each k.
To find this in O(1) time, we precompute for each
` an array of partial sums Q`[s, t]
def
=
?
s?k?t q?Lk` .
SinceQ`[s, t] = Q`[s, t?1]+ q?Lt` , we can compute
each entry in O(1) time. The total precomputation
time over all `, s, t is then O(n3), with the array Q`
shared across all factors NOCROSSj?`. The crossing
sum is respectivelyQ`[0, i?1]+Q`[j+1, n],Q`[i+
1, j ? 1], or 0 according to whether ` ? (i, j), ` /?
[i, j], or ` = i.21 The non-crossing sum is Q`[0, n]
minus the crossing sum.
CHILDSEQi , like TRIGRAM, is propagated by a
forward-backward algorithm. In this case, the al-
gorithm is easiest to describe by replacing CHILD-
SEQi in the factor graph by a collection of local
subfactors, which pass messages in the ordinary
way.22 Roughly speaking,23 at each j ? [1, n],
we introduce a new variable Cij?a hidden state
whose value is the position of i?s previous child,
if any (so 0 ? Cij < j). So the ternary sub-
factor on (Cij , Lij , Ci,j+1) has value 1 if Lij =
false and Ci,j+1 = Ci,j ; a sibling-bigram score
(PAIRiCij ,iCi,j+1) if Lij = true and Ci,j+1 = j; and
0 otherwise. The sparsity of this factor, which is 0
almost everywhere, is what gives CHILDSEQi a total
runtime of O(n2) rather than O(n3). It is equivalent
to forward-backward on an HMM with n observa-
tions (the Lij) and n states per observation (the Cj),
with a deterministic (thus sparse) transition function.
6 Decoding Trees
BP computes local beliefs, e.g. the conditional prob-
ability that a link Lij is present. But if we wish
to output a single well-formed dependency tree, we
need to find a single assignment to all the {Lij} that
satisfies the TREE (or PTREE) constraint.
Our final belief about the TREE factor is a distri-
bution over such assignments, in which a tree?s prob-
ability is proportional to the probability of its edge
weights q?Lij (incoming messages). We could simply
return the mode of this distribution (found by using
a 1-best first-order parser) or the k-best trees, or take
samples.
21There are no NOCROSSj` factors with ` = j.
22We still treat CHILDSEQi as a global factor and compute all
its correct outgoing messages on a single BP iteration, via serial
forward and backward sweeps through the subfactors. Handling
the subfactors in parallel, (3)?(4), would need O(n) iterations.
23Ignoring the treatment of boundary symbols ?#? (see ?3.4).
151
In our experiments, we actually take the edge
weights to be not the messages q?Lij from the links,
but the full beliefs b?Lij at the links (where b?Lij
def
=
log bLij (true)/bLij (false)). These are passed into a
fast algorithm for maximum spanning tree (Tarjan,
1977) or maximum projective spanning tree (Eis-
ner, 1996). This procedure is equivalent to minimum
Bayes risk (MBR) parsing (Goodman, 1996) with a
dependency accuracy loss function.
Notice that the above decoding approaches do not
enforce any hard constraints other than TREE in the
final output. In addition, they only recover values
of the Lij variables. They marginalize over other
variables such as tags and link roles. This solves
the problem of ?nuisance? variables (which merely
fragment probability mass among refinements of a
parse). On the other hand, it may be undesirable for
variables whose values we desire to recover.24
7 Training
Our training method also uses beliefs computed by
BP, but at the factors. We choose the weight vector
? by maximizing the log-probability of training data
24An alternative is to attempt to find the most probable
(?MAP?) assignment to all variables?using the max-product
algorithm (footnote 13) or one of its recent variants. The esti-
mated marginal beliefs become ?max marginals,? which assess
the 1-best assignment consistent with each value of the variable.
We can indeed build max-product propagators for our global
constraints. PTREE still propagates in O(n3) time: simply
change the first-order parser?s semiring (Goodman, 1999) to use
max instead of sum. TREE requires O(n4) time: it seems that
the O(n2) max marginals must be computed separately, each
requiring a separate call to an O(n2) maximum spanning tree
algorithm (Tarjan, 1977).
If max-product BP converges, we may simply output each
variable?s favorite value (according to its belief), if unique.
However, max-product BP tends to be unstable on loopy graphs,
and we may not wish to wait for full convergence in any case. A
more robust technique for extracting an assignment is to mimic
Viterbi decoding, and ?follow backpointers? of the max-product
computation along some spanning subtree of the factor graph.
A slower but potentially more stable alternative is determin-
istic annealing. Replace each factor Fm(A) with Fm(A)1/T ,
where T > 0 is a temperature. As T ? 0 (?quenches?), the
distribution (1) retains the same mode (the MAP assignment),
but becomes more sharply peaked at the mode, and sum-product
BP approaches max-product BP. Deterministic annealing runs
sum-product BP while gradually reducing T toward 0 as it it-
erates. By starting at a high T and reducing T slowly, it often
manages in practice to find a good local optimum. We may then
extract an assignment just as we do for max-product.
under equation (1), regularizing only by early stop-
ping. If all variables are observed in training, this
objective function is convex (as for any log-linear
model).
The difficult step in computing the gradient of
our objective is finding ?? logZ, where Z in equa-
tion (1) is the normalizing constant (partition func-
tion) that sums over all assignments A. (Recall that
Z, like each Fm, depends implicitly on W and ?.)
As usual for log-linear models,
?? logZ =
?
m
Ep(A)[??Fm(A)] (10)
Since ??Fm(A) only depends on the assignment
A?s values for variables that are connected to Fm
in the factor graph, its expectation under p(A) de-
pends only on the marginalization of p(A) to those
variables jointly. Fortunately, BP provides an esti-
mate of that marginal distribution, namely, its belief
about the factor Fm, given W and ? (?4.2).25
Note that the hard constraints do not depend on ?
at all; so their summands in equation (10) will be 0.
We employ stochastic gradient descent (Bottou,
2003), since this does not require us to compute
the objective function itself but only to (approxi-
mately) estimate its gradient as explained above. Al-
ternatively, given any of the MAP decoding proce-
dures from ?6, we could use an error-driven learning
method such as the perceptron or MIRA.26
8 Experiments
We asked: (1) For projective parsing, where higher-
order factors have traditionally been incorporated
into slow but exact dynamic programming (DP),
what are the comparative speed and quality of the
BP approximation? (2) How helpful are such higher-
order factors?particularly for non-projective pars-
ing, where BP is needed to make them tractable?
(3) Do our global constraints (e.g., TREE) contribute
to the goodness of BP?s approximation?
25One could use coarser estimates at earlier stages of training,
by running fewer iterations of BP.
26The BP framework makes it tempting to extend an MRF
model with various sorts of latent variables, whose values are
not specified in training data. It is straightforward to train under
these conditions. When counting which features fire on a train-
ing parse or (for error-driven training) on an current erroneous
parse, we can find expected counts if these parses are not fully
observed, by using BP to sum over latent variables.
152
0 10 20 30 40 50 60 70
0
20
40
60
Sentence length
Av
era
ge
 pa
rsi
ng
 tim
e i
n s
ec
on
ds
MBR by DP
Viterbi DP
2 iterations of BP
3 iterations of BP
5 iterations of BP
10 iterations of BP
Figure 2: Runtime of BP parser on various sentence
lengths compared to O(n4) dynamic programming.
8.1 Data
We trained and tested on three languages from the
CoNLL Dependency Parsing Shared Task (Nivre et
al., 2007). The English data for that task were
converted from the Penn Treebank to dependen-
cies using a trace-recovery algorithm that induced
some very slight non-projectivity?about 1% of
links crossed other links. Danish is a slightly more
non-projective language (3% crossing links). Dutch
is the most non-projective language in the corpus
(11%). In all cases, the test input W consists of
part-of-speech-tagged words, so T variables were
not used.
8.2 Features
Although BP makes it cheap to incorporate many
non-local features and latent variables at once, we
kept our models relatively simple in this paper.
Our first-order LINKij factors replicate McDon-
ald et al (2005). Following equation (2), they are
defined using binary features that look at words i
and j, the distance j ? i, and the tags (provided in
W ) of words at, around, and between i and j.
Our second-order features are similar. In the
GRAND factors, features fire for particular triples
of tags and of coarse tags. A feature also fires if
the grandparent falls between the child and parent,
inducing crossing dependency links. The CHILD-
SEQ factors included features for tags, and like-
wise coarse tags, on adjacent sibling pairs and
0 10 20 30 40
0
20
40
60
80
10
0
12
0
14
0
Sentence length
Av
era
ge
 pa
rsi
ng
 tim
e i
n s
ec
on
ds
MBR by DP
Viterbi DP
2 iterations of BP
3 iterations of BP
5 iterations of BP
10 iterations of BP
Figure 3: Runtime of BP parser on various sentence
lengths compared to O(n5) dynamic programming. DP
is so slow for length > 45 that we do not even show it.
parent-sibling-sibling triples. Each of these fea-
tures also have versions that were conjoined with
link direction?pairs of directions in the grandpar-
ent case?or with signed link length of the child or
farther sibling. Lengths were binned per McDonald
et al (2005). The NOCROSSj` factors consider the
tag and coarse tag attributes of the two child words
j and `, separately or jointly.
8.3 Experimental procedures
We trained all models using stochastic gradient de-
scent (?7). SGD initialized ~? = 0 and ran for 10 con-
secutive passes over the data; we picked the stopping
point that performed best on held-out data.
When comparing runtimes for projective parsers,
we took care to produce comparable implementa-
tions. All beliefs and dynamic programming items
were stored and indexed using the high-level Dyna
language,27 while all inference and propagation was
written in C++. The BP parser averaged 1.8 seconds
per sentence for non-projective parsing and 1.5 sec-
onds per sentence for projective parsing (1.2 and 0.9
seconds/sentence for ? 40 words), using our stan-
dard setup, which included five iterations of BP and
the final MBR tree decoding pass.
In our tables, we boldface the best result in each
column along with any results that are not signifi-
cantly worse (paired permutation test, p < .05).
27This dominates runtime, and probably slows down all our
parsers by a factor of 4?11 owing to known inefficiencies in the
Dyna prototype we used (Eisner et al, 2005).
153
0 20 40 60
0.0
0.1
0.2
0.3
0.4
Parsing time in seconds
Err
or 
rel
ati
ve
 to
 ex
ac
t M
BR
Input length
40 words
50 words
60 words
70 words
2 iterations of BP
3 iterations of BP
5 iterations of BP
10 iterations of BP
MBR DP
Figure 4: Runtime vs. search error after different num-
bers of BP iterations. This shows the simpler model of
Fig. 2, where DP is still relatively fast.
8.4 Faster higher-order projective parsing
We built a first-order projective parser?one that
uses only factors PTREE and LINK?and then com-
pared the cost of incorporating second-order factors,
GRAND and CHILDSEQ, by BP versus DP.28
Under DP, the first-order runtime of O(n3) is in-
creased to O(n4) with GRAND, and to O(n5) when
we add CHILDSEQ as well. BP keeps runtime down
to O(n3)?although with a higher constant factor,
since it takes several rounds to converge, and since
it computes more than just the best parse.29
Figures 2?3 compare the empirical runtimes for
various input sentence lengths. With only the
GRAND factor, exact DP can still find the Viterbi
parse (though not the MBR parse29) faster than ten
iterations of the asymptotically better BP (Fig. 2),
at least for sentences with n ? 75. However, once
we add the CHILDSEQ factor, BP is always faster?
dramatically so for longer sentences (Fig. 3). More
complex models would widen BP?s advantage.
Fig. 4 shows the tradeoff between runtime and
search error of BP in the former case (GRAND only).
To determine BP?s search error at finding the MBR
parse, we measured its dependency accuracy not
28We trained these parsers using exact DP, using the inside-
outside algorithm to compute equation (10). The training and
test data were English, and for this section we filtered out sen-
tences with non-projective links.
29Viterbi parsing in the log domain only needs the (max,+)
semiring, whereas both BP and any MBR parsing must use the
slower (+, log+) so that they can compute marginals.
Danish Dutch English
(a) TREE+LINK 85.5 87.3 88.6
+NOCROSS 86.1 88.3 89.1
+GRAND 86.1 88.6 89.4
+CHILDSEQ 86.5 88.5 90.1
(b) Proj. DP 86.0 84.5 90.2
+hill-climbing 86.1 87.6 90.2
Table 2: (a) Percent unlabeled dependency accuracy for
various non-projective BP parsers (5 iterations only),
showing the cumulative contribution of different features.
(b) Accuracy for an projective DP parser with all features.
For relatively non-projective languages (Danish and espe-
cially Dutch), the exact projective parses can be improved
by non-projective hill-climbing?but in those cases, just
running our non-projective BP is better and faster.
against the gold standard, but against the optimal
MBR parse under the model, which DP is able to
find. After 10 iterations, the overall macro-averaged
search error compared to O(n4) DP MBR is 0.4%;
compared to O(n5) (not shown), 2.4%. More BP
iterations may help accuracy. In future work, we
plan to compare BP?s speed-accuracy curve on more
complex projective models with the speed-accuracy
curve of pruned or reranked DP.
8.5 Higher-order non-projective parsing
The BP approximation can be used to improve
the accuracy of non-projective parsing by adding
higher-order features. These would be NP-hard to
incorporate exactly; DP cannot be used.
We used BP with a non-projective TREE factor
to train conditional log-linear parsing models of two
highly non-projective languages, Danish and Dutch,
as well as slightly non-projective English (?8.1).
In all three languages, the first-order non-projective
parser greatly overpredicts the number of crossing
links. We thus added NOCROSS factors, as well
as GRAND and CHILDSEQ as before. All of these
significantly improve the first-order baseline, though
not necessarily cumulatively (Table 2).
Finally, Table 2 compares loopy BP to a previ-
ously proposed ?hill-climbing? method for approx-
imate inference in non-projective parsing McDon-
ald and Pereira (2006). Hill-climbing decodes our
richest non-projective model by finding the best pro-
jective parse under that model?using slow, higher-
order DP?and then greedily modifies words? par-
ents until the parse score (1) stops improving.
154
Decoding Danish Dutch English
NOT2 81.8 (76.7) 83.3 (75.0) 87.5 (66.4)
ATMOST1 85.4 (82.2) 87.3 (86.3) 88.5 (84.6)
EXACTLY1 85.7 (85.0) 87.0 (86.7) 88.6 (86.0)
+ NO2CYCLE 85.0 (85.2) 86.2 (86.7) 88.5 (86.2)
TREE 85.5 (85.5) 87.3 (87.3) 88.6 (88.6)
PTREE 85.8 83.9 88.8
Table 3: After training a non-projective first-order model
with TREE, decoding it with weaker constraints is asymp-
totically faster (except for NOT2) but usually harm-
ful. (Parenthetical numbers show that the harm is com-
pounded if the weaker constraints are used in training
as well; even though this matches training to test con-
ditions, it may suffer more from BP?s approximate gradi-
ents.) Decoding the TREE model with the even stronger
PTREE constraint can actually be helpful for a more pro-
jective language. All results use 5 iterations of BP.
BP for non-projective languages is much faster
and more accurate than the hill-climbing method.
Also, hill-climbing only produces an (approximate)
1-best parse, but BP also obtains (approximate)
marginals of the distribution over all parses.
8.6 Importance of global hard constraints
Given the BP architecture, do we even need the hard
TREE constraint? Or would it suffice for more local
hard constraints to negotiate locally via BP?
We investigated this for non-projective first-order
parsing. Table 3 shows that global constraints are
indeed important, and that it is essential to use TREE
during training. At test time, the weaker but still
global EXACTLY1 may suffice (followed by MBR
decoding to eliminate cycles), for total time O(n2).
Table 3 includes NOT2, which takes O(n3) time,
merely to demonstrate how the BP approximation
becomes more accurate for training and decoding
when we join the simple NOT2 constraints into more
global ATMOST1 constraints. This does not change
the distribution (1), but makes BP enforce stronger
local consistency requirements at the factors, rely-
ing less on independence assumptions. In general,
one can get better BP approximations by replacing a
group of factors Fm(A) with their product.30
The above experiments concern gold-standard
30In the limit, one could replace the product (1) with a sin-
gle all-purpose factor; then BP would be exact?but slow. (In
constraint satisfaction, joining constraints similarly makes arc
consistency slower but better at eliminating impossible values.)
accuracy under a given first-order, non-projective
model. Flipping all three of these parameters for
Danish, we confirmed the pattern by instead mea-
suring search error under a higher-order, projective
model (PTREE+LINK+GRAND), when PTREE was
weakened during decoding. Compared to the MBR
parse under that model, the search errors from de-
coding with weaker hard constraints were 2.2% for
NOT2, 2.1% for EXACTLY1, 1.7% for EXACTLY1
+ NO2CYCLE, and 0.0% for PTREE.
9 Conclusions and Future Work
Belief propagation improves non-projective depen-
dency parsing with features that would make ex-
act inference intractable. For projective parsing, it
is significantly faster than exact dynamic program-
ming, at the cost of small amounts of search error,
We are interested in extending these ideas to
phrase-structure and lattice parsing, and in try-
ing other higher-order features, such as those used
in parse reranking (Charniak and Johnson, 2005;
Huang, 2008) and history-based parsing (Nivre and
McDonald, 2008). We could also introduce new
variables, e.g., nonterminal refinements (Matsuzaki
et al, 2005), or secondary linksMij (not constrained
by TREE/PTREE) that augment the parse with repre-
sentations of control, binding, etc. (Sleator and Tem-
perley, 1993; Buch-Kromann, 2006).
Other parsing-like problems that could be at-
tacked with BP appear in syntax-based machine
translation. Decoding is very expensive with a syn-
chronous grammar composed with an n-gram lan-
guage model (Chiang, 2007)?but our footnote 10
suggests that BP might incorporate a language
model rapidly. String alignment with synchronous
grammars is quite expensive even for simple syn-
chronous formalisms like ITG (Wu, 1997)?but
Duchi et al (2007) show how to incorporate bipar-
tite matching into max-product BP.
Finally, we can take advantage of improvements
to BP proposed in the context of other applications.
For example, instead of updating all messages in
parallel at every iteration, it is empirically faster to
serialize updates using a priority queue (Elidan et
al., 2006; Sutton and McCallum, 2007).31
31These methods need alteration to handle our global propa-
gators, which do update all their outgoing messages at once.
155
References
C. Bessie`re and J.-C. Re?gin. 1997. Arc consistency for
general constraint networks: preliminary results. In
IJCAI, pages 398?404.
L. Bottou. 2003. Stochastic learning. In Advanced Lec-
tures in Machine Learning, pages 146?168. Springer.
A. Braunstein, M. Mezard, and R. Zecchina. 2005. Sur-
vey propagation: An algorithm for satisfiability. Ran-
dom Structures and Algorithms, 27:201?226.
M. Buch-Kromann. 2006. Discontinuous Grammar.
A Model of Human Parsing and Language Acquisi-
tion?. Dr.ling.merc. dissertation, Copenhagen Busi-
ness School.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
ACL, pages 173?180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
R. Dechter. 2003. Constraint Processing. Morgan Kauf-
mann.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. In NIPS 2006, pages 369?376.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In ACL, pages 457?480.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compiling
comp ling: Weighted dynamic programming and the
dyna language. In HLT-EMNLP, pages 281?290.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In COLING.
G. Elidan, I. McGraw, and D. Koller. 2006. Resid-
ual belief propagation: Informed scheduling for asyn-
chronous message passing. In UAI.
J. T. Goodman. 1996. Parsing algorithms and metrics.
In ACL, pages 177?183.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25(4):573?605.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In ACL, pages 586?594.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the Matrix-Tree The-
orem. In EMNLP-CoNLL.
D. MacKay. 2003. Information Theory, Inference, and
Learning Algorithms. Cambridge.
A. Mackworth. 1977. Consistency in networks of rela-
tions. Artificial Intelligence, 8(1):99?118.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL, pages 75?82.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL.
J.-C. Re?gin. 1994. A filtering algorithm for constraints
of difference in csps. In AAAI, pages 362?367.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP, pages 129?137.
J. Sherman and W. J. Morrison. 1950. Adjustment of an
inverse matrix corresponding to a change in one ele-
ment of a given matrix. Ann. Math. Stat., 21:124?127.
D. Sleator and D. Temperley. 1993. Parsing English with
a link grammar. In IWPT, pages 277?291, August.
D. A. Smith and N. A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In EMNLP-
CoNLL.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information ex-
traction. In ICML Workshop on Statistical Relational
Learning.
C. Sutton and A. McCallum. 2007. Improved dynamic
schedules for belief propagation. In UAI.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In ICML.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7:25?35.
R. W. Tromble and J. Eisner. 2006. A fast finite-state
relaxation method for enforcing global constraints on
sequence decoding. In HLT-NAACL, pages 423?430.
Y. Weiss and W. T. Freedman. 2001. On the optimal-
ity of solutions of the max-product belief propagation
algorithm in arbitrary graphs. IEEE Transactions on
Information Theory, 47.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. CL,
23(3):377?404.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2004. Con-
structing free-energy approximations and generalized
belief approximation algorithms. MERL TR2004-
040, Mitsubishi Electric Research Laboratories.
156
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 475?482, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Context-Based Morphological Disambiguation with Random Fields?
Noah A. Smith and David A. Smith and Roy W. Tromble
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,dasmith,royt}@cs.jhu.edu
Abstract
Finite-state approaches have been highly successful at describ-
ing the morphological processes of many languages. Such
approaches have largely focused on modeling the phone- or
character-level processes that generate candidate lexical types,
rather than tokens in context. For the full analysis of words
in context, disambiguation is also required (Hakkani-Tu?r et al,
2000; Hajic? et al, 2001). In this paper, we apply a novel
source-channel model to the problem of morphological disam-
biguation (segmentation into morphemes, lemmatization, and
POS tagging) for concatenative, templatic, and inflectional lan-
guages. The channel model exploits an existing morphological
dictionary, constraining each word?s analysis to be linguistically
valid. The source model is a factored, conditionally-estimated
random field (Lafferty et al, 2001) that learns to disambiguate
the full sentence by modeling local contexts. Compared with
baseline state-of-the-art methods, our method achieves statisti-
cally significant error rate reductions on Korean, Arabic, and
Czech, for various training set sizes and accuracy measures.
1 Introduction
One of the great successes in computational linguistics
has been the construction of morphological analyzers for
diverse languages. Such tools take in words and enu-
merate the possible morphological analyses?typically a
sequence of morphemes, perhaps part-of-speech tagged.
They are often encoded as finite-state transducers (Ka-
plan and Kay, 1981; Koskenniemi, 1983; Beesley and
Karttunen, 2003).
What such tools do not provide is a means to dis-
ambiguate a word in context. For languages with com-
plex morphological systems (inflective, agglutinative,
and polysynthetic languages, for example), a word form
may have many analyses. To pick the right one, we
must consider the word?s context. This problem has
been tackled using statistical sequence models for Turk-
ish (Hakkani-Tu?r et al, 2000) and Czech (Hajic? et al,
2001); their approaches (and ours) are not unlike POS
tagging, albeit with complex tags.
?This work was supported by a Fannie and John Hertz
Foundation Fellowship, a NSF Fellowship, and a NDSEG Fel-
lowship (sponsored by ARO and DOD). The views expressed
are not necessarily endorsed by sponsors. We thank Eric Gold-
lust and Markus Dreyer for Dyna language support and Jason
Eisner, David Yarowsky, and three anonymous reviewers for
comments that improved the paper. We also thank Jan Hajic?
and Pavel Krbec for sharing their Czech tagger.
In this paper, we describe context-based models for
morphological disambiguation that take full account of
existing morphological dictionaries by estimating condi-
tionally against only dictionary-accepted analyses of a
sentence (?2). These models are an instance of condi-
tional random fields (CRFs; Lafferty et al, 2001) and
include overlapping features. Our applications include
diverse disambiguation frameworks and we make use of
linguistically-inspired features, such as local lemma de-
pendencies and inflectional agreement. We apply our
model to Korean and Arabic, demonstrating state-of-the-
art results in both cases (?3). We then describe how our
model can be expanded to complex, structured morpho-
logical tagging, including an efficient estimation method,
demonstrating performance on Czech (?4).
2 Modeling Framework
Our framework is a source-channel model (Jelinek,
1976). The source (modeled probabilistically by ps) gen-
erates a sequence of unambiguous tagged morphemes
y = ?y1, y2, ...? ? Y+ (Y is the set of unambiguous
tagged morphemes in the language).1 The precise con-
tents of the tag will vary by language and corpus but
will minimally include POS. y passes through a chan-
nel (modeled by pc), which outputs x = ?x1, x2, ...? ?
(X ? {OOV})+, a sequence of surface-level words in the
language and out-of-vocabulary words (OOV; X is the
language?s vocabulary). Note that |x| may be smaller
than |y|, since some morphemes may combine to make
a word. We will denote by yi the contiguous subse-
quence of y that generates xi; ~y will refer to a dictionary-
recognized type in Y+.
At test time, we decode the observed x into the most
probable sequence of tag/morpheme pairs:
y? = argmax
y
p(y | x) = argmax
y
ps(y) ? pc(x | y) (1)
Training involves constructing ps and pc. We assume
that there exists a training corpus of text (each word xi
annotated with its correct analysis y?i ) and a morpholog-
ical dictionary. We next describe the channel model and
the source model.
1The sequence also includes segmentation markings be-
tween words, not shown to preserve clarity.
475
a. There are many kinds of trench mortars.
b. . ? 1998 1998?Sanaa accuses Riyadh of occupying border territories.
0 1NUM/1998 2PUNC/- 3NOUN_PROP/SnEA?NOUN_PROP/SanoEA? 4
IV3FS/tuIV2MS/tu
5IV3FS/taIV2MS/ta 6NOUN_PROP/tthm
IV_PASS/t~ahamIV/t~ahim 7 8 9 10 11 12 13 14
c. Klimatizovana? j??delna, sve?tla? m??stnost pro sn??dane?. Air-conditioned dining room, well-lit breakfast room.
0 1
Adj {Neu Pl Acc Pos Aff}/klimatizovan?Adj {Neu Pl Voc Pos Aff}/klimatizovan?Adj {Fem Si Voc Pos Aff}/klimatizovan?Adj {Fem Si Nom Pos Aff}/klimatizovan?Adj {Neu Pl Nom Pos Aff}/klimatizovan? 2Noun {Fem Si Nom Aff}/j?delna 3Punc/, 4
Adj {Neu Pl Pos Aff}/svetl?Adj {Fem Si Voc Pos Aff}/svetl?Adj {Neu Pl Acc Pos Aff}/svetl?Adj {Neu Pl Voc Pos Aff}/svetl?Adj {Fem Si Nom Pos Aff}/svetl? 5Noun {Fem Si Acc Aff}/m?stnostNoun {Fem Si Nom Aff}/m?stnost 6 7 8
Figure 1: Lattices for example sentences in Korean (a), Arabic (b), and Czech (c). Arabic lemmas are not shown, and some Arabic
and Czech arcs are unlabeled, for readability. The Arabic morphemes are shown in Buckwalter?s encoding. The arcs in the correct
path through each lattice are solid (incorrect arcs are dashed). Note the adjective-noun agreement in the correct path through the
Czech lattice (c). The Czech lattice has no lemma-ambiguity; this is typical in Czech (see ?4).
2.1 Morphological dictionaries and the channel
A great deal of research has gone into developing mor-
phological analysis tools that enumerate valid analyses
~y ? Y+ for a particular word x ? X. Typically these
tools are unweighted and therefore do not enable token
disambiguation.2
They are available for many languages. We will refer
to this source of categorial lexical information as a mor-
phological dictionary d that maps X ? 2Y+ . The set d(x)
is the set of analyses for word x; the set d(x) is the set of
whole-sentence analyses for sentence x = ?x1, x2, ...?.
d(x) can be represented as an acyclic lattice with a
?sausage? shape familiar from work in speech recogni-
tion (Mangu et al, 1999). Note that for languages with
bound morphemes, d(x) will consist of a set of sequences
of tokens, so a given ?link? in the sausage lattice may
contain paths of different lengths. Fig. 1 shows sausage
lattices for sentences in three languages.
In this paper, the dictionary defines the support set of
the channel model. That is, pc(x | y) > 0 if and only
if y ? d(x). This is a clean way to incorporate do-
main knowledge into the probabilistic model; this kind
of constraint has been applied in previous work at decod-
ing time (Hakkani-Tu?r et al, 2000; Hajic? et al, 2001). In
such a model, each word is independent of its neighbors
(because the dictionary ignores context).
Estimation. A unigram channel model defines
2Probabilistic modeling of what we call the morphologi-
cal channel was first carried out by Levinger et al (1995), who
used unlabeled data to estimate p(~y | x) for Hebrew, with the
support defined by a dictionary.
pc(x | y)
def
=
|x|?
i=1
p(xi | yi) (2)
The simplest estimate of this model is to make p(?, ?)
uniform over (x, ~y) such that ~y ? d(x). Doing so and
marginalizing to get p(x | ~y) makes the channel model
encode categorial information only, leaving all learning
to the source model.3
Another way to estimate this model is, of course,
from data. This is troublesome, because?modulo
optionality?x is expected to be known given ~y, result-
ing in a huge model with mostly 1-valued probabili-
ties. Our solution is to take a projection pi of ~y and let
p(? | ~y) ? p(? | pi(~y)). In this paper, pi maps the analysis
to its morphological tag (or tag sequence). We will refer
to this as the ?tag channel.?
OOV. Morphological dictionaries typically do not have
complete coverage of a language. We can augment them
in two ways using the training data. If a known word x
(one for which d(x) is non-empty) appears in the training
dataset with an analysis not in d(x), we add the entry to
the dictionary. Unknown words (those not recognized by
the dictionary) are replaced by an OOV symbol. d(OOV)
is taken to be the set of all analyses for any OOV word
seen in training. Rather than attempt to recover the mor-
pheme sequence for an OOV word, in this paper we try
only for the tag sequence, replacing all of an OOV?s mor-
phemes with the OOV symbol. Since OOV symbols ac-
count for less than 2% of words in our corpora, we leave
3Note that this makes the channel term in Eq. 1 a constant.
Then decoding means maximizing ps(y) over y ? d(x), equiv-
alently maximizing p(y | d(x)).
476
more sophisticated channel models to future work.
2.2 The source model
The source model ps defines a probability distribution
over Y+, sequences of (tag, morpheme) pairs. Our source
models can be viewed as weighted multi-tape finite-state
automata, where the weights are associated with local, of-
ten overlapping features of the path through the automa-
ton.
Estimation. We estimate the source conditionally from
annotated data. That is, we maximize
?
(x,y)?X+?Y+
p?(x,y) log ps
(
y | d(x), ~?
)
(3)
where p?(?, ?) is the empirical distribution defined by the
training data and ~? are the model parameters. In terms
of Fig. 1, our learner maximizes the weight of the correct
(solid) path through each lattice, at the expense of the
other incorrect (dashed) paths. Note that
log ps
(
y | d(x), ~?
)
= log
ps
(
y | ~?
)
?
y??d(x) ps
(
y? | ~?
) (4)
The sum in the denominator is computed using a dynamic
programming algorithm (akin to the forward algorithm);
it involves computing the sum of all paths through the
?sausage? lattice of possible analyses for x. By doing
this, we allow knowledge of the support of the channel
model to enter into our estimation of the source model. It
is important to note that the estimation of the model (the
objective function used in training, Eq. 3) is distinct from
the source-channel structure of the model (Eq. 1).
The lattice-conditional estimation approach was
first used by Kudo et al (2004) for Japanese seg-
mentation and hierarchical POS-tagging and by
Smith and Smith (2004) for Korean morphological
disambiguation. The resulting model is an instance of
a conditional random field (CRF; Lafferty et al, 2001).
When training a CRF for POS tagging, IOB chunking
(Sha and Pereira, 2003), or word segmentation (Peng
et al, 2004), one typically structures the conditional
probabilities (in the objective function) using domain
knowledge: in POS tagging, the set of allowed tags for
a word is used; in IOB chunking, the bigram ?O I? is
disallowed; and in segmentation, a lexicon is used to
enumerate the possible word boundaries.4
4This refinement is in the same vein as the move from max-
imum likelihood estimation to conditional estimation. MLE
would make the sum in the denominator of Eq. 4 Y+, which
for log-linear models is often intractable to compute (and for
sequence models may not converge). Conditional estimation
limits the sum to the subset of Y+ that is consistent with x, and
our variant further stipulates consistency with the dictionary en-
tries for x.
Our approach is the same, with two modifications.
First, we model the relationship between labels yi and
words xi in a separately-estimated channel model (?2.1).
Second, our labels are complex. Each word xi is tagged
with a sequence of one or more tagged morphemes; the
tags may include multiple fields. This leads to models
with more parameters. It also makes the dictionary es-
pecially important for limiting the size of the sum in the
denominator, since a complex label set Y could in prin-
ciple lead to a huge hypothesis space for a given sen-
tence x. Importantly, it makes training conditions more
closely match testing conditions, ruling out hypotheses a
dictionary-aware decoder would never consider.
Optimization. The objective function (Eq. 3) is con-
cave and known to have a unique global maximum. Be-
cause log-linear models and CRFs have been widely de-
scribed elsewhere (e.g., Lafferty, 2001), we note only that
we apply a standard first-order numerical optimization
method (L-BFGS; Liu and Nocedal, 1989). The struc-
ture, features, and regularization of our models will be
described in ?3 and ?4.
Prior work (morphological source models).
Hakkani-Tu?r et al (2000) described a system for Turkish
that was essentially a source model; Hajic? et al (2001)
described an HMM-based system for Czech that could
be viewed as a combined source and channel. Both
used dictionaries and estimated their (generative) models
using maximum likelihood (with smoothing).5 Given
enough data, a ML-estimated model will learn to recog-
nize a good path y, but it may not learn to discriminate
a good y from wrong alternatives per se. The generative
framework is limiting as well, disallowing the straight-
forward inclusion of arbitrary overlapping features. We
present a competitive Czech model in ?4.
3 Concatenative Models
The beauty of log-linear models is that estimation is
straightforward given any features, even ones that are
not orthogonal (i.e., ?overlap?). This permits focusing
on feature (or feature template) selection without worries
about the mathematics of training.
We consider two languages modeled by concatenative
processes with surface changes at morpheme boundaries:
Korean and Arabic.
Our model includes features for tag n-grams, mor-
pheme n-grams, and pairs of the two (possibly of differ-
ent lengths and offsets). Fig. 2 illustrates TM3, our base
model. TM3 includes feature templates for some tuples
of three or fewer elements, plus begin and end templates.
5Hajic? et al also included a rule-based system for pruning
hypotheses, which gave slight performance gains.
477
i?1
i?1
T
M
Tn
Mn
?2iT
?2iMM1
1T Ti
Mi
morpheme trigram
tag trigram
begin
features
end
features
tag/morpheme pair
tag + prev. morpheme
tag bigram
morpheme unigram
Figure 2: The base two-level trigram source model, TM3. Each
polygon corresponds to a feature template. This is a two level,
second-order Markov model (weighted finite-state machine) pa-
rameterized with overlapping features. Note that only some fea-
tures are labeled in the diagram.
A variant, TM3H, includes all of the same templates,
plus a similar set of templates that look only at head mor-
phemes. For instance, a feature fires for each trigram
of heads, even though there are (bound) morphemes be-
tween them. This increases the domain of locality for se-
mantic content-bearing morphemes. This model requires
slight changes to the dynamic programming algorithms
for inference and training (the previous two heads must
be remembered at each state).
Every instantiation of the templates seen in any lattice
d(x) built from training data is included in the model, not
just those seen in correct analyses y?.6
3.1 Experimental design
In all of our experiments, we vary the training set size
and the amount of smoothing, which is enforced by a di-
agonal Gaussian prior (L2 regularizer) with variance ?2.
The ?2 = ? case is equivalent to not smoothing. We
compare performance to the expected performance of a
randomized baseline that picks for each word token x an
analysis from d(x); this gives a measure of the amount of
ambiguity and is denoted ?channel only.? Performance
of unigram, bigram, and trigram HMMs estimated us-
ing maximum likelihood (barely smoothed, using add-
10?14) is also reported. (The unigram HMM simply
picks the most likely ~y for each x, based on training data
and is so marked.)
In the experiments in this section, we report three per-
formance measures. Tagging accuracy is the fraction
of words whose tag sequence was correctly identified
in entirety; morpheme accuracy is defined analogously.
6If we used only features observed to occur in y?, we would
not be able to learn negative weights for unlikely bits of structure
seen in the lattice d(x) but not in y?.
Lemma accuracy is the fraction of words whose lemma
was correctly identified.
3.2 Korean experiments
We applied TM3 and TM3H to Korean. The dataset is
the Korean Treebank (Han et al, 2002), with up to 90%
used for training and 10% (5K words) for test. The mor-
phological dictionary is klex (Han, 2004). There are 27
POS tags in the tag set; the corpus contains 10K word
types and 3,272 morpheme types. There are 1.7 mor-
phemes per word token on average (? = 0.75). A Ko-
rean word generally consists of a head morpheme with a
series of enclitic suffixes. In training the head-augmented
model TM3H, we assume the first morpheme of every
word is the head and lemma.
Results are shown in Tab. 1. TM3H achieved very slight
gains over TM3, and the tag channel model was helpful
only with the smaller training set. The oracle (last line
of Tab. 1) demonstrates that the coverage of the dictio-
nary remains an obstacle, particularly for recovering mor-
phemes. Another limitation is the small amount of train-
ing data, which may be masking differences among esti-
mation conditions. We report the performance of TM3H
with ?factored? estimation. This will be discussed in
detail in ?4; it means that a model containing only the
head features was trained on its own, then combined with
the independently trained TM3 model at test time. Fac-
tored training was slightly faster and did not affect per-
formance at all; accuracy scores were identical with un-
factored training.
Prior work (Korean). Similar results were presented
by Smith and Smith (2004), using a similar estimation
strategy with a model that included far more feature tem-
plates. TM3 has about a third as many parameters and
TM3H about half; performance is roughly the same (num-
bers omitted for space). Korean disambiguation results
were also reported by Cha et al (1998), who applied a
deterministic morpheme pattern dictionary to segment
words, then used a bigram HMM tagger. They also ap-
plied transformation-based learning to fix common er-
rors. Due to differences in tag set and data, we cannot
compare to that model; a bigram baseline is included.
3.3 Arabic experiments
We applied TM3 and TM3H to Arabic. The dataset is the
Arabic Treebank (Maamouri et al, 2003), with up to 90%
used for training and 10% (13K words) for test. The mor-
phological dictionary is Buckwalter?s analyzer (version
2), made available by the LDC (Buckwalter, 2004).7 This
analyzer has total coverage of the corpus; there are no
7Arabic morphological processing was also addressed by
Kiraz (2000), who gives a detailed review of symbolic work in
that area, and by Darwish (2002).
478
Korean Arabic
POS tagging morpheme lemma POS tagging morpheme lemma
accuracy accuracy accuracy accuracy accuracy accuracy
?2 32K 49K 32K 49K 32K 49K 38K 76K 114K 38K 76K 114K 38K 76K 114K
most likely ~y 86.0 86.9 87.5 88.8 95.3 95.7 84.5 87.0 88.3 83.2 86.2 87.0 37.9 39.8 40.9
channel only 62.6 62.6 70.3 70.8 86.4 86.4 43.7 43.7 43.7 41.2 41.2 41.2 27.2 27.2 27.2
bigram HMM 90.7 91.2 83.2 86.1 96.9 97.2 90.3 92.0 92.8 89.2 91.4 91.6 85.7? 87.8? 87.9?
trigram HMM 91.5 91.8 83.3 86.0 97.0 97.2 89.8 92.0 93.0 88.5 91.3 91.3 85.2? 87.8? 87.7?
TM3 ? 90.7 91.3 89.3 90.5 97.1 97.4 94.6 95.4 95.9 93.4 94.3 94.9 89.7? 90.5? 90.7?
u
n
ifo
rm
ch
an
ne
l
10 91.2 91.7 89.4 90.6 97.1 97.6 95.3 95.7 96.1 93.9 94.5 95.0 90.2? 90.6? 91.1?
1 91.5 92.2 89.4 90.6 97.1 97.5 95.2 95.7 96.0 93.9 94.5 94.7 90.0? 90.7? 91.0?
TM3H ? 91.1 91.1 89.3 90.4 97.2 97.5 95.0 95.7 96.0 94.0 94.8 95.3 93.3 93.9 94.2
(factored) 10 91.3 91.9 89.5 90.6 97.3 97.6 95.3 95.7 96.1 94.2 94.7 95.4 93.4 93.6 94.4
1 91.4 92.2 89.5 90.7 97.3 97.6 95.4 95.8 96.1 94.4 94.8 95.1 93.3 93.8 94.2
channel only 51.4 51.3 60.6 60.4 81.2 81.7 41.4 40.6 40.1 39.9 39.1 38.6 26.7? 26.5? 26.4?
bigram HMM 91.2 90.9 88.9 90.1 97.0 97.3 91.0 92.3 93.4 89.7 91.5 91.9 88.1? 89.9? 90.0?
trigram HMM 91.6 91.9 88.9 90.2 97.1 97.4 91.1 92.9 93.7 89.6 92.2 92.0 88.1? 90.6? 90.4?
TM3 ? 90.8 91.0 89.5 90.5 97.4 97.5 95.1 95.7 96.0 93.8 94.6 95.0 92.2? 93.1? 93.2?
ta
g
ch
an
ne
l
10 90.6 91.1 89.5 90.7 97.2 97.6 95.2 95.6 96.0 93.9 94.7 95.0 92.4? 93.2? 93.5?
1 90.1 90.9 89.5 90.7 97.1 97.6 94.9 95.5 95.8 93.8 94.5 94.8 92.2? 93.0? 93.1?
TM3H ? 91.0 91.0 89.4 90.5 97.2 97.6 95.1 95.8 96.0 94.0 95.1 95.4 93.3 94.3 94.4
(factored) 10 90.4 91.2 89.6 90.7 97.4 97.6 95.2 95.7 96.0 94.1 94.8 95.4 93.3 94.0 94.6
1 90.1 91.0 89.5 90.7 97.3 97.6 95.1 95.5 95.9 94.1 94.9 95.1 93.3 94.0 94.4
oracle given d(x) 95.3 95.7 90.2 91.2 98.1 98.3 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
Table 1: Korean (left, 5K test-set) and Arabic (right, 13K test-set) disambiguation. A word is marked correct only if its entire
tag (or morpheme) sequence (or lemma) was correctly identified. Morpheme and lemma accuracy do not include OOV words. The
oracle is an upper bound on accuracy given the morphological dictionary. ?These models do not explicitly predict lemmas; the
lemma is chosen arbitrarily from those that match the hypothesized tag/morpheme sequence for each word. Bold scores indicate a
significant improvement over the trigram HMM (binomial sign test, p < 0.05).
OOV words. There are 139 distinct POS tags; these con-
tain some inflectional information which we treat atom-
ically. For speed, TM3H was trained in two separate
pieces: TM3 and the lemma features added by TM3H.
Arabic has a templatic morphology in which conso-
nantal roots are transformed into surface words by the
insertion of vowels and ancillary consonants. Our sys-
tem does not model this process except through the use
of Buckwalter?s dictionary to define the set of analyses
for each word (cf., Daya et al, 2004, who modeled inter-
digitation in Hebrew). We treat the analysis of an Ara-
bic word as a sequence ~y of pairs of morphemes and
POS tags, plus a lemma. The lemma, given in the dic-
tionary, provides further disambiguation beyond the head
morpheme. The lemma is a standalone dictionary head-
word and not merely the consonantal root, as in some
other work. The ?heads? modeled by TM3H correspond
to these lemmas. There are 20K word types, and 34K
morpheme types. There are 1.7 morphemes per word to-
ken on average (? = 0.77).
Results are shown in Tab. 1. Across tasks and training
set sizes, our models reduce error rates by more than 36%
compared to the trigram HMM source with tag channel.
The TM3H model and the tag channel offer slight gains
over the base TM3 model (especially on lemmatization),
though the tag channel offers no help in POS tagging.
Prior work (Arabic). Both Diab et al (2004) and
Habash and Rambow (2005) use support-vector ma-
chines with local features; the former for tokenization,
POS tagging, and base phrase chunking; the latter for
full morphological disambiguation. Diab et al report
results for a coarsened 24-tag set, while we use the full
139 tags from the Arabic Treebank, so the systems are
not directly comparable. Habash and Rambow present
even better results on the same POS tag set. Our full dis-
ambiguation results appear to be competitive with theirs.
Khoja (2001) and Freeman (2001) describe Arabic POS
taggers and many of the issues involved in developing
them, but because tagged corpora did not yet exist, there
are no comparable quantitative results.
4 Czech: Model and Experiments
Inflective languages like Czech present a new set of chal-
lenges. Our treatment of Czech is not concatenative;
following prior work, the analysis for each word x is a
single tag/lemma pair y. Inflectional affixes in the sur-
face form are represented as features in the tag. While
lemmatization of Czech is not hard (there is little ambi-
guity), tagging is quite difficult, because morphological
tags are highly complex. Our tag set is the Prague Depen-
dency Treebank (PDT; Hajic?, 1998) set, which consists of
fifteen-field tags that indicate POS as well as inflectional
information (case, number, gender, etc.). There are over
479
full model (decoding) factored models (training)
gender
number case
POSmo
rpholo
gical tagsmorph. tag
num.gen. case POS
lemma lemm
asy
y
y
y
1
2
3
4
Figure 3: The Czech model, shown as an undirected graphi-
cal model. The structure of the full model is on the left; fac-
tored components for estimation are shown on the right. Each
of these five models contains a subset of the TM3 features. The
full model is only used to decode. The factored models make
training faster and are used for pruning.
1,400 distinct tag types in the PDT.
Czech has been treated probabilistically before, per-
haps most successfully by Hajic? et al (2001).8 In con-
trast, we estimate conditionally (rather than by maximum
likelihood for a generative HMM) and separate the train-
ing of the source and the channel. We also introduce a
novel factored treatment of the morphological tags.
4.1 Factored tags and estimation
Because Czech morphological tags are not monolithic,
the choice among them can be treated as several more or
less orthogonal decisions. The case feature of one word,
for example, is expected to be conditionally independent
of the next word?s gender, given the next word?s case.
Constraints in the language are expected to cause features
like case, number, and gender to agree locally (on words
that have such features) and somewhat independently of
each other. Coarser POS tagging may be treated as an-
other, roughly independent stream.
Log-linear models and the use of a morphological dic-
tionary make this kind of tag factoring possible. Our
approach is to separately train five log-linear models.
Each model is itself an instance of some of the templates
from TM3, modeling a projection of the full analysis.
The model and its factored components are illustrated in
Fig. 3.
POS model. The full tag is replaced by the POS tag
(the first two fields); there are 60 POS tags. The TM3
8Czech morphological processing was studied by
Petkevic? (2001), Hlava?cova? (2001) (who focuses on han-
dling OOV words), and Mra?kova? and Sedlacek (2003) (who use
partial parsing to reduce the set of possible analyses), inter alia.
feature templates are included twice: once for the full tag
and once for a coarser tag (the first PDT field, for which
there are 12 possible values).9
Gender, number, and case models. The full tag is re-
placed by the gender (or case or number) field. This
model includes bigrams and trigrams as well as field-
morpheme unigram features. These models are intended
to learn to predict local agreement.
Tag-lemma model. This model contains unigram fea-
tures of full PDT tags, both alone and with lemmas. It is
intended to learn to penalize morphological tags that are
rare, or that are rare with a particular lemma. In our for-
mulation, this is not a channel model, because it ignores
the surface word forms.
Each model is estimated independently of the others.
The lattice d(x) against which the conditional probabili-
ties are estimated contains the relevant projection of the
full morphological tags (with lemmas). To decode, we
run a Viterbi-like algorithm that uses the union of all
models? features to pick the best analysis (full morpho-
logical tags and lemmas) allowed by the dictionary.
There are two important advantages of factored train-
ing. First, each model is faster to train alone than a model
with all features merged; in fact, training the fully merged
model takes far too long to be practical. Second, factored
models can be held out at test time to measure their effect
on the system, without retraining.
Prior work (factored training). Separately training
different models that predict the same variables (e.g., x
and y) then combining them for consensus-based infer-
ence (either through a mixture or a product of proba-
bilities) is an old idea (Genest and Zidek, 1986). Re-
cent work in learning weights for the component ?ex-
pert? models has turned to cooperative techniques (Hin-
ton, 1999). Decoding that finds y (given x) to maximize
some weighted average of log-probabilities is known as
a logarithmic opinion pool (LOP). LOPs were applied
to CRFs (for named entity recognition and tagging) by
Smith et al (2005), with an eye toward regularization.
Their experts (each a CRF) contained overlapping feature
sets, and the combined model achieved much the same
effect as training a single model with smoothing. Note
that our models, unlike theirs, partition the feature space;
there is only one CRF, but some parameters are ignored
when estimating other parameters. We have not estimated
log-domain mixing coefficients?we weight all models?
contributions equally. Sutton and McCallum (2005) have
applied factored estimation to CRFs, motivated (like us)
by speed; they also describe how factored estimation
9Lemma-trigram and fine POS-unigram/lemma-bigram fea-
tures were eliminated to limit model size.
480
full morph. lemma POS OOV POS
accuracy accuracy accuracy accuracy
?2 376K 768K 376K 768K 376K 768K 376K 768K
channel only 61.4 60.3 85.1 84.2 88.5 87.2 17.8 16.4
most likely ~y 80.0 80.8 98.1 98.1 97.9 97.8 52.0 52.0
Hajic? et al HMM 88.8 89.2 97.9 97.9 95.8 95.8 52.0 52.0
+ OOV model 90.5 90.8 97.9 97.9 96.7 96.6 93.0 92.9
full ? 88.1 88.5 98.3 98.5 98.3 98.3 60.2 61.8
oracle given pruning 98.6 99.3 99.5 99.6 99.1 99.7 60.2 90.3
10 88.4 88.5 98.4 98.4 98.3 98.2 61.8 59.4
oracle given pruning 99.3 99.3 99.5 99.6 99.8 99.7 93.4 90.6
1 88.6 88.6 98.4 98.4 98.2 98.1 60.0 56.7
oracle given pruning 99.3 99.3 99.5 99.6 99.8 99.8 95.0 94.0
? POS ? 87.9 88.0? 98.2 98.2? 98.0 97.9? 55.7 51.7?
10 88.1 88.3? 98.2 98.3? 98.0 97.9? 55.4 51.6?
1 88.4 88.5? 98.2 98.2? 98.0 97.9? 55.0 51.9?
? tag-lemma ? 87.8 88.3 98.3 98.6 98.3 98.3 60.2 59.7
10 88.0 88.1 98.4 98.5 98.3 98.2 59.1 59.1
1 88.0 88.1 98.4 98.4 98.2 98.1 59.0 58.1
POS only ? 65.6? 65.5? 98.3 98.6 98.3 98.4 60.2 63.7
10 65.7? 65.5? 98.5 98.6 98.5 98.5 65.2 66.4
1 65.7? 65.5? 98.6 98.7 98.6 98.6 67.2 67.2
POS & ? 81.2 82.3 98.3 98.6 98.3 98.4 60.2 63.9
tag-lemma? 10 81.9 82.3 98.5 98.6 98.4 98.5 65.8 67.2
1 82.0 82.3 98.4 98.5 98.5 98.4 67.8 66.3
oracle given d(x) 99.8 99.8 99.5 99.6 99.9 99.9 100.0 100.0
Table 2: Czech disambiguation:
test-set (109K words) accuracy. A
word is marked correct only if its
entire morphological tag (or mor-
pheme or POS tag) was correctly
identified. Note that the full tag
is a complex, 15-field morphologi-
cal label, while ?POS? is a projec-
tion down to a tagset of size 60.
Lemma accuracy does not include
OOV words. ?The POS-only model
selects only POS, not full tags; these
measures are expected performance
if the full tag is selected randomly
from those in the dictionary that
match the selected POS. ?Required
more aggressive pruning. Bold
scores were significantly better than
the HMM of Hajic? et al (binomial
sign test, p < 0.05). Our models
were slightly but significantly worse
on full tagging, but showed signif-
icant improvements on recovering
POS tags and lemmas.
maximizes a lower bound on the unfactored objective.
Smith and Smith (2004) applied factored estimation to a
bilingual weighted grammar, driven by data limitations.
4.2 Experiments
Our corpus is the PDT (Hajic?, 1998), with up to 60% used
for training and 10% (109K words) used for test.10 The
morphological dictionary is the one packaged with the
PDT; it covers about 98% of the tokens in the corpus. The
remaining 2% have (unsurprisingly) a diverse set of 300?
400 distinct tags, depending on the training set size.11
Results are shown in Tab. 2. We compare to the HMM
of (Hajic? et al, 2001) without its OOV component.12 We
report morphological tagging accuracy on words; we also
report lemma accuracy (on non-OOV words), POS accu-
10We used less than the full corpus to keep training time
down; note that the training sets are nonetheless substantially
larger than in the Korean and Arabic experiments.
11During training, these project down to manageable num-
bers of hypotheses in the factored models. At test-time, how-
ever, Viterbi search is quite difficult when OOV symbols occur
consecutively. To handle this, we prune OOV arcs from the lat-
tices using the factored POS and inflectional models. For each
OOV, every model prunes a projection of the analysis (e.g., the
POS model prunes POS tags) until 90% of the posterior mass or
3 arcs remain (whichever is more conservative). Viterbi decod-
ing is run on a lattice containing OOV arcs consistent with the
pruned projected lattices.
12Results with the OOV component are also reported in Tab. 2,
but we cannot guarantee their experimental validity, since the
OOV component is pre-trained and may have been trained on
data in our test set.
racy on all words, and POS accuracy on OOV words. The
channel model (not shown) tended to have a small, harm-
ful effect on performance.
Without any explicit OOV treatment, our POS-only
component model significantly reduces lemma and POS
errors compared to Hajic? et al?s model. On recovering
full morphological tags, our full model is close in perfor-
mance to Hajic? et al, but still significantly worse. It is
likely that for many tasks, these performance gains are
more helpful than the loss on full tagging is harmful.
Why doesn?t our full model perform as well as Hajic? et
al.?s model? An error analysis reveals that our full model
(768K, ?2 = 1), compared to the HMM (768K) had 91%
as many number errors but 0.1% more gender and 31%
more case errors. Taking out those three models (?POS
& tag-lemma? in Fig. 2) is helpful on all measures ex-
cept full tagging accuracy, due in part to substantially
increased errors on gender (87% increase), case (54%),
and number (35%). The net effect of these components,
then, is helpful, but not quite helpful enough to match
a well-smoothed HMM on complex tagging. We com-
pared the models on the training set and found the same
pattern, demonstrating that this is not merely a matter of
over-fitting.
5 Future Work
Two clear ways to improve our models present them-
selves. The first is better OOV handling, perhaps through
an improved channel model. Possibilities include learn-
ing weights to go inside the FST-encoded dictionaries and
481
directly modeling spelling changes. The second is to turn
our factored model into a LOP. Training the mixture co-
efficients should be straightforward (if time-consuming)
with a development dataset.
A drawback of our system (especially for Czech) is
that some components (most notably, the Czech POS
model) take a great deal of time to train (up to two weeks
on 2GHz Pentium systems). Speed improvements are
expected to come from eliminating some of the over-
lapping feature templates, generalized speedups for log-
linear training, and perhaps further factoring.
6 Conclusion
We have explored morphological disambiguation of di-
verse languages using log-linear sequence models. Our
approach reduces error rates significantly on POS tag-
ging (Arabic and Czech), morpheme sequence recovery
(Korean and Arabic), and lemmatization (all three lan-
guages), compared to baseline state-of-the-art methods
For complex analysis tasks (e.g., Czech tagging), we have
demonstrated that factoring a large model into smaller
components can simplify training and achieve excel-
lent results. We conclude that a conditionally-estimated
source model informed by an existing morphological dic-
tionary (serving as an unweighted channel) is an effective
approach to morphological disambiguation.
References
K. R. Beesley and L. Karttunen. 2003. Finite State Morphol-
ogy. CSLI.
T. Buckwalter. 2004. Arabic morphological analyzer version
2.0. LDC2004L02.
J. Cha, G. Lee, and J.-H. Lee. 1998. Generalized unknown
morpheme guessing for hybrid POS tagging of Korean. In
Proc. of VLC.
K. Darwish. 2002. Building a shallow Arabic morphological
analyser in one day. In Proc. of ACL Workshop on Computa-
tional Approaches to Semitic Languages.
E. Daya, D. Roth, and S. Wintner. 2004. Learning Hebrew
roots: Machine learning with linguistic constraints. In Proc.
of EMNLP.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic tag-
ging of Arabic text: From raw text to base phrase chunks. In
Proc. of HLT-NAACL.
A. Freeman. 2001. Brill?s POS tagger and a morphology parser
for Arabic. In Proc. of ACL Workshop on Arabic Language
Processing.
C. Genest and J. V. Zidek. 1986. Combining probability distri-
butions: A critique and an annotated bibliography. Statistical
Science, 1:114?48.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-
of-speech tagging and morphological disambiguation in one
fell swoop. In Proc. of ACL.
J. Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petkevic?. 2001.
Serial combination of rules and statistics: A case study in
Czech tagging. In Proc. of ACL.
J. Hajic?. 1998. Building a syntactically annotated corpus:
The Prague Dependency Treebank. In Issues of Valency and
Meaning.
D. Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2000. Statistical
morphological disambiguation for agglutinative languages.
In Proc. of COLING.
C.-H. Han, N.-R. Han, E.-S. Ko, H. Yi, and M. Palmer. 2002.
Penn Korean Treebank: Development and evaluation. In
Proc. Pacific Asian Conf. Language and Comp.
N.-R. Han. 2004. Klex: Finite-state lexical transducer for Ko-
rean. LDC2004L01.
G. Hinton. 1999. Products of experts. In Proc. of ICANN.
J. Hlava?cova?. 2001. Morphological guesser of Czech words.
In Proc. of TSD.
F. Jelinek. 1976. Continuous speech recognition by statistical
methods. Proc. of the IEEE, 64(4):532?557.
R. M. Kaplan and M. Kay. 1981. Phonological rules and finite-
state transducers. Presented at Linguistic Society of Amer-
ica.
S. Khoja. 2001. APT: Arabic part-of-speech tagger. In Proc.
of NAACL Student Workshop.
G. Kiraz. 2000. Multitiered nonlinear morphology using mul-
titape finite automata: A case study on Syriac and Arabic.
Computational Linguistics, 26(1):77?105.
K. Koskenniemi. 1983. Two-level morphology: A general
computational model of word-form recognition and produc-
tion. Technical Report 11, University of Helsinki.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Applying
conditional random fields to Japanese morphological analy-
sis. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
M. Levinger, U. Ornan, and A. Itai. 1995. Learning morpho-
lexical probabilities from an untagged corpus with an appli-
cation to Hebrew. Computational Linguistics, 21(3):383?
404.
D. C. Liu and J. Nocedal. 1989. On the limited memory method
for large scale optimization. Mathematical Programming B,
45(3):503?28.
M. Maamouri, A. Bies, H. Jin, and T. Buckwalter. 2003. Arabic
Treebank part 1 version 2.0. LDC2003T06.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding consensus
among words: Lattice-based word error minimization. In
Proc. of ECSCT.
E. Mra?kova? and R. Sedlacek. 2003. From Czech morphol-
ogy through partial parsing to disambiguation. In Proc. of
CLITP.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random fields.
In Proc. of COLING.
V. Petkevic?. 2001. Grammatical agreement and automatic
morphological disambiguation of inflectional languages. In
Proc. of TSD.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of HLT-NAACL.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing with
factored estimation: Using English to parse Korean. In Proc.
of EMNLP.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opin-
ion pools for conditional random fields. In Proc. of ACL.
C. Sutton and A. McCallum. 2005. Cliquewise training for
undirected models. In Proc. of UAI.
482
A Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787?794,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Minimum Risk Annealing for Training Log-Linear Models?
David A. Smith and Jason Eisner
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
When training the parameters for a natural language system,
one would prefer to minimize 1-best loss (error) on an eval-
uation set. Since the error surface for many natural language
problems is piecewise constant and riddled with local min-
ima, many systems instead optimize log-likelihood, which is
conveniently differentiable and convex. We propose training
instead to minimize the expected loss, or risk. We define this
expectation using a probability distribution over hypotheses
that we gradually sharpen (anneal) to focus on the 1-best hy-
pothesis. Besides the linear loss functions used in previous
work, we also describe techniques for optimizing nonlinear
functions such as precision or the BLEU metric. We present
experiments training log-linear combinations of models for
dependency parsing and for machine translation. In machine
translation, annealed minimum risk training achieves signif-
icant improvements in BLEU over standard minimum error
training. We also show improvements in labeled dependency
parsing.
1 Direct Minimization of Error
Researchers in empirical natural language pro-
cessing have expended substantial ink and effort in
developing metrics to evaluate systems automati-
cally against gold-standard corpora. The ongoing
evaluation literature is perhaps most obvious in the
machine translation community?s efforts to better
BLEU (Papineni et al, 2002).
Despite this research, parsing or machine trans-
lation systems are often trained using the much
simpler and harsher metric of maximum likeli-
hood. One reason is that in supervised training,
the log-likelihood objective function is generally
convex, meaning that it has a single global max-
imum that can be easily found (indeed, for su-
pervised generative models, the parameters at this
maximum may even have a closed-form solution).
In contrast to the likelihood surface, the error sur-
face for discrete structured prediction is not only
riddled with local minima, but piecewise constant
?This work was supported by an NSF graduate research
fellowship for the first author and by NSF ITR grant IIS-
0313193 and ONR grant N00014-01-1-0685. The views ex-
pressed are not necessarily endorsed by the sponsors. We
thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and
the reviewers for helpful discussions and comments.
and not everywhere differentiable with respect to
the model parameters (Figure 1). Despite these
difficulties, some work has shown it worthwhile
to minimize error directly (Och, 2003; Bahl et al,
1988).
We show improvements over previous work on
error minimization by minimizing the risk or ex-
pected error?a continuous function that can be
derived by combining the likelihood with any eval-
uation metric (?2). Seeking to avoid local min-
ima, deterministic annealing (Rose, 1998) gradu-
ally changes the objective function from a convex
entropy surface to the more complex risk surface
(?3). We also discuss regularizing the objective
function to prevent overfitting (?4). We explain
how to compute expected loss under some evalu-
ation metrics common in natural language tasks
(?5). We then apply this machinery to training
log-linear combinations of models for dependency
parsing and for machine translation (?6). Finally,
we note the connections of minimum risk training
to max-margin training and minimum Bayes risk
decoding (?7), and recapitulate our results (?8).
2 Training Log-Linear Models
In this work, we focus on rescoring with log-
linear models. In particular, our experiments con-
sider log-linear combinations of a relatively small
number of features over entire complex structures,
such as trees or translations, known in some pre-
vious work as products of experts (Hinton, 1999)
or logarithmic opinion pools (Smith et al, 2005).
A feature in the combined model might thus be
a log probability from an entire submodel. Giv-
ing this feature a small or negative weight can
discount a submodel that is foolishly structured,
badly trained, or redundant with the other features.
For each sentence xi in our training corpus S,
we are given Ki possible analyses yi,1, . . . yi,Ki .
(These may be all of the possible translations or
parse trees; or only the Ki most probable under
787
Figure 1: The loss surface for a machine translation sys-
tem: while other parameters are held constant, we vary the
weights on the distortion and word penalty features. Note the
piecewise constant regions with several local maxima.
some other model; or only a random sample of
size Ki.) Each analysis has a vector of real-valued
features (i.e., factors, or experts) denoted fi,k. The
score of the analysis yi,k is ? ? fi,k, the dot prod-
uct of its features with a parameter vector ?. For
each sentence, we obtain a normalized probability
distribution over the Ki analyses as
p?(yi,k | xi) =
exp ? ? fi,k
?Ki
k?=1 exp ? ? fi,k?
(1)
We wish to adjust this model?s parameters ?
to minimize the severity of the errors we make
when using it to choose among analyses. A loss
function Ly?(y) assesses a penalty for choosing
y when y? is correct. We will usually write this
simply as L(y) since y? is fixed and clear from
context. For clearer exposition, we assume below
that the total loss over some test corpus is the sum
of the losses on individual sentences, although we
will revisit that assumption in ?5.
2.1 Minimizing Loss or Expected Loss
One training criterion directly mimics test condi-
tions. It looks at the loss incurred if we choose the
best analysis of each xi according to the model:
min
?
?
i
L(argmax
yi
p?(yi | xi)) (2)
Since small changes in ? either do not change
the best analysis or else push a different analy-
sis to the top, this objective function is piecewise
constant, hence not amenable to gradient descent.
Och (2003) observed, however, that the piecewise-
constant property could be exploited to character-
ize the function exhaustively along any line in pa-
rameter space, and hence to minimize it globally
along that line. By calling this global line mini-
mization as a subroutine of multidimensional opti-
mization, he was able to minimize (2) well enough
to improve over likelihood maximization for train-
ing factored machine translation systems.
Instead of considering only the best hypothesis
for any ?, we can minimize risk, i.e., the expected
loss under p? across all analyses yi:
min
?
Ep?L(yi,k)
def
= min
?
?
i
?
k
L(yi,k)p?(yi,k | xi)
(3)
This ?smoothed? objective is now continuous and
differentiable. However, it no longer exactly mim-
ics test conditions, and it typically remains non-
convex, so that gradient descent is still not guaran-
teed to find a global minimum. Och (2003) found
that such smoothing during training ?gives almost
identical results? on translation metrics.
The simplest possible loss function is 0/1 loss,
where L(y) is 0 if y is the true analysis y?i and
1 otherwise. This loss function does not at-
tempt to give partial credit. Even in this sim-
ple case, assuming P 6= NP, there exists no gen-
eral polynomial-time algorithm for even approx-
imating (2) to within any constant factor, even
for Ki = 2 (Hoffgen et al, 1995, from Theo-
rem 4.10.4).1 The same is true for for (3), since
for Ki = 2 it can be easily shown that the min 0/1
risk is between 50% and 100% of the min 0/1 loss.
2.2 Maximizing Likelihood
Rather than minimizing a loss function suited to
the task, many systems (especially for language
modeling) choose simply to maximize the prob-
ability of the gold standard. The log of this likeli-
hood is a convex function of the parameters ?:
max
?
?
i
log p?(y
?
i | xi) (4)
where y?i is the true analysis of sentence xi. The
only wrinkle is that p?(y?i | xi) may be left unde-
fined by equation (1) if y?i is not in our set of Ki
hypotheses. When maximizing likelihood, there-
fore, we will replace y?i with the min-loss analy-
sis in the hypothesis set; if multiple analyses tie
1Known algorithms are exponential but only in the dimen-
sionality of the feature space (Johnson and Preparata, 1978).
788
?10 ?5 0 5 10
17.
5
18.0
18.5
19.0
Translation model 1
Ble
u %
? = ?? = 0.1? = 1? = 10
Figure 2: Loss and expected loss as one translation model?s
weight varies: the gray line (? = ?) shows true BLEU (to be
optimized in equation (2)). The black lines show the expected
BLEU as ? in equation (5) increases from 0.1 toward?.
for this honor, we follow Charniak and Johnson
(2005) in summing their probabilities.2
Maximizing (4) is equivalent to minimizing an
upper bound on the expected 0/1 loss
?
i(1 ?
p?(y?i | xi)). Though the log makes it tractable,
this remains a 0/1 objective that does not give par-
tial credit to wrong answers, such as imperfect but
useful translations. Most systems should be eval-
uated and preferably trained on less harsh metrics.
3 Deterministic Annealing
To balance the advantages of direct loss minimiza-
tion, continuous risk minimization, and convex
optimization, deterministic annealing attempts
the solution of increasingly difficult optimization
problems (Rose, 1998). Adding a scale hyperpa-
rameter ? to equation (1), we have the following
family of distributions:
p?,?(yi,k | xi) =
(exp ? ? fi,k)
?
?Ki
k?=1
(
exp ? ? fi,k?
)? (5)
When ? = 0, all yi,k are equally likely, giving
the uniform distribution; when ? = 1, we recover
the model in equation (1); and as ? ? ?, we
approach the winner-take-all Viterbi function that
assigns probability 1 to the top-scoring analysis.
For a fixed ?, deterministic annealing solves
min
?
Ep?,? [L(yi,k)] (6)
2An alternative would be to artificially add y?i (e.g., the
reference translation(s)) to the hypothesis set during training.
We then increase ? according to some schedule
and optimize ? again. When ? is low, the smooth
objective might allow us to pass over local min-
ima that could open up at higher ?. Figure 3 shows
how the smoothing is gradually weakened to reach
the risk objective (3) as ? ? 1 and approach the
true error objective (2) as ? ? ?.
Our risk minimization most resembles the work
of Rao and Rose (2001), who trained an isolated-
word speech recognition system for expected
word-error rate. Deterministic annealing has also
been used to tackle non-convex likelihood sur-
faces in unsupervised learning with EM (Ueda and
Nakano, 1998; Smith and Eisner, 2004). Other
work on ?generalized probabilistic descent? mini-
mizes a similar objective function but with ? held
constant (Katagiri et al, 1998).
Although the entropy is generally higher at
lower values of ?, it varies as the optimization
changes ?. In particular, a pure unregularized log-
linear model such as (5) is really a function of ? ??,
so the optimizer could exactly compensate for in-
creased ? by decreasing the ? vector proportion-
ately!3 Most deterministic annealing procedures,
therefore, express a direct preference on the en-
tropy H , and choose ? and ? accordingly:
min
?,?
Ep?,? [L(yi,k)] ? T ?H(p?,?) (7)
In place of a schedule for raising ?, we now use
a cooling schedule to lower T from ? to ??,
thereby weakening the preference for high en-
tropy. The Lagrange multiplier T on entropy is
called ?temperature? due to a satisfying connec-
tion to statistical mechanics. Once T is quite cool,
it is common in practice to switch to raising ? di-
rectly and rapidly (quenching) until some conver-
gence criterion is met (Rao and Rose, 2001).
4 Regularization
Informally, high temperature or ? < 1 smooths
our model during training toward higher-entropy
conditional distributions that are not so peaked at
the desired analyses y?i . Another reason for such
smoothing is simply to prevent overfitting to these
training examples.
A typical way to control overfitting is to use a
quadratic regularizing term, ||?||2 or more gener-
ally
?
d ?
2
d/2?
2
d. Keeping this small keeps weights
3For such models, ? merely aids the nonlinear optimizer
in its search, by making it easier to scale all of ? at once.
789
low and entropy high. We may add this regularizer
to equation (6) or (7). In the maximum likelihood
framework, we may subtract it from equation (4),
which is equivalent to maximum a posteriori esti-
mation with a diagonal Gaussian prior (Chen and
Rosenfeld, 1999). The variance ?2d may reflect a
prior belief about the potential usefulness of fea-
ture d, or may be tuned on heldout data.
Another simple regularization method is to stop
cooling before T reaches 0 (cf. Elidan and Fried-
man (2005)). If loss on heldout data begins to
increase, we may be starting to overfit. This
technique can be used along with annealing or
quadratic regularization and can achieve addi-
tional accuracy gains, which we report elsewhere
(Dreyer et al, 2006).
5 Computing Expected Loss
At each temperature setting of deterministic an-
nealing, we need to minimize the expected loss on
the training corpus. We now discuss how this ex-
pectation is computed. When rescoring, we as-
sume that we simply wish to combine, in some
way, statistics of whole sentences4 to arrive at the
overall loss for the corpus. We consider evalua-
tion metrics for natural language tasks from two
broadly applicable classes: linear and nonlinear.
A linear metric is a sum (or other linear combi-
nation) of the loss or gain on individual sentences.
Accuracy?in dependency parsing, part-of-speech
tagging, and other labeling tasks?falls into this
class, as do recall, word error rate in ASR, and
the crossing-brackets metric in parsing. Thanks to
the linearity of expectation, we can easily compute
our expected loss in equation (6) by adding up the
expected loss on each sentence.
Some other metrics involve nonlinear combi-
nations over the sentences of the corpus. One
common example is precision, P
def
=
?
i ci/
?
i ai,
where ci is the number of correctly posited ele-
ments, and ai is the total number of posited ele-
ments, in the decoding of sentence i. (Depend-
ing on the task, the elements may be words, bi-
grams, labeled constituents, etc.) Our goal is to
maximize P , so during a step of deterministic an-
nealing, we need to maximize the expectation of
P when the sentences are decoded randomly ac-
cording to equation (5). Although this expectation
is continuous and differentiable as a function of
4Computing sentence xi?s statistics usually involves iter-
ating over hypotheses yi,1, . . . yi,Ki . If these share substruc-
ture in a hypothesis lattice, dynamic programming may help.
?, unfortunately it seems hard to compute for any
given ?. We observe however that an equivalent
goal is to minimize ? logP . Taking that as our
loss function instead, equation (6) now needs to
minimize the expectation of ? logP ,5 which de-
composes somewhat more nicely:
E[? logP ] = E[log
?
i
ai ? log
?
i
ci]
= E[logA] ? E[logC] (8)
where the integer random variables A =
?
i ai
and C =
?
i ci count the number of posited and
correctly posited elements over the whole corpus.
To approximate E[g(A)], where g is any twice-
differentiable function (here g = log), we can ap-
proximate g locally by a quadratic, given by the
Taylor expansion of g aboutA?s mean ?A = E[A]:
E[g(A)] ? E[g(?A) + (A? ?A)g?(?A)
+
1
2
(A? ?A)
2g??(?A)]
= g(?A) + E[A? ?A]g?(?A)
+
1
2
E[(A? ?A)2]g??(?A)
= g(?A) +
1
2
?2Ag
??(?A).
Here ?A =
?
i ?ai and ?
2
A =
?
i ?
2
ai , since A
is a sum of independent random variables ai (i.e.,
given the current model parameters ?, our ran-
domized decoder decodes each sentence indepen-
dently). In other words, given our quadratic ap-
proximation to g, E[g(A)] depends on the (true)
distribution of A only through the single-sentence
means ?ai and variances ?
2
ai , which can be found
by enumerating the Ki decodings of sentence i.
The approximation becomes arbitrarily good as
we anneal ? ? ?, since then ?2A ? 0 and
E[g(A)] focuses on g near ?A. For equation (8),
E[g(A)] = E[logA] ? log(?A) ?
?2A
2?2A
and E[logC] is found similarly.
Similar techniques can be used to compute the
expected logarithms of some other non-linear met-
rics, such as F-measure (the harmonic mean of
precision and recall)6 and Papineni et al (2002)?s
5This changes the trajectory that DA takes through pa-
rameter space, but ultimately the objective is the same: as
? ? ? over the course of DA, minimizing E[? logP ] be-
comes indistinguishable from maximizing E[P ].
6R
def
= C/B; the count B of correct elements is known.
So logF
def
= log 2PR/(P + R) = log 2R/(1 + R/P ) =
log 2C/B ? log(1 +A/B). Consider g(x) = log 1 + x/B.
790
BLEU translation metric (the geometric mean of
several precisions). In particular, the expectation
of log BLEU distributes over its N +1 summands:
log BLEU = min(1 ?
r
A1
, 0) +
N?
n=1
wn logPn
where Pn is the precision of the n-gram elements
in the decoding.7 As is standard in MT research,
we take wn = 1/N and N = 4. The first term in
the BLEU score is the log brevity penalty, a con-
tinuous function of A1 (the total number of uni-
gram tokens in the decoded corpus) that fires only
ifA1 < r (the average word count of the reference
corpus). We again use a Taylor series to approxi-
mate the expected log brevity penalty.
We mention an alternative way to compute (say)
the expected precisionC/A: integrate numerically
over the joint density of C and A. How can we
obtain this density? As (C,A) =
?
i(ci, ai) is a
sum of independent random length-2 vectors, its
mean vector and 2 ? 2 covariance matrix can be
respectively found by summing the means and co-
variance matrices of the (ci, ai), each exactly com-
puted from the distribution (5) over Ki hypothe-
ses. We can easily approximate (C,A) by the
(continuous) bivariate normal with that mean and
covariance matrix8?or else accumulate an exact
representation of its (discrete) probability mass
function by a sequence of numerical convolutions.
6 Experiments
We tested the above training methods on two
different tasks: dependency parsing and phrase-
based machine translation. Since the basic setup
was the same for both, we outline it here before
describing the tasks in detail.
In both cases, we start with 8 to 10 models
(the ?experts?) already trained on separate training
data. To find the optimal coefficients ? for a log-
linear combination of these experts, we use sepa-
rate development data, using the following proce-
dure due to Och (2003):
1. Initialization: Initialize ? to the 0 vector. For
each development sentence xi, set its Ki-best
list to ? (thus Ki = 0).
7BLEU is careful when measuring ci on a particular de-
coding yi,k. It only counts the first two copies of the (e.g.) as
correct if the occurs at most twice in any reference translation
of xi. This ?clipping? does not affect the rest of our method.
8Reasonable for a large corpus, by Lyapunov?s central
limit theorem (allows non-identically distributed summands).
2. Decoding: For each development sentence
xi, use the current ? to extract the 200 anal-
yses yi,k with the greatest scores exp ? ? fi,k.
Calcuate each analysis?s loss statistics (e.g.,
ci and ai), and add it to the Ki-best list if it is
not already there.
3. Convergence: If Ki has not increased for
any development sentence, or if we have
reached our limit of 20 iterations, stop: the
search has converged.
4. Optimization: Adjust ? to improve our ob-
jective function over the whole development
corpus. Return to step 2.
Our experiments simply compare three proce-
dures at step 4. We may either
? maximize log-likelihood (4), a convex func-
tion, at a given level of quadratic regulariza-
tion, by BFGS gradient descent;
? minimize error (2) by Och?s line search
method, which globally optimizes each com-
ponent of ? while holding the others con-
stant;9 or
? minimize the same error (2) more effectively,
by raising ? ? ? while minimizing the an-
nealed risk (6), that is, cooling T ? ?? (or
? ? ?) and at each value, locally minimiz-
ing equation (7) using BFGS.
Since these different optimization procedures
will usually find different ? at step 4, their K-best
lists will diverge after the first iteration.
For final testing, we selected among several
variants of each procedure using a separate small
heldout set. Final results are reported for a larger,
disjoint test set.
6.1 Machine Translation
For our machine translation experiments, we
trained phrase-based alignment template models
of Finnish-English, French-English, and German-
English, as follows. For each language pair, we
aligned 100,000 sentence pairs from European
Parliament transcripts using GIZA++. We then
used Philip Koehn?s phrase extraction software
to merge the GIZA++ alignments and to extract
9The component whose optimization achieved the lowest
loss is then updated. The process iterates until no lower loss
can be found. In contrast, Papineni (1999) proposed a linear
programming method that may search along diagonal lines.
791
and score the alignment template model?s phrases
(Koehn et al, 2003).
The Pharaoh phrase-based decoder uses pre-
cisely the setup of this paper. It scores a candidate
translation (including its phrasal alignment to the
original text) as ? ? f , where f is a vector of the
following 8 features:
1. the probability of the source phrase given the
target phrase
2. the probability of the target phrase given the
source phrase
3. the weighted lexical probability of the source
words given the target words
4. the weighted lexical probability of the target
words given the source words
5. a phrase penalty that fires for each template
in the translation
6. a distortion penalty that fires when phrases
translate out of order
7. a word penalty that fires for each English
word in the output
8. a trigram language model estimated on the
English side of the bitext
Our goal was to train the weights ? of these 8
features. We used the method described above,
employing the Pharaoh decoder at step 2 to gener-
ate the 200-best translations according to the cur-
rent ?. As explained above, we compared three
procedures at step 4: maximum log-likelihood by
gradient ascent; minimum error using Och?s line-
search method; and annealed minimum risk. As
our development data for training ?, we used 200
sentence pairs for each language pair.
Since our methods can be tuned with hyperpa-
rameters, we used performance on a separate 200-
sentence held-out set to choose the best hyper-
parameter values. The hyperparameter levels for
each method were
? maximum likelihood: a Gaussian prior with
all ?2d at 0.25, 0.5, 1, or ?
? minimum error: 1, 5, or 10 different ran-
dom starting points, drawn from a uniform
Optimization Finnish- French- German-
Procedure English English English
Max. like. 5.02 5.31 7.43
Min. error 10.27 26.16 20.94
Ann. min. risk 16.43 27.31 21.30
Table 1: BLEU 4n1 percentage on translating 2000-
sentence test corpora, after training the 8 experts on 100,000
sentence pairs and fitting their weights ? on 200 more, using
settings tuned on a further 200. The current minimum risk an-
nealing method achieved significant improvements over min-
imum error and maximum likelihood at or below the 0.001
level, using a permutation test with 1000 replications.
distribution on [?1, 1]? [?1, 1]? ? ? ? , when
optimizing ? at an iteration of step 4.10
? annealed minimum risk: with explicit en-
tropy constraints, starting temperature T ?
{100, 200, 1000}; stopping temperature T ?
{0.01, 0.001}. The temperature was cooled
by half at each step; then we quenched by
doubling ? at each step. (We also ran exper-
iments with quadratic regularization with all
?2d at 0.5, 1, or 2 (?4) in addition to the en-
tropy constraint. Also, instead of the entropy
constraint, we simply annealed on ? while
adding a quadratic regularization term. None
of these regularized models beat the best set-
ting of standard deterministic annealing on
heldout or test data.)
Final results on a separate 2000-sentence test set
are shown in table 1. We evaluated translation us-
ing BLEU with one reference translation and n-
grams up to 4. The minimum risk annealing pro-
cedure significantly outperformed maximum like-
lihood and minimum error training in all three lan-
guage pairs (p < 0.001, paired-sample permuta-
tion test with 1000 replications).
Minimum risk annealing generally outper-
formed minimum error training on the held-out
set, regardless of the starting temperature T . How-
ever, higher starting temperatures do give better
performance and a more monotonic learning curve
(Figure 3), a pattern that held up on test data.
(In the same way, for minimum error training,
10That is, we run step 4 from several starting points, finish-
ing at several different points; we pick the finishing point with
lowest development error (2). This reduces the sensitivity of
this method to the starting value of ?. Maximum likelihood
is not sensitive to the starting value of ? because it has only a
global optimum; annealed minimum risk is not sensitive to it
either, because initially ? ? 0, making equation (6) flat.
792
5 10 15 20
16
18
20
22
Iteration
Bleu
T=1000T=200T=100Min. error
Figure 3: Iterative change in BLEU on German-English de-
velopment (upper) and held-out (lower), under annealed min-
imum risk training with different starting temperatures, ver-
sus minimum error training with 10 random restarts.
5 10 15 20
5
10
15
20
Iteration
Bleu
10 restarts1 restart
Figure 4: Iterative change in BLEU on German-English
development (upper) and held-out (lower), using 10 random
restarts vs. only 1.
more random restarts give better performance and
a more monotonic learning curve?see Figure 4.)
Minimum risk annealing did not always win on
the training set, suggesting that its advantage is
not superior minimization but rather superior gen-
eralization: under the risk criterion, multiple low-
loss hypotheses per sentence can help guide the
learner to the right part of parameter space.
Although the components of the translation and
language models interact in complex ways, the im-
provement on Finnish-English may be due in part
to the higher weight that minimum risk annealing
found for the word penalty. That system is there-
fore more likely to produce shorter output like i
have taken note of your remarks and i also agree
with that . than like this longer output from the
minimum-error-trained system: i have taken note
of your remarks and i shall also agree with all that
the union .
We annealed using our novel expected-BLEU
approximation from ?5. We found this to perform
significantly better on BLEU evaluation than if we
trained with a ?linearized? BLEU that summed
per-sentence BLEU scores (as used in minimum
Bayes risk decoding by Kumar and Byrne (2004)).
6.2 Dependency Parsing
We trained dependency parsers for three different
languages: Bulgarian, Dutch, and Slovenian.11 In-
put sentences to the parser were already tagged for
parts of speech. Each parser employed 10 experts,
each parameterized as a globally normalized log-
linear model (Lafferty et al, 2001). For example,
the 9th component of the feature vector fi,k (which
described the kth parse of the ith sentence) was the
log of that parse?s normalized probability accord-
ing to the 9th expert.
Each expert was trained separately to maximize
the conditional probability of the correct parse
given the sentence. We used 10 iterations of gradi-
ent ascent. To speed training, for each of the first
9 iterations, the gradient was estimated on a (dif-
ferent) sample of only 1000 training sentences.
We then trained the vector ?, used to combine
the experts, to minimize the number of labeled de-
pendency attachment errors on a 200-sentence de-
velopment set. Optimization proceeded over lists
of the 200-best parses of each sentence produced
by a joint decoder using the 10 experts.
Evaluating on labeled dependency accuracy on
200 test sentences for each language, we see that
minimum error and annealed minimum risk train-
ing are much closer than for MT. For Bulgarian
and Dutch, they are statistically indistinguishable
using a paired-sample permutations test with 1000
replications. Indeed, on Dutch, all three opti-
mization procedures produce indistinguishable re-
sults. On Slovenian, annealed minimum risk train-
ing does show a significant improvement over the
other two methods. Overall, however, the results
for this task are mediocre. We are still working on
improving the underlying experts.
7 Related Work
We have seen that annealed minimum risk train-
ing provides a useful alternative to maximum like-
lihood and minimum error training. In our ex-
periments, it never performed significantly worse
11For information on these corpora, see the CoNLL-X
shared task on multilingual dependency parsing: http:
//nextens.uvt.nl/?conll/.
793
Optimization labeled dependency acc. [%]
Procedure Slovenian Bulgarian Dutch
Max. like. 27.78 47.23 36.78
Min. error 22.52 54.72 36.78
Ann. min. risk 31.16 54.66 36.71
Table 2: Labeled dependency accuracy on parsing 200-
sentence test corpora, after training 10 experts on 1000 sen-
tences and fitting their weights ? on 200 more. For Slove-
nian, minimum risk annealing is significantly better than the
other training methods, while minimum error is significantly
worse. For Bulgarian, both minimum error and annealed min-
imum risk training achieve significant gains over maximum
likelihood, but are indistinguishable from each other. For
Dutch, the three methods are indistinguishable.
than either and in some cases significantly helped.
Note, however, that annealed minimum risk train-
ing results in a deterministic classifier just as these
other training procedures do. The orthogonal
technique of minimum Bayes risk decoding has
achieved gains on parsing (Goodman, 1996) and
machine translation (Kumar and Byrne, 2004). In
speech recognition, researchers have improved de-
coding by smoothing probability estimates numer-
ically on heldout data in a manner reminiscent of
annealing (Goel and Byrne, 2000). We are inter-
ested in applying our techniques for approximat-
ing nonlinear loss functions to MBR by perform-
ing the risk minimization inside the dynamic pro-
gramming or other decoder.
Another training approach that incorporates ar-
bitrary loss functions is found in the structured
prediction literature in the margin-based-learning
community (Taskar et al, 2004; Crammer et al,
2004). Like other max-margin techniques, these
attempt to make the best hypothesis far away from
the inferior ones. The distinction is in using a loss
function to calculate the required margins.
8 Conclusions
Despite the challenging shape of the error sur-
face, we have seen that it is practical to opti-
mize task-specific error measures rather than op-
timizing likelihood?it produces lower-error sys-
tems. Different methods can be used to attempt
this global, non-convex optimization. We showed
that for MT, and sometimes for dependency pars-
ing, an annealed minimum risk approach to opti-
mization performs significantly better than a pre-
vious line-search method that does not smooth the
error surface. It never does significantly worse.
With such improved methods for minimizing er-
ror, we can hope to make better use of task-specific
training criteria in NLP.
References
L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-
cer. 1988. A new algorithm for the estimation of hidden
Markov model parameters. In ICASSP, pages 493?496.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In ACL,
pages 173?180.
S. F. Chen and R. Rosenfeld. 1999. A gaussian prior for
smoothing maximum entropy models. Technical report,
CS Dept., Carnegie Mellon University.
K. Crammer, R. McDonald, and F. Pereira. 2004. New large
margin algorithms for structured prediction. In Learning
with Structured Outputs (NIPS).
M. Dreyer, D. A. Smith, and N. A. Smith. 2006. Vine parsing
and minimum risk reranking for speed and precision. In
CoNLL.
G. Elidan and N. Friedman. 2005. Learning hidden variable
networks: The information bottleneck approach. JMLR,
6:81?127.
V. Goel and W. J. Byrne. 2000. Minimum Bayes-Risk au-
tomatic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
J. T. Goodman. 1996. Parsing algorithms and metrics. In
ACL, pages 177?183.
G. Hinton. 1999. Products of experts. In Proc. of ICANN,
volume 1, pages 1?6.
K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. 1995.
Robust trainability of single neurons. J. of Computer and
System Sciences, 50(1):114?125.
D. S. Johnson and F. P. Preparata. 1978. The densest hemi-
sphere problem. Theoretical Comp. Sci., 6(93?107).
S. Katagiri, B.-H. Juang, and C.-H. Lee. 1998. Pattern recog-
nition using a family of design algorithms based upon the
generalized probabilistic descent method. Proc. IEEE,
86(11):2345?2373, November.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 48?54.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk decod-
ing for statistical machine translation. In HLT-NAACL.
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmenting
and labeling sequence data. In ICML.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In ACL, pages 311?318.
K. A. Papineni. 1999. Discriminative training via linear
programming. In ICASSP.
A. Rao and K. Rose. 2001. Deterministically annealed de-
sign of Hidden Markov Model speech recognizers. IEEE
Trans. on Speech and Audio Processing, 9(2):111?126.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimiza-
tion problems. Proc. IEEE, 86(11):2210?2239.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In ACL, pages
486?493.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL, pages
18?25.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In EMNLP, pages 1?8.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271?282.
794
Bootstrapping toponym classifiers
David A. Smith and Gideon S. Mann
Center for Language and Speech Processing
Computer Science Department, Johns Hopkins University
Baltimore, MD 21218, USA
{dasmith,gsm}@cs.jhu.edu
Abstract
We present minimally supervised methods for
training and testing geographic name disam-
biguation (GND) systems. We train data-driven
place name classifiers using toponyms already
disambiguated in the training text ? by such
existing cues as ?Nashville, Tenn.? or ?Spring-
field, MA? ? and test the system on texts
where these cues have been stripped out and
on hand-tagged historical texts. We experiment
on three English-language corpora of varying
provenance and complexity: newsfeed from the
1990s, personal narratives from the 19th cen-
tury American west, and memoirs and records
of the U.S. Civil War. Disambiguation accu-
racy ranges from 87% for news to 69% for
some historical collections.
1 Scope and Prior Work
We present minimally supervised methods for training
and testing geographic name disambiguation (GND) sys-
tems. We train data-driven place name classifiers using
toponyms already disambiguated in the training text ?
by such existing cues as ?Nashville, Tenn.? or ?Spring-
field, MA? ? and test the system on text where these
cues have been stripped out and on hand-tagged histori-
cal texts.
As in early work with such named-entity recognition
systems as Nominator (Wacholder et al, 1997), much
previous work in GND has relied on heuristic rules (Ol-
ligschlaeger and Hauptmann, 1999; Kanada, 1999) and
such culturally specific and knowledge intensive tech-
niques as postal codes, addresses, and telephone num-
bers (McCurley, 2001). In previous work, we used the
heuristic technique of calculating weighted centroids of
geographic focus in documents (Smith and Crane, 2001).
Sites closer to the centroid were weighted more heavily
than sites far away unless they had some countervailing
importance such as being a world capital.
News texts offer two principal advantages for boot-
strapping geocoding applications. Just as journalistic
style prefers identifying persons by full name and title on
first mention, place names, when not of major cities, are
often first mentioned followed by the name of their state,
province, or country. Even if a toponym is strictly unam-
biguous, it may still be labelled to provide the reader with
some ?backoff? recognition. Although there is only one
place in the world named ?Wye Mills?, an author would
still usually append ?Maryland? to it so that a reader who
doesn?t recognize the place name can still situate it within
a rough area. In any case, the goal is to generalize from
the kinds of contexts in which writers use a disambiguat-
ing label to one in which they do not.
Since news stories also tend to be relatively short and
focused on a single topic, we can also exploit the heuristic
of ?one sense per discourse?: unless otherwise indicated
? e.g., by a different state label ? subsequent mentions
of the toponym in the story can be identified with the first,
unambiguous reference. News stories often also have to-
ponyms in their datelines that are disambiguated. Our
news training corpus consists of two years (1989-90) of
AP wire and two months (October, November, 1998) of
Topic Detection and Tracking (TDT) data. The test set
is the December, 1998, TDT data. See table 1 for the
numbers of toponyms in the corpora.
In contrast to news texts, historical documents exhibit
a higher density of geographical reference and level of
ambiguity. To test the performance of our minimally-
supervised classifiers in a particularly challenging do-
main, we test it on a corpus of historical documents where
all place names have been marked and disambiguated. As
with news texts, we initially train and test our classifiers
on raw text. The range of geographic reference in these
texts is somewhat similar to American news text: the cor-
pus comprises the Personal Memoirs of Ulysses S. Grant
and two nineteenth-century books of travel about Califor-
nia and Minnesota from the Library of Congress? Amer-
ican Memory project.1 In all, we thus have about 600
pages of tagged historical text.
2 Experimental Setup
Dividing the corpora in training and test data, we train
Naive Bayes classifiers on all examples of disambiguated
toponyms in the training set. Although it is not uncom-
mon for two places in the same state, for example, to
share a name, we define disambiguation for purposes of
these experiments as finding the correct U.S. state or for-
eign country. This asymmetry is reflected in U.S. news
and historical text of the training data, where toponyms
are specified by U.S. states or by foreign countries. We
then run the classifiers on the test text with disambiguat-
ing labels, such as state or country names that immedi-
ately follow the city name, removed.
Since not all toponyms in the test set will have been
seen in training, we also train backoff classifiers to guess
the states and countries related to a story. If, for exam-
ple, we cannot find a classifier for ?Oxford?, but can
tell that a story is about Mississippi, we will still be
able to disambiguate. We use a gazetteer to restrict the
set of candidate states and countries for a given place
name. In trying to disambiguate ?Portland?, we would
thus consider Oregon, Maine, and England, among other
options, but not Maryland. As in the word sense dis-
ambiguation task as usually defined, we are classifying
names and not clustering them. This approach is prac-
tical for geographic names, for which broad-coverage
gazetteers exist, though less so for personal names (Mann
and Yarowsky, 2003). System performance is measured
with reference to the naive baseline where each ambigu-
ous toponym is guessed to be the most commonly oc-
curring place. London, England, would thus always
be guessed rather than London, Ontario. Bootstrapping
methods similar to ours have been shown to be compet-
itive in word sense disambiguation (Yarowsky and Flo-
rian, 2003; Yarowsky, 1995).
3 Difficulty of the Task
Our ability to disambiguate place names should be
weighed against the ease or difficulty of the task. In a
world where most toponyms referred unambiguously to
one place, we would not be impressed by near-perfect
performance.
Before considering how toponyms are used in text, we
can examine the inherent ambiguity of place names in
1Our annotated data also includes disambiguated texts of
Herodotus? Histories and Caesar?s Gallic War, but toponyms in
the ancient (especially Greek) world do not show enough ambi-
guity with personal names or with each other to be interesting.
Corpus Train Test Tagged
News 80,366 1464 0
Am. Mem. 11,877 3782 342
Civ. War 59,994 787 4153
Table 1: Experimental corpora with toponym counts in
unsupervised training and test and hand-tagged test sec-
tions.
Continent % places % names
w/mult. names w/mult. places
N. & Cent. America 11.5 57.1
Oceania 6.9 29.2
South America 11.6 25.0
Asia 32.7 20.3
Africa 27.0 18.2
Europe 18.2 16.6
Table 2: Places with multiple names and names applied
to more than one place in the Getty Thesaurus of Geo-
graphic Names
isolation. The Getty Thesaurus of Geographic Names,
with over a million toponyms, not only synthesizes many
contemporary gazetteers but also contains a wealth of his-
torical names. In table 2, we summarize for each conti-
nent the proportion of places that have multiple names
and of names that can refer to more than one place. Al-
though these proportions are dependent on the names
and places selected for inclusion in this gazetteer, the
relative rankings are suggestive. In areas with more
copious historical records?such as Asia, Africa, and
Europe?a place may be called by many names over
time, but individual names are often distinct. With the
increasing tempo of settlement in modern times, how-
ever, many places may be called by the same name, par-
ticularly by nostalgic colonists in the New World. Other
ambiguities arise when people and places share names.
Very few Greek and Latin place names are also personal
names.2 This is less true of Britain, where surnames
(and surnames used as given names) are often taken from
place names; in America, the confusion grows as numer-
ous towns are named after prominent or obscure peo-
ple. What may be called a lack of imagination in the
many 41 Oxfords, 73 Springfields, 91 Washingtons, and
97 Georgetowns seems to plague the very area ? North
America ? covered by our corpora.
If, however, one Washington or Portland predominates
in actual usage, things are not as bad as they seem. At the
2In Herodotus, for example, the only ambiguities between
people and places are for foreign names such as ?Ninus?, the
name used of Nineveh and of its mythical king.
Corpus H(class) H(class|name) % ambig.
News 6.453 0.241 12.71
Am. Mem. 4.519 0.525 18.81
Civ. War 4.323 0.489 18.49
Table 3: Entropy (H) of the state/country classification
task
very worst, for a baseline system, one can always guess
the most predominant referent. We quantify the level of
uncertainty in our corpora using entropy and average con-
ditional entropy. As stated above, we have simplified the
disambiguation problem to finding the state or country to
which a place belongs. For our training corpora, we can
thus measure the entropy of the classification and the av-
erage conditional entropy of the classification given the
specific place name (table 3). These entropies were cal-
culated using unsmoothed relative frequencies. The con-
ditional entropy, not surprisingly, is fairly low, given that
the percentage of toponyms that refer to more than one
place in the training data is quite low. Since training data
do not perfectly predict test data, however, we have to
smooth these probabilities and entropy goes up.
4 Evaluation
We evaluate our system?s performance on geographic
name disambiguation using two tasks. For the first task,
we use the same sort of untagged raw text used in train-
ing. We simply find the toponyms with disambiguating
labels ? e.g., ?Portland, Maine? ?, remove the labels,
and see if the system can restore them from context. For
the second task, we use texts all of whose toponyms have
been marked and disambiguated. The earlier heuristic
system described in (Smith and Crane, 2001) was run on
the texts and all disambiguation choices were reviewed
by a human editor.
Table 4 shows the results of these experiments. The
baseline accuracy was briefly mentioned above: if a to-
ponym has been seen in training, select the state or coun-
try with which it was most frequently associated. If a site
was not seen, select the most frequent state or country
from among the candidates in the gazetteer. The columns
for ?seen? and ?new? provide separate accuracy rates for
toponyms that were seen in training and for those that
were not. Finally, the overall accuracy of the trained sys-
tem is reported. For the American Memory and Civil War
corpora, we report results on the hand-tagged as well as
the raw text.
Not surprisingly, in light of its lower conditional en-
tropy, disambiguation in news text was the most accurate,
at 87.38%. Not only was the system accurate on news text
overall, but it degraded the least for unseen toponyms.
The relative accuracy on the American Memory and Civil
Corpus Baseline Seen New Overall
News 86.36 87.10 69.72 87.38
Am. Mem. 68.48 74.60 46.34 69.57
(tagged) 80.12 91.74 10.61 77.19
Civ. War 78.27 77.23 33.33 78.65
(tagged) 21.94 71.07 9.38 21.82
Table 4: Disambiguation accuracy (%) on test corpora.
Hand-tagged data were available for the American Mem-
ory and Civil War corpora.
War texts is also consistent with the entropies presented
above. The classifier shows a more marked degradation
when disambiguating toponyms not seen in training.
The accuracy of the classifier on restoring states and
countries in raw text is significantly, but not considerably,
higher than the baseline. It seems that many of toponyms
mentioned in text might be only loosely connected to the
surrounding discourse. An obituary, for example, might
mention that the deceased left a brother, John Doe, of Ar-
lington, Texas. Without tagging our test sets to mark such
tangential statements, it would be hard to weigh errors in
such cases appropriately.
Although accuracy on the hand-tagged data from the
American memory corpus was better than for the raw
text, performance on the Civil War tagged data (Grant?s
Memoirs) was abysmal. Most of this error seems came
from toponyms unseen in training, for with the accuracy
was 9.38%. In both sets of tagged text, moreover, the full
classifier performed below baseline accuracy due to prob-
lems with unseen toponyms. The back-off state models
are clearly inadequate for the minute topographical refer-
ences Grant makes in his descriptions of campaigns. In-
cluding proximity to other places mentioned is probably
the best way to overcome this difficulty. These problems
suggest that we need to more robustly generalize from the
kinds of environments with labelled toponyms to those
without.
5 Conclusions
Lack of labelled training or test data is the bane of many
word sense disambiguation efforts. For geographic name
disambiguation, we can extract training and test instances
from contexts where the toponyms are disambiguated by
the document?s author. Tagging accuracy is quite good,
especially for news texts, which have a lower entropy
in the disambiguation task. In real applications, how-
ever, we do not usually need to disambiguate toponyms
that already have state or country labels; we need to dis-
ambiguate unmarked place names. We investigated the
ability of our classifier to generalize by evaluating on
hand-corrected texts with all toponyms marked and dis-
ambiguated. The mixed results show that more gener-
alization power is needed in our models, particularly the
back-off models that handle toponyms unseen in training.
In future work, we hope to try further methods from
WSD such as decision lists and transformation-based
learning on the GND task. In any event, we hope that
this should improve the accuracy on toponyms seen in
training. As for disambiguating unseen toponyms, incor-
porating our prior work on heuristic proximity-base dis-
ambiguation into the probabilistic framework would be a
natural extension. A fully hand-corrected test corpus of
news text would also provide us with more robust evi-
dence for classifier generalization.
Evidence learned by classifiers to disambiguate to-
ponyms includes the names of prominent people and in-
dustries in a particular place, as well as the topics and
dates of current and historical events, and the titles of
newspapers (see figures 1 and 2). In our news training
corpus, for example, Hawaii was most strongly collo-
cated with ?lava? and Poland with ?solidarity? (case was
ignored). In addition to their use for GND, such associa-
tions should be useful in their own right for event detec-
tion (Smith, 2002), personal name disambiguation, and
augmenting the information in gazetteers.
References
[Kanada1999] Yasusi Kanada. 1999. A method of geo-
graphical name extraction from Japanese text for the-
matic geographical search. In Proceedings of the
Eighth International Conference on Information and
Knowledge Management, pages 46?54, Kansas City,
Missouri, November.
[Mann and Yarowsky2003] Gideon S. Mann and David
Yarowsky. 2003. Unsupervised personal name disam-
biguation. In CoNLL, Edmonton, Alberta. (to appear).
[McCurley2001] Kevin S. McCurley. 2001. Geospatial
mapping and navigation of the web. In Proceedings of
the Tenth International WWW Conference, pages 221?
229, Hong Kong, 1?5 May.
[Olligschlaeger and Hauptmann1999] Andreas M. Ol-
ligschlaeger and Alexander G. Hauptmann. 1999.
Multimodal information systems and GIS: The In-
formedia digital video library. In Proceedings of the
ESRI User Conference, San Diego, California, July.
[Smith and Crane2001] David A. Smith and Gregory
Crane. 2001. Disambiguating geographic names in
a historical digital library. In Proceedings of ECDL,
pages 127?136, Darmstadt, 4-9 September.
[Smith2002] David A. Smith. 2002. Detecting and
browsing events in unstructured text. In Proceedings
of the 25th Annual ACM SIGIR Conference, pages 73?
80, Tampere, Finland, August.
[Wacholder et al1997] Nina Wacholder, Yael Ravin, and
Misook Choi. 1997. Disambiguation of proper names
in text. In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 202?
208, Washington, DC, April. Association for Compu-
tational Linguistics.
[Yarowsky and Florian2003] David Yarowsky and Radu
Florian. 2003. Evaluating sense disambiguation per-
formance across diverse parameter spaces. Journal of
Natural Language Engineering, 9(1).
[Yarowsky1995] David Yarowsky. 1995. Unsuper-
vised word sense disambiguation rivaling supervised
mehtods. In Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 189?196.
NASHVILLE , Tenn - Singer Marie Osmond will
receive the 1988 Roy Acuff Community Service
Award from the Country Music Foundation. She will
be honored for her work as national chairwoman of
the Osmond Foundation ... The honor is named for a
Grand Ole Opry star known as ?the king of country
music?.
NASHVILLE , Tenn - The home of country music
is singing the blues after the sale of its last locally
owned music publising company to CBS Records.
Tree International Publishing, ranked as Billboard
magazine ?s No. 1 country music publisher for
the last 16 years, is being sold to New York-based
CBS for a reported $45 million to $50 million, The
Tennessean reported today.
NASHVILLE , Tenn - Country music entertainer
Johnny Cash was scheduled to be released from Bap-
tist Hospital Tuesday, two weeks after undergoing
heart bypass surgery, a hospital spokeswoman said
Monday ...
Figure 1: Documents with Dateline of Nashville, having
strong collocation country music
PORTLAND, Ore - Federal court hearing on whether
to permit logging on timber tracts where northern
spotted owl nests.
GRANTS PASS, Ore - ... ?As more and more federal
lands are set aside for spotted owls and other types
of wildlife and recreation areas, the land available
for perpetual commercial timber management de-
creases?...
SEATTLE - Interior Secretary Manuel Lujan says
federal law should allow economic considerations to
be taken into account in deciding whether to protect
species like the northern spotted owl...
SAN FRANCISCO - Environmental groups can sue
the government to try to stop logging of old-growth fir
near spotted owl nests in western Oregon, a federal
appeals court ruled Tuesday...
PORTLAND, Ore - Environmentalists trying to
protect the northern spotted owl cheered a federal
judge?s decision halting logging on five timber tracts...
WASHINGTON - Are the spotted owls that live in the
ancient forest of the Northwest really endangered or
are they being victimized by the miniature radio trans-
mitters that scientists use to track their movements?
SEATTLE - A federal court extended a ban Thursday
on U.S Forest Servi ce plans to sell nearly 1 billion
board feet of ancient timber from nine nationa l forests
in two states where the northern spotted owl lives.
Figure 2: A sample of new stories with the keyword spot-
ted owl, most are Oregon/Washington
Bilingual Parsing with Factored Estimation:
Using English to Parse Korean
David A. Smith and Noah A. Smith
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{d,n}asmith@cs.jhu.edu
Abstract
We describe how simple, commonly understood statisti-
cal models, such as statistical dependency parsers, proba-
bilistic context-free grammars, and word-to-word trans-
lation models, can be effectively combined into a uni-
fied bilingual parser that jointly searches for the best En-
glish parse, Korean parse, and word alignment, where
these hidden structures all constrain each other. The
model used for parsing is completely factored into the
two parsers and the TM, allowing separate parameter es-
timation. We evaluate our bilingual parser on the Penn
Korean Treebank and against several baseline systems
and show improvements parsing Korean with very lim-
ited labeled data.
1 Introduction
Consider the problem of parsing a language L for which
annotated resources like treebanks are scarce. Suppose
we have a small amount of text data with syntactic an-
notations and a fairly large corpus of parallel text, for
which the other language (e.g., English) is not resource-
impoverished. How might we exploit English parsers to
improve syntactic analysis tools for this language?
One idea (Yarowsky and Ngai, 2001; Hwa et al, 2002)
is to project English analysis onto L data, ?through?
word-aligned parallel text. To do this, we might use an
English parser to analyze the English side of the parallel
text and a word-alignment algorithm to induce word cor-
respondences. By positing a coupling of English syntax
with L syntax, we can induce structure on the L side of
the parallel text that is in some sense isomorphic to the
English parse.
We might take the projection idea a step farther. A
statistical English parser can tell us much more than the
hypothesized best parse. It can be used to find every
parse admitted by a grammar, and also scores of those
parses. Similarly, translation models, which yield word
alignments, can be used in principle to score competing
alignments and offer alternatives to a single-best align-
ment. It might also be beneficial to include the predic-
tions of an L parser, trained on any available annotated
L data, however few.
This paper describes how simple, commonly under-
stood statistical models?statistical dependency parsers,
probabilistic context-free grammars (PCFGs), and word
translation models (TMs)?can be effectively combined
into a unified framework that jointly searches for the best
English parse, L parse, and word alignment, where these
hidden structures are all constrained to be consistent.
This inference task is carried out by a bilingual parser.
At present, the model used for parsing is completely fac-
tored into the two parsers and the TM, allowing separate
parameter estimation.
First, we discuss bilingual parsing (?2) and show how
it can solve the problem of joint English-parse, L-parse,
and word-alignment inference. In ?3 we describe param-
eter estimation for each of the factored models, includ-
ing novel applications of log-linear models to English
dependency parsing and Korean morphological analysis.
?4 presents Korean parsing results with various mono-
lingual and bilingual algorithms, including our bilingual
parsing algorithm. We close by reviewing prior work in
areas related to this paper (?5).
2 Bilingual parsing
The joint model used by our bilingual parser is an in-
stance of a stochastic bilingual multitext grammar (2-
MTG), formally defined by Melamed (2003). The 2-
MTG formalism generates two strings such that each
syntactic constituent?including individual words?in
one side of the bitext corresponds either to a constituent
in the other side or to ?.
Melamed defines bilexicalized MTG (L2MTG), which
is a synchronous extension of bilexical grammars such
as those described in Eisner and Satta (1999) and applies
the latter?s algorithmic speedups to L2MTG-parsing.
Our formalism is not a precise fit to either unlexical-
ized MTG or L2MTG since we posit lexical dependency
structure only in one of the languages (English). The pri-
mary rationale for this is that we are dealing with only a
small quantity of labeled data in language L and there-
fore do not expect to be able to accurately estimate its
lexical affinities. Further, synchronous parsing is in prac-
tice computationally expensive, and eliminating lexical-
ization on one side reduces the run-time of the parser
from O(n8) to O(n7). Our parsing algorithm is a simple
transformation of Melamed?s R2D parser that eliminates
head information in all Korean parser items.
The model event space for our stochastic ?half-
bilexicalized? 2-MTG consists of rewrite rules of the fol-
lowing two forms, with English above and L below:
(
X[h1]
A ?
h1
h2
)
,
(
X[h1]
A ?
Y [h1]Z[c1]
BC
)
where upper-case symbols are nonterminals and lower-
case symbols are words (potentially ?). One approach
to assigning a probability to such a rule is to make an
independence assumption, for example:
Pr
bi
(
X[h]
A ?
Y [h1]Z[c]
BC
)
=
Pr
English
(X[h] ? Y [h1]Z[c1]) ? Pr
L
(A ? BC)
There are two powerful reasons to model the bilingual
grammar in this factored way. First, we know of no tree-
aligned corpora from which bilingual rewrite probabili-
ties could be estimated; this rules out the possibility of
supervised training of the joint rules. Second, separat-
ing the probabilities allows separate estimation of the
probabilities?resulting in two well-understood param-
eter estimation tasks which can be carried out indepen-
dently.1
This factored modeling approach bears a strong re-
semblance to the factored monolingual parser of Klein
and Manning (2002), which combined an English depen-
dency model and an unlexicalized PCFG. The generative
model used by Klein and Manning consisted of multiply-
ing the two component models; the model was therefore
deficient.
We go a step farther, replacing the deficient genera-
tive model with a log-linear model. The underlying pars-
ing algorithm remains the same, but the weights are no
longer constrained to sum to one. (Hereafter, we assume
weights are additive real values; a log-probability is an
example of a weight.) The weights may be estimated
using discriminative training (as we do for the English
model, ?3.1) or as if they were log-probabilities, using
smoothed maximum likelihood estimation (as we do for
the Korean model, ?3.3). Because we use this model only
for inference, it is not necessary to compute a partition
function for the combined log-linear model.
In addition to the two monolingual syntax models, we
add a word-to-word translation model to the mix. In
this paper we use a translation model to induce only a
single best word matching, but in principle the transla-
tion model could be used to weight all possible word-
word links, and the parser would solve the joint align-
ment/parsing problem.2
As a testbed for our experiments, the Penn Korean
Treebank (KTB; Han et al, 2002) provides 5,083 Ko-
rean constituency trees along with English translations
and their trees. The KTB also analyzes Korean words
into their component morphemes and morpheme tags,
which allowed us to train a morphological disambigua-
tion model.
To make the most of this small corpus, we performed
all our evaluations using five-fold cross-validation. Due
to the computational expense of bilingual parsing, we
1Of course, it might be the case that some information is known
about the relationship between the two languages. In that case, our log-
linear framework would allow the incorporation of additional bilingual
production features.
2Although polynomial, we found this to be too computationally de-
manding to do with our optimal parser in practice, but with pruning
and/or A? heuristics it is likely to be feasible.
produced a sub-corpus of the KTB limiting English sen-
tence length to 10 words, or 27% of the full data. We then
randomized the order of sentences and divided the data
into five equal test sets of 280 sentences each (?1,700
Korean words, ?2,100 English words). Complementing
each test set, the remaining data were used for training
sets of increasing size to simulate various levels of data
scarcity.
3 Parameter estimation
We now describe parameter estimation for the four com-
ponent models that combine to make our full system (Ta-
ble 1).
3.1 English syntax model
Our English syntax model is based on weighted bilexi-
cal dependencies. The model predicts the generation of
a child (POS tag, word) pair, dependent upon its parent
(tag, word) and the tag of the parent?s most recent child
on the same side (left or right). These events correspond
quite closely to the parser described by Eisner?s (1996)
model C, but instead of the rules receiving conditional
probabilities, we use a log-linear model and allow arbi-
trary weights. The model does not predict POS tags; it
assumes they are given, even in test.
Note that the dynamic program used for inference
of bilexical parses is indifferent to the origin of the
rule weights; they could be log-probabilities or arbitrary
numbers, as in our model. The parsing algorithm need
not change to accommodate the new parameterization.
In this model, the probability of a (sentence, tree) pair
(E, T ) is given by:
Pr(E, T ) =
exp (f(E, T ) ? ?)
?
E?,T ? exp(f(E
?, T ?) ? ?)
(1)
where ? are the model parameters and f is a vector func-
tion such that fi is equal to the number of times a feature
(e.g., a production rule) fires in (E, T ).
Parameter estimation consists of selecting weights ?
to maximize the conditional probability of the correct
parses given observed sentences:3
?
i
Pr(Ti|Si) =
?
i
exp (f(Ei, Ti) ? ?)
?
T ? exp(f(Ei, T
?) ? ?)
(2)
Another important advantage of moving to log-linear
models is the simple handling of data sparseness. The
feature templates used by our model are shown in Ta-
ble 2. The first feature corresponds to the fully-described
child-generation event; others are similar but less infor-
mative. These ?overlapping? features offer a kind of
backoff, so that each child-generation event?s weight re-
ceives a contribution from several granularities of de-
scription.
Feature selection is done by simple thresholding: if a
feature is observed 5 times or more in the training set,
its weight is estimated; otherwise its weight is locked at
3This can be done using iterative scaling or gradient-based numeri-
cal optimization methods, as we did.
Model Formalism Estimation Role
English syntax (?3.1) bilexical dependency discriminative estimation combines with Korean
grammar syntax for bilingual parsing
Korean morphology (?3.2) two-sequence discriminative estimation best analysis used as input
trigram model over a lattice to TM training and to parsing
Korean syntax (?3.3) PCFG smoothed MLE combines with English
syntax for bilingual parsing
Translation model (?3.4) IBM models 1?4, unsupervised estimation best analysis used as
both directions (approximation to EM) input to bilingual parsing
Table 1: A summary of the factored models described in this paper and their interactions.
?TP ,WP , TA, TC ,WC , D? ?TP , TA, TC , D?
?TP , TA, TC ,WC , D? ?TP ,WP , TA, TC , D?
?TP ,WP , stop, TC , D? ?TP , stop, TC , D?
Table 2: Feature templates used by the English depen-
dency parser. TX is a tag and WX is a word. P indi-
cates the parent, A the previous child, and C the next-
generated child. D is the direction (left or right). The
last two templates correspond to stopping.
0. If a feature is never seen in training data, we give it
the same weight as the minimum-valued feature from the
training set (?min). To handle out-of-vocabulary (OOV)
words, we treat any word seen for the first time in the
final 300 sentences of the training corpus as OOV. The
model is smoothed using a Gaussian prior with unit vari-
ance on every weight.
Because the left and right children of a parent are in-
dependent of each other, our model can be described as
a weighted split head automaton grammar (Eisner and
Satta, 1999). This allowed us to use Eisner and Satta?s
O(n3) parsing algorithm to speed up training.4 This
speedup could not, however, be applied to the bilingual
parsing algorithm since a split parsing algorithm will pre-
clude inference of certain configurations of word align-
ments that are allowed by a non-split parser (Melamed,
2003).
We trained the parser on sentences of 15 words or
fewer in the WSJ Treebank sections 01?21.5 99.49% de-
pendency attachment accuracy was achieved on the train-
ing set, and 76.68% and 75.00% were achieved on sec-
tions 22 and 23, respectively. Performance on the En-
glish side of our KTB test set was 71.82% (averaged
across 5 folds, ? = 1.75).
This type of discriminative training has been applied
to log-linear variants of hidden Markov models (Lafferty
et al, 2001) and to lexical-functional grammar (Johnson
et al, 1999; Riezler et al, 2002). To our knowledge, it
has not been explored for context-free models (includ-
ing bilexical dependency models like ours). A review
4Our split HAG?s head automaton states correspond to the POS tags
of the dependent words; this makes the head automaton deterministic
and offers an additional speedup.
5The parser does not model POS-tags; we assume they are known.
Head words in the WSJ corpus were obtained using R. Hwa?s
const2dep tool.
of discriminative approaches to parsing can be found in
Chiang (2003).
3.2 Korean morphological analysis
A Korean word typically consists of a head morpheme
followed by a series of closed-class dependent mor-
phemes such as case markers, copula, topicalizers, and
conjunctions. Since most of the semantic content re-
sides in the leading head morpheme, we eliminate for
word alignment all trailing morphemes, which reduces
the KTB?s vocabulary size from 10,052 to 3,104.
Existing morphological processing tools for many lan-
guages are often unweighted finite-state transducers that
encode the possible analyses for a surface form word.
One such tool, klex, is available for Korean (Han,
2004).
Unfortunately, while the unweighted FST describes
the set of valid analyses, it gives no way to choose
among them. We treat this as a noisy channel: Korean
morpheme-tag pairs are generated in sequence by some
process, then passed through a channel that turns them
into Korean words (with loss of information). The chan-
nel is given by the FST, but without any weights. To
select the best output, we model the source process.
We model the sequence of morphemes and their tags
as a log-linear trigram model. Overlapping trigram, bi-
gram, and unigram features provide backoff information
to deal with data sparseness (Table 3). For each training
sentence, we used the FST-encoded morphological dic-
tionary to construct a lattice of possible analyses. The
lattice has a ?sausage? form with all paths joining be-
tween each word.
We train the feature weights to maximize the weight
of the correct path relative to all paths in the lattice. In
contrast, Lafferty et al (2001) train to maximize the the
probability of the tags given the words. Over training
sentences, maximize:
?
i
Pr(Ti,Mi|lattice)
=
?
i
exp(f(Ti,Mi) ? ?)
?
(T ?,M ?)?lattice exp(f(T
?,M ?) ? ?)
(3)
where Ti is the correct tagging for sentence i, Mi is the
correct morpheme sequence.
There are a few complications. First, the coverage of
the FST is of course not universal; in fact, it cannot ana-
lyze 4.66% of word types (2.18% of tokens) in the KTB.
?Ti?2,Mi?2, Ti?1,Mi?1, Ti,Mi? ?Ti?
?Ti?2,Mi?2, Ti?1,Mi?1, Ti? ?Ti,Mi?
?Ti?2, Ti?1,Mi?1, Ti,Mi? ?Ti?1, Ti?
?Ti?2, Ti?1,Mi?1, Ti, ? ?Ti?1, Ti,Mi?
?Ti?2, Ti?1, Ti,Mi? ?Ti?1,Mi?1, Ti, ?
?Ti?1,Mi?1, Ti,Mi? ?Ti?2, Ti?1, Ti?
Table 3: Feature templates used by the Korean morphol-
ogy model. Tx is a tag, Mx is a morpheme.
We tag such words as atomic common nouns (the most
common tag). Second, many of the analyses in the KTB
are not admitted by the FST: 21.06% of correct analy-
ses (by token) are not admitted by the FST; 6.85% do
not have an FST analysis matching in the first tag and
morpheme, 3.63% do not have an FST analysis matching
the full tag sequence, and 1.22% do not have an analysis
matching the first tag. These do not include the 2.18%
of tokens with no analysis at all. When this happened in
training, we added the correct analysis to the lattice.
To perform inference on new data, we construct a lat-
tice from the FST (adding in any analyses of the word
seen in training) and use a dynamic program (essentially
the Viterbi algorithm) to find the best path through the
lattice. Unseen features are given the weight ?min. Ta-
ble 4 shows performance on ambiguous tokens in train-
ing and test data (averaged over five folds).
3.3 Korean syntax model
Because we are using small training sets, parameter esti-
mates for a lexicalized Korean probabilistic grammar are
likely to be highly unreliable due to sparse data. There-
fore we use an unlexicalized PCFG. Because the POS
tags are given by the morphological analyzer, the PCFG
need not predict words (i.e., head morphemes), only POS
tags.
Rule probabilities were estimated with MLE. Since
only the sentence nonterminal S was smoothed (using
add-0.1), the grammar could parse any sequence of tags
but was relatively sparse, which kept bilingual run-time
down. 6
When we combine the PCFG with the other models to
do joint bilingual parsing, we simply use the logs of the
PCFG probabilities as if they were log-linear weights.
A PCFG treated this way is a perfectly valid log-linear
model; the exponentials of its weights just happen to sat-
isfy certain sum-to-one constraints.
In the spirit of joint optimization, we might have also
combined the Korean morphology and syntax models
into one inference task. We did not do this, largely out of
concerns over computational expense (see the discussion
of translation models in ?3.4). This parser, independent
of the bilingual parser, is evaluated in ?4.
6We also found that this type of smoothing and smoothing all non-
terminals gave indistinguishable results on monolingual parsing. Al-
ternatively, we could have trained the PCFG discriminatively (treating
the PCFG rules as log-linear features), but because our training sets are
small we do not expect such training to be very different from training
the PCFG as a generative model with probabilities.
3.4 Translation model
In our bilingual parser, the English and Korean parses are
mediated through word-to-word translational correspon-
dence links. Unlike the syntax models, the translation
models were trained without the benefit of labeled data.
We used the GIZA++ implementation of the IBM statisti-
cal translation models (Brown et al, 1993; Och and Ney,
2003).
To obtain reliable word translation estimates, we
trained on a bilingual corpus in addition to the KTB
training set. The Foreign Broadcast Information Service
dataset contains about 99,000 sentences of Korean and
72,000 of English translation. For our training, we ex-
tracted a relatively small parallel corpus of about 19,000
high-confidence sentence pairs.
As noted above, Korean?s productive agglutinative
morphology leads to sparse estimates of word frequen-
cies. We therefore trained our translation models af-
ter replacing each Korean word with its first morpheme
stripped of its closed-class dependent morphemes, as de-
scribed in ?3.2.
The size of the translation tables made optimal bilin-
gual parsing prohibitive by exploding the number of
possible analyses. We therefore resorted to using
GIZA++?s hypothesized alignments. Since the IBM
models only hypothesize one-to-many alignments from
target to source, we trained using each side of the bitext
as source and target in turn. We could then produce two
kinds of alignment graphs by taking either the intersec-
tion or the union of the links in the two GIZA++ align-
ment graphs. All words not in the resulting alignment
graph are set to align to ?.
Our bilingual parser deals only in one-to-one align-
ments (mappings); the intersection graph yields a map-
ping. The union graph yields a set of links which may
permit different one-to-one mappings. Using the union
graph therefore allows for flexibility in the word align-
ments inferred by the bilingual parser, but this comes at
computational expense (because more analyses are per-
mitted).
Even with over 20,000 sentence pairs of training data,
the hypothesized alignments are relatively sparse. For
the intersection alignments, an average of 23% of non-
punctuation Korean words and 17% of non-punctuation
English words have a link to the other language. For the
union alignments, this improves to 88% for Korean and
22% for English.
A starker measure of alignment sparsity is the accu-
racy of English dependency links projected onto Korean.
Following Hwa et al (2002), we looked at dependency
links in the true English parses from the KTB where both
the dependent and the head were linked to words on the
Korean side using the intersection alignment. Note that
Hwa et al used not only the true English trees, but also
hand-produced alignments. If we hypothesize that, if En-
glish words i and j are in a parent-child relationship,
then so are their linked Korean words, then we infer an
incomplete dependency graph for the Korean sentences
whose precision is around 49%?53% but whose recall is
Training sentences All tags All morphemes First tag First morpheme
Training set accuracy 32 91.14 (1.41) 94.25 (2.59) 91.14 (1.41) 95.74 (2.49)
on ambiguous tokens 64 89.76 (0.34) 93.39 (1.12) 89.76 (0.34) 95.23 (1.43)
128 88.19 (0.91) 92.48 (1.25) 88.38 (1.08) 94.43 (1.02)
512 83.69 (0.94) 89.59 (0.27) 85.03 (1.08) 91.95 (0.21)
1024 82.55 (0.68) 89.28 (0.30) 84.22 (0.77) 91.67 (0.19)
Test set accuracy 32 59.34 (2.52) 53.13 (2.09) 72.81 (1.96) 84.99 (3.11)
on ambiguous tokens 64 59.34 (2.41) 54.76 (1.64) 72.68 (1.79) 85.54 (2.03)
128 60.85 (2.15) 57.20 (2.01) 74.44 (1.17) 86.29 (1.14)
512 63.99 (2.02) 63.24 (1.28) 75.14 (0.86) 85.82 (1.01)
1024 65.26 (1.85) 66.03 (1.72) 75.22 (1.25) 85.62 (1.08)
Table 4: Korean morphological analysis accuracy on ambiguous tokens in the training and test sets: means (and
standard deviations) are shown over five-fold cross-validation. Over 65% of word tokens are ambiguous. The accuracy
of the first tag in each word affects the PCFG and the accuracy of the first morpheme affects the translation model
(under our aggressive morphological lemmatization).
an abysmal 2.5%?3.6%. 7
4 Evaluation
Having trained each part of the model, we bring them
together in a unified dynamic program to perform infer-
ence on the bilingual text as described in ?2. In order to
experiment easily with different algorithms, we imple-
mented all the morphological disambiguation and pars-
ing models in this paper in Dyna, a new language for
weighted dynamic programming (Eisner et al, 2004).
For parameter estimation, we used the complementary
DynaMITE tool. Just as CKY parsing starts with words
in its chart, the dynamic program chart for the bilingual
parser is seeded with the links given in the hypothesized
word alignment.
All our current results are optimal under the model,
but as we scale up to more complex data, we might in-
troduce A? heuristics or, at the possible expense of opti-
mality, a beam search or pruning techniques. Our agenda
discipline is uniform-cost search, which guarantees that
the first full parse discovered will be optimal?if none of
the weights are positive. In our case we are maximizing
sums of negative weights, as if working with log proba-
bilities.8
When evaluating our parsing output against the test
data from the KTB, we do not claim credit for the sin-
gle outermost bracketing or for unary productions. Since
unary productions do not translate well from language to
language (Hwa et al, 2002), we collapse them to their
lower nodes.
4.1 Baseline systems
We compare our bilingual parser to several baseline sys-
tems. The first is the Korean PCFG trained on the small
7We approximated head-words in the Korean gold-standard trees
by assuming all structures to be head-final, with the exception of punc-
tuation. That is, the head-words of sister constituents will elect the
right-most, non-punctuation word among them as the head.
8In fact the English syntax model is not constrained to have non-
positive weights, but we decrement every parameter by ?max. For a
given sentence, this will reduce every possible parse?s weight by a con-
stant value, since the same number of features fire in every parse; thus,
the classification properties of the parser are not affected.
KTB training sets, as described in ?3.3. We also consider
Wu?s (1997) stochastic inversion transduction grammar
(SITG) as well as strictly left- and right-branching trees.
We report the results of five-fold cross-validation with
the mean and standard deviation (in parentheses).
Since it is unlexicalized, the PCFG parses sequences
of tags as output by the morphological analysis model.
By contrast, we can build translation tables for the SITG
directly from surface words?and thus not use any la-
beled training data at all?or from the sequence of head
morphemes. Experiments showed, however, that the
SITG using words consistently outperformed the SITG
using morphemes. We also implemented Wu?s tree-
transformation algorithm to turn full binary-branching
SITG output into flatter trees. Finally, we can provide
extra information to the SITG by giving it a set of En-
glish bracketings that it must respect when constructing
the joint tree. To get an upper bound on performance, we
used the true parses from the English side of the KTB.
Only the PCFG, of course, can be evaluated on la-
beled bracketing (Table 6). Although labeled precision
and recall on test data generally increase with more train-
ing data, the slightly lower performance at the highest
training set size may indicate overtraining of this simple
model. Unlabeled precision and recall show continued
improvement with more Korean training data.
Even with help from the true English trees, the unsu-
pervised SITGs underperform PCFGs trained on as few
as 32 sentences, with the exception of unlabeled recall in
one experiment. It seems that even some small amount
of knowledge of the language helps parsing. Crossing
brackets for the flattened SITG parses are understandably
lower.
4.2 Bilingual parsing
The output of our bilingual parser contains three types of
constituents: English-only (aligned to ?), Korean-only
(aligned to ?), and bilingual. The Korean parse induced
by the Korean-only and bilingual constituents is filtered
so constituents with intermediate labels (generated by the
binarization process) are eliminated.
A second filter we consider is to keep only the (re-
maining) bilingual constituents corresponding to an En-
glish head word?s maximal span. This filter will elimi-
nate constituents whose English correspondent is a head
word with some (but not all) of its dependents. Such par-
tial English constituents are by-products of the parsing
and do not correspond to the modeled syntax.
With good word alignments, the English parser can
help disambiguate Korean phrase boundaries and over-
come erroneous morphological analyses (Table 5). Re-
sults without and with the second filter are shown in
Table 7. Because larger training datasets lead to larger
PCFGs (with more rules), the grammar constant in-
creases. Our bilingual parser implementation is on the
cusp of practicality (in terms of memory requirements);
when the grammar constant increased, we were unable
to parse longer sentences. Therefore the results given for
bilingual parsing are on reduced test sets, where a length
filter was applied: sentences with |E| + |F | > ? were
removed, for varying values of ? .
4.3 Discussion
While neither bilingual parser consistently beats the
PCFG on its own, they offer slight, complementary im-
provements on small training datasets of 32 and 64 sen-
tences (Table 7). The bilingual parser without the En-
glish head span filter gives a small recall improvement
on average at similar precision. Neither of these differ-
ences is significant when measured with a paired-sample
t-test.
In contrast, the parser with the English head span filter
sacrifices significantly on recall for a small but signifi-
cant gain in precision at the 0.01 level. Crossing brack-
ets at all levels are significantly lower with the English
head span filter. We can describe this effect as a filtering
of Korean constituents by the English model and word
alignments. Constituents that are not strongly evident on
the English side are simply removed. On small train-
ing datasets, this effect is positive: although good con-
stituents are lost so that recall is poor compared to the
PCFG, precision and crossing brackets are improved.
As one would expect, as the amount of training data
increases, the advantage of using a bilingual parser
vanishes?there is no benefit from falling back on the
English parser and word alignments to help disambiguate
the Korean structure. Since we have not pruned our
search space in these experiments, we can be confident
that all variations are due to the influence of the transla-
tion and English syntax models.
Our approach has this principal advantage: the various
morphology, parsing, and alignment components can be
improved or replaced easily without needing to retrain
the other modules. The low dependency projection re-
sults (?3.4), in conjunction with our modest overall gains,
indicate that the alignment/translation model should re-
ceive the most attention. In all the bilingual experiments,
there is a small positive correlation (0.3), for sentences
at each length, between the proportion of Korean words
aligned to English and measures of parsing accuracy. Im-
proved English parsers?such as Collins? models?have
also been implemented in Dyna, the dynamic program-
ming framework used here (Eisner et al, 2004).
5 Prior work
Combining separately trained systems and then search-
ing for an (ideally) optimal solution is standard prac-
tice in statistical continuous speech recognition (Jelinek,
1998) and statistical machine translation (Brown et al,
1990). Composition is even more of a staple in finite-
state frameworks (Knight and Graehl, 1998). Finally,
factored models involving parses have been used to guide
search. Charniak et al (2003) combine separately
trained parse production probabilities with translation
probabilities to prune a parse forest hypothesized by the
translation model. As discussed in ?2, Klein and Man-
ning (2002) guide their parser?s search using a combina-
tion of separate unlexicalized PCFG and lexical depen-
dency models.
The extent to which assumptions about similarity of
syntax across languages are empirically valid has re-
ceived attention in a few pilot studies. Fox (2002) has
considered English and French, and Hwa et al (2002) in-
vestigate Chinese and English. Xia et al (2000) compare
the rule templates of lexicalized tree adjoining grammars
extracted from treebanks in English, Chinese, and Ko-
rean. In the context of machine translation, Dorr (1994)
investigated divergences between two languages? struc-
tures.
Some proposals have sidestepped the empirical issue
entirely. Wu (1997) and Alshawi et al (2000) used un-
supervised learning on parallel text to induce syntactic
analysis that was useful for their respective applications
in phrasal translation extraction and speech translation,
though not necessarily similar to what a human anno-
tator would select. Note a point of divergence of the
SITG from our bilingual parsing system: SITG only al-
lows words, but not higher structures, to match null in the
other language and thus requires that the trees in parallel
sentences be isomorphic. Yamada and Knight (2001) in-
troduced tree-to-string alignment on Japanese data, and
Gildea (2003) performed tree-to-tree alignment on the
Korean Treebank, allowing for non-isomorphic struc-
tures; he applied this to word-to-word alignment. Fi-
nally, inspired by these intuitive notions of translational
correspondence, Cherry and Lin (2003) include depen-
dency features in a word alignment model to improve
non-syntactic baseline systems.
In more formal work, Melamed (2003) proposes
multitext grammars and algorithms for parsing them.
Shieber and Schabes (1990) describe a synchronous tree
adjoining grammar. While both of these formalisms re-
quire bilingual grammar rules, Eisner (2003) describes
an algorithm for learning tree substitution grammars
from unaligned trees.
Working on the Penn Korean Treebank, Sarkar and
Han (2002) made a single training/test split and used
91% of the sentences to train a morphological disam-
biguator and lexicalized tree adjoining grammar (LTAG)
based parsing system.
For a monolingual approach to training a parser with
scarce resources, see (Steedman et al, 2003), who apply
co-training and corrected co-training to bootstrapping an
English parser starting with 1000 parsed training sen-
Truth [TOP [NP ngyen.tay/NNC kong.pyeng/NNC cwung.tay/NNC] [VP [NP ku/DAN to/NNC] ken.sel/NNC] ./SFN]
PCFG [TOP ngyen.tay/VV [S [NP kong.pyeng/NNC cwung.tay/NNC] [VP [NP ku/NPN to/NNX] ken.sel/NNC] ./SFN]]
Bilingual [TOP [NP ngyen.tay/VV kong.pyeng/NNC1 cwung.tay/NNC] [VP [NP ku/NPN to/NNX] ken.sel/NNC] ./SFN2]
Translation The regimental1 engineer company constructed that road .2
Truth [TOP [NP ku/DAN sa.lam/NNC] [NP ceng.chi/NNC kwun.kwan/NNC] ?/SFN]
PCFG [TOP [VP [NP ku/DAN sa.lam/NNC ceng.chi/NNC] kwun.kwan/NNC] ?/SFN]
Bilingual [TOP [NP ku/DAN1 sa.lam/NNC] [NP ceng.chi/NNC2 kwun.kwan/NNC3] ?/SFN4]
Translation He1 is a political2 officer3 ?4
Table 5: The gold-standard parse, PCFG parse, bilingual parse, and English translation for two selected test sentences.
GIZA-aligned words are coindexed with subscripts. The bilingual parser recovers from erroneous morphological
tagging in the first sentence and finds the proper NP bracketing in the second.
Method Training Unlabeled Unlabeled Labeled Labeled Crossing
Sentences Precision Recall Precision Recall Brackets
PCFG training 32 57.03 (5.45) 78.45 (5.71) 51.13 (6.14) 70.26 (6.40) 0.71 (0.22)
64 54.96 (4.98) 76.91 (6.71) 46.94 (4.38) 65.69 (5.99) 0.72 (0.25)
128 52.60 (3.15) 73.20 (4.97) 43.46 (3.34) 60.48 (5.14) 0.82 (0.18)
512 50.82 (1.46) 70.98 (2.00) 39.47 (2.49) 55.12 (3.42) 0.87 (0.06)
1024 50.25 (0.82) 70.31 (1.32) 37.93 (1.45) 53.07 (2.16) 0.89 (0.04)
PCFG test 32 43.63 (4.40) 45.96 (5.38) 31.67 (3.47) 33.36 (4.19) 1.27 (0.16)
64 45.90 (2.30) 46.68 (2.92) 34.29 (2.35) 34.91 (3.22) 1.18 (0.12)
128 48.07 (4.14) 48.47 (4.45) 36.39 (3.37) 36.68 (3.50) 1.15 (0.14)
512 50.88 (2.97) 51.89 (2.92) 38.10 (3.22) 38.82 (2.68) 1.10 (0.10)
1024 51.15 (2.17) 52.65 (1.74) 37.47 (1.89) 38.58 (1.64) 1.12 (0.08)
SITG ? 30.65 (1.97) 45.22 (3.43) ? ? 1.93 (0.17)
Flat SITG ? 41.78 (1.98) 33.59 (3.36) ? ? 0.94 (0.08)
SITG w/Eng. constit. ? 36.28 (0.70) 52.68 (1.03) ? ? 1.60 (0.07)
Flat SITG w/Eng. constit. ? 42.55 (1.32) 30.64 (1.37) ? ? 0.77 (0.06)
L-branching ? 25.62 (1.07) 35.83 (1.39) ? ? 2.04 (0.04)
R-branching ? 27.59 (1.03) 38.60 (1.75) ? ? 2.06 (0.11)
Table 6: Baseline parsing performance on Korean: the table shows means (and standard deviations) for five-fold cross-
validation. The SITG system is evaluated on test data, but is trained without labeled data; the SITG with English trees
uses true treebank English parses to constrain the search and thus represents an upper bound. The table shows means
and standard deviations for five-fold cross-validation. The best test results in each column are in bold.
Method Max. |E| + |F | Training Unlabeled Unlabeled Labeled Labeled Crossing
Test Sen. Length Sentences Precision Recall Precision Recall Brackets
PCFG 20 32 44.19 (4.41) 46.51 (5.32) 32.10 (3.47) 33.78 (4.14) 1.23 (0.16)
20 64 46.39 (2.45) 47.03 (3.01) 34.69 (2.40) 35.20 (3.22) 1.15 (0.11)
18 128 49.86 (4.83) 49.63 (4.74) 37.78 (3.74) 37.60 (3.61) 1.03 (0.13)
17 512 53.89 (3.60) 54.60 (3.73) 40.61 (3.84) 41.10 (3.19) 0.87 (0.11)
15 1024 57.87 (3.75) 59.39 (3.35) 43.92 (3.52) 45.07 (3.26) 0.61 (0.09)
Bilingual 20 32 44.17 (3.97) 47.10 (4.81) 31.67 (3.65) 33.78 (4.29) 1.22 (0.14)
parsing 20 64 46.30 (2.46) 47.73 (2.83) 34.14 (2.60) 35.23 (3.35) 1.15 (0.12)
18 128 48.75 (3.64) 49.51 (4.08) 36.95 (2.65) 37.52 (2.92) 1.04 (0.10)
17 512 52.77 (3.92) 54.21 (4.42) 39.73 (3.68) 40.78 (3.56) 0.88 (0.12)
15 1024 56.70 (4.79) 58.85 (4.10) 43.09 (4.24) 44.71 (3.69) 0.60 (0.12)
Bilingual 20 32 45.65 (5.81) 28.83 (4.35) 32.92 (4.60) 20.82 (3.53) 0.72 (0.11)
parsing, 20 64 47.15 (2.88) 28.73 (1.79) 34.65 (2.36) 21.14 (1.73) 0.68 (0.08)
English 18 128 49.65 (4.52) 28.74 (2.30) 38.62 (3.69) 22.35 (1.76) 0.59 (0.09)
head span 17 512 52.03 (4.21) 29.47 (2.71) 39.80 (2.92) 22.51 (1.32) 0.50 (0.08)
filter 15 1024 54.78 (5.20) 29.74 (1.91) 42.01 (5.05) 22.78 (1.84) 0.34 (0.09)
Table 7: Bilingual parsing performance on Korean: the table shows means (and standard deviations) for five-fold cross-
validation. Bold-faced numbers in the bilingual parsers indicate significant improvements on the PCFG baseline using
the paired-sample t-test at the 0.01 level.
tences. Although this technique has interesting proper-
ties, our combined optimization should be more stable
since it does not involve iterative example selection.
6 Conclusion
We have presented a novel technique for merging sim-
ple, separately trained models for Korean parsing, En-
glish dependency parsing, and word translation, and opti-
mizing the joint result using dynamic programming. We
showed small but significant improvements for Korean
parsers trained on small amounts of labeled data.
7 Acknowledgements
We would like to thank Elliott Dra?bek, Jason Eisner,
Eric Goldlust, Philip Resnik, Charles Schafer, David
Yarowsky, and the reviewers for their comments and as-
sistance and Chung-hye Han, Na-Rae Han, and Anoop
Sarkar for their help with the Korean resources. This
work was supported under a National Science Founda-
tion Graduate Research Fellowship and a Fannie and
John Hertz Foundation Fellowship.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learn-
ing dependency translation models as collections of
finite-state head transducers. Computational Linguis-
tics, 26(1):45?60.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.
Roossin. 1990. A statistical approach to machine trans-
lation. Computational Linguistics, 16(2):79?85.
P. E. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proc. MT Summit IX.
C. Cherry and D. Lin. 2003. A probability model to im-
prove word alignment. In Proc. ACL.
D. Chiang. 2003. Mildly context-sensitive grammars for
estimating maximum entropy models. In Proc. Formal
Grammar.
B. J. Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational
Linguistics, 20(4):597?633.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head automaton grammars.
In Proc. ACL.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna:
A declarative language for implementing dynamic pro-
grams. In ACL Companion Vol.
J. Eisner. 1996. An empirical comparison of probabil-
ity models for dependency grammar. Technical Report
IRCS-96-11, U. Penn.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL Companion Vol.
H. J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. EMNLP.
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. ACL.
C.-H. Han, N.-R. Han, E.-S. Ko, H. Yi, and M. Palmer.
2002. Penn Korean Treebank: Development and evalu-
ation. In Proc. Pacific Asian Conf. Language and Comp.
N.-R. Han. 2004. Klex: Finite-state lexical trans-
ducer for Korean. http://wave.ldc.upenn.edu/Catalog/-
CatalogEntry.jsp?catalogId=LDC2004L01.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using annota-
tion projection. In Proc. ACL.
F. Jelinek. 1998. Statistical Methods for Speech Recogni-
tion. MIT Press, Cambridge, MA.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Rie-
zler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proc. ACL.
D. Klein and C. D. Manning. 2002. Fast exact natural
language parsing with a factored model. In NIPS.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4).
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
I. D. Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proc. HLT-NAACL.
F.-J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computational
Linguistics, 29(1):19?51.
S. Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell,
and M. Johnson. 2002. Parsing the WSJ using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. ACL.
A. Sarkar and C.-H. Han. 2002. Statistical morphological
tagging and parsing of Korean with an LTAG grammar.
In Proc. TAG+6, pages 48?56.
S. M. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In Proc. ACL, pages 253?258.
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003.
Example selection for bootstrapping statistical parsers.
In Proc. HLT-NAACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?404.
F. Xia, C.-H. Han, M. Palmer, and A. Joshi. 2000. Com-
paring lexicalized treebank grammars extracted from
Chinese, Korean, and English corpora. In Proc. 2nd
Chinese Language Processing Workshop.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proc. ACL.
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proc. NAACL.
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 201?205, New York City, June 2006. c?2006 Association for Computational Linguistics
Vine Parsing and Minimum Risk Reranking for Speed and Precision?
Markus Dreyer, David A. Smith, and Noah A. Smith
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{markus,{d,n}asmith}@cs.jhu.edu
Abstract
We describe our entry in the CoNLL-X shared task.
The system consists of three phases: a probabilistic
vine parser (Eisner and N. Smith, 2005) that pro-
duces unlabeled dependency trees, a probabilistic
relation-labeling model, and a discriminative mini-
mum risk reranker (D. Smith and Eisner, 2006). The
system is designed for fast training and decoding and
for high precision. We describe sources of cross-
lingual error and ways to ameliorate them. We then
provide a detailed error analysis of parses produced
for sentences in German (much training data) and
Arabic (little training data).
1 Introduction
Standard state-of-the-art parsing systems (e.g.,
Charniak and Johnson, 2005) typically involve two
passes. First, a parser produces a list of the most
likely n parse trees under a generative, probabilistic
model (usually some flavor of PCFG). A discrim-
inative reranker then chooses among trees in this
list by using an extended feature set (Collins, 2000).
This paradigm has many advantages: PCFGs are
fast to train, can be very robust, and perform bet-
ter as more data is made available; and rerankers
train quickly (compared to discriminative models),
require few parameters, and permit arbitrary fea-
tures.
We describe such a system for dependency pars-
ing. Our shared task entry is a preliminary system
developed in only 3 person-weeks, and its accuracy
is typically one s.d. below the average across sys-
tems and 10?20 points below the best system. On
?This work was supported by NSF ITR grant IIS-0313193,
an NSF fellowship to the second author, and a Fannie and John
Hertz Foundation fellowship to the third author. The views ex-
pressed are not necessarily endorsed by the sponsors. We thank
Charles Schafer, Keith Hall, Jason Eisner, and Sanjeev Khudan-
pur for helpful conversations.
the positive side, its decoding algorithms have guar-
anteed O(n) runtime, and training takes only a cou-
ple of hours. Having designed primarily for speed
and robustness, we sacrifice accuracy. Better esti-
mation, reranking on larger datasets, and more fine-
grained parsing constraints are expected to boost ac-
curacy while maintaining speed.
2 Notation
Let a sentence x = ?x1, x2, ..., xn?, where each xi is
a tuple containing a part-of-speech tag ti and a word
wi, and possibly more information.1 x0 is a special
wall symbol, $, on the left. A dependency tree y
is defined by three functions: yleft and yright (both
{0, 1, 2, ..., n} ? 2{1,2,...,n}) that map each word to
its sets of left and right dependents, respectively, and
ylabel : {1, 2, ..., n} ? D, which labels the relation-
ship between word i and its parent from label set D.
In this work, the graph is constrained to be a pro-
jective tree rooted at $: each word except $ has a sin-
gle parent, and there are no cycles or crossing depen-
dencies. Using a simple dynamic program to find the
minimum-error projective parse, we find that assum-
ing projectivity need not harm accuracy very much
(Tab. 1, col. 3).
3 Unlabeled Parsing
The first component of our system is an unlabeled
parser that, given a sentence, finds the U best un-
labeled trees under a probabilistic model using a
bottom-up dynamic programming algorithm.2 The
model is a probabilistic head automaton grammar
(Alshawi, 1996) that assumes conditional indepen-
1We used words and fine tags in our parser and labeler, with
coarse tags in one backoff model. Other features are used in
reranking; we never used the given morphological features or
the ?projective? annotations offered in the training data.
2The execution model we use is best-first, exhaustive search,
as described in Eisner et al (2004). All of our dynamic pro-
gramming algorithms are implemented concisely in the Dyna
language.
201
B` Br
projective oracle
(B
` , B
r )-vine oracle
20-best unlabeled oracle
1-best unlabeled
unlabeled, reranked
20?50-best labeled oracle
1?1-best labeled
reranked (labeled)
(unlabeled)
(non-$ unl. recall)
(non-$ unl. precision)
Arabic 10 4 99.8 90.7 71.5 68.1 68.7 59.7 52.0 53.4 68.5 63.4 76.0
Bulgarian 5 4 99.6 90.7 86.4 80.1 80.5 85.1 73.0 74.8 82.0 74.3 86.3
Chinese 4 4 100.0 93.1 89.9 79.4 77.7 88.6 72.6 71.6 77.6 61.4 80.8
Czech 6 4 97.8 90.5 79.2 70.3 71.5 72.8 58.1 60.5 70.7 64.8 75.7
Danish 5 4 99.2 91.4 84.6 77.7 78.6 79.3 65.5 66.6 77.5 71.4 83.4
Dutch 6 5 94.6 88.3 77.5 67.9 68.8 73.6 59.4 61.6 68.3 60.4 73.0
German 8 7 98.8 90.9 83.4 75.5 76.2 82.3 70.1 71.0 77.0 70.2 82.9
Japanese 4 1 99.2 92.2 90.7 86.3 85.1 89.4 81.6 82.9 86.0 68.5 91.5
Portuguese 5 5 98.8 91.5 85.9 81.4 82.5 83.7 73.4 75.3 82.4 76.2 87.0
Slovene 6 4 98.5 91.7 80.5 72.0 73.3 72.8 57.5 58.7 72.9 66.3 78.5
Spanish 5 6 100.0 91.2 77.3 71.5 72.6 74.9 66.2 67.6 72.9 69.3 80.7
Swedish 4 5 99.7 94.0 87.5 79.3 79.6 81.0 65.5 67.6 79.5 72.6 83.3
Turkish 6 1 98.6 89.5 73.0 61.0 61.8 64.4 44.9 46.1 60.5 48.5 61.6
parser reranker labeler reranker
1 2 3 4 5 6 7 8 9 10 11 12 13
Table 1: Parameters and performance on test data. B` and Br were chosen to retain 90% of dependencies
in training data. We show oracle, 1-best, and reranked performance on the test set at different stages of the
system. Boldface marks oracle performance that, given perfect downstream modules, would supercede the
best system. Italics mark the few cases where the reranker increased error rate. Columns 8?10 show labeled
accuracy; column 10 gives the final shared task evaluation scores.
dence between the left yield and the right yield of
a given head, given the head (Eisner, 1997).3 The
best known parsing algorithm for such a model is
O(n3) (Eisner and Satta, 1999). The U -best list is
generated using Algorithm 3 of Huang and Chiang
(2005).
3.1 Vine parsing (dependency length bounds)
Following Eisner and N. Smith (2005), we also im-
pose a bound on the string distance between every
3To empirically test this assumption across languages, we
measured the mutual information between different features of
yleft(j) and yright(j), given xj . (Mutual information is a statis-
tic that equals zero iff conditional independence holds.) A de-
tailed discussion, while interesting, is omitted for space, but we
highlight some of our findings. First, unsurprisingly, the split-
head assumption appears to be less valid for languages with
freer word order (Czech, Slovene, German) and more valid for
more fixed-order languages (Chinese, Turkish, Arabic) or cor-
pora (Japanese). The children of verbs and conjunctions are the
most frequent violators. The mutual information between the
sequence of dependency labels on the left and on the right, given
the head?s (coarse) tag, only once exceeded 1 bit (Slovene).
child and its parent, with the exception of nodes at-
taching to $. Bounds of this kind are intended to im-
prove precision of non-$ attachments, perhaps sac-
rificing recall. Fixing bound B`, no left dependency
may exist between child xi and parent xj such that
j?i > B` (similarly for right dependencies and Br).
As a result, edge-factored parsing runtime is reduced
from O(n3) to O(n(B2` +B2r )). For each language,
we choose B` (Br) to be the minimum value that
will allow recovery of 90% of the left (right) depen-
dencies in the training corpus (Tab. 1, cols. 1, 2, and
4). In order to match the training data to the parsing
model, we re-attach disallowed long dependencies
to $ during training.
3.2 Estimation
The probability model predicts, for each parent word
xj , {xi}i?yleft (j) and {xi}i?yright (j). An advantage
of head automaton grammars is that, for a given par-
ent node xj , the children on the same side, yleft(j),
202
for example, can depend on each other (cf. McDon-
ald et al, 2005). Child nodes in our model are gener-
ated outward, conditional on the parent and the most
recent same-side sibling (MRSSS). This increases
our parser?s theoretical runtime to O(n(B3` + B3r )),
which we found was quite manageable.
Let pary : {1, 2, ..., n} ? {0, 1, ..., n} map each
node to its parent in y. Let predy : {1, 2, ..., n} ?
{?, 1, 2, ..., n} map each node to the MRSSS in y if
it exists and ? otherwise. Let ?i = |i ? j| if j is i?s
parent. Our (probability-deficient) model defines
p(y) =
n?
j=1
?
?
?
i?yleft (j)
p(xi,?i | xj , xpredy(i), left)
?
?
?p(STOP | xj , xminyleft (j) j , left)
?
?
?
?
i?yright (j)
p(xi,?i | xj ,predy(i), right)
?
?
?p(STOP | xj , xmaxyright (j) j , right) (1)
Due to the familiar sparse data problem, a maxi-
mum likelihood estimate for the ps in Eq. 1 performs
very badly (2?23% unlabeled accuracy). Good sta-
tistical parsers smooth those distributions by mak-
ing conditional independence assumptions among
variables, including backoff and factorization. Ar-
guably the choice of assumptions made (or interpo-
lated among) is central to the success of many exist-
ing parsers.
Noting that (a) there are exponentially many such
options, and (b) the best-performing independence
assumptions will almost certainly vary by language,
we use a mixture among 8 such models. The same
mixture is used for all languages. The models were
not chosen with particular care,4 and the mixture is
not trained?the coefficients are fixed at uniform,
with a unigram coarse-tag model for backoff. In
principle, this mixture should be trained (e.g., to
maximize likelihood or minimize error on a devel-
opment dataset).
The performance of our unlabeled model?s top
choice and the top-20 oracle are shown in Tab. 1,
cols. 5?6. In 5 languages (boldface), perfect label-
ing and reranking at this stage would have resulted in
performance superior to the language?s best labeled
4Our infrastructure provides a concise, interpreted language
for expressing the models to be mixed, so large-scale combina-
tion and comparison are possible.
system, although the oracle is never on par with the
best unlabeled performance.
4 Labeling
The second component of our system is a labeling
model that independently selects a label from D for
each parent/child pair in a tree. Given the U best
unlabeled trees for a sentence, the labeler produces
the L best labeled trees for each unlabeled one.
The computation involves an O(|D|n) dynamic pro-
gramming algorithm, the output of which is passed
to Huang and Chiang?s (2005) algorithm to generate
the L-best list.
We separate the labeler from the parser for two
reasons: speed and candidate diversity. In prin-
ciple the vine parser could jointly predict depen-
dency labels along with structures, but parsing run-
time would increase by at least a factor of |D|. The
two stage process also forces diversity in the candi-
date list (20 structures with 50 labelings each); the
1,000-best list of jointly-decoded parses often con-
tained many (bad) relabelings of the same tree.
In retrospect, assuming independence among de-
pendency labels damages performance substantially
for some languages (Turkish, Czech, Swedish, Dan-
ish, Slovene, and Arabic); note the often large drop
in oracle performance between Tab. 1, cols. 5 and
8. This assumption is necessary in our framework,
because the O(|D|M+1n) runtime of decoding with
an M th-order Markov model of labels5 is in general
prohibitive?in some cases |D| > 80. Pruning and
search heuristics might ameliorate runtime.
If xi is a child of xj in direction D, and xpred is
the MRSSS (possibly ?), where ?i = |i? j|, we es-
timate p(`, xi, xj , xpred ,?i | D) by a mixture (un-
trained, as in the parser) of four backed-off, factored
estimates.
After parsing and labeling, we have for each sen-
tence a list of U ? L candidates. Both the oracle
performance of the best candidate in the (20 ? 50)-
best list and the performance of the top candidate are
shown in Tab. 1, cols. 8?9. It should be clear from
the drop in both oracle and 1-best accuracy that our
labeling model is a major source of error.
5We tested first-order Markov models that conditioned on
parent or MRSSS dependency labels.
203
5 Reranking
We train a log-linear model combining many feature
scores (see below), including the log-probabilities
from the parser and labeler. Training minimizes
the expected error under the model; we use deter-
ministic annealing to smooth the error surface and
avoid local minima (Rose, 1998; D. Smith and Eis-
ner, 2006).
We reserved 200 sentences in each language for
training the reranker, plus 200 for choosing among
rerankers trained on different feature sets and differ-
ent (U ? L)-best lists.6
Features Our reranking features predict tags, la-
bels, lemmata, suffixes and other information given
all or some of the following non-local conditioning
context: bigrams and trigrams of tags or dependency
labels; parent and grandparent dependency labels;
subcategorization frames (in terms of tags or depen-
dency labels); the occurrence of certain tags between
head and child; surface features like the lemma7 and
the 3-character suffix. In some cases the children of
a node are considered all together, and in other cases
left and right are separated.
The highest-ranked features during training, for
all languages, are the parser and labeler probabil-
ities, followed by p(?i | tparent), p(direction |
tparent), p(label | labelpred , label succ , subcat), and
p(coarse(t) | D, coarse(tparent),Betw), where
Betw is TRUE iff an instance of the coarse tag type
with the highest mutual information between its left
and right children (usually verb) is between the child
and its head.
Feature and Model Selection For training speed
and to avoid overfitting, only a subset of the above
features are used in reranking. Subsets of differ-
ent sizes (10, 20, and 40, plus ?all?) are identified
for each language using two na??ve feature-selection
heuristics based on independent performance of fea-
tures. The feature subset with the highest accuracy
on the 200 heldout sentences is selected.
6In training our system, we made a serious mistake in train-
ing the reranker on only 200 sentences. As a result, our pre-
testing estimates of performance (on data reserved for model
selection) were very bad. The reranker, depending on condition,
had only 2?20 times as many examples as it had parameters to
estimate, with overfitting as the result.
7The first 4 characters of a word are used where the lemma
is not available.
Performance Accuracy of the top parses after
reranking is shown in Tab. 1, cols. 10?11. Reranking
almost always gave some improvement over 1-best
parsing.8 Because of the vine assumption and the
preprocessing step that re-attaches all distant chil-
dren to $, our parser learns to over-attach to $, treat-
ing $-attachment as a default/agnostic choice. For
many applications a local, incomplete parse may be
sufficiently useful, so we also measured non-$ unla-
beled precision and recall (Tab. 1, cols. 12?13); our
parser has > 80% precision on 8 of the languages.
We also applied reranking (with unlabeled features)
to the 20-best unlabeled parse lists (col. 7).
6 Error Analysis: German
The plurality of errors (38%) in German were er-
roneous $ attachments. For ROOT dependency la-
bels, we have a high recall (92.7%), but low pre-
cision (72.4%), due most likely to the dependency
length bounds. Among the most frequent tags, our
system has most trouble finding the correct heads of
prepositions (APPR), adverbs (ADV), finite auxil-
iary verbs (VAFIN), and conjunctions (KON), and
finding the correct dependency labels for preposi-
tions, nouns, and finite auxiliary verbs.
The German conjunction und is the single word
with the most frequent head attachment errors. In
many of these cases, our system does not learn
the subtle difference between enumerations that are
headed by A in A und B, with two children und and
B on the right, and those headed by B, with und and
A as children on its left.
Unlike in some languages, our labeled oracle ac-
curacy is nearly as good as our unlabeled oracle ac-
curacy (Tab. 1, cols. 8, 5). Among the ten most fre-
quent dependency labels, our system has the most
difficulty with accusative objects (OA), genitive at-
tributes (AG), and postnominal modifiers (MNR).
Accusative objects are often mistagged as subject
(SB), noun kernel modifiers (NK), or AG. About
32% of the postnominal modifier relations (ein Platz
in der Geschichte, ?a place in history?) are labeled
as modifiers (in die Stadt fliegen, ?fly into the city?).
Genitive attributes are often tagged as NK since both
are frequently realized as nouns.
8The exception is Chinese, where the training set for rerank-
ing is especially small (see fn. 6).
204
7 Error Analysis: Arabic
As with German, the greatest portion of Arabic er-
rors (40%) involved attachments to $. Prepositions
are consistently attached too low and accounted for
26% of errors. For example, if a form in construct
(idafa) governed both a following noun phrase and
a prepositional phrase, the preposition usually at-
taches to the lower noun phrase. Similarly, prepo-
sitions usually attach to nearby noun phrases when
they should attach to verbs farther to the left.
We see a more serious casualty of the dependency
length bounds with conjunctions. In ground truth
test data, 23 conjunctions are attached to $ and 141
to non-$ to using the COORD relation, whereas 100
conjunctions are attached to $ and 67 to non-$ us-
ing the AUXY relation. Our system overgeneralizes
and attaches 84% of COORD and 71% of AUXY
relations to $. Overall, conjunctions account for
15% of our errors. The AUXY relation is defined
as ?auxiliary (in compound expressions of various
kinds)?; in the data, it seems to be often used for
waw-consecutive or paratactic chaining of narrative
clauses. If the conjunction wa (?and?) begins a sen-
tence, then that conjunction is tagged in ground truth
as attaching to $; if the conjunction appears in the
middle of the sentence, it may or may not be at-
tached to $.
Noun attachments exhibit a more subtle problem.
The direction of system attachments is biased more
strongly to the left than is the case for the true data.
In canonical order, Arabic nouns do generally attach
on the right: subjects and objects follow the verb; in
construct, the governed noun follows its governor.
When the data deviate from this canonical order?
when, e.g, a subject precedes its verb?the system
prefers to find some other attachment point to the
left. Similarly, a noun to the left of a conjunction
often erroneously attaches to its left. Such ATR re-
lations account for 35% of noun-attachment errors.
8 Conclusion
The tradeoff between speed and accuracy is famil-
iar to any parsing researcher. Rather than starting
with an accurate system and then applying corpus-
specific speedups, we start by imposing carefully-
chosen constraints (projectivity and length bounds)
for speed, leaving accuracy to the parsing and
reranking models. As it stands, our system performs
poorly, largely because the estimation is not state-
of-the-art, but also in part due to dependency length
bounds, which are rather coarse at present. Better re-
sults are achievable by picking different bounds for
different head tags (Eisner and N. Smith, 2005). Ac-
curacy should not be difficult to improve using bet-
ter learning methods, especially given our models?
linear-time inference and decoding.
References
H. Alshawi. 1996. Head automata and bilingual
tiling: Translation with minimal representations.
In Proc. of ACL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL.
M. Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
J. Eisner and G. Satta. 1999. Efficient parsing
for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In
Proc. of IWPT.
J. Eisner, E. Goldlust, and N. A. Smith. 2004.
Dyna: A declarative language for implementing
dynamic programs. In Proc. of ACL (companion
volume).
J. Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proc. of IWPT.
L. Huang and D. Chiang. 2005. Better k-best pars-
ing. In Proc. of IWPT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proc. of HLT-
EMNLP.
K. Rose. 1998. Deterministic annealing for cluster-
ing, compression, classification, regression, and
related optimization problems. Proc. of the IEEE,
86(11):2210?2239.
D. A. Smith and J. Eisner. 2006. Minimum risk an-
nealing for training log-linear models. To appear
in Proc. of COLING-ACL.
205
Proceedings of the Workshop on Statistical Machine Translation, pages 23?30,
New York City, June 2006. c?2006 Association for Computational Linguistics
Quasi-Synchronous Grammars:
Alignment by Soft Projection of Syntactic Dependencies
David A. Smith and Jason Eisner
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
Many syntactic models in machine trans-
lation are channels that transform one
tree into another, or synchronous gram-
mars that generate trees in parallel. We
present a newmodel of the translation pro-
cess: quasi-synchronous grammar (QG).
Given a source-language parse tree T1, a
QG defines a monolingual grammar that
generates translations of T1. The trees
T2 allowed by this monolingual gram-
mar are inspired by pieces of substruc-
ture in T1 and aligned to T1 at those
points. We describe experiments learning
quasi-synchronous context-free grammars
from bitext. As with other monolingual
language models, we evaluate the cross-
entropy of QGs on unseen text and show
that a better fit to bilingual data is achieved
by allowing greater syntactic divergence.
When evaluated on a word alignment task,
QG matches standard baselines.
1 Motivation and Related Work
1.1 Sloppy Syntactic Alignment
This paper proposes a new type of syntax-based
model for machine translation and alignment. The
goal is to make use of syntactic formalisms, such as
context-free grammar or tree-substitution grammar,
without being overly constrained by them.
Let S1 and S2 denote the source and target sen-
tences. We seek to model the conditional probability
p(T2, A | T1) (1)
where T1 is a parse tree for S1, T2 is a parse tree
for S2, and A is a node-to-node alignment between
them. This model allows one to carry out a variety
of alignment and decoding tasks. Given T1, one can
translate it by finding the T2 and A that maximize
(1). Given T1 and T2, one can align them by finding
the A that maximizes (1) (equivalent to maximizing
p(A | T2, T1)). Similarly, one can align S1 and S2
by finding the parses T1 and T2, and alignment A,
that maximize p(T2, A | T1) ? p(T1 | S1), where
p(T1 | S1) is given by a monolingual parser. We
usually accomplish such maximizations by dynamic
programming.
Equation (1) does not assume that T1 and T2 are
isomorphic. For example, a model might judge T2
and A to be likely, given T1, provided that many?
but not necessarily all?of the syntactic dependen-
cies in T1 are aligned with corresponding depen-
dencies in T2. Hwa et al (2002) found that hu-
man translations from Chinese to English preserved
only 39?42% of the unlabeled Chinese dependen-
cies. They increased this figure to 67% by using
more involved heuristics for aligning dependencies
across these two languages. That suggests that (1)
should be defined to consider more than one depen-
dency at a time.
This inspires the key novel feature of our models:
A does not have to be a ?well-behaved? syntactic
alignment. Any portion of T2 can align to any por-
tion of T1, or to NULL. Nodes that are syntactically
related in T1 do not have to translate into nodes that
are syntactically related in T2?although (1) is usu-
ally higher if they do.
This property makes our approach especially
promising for aligning freely, or erroneously, trans-
lated sentences, and for coping with syntactic diver-
23
gences observed between even closely related lan-
guages (Dorr, 1994; Fox, 2002). We can patch to-
gether an alignment without accounting for all the
details of the translation process. For instance, per-
haps a source NP (figure 1) or PP (figure 2) appears
?out of place? in the target sentence. A linguist
might account for the position of the PP auf diese
Frage either syntactically (by invoking scrambling)
or semantically (by describing a deep analysis-
transfer-synthesis process in the translator?s head).
But an MT researcher may not have the wherewithal
to design, adequately train, and efficiently compute
with ?deep? accounts of this sort. Under our ap-
proach, it is possible to use a simple, tractable syn-
tactic model, but with some contextual probability
of ?sloppy? transfer.
1.2 From Synchronous to Quasi-Synchronous
Grammars
Because our approach will let anything align to
anything, it is reminiscent of IBM Models 1?5
(Brown et al, 1993). It differs from the many ap-
proaches where (1) is defined by a stochastic syn-
chronous grammar (Wu, 1997; Alshawi et al, 2000;
Yamada and Knight, 2001; Eisner, 2003; Gildea,
2003; Melamed, 2004) and from transfer-based sys-
tems defined by context-free grammars (Lavie et al,
2003).
The synchronous grammar approach, originally
due to Shieber and Schabes (1990), supposes that T2
is generated in lockstep to T1.1 When choosing how
to expand a certain VP node in T2, a synchronous
CFG process would observe that this node is aligned
to a node VP? in T1, which had been expanded in T1
by VP? ? NP? V?. This might bias it toward choos-
ing to expand the VP in T2 as VP ? V NP, with the
new children V aligned to V? and NP aligned to NP?.
The process then continues recursively by choosing
moves to expand these children.
One can regard this stochastic process as an in-
stance of analysis-transfer-synthesis MT. Analysis
chooses a parse T1 given S1. Transfer maps the
context-free rules in T1 to rules of T2. Synthesis
1The usual presentation describes a process that generates
T1 and T2 jointly, leading to a joint model p(T2, A, T1). Divid-
ing by the marginal p(T1) gives a conditional model p(T2, A |
T1) as in (1). In the text, we directly describe an equivalent
conditional process for generating T2, A given T1.
deterministically assembles the latter rules into an
actual tree T2 and reads off its yield S2.
What is worrisome about the synchronous pro-
cess is that it can only produce trees T2 that are
perfectly isomorphic to T1. It is possible to relax
this requirement by using synchronous grammar for-
malisms more sophisticated than CFG:2 one can per-
mit unaligned nodes (Yamada and Knight, 2001),
duplicated children (Gildea, 2003)3, or alignment
between elementary trees of differing sizes rather
than between single rules (Eisner, 2003; Ding and
Palmer, 2005; Quirk et al, 2005). However, one
would need rather powerful and slow grammar for-
malisms (Shieber and Schabes, 1990; Melamed et
al., 2004), often with discontiguous constituents, to
account for all the linguistic divergences that could
arise from different movement patterns (scrambling,
wh-in situ) or free translation. In particular, a syn-
chronous grammar cannot practically allow S2 to be
any permutation of S1, as IBM Models 1?5 do.
Our alternative is to define a ?quasi-synchronous?
stochastic process. It generates T2 in a way that is
not in thrall to T1 but is ?inspired by it.? (A human
translator might be imagined to behave similarly.)
When choosing how to expand nodes of T2, we are
influenced both by the structure of T1 and by mono-
lingual preferences about the structure of T2. Just as
conditional Markov models can more easily incor-
porate global features than HMMs, we can look at
the entire tree T1 at every stage in generating T2.
2 Quasi-Synchronous Grammar
Given an input S1 or its parse T1, a quasi-
synchronous grammar (QG) constructs a monolin-
gual grammar for parsing, or generating, the possi-
ble translations S2?that is, a grammar for finding
appropriate trees T2. What ties this target-language
grammar to the source-language input? The gram-
mar provides for target-language words to take on
2When one moves beyond CFG, the derived trees T1 and
T2 are still produced from a single derivation tree, but may be
shaped differently from the derivation tree and from each other.
3For tree-to-tree alignment, Gildea proposed a clone opera-
tion that allowed subtrees of the source tree to be reused in gen-
erating a target tree. In order to preserve dynamic programming
constraints, the identity of the cloned subtree is chosen indepen-
dently of its insertion point. This breakage of monotonic tree
alignment moves Gildea?s alignment model from synchronous
to quasi-synchronous.
24
Then:1 we:2
could:3
deal:4 .:10
with:5 later:9
Chernobyl:6
some:7
time:8
Tschernobyl/NE:6
koennte/VVFIN:3
dann/ADV:1 etwas/ADV:0 spaeter/ADJ:1 an/PREP:0 kommen/VVINF:0 ./S-SYMBOL:10
Reihe/NN:0
die/ART:0
Figure 1: German and English dependency parses and their alignments from our system where German
is the target language. Tschernobyl depends on ko?nnte even though their English analogues are not in a
dependency relationship. Note the parser?s error in not attaching etwas to spa?ter.
German: Tschernobyl ko?nnte dann etwas spa?ter an die Reihe kommen .
Literally: Chernobyl could then somewhat later on the queue come.
English: Then we could deal with Chernobyl some time later .
I:1
did:2
not:3 unfortunately:4 receive:5 .:11
answer:7
an:6 to:8
question:10
this:9
Auf/PREP:8
Frage/NN:10
diese/DEM:9
habe/VHFIN:2 ich/PPRO:1 leider/ADV:4
keine/INDEF:3
Antwort/NN:7
bekommen/VVpast:5
./S-SYMBOL:11
Figure 2: Here the German sentence exhibits scrambling of the phrase auf diese Frage and negates the object
of bekommen instead of the verb itself.
German: Auf diese Frage habe ich leider keine Antwort bekommen .
Literally: To this question have I unfortunately no answer received.
English: I did not unfortunately receive an answer to this question .
25
multiple hidden ?senses,? which correspond to (pos-
sibly empty sets of) word tokens in S1 or nodes in
T1. To take a familiar example, when parsing the
English side of a French-English bitext, the word
bank might have the sense banque (financial) in one
sentence and rive (littoral) in another.
The QG4 considers the ?sense? of the former bank
token to be a pointer to the particular banque token
to which it aligns. Thus, a particular assignment of
S1 ?senses? to word tokens in S2 encodes a word
alignment.
Now, selectional preferences in the monolingual
grammar can be influenced by these T1-specific
senses. So they can encode preferences for how T2
ought to copy the syntactic structure of T1. For ex-
ample, if T1 contains the phrase banque nationale,
then the QG for generating a corresponding T2 may
encourage any T2 English noun whose sense is
banque (more precisely, T1?s token of banque) to
generate an adjectival English modifier with sense
nationale. The exact probability of this, as well as
the likely identity and position of that English mod-
ifier (e.g., national bank), may also be influenced by
monolingual facts about English.
2.1 Definition
A quasi-synchronous grammar is a monolingual
grammar that generates translations of a source-
language sentence. Each state of this monolingual
grammar is annotated with a ?sense??a set of zero
or more nodes from the source tree or forest.
For example, consider a quasi-synchronous
context-free grammar (QCFG) for generating trans-
lations of a source tree T1. The QCFG generates the
target sentence using nonterminals from the cross
product U ? 2V1 , where U is the set of monolingual
target-language nonterminals such as NP, and V1 is
the set of nodes in T1.
Thus, a binarized QCFG has rules of the form
?A,?? ? ?B, ???C, ?? (2)
?A,?? ? w (3)
where A,B,C ? U are ordinary target-language
nonterminals, ?, ?, ? ? 2V1 are sets of source tree
4By abuse of terminology, we often use ?QG? to refer to the
T1-specific monolingual grammar, although the QG is properly
a recipe for constructing such a grammar from any input T1.
nodes to which A,B,C respectively align, and w is
a target-language terminal.
Similarly, a quasi-synchronous tree-substitution
grammar (QTSG) annotates the root and frontier
nodes of its elementary trees with sets of source
nodes from 2V1 .
2.2 Taming Source Nodes
This simple proposal, however, presents two main
difficulties. First, the number of possible senses for
each target node is exponential in the number of
source nodes. Second, note that the senses are sets
of source tree nodes, not word types or absolute sen-
tence positions as in some other translation models.
Except in the case of identical source trees, source
tree nodes will not recur between training and test.
To overcome the first problem, we want further re-
strictions on the set ? in a QG state such as ?A,??. It
should not be an arbitrary set of source nodes. In the
experiments of this paper, we adopt the simplest op-
tion of requiring |?| ? 1. Thus each node in the tar-
get tree is aligned to a single node in the source tree,
or to ? (the traditional NULL alignment). This allows
one-to-many but not many-to-one alignments.
To allow many-to-many alignments, one could
limit |?| to at most 2 or 3 source nodes, perhaps fur-
ther requiring the 2 or 3 source nodes to fall in a par-
ticular configuration within the source tree, such as
child-parent or child-parent-grandparent. With that
configurational requirement, the number of possi-
ble senses ? remains small?at most three times the
number of source nodes.
We must also deal with the menagerie of differ-
ent source tree nodes in different sentences. In other
words, how can we tie the parameters of the different
QGs that are used to generate translations of differ-
ent source sentences? The answer is that the proba-
bility or weight of a rule such as (2) should depend
on the specific nodes in ?, ?, and ? only through
their properties?e.g., their nonterminal labels, their
head words, and their grammatical relationship in
the source tree. Such properties do recur between
training and test.
For example, suppose for simplicity that |?| =
|?| = |?| = 1. Then the rewrite probabilities of (2)
and (3) could be log-linearly modeled using features
that ask whether the single node in ? has two chil-
dren in the source tree; whether its children in the
26
source are the nodes in ? and ?; whether its non-
terminal label in the source is A; whether its fringe
in the source translates as w; and so on. The model
should also consider monolingual features of (2) and
(3), evaluating in particular whether A ? BC is
likely in the target language.
Whether rule weights are given by factored gener-
ative models or by naive Bayes or log-linear models,
we want to score QG productions with a small set of
monolingual and bilingual features.
2.3 Synchronous Grammars Again
Finally, note that synchronous grammar is a special
case of quasi-synchronous grammar. In the context-
free case, a synchronous grammar restricts senses to
single nodes in the source tree and the NULL node.
Further, for any k-ary production
?X0, ?0? ? ?X1, ?1? . . . ?Xk, ?k?
a synchronous context-free grammar requires that
1. (?i 6= j) ?i 6= ?j unless ?i = NULL,
2. (?i > 0) ?i is a child of ?0 in the source tree,
unless ?i = NULL.
Since NULL has no children in the source tree, these
rules imply that the children of any node aligned to
NULL are themselves aligned to NULL. The con-
struction for synchronous tree-substitution and tree-
adjoining grammars goes through similarly but op-
erates on the derivation trees.
3 Parameterizing a QCFG
Recall that our goal is a conditional model of
p(T2, A | T1). For the remainder of this paper, we
adopt a dependency-tree representation of T1 and
T2. Each tree node represents a word of the sentence
together with a part-of-speech tag. Syntactic depen-
dencies in each tree are represented directly by the
parent-child relationships.
Why this representation? First, it helps us con-
cisely formulate a QG translation model where the
source dependencies influence the generation of tar-
get dependencies (see figure 3). Second, for evalu-
ation, it is trivial to obtain the word-to-word align-
ments from the node-to-node alignments. Third, the
part-of-speech tags are useful backoff features, and
in fact play a special role in our model below.
When stochastically generating a translation T2,
our quasi-synchronous generative process will be in-
fluenced by both fluency and adequacy. That is, it
considers both the local well-formedness of T2 (a
monolingual criterion) and T2?s local faithfulness
to T1 (a bilingual criterion). We combine these in
a simple generative model rather than a log-linear
model. When generating the children of a node in
T2, the process first generates their tags using mono-
lingual parameters (fluency), and then fills in in the
words using bilingual parameters (adequacy) that se-
lect and translate words from T1.5
Concretely, each node in T2 is labeled by a triple
(tag, word, aligned word). Given a parent node
(p, h, h?) in T2, we wish to generate sequences of
left and right child nodes, of the form (c, a, a?).
Our monolingual parameters come from a simple
generative model of syntax used for grammar induc-
tion: the Dependency Model with Valence (DMV) of
Klein and Manning (2004). In scoring dependency
attachments, DMV uses tags rather than words. The
parameters of the model are:
1. pchoose(c | p, dir): the probability of generat-
ing c as the next child tag in the sequence of
dir children, where dir ? {left, right}.
2. pstop(s | h, dir, adj): the probability of gener-
ating no more child tags in the sequence of dir
children. This is conditioned in part on the ?ad-
jacency? adj ? {true, false}, which indicates
whether the sequence of dir children is empty
so far.
Our bilingual parameters score word-to-word
translation and aligned dependency configurations.
We thus use the conditional probability ptrans(a |
a?) that source word a?, which may be NULL, trans-
lates as target word a. Finally, when a parent word
h aligned to h? generates a child, we stochastically
decide to align the child to a node a? in T1 with
one several possible relations to h?. A ?monotonic?
dependency alignment, for example, would have
h? and a? in a parent-child relationship like their
target-tree analogues. In different versions of the
model, we allowed various dependency alignment
configurations (figure 3). These configurations rep-
5This division of labor is somewhat artificial, and could be
remedied in a log-linear model, Naive Bayes model, or defi-
cient generative model that generates both tags and words con-
ditioned on both monolingual and bilingual context.
27
resent cases where the parent-child dependency be-
ing generated by the QG in the target language maps
onto source-language child-parent, for head swap-
ping; the same source node, for two-to-one align-
ment; nodes that are siblings or in a c-command re-
lationship, for scrambling and extraposition; or in
a grandparent-grandchild relationship, e.g. when a
preposition is inserted in the source language. We
also allowed a ?none-of-the-above? configuration, to
account for extremely mismatched sentences.
The probability of the target-language depen-
dency treelet rooted at h is thus:
P (D(h) | h, h?, p) =
?
dir?{l,r}
?
c?depsD(p,dir)
P (D(c) | a, a?, c) ? pstop(nostop | p, dir, adj)
?pchoose(c | p, dir)
?pconfig(config) ? ptrans(a | a
?)
pstop(stop | p, dir, adj)
4 Experiments
We claim that for modeling human-translated bitext,
it is better to project syntax only loosely. To evaluate
this claim, we train quasi-synchronous dependency
grammars that allow progressively more divergence
from monotonic tree alignment. We evaluate these
models on cross-entropy over held-out data and on
error rate in a word-alignment task.
One might doubt the use of dependency trees
for alignment, since Gildea (2004) found that con-
stituency trees aligned better. That experiment, how-
ever, aligned only the 1-best parse trees. We too will
consider only the 1-best source tree T1, but in con-
strast to Gildea, we will search for the target tree T2
that aligns best with T1. Finding T2 and the align-
ment is simply a matter of parsing S2 with the QG
derived from T1.
4.1 Data and Training
We performed our modeling experiments with the
German-English portion of the Europarl European
Parliament transcripts (Koehn, 2002). We obtained
monolingual parse trees from the Stanford German
and English parsers (Klein and Manning, 2003).
Initial estimates of lexical translation probabilities
came from the IBM Model 4 translation tables pro-
duced by GIZA++ (Brown et al, 1993; Och and
Ney, 2003).
All text was lowercased and numbers of two or
more digits were converted to an equal number of
hash signs. The bitext was divided into training
sets of 1K, 10K, and 100K sentence pairs. We held
out one thousand sentences for evaluating the cross-
entropy of the various models and hand-aligned
100 sentence pairs to evaluate alignment error rate
(AER).
We trained the model parameters on bitext using
the Expectation-Maximization (EM) algorithm. The
T1 tree is fully observed, but we parse the target lan-
guage. As noted, the initial lexical translation proba-
bilities came from IBM Model 4. We initialized the
monolingual DMV parameters in one of two ways:
using either simple tag co-occurrences as in (Klein
andManning, 2004) or ?supervised? counts from the
monolingual target-language parser. This latter ini-
tialization simulates the condition when one has a
small amount of bitext but a larger amount of tar-
get data for language modeling. As with any mono-
lingual grammar, we perform EM training with the
Inside-Outside algorithm, computing inside prob-
abilities with dynamic programming and outside
probabilities through backpropagation.
Searching the full space of target-language depen-
dency trees and alignments to the source tree con-
sumed several seconds per sentence. During train-
ing, therefore, we constrained alignments to come
from the union of GIZA++ Model 4 alignments.
These constraints were applied only during training
and not during evaluation of cross-entropy or AER.
4.2 Conditional Cross-Entropy of the Model
To test the explanatory power of our QCFG, we eval-
uated its conditional cross-entropy on held-out data
(table 1). In other words, we measured how well a
trained QCFG could predict the true translation of
novel source sentences by summing over all parses
of the target given the source. We trained QCFG
models under different conditions of bitext size and
parameter initialization. However, the principal in-
dependent variable was the set of dependency align-
ment configurations allowed.
From these cross-entropy results, it is clear that
strictly synchronous grammar is unwise. We ob-
28
(a) parent-child (b) child-parent (c) same node
sehe
ich
see
I
schwimmt
gern swimming
likes Voelkerrecht law
international
(d) siblings (e) grandparent-grandchild (f) c-commandbekommen
auf Antwort to
answer Wahlkampf
von
campaign
2003
2003
sagte
Was dass what
kaufte
bought
Figure 3: When a head h aligned to h? generates a new child a aligned to a? under the QCFG, h? and a? may be related in the
source tree as, among other things, (a) parent?child, (b) child?parent, (c) identical nodes, (d) siblings, (e) grandparent?grandchild,
(f) c-commander?c-commandee, (g) none of the above. Here German is the source and English is the target. Case (g), not pictured
above, can be seen in figure 1, in English-German order, where the child-parent pair Tschernobyl ko?nnte correspond to the words
Chernobyl and could, respectively. Since could dominates Chernobyl, they are not in a c-command relationship.
Permitted configurations CE CE CE
at 1k 10k 100k
? or parent-child (a) 43.82 22.40 13.44
+ child-parent (b) 41.27 21.73 12.62
+ same node (c) 41.01 21.50 12.38
+ all breakages (g) 35.63 18.72 11.27
+ siblings (d) 34.59 18.59 11.21
+ grandparent-grandchild (e) 34.52 18.55 11.17
+ c-command (f) 34.46 18.59 11.27
No alignments allowed 60.86 53.28 46.94
Table 1: Cross-entropy on held-out data with different depen-
dency configurations (figure 3) allowed, for 1k, 10k, and 100k
training sentences. The big error reductions arrive when we
allow arbitrary non-local alignments in condition (g). Distin-
guishing some common cases of non-local alignments improves
performance further. For comparison, we show cross-entropy
when every target language node is unaligned.
tain comparatively poor performance if we require
parent-child pairs in the target tree to align to parent-
child pairs in the source (or to parent-NULL or
NULL-NULL). Performance improves as we allow
and distinguish more alignment configurations.
4.3 Word Alignment
We computed standard measures of alignment preci-
sion, recall, and error rate on a test set of 100 hand-
aligned German sentence pairs with 1300 alignment
links. As with many word-alignment evaluations,
we do not score links to NULL. Just as for cross-
entropy, we see that more permissive alignments
lead to better performance (table 2).
Having selected the best system using the cross-
entropy measurement, we compare its alignment er-
ror rate against the standard GIZA++ Model 4 base-
lines. As Figure 4 shows, our QCFG for German ?
English consistently produces better alignments than
the Model 4 channel model for the same direction,
German ? English. This comparison is the appro-
priate one because both of these models are forced
to align each English word to at most one German
word. 6
5 Conclusions
With quasi-synchronous grammars, we have pre-
sented a new approach to syntactic MT: construct-
ing a monolingual target-language grammar that de-
scribes the aligned translations of a source-language
sentence. We described a simple parameterization
6For German ? English MT, one would use a German ?
English QCFG as above, but an English ? German channel
model. In this arguably inappropriate comparison, Figure 4
shows, the Model 4 channel model produces slightly better
word alignments than the QG.
29
Permitted configurations AER AER AER
at 1k 10k 100k
? or parent-child (a) 40.69 39.03 33.62
+ child-parent (b) 43.17 39.78 33.79
+ same node (c) 43.22 40.86 34.38
+ all breakages (g) 37.63 30.51 25.99
+ siblings (d) 37.87 33.36 29.27
+ grandparent-grandchild (e) 36.78 32.73 28.84
+ c-command (f) 37.04 33.51 27.45
Table 2: Alignment error rate (%) with different dependency
configurations allowed.
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 1000  10000  100000  1e+06
alig
nm
ent
 err
or r
ate
training sentence pairs
QCFGGiza4Giza4 bk
Figure 4: Alignment error rate with best model (all break-
ages). The QCFG consistently beat one GIZA++ model and
was close to the other.
with gradually increasing syntactic domains of lo-
cality, and estimated those parameters on German-
English bitext.
The QG formalism admits many more nuanced
options for features than we have exploited. In par-
ticular, we now are exploring log-linear QGs that
score overlapping elementary trees of T2 while con-
sidering the syntactic configuration and lexical con-
tent of the T1 nodes to which each elementary tree
aligns.
Even simple QGs, however, turned out to do quite
well. Our evaluation on a German-English word-
alignment task showed them to be competitive with
IBM model 4?consistently beating the German-
English direction by several percentage points of
alignment error rate and within 1% AER of the
English-German direction. In particular, alignment
accuracy benefited from allowing syntactic break-
ages between the two dependency structures.
We are also working on a translation decoding us-
ing QG. Our first system uses the QG to find optimal
T2 aligned to T1 and then extracts a synchronous
tree-substitution grammar from the aligned trees.
Our second system searches a target-language vo-
cabulary for the optimal T2 given the input T1.
Acknowledgements
This work was supported by a National Science
Foundation Graduate Research Fellowship for the
first author and by NSF Grant No. 0313193.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite state
head transducers. CL, 26(1):45?60.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. CL, 19(2):263?311.
Y. Ding and M. Palmer. 2005. Machine translation using prob-
abilistic synchronous dependency insertion grammars. In
ACL, pages 541?548.
B. J. Dorr. 1994. Machine translation divergences: A formal
description and proposed solution. Computational Linguis-
tics, 20(4):597?633.
J. Eisner. 2003. Learning non-isomorphic tree mappings for
machine translation. In ACL Companion Vol.
H. J. Fox. 2002. Phrasal cohesion and statistical machine trans-
lation. In EMNLP, pages 392?399.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In ACL, pages 80?87.
D. Gildea. 2004. Dependencies vs. constituents for tree-based
alignment. In EMNLP, pages 214?221.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evalu-
ating translational correspondence using annotation projec-
tion. In ACL.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL, pages 423?430.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL, pages 479?486.
P. Koehn. 2002. Europarl: A multilingual
corpus for evaluation of machine translation.
http://www.iccs.informatics.ed.ac.uk/?pkoehn/-
publications/europarl.ps.
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A. F.
Llitjo?s, R. Reynolds, J. Carbonell, and R. Cohen. 2003. Ex-
periments with a Hindi-to-English transfer-basedMT system
under a miserly data scenario. ACM Transactions on Asian
Language Information Processing, 2(2):143 ? 163.
I. D. Melamed, G. Satta, and B. Wellington. 2004. Generalized
multitext grammars. In ACL, pages 661?668.
I. D. Melamed. 2004. Statistical machine translation by pars-
ing. In ACL, pages 653?660.
F. J. Och and H. Ney. 2003. A systematic comparison of various
statistical alignment models. CL, 29(1):19?51.
C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency
treelet translation: Syntactically informed phrasal SMT. In
ACL, pages 271?279.
S. M. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In ACL, pages 253?258.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. CL, 23(3):377?403.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In ACL.
30
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 732?743, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Parse, Price and Cut?Delayed Column and Row Generation for Graph
Based Parsers
Sebastian Riedel David Smith Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{riedel,dasmith,mccallum}@cs.umass.edu
Abstract
Graph-based dependency parsers suffer from
the sheer number of higher order edges they
need to (a) score and (b) consider during opti-
mization. Here we show that when working
with LP relaxations, large fractions of these
edges can be pruned before they are fully
scored?without any loss of optimality guar-
antees and, hence, accuracy. This is achieved
by iteratively parsing with a subset of higher-
order edges, adding higher-order edges that
may improve the score of the current solu-
tion, and adding higher-order edges that are
implied by the current best first order edges.
This amounts to delayed column and row gen-
eration in the LP relaxation and is guaranteed
to provide the optimal LP solution. For second
order grandparent models, our method consid-
ers, or scores, no more than 6?13% of the sec-
ond order edges of the full model. This yields
up to an eightfold parsing speedup, while pro-
viding the same empirical accuracy and cer-
tificates of optimality as working with the full
LP relaxation. We also provide a tighter LP
formulation for grandparent models that leads
to a smaller integrality gap and higher speed.
1 Introduction
Many problems in NLP, and structured prediction in
general, can be cast as finding high-scoring struc-
tures based on a large set of candidate parts. For
example, in second order graph-based dependency
parsing (K?bler et al2009) we have to choose a
quadratic number of first order and a cubic number
of second order edges such that the graph is both
high-scoring and a tree. In coreference, we have
to select high-scoring clusters of mentions from an
exponential number of candidate clusters, such that
each mention is in exactly one cluster (Culotta et
al., 2007). In segmentation of citation strings, we
need to consider a quadratic number of possible seg-
ments such that every token is part of exactly one
segment (Poon and Domingos, 2007).
What makes such problems challenging is the
large number of possible parts to consider. This
number not only affects the cost of search or opti-
mization but also slows down the process of scor-
ing parts before they enter the optimization prob-
lem. For example, the cubic grandparent edges in
second-order dependency parsing slow down dy-
namic programs (McDonald and Pereira, 2006), be-
lief propagation (Smith and Eisner, 2008) and LP
solvers (Martins et al2009), since there are more
value functions to evaluate, more messages to pass,
or more variables to consider. But to even calculate
the score for each part we need a cubic number of
operations that usually involve expensive feature ex-
traction. This step often becomes a major bottleneck
in parsing, and structured prediction in general.
Candidate parts can often be heuristically pruned.
In the case of dependency parsing, previous work
has used coarse-to-fine strategies where simpler first
order models are used to prune unlikely first or-
der edges, and hence all corresponding higher or-
der edges (Koo and Collins, 2010; Martins et al
2009; Riedel and Clarke, 2006). While such meth-
ods can be effective, they are more convoluted, often
require training of addition models as well as tuning
of thresholding hyper-parameters, and usually pro-
vide no guarantees of optimality.
We present an approach that can solve problems
with large sets of candidate parts without consider-
ing all of these parts in either optimization or scor-
732
ing. And in contrast to most pruning heuristics, our
algorithm can give certificates of optimality before
having optimized over, or even scored, all parts. It
does so without the need of auxiliary models or tun-
ing of threshold parameters. This is achieved by a
delayed column and row generation algorithm that
iteratively solves an LP relaxation over a small sub-
set of current candidate parts, and then finds new
candidates that score highly and can be inserted into
the current optimal solution without removing high
scoring existing structure. The latter step subtracts
from the cost of a part the price of resources the part
requires, and is often referred as pricing. Sometimes
parts may score highly after pricing, but are neces-
sary in order to make the current solution feasible.
We add such parts in a step that roughly amounts to
violated cuts to the LP.
We illustrate our approach in terms of a second-
order grandparent model for dependency parsing.
We solve these models by iteratively parsing, pric-
ing, and cutting. To this end we use a variant of the
LP relaxation formulated by Martins et al2009).
Our variant of this LP is designed to be amenable to
column generation. It also turns out to be a tighter
outer bound that leads to fewer fractional solutions
and faster runtimes. To find high scoring grandpar-
ent edges without explicitly enumerating all of them,
we prune out a large fraction using factorized upper
bounds on grandparent scores.
Our parse, price and cut algorithm is evaluated
using a non-projective grandparent model on three
languages. Compared to a brute force approach of
solving the full LP, we only score about 10% of the
grandparent edges, consider only 8% in optimiza-
tion, and so observe an increase in parsing speed of
up to 750%. This is possible without loss of opti-
mality, and hence accuracy. We also find that our
extended LP formulation leads to a 15% reduction
of fractional solutions, up to 12 times higher speed,
and generally higher accuracy when compared to the
grandparent formulation of Martins et al2009).
2 Graph-Based Dependency Parsing
Dependency trees are representations of the syntac-
tic structure of a sentence (Nivre et al2007). They
determine, for each token of a sentence, the syntac-
tic head the token is modifying. As a lightweight al-
ternative to phrase-based constituency trees, depen-
dency representations have by now seen widespread
use in the community in various domains such as
question answering, machine translation, and infor-
mation extraction.
To simplify further exposition, we now formalize
the task, and mostly follow the notation of Martins et
al. (2009). Consider a sentence x = ?t0, t1, . . . , tn?
where t1, . . . , tn correspond to the n tokens of the
sentence, and t0 is an artificial root token. Let
V , {0, . . . , n} be a set of vertices corresponding
to the tokens in x, and C ? V ?V a set of candidate
directed edges. Then a directed graph y ? C is a
legal dependency parse if and only if it is a tree over
V rooted at vertex 0. Given a sentence x, we use Y
to denote the set of its legal parses. Note that all of
the above definitions depend on x, but for simplicity
we omit this dependency in our notation.
2.1 Arc-Factored Models
Graph-based models define parametrized scoring
functions that are trained to discriminate between
correct and incorrect parse trees. So called arc-
factored or first order models are the most basic
variant of such functions: they assess the quality of a
tree by scoring each edge in isolation (McDonald et
al., 2005b; McDonald et al2005a). Formally, arc-
factored models are scoring functions of the form
s (y;x,w) =
?
?h,m??y
s?h,m? (x,w) (1)
where w is a weight vector and s?h,m? (x,w) scores
the edge ?h,m? with respect to sentence x and
weightsw. From here on we will omit both x andw
from our notation if they are clear from the context.
Given such a scoring function, parsing amounts to
solving:
maximize
y
?
?h,m??y
s?h,m?
subject to y ? Y.
(2)
2.2 Higher Order Models
Arc-factored models cannot capture higher order de-
pendencies between two or more edges. Higher
order models remedy this by introducing scores
for larger configurations of edges appearing in the
733
tree (McDonald and Pereira, 2006). For example,
in grandparent models, the score of a tree also in-
cludes a score sgp?g,p,c? for each grandparent-parent-
child triple ?g, p, c?:
s (y) =
?
?h,m??y
s?h,m? +
?
?g,p??y,?p,c??y
sgp?g,p,c? (3)
There are other variants of higher order models
that include, in addition to grandparent triples, pairs
of siblings (adjacent or not) or third order edges.
However, to illustrate our approach we will focus
on grandparent models and note that most of what
we present can be generalized to other higher order
models.
2.3 Feature Templates
For our later exposition the factored and
parametrized nature of the scoring functions
will be crucial. In the following we therefore
illustrate this property in more detail.
The scoring functions for arcs or higher order
edges usually decompose into a sum of feature tem-
plate scores. For example, the grandparent edge
score sgp?g,p,c? is defined as
sgp?g,p,c? ,
?
t?T gp
sgp,t?g,p,c? (4)
where T gp is the set of grandparent templates, and
each template t ? T gp defines a scoring func-
tion sgp,t?g,p,c? to assess a specific property of the
grandparent-parent-child edge ?g, p, c?.
The template scores again decompose. Consider-
ing grandparent scores, we get
st?g,p,c? , w
>
t f
t (htg, h
t
p, h
t
c, d
t
g,p,c
)
(5)
where hti is an attribute of token ti, say h
101
i =
Part-of-Speech (ti). The term dtg,p,c corresponds to
a representation of the relation between tokens cor-
responding to g, p and g. For example, for template
101 it could return their relative positions to each
other:
d101g,p,c , ?I [g > p] , I [g > c] , I [p > c]? . (6)
The feature function f t maps the representations
of g, p and c into a vector space. For the purposes of
our work this mapping is not important, and hence
we omit details.
2.4 Learning
The scoring functions we consider are parametrized
by a family of per-template weight vectors w =
?wt?t?T . During learning we need to estimate w
such that our scoring functions learns to differenti-
ate between correct and incorrect parse trees. This
can be achieved in many ways: large margin train-
ing, maximizing conditional likelihood, or variants
in between. In this work we follow Smith and Eis-
ner (2008) and train the models with stochastic gra-
dient descent on the conditional log-likelihood of the
training data, using belief propagation in order to
calculate approximate gradients.
3 LP and ILP Formulations
Riedel and Clarke (2006) showed that dependency
parsing can be framed as Integer Linear Pro-
gram (ILP), and efficiently solved using an off-the-
shelf optimizer if a cutting plane approach is used.1
Compared to tailor made dynamic programs, such
generic solvers give the practitioner more modeling
flexibility (Martins et al2009), albeit at the cost
of efficiency. Likewise, compared to approximate
solvers, ILP and Linear Program (LP) formulations
can give strong guarantees of optimality. The study
of Linear LP relaxations of dependency parsing has
also lead to effective alternative methods for parsing,
such as dual decomposition (Koo et al2010; Rush
et al2010). As we see later, the capability of LP
solvers to calculate dual solutions is also crucial for
efficient and exact pruning. Note, however, that dy-
namic programs provide dual solutions as well (see
section 4.5 for more details).
3.1 Arc-Factored Models
To represent a parse y ? Y we first introduce an
vector of variables z , ?za?a where za is 1 if a ? y
and 0 otherwise. With this representation parsing
amounts to finding a vector z that corresponds to a
legal parse tree and that maximizes
?
a zasa. One
way to achieve this is to search through the convex
hull of all legal incidence vectors, knowing that any
linear objectives would take on its maximum on one
of the hull?s vertices. We will use Z to denote this
convex hull of incidence vectors of legal parse trees,
1Such as the highly efficient and free-for-academic-use
Gurobi solver.
734
and callZ the arborescence polytope (Martins et al
2009). The Minkowski-Weyl theorem tells us thatZ
can be represented as an intersection of halfspaces,
or constraints, Z = {z|Az ? b}. Hence optimal
dependency parsing, in theory, can be addressed us-
ing LPs.
However, it is difficult to describe Z with a com-
pact number of constraints and variables that lend
themselves to efficient optimization. In general we
therefore work with relaxations, or outer bounds, on
Z . Such outer bounds are designed to cut off all
illegal integer solutions of the problem, but still al-
low for fractional solutions. In case the optimum is
achieved at an integer vertex of the outer bound, it
is clear that we have found the optimal solution to
the original problem. In case we find a fractional
point, we need to map it onto Z (e.g., by projection
or rounding). Alternatively, we can use the outer
bound together with 0/1 constraints on z, and then
employ an ILP solver (say, branch-and-bound) to
find the true optimum. Given the NP-hardness of
ILP, this will generally be slow.
In the following we will present the outer bound
Z? ? Z proposed by Martins et al2009).
Compared to the representation Riedel and Clarke
(2006), this bound has the benefit a small polyno-
mial number of constraints. Note, however, that of-
ten exponentially many constraints can be efficiently
handled if polynomial separation algorithms exists,
and that such representations can lead to tighter
outer bounds.
The constraints we employ are:
No Head For Root In a dependency tree the root
node never has a head. While this could be captured
through linear constraints, it is easier to simply re-
strict the candidate set C to never contain edges of
the form ??, 0?.
Exactly One Head for Non-Roots Any non-root
token has to have exactly one head token. We can
enforce this property through the set of constraints:
m > 0 :
?
h
z?h,m? = 1. (OneHead)
No Cycles A parse tree cannot have cycles. This is
equivalent, together with the head constraints above,
to enforcing that the tree be fully connected. Mar-
tins et al2009) capture this connectivity constraint
using a single commodity flow formulation. This
requires the introduction of flow variables ? ,
??a?a?C . By enforcing that token 0 has n outgoing
flow, ?
m>0
??0,m? = n, (Source)
that any other token consumes one unit of flow,
t > 0 :
?
h
??h,t? ?
?
m>0
??t,m? = 1 (Consume)
and that flow is zero on disabled arcs
??h,m? ? nz?h,m?, (NoFlow)
connectivity can be ensured.
Assuming we have such a representation, parsing
with an LP relaxation amounts to solving
maximize
z?0
?
a?A
zasa
subject to A
[
z
?
]
? b.
(7)
3.2 Higher Order Models
The 1st-Order LP can be easily extended to capture
second (or higher) order models. For for the case
of grandparent models, this amounts to introduc-
ing another class of variables, zgpg,p,c, that indicate if
the parse contains both the edge ?g, p? and the edge
?p, c?. With the help of the indicators zgp we can rep-
resent the second order objective as a linear function.
We now need an outer bound on the convex hull of
vectors ?z, zgp? where z is a legal parse tree and zgp
is a consistent set of grandparent indicators. We will
refer to this convex hull as the grandparent polytope
Zgp.
We can re-use the constraints A of section 3.1 to
ensure that z is in Z . To make sure zgp is consistent
with z, Martins et al2009) linearize the equiva-
lence zgpg,p,c ? zg,p ? zp,c we know to hold for legal
incidence vectors, yielding
g, p, c : z?g,p? + z?p,c? ? z
gp
?g,p,c? ? 1 (ArcGP)
and
g, p, c : z?g,p? ? z
gp
?g,p,c?, z?p,c? ? z
gp
?g,p,c? (GPArc)
There are additional constraints we know to hold in
Zgp. First, we know that for any active edge ?p, c? ?
735
y with p > 0 there is exactly one grandparent edge
?g, p, c?. Likewise, for an inactive edge ?p, c? /? y
there must be no grandparent edge ?g, p, c?. This
can be captured through the constraint:
p > 0, c :
?
g
zgp?g,p,c? = z?p,c?. (OneGP)
We also know that if an edge ?g, p? in inactive,
there must not be any grandparent edge ?g, p, c? that
goes through ?g, p?:
g, p :
?
c
zgp?g,p,c? ? nz?g,p?. (NoGP)
It can be easily shown that for integer solu-
tions the constraints ArcGP and GPArc of Martins
et al2009) are sufficient conditions for consis-
tency between z and zgp. It can equally be shown
that the same holds for the constraints OneGP and
NoGP. However, when working with LP relax-
ations, the two polytopes have different fractional
vertices. Hence, by combining both constraint sets,
we can get a tighter outer bound on the grandparent
polytope Zgp. In section 6 we show empirically that
this combined polytope in fact leads to fewer frac-
tional solutions. Note that when using the union of
all four types of constraints, the NoGP constraint is
implied by the constraint GPArc (left) by summing
over c on both sides, and can hence be omitted.
4 Parse, Price and Cut
We now introduce our parsing algorithm. To this
end, we first give a general description of column
and row generation for LPs; then, we illustrate how
these techniques can be applied to dependency pars-
ing.
4.1 Column and Row Generation
LPs often have too many variables and constraints
to be efficiently solved. In such cases delayed
column and row generation can substantially re-
duce runtime by lazily adding variables only when
needed (Gilmore and Gomory, 1961; L?bbecke and
Desrosiers, 2004).
To illustrate column and row generation let us
consider the following general primal LP and its cor-
responding dual problem:
Primal
maximize
z?0
s?z
subject to Az ? b
Dual
minimize
??0
??b
subject to A?? ? s.
Say you are given a primal feasible z? and a dual fea-
sible ?? for which complementary slackness holds:
for all variables i we have z?i > 0? si =
?
j ?
?
jai,j
and for all constraints j we have ??j > 0 ? bj =?
i z
?
iai,j . In this case it is easy to show that z
? is
an optimal primal solution, ?? and optimal dual so-
lution, and that both objectives meet at these val-
ues (Bertsekas, 1999).
The idea behind delayed column and row gener-
ation is to only consider a small subset of variables
(or columns) I and subset of constraints (or rows) J .
Optimizing over this restricted problem, either with
an off-the-shelf solver or a more specialized method,
yields the pair
(
z?I ,?
?
J
)
of partial primal and dual
solutions. This pair is feasible and complementary
with respect to variables I and constraints J . We
can extend it to a solution (z?,y?) over all variables
and constraints by heuristically setting the remain-
ing primal and dual variables. If it so happens that
(z?,y?) is feasible and complementary for all vari-
ables and constraints, we have found the optimal so-
lution. If not, we add the constraints and variables
for which feasibility and slackness are violated, and
resolve the new partial problem.
In practice, the uninstantiated primal and dual
variables are often set to 0. In this case complemen-
tary slackness holds trivially, and we only need to
find violated primal and dual constraints. For primal
constraints,
?
i ziai,j ? bi, searching for violating
constraints j is the well-known separation step in
cutting plane algorithms. For the dual constraints,
?
j ?jai,j ? si, the same problem is referred to
as pricing. Pricing is often framed as searching for
all, or some, variables i with positive reduced cost
ri , si?
?
j ?jai,j . Note that while these problems
are, naturally, dual to each other, they can have very
different flavors. When we assess dual constraints
we need to calculate a cost si for variable i, and
usually this cost would be different for different i.
For primal constraints the corresponding right-hand-
sides are usually much more homogenous.
736
Algorithm 1 Parse, Price and Cut.
Require: Initial candidate edges and hyperedges P .
Ensure: The optimal z.
1: repeat
2: z,? ? parse(P )
3: N ? price(?)
4: M ? cut(z)
5: P ? P ?N ?M
6: until N = ? ?M = ?
7: return z
The reduced cost ri = si ?
?
j ?jai,j has sev-
eral interesting interpretations. First, intuitively it
measures the score we could gain by setting zi = 1,
and subtracts an estimate of what we would loose
because zi = 1 may compete with other variables
for shared resources (constraints). Second, it cor-
responds to the coefficient of zi in the Lagrangian
L (?, z) , s?z + ? [b?Az]. For any ?, Uzi=k =
maxz?0,zi=k L (?, z) is an upper bound on the best
possible primal objective with zi = k. This means
that ri = Uzi=1 ? Uzi=0 is the difference between
an upper bound that considers zi = 1, and one that
considers zi = 0. The tighter the bound Uzi=0 is,
the closer ri is to an upper bound on the maximal
increase we can get for setting zi to 1. At conver-
gence of column generation, complementary slack-
ness guarantees that Uzi=0 is tight for all z
?
i = 0, and
hence ri is a true an upper bound.
4.2 Application to Dependency Parsing
The grandparent formulation in section 3.2 has a cu-
bic number of variables z?g,p,c? as well as a cubic
number of constraints. For longer sentences this
number can slow us down in two ways. First, the
optimizer works with a large search space, and will
naturally become slower. Second, for every grand-
parent edge we need to calculate the score s?g,p,c?,
and this calculation can often be a major bottleneck,
in particular when using complex feature functions.
To overcome this bottleneck, our parse, price and cut
algorithm, as shown in algorithm 1, uses column and
row generation. In particular, it lazily instantiates
the grandparent edge variables zgp?g,p,c?, and the cor-
responding cubic number of constraints. All unin-
stantiated variables are implicitly set to 0.
The algorithm requires some initial set of vari-
ables to start with. In our case this set P contains all
first-order edges ?h,m? in the candidate set C, and
for each of these one grandparent edge ?0, h,m?.
The primary purpose of these grandparent edges is
to ensure feasibility of the OneGP constraints.
In step 2, the algorithm parses with the current
set of candidates P by solving the corresponding LP
relaxation. The LP contains all columns and con-
straints that involve the edges and grandparent edges
of P . The solver returns both the best primal solu-
tion z (for both edges and grandparents), and a com-
plementary dual solution ?.
In step 3 the dual variables? are used to find unin-
stantiated grandparent edges ?g, p, c? with positive
reduced cost. The price routine returns such edges
in N . In step 4 the primal solution is inspected for
violations of constraint ArcGP. The cut routine per-
forms this operation, and returns M , the set of edges
?g, p, c? that violate ArcGP.
In step 5 the algorithm converges if no more con-
straint violations, or promising new columns, can
be found. If there have been violations (M 6= ?)
or promising columns (N 6= ?), steps 2 to 4 are
repeated, with the newly found parts added to the
problem. Note that LP solvers can be efficiently
warm-started after columns and rows have been
added, and hence the cost of calls to the solver in
step 2 is substantially reduced after the first itera-
tion.
4.3 Pricing
In the pricing step we need to efficiently find a
set of grandparent edge variables zgp?g,p,c? with posi-
tive reduced cost, or the empty set if no such vari-
ables exist. Let ?OneGP?p,c? be the dual variables for
the OneGP constraints and ?NoGP?g,p? the duals for con-
straints NoGP. Then for the reduced cost of zgp?g,p,c?
we know that:
r?g,p,c? = s?g,p,c? ? ?
OneGP
?p,c? ? ?
NoGP
?g,p? . (8)
Notice that the duals for the remaining two con-
straints ArcGP and GPArc do not appear in this
equation. This is valid because we can safely set
their duals to zero without violating dual feasibility
or complementary slackness of the solution returned
by the solver.
737
4.3.1 Upper Bounds for Efficient Pricing
A naive pricing implementation would exhaus-
tively iterate over all ?g, p, c? and evaluate r?g,p,c?
for each. In this case we can still substantially re-
duce the number of grandparent variables that en-
ter the LP, provided many of these variables have
non-positive reduced cost. However, we still need to
calculate the score s?g,p,c? for each ?g, p, c?, an ex-
pensive operation we hope to avoid. In the follow-
ing we present an upper bound on the reduced cost,
r?gp?g,p,c? ? r
gp
?g,p,c?, which decomposes in a way that
allows for more efficient search. Using this bound,
we find all new grandparent edges N? for which this
upper bound is positive:
N? ?
{
?g, p, c? |r?gp?g,p,c? > 0
}
. (9)
Next we prune away all but the grandparent edges
for which the exact reduced cost is positive:
N ? N? \ {e : rgpe > 0} . (10)
Our bound r?gp?g,p,c? on the reduced cost of ?g, p, c?
is based on an upper bound s?gp?g,p,?? ? maxc s
gp
?g,p,c?
on the grandparent score involving ?g, p? as grand-
parent and parent, and the bound s?gp??,p,c? ?
maxg s
gp
?g,p,c? on the grandparent score involving
?p, c? as parent and child. Concretely, we have
r?gp?g,p,c? , min
(
s?gp?g,p,??, s?
gp
??,p,c?
)
? ?OneGP?p,c? ? ?
NoGP
?g,p? .
(11)
To find edges ?g, p, c? for which this bound is
positive, we can filter out all edges ?p, c? such that
sgp??,p,c?? ?
OneGP
?p,c? is non-positive. This is possible be-
cause NoGP is a? constraint and therefore ?NoGP?g,p? ?
0.2 Hence r?gp?g,p,c? is at most s?
gp
??,p,c? ? ?
OneGP
?p,c? . This
filtering step cuts off a substantial number of edges,
and is the main reason why can avoid scoring all
edges.
Next we filter, for each remaining ?p, c?, all pos-
sible grandparents g according to the definition of
r?gp?g,p,c?. This again allows us to avoid calling the
2Notice that in section 4.1 we discussed the LP dual in
case were all constraints are inequalities. When equality con-
straints are used, the corresponding dual variables have no sign
constraints. Hence we could not make the same argument for
?OneGP?p,c? .
grandparent scoring function on ?g, p, c?, and yields
the candidate set N? . Only if r?gp?g,p,c? is positive do we
have to evaluate the exact reduced cost and score.
4.3.2 Upper Bounds on Scores
What remains to be done is the calculation of up-
per bounds s?gp?g,p,?? and s?
gp
??,p,c?. Our bounds factor
into per-template bounds according to the definitions
in section 2.3. In particular, we have
s?gp??,p,c? ,
?
t?T gp
s?gp,t??,p,c? (12)
where s?t??,p,c? is a per-template upper bound defined
as
s?gp,t??,p,c? , max
v?range(ht)
e?range
`
dt
?
w>t f
t (v, htp, h
t
c, e
)
. (13)
That is, we maximize over all possible attribute val-
ues v any token g could have, and any possible rela-
tion e a token g can have to p and c.
Notice that these bounds can be calculated offline,
and hence amortize after deployment of the parser.
4.3.3 Tightening Duals
To price variables, we use the duals returned by
the solver. This is a valid default strategy, but may
lead to ? with overcautious reduced costs. Note,
however, that we can arbitrary alter ? to minimize
reduced costs of uninstantiated variables, as long as
we ensure that feasibility and complementary slack-
ness are maintained for the instantiated problem.
We use this flexibility for increasing ?OneGP?p,c? , and
hence lowering reduced costs zgp?g,p,c? for all tokens c.
Assume that z?p,c? = 0 and let r?p,c? = ?
OneGP
?p,c? + K
be the current reduced cost for z?p,c? in the instanti-
ated problem. Here K is a value depending on s?p,c?
and the remaining constraints z?p,c? is involved in.
We know that r?p,c? ? 0 due to dual feasibility
and hence r?p,c? may be 0, but note that r?p,c? < 0 in
many cases. In such cases we can increase ?OneGP?p,c?
to ?K and get r?p,c? = 0. With respect to z?p,c? this
maintains dual feasibility (because r?p,c? ? 0) and
complementary slackness (because z?p,c? = 0). Fur-
thermore, with respect to the zgp?g,p,c? for all tokens c
this also maintains feasibility (because the increased
?OneGP?p,c? appears with negative sign in 8) and com-
plementary slackness (because zgp?g,p,c? = 0 due to
z?p,c? = 0).
738
4.4 Separation
What happens if both z?g,p? and z?p,c? are active
while zgp?g,p,c? is still implicitly set to 0? In this case
we violate constraint ArcGP. We could remedy this
by adding the cut z?g,p? + z?p,c? ? 1, resolve the
LP, and then use the dual variable corresponding to
this constraint to get an updated reduced cost r?g,p,c?.
However, in practice we found this does not happen
as often, and when it does, it is cheaper for us to add
the corresponding column r?g,p,c? right away instead
of waiting to the next iteration to price it.
To find all pairs of variables for z?g,p? + z?p,c? ? 1
is violated, we first filter out all edges ?h,m? for
which z?h,m? = 0 as these automatically satisfy
any ArcGP constraint they appear in. Now for each
z?g,p? > 0 all z?p,c? > 0 are found, and if their sum
is larger than 1, the corresponding grandparent edge
?g, p, c? is returned in the result set.
4.5 Column Generation in Dynamic Programs
Column and Row Generation can substantially re-
duce the runtime of an off-the-shelf LP solver, as
we will find in section 6. Perhaps somewhat sur-
prisingly, it can also be applied in the context of dy-
namic programs. It is well known that for each dy-
namic program there is an equivalent polynomial LP
formulation (Martin et al1990). Roughly speak-
ing, in this formulation primal variables correspond
to state transitions, and dual variables to value func-
tions (e.g., the forward scores in the Viterbi algo-
rithm).
In pilot studies we have already used DCG to
speed up (exact) Viterbi on linear chains (Belanger
et al2012). We believe it could be equally applied
to dynamic programs for higher order dependency
parsing.
5 Related Work
Our work is most similar in spirit to the relaxation
method presented by Riedel and Smith (2010) that
incrementally adds second order edges to a graphi-
cal model based on a gain measure?the analog of
our reduced cost. However, they always score every
higher order edge, and also provide no certificates of
optimality.
Several works in parsing, and in MAP inference
in general, perform some variant of row genera-
tion (Riedel and Clarke, 2006; Tromble and Eis-
ner, 2006; Sontag and Jaakkola, 2007; Sontag et al
2008). However, none of the corresponding methods
lazily add columns, too. The cutting plane method
of Riedel (2008) can omit columns, but only if their
coefficient is negative. By using the notion of re-
duced costs we can also omit columns with positive
coefficient. Niepert (2010) applies column gener-
ation, but his method is limited to the case of k-
Bounded MAP Inference.
Several ILP and LP formulations of dependency
parsing have been proposed. Our formulation is in-
spired by Martins et al2009), and hence uses fewer
constraints than Riedel and Clarke (2006). For the
case of grandparent edges, our formulation also im-
proves upon the outer bound of Martins et al2009)
in terms of speed, tightness, and utility for column
generation. Other recent LP relaxations are based
on dual decomposition (Rush et al2010; Koo et
al., 2010; Martins et al2011). These relaxations
allow the practitioner to utilize tailor-made dynamic
programs for tractable substructure, but still every
edge needs to be scored. Given that column gener-
ation can also be applied in dynamic programs (see
section 4.5), our algorithm could in fact accelerate
dual decomposition parsing as well.
Pruning methods are a major part of many struc-
tured prediction algorithms in general, and of pars-
ing algorithms in particular (Charniak and Johnson,
2005; Martins et al2009; Koo and Collins, 2010;
Rush and Petrov, 2012). Generally these meth-
ods follow a coarse-to-fine scheme in which sim-
pler models filter out large fractions of edges. Such
methods are effective, but require tuning of thresh-
old parameters, training of additional models, and
generally lead to more complex pipelines that are
harder to analyze and have fewer theoretical guar-
antees.
A* search (Ahuja et al1993) has been used
to search for optimal parse trees, for example by
Klein and Manning (2003) or, for dependency pars-
ing, by Dienes et al2003). There is a direct rela-
tion between both A* and Column Generation based
on an LP formulation of the shortest path problem.
Roughly speaking, in this formulation any feasible
dual assignments correspond to a consistent (and
thus admissible) heuristic, and the corresponding re-
duced costs can be used as edge weights. Run-
739
ning Dijkstra?s algorithm with these weights then
amounts to A*. Column generation for the shortest
path problem can then be understood as a method to
lazily construct a consistent heuristic. In every step
this method finds edges for which consistency is vi-
olated, and updates the heuristic such that all these
edges are consistent.
6 Experiments
We claim that LP relaxations for higher order pars-
ing can be solved without considering, and scoring,
all candidate higher order edges. In practice, how
many grandparent edges do we need to score, and
how many do we need to add to the optimization
problem? And what kind of reduction in runtime
does this reduction in edges lead to?
We have also pointed out that our outer bound on
the grandparent polytope of legal edge and grand-
parent vectors is tighter than the one presented by
Martins et al2009). What effect does this bound
have on the number of fractional solutions and the
overall accuracy?
To answer these questions we will focus on a set
of non-projective grandparent models, but point out
that our method and formulation can be easily ex-
tended to projective parsing as well as other types
of higher order edges. We use the Danish test data
of Buchholz and Marsi (2006) and the Italian and
Hungarian test datasets of Nivre et al2007).
6.1 Impact of Price and Cut
Table 1 compares brute force optimization (BF) with
the full model, in spirit of Martins et al2009),
to running parse, price and cut (PPC) on the same
model. This model contains all constraints presented
in 3.2. The table shows the average number of
parsed sentences per second, the average objective,
number of grandparent edges scored and added, all
relative to the brute force approach. We also present
the average unlabeled accuracy, and the percentage
of sentences with integer solutions. This number
shows us how often we not only found the optimal
solution to the LP relaxation, but also the optimal
solution to the full ILP.
We first note that both systems achieve the same
objective, and therefore, also the same accuracy.
This is expected, given that column and row gen-
eration are known to yield optimal solutions. Next
we see that the number of grandparent edges scored
and added to the problem is reduced to 5?13% of the
full model. This leads to up to 760% improvement
in speed. This improvement comes for free, without
any sacrifice in optimality or guarantees. We also
notice that in all cases at least 97% of the sentences
have no fractional solutions, and are therefore opti-
mal even with respect to the ILP. Table 1 also shows
that our bounds on reduced costs are relatively tight.
For example, in the case of Italian we score only
one percent more grandparent edges than we actu-
ally need to add.
Our fastest PCC parser processes about one sen-
tence per second. This speed falls below the reported
numbers of Martins et al2009) of about 0.6 sec-
onds per sentence. Crucially, however, in contrast to
their work, our speed is achieved without any first-
order pruning. In addition, we expect further im-
provements in runtime by optimizing the implemen-
tation of our pricing algorithm.
6.2 Tighter Grandparent Polytope
To investigate how the additional grandparent con-
straints in section 3.2 help, we compare three mod-
els, this time without PPC. The first model follows
Martins et al2009) and uses constraints ArcGP and
GPArc only. The second model uses only constraints
OneGP and NoGP. The final model incorporates all
four constraints.
Table 2 shows speed relative to the baseline model
with constraints ArcGP and GPArc, as well as the
percentage of integer solutions and the average un-
labeled accuracy?all for the Italian and Hungarian
datasets. We notice that the full model has less frac-
tional solutions than the partial models, and either
substantially (Italian) or slightly (Hungarian) faster
runtimes than ArcGP+GPArc. Interestingly, both
sets of constraints in isolation perform worse, in par-
ticular the OneGP and NoGP model.
7 Conclusion
We have presented a novel method for parsing in
second order grandparent models, and a general
blueprint for more efficient and optimal structured
prediction. Our method lazily instantiates candidate
parts based on their reduced cost, and on constraint
740
Italian Hungarian Danish
BF PPC BF PPC BF PPC
Sent./sec. relative to BF 100% 760% 100% 380% 100% 390%
GPs Scored relative to BF 100% 6% 100% 12% 100% 13%
GPs Added relative to BF 100% 5% 100% 7% 100% 7%
Objective rel. to BF 100% 100% 100% 100% 100% 100%
% of Integer Solutions 98% 98% 97% 97% 97% 97%
Unlabeled Acc. 88% 88% 81% 81% 88% 88%
Table 1: Parse, Price and Cut (PPC) vs Brute Force (BF). Speed is the number of sentences per second,
relative to the speed of BF. Objective, GPs scored and added are also relative to BF.
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 1000% 1200%
% Integer 77% 9% 98%
Unlabeled Acc. 87% 85% 88%
(a) Italian
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 162% 105%
% Integer 71% 3% 97%
Unlabeled Acc. 80% 77% 81%
(b) Hungarian
Table 2: Different outer bounds on the grandpar-
ent polytope, for nonprojective parsing of Italian and
Danish.
violations. This allows us to discard a large fraction
of parts during both scoring and optimization, lead-
ing to nearly 800% speed-ups without loss of accu-
racy and certificates. We also present a tighter bound
on the grandparent polytope that is useful in its own
right.
Delayed column and row generation is very useful
when solving large LPs with off-the-shelf solvers.
Given the multitude of work in NLP that uses LPs
and ILPs in this way (Roth and Yih, 2004; Clarke
and Lapata, 2007), we hope that our approach will
prove itself useful for other applications. We stress
that this approach can also be used when working
with dynamic programs, as pointed out in section
4.5, and therefore also in the context of dual de-
composition. This suggests even wider applicabil-
ity, and usefulness in various structured prediction
problems.
The underlying paradigm could also be useful for
more approximate methods. In this paradigm, al-
gorithms maintain an estimate of the cost of certain
resources (duals), and use these estimates to guide
search and the propose new structures. For exam-
ple, a local-search based dependency parser could
estimate how contested certain tokens, or edges, are,
and then use these estimates to choose better next
proposals. The notion of reduced cost can give guid-
ance on what such estimates should look like.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval and the Univer-
sity of Massachusetts and in part by UPenn NSF
medium IIS-0803847. We gratefully acknowledge
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
References
Ravindra K. Ahuja, Thomas L. Magnanti, and James B.
Orlin. 1993. Network Flows: Theory, Algorithms, and
Applications. Prentice Hall, 1 edition, February.
David Belanger, Alexandre Passos, Sebastian Riedel, and
Andrew McCallum. 2012. A column generation ap-
proach to connecting regularization and map infer-
ence. In Inferning: Interactions between Inference
and Learning, ICML 2012 Workshop.
741
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific, 2nd edition, September.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natu-
ral Language Learning (CoNLL? 06), CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ?05),
pages 173?180.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
?07), pages 1?11.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ?07), pages 81?88.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical a-star dependency parsing. In Pro-
ceedings of the workshop on Prospects and Advances
of the Syntax/Semantics Interface, Nancy, 2003, pp.85-
89.
P.C. Gilmore and R.E. Gomory. 1961. A linear program-
ming approach to the cutting-stock problem. Opera-
tions research, pages 849?859.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact viterbi parse selection. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL ?03), pages 119?126.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ?11).
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ?10).
Sandra K?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Marco L?bbecke and Jacques Desrosiers. 2004. Selected
topics in column generation. Operations Research,
53:1007?1023.
R. Kipp Martin, Ronald L. Rardin, and Brian A. Camp-
bell. 1990. Polyhedral characterization of discrete
dynamic programming. Oper. Res., 38(1):127?138,
February.
Andr? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP (ACL
?09), pages 342?350, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Andr? F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,
and M?rio A. T. Figueiredo. 2011. Dual decomposi-
tion with many overlapping components. In Proceed-
ings of the Conference on Empirical methods in natu-
ral language processing (EMNLP ?11), EMNLP ?11,
pages 238?249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ?06), pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?05), pages
91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In HLT-EMNLP, 2005.
Mathias Niepert. 2010. A delayed column generation
strategy for exact k-bounded map inference in markov
logic networks. In Proceedings of the 26th Annual
Conference on Uncertainty in AI (UAI ?10), pages
384?391, Corvallis, Oregon. AUAI Press.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Conference on Em-
pirical Methods in Natural Language Processing and
Natural Language Learning, pages 915?932.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
22nd AAAI Conference on Artificial Intelligence (AAAI
?07), pages 913?918.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?06), pages 129?137.
Sebastian Riedel and David A. Smith. 2010. Relaxed
marginal inference and its application to dependency
742
parsing. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ?10), pages 760?768, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ?08), pages 468?475.
D. Roth andW. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL? 04), pages 1?8.
Alexander Rush and Slav Petrov. 2012. Vine pruning for
efficient multi-pass dependency parsing. In Joint Hu-
man Language Technology Conference/Annual Meet-
ing of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL ?12).
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?10).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 145?156, Hon-
olulu, October.
D. Sontag and T. Jaakkola. 2007. New outer bounds on
the marginal polytope. In Advances in Neural Infor-
mation Processing Systems (NIPS ?07), pages 1393?
1400.
David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proceedings of the 24th An-
nual Conference on Uncertainty in AI (UAI ?08).
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Joint Human Lan-
guage Technology Conference/Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ?06), pages 423?
430.
743
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 810?820, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Improving NLP through Marginalization of Hidden Syntactic Structure
Jason Naradowsky, Sebastian Riedel, and David A. Smith
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, U.S.A.
{narad, riedel, dasmith}@cs.umass.edu
Abstract
Many NLP tasks make predictions that are in-
herently coupled to syntactic relations, but for
many languages the resources required to pro-
vide such syntactic annotations are unavail-
able. For others it is unclear exactly how
much of the syntactic annotations can be ef-
fectively leveraged with current models, and
what structures in the syntactic trees are most
relevant to the current task.
We propose a novel method which avoids
the need for any syntactically annotated data
when predicting a related NLP task. Our
method couples latent syntactic representa-
tions, constrained to form valid dependency
graphs or constituency parses, with the predic-
tion task via specialized factors in a Markov
random field. At both training and test time we
marginalize over this hidden structure, learn-
ing the optimal latent representations for the
problem. Results show that this approach pro-
vides significant gains over a syntactically un-
informed baseline, outperforming models that
observe syntax on an English relation extrac-
tion task, and performing comparably to them
in semantic role labeling.
1 Introduction
Many NLP tasks are inherently tied to syntax, and
state-of-the-art solutions to these tasks often rely on
syntactic annotations as either a source for useful
features (Zhang et al2006, path features in relation
extraction) or as a scaffolding upon which a more
narrow, specialized classification can occur (as of-
ten done in semantic role labeling). This decou-
pling of the end task from its intermediate repre-
sentation is sometimes known as the two-stage ap-
proach (Chang et al2010) and comes with several
drawbacks. Most notably this decomposition pro-
hibits the learning method from utilizing the labels
from the end task when predicting the intermediate
representation, a structure which must have some
correlation to the end task to provide any benefit.
Relying on intermediate representations that are
specifically syntactic in nature introduces its own
unique set of problems. Large amounts of syntac-
tically annotated data is difficult to obtain, costly
to produce, and often tied to a particular domain
that may vary greatly from that of the desired end
task. Additionally, current systems often utilize only
a small amount of the annotation for any particular
task. For instance, performing named entity recogni-
tion (NER) jointly with constituent parsing has been
shown to improve performance on both tasks, but
the only aspect of the syntax which is leveraged by
the NER component is the location of noun phrases
(Finkel and Manning, 2009). By instead discover-
ing a latent representation jointly with the end task
we address all of these concerns, alleviating the need
for any syntactic annotations, while simultaneously
attempting to learn a latent syntax relevant to both
the particular domain and structure of the end task.
We phrase the joint model as factor graph and
marginalize over the hidden structure of the inter-
mediate representation at both training and test time,
to optimize performance on the end task. Infer-
ence is done via loopy belief propagation, making
this framework trivially extensible to most graph
structures. Computation over latent syntactic rep-
810
resentations is made tractable with the use of special
combinatorial factors which implement unlabeled
variants of common dynamic-programming parsing
algorithms, constraining the hidden representation
to realize valid dependency graphs or constituency
trees.
We apply this strategy to two common NLP tasks,
coupling a model for the end task prediction with
latent and general syntactic representations via spe-
cialized logical factors which learn associations be-
tween latent and observed structure. In comparisons
with identical models which observe ?gold? syntac-
tic annotations, derived from off-the-shelf parsers or
provided with the corpora, we find that our hidden
marginalization method is comparable in both tasks
and almost every language tested, sometimes signifi-
cantly outperforming models which observe the true
syntax.
The following sections serves as a preliminary,
introducing an inventory of factors and variables
for constructing factor graph representations of
syntactically-coupled NLP tasks. Section 3 explores
the benefits of this method on relation extraction
(RE), where we compare the use dependency and
constituency structure as latent representations. We
then turn to a more established semantic role label-
ing (SRL) task (?4) where we evaluate across a wide
range of languages.
2 Latent Pseudo-Syntactic Structure
The models presented in this paper are phrased in
terms of variables in an undirected graphical model,
Markov random field. More specifically, we imple-
ment the model as a factor graph, a bipartite graph
composed of factors and variables in which we can
efficiently compute the marginal beliefs of any vari-
able set with the sum-product algorithm for cyclic
graphs, loopy belief propagation,. We now intro-
duce the basic variable and factor components used
throughout the paper.
2.1 Latent Dependency Structure
Dependency grammar is a lexically-oriented syn-
tactic formalism in which syntactic relationships
are expressed as dependencies between individual
words. Each non-root word specifies another as
its head, provided that the resulting structure forms
a valid directed graph, ie. there are no cycles in
the graph. Due to the flexibility of this representa-
tion it is often used to describe free-word-order lan-
guages, and increasingly preferred in NLP for more
language-in-use scenarios. A dependency graph can
be modeled with the following nodes, as first pro-
posed by Smith and Eisner (2008):
? Let {Link(i, j) : 0 ? i ? j ? n, n 6= j}
be O(n2) boolean variables corresponding to
the possible links in a dependency parse. Li,j
= true implies that there is a dependency from
parent i to child j.
? Let {LINK(i, j) : 0 ? i ? j ? n, n 6= j}
be O(n2) unary factors, each paired with a cor-
responding Link(i, j) variable and expressing
the independent belief that Link(i, j) = true.
2.2 Latent Constituency Structure
Alternatively we can describe the more structured
constituency formalism by setting up a representa-
tion over span variables:
? Let {Span(i, j) : 0 ? i < j ? n} be O(n2)
boolean variables such that Span(i, j) = true
iff there is a bracket spanning i to j 1.
? Let {SPAN(i, j) : 0 ? i < j ? n} be O(n2)
unary factors, each attached to the correspond-
ing Span(i, j) variable. These factors score the
independent suitability of each span to appear
in an unlabeled constituency tree.
All boolean variables presented in this paper will
be paired to unary factors in this manner, which
we will omit in future descriptions. This encom-
passes the necessary representational structure for
both syntactic formalisms, but nothing introduced
up to this point guarantees that either of these rep-
resentations will form a valid tree or DAG.
2.3 Combinatorial Factors
Naively constraining these latent representations
through the introduction of many interconnected
ternary factors is possible, but would likely be com-
putationally intractable. However, as observed in
1In practice, we do not need to include variables for spans
of width 1 or n, since they will always be true.
811
Smith and Eisner (2008), we can encapsulating
common dynamic programming algorithms within
special-purpose factors to efficiently globally con-
strain variable configurations . Since the outgoing
messages from such factors to a variable can be com-
puted from the factor?s posterior beliefs about that
variable, there is no difficulty in exchanging beliefs
between these special-purpose factors and the rest
of the graph, and inference can proceed using the
standard sum-product or max-product belief prop-
agation. Here we present two combinatorial factors
that provide efficient ways of constraining the model
to fit common syntactic frameworks.
? Let CKYTREE be a global combinatorial fac-
tor, as used in previous work in efficient pars-
ing (Naradowsky and Smith, 2012), attached to
all the Span(i, j) variables. This factor con-
tributes a factor of 1 to the model?s score iff the
span variables collectively form a legal, binary
bracketing and a factor of 0 otherwise. It en-
forces, therefore, a hard constraint on the vari-
ables, computing beliefs via an unlabeled vari-
ant of the inside-outside algorithm.
? Let DEP-TREE be a global combinatorial fac-
tor, as presented in Smith and Eisner (2008),
which attaches to all Link(i, j) variables and
similarly contributes a factor of 1 iff the config-
uration of Link variables forms a valid projec-
tive dependency graph. A graph is projective if
its edges do not cross.
2.4 Marginal MAP Inference
It is straightforward to train these latent variable
models to maximize the marginal probability of their
outputs, conditioning on their inputs, and marginal-
izing out the latent syntactic variables. To compute
feature expectations, we can use marginal inference
techniques such as sampling and sum-product belief
propagation to compute marginal probabilities.
A knottier problem arises when we want to find
the best assignment to the variables of interest
while marginalizing out ?nuisance? latent variables.
This is the problem of marginal MAP inference?
sometimes known as consensus decoding?which
has been shown to be NP-hard and without a poly-
nomial time approximation scheme (Sima?an, 1996;
Casacuberta and Higuera, 2000). In the NLP com-
munity, these inference problems often arise when
dealing with spurious ambiguity where multiple
derivations can lead to the same derived structure. In
tree substitution grammars, for instance, there may
be many ways of combining elementary trees to pro-
duce the same output tree; in machine translation,
many different elementary phrases or elementary
tree pairs might produce the same output string. For
syntactic parsing, Goodman (1996) proposed a vari-
ational method for summing out spurious ambiguity
that was equivalent to minimum Bayes risk decoding
(Goel and Byrne, 2000; Kumar and Byrne, 2004)
with a constituent-recall loss function. For MT,
May and Knight (2006) proposed methods for de-
terminizing tree automata to reduce ambiguity, and
Li et al2009) proposed a variational method based
on n-gram loss functions. More recently, Liu and Ih-
ler (2011) analyzed message-passing algorithms for
marginal MAP.
In this paper, we adopt a simple minimum Bayes
risk decoding scheme. First, we perform sum-
product belief propagation on the full factor graph.
Then, we maximize the expected accuracy of the
variables of interest, subject to any hard constraints
on them (such as mutual exclusion among labels). In
some cases with complex combinatorial constraints,
this simple MBR scheme has proved more effec-
tive than exact decoding over all variables (Auli and
Lopez, 2011).
3 Relation Extraction
Performing a syntax-based NLP task in most real-
world scenarios requires that the incoming data first
be parsed using a pre-trained parsing model. For
some tasks, like relation extraction, many data sets
lack syntactic annotation and these circumstances
persist even into the training phase. In this sec-
tion we explore such scenarios and contrast the use
of parser-provided syntactic annotation to marginal-
izing over latent representations of constituency or
dependency syntax. We show the hidden syntactic
models are not just competitive with these ?oracle?
models, but in some configurations can actually out-
perform them.
Relation extraction is the task of identifying se-
mantic relations between sets of entities in text (as
812
illustrated in Fig. 1b), and a good proving ground
for latent syntactic methods for two reasons. First,
because entities share a semantic relationship, un-
der most linguistic analyses these entities will also
share some syntactic relation. Indeed, syntactic fea-
tures have long been an extremely useful source
of information for relation extraction systems (Cu-
lotta and Sorensen, 2004; Mintz et al2009). Sec-
ondly, relation extraction has been a common task
for pioneering efforts in processing data mined from
the internet, and otherwise noisy or out-of-domain
data. In particular, large noisily-annotated data sets
have been generated by leveraging freely available
knowledge bases such as Freebase (Bollacker et al
2008; Mintz et al2009). Such data sets have been
utilized successfully for relation extraction from the
web (Bunescu and Mooney, 2007).
3.1 Model
We present a simple model for representing rela-
tional structure, with the only variables present be-
ing a set of boolean-valued variables representing an
undirected dependency between two entities, and an
additional set of boolean label variables representing
the type label of the relation.
? Let {Rel(i, j : 0 ? i < j ? n} be O(n2)
boolean variables such that Rel(i, j) = true iff
there is a relation spanning i to j.
? Let {Rel-Label(i, j, ?) : ? ? L, and 0 ? i <
j ? n} be O(|L|n2) boolean variables such
that Rel-Label(i, j, ?) = true iff there is a re-
lation spanning i to j with relation type ?.
? Let {ATMOST1(i, j) : 0 ? i < j ? n} be
O(n2) factors, each coordinating the set L of
possible nonterminal variables to the Rel vari-
able at each i, j tuple, allowing a Rel-Label
variable to be true iff all other label variables
are false and Rel(i, j) = true.
Here the Rel(i, j) and Rel-Label(i, j) variables
simply express the representation of the problem,
while the ATMOST1 factors are logical constraints
ensuring that only one label will apply to a particu-
lar relation.
3.2 Coordination Factors
An important contribution of this work is the intro-
duction of a flexible, general framework for connect-
ing the latent and observable partitions of the model.
We accomplish this through the use of two addi-
tional factors, each expressing the same basic logic,
which learn when to coordinate and when to ignore
correlations between the latent syntax and the end
task. While here we specify binary and ternary ver-
sions of these factors, they also generalize to higher
dimensions.
? Let {D-CONNECT(i, j, k) : 0 ? i < j ?
n; 0 ? k ? n} be O(n3) factors coordinating
any number of dependency syntax Link(i, j)
variables with representational variables on the
end task, multiplying in 1 to the model score
unless all variables are on, in which case it mul-
tiplies a connective potential ? derived from
its features. Thus it functions logically as a
soft NAND factor. In this ternary formulation k
represents a hidden dependency head or pivot
which is shared between two syntactic depen-
dencies anchored at the indices of the entities
in the relation (as illustrated in Fig. 1).
? Let {C-CONNECT(i, j) : 0 ? i < j ?
n} be O(n2) factors coordinating syntactic
Span(i, j) and relation arc Rel(i, j), identi-
cally to D-CONNECT but with a 1-to-1 map-
ping. Intuitively the joint model might learn
? > 1, i.e., constituency spans and task predic-
tion relations are more likely to be coterminous.
The difficulty in working with latent dependency
syntax is that we posit that the RE variables do not
share a 1-to-1 mapping with variables in the hid-
den representation. We expect instead, according
to linguistic intuition, that a relation between enti-
ties at position i and j in the sentence should have
corresponding syntactic dependencies but that they
are likely to realize this by sharing the same head
word (as depicted in Fig.1), a word whose identity
should help label the relation. Therefore we intro-
duce a special coordination factor, D-CONNECT as
a ternary factor to capture the relationship between
pairs of latent syntactic variables and a single rela-
tion variable, pivoting on the same unknown head
word.
813
Figure 1: Latent Dependency coupling for the RE task.
The D-CONNECT factor expresses ternary connection re-
lations because the shared head word of the proposed re-
lation is unknown. As is convention, variables are repre-
sented by circles, factors by rectangles.
We introduce six model scenarios.
? Baseline, simply the arc-factored model con-
sisting only of Rel and corresponding Label
variables for each entity. Features on the re-
lation factors, which are common to all model
configurations, are combinations of lexical in-
formation (i.e., the words that form the entity,
the pos-tags of the entities, etc.) as well as the
distance between the relation. This is a light-
weight model and generally does not attempt
to exhaustively leverage all possible proven
sources of useful features (Zhou et al2005)
towards a higher absolute score, but rather to
serve as a point of comparison to the models
which rely on syntactic information.
? Baseline-Ent, a variant of Baseline with addi-
tional features which include combinations of
mention type, entity type, and entity sub-type.
? Oracle D-Parse, in which we also instantiate a
full set of latent dependency syntax variables,
and connect them to the baseline model us-
ing D-CONNECT factors. Syntax variables are
clamped to their true values.
? Oracle C-Parse, the constituency syntax ana-
logue of Oracle D-Parse.
? Hidden D-Parse, which is an extension of Or-
acle D-Parse in which we connect all syntax
variables to a DEP-TREE factor, syntax vari-
ables are unobserved, and are learned jointly
with the end task. The features for latent syntax
are a subset of those used in dependency pars-
ing (McDonald et al2005).
? Hidden C-Parse, the constituency syntax ana-
logue of Hidden D-Parse. The feature set is
similar but bigrams are taken over the words
defining the constituent span, rather than the
words defining the head/modifier relation.
Coordination factor features for the syntactically-
informed models are particularly important. This
became evident in initial experiments where the
baseline was often able to outperform the hidden
syntactic model. However, inclusion of entity and
mention label features into the connection factors
provides the model with greater reasoning over
when to coordinate or ignore the relation predictions
with the underlying syntax. These are a proper sub-
set of the Baseline-Ent features.
3.3 Data
We evaluate these models using the 2005 Auto-
matic Content Extraction (ACE) data set (Walker,
2006), using the English (dual-annotated) and Chi-
nese (solely annotator #1 data set) sections. Each
corpus is annotated with entity mentions?tagged as
PER, ORG, LOC, or MISC?and, where applica-
ble, what type of relation exists between them (e.g.,
coarse: PHYS; fine: Located). But like most cor-
pora available for the task, the burden of acquiring
corresponding syntactic annotation is left to the re-
searcher. In this situation it is common to turn to
existing pre-trained parsing models.
We generate our data by first splitting the raw
text paragraphs into sentences. Chinese sentences
814
ACE Results
English Chinese
Unlabeled Labeled Unlabeled Labeled
Model P R F1 P R F1 P R F1 P R F1
Baseline 85.4 57.0 68.4 83.0 55.3 66.4 42.9 26.8 33.0 42.6 21.3 28.4
Baseline-Ent 87.2 65.4 74.8 85.8 64.4 73.6 55.2 31.1 39.8 51.2 29.4 37.4
Oracle D-Parse 89.3 67.4 76.8 89.3 66.2 75.4 60.0 32.6 42.2 58.1 31.3 40.7
Hidden D-Parse 87.8 69.8 77.7 85.3 67.8 75.6 48.0 32.0 38.4 47.2 30.0 36.7
Oracle C-Parse 89.1 68.7 77.6 87.5 67.5 76.2 66.8 37.8 48.3 63.8 37.0 46.8
Hidden C-Parse 90.5 69.9 78.9 88.8 68.6 77.4 56.3 32.3 41.0 53.4 31.6 39.7
Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the
syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on
the smaller Chinese data set.
are also tokenized according to Penn Chinese Tree-
bank standards (Xue et al2005). The sentences are
then tagged and parsed using the Stanford CoreNLP
tools, using the standard pre-trained models for tag-
ging (Toutanvoa and Manning, 2000), and the fac-
tored parsing model of Klein and Manning (2002).
The distributed grammar is trained on a variety of
sources, including the standard Wallstreet Journal
corpus, but also biomedical, translation, and ques-
tions. We then apply entity and relation annota-
tions noisily to the data, collapsing multi-word en-
tities into one term. We filter out sentences with
fewer than two entities (and are thus incapable of
containing relations) and sentences with more than
40 words (to keep the parses more reliable). This
yields 6966 sentences for English data, but unfortu-
nately only 747 sentences for the Chinese. Nine of
every ten sentences comprise the training set, with
every tenth sentence reserved for test.
3.4 Results
We train all models using 20 iterations of stochastic
gradient descent, each with a maximum of 10 BP it-
erations (though in practice we find convergence to
often occur much earlier). The results are presented
in Table 1, showing precision, recall, and F-measure
for both labeled and unlabeled prediction. For En-
glish, not only is the hidden marginalization method
a suitable replacement for the syntactic trees pro-
vided by pre-trained, state-of-the-art models, but in
both configurations we find that inducing an optimal
hidden structure is preferable to the parser-produced
annotations. On Chinese, where the data set is atyp-
ically small, we still observe improved performance
over the baseline in the constituency-based model
though it is not able to match the observed syntax
model.
Despite the intuition that both entities occupy
roles as modifiers of the same verb, we find that
the Hidden D-Parse model often fails to recover the
correct latent structure, and that even when success-
ful dependency parses are observed, the head word
is often not uniquely indicative of the relation type
(as known is not strongly correlated with the relation
type EMPLOYS in the phrase: Shigeru Miyamoto,
best known for his work at the video game company
Nintendo). Hence when it comes to relation extrac-
tion, at least on our relatively small data sets, we find
the simplest approach to latent syntactic structure is
the best.
We now turn to the task of semantic role label-
ing to evaluate this method on a more established
hand-annotated data set, and a more varied set of
languages.
4 Semantic Role Labeling
The task of semantic role labeling (SRL) aims to
detect and label the semantic relationships between
particular words, most commonly verbs (referred to
in the domain as predicates), and their arguments
(Meza-Ruiz and Riedel, 2009).
In a manner similar to RE, there is a strong corre-
lation between the presence of an SRL relation and
there existing an underlying syntactic dependency,
though this is not always expressed as directly as a
1-to-1 correspondence. This has historically moti-
vated a reliance on syntactic annotation, and some
of the most successful methods have simply applied
815
Pred
5
At Most 1
sense.
01
sense.
02
sense. 
|S|
.
d.) Sense Prediction
Arg
5, 1
Arg
5, 2
role
A0
role
A1
.  .  .
c.) Argument Prediction
b.) Syntactic Layer
Link
5, 1
D-Connect
5, 1
a.) Syntactic Combinatorial Constraint
DEP-Tree
Link
5, 2
D-Connect
5, 2
Link
5, 3
D-Connect
5, 3
At Most 1
Arg
5, 3
Link
5, n
role
A2
.   .   .
D-Connect
5, n
role
A-TM
.
.
Figure 2: A tiered graphic representing the three different SRL model configurations. The baseline system is described
in the bottom (c & d), the separate panels highlighting the independent predictions of this model: sense labels are
assigned in an entirely separate process from argument prediction. Pruning in the model takes place primarily in
this tier, since we observe true predicates we only instantiate over these indices. The middle tier (b.) illustrates the
syntactic representation layer, and the connective factors between syntax and SRL. In the observed syntax model
the Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a valid
tree. Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.) over syntactic
variables. In this scenario all labels in (b.) are hidden at both training and test time.
feature-rich classifiers to the parsed trees. Related
work has recognized the large annotation burden the
task demands, but aimed to keep the syntactic anno-
tations and induce semantic roles (Lang and Lapata,
2010). In this section we will take the opposite ap-
proach, disregarding the syntactic annotations which
we argue are more costly to acquire, as they require
more formal linguistic training to produce.
4.1 Model
We present a simple, flexible model for SRL in
which sense predictions are made independently of
the rest of the model, and argument predictions are
made independently of each other. The model struc-
ture is composed as depicted in Fig. 2.
? Let {Arg(i, j) : 0 ? i < j ? n} be O(n2)
boolean variables such that Arg(i, j) = true
iff predicate i takes token j as an argument.
? Let {Role(i, j, ?) : ? ? L, and 0 ? i <
j ? n} be O(|L|n2) boolean variables such
that Role(i, j, ?) = true iff Arg(i, j) is true
and takes the role label ?.
? Let {Sense(i, ?) : ? ? S, and 0 ? i ?
n} be O(|S|n) boolean variables such that
Sense(i, ?) = true iff predicate i has sense
?.
4.1.1 Features
At the coarsest level both the SRL and RE models
are specifying binary predictions between a pair of
indices in the sentence, and a set of labels for each
dependency that happens to be true. Similarly we
use almost identical features in SRL as we did in
816
Figure 3: Examining the learned hidden representation for SRL. In this example the syntactic dependency arcs derived
from gold standard syntactic annotations (left) are entirely disjoint from the correct predicate/arguments pairs (shown
in the heatmaps by the squares outlined in black), and the observed syntax model fails to recover any of the correct
predictions. In contrast, the hidden model structure (right) learns a representation that closely parallels the desired end
task predictions, helping it recover three of the four correct SRL predictions (shaded arcs: red corresponds to a correct
prediction, with true labels GA, KARA, etc.), and providing some evidence towards the fourth. The dependency tree
corresponding to the hidden structure is derived by edge-factored decoding: dependency variables whose beliefs> 0.5
are classified as true (though some arcs not relevant to the SRL predictions are omitted for clarity).
RE, with the sole exception that we incorporate the
observable lemma and morphological features into
bigrams on predicate/argument pairs. For sense pre-
diction we rely only on unigram features taken in a
close (w = 2) window of the target predicate.
For the coordinating factors we use subsets of
combinations of word, part-of-speech, and capital-
ization features taken between head and argument,
and concatenate these with the distance and direc-
tion between the predicate and argument. We do not
find the performance of the system to be as sensi-
tive to which features are present in the coordinating
factors as we did in the RE task.
4.2 Data
We evaluate our SRL model using the data set devel-
oped for the CoNLL 2009 shared task competition
(Hajic? et al2009), which features seven languages
and provides an ideal opportunity to measure the
ability of the hidden structure to generalize across
languages of disparate origin and varied character-
istics. It also provides the opportunity to observe
a variety of different annotation styles and biases,
some of which our model was able to uncover as ill-
suited to common models for the task. The data it-
self provides word, lemma, part-of-speech, and mor-
phological feature information, along with gold de-
pendency parses. Words which denote predicates are
identified, and their (train time) arguments are pro-
vided. They are also annotated with a sense label
for each predicate, which is scored as an additional
SRL dependency. Thus the task involves predicting
for each predicate a set of argument dependencies
and the sense label associated with that predicate.
817
Unlabeled Labeled CoNLL 2009 F1
Data Model P R F1 P R F1 MAX. MEAN MED.
Catalan
Baseline 92.20 62.43 74.48 73.80 58.76 65.43
Oracle Syn. 98.48 96.17 97.31 70.42 68.78 69.59 80.3 71.0 74.1
Hidden Syn. 95.21 92.84 94.01 68.86 67.15 67.99
Chinese
Baseline 72.48 64.82 68.44 65.97 59.00 62.29
Oracle Syn. 98.57 78.98 87.69 87.64 70.22 77.97 78.6 72.2 70.4
Hidden Syn. 90.79 79.09 84.53 81.97 71.40 76.32
Czech
Baseline 97.73 56.50 71.61 84.80 48.80 61.84
Oracle Syn. 98.62 81.25 89.09 92.94 68.25 74.84 85.4 72.4 71.7
Hidden Syn. 92.39 89.35 90.85 74.41 71.96 73.16
English
Baseline 92.46 71.56 80.68 84.56 65.45 73.78
Oracle Syn. 96.75 82.25 88.91 85.48 72.67 78.55 85.6 75.6 72.1
Hidden Syn. 95.06 79.06 86.32 83.82 69.72 76.12
German
Baseline 93.49 44.24 60.06 75.00 35.49 48.18
Oracle Syn. 95.18 79.11 86.41 73.24 60.87 66.49 79.7 68.1 67.8
Hidden Syn. 91.92 86.26 89.00 69.47 65.19 67.26
Japanese
Baseline 91.64 43.36 58.87 80.41 38.05 51.66
Oracle Syn. 93.84 48.15 63.64 90.06 46.21 61.08 78.2 62.7 72.0
Hidden Syn. 90.88 73.47 81.25 73.42 59.36 65.65
Spanish
Baseline 82.90 39.47 53.48 67.64 32.21 43.64
Oracle Syn. 98.96 94.19 96.52 70.68 67.27 68.93 80.5 70.4 73.4
Hidden Syn. 96.15 90.53 93.25 68.81 64.79 66.74
Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained
using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where
poor sense prediction hindered absolute performance.
4.3 Results
We evaluate across a set of model configurations
analogous to before. All experiments used 30 itera-
tions of SGD with a Gaussian prior, and a max 10 it-
erations of BP to compute the marginals for each ex-
ample. In comparison to the CoNLL competition en-
tries (Table 2, rightmost columns) our syntactically-
informed models generally fall in the middle of the
rankings. This is not surprising given the indepen-
dent predictions of the model and the very general,
language universal assumptions we have made in the
model structure and feature sets. However, in terms
of gauging the usefulness of the hidden syntactic
marginalization method the results are extremely
compelling, with only marginal differences between
the performance of the observed-syntax model, es-
pecially relative to the baseline.
And despite the simplicity of the model, we still
manage to perform at state-of-the-art levels in a
few instances, sometimes outperforming most of the
competition entries without observing any syntax.
The performance on Chinese is an example of this,
with our system outperforming all but the best sys-
tem, and the hidden syntactic model only slightly
behind.
Abstracting away from the performance compar-
isons against other systems, the unlabeled results are
the more revealing evidence for the use of hidden
syntactic structure. Here the average hidden model
score (88.89) almost outperforms the observed syn-
tax model (90.22, and vs. 66.80 baseline), mostly
due to the large margins on the unlabeled Japanese
scores. The strong independence between sense
prediction and argument prediction hinders perfor-
mance on the labeled task, but on all languages we
find an extremely significant improvement exploit-
ing hidden syntactic structure in comparison to the
baseline system?the hidden model recovers more
than 92% of the gap between the baseline and the
observed syntax model. It is also interesting to note
that in the shared task competition the two languages
which systems lost the most performance between
their parsing F1 and their SRL F1 were Japanese
and German. As illustrated in Fig. 3, the corre-
818
spondence between syntax and SRL are extremely,
and systematically, poor. In this example our hid-
den structure model was able to assign strong beliefs
to the latent syntactic variables which correspond to
the correct predicate/argument pairs, allowing it to
correctly identify three of the four SRL arguments
when the joint model failed to recover one.
5 Related Work
This work is perhaps mostly closely related to
the Learning over Constrained Latent Representa-
tions (LCLR) framework of Chang et al2010).
Their abstract problem formulation is identical: both
paradigms seek to couple the end task to an interme-
diate representation which is not accessible to the
learning algorithm. However much of the intent,
scale, and methodology is different. LCLR aims
to provide a flexible latent structure for increasing
the representational power of the model in a use-
ful way, and is demonstrated on tasks and domains
where data availability is not a key concern. In con-
trast, while our hidden structure models may outper-
form their observed syntax counterparts, our focus
is as much on alleviating the burden of procuring
large amounts of syntactic annotation as it is about
increasing the expressiveness of the model. To that
end we constrain a more sophisticated latent repre-
sentation and couple it to highly structured output
predictions, opposed to binary classification prob-
lems. In methodology, we perform the more com-
putationally intensive marginalization operation in-
stead of maximizing.
Marginalization of hidden structure is also funda-
mental to other work, and featured most prominently
in generative Bayesian latent variable models (Teh
et al2006). Our approach is trained discrimina-
tively, affording the use of very rich feature sets and
the prediction of partial structures without needing
to specify a full derivation. Similar approaches have
been used in more linear latent variable CRF-based
models (McCallum et al2005), but these must only
marginalize only over hidden states of a much more
compact representation. Naively extending this to
tree-based constraints would often be computation-
ally inefficient, and we avoid intractability through
the encapsulation of much of the dynamic program-
ming machinery into specialized factors. Moreover,
using loopy belief propagation means that the in-
ference method is not closely coupled to the task
structure, and need not change when applying this
method to other types of graphs.
6 Conclusion
We have presented a novel method of coupling
syntactically-oriented NLP tasks to combinatorially-
constrained hidden syntactic representations, and
have shown that marginalizing over this latent rep-
resentation not only provides significant improve-
ments over syntactically-uninformed baselines, but
occasionally improves performance when compared
to systems which observe syntax. On the task of
relation extraction we find that a constituency rep-
resentation provides the most improvement over the
baseline, while in the SRL domain our model is ex-
tremely competitive with the best reported results on
Chinese, and outperforms the model using the pro-
vided parses on German and Japanese.
We believe this method delivers very promising
results in our presented tasks, opening the door to
new lines of research examining what types of con-
straints and what configurations of hidden struc-
ture are most beneficial for particular tasks and lan-
guages. Moreover, we present one type of coordinat-
ing factor, as both D-CONNECT and C-CONNECT
logically express a soft NAND function, but more
sophisticated coupling schemes are another natural
direction to pursue. Finally, we use sum-product
variant of belief propagation inference, but more
specialized inference schemes may show additional
benefits.
Acknowledgements
We would like to thank Andrea Gesmundo for help in
procuring sections of the CoNLL 2009 shared task data.
This work was supported in part by the Center for Intel-
ligent Information Retrieval and in part by Army prime
contract number W911NF-07-1-0216 and University of
Pennsylvania subaward number 103-548106. The Uni-
versity of Massachusetts also gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0181. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
view of the DARPA, AFRL, or the US government.
819
References
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In ACL,
pages 470?480.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In SIGMOD, pages 1247?1250, New
York, NY, USA. ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In ACL.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In ICGI, pages 15?24.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained latent
representations. In NAACL.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL, Barcelona,
Spain.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In
NAACL, pages 326?334.
Vaibbhava Goel and William J. Byrne. 2000. Minimum
Bayes risk automatic speech recognition. Computer
Speech and Language, 14(2):115?135.
Joshua T. Goodman. 1996. Parsing algorithms and met-
rics. In ACL, pages 177?183.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL: Shared Task,
pages 1?18.
Dan Klein and Chris Manning. 2002. Fast exact infer-
ence with a factored model for natural language pro-
cessing. In NIPS.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL, pages 169?176.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In HLT-NAACL, pages 939?
947.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Qiang Liu and Alexander Ihler. 2011. Variational algo-
rithms for marginal MAP. In UAI, pages 453?462.
Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. In HLT-NAACL, pages 351?358.
Andrew McCallum, Kedar Bellare, and Fernando C. N.
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In UAI, pages 388?395.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT-EMNLP,
pages 523?530.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL, pages 1003?1011.
Jason Naradowsky and David A. Smith. 2012. Combina-
torial constraints for constituency parsing in graphical
novels. Technical report, University of Massachusetts
Amherst.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175?1180.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145?
156.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Kristina Toutanvoa and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP, pages 63?
70.
Christopher Walker. 2006. Ace 2005 multilingual train-
ing corpus. number ldc2006t06. In Linguistic Data
Consortium.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, pages 207?238.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In NAACL, pages 288?295.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In ACL, pages 427?434.
820
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 760?768,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Relaxed Marginal Inference and its Application to Dependency Parsing
Sebastian Riedel David A. Smith
Department of Computer Science
University of Massachusetts, Amherst
{riedel,dasmith}@cs.umass.edu
Abstract
Recently, relaxation approaches have been
successfully used for MAP inference on NLP
problems. In this work we show how to extend
the relaxation approach to marginal inference
used in conditional likelihood training, pos-
terior decoding, confidence estimation, and
other tasks. We evaluate our approach for the
case of second-order dependency parsing and
observe a tenfold increase in parsing speed,
with no loss in accuracy, by performing in-
ference over a small subset of the full factor
graph. We also contribute a bound on the error
of the marginal probabilities by a sub-graph
with respect to the full graph. Finally, while
only evaluated with BP in this paper, our ap-
proach is general enough to be applied with
any marginal inference method in the inner
loop.
1 Introduction
In statistical natural language processing (NLP) we
are often concerned with finding the marginal proba-
bilities of events in our models or the expectations of
features. When training to optimize conditional like-
lihood, feature expectations are needed to calculate
the gradient. Marginalization also allows a statis-
tical NLP component to give confidence values for
its predictions or to marginalize out latent variables.
Finally, given the marginal probabilities of variables,
we can pick the values that maximize these marginal
probabilities (perhaps subject to hard constraints) in
order to predict a good variable assignment.1
1With a loss function that decomposes on the variables, this
amounts to Minimum Bayes Risk (MBR) decoding, which is
Traditionally, marginal inference in NLP has been
performed via dynamic programming (DP); how-
ever, because this requires the model to factor in
a way that lends itself to DP algorithms, we have
to restrict the class of probabilistic models we con-
sider. For example, since we cannot derive a dy-
namic program for marginal inference in second or-
der non-projective dependency parsing (McDonald
and Satta, 2007), we have non-projective languages
such as Dutch using second order projective mod-
els if we want to apply DP. Some previous work
has circumvented this problem for MAP inference
by starting with a second-order projective solution
and then greedily flipping edges to find a better non-
projective solution (McDonald and Pereira, 2006).
In order to explore richer model structures, the
NLP community has recently started to investigate
the use of other, well-known machine learning tech-
niques for marginal inference. One such technique is
Markov chain Monte Carlo, and in particular Gibbs
sampling (Finkel et al, 2005), another is (loopy)
sum-product belief propagation (Smith and Eisner,
2008). In both cases we usually work in the frame-
work of graphical models?in our case, with factor
graphs that describe our distributions through vari-
ables, factors, and factor potentials. In theory, meth-
ods such as belief propagation can take any graph
and perform marginal inference. This means that we
gain a great amount of flexibility to represent more
global and joint distributions for NLP tasks.
The graphical models of interest, however, are
often too large and densely connected for efficient
inference in them. For example, in second order
often very effective.
760
dependency parsing models, we have O(n2) vari-
ables and O(n3) factors, each of which may have
to be inspected several times. While belief prop-
agation is still tractable here (assuming we follow
the approach of Smith and Eisner (2008) to enforce
tree constraints), it is still much slower than sim-
pler greedy parsing methods, and the advantage sec-
ond order models give in accuracy is often not sig-
nificant enough to offset the lack of speed in prac-
tice. Moreover, if we extend such parsing models to,
say, penalizing all pairs of crossing edges or scoring
syntax-based alignments, we will need to inspect at
least O
(
n4
)
factors, increasing our efficiency con-
cerns.
When looking at the related task of finding the
most likely assignment in large graphical models
(i.e., MAP inference), we notice that several recent
approaches have significantly sped up computation
through relaxation methods (Tromble and Eisner,
2006; Riedel and Clarke, 2006). Here we start with
a small subset of the full graph, and run inference
for this simpler problem. Then we search for factors
that are ?violated? in the solution, and add them to
the graph. This is repeated until no more new factors
can be added. Empirically this approach has shown
impressive success. It often dramatically reduces the
effective network size, with no loss in accuracy.
How can we extend or generalize MAP relax-
ation algorithms to the case of marginal inference?
Roughly speaking, we answer it by introducing a
notion of factor gain that is defined as the KL di-
vergence between the current distribution with and
without the given factor. This quantity is then used
in an algorithm that starts with a sub-model, runs
marginal inference in it and then determines the
gains of the not-yet-added factors. In turn, all fac-
tors for which the gain exceeds some threshold are
added to the current model. This process is repeated
until no more new factors can be found or a maxi-
mum number of iterations is reached.
We evaluate this form of relaxed marginal infer-
ence for the case of second-order dependency pars-
ing. We follow Smith and Eisner?s tree-aware be-
lief propagation procedure for inference in the inner
loop of our algorithm. This leads to a tenfold in-
crease in parsing speed with no loss in accuracy.
We also contribute a bound on the error on
marginal probabilities the sub-graph defines with re-
spect to the full graph. This bound can be used both
for terminating (although not done here) and under-
standing the dynamics of inference. Finally, while
only evaluated with BP so far, it is general enough
to be applied with any marginal inference method in
the inner loop.
In the following, we first give a sketch of the
graphical model we apply. Then we briefly discuss
marginal inference. In turn we describe our relax-
ation algorithm for marginal inference and some of
its theoretic guarantees. Then we present empirical
support for the effectiveness of our approach, and
conclude.
2 Graphical Models of Dependency Trees
We give a brief overview of the graphical model we
apply in our experiments. We chose the grandpar-
ents and siblings model, together with language spe-
cific multiroot and projectivity options as taken from
Smith and Eisner (2008). All our models are defined
over a set of binary variables Lij that indicate a de-
pendency between token i and j of the input sen-
tence W .
2.1 Markov Random Fields
Following Smith and Eisner (2008), we define a
probability distribution over all dependency trees as
a collection of edges y for a fixed input sentence
W . This distribution is represented by an undirected
graphical model, or Markov random field (MRF):
pF (y)
def
=
1
Z
?
i?F
?i (y) (1)
specified by an index set F and a corresponding
family (?i)F of factors ?i : Y 7? <
+. Here Z
is the partition function ZF =
?
y
?
i ?i (y).
We will restrict our attention to binary factors that
can be represented as ?i (y) = e?i?i(y) with binary
functions ?i (y) ? {0, 1} and weights ?i ? <.2 This
2These ?i are also called sufficient statistics or feature func-
tions, not to be confused with the features whose weighted sum
forms the weight ?i. The restriction to binary functions is with-
out loss of generality since we can combine constraints on par-
ticular variable assignments into potential tables with several
dimensions.
761
leads to
pF (y)
def
=
1
Z
exp
(
?
i?F
?i?i (y)
)
as an alternative representation for pF . Note that
when ?i (y) = 1 we will say that ?i fires for y.
Note that a factor function ?i(y) can depend on
any part of the observed input sentenceW ; however,
for brevity we will suppress this extra argument to
?i.
2.2 Hard and Soft Constraints on Trees
A particular model specifies its preference for set of
dependency edges over another by a set of hard and
soft constraints. We use hard constraints to rule out
a priori illegal structures, such as trees where a word
has two parents, and soft constraints to raise or lower
the score of trees that contain particular good or bad
substructures.
A hard factor (or constraint) ?i evaluates an as-
signment y with respect to some specified condi-
tion and fires only if this condition is violated; in
this case it evaluates to 0. It is therefore ruling out
all configurations in which the condition does not
hold. Note that a hard constraint ?i corresponds to
?i = ?? in our loglinear representation.
For dependency parsing, we consider two partic-
ular hard constraints, each of which touches all edge
variables in y: the constraint Tree requires that all
edges form a directed spanning tree rooted at the
root node 0; the constraint PTree enforces the more
stringent condition that all edges form a projective
directed tree. As in (Smith and Eisner, 2008), we
used algorithms from edge-factored parsing to com-
pute BP messages for these factors. In our experi-
ments, we enforced one or the other constraint de-
pending on the projectivity of given treebank data.
A soft factor ?i acts as a soft constraint that
prefers some assignments to others. This is equiv-
alent to saying that its weight ?i is finite. Note that
the weight of a soft factor is usually itself composed
as a sum of (sub-)weights wj for feature functions
that have the same input-output behavior as ?i (y)
when conditioned on the current sentence. It is these
wj which are adjusted at training time.
We use three kinds of soft factors from Smith and
Eisner (2008). In the full model, there are: O(n2)
LINKi,j factors that judge dependency edges in iso-
lation; O(n3) GRANDi,j,k factors that judge pairs
of dependency edges in a grandparent-parent-child
chain; and O(n3) SIBi,j,k factors that judge pairs of
dependency edges that share the same parent.
3 Marginal Inference
Formally, given our set of factors F and an observed
sentence W , marginal inference amounts to calcu-
lating the probability ?Fi that our binary features ?i
are active. That is, for each factor ?i
?Fi
def
=
?
?i(y)=1
pF (y) = EF [?i] (2)
For compactness, we follow the convention of Wain-
wright and Jordan (2008) and represent the belief for
a variable using the marginal probability of its cor-
responding unary factor. Hence, if we want to calcu-
late pF (Lij) we use ?FLINKij in place. Moreover we
will use ?F?i
def
= 1??Fi when we need the probability
of the event ?i (y) = 0.
The two most prominent approaches to marginal
inference in general graphical models are Markov
Chain Monte Carlo (MCMC) and variational meth-
ods. In a nutshell, MCMC iteratively generates a
Markov chain that yields pF as its stationary distri-
bution. Any expectation ?Fi can then be calculated
simply by counting the corresponding statistics in
the generated chain.
Generally speaking, variational methods frame
marginal inference as an optimization problem. Ei-
ther in the sense of minimizing the KL divergence
of a much simpler distribution to the actual distribu-
tion pF , as in mean field methods. Or in the sense of
maximizing a variational representation of the log-
partition function over the setM of valid mean vec-
tors (Wainwright and Jordan, 2008). Note that the
variational representation of the log partition func-
tion involves an entropy term that is intractable to
calculate in general and therefore usually approxi-
mated. Likewise, the set of constraints that guaran-
tee vectors ? to be valid mean vectors is intractably
large and is often simplified.
Because we use belief propagation (BP) as base-
line to compare to, and as a subroutine in our pro-
posed algorithm, a brief characterization of it is in
order. BP can be seen as a variational method that
762
uses the Bethe Free Energy as approximation to the
entropy, and the setML of locally consistent mean
vectors as an outer bound onM. A mean vector is
locally consistent if its beliefs on factors are consis-
tent with the beliefs of the factor neighbors.
BP solves the variational problem by iteratively
updating the beliefs of factors and variables based
on the current beliefs of their neighbors. When ap-
plied to acyclic graphical models BP yields the exact
marginals at convergence. For general graphs, BP is
not guaranteed to converge, and the beliefs it calcu-
lates are generally not the true marginals; however,
in practice BP often does converge and lead to accu-
rate marginals.
4 Relaxed Incremental Marginal Inference
Generally the runtime and accuracy of a marginal in-
ference method depends on size, density, tree-width
and interaction strength (i.e. the magnitude of its
weights) of the Graphical Model. For example, in
Belief Propagation the number of messages we have
to send in each iteration scales with the number of
factors (and their degrees). This means that when
we add a large number of extra factors to our model,
such as the O(n3) grandparent and sibling factors
for dependency parsing, we have to pay a price in
terms of speed, sometimes even accuracy.
However, on close inspection often many of the
additional factors we use to model some higher or-
der interactions are somewhat unnecessary or redun-
dant. To illustrate this, let us look at a second or-
der parsing model with grandparent factors. Surely
determiners are not heads of other determiners, and
this should be easy to encourage using LINK fea-
tures only. Hence, a grandparent factor that dis-
courages a determiner-determiner-determiner chain
seems unnecessary.
This raises two questions: (a) can we get away
without most of these factors, and (b) can we effi-
ciently tell which factors should be discarded. We
will see in section 5 that question (a) can be an-
swered affirmatively: with a only fraction of all sec-
ond order factors we can calculate marginals that are
very close to the BP marginals, and when used in
MBR decoding, lead to the same trees.
Question (b) can be approached by looking at how
a similar problem has been tackled in combinato-
rial optimization and MAP inference. Riedel and
Clarke (2006) tackled the MAP problem for depen-
dency parsing by an incremental approach that starts
with a relaxation of the problem, solves it, and adds
additional constraints only if they are violated. If
constraints were added, the process is repeated, oth-
erwise we terminate.
4.1 Evaluating Candidate Factors
To develop such an incremental relaxation approach
to marginal inference, we generalize the notion of a
violated constraint. What does it mean for a factor to
be violated with respect to the solution of a marginal
inference problem?
One answer is to interpret the violation of a con-
straint as ?adding this constraint will impact our cur-
rent belief?. To assess the impact of adding factor
?i to a sub-graph F ? ? F we can then use the fol-
lowing intuition: if the distribution F ? ? {i} is very
similar to the distribution corresponding to F ?, it is
probably safe to say that the marginals we get from
both are close, too. If we use the KL divergence be-
tween the (distributions of) F ? ? {i} and F ? for our
interpretation of the above mentioned closeness, we
can define a potential gain for adding ?i as follows:
gF ? (?i)
def
= DKL
(
pF ? ||pF ??{i}
)
.
Together with a threshold  on this gain we can
now adapt the relaxation approach to marginal in-
ference by simply replacing the question, ?Is ?i vi-
olated?? with the question, ?Is gF ? (i) > ?? We
can see the latter question as a generalization of the
former if we interpret MAP inference as the zero-
temperature limit of marginal inference (Wainwright
and Jordan, 2008).
The form of the gain function is chosen to be eas-
ily evaluated using the beliefs we have already avail-
able for the current sub-graph F ?. It is easy to show
(see Appendix) that the following holds:
Proposition 1. The gain of a factor ?i with respect
to the sub-graph F ? ? F is
gF ? (?i) = log
(
?F
?
?i + ?
F ?
i e
?i
)
? ?F
?
i ?i (3)
That is, the gain of a factor ?i depends on two
properties of ?i. First, the expectation ?F
?
i that
?i fires under the current model F ?, and second,
763
its loglinear weight ?i. To get an intuition for this
gain, consider the limit lim?F?i ?1
gF ? (?i) of a fac-
tor with positive weight that is expected to be active
under F ?. In this case the gain becomes zero, mean-
ing that the more likely ?i fires under the current
model, the less useful will it be to add according to
our gain. For lim?F?i ?0
gF ? (?i) the gain also disap-
pears. Here the confidence of the current model in ?i
being inactive is so high that any single factor which
indicates the opposite cannot make a difference.
Fortunately, the marginal probability ?F
?
i is usu-
ally available after inference, or can be approxi-
mated. This allows us to maintain the same basic
algorithm as in the MAP case: in each ?inspection
step? we can use the results of the last run of infer-
ence in order to evaluate whether a factor has to be
added or not.
4.2 Algorithm
Algorithm 1 shows our proposed algorithm, Relaxed
Marginal Inference. We are given an initial factor
graph (for example, the first order dependency pars-
ing model), a threshold  on the minimal gain a fac-
tor needs to have in order to be added, and a solver S
for marginal inference in the partial graphs we gen-
erate along the way.
We start by finding the marginals ? for the initial
graph. These marginals are then used in step 4 to
find the factors that would, when added in isolation,
change the distribution substantially (i.e., by more
than  in terms of KL divergence). We will refer
to this step as separation, in line with cutting plane
terminology. The factors are added to the current
graph, and we start from the top unless there were
no new factors added. In this case we return the last
marginals ?.
Clearly, this algorithm is guaranteed to converge:
either we add at least one factor per iteration until
we reach the full graph F , or we converge before.
However, it is difficult to make any general state-
ments about the number of iterations it takes until
convergence. Nevertheless, in our experiments we
find that algorithm 1 converges to a much smaller
graph after a small number of iterations, and hence
we are always faster than inference on the full graph.
Finally, note that calculating the gain for all fac-
tors in F \ F ? in step 4 (separation) takes time pro-
Algorithm 1 Relaxed Marginal Inference.
1: require:
F ?:init. graph, : threshold, S:solver, R: max. it
2: repeat
Find current marginals using solver S
3: ?? marginals(F
?
, S)
Find factors with high gain not yet added
4: ?F ? {i ? F \ F
?
|gF ? (?i) > }
Add factors to current graph
5: F ? ? F ? ??F
Check: no more new factors were added or R reached
6: until ?F = ? or iteration >R
return the marginals for the last graph F ?
7: return ?
portional to |F \ F ?|.
4.3 Accuracy
We have seen how to evaluate the potential gain
when adding a single factor. However, this does
not tell us how good the current sub-model is with
respect to the complete graph. After all, while all
remaining factors individually might not contribute
much, in concert they may. We therefore present a
(calculable) bound on the KL divergence of the par-
tial graph from the full graph that can give us confi-
dence in the solutions we return at convergence.
Note that for this bound we still only need fea-
ture expectations from the current model. More-
over, we assume all weights ?i are positive?without
loss of generality since we can always replace ?i
with its negation 1 ? ?i and then change the sign
of ?i (Richardson and Domingos, 2006).
Proposition 2. Assume non-negative weights, let
F ? ? F be a subset of factors, G
def
= F \ F ? and
?
def
= ??G?1 ? ??G, ?G? ? 0. Then
1. for the KL divergence between F ? and the full
network F we have:
DKL
(
pF ? ||pF
)
? ?.
2. for the error we make when estimating ?i?s true
expectation ?Fi by ?
F ?
i we have:
? (e? ? 1)?F
?
?i ? ?
F
i ? ?
F ?
i ? (e
? ? 1)?F
?
i .
764
This says that (1) we get closer to the full distri-
bution and that (2) our marginals closer to the true
marginals, if the remaining factors G either have
a low total weight ??G?, or the current belief ?G
already assigns high probability to the features ?G
being active (and hence ???G, ?G? is small). The
latter condition is the probabilistic analog to con-
straints already being satisfied. Finally, since ? can
be easily calculated, we plan to investigate its utility
as a convergence criterion in future work.
4.4 Related Work
Our approach is inspired by earlier work on re-
laxation algorithms for performing MAP inference
by incrementally tightening relaxations of a graph-
ical model (Anguelov et al, 2004; Riedel, 2008),
weighted Finite State Machine (Tromble and Eisner,
2006), Integer Linear Program (Riedel and Clarke,
2006) or Marginal Polytope (Sontag et al, 2008).
However, none of these methods apply to marginal
inference.
Sontag and Jaakkola (2007) compute marginal
probabilities by using a cutting plane approach that
starts with the local polytope and then optimizes
some approximation of the log partition function.
Cycle consistency constraints are added if they are
violated by the current marginals, and the process is
repeated until no more violations appear. While this
approach does tackle marginalization, it is focused
on improving its accuracy. In particular, the opti-
mization problems they solve in each iteration are in
fact larger than the problem we want to relax.
Our approach is also related to edge deletion
in Bayesian networks (Choi and Darwiche, 2006).
Here edges are removed from a Bayesian network in
order to find a close approximation to the full net-
work useful for other inference-related tasks (such
as combined marginal and MAP inference). The
core difference to our approach is the fact that they
ask which edges to remove from the full graph, in-
stead of which to add to a partial graph. This re-
quires inference in the full model?the very opera-
tion we want to avoid.
5 Experiments
In our experiments we seek to answer the following
questions. First, how fast is our relaxation approach
compared to full marginal inference at comparable
dependency accuracy? This requires us to find the
best tree in terms of marginal probabilities on the
link variables (Smith and Eisner, 2008). Second,
how good is the final relaxed graph as an approxima-
tion of the full graph? Finally, how does incremental
relaxation scale with sentence length?
5.1 Data and Models
We trained and tested on a subset of languages
from the CoNLL Dependency Parsing Shared
Tasks (Nivre et al, 2007): Dutch, Danish, Italian,
and English. We apply non-projective second order
models for Dutch, Danish and Italian, and a projec-
tive second order model for English. To be able to
compare inference on the same model, we trained
using BP on the full set of LINK, GRAND, and SIB
factors.
Note that our models would rank highly among
the shared task submissions, but could surely be fur-
ther improved. For example, we do not use any lan-
guage specific features. Since our focus in this paper
is speeding up marginal inference, we will search for
better models in future work.
5.2 Runtime and Dependency Accuracy
In our first set of experiments we explore the speed
and accuracy of relaxed BP in comparison to full BP.
To this end we first tested BP configurations with at
most 5, at most 10, and at most 50 iterations to find
the best setup in terms of speed and accuracy. Smith
and Eisner (2008) use 5 iterations but we found that
by using 10 iterations accuracy could be slightly im-
proved. Running at most 50 iterations led to the
same accuracy but was significantly slower. Hence
we only report BP results with 10 iterations here.
For relaxed BP we tested along three dimensions:
the threshold  on the gain of factors, the maximum
number of BP iterations in the inner loop of relaxed
BP, and the maximum number of relaxation itera-
tions. A configuration with maximum relaxation it-
erations R, threshold , and maximum BP iterations
B will be identified by RelR,,B . In all settings we
use the LINK factors and the hard factors as initial
graph F ?.
Table 1 shows the results for several configura-
tions and our four languages in terms of unlabeled
dependency accuracy (percentage of correctly iden-
765
Dutch Danish English Italian
Configuration Acc. Time Acc. Time Acc. Time Acc. Time
BP 84.9 0.665 88.1 1.44 88.3 2.43 87.4 1.68
Rel?,0.0001,5 85.0 0.120 88.1 0.234 88.2 0.575 87.4 0.261
Rel?,0.0001,50 84.9 0.121 88.2 0.236 88.3 0.728 87.4 0.266
Rel1,0.0001,50 84.9 0.060 88.2 0.110 88.4 0.352 87.4 0.132
Table 1: Dependency accuracy (%) and average parsing time (sec.) using second order models.
tified heads) in comparison to the gold data, and av-
erage parsing time in seconds. Here parsing time
includes both time spent for marginal inference and
the MBR decoding step after the marginals are avail-
able.
We notice that by relaxing BP with no limit on the
number of iterations we gain a 4-6 fold increase in
parsing speed across all languages when using the
threshold  = 0.0001, while accuracy remains as
high as for full BP. This can be achieved with fewer
BP iterations (at most 5) in each round of relaxation
than full BP needs per sentence (at most 10). Intu-
itively this makes sense: since our factor graphs are
smaller in each iteration there will be fewer cycles
to slow down convergence. This only has a small
impact on overall parsing time for languages other
than English, since for most sentences even full BP
converges after less than 10 iterations.
We also observe that running just one iteration of
our relaxation algorithm (Rel1,0.0001,50) is enough to
achieve accurate solutions. This leads to a twofold
speed-up in comparison to running relaxation until
convergence (primarily because of fewer calls to the
separation routine), and a 7-13 fold speed-up (ten-
fold on average) when compared to full BP.
5.3 Quality of Relaxed Subgraphs
How large is the fraction of the full graph needed
for accurate marginal probabilities? And do we re-
ally need our relaxation algorithm with repeated in-
ference or could we instead just prune the graph in
advance? Here we try to answer these questions, and
will focus on the Danish dataset. Note that our re-
sults for the other languages follow the same pattern.
In table 2, we present the average ratio of the sizes
of the partial and the full graph in terms of the sec-
ond order factors. We also show the total runtime
needed to find the subgraph and run inference in it.
Configuration Size Time Err. Acc.
BP 100% 1.44 ? 88.1
Rel?,0.1,50 ? 0% 0.12 0.20 87.5
Rel?,0.0001,50 0.8% 0.24 0.012 88.2
Rel1,0.0001,50 0.8% 0.11 0.015 88.2
Pruned0.1 42% 0.56 0.022 88.0
Pruned0.5 22% 0.40 0.098 87.7
Table 2: Ratio of partial and full graph size (Size),
runtime in seconds (Time), avg. error on marginals
(Err.) and tree accuracy (Acc.) for Danish.
As a measure of accuracy for marginal probabilities
we find the average error in marginal probability for
the variables of a sentence. Note that this measure
does not necessarily correspond to the true error of
our marginals because BP itself is approximate and
may not return the correct marginals.
The first row shows the full BP system, working
on 100% of the factor graph. The next three rows
look at relaxed marginal inference. We notice that
with a low threshold  = 0.1 we pick almost no ad-
ditional factors (0.003%), and this does affect accu-
racy. However, by lowering the threshold to 0.0001
and adding about 0.8% of the second order factors,
we already match the dependency accuracy of full
BP. On average we are also very close to the BP
marginals.
Can we find such small graphs without running
extra iterations of inference? One approach could
be to simply cut off factors ?i with absolute weights
|?i| that fall under a certain threshold t. In the final
rows of the table we test such an approach with t =
0.1, 0.5.
We notice that pruning can reduce the second or-
der factors to 42% while yielding (almost) the same
accuracy, and close marginals. However, it is 5 times
slower than our fastest approach. When reducing
766
0 20 40 60
0
20
40
60
Sentence Length
Tim
e
BP
Pruned
Relaxed
Relaxed 1 It.
Figure 1: Total runtimes by sentence length.
size further to about 20%, accuracy drops below the
values we achieved with our relaxation approach at
0.8% of the second order factors. Hence simple
pruning removes factors that do have a low weight,
but are still important to keep.
5.4 Runtime with Varying Sentence Length
We have seen how relaxed BP is faster than full
BP on average. But how does its speed scale with
sentence length? To answer this question figure 1
shows a plot of runtime by sentence length for full
BP, pruned BP with threshold 0.1, Rel?,0.0001,50 and
Rel1,0.0001,50.
The graph indicates that the advantage of relaxed
BP over both full BP and Pruned BP becomes even
more significant for longer sentences, in particular
when running only one iteration. This shows that by
using our technique, second order parsing becomes
more practical, in particular for very long sentences.
6 Conclusion
We have presented a novel incremental relaxation al-
gorithm that can be applied to marginal inference.
Instead of adding violated constraints in each iter-
ation, it adds factors that significantly change the
distribution of the graph. This notion is formalized
by the introduction of a gain function that calculates
the KL divergence between the current network with
and without the candidate factor. We show how this
gain can be calculated and provide bounds on the er-
ror made by the marginals of the relaxed graph in
place of the full one.
Our algorithm led to a tenfold reduction in run-
time at comparable accuracy when applied to multi-
lingual dependency parsing with Belief Propagation.
It is five times faster than pruning factors by their
absolute weight, and results in smaller graphs with
better marginals.
In future work we plan to apply relaxed marginal
inference to larger joint inference problems within
NLP, and test its effectiveness with other marginal
inference algorithms as solvers in the inner loop.
Acknowledgments
This work was supported in part by the Center for
Intelligent Information Retrieval and in part by SRI
International subcontract #27-001338 and ARFL
prime contract #FA8750-09-C-0181. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are the authors? and do not
necessarily reflect those of the sponsor.
Appendix: Proof Sketches
For Proposition 1 we use the primal form of the KL diver-
gence (Wainwright and Jordan, 2008)
D
`
p?F ||pF
?
= log
`
ZFZ
?1
F?
?
? ??F ? , ?F ? ?F??
and represent the ratio ZFZ?1F? of partition functions as
ZF
ZF?
=
X
y
e??F? ,?F? (y)?
ZF?
e??G,?G(y)? = EF?
h
e??G,?G?
i
where G
def
= F \ F ?. With G = {i} we get the desired gain.
For Proposition 2, part 1, we first pick a simple upper bound
on ZFZ?1F? by replacing the expectation with e
??G?1 . Insert-
ing this into the primal form KL divergence leads to the given
bound. For part 2 we represent pF using pF?
pF (y) = ZF?Z
?1
F e
??G,?G(y)?pF? (y)
and reuse our above representation of ZFZ?1F? . This gives
pF (y) = EF?
h
e??G,?G(y)?
i?1
pF? (y) e
??G,?G(y)?
which can be upper bounded by lower bounding the expectation
and upper bounding the log-linear term. For the latter we use
e??G?1 , for the first Jensen?s inequality gives
EF?
h
e??G,?G(y)?
i?1
? eEF? [??G,?G(y)?] = e
D
?G,?
F?
G
E
where the equality follows from linearity of expectations. This
yields pF (y) ? pF? (y) e
? and therefore upper bounds on ?Fi
and ?F?i. Basic algebra then gives the desired error interval for
?Fi in terms of ?
F?
i .
767
References
D. Anguelov, D. Koller, P. Srinivasan, S. Thrun, H.-C.
Pang, and J. Davis. 2004. The correlated correspon-
dence algorithm for unsupervised registration of non-
rigid surfaces. In Advances in Neural Information
Processing Systems (NIPS ?04), pages 33?40.
Arthur Choi and Adnan Darwiche. 2006. A varia-
tional approach for approximating bayesian networks
by edge deletion. In Proceedings of the Proceedings
of the Twenty-Second Conference Annual Conference
on Uncertainty in Artificial Intelligence (UAI-06), Ar-
lington, Virginia. AUAI Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL?
05), pages 363?370, June.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ?06), pages 81?88.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In IWPT ?07: Proceedings of the 10th Inter-
national Conference on Parsing Technologies, pages
121?132, Morristown, NJ, USA. Association for Com-
putational Linguistics.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Conference on Em-
pirical Methods in Natural Language Processing and
Natural Language Learning, pages 915?932.
Matt Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107?136.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?06), pages 129?137.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ?08), pages 468?475.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 145?156, Hon-
olulu, October.
D. Sontag and T. Jaakkola. 2007. New outer bounds on
the marginal polytope. In Advances in Neural Infor-
mation Processing Systems (NIPS ?07), pages 1393?
1400.
David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proceedings of the 24th An-
nual Conference on Uncertainty in AI (UAI ?08).
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Joint Human Lan-
guage Technology Conference/Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ?06), pages 423?
430.
Martin Wainwright and Michael Jordan. 2008. Graphi-
cal Models, Exponential Families, and Variational In-
ference. Now Publishers.
768
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102?111,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Annotation of Search Queries
Michael Bendersky
Dept. of Computer Science
University of Massachusetts
Amherst, MA
bemike@cs.umass.edu
W. Bruce Croft
Dept. of Computer Science
University of Massachusetts
Amherst, MA
croft@cs.umass.edu
David A. Smith
Dept. of Computer Science
University of Massachusetts
Amherst, MA
dasmith@cs.umass.edu
Abstract
Marking up search queries with linguistic an-
notations such as part-of-speech tags, cap-
italization, and segmentation, is an impor-
tant part of query processing and understand-
ing in information retrieval systems. Due
to their brevity and idiosyncratic structure,
search queries pose a challenge to existing
NLP tools. To address this challenge, we
propose a probabilistic approach for perform-
ing joint query annotation. First, we derive
a robust set of unsupervised independent an-
notations, using queries and pseudo-relevance
feedback. Then, we stack additional classi-
fiers on the independent annotations, and ex-
ploit the dependencies between them to fur-
ther improve the accuracy, even with a very
limited amount of available training data. We
evaluate our method using a range of queries
extracted from a web search log. Experimen-
tal results verify the effectiveness of our ap-
proach for both short keyword queries, and
verbose natural language queries.
1 Introduction
Automatic mark-up of textual documents with lin-
guistic annotations such as part-of-speech tags, sen-
tence constituents, named entities, or semantic roles
is a common practice in natural language process-
ing (NLP). It is, however, much less common in in-
formation retrieval (IR) applications. Accordingly,
in this paper, we focus on annotating search queries
submitted by the users to a search engine.
There are several key differences between user
queries and the documents used in NLP (e.g., news
articles or web pages). As previous research shows,
these differences severely limit the applicability of
standard NLP techniques for annotating queries and
require development of novel annotation approaches
for query corpora (Bergsma and Wang, 2007; Barr et
al., 2008; Lu et al, 2009; Bendersky et al, 2010; Li,
2010).
The most salient difference between queries and
documents is their length. Most search queries
are very short, and even longer queries are usually
shorter than the average written sentence. Due to
their brevity, queries often cannot be divided into
sub-parts, and do not provide enough context for
accurate annotations to be made using the stan-
dard NLP tools such as taggers, parsers or chun-
kers, which are trained on more syntactically coher-
ent textual units.
A recent analysis of web query logs by Bendersky
and Croft (2009) shows, however, that despite their
brevity, queries are grammatically diverse. Some
queries are keyword concatenations, some are semi-
complete verbal phrases and some are wh-questions.
It is essential for the search engine to correctly an-
notate the query structure, and the quality of these
query annotations has been shown to be a crucial
first step towards the development of reliable and
robust query processing, representation and under-
standing algorithms (Barr et al, 2008; Guo et al,
2008; Guo et al, 2009; Manshadi and Li, 2009; Li,
2010).
However, in current query annotation systems,
even sentence-like queries are often hard to parse
and annotate, as they are prone to contain mis-
spellings and idiosyncratic grammatical structures.
102
(a) (b) (c)
Term CAP TAG SEG
who L X B
won L V I
the L X B
2004 L X B
kentucky C N B
derby C N I
Term CAP TAG SEG
kindred C N B
where C X B
would C X I
i C X I
be C V I
Term CAP TAG SEG
shih C N B
tzu C N I
health L N B
problems L N I
Figure 1: Examples of a mark-up scheme for annotating capitalization (L ? lowercase, C ? otherwise), POS tags (N ?
noun, V ? verb, X ? otherwise) and segmentation (B/I ? beginning of/inside the chunk).
They also tend to lack prepositions, proper punctu-
ation, or capitalization, since users (often correctly)
assume that these features are disregarded by the re-
trieval system.
In this paper, we propose a novel joint query an-
notation method to improve the effectiveness of ex-
isting query annotations, especially for longer, more
complex search queries. Most existing research fo-
cuses on using a single type of annotation for infor-
mation retrieval such as subject-verb-object depen-
dencies (Balasubramanian and Allan, 2009), named-
entity recognition (Guo et al, 2009), phrase chunk-
ing (Guo et al, 2008), or semantic labeling (Li,
2010).
In contrast, the main focus of this work is on de-
veloping a unified approach for performing reliable
annotations of different types. To this end, we pro-
pose a probabilistic method for performing a joint
query annotation. This method allows us to exploit
the dependency between different unsupervised an-
notations to further improve the accuracy of the en-
tire set of annotations. For instance, our method
can leverage the information about estimated parts-
of-speech tags and capitalization of query terms to
improve the accuracy of query segmentation.
We empirically evaluate the joint query annota-
tion method on a range of query types. Instead of
just focusing our attention on keyword queries, as
is often done in previous work (Barr et al, 2008;
Bergsma and Wang, 2007; Tan and Peng, 2008;
Guo et al, 2008), we also explore the performance
of our annotations with more complex natural lan-
guage search queries such as verbal phrases and wh-
questions, which often pose a challenge for IR appli-
cations (Bendersky et al, 2010; Kumaran and Allan,
2007; Kumaran and Carvalho, 2009; Lease, 2007).
We show that even with a very limited amount of
training data, our joint annotation method signifi-
cantly outperforms annotations that were done in-
dependently for these queries.
The rest of the paper is organized as follows. In
Section 2 we demonstrate several examples of an-
notated search queries. Then, in Section 3, we in-
troduce our joint query annotation method. In Sec-
tion 4 we describe two types of independent query
annotations that are used as input for the joint query
annotation. Section 5 details the related work and
Section 6 presents the experimental results. We draw
the conclusions from our work in Section 7.
2 Query Annotation Example
To demonstrate a possible implementation of lin-
guistic annotation for search queries, Figure 1
presents a simple mark-up scheme, exemplified us-
ing three web search queries (as they appear in a
search log): (a) who won the 2004 kentucky derby,
(b) kindred where would i be, and (c) shih tzu health
problems. In this scheme, each query is marked-
up using three annotations: capitalization, POS tags,
and segmentation indicators.
Note that all the query terms are non-capitalized,
and no punctuation is provided by the user, which
complicates the query annotation process. While
the simple annotation described in Figure 1 can be
done with a very high accuracy for standard docu-
ment corpora, both previous work (Barr et al, 2008;
Bergsma and Wang, 2007; Jones and Fain, 2003)
and the experimental results in this paper indicate
that it is challenging to perform well on queries.
The queries in Figure 1 illustrate this point. Query
(a) in Figure 1 is a wh-question, and it contains
103
a capitalized concept (?Kentucky Derby?), a single
verb, and four segments. Query (b) is a combination
of an artist name and a song title and should be inter-
preted as Kindred ? ?Where Would I Be?. Query (c)
is a concatenation of two short noun phrases: ?Shih
Tzu? and ?health problems?.
3 Joint Query Annotation
Given a search query Q, which consists of a se-
quence of terms (q1, . . . , qn), our goal is to anno-
tate it with an appropriate set of linguistic structures
ZQ. In this work, we assume that the setZQ consists
of shallow sequence annotations zQ, each of which
takes the form
zQ = (?1, . . . , ?n).
In other words, each symbol ?i ? zQ annotates a
single query term.
Many query annotations that are useful for IR
can be represented using this simple form, includ-
ing capitalization, POS tagging, phrase chunking,
named entity recognition, and stopword indicators,
to name just a few. For instance, Figure 1 demon-
strates an example of a set of annotations ZQ. In
this example,
ZQ = {CAP,TAG,SEG}.
Most previous work on query annotation makes
the independence assumption ? every annotation
zQ ? ZQ is done separately from the others. That is,
it is assumed that the optimal linguistic annotation
z?(I)Q is the annotation that has the highest probabil-
ity given the query Q, regardless of the other anno-
tations in the set ZQ. Formally,
z?(I)Q = argmax
zQ
p(zQ|Q) (1)
The main shortcoming of this approach is in the
assumption that the linguistic annotations in the set
ZQ are independent. In practice, there are depen-
dencies between the different annotations, and they
can be leveraged to derive a better estimate of the
entire set of annotations.
For instance, imagine that we need to perform two
annotations: capitalization and POS tagging. Know-
ing that a query term is capitalized, we are more
likely to decide that it is a proper noun. Vice versa,
knowing that it is a preposition will reduce its proba-
bility of being capitalized. We would like to capture
this intuition in the annotation process.
To address the problem of joint query annotation,
we first assume that we have an initial set of annota-
tions Z?(I)Q , which were performed for query Q in-
dependently of one another (we will show an exam-
ple of how to derive such a set in Section 4). Given
the initial set Z?(I)Q , we are interested in obtaining
an annotation set Z?(J)Q , which jointly optimizes the
probability of all the annotations, i.e.
Z?(J)Q = argmax
ZQ
p(ZQ|Z?(I)Q ).
If the initial set of estimations is reasonably ac-
curate, we can make the assumption that the anno-
tations in the set Z?(J)Q are independent given the
initial estimates Z?(I)Q , allowing us to separately op-
timize the probability of each annotation z?(J)Q ?
Z?(J)Q :
z?(J)Q = argmax
zQ
p(zQ|Z?(I)Q ). (2)
From Eq. 2, it is evident that the joint an-
notation task becomes that of finding some opti-
mal unobserved sequence (annotation z?(J)Q ), given
the observed sequences (independent annotation set
Z?(I)Q ).
Accordingly, we can directly use a supervised se-
quential probabilistic model such as CRF (Lafferty
et al, 2001) to find the optimal z?(J)Q . In this CRF
model, the optimal annotation z?(J)Q is the label we
are trying to predict, and the set of independent an-
notations Z?(I)Q is used as the basis for the features
used for prediction. Figure 2 outlines the algorithm
for performing the joint query annotation.
As input, the algorithm receives a training set of
queries and their ground truth annotations. It then
produces a set of independent annotation estimates,
which are jointly used, together with the ground
truth annotations, to learn a CRF model for each an-
notation type. Finally, these CRF models are used
to predict annotations on a held-out set of queries,
which are the output of the algorithm.
104
Input: Qt ? training set of queries.
ZQt ? ground truth annotations for the training set of queries.
Qh ? held-out set of queries.
(1) Obtain a set of independent annotation estimates Z?(I)Qt
(2) Initialize Z?(J)Qt ? ?
(3) for each z?(I)Qt ? Z
?(I)
Qt :
(4) Z ?Qt ? Z
?(I)
Qt \ z
?(I)
Qt
(5) Train a CRF model CRF(zQt) using zQt as a label and Z ?Qt as features.
(6) Predict annotation z?(J)Qh , using CRF(zQt).
(7) Z?(J)Qh ? Z
?(J)
Qh ? z
?(J)
Qh .
Output: Z?(J)Qh ? predicted annotations for the held-out set of queries.
Figure 2: Algorithm for performing joint query annotation.
Note that this formulation of joint query anno-
tation can be viewed as a stacked classification, in
which a second, more effective, classifier is trained
using the labels inferred by the first classifier as fea-
tures. Stacked classifiers were recently shown to be
an efficient and effective strategy for structured clas-
sification in NLP (Nivre and McDonald, 2008; Mar-
tins et al, 2008).
4 Independent Query Annotations
While the joint annotation method proposed in Sec-
tion 3 is general enough to be applied to any set of
independent query annotations, in this work we fo-
cus on two previously proposed independent anno-
tation methods based on either the query itself, or
the top sentences retrieved in response to the query
(Bendersky et al, 2010). The main benefits of these
two annotation methods are that they can be easily
implemented using standard software tools, do not
require any labeled data, and provide reasonable an-
notation accuracy. Next, we briefly describe these
two independent annotation methods.
4.1 Query-based estimation
The most straightforward way to estimate the con-
ditional probabilities in Eq. 1 is using the query it-
self. To make the estimation feasible, Bendersky et
al. (2010) take a bag-of-words approach, and assume
independence between both the query terms and the
corresponding annotation symbols. Thus, the inde-
pentent annotations in Eq. 1 are given by
z?(QRY )Q = argmax
(?1,...,?n)
?
i?(1,...,n)
p(?i|qi). (3)
Following Bendersky et al (2010) we use a large
n-gram corpus (Brants and Franz, 2006) to estimate
p(?i|qi) for annotating the query with capitalization
and segmentation mark-up, and a standard POS tag-
ger1 for part-of-speech tagging of the query.
4.2 PRF-based estimation
Given a short, often ungrammatical query, it is hard
to accurately estimate the conditional probability in
Eq. 1 using the query terms alone. For instance, a
keyword query hawaiian falls, which refers to a lo-
cation, is inaccurately interpreted by a standard POS
tagger as a noun-verb pair. On the other hand, given
a sentence from a corpus that is relevant to the query
such as ?Hawaiian Falls is a family-friendly water-
park?, the word ?falls? is correctly identified by a
standard POS tagger as a proper noun.
Accordingly, the document corpus can be boot-
strapped in order to better estimate the query anno-
tation. To this end, Bendersky et al (2010) employ
the pseudo-relevance feedback (PRF) ? a method
that has a long record of success in IR for tasks such
as query expansion (Buckley, 1995; Lavrenko and
Croft, 2001).
In the most general form, given the set of all re-
trievable sentences r in the corpus C one can derive
p(zQ|Q) =
?
r?C
p(zQ|r)p(r|Q).
Since for most sentences the conditional proba-
bility of relevance to the query p(r|Q) is vanish-
ingly small, the above can be closely approximated
1http://crftagger.sourceforge.net/
105
by considering only a set of sentences R, retrieved
at top-k positions in response to the query Q. This
yields
p(zQ|Q) ?
?
r?R
p(zQ|r)p(r|Q).
Intuitively, the equation above models the query as
a mixture of top-k retrieved sentences, where each
sentence is weighted by its relevance to the query.
Furthermore, to make the estimation of the condi-
tional probability p(zQ|r) feasible, it is assumed that
the symbols ?i in the annotation sequence are in-
dependent, given a sentence r. Note that this as-
sumption differs from the independence assumption
in Eq. 3, since here the annotation symbols are not
independent given the query Q.
Accordingly, the PRF-based estimate for indepen-
dent annotations in Eq. 1 is
z?(PRF )Q = argmax
(?1,...,?n)
?
r?R
?
i?(1,...,n)
p(?i|r)p(r|Q).
(4)
Following Bendersky et al (2010), an estimate of
p(?i|r) is a smoothed estimator that combines the
information from the retrieved sentence r with the
information about unigrams (for capitalization and
POS tagging) and bigrams (for segmentation) from
a large n-gram corpus (Brants and Franz, 2006).
5 Related Work
In recent years, linguistic annotation of search
queries has been receiving increasing attention as an
important step toward better query processing and
understanding. The literature on query annotation
includes query segmentation (Bergsma and Wang,
2007; Jones et al, 2006; Guo et al, 2008; Ha-
gen et al, 2010; Hagen et al, 2011; Tan and Peng,
2008), part-of-speech and semantic tagging (Barr et
al., 2008; Manshadi and Li, 2009; Li, 2010), named-
entity recognition (Guo et al, 2009; Lu et al, 2009;
Shen et al, 2008; Pas?ca, 2007), abbreviation disam-
biguation (Wei et al, 2008) and stopword detection
(Lo et al, 2005; Jones and Fain, 2003).
Most of the previous work on query annotation
focuses on performing a particular annotation task
(e.g., segmentation or POS tagging) in isolation.
However, these annotations are often related, and
thus we take a joint annotation approach, which
combines several independent annotations to im-
prove the overall annotation accuracy. A similar ap-
proach was recently proposed by Guo et al (2008).
There are several key differences, however, between
the work presented here and their work.
First, Guo et al (2008) focus on query refine-
ment (spelling corrections, word splitting, etc.) of
short keyword queries. Instead, we are interested
in annotation of queries of different types, includ-
ing verbose natural language queries. While there
is an overlap between query refinement and annota-
tion, the focus of the latter is on providing linguistic
information about existing queries (after initial re-
finement has been performed). Such information is
especially important for more verbose and gramat-
ically complex queries. In addition, while all the
methods proposed by Guo et al (2008) require large
amounts of training data (thousands of training ex-
amples), our joint annotation method can be effec-
tively trained with a minimal human labeling effort
(several hundred training examples).
An additional research area which is relevant to
this paper is the work on joint structure model-
ing (Finkel and Manning, 2009; Toutanova et al,
2008) and stacked classification (Nivre and Mc-
Donald, 2008; Martins et al, 2008) in natural lan-
guage processing. These approaches have been
shown to be successful for tasks such as parsing and
named entity recognition in newswire data (Finkel
and Manning, 2009) or semantic role labeling in the
Penn Treebank and Brown corpus (Toutanova et al,
2008). Similarly to this work in NLP, we demon-
strate that a joint approach for modeling the linguis-
tic query structure can also be beneficial for IR ap-
plications.
6 Experiments
6.1 Experimental Setup
For evaluating the performance of our query anno-
tation methods, we use a random sample of 250
queries2 from a search log. This sample is manually
labeled with three annotations: capitalization, POS
tags, and segmentation, according to the description
of these annotations in Figure 1. In this set of 250
queries, there are 93 questions, 96 phrases contain-
2The annotations are available at
http://ciir.cs.umass.edu/?bemike/data.html
106
CAP
F1 (% impr) MQA (% impr)
i-QRY 0.641 (-/-) 0.779 (-/-)
i-PRF 0.711?(+10.9/-) 0.811?(+4.1/-)
j-QRY 0.620?(-3.3/-12.8) 0.805?(+3.3/-0.7)
j-PRF 0.718?(+12.0/+0.9) 0.840??(+7.8/+3.6)
TAG
Acc. (% impr) MQA (% impr)
i-QRY 0.893 (-/-) 0.878 (-/-)
i-PRF 0.916?(+2.6/-) 0.914?(+4.1/-)
j-QRY 0.913?(+2.2/-0.3) 0.912?(+3.9/-0.2)
j-PRF 0.924?(+3.5/+0.9) 0.922?(+5.0/+0.9)
SEG
F1 (% impr) MQA (% impr)
i-QRY 0.694 (-/-) 0.672 (-/-)
i-PRF 0.753?(+8.5/-) 0.710?(+5.7/-)
j-QRY 0.817??(+17.7/+8.5) 0.803??(+19.5/+13.1)
j-PRF 0.819??(+18.0/+8.8) 0.803??(+19.5/+13.1)
Table 1: Summary of query annotation performance for
capitalization (CAP), POS tagging (TAG) and segmenta-
tion. Numbers in parentheses indicate % of improvement
over the i-QRY and i-PRF baselines, respectively. Best
result per measure and annotation is boldfaced. ? and ?
denote statistically significant differences with i-QRY and
i-PRF, respectively.
ing a verb, and 61 short keyword queries (Figure 1
contains a single example of each of these types).
In order to test the effectiveness of the joint query
annotation, we compare four methods. In the first
two methods, i-QRY and i-PRF the three annotations
are done independently. Method i-QRY is based on
z?(QRY )Q estimator (Eq. 3). Method i-PRF is based
on the z?(PRF )Q estimator (Eq. 4).
The next two methods, j-QRY and j-PRF, are joint
annotation methods, which perform a joint optimiza-
tion over the entire set of annotations, as described
in the algorithm in Figure 2. j-QRY and j-PRF differ
in their choice of the initial independent annotation
set Z?(I)Q in line (1) of the algorithm (see Figure 2).
j-QRY uses only the annotations performed by i-
QRY (3 initial independent annotation estimates),
while j-PRF combines the annotations performed by
i-QRY with the annotations performed by i-PRF (6
initial annotation estimates). The CRF model train-
ing in line (6) of the algorithm is implemented using
CRF++ toolkit3.
3http://crfpp.sourceforge.net/
The performance of the joint annotation methods
is estimated using a 10-fold cross-validation. In or-
der to test the statistical significance of improve-
ments attained by the proposed methods we use a
two-sided Fisher?s randomization test with 20,000
permutations. Results with p-value < 0.05 are con-
sidered statistically significant.
For reporting the performance of our meth-
ods we use two measures. The first measure is
classification-oriented ? treating the annotation de-
cision for each query term as a classification. In case
of capitalization and segmentation annotations these
decisions are binary and we compute the precision
and recall metrics, and report F1 ? their harmonic
mean. In case of POS tagging, the decisions are
ternary, and hence we report the classification ac-
curacy.
We also report an additional, IR-oriented perfor-
mance measure. As is typical in IR, we propose
measuring the performance of the annotation meth-
ods on a per-query basis, to verify that the methods
have uniform impact across queries. Accordingly,
we report the mean of classification accuracies per
query (MQA). Formally, MQA is computed as
?N
i=1 accQi
N ,
where accQi is the classification accuracy for query
Qi, and N is the number of queries.
The empirical evaluation is conducted as follows.
In Section 6.2, we discuss the general performance
of the four annotation techniques, and compare the
effectiveness of independent and joint annotations.
In Section 6.3, we analyze the performance of the
independent and joint annotation methods by query
type. In Section 6.4, we compare the difficulty
of performing query annotations for different query
types. Finally, in Section 6.5, we compare the effec-
tiveness of the proposed joint annotation for query
segmentation with the existing query segmentation
methods.
6.2 General Evaluation
Table 1 shows the summary of the performance of
the two independent and two joint annotation meth-
ods for the entire set of 250 queries. For independent
methods, we see that i-PRF outperforms i-QRY for
107
CAP Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.750 0.862 0.590 0.839 0.784 0.687
j-PRF 0.687?(-8.4%) 0.839?(-2.7%) 0.671?(+13.7%) 0.913?(+8.8%) 0.814 (+3.8%) 0.732? (+6.6%)
TAG Verbal Phrases Questions Keywords
Acc. MQA Acc. MQA Acc. MQA
i-PRF 0.908 0.908 0.932 0.935 0.880 0.890
j-PRF 0.904 (-0.4%) 0.906 (-0.2%) 0.951? (+2.1%) 0.953? (+1.9%) 0.893 (+1.5%) 0.900 (+1.1%)
SEG Verbal Phrases Questions Keywords
F1 MQA F1 MQA F1 MQA
i-PRF 0.751 0.700 0.740 0.700 0.816 0.747
j-PRF 0.772 (+2.8%) 0.742?(+6.0%) 0.858?(+15.9%) 0.838?(+19.7%) 0.844 (+3.4%) 0.853?(+14.2%)
Table 2: Detailed analysis of the query annotation performance for capitalization (CAP), POS tagging (TAG) and
segmentation by query type. Numbers in parentheses indicate % of improvement over the i-PRF baseline. Best result
per measure and annotation is boldfaced. ? denotes statistically significant differences with i-PRF.
all annotation types, using both performance mea-
sures.
In Table 1, we can also observe that the joint anno-
tation methods are, in all cases, better than the cor-
responding independent ones. The highest improve-
ments are attained by j-PRF, which always demon-
strates the best performance both in terms of F1 and
MQA. These results attest to both the importance of
doing a joint optimization over the entire set of an-
notations and to the robustness of the initial annota-
tions done by the i-PRF method. In all but one case,
the j-PRF method, which uses these annotations as
features, outperforms the j-QRY method that only
uses the annotation done by i-QRY .
The most significant improvements as a result of
joint annotation are observed for the segmentation
task. In this task, joint annotation achieves close to
20% improvement in MQA over the i-QRY method,
and more than 10% improvement in MQA over the i-
PRF method. These improvements indicate that the
segmentation decisions are strongly guided by cap-
italization and POS tagging. We also note that, in
case of segmentation, the differences in performance
between the two joint annotation methods, j-QRY
and j-PRF, are not significant, indicating that the
context of additional annotations in j-QRY makes up
for the lack of more robust pseudo-relevance feed-
back based features.
We also note that the lowest performance im-
provement as a result of joint annotation is evi-
denced for POS tagging. The improvements of joint
annotation method j-PRF over the i-PRF method are
less than 1%, and are not statistically significant.
This is not surprising, since the standard POS tag-
gers often already use bigrams and capitalization at
training time, and do not acquire much additional
information from other annotations.
6.3 Evaluation by Query Type
Table 2 presents a detailed analysis of the perfor-
mance of the best independent (i-PRF) and joint (j-
PRF) annotation methods by the three query types
used for evaluation: verbal phrases, questions and
keyword queries. From the analysis in Table 2, we
note that the contribution of joint annotation varies
significantly across query types. For instance, us-
ing j-PRF always leads to statistically significant im-
provements over the i-PRF baseline for questions.
On the other hand, it is either statistically indistin-
guishable, or even significantly worse (in the case of
capitalization) than the i-PRF baseline for the verbal
phrases.
Table 2 also demonstrates that joint annotation
has a different impact on various annotations for the
same query type. For instance, j-PRF has a signif-
icant positive effect on capitalization and segmen-
tation for keyword queries, but only marginally im-
proves the POS tagging. Similarly, for the verbal
phrases, j-PRF has a significant positive effect only
for the segmentation annotation.
These variances in the performance of the j-PRF
method point to the differences in the structure be-
108
Annotation Performance by Query Type
F1
Verbal Phrases Questions Keyword Queries
60
65
70
75
80
85
90
95
10
0
CAP
SEG
TAG
Figure 3: Comparative performance (in terms of F1 for
capitalization and segmentation and accuracy for POS
tagging) of the j-PRF method on the three query types.
tween the query types. While dependence between
the annotations plays an important role for question
and keyword queries, which often share a common
grammatical structure, this dependence is less use-
ful for verbal phrases, which have a more diverse
linguistic structure. Accordingly, a more in-depth
investigation of the linguistic structure of the verbal
phrase queries is an interesting direction for future
work.
6.4 Annotation Difficulty
Recall that in our experiments, out of the overall 250
annotated queries, there are 96 verbal phrases, 93
questions and 61 keyword queries. Figure 3 shows a
plot that contrasts the relative performance for these
three query types of our best-performing joint an-
notation method, j-PRF, on capitalization, POS tag-
ging and segmentation annotation tasks. Next, we
analyze the performance profiles for the annotation
tasks shown in Figure 3.
For the capitalization task, the performance of j-
PRF on verbal phrases and questions is similar, with
the difference below 3%. The performance for key-
word queries is much higher ? with improvement
over 20% compared to either of the other two types.
We attribute this increase to both a larger number
of positive examples in the short keyword queries
(a higher percentage of terms in keyword queries is
capitalized) and their simpler syntactic structure (ad-
SEG F1 MQA
SEG-1 0.768 0.754
SEG-2 0.824? 0.787?
j-PRF 0.819? (+6.7%/-0.6%) 0.803? (+6.5%/+2.1%)
Table 3: Comparison of the segmentation performance
of the j-PRF method to two state-of-the-art segmentation
methods. Numbers in parentheses indicate % of improve-
ment over the SEG-1 and SEG-2 baselines respectively.
Best result per measure and annotation is boldfaced. ?
denotes statistically significant differences with SEG-1.
jacent terms in these queries are likely to have the
same case).
For the segmentation task, the performance is at
its best for the question and keyword queries, and at
its worst (with a drop of 11%) for the verbal phrases.
We hypothesize that this is due to the fact that ques-
tion queries and keyword queries tend to have repet-
itive structures, while the grammatical structure for
verbose queries is much more diverse.
For the tagging task, the performance profile is re-
versed, compared to the other two tasks ? the per-
formance is at its worst for keyword queries, since
their grammatical structure significantly differs from
the grammatical structure of sentences in news arti-
cles, on which the POS tagger is trained. For ques-
tion queries the performance is the best (6% increase
over the keyword queries), since they resemble sen-
tences encountered in traditional corpora.
It is important to note that the results reported in
Figure 3 are based on training the joint annotation
model on all available queries with 10-fold cross-
validation. We might get different profiles if a sep-
arate annotation model was trained for each query
type. In our case, however, the number of queries
from each type is not sufficient to train a reliable
model. We leave the investigation of separate train-
ing of joint annotation models by query type to fu-
ture work.
6.5 Additional Comparisons
In order to further evaluate the proposed joint an-
notation method, j-PRF, in this section we compare
its performance to other query annotation methods
previously reported in the literature. Unfortunately,
there is not much published work on query capi-
talization and query POS tagging that goes beyond
the simple query-based methods described in Sec-
109
tion 4.1. The published work on the more advanced
methods usually requires access to large amounts of
proprietary user data such as query logs and clicks
(Barr et al, 2008; Guo et al, 2008; Guo et al, 2009).
Therefore, in this section we focus on recent work
on query segmentation (Bergsma and Wang, 2007;
Hagen et al, 2010). We compare the segmentation
effectiveness of our best performing method, j-PRF,
to that of these query segmentation methods.
The first method, SEG-1, was first proposed by
Hagen et al (2010). It is currently the most effective
publicly disclosed unsupervised query segmentation
method. SEG-1 method requires an access to a large
web n-gram corpus (Brants and Franz, 2006). The
optimal segmentation for query Q, S?Q, is then ob-
tained using
S?Q = argmax
S?SQ
?
s?S,|s|>1
|s||s|count(s),
where SQ is the set of all possible query segmenta-
tions, S is a possible segmentation, s is a segment
in S, and count(s) is the frequency of s in the web
n-gram corpus.
The second method, SEG-2, is based on a success-
ful supervised segmentation method, which was first
proposed by Bergsma and Wang (2007). SEG-2 em-
ploys a large set of features, and is pre-trained on the
query collection described by Bergsma and Wang
(2007). The features used by the SEG-2 method are
described by Bendersky et al (2009), and include,
among others, n-gram frequencies in a sample of a
query log, web corpus and Wikipedia titles.
Table 3 demonstrates the comparison between the
j-PRF, SEG-1 and SEG-2 methods. When com-
pared to the SEG-1 baseline, j-PRF is significantly
more effective, even though it only employs bigram
counts (see Eq. 4), instead of the high-order n-grams
used by SEG-1, for computing the score of a seg-
mentation. This results underscores the benefit of
joint annotation, which leverages capitalization and
POS tagging to improve the quality of the segmen-
tation.
When compared to the SEG-2 baseline, j-PRF
and SEG-2 are statistically indistinguishable. SEG-2
posits a slightly better F1, while j-PRF has a better
MQA. This result demonstrates that the segmenta-
tion produced by the j-PRF method is as effective as
the segmentation produced by the current supervised
state-of-the-art segmentation methods, which em-
ploy external data sources and high-order n-grams.
The benefit of the j-PRF method compared to the
SEG-2 method, is that, simultaneously with the seg-
mentation, it produces several additional query an-
notations (in this case, capitalization and POS tag-
ging), eliminating the need to construct separate se-
quence classifiers for each annotation.
7 Conclusions
In this paper, we have investigated a joint approach
for annotating search queries with linguistic struc-
tures, including capitalization, POS tags and seg-
mentation. To this end, we proposed a probabilis-
tic approach for performing joint query annotation
that takes into account the dependencies that exist
between the different annotation types.
Our experimental findings over a range of queries
from a web search log unequivocally point to the su-
periority of the joint annotation methods over both
query-based and pseudo-relevance feedback based
independent annotation methods. These findings in-
dicate that the different annotations are mutually-
dependent.
We are encouraged by the success of our joint
query annotation technique, and intend to pursue the
investigation of its utility for IR applications. In the
future, we intend to research the use of joint query
annotations for additional IR tasks, e.g., for con-
structing better query formulations for ranking al-
gorithms.
8 Acknowledgment
This work was supported in part by the Center for In-
telligent Information Retrieval and in part by ARRA
NSF IIS-9014442. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect those of the sponsor.
110
References
Niranjan Balasubramanian and James Allan. 2009. Syn-
tactic query models for restatement retrieval. In Proc.
of SPIRE, pages 143?155.
Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of english web-search queries. In
Proc. of EMNLP, pages 1021?1030.
Michael Bendersky and W. Bruce Croft. 2009. Analysis
of long queries in a large scale search log. In Proc. of
Workshop on Web Search Click Data, pages 8?14.
Michael Bendersky, David Smith, and W. Bruce Croft.
2009. Two-stage query segmentation for information
retrieval. In Proc. of SIGIR, pages 810?811.
Michael Bendersky, W. Bruce Croft, and David A. Smith.
2010. Structural annotation of search queries using
pseudo-relevance feedback. In Proc. of CIKM, pages
1537?1540.
Shane Bergsma and Qin I. Wang. 2007. Learning noun
phrase query segmentation. In Proc. of EMNLP, pages
819?826.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Chris Buckley. 1995. Automatic query expansion using
SMART. In Proc. of TREC-3, pages 69?80.
Jenny R. Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc.
of NAACL, pages 326?334.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Proc. of SIGIR, pages 379?386.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proc. of SIGIR,
pages 267?274.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Braeutigam. 2010. The power of naive query
segmentation. In Proc. of SIGIR, pages 797?798.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query segmentation re-
visited. In Proc. of WWW, pages 97?106.
Rosie Jones and Daniel C. Fain. 2003. Query word dele-
tion prediction. In Proc. of SIGIR, pages 435?436.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proc. of WWW, pages 387?396.
Giridhar Kumaran and James Allan. 2007. A case for
shorter queries, and helping user create them. In Proc.
of NAACL, pages 220?227.
Giridhar Kumaran and Vitor R. Carvalho. 2009. Re-
ducing long queries using query quality predictors. In
Proc. of SIGIR, pages 564?571.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. of ICML, pages 282?289.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance
based language models. In Proc. of SIGIR, pages 120?
127.
Matthew Lease. 2007. Natural language processing for
information retrieval: the time is ripe (again). In Pro-
ceedings of PIKM.
Xiao Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proc. of ACL, pages 1337?
1345, Morristown, NJ, USA.
Rachel T. Lo, Ben He, and Iadh Ounis. 2005. Auto-
matically building a stopword list for an information
retrieval system. In Proc. of DIR.
Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, and
Benoit Dumoulin. 2009. Improving Web search rel-
evance with semantic features. In Proc. of EMNLP,
pages 648?657.
Mehdi Manshadi and Xiao Li. 2009. Semantic Tagging
of Web Search Queries. In Proc. of ACL, pages 861?
869.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP, pages 157?166.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL, pages 950?958.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM, pages 683?690.
Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz,
and Ying Li. 2008. Personal name classification in
web queries. In Proc. of WSDM, pages 149?158.
Bin Tan and Fuchun Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In Proc. of WWW, pages 347?356.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34:161?191,
June.
Xing Wei, Fuchun Peng, and Benoit Dumoulin. 2008.
Analyzing web text association to disambiguate abbre-
viation in queries. In Proc. of SIGIR, pages 751?752.
111
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 885?894,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Discriminative Model for Joint Morphological Disambiguation and
Dependency Parsing
John Lee
Department of Chinese,
Translation and Linguistics
City University of Hong Kong
jsylee@cityu.edu.hk
Jason Naradowsky, David A. Smith
Department of Computer Science
University of Massachusetts, Amherst
{narad,dasmith}@cs.umass.edu
Abstract
Most previous studies of morphological dis-
ambiguation and dependency parsing have
been pursued independently. Morphological
taggers operate on n-grams and do not take
into account syntactic relations; parsers use
the ?pipeline? approach, assuming that mor-
phological information has been separately
obtained.
However, in morphologically-rich languages,
there is often considerable interaction between
morphology and syntax, such that neither can
be disambiguated without the other. In this pa-
per, we propose a discriminative model that
jointly infers morphological properties and
syntactic structures. In evaluations on various
highly-inflected languages, this joint model
outperforms both a baseline tagger in morpho-
logical disambiguation, and a pipeline parser
in head selection.
1 Introduction
To date, studies of morphological analysis and
dependency parsing have been pursued more or
less independently. Morphological taggers dis-
ambiguate morphological attributes such as part-
of-speech (POS) or case, without taking syntax
into account (Hakkani-Tu?r et al, 2000; Hajic? et
al., 2001); dependency parsers commonly assume
the ?pipeline? approach, relying on morphologi-
cal information as part of the input (Buchholz and
Marsi, 2006; Nivre et al, 2007). This approach
serves many languages well, especially those with
less morphological ambiguity. In English, for ex-
ample, accuracy of POS tagging has risen above
97% (Toutanova et al, 2003), and that of depen-
dency parsing has reached the low nineties (Nivre
et al, 2007). For these languages, there may be little
to be gained to justify the computational cost of in-
corporating syntactic inference during the morpho-
logical tagging task; conversely, it is doubtful that
errorful morphological information is a main cause
of errors in English dependency parsing.
However, the pipeline approach seems more prob-
lematic for morphologically-rich languages with
substantial interactions between morphology and
syntax (Tsarfaty, 2006). Consider the Latin sen-
tence, Una dies omnis potuit praecurrere amantis,
?One day was able to make up for all the lovers?1. As
shown in Table 1, the adjective omnis (?all?) is am-
biguous in number, gender, and case; there are seven
valid analyses. From the perspective of a finite-
state morphological tagger, the most attractive anal-
ysis is arguably the singular nominative, since omnis
is immediately followed by the singular verb potuit
(?could?). Indeed, the baseline tagger used in this
study did make this decision. Given its nominative
case, the pipeline parser assigned the verb potuit to
be its head; the two words form the typical subject-
verb relation, agreeing in number.
Unfortunately, as shown in Figure 1, the word om-
nis in fact modifies the noun amantis, at the end of
the sentence. As a result, despite the distance be-
tween them, they must agree in number, gender and
case, i.e., both must be plural masculine (or femi-
nine) accusative. The pipeline parser, acting on the
input that omnis is nominative, naturally did not see
1Taken from poem 1.13 by Sextus Propertius, English trans-
lation by Katz (2004).
885
Latin Una dies omnis potuit praecurrere amantis
English one day all could to surpass lovers
Number sg pl sg pl sg sg pl sg - sg pl
Gender f n m/f m/f m/f m/f/n m/f - - m/f/n m/f
Case nom/ab nom/acc nom nom/acc nom gen acc - - gen acc
Table 1: The Latin sentence ?Una dies omnis potuit praecurrere amantis?, meaning ?One day was able to make up
for all the lovers?, shown with glosses and possible morphological analyses. The correct analyses are shown in bold.
The word omnis has 7 possible combinations of number, gender and case, while amantis has 5. Disambiguation partly
depends on establishing amantis as the head of omnis, and so the two must agree in all three attributes.
this agreement, and therefore did not consider this
syntactic relation likely.
Such a dilemma is not uncommon in languages
with relatively free word order. On the one hand,
it appears difficult to improve morphological tag-
ging accuracy on words like omnis without syntactic
knowledge; on the other hand, a parser cannot reli-
ably disambiguate syntax unless it has accurate mor-
phological information, in this example the agree-
ment in number, gender, and case.
In this paper we propose to attack this chicken-
and-egg problem with a discriminative model that
jointly infers morphological and syntactic properties
of a sentence, given its words as input. In eval-
uations on various highly-inflected languages, the
model outperforms both a baseline tagger in mor-
phological disambiguation, and a pipeline parser in
head selection.
After a description of previous work (?2), the
joint model (?3) will be contrasted with the base-
line pipeline model (?4). Experimental results (?5-
6) will then be presented, followed by conclusions
and future directions.
2 Previous Work
Since space does not allow a full review of the vast
literature on morphological analysis and parsing, we
focus only on past research involving joint morpho-
logical and syntactic inference (?2.1); we then dis-
cuss Latin (?2.2), a language representative of the
challenges that motivated our approach.
2.1 Joint Morphological and Syntactic
Inference
Most previous work in morphological disambigua-
tion, even when applied on morphologically com-
plex languages with relatively free word order,
potuit
could
dies
day
una
one
praecurrere
to surpass
amantis
lovers
omnis
all
Figure 1: Dependency tree for the sentence ?Una dies
omnis potuit praecurrere amantis?. The word omnis is
an adjective modifying the noun amantis. This informa-
tion is key to the morphological disambiguation of both
words, as shown in Table 1.
such as Turkish (Hakkani-Tu?r et al, 2000) and
Czech (Hajic? et al, 2001), did not consider syn-
tactic relationships between words. In the litera-
ture on data-driven parsing, two recent studies at-
tempted joint inference on morphology and syntax,
and both considered phrase-structure trees for Mod-
ern Hebrew (Cohen and Smith, 2007; Goldberg and
Tsarfaty, 2008).
The primary focus of morphological processing in
Modern Hebrew is splitting orthographic words into
morphemes: clitics such as prepositions, pronouns,
and the definite article must be separated from the
core word. Each of the resulting morphemes is then
tagged with an atomic ?part-of-speech? to indicate
word class and some morphological features. Sim-
ilarly, the English POS tags in the Penn Treebank
combine word class information with morphologi-
886
cal attributes such as ?plural? or ?past tense?.
Cohen and Smith (2007) separately train a dis-
criminative conditional random field (CRF) for seg-
mentation and tagging, and a generative probabilis-
tic context-free grammar (PCFG) for parsing. At de-
coding time, the two models are combined as a prod-
uct of experts. Goldberg and Tsarfaty (2008) pro-
pose a generative joint model. This paper is the first
to use a fully discriminative model for joint morpho-
logical and syntactic inference on dependency trees.
2.2 Latin
Unlike Modern Hebrew, Latin does not require ex-
tensive morpheme segmentation2. However, it does
have a relatively free word order, and is also highly
inflected, with each word having up to nine morpho-
logical attributes, listed in Table 2. In addition to its
absolute numbers of cases, moods, and tenses, Latin
morphology is fusional. For instance, the suffix
?is in omnis cannot be segmented into morphemes
that separately indicate gender, number, and case.
According to the Latin morphological database en-
coded in MORPHEUS (Crane, 1991), 30% of Latin
nouns can be parsed as another part-of-speech, and
on average each has 3.8 possible morphological in-
terpretations.
We know of only one previous attempt in data-
driven dependency parsing for Latin (Bamman and
Crane, 2008), with the goal of constructing a dy-
namic lexicon for a digital library. Parsing is per-
formed using the usual pipeline approach, first with
the TreeTagger analyzer (Schmid, 1994) and then
with a state-of-the-art dependency parser (McDon-
ald et al, 2005). Head selection accuracy was
61.49%, and rose to 64.99% with oracle morpho-
logical tags. Of the nine morphological attributes,
gender and especially case had the lowest accu-
racy. This observation echoes the findings for
Czech (Smith et al, 2005), where case was also the
most difficult to disambiguate.
3 Joint Model
This section describes a model that jointly infers
morphological and syntactic properties of a sen-
tence. It will be presented as a graphical model,
2Except for enclitics such as -que, -ve, and -ne, but their
segmentation is rather straightforward compared to Modern He-
brew or other Semitic languages.
Attribute Values
Part-of- noun, verb, participle, adjective,
speech adverb, conjunction, preposition,
(POS) pronoun, numeral, interjection,
exclamation, punctuation
Person first, second, third
Number singular, plural
Tense present, imperfect, perfect,
pluperfect, future perfect, future
Mood indicative, subjunctive, infinitive,
imperative, participle, gerund,
gerundive, supine
Voice active, passive
Gender masculine, feminine, neuter
Case nominative, genitive, dative,
accusative, ablative, vocative,
locative
Degree comparative, superlative
Table 2: Morphological attributes and values for Latin.
Ancient Greek has the same attributes; Czech and Hun-
garian lack some of them. In all categories except POS,
a value of null (?-?) may also be assigned. For example, a
noun has ?-? for the tense attribute.
starting with the variables and then the factors,
which represents constraints on the variables. Let
n be the number of words and m be the number of
possible values for a morphological attribute. The
variables are:
? WORD: the n words w1,...,wn of the input sen-
tence, all observed.
? TAG: O(nm) boolean variables3 Ta,i,v, corre-
sponding to each value of the morphological at-
tributes listed in Table 2. Ta,i,v = true when
the word wi has value v as its morphological
attribute a. In Figure 2, CASE3,acc is the short-
hand representing the variable Tcase,3,acc. It is
set to true since the wordw3 has the accusative
case.
? LINK: O(n2) boolean variables Li,j corre-
sponding to a possible link between each pair
3The TAG variables were actually implemented as multino-
mials, but are presented here as booleans for ease of understand-
ing.
887
UNIGRAMCASE?
UNIGRAMCASE?
CASE?
LINK
CASE?
LINK
CASE?
LINK
CASE?
LINK
CASE    6,gen
CASE    3,gen
CASE    3,nom
 3,accCASE
UNIGRAMCASE?
UNIGRAMCASE?
UNIGRAMCASE?
CASE    2,...
CASE
LINK
CASE
 6,acc
CASE?
BIGRAMCASE?BIGRAM
TREE WORD?LINKWORDLINK
CASE    5,...
L L3,6 4,6
Figure 2: The joint model (?3) depicted as a graphical model. The variables, all boolean, are represented by circles and
are bolded if their correct values are true. Factors are represented by rectangles and are bolded if they fire. For clarity,
this graph shows only those variables and factors associated with one pair of words (i.e., w3=omnis and w6=amantis)
and with one morphological attribute (i.e., case). The variables L3,6, CASE3,acc and CASE6,acc are bolded, indicating
that w3 and w6 are linked and both have the accusative case. The ternary factor CASE-LINK, that connects to these
three variable, therefore fires.
of words4. Li,j = true when there is a depen-
dency link from the word wi to the word wj . In
Figure 2, the variable L3,6 is set to true since
there is a dependency link between the words
w3 and w6.
We define a probability distribution over all joint as-
signments A to the above variables,
p(A) =
1
Z
?
k
Fk(A) (1)
where Z is a normalizing constant. The assign-
ment A is subject to a hard constraint, represented
in Figure 2 as TREE, requiring that the values of
the LINK variables must yield a tree, which may
be non-projective. The factors Fk(A) represent soft
constraints evaluating various aspects of the ?good-
ness? of the tree structure implied by A. We say a
factor ?fires? when all its neighboring variables are
4Variables for link labels can be integrated in a straightfor-
ward manner, if desired.
true and it evaluates to a non-negative real num-
ber; otherwise, it evaluates to 1 and has no effect
on the product in equation (1). Soft constraints in
the model are divided into local and link factors, to
which we now turn.
3.1 Local Factors
The local factors consult either one word or two
neighboring words, and their morphological at-
tributes. These factors express the desirability of the
assignments of morphological attributes based on lo-
cal context. There are three types:
? TAG-UNIGRAM: There are O(nm) such unary
factors, each instance of which is connected to
a TAG variable. The factor fires when Ta,i,v
is true. The features consist of the value v
of the morphological attribute concerned, com-
bined with the word identity of wi, with back-
off using all suffixes of the word. The CASE-
UNIGRAM factors shown in Figure 2 are ex-
amples of this family of factors.
888
? TAG-BIGRAM: There are O(nm2) of such bi-
nary factors, each connected to the TAG vari-
ables of a pair of neighboring words. The factor
fires when Ta,i,v1 and Ta,i+1,v2 are both true.
The CASE-BIGRAM factors shown in Figure 2
are examples of this family of factors.
? TAG-CONSISTENCY: For each word, the TAG
variables representing the possible POS val-
ues are connected to those representing the val-
ues of other morphological attributes, yield-
ing O(nm2) binary factors. They fire when
Tpos,i,v1 and Ta,i,v2 are both true. These fac-
tors are intended to discourage inconsistent as-
signments, such as a non-null tense for a noun.
It is clear that so far, none of these factors are aware
of the morphological agreement between omnis and
amantis, crucial for inferring their syntactic relation.
We now turn our attention to link factors, which
serve this purpose.
3.2 Link Factors
The link factors consult all pairs of words, possibly
separated by a long distance, that may have a de-
pendency link. These factors model the likelihood
of such a link based on the word identities and their
morphological attributes:
? WORD-LINK: There areO(n2) such unary fac-
tors, each connected to a LINK variable, as
shown in Figure 2. The factor fires when Li,j
is true. Features include various combina-
tions of the word identities of the parent wi and
child wj , and 5-letter prefixes of these words,
replicating the so-called ?basic features? used
by McDonald et al (2005).
? POS-LINK: There are O(n2m2) such ternary
factors, each connected to the variables Li,j ,
Ti,pos,vi and Tj,pos,vj . It fires when all three are
true or, in other words, when the parent word
wi has POS vi, and the child wj has POS vj .
Features replicate all the so-called ?basic fea-
tures? used by McDonald et al (2005) that in-
volve POS. These factors are not shown in Fig-
ure 2, but would have exactly the same struc-
ture as the CASE-LINK factors.
Beyond these basic features, McDonald et al
(2005) also utilize POS trigrams and POS 4-
grams. Both include the POS of two linked
words, wi and wj . The third component in the
trigrams is the POS of each word wk located
between wi and wj , i < k < j. The two ad-
ditional components that make up the 4-grams
are subsets of the POS of words located to the
immediate left and right of wi and wj .
If fully implemented in our joint model, these
features would necessitate two separate fami-
lies of link factors: O(n3m3) factors for the
POS trigrams, and O(n2m4) factors for the
POS 4-grams. To avoid this substantial in-
crease in model complexity, these features are
instead approximated: the POS of all words
involved in the trigrams and 4-grams, except
those of wi and wj , are regarded as fixed, their
values being taken from the output of a mor-
phological tagger (?4.1), rather than connected
to the appropriate TAG variables. This approxi-
mation allows these features to be incorporated
in the POS-LINK factors.
? MORPH-LINK: There are O(n2m2) such
ternary factors, each connected to the variables
Li,j , Ti,a,vi and Tj,a,vj , for every attribute a
other than POS. The factor fires when all three
variables are true, and both vi and vj are non-
null; i.e., it fires when the parent word wi has
vi as its morphological attribute a, and the child
wj has vj . Features include the combination of
vi and vj themselves, and agreement between
them. The CASE-LINK factors in Figure 2 are
an example of this family of factors.
4 Baselines
To ensure a meaningful comparison with the joint
model, our two baselines are both implemented in
the same graphical model framework, and trained
with the same machine-learning algorithm. Roughly
speaking, they divide up the variables and factors of
the joint model and train them separately. For mor-
phological disambiguation, we use the baseline tag-
ger described in ?4.1. For dependency parsing, our
baseline is a ?pipeline? parser (?4.2) that infers syn-
tax upon the output of the baseline tagger.
889
4.1 Baseline Morphological Tagger
The tagger is a graphical model with the WORD
and TAG variables, connected by the local fac-
tors TAG-UNIGRAM, TAG-BIGRAM, and TAG-
CONSISTENCY, all used in the joint model (?3).
4.2 Baseline Dependency Parser
The parser has no local factors, but has the same
variables as the joint model and the same features
from all three families of link factors (?3). However,
since it takes as input the morphological attributes
predicted by the tagger, the TAG variables are now
observed. This leads to a change in the structure
of the link factors ? all features from the POS-
LINK factors now belong to the WORD-LINK fac-
tors, since the POS of all words are observed. In
short, the features of the parser are a replication of
(McDonald et al, 2005), but also extended beyond
POS to the other morphological attributes, with the
features in the MORPH-LINK factors incorporated
into WORD-LINK for similar reasons.
5 Experimental Set-up
5.1 Data
Our evaluation focused on the Latin Dependency
Treebank (Bamman and Crane, 2006), created at
the Perseus Digital Library by tailoring the Prague
Dependency Treebank guidelines for the Latin lan-
guage. It consists of excerpts from works by eight
Latin authors. We randomly divided the 53K-word
treebank into 10 folds of roughly equal sizes, with an
average of 5314 words (347 sentences) per fold. We
used one fold as the development set and performed
cross-validation on the other nine.
To measure how well our model generalizes
to other highly-inflected, relatively free-word-order
languages, we considered Ancient Greek, Hungar-
ian, and Czech. Their respective datasets consist of
8000 sentences from the Ancient Greek Dependency
Treebank (Bamman et al, 2009), 5800 from the
Hungarian Szeged Dependency Treebank (Vincze et
al., 2010), and a subset of 3100 from the Prague De-
pendency Treebank (Bo?hmova? et al, 2003).
5.2 Training
We define each factor in (1) as a log-linear function:
Fk(A) = exp
?
h
?hfh(A,W, k) (2)
Given an assignment A and words W , fh is an
indicator function describing the presence or ab-
sence of the feature, and ?h is the corresponding set
of weights learned using stochastic gradient ascent,
with the gradients inferred by loopy belief propaga-
tion (Smith and Eisner, 2008). The variance of the
Gaussian prior is set to 1. The other two parameters
in the training process, the number of belief propa-
gation iterations and the number of training rounds,
were tuned on the development set.
5.3 Decoding
The output of the joint model is the assignment to
the TAG and LINK variables. Loopy belief propaga-
tion (BP) was used to calculate the posterior proba-
bilities of these variables. For TAG, we emit the tag
with the highest posterior probability as computed
by sum-product BP. We produced head attachments
by first calculating the posteriors of the LINK vari-
ables with BP and then passing them to an edge-
factored tree decoder. This is equivalent to mini-
mum Bayes risk decoding (Goodman, 1996), which
is used by Cohen and Smith (2007) and Smith and
Eisner (2008). This MBR decoding procedure en-
forces the hard constraint that the output be a tree
but sums over possible morphological assignments.5
5.4 Reducing Model Complexity
In principle, the joint model should consider every
possible combination of morphological attributes for
every word. In practice, to reduce the complexity
of the model, we used a pre-existing morphological
database, MORPHEUS (Crane, 1991), to constrain
the range of possible values of the attributes listed
in Table 2; more precisely, we add a hard constraint,
requiring that assignments to the TAG variables be
compatible with MORPHEUS. This constraint signif-
icantly reduces the value of m in the big-O notation
5This approach to nuisance variables has also been used
effectively for parsing with tree-substitution grammars, where
several derived trees may correspond to each derivation tree,
and parsing with PCFGs with latent annotations.
890
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 94.4 94.5 94.4 94.5
Person 99.4 99.5 97.1 97.6
Number 95.3 95.9 93.7 94.5
Tense 98.0 98.2 93.2 93.9
Mood 98.1 98.3 93.8 94.4
Voice 98.5 98.6 95.3 95.7
Gender 93.1 93.9 87.7 89.1
Case 89.3 90.0 79.9 81.2
Degree 99.9 99.9 86.4 90.8
UAS 61.0 61.9 ? ?
Table 3: Latin morphological disambiguation and pars-
ing. For some attributes, such as degree, a substan-
tial portion of words have the null value. The non-null
columns provides a sharper picture by excluding these
?easy? cases. Note that POS is never null.
for the number of variables and factors described in
?3. To illustrate the effect, the graphical model of
the sentence in Table 1, whose six words are all cov-
ered by the database, has 1,866 factors; without the
benefit of the database, the full model would have
31,901 factors.
The MORPHEUS database was automatically gen-
erated from a list of stems, inflections, irregular
forms and morphological rules. It covers about 99%
of the distinct words in the Latin Dependency Tree-
bank. At decoding time, for each fold, the database
is further augmented with tags seen in training data.
After this augmentation, an average of 44 words are
?unseen? in each fold.
Similarly, we constructed morphological dictio-
naries for Czech, Ancient Greek, and Hungarian
from words that occurred at least five times in the
training data; words that occurred fewer times were
unrestricted in the morphological attributes they
could take on.
6 Experimental Results
We compare the performance of the pipeline model
(?4) and the joint model (?3) on morphological dis-
ambiguation and unlabeled dependency parsing.
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 95.5 95.7 95.5 95.7
Person 98.4 98.8 93.5 95.6
Number 91.2 92.3 87.0 88.4
Tense 98.4 98.8 92.7 96.1
Voice 98.5 98.7 93.2 95.8
Gender 86.6 87.9 75.6 78.0
Case 84.1 85.6 74.3 76.5
Degree 97.9 98.0 90.1 90.1
UAS 67.4 68.7 ? ?
Table 4: Czech morphological disambiguation and pars-
ing. As with Latin, the model is least accurate with
noun/adjective categories of gender number, and case,
particularly when considering only words whose true
value is non-null for those attributes. Joint inference with
syntactic features improves accuracy across the board.
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 94.9 95.7 94.9 95.7
Person 98.7 99.0 92.2 94.6
Number 97.4 97.9 96.5 97.1
Tense 96.8 97.2 84.1 86.8
Mood 97.9 98.3 91.4 93.2
Voice 97.8 98.0 91.3 92.4
Gender 95.4 96.1 90.7 91.9
Case 95.9 96.3 92.0 92.6
Degree 99.8 99.9 33.3 55.6
UAS 68.0 70.5 ? ?
Table 5: Ancient Greek morphological disambiguation
and parsing. Noun/adjective morphology is more accu-
rate, but verbal morphology is more problematic.
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 95.8 95.8 95.8 95.8
Person 98.5 98.6 94.9 94.1
Number 97.4 97.5 96.8 96.6
Tense 98.9 99.3 97.2 97.3
Mood 98.7 99.2 95.8 97.3
Case 96.7 97.0 94.5 94.9
Degree 97.9 98.1 87.5 88.6
UAS 78.2 78.8 ? ?
Table 6: Hungarian morphological disambiguation and
parsing. The agglutinative morphological system makes
local cues more effective, but syntactic information helps
in almost all categories.
891
6.1 Morphological Disambiguation
As seen in Table 3, the joint model outperforms6
the baseline tagger in all attributes in Latin morpho-
logical disambiguation. Among words not covered
by the morphological database, accuracy in POS is
slightly better, but lower for case, gender and num-
ber.
The joint model made the most gains on adjec-
tives and participles. Both parts-of-speech are par-
ticularly ambiguous: according to MORPHEUS, 43%
of the adjectives can be interpreted as another POS,
most frequently nouns; while participles have an av-
erage of 5.5 morphological interpretations. Both
also often have identical forms for different genders,
numbers and cases. In these situations, syntactic
considerations help nudge the joint model to the cor-
rect interpretations.
Experiments on the other three languages bear out
similar results: the joint model improves morpho-
logical disambiguation. The performance of Czech
(Table 4) exhibits the closest analogue to Latin: gen-
der, number, and case are much less accurately pre-
dicted than are the other morphological attributes.
Like Latin, Czech lacks definite and indefinite arti-
cles to provide high-confidence cues for noun phrase
boundaries.
The Ancient Greek treebank comprises both ar-
chaic texts, before the development of a definite ar-
ticle, and later classic Greek, which has a definite
article; Hungarian has both a definite and an indefi-
nite article. In both languages (Tables 5 and 6), noun
and adjective gender, number, and case are more
accurately predicted than in Czech and Latin. The
verbal system of ancient Greek, in contrast, is more
complex than that of the other languages, so mood,
voice, and tense accuracy are lower.
6.2 Dependency Parsing
In addition to morphological disambiguation, we
also measured the performance of the joint model
on dependency parsing of Latin and the other lan-
guages. The baseline pipeline parser (?4.2) yielded
61.00% head selection accuracy (i.e., unlabeled at-
tachment score, UAS), outperformed7 by the joint
6The differences are statistically significant in all (p < 0.01
by McNemar?s Test) but POS (p = 0.5).
7Significant at p < e?11 by McNemar?s Test.
model at 61.88%. The joint model showed simi-
lar improvements in Ancient Greek, Hungarian, and
Czech.
Wrong decisions made by the baseline tagger of-
ten misled the pipeline parser. For adjectives, the ex-
ample shown in Table 1 and Figure 1 is a typical sce-
nario, where an accusative adjective was tagged as
nominative, and was then misanalyzed by the parser
as modifying a verb (as a subject) rather than mod-
ifying an accusative noun. For participles modify-
ing a noun, the wrong noun was often chosen based
on inaccurate morphological information. In these
cases, the joint model, entertaining all morpholog-
ical possibilities, was able to find the combination
of links and morphological analyses that are collec-
tively more likely.
The accuracy figures of our baselines are compa-
rable, but not identical, to their counterparts reported
in (Bamman and Crane, 2008). The differences may
partially be attributed to the different morphologi-
cal tagger used, and the different learning algorithm,
namely Margin Infused Relaxed Algorithm (MIRA)
in (McDonald et al, 2005) rather than maximum
likelihood. More importantly, the Latin Dependency
Treebank has grown from about 30K at the time of
the previous work to 53K at present, resulting in sig-
nificantly different training and testing material.
Gold Pipeline Parser When given perfect mor-
phological information, the Latin parser performs at
65.28% accuracy in head selection. Despite the or-
acle morphology, the head selection accuracy is still
below other languages. This is hardly surprising,
given the relatively small training set, and that the
?the most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection?, as observed at the recent dependency pars-
ing shared task (Nivre et al, 2007); both of these are
characteristics of Latin.
A particularly troublesome structure is coordina-
tion; the most frequent link errors all involve either a
parent or a child as a conjunction. In a list of words,
all words and coordinators depend on the final coor-
dinator. Since the factors in our model consult only
one link at a time, they do not sufficiently capture
this kind of structures. Higher-order features, partic-
ularly those concerned with links with grandparents
and siblings, have been shown to benefit dependency
892
parsing (Smith and Eisner, 2008) and may be able to
address this issue.
7 Conclusions and Future Work
We have proposed a discriminative model that
jointly infers morphological properties and syntactic
structures. In evaluations on various highly-inflected
languages, this joint model outperforms both a base-
line tagger in morphological disambiguation, and a
pipeline parser in head selection.
This model may be refined by incorporating richer
features and improved decoding. In particular, we
would like to experiment with higher-order features
(?6), and with maximum a posteriori decoding, via
max-product BP or (relaxed) integer linear program-
ming. Further evaluation on other morphological
systems would also be desirable.
Acknowledgments
We thank David Bamman and Gregory Crane for
their feedback and support. Part of this research
was performed by the first author while visiting
Perseus Digital Library at Tufts University, un-
der the grants A Reading Environment for Ara-
bic and Islamic Culture, Department of Education
(P017A060068-08) and The Dynamic Lexicon: Cy-
berinfrastructure and the Automatic Analysis of His-
torical Languages, National Endowment for the Hu-
manities (PR-50013-08). The latter two authors
were supported by Army prime contract #W911NF-
07-1-0216 and University of Pennsylvania subaward
#103-548106; by SRI International subcontract #27-
001338 and ARFL prime contract #FA8750-09-C-
0181; and by the Center for Intelligent Information
Retrieval. Any opinions, findings, and conclusions
or recommendations expressed in this material are
the authors? and do not necessarily reflect those of
the sponsors.
References
David Bamman and Gregory Crane. 2006. The Design
and Use of a Latin Dependency Treebank. Proc. Work-
shop on Treebanks and Linguistic Theories (TLT).
Prague, Czech Republic.
David Bamman and Gregory Crane. 2008. Building a
Dynamic Lexicon from a Digital Library. Proc. 8th
ACM/IEEE-CS Joint Conference on Digital Libraries
(JCDL 2008). Pittsburgh, PA.
David Bamman, Francesco Mambrini, and Gregory
Crane. 2009. An Ownership Model of Anno-
tation: The Ancient Greek Dependency Treebank.
Proc. Workshop on Treebanks and Linguistic Theories
(TLT).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The PDT: a 3-level Annotation Scenario. In
Treebanks: Building and Using Parsed Corpora, A.
Abeille? (ed). Kluwer.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
Proc. CoNLL. New York, NY.
Shay B. Cohen and Noah A. Smith. 2007. Joint Morpho-
logical and Syntactic Disambiguation. Proc. EMNLP-
CoNLL. Prague, Czech Republic.
Gregory Crane. 1991. Generating and Parsing Classical
Greek. Literary and Linguistic Computing 6(4):243?
245.
Yoav Goldberg and Reut Tsarfaty. 2008. A Single Gen-
erative Model for Joint Morphological Segmentation
and Syntactic Parsing. Proc. ACL. Columbus, OH.
Joshua Goodman. 1996. Parsing Algorithms and Met-
rics. Proc. ACL.
J. Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petkevic?.
2001. Serial Combination of Rules and Statistics: A
Case Study in Czech Tagging. Proc. ACL.
D. Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2000. Statis-
tical Morphological Disambiguation for Agglutinative
Languages. Proc. COLING.
Vincent Katz. 2004. The Complete Elegies of Sextus
Propertius. Princeton University Press, Princeton, NJ.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jana Hajic?. 2005. Non-projective Dependency
Parsing using Spanning Tree Algorithms. Proc.
HLT/EMNLP.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. Proc. ACL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. Proc. CoNLL Shared Task Session
of EMNLP-CoNLL. Prague, Czech Republic.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging using Decision Trees. Proc. International
Conference on New Methods in Language Processing.
Manchester, UK.
Noah A. Smith, David A. Smith and Roy W. Tromble.
2005. Context-Based Morphological Disambiguation
with Random Fields. Proc. HLT/EMNLP. Vancouver,
Canada.
893
David Smith and Jason Eisner. 2008. Dependency Pars-
ing by Belief Propagation. Proc. EMNLP. Honolulu,
Hawaii.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
Proc. HLT-NAACL. Edmonton, Canada.
Reut Tsarfaty. 2006. Integrated Morphological and
Syntactic Disambiguation for Modern Hebrew. Proc.
COLING-ACL Student Research Workshop.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian Dependency Treebank. Proc. LREC.
894
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 207?216,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Minimally Supervised Approach for Detecting and Ranking Document 
Translation Pairs 
 
 
Kriste Krstovski David A. Smith 
Department of Computer Science  Department of Computer Science 
University of Massachusetts Amherst University of Massachusetts Amherst 
Amherst, MA 01003, USA Amherst, MA 01003, USA 
kriste@cs.umass.edu dasmith@cs.umass.edu 
 
 
 
 
 
 
Abstract 
We describe an approach for generating a 
ranked list of candidate document transla-
tion pairs without the use of bilingual dic-
tionary or machine translation system. We 
developed this approach as an initial, filter-
ing step, for extracting parallel text from 
large, multilingual?but non-parallel?
corpora. We represent bilingual documents 
in a vector space whose basis vectors are 
the overlapping tokens found in both lan-
guages of the collection. Using this repre-
sentation, weighted by tf?idf, we compute 
cosine document similarity to create a 
ranked list of candidate document transla-
tion pairs. Unlike cross-language informa-
tion retrieval, where a ranked list in the 
target language is evaluated for each source 
query, we are interested in, and evaluate, 
the more difficult task of finding translated 
document pairs. We first perform a feasi-
bility study of our approach on parallel col-
lections in multiple languages, representing 
multiple language families and scripts. The 
approach is then applied to a large bilingual 
collection of around 800k books. To avoid 
the computational cost of )( 2nO document 
pair comparisons, we employ locality sen-
sitive hashing (LSH) approximation algo-
rithm for cosine similarity, which reduces 
our time complexity to )log( nnO . 
1 Introduction 
A dearth of parallel data has been, and still is, a 
major problem for developing highly reliable sta-
tistical machine translation systems in many lan-
guages and domains. There have been many 
proposed approaches for alleviating this problem 
by utilizing techniques for creating and extracting 
parallel documents, sentences or phrases from 
comparable bilingual data available on the open 
web (Resnik and Smith, 2003), such as Wikipedia 
articles (Smith et. al, 2010), to name a few, or 
through digitized archives from various sources 
(Zhao and Vogel, 2002), (Munteanu and Marcu, 
2005). 
In general, in the process of utilizing comparable 
corpora to obtain sentence-aligned bilingual text, 
the first step involves performing initial filtering 
where text entities from both language collections 
are compared to each other and based on compari-
son score they are matched and grouped as poten-
tial translation candidate pairs. After this initial 
step, text entity pairs or tuples are further analyzed 
in order to extract parallel sentence pairs. In this 
paper we only focus on this initial step. We present 
a novel exploration of approaches that retrieve ac-
tual document translation pairs without the use of 
any bilingual resources such as lexicons or sen-
tence aligned bitext. 
Rather than solving separate retrieval or translation 
problems for each source language document, we 
retrieve translation pairs from the space of all pos-
sible bilingual document pairs. Most machine 
207
translation (MT) and information retrieval (IR) 
systems rely on conditional probabilities; in con-
trast, we require comparable scores or probabilities 
over all document pairs. To avoid directly comput-
ing the similarity of all pairs, we use a randomized 
approximation algorithm based on locality sensi-
tive hashing (LSH).  
For this joint approach, we represent each docu-
ment in both languages using an n-dimensional 
feature vector template which consists of the set of 
intersecting words which are found across all 
documents in both language collections. For each 
dimension i.e. word, in the feature vector template 
we calculate tf?idf score for the given document. 
Unlike other approaches, where documents or their 
word representations are first translated from for-
eign language to English using bilingual dictionary 
(Fung and Cheung, 2004), (Munteanu and Marcu, 
2005) and (Uszkoreit et. al., 2010) in our approach 
we don?t utilize any existing MT type artifact. In 
other words, for a given language pair we don?t use 
translation lexicon by training an existing statisti-
cal machine translation system using sentence 
aligned parallel bilingual data in the same language 
or existing translation lexicon. Earlier work done 
by Enright and Kondrak (2007) uses only hapax 
words to represent and rank (based on the overlap 
number) translation documents pair in a parallel 
bilingual collection which is an easier task to 
evaluation due to the presence of a one-to-one 
matching among the bilingual documents. Most 
recently, Patry and Langlais (2011) show an im-
provement over this method by using an IR system 
to first retrieve translation document candidates 
and then identify translation document pairs by 
training a classifier.  
We start off by giving detailed explanation of the 
above mentioned data representation. We then test 
the feasibility of our approach using aligned paral-
lel document data from three different bilingual 
collections in several languages and writing sys-
tems. Results from these tests are given in section 
3. The goal of developing our approach was to util-
ize it as an initial filtering step in developing paral-
lel corpora from large, multilingual collections, 
such as the collection of more than 800K English 
and German books we describe in section 4. Since 
we start with no information on the possible trans-
lation pairs in our large collection and in order to 
verify the potential of our method, we first show 
results on retrieving 17 known parallel book pairs 
embedded in a small randomly selected subset of 
1K books (section 4.1). Since performing cosine 
similarity across all document pairs is computa-
tionally expensive with time complexity of 
)( 2nO we utilize the LSH based approximation 
algorithm for the cosine similarity measurement 
based on the work by Ravichandran et. al (2005). 
A brief overview of this approach is given in Sec-
tion 5, which is followed by our implementation 
results explained and analyzed in section 6. To 
conclude the paper, we give a brief outlook on fu-
ture work. 
2 Document Representation 
In Figure 1, we depict the process that we use to 
represent documents from bilingual collections in 
vector space and perform similarity measurements. 
We start by computing a word frequency count for 
each of the documents in our collection and creat-
ing a word frequency list. For each language, we 
take a union of the words in each document?s fre-
quency list to construct a global word list for the 
given language. The two global word lists are then 
intersected, and a list of overlapping words is cre-
ated. From the initial list of overlapping words in 
both languages, we remove stop words by using 
stop word lists (words with high document fre-
quency). The space-separated tokens extracted in 
this process are not necessarily words in the lin-
guistic sense; therefore, we further refine the over-
lapping word list by removing tokens that contain 
non-alphanumeric characters. We make one excep-
tion for tokens (such as might appear in a time/date 
format) that contain hyphens, backslashes, apos-
trophes, and periods so long as these characters do 
not occur at the beginning or at the end of the to-
ken.  
We call this list of overlapping tokens a feature 
vector template, where each token in the list is one 
feature. Using this feature vector template we go 
back and represent each document in the bilingual 
collection using the template vector by computing 
the tf?idf value for each token in the template vec-
tor over each particular document. Now that we 
have the original documents from both languages 
represented in a language-independent space, we 
compute vector similarity across all document 
pairs in order to come up with a single ranked list. 
We talk more in detail about the similarity metrics 
208
that we have considered and decided to use in the 
following section.  
 
  
Figure 1. Process of creating and representing each 
document of a bilingual collection in an independ-
ent vector space.  
3 Motivational Experiments 
3.1 Evaluation Collections 
We start off by evaluating the above proposed ap-
proach of determining candidate document transla-
tion pairs using three different parallel collections: 
Europarl, created by Koehn (2005), UN Arabic 
English Parallel Text (LDC2004E13) and the Ara-
bic News Translation Part 1 (LDC2004T17). The 
purposes of first testing our approach using the 
Europarl corpus were twofold: This collection con-
tains parallel documents (sessions of the European 
Parliament) that are further aligned at the speech 
and sentence level, which allows us to test align-
ment accuracy at several levels of granularity. Sec-
ond, this collection contains parallel data from 
different groups of languages (Germanic, Ro-
mance, Slavic, Hellenic, etc.) and therefore is use-
ful to observe the performance of our approach 
across different language families, which in turn 
are important to observe the difference in the cog-
nate rates and the size of the overlapping words. In 
addition to the Europarl corpus we use the two 
English-Arabic parallel collections to test our ap-
proach across various alphabets (Arabic in addition 
to the Latin, Greek and Cyrillic found in the Eu-
roparl collection). Shown in Table 1 are basic sta-
tistics for all 3 corpora on the language pairs 
considered. We give min, max and median values 
over the number of words in each document. 
 
Collection # doc. Pairs Lang. Min Max  Median
En 92 109030 46800.5Europarl 
en-de 654 De 95 99753 43161.0
En 4872 59284 10706.5Europarl 
en-bg 430 Bg 4771 56907 10167.0
En 92 109793 46790.5Europarl 
en-es 642 Es 104 114770 48989.0
En 92 93886 21290.0Europarl 
en-gr 412 Gr 103 93304 21122.0
En 66 47784 691.5Newswire 
en-ar 230 Ar 62 34272 560.0
En 17672 71594 23027.0UN en-ar 430 Ar 15478 62448 19682.0
 
Table 1. Document length statistics over 6 Parallel 
Collections. 
 
From the Europarl collection we sentence aligned 
sessions in the following four language pairs where 
the English language is the source language: Eng-
lish-German, English-Spanish, English-Bulgarian 
and English-Greek. The foreign language in all 
four language pairs is selected from a different 
language group (Germanic, Romanic, Slavic), with 
Greek being a more isolated branch. For the Arabic 
language we used two parallel document collec-
tions in different domains ? newswire and docu-
ments published by the United Nations. The 
Newswire parallel collection consisted of 1526 
news stories which we combined based on the 
news story publication date and obtained 230 par-
allel documents. The purpose of combining the 
news articles is to increase the number of words 
present in each document since the original size of 
209
the news articles was not at a level to be treated as 
a document as in the case of the remaining two 
collections. The UN parallel collection consists of 
34,575 document pairs.  
3.2 Similarity Metrics 
We considered five similarity metrics proposed at 
one time or another for vector space models in IR: 
Cosine (shown below), Dice, Product, Jaccard and 
Euclidean. 
 
  ? ?
?
22
ii
ii
yx
yx                        (1)  
 
Document similarity using the cosine metric relies 
on the angle between the vector representations 
and it is length invariant. The Dice metric relies on 
the number of common tokens between the two 
documents. Euclidean computes the similarity as a 
point distance between the two vector representa-
tions and is not normalized by the vector length 
which does not make it vector invariant. Jaccard 
distance is the ratio of the intersection and the un-
ion of the two vector representations while the 
product coefficient is simply the inner product of 
the two vectors. While there is no clear evidence 
across the literature whether one similarity metric 
is more useful across a range of tasks compared to 
another, the cosine similarity metric is mostly pre-
ferred. Shown in Figure 2 are the precision vs. re-
call plots of the above similarity measurements 
when used with our method. Tests were done on 
our set of 654 English-German sessions from the 
Europarl collections. To test the impact of the 
document length on the performance of the metric 
we performed two types of tests across all 5 met-
rics. In the first type we performed similarity 
analysis on the full document length (marked as 
100%) and on the final 10% of each document 
(marked as 10%). We deliberately omitted the top 
part of the document to avoid any inadvertent in-
clusion of session date, topic, title, etc. (As it 
turned out, this was not a problem in our data.) We 
perform similarity measurements across all docu-
ment pairs, and we generate a single ranked list. As 
can be seen from the plot, all five metrics yield 
better performance when all words in documents 
are considered compared to only considering 10%. 
The performance ranking of all five metrics was 
identical on both versions of the document set. 
Even though depicted in the above plot, the Jac-
card distance performed pretty much the same as 
the Dice distance and therefore there is no visible 
difference between the two. While on the 10% ver-
sion of the collection, the Euclidean distance has 
the worst precision, it could still be explored as a 
metric to obtain document translation pairs with 
the original collection with a modest to moderate 
recall range for P=1. The Jaccard distance along 
with the Dice distance yield the highest precision 
values across all recall values but they achieve the 
same recall range for P=1 as the Cosine metric. 
Since we are only interested in top-N document 
pairs that have P=1 and furthermore there are ap-
proximate algorithms for the Cosine similarity 
metrics we decided to further utilize this metric. 
The same metric has been previously used in de-
termining potential translation candidates on sen-
tence level by Munteanu and Marcu (2005) and in 
our case we are extending it to perform pair-wise 
document similarity.  
 
  
Figure 2. Precision vs. recall plot using various 
similarity measurements on the Europarl English-
German collection. 
 
When run on the same English-German collection, 
Enright?s and Kondrak?s (2007) approach achieves 
mean reciprocal rank (MRR) of 0.989 when using 
document specific hapax words and MRR=0.795 
when using collection specific hapax words. With 
the above explained approach we obtain 
MRR=0.995. 
210
3.3 Post Filtering Approaches 
To further improve the precision of our approach 
we tested out two types of filtering the initial re-
sults. Since we threat documents as ?bag of words? 
and since the Cosine metric uses the angle between 
the vector representations and is length invariant 
there may be instances of source documents that 
would yield high cosine coefficients over all target 
documents. In these instances, multiple document 
pairs with the same source document may be 
ranked high. To alleviate this problem, we consider 
two types of filtering the initial results. We go over 
the single ranked list and we only keep the top five 
document pairs for a given source document, thus 
introducing ?diversity? in the ranked list. The sec-
ond filter is motivated by the basic assumption 
used in the machine translation field that the length 
of the target sentence is in a given length range of 
the source sentence. We extend this assumption on 
a document level and we filter out all document 
pairs from the ranked list that are not in the ?20% 
range of the source document length. Both of the 
above values were selected based on empirical 
evidence without detailed explanation. Shown in 
Figure 3 are the effects of these two simple filter-
ing techniques.  
 
  
Figure 3. Diversity and length based filtering ef-
fects on the English-German Europarl collection. 
 
Compared to the diversity filter, the length based 
filter yields better gain in precision while a combi-
nation of both methods achieves the highest recall 
range for P=1. 
3.4 Target Languages and Writing Systems 
Shown in Figure 4 are the precision/recall results 
on all six collections explained in Section 3.1. 
Post-filtering steps explained in the previous sec-
tion were not utilized on these results. Our ap-
proach yields best precision on the Arabic News 
Translation Part 1 collection while the worst per-
formance is on the UN Arabic English Parallel 
Text. While the performance on the English-
German and English-Spanish collections is some-
what the same, out of all 4 Europarl collections we 
achieve best results on the Greek collection and 
worst results on the Bulgarian target language.  
 
  
Figure 4. Precision vs. recall on 5 different lan-
guage pairs using cosine similarity distance metric.  
 
In Table 2, we give the vector template length for 
each collection. 
 
Collection # of overlapping tokens 
Europarl en-de 37785
Europarl en-es 36476
Europarl en-bg 29360
Europarl en-gr 17220
UN en-ar 3945
Newswire en-ar 1262
 
Table 2. Number of overlapping words (vector 
template length) in the six parallel collections. 
 
Unsurprisingly, due to the difference in script and 
language family, the feature vector templates for 
the English-Arabic collections have the smallest 
lengths. 
211
Shown in Figure 5 are effects of the trivial diver-
sity and length based filtering on the above preci-
sion vs. recall results. Bulgarian has improve 
substantially and so has the UN Arabic, but recall 
on the Arabic newswire is truncated on reaching 
P=0.4. 
 
  
Figure 5. Precision vs. recall on 6 collections using 
div=5 and length filtering with ?20%. 
3.5 Randomly Selected Documents 
While useful to evaluate the feasibility of our ap-
proach, the previous parallel bilingual collections 
are unrealistic because there is, by the corpus? de-
sign, a translation for each document. To observe 
the performance on a bilingual document collec-
tion where there is no a priori information on trans-
lation pairs we created ten random subsets from the 
Europarl English-German collection. These subsets 
were created by randomly selecting 50% (328 
documents) of the English and 5% (33 documents) 
of the German documents for each subset collec-
tion. Shown in  is interpolated average precision 
over the ten subsets. The Mean Average Precision 
(MAP) obtained was 0.986. 
4 Multilingual Book Collection 
Our multilingual book collection consists of 
around 800k books in German and English lan-
guages. It is a subset of a larger Internet Archive1 
collection of books in over 200 languages. The 
whole collection consists of OCRed books incor-
porating a small number of human transcribed 
                                                          
1 http://www.archive.org/details/texts/ 
books from Project Gutenberg2. The collection was 
initially annotated with author and language infor-
mation using the existing database obtained from 
the Internet Archive. This database originally con-
tained incorrect language metadata. Using the 
freely available language identifier TextCat (Cav-
nar and Trenkle, 2005) we tagged the whole book 
collection and extracted 705692 English and 96752 
German books. This process had the additional 
benefit of cleaning the German book collection of 
books written in the Fraktur script due to the bad 
OCR output. (Incredibly noisy OCR was simply 
recognized as ?not German? by the character n-
gram models.) Shown in Table 3 are word length 
statistics over the books in the collection.  
 
Language # of books
# of uniq. 
words Min Max 
Me-
dian 
German 96752 5030095 33 2372278 109820
English 705692 20001702 37 5155032 75016
 
Table 3. Bilingual book collection statistics. 
 
  
Figure 6. Average precision interpolated at 11 
points over ten randomly created subsets consisting 
of 50% English and 10% German documents from 
the English-German Europarl collection. 
4.1 Development Set 
Moving onto our book collection, we start off by 
evaluating the method on a smaller randomly se-
lected subset of 1000 books in both languages. 
Since it is not feasible to perform a full recall 
                                                          
2 http://www.gutenberg.org 
212
evaluation on the whole book set we include 17 
known book translation pairs in the 1000 random 
bilingual book collection. The 17 book translation 
pairs were constructed by hand by running a previ-
sion version of our full algorithm and indentifying 
translation pairs. Shown in Figure 7 is the preci-
sion vs. recall plot on the 17 book pairs. As in the 
case of the 10 randomly selected Europarl subsets, 
we also performed diversity and length based fil-
tering of the initial results prior to computing pre-
cision vs. recall. 
 
  
Figure 7. Precision vs. recall running our method 
on a 1000 randomly selected bilingual book subset 
with 17 book translation pairs inserted. 
5 LSH Based Approximate Algorithm for 
Cosine Similarity 
Due to the collection size and length of each book 
it is infeasible to perform cosine similarity over all 
possible book pairs, i.e. approximately 68.2B com-
parisons. This brute force approach has time com-
plexity of )( 2knO  where n is the number of books 
in the collection and k is the vector template 
length. We therefore employ a fast cosine similar-
ity calculation approach developed by Charikar 
(2002) and utilized by Ravichandran et. al (2005) 
for creating similarity lists of nouns in  large col-
lection. In this section we give a summary of this 
approach and explain how it was applied for our 
task.  
Locality Sensitive Hashing (LSH), initially intro-
duced by Idyik and Motwani (1998), is used for 
finding approximate nearest neighbors in high di-
mensional spaces. In general, their approach 
hashes query vectors into bins where the probabil-
ity of collision is higher due to the fact that vectors 
in the same bin share the same locality. Their ap-
proach reduces the approximate nearest neighbor 
problem on the Hamming space.  
Charikar expanded this approach and showed that 
the probability of collision of hashed vectors for 
appropriately chosen hash function h is related to 
the angle between the vectors as: 
 
 ?
? ),(1)]()(Pr[ yxyhxh ???  (2) 
 
This is closely related to the cosine function. From 
the above equation we thus have: 
 
})])()(Pr[1cos{()),(cos( ?? yhxhyx ???   (3) 
 
Charikar uses a hash function based on random 
hyperplanes and creates a fingerprint for each 
original vector using the following approach: 
Generate d, k-dimensional random vectors from a 
standard normal (Gaussian) distribution: 
{ 1r , 2r ,?.. }dr . For each original vector x use the 
following hash function to generate a fingerprint of 
d bits: 
 
??
??
?
?
?? ?
?
01
00)(
ii
ii
r rxif
rxifxh   (4) 
 
By doing this we represent each vector in our 
original vector set into a bit stream that reduces our 
vector space representation from k to d dimensions, 
where d << k. Having bit stream as our data repre-
sentation, the probability of hash collision, i.e. the 
probability of two vectors being equal 
)]()(Pr[ yhxh ? , is equivalent to the Hamming 
distance between the two bit streams: 
 
         Pr[h(x) ? h(y)] ? HDd   (5) 
  
Therefore, performing fast cosine similarity boils 
down to finding the Hamming distance between 
the two bit streams.  
Now that we have an approximate method of find-
ing the cosine similarity between two vectors, we 
use Ravichandran?s (2005) formulation of the fast 
213
search algorithm developed by Charikar, which in 
turn used Indyk and Motwani?s orginal PLEB 
(Point Location in Equal Balls) algorithm as a 
starting point. The steps of this algorithm are out-
lined in the next subsection. For more detailed ex-
planation of this algorithm the reader is referred to 
Section 5 of Charikar?s work (2002).  
5.1 Nearest Neighbor Search Algorithm 
We now outline the steps of the fast search algo-
rithm. For more detailed explanation of the algo-
rithmic implementation users are referred to 
Section 3 of Ravichandran?s work (2005): 
 
? For all m documents represented in the vector 
space using the template vector, compute LSH 
d-bit signature using the formula given in (4).  
? Generate q permutations of length d.  
? For each of the q permutations, generate m 
permuted LSH signatures. 
? For each of the q permutation bins, 
lexicographically sort the m permutated bit 
vectors.  
? For each lexicographically sorted bin, go over 
the m bit streams and compute the Hamming 
distance between the current bit stream and the 
subsequent b bit streams in the sorted list start-
ing from the top. 
? If the Hamming distance is above a previously 
set threshold, output the book pair along with 
the Hamming distance result. 
 
Compared to Ravichandran?s algorithm for creat-
ing noun similarity lists, in our approach we deal 
with two distinct groups of documents: those in 
each language. We start off by creating a single list 
of documents and we represent each document in 
this list using the LSH based fingerprint. We then 
generate q permutation vector bins, and we 
lexicographically sort each bin. In our beam search 
approach, since we have documents in two differ-
ent languages, we only consider documents that 
have a different language. The results of the beam 
search for each bin are then combined. Since in 
each beam the same permutation is performed over 
all fingerprints, the Hamming distance across all 
bins for a given document pair would be the same. 
Therefore after combining the results we remove 
duplicate document pairs and sort by the Hamming 
distance to obtain the final ranked list.  The run-
time of this algorithm is dominated by the 
O(qn logn)  step of sorting the permuted bit vec-
tors in each of the bins. 
6 Detecting and Ranking Book Transla-
tion Pairs in a Large Book Collection 
Using the previously explained method we proc-
essed the large book collection by first computing 
the vector template. For the large book collection, 
the vector template size k, i.e. the number of over-
lapping tokens obtained, was 638,005. After re-
moving stop words and unwanted tokens 
(explained in Section 2) the template vector length 
was reduced to 563,053. Shown in Table 4 are sta-
tistics over the number of vector template tokens 
whose tf?idf values are greater than zero across the 
two languages.  
 
Language Min Max Median
German 7 7212 229
English 11 6637 585
 
Table 4. Statistics over the number of tokens in the 
vector representation of each book whose tf?idf are 
greater than zero. 
 
Once processed and represented in vector space, 
we proceed with computing the approximate co-
sine similarity across the bilingual collection. We 
precompute the Hamming distance based on a co-
sine similarity threshold of 0.18 which is equiva-
lent to different Hamming distance values 
depending on the length of the LSH based finger-
print. For the book collection we experimented 
with 4 different sets of values for the number of 
hyperplane based hash functions, the number of 
permutations and the length of the beam search. 
For each of these parameters in our setup we cre-
ated ranked lists as explained in Section 5.1. We 
then went over the top 300 book pairs in each list 
and annotated the correct book translations. Based 
on the human annotation we then computed aver-
age precision over the ranked list. Shown in Table 
5 are the results for LSH based fingerprint of size 
d=500. Due to the randomness introduced by the 
permutations, there is not a monotonic increase in 
accuracy, but in general more permutations and 
wider beams show substantial improvements. 
 
214
q\b AP Time [hrs] 
b=25 0.307 24.9
b=50 0.213 41.1q=25 
b=100 0.280 67.2
b=25 0.488 99.6
b=50 0.388 164.4q=100 
b=100 0.461 269.1
b=25 0.357 199.2
b=50 0.412 328.8q=200 
b=100 0.455 538.2
b=25 0.489 498.1
b=50 0.490 822.0q=500 
b=100 0.493 1345.5
 
Table 5. Average precision on the large English-
German book collection across various parameters 
of the LSH based search algorithm. 
 
For the above given results for d=500, we calcu-
lated an estimated time that it would take to per-
form the fast cosine similarity if the algorithm 
were to be run in serial fashion. Shown in Figure 8 
is a scatter plot of the time vs. the average preci-
sion obtained. 
 
  
Figure 8. Estimated serial time vs. average preci-
sion with d=500 dimensional LSH based finger-
prints. 
 
In summary, while increasing the number of per-
mutations and the beam search over different val-
ues increases the average precision the time cost 
required is significantly larger especially for in-
creasing the number of permutations. 
7 Future Work 
In the future we plan on experimenting with larger 
dimensionality d for the LSH fingerprint, the num-
ber of random permutations q i.e. bins and the 
beam search parameter b. In order to further im-
prove the average precision we would also like to 
experiment with different longest common subse-
quence (LCS) based approaches for re-ranking the 
cosine based ranked lists. Furthermore, we plan on 
exploring more accurate joint models of transla-
tion. It would also be interesting to observe the 
performance of our system on other language pairs, 
such as English-Chinese and languages with 
resource-poor bilingual collections.  
8 Conclusion 
This paper presents and evaluates a new approach 
to detecting and ranking document translation 
pairs. We showed that this simple method achieves 
high precision vs. recall on parallel bilingual col-
lections where there is one document translation 
for each source document. We also showed that the 
method is capable of detecting document transla-
tions in random subsets where no known document 
translation information is available. Using an ap-
proximation algorithm for cosine similarity, we 
showed that this method is useful for detecting and 
ranking document translation pairs in a large 
bilingual collection with hundreds of thousands of 
books and billions of possible book pairs. This 
method is conceivable to be used for other lan-
guages and collection genres and also on other 
types of translation methods such as transliteration. 
While in some instances other simple methods of 
aligning the dictionaries might be needed, as in the 
case of the Chinese language. 
Acknowledgments 
This work was supported in part by the Center for 
Intelligent Information Retrieval and in part by 
NSF grant #IIS-0910884. Any opinions, findings 
and conclusions or recommendations expressed in 
this material are the authors' and do not necessarily 
reflect those of the sponsor. 
References  
Alexandre Patry and Philippe Langlais, 2011. Identify-
ing Parallel Documents from a Large Bilingual Col-
lection of Texts: Application to Parallel Article 
215
Extraction in Wikipedia. Proceedings of the 4th 
Workshop on Building and Using Comparable Cor-
pora, pages 87-95, Portland, OR. 
Bing Zhao and Stephan Vogel. 2002. Adaptive Parallel 
Sentences Mining from Web Bilingual News Collec-
tion. Proceedings of IEEE International Conference 
on Data Mining, pages 745-750. Maebashi City, Ja-
pan. 
Deepak Ravichandran, Patrick Pantel, and Eduard 
Hovy. 2005. Randomized Algorithms and NLP: Us-
ing Locality Sensitive Hash Function for High Speed 
Noun Clustering. Proceedings of the 43rd Annual 
Meeting on Association for Computational Linguis-
tics, pages 622?629, Morristown, NJ. 
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
Jacob Uszkoreit, Jay Ponte, Ashok Popat and Moshe 
Dubiner, 2010. Large Scale Parallel Document Min-
ing for Machine Translation. Proceedings of the 23rd 
International Conference on Computational Linguis-
tics (Coling 2010), pp. 1101-1109. Beijing, China. 
Jason R. Smith, Chris Quirk, and Kristina Toutanova, 
2010. Extracting Parallel Sentences from Compara-
ble Corpora using Document Level Alignment, Pro-
ceedings of Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the ACL (HLT NAACL?10), Los Ange-
les, California. 
Jessica Enright and Grzegorz Kondrak 2007. A Fast 
Method for Parallel Document Identification, Pro-
ceedings of Human Language Technologies: The 
Conference of the North American Chapter of the 
Association for Computational Linguistics (HLT-
NAACL?07) companion volume, pages 29-32, Roch-
ester, NY. 
Matthew Snover, Bonnie Dorr, and Richard Schwartz. 
2008. Language and Translation Model Adaptation 
using Comparable Corpora. Proceedings of Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP?08), pages 856?865, Honolulu, 
HI. 
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings of 
the thiry-fourth annual ACM symposium on Theory 
of computing (STOC?02), pages 380?388, New 
York, NY. 
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and Lexicon 
Extraction via Bootstrapping and EM. In Proceedings 
of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?04), Barcelona, Spain. 
Philip Resnik and Noah Smith. 2003. The Web as a 
Parallel Corpus. Computational Linguistics, 29(3): 
349-380. 
Philipp Koehn, 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. MT Summit 2005. 
Phuket, Thailand. 
Piotr Indyk and Rajeev Motwani. 1998. Approximate 
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of the thirtieth annual 
ACM symposium on Theory of computing (STOC 
?98), pages 604?613, New York, NY. 
William B. Cavnar and John M. Trenkle. 1994. N-
Gram-Based Text Categorization. Proceedings of the 
Third Annual Symposium on Document Analysis 
and Information Retrieval, pages 161-175, Las Ve-
gas, NV.  
216
Workshop on Computational Linguistics for Literature, pages 69?77,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Dictionary of Wisdom and Wit:
Learning to Extract Quotable Phrases?
Michael Bendersky
Dept. of Computer Science
University of Massachusetts
Amherst, MA
bemike@cs.umass.edu
David A. Smith
Dept. of Computer Science
University of Massachusetts
Amherst, MA
dasmith@cs.umass.edu
Abstract
Readers suffering from information overload
have often turned to collections of pithy and
famous quotations. While research on large-
scale analysis of text reuse has found effective
methods for detecting widely disseminated
and famous quotations, this paper explores the
complementary problem of detecting, from
internal evidence alone, which phrases are
quotable. These quotable phrases are mem-
orable and succinct statements that people are
likely to find useful outside of their original
context. We evaluate quotable phrase extrac-
tion using a large digital library and demon-
strate that an integration of lexical and shallow
syntactic features results in a reliable extrac-
tion process. A study using a reddit commu-
nity of quote enthusiasts as well as a simple
corpus analysis further demonstrate the prac-
tical applications of our work.
1 Introduction
Readers have been anxious about information over-
load for a long time: not only since the rise of the
web, but with the earlier explosion of printed books,
and even in manuscript culture (Blair, 2010). One
traditional response to the problem has been ex-
cerpting passages that might be useful outside their
original sources, copying them into personal com-
monplace books, and publishing them in dictionaries
such as Bartlett?s Familiar Quotations or the Oxford
?
?The book is a dictionary of wisdom and wit...? (Samuel
Smiles, ?A Publisher and His Friends?) This and all the subse-
quent quotations in this paper were discovered by the proposed
quotable phrase extraction process.
Dictionary of Quotations. Even on the web, collec-
tion of quotable phrases continues to thrive1, as evi-
denced by the popularity of quotation websites such
as BrainyQuote and Wikiquote.
According to a recent estimate, there are close
to 130 million unique book records in world li-
braries today (Taycher, 2010). Many of these
books are being digitized and stored by commercial
providers (e.g., Google Books and Amazon), as well
as non-profit organizations (e.g., Internet Archive
and Project Gutenberg).
As a result of this digitization, the development
of new methods for preserving, accessing and ana-
lyzing the contents of literary corpora becomes an
important research venue with many practical appli-
cations (Michel et al, 2011). One particularly in-
teresting line of work in these large digital libraries
has focused on detecting text reuse, i.e., passages
from one source that are quoted in another (Kolak
and Schilit, 2008).
In contrast, in this paper we explore the modeling
of phrases that are likely to be quoted. This phrase
modeling is done based on internal evidence alone,
regardless of whether or not the phrase actually is
quoted in existing texts.
We call such potential quotation a quotable
phrase ? a meaningful, memorable, and succinct
statement that can be quoted without its original
context. This kind of phrases includes aphorisms,
epigrams, maxims, proverbs, and epigraphs.
1
?Nothing is so pleasant as to display your worldly wis-
dom in epigram and dissertation, but it is a trifle tedious to
hear another person display theirs.? (Kate Sanborn, ?The Wit
of Women?)
69
Book 
Sentence 
Segmentation
Naive Bayes 
Filtering
Quotable Phrase 
Detection
Labeled 
Quotation Set
External
Quotation Set
Extracted 
Quotes
Figure 1: Diagram of the quotable phrase extraction process.
A computational approach to quotable phrase ex-
traction has several practical applications. For in-
stance, it can be used to recommend new additions to
existing quotable phrase collections, especially fo-
cusing on lesser read and studied authors and liter-
ary works2. It can also generate quotable phrases
that will serve as catchy and entertaining previews
for book promotion and advertisement3 .
In this work, we describe such a computational
approach to quotable phrase extraction. Our ap-
proach leverages the Project Gutenberg digital li-
brary and an online collection of quotations to build
a quotable language model. This language model
is further refined by a supervised learning algorithm
that combines lexical and shallow syntactic features.
In addition, we demonstrate that a computational
approach can help to address some intriguing ques-
tions about the nature of quotability. What are the
lexical and the syntactic features that govern the
quotability of a phrase? Which authors and books
are highly quotable? How much variance is there in
the perceived quotability of a given phrase?
The remainder of this paper is organized as fol-
lows. In Section 2 we provide a detailed description
of the entire process of quotable phrase extraction.
In Section 3 we review the related work. In Sections
4 and 5 we evaluate the quotable phrase extraction
process, and provide some corpus quotability analy-
sis. We conclude the paper in Section 6.
2 Quotable Phrase Extraction
There are three unique challenges that need to be
addressed in the design of the process of quotable
2
?There is life in a poet so long as he is quoted...? (Sir Alfred
Comyn Lyall, ?Studies in Literature and History?)
3As an example, see the ?Popular Highlights? feature for
Kindle e-books in the Amazon bookstore.
phrase extraction. The first challenge stems from the
fact that the boundaries of potential quotes are often
ambiguous. A quotable phrase can consist of a sen-
tence fragment, a complete sentence, or a passage of
text that spans several sentences.
The second challenge is that the occurrence of
quotable phrases is a rare phenomena in literary cor-
pora. A randomly selected book passage is unlikely
to be quotable without any additional context.
The third challenge is related to the syntax and se-
mantics of quotable phrases. For instance, consider
the phrase
?Evil men make evil use of the law, though
the law is good, while good men die well, al-
though death is an evil.? (Thomas Aquinas,
?Summa Theologica?)
and contrast it with
?Of the laws that he can see, the great se-
quences of life to death, of evil to sorrow,
of goodness to happiness, he tells in burning
words.? (Henry Fielding, ?The Soul of a Peo-
ple?)
While both of these phrases share a common vocab-
ulary (law, death, good and evil), the latter sentence
contains unresolved pronouns (he, twice) that make
it less amenable to quotation out of context.
Accordingly, we design a three-stage quotable
phrase extraction process, with each stage corre-
sponding to one of challenges described above. The
diagram in Figure 1 provides a high-level overview
of the entire extraction process on a single book.
Next, we provide a brief description of this diagram.
Then, in the following sections, we focus on individ-
ual stages of the extraction process.
To address the first challenge of quote boundary
detection, at the first stage of the extraction process
70
(Sentence Segmentation) we segment the text of the
input book into sentences using an implementation
of the Punkt sentence boundary detection algorithm
(Kiss and Strunk, 2006). In an initial experiment, we
found that 78% of the approximately 4,000 quota-
tions collected from the QuotationsPage4 consist of
a single sentence. From now on, therefore, we make
a simplifying assumption that an extracted quotable
phrase is confined within the sentence boundaries.
The second processing stage (Na??ve Bayes Filter-
ing) aims to address the second challenge (the rar-
ity of quotable phrases) and significantly increases
the ratio of quotable phrases that are considered as
candidates in the final processing stage (Quotable
Phrase Detection). To this end, we use a set of quo-
tations collected from an external resource to build a
quotable language model. Only sentences that have
a sufficiently high likelihood of being drawn from
this language model are considered at the final pro-
cessing stage.
For this final processing stage (Quotable Phrase
Detection), we develop a supervised algorithm that
focuses on the third challenge, and analyzes the syn-
tactic structure of the input sentences. This super-
vised algorithm makes use of structural and syntac-
tic features that may effect phrase quotability, in ad-
dition to the vocabulary of the phrase.
2.1 Na??ve Bayes Filtering
In order to account for the rarity of quotable phrases
in the book corpus, we use a filtering approach based
on a pre-built quotable language model. Using this
filtering approach, we significantly reduce the num-
ber of sentences that need to be considered in the su-
pervised quotable phrase detection stage (described
in Section 2.2). In addition, this approach increases
the ratio of quotable phrases considered at the super-
vised stage, addressing the problem of the sparsity of
positive examples.
To build the quotable language model, we boot-
strap the existing quotation collections on the web.
In particular, we collect approximately 4,000 quotes
on more than 200 subjects from the QuotationsPage.
This collection provides a diverse set of high-quality
quotations on subjects ranging from Laziness and
Genius to Technology and Taxes.
4www.quotationspage.com
Then, we build two separate unigram language
models. The first one is the quotable language
model, which is built using the collected quotations
(LQ). The second one is the background language
model, which is built using the entire book corpus
(LC). Using these language models we compute a
log-likelihood ratio for each processed sentence s,
as
LLRs =
?
w?s
ln p(w|LQ)p(w|LC)
, (1)
where the probabilities p(w|?) are computed using a
maximum likelihood estimate with add-one smooth-
ing.
A sentence s is allowed to pass the filtering stage
if and only if LLRs ? [?, ?], where ?, ? are posi-
tive constants5. The lower bound on the LLRs, ?,
requires the sentence to be highly probable given the
quotable language model LQ. The upper bound on
the LLRs, ?, filters out sentences that are highly im-
probable given the background language model LC .
Finally, the sentences for which LLRs ? [?, ?]
are allowed to pass through the Na??ve Bayes filter.
They are forwarded to the next stage, in which a su-
pervised quotable phrase detection is performed.
2.2 Supervised Quotable Phrase Detection
In a large corpus, a supervised quotable phrase de-
tection method needs to handle a significant num-
ber of input instances (in our corpus, an average-
sized book contains approximately 2,000 sentences).
Therefore, we make use of a simple and efficient
perceptron algorithm, which is implemented follow-
ing the description by Bishop (2006).
We note, however, that the proposed supervised
detection method can be also implemented using a
variety of other binary prediction techniques. In
an initial experiment, we found that more complex
methods (e.g., decision trees) were comparable to or
worse than the simple perceptron algorithm.
Formally, we define a binary function f(s) which
determines whether an input sentence s is a quotable
(q) or a non-quotable (q) phrase, based on:
f(s) =
{
q if wxs > 0
q else, (2)
5In this work, we set ? = 1, ? = 25. This setting is done
prior to seeing any labeled data.
71
Feature Description
Lexical
LLR Sentence log-likelihood ratio (Eq. 1)
#word Number of words in s.
#char Number of characters in s.
wordLenAgg Feature for each aggregate Agg of word length in s.
Agg = {min, max, mean}
#capital Number of capitalized words in s.
#quantifier Number of universal quantifiers in s (from a list of 13 quantifiers, e.g., all, whole, nobody).
#stops Number of common stopwords in s.
beginStop True if s begins with a stopword, False otherwise.
hasDialog True if s contains at least one of the three common dialog terms {say, says, said}.
#abstract Number of abstract concepts (e.g., adventure, charity, stupidity ) in s.
Punctuation
hasP Five features to indicate whether punctuation of type P is present in s.
P = {quotations, parentheses, colon, dash, semi-colon}.
Parts of Speech
#POS Four features for the number of occurrences of part-of-speech POS in s.
POS = {noun, verb, adjective, adverb, pronoun}.
hasComp True if s contains a comparative adjective or adverb, False otherwise.
hasSuper True if s contains a superlative adjective or adverb, False otherwise.
hasPP True if s contains a verb in past participle, False otherwise.
#IGSeq[i] Count of the POS sequence with the i-th highest IG(X,Y ) (Eq. 3) in s.
Table 1: Description of the quotability features that are computed for each sentence s .
where xs is a vector of quotability features com-
puted for the sentence s, and w is a weight vector
associated with these features. The weight vector w
is updated using stochastic gradient descent on the
perceptron error function (Bishop, 2006).
Since Eq. 2 demonstrates that the supervised
quotable phrase detection can be formulated as a
standard binary classification problem, its success
will be largely determined by an appropriate choice
of feature vector xs. As we are unaware of any
previous work on supervised detection of quotable
phrases, we develop an initial set of easy-to-compute
features that considers the lexical and shallow syn-
tactic structure of the analyzed sentence.
2.3 Quotability Features
A decision about phrase quotability is often sub-
jective; it is strongly influenced by personal taste
and circumstances6 . Therefore, the set of features
that we describe in this section is merely a coarse-
grained approximation of the true intrinsic qualities
of a quotable phrase. Nevertheless, it is important to
6
?One man?s beauty another?s ugliness; one man?s wisdom
another?s folly.? (Ralph Waldo Emerson, ?Essays?)
note that these features do prove to be beneficial in
the context of the quote detection task, as is demon-
strated by our empirical evaluation in Section 5.
Table 1 details the quotability features, which are
divided into 3 groups: lexical, punctuation-based
and POS-based features. All of these features are
conceptually simple and can be efficiently computed
even for a large number of input sentences.
Some of these features are inspired by existing
text analysis tasks. For instance, work on readabil-
ity detection for the web (Kanungo and Orr, 2009;
Si and Callan, 2001) examined features which are
similar to the lexical features in Table 1. Parts of
speech features (e.g., the presence of comparative
and superlative adjectives and adverbs) have been
extensively used for sentiment analysis and opinion
mining (Pang and Lee, 2008).
In addition, we use a number of features based on
simple hand-crafted word lists. These lists include
word categories that could be potential indicators of
quotable phrases such as universal quantifiers (e.g.,
all, everyone) and abstract concepts7.
7For abstract concept modeling we use a list of 176 abstract
nouns available at www.englishbanana.com.
72
The novel features in Table 1 that are specifically
designed for quotable phrase detection are based on
part of speech sequences that are highly indicative
of quotable (or, conversely, non-quotable) phrase
(features #IGSeq[i]). In order to compute these
features we first manually label a validation set of
500 sentences that passed the Na??ve Bayes Filtering
(Section 2.1). Then, we apply a POS tagger to these
sentences, and for each POS tag sequence of length
n, seqn, we compute its information gain
IG(X,Y ) = H(X) ?H(X|Y ). (3)
In Eq. 3, X is a binary variable indicating the pres-
ence or the absence of seqn in the sentence, and
Y ? {q, q}.
We select k sequences seqn with the highest value
of IG(X,Y )8. We use the count in the sentence of
the sequence seqn with the i-th highest information
gain as the feature #IGSeq[i]. Intuitively, the fea-
tures #IGSeq[i] measure how many POS tag se-
quences that are indicative of a quotable phrase (or,
conversely, indicative of a non-quotable phrase) the
sentence contains.
3 Related Work
The increasing availability of large-scale digital li-
braries resulted in a recent surge of interest in com-
putational approaches to literary analysis. To name
just a few examples, Genzel et al (2010) examined
machine translation of poetry; Elson et al (2010)
extracted conversational networks from Victorian
novels; and Faruqui and Pado? (2011) predicted for-
mal and informal address in English literature.
In addition, computational methods are increas-
ingly used for identification of complex aspects
of writing such as humor (Mihalcea and Pulman,
2007), double-entendre (Kiddon and Brun, 2011)
and sarcasm (Tsur et al, 2010). However, while
successful, most of this work is still limited to an
analysis of a single aspect of writing style.
In this work, we propose a more general compu-
tational approach that attempts to extract quotable
phrases. A quotability of a phrase can be affected
by various aspects of writing including (but not lim-
8In this work, we set n = 3, k = 50. This setting is done
prior to seeing any labeled data.
Number of books 21, 492
Number of authors 8, 889
Total sentences 4.45 ? 107
After Na??ve Bayes filtering 1.75 ? 107
Table 2: Summary of the Project Gutenberg corpus.
ited to) humor and irony9, use of metaphors10 , and
hyperbole11 .
It is important to note that our approach is con-
ceptually different from the previous work on para-
phrase and quote detection in book corpora (Kolak
and Schilit, 2008), news stories (Liang et al, 2010)
and movie scripts (Danescu-Niculescu-Mizil et al,
2012). While this previous work focuses on mining
popular and oft-used quotations, we are mainly in-
terested in discovering quotable phrases that might
have never been quoted by others.
4 Experimental Setup
To evaluate the quotable phrase extraction process
in its entirety (see Figure 1), we use a collection of
Project Gutenberg (PG) books12 ? a popular digital
library containing full texts of public domain books
in a variety of formats. In particular, we harvest the
entire corpus of 21,492 English books in textual for-
mat from the PG website.
The breakdown of the PG corpus is shown in Ta-
ble 2. The number of detected sentences in the PG
corpus exceeds 44 million. Roughly a third of these
sentences are able to pass through the Na??ve Bayes
Filtering (described in Section 2.1) to the supervised
quotable phrase detection stage (Section 2.2).
For each of these sentences, we compute a set of
lexical and syntactic features described in Section
2.3. For computing the features that require the part
of speech tags, we use the MontyLingua package
(Liu, 2004).
9
?To be born with a riotous imagination and then hardly ever
to let it riot is to be a born newspaper man.? (Zona Gale, ?Ro-
mance Island?)
10
?If variety is the spice of life, his life in the north has been
one long diet of paprika.? (Fullerton Waldo, ?Grenfell: Knight-
Errant of the North?)
11
?The idea of solitude is so repugnant to human nature, that
even death would be preferable.? (William O.S. Gilly, ?Nar-
ratives of Shipwrecks of the Royal Navy; between 1793 and
1849?)
12http://www.gutenberg.org/
73
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Precision?Recall Curves
Recall
Pr
ec
is
io
n
Word Features
All Features
Figure 2: Prec. vs. recall for quotable phrase detection.
We find that the extraction process shown in Fig-
ure 1 is efficient and scalable. On average, the entire
process requires less than ten seconds per book on a
single machine.
The complete set of extracted quotable phrases
and annotations is available upon request from the
authors. In addition, the readers are invited to visit
www.noisypearls.com, where a quotable phrase
from the set is published daily.
5 Evaluation and Analysis
5.1 Na??ve Bayes Filtering Evaluation
In the Na??ve Bayes Filtering stage (see Section 2.1)
we evaluate two criteria. First, we measure its abil-
ity to reduce the number of sentences that pass to the
supervised quotable phrase detection stage. As Ta-
ble 2 shows, the Na??ve Bayes Filtering reduces the
number of these sentences by more than 60%.
Second, we evaluate the recall of the Na??ve Bayes
Filtering. We are primarily interested in its ability
to reliably detect quotable phrases and pass them
through to the next stage, while still reducing the
total number of sentences.
For recall evaluation, we collect a set of
2, 817 previously unseen quotable phrases from the
Goodreads website13, and run them through the
Na??ve Bayes Filtering stage. 2, 262 (80%) of the
quotable phrases pass the filter, indicating a high
quotable phrase recall.
13http://www.goodreads.com/quotes
1 #abstract +91.64
2 #quantifier +61.67
3 hasPP ?60.34
4 #IGSeq[16](VB IN PRP) +39.71
5 #IGSeq[6](PRP MD VB) ?38.78
6 #adjective +37.71
7 #IGSeq[14](DT NN VBD) ?36.88
8 #verb +35.22
9 beginStop +31.73
10 #noun +29.63
Table 3: Top quotability features.
Based on these findings, we conclude that the pro-
posed Na??ve Bayes Filtering is able to reliably detect
quotable phrases, while filtering out a large number
of non-quotable ones. It can be further calibrated to
reduce the number of non-quotable sentences or to
increase the quotable phrase recall, by changing the
setting of the parameters ? and ?, described in Sec-
tion 2.1. In the remainder of this section, we use its
output to analyze the performance of the supervised
quotable phrase detection stage.
5.2 Quotable Phrase Detection Evaluation
To evaluate the performance of the supervised
quotable phrase detection stage (see Section 2.2) we
randomly sample 1,500 sentences that passed the
Na??ve Bayes Filtering (this sample is disjoint from
the sample of 500 sentences used for computing
the IGTagSeq feature in Section 2.3). We anno-
tate these sentences with q (Quotable) and q (Non-
Quotable) labels.
Of these sentences, 381 (25%) are labeled as
Quotable. This ratio of quotable phrases is much
higher than what is expected from a non-filtered con-
tent of a book, which provides an indication that the
Na??ve Bayes Filtering provides a relatively balanced
input to the supervised detection stage.
We use this random sample of 1,500 labeled sen-
tences to train a perceptron algorithm (as described
in Section 2.2) using 10-fold cross-validation. We
train two variants of the perceptron. The first variant
is trained using only the lexical features in Table 1,
while the second variant uses all the features.
Figure 2 compares the precision-recall curves of
these two variants. It demonstrates that using the
syntactic features based on punctuation and part of
speech tags significantly improves the precision of
74
Popular ?? 10 12
Upvoted 1 ??? 10 34
No upvotes ?? 0 14
p(?> 0) = .77
Table 4: Distribution of reddit upvote scores.
quote phrase detection at all recall levels. For in-
stance at the 0.4 recall level, it can improve precision
by almost 25%.
Figure 2 also shows that the proposed method
is reliable for high-precision quotable phrase de-
tection. This is especially important for applica-
tions where recall is given less consideration, such
as book preview using quotable phrases. The pro-
posed method reaches a precision of 0.7 at the 0.1
recall level.
It is also interesting to examine the importance of
different features for the quotable phrase detection.
Table 3 shows the ten highest-weighted features, as
learned by the perceptron algorithm on the entire set
of 1,500 labeled examples.
The part of speech features #IGTagSeq[i] oc-
cupy three of the positions in the Table 3. It is inter-
esting to note that two of them have a high negative
weight. In other words, some of the POS sequences
that have the highest information gain (see Eq. 3)
are sequences that are indicative of non-quotable
phrases, rather than quotable phrases.
The two highest-weighted features are based
on handcrafted word lists (#abstract and
#quantifier, respectively). This demonstrates
the importance of task-specific features such as
these for quotability detection.
Finally, the presence of different parts of speech
in the phrase (nouns, verbs and adjectives), as well
as their verb tenses, are important features. For
instance, the presence of a verb in past participle
(hasPP) is a strong negative indicator of phrase
quotability.
5.3 The reddit Study
As mentioned in Section 2.3, the degree of the
phrase quotability is often subjective, and therefore
its estimation may vary among individuals. To val-
idate that our quotability detection method is not
biased by our training data, and that the detected
quotes will have a universal appeal, we set up a veri-
fication study that leverages an online community of
quote enthusiasts.
For our study, we use reddit, a social content web-
site where the registered users submit content, in the
form of either a link or a text post. Other regis-
tered users then upvote or downvote the submission,
which is used to rank the post.
Specifically, we use the Quotes subreddit14, an ac-
tive reddit community devoted to discovering and
sharing quotable phrases. At the time of this writ-
ing, the Quotes subreddit has more than 12,000 sub-
scribers. A typical post to this subreddit contains a
single quotable phrase with attribution. Any reddit
user can then upvote or downvote the quote based on
its perceived merit.
To validate the quality of the quotes which were
used for training the perceptron algorithm, we sub-
mitted 60 quotes, which were labeled as quotable by
one of the authors, to the Quotes subreddit. At most
one quote per day was submitted, to avoid negative
feedback from the community for ?spamming?.
Table 4 presents the upvote scores of the submit-
ted quotes. An upvote score, denoted ?, is computed
as
?= # upvotes ? # downvotes.
Table 4 validates that the majority of the quotes la-
beled as quotable, were also endorsed by the red-
dit community, and received a non-negative upvote
score. As an illustration, in Table 5, we present five
quotes with the highest upvote scores. Anecdotally,
at the time of this writing, only one of the quotes
in Table 5 (a quote by Mark Twain) appeared in
web search results in contexts other than the origi-
nal book.
5.4 Project Gutenberg Corpus Analysis
In this section, we briefly highlight an interesting ex-
ample of how the proposed computational approach
to quotable phrase extraction can be used for a liter-
ary analysis of the PG digital library. To this end,
we train the supervised quotable phrase detection
method using the entire set of 1,500 manually la-
beled sentences. We then run this model over all the
17.5 million sentences that passed the Na??ve Bayes
filtering stage, and retain only the sentences that get
positive perceptron scores (Eq. 2).
14http://www.reddit.com/r/quotes
75
Quote ?
?One hour of deep agony teaches man more love and wisdom than a whole long life of happiness.? 49
(Walter Elliott, ?Life of Father Hecker?)
?As long as I am on this little planet I expect to love a lot of people and I hope they will love me in return.? 43
(Kate Langley, Bosher, ?Kitty Canary?)
?None of us could live with an habitual truth-teller; but thank goodness none of us has to.? 40
(Mark Twain, ?On the Decay of the Art of Lying?)
?A caged bird simply beats its wings and dies, but a human being does not die of loneliness, even when he prays for death.? 33
(George Moore, ?The Lake?)
?Many will fight as long as there is hope, but few will go down to certain death.? 30
(G. A. Henty, ?For the Temple?)
Table 5: Five quotes with the highest upvote scores on reddit.
(a) Authors (b) Books
1 Henry Drummond .045
2 Ella Wheeler Wilcox .041
3 S. D. Gordon .040
4 Andrew Murray .038
5 Ralph Waldo Emerson .037
6 Orison Swett Marden .034
7 Mary Baker Eddy .031
8 ?Abdu?l-Baha? .029
9 John Hartley .029
10 Rabindranath Tagore .028
1 ?Friendship? (Hugh Black) .113
2 ?The Dhammapada? (Translated by F. Max Muller ) .112
3 ?The Philosophy of Despair? (David Starr Jordan) .106
4 ?Unity of Good? (Mary Baker Eddy) .097
5 ?Laments? (Jan Kochanowski) .084
6 ?Joy and Power? (Henry van Dyke) .079
7 ?Polyeucte? (Pierre Corneille) .078
8 ?The Forgotten Threshold? (Arthur Middleton) .078
9 ?The Silence? (David V. Bush) .077
10 ?Levels of Living? (Henry Frederick Cope) .075
Table 6: Project Gutenberg (a) authors and (b) books with the highest quotability index.
This procedure yields 701,418 sentences, which
we call quotable phrases in the remainder of this
section. These quotable phrases are less than 2% of
the entire Project Gutenberg corpus; however, they
still constitute a sizable collection with some poten-
tially interesting properties.
We propose a simple example of a literary anal-
ysis that can be done using this set of quotable
phrases. We detect books and authors that have a
high quotability index, which is formally defined as
QI(x) = # quotable phrases(x)
# total sentences(x) ,
where x is either a book or an author. To ensure the
statistical validity of our analysis, we limit our atten-
tion to books with at least 25 quotable phrases and
authors with at least 5 books in the PG collection.
Using this definition, we can easily compile a list
of authors and books with the highest quotability in-
dex (see Table 6). An interesting observation is that
many of the authors and books in Table 6 deal with
religious themes: Christianity (e.g., Mary Baker
Eddy, S. D. Gordon), Baha????sm (?Abdu?l-Baha?) and
Buddhism (?The Dhammapada?). This is not sur-
prising considering the figurative language common
in the religious prose15.
15
?If a man speaks or acts with an evil thought, pain follows
6 Conclusions
As the number of digitized books increases, a com-
putational analysis of literary corpora becomes an
active research field with many practical applica-
tions. In this paper, we focus on one such appli-
cation: extraction of quotable phrases from books.
Quotable phrase extraction can be used, among
other things, for finding new original quotations
for dictionaries and online quotation repositories, as
well as for generating catchy previews for book ad-
vertisement.
We develop a quotable phrase extraction process
that includes sentence segmentation, unsupervised
sentence filtering based on a quotable language
model, and a supervised quotable phrase detection
using lexical and syntactic features. Our evaluation
demonstrates that this process can be used for high-
precision quotable phrase extraction, especially in
applications that can tolerate lower recall. A study
using a reddit community of quote enthusiasts as
well as a simple corpus analysis further demonstrate
the practical applications of our work.
him, as the wheel follows the foot of the ox that draws the car-
riage.?(?The Dhammapada?, translated by F. Max Muller )
76
7 Acknowledgments
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by NSF
grant IIS-0910884 and in part by ARRA NSF IIS-
9014442. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
those of the sponsor.
References
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Ann M. Blair. 2010. Too Much to Know: Managing
Scholarly Information before the Modern Age. Yale
University Press.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Proc.
of ACL, page To appear.
David K. Elson, Nicholas Dames, and Kathleen R. McK-
eown. 2010. Extracting social networks from literary
fiction. In Proc. of ACL, pages 138?147.
Manaal Faruqui and Sebastian Pado?. 2011. ?I thou thee,
thou traitor?: predicting formal vs. informal address in
English literature. In Proceedings of ACL-HLT, pages
467?472.
Dmitriy Genzel, Jakob Uszkoreit, and Franz Och. 2010.
?Poetic? Statistical Machine Translation: Rhyme and
Meter. In Proc. of EMNLP, pages 158?166.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proc. of
WSDM, pages 202?211.
Chloe Kiddon and Yuriy Brun. 2011. That?s What She
Said: Double Entendre Identification. In Proc. of
ACL-HLT, pages 89?94.
T. Kiss and J. Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguis-
tics, 32(4):485?525.
Okan Kolak and Bill N. Schilit. 2008. Generating links
by mining quotations. In Proc. of 19th ACM confer-
ence on Hypertext and Hypermedia, pages 117?126.
Jisheng Liang, Navdeep Dhillon, and Krzysztof Koper-
ski. 2010. A large-scale system for annotating and
querying quotations in news feeds. In Proc. of Sem-
Search.
Hugo Liu. 2004. Montylingua: An end-to-end natural
language processor with common sense. Available at:
web.media.mit.edu/?hugo/montylingua.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
Martin A. Nowak, and Erez Lieberman Aiden. 2011.
Quantitative analysis of culture using millions of digi-
tized books. Science, 331(6014):176?182.
Rada Mihalcea and Stephen Pulman. 2007. Character-
izing humour: An exploration of features in humorous
texts. In Proc. of CICLing, pages 337?347.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM, pages 574?
576.
Leonid Taycher. 2010. Books of the world, stand up and
be counted! All 129,864,880 of you. Inside Google
Books blog.
Oren Tsur, Dimitry Davidov, and Avi Rappoport. 2010.
ICWSM?A great catchy name: Semi-supervised
recognition of sarcastic sentences in online product re-
views. In Proc. of ICWSM, pages 162?169.
77
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 22?32,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Discovering Factions in the Computational Linguistics Community
Yanchuan Sim Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ysim,nasmith}@cs.cmu.edu
David A. Smith
Department of Computer Science
University of Massachusetts
Amherst, MA 01003, USA
dasmith@cs.umass.edu
Abstract
We present a joint probabilistic model of who
cites whom in computational linguistics, and
also of the words they use to do the citing. The
model reveals latent factions, or groups of in-
dividuals whom we expect to collaborate more
closely within their faction, cite within the fac-
tion using language distinct from citation out-
side the faction, and be largely understandable
through the language used when cited from
without. We conduct an exploratory data anal-
ysis on the ACL Anthology. We extend the
model to reveal changes in some authors? fac-
tion memberships over time.
1 Introduction
The ACL Anthology presents an excellent dataset
for studying both the language and the social con-
nections in our evolving research field. Extensive
studies using techniques from the field of biblio-
metrics have been applied to this dataset (Radev et
al., 2009a), quantifying the importance and impact
factor of both authors and articles in the commu-
nity. Moreover, recent work has leveraged the avail-
ability of digitized publications to study trends and
influences within the ACL community (Hall et al,
2008; Gerrish and Blei, 2010; Yogatama et al, 2011)
and to analyze academic collaborations (Johri et al,
2011).
To the best of our knowledge, however, existing
work has mainly pursued ?macroscopic? investiga-
tions of the interaction of authors in collaboration,
citation networks, or the textual content of whole
papers. We seek to complement these results with a
?microscopic? investigation of authors? interactions
by considering the individual sentences authors use
to cite each other.
In this paper, we present a joint model of who
cites whom in computational linguistics, and also of
how they do the citing. Central to this model is the
idea of factions, or groups of individuals whom we
expect to (i) collaborate more closely within their
faction, (ii) cite within the faction using language
distinct from citation outside the faction, (iii) be
largely understandable through the language used
when cited from without, and (iv) evolve over time.
1
Factions can be thought of as ?communities,? which
are loosely defined in the literature on networks
as subgraphs where internal connections are denser
than external ones (Radicchi et al, 2004). The dis-
tinction here is that the strength of connections de-
pends on a latent language model estimated from ci-
tation contexts.
This paper is an exploratory data analysis using a
Bayesian generative model. We aim both to discover
meaningful factions in the ACL community and also
to illustrate the use of a probabilistic model for such
discovery. As such, we do not present any objective
evaluation of the model or make any claims that the
factions optimally explain the research community.
Indeed, we suspect that reaching a broad consensus
among community members about factions (i.e., a
?gold standard?) would be quite difficult, as any so-
cial community?s factions are likely perceived very
1
Our factions are computational abstractions?clusters of
authors?discovered entirely from the corpus. We do not claim
that factions are especially contentious, any more than ?sub-
communities? in social networks are especially collegial.
22
subjectively. It is for this reason that a probabilistic
generative model, in which all assumptions are made
plain, is appropriate for the task. We hope this analy-
sis will prove useful in future empirical research on
social communities (including scientific ones) and
their use of language.
2 Model
In this paper, our approach is a probabilistic model
over (i) coauthorship relations and (ii) the words
in sentences containing citations. The words are
assumed to be generated by a distribution that de-
pends on the (latent) faction memberships of the cit-
ing authors, the cited authors, and whether the au-
thors have coauthored before. To model these dif-
ferent effects on language, we use a sparse additive
generative (SAGE) model (Eisenstein et al, 2011).
In contrast to the popular Dirichlet-multinomial for
topic modeling, which directly models lexical prob-
abilities associated with each (latent) topic, SAGE
models the deviation in log frequencies from a back-
ground lexical distribution. Imposing a sparsity-
inducing prior on the deviation vectors limits the
number of terms whose probabilities diverge from
the background lexical frequencies, thereby increas-
ing robustness to limited training data. SAGE can be
used with or without latent topics; our model does
not include topics. Figure 1 shows the plate diagram
for our model.
We describe the generative process:
? Generate the multinomial distribution over fac-
tion memberships from a Dirichlet distribution:
? ? Dir(?).
? Generate the binomial distribution for whether
two authors coauthor, given that they are in the
same faction, from a Beta distribution: ?
same
?
Beta(?same0 , ?
same
1 ). Generate the analogous bi-
nomial, given that they are in different factions:
?
diff
? Beta(?diff0 , ?
diff
1 ).
? For each author i, draw a faction indicator
ai ? Multinomial(?).
? For all ordered pairs of factions (g, h), draw a
deviation vector ?
(g,h)
? Laplace(0, ?). This
vector, which will be sparse, corresponds to the
?
?
a
(i)
a
(j)
z
(i,j)
?
same
?
same
?
diff
?
diff
w
(i,j)
N
(i,j)
A?A
m
?
(g,h)
?
G?G
Figure 1: Plate diagram for our graphical model. A and
G are the fixed numbers of authors and factions, respec-
tively. m is the background word distribution, ?, ? , ?
are hyperparameters, a are latent author factions, z and
w are the observed coauthorship relations and observed
words in citation sentences between authors, respectively.
Each of the a
(i)
, denoting author i?s faction alignment,
are sampled once every iteration conditioned on all the
other a
(j)
. If i and j are coauthors or i cited j in some
publication, a
(i)
and a
(j)
will not be conditionally inde-
pendent due to the v-structure. ?
same
and ?
diff
are bino-
mial distributions over whether two authors have collab-
orated together before, given that they are assigned to the
same/different factions. Dashed variables are collapsed
out in the Gibbs sampler, while double bordered variables
are optimized in the M-step.
deviations in word log-frequencies when fac-
tion g is citing faction h.
? For each word v in the vocabulary, let the uni-
gram probability that an author in faction g uses
to cite an author in faction h be
?
(g,h)
v =
exp(?(g,h)v +mv)
?
v? exp(?
(g,h)
v? +mv?)
.
? For each ordered pair of authors (i, j),
? For each word that i uses to cite j, draw
w
(i,j)
k ? Multinomial(?
(a(i),a(j))).
? If the authors are from the same faction,
i.e., a
(i) = a(j), draw coauthorship indi-
23
cator z
(i,j)
? Binomial(?same); else, draw
z
(i,j)
? Binomial(?diff).
Thus, our goal is to maximize the conditional like-
lihood of the observed data
p(w, z | ?,?, ?,m,?) =
?
?
?
?
?
a
p(w, z,?,?,a | ?,?, ?,m,?)
with respect to ? and ?. We fix ? and ?, which are
hyperparameters that encode our prior beliefs, and
m, which we assume to be a fixed background word
distribution.
Exact inference in this model is intractable, so we
resort to an approximate inference technique based
on Markov Chain Monte Carlo simulation. We per-
form Bayesian inference over the latent author fac-
tions while using maximum a posteriori estimates
of ? because Bayesian inference of ? is problematic
due to the logistic transformation. We refer the in-
terested reader to Eisenstein et al (2011). We take
an empirical Bayes approach to setting the hyper-
parameter ?. Our overall learning procedure is a
Monte Carlo Expectation Maximization algorithm
(Wei and Tanner, 1990).
3 Learning and Inference
Our learning algorithm is a two-step iterative pro-
cedure. During the E-step, we perform collapsed
Gibbs sampling to obtain distributions over factions
for each author, given the current setting of the hy-
perparameters. In the M-step, we obtain point es-
timates for the hyperparameters ? and ? given the
current posterior distributions for the author fac-
tions.
3.1 E-step
As the Dirichlet and Beta distributions are conjugate
priors to the multinomial and binomial respectively,
we can integrate out the latent variables ?, ?
(same)
and ?
(diff)
. For an author i, we sample his faction
alignment a
(i)
conditioned on faction assignments
to all other authors and citation words between i and
other authors (in both directions). Denoting a
?i
as
the current faction assignments for all the authors
except i,
p(a(i) = g | a(?i),w,?,?,?)
? p(a(i) = g,a(?i),w | ?,?,?)
? (Ng + ?g)
A?
j
?

z +N

z
?

0 + ?

1 +N

0 +N

1
p(w(i) | ?)
where Ng is the number of authors (except i) who
are assigned to faction g, ij = ?same? if g = a(j)
and ij = ?diff? otherwise, and N 1, N

0 denotes
the number of author pairs that have/have not coau-
thored before respectively, given the status of their
factions . We elide the subscripts of  and super-
script of z for notational simplicity and abuse nota-
tion to let w
(i)
refer to all author i?s citation words,
both incoming and outgoing. Using SAGE, the fac-
tor for an author?s words is
p(w(i) | ?) =
?
j
?
v
(
?
(g,a(j))
v
)w(i,j)v (
?
(a(j),g)
v
)w(j,i)v
where w
(i,j)
v is the observed count of the number of
times word v has been used when author i cites j; j
ranges over the A authors.
We sample each author?s faction in turn and do so
several times during the E-step, collecting samples
to estimate our posterior distribution over a.
3.2 M-step
In the M-step, we optimize all ?
(g,h)
and ? given
the posterior distribution over author factions.
Optimizing ?. Eisenstein et al (2011) postu-
lated that the components of ? are drawn from
a compound model
?
N (?;?, ?)E(?; ?)d?, where
E(?; ?) indicates the Exponential distribution. They
fit a variational distribution Q(?) and optimized the
log-likelihood of the data by iteratively fitting the
parameters ? using a Newton optimization step and
maximizing the variational bound.
The compound model described is equivalent to
the Laplace distribution L(?;?, ?) (Lange and Sin-
sheimer, 1993; Figueiredo, 2003). Moreover, a zero
mean Laplace prior has the same effect as placing an
L1 regularizer on ?. Therefore, we can equivalently
24
maximize the regularized likelihood
?c
(g,h)
?
T
?
(g,h)
? ?C
(g,h)
? log
?
v
exp(?(g,h)v +mv)
? ?
?
?
??
(g,h)
?
?
?
1
with respect to ?
(g,h)
. ?c
(g,h)
? is a vector of expected
count of the words that faction g used when citing
faction h, ?c
(g,h)
? =
?
v ?c
(g,h)
v ? and ? is the regu-
larization constant. The regularization constant and
Laplace variance are related by ? = ??1 (Tibshirani,
1996).
We use the gradient-based optimization routine
OWL-QN (Andrew and Gao, 2007) to maximize the
above objective function with respect to ?
(g,h)
for
each pair of factions g and h.
Optimizing ?. As in the empirical Bayes ap-
proach, we learn the hyperparameter setting of ?
from the data by maximizing the log likelihood
with respect to ?. By treating ? as the parame-
ter of a Dirichlet-multinomial compound distribu-
tion, we can directly use the samples of author fac-
tions produced by our Gibbs sampler to estimate
?. Minka (2009) describes in detail several itera-
tive approaches to estimate ?; we use the linear-
time Newton-Raphson iterative update to estimate
the components of ?.
4 Data Analysis
4.1 Dataset
We used the ACL Anthology Network Corpus
(Radev et al, 2009b), which currently contains
18,041 papers written by 12,777 authors. These pa-
pers are published in the field of computational lin-
guistics between 1965 and 2011.
2
Furthermore, the
corpus provides bibliographic data such as authors
of the papers and bibliographic references between
each paper in the corpus. We extracted sentences
containing citations using regular expressions and
linked them between authors with the help of meta-
data provided in the corpus.
We tokenized the extracted sentences and down-
cased them. Words that are numeric, appear less
2
For a list of the journals, conferences and workshops
archived by the ACL anthology, please visit http://
aclweb.org/anthology-new.
than 20 times, or are in a stop word list are dis-
carded. For papers with multiple authors, we divided
the word counts by the number of pairings between
authors in both papers, assigning each word to each
author-pair (i.e., a count of
1
nn? if a paper with n au-
thors cites a paper with n
?
authors).
Due to the large number of authors, we only used
the 500 most cited authors (within the corpus) who
have published at least 5 papers. Papers with no au-
thors left are removed from the dataset. As a re-
sult, we have 8,144 papers containing 80,776 cita-
tion sentences (31,659 citation pairs). After text pro-
cessing, there are 391,711 tokens and 3,037 word
types.
In each iteration of the EM algorithm, we run the
E-step Gibbs sampler for 300 iterations, discarding
the first 100 samples for burn-in and collecting sam-
ples at every 3rd iteration to avoid autocorrelation.
At the M-step, we update our ? and ? using the
samples collected. We run the model for 100 EM
iterations.
We fixed ? = 5, ?same = (0.5, 1) and ?diff =
(1, 0.5). Our setting of ? reflects our prior beliefs
that coauthors tend to be from the same faction.
4.2 Factions in ACL (1965?2011)
We ran the model withG = 30 factions and selected
the most probable faction for each author from the
posterior distribution of the author-faction alignment
obtained in the final E step. Only 26 factions were
selected as most probable for some author.
3
Table 1
presents members of selected factions, along with
citation words that have the largest positive log fre-
quency deviation from the background distribution.
4
Table 2 shows a list of the top three authors associ-
ated with factions not shown in Table 1. Incoming
(outgoing) citation words are found by summing the
log deviation vectors ? across citing (cited) factions.
The author factions are manually labeled.
We see from Table 1, the model has selected key-
words that are arguably significant in certain sub-
fields in computational linguistics. Incoming cita-
tions are generally indicative of the subject areas in
3
In future work, nonparametric priors might be employed to
automate the selection of G.
4
We found it quite difficult to make sense of terms with neg-
ative log frequency deviations. This suggests exploring a model
allowing only positive deviations; we leave that for future work.
25
Formalisms (31) Fernando Pereira, Jason M. Eisner, Stuart M. Shieber, Walter Daelemans, Hitoshi Isa-
hara
Self cites: parsing
In cites: parsing, semiring, grammars, tags, grammar, tag, lexicalized, dependency
Out cites: tagger, regular, dependency, transformationbased, tagging, stochastic, grammars, sense
Evaluation (17) Salim Roukos, Eduard Hovy, Marti A. Hearst, Chin-Yew Lin, Dekang Lin
Self cites: automatic, bleu, linguistics, evaluation, computational, text, proceedings
In cites: automatic, bleu, segmentation, method, proceedings, dependency, parses, text
Out cites: paraphrases, cohesion, agreement, hierarchical, entropy, phrasebased, evaluation, tree-
bank
Semantics (26) Martha Palmer, Daniel Jurafsky, Mihai Surdeanu, David Weir, German Rigau
Self cites: sense, semantic, wordnet
In cites: framenet, sense, semantic, task, wordnet, word, project, question
Out cites: sense, wordnet, moses, preferences, distributional, semantic, focus, supersense
Machine Translation
(MT1) (9)
Kevin Knight, Michel Galley, Jonathan Graehl, Wei Wang, Sanjeev P. Khudanpur
Self cites: inference, scalable, model
In cites: scalable, inference, machine, training, generation, translation, model, syntaxbased
Out cites: phrasebased, hierarchical, inversion, forest, transduction, translation, ibm, discourse
Word Sense Disam-
biguation (WSD) (42)
David Yarowsky, Rada Mihalcea, Eneko Agirre, Ted Pedersen, Yorick Wilks
Self cites: sense, word
In cites: sense, preferences, wordnet, acquired, semcor, word, semantic, calle
Out cites: sense, subcategorization, acquisition, automatic, corpora, lexical, processing, wordnet
Parsing (20) Michael John Collins, Eugene Charniak, Mark Johnson, Stephen Clark, Massimiliano
Ciaramita
Self cites: parser, parsing, model, perceptron, parsers, dependency
In cites: parser, perceptron, supersense, parsing, dependency, results, hmm, models
Out cites: parsing, forest, treebank, model, coreference, stochastic, grammar, task
Discourse (29) Daniel Marcu, Aravind K. Joshi, Barbara J. Grosz, Marilyn A. Walker, Bonnie Lynn
Webber
Self cites: discourse, structure, centering
In cites: discourse, phrasebased, centering, tag, focus, rhetorical, tags, lexicalized
Out cites: discourse, rhetorical, framenet, realizer, tags, resolution, grammars, synonyms
Machine Translation
(MT2) (9)
Franz Josef Och, Hermann Ney, Mitchell P. Marcus, David Chiang, Dekai Wu
Self cites: training, error
In cites: error, giza, rate, alignment, training, minimum, translation, phrasebased
Out cites: forest, subcategorization, arabic, model, translation, machine, models, heuristic
Table 1: Key authors and citation words associated with some factions. For each faction, we show the 5 authors with
highest expected incoming citations (i.e p(faction | author) ? citations). Factions are labeled manually, referring to
key sub-fields in computational linguistics. Faction sizes are in parenthesis following the labels. The citation words
with the strongest positive weights in the deviation vectors are shown.
which the faction holds recognized expertise. For
instance, the faction labeled ?semantics? has cita-
tion terms commonly associated with propositional
semantics: sense, framenet, wordnet. On the other
hand, outgoing citations hint at the related work that
a faction builds on; discourse might require building
on components involving framenet, grammars, syn-
onyms, while word sense disambiguation involves
solving problems like acquisition and modeling sub-
categorization.
4.3 Sensitivity
Given the same initial parameters, we found our
model to be fairly stable across iterations of Monte
26
Adam Lopez, Paul S. Jacobs (2)
Regina Barzilay, Judith L. Klavans, Robert T. Kasper (3)
Lauri Karttunen, Kemal Oflazer, Kimmo Koskenniemi (3)
John Carroll, Ted Briscoe, Scott Miller (7)
Vincent J. Della Pietra, Stephen A. Della Pietra, Robert L.
Mercer (25)
Thorsten Brants, Liang Huang, Anoop Sarkar (9)
Christoph Tillmann, Kenji Yamada, Sharon Goldwater (7)
Alex Waibel, Keh-Jiann Chen, Katrin Kirchhoff (3)
Lynette Hirschman, Claire Cardie, Vincent Ng (26)
Erik F. Tjong Kim Sang, Ido Dagan, Marius Pas?ca (21)
Yuji Matsumoto, Dragomir R. Radev, Chew Lim Tan (18)
Christopher D. Manning, Owen Rambow, Ellen Riloff (19)
Richard Zens, Hieu Hoang, Nicola Bertoldi (9)
Dan Klein, Jun?ichi Tsujii, Yusuke Miyao (6)
Janyce Wiebe, Mirella Lapata, Kathleen R. McKeown (50)
I. Dan Melamed, Ryan McDonald, Joakim Nivre (10)
Philipp Koehn, Lillian Lee, Chris Callison-Burch (80)
Kenneth Ward Church, Eric Brill, Richard M. Schwartz
(19)
Table 2: Top 3 authors of the remaining 18 factions not
displayed in Table 1.
Carlo EM. We found that when G was too small
(e.g., 10), groups were more mixed and the ? vectors
could not capture variation among them well. When
G was larger, the factions were subjectively cleaner,
but fields like translation split into many factions (as
is visible in the G = 30 case illustrated in Tables 1
and 2. Strengthening the L1 penalty made ? more
sparse, of course, but gave less freedom in fitting the
data and therefore more grouping of authors into a
fewer effective factions.
4.4 Inter-Faction Relationships
By using the most probable a posteriori faction for
each author, we can compute the number of cita-
tions between factions. We define the average inter-
faction citations by:
IFC(g, h) =
?(g ? h) + ?(h? g)
Ng +Nh
(1)
where ?(g ? h) is the total number of papers writ-
ten by authors in g that cite papers written by authors
in h.
Figure 2 presents a graph of selected factions
and how these factions talk about each other. As
we would expect, the machine translation faction is
quite strongly connected to formalisms and parsing
factions, reflecting the heavy use of grammars and
Av
era
ge 
ou
t-c
ita
tio
n c
ou
nts
Formalisms
Evaluation
MT 1
Parsing
MT 2
Semantics
WSD
Discourse
Formalisms
Evaluation
MT 1
Parsing
MT 2
Semantics
WSD
Discourse
Figure 3: Heat map showing citation rates across selected
factions. Factions on the horizontal axis are being cited;
factions on the vertical axis are citing. Darker shades de-
note higher average
?(g?h)
Ng
.
parsing algorithms in translation. Moreover, we can
observe that ?deeper? linguistics research, such as
semantics and discourse, are less likely to be cited
by the other factions. This is reflected in Figure 3,
where the statistical MT and parsing factions in the
bottom left exhibit higher citation activity amongst
each other. In addition, we note that factions tend to
self-cite more often than out of their own factions;
this is unsurprising given the prior we selected.
The IFC between discourse and MT2 (as shown
by the edge thickness in figure 2) is higher than ex-
pected, given our prior knowledge of the computa-
tional linguistics community. Further investigation
revealed that, Daniel Marcu, posited by our model
to be a member of the discourse faction, has coau-
thored numerous highly cited papers in MT in re-
cent years (Marcu and Wong, 2002). However, the
model split the translation field, which fragmented
the counts of MT related citation words. Thus,
assigning Daniel Marcu to the discourse faction,
which also has a less diverse citation vocabulary, is
more probable than assigning him to one of the MT
factions. In ?4.6, we consider a model of factions
over time to mitigate this problem.
4.5 Comparison to Graph Clustering
Work in the field of bibliometrics has largely fo-
cused on using the link structure of citation net-
works to study higher level structures. See Osareh
(1996) for a review. Popular methods include bib-
liographic coupling (Kessler, 1963), and co-citation
27
Discourse
Formalisms
MT 2
Parsing
Semantics
Word Sense
Disambiguation
?
p
a
r
s
e
,
p
a
r
s
i
n
g
,
t
r
a
i
n
i
n
g
?
m
o
d
e
l
,
a
l
g
o
r
i
t
h
m
s
,
g
r
a
m
m
a
r
?
a
l
i
g
n
m
e
n
t
,
g
i
z
a
,
u
s
i
n
g
,
m
o
d
e
l
?
p
a
r
s
i
n
g
,
p
a
r
s
e
r
,
p
e
r
c
e
p
-
t
r
o
n
,
h
m
m
,
d
e
p
e
n
d
e
n
c
y
?alignment, giza, training
?phrase, model, joint,
translation, probability
?parsing
?parsing
?
p
r
e
f
e
r
e
n
c
e
s
,
s
e
n
s
e
,
w
o
r
d
-
n
e
t
,
a
c
q
u
i
r
e
d
,
s
e
m
c
o
r
?
s
e
n
s
e
,
s
e
m
a
n
t
i
c
,
l
e
x
i
c
a
l
,
w
o
r
d
n
e
t
,
d
i
s
a
m
b
i
g
u
a
t
i
o
n
?using, alignment,
giza, translation, model
?memory, judges,
voice, allow, sequences
?
t
a
g
s
,
l
e
x
i
c
a
l
i
z
e
d
,
g
r
a
m
-
m
a
r
s
,
a
d
j
o
i
n
i
n
g
,
t
r
e
e
s
?
t
a
g
s
,
g
r
a
m
m
a
r
s
,
l
e
x
i
c
a
l
-
i
z
e
d
,
s
y
n
c
h
r
o
n
o
u
s
,
f
o
r
m
a
l
i
s
m
?
s
u
p
e
r
s
e
n
s
e
,
r
e
s
u
l
t
s
,
w
o
r
d
n
e
t
,
p
a
r
s
i
n
g
,
p
e
r
c
e
p
t
r
o
n
?
t
a
s
k
,
i
n
f
o
r
m
a
t
i
o
n
Figure 2: Citations among some factions. The size of a node is relative to the faction size and edge thickness is relative
to the average number of inter-faction citations (equation 1). The words on the edges are the highest weighted words
from the deviation vectors ?, with the arrow denoting the direction of the citation. Edges with below average IFC
scores are represented as dashed lines, and their citations words are not shown to preserve readability.
analysis (Small, 1973). By using authors as an unit
of analysis in co-citation pairs, author co-citations
have been presented as a technique to analyze their
subject specialties (White and Griffith, 1981). Using
standard graph clustering algorithms on these author
co-citation networks, one can obtain a semblance of
author factions. Hence, we performed graph clus-
tering on both collaboration and citation graphs
5
of
authors in our dataset using Graclus
6
, a graph clus-
tering implementation based on normalized cuts and
ratio associations (Dhillon et al, 2004).
In Table 3, we compare, for selected authors,
how their faction-mates obtained by our model and
graph clustering differ. When clustering on the au-
thor collaboration network, we obtained some clus-
ters easily identified with research labs (e.g., Daniel
Marcu at the Information Sciences Institute). The
co-citation graph leads to groupings dominated by
5
We converted the directed citation graph into a symmetric
graph by performing bibliometric symmetrization described in
Satuluri and Parthasarathy (2011, section 3.3).
6http://www.cs.utexas.edu/users/dml/
Software/graclus.html
heavily co-cited papers in major research areas.
While we do not have an objective measurement
of quality or usefulness, we believe that the fac-
tions identified by our model align somewhat bet-
ter with familiar technical themes around which
sub-communities naturally form than major research
problems or institutions.
4.6 Factions over Time
Faction alignments may be dynamic; we expect that,
over time, individual researchers may move from
one faction to another as their interests evolve. We
consider a slightly modified model whereby authors
are split into different copies of themselves during a
non-overlapping set of discrete time periods. Given
a set of disjoint time periods T , we denote each
author-faction node by {a
(i,t)
| (i, t) ? A? T}. As
we treat each ?incarnation? of an author as a distinct
individual, we can simply use the same inference al-
gorithm described in ?2. (In future work we might
impose an expectation of gradual changes along a
more continuous representation of time.)
28
Our Model Collaboration Network Co-citation Network
Franz Josef Och
Franz Josef Och, Hermann Ney,
Mitchell P. Marcus, David Chiang,
Dekai Wu
Franz Josef Och, Hermann Ney, Richard
Zens, Stephan Vogel, Nicola Ueffing
Franz Josef Och, Hermann Ney, Vincent
J. Della Pietra, Daniel Marcu, Robert L.
Mercer
error, giza, rate, alignment, training giza, mert, popovic, moses, alignments giza, bleu, phrasebased, alignment, mert
Daniel Marcu
Daniel Marcu, Aravind K. Joshi, Bar-
bara J. Grosz, Marilyn A. Walker, Bon-
nie Lynn Webber
Daniel Marcu, Kevin Knight, Daniel
Gildea, David Chiang, Liang Huang
Franz Josef Och, Hermann Ney, Vincent
J. Della Pietra, Daniel Marcu, Robert L.
Mercer
discourse, phrasebased, centering, tag,
focus
phrasebased, forest, cube, spmt, hiero giza, bleu, phrasebased, alignment, mert
Michael John Collins
Eugene Charniak, Michael John Collins,
Mark Johnson, Stephen Clark, Massim-
iliano Ciaramita
Michael John Collins, Joakim Nivre,
Llu??s M?arquez, Xavier Carreras, Jan
Haji?c
Michael John Collins, Christopher D.
Manning, Dan Klein, Eugene Charniak,
Mark Johnson
parser, perceptron, supersense, parsing,
dependency
pseudoprojective, maltparser, percep-
tron, malt, averaged
tnt, prototypedriven, perceptron,
coarsetofine, pcfg
Kathleen R. McKeown
Mirella Lapata, Janyce Wiebe, Kathleen
R. McKeown, Dan Roth, Ralph Grish-
man
Kathleen R. McKeown, Regina Barzi-
lay, Owen Rambow, Marilyn A. Walker,
Srinivas Bangalore
Kenneth Ward Church, David
Yarowsky, Eduard Hovy, Kathleen
R. McKeown, Lillian Lee
semantic, work, learning, corpus, model centering, arabic, pyramid, realpro, cue rouge, minipar, nltk, alignment, mon-
treal
Table 3: Comparing selected factions between our model and graph clustering algorithms. Authors with highest
incoming citations are shown. For our model, we show the largest weighted words in the SAGE vector of incoming
citations for the faction, while for graph clustering, we show words with the highest tf-idf weight.
We split the same data as the earlier sections into
four disjoint time periods, 1965?1989, 1990?1999,
2000?2005 and 2006?2011. The split across time
is unequal due to the number of papers published in
each period: these four periods include 1,917, 3,874,
3,786, and 8,105 papers, respectively. Here we used
G = 20 factions for faster runtime, leading to di-
minished interpretability, though the sparsity of the
deviation vectors mitigates this problem somewhat.
Figure 4 shows graphical plots of selected authors
and their faction membership posteriors over time
(drawn from the final E-step).
With a simple extension of the original model,
we can learn shifts in the subject area the author is
publishing about. Consider Eugene Charniak: the
model observed a major change in faction align-
ment around 2000, when one of the popular Char-
niak parsers (Charniak, 2000) was released; this is
somewhat later than Charniak?s interests shifted, and
the earlier faction?s words are not clearly an ac-
curate description of his work at that time. More
fine-grained modeling of time and also accounting
for the death and birth of factions might ameliorate
these inconsistencies with our background knowl-
edge about Charniak. The model finds that Ar-
avind Joshi was associated with the tagging/parsing
faction in the 1990s and in recent years moved
back towards discourse (Prasad et al, 2008). David
Yarowsky, known for his early work on word sense
disambiguation, has since focused on applying word
sense disambiguation techniques in a multilingual
context (Garera et al, 2009; Bergsma et al, 2011).
As mentioned in the previous section, we observe
that the extended model is able to capture Daniel
Marcu?s shift from discourse-related work to MT
with his work in phrase-based statistical MT (Marcu
and Wong, 2002).
5 Related Work
A number of algorithms use topic modeling to an-
alyze the text in the articles. Topic models such
as latent Dirichlet alocation (Blei et al, 2003) and
its variations have been increasingly used to study
trends in scientific literature (McCallum et al, 2006;
Dietz et al, 2007; Hall et al, 2008; Gerrish and Blei,
2010), predict citation information (McNee et al,
29
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Eugene Charniak
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Aravind K. Joshi
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Daniel Marcu
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
David Y owsky
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Kathleen R. McKeown
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Michael J. Collins
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Martha Palmer
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Daniel Jurafsky
parser, parsing, stylistic, treebank, reduction
sense, npcomplete, inducing, wsd, unsupervised
building, annotated, discourse, treebank, kappa
cotraining, scalable, moses, open, implemen
framenet, roles, variation, semantic, propbank
moses, meteor, open, bbn, discovery
bleu, automatic, method, rouge, eval
pcfg, temporal, logic, linguistic, noun
bengston, shallow, conll, learning, kernel
multitext, linking, alignment, competitive, bilingu
phrasebased, forest, joint, hierarchical, kbest
whats, moses, open, rule, source, syntaxbased
human, metric, spade, evaluation, metrics
distributional, rasp, similarity, clustering, deep
tagger, pos, entropy, partofspeech, mathematics
propbank, labelled, dependency, lfg, correlation
vari, perceptron, ccg, counts, connectives
dependency, parser, proc, parse, parsing
contrastive, minimize, synchron, anneal, logist
giza, lins, minipar, error, alignment
Figure 4: Posterior probability of faction alignment over time periods for eight researchers with significant publication
records in at least three periods. The key for each entry contains the five highest weighted words in the deviation
vectors for the faction?s incoming citations. For each author, we show factions with which he or she is associated with
probability > 0.1 in at least one time period.
2002; Ib?a?nez et al, 2009; Nallapati et al, 2008) and
analyze authorship (Rosen-Zvi et al, 2004; Johri et
al., 2011).
Assigning author factions can be seen as network
classification problem, where the goal is to label
nodes in a network such that there is (i) a corre-
lation between a node?s label and its observed at-
tributes and (ii) a correlation between labels of in-
terconnected nodes (Sen et al, 2008). Such collec-
tive network-based approaches have been used on
scientific literature to classify papers/web pages into
its subject categories (Kubica et al, 2002; Getoor,
2005; Angelova and Weikum, 2006). If we knew
the word distributions between factions beforehand,
learning the author factions in our model would be
equivalent to the network classification task, where
our edge weights are proportional to the probability
of coauthorship multiplied by the probability of ob-
serving the citation words given the author?s faction
labels.
6 Conclusion
In this work, we have defined factions in terms of
how authors talk about each other?s work, going be-
yond co-authorship and citation graph representa-
tions of a research community. We take a first step
toward computationally modeling faction formation
by using a latent author faction model and applied
it to the ACL community, revealing both factions
and how they cite each other. We also extended the
model to capture authors? faction changes over time.
30
Acknowledgments
The authors thank members of the ARK group and the
anonymous reviewers for helpful feedback. We gratefully
acknowledge technical assistance from Matthew Fiorillo.
This research was supported in part by an A
?
STAR fel-
lowship to Y. Sim, NSF grant IIS-0915187 to N. Smith,
and the Center for Intelligent Information Retrieval and
NSF grant IIS-0910884 for D. Smith.
References
G. Andrew and J. Gao. 2007. Scalable training of L
1
-
regularized log-linear models. In Proc. of ICML.
R. Angelova and G. Weikum. 2006. Graph-based text
classification: learn from your neighbors. In Proc. of
SIGIR.
S. Bergsma, D. Yarowsky, and K. Church. 2011. Using
large monolingual and bilingual corpora to improve
coordination disambiguation. In Proc. of ACL.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
I. S. Dhillon, Y. Guan, and B. Kulis. 2004. Kernel k-
means: spectral clustering and normalized cuts. In
Proc. of KDD.
L. Dietz, S. Bickel, and T. Scheffer. 2007. Unsupervised
prediction of citation influences. In Proc. of ICML.
J. Eisenstein, A. Ahmed, and E. P. Xing. 2011. Sparse
additive generative models of text. In Proc. of ICML.
M. A. T. Figueiredo. 2003. Adaptive sparseness for
supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 25(9):1150?1159.
N. Garera, C. Callison-Burch, and D. Yarowsky. 2009.
Improving translation lexicon induction from mono-
lingual corpora via dependency contexts and part-of-
speech equivalences. In Proc. of CoNLL.
S. Gerrish and D. M. Blei. 2010. A language-based
approach to measuring scholarly impact. In Proc. of
ICML.
L. Getoor. 2005. Link-based classification. In Ad-
vanced Methods for Knowledge Discovery from Com-
plex Data, pages 189?207. Springer.
D. Hall, D. Jurafsky, and C. D. Manning. 2008. Studying
the history of ideas using topic models. In Proc. of
EMNLP.
A. Ib?a?nez, P. Larra?naga, and C. Bielza. 2009. Predict-
ing citation count of bioinformatics papers within four
years of publication. Bioinformatics, 25(24):3303?
3309.
N. Johri, D. Ramage, D. A. McFarland, and D. Juraf-
sky. 2011. A study of academic collaborations in
computational linguistics using a latent mixture of au-
thors model. In Proc. of the ACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities.
M. M. Kessler. 1963. Bibliographic coupling between
scientific papers. American documentation, 14(1):10?
25.
J. Kubica, A. Moore, J. Schneider, and Y. Yang. 2002.
Stochastic link and group detection. In Proc. of AAAI.
K. Lange and J. S. Sinsheimer. 1993. Nor-
mal/independent distributions and their applications
in robust regression. Journal of Computational and
Graphical Statistics, 2(2):175?198.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP.
A. McCallum, G. S. Mann, and D. Mimno. 2006. Biblio-
metric impact measures leveraging topic analysis. In
Proc. of JCDL.
S. M. McNee, I. Albert, D. Cosley, P. Gopalkrishnan,
S. K. Lam, A. M. Rashid, J. A. Konstan, and J. Riedl.
2002. On the recommending of citations for research
papers. In Proc. of CSCW.
T. P. Minka. 2009. Estimating a Dirich-
let distribution. Available online at http:
//research.microsoft.com/en-us/
um/people/minka/papers/dirichlet/
minka-dirichlet.pdf.
R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. Cohen.
2008. Joint latent topic models for text and citations.
In Proc. of KDD.
F. Osareh. 1996. Bibliometrics, citation analysis and
co-citation analysis: A review of literature I. Libri,
46(3):149?158.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The Penn discourse
treebank 2.0. In Proc. of LREC.
D. R. Radev, M. T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009a. A bibliometric and network analysis of
the field of computational linguistics. Journal of the
American Society for Information Science and Tech-
nology.
D. R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009b.
The ACL Anthology Network corpus. In Proceed-
ings of the Workshop on Text and Citation Analysis for
Scholarly Digital Libraries.
F. Radicchi, C. Castellano, F. Cecconi, V. Loreto,
D. Parisi, and G. Parisi. 2004. Defining and iden-
tifying communities in networks. Proceedings of the
National Academy of Sciences of the United States of
America, 101(9):2658?2663.
31
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and docu-
ments. In Proc. of UAI.
V. Satuluri and S. Parthasarathy. 2011. Symmetrizations
for clustering directed graphs. In Proc. of Interna-
tional Conference on Extending Database Technology.
P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher,
and T. Eliassi-Rad. 2008. Collective classification in
network data. AI magazine, 29(3):93.
H. Small. 1973. Co-citation in the scientific literature:
A new measure of the relationship between two docu-
ments. Journal of the American Society for informa-
tion Science, 24(4):265?269.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), 58(1):267?288.
G. C. G. Wei and M. A. Tanner. 1990. A Monte Carlo
implementation of the EM algorithm and the poor
man?s data augmentation algorithms. Journal of the
American Statistical Association, 85(411):699?704.
H. D. White and B. C. Griffith. 1981. Author cocitation:
A literature measure of intellectual structure. Jour-
nal of the American Society for Information Science,
32(3):163?171.
D. Yogatama, M. Heilman, B. O?Connor, C.Dyer, B. R.
Routledge, and N. A. Smith. 2011. Predicting a sci-
entific community?s response to an article. In Proc. of
EMNLP.
32
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 252?261,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Online Polylingual Topic Models for Fast Document Translation Detection
Kriste Krstovski
School of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003
kriste@cs.umass.edu
David A. Smith
School of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003
dasmith@cs.umass.edu
Abstract
Many tasks in NLP and IR require ef-
ficient document similarity computations.
Beyond their common application to ex-
ploratory data analysis, latent variable
topic models have been used to represent
text in a low-dimensional space, indepen-
dent of vocabulary, where documents may
be compared. This paper focuses on the
task of searching a large multilingual col-
lection for pairs of documents that are
translations of each other. We present
(1) efficient, online inference for repre-
senting documents in several languages in
a common topic space and (2) fast ap-
proximations for finding near neighbors in
the probability simplex. Empirical evalu-
ations show that these methods are as ac-
curate as?and significantly faster than?
Gibbs sampling and brute-force all-pairs
search.
1 Introduction
Statistical topic models, such as latent Dirich-
let alocation (LDA) (Blei et al, 2003), have
proven to be highly effective at discovering hid-
den structure in document collections (Hall et al,
2008, e.g.). Often, these models facilitate ex-
ploratory data analysis, by revealing which col-
locations of terms are favored in different kinds
of documents or which terms and topics rise and
fall over time (Blei and Lafferty, 2006; Wang and
McCallum, 2006). One of the greatest advan-
tages in using topic models to analyze and process
large document collections is their ability to rep-
resent documents as probability distributions over
a small number of topics, thereby mapping doc-
uments into a low-dimensional latent space?the
T -dimensional probability simplex, where T is the
number of topics. A document, represented by
some point in this simplex, is said to have a par-
ticular ?topic distribution?.
Representing documents as points in a low-
dimensional shared latent space abstracts away
from the specific words used in each document,
thereby facilitating the analysis of relationships
between documents written using different vocab-
ularies. For instance, topic models have been used
to identify scientific communities working on re-
lated problems in different disciplines, e.g., work
on cancer funded by multiple Institutes within the
NIH (Talley et al, 2011). While vocabulary mis-
match occurs within the realm of one language,
naturally this mismatch occurs across different
languages. Therefore, mapping documents in dif-
ferent languages into a common latent topic space
can be of great benefit when detecting document
translation pairs (Mimno et al, 2009; Platt et al,
2010). Aside from the benefits that it offers in the
task of detecting document translation pairs, topic
models offer potential benefits to the task of creat-
ing translation lexica, aligning passages, etc.
The process of discovering relationship be-
tween documents using topic models involves: (1)
representing documents in the latent space by in-
ferring their topic distributions and (2) comparing
pairs of topic distributions to find close matches.
Many widely used techniques do not scale ef-
ficiently, however, as the size of the document
collection grows. Posterior inference by Gibbs
sampling, for instance, may make thousands of
passes through the data. For the task of comparing
topic distributions, recent work has also resorted
to comparing all pairs of documents (Talley et al,
2011).
This paper presents efficient methods for both
252
of these steps and performs empirical evaluations
on the task of detected translated document pairs
embedded in a large multilingual corpus. Unlike
some more exploratory applications of topic mod-
els, translation detection is easy to evaluate. The
need for bilingual training data in many language
pairs and domains also makes it attractive to mit-
igate the quadratic runtime of brute force transla-
tion detection. We begin in ?2 by extending the
online variational Bayes approach of Hoffman et
al. (2010) to polylingual topic models (Mimno et
al., 2009). Then, in ?3, we build on prior work
on efficient approximations to the nearest neighbor
problem by presenting theoretical and empirical
evidence for applicability to topic distributions in
the probability simplex and in ?4, we evaluate the
combination of online variational Bayes and ap-
proximate nearest neighbor methods on the trans-
lation detection task.
2 Online Variational Bayes for
Polylingual Topic Models
Hierarchical generative Bayesian models, such as
topic models, have proven to be very effective
for modeling document collections and discover-
ing underlying latent semantic structures. Most
current topic models are based on Latent Dirich-
let Allocation (LDA) (Blei et al, 2003). In some
early work on the subject, Blei and Jordan (2003)
showed the usefulness of LDA on the task of auto-
matic annotation of images. Hall et al (2008) used
LDA to analyze historical trends in the scientific
literature; Wei and Croft (2006) showed improve-
ments on an information retrieval task. More re-
cently Eisenstein et al (2010) modeled geographic
linguistic variation using Twitter data.
Aside from their widespread use on monolin-
gual text, topic models have also been used to
model multilingual data (Boyd-Graber and Blei,
2009; Platt et al, 2010; Jagarlamudi and Daume?,
2010; Fukumasu et al, 2012), to name a few.
In this paper, we focus on the Polylingual Topic
Model, introduced by Mimno et al (2009). Given
a multilingual set of aligned documents, the PLTM
assumes that across an aligned multilingual doc-
ument tuple, there exists a single, tuple-specific,
distribution across topics. In addition, PLTM as-
sumes that for each language?topic pair, there ex-
ists a distribution over words in that language ?l.
As such, PLTM assumes that the multilingual cor-
pus is created through a generative process where
D T
T
...
D
wz
N1
wz
NL
...
1E
LE
1K
LK
Figure 1: Polylingual topic model (PLTM)
first a document tuple is generated by drawing a
tuple-specific distribution over topics ?1 which, as
it is the case with LDA, is drawn from a Dirich-
let prior ? ? Dir (?) . For each of the languages
l in the tuple and for each of the N words wln in
the document the generative process: first chooses
a topic assignment zln ?Multinomial (?) which
is then followed by choosing a word wln from a
multinomial distribution conditioned on the topic
assignment and the language specific topics distri-
bution over words ?l?Dir (?l). Both? and ?1,...,L
are symmetric priors, i.e. the priors are exchange-
able Dirichlet distributions. Finally, each word
is generated from a language- and topic-specific
multinomial distribution ?lt as selected by the topic
assignment variable zln:
wln ? p
(
wln | zln, ?ln
)
(1)
Figure 1 shows a graphical representation of
the PLTM using plate notation. In their original
work Mimno et al (2009) used the Gibbs sam-
pling approach as a posterior inference algorithm
to assign topics distributions over their test collec-
tion. While more straightforward to implement,
this sampling approach is inherently slow when
applied to large collections which makes the orig-
inal PLTM work practically infeasible to be used
on real-world data sets.
In general, performing posterior inference over
the latent variables of a Bayesian model is usu-
ally done with two of the three approximate ap-
proaches, Gibbs sampling, variational Bayes (VB)
and expectation-propagation. While Gibbs Sam-
pling is a variation of Markov Chain Monte Carlo
method (MCMC) which generates a sample from
the true posterior after converging to a stationary
1In the traditional LDA model ? is used to specify the
document specific distribution over topics.
253
distribution; in VB, a set of free variational param-
eters characterizes a simpler family of probabil-
ity distributions. These variational parameters are
then optimized by finding the minimum Kullback-
Leibler (KL) divergence between the variational
distribution q (?, z, ?|?, ?, ?) and the true pos-
terior P (?, z, ?|w,?, ?). From an algorithmic
perspective, the variational Bayes approach fol-
lows the Expectation-Maximization (EM) proce-
dure where for a given document, the E-step up-
dates the per document variational parameters ?d
and ?d while holding the per words-topic distribu-
tion parameter ? fixed. It then updates the vari-
ational parameter ? using the sufficient statistics
computed in the E step. In order to converge to
a stationary point, both approaches require going
over the whole collection multiple times which
makes their time complexity to grown linearly
with the size of the data collection. The mere fact
that they require continuous access to the whole
collection makes both inference approaches im-
practicable to use on very large or streaming col-
lections. To alleviate this problem, several algo-
rithms have been proposed that draws from belief
propagation (Zeng et al, 2012), the Gibbs sam-
pling approach such as (Canini et al, 2009), vari-
ational Bayes (Hoffman et al, 2010) as well as
a combination of the latter two (Hoffman et al,
2012) to name a few. In this paper we use Hoff-
man et al (2010) approach. Hoffman et al (2010)
proposed a new inference approach called Online
LDA which relies on the stochastic gradient de-
scent to optimize the variational parameters. This
approach can produce good estimates of LDA pos-
teriors in a single pass over the whole collection.
2.1 Algorithmic Implementation
We now derive an online variational Bayes algo-
rithm for PLTM to infer topic distributions over
multilingual collections. Figure 2 shows the vari-
ational model and free parameters used in our ap-
proach. As in the case of Hoffman et al (2010),
our algorithm updates the variational parameters
?ld and ?ld on each batch of documents while the
variational parameter ? is computed as a weighted
average of the value on the previous batch and its
approximate version ??. Averaging is performed
using a decay function whose parameters control
the rate at which old values of ?l are forgotten.
Within the E step of the VB approach, we com-
pute the updates over the variational parameter ?l
T
. . .
D
T z
N1
z
NL
. . .
J 1I LI
1E LE
1O LO
Figure 2: Graphical model representation of the
free variational parameters for the online varia-
tional Bayes approximation of the PLTM posterior
for each language L present in our document tuple
while the update on the ? parameter accumulates
the language specific sufficient statistics:
?mk = ?+
?
l
?
w
?mlwk nmlw (2)
We detail these steps in Algorithm 1.
2.2 Performance Analysis
To demonstrate the efficacy of online PLTM, we
ran topic inference on a subset of the English-
Spanish Europarl collection consisting of ?64k
parallel speeches and compared the accuracy re-
sults vs. the training and inference speed against
the original PLTM model using topic sets of
T=50,100, 200 and 500. We explain in details
the evaluation task and the performance metric
used in ?4. Shown in Figure 3 are the results of
these comparisons. Our speed measurements were
performed on Xeon quad processors with a clock
speed of 2.66GHz and a total of 16GB of memory.
As we increase the number of topics we gain in
accuracy over the evaluation task across both in-
ference approaches. When we increase the num-
ber of topics from 50 to 500 the speed improve-
ment obtained by Online VB PLTM drops by a
factor of 2.9 within the training step and by a
factor of 4.45 in the test step. Our total running
time for the Online VB PLTM with T=500 ap-
proaches the running time of the Gibbs sampling
approach with T=50. The gradual drop in speed
improvement with the increase of the number top-
ics is mostly attributed to the commutation of the
254
Algorithm 1 Online variational Bayes for PLTM
initialize ?l randomly
obtain the tth mini-batch of tuples Mt
for t = 1 to ? do
?t ?
(
1
t0+t
)?
E step:
initialize ?t randomly
for each document tuple in mini-batch t
for m in Mt do
repeat
for l ? 1, . . . ,L do
?mlwk ?
exp {Eq [log ?mk ]} ?
exp
{
Eq
[
log ?mlkw
]}
end for
?mk = ?+
?
l
?
w ?mlwk nmlwuntil convergence
end for
M step:
for l ? 1, . . . ,L do
??lkw = ? +D
?
m ?mlwknmlw
?ltkw ? (1? ?t)?
l(t?1)
kw + ?t??lkwend for
end for
0 2000 4000 6000 8000 10000 12000
0
10
20
30
40
50
60
70
80
90
100
A
cc
ur
ac
y 
[%
 @
 R
an
k 1
.]
Running time [sec]
Accuracy vs. Running time
 
 
Gibbs sampling
Online VB
T=50
T=100
T=200 T=500 T=500T=200T=100
T=50
Figure 3: Speed vs. accuracy comparison between
Online VB PLTM and Gibbs Sampling PLTM at
T=50,100, 200 and 500. We used a Python imple-
mentation of Online VB and Mallet?s Java imple-
mentation of PLTM with in-memory Gibbs Sam-
pling using 1000 iterations.
0 50 100 250 500 750 1,000
0
20,000
40,000
60,000
80,000
100,000
120,000
140,000
160,000
180,000
200,000
Collection size [k]
Tr
ai
ni
ng
 ti
m
e 
[se
c]
Collection size vs. training time
 
 
Gibbs sampling T=50
Online VB T=50
Gibbs sampling T=500
Online VB T=500
Figure 4: Collection size vs. training time compar-
ison between Online VB PLTM and Gibbs Sam-
pling PLTM using multilingual collections of 50k,
100k, 250k, 500k, 750k and 1M speech pairs.
digamma function (Asuncion et al, 2009) whose
time complexity increases linearly with the num-
ber of topics.
While a multilingual collection of ?64k docu-
ment pairs is considered relatively big, our goal
of deriving the Online VB PLTM approach was to
be able to utilize PLTM on very large multilingual
collections. To analyze the potential of using On-
line VB PLTM on such collections we ran speed
comparisons within the training step by creating
multilingual collections of different lengths multi-
plying the original English-Spanish Europarl col-
lection. Speed comparisons using collections of
length 50K, 100K, 250K, 500K, 750K and 1M are
shown in Figure 4. Training was performed with
the number of topics T set to T=50 and T=500.
As we increase the collection size we observe
the real benefit of using Online VB compared to
Gibbs sampling. This is mostly attributed to the
fact that the Gibbs sampling approach requires
multiple iterations over the whole collection in or-
der to achieve a convergence point. For collec-
tion sizes of 50k and 100k the training time for
the Online VB PLTM with T=500 approaches the
training time of Gibbs sampling with T=50 and as
we increase the collection size this proximity dis-
sipates.
In Figure 5 we show a sample set of the aligned
topics extracted using Online VB PLTM with
T=400 on the English-Spanish Europarl collec-
tion. For a given topic tuple words are ordered
based on probability of occurrence within the
given topic.
255
	




	



	

Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 50?57,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
Detecting and Evaluating Local Text Reuse in Social Networks
Shaobin Xu
*
, David A. Smith
*
, Abigail Mullen
?
, and Ryan Cordell
?
NULab for Texts, Maps, and Networks
College of Computer and Information Science
*
, Department of History
?
, Department of English
?
Northeastern University, Boston, MA
Abstract
Texts propagate among participants in
many social networks and provide evi-
dence for network structure. We describe
intrinsic and extrinsic evaluations for algo-
rithms that detect clusters of reused pas-
sages embedded within longer documents
in large collections. We explore applica-
tions of these approaches to two case stud-
ies: the culture of free reprinting in the
nineteenth-century United States and the
use of similar language in the public state-
ments of U.S. members of Congress.
1 Introduction
While many studies of social networks use surveys
and direct observation to catalogue actors (nodes)
and their interactions (edges), we often cannot di-
rectly observe network links. Instead, we might
observe behavior by network participants that pro-
vides indirect evidence for social ties.
One revealing form of shared behavior is the
reuse of text by different social actors. Meth-
ods to uncover invisible links among sources of
text methods would have broad applicability be-
cause of the very general nature of the problem?
sources of text include websites, newspapers, in-
dividuals, corporations, political parties, and so
on. Further, discerning those hidden links be-
tween sources would provide more effective ways
of identifying the provenance and diverse sources
of information, and to build predictive models of
the diffusion of information.
There are substantial challenges, however, in
building a methodology to study text reuse, includ-
ing: scalable detection of reused passages; iden-
tification of appropriate statistical models of text
mutation; inference methods for characterizing
missing nodes that originate or mediate text trans-
mission; link inference conditioned on textual
topics; and the development of testbeds through
which predictions of the resulting models might
be validated against some broader understanding
of the processes of transmission.
In this paper, we sketch relevant features of our
two testbed collections (?2) and then describe ini-
tial progress on developing algorithms for detect-
ing reused passages embedded within the larger
text output of social network nodes (?3). We then
describe an intrinsic evaluation of the efficiency of
these techniques for scaling up text reuse detec-
tion (?4). Finally, we perform an extrinsic evalua-
tion of the network links inferred from text reuse
by correlating them with side information about
the underlying social networks (?5). A prelimi-
nary version of the text reuse detection system was
presented for a single, smaller corpus in (Anony-
mous, 2013), but without the extrinsic or much of
the intrinsic evaluation and without data on the un-
derlying networks.
2 Case Studies in Text Reuse
The case studies in this paper, which form the
basis for our experimental evaluations below, in-
volve two fairly divergent domains: the infor-
mational and literary ecology of the nineteenth-
century United States and of twenty-first century
U.S. legislators.
2.1 Tracking Viral Texts in 19c Newspapers
In American Literature and the Culture of Reprint-
ing, McGill (2003) argues that American literary
culture in the nineteenth century was shaped by the
widespread practice of reprinting stories and po-
ems, usually without authorial permission or even
50
knowledge, in newspapers, magazines, and books.
Without substantial copyright enforcement, texts
circulated promiscuously through the print market
and were often revised by editors during the pro-
cess. These ?viral? texts?be they news stories,
short fiction, or poetry?are much more than his-
torical curiosities. The texts that editors chose to
pass on are useful barometers of what was excit-
ing or important to readers during the period, and
thus offer significant insight into the priorities and
concerns of the culture.
Nineteenth-century U.S. newspapers were usu-
ally associated with a particular political party, re-
ligious denomination, or social cause (e.g., tem-
perance or abolition). Mapping the specific lo-
cations and venues in which varied texts circu-
lated would therefore allow us to answer ques-
tions about how reprinting and the public sphere in
general were affected by geography, communica-
tion and transportation networks, and social, polit-
ical, and religious affinities. These effects should
be particularly observable in the period before the
Civil War and the rise of wire services that broad-
cast content at industrial scales (Figure 1).
To study the reprint culture of this period, we
crawled the online newspaper archives of the Li-
brary of Congress?s Chronicling America project
(chroniclingamerica.loc.gov). Since
the Chronicling America project aggregates state-
level digitization efforts, there are some significant
gaps: e.g., there are no newspapers from Mas-
sachusetts, which played a not insubstantial role
in the literary culture of the period. While we con-
tinue to collect data from other sources in order to
improve our network analysis, the current dataset
remains a useful, and open, testbed for text reuse
detection and analysis of overall trends. For the
pre-Civil War period, this corpus contains 1.6 bil-
lion words from 41,829 issues of 132 newspapers.
Another difficulty with this collection is that it
consists of the OCR?d text of newspaper issues
without any marking of article breaks, headlines,
or other structure. The local alignment methods
described in ?3 are designed not only to mitigate
this problem, but also to deal with partial reprint-
ing. One newspaper issue, for instance, might
reprint chapters 4 and 5 of a Thackeray novel
while another issue prints only chapter 5.
Since our goal is to detect texts that spread from
one venue to another, we are not interested in texts
that were reprinted frequently in the same newspa-
Figure 1: Newspaper issues mentioning ?associ-
ated press? by year, from the Chronicling America
corpus. The black regression line fits the raw num-
ber of issues; the red line fits counts corrected for
the number of times the Associated Press is men-
tioned in each issue.
per, or series, to use the cataloguing term. This in-
cludes material such as mastheads and manifestos
and also the large number of advertisements that
recur week after week in the same newspaper.
2.2 Statements by Members of Congress
Members of the U.S. Congress are of course even
more responsive to political debates and incentives
than nineteenth-century newspapers. Representa-
tives and senators are also a very well-studied so-
cial network. Following Margolin et al. (2013),
we analyzed a dataset of more than 400,000 pub-
lic statements made by members of the 112th Sen-
ate and House between January 2011 and August
2012. The statements were downloaded from the
Vote Smart Project website (votesmart.com).
According to Vote Smart, the Members? pub-
lic statements include any press releases, state-
ments, newspaper articles, interviews, blog en-
tries, newsletters, legislative committee websites,
campaign websites and cable news show websites
(Meet the Press, This Week, etc.) that contain
direct quotes from the Member. Since we are
primarily interested in the connections between
Members, we will, as we see below, want to fil-
ter out reuse among different statements by the
same member. That information could be interest-
ing for other reasons?for instance, tracking slight
51
changes in the phrasing of talking points or sub-
stantive positions.
We supplemented these texts with categorical
data chambers and parties and with continuous
representations of ideology using the first dimen-
sion of the DW-NOMINATE scores (Carroll et al.,
2009).
3 Text Reuse Detection
As noted above, we are interested in detecting pas-
sages of text reuse (poems or stories; political talk-
ing points) that comprise a small fraction of the
containing documents (newspaper issues; political
speeches). Using the terminology of biological se-
quence alignment, we are interested in local align-
ments between documents. In text reuse detection
research, two primary methods are n-gram shin-
gling and locality-sensitive hashing (LSH) (Hen-
zinger, 2006). The need for local alignments
makes LSH less practical without performing a
large number of sliding-window matches.
In contrast to work on near-duplicate document
detection and to work on ?meme tracking? that
takes text between quotation marks as the unit of
reuse (Leskovec et al., 2009; Suen et al., 2013),
here the boundaries of the reused passages are not
known. Also in contrast to work on the contempo-
rary news cycle and blogosphere, we are interested
both in texts that are reprinted within a few days
and after many years. We thus cannot exclude
potentially matching documents for being far re-
moved in time. Text reuse that occurs only among
documents from the same ?source? (run of news-
papers; Member of Congress) should be excluded.
Similarly, Henzinger (2006) notes that many of the
errors in near-duplicate webpage detection arose
from false matches among documents from the
same website that shared boilerplate navigational
elements.
3.1 Efficient N-gram Indexing
The first step is to build for each n-gram feature an
inverted index of the documents where it appears.
As in other duplicate detection and text reuse ap-
plications, we are only interested in the n-grams
shared by two or more documents. The index,
therefore, does not need to contain entries for the
n-grams that occur only once. We use the two-
pass space-efficient algorithm described in Huston
et al. (2011), which, empirically, is very efficient
on large collections. In a first pass, n-grams are
hashed into a fixed number of bins. On the sec-
ond pass, n-grams that hash to bins with one oc-
cupant can be discarded; other postings are passed
through. Due to hash collisions, there may still
be a small number of singleton n-grams that reach
this stage. These singletons are filtered out as the
index is written.
In building an index of n-grams, an index of
(n-1)-grams can also provide a useful filter. No
5-gram, for example, can occur twice unless its
constituent 4-grams occur at least twice. We do
not use this optimization in our experiments; in
practice, n-gram indexing is less expensive than
the later steps.
3.2 Extracting and Ranking Candidate Pairs
Once we have an inverted index of the documents
that contain each (skip) n-gram, we use it to gen-
erate and rank document pairs that are candidates
for containing reprinted texts. Each entry, or post-
ing list, in the index may be viewed as a set of pairs
(d
i
, p
i
) that record the document identifier and po-
sition in that document of that n-gram.
Once we have a posting list of documents con-
taining each distinct n-gram, we output all pairs of
documents in each list. We suppress repeated n-
grams that appear in different issues of the same
newspaper. These repetitions often occur in edito-
rial boilerplate or advertisements, which, while in-
teresting, are outside the scope of this project. We
also suppress n-grams that generate more than
(
u
2
)
pairs, where u is a parameter.
1
These frequent n-
grams are likely to be common fixed phrases. Fil-
tering terms with high document frequency has led
to significant speed increases with small loss in ac-
curacy in other document similarity work (Elsayed
et al., 2008). We then sort the list of repeated n-
grams by document pair, which allows us to assign
a score to each pair based on the number of over-
lapping n-grams and the distinctiveness of those
n-grams. Table 1 shows the parameters for trading
off recall and precision at this stage.
3.3 Computing Local Alignments
The initial pass returns a large ranked list of can-
didate document pairs, but it ignores the order
of the n-grams as they occur in each document.
We therefore employ local alignment techniques
to find compact passages with the highest proba-
bility of matching. The goal of this alignment is
1
The filter is parameterized this way because it is applied
after removing document pairs in the same series.
52
n n-gram order
w maximum width of skip n-grams
g minimum gap of skip n-grams
u maximum distinct series in the posting list
Table 1: Parameters for text reuse detection
to increase the precision of the detected document
pairs while maintaining high recall. Due to the
high rate of OCR errors, many n-grams in match-
ing articles will contain slight differences.
Unlike some partial duplicate detection tech-
niques based on global alignment (Yalniz et al.,
2011), we cannot expect all or even most of the
articles in two newspaper issues, or the text in two
books with a shared quotation, to align. Rather,
as in some work on biological subsequence align-
ment (Gusfield, 1997), we are looking for re-
gions of high overlap embedded within sequences
that are otherwise unrelated. We therefore em-
ploy the Smith-Waterman dynamic programming
algorithm with an affine gap penalty. This use
of model-based alignment distinguishes this ap-
proach for other work, for detecting shorter quota-
tions, that greedily expands areas of n-gram over-
lap (Kolak and Schilit, 2008; Horton et al., 2010).
We do, however, prune the dynamic programming
search by forcing the alignment to go through po-
sition pairs that contain a matching n-gram from
the previous step, as long as the two n-grams are
unique in their respective texts. Even the exact
Smith-Waterman algorithm, however, is an ap-
proximation to the problem we aim to solve. If,
for instance, two separate articles from one news-
paper issue were reprinted in another newspaper
issue in the opposite order?or separated by a long
span of unrelated matter?the local alignment al-
gorithm would simply output the better-aligned ar-
ticle pair and ignore the other. Anecdotally, we
only observed this phenomenon once in the news-
paper collection, where two different parodies of
the same poem were reprinted in the same issue.
In any case, our approach can easily align differ-
ent passages in the same document to passages in
two other documents.
The dynamic program proceeds as follows. In
this paper, two documents would be treated as se-
quences of text X and Y whose individual charac-
ters are indexed as X
i
and Y
j
. Let W (X
i
, Y
j
) be
the score of aligning character X
i
to character Y
j
.
Higher scores are better. We use a scoring function
where only exact character matches get a positive
score and any other pair gets a negative score. We
also account for additional text appearing on either
X or Y . Let W
g
be the score, which is negative,
of starting a ?gap?, where one sequence includes
text not in the other. Let W
c
be the cost for con-
tinuing a gap for one more character. This ?affine
gap? model assigns a lower cost to continuing a
gap than to starting one, which has the effect of
making the gaps more contiguous. We use an as-
signment of weights fairly standard in genetic se-
quences where matching characters score 2, mis-
matched characters score -1, beginning a gap costs
-5, and continuing a gap costs -0.5. We leave for
future work the optimization of these weights for
the task of capturing shared policy ideas.
As with other dynamic programming algo-
rithms such as Levenshtein distance, the Smith-
Waterman algorithm operates by filling in a
?chart? of partial results. The chart in this case
is a set of cells indexed by the characters in X and
Y , and we initialize it as follows:
H(0, 0) = 0
H(i, 0) = E(i, 0) = W
g
+ i ?W
c
H(0, j) = F (0, j) = W
g
+ j ?W
c
The algorithm is then defined by the following re-
currence relations:
H(i, j) = max
?
?
?
?
?
?
?
0
E(i, j)
F (i, j)
H(i? 1, j ? 1) +W (X
i
, Y
j
)
E(i, j) = max
{
E(i, j ? 1) +W
c
H(i, j ? 1) +W
g
+W
c
F (i, j) = max
{
F (i? 1, j) +W
c
H(i? 1, j) +W
g
+W
c
The main entry in each cell H(i, j) represents the
score of the best alignment that terminates at po-
sition i and j in each sequence. The intermediate
quantities E and F are used for evaluating gaps.
Due to taking a max with 0, H(i, j) cannot be neg-
ative. This is what allows Smith-Waterman to ig-
nore text before and after the locally aligned sub-
strings of each input.
After completing the chart, we then find the op-
timum alignment by tracing back from the cell
with the highest cumulative value H(i, j) until a
53
cell with a value of 0 is reached. These two cells
represent the bounds of the sequence, and the over-
all SW alignment score reflects the extent to which
the characters in the sequences align and the over-
all length of the sequence.
In our implementation, we include one further
speedup: since in a previous step we identified n-
grams that are shared between the two documents,
we assume that any alignment of those documents
must include those n-grams as matches. In some
cases, this anchoring of the alignment might lead
to suboptimal SW alignment scores.
4 Intrinsic Evaluation
To evaluate the precision and recall of text reuse
detection, we create a pseudo-relevant set of doc-
ument pairs by pooling the results of several runs
with different parameter settings. For each doc-
ument pair found in the union of these runs, we
observe the length, in matching characters, of the
longest local alignment. (Using matching charac-
ter length allows us to abstract somewhat from the
precise cost matrix.) We can then observe how
many aligned passages each method retrieves that
are at least 50,000 character matches in length, at
least 20,000 character matches in length, and so
on. The candidate pairs are sorted by the number
of overlapping n-grams; we measure the pseudo-
recall at several length cutoffs. For each position
in a ranked list of document pairs, we then mea-
sure the precision: what proportion of documents
retrieved are in fact 50k, 20k, etc., in length? Since
we wish to rank documents by the length of the
aligned passages they contain, this is a reason-
able metric. One summary of these various values
is the average precision: the mean of the preci-
sion at every rank position that contains an actu-
ally relevant document pair. One of the few earlier
evaluations of local text reuse, by Seo and Croft
(2008), compared fingerprinting methods to a tri-
gram baseline. Since their corpus contained short
individual news articles, the extent of the reused
passages was evaluated qualitatively rather than by
alignment.
Figure 2 shows the average precision of differ-
ent parameter settings on the newspaper collec-
tion, ranked by the number of pairs each returns.
If the pairwise document step returns a large num-
ber of pairs, we will have to perform a large num-
ber of more costly Smith-Waterman alignments.
On this collection, a good tradeoff between space
5e+06 1e+07 2e+07 5e+07
0.0
0.1
0.2
0.3
0.4
0.5
Pairs examined
Ave
rag
e p
rec
isio
n
n2.u
100
.w10
5.g9
5
n2.u
100
.w25
.g15
n2.u
100
.w55
.g45
n4.u
100
n5.u
100
n5.u
50
n7.u
100
n7.u
50
10k
10k
10k 10k
10k
10k
10k
10k
5k
5k
5k
5k
5k
5k
5k
5k
2k
2k
2k
2k
2k
2k
2k
2k
1k
1k
1k
1k
1k
1k
1k
1k
Figure 2: Average precision for aligned passages
of different minimum length in characters. Verti-
cal red lines indicate the performance of different
parameter settings (see Table 1).
and speed is achieved by skip bigram features. In
the best case, we look at bigrams where there is a
gap of at least 95, and not more than 105, words
between the first and second terms (n=2 u=100
w=105 g=95).
While average precision is a good summary of
the quality of the ranked list at any one point,
many applications will simply be concerned with
the total recall after some fixed amount of pro-
cessing. Figure 3 also summarizes these recall re-
sults by the absolute number of document pairs
examined. From these results, it is clear the
several good settings perform well at retrieving
all reprinted passages of at least 5000 charac-
ters. Even using the pseudo-recall metric, how-
ever, even the best operating points fail in the end
to retrieve about 10% of the reprints detected by
some other setting for all documents of at least
1000 characters.
5 Extrinsic Evaluation
While political scientists, historians, and literary
scholars will, we hope, find these techniques use-
ful and perform close reading and manual analysis
on texts of interest, we would like to validate our
results without a costly annotation campaign. In
this paper, we explore the correlation of patterns of
text reuse with what is already known from other
54
1e+01 1e+03 1e+05 1e+07
0.0
0.2
0.4
0.6
0.8
1.0
Pairs examined
Rec
all
1000200050001000
0
2000
0
5000
0
Figure 3: (Pseudo-)Recall for aligned passages of
different minimum lengths in characters.
sources about the connections among Members of
Congress, newspaper editors, and so on. This idea
was inspired by Margolin et al. (2013), who used
these techniques to test rhetorical theories of ?se-
mantic organizing processes? on the congressional
statements corpus.
The approach is quite simple: measure the cor-
relation between some metric of text reuse be-
tween actors in a social network and other features
of the network links between those actors. The
metric of text reuse might be simply the number of
exact n-grams shared by the language of two au-
thors (Margolin et al., 2013); alternately, it might
be the absolute or relative length of all the aligned
passages shared by two authors or the tree distance
between them in a phylogenetic reconstruction. To
measure the correlation of a text reuse metric with
a single network, we can simply use Pearson?s cor-
relation; for more networks, we can use multivari-
ate regression. Due to, for instance, autocorrela-
tion among edges arising from a particular node,
we cannot proceed as if the weight of each edge in
the text reuse network can be compared indepen-
dently to the weight of the corresponding edges in
other networks. We therefore use nonparametric
permutation tests using the quadratic assignment
procedure (QAP) to resample several networks
with the same structure but different labels and
weights. The QAP achieves this by reordering the
rows and columns of one network?s adjacency ma-
trix according to the same permutation. The per-
muted network then has the same structure?e.g.,
degree distribution?but should no longer exhibit
the same correlations with the other network(s).
We can run QAP to generate confidence intervals
for both single (Krackhardt, 1987) and multiple
correlations (Dekker et al., 2007).
5.1 Congressional Statements
We model the connection between the log magni-
tude of reused text and the strength of ties among
Members according to whether they are in the
same chamber and how similar they are on the
first dimension of the DW-nominate ideological
scale (Carroll et al., 2009). On the left side of Ta-
ble 2 are shown the results for correlating reused
passages of certain minimum lengths (10, 16, 32
words) with these underlying features. On the
right are shown the similar results of (Margolin
et al., 2013) that simply used the exact size of
the n-gram overlap between Members? statements
for increasing values of n. The alignment anal-
ysis proposed in this paper achieves similar re-
sults when passages and n-grams are short. Our
analysis, however, achieves higher single and mul-
tiple correlations among networks are the pas-
sages grow longer. This is unsurprising since the
probability of an exact 32-gram match is much
smaller than that of a 32-word-long alignment that
might contain a few differences. In particular,
the much higher coefficients for DW-nominate at
longer aligned lengths suggests that ideological in-
fluence still dominates over similarities induced by
the procedural environment of each congressional
chamber.
5.2 Network Connections of 19c Reprints
For the antebellum newspaper corpus, we are also
interested in how political affinity correlates with
reprinting similar texts. We have also added
variables for social causes such as temperance,
women?s rights, and abolition that?while cer-
tainly not orthogonal to political commitments?
might sometimes operate independently. In addi-
tion, we also added a ?shared state? variable to ac-
count for shared political and social environments
of more limited scope. Figure 4 shows a partic-
ularly strong example of a geographic effect: the
statement of the radical abolitionist John Brown
after being condemned to death for attacking a
federal arsenal and attempting to raise a slave re-
bellion was very unlikely to be published in the
55
aligned passages of ? n words n-grams of length
10 16 32 8 16 32
First-order Pearson correlations
DW-nominate 0.26*** 0.25*** 0.23*** 0.26*** 0.22*** 0.16***
same chamber 0.05* 0.08** 0.13*** -0.05*** 0.21*** 0.10***
Regression coefficients
DW-nominate 0.72*** 0.75*** 0.74*** 1.31*** 2.67*** 0.36
same chamber 0.15** 0.27*** 0.42*** 0.20 3.14*** 0.81***
R-squared .069 .070 .073 .068 .073 .010
Table 2: Correlations between log length of aligned text and other author networks in public statements
by Members of Congress. ?p < .05, ? ? p < .01, ? ? ?p < .001
South.
Using information from the Chronicling Amer-
ica cataloguing and from other newspaper histo-
ries, we coded each of the 132 newspapers in the
corpus with these political and social affinities.
We then counted the number of reprinted passages
shared by each pair of newspapers. There is not
a deterministic relationship between the number
of pairs of newspapers sharing an affinity and the
number of reprints shared by those papers. While
our admittedly partial corpus only contains a sin-
gle pair of avowedly abolitionist papers?a radical
position at the time?those two papers shared ar-
ticles 306 times, compared for instance to the 71
stories shared among the 6 pairs of ?nativist? pa-
pers.
Table 3 shows that geographic proximity had by
far the strongest correlation with (log) reprinting
counts. Interestingly, the only political affinity to
show as strong a correlation was the Republican
party, which in this period had just been organized
and, one might suppose, was trying to control
its ?message?. The Republicans were more ge-
ographically concentrated in any case, compared
to the sectionally more diffuse Democrats. An-
other counterexample is the Whigs, the party from
which the new Republican party drew many of its
members, which also has a slight negative effect
on reprinting. The only other large coefficients
are in the complete model for smaller movements
such as nativism and abolition. It is interesting to
speculate about whether the speed or faithfulness
of reprinting?as opposed to the volume?might
be correlated with more of these variables.
6 Conclusions
We have presented techniques for detecting reused
passages embedded within the larger discourses
Figure 4: Reprints of John Brown?s 1859 speech
at his sentencing. Counties are shaded with histor-
ical population data, where available. Even taking
population differences into account, few newspa-
pers in the South printed the abolitionist?s state-
ment.
produced by actors in social networks. Some of
this shared content is as brief as partisan talking
points or lines of poetry; other reprints can en-
compass extensive legislative boilerplate or chap-
ters of novels. The longer passages are easier to
detect, with prefect pseudo-recall without exhaus-
tive scanning of the corpus. Precision-recall trade-
offs will vary with the density of text reuse and
the noise introduced by optical character recog-
nition and other features of data collection. We
then showed the feasibility of using network re-
gression to measure the correlations between con-
nections inferred from text reuse and networks de-
rived from outside information.
References
Royce Carroll, Jeff Lewis, James Lo, Nolan McCarty,
Keith Poole, and Howard Rosenthal. 2009. Measur-
56
newspaper pairs of regression w/pairs
affinity papers reprints ? 1 ? 10 ? 100
Republican 1176 134,302 0.74*** 0.73* 0.72***
Whig 1176 91,139 -0.35 -0.34 -0.35
Democrat 1081 62,609 -0.08 -0.09 -0.07
same state 672 103,057 1.12*** 1.11*** 1.13***
anti-secession 435 22,009 -0.58* -0.58 -0.60
anti-slavery 231 12,742 -0.65 -0.64 -0.60
pro-slavery 120 11,040 -0.35 -0.35 -0.27
Free-State 15 1,194 0.80 0.80
Constitutional Union 15 1,070 -0.21 -0.21
pro-secession 15 529 0.11 0.11
Free Soil 10 1,936 -0.42 -0.42
Copperhead 10 797 1.53 1.54
temperance 6 560 0.65
independent 6 186 -0.22
nativist 6 71 -1.93*
women?s rights 3 721 1.91
abolitionist 1 306 3.49**
Know-Nothing 1 25 1.33
Mormon 1 3 -1.13
R-squared ? ? .065 .063 .062
Table 3: Correlations between shared reprints between 19c newspapers and political and other affinities.
While many Whig papers became Republican, they do not completely overlap in our dataset; the identical
number of pairs is coincidental.
ing bias and uncertainty in DW-NOMINATE ideal
point estimates via the parametric bootstrap. Politi-
cal Analysis, 17(3).
David Dekker, David Krackhardt, and Tom Snijders.
2007. Sensitivity of MRQAP tests to collinear-
ity and autocorrelation conditions. Psychometrika,
72(4):563?581.
Tamer Elsayed, Jimmy Lin, and Douglas W. Oard.
2008. Pairwise document similarity in large collec-
tions with MapReduce. In ACL Short Papers.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
Monika Henzinger. 2006. Finding near-duplicate web
pages: A large-scale evaluation of algorithms. In
SIGIR.
Russell Horton, Mark Olsen, and Glenn Roe. 2010.
Something borrowed: Sequence alignment and the
identification of similar passages in large text collec-
tions. Digital Studies / Le champ num?erique, 2(1).
Samuel Huston, Alistair Moffat, and W. Bruce Croft.
2011. Efficient indexing of repeated n-grams. In
WSDM.
Okan Kolak and Bill N. Schilit. 2008. Generating links
by mining quotations. In Hypertext.
David Krackhardt. 1987. QAP partialling as a test of
spuriousness. Social Networks, 9(2):171?186.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD.
Drew Margolin, Yu-Ru Lin, and David Lazer. 2013.
Why so similar?: Identifying semantic organizing
processes in large textual corpora. SSRN.
Meredith L. McGill. 2003. American Literature and
the Culture of Reprinting, 1834?1853. U. Penn.
Press.
Jangwon Seo and W. Bruce Croft. 2008. Local text
reuse detection. In SIGIR.
Caroline Suen, Sandy Huang, Chantat Eksombatchai,
Rok Sosi?c, and Jure Leskovec. 2013. NIFTY: A
system for large scale information flow tracking and
clustering. In WWW.
Ismet Zeki Yalniz, Ethem F. Can, and R. Manmatha.
2011. Partial duplicate detection for large book col-
lections. In CIKM.
57
