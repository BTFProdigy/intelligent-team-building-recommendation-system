Proceedings of NAACL HLT 2007, Companion Volume, pages 173?176,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Effects of Word Prediction on Communication Rate for AAC
Keith Trnka, Debra Yarrington, John McCaw,
and Kathleen F. McCoy
Department of Computer and Information Sciences
University of Delaware Newark, DE 19716
trnka,yarringt,mccaw,mccoy@cis.udel.edu
Christopher Pennington
AgoraNet, Inc.
314 East Main Street, Suite 1
Newark, DE 19711
penningt@agora-net.com
Abstract
Individuals using an Augmentative and
Alternative Communication (AAC) de-
vice communicate at less than 10% of
the speed of ?traditional? speech, creat-
ing a large communication gap. In this
user study, we compare the communica-
tion rate of pseudo-impaired individuals
using two different word prediction algo-
rithms and a system without word pre-
diction. Our results show that word pre-
diction can increase AAC communication
rate and that more accurate predictions
significantly improve communication rate.
1 Introduction
Communication is a significant quality-of-life issue
for individuals with severe speech impairments. The
field of Augmentative and Alternative Communica-
tion (AAC) is concerned with mitigating commu-
nication barriers that would otherwise isolate indi-
viduals from society. Most high-tech AAC devices
provide the user with an electronic letter and word
board to input messages which are output via speech
synthesis. However, even with substantial user inter-
face optimization, communication rate is often less
than 10 words per minute (Newell et al, 1998) as
compared to about 150-200 words per minute for
unimpaired speech.
One way to improve communication rate is to de-
crease the number of keys entered to form a mes-
sage. Word prediction is an application of language
modeling to allowing the user to access words they
may be spelling at a cost of one keystroke. Many
commercial AAC devices use word prediction, such
as PRC?s PathfinderTM, Dynavox Technology?s Dy-
navox 4TM, and Saltillo?s ChatPCTM.
Although word prediction is used in AAC de-
vices, researchers have questioned whether it ac-
tually increases communication rate (Venkatagiri,
1993; Koester and Levine, 1997; Anson et al,
2004). These works note the additional cognitive
demands and cost of using word prediction in con-
junction with a letter-by-letter interface, such as the
need to shift the focus of attention to the prediction
list, the time to scan the prediction list, and the cog-
nitive effort required for making decisions about the
predicted words. Obviously the design of the par-
ticular interface (e.g., the ease of using word pre-
diction) will affect these results. In addition, these
studies used a single, simplistic method of generat-
ing predictions, and this may also be responsible for
some of their results.
In contrast, other researchers (Lesher and Hig-
ginbotham, 2005; Li and Hirst, 2005; Trnka et
al., 2006) have continued to investigate various im-
provements to language modeling for word pre-
diction in order to save the user more keystrokes.
Newer methods such as topic modeling yield sta-
tistically significant keystroke savings over previ-
ous methods. However, the question remains as to
whether improvements in prediction methods trans-
late to an enhanced communication rate. We hypoth-
esize that it will.
In this paper we study (1) whether a word pre-
diction interface increases communication rate over
173
letter-by-letter typing when a reasonable prediction
method is employed and (2) whether an advanced
word prediction method increases communication
rate over a basic word prediction method to a degree
greater than that afforded by the difference in theo-
retical keystroke savings between the two methods.
We expect that the communication rate gain due to
the better word prediction method will exceed the
gains from the poorer system. Our reasons for this
expectation has to do with not only users wasting
time scanning lists that do not contain the desired
word, but also the tendency for a user to give up on
such a system (i.e., choosing to ignore the predic-
tions) and thus missing the predicted word even if it
does appear in the list. Validating these hypotheses
will motivate continued improvements in word pre-
diction methods for increased communication rate.
The target population of our research is adult
AAC users without significant cognitive impair-
ments. Including actual AAC users in the study
poses several significant complications, perhaps the
largest of which concerns the user interface. AAC
devices vary significantly in the physical interfaces
available, in accordance with the variety of physi-
cal abilities of AAC users. This diversity has caused
different word prediction interfaces to be developed
for each physical interface. Moreover, it would be
impossible to mimic our word prediction layout in a
consistent fashion on all of the major AAC devices
used. Because of this, we conducted this pilot study
using subjects that are pseudo-impaired: the subjects
have no motor impairments but we have simulated
a motor impairment by providing an interface that
emulates the communication rate of a typical AAC
user. Future work includes the verification of the re-
sults using a smaller number of actual AAC users.
2 Approach
The purpose of the study was to measure the effects
of word prediction methods on communication rate.
To this end, the interface used for text entry was opti-
mized for ease-of-use and kept constant across trials.
Subjects were asked to enter text on a touchscreen
monitor using WivikTM, an on-screen keyboard. Be-
cause we wanted to simulate AAC users with mo-
tor impairments, we programmed a 1.5 second de-
lay between a key press and its registration in the
system. The artificial impairment gave the subjects
the same incentive to use word prediction that AAC
users face every day, whereas users with fine motor
control tend to ignore word prediction (e.g., in com-
mon word processing software). The delay slows the
input rate of our subjects down to a rate more typical
of AAC users (about 8-10 words per minute).
Seventeen adult, native speakers of English with
no visual, cognitive, or motor impairments partic-
ipated in the study. These subjects were asked to
type in three different excerpts from held-out data of
the Switchboard corpus on three different days.1 In
each of these sessions, a different prediction method
was used and the order of prediction methods was
randomized across subjects. Keystrokes and pre-
dictions were logged and then post-processed to
compute the words produced per minute, seconds
per keystroke, and keystroke savings, among other
statistics.
2.1 Independent variable: prediction methods
The independent variable in our study is the method
of text entry used: (1) letter-by-letter typing using
the Wivik keyboard with no word prediction, (2)
letter-by-letter typing augmented with word predic-
tions produced by a basic prediction method, (3)
letter-by-letter typing augmented with word predic-
tions produced by an advanced prediction method.
Basic prediction generates predictions from the
combination of a recency model of the text entered
so far in conjunction with a large word list. The
recency model is given priority in generating pre-
dictions. This model is similar to language models
used in AAC devices with the exception that many
devices use a unigram model in lieu of a word list.
Advanced prediction generates predictions on
the basis of a trigram model with backoff. A spe-
cial unigram model is used for the first word in
each sentence. This language model is constructed
from the transcribed telephone conversations of the
Switchboard corpus. If the prediction list isn?t filled
from this model?s predictions, then predictions are
selected from a recency model and then a word list,
as in the basic prediction method.
1Switchboard was chosen because our prediction models
were trained using another portion of the corpus. A copy task
was chosen for more controlled experimental conditions.
174
Adv. prediction Basic prediction No prediction
Words per minute (wpm) 8.09 5.50 5.06
Time (seconds) 1316s 1808s 2030s
Seconds per keystroke (spk) 2.92s 2.58s 2.28s
Keystroke savings (ks) 50.3% 18.2% -
Potential keystroke savings (pks) 55.2% 25.0% -
Prediction utilization (pru) 90.9% 73.3% -
Figure 1: Average statistics for each method.
3 Results
Once the data was collected, we post-processed the
logs and accumulated statistics. Average values for
each method are shown in Figure 1 and comparative
values are shown in Figure 2.
3.1 Communication rate (output rate)
The overall average words per minute and task com-
pletion time for each method is shown in Figure 1,
and Figure 2 shows comparative data for the three
methods. As hypothesized, advanced prediction was
found to be significantly faster than basic prediction
and basic prediction was found to be significantly
faster than no prediction (? = 0.01). For example,
users produced 59.9% more words per minute using
advanced prediction compared to no prediction. Ad-
vanced prediction was 44.4% faster than basic pre-
diction but basic prediction was only 10.1% faster
than no prediction.
Additionally, the relative task completion time is
shown in Figure 2. The copy tasks with advanced
prediction were completed in 64.5% of the time it
took to complete without word prediction. The trend
shown with relative task completion time reinforces
the trends shown with words per minute ? advanced
prediction offers a large speedup over no prediction
and basic prediction, but basic prediction offers a
much smaller increase over no prediction.
Our results show that basic word prediction sig-
nificantly boosts communication rate and that ad-
vanced word prediction substantially increases com-
munication rate beyond basic prediction.
3.2 Input rate (seconds per keystroke)
Figures 1 and 2 indicate that there were significant
differences (at ? = 0.01) in the methods in terms
of the rate at which keys were pressed. In partic-
ular, while overall communication rate was signif-
icantly faster with advanced prediction, users took
0.641 seconds longer for each key press from us-
ing advanced prediction compared to entry without
prediction. Similarly, users spent 0.345s longer to
enter each key using advanced as opposed to basic
prediction and basic prediction required more time
per keystroke than no prediction. The slower input
rate can be attributed to the additional demands of
searching through a prediction list and making a de-
cision about selecting a word from that list over con-
tinuing to type letters.
3.3 Keystroke savings / prediction utilization
The difference between the potential keystroke sav-
ings offered by advanced and basic prediction is sub-
stantial: 55.2% vs. 25.0%, as shown in Figure 1.
Accordingly, the actual keystroke savings that users
realized under each prediction method shows a wide
separation: 50.3% for advanced and 18.2% for ba-
sic. The keystroke savings that users of basic predic-
tion achieved seems quite a bit lower than the poten-
tial keystroke savings offered by the predictions. In
other words, the prediction utilization of basic pre-
diction was much lower than that of advanced pre-
diction. Comparative analysis shows a 17.1% im-
provement in prediction utilization from advanced
over basic prediction.
4 Discussion
The results show that communication rate increased
despite the decreased input rate due to a large reduc-
tion in the amount of input required (high keystroke
savings). In the past, researchers have noted that the
cognitive load of using word prediction was consid-
erable, so that the keystroke savings of word pre-
175
Adv. over None Adv. over Basic Basic over None
Relative task completion time 0.6451 0.7011 0.9191
Words per minute (wpm) 59.9% faster2 44.4% faster2 10.1% faster2
Seconds per keystroke (spk) 0.641s2 0.345s2 0.286s2
Prediction utilization (pru) 17.1%2
Figure 2: Average per-subject improvements. (1 Significance not tested. 2 Significant at ? = 0.01.)
diction was outweighed by the overhead of using
it. However, we have shown that despite significant
cognitive load, the reduction in keystroke savings
dominates the effect on output rate.
In contrast to earlier studies, our basic method
showed a significantly improved communication
rate over no prediction. One reason for this could
be the intuitiveness of our user interface. A second
reason could be related to the consistency of the ba-
sic prediction method. In particular, at least some
subjects using the basic prediction method learned
to scan the prediction list when the desired word was
recently used and mentioned it in the exit survey. At
other times they simply ignored the prediction list
and proceeded with letter-by-letter typing. This be-
havior would also explain why the input was sig-
nificantly slower with the advanced method over the
basic method ? users found that scanning the predic-
tion list more often was worth the added effort. This
also explains the significant difference in prediction
utilization between the methods.
The relationship between keystroke savings and
communication rate is a trend of increasing rate
enhancement with increasingly accurate prediction
methods. Improved prediction methods offer greater
potential keystroke savings to users and users see
increased keystroke savings in practice. Addition-
ally, users rely on better predictions more and thus
lose less of the potential keystroke savings offered
by the method. We expect that keystroke savings
will see substantial increases from improved poten-
tial keystroke savings until prediction utilization is
closer to 100%.
5 Conclusions
Word prediction in an experimental AAC device
with simulated AAC users significantly enhances
communication rate. The difference between an ad-
vanced and basic prediction method demonstrates
that further improvements in language modeling for
word prediction are likely to appreciably increase
communication rate. Therefore, further research in
improving word prediction is likely to have an im-
portant impact on quality-of-life for AAC users. We
plan to improve word prediction and validate these
results using AAC users as future work.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051.
References
Denis Anson, Penni Moist, Mary Przywars, Heather
Wells, Heather Saylor, and Hantz Maxime. 2004.
The effects of word completion and word prediction
on typing rates using on-screen keyboards. Assistive
Technology, 18.
Heidi Horstmann Koester and Simon P. Levine. 1997.
Keystroke-level models for user performance with
word prediction. Augmentative and Alternative Com-
munication, 13:239?257, December.
Gregory W. Lesher and D. Jeffery Higginbotham. 2005.
Using web content to enhance augmentative commu-
nication. In Proceedings of CSUN 2005.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS ?05, pages 121?
128.
Alan Newell, Stefan Langer, andMarianne Hickey. 1998.
The ro?le of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):1?16.
Keith Trnka, Debra Yarrington, Kathleen F. McCoy, and
Christopher A. Pennington. 2006. Topic modeling in
fringe word prediction for aac. In IUI ?06, pages 276?
278.
Horabail S. Venkatagiri. 1993. Efficiency of lexical
prediction as a communication acceleration technique.
Augmentative and Alternative Communication, 9:161?
167, September.
176
Proceedings of the 43rd Annual Meeting of the ACL, pages 223?230,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploring and Exploiting the Limited Utility of Captions in Recognizing
Intention in Information Graphics?
Stephanie Elzer1 and Sandra Carberry2 and Daniel Chester2 and Seniz Demir2 and
Nancy Green3 and Ingrid Zukerman4 and Keith Trnka2
1Dept. of Computer Science, Millersville University, Millersville, PA 17551
2Dept. of Computer Science, University of Delaware, Newark, DE 19716
3Dept. of Mathematical Sciences, Univ. of NC at Greensboro, Greensboro, NC 27402
4School of CS & Software Engrg, Monash Univ., Clayton, Victoria 3800 Australia
Abstract
This paper presents a corpus study that ex-
plores the extent to which captions con-
tribute to recognizing the intended mes-
sage of an information graphic. It then
presents an implemented graphic interpre-
tation system that takes into account a va-
riety of communicative signals, and an
evaluation study showing that evidence
obtained from shallow processing of the
graphic?s caption has a significant impact
on the system?s success. This work is part
of a larger project whose goal is to provide
sight-impaired users with effective access
to information graphics.
1 Introduction
Language research has posited that a speaker or
writer executes a speech act whose intended mean-
ing he expects the listener to be able to deduce, and
that the listener identifies the intended meaning by
reasoning about the observed signals and the mutual
beliefs of author and interpreter (Grice, 1969; Clark,
1996). But as noted by Clark (Clark, 1996), lan-
guage is more than just words. It is any ?signal? (or
lack of signal when one is expected), where a sig-
nal is a deliberate action that is intended to convey a
message.
Although some information graphics are only in-
tended to display data values, the overwhelming ma-
jority of the graphics that we have examined (taken
?Authors can be reached via email as fol-
lows: elzer@cs.millersville.edu, nlgreen@uncg.edu,
{carberry, chester, demir, trnka}@cis.udel.edu, In-
grid.Zukerman@infotech.monash.edu.au.
1998 1999 2000 20011000
1500
2000
2500
3000
personal filingsLocal bankruptcy
Figure 1: Graphic from a 2001 Local Newspaper
from newspaper, magazine, and web articles) ap-
pear to have some underlying goal or intended mes-
sage, such as the graphic in Figure 1 whose com-
municative goal is ostensibly to convey the sharp in-
crease in local bankruptcies in the current year com-
pared with the previous decreasing trend. Applying
Clark?s view of language, it is reasonable to presume
that the author of an information graphic expects the
viewer to deduce from the graphic the message that
the graphic was intended to convey, by reasoning
about the graphic itself, the salience of entities in
the graphic, and the graphic?s caption.
This paper adopts Clark?s view of language as any
deliberate signal that is intended to convey a mes-
sage. Section 3 investigates the kinds of signals used
in information graphics. Section 4 presents a cor-
pus study that investigates the extent to which cap-
tions capture the message of the graphic, illustrates
the issues that would arise in trying to fully under-
stand such captions, and proposes shallow process-
ing of the caption to extract evidence from it. Sec-
tion 5 then describes how evidence obtained from
a variety of communicative signals, including shal-
low processing of the graphic?s caption, is used in a
probabilistic system for hypothesizing the intended
message of the graphic. Section 6 presents an eval-
223
10
 5
15
0?680+ 65?79 7?19 35?4980+65?7950?6435?49
10
 5
15
20?347?190?6 20?3450?64
(a) (b)
Figure 2: Two Alternative Graphs from the Same Data
uation showing the system?s success, with particu-
lar attention given to the impact of evidence from
shallow processing of the caption, and Section 7 dis-
cusses future work.
Although we believe that our findings are ex-
tendible to other kinds of information graphics, our
current work focuses on bar charts. This research is
part of a larger project whose goal is a natural lan-
guage system that will provide effective access to
information graphics for individuals with sight im-
pairments, by inferring the intended message under-
lying the graphic, providing an initial summary of
the graphic that includes the intended message along
with notable features of the graphic, and then re-
sponding to follow-up questions from the user.
2 Related Work
Our work is related to efforts on graph summariza-
tion. (Yu et al, 2002) used pattern recognition tech-
niques to summarize interesting features of automat-
ically generated graphs of time-series data from a
gas turbine engine. (Futrelle and Nikolakis, 1995)
developed a constraint grammar for parsing vector-
based visual displays and producing representations
of the elements comprising the display. The goal
of Futrelle?s project is to produce a graphic that
summarizes one or more graphics from a document
(Futrelle, 1999). The summary graphic might be a
simplification of a graphic or a merger of several
graphics from the document, along with an appropri-
ate summary caption. Thus the end result of summa-
rization will itself be a graphic. The long range goal
of our project, on the other hand, is to provide alter-
native access to information graphics via an initial
textual summary followed by an interactive follow-
up component for additional information. The in-
tended message of the graphic will be an important
component of the initial summary, and hypothesiz-
ing it is the goal of our current work.
3 Evidence about the Intended Message
The graphic designer has many alternative ways of
designing a graphic; different designs contain differ-
ent communicative signals and thus convey differ-
ent communicative intents. For example, consider
the two graphics in Figure 2. The graphic in Fig-
ure 2a conveys that average doctor visits per year
is U-shaped by age; it starts out high when one is
very young, decreases into middle age, and then
rises again as one ages. The graphic in Figure 2b
presents the same data; but instead of conveying a
trend, this graphic seems to convey that the elderly
and the young have the highest number of doctor vis-
its per year. These graphics illustrate how choice of
design affects the message that the graphic conveys.
Following the AutoBrief work (Kerpedjiev and
Roth, 2000) (Green et al, 2004) on generating
graphics that fulfill communicative goals, we hy-
pothesize that the designer chooses a design that best
facilitates the perceptual and cognitive tasks that
are most important to conveying his intended mes-
sage, subject to the constraints imposed by compet-
ing tasks. By perceptual tasks we mean tasks that
can be performed by simply viewing the graphic,
such as finding the top of a bar in a bar chart; by
cognitive tasks we mean tasks that are done via men-
tal computations, such as computing the difference
between two numbers.
Thus one source of evidence about the intended
message is the relative difficulty of the perceptual
tasks that the viewer would need to perform in order
to recognize the message. For example, determining
224
the entity with maximum value in a bar chart will be
easiest if the bars are arranged in ascending or de-
scending order of height. We have constructed a set
of rules, based on research by cognitive psycholo-
gists, that estimate the relative difficulty of perform-
ing different perceptual tasks; these rules have been
validated by eye-tracking experiments and are pre-
sented in (Elzer et al, 2004).
Another source of evidence is entities that have
been made salient in the graphic by some kind of fo-
cusing device, such as coloring some elements of the
graphic, annotations such as an asterisk, or an arrow
pointing to a particular location in a graphic. Enti-
ties that have been made salient suggest particular
instantiations of perceptual tasks that the viewer is
expected to perform, such as comparing the heights
of two highlighted bars in a bar chart.
And lastly, one would expect captions to help con-
vey the intended message of an information graphic.
The next section describes a corpus study that we
performed in order to explore the usefulness of cap-
tions and how we might exploit evidence from them.
4 A Corpus Study of Captions
Although one might suggest relying almost ex-
clusively on captions to interpret an information
graphic, (Corio and Lapalme, 1999) found in a cor-
pus study that captions are often very general. The
objective of their corpus study was to categorize the
kinds of information in captions so that their find-
ings could be used in forming rules for generating
graphics with captions.
Our project is instead concerned with recogniz-
ing the intended message of an information graphic.
To investigate how captions might be used in a sys-
tem for understanding information graphics, we per-
formed a corpus study in which we analyzed the
first 100 bar charts from our corpus of information
graphics; this corpus contains a variety of bar charts
from different publication venues. The following
subsections present the results of this corpus study.
4.1 Do Captions Convey the Intended
Message?
Our first investigation explored the extent to which
captions capture the intended message of an infor-
mation graphic. We extracted the first 100 graphics
Category #
Category-1: Captures intention (mostly) 34
Category-2: Captures intention (somewhat) 15
Category-3: Hints at intention 7
Category-4: No contribution to intention 44
Figure 3: Analysis of 100 Captions on Bar Charts
from our corpus of bar charts. The intended mes-
sage of each bar chart had been previously annotated
by two coders. The coders were asked to identify
1) the intended message of the graphic using a list
of 12 high-level intentions (see Section 5 for exam-
ples) and 2) the instantiation of the parameters. For
example, if the coder classified the intended mes-
sage of a graphic as Change-trend, the coder was
also asked to identify where the first trend began,
its general slope (increasing, decreasing, or stable),
where the change in trend occurred, the end of the
second trend, and the slope of the second trend. If
there was disagreement between the coders on either
the intention or the instantiation of the parameters,
we utilized consensus-based annotation (Ang et al,
2002), in which the coders discussed the graphic to
try to come to an agreement. As observed by (Ang
et al, 2002), this allowed us to include the ?harder?
or less obvious graphics in our study, thus lowering
our expected system performance. We then exam-
ined the caption of each graphic, and determined to
what extent the caption captured the graphic?s in-
tended message. Figure 3 shows the results. 44%
of the captions in our corpus did not convey to any
extent the message of the information graphic. The
following categorizes the purposes that these cap-
tions served, along with an example of each:
? general heading (8 captions): ?UGI Monthly
Gas Rates? on a graphic conveying a recent
spike in home heating bills.
? reference to dependent axis (15 captions):
?Lancaster rainfall totals for July? on a
graphic conveying that July-02 was the driest
of the previous decade.
? commentary relevant to graphic (4 captions):
?Basic performers: One look at the best per-
forming stocks in the Standard&Poor?s 500 in-
dex this year shows that companies with ba-
sic businesses are rewarding investors? on a
225
graphic conveying the relative rank of different
stocks, some of which were basic businesses
and some of which were not. This type of in-
formation was classified as deductive by (Corio
and Lapalme, 1999) since it draws a conclusion
from the data depicted in the graphic.
? commentary extending message of graphic (8
captions): ?Profits are getting squeezed? on
a graphic conveying that Southwest Airlines
net income is estimated to increase in 2003 af-
ter falling the preceding three years. Here the
commentary does not draw a conclusion from
the data in the graphic but instead supplements
the graphic?s message. However this type of
caption would probably fall into the deductive
class in (Corio and Lapalme, 1999).
? humor (7 captions): ?The Sound of Sales? on
a graphic conveying the changing trend (down-
ward after years of increase) in record album
sales. This caption has nothing to do with the
change-trend message of the graphic, but ap-
pears to be an attempt at humor.
? conclusion unwarranted by graphic (2 cap-
tions): ?Defense spending declines? on a
graphic that in fact conveys that recent defense
spending is increasing.
Slightly over half the captions (56%) contributed
to understanding the graphic?s intended message.
34% were judged to convey most of the intended
message. For example, the caption ?Tennis play-
ers top nominees? appeared on a graphic whose in-
tended message is to convey that more tennis players
were nominated for the 2003 Laureus World Sports
Award than athletes from any other sport. Since we
argue that captions alone are insufficient for inter-
preting information graphics, in the few cases where
it was unclear whether a caption should be placed
in Category-1 or Category-2, we erred on the side
of over-rating the contribution of a caption to the
graphic?s intended message. For example, consider
the caption ?Chirac is riding high in the polls?
which appeared on a graphic conveying that there
has been a steady increase in Chirac?s approval rat-
ings from 55% to about 75%. Although this caption
does not fully capture the communicative intention
of the graphic (since it does not capture the steady
increase conveyed by the graphic), we placed it in
the first category since one might argue that riding
high in the polls would suggest both high and im-
proving ratings.
15% of the captions were judged to convey only
part of the graphic?s intended message; an example
is ?Drug spending for young outpace seniors? that
appears on a graphic whose intended message ap-
pears to be that there is a downward trend by age for
increased drug spending; we classified the caption
in Category-2 since the caption fails to capture that
the graphic is talking about percent increases in drug
spending, not absolute drug spending, and that the
graphic conveys the downward trend for increases in
drug spending by age group, not just that increases
for the young were greater than for the elderly.
7% of the captions were judged to only hint at the
graphic?s message. An example is ?GM?s Money
Machine? which appeared on a graphic whose in-
tended message was a contrast of recent perfor-
mance against the previous trend ? ie., that al-
though there had been a steady decrease in the per-
centage of GM?s overall income produced by its fi-
nance unit, there was now a substantial increase in
the percentage provided by the finance unit. Since
the term money machine is a colloquialism that sug-
gests making a lot of money, the caption was judged
to hint at the graphic?s intended message.
4.2 Understanding Captions
For the 49 captions in Category 1 or 2 (where the
caption conveyed at least some of the message of
the graphic), we examined how well the caption
could be parsed and understood by a natural lan-
guage system. We found that 47% were fragments
(for example, ?A Growing Biotech Market?), or in-
volved some other kind of ill-formedness (for ex-
ample, ?Running tops in sneaker wear in 2002? or
?More seek financial aid?1). 16% would require ex-
tensive domain knowledge or analogical reasoning
to understand. One example is ?Chirac is riding
high in the polls? which would require understand-
ing the meaning of riding high in the polls. Another
example is ?Bad Moon Rising?; here the verb ris-
ing suggests that something is increasing, but the
1Here we judge the caption to be ill-formed due to the ellip-
sis since More should be More students.
226
system would need to understand that a bad moon
refers to something undesirable (in this case, delin-
quent loans).
4.3 Simple Evidence from Captions
Although our corpus analysis showed that captions
can be helpful in understanding the message con-
veyed by an information graphic, it also showed that
full understanding of a caption would be problem-
atic; moreover, once the caption was understood, we
would still need to relate it to the information ex-
tracted from the graphic itself, which appears to be
a difficult problem.
Thus we began investigating whether shallow pro-
cessing of the caption might provide evidence that
could be effectively combined with other evidence
obtained from the graphic itself. Our analysis pro-
vided the following observations:
? Verbs in a caption often suggest the kind of
message being conveyed by the graphic. An
example from our corpus is ?Boating deaths
decline?; the verb decline suggests that the
graphic conveys a decreasing trend. Another
example from our corpus is ?American Express
total billings still lag?; the verb lag suggests
that the graphic conveys that some entity (in
this case American Express) is ranked behind
some others.
? Adjectives in a caption also often suggest the
kind of message being conveyed by the graphic.
An example from our corpus is ?Air Force has
largest percentage of women?; the adjective
largest suggests that the graphic is conveying
an entity whose value is largest. Adjectives de-
rived from verbs function similarly to verbs.
An example from our corpus is ?Soaring De-
mand for Servers? which is the caption on a
graphic that conveys the rapid increase in de-
mand for servers. Here the adjective soaring is
derived from the verb soar, and suggests that
the graphic is conveying a strong increase.
? Nouns in a caption often refer to an entity that
is a label on the independent axis. When this
occurs, the caption brings the entity into focus
and suggests that it is part of the intended mes-
sage of the graphic. An example from our cor-
pus is ?Germans miss their marks? where the
graphic displays a bar chart that is intended to
convey that Germans are the least happy with
the Euro. Words that usually appear as verbs,
but are used in the caption as a noun, may func-
tion similarly to verbs. An example is ?Cable
On The Rise?; in this caption, rise is used as a
noun, but suggests that the graphic is conveying
an increase.
5 Utilizing Evidence
We developed and implemented a probabilistic
framework for utilizing evidence from a graphic and
its caption to hypothesize the graphic?s intended
message. To identify the intended message of a
new information graphic, the graphic is first given
to a Visual Extraction Module (Chester and Elzer,
2005) that is responsible for recognizing the indi-
vidual components of a graphic, identifying the re-
lationship of the components to one another and to
the graphic as a whole, and classifying the graphic
as to type (bar chart, line graph, etc.); the result is
an XML file that describes the graphic and all of its
components.
Next a Caption Processing Module analyzes the
caption. To utilize verb-related evidence from cap-
tions, we identified a set of verbs that would indicate
each category of high-level goal2, such as recover
for Change-trend and beats for Relative-difference;
we then extended the set of verbs by examining
WordNet for verbs that were closely related in mean-
ing, and constructed a verb class for each set of
closely related verbs. Adjectives such as more and
most were handled in a similar manner. The Caption
Processing Module applies a part-of-speech tagger
and a stemmer to the caption in order to identify
nouns, adjectives, and the root form of verbs and
adjectives derived from verbs. The XML represen-
tation of the graphic is augmented to indicate any
independent axis labels that match nouns in the cap-
tion, and the presence of a verb or adjective class in
the caption.
The Intention Recognition Module then analyzes
the XML file to build the appropriate Bayesian net-
work; the current system is limited to bar charts, but
2As described in the next paragraph, there are 12 categories
of high-level goals.
227
the principles underlying the system should be ex-
tendible to other kinds of information graphics. The
network is described in (Elzer et al, 2005). Very
briefly, our analysis of simple bar charts has shown
that the intended message can be classified into one
of 12 high-level goals; examples of such goals in-
clude:
? Change-trend: Viewer to believe that there
is a <slope-1> trend from <param1>
to <param2> and a significantly differ-
ent <slope-2> trend from <param3> to
<param4>
? Relative-difference: Viewer to believe that the
value of element <param1> is <comparison>
the value of element <param2> where
<comparison> is greater-than, less-than, or
equal-to.
Each category of high-level goal is represented by a
node in the network (whose parent is the top-level
goal node), and instances of these goals (ie., goals
with their parameters instantiated) appear as chil-
dren with inhibitory links (Huber et al, 1994) cap-
turing their mutual exclusivity. Each goal is broken
down further into subtasks (perceptual or cognitive)
that the viewer would need to perform in order to
accomplish the goal of the parent node. The net-
work is built dynamically when the system is pre-
sented with a new information graphic, so that nodes
are added to the network only as suggested by the
graphic. For example, low-level nodes are added for
the easiest primitive perceptual tasks and for per-
ceptual tasks in which a parameter is instantiated
with a salient entity (such as an entity colored dif-
ferently from others in the graphic or an entity that
appears as a noun in the caption), since the graphic
designer might have intended the viewer to perform
these tasks; then higher-level goals that involve these
tasks are added, until eventually a link is established
to the top-level goal node.
Next evidence nodes are added to the network to
capture the kinds of evidence noted in Sections 3
and 4.3. For example, evidence nodes are added to
the network as children of each low-level perceptual
task; these evidence nodes capture the relative dif-
ficulty (categorized as easy, medium, hard, or im-
possible) of performing the perceptual task as esti-
mated by our effort estimation rules mentioned in
Section 3, whether a parameter in the task refers to
an entity that is salient in the graphic, and whether
a parameter in the task refers to an entity that is a
noun in the caption. An evidence node, indicating
for each verb class whether that verb class appears
in the caption (either as a verb, or as an adjective de-
rived from a verb, or as a noun that can also serve as
a verb) is added as a child of the top level goal node.
Adjectives such as more and most that provide evi-
dence are handled in a similar manner.
In a Bayesian network, conditional probability ta-
bles capture the conditional probability of a child
node given the value of its parent(s). For example,
the network requires the conditional probability of
an entity appearing as a noun in the caption given
that recognizing the intended message entails per-
forming a particular perceptual task involving that
entity. Similarly, the network requires the condi-
tional probability, for each class of verb, that the
verb class appears in the caption given that the in-
tended message falls into a particular intention cat-
egory. These probabilities are learned from our cor-
pus of graphics, as described in (Elzer et al, 2005).
6 Evaluation
In this paper, we are particularly interested in
whether shallow processing of captions can con-
tribute to recognizing the intended message of an
information graphic. As mentioned earlier, the in-
tended message of each information graphic in our
corpus of bar charts had been previously annotated
by two coders. To evaluate our approach, we used
leave-one-out cross validation. We performed a se-
ries of experiments in which each graphic in the cor-
pus is selected once as the test graphic, the probabil-
ity tables in the Bayesian network are learned from
the remaining graphics, and the test graphic is pre-
sented to the system as a test case. The system was
judged to fail if either its top-rated hypothesis did
not match the intended message that was assigned
to the graphic by the coders or the probability rat-
ing of the system?s top-rated hypothesis did not ex-
ceed 50%. Overall success was then computed by
averaging together the results of the whole series of
experiments.
Each experiment consisted of two parts, one in
228
Diner?s Club
Discover
American Express
Mastercard
Visa
400 600200
Total credit card purchases per year in billions
Figure 4: A Graphic from Business Week3
which captions were not taken into account in the
Bayesian network and one in which the Bayesian
network included evidence from captions. Our
overall accuracy without the caption evidence was
64.5%, while the inclusion of caption evidence in-
creased accuracy to 79.1% for an absolute increase
in accuracy of 14.6% and a relative improvement of
22.6% over the system?s accuracy without caption
evidence. Thus we conclude that shallow process-
ing of a caption provides evidence that can be effec-
tively utilized in a Bayesian network to recognize
the intended message of an information graphic.
Our analysis of the results provides some interest-
ing insights on the role of elements of the caption.
There appear to be two primary functions of verbs.
The first is to reflect what is in the data, thereby
strengthening the message that would be recognized
without the caption. One example from our corpus
is a graphic with the caption ?Legal immigration to
the U.S. has been rising for decades?. Although
the early part of the graphic displays a change from
decreasing immigration to a steadily increasing im-
migration trend, most of the graphic focuses on the
decades of increasing immigration and the caption
strengthens increasing trend in immigration as the
intended message of the graphic. If we do not in-
clude the caption, our system hypothesizes an in-
creasing trend message with a probability of 66.4%;
other hypotheses include an intended message that
emphasizes the change in trend with a probability
of 15.3%. However, when the verb increasing from
the caption is taken into account, the probability of
increasing trend in immigration being the intended
message rises to 97.9%.
3This is a slight variation of the graphic from Business
Week. In the Business Week graphic, the labels sometimes ap-
The second function of a verb is to focus atten-
tion on some aspect of the data. For example, con-
sider the graphic in Figure 4. Without a caption, our
system hypothesizes that the graphic is intended to
convey the relative rank in billings of different credit
card issuers and assigns it a probability of 72.7%.
Other possibilities have some probability assigned
to them. For example, the intention of conveying
that Visa has the highest billings is assigned a prob-
ability of 26%. Suppose that the graphic had a cap-
tion of ?Billings still lag?; if the verb lag is taken
into account, our system hypothesizes an intended
message of conveying the credit card issuer whose
billings are lowest, namely Diner?s Club; the prob-
ability assigned to this intention is now 88.4%, and
the probability assigned to the intention of convey-
ing the relative rank of different credit card issuers
drops to 7.8%. This is because the verb class con-
taining lag appeared in our corpus as part of the cap-
tion for graphics whose message conveyed an en-
tity with a minimum value, and not with graphics
whose message conveyed the relative rank of all the
depicted entities. On the other hand, if the caption
is ?American Express total billings still lag? (which
is the caption associated with the graphic in our cor-
pus), then we have two pieces of evidence from the
caption ? the verb lag, and the noun American Ex-
press which matches a label. In this case, the proba-
bilities change dramatically; the hypothesis that the
graphic is intended to convey the rank of American
Express (namely third behind Visa and Mastercard)
is assigned a probability of 76% and the probability
drops to 24% that the graphic is intended to con-
vey that Diner?s Club has the lowest billings. This is
not surprising. The presence of the noun American
Express in the caption makes that entity salient and
is very strong evidence that the intended message
places an emphasis on American Express, thus sig-
nificantly affecting the probabilities of the different
hypotheses. On the other hand, the verb class con-
taining lag occurred both in the caption of graphics
whose message was judged to convey the entity with
the minimum value and in the caption of graphics
pear on the bars and sometimes next to them, and the heading
for the dependent axis appears in the empty white space of the
graphic instead of below the values on the horizontal axis as we
show it. Our vision system does not yet have heuristics for rec-
ognizing non-standard placement of labels and axis headings.
229
that conveyed an entity ranked behind some others.
Therefore, conveying the entity with minimum value
is still assigned a non-negligible probability.
7 Future Work
It is rare that a caption contains more than one verb
class; when it does happen, our current system by
default uses the first one that appears. We need to
examine how to handle the occurrence of multiple
verb classes in a caption. Occasionally, labels in the
graphic appear differently in the caption. An exam-
ple is DJIA (for Dow Jones Industrial Average) that
occurs in one graphic as a label but appears as Dow
in the caption. We need to investigate resolving such
coreferences.
We currently limit ourselves to recognizing what
appears to be the primary communicative intention
of an information graphic; in the future we will also
consider secondary intentions. We will also extend
our work to other kinds of information graphics such
as line graphs and pie charts, and to complex graph-
ics, such as grouped and composite bar charts.
8 Summary
To our knowledge, our project is the first to inves-
tigate the problem of understanding the intended
message of an information graphic. This paper
has focused on the communicative evidence present
in an information graphic and how it can be used
in a probabilistic framework to reason about the
graphic?s intended message. The paper has given
particular attention to evidence provided by the
graphic?s caption. Our corpus study showed that
about half of all captions contain some evidence that
contributes to understanding the graphic?s message,
but that fully understanding captions is a difficult
problem. We presented a strategy for extracting ev-
idence from a shallow analysis of the caption and
utilizing it, along with communicative signals from
the graphic itself, in a Bayesian network that hy-
pothesizes the intended message of an information
graphic, and our results demonstrate the effective-
ness of our methodology. Our research is part of a
larger project aimed at providing alternative access
to information graphics for individuals with sight
impairments.
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. of the Int?l Conf. on Spoken Language Process-
ing (ICSLP).
D. Chester and S. Elzer. 2005. Getting computers to see
information graphics so users do not have to. To ap-
pear in Proc. of the 15th Int?l Symposium on Method-
ologies for Intelligent Systems.
H. Clark. 1996. Using Language. Cambridge University
Press.
M. Corio and G. Lapalme. 1999. Generation of texts
for information graphics. In Proc. of the 7th European
Workshop on Natural Language Generation, 49?58.
S. Elzer, S. Carberry, N. Green, and J. Hoffman. 2004.
Incorporating perceptual task effort into the recogni-
tion of intention in information graphics. In Proceed-
ings of the 3rd Int?l Conference on Diagrams, LNAI
2980, 255?270.
S. Elzer, S. Carberry, I. Zukerman, D. Chester, N. Green,
S. Demir. 2005. A probabilistic framework for recog-
nizing intention in information graphics. To appear in
Proceedings of the Int?l Joint Conf. on AI (IJCAI).
R. Futrelle and N. Nikolakis. 1995. Efficient analysis of
complex diagrams using constraint-based parsing. In
Proc. of the Third International Conference on Docu-
ment Analysis and Recognition.
R. Futrelle. 1999. Summarization of diagrams in docu-
ments. In I. Mani and M. Maybury, editors, Advances
in Automated Text Summarization. MIT Press.
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev,
Joe Mattis, Johanna Moore, and Steven Roth. Auto-
brief: an experimental system for the automatic gen-
eration of briefings in integrated text and information
graphics. International Journal of Human-Computer
Studies, 61(1):32?70, 2004.
H. P. Grice. 1969. Utterer?s Meaning and Intentions.
Philosophical Review, 68:147?177.
M. Huber, E. Durfee, and M. Wellman. 1994. The auto-
mated mapping of plans for plan recognition. In Proc.
of Uncertainty in AI, 344?351.
S. Kerpedjiev and S. Roth. 2000. Mapping communica-
tive goals into conceptual tasks to generate graphics in
discourse. In Proc. of Int. Conf. on Intelligent User
Interfaces, 60?67.
J. Yu, J. Hunter, E. Reiter, and S. Sripada. 2002.
Recognising visual patterns to communicate gas tur-
bine time-series data. In ES2002, 105?118.
230
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 261?264,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating Word Prediction: Framing Keystroke Savings
Keith Trnka and Kathleen F. McCoy
University of Delaware
Newark, DE 19716
trnka@cis.udel.edu
Abstract
Researchers typically evaluate word predic-
tion using keystroke savings, however, this
measure is not straightforward. We present
several complications in computing keystroke
savings which may affect interpretation and
comparison of results. We address this prob-
lem by developing two gold standards as a
frame for interpretation. These gold standards
measure the maximum keystroke savings un-
der two different approximations of an ideal
language model. The gold standards addition-
ally narrow the scope of deficiencies in a word
prediction system.
1 Introduction
Word prediction is an application of language mod-
eling to speeding up text entry, especially to entering
utterances to be spoken by an Augmentative and Al-
ternative Communication (AAC) device. AAC de-
vices seek to address the dual problem of speech and
motor impairment by attempting to optimize text in-
put. Even still, communication rates with AAC de-
vices are often below 10 words per minute (Newell
et al, 1998), compared to the common 130-200
words per minute speech rate of speaking people.
Word prediction addresses these issues by reducing
the number of keystrokes required to produce a mes-
sage, which has been shown to improve communi-
cation rate (Trnka et al, 2007). The reduction in
keystrokes also translates into a lower degree of fa-
tigue from typing all day (Carlberger et al, 1997).
Word prediction systems present multiple com-
pletions of the current word to the user. Systems
generate a list of W predictions on the basis of the
word being typed and a language model. The vo-
cabulary is filtered to match the prefix of the current
word and the language model ranks the words ac-
cording to their likelihood. In the case that no letters
of the current word have been entered, the language
model is the sole factor in generating predictions.
Systems often use a touchscreen or function/number
keys to select any of the predicted words.
Because the goal of word prediction systems is
to reduce the number of keystrokes, the primary
evaluation for word prediction is keystroke savings
(Garay-Vitoria and Abascal, 2006; Newell et al,
1998; Li and Hirst, 2005; Trnka and McCoy, 2007;
Carlberger et al, 1997). Keystroke savings (KS)
measures the percentage reduction in keys pressed
compared to letter-by-letter text entry.
KS =
keysnormal ? keyswith prediction
keysnormal
? 100%
A word prediction system that offers higher savings
will benefit a user more in practice.
However, the equation for keystroke savings has
two major deficiencies. Firstly, the equation alone
is not enough to compute keystroke savings ? actu-
ally computing keystroke savings requires a precise
definition of a keystroke and also requires a method
for determining howmany keystrokes are used when
predictions are available, discussed in Section 2. Be-
yond simply computing keystroke savings, the equa-
tion alone does not provide much in the way of inter-
pretation? is 60% keystroke savings good? Can we
do better? Section 3 will present two gold standards
to allow better interpretation of keystroke savings.
261
2 Computing Keystroke Savings
We must have a way to determine how many
keystrokes a user would take under both letter-
by-letter entry and word prediction to compute
keystroke savings. The common trend in research
is to simulate a ?perfect? user that will never make
typing mistakes and will select a word from the pre-
dictions as soon as it appears.
Implementation of perfect utilization of the pre-
dictions is not always straightforward. For exam-
ple, consider the predictive interface in Microsoft
WordTM: a single prediction is offered as an inline
completion. If the prediction is selected, the user
may backspace and edit the word. However, this
freedom makes finding the minimum sequence of
keys more difficult ? now the user may select a
prediction with the incorrect suffix and correct the
suffix as the optimal action. We feel that a more in-
tuitive interface would allow a user to undo the pre-
diction selection by pressing backspace, an interface
which does not support backspace-editing. In addi-
tion to backspacing, future research in multi-word
prediction will face a similar problem, analogous to
the garden-path problem in parsing, where a greedy
approach does not always give the optimal result.
The keystrokes used for training and testing word
prediction systems can affect the results. We at-
tempt to evaluate word prediction as realistically as
possible. Firstly, many corpora have punctuation
marks, but an AAC user in a conversational setting
is unlikely to use punctuation due to the high cost
of each key press. Therefore, we remove punctua-
tion on the outside of words, such as commas and
periods, but leave word-internal punctuation intact.
Also, we treat capital letters as a single key press,
reflecting the trend of many AAC users to avoid cap-
italization. Another problem occurs for a newline or
?speak key?, which the user would press after com-
pleting an utterance. In pilot studies, including the
simulation of a speak key lowered keystroke savings
by 0.8?1.0% for window sizes 1?10, because new-
lines are not able to be predicted in the system. How-
ever, we feel that the simulation of a speak key will
produce an evaluation metric that is closer to the ac-
tual user?s experience, therefore we include a speak
key in our evaluations.
An evaluation of word prediction must address
these issues, if only implicitly. The effect of these
potentially implicit decisions on keystroke savings
can make comparison of results difficult. However,
if results are presented in reference to a gold stan-
dard under the same assumptions, we can draw more
reliable conclusions from results.
3 Towards a Gold Standard
In trying to improve the state of word prediction,
several researchers have noted that it seems ex-
tremely difficult to improve keystroke savings be-
yond a certain point. Copestake (1997) discussed
the entropy of English to conclude that 50?60%
keystroke savings may be the most we can expect
in practice. Lesher et al (2002) replaced the lan-
guage model in a word prediction system with a
human to try and estimate the limit of keystroke
savings. They found that humans could achieve
59% keystroke savings with access to their ad-
vanced language model and that their advanced lan-
guage model alone achieved 54% keystroke savings.
They noted that one subject achieved nearly 70%
keystroke savings on one particular text, and con-
cluded that further improvements on current meth-
ods are possible. Garay-Vitoria and Abascal (2006)
survey many prediction systems, showing a wide
spectrum of savings, but no system offers more than
70% keystroke savings.
We investigated the problem of the limitations
of keystroke savings first from a theoretical per-
spective, seeking a clearly defined upper boundary.
Keystroke savings can never reach 100%? it would
mean that the system divined the entire text they in-
tended without a single key.
3.1 Theoretical keystroke savings limit
The minimum amount of input required corresponds
to a perfect system ? one that predicts every word
as soon as possible. In a word completion sys-
tem, the predictions are delayed until after the first
character of the word is entered. In such a sys-
tem, the minimum amount of input using a perfect
language model is two keystrokes per word ? one
for the first letter and one to select the prediction.
The system would also require one keystroke per
sentence. In a word prediction system, the predic-
tions are available immediately, so the minimal in-
262
put for a perfect system is one keystroke per word
(to select the prediction) and one keystroke per sen-
tence. We added the ability to measure the minimum
number of keystrokes and maximum savings to our
simulation software, which we call the theoretical
keystroke savings limit.
We evaluated a baseline trigram model under two
conditions with different keystroke requirements on
the Switchboard corpus. The simulation software
was modified to output the theoretical limit in ad-
dition to actual keystroke savings at various window
sizes. To demonstrate the effect of the theoretical
keystroke savings limit on actual savings, we eval-
uated the trigram model under conditions with two
different limits ? word prediction and word com-
pletion. The evaluation of the trigram model using
word completion is shown in Figure 1. The actual
keystroke savings is graphed by window size in ref-
erence to the theoretical limit. As noted by other re-
searchers, keystroke savings increases with window
size, but with diminishing returns (this is the effect
of placing the most probable words first). One of
0%
10%
20%
30%
40%
50%
60%
1 2 3 4 5 6 7 8 9 10
Key
stro
ke 
sav
ing
s
Window size
Word completionTheoretical limit
Figure 1: Keystroke savings and the limit vs. window
size for word completion.
the problems with word completion is that the the-
oretical limit is so close to actual performance ?
around 58.5% keystroke savings compared to 50.8%
keystroke savings with five predictions. At only five
predictions, the system has already realized 87% of
the possible keystroke savings. Under these circum-
stances, it would take a drastic change in the lan-
guage model to impact keystroke savings.
We repeated this analysis for word prediction,
shown in Figure 2 alongside word completion. Word
prediction is much higher than completion, both the-
oretically (the limit) and in actual keystroke savings.
0%
10%
20%
30%
40%
50%
60%
70%
80%
1 2 3 4 5 6 7 8 9 10
Key
stro
ke 
sav
ing
s
Window size
Word predictionWord prediction limitWord completionWord completion limit
Figure 2: Keystroke savings and the limit vs. window
size for word prediction compared to word completion.
Word prediction offers much more headroom in
terms of improvements in keystroke savings. There-
fore our ongoing research will focus on word pre-
diction over word completion.
This analysis demonstrates a limit to keystroke
savings, but this limit is slightly different than
Copestake (1997) and Lesher et al (2002) seek to
describe ? beyond the limitations of the user in-
terface, there seems to be a limitation on the pre-
dictability of English. Ideally, we would like to have
a gold standard that is a closer estimate of an ideal
language model.
3.2 Vocabulary limit
We can derive a more practical limit by simulating
word prediction using a perfect model of all words
that occur in the training data. This gold standard
will predict the correct word immediately so long as
it occurs in the training corpus. Words that never oc-
curred in training require letter-by-letter entry. We
call this measure the vocabulary limit and apply it to
evaluate whether the difference between training and
testing vocabulary is significant. Previous research
has focused on the percentage of out-of-vocabulary
(OOV) terms to explain changes in keystroke sav-
ings (Trnka and McCoy, 2007; Wandmacher and
Antoine, 2006). In contrast, the vocabulary limit
gives more guidance for research by translating the
problem of OOVs into keystroke savings.
Expanding the results from the theoretical limit,
the vocabulary limit is 77.6% savings, compared to
78.4% savings for the theoretical limit and 58.7%
actual keystroke savings with 5 predictions. The
practical limit is very close to the theoretical limit
263
in the case of Switchboard. Therefore, the remain-
ing gap between the practical limit and actual per-
formance must be due to other differences between
testing and training data, limitations of the model,
and limitations of language modeling.
3.3 Application to corpus studies
We applied the gold standards to our corpus study, in
which a trigram model was individually trained and
tested on several different corpora (Trnka and Mc-
Coy, 2007). In contrast to the actual trigram model
Corpus Trigram Vocab.
limit
Theor.
limit
AAC Email 48.92% 61.94% 84.83%
Callhome 43.76% 54.62% 81.38%
Charlotte 48.30% 65.69% 83.74%
SBCSAE 42.30% 60.81% 79.86%
Micase 49.00% 69.18% 84.08%
Switchboard 60.35% 80.33% 82.57%
Slate 53.13% 81.61% 85.88%
Table 1: A trigram model compared to the limits.
performance, the theoretical limits all fall within a
relatively narrow range, suggesting that the achiev-
able keystroke savings may be similar even across
different domains. The more technical and formal
corpora (Micase, Slate, AAC) show higher limits, as
the theoretical limit is based on the length of words
and sentences in each corpus. The practical limit
exhibits much greater variation. Unlike the Switch-
board analysis, many other corpora have a substan-
tial gap between the theoretical and practical limits.
Although the practical measure seems to match the
actual savings similarly to OOVs testing with cross-
validation (Trnka and McCoy, 2007), this measure
more concretely illustrates the effect of OOVs on
actual keystroke savings ? 60% keystroke savings
when training and testing on AAC Email would be
extraordinary.
4 Conclusions
Although keystroke savings is the predominant eval-
uation for word prediction, this evaluation is not
straightforward, exacerbating the problem of inter-
preting and comparing results. We have presented
a novel solution ? interpreting results alongside
gold standards which capture the difficulty of the
evaluation. These gold standards are also applica-
ble to drive future research ? if actual performance
is very close to the theoretical limit, then relaxing
the minimum keystroke requirements should be the
most beneficial (e.g., multi-word prediction). Sim-
ilarly, if actual performance is very close to the
vocabulary limit, then the vocabulary of the lan-
guage model must be improved (e.g., cache mod-
eling, adding general-purpose training data). In the
case that keystroke savings is far from either limit,
then research into improving the language model is
likely to be the most beneficial.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051.
References
Alice Carlberger, John Carlberger, Tina Magnuson,
M. Sharon Hunnicutt, Sira Palazuelos-Cagigas, and
Santiago Aguilera Navarro. 1997. Profet, a new gen-
eration of word prediction: An evaluation study. In
ACL-97 workshop on Natural Language Processing
for Communication Aids.
Ann Copestake. 1997. Augmented and alternative NLP
techniques for augmentative and alternative commu-
nication. In ACL-97 workshop on Natural Language
Processing for Communication Aids, pages 37?42.
Nestor Garay-Vitoria and Julio Abascal. 2006. Text pre-
diction systems: a survey. Univ Access Inf Soc, 4:183?
203.
Gregory W. Lesher, Bryan J. Moulton, D Jeffery Higgin-
botham, and Brenna Alsofrom. 2002. Limits of hu-
man word prediction performance. In CSUN.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS, pages 121?128.
Alan Newell, Stefan Langer, andMarianne Hickey. 1998.
The ro?le of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):1?16.
Keith Trnka and Kathleen F. McCoy. 2007. Corpus Stud-
ies in Word Prediction. In ASSETS, pages 195?202.
Keith Trnka, Debra Yarrington, JohnMcCaw, Kathleen F.
McCoy, and Christopher Pennington. 2007. The Ef-
fects of Word Prediction on Communication Rate for
AAC. In NAACL-HLT; Companion Volume: Short Pa-
pers, pages 173?176.
Tonio Wandmacher and Jean-Yves Antoine. 2006.
Training Language Models without Appropriate Lan-
guage Resources: Experiments with an AAC System
for Disabled People. In Eurospeech.
264
Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 61?66,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Adaptive Language Modeling for Word Prediction
Keith Trnka
University of Delaware
Newark, DE 19716
trnka@cis.udel.edu
Abstract
We present the development and tuning of a
topic-adapted language model for word pre-
diction, which improves keystroke savings
over a comparable baseline. We outline our
plans to develop and integrate style adap-
tations, building on our experience in topic
modeling to dynamically tune the model to
both topically and stylistically relevant texts.
1 Introduction
People who use Augmentative and Alternative Com-
munication (AAC) devices communicate slowly, of-
ten below 10 words per minute (wpm) compared to
150 wpm or higher for speech (Newell et al, 1998).
AAC devices are highly specialized keyboards with
speech synthesis, typically providing single-button
input for common words or phrases, but requiring a
user to type letter-by-letter for other words, called
fringe vocabulary. Many commercial systems (e.g.,
PRC?s ECO) and researchers (Li and Hirst, 2005;
Trnka et al, 2006; Wandmacher and Antoine, 2007;
Matiasek and Baroni, 2003) have leveraged word
prediction to help speed AAC communication rate.
While the user is typing an utterance letter-by-letter,
the system continuously provides potential comple-
tions of the current word to the user, which the user
may select. The list of predicted words is generated
using a language model.
At best, modern devices utilize a trigram model
and very basic recency promotion. However, one of
the lamented weaknesses of ngram models is their
sensitivity to the training data. They require sub-
stantial training data to be accurate, and increasingly
more data as more of the context is utilized. For ex-
ample, Lesher et al (1999) demonstrate that bigram
and trigram models for word prediction are not satu-
rated even when trained on 3 million words, in con-
trast to a unigram model. In addition to the prob-
lem of needing substantial amounts of training text
to build a reasonable model, ngrams are sensitive
to the difference between training and testing/user
texts. An ngram model trained on text of a differ-
ent topic and/or style may perform very poorly com-
pared to a model trained and tested on similar text.
Trnka and McCoy (2007) and Wandmacher and An-
toine (2006) have demonstrated the domain sensitiv-
ity of ngram models for word prediction.
The problem of utilizing ngram models for con-
versational AAC usage is that no substantial cor-
pora of AAC text are available (much less conver-
sational AAC text). The most similar available cor-
pora are spoken language, but are typically much
smaller than written corpora. The problem of cor-
pora for AAC is that similarity and availability are
inversely related, illustrated in Figure 1. At one ex-
treme, a very large amount of formal written English
is available, however, it is very dissimilar from con-
versational AAC text, making it less useful for word
prediction. At the other extreme, logged text from
the current conversation of the AAC user is the most
highly related text, but it is extremely sparse. While
this trend is demonstrated with a variety of language
modeling applications, the problem is more severe
for AAC due to the extremely limited availability of
AAC text. Even if we train our models on both a
large number of general texts in addition to highly
related in-domain texts to address the problem, we
61
Figure 1: The most relevant text available is often the smallest, while the largest corpora are often the least relevant
for AAC word prediction. This problem is exaggerated for AAC.
must focus the models on the most relevant texts.
We address the problem of balancing training size
and similarity by dynamically adapting the language
model to the most topically relevant portions of the
training data. We present the results of experiment-
ing with different topic segmentations and relevance
scores in order to tune existing methods to topic
modeling. Our approach is designed to seamlessly
degrade to the baseline model when no relevant top-
ics are found, by interpolating frequencies as well as
ensuring that all training documents contribute some
non-zero probabilities to the model. We also out-
line our plans to adapt ngram models to the style of
discourse and then combine the topical and stylistic
adaptations.
1.1 Evaluating Word Prediction
Word prediction is evaluated in terms of keystroke
savings ? the percentage of keystrokes saved by
taking full advantage of the predictions compared to
letter-by-letter entry.
KS =
keysletter-by-letter ? keyswith prediction
keysletter-by-letter
? 100%
Keystroke savings is typically measured automati-
cally by simulating a user typing the testing data of a
corpus, where any prediction is selected with a sin-
gle keystroke and a space is automatically entered
after selecting a prediction. The results are depen-
dent on the quality of the language model as well as
the number of words in the prediction window. We
focus on 5-word prediction windows. Many com-
mercial devices provide optimized input for the most
common words (called core vocabulary) and offer
word prediction for all other words (fringe vocabu-
lary). Therefore, we limit our evaluation to fringe
words only, based on a core vocabulary list from
conversations of young adults.
We focus our training and testing on Switchboard,
which we feel is similar to conversational AAC text.
Our overall evaluation varies the training data from
Switchboard training to training on out-of-domain
data to estimate the effects of topic modeling in real-
world usage.
2 Topic Modeling
Topic models are language models that dynamically
adapt to testing data, focusing on the most related
topics in the training data. It can be viewed as a
two stage process: 1) identifying the relevant topics
by scoring and 2) tuning the language model based
on relevant topics. Various other implementations
of topic adaptation have been successful in word
prediction (Li and Hirst, 2005; Wandmacher and
Antoine, 2007) and speech recognition (Bellegarda,
2000; Mahajan et al, 1999; Seymore and Rosen-
feld, 1997). The main difference of the topic mod-
eling approach compared to Latent Semantic Anal-
ysis (LSA) models (Bellegarda, 2000) and trigger
pair models (Lau et al, 1993; Matiasek and Baroni,
2003) is that topic models perform the majority of
generalization about topic relatedness at testing time
rather than training time, which potentially allows
user text to be added to the training data seamlessly.
Topic modeling follows the framework below
Ptopic(w | h) =
?
t?topics
P (t | h) ? P (w | h, t)
where w is the word being predicted/estimated, h
represents all of the document seen so far, and t rep-
resents a single topic. The linear combination for
topic modeling shows the three main areas of vari-
ation in topic modeling. The posterior probability,
62
P (w | h, t) represents the sort of model we have;
how topic will affect the adapted language model in
the end. The prior, P (t | h), represents the way topic
is identified. Finally, the meaning of t ? topics, re-
quires explanation ? what is a topic?
2.1 Posterior Probability ? Topic Application
The topic modeling approach complicates the esti-
mation of probabilities from a corpus because the
additional conditioning information in the posterior
probability P (w | h, t) worsens the data sparseness
problem. This section will present our experience in
lessening the data sparseness problem in the poste-
rior, using examples on trigram models.
The posterior probability requires more data
than a typical ngrammodel, potentially causing data
sparseness problems. We have explored the pos-
sibility of estimating it by geometrically combin-
ing a topic-adapted unigram model (i.e., P (w | t))
with a context-adapted trigram model (i.e., P (w |
w?1, w?2)), compared to straightforward measure-
ment (P (w | w?1, w?2, t)). Although the first
approach avoids the additional data sparseness, it
makes an assumption that the topic of discourse
only affects the vocabulary usage. Bellegarda (2000)
used this approach for LSA-adapted modeling, how-
ever, we found this approach to be inferior to di-
rect estimation of the posterior probability for word
prediction (Trnka et al, 2006). Part of the reason
for the lesser benefit is that the overall model is
only affected slightly by topic adaptations due to
the tuned exponential weight of 0.05 on the topic-
adapted unigram model. We extended previous re-
search by forcing trigram predictions to occur over
bigrams and so on (rather than backoff) and using
the topic-adapted model for re-ranking within each
set of predictions, but found that the forced ordering
of the ngram components was overly detrimental to
keystroke savings.
Backoff models for topic modeling can be con-
structed either before or after the linear interpola-
tion. If the backoff is performed after interpolation,
we must also choose whether smoothing (a prereq-
uisite for backoff) is performed before or after the
interpolation. If we smooth before the interpolation,
then the frequencies will be overly discounted, be-
cause the smoothing method is operating on a small
fraction of the training data, which will reduce the
benefit of higher-order ngrams in the overall model.
Also, if we combine probability distributions from
each topic, the combination approach may have dif-
ficulties with topics of varying size. We address
these issues by instead combining frequencies and
performing smoothing and backoff after the combi-
nation, similar to Adda et al (1999), although they
used corpus-sized topics. The advantage of this ap-
proach is that the held-out probability for each dis-
tribution is appropriate for the training data, because
the smoothing takes place knowing the number of
words that occurred in the whole corpus, rather than
for each small segment. This is especially important
when dealing with small and different sized topics.
The linear interpolation affects smoothing
methods negatively ? because the weights are less
than one, the combination decreases the total sum
of each conditional distribution. This will cause
smoothing methods to underestimate the reliability
of the models, because smoothing methods estimate
the reliability of a distribution based on the absolute
number of occurrences. To correct this, after inter-
polating the frequencies we found it useful to scale
the distribution back to its original sum. The scal-
ing approach improved keystroke savings by 0.2%?
0.4% for window size 2?10 and decreased savings
by 0.1% for window size 1. Because most AAC sys-
tems provide 5?7 predictions, we use this approach.
Also, because some smoothing methods operate on
frequencies, but the combination model produces
real-valued weights for each word, we found it nec-
essary to bucket the combined frequencies to convert
them to integers.
Finally, we required an efficient smoothing
method that could discount each conditional distri-
bution individually to facilitate on-demand smooth-
ing for each conditional distribution, in contrast to
a method like Katz? backoff (Katz, 1987) which
smoothes an entire ngram model at once. Also,
Good-Turing smoothing proved too cumbersome, as
we were unable to rely on the ratio between words in
given bins and also unable to reliably apply regres-
sion. Instead, we used an approximation of Good-
Turing smoothing that performed similarly, but al-
lowed for substantial optimization.
63
2.2 Prior Probability ? Topic Identification
The topic modeling approach uses the current testing
document to tune the language model to the most
relevant training data. The benefit of adaptation is
dependent on the quality of the similarity scores. We
will first present our representation of the current
document, which is compared to unigram models of
each topic using a similarity function. We determine
the weight of each word in the current document us-
ing frequency, recency, and topical salience.
The recency of use of a word contributes to the
relevance of the word. If a word was used somewhat
recently, we would expect to see the word again. We
follow Bellegarda (2000) in using an exponentially
decayed cache with weight of 0.95 to model this ef-
fect of recency on importance at the current position
in the document. The weight of 0.95 represents a
preservation in topic, but with a decay for very stale
words, whereas a weight of 1 turns the exponen-
tial model into a pure frequency model and lower
weights represent quick shifts in topic.
The importance of each word occurrence in the
current document is a factor of not just its frequency
and recency, but also it?s topical salience ? how
well the word discriminates between topics. For this
reason, we decided to use a technique like Inverse
Document Frequency (IDF) to boost the weight of
words that occur in only a few documents and de-
press the weights of words that occur in most docu-
ments. However, instead of using IDF to measure
topical salience, we use Inverse Topic Frequency
(ITF), which is more specifically tailored to topic
modeling and the particular kinds of topics used.
We evaluated several similarity functions for
topic modeling, initially using the cosine measure
for similarity scoring and scaling the scores to be
a probability distribution, following Florian and
Yarowsky (1999). The intuition behind the co-
sine measure is that the similarity between two dis-
tributions of words should be independent of the
length of either document. However, researchers
have demonstrated that cosine is not the best rele-
vance metric for other applications, so we evaluated
two other topical similarity scores: Jacquard?s coef-
ficient, which performed better than most other sim-
ilarity measures in a different task for Lee (1999)
and Na??ve Bayes, which gave better results than co-
sine in topic-adapted language models for Seymore
and Rosenfeld (1997). We evaluated all three simi-
larity metrics using Switchboard topics as the train-
ing data and each of our corpora for testing us-
ing cross-validation. We found that cosine is con-
sistently better than both Jacquard?s coefficient and
Na??ve Bayes, across all corpora tested. The differ-
ences between cosine and the other methods are sta-
tistically significant at p < 0.001. It may be possible
that the ITF or recency weighting in the cache had a
negative interaction with Nav?e Bayes; traditionally
raw frequencies are used.
We found it useful to polarize the similarity
scores, following Florian and Yarowsky (1999),
who found that transformations on cosine similarity
reduced perplexity. We scaled the scores such that
the maximum score was one and the minimum score
was zero, which improved keystroke savings some-
what. This helps fine-tune topic modeling by further
boosting the weights of the most relevant topics and
depressing the weights of the less relevant topics.
Smoothing the scores helps prevent some scores
from being zero due to lack of word overlap. One of
the motivations behind using a linear interpolation of
all topics is that the resulting ngram model will have
the same coverage of ngrams as a model that isn?t
adapted by topic. However, the similarity score will
be zero when no words overlap between the topic
and history. Therefore we decided to experiment
with similarity score smoothing, which records the
minimum nonzero score and then adds a fraction of
that score to all scores, then only apply upscaling,
where the maximum is scaled to 1, but the minimum
is not scaled to zero. In pilot experiments, we found
that smoothing the scores did not affect topic mod-
eling with traditional topic clusters, but gave minor
improvements when documents were used as topics.
Stemming is another alternative to improving the
similarity scoring. This helps to reduce problems
with data sparseness by treating different forms of
the same word as topically equivalent. We found
that stemming the cache representations was very
useful when documents were treated as topics (0.2%
increase across window sizes), but detrimental when
larger topics were used (0.1?0.2% decrease across
window sizes). Therefore, we only use stemming
when documents are treated as topics.
64
2.3 What?s in a Topic ? Topic Granularity
We adapt a language model to the most relevant top-
ics in training text. But what is a topic? Tradition-
ally, document clusters are used for topics, where
some researchers use hand-crafted clusters (Trnka
et al, 2006; Lesher and Rinkus, 2001) and oth-
ers use automatic clustering (Florian and Yarowsky,
1999). However, other researchers such as Mahajan
et al (1999) have used each individual document as
a topic. On the other end of the spectrum, we can
use whole corpora as topics when training on mul-
tiple corpora. We call this spectrum of topic defini-
tions topic granularity, where manual and automatic
document clusters are called medium-grained topic
modeling. When topics are individual documents,
we call the approach fine-grained topic modeling. In
fine-grained modeling, topics are very specific, such
as seasonal clothing in the workplace, compared to
a medium topic for clothing. When topics are whole
corpora, we call the approach coarse-grained topic
modeling. Coarse-grained topics model much more
high-level topics, such as research or news.
The results of testing on Switchboard across dif-
ferent topic granularities are showin in Table 1. The
in-domain test is trained on Switchboard only. Out-
of-domain training is performed using all other cor-
pora in our collection (a mix of spoken and writ-
ten language). Mixed-domain training combines the
two data sets. Medium-grained topics are only pre-
sented for in-domain training, as human-annotated
topics were only available for Switchboard. Stem-
ming was used for fine-grained topics, but similarity
score smoothing was not used due to lack of time.
The topic granularity experiment confirms our
earlier findings that topic modeling can significantly
improve keystroke savings. However, the variation
of granularity shows that the size of the topics has
a strong effect on keystroke savings. Human anno-
tated topics give the best results, though fine-grained
topic modeling gives similar results without the need
for annotation, making it applicable to training on
not just Switchboard but other corpora as well. The
coarse grained topic approach seems to be limited
to finding acceptable interpolation weights between
very similar and very dissimilar data, but is poor at
selecting the most relevant corpora from a collection
of very different corpora in the out-of-domain test.
Another problem may be that many of the corpora
are only homogeneous in style but not topic. We
would like to extend our work in topic granularity to
testing on other corpora in the future.
3 Future Work ? Style and Combination
Topic modeling balances the similarity of the train-
ing data against the size by tuning a large training
set to the most topically relevant portions. However,
keystroke savings is not only affected by the topical
similarity of the training data, but also the stylistic
similarity. Therefore, we plan to also adapt models
to the style of text. Our success in adapting to the
topic of conversation leads us to believe that a sim-
ilar process may be applicable to style modeling ?
splitting the model into style identification and style
application. Because we are primarily interested in
syntactic style, we will focus on part of speech as
the mechanism for realizing grammatical style. As
a pilot experiment, we compared a collection of our
technical writings on word prediction with a collec-
tion of our research emails on word prediction, find-
ing that we could observe traditional trends in the
POS ngram distributions (e.g., more pronouns and
phrasal verbs in emails). Therefore, we expect that
distributional similarity of POS tags will be useful
for style identification. We envision a single style s
affecting the likelihood of each part of speech p in a
POS ngram model like the one below:
P (w | w?1,w?2, s) =
?
p?POS(w)
P (p | p?1, p?2, s) ? P (w | p)
In this reformulation of a POS ngram model, the
prior is conditioned on the style and the previous
couple tags. We will use the overall framework to
combine style identification and modeling:
Pstyle(w | h) =
?
s?styles
P (s | h) ? P (w | w?1, w?2, s)
The topical and stylistic adaptations can be com-
bined by adding topic modeling into the style model
shown above. The POS posterior probability P (w |
p) can be additionally conditioned on the topic of
discourse. Topic identification and the topic sum-
mation would be implemented consistently with the
standalone topic model. Also, the POS framework
65
Model type In-domain Out-of-domain Mixed-domain
Trigram baseline 60.35% 53.88% 59.80%
Switchboard topics (medium grained) 61.48% (+1.12%) ? ?
Document as topic (fine grained) 61.42% (+1.07%) 54.90% (+1.02%) 61.17% (+1.37%)
Corpus as topic (coarse grained) ? 52.63% (-1.25%) 60.62% (+0.82%)
Table 1: Keystroke savings across different granularity topics and training domains, tested on Switchboard. Improve-
ment over baseline is shown in parentheses. All differences from baseline are significant at p < 0.001
facilitates cache modeling in the posterior, allowing
direct adaptation to the current text, but with less
sparseness than other context-aware models.
4 Conclusions
We have created a topic adapted language model that
utilizes the full training data, but with focused tuning
on the most relevant portions. The inclusion of all
the training data as well as the usage of frequencies
addresses the problem of sparse data in an adaptive
model. We have demonstrated that topic modeling
can significantly increase keystroke savings for tra-
ditional testing as well as testing on text from other
domains. We have also addressed the problem of
annotated topics through fine-grained modeling and
found that it is also a significant improvement over a
baseline ngram model. We plan to extend this work
to build models that adapt to both topic and style.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051. I would like to thank
my advisor, Kathy McCoy, for her help as well as
the many excellent and thorough reviewers.
References
Gilles Adda, Miche`le Jardino, and Jean-Luc Gauvain.
1999. Language modeling for broadcast news tran-
scription. In Eurospeech, pages 1759?1762.
Jerome R. Bellegarda. 2000. Large vocabulary
speech recognition with multispan language models.
IEEE Transactions on Speech and Audio Processing,
8(1):76?84.
Radu Florian and David Yarowsky. 1999. Dynamic
Nonlocal Language Modeling via Hierarchical Topic-
Based Adaptation. In ACL, pages 167?174.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 35(3):400?401.
R. Lau, R. Rosenfeld, and S. Roukos. 1993. Trigger-
based language models: a maximum entropy ap-
proach. In ICASSP, volume 2, pages 45?48.
Lillian Lee. 1999. Measures of distributional similarity.
In ACL, pages 25?32.
Gregory Lesher and Gerard Rinkus. 2001. Domain-
specific word prediction for augmentative communi-
cation. In RESNA, pages 61?63.
Gregory W. Lesher, Bryan J. Moulton, and D. Jeffery
Higgonbotham. 1999. Effects of ngram order and
training text size on word prediction. In RESNA, pages
52?54.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS, pages 121?128.
Milind Mahajan, Doug Beeferman, and X. D. Huang.
1999. Improved topic-dependent language modeling
using information retrieval techniques. In ICASSP,
volume 1, pages 541?544.
Johannes Matiasek and Marco Baroni. 2003. Exploiting
long distance collocational relations in predictive typ-
ing. In EACL-03 Workshop on Language Modeling for
Text Entry, pages 1?8.
Alan Newell, Stefan Langer, andMarianne Hickey. 1998.
The ro?le of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):1?16.
Kristie Seymore and Ronald Rosenfeld. 1997. Using
Story Topics for Language Model Adaptation. In Eu-
rospeech, pages 1987?1990.
Keith Trnka and Kathleen F. McCoy. 2007. Corpus Stud-
ies in Word Prediction. In ASSETS, pages 195?202.
Keith Trnka, Debra Yarrington, Kathleen McCoy, and
Christopher Pennington. 2006. Topic Modeling in
Fringe Word Prediction for AAC. In IUI, pages 276?
278.
Tonio Wandmacher and Jean-Yves Antoine. 2006.
Training Language Models without Appropriate Lan-
guage Resources: Experiments with an AAC System
for Disabled People. In LREC.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In EMNLP, pages
506?513.
66
