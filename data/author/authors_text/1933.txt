Unsupervised Construction of Large Paraphrase Corpora: 
Exploiting Massively Parallel News Sources 
Bill DOLAN, Chris QUIRK, and Chris BROCKETT 
Natural Language Processing Group, Microsoft Research  
One Microsoft Way 
Redmond, WA 90852, USA 
{billdol,chrisq,chrisbkt}@microsoft.com 
 
Abstract 
We investigate unsupervised techniques for 
acquiring monolingual sentence-level 
paraphrases from a corpus of temporally and 
topically clustered news articles collected from 
thousands of web-based news sources. Two 
techniques are employed: (1) simple string edit 
distance, and (2) a heuristic strategy that pairs 
initial (presumably summary) sentences from 
different news stories in the same cluster. We 
evaluate both datasets using a word alignment 
algorithm and a metric borrowed from machine 
translation. Results show that edit distance data 
is cleaner and more easily-aligned than the 
heuristic data, with an overall alignment error 
rate (AER) of 11.58% on a similarly-extracted 
test set.  On test data extracted by the heuristic 
strategy, however, performance of the two 
training sets is similar, with AERs of 13.2% 
and 14.7% respectively. Analysis of 100 pairs 
of sentences from each set reveals that the edit 
distance data lacks many of the complex lexical 
and syntactic alternations that characterize 
monolingual paraphrase. The summary 
sentences, while less readily alignable, retain 
more of the non-trivial alternations that are of 
greatest interest learning paraphrase 
relationships.    
 
1 Introduction 
The importance of learning to manipulate 
monolingual paraphrase relationships for 
applications like summarization, search, and dialog 
has been highlighted by a number of recent efforts 
(Barzilay & McKeown 2001; Shinyama et al 
2002; Lee & Barzilay 2003; Lin & Pantel 2001). 
While several different learning methods have 
been applied to this problem, all share a need for 
large amounts of data in the form of pairs or sets of 
strings that are likely to exhibit lexical and/or 
structural paraphrase alternations. One approach1 
                                                    
1
 An alternative approach involves identifying anchor 
points--pairs of words linked in a known way--and 
collecting the strings that intervene. (Shinyama, et al 
2002; Lin & Pantel 2001). Since our interest is in 
that has been successfully used is edit distance, a 
measure of similarity between strings. The 
assumption is that strings separated by a small edit 
distance will tend to be similar in meaning: 
   
The leading indicators measure the economy? 
The leading index measures the economy?. 
 
Lee & Barzilay (2003), for example, use Multi-
Sequence Alignment (MSA) to build a corpus of 
paraphrases involving terrorist acts.  Their goal is 
to extract sentential templates that can be used in 
high-precision generation of paraphrase alter-
nations within a limited domain.  
 Our goal here is rather different: our interest lies 
in constructing a monolingual broad-domain 
corpus of pairwise aligned sentences. Such data 
would be amenable to conventional statistical 
machine translation (SMT) techniques (e.g., those 
discussed in Och & Ney 2003).2 In what follows 
we compare two strategies for unsupervised 
construction of such a corpus, one employing 
string similarity and the other associating sentences 
that may overlap very little at the string level. We 
measure the relative utility of the two derived 
monolingual corpora in the context of word 
alignment techniques developed originally for 
bilingual text.   
We show that although the edit distance corpus is 
well-suited as training data for the alignment 
algorithms currently used in SMT, it is an 
incomplete source of information about paraphrase 
relations, which exhibit many of the characteristics 
of comparable bilingual corpora or free 
translations. Many of the more complex 
alternations that characterize monolingual 
paraphrase, such as large-scale lexical alternations 
and constituent reorderings, are not readily 
                                                                                 
learning sentence level paraphrases, including major 
constituent reorganizations, we do not address this 
approach here.  
2
 Barzilay & McKeown (2001) consider the 
possibility of using SMT machinery, but reject the 
idea because of the noisy, comparable nature of their 
dataset. 
captured by edit distance techniques, which 
conflate semantic similarity with formal similarity.  
We conclude that paraphrase research would 
benefit by identifying richer data sources and 
developing appropriate learning techniques.  
2 Data/Methodology 
Our two paraphrase datasets are distilled from a 
corpus of news articles gathered from thousands of 
news sources over an extended period. While the 
idea of exploiting multiple news reports for 
paraphrase acquisition is not new, previous efforts 
(for example, Shinyama et al 2002; Barzilay and 
Lee 2003) have been restricted to at most two news 
sources. Our work represents what we believe to 
be the first attempt to exploit the explosion of news 
coverage on the Web, where a single event can 
generate scores or hundreds of different articles 
within a brief period of time. Some of these articles 
represent minor rewrites of an original AP or 
Reuters story, while others represent truly distinct 
descriptions of the same basic facts.  The massive 
redundancy of information conveyed with widely 
varying surface strings is a resource begging to be 
exploited. 
Figure 1 shows the flow of our data collection 
process. We begin with sets of pre-clustered URLs 
which point to news articles on the Web, 
representing thousands of different news sources. 
The clustering algorithm takes into account the full 
text of each news article, in addition to temporal 
cues, to produce a set of topically and temporally 
related articles. Our method is believed to be 
independent of the specific clustering technology 
used. The story text is isolated from a sea of 
advertisements and other miscellaneous text 
through use of a supervised HMM.  
Altogether we collected 11,162 clusters in an 8-
month period, assembling 177,095 articles with an 
average of 15.8 articles per cluster.  The clusters 
are generally coherent in topic and focus. Discrete 
events like disasters, business announcements, and 
deaths tend to yield tightly focused clusters, while 
ongoing stories like the SARS crisis tend to 
produce less focused clusters. While exact 
duplicate articles are filtered out of the clusters, 
many slightly-rewritten variants remain. 
 
2.1 Extracting Sentential Paraphrases 
Two separate techniques were employed to 
extract likely pairs of sentential paraphrases from 
these clusters. The first used string edit distance, 
counting the number of lexical deletions and 
insertions needed to transform one string into 
another. The second relied on a discourse-based 
heuristic, specific to the news genre, to identify 
likely paraphrase pairs even when they have little 
superficial similarity. 
 
3 Levenshtein Distance 
A simple edit distance metric (Levenshtein 
1966) was used to identify pairs of sentences 
within a cluster that are similar at the string level.  
First, each sentence was normalized to lower case 
and paired with every other sentence in the cluster. 
Pairings that were identical or differing only by 
punctuation were rejected, as were those where the 
shorter sentence in the pair was less than two thirds 
the length of the longer, this latter constraint in 
effect placing an upper bound on edit distance 
relative to the length of the sentence. Pairs that had 
been seen before in either order were also rejected. 
Filtered in this way, our dataset yields 139K non-
identical sentence pairs at a Levenshtein distance 
of n ? 12. 3  Mean Levenshtein distance was 5.17, 
and mean sentence length was 18.6 words. We will 
refer to this dataset as L12. 
 
3.1.1 First sentences  
The second extraction technique was 
specifically intended to capture paraphrases which 
might contain very different sets of content words, 
word order, and so on. Such pairs are typically 
used to illustrate the phenomenon of paraphrase, 
but precisely because their surface dissimilarity 
renders automatic discovery difficult, they have 
generally not been the focus of previous 
computational approaches.  
In order to automatically identify sentence pairs 
of this type, we have attempted to take advantage 
of some of the unique characteristics of the dataset. 
The topical clustering is sufficiently precise to 
ensure that, in general, articles in the same cluster 
overlap significantly in overall semantic content. 
Even so, any arbitrary pair of sentences from 
different articles within a cluster is unlikely to 
exhibit a paraphrase relationship: 
 
The Phi-X174 genome is short and compact. 
This is a robust new step that allows us to make much 
larger pieces. 
 
To isolate just those sentence pairs that represent 
likely paraphrases without requiring significant 
string similarity, we exploited a common 
journalistic convention: the first sentence or two of 
                                                    
3A maximum Levenshtein distance of 12 was selected 
for the purposes of this paper on the basis of 
experiments with corpora extracted at various edit 
distances.  
a newspaper article typically summarize its 
content. One might reasonably expect, therefore, 
that initial sentences from one article in a cluster 
will be paraphrases of the initial sentences in other 
articles in that cluster. This heuristic turns out to be 
a powerful one, often correctly associating 
sentences that are very different at the string level: 
 
In only 14 days, US researchers have created an 
artificial bacteria-eating virus from synthetic 
genes. 
An artificial bacteria-eating virus has been made from 
synthetic genes in the record time of just two weeks. 
 
Also consider the following example, in which 
related words are obscured by different parts of 
speech: 
   
Chosun Ilbo, one of South Korea's leading newspapers, 
said North Korea had finished developing a new 
ballistic missile last year and was planning to 
deploy it.  
The Chosun Ilbo said development of the new missile, 
with a range of up to %%number%% kilometres 
(%%number%% miles), had been completed and 
deployment was imminent. 
 
A corpus was produced by extracting the first 
two sentences of each article, then pairing these 
across documents within each cluster. We will 
refer to this collection as the F2 corpus.  The 
combination of the first-two sentences heuristic 
plus topical article clusters allows us to take 
advantage of meta-information implicit in our 
corpus, since clustering exploits lexical 
information from the entire document, not just the 
few sentences that are our focus. The assumption 
that two first sentences are semantically related is 
thus based in part on linguistic information that is 
external to the sentences themselves. 
Sometimes, however, the strategy of pairing 
sentences based on their cluster and position goes 
astray. This would lead us to posit a paraphrase 
relationship where there is none: 
 
Terence Hope should have spent most of yesterday in 
hospital performing brain surgery. 
A leading brain surgeon has been suspended from work 
following a dispute over a bowl of soup. 
 
To prevent too high an incidence of unrelated 
sentences, one string-based heuristic filter was 
found useful: a pair is discarded if the sentences do 
not share at least 3 words of 4+ characters. This 
constraint succeeds in filtering out many unrelated 
pairs, although it can sometimes be too restrictive, 
excluding completely legitimate paraphrases:   
 
There was no chance it would endanger our planet, 
astronomers said. 
NASA emphasized that there was never danger of a 
collision. 
 
An additional filter ensured that the word count 
of the shorter sentence is at least one-half that of 
the longer sentence. Given the relatively long 
sentences in our corpus (average length 18.6 
words), these filters allowed us to maintain a 
degree of semantic relatedness between sentences. 
Accordingly, the dataset encompasses many 
paraphrases that would have been excluded under a 
more stringent edit-distance threshold, for 
example, the following non-paraphrase pair that 
contain an element of paraphrase:  
 
A staggering %%number%% million Americans have 
been victims of identity theft in the last five years , 
according to federal trade commission survey out 
this week.  
In the last year alone, %%number%% million people 
have had their identity purloined. 
 
Nevertheless, even after filtering in these ways,  
a significant amount of unfiltered noise remains in 
the F2 corpus, which consisted of 214K sentence 
pairs. Out of a sample of 448 held-out sentence 
pairs, 118 (26.3%) were rated by two independent 
human evaluators as sentence-level paraphrases, 
while 151 (33.7%) were rated as partial 
paraphrases. The remaining ~40% were assessed as 
 
News article clusters: URLs
Download URLs,
Isolate content (HMM),
Sentence separate
Textual content of articles
Select and filter
first sentence pairs
Approximately parallel
monolingual corpus
 
 
 
Figure 1. Data collection 
 
 
unrelated. 4   Thus, although the F2 data set is 
nominally larger than the L12 data set, when the 
noise factor is taken into account, the actual 
number of full paraphrase sentences in this data set 
is estimated to be in the region of 56K sentences, 
with a further estimated 72K sentences containing 
some paraphrase material that might be a potential 
source of alignment.  
Some of these relations captured in this data can 
be complex. The following pair, for example, 
would be unlikely to pass muster on edit distance 
grounds, but nonetheless contains an inversion of 
deep semantic roles, employing different lexical 
items.    
 
The Hartford Courant reported %%day%% that Tony 
Bryant said two friends were the killers.  
A lawyer for Skakel says there is a claim that the 
murder was carried out by two friends of one of 
Skakel's school classmates, Tony Bryan. 
 
The F2 data also retains pairs like the following 
that involve both high-level semantic alternations 
and long distance dependencies:  
 
Two men who robbed a jeweller's shop to raise funds 
for the Bali bombings were each jailed for 
%%number%% years by Indonesian courts today.  
An Indonesian court today sentenced two men to 
%%number%% years in prison for helping 
finance last year's terrorist bombings in Bali by 
robbing a jewelry store. 
 
These examples do not by any means exhaust 
the inventory of complex paraphrase types that are 
commonly encountered in the F2 data. We 
encounter, among other things, polarity 
alternations, including those involving long-
distance dependencies, and a variety of distributed 
paraphrases, with alignments spanning widely 
separated elements. 
 
3.2 Word Error Alignment Rate 
An objective scoring function was needed to 
compare the relative success of the two data 
collection strategies sketched in 2.1.1 and 2.1.2. 
Which technique produces more data? Are the 
types of data significantly different in character or 
utility? In order to address such questions, we used 
word Alignment Error Rate (AER), a metric 
borrowed from the field of statistical machine 
translation (Och & Ney 2003). AER measures how 
accurately an automatic algorithm can align words 
in corpus of parallel sentence pairs, with a human-
                                                    
4
  This contrasts with 16.7% pairs assessed as 
unrelated in a 10,000 pair sampling of the L12 data.   
tagged corpus of alignments serving as the gold 
standard. Paraphrase data is of course monolingual, 
but otherwise the task is very similar to the MT 
alignment problem, posing the same issues with 
one-to-many, many-to-many, and one/many-to-
null word mappings. Our a priori assumption was 
that the lower the AER for a corpus, the more 
likely it would be to yield learnable information 
about paraphrase alternations.  
We closely followed the evaluation standards 
established in Melamed (2001) and Och & Ney 
(2000, 2003). Following Och & Ney?s 
methodology, two annotators each created an 
initial annotation for each dataset, subcategorizing 
alignments as either SURE (necessary) or POSSIBLE 
(allowed, but not required). Differences were then 
highlighted and the annotators were asked to 
review these cases.  Finally we combined the two 
annotations into a single gold standard in the 
following manner: if both annotators agreed that an 
alignment should be SURE, then the alignment was 
marked as sure in the gold-standard; otherwise the 
alignment was marked as POSSIBLE. 
To compute Precision, Recall, and Alignment 
Error Rate (AER) for the twin datasets, we used 
exactly the formulae listed in Och & Ney (2003).  
Let A be the set of alignments in the comparison, S 
be the set of SURE alignments in the gold standard, 
and P be the union of the SURE and POSSIBLE 
alignments in the gold standard.  Then we have:  
 
||
||precision
A
PA ?
=   
 
||
||
  recall
S
SA ?
=
 
 
||
||AER
SA
SAPA
+
?+?
=  
 
 
We held out a set of news clusters from our 
training data and randomly extracted two sets of 
sentence pairs for blind evaluation. The first is a 
set of 250 sentence pairs extracted on the basis of 
an edit distance of 5 ? n ? 20, arbitrarily chosen to 
allow a range of reasonably divergent candidate 
pairs. These sentence pairs were checked by an 
independent human evaluator to ensure that they 
contained paraphrases before they were tagged for 
alignments. The second set comprised 116 
sentence pairs randomly selected from the set of 
first-two sentence pairs. These were likewise hand-
vetted by independent human evaluators. After an 
initial training pass and refinement of the linking 
specification, interrater agreement measured in 
terms of AER5 was 93.1% for the edit distance test 
set versus 83.7% for the F2 test set, suggestive of 
the greater variability in the latter data set.  
3.3 Data Alignment  
Each corpus was used as input to the word 
alignment algorithms available in Giza++ (Och & 
Ney 2000).  Giza++ is a freely available 
implementation of IBM Models 1-5 (Brown et al 
1993) and the HMM alignment (Vogel et al 1996), 
along with various improvements and 
modifications motivated by experimentation by 
Och & Ney (2000).  Giza++ accepts as input a 
corpus of sentence pairs and produces as output a 
Viterbi alignment of that corpus as well as the 
parameters for the model that produced those 
alignments.  
While these models have proven effective at the 
word alignment task (Mihalcea & Pedersen 2003), 
there are significant practical limitations in their 
output. Most fundamentally, all alignments have 
either zero or one connection to each target word. 
Hence they are unable to produce the many-to-
many alignments required to identify 
correspondences with idioms and other phrasal 
chunks. 
To mitigate this limitation on final mappings, 
we follow the approach of Och (2000): we align 
once in the forward direction and again in the 
backward direction.  These alignments can 
subsequently be recombined in a variety of ways, 
                                                    
5
 The formula for AER given here and in Och & Ney 
(2003) is intended to compare an automatic alignment 
against a gold standard alignment. However, when 
comparing one human against another, both comparison 
and reference distinguish between SURE and POSSIBLE 
links. Because the AER is asymmetric (though each 
direction differs by less than 5%), we have presented the 
average of the directional AERs. 
such as union to maximize recall or intersection to 
maximize precision. Och also documents a method 
for heuristically recombining the unidirectional 
alignments intended to balance precision and 
recall. In our experience, many alignment errors 
are present in one side but not the other, hence this 
recombination also serves to filter noise from the 
process. 
4 Evaluation 
Table 1 shows the results of training translation 
models on data extracted by both methods and then 
tested on the blind data. The best overall 
performance, irrespective of test data type, is 
achieved by the L12 training set, with an 11.58% 
overall AER on the 250 sentence pair edit distance 
test set (20.88% AER for non-identical words). 
The F2 training data is probably too sparse and, 
with 40% unrelated sentence pairs, too noisy to 
achieve equally good results; nevertheless the gap 
between the results for the two training data types 
is dramatically narrower on the F2 test data. The 
nearly comparable numbers for the two training 
data sets, at 13.2% and 14.7% respectively, suggest 
that the L12 training corpus provides no 
substantive advantage over the F2 data when tested 
on the more complex test data. This is particularly 
striking given the noise inherent in the F2 training 
data. 
5 Analysis/Discussion 
To explore some of the differences between the 
training sets, we hand-examined a random sample 
of sentence pairs from each corpus type. The most 
common paraphrase alternations that we observed 
fell into the following broad categories: 
 
? Elaboration: Sentence pairs can differ in total 
information content, with an added word, 
phrase or clause in one sentence that has no 
Training Data Type: L12 F2 L12 F2 
Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic 
Precision   87.46% 86.44% 85.07% 84.16% 
Recall      89.52% 82.64% 88.70% 86.55% 
AER         11.58% 15.41% 13.24% 14.71% 
Identical word precision   89.36% 88.79% 92.92% 93.41% 
Identical word recall      89.50% 83.10% 93.49% 92.47% 
Identical word AER         10.57% 14.14% 6.80% 7.06% 
Non-Identical word precision   76.99% 71.86% 60.54% 53.69% 
Non-Identical word recall      90.22% 69.57% 59.50% 50.41% 
Non-Identical word AER         20.88% 28.57% 39.81% 47.46% 
 
 
Table 1.  Precision, recall, and alignment error rates (AER) for F2 and L12 
 
counterpart in the other (e.g. the NASDAQ /  
the tech-heavy NASDAQ). 
? Phrasal: An entire group of words in one 
sentence alternates with one word or a phrase 
in the other.  Some are non-compositional 
idioms (has pulled the plug on / is dropping 
plans for); others involve different phrasing 
(electronically / in electronic form, more than 
a million people / a massive crowd). 
? Spelling: British/American sources system-
atically differ in spellings of common words 
(colour / color); other variants also appear 
(email / e-mail). 
? Synonymy:  Sentence pairs differ only in one 
or two words (e.g. charges / accusations), 
suggesting an editor?s hand in modifying a 
single source sentence. 
? Anaphora: A full NP in one sentence  
corresponds to an anaphor in the other (Prime 
Minister Blair / He). Cases of NP anaphora 
(ISS / the Atlanta-based security company) are 
also common in the data, but in quantifying 
paraphrase types we restricted our attention to 
the simpler case of pronominal anaphora.  
? Reordering: Words, phrases, or entire 
constituents occur in different order in two 
related sentences, either because of major 
syntactic differences (e.g. topicalization, voice 
alternations) or more local pragmatic choices 
(e.g. adverb or prepositional phrase placement).  
 
These categories do not cover all possible 
alternations between pairs of paraphrased 
sentences; moreover, categories often overlap in 
the same sequence of words. It is common, for 
example, to find instances of clausal Reordering 
combined with Synonymy. 
Figure 2 shows a hand-aligned paraphrase pair 
taken from the F2 data. This pair displays one 
Spelling alternation (defence / defense), one 
Reordering (position of the ?since? phrase), and 
one example of Elaboration (terror attacks occurs 
in only one sentence).    
To quantify the differences between L12 and F2, 
we randomly chose 100 sentence pairs from each 
dataset and counted the number of times each 
phenomenon was encountered. A given sentence 
pair might exhibit multiple instances of a single 
phenomenon, such as two phrasal paraphrase 
changes or two synonym replacements.  In this 
case all instances were counted. Lower-frequency 
changes that fell outside of the above categories 
were not tallied: for example, the presence or 
absence of a definite article (had authority / had 
the authority) in Figure 2 was ignored.  After 
summing all alternations in each sentence pair, we 
calculated the average number of occurrences of 
each paraphrase type in each data set.  The results 
are shown in Table 2. 
Several major differences stand out between the 
two data sets.  First, the F2 data is less parallel, as 
evidenced by the higher percentage of Elaborations 
found in those sentence pairs. Loss of parallelism, 
however, is offset by greater diversity of 
paraphrase types encountered in the F2 data. 
Phrasal alternations are more than 4x more 
common, and Reorderings occur over 20x more 
frequently.   Thus while string difference methods 
may produce relatively clean training data, this is 
achieved at the cost of filtering out common (and 
interesting) paraphrase relationships. 
 
6 Conclusions and Future Work 
Edit distance identifies sentence pairs that 
exhibit lexical and short phrasal alternations that 
can be aligned with considerable success. Given a 
large dataset and a well-motivated clustering of 
documents, useful datasets can be gleaned even 
without resorting to more sophisticated techniques 
 
 
Figure 2.   Sample human-aligned paraphrase 
 
 L12 F2 
Elaboration 0.83 1.3 
Phrasal 0.14 0.69 
Spelling 0.12 0.01 
Synonym 0.18 0.25 
Anaphora 0.1 0.13 
Reordering 0.02 0.41 
 
 
Table 2.  Mean number of instances of 
paraphrase phenomena per sentence 
 
(such as Multiple Sequence Alignment, as 
employed by Barzilay & Lee 2003).  
However, there is a disparity between the kinds 
of paraphrase alternations that we need to be able 
to align and those that we can already align well 
using current SMT techniques. Based solely on the 
criterion of word AER, the L12 data would seem to 
be superior to the F2 data as a source of paraphrase 
knowledge.  Hand evaluation, though, indicates 
that many of the phenomena that we are interested 
in learning may be absent from this L12 data. 
String edit distance extraction techniques involve 
assumptions about the data that are inadequate, but 
achieve high precision.  Techniques like our F2 
extraction strategies appear to extract a more 
diverse variety of data, but yield more noise.  We 
believe that an approach with the strengths of both 
methods would lead to significant improvement in 
paraphrase identification and generation.   
In the near term, however, the relatively similar 
performances of F2 and L12-trained models on the 
F2 test data suggest that with further refinements, 
this more complex type of data can achieve good 
results. More data will surely help. 
One focus of future work is to build a classifier 
to predict whether two sentences are related 
through paraphrase. Features might include edit 
distance, temporal/topical clustering information, 
information about cross-document discourse 
structure, relative sentence length, and synonymy 
information. We believe that this work has 
potential impact on the fields of summarization, 
information retrieval, and question answering.   
Our ultimate goal is to apply current SMT 
techniques to the problems of paraphrase 
recognition and generation. We feel that this is a 
natural extension of the body of recent 
developments in SMT; perhaps explorations in 
monolingual data may have a reciprocal impact. 
The field of SMT, long focused on closely aligned 
data, is only now beginning to address the   kinds 
of problems immediately encountered in 
monolingual paraphrase (including phrasal 
translations and large scale reorderings).  
Algorithms to address these phenomena will be 
equally applicable to both fields. Of course a 
broad-domain SMT-influenced paraphrase solution 
will require very large corpora of sentential 
paraphrases. In this paper we have described just 
one example of a class of data extraction 
techniques that we hope will scale to this task. 
Acknowledgements 
We are grateful to the Mo Corston-Oliver, Jeff 
Stevenson and Amy Muia of the Butler Hill Group 
for their work in annotating the data used in the 
experiments. We have also benefited from 
discussions with Ken Church, Mark Johnson, 
Daniel Marcu and Franz Och. We remain, 
however, responsible for all content.  
References  
R. Barzilay and K. R. McKeown. 2001. Extracting 
Paraphrases from a parallel corpus. In Proceedings of 
the ACL/EACL. 
R. Barzilay and  L. Lee. 2003. Learning to Paraphrase: 
an unsupervised approach using multiple-sequence 
alignment. In Proceedings of HLT/NAACL. 
P. Brown, S. A. Della Pietra, V.J. Della Pietra and R. L. 
Mercer. 1993. The Mathematics of Statistical 
Machine Translation. Computational Linguistics, 
19(2): 263-311. 
V. Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions, and reversals. Soviet 
Physice-Doklady, 10:707-710. 
D. Lin and P. Pantel. 2001. DIRT - Discovery of 
Inference Rules from Text. In Proceedings of ACM 
SIGKDD Conference on Knowledge Discovery and 
Data Mining. 
I. D. Melamed. 2001. Empirical Methods for Exploiting 
Parallel Texts.  MIT Press.  
R. Mihalcea and T. Pedersen. 2003 An Evaluation 
Exercise for Word Alignment. In Proceedings of the 
Workshop on Building and Using Parallel Texts: 
Data Driven Machine Translation and Beyond. May 
31, 2003. Edmonton, Canada. 
F. Och and H. Ney. 2000. Improved Statistical 
Alignment Models.  In Proceedings of the 38th 
Annual Meeting of the ACL, Hong Kong, China. 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models.  
Computational Linguistics, 29(1):19-52. 
Y. Shinyama, S. Sekine and K. Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles.  In 
Proceedings of NAACL-HLT. 
S. Vogel, H. Ney and C. Tillmann. 1996. HMM-Based 
Word Alignment in Statistical Translation. In 
Proceedings of the Annual Meeting of the ACL, 
Copenhagen, Denmark.  
 
 
 
Support Vector Machines for Paraphrase Identification  
and Corpus Construction 
Chris Brockett and William B. Dolan 
Natural Language Processing Group 
Microsoft Research 
One Microsoft Way, Redmond, WA 98502, U.S.A. 
{chrisbkt, billdol}@microsoft.com 
Abstract 
The lack of readily-available large cor-
pora of aligned monolingual sentence 
pairs is a major obstacle to the devel-
opment of Statistical Machine Transla-
tion-based paraphrase models. In this 
paper, we describe the use of annotated 
datasets and Support Vector Machines 
to induce larger monolingual para-
phrase corpora from a comparable cor-
pus of news clusters found on the 
World Wide Web.  Features include: 
morphological variants; WordNet 
synonyms and hypernyms; log-
likelihood-based word pairings dy-
namically obtained from baseline sen-
tence alignments; and formal string 
features such as word-based edit dis-
tance. Use of this technique dramati-
cally reduces the Alignment Error Rate 
of the extracted corpora over heuristic 
methods based on position of the sen-
tences in the text.  
1 Introduction 
Paraphrase detection?the ability to determine 
whether or not two formally distinct strings are 
similar in meaning?is increasingly recognized 
as crucial to future applications in multiple 
fields including Information Retrieval, Question 
Answering, and Summarization. A growing 
body of recent research has focused on the prob-
lems of identifying and generating paraphrases, 
e.g., Barzilay & McKeown (2001), Lin & Pantel 
(2002), Shinyama et al (2002), Barzilay & Lee 
(2003), and Pang et al (2003). One promising 
approach extends standard Statistical Machine 
Translation (SMT) techniques (e.g., Brown et al, 
1993; Och & Ney, 2000, 2003) to the problems 
of monolingual paraphrase identification and 
generation. Finch et al (2004) have described 
several MT based paraphrase systems within the 
context of improving machine translation output. 
Quirk et al (2004) describe an end-to-end para-
phrase identification and generation system us-
ing GIZA++ (Och & Ney, 2003) and a 
monotone decoder to generate information-
preserving paraphrases.  
As with conventional SMT systems, SMT-
based paraphrase systems require extensive 
monolingual parallel training corpora. However, 
while translation is a common human activity, 
resulting in large corpora of human-translated 
bilingual sentence pairs being relatively easy to 
obtain across multiple domains and language 
pairs, this is not the case in monolingual para-
phrase, where naturally-occurring parallel data 
are hard to come by. The paucity of readily 
available monolingual parallel training corpora 
poses a formidable obstacle to the development 
of SMT-based paraphrase systems.   
The present paper describes the extraction of 
parallel corpora from clustered news articles 
using annotated seed corpora and an SVM clas-
sifier, demonstrating that large parallel corpora 
can be induced by a classifier that includes mor-
phological and synonymy features derived from 
both static and dynamic resources.  
2 Background 
Two broad approaches have dominated the lit-
erature on constructing paraphrase corpora. One 
1
approach utilizes multiple translations of a sin-
gle source language text, where the source lan-
guage text guarantees semantic equivalence in 
the target language texts (e.g., Barzilay & 
McKeown, 2001; Pang et al, 2003). Such cor-
pora are of limited availability, however, since 
multiple translations of the same document are 
uncommon in non-literary domains.  
The second strain of corpora construction in-
volves mining paraphrase strings or sentences 
from news articles, with document clustering 
typically providing the topical coherence neces-
sary to boost the likelihood that any two arbi-
trary sentences in the cluster are paraphrases. In 
this vein, Shinyama et al (2002) use named en-
tity anchors to extract paraphrases within a nar-
row domain. Barzilay & Lee (2003) employ 
Multiple Sequence Alignment (MSA, e.g., 
Durbin et al, 1998) to align strings extracted 
from closely related news articles. Although the 
MSA approach can produce dramatic results, it 
is chiefly effective in extracting highly templatic 
data, and appears to be of limited extensibility to 
broad domain application (Quirk et al 2004).  
Recent work by Dolan, et al (2004) describes 
the construction of broad-domain corpora of 
aligned paraphrase pairs extracted from news-
cluster data on the World Wide Web using two 
heuristic strategies: 1) pairing sentences based 
on a word-based edit distance heuristic; and 2) a 
naive text-feature-based heuristic in which the 
first two sentences of each article in a cluster are 
cross-matched with each other, their assumption 
being that the early sentences of a news article 
will tend to summarize the whole article and are 
thus likely to contain the same information as 
other early sentences of other articles in the 
cluster. The word-based edit distance heuristic 
yields pairs that are relatively clean but offer 
relatively minor rewrites in generation, espe-
cially when compared to the MSA model of 
(Barzilay & Lee, 2003). The text-based heuristic, 
on the other hand, results in a noisy ?compara-
ble? corpus: only 29.7% of sentence pairs are 
paraphrases, resulting in degraded performance 
on alignment metrics. This latter technique, 
however, does afford large numbers of pairings 
that are widely divergent at the string level; cap-
turing these is of primary interest to paraphrase 
research. In this paper, we use an annotated cor-
pus and an SVM classifier to refine the output of 
this second heuristic in an attempt to better iden-
tify sentence pairs containing richer paraphrase 
material, and minimize the noise generated by 
unwanted and irrelevant data. 
3 Constructing a Classifier 
3.1 Sequential Minimal Optimization 
Although any of a number of machine learning 
algorithms, including Decision Trees, might be 
equally applicable here, Support Vector Ma-
chines (Vapnik, 1995) have been extensively 
used in text classification  problems and with 
considerable success (Dumais 1998; Dumais et 
al., 1998; Joachims 2002). In particular, SVMs 
are known to be robust in the face of noisy train-
ing data. Since they permit solutions in high di-
mensional space, SVMs lend themselves readily 
to bulk inclusion of lexical features such as 
morphological and synonymy information. 
For our SVM, we employed an off-the-shelf 
implementation of the Sequential Minimal Op-
timization (SMO) algorithm described in Platt 
(1999). 1  SMO offers the benefit of relatively 
short training times over very large feature sets, 
and in particular, appears well suited to handling 
the sparse features encountered in natural lan-
guage classification tasks. SMO has been de-
1 The pseudocode for SMO may be found in the appendix of Platt (1999)
Edit Distance
(e ? 12) 
San Jose Medical Center announced 
Wednesday that it would close its 
doors by Dec. 1, 2004. 
San Jose Medical Center has an-
nounced that it will close its 
doors by Dec. 1, 2004. 
First Two 
Sentences 
The genome of the fungal pathogen 
that causes Sudden Oak Death has 
been sequenced by US scientists
Researchers announced Thursday 
they've completed the genetic 
blueprint of the blight-causing 
culprit responsible for Sudden Oak 
Death
Table 1.  Paraphrase Examples Identified by Two Heuristics  
2
ployed a variety of text classification tasks (e.g., 
Dumais 1998; Dumais et al, 1998). 
3.2 Datasets 
To construct our corpus, we collected news arti-
cles from news clusters on the World Wide Web. 
A database of 13,127,938 candidate sentence 
pairs was assembled from 9,516,684 sentences 
in 32,408 clusters collected over a 2-year period, 
using simple heuristics to identify those sen-
tence pairs that were most likely to be para-
phrases, and thereby prune the overall search 
space.  
Word-based Levenshtein edit distance 
of 1 < e ? 20; and a length ratio 
> 66%; OR
Both sentences in the first three 
sentences of each file; and length 
ratio > 50%. 
From this database, we extracted three data-
sets. The extraction criteria, and characteristics 
of these datasets are given in Table 2. The data 
sets are labled L(evenshtein) 12, F(irst) 2 and 
F(irst) 3 reflecting their primary selection char-
acteristics. The L12 dataset represents the best 
case achieved so far, with Alignment Error 
Rates beginning to approach those reported for 
alignment of closely parallel bilingual corpora. 
The F2 dataset was constructed from the first 
two sentences of the corpus on the same as-
sumptions as those used in Dolan et al (2004). 
To avoid conflating the two data types, however, 
sentence pairs with an edit distance of 12 or less 
were excluded. Since this resulted in a corpus 
that was significantly smaller than that desirable 
for exploring extraction techniques, we also cre-
ated a third data set, F3 that consisted of the 
cross-pairings of the first three sentences of each 
article in each cluster, excluding those where 
the edit distance is e ? 12.   
3.3 Training Data 
Our training data consisted of 10,000 sentence 
pairs extracted from randomly held-out clusters 
and hand-tagged by two annotators according to 
whether in their judgment (1 or 0) the sentence 
pairs constituted paraphrases. The annotators 
were presented with the sentences pairs in isola-
tion, but were informed that they came from 
related document sets (clusters). A conservative 
interpretation of valid paraphrase was adopted: 
if one sentence was a superstring of the other, 
e.g., if a clause had no counterpart in the other 
sentence, the pair was counted as a non-
paraphrase. Wherever the two annotators dis-
agreed, the pairs were classed as non-
paraphrases. The resultant data set contains 2968 
positive and 7032 negative examples.  
3.4 Features 
Some 264,543 features, including overt lexical 
pairings, were in theory available to the classi-
fier. In practice, however, the number of dimen-
sions used typically fell to less than 1000 after 
the lowest frequency features are eliminated (see 
Table 4.) The main feature classes were: 
String Similarity Features: All sentence pairs 
were assigned string-based features, includ-
ing absolute and relative length in words, 
number of shared words, word-based edit 
distance, and lexical distance, as measured 
by converting the sentences into alphabet-
ized strings of unique words and applying 
word based edit distance. 
Morphological Variants: Another class of 
features was co-ocurrence of morphological 
variants in sentence pairs. Approximately 
490,000 sentences in our primary datasets 
were stemmed using a rule-based stemmer, 
to yield a lexicon of 95,422 morphologically 
variant word pairs. Each word pair was 
treated as a feature. Examples are: 
orbit|orbital
orbiter|orbiting
WordNet Lexical Mappings: Synonyms and 
hypernyms were extracted from WordNet, 
L12 F2 F3 
Corpus size 253,725 51,933 235,061 
Levenshtein 
edit distance 1 < e ? 12 e > 12 e > 12 
Sentence range 
in article All First two First three
Length 5 < n < 30 5 < n < 30 5 < n < 30
Length ratio 66% 50% 50% 
Shared words 3 3 3 
Table 2. Characteristics of L(evenshtein) 12, 
F(irst) 2, and F(irst) 3 Data 
3
(http://www.cogsci.princeton.edu/~wn/;
Fellbaum, 1998), using the morphological 
variant lexicon from the 490,000 sentences 
as keywords. The theory here is that as addi-
tional paraphrase pairs are identified by the 
classifier, new information will ?come 
along for the ride,? thereby augmenting the 
range of paraphrases available to be learned. 
A lexicon of 314,924 word pairs of the fol-
lowing form created. Only those pairs iden-
tified as occurring in either training data or 
the corpus to be classified were included in 
the final classifier. 
operation|procedure
operation|work
Word Association Pairs: To augment the 
above resources, we dynamically extracted 
from the L12 corpus a lexicon of 13001 
possibly-synonymous word pairs using a 
log-likelihood algorithm described in Moore 
(2001) for machine translation. To minimize 
the damping effect of the overwhelming 
number of identical words, these were de-
leted from each sentence pair prior to proc-
essing; the algorithm was then run on the 
non-identical residue as if it were a bilingual 
parallel corpus.  
To deploy this data in the SVM feature set, 
a cutoff was arbitrarily selected that yielded 
13001 word pairs. Some exemplars (not 
found in WordNet) include:
straight|consecutive
vendors|suppliers
Fig. 1 shows the distribution of word pair-
ings obtained by this method on the L12 
corpus in comparison with WordNet. Ex-
amination of the top-ranked 1500 word 
pairs reveals that 46.53% are found in 
WordNet and of the remaining 53.47%, 
human judges rated 56% as good, yielding 
an overall ?goodness score? of 76.47%. 
Judgments were by two independent raters. 
For the purposes of comparison, we auto-
matically eliminated pairs containing trivial 
substring differences, e.g., spelling errors, 
British vs. American spellings, singu-
lar/plural alternations, and miscellaneous 
short abbreviations. All pairs on which the 
raters disagreed were discarded. Also dis-
carded were a large number of partial 
phrasal matches of the ?reported|according? 
and ?where|which? type, where part of a 
phrase (?according to?, ?in which?) was 
missing. Although viewed in isolation these 
do not constitute valid synonym or hyper-
rnym pairs, the ability to identify these par-
tial matchings is of central importance 
within an SMT-framework of paraphrase 
alignment and generation. These results 
suggest, among other things, that dynami-
cally-generated lexical data of this kind 
might be useful in increasing the coverage 
of hand-built synonymy resources. 
Composite Features: From each of the lexi-
cal feature classes, we derived a set of more 
abstract features that summarized the fre-
quency with which each feature or class of 
features occurred in the training data, both 
independently, and in correlation with others.  
These had the effect of performing normali-
zation for sentence length and other factors. 
Some examples are: 
No_of_List_2_Words (i.e., the 
count of Wordnet matches)
30.00%
35.00%
40.00%
45.00%
50.00%
55.00%
60.00%
500
1000
1500
2000Word Pairs
Not in Wordnet
In WordNet
Fig. 1.  WordNet Coverage in Word Associa-
tion Output 
4
External_Matches_2_LED (i.e,, 
the ratio of total lexical matches to 
Levenshtein edit distance.) 
4 Evaluation
4.1 Methodology 
Evaluation of paraphrase recognition within an 
SMT framework is highly problematic, since no 
technique or data set is standardly recognized. 
Barzilay & Lee (2003) and Quirk et al (2004) 
use human evaluations of end-to-end generation, 
but these are not very useful here, since they add 
an additional layer of uncertainty into the 
evaluation, and depend to a significant extent on 
the quality and functionality of the decoder.  
Dolan & Brockett (2005) report extraction pre-
cision of 67% using a similar classifier, but with 
the explicit intention of creating a corpus that 
contained a significant number of naturally-
occuring paraphrase-like negative examples. 
Since our purpose in the present work  is non-
application specific corpus construction, we ap-
ply an automated technique that is widely used 
for reporting intermediate results in the SMT 
community, and is being extended in other fields 
such as summarization (Daum? and Marcu, 
forthcoming), namely word-level alignment us-
ing an off-the-shelf implementation of the SMT 
system GIZA++ (Och & Ney, 2003). Below, we 
use Alignment Error Rate (AER), which is in-
dicative of how far the corpus is from providing 
a solution under a standard SMT tool. This al-
lows the effective coverage of an extracted cor-
pus to be evaluated efficiently, repeatedly 
against a single standard, and at little cost after 
the initial tagging. Further, if used as an objec-
tive function, the AER technique offers the 
prospect of using hillclimbing or other optimiza-
tion techniques for non-application-specific cor-
pus extraction. 
To create the test set, two human annotators 
created a gold standard word alignment on held 
out data consisting of 1007 sentences pairs. Fol-
lowing the practice of Och & Ney (2000, 2003), 
the annotators each created an initial annotation, 
categorizing alignments as either SURE (neces-
sary) or POSSIBLE (allowed, but not required). In 
the event of differences, annotators were asked 
to review their choices. First pass inter-rater 
agreement was 90.28%, climbing to 94.43% on 
the second pass. Finally we combined the anno-
tations into a single gold standard as follows: if 
both annotators agreed that an alignment was 
SURE, it was tagged as SURE in the gold-
standard; otherwise it was tagged as POSSIBLE.
To compute Precision, Recall, and Alignment 
Error Rate (AER), we adhere to the formulae 
listed in Och & Ney (2003). Let A be the set of 
alignments in the comparison, S be the set of 
SURE alignments in the gold standard, and P be 
the union of the SURE and POSSIBLE alignments 
in the gold standard:  
||
||precision
A
PA?
= ; ||
||
recall
S
SA?
=
||
||||AER
SA
SAPA
+
?+?
=
4.2 Baselines 
Evaluations were performed on the heuristi-
cally-derived L12, F2, and F3 datasets using the 
above formulation. Results are shown in Table 3.  
L12 represents the best case, followed respec-
tively by F3 and F2.  AERs were also computed 
separately for identical (Id) and non-identical 
(Non-Id) word mappings in order to be able to  
Corpus 
Size
(pairs) 
Precision Recall AER Id AER Non Id AER 
L12 ~254 K 87.42% 87.66% 12.46% 11.57% 21.25% 
F2 ~52 K 85.56% 83.31% 15.57% 13.19% 39.08% 
F3 ~235K 86.53% 81.57% 15.99% 14.24% 33.83% 
10K Trained ~24 K 86.93% 87.24% 12.92% 11.69% 24.70% 
MSR Trained  ~50 K 86.76% 86.39% 13.42% 11.92% 28.31% 
Table 3.  Precision, Recall and Alignment Error Rates 
5
drill down on the extent to which new non-
identical mappings are being learned from the 
data. A high Id error rate can be considered in-
dicative of noise in the data. The score that we 
are most interested in, however, is the Non-Id 
alignment error rate, which can be considered 
indicative of coverage as represented by the 
Giza++ alignment algorithm?s ability to learn 
new mappings from the training data. It will be 
observed that the F3 dataset non-Id AER is 
smaller than that of the F2 dataset: it appears 
that more data is having the desired effect.  
Following accepted SMT practice, we added 
a lexicon of identical word mappings to the 
training data, since Giza++ does not directly 
model word identity, and cannot easily capture 
the fact that many words in paraphrase sentence 
may translate as themselves. We did not add in 
word pairs derived from word association data 
or other supplementary resources that might 
help resolve matches between unlike but seman-
tically similar words.  
4.3 Training on the 10K Data 
We trained an SVM on the 10 K training set 
employing 3-fold cross-validation on the train-
ing set itself.  Validation errors were typically in 
the region of 16-17%. Linear kernels with de-
fault parameters (tolerance=1e-3; margin size 
computed automatically; error probability=0.5) 
were employed throughout. Applying the SVM 
to the F3 data, using 946 features encountered in 
the training data with frequency > 4, this classi-
fier yielded a set of 24588 sentence pairs, which 
were then aligned using Giza++.   
The alignment result is shown in Table 3. The 
?10K Trained? row represents the results of ap-
plying Giza++ to the data extracted by the SVM. 
Non-identical word AER, at 24.70%, shows a 
36.9% reduction in the non-identical word AER 
over the F2 dataset (which is approximately 
double the size), and approximately 28% over 
the original F3 dataset. This represents a huge 
improvement in the quality of the data collected 
by using the SVM and is within striking distance 
of the score associated with the L12 best case. 
The difference is especially significant when it 
is considered that the newly constructed corpus 
is less than one-tenth the size of the best-case 
corpus. Table 5 shows sample extracted sen-
tences. 
To develop insights into the relative contribu-
tions of the different feature classes, we omitted 
some feature classes from several runs. The re-
sults were generally indistinguishable, except 
for non-Id AER, shown in Table 4, a fact that 
may be taken to indicate that string-based fea-
tures such as edit distance still play a major role.  
Eliminating information about morphological 
alternations has the largest overall impact, pro-
ducing a degradation of a 0.94 in on Non-Id 
AER. Of the three feature classes, removal of 
WordNet appears to have the least impact, 
showing the smallest change in Non-Id AER.  
When the word association algorithm is ap-
plied to the extracted ~24K-sentence-pair set, 
degradation in word pair quality occurs signifi-
cantly earlier than observed for  the L12 data; 
after removing ?trivial? matches, 22.63% of 
word pairs in the top ranked 800 were found in 
Wordnet, while 25.3% of the remainder were 
judged to be ?good? matches. This is equivalent 
to an overall ?goodness score? of 38.25%. The 
rapid degradation of goodness may be in part 
attributable to the smaller corpus size yielded by 
the classifier. Nevertheless, the model learns 
many valid new word pairs. Given enough data 
with which to bootstrap, it may be possible to do 
away with static resources such as Wordnet, and 
rely entirely on dynamically derived data.   
4.4 Training on the MSR Training Set 
By way of comparison, we also explored appli-
cation of the SVM to the training data in the 
MSR Paraphrase corpus. For this purpose we 
used the 4076-sentence-pair ?training? section 
of the MSR corpus, comprising 2753 positive 
and 1323 negative examples. The results at de-
fault parameter settings are given in Table 3, 
with respect to all features that were observed to 
occur with frequency greater than 4. Although 
the 49914 sentence pairs yielded by using the 
    Dimensions Non Id AER
All (fq > 4) 946 24.70
No Lexical Pairs 230 25.35 
No Word  
Association 470 25.35 
No WordNet  795 25.24 
No Morphology 813 25.64 
Table 4.  Effect of Eliminating Feature Classes 
on 10K Training Set 
6
MSR Paraphrase Corpus is nearly twice that of 
the 10K training set, AER performance is meas-
urably degraded. Nevertheless, the MSR-trained 
corpus outperforms the similar-sized F12, yield-
ing a reduction in Non-Id AER of a not insig-
nificant 16%.   
The fact that the MSR training data does not 
perform as well as the 10 K training set probably 
reflects its derivative nature, since it was origi-
nally constructed with data collected using the 
10K training set, as described in Dolan & 
Brockett (2005). The performance of the MSR 
corpus is therefore skewed to reflect the biases 
inherent in its original training, and therefore 
exhibits the performance degradation commonly 
associated with bootstrapping. It is also a sig-
nificantly smaller training set, with a higher 
proportion of negative examples than in typical 
in real world data. It will probably be necessary 
to augment the MSR training corpus with further 
negative examples before it can be utilized ef-
fectively for training classifiers. 
5 Discussion and Future Work 
These results show that it is possible to use ma-
chine learning techniques to induce a corpus of 
likely sentential paraphrase pairs whose align-
ment properties measured in terms of AER ap-
proach those of a much larger, more 
homogeneous dataset collected using a string-
edit distance heuristic. This result supports the 
idea that an abstract notion of paraphrase can be 
captured in a high dimensional model.  
Future work will revolve around optimizing 
classifiers for different domains, corpus types 
and training sets. It seems probable that the ef-
fect of the 10K training corpus can be greatly 
augmented by adding sentence pairs that have 
been aligned from multiple translations using 
the techniques described in, e.g., Barzilay & 
McKeown (2001) and Pang et al (2003).  
6 Conclusions
We have shown that supervised machine 
learning techniques such as SVMs can signifi-
cantly expand available paraphrase corpora, and 
achieve a reduction of noise as measured by 
AER on non-identical words.  
Although from the present research has fo-
cused on ?ready-made? news clusters found on 
the web, nothing in this paper depends on the 
availability of such clusters. Given standard 
clustering techniques, the approach that we have 
described for inductive classifier learning should 
in principle be applicable to any flat corpus 
which contains multiple sentences expressing 
similar content. We expect also that the tech-
niques described here could be extended to iden-
tify bilingual sentence pairs in comparable 
corpora, helping automate the construction of 
corpora for machine translation. 
The ultimate test of paraphrase identification 
technologies lies in applications. These are 
likely to be in fields such as extractive multi-
document summarization where paraphrase de-
tection might eliminate sentences with compara-
ble content and Question Answering, for both 
identifying sentence pairs with comparable con-
tent and generating unique new text. Such prac-
young female chimps learn skills 
earlier , spend more time studying 
and tend to do better than young 
male chimpanzees - at least when it 
comes to catching termites . 
young female chimpanzees are better stu-
dents than males , at least when it 
comes to catching termites , according 
to a study of wild chimps in tanzania 's 
gombe national park . Paraphrase
(accepted)  a %%number%% -year-old girl was 
arrested , handcuffed and taken 
into custody on charges of stealing 
a rabbit and a small amount of 
money from a neighbor 's home . 
sheriff 's deputies in pasco county , 
fla. , this week handcuffed and ques-
tioned a %%number%% -year-old girl who 
was accused of stealing a rabbit 
and  %%money%%  from a neighbor 's 
home . 
Non-
Paraphrase
(rejected) 
roy moore , the chief justice of 
alabama , installed the two-ton 
sculpture in the rotunda of his 
courthouse in montgomery , and has 
refused to remove it . 
the eight associate justices of alabama 
's supreme court voted unani-
mously  %%day%%  to overrule moore and 
comply with u.s. district judge myron 
thompson 's order to remove the monu-
ment . 
Table 5.  Sample Pairs Extracted and Rejected by the SVM Trained on the 10K Corpus 
7
tical applications will only be possible once 
large corpora are available to permit the devel-
opment of robust paraphrase models on the scale 
of the best SMT models. We believe that the 
corpus construction techniques that we have de-
scribed here represent an important contribution 
to this goal.      
Acknowledgements 
We would like to thank Monica Corston-Oliver, 
Jeff Stevenson, Amy Muia and Margaret Salome 
of Butler Hill Group LLC for their assistance in 
annotating and evaluating our data. This paper 
has also benefited from feedback from several 
anonymous reviewers. All errors and omissions 
are our own. 
References  
Regina Barzilay and Katherine. R. McKeown. 2001. 
Extracting Paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL.
Regina Barzilay and  Lillian Lee. 2003. Learning to 
Paraphrase; an unsupervised approach using mul-
tiple-sequence alignment. In Proceedings of 
HLT/NAACL 2003.
P. Brown, S. A. Della Pietra, V.J. Della Pietra and R. 
L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation. Computational Linguistics,
Vol. 19(2): 263-311. 
Hal Daum? III and Daniel Marcu. (forthcoming)  
Induction of Word and Phrase Alignments for 
Automatic Document Summarization. To appear 
in Computational Linguistics.
William. B. Dolan, Chris Quirk, and Chris Brockett. 
2004. Unsupervised Construction of Large Para-
phrase Corpora: Exploiting Massively Parallel 
News Sources. Proceedings of COLING 2004,
Geneva, Switzerland.  
William B. Dolan and Chris Brockett. 2005. Auto-
matically Constructing a Corpus of Sentential 
Paraphrases. In Proceedings of The Third Interna-
tional Workshop on Paraphrasing (IWP2005), 
Jeju, Republic of Korea.
Susan Dumais. 1998. Using SVMs for Text Catego-
rization. IEEE Intelligent Systems, Jul.-Aug. 1998: 
21-23 
Susan Dumais, John Platt, David Heckerman, Me-
hran Sahami. 1998. Inductive learning algorithms 
and representations for text categorization. In Pro-
ceedings of the Seventh International Conference 
on Information and Knowledge Management. 
Richard Durbin, Sean R. Eddy, Anders Krogh, and 
Graeme Mitchison. 1998. Biological sequence 
analysis: Probabilistic models of proteins and nu-
cleic acids. Cambridge University Press.  
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press. 
Andrew Finch, Taro Watanabe, Yasuhiro Akiba and 
Eiichiro Sumita. 2004. Paraphrasing as Machine 
Translation. Journal of Natural Language Proc-
essing, 11(5), pp 87-111. 
Thorsten Joachims. 2002.  Learning to Classify Text 
Using Support Vector Machines: Methods, Theory, 
and Algorithms. Kluwer Academic Publishers, 
Boston/Dordrecht/London. 
Microsoft Research Paraphrase Corpus. 
http://research.microsoft.com/research/downloads/
default.aspx 
Robert C. Moore. 2001.  Towards a Simple and Ac-
curate Statistical Approach to Learning Transla-
tion Relationships among Words. In Proceedings 
of the Workshop on Data-Driven Machine Trans-
lation, ACL 2001. 
Franz Joseph Och and H. Ney. 2000. Improved Sta-
tistical Alignment Models.  In Proceedings of the 
38th Annual Meeting of the ACL, Hong Kong, 
China, pp 440-447. 
Franz Joseph Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models.  Computational Linguistics, 29 (1): 
19-52. 
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. 
Syntax-based Alignment of Multiple Translations: 
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of NAACL-HLT.
John C. Platt. 1999. Fast Training of Support Vector 
Machines Using Sequential Minimal Optimization. 
In Bernhard Sch?lkopf, Christopher J. C. Burges 
and Alexander J. Smola (eds.). 1999.  Advances in 
Kernel Methods: Support Vector Learning. The 
MIT Press, Cambridge, MA. 185-208. 
Quirk, Chris, Chris Brockett, and William B. Dolan. 
2004. Monolingual Machine Translation for Para-
phrase Generation, In Proceedings of the 2004 
Conference on Empirical Methods in Natural 
Language Processing, 25-26 July 2004, Barcelona 
Spain, pp. 142-149. 
Bernhard Sch?lkopf and Alexander J. Smola. 2002.  
Learning with Kernels: Support Vector Machines, 
Regularization, Optimization, and Beyond. The 
MIT Press, Cambridge, MA. 
Y. Shinyama, S. Sekine and K. Sudo 2002. Auto-
matic Paraphrase Acquisition from News Articles.
In Proceedings of NAACL-HLT. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
8
Automatically Constructing a Corpus of Sentential Paraphrases 
William B. Dolan and Chris Brockett 
Natural Language Processing Group 
Microsoft Research 
Redmond, WA, 98052, USA 
{billdol,chrisbkt}@microsoft.com
Abstract 
An obstacle to research in automatic 
paraphrase identification and genera-
tion is the lack of large-scale, publicly-
available labeled corpora of sentential 
paraphrases. This paper describes the 
creation of the recently-released Micro-
soft Research Paraphrase Corpus, 
which contains 5801 sentence pairs, 
each hand-labeled with a binary judg-
ment as to whether the pair constitutes 
a paraphrase. The corpus was created 
using heuristic extraction techniques in 
conjunction with an SVM-based classi-
fier to select likely sentence-level para-
phrases from a large corpus of topic-
clustered news data. These pairs were 
then submitted to human judges, who 
confirmed that 67% were in fact se-
mantically equivalent. In addition to 
describing the corpus itself, we explore 
a number of issues that arose in defin-
ing guidelines for the human raters. 
1 Introduction 
The Microsoft Research Paraphrase Corpus 
(MSRP), available for download at 
http://research.microsoft.com/research/nlp/msr_
paraphrase.htm, consists of 5801 pairs of sen-
tences, each accompanied by a binary judgment 
indicating whether human raters considered the 
pair of sentences to be similar enough in mean-
ing to be considered close paraphrases. This data 
has been published for the purpose of encourag-
ing research in areas relating to paraphrase and 
sentential synonymy and inference, and to help 
establish a discourse on the proper construction 
of paraphrase corpora for training and evalua-
tion.  It is hoped that by releasing this corpus, 
we will stimulate the publication of similar cor-
pora by others and help move the field toward 
adoption of a shared dataset that will permit use-
ful comparisons of results across research efforts.    
 
2 Motivation
The success of Statistical Machine Translation 
(SMT) has sparked a successful line of investi-
gation that treats paraphrase acquisition and 
generation essentially as a monolingual machine 
translation problem (e.g., Barzilay & Lee, 2003; 
Pang et al, 2003; Quirk et al, 2004; Finch et al, 
2004). However, a lack of standardly-accepted 
corpora on which to train and evaluate models is 
a major stumbling block to the successful appli-
cation of SMT models or other machine learning 
algorithms to paraphrase tasks.  Since para-
phrase is not apparently a common ?natural? 
task?under normal circumstances people do not 
attempt to create extended paraphrase texts?the 
field lacks a large readily identifiable dataset 
comparable to, for example, the Canadian Han-
sard corpus in SMT that can serve as a standard 
against which algorithms can be trained and 
evaluated.  
What paraphrase data is currently available is 
usually too small to be viable for either training 
or testing, or exhibits narrow topic coverage, 
limiting its broad-domain applicability. One 
class of paraphrase data that is relatively widely 
available is multiple translations of sentences in 
a second language. These, however, tend to be 
rather restricted in their domain (e.g. the ATR 
English-Chinese paraphrase corpus, which con-
9
sists of translations of travel phrases (Zhang & 
Yamamoto, 2002)), are limited to short hand-
crafted predicates (e.g. the ATR Japanese-
English corpus (Shirai, et al, 2002)), or exhibit 
quality problems stemming from insufficient 
command of the target language by the transla-
tors of the documents in question, e.g. the Lin-
guistic Data Consortium?s Multiple-Translation 
Chinese Corpus (Huang et al, 2002).  Multiple 
translations of novels, such as those used in 
(Barzilay & McKeown, 2001) provide a rela-
tively limited dataset to work with, and ? since 
these usually involve works that are out of copy-
right ?  usually exhibit older styles of language 
that have little in common with modern lan-
guage resources or application requirements.   
Likewise, the data made available by (Barzi-
lay & Lee, 2003: http://www.cs.cornell.edu/ 
Info/Projects/NLP/statpar.html), while invalu-
able in understanding and evaluating their re-
sults, is too limited in size and domain coverage 
to serve as either training or test data. 
Attempting to evaluate models of paraphrase 
acquisition and generation under limitations can 
thus be an exercise in frustration. Accordingly, 
we have tried to create a reasonably large corpus 
of naturally-occurring, non-handcrafted sentence 
pairs, along with accompanying human judg-
ments, that can be used as a resource for training 
or testing purposes. Since the search space for 
identifying any two sentence pairs occurring ?in 
the wild? is huge, and provides far too many 
negative examples for humans to wade through, 
clustered news articles were used to constrain 
the initial search space to data that was likely to 
yield paraphrase pairs.   
3 Source Data 
The Microsoft Research Paraphrase Corpus 
(MSRP) is distilled from a database of 
13,127,938 sentence pairs, extracted from 
9,516,684 sentences in 32,408 news clusters 
collected from the World Wide Web over a 2-
year period, The methods and assumptions used 
in building this initial data set are discussed in 
Quirk et al (2004) and Dolan et al (2004). Two 
heuristics based on shared lexical properties and 
sentence position in the document were em-
ployed to construct the initial database:  
Word-based Levenshtein edit distance 
of 1 < e    20; and a length ratio 
> 66%; OR
Both sentences in the first three 
sentences of each file; and length 
ratio > 50%. 
Within this initial dataset we were able to 
automatically identify the names of both authors 
and copyright holders of 61,618 articles.1  Limit-
ing ourselves only to sentences found in those 
articles, we further narrowed the range of candi-
date pairs using the following criteria: 
The number of words in both sentences 
in words is 5 ?  n ? 40; 
The two sentences shared at least 
three words in common; 
The length of the shorter of the two 
sentences, in words, is at least 
66.6% that of the longer; and
The two sentences had a bag-of-words 
lexical distance of e ? 8 edits.
This enabled us extract a set of 49,375 initial 
candidate sentence pairs whose author was 
known,  The purpose of these heuristics was 
two-fold: 1) to narrow the search space for sub-
sequent application of the classifier algorithm 
and human evaluation, and 2) to ensure at least 
some diversity among the sentences. In particu-
lar, we sought to exclude the large number of 
sentence pairs whose differences might be at-
tributable only to typographical errors, variance 
between British and American spellings, and 
minor editorial variations. Lexical distance was 
computed by constructing an alphabetized list of 
unique vocabulary items from each of the sen-
tences and measuring the number of insertions 
and deletions. Note that the number of sentence 
pairs collected in this first pass was relatively 
small compared with the overall size of the data-
set; the requirement of author identification sig-
nificantly circumscribed the available dataset.   
1
 Author identification was performed on the basis of pat-
tern matching datelines and other textual information.  We 
made a strong effort to ensure correct attribution. 
10
4 Constructing a Classifier 
4.1 Sequential Minimal Optimization 
To extract candidate pairs from this ~49K list, 
we used a Support Vector Machine. (Vapnik, 
1995), in this case an implementation of the Se-
quential Minimal Optimization (SMO) algo-
rithm described in Platt (1999),2  which has been 
shown to be useful in text classification tasks 
(Dumais 1998; Dumais et al, 1998). 
4.2 Training Set 
A separate set of 10,000 sentence pairs had 
previously been extracted from randomly held-
out clusters and hand-tagged by two annotators 
according to whether the sentence pairs consti-
tuted paraphrases. This yielded a set of 2968 
positive examples and 7032 negative examples. 
The sentences represented a random mixture of 
held out sentences; no attempt was made to 
match their characteristics to those of the candi-
date data set.  
4.3 Classifiers 
In the classifier we restricted the feature set to a 
small set of feature classes. The main classes are 
given below. More details can be found in 
Brockett and Dolan (2005). 
String Similarity Features: Absolute and rela-
tive length in words, number of shared 
words, word-based edit distance, and bag-
of-words-based lexical distance. 
Morphological Variants: A morphological 
variant lexicon consisting of 95,422 word 
pairs was created using a hand-crafted 
stemmer. Each pair is then treated as a 
feature in the classifier.  
WordNet Lexical Mappings: 314,924 word 
synonyms and hypernym pairs were ex-
tracted from WordNet, (Fellbaum, 1998; 
http://www.cogsci.princeton.edu/~wn/). 
Only pairs identified as occurring in either 
training data or the corpus to be classified 
were included in the final classifier.  
2
 The pseudocode for SMO may be found in the appendix 
of Platt (1999) 
Encarta Thesaurus: 125,054 word synonym 
pairs were extracted from the Encarta The-
saurus (Rooney, 2001). 
Composite Features: Additional, more ab-
stract features summarized the frequency 
with which each feature or class of features 
occurred in the training data, both inde-
pendently, and in correlation with other fea-
tures or feature classes.  
4.4 Results of Applying the Classifier  
Since our purpose was not to evaluate the poten-
tial effectiveness of the classifier itself, but to 
identify a reasonably large set of both positive 
and plausible ?near-miss? negative examples, 
the classifier was applied with output probabili-
ties deliberately skewed towards over-
identification, i.e., towards Type 1 errors, as-
suming non-paraphrase (0) as null hypothesis.  
This yielded 20,574 pairs out the initial 49,375-
pair data set, from which 5801 pairs were then 
further randomly selected for human assessment. 
5 Human Evaluation  
The 5801 sentences selected by the classifier as 
likely paraphrase pairs were examined by two 
independent human judges. Each judge was 
asked whether the two sentences could be con-
sidered ?semantically equivalent?. Disagree-
ments were resolved by a 3rd judge, with the 
final binary judgment reflecting the majority 
vote.3 After resolving differences between raters, 
3900 (67%) of the original pairs were judged 
?semantically equivalent?. 
5.1 Semantic Divergence 
In many instances, the two sentences judged 
?semantically equivalent? in fact diverge seman-
tically to at least some degree. For instance, both 
judges considered the following two to be para-
phrases: 
3
 This annotation task was carried out by an independent 
company, the Butler Hill Group, LLC. Monica Corston-
Oliver directed the effort, with Jeff Stevenson, Amy Muia, 
and David Rojas acting as raters.  
11
Charles O. Prince, 53, was named as 
Mr. Weill?s successor. 
Mr. Weill?s longtime confidant, 
Charles O. Prince, 53, was named 
as his successor. 
If a full paraphrase relationship can be de-
scribed as ?bidirectional entailment?, then the 
majority of the ?equivalent? pairs in this dataset 
exhibit ?mostly bidirectional entailments?, with 
one sentence containing information that differs 
from or is not contained in the other. Our deci-
sion to adopt this relatively loose tagging crite-
rion was ultimately a practical one: insisting on 
complete sets of bidirectional entailments would 
have limited the dataset to pairs of sentences 
that are practically identical at the string level, 
as in the following examples.  
The euro rose above US$1.18, the 
highest price since its January 
1999 launch. 
The euro rose above $1.18 the high-
est level since its launch in 
January 1999. 
However, without a carefully con-
trolled study, there was little 
clear proof that the operation ac-
tually improves people?s lives. 
But without a carefully controlled 
study, there was little clear 
proof that the operation improves 
people?s lives. 
Such pairs are commonplace in the raw data, 
reflecting the tendency of news agencies to pub-
lish and republish the same articles, with editors 
introducing small and often inexplicable 
changes (is ?however? really better than ?but??) 
along the way. The resulting alternations are 
useful sources of information about synonymy 
and local syntactic changes, but our goal was to 
produce a richer type of corpus; one that pro-
vides information about the large-scale alterna-
tions that typify complex paraphrases.4
4
 Recall that in an effort to focus on sentence pairs that are 
not simply trivial variants of some original single source, 
we restricted our original dataset by removing all pairs with 
a minimum word-based Levenshtein distance of ? 8. 
5.2 Complex Alternations 
Some sentence pairs in the news data capture 
complex and full paraphrase alternations: 
Wynn paid $23.5 million for Re-
noir?s ?In the Roses (Madame Leon 
Clapisson)? at a Sotheby auction 
on Tuesday 
Wynn nabbed Renoir?s ?In the Roses 
(Madame Leon Clapisson)? for $23.5 
on Tuesday at Sotheby?s
Far more frequently, however, interesting 
paraphrases in the data are accompanied by at 
least minor differences in content: 
David Gest has sued his estranged 
wife Liza Minelli for %MONEY% mil-
lion for beating him when she was 
drunk
Liza Minelli?s estranged husband is 
taking her to court for %MONEY% 
million after saying she threw a 
lamp at him and beat him in 
drunken rages 
It quickly became clear, that in order to col-
lect significant numbers of sentential paraphrase 
pairs, our standards for what constitutes ?seman-
tic equivalence? would have to be relaxed.  
5.3 Rater Instructions 
Raters were told to use their best judgment in 
deciding whether 2 sentences, at a high level, 
?mean the same thing?. Under our relatively 
loose definition of semantic equivalence, any 2 
of the following sentences would have qualified 
as ?paraphrases?, despite obvious differences in 
information content: 
The genome of the fungal pathogen 
that causes Sudden Oak Death has 
been sequenced by US scientists 
Researchers announced Thursday 
they've completed the genetic 
blueprint of the blight-causing 
culprit responsible for sudden oak 
death
Scientists have figured out the 
complete genetic code of a viru-
lent pathogen that has killed tens 
12
of thousands of California native 
oaks
The East Bay-based Joint Genome In-
stitute said Thursday it has un-
raveled the genetic blueprint for 
the diseases that cause the sudden 
death of oak trees 
Several classes of named entities were re-
placed by generic tags in sentences presented to 
the raters, so that ?Tuesday? be-
came %%DAY%%, ?$10,000? became 
?%%MONEY%%, and so on. In the released 
version of the dataset, however, these place-
holders were replaced by the original strings. 
After a good deal of trial-and-error, some 
specific rating criteria were developed and in-
cluded in a tagging specification. For the most 
part, though, the degree of mismatch allowed 
before the pair was judged ?non-equivalent? was 
left to the discretion of the individual rater: did a 
particular set of asymmetries alter the meanings 
of the sentences so much that they could not be 
regarded as paraphrases? The following sen-
tences, for example, were judged ?not equiva-
lent? despite some significant content overlap: 
The Gerontology Research Group said 
Slough was born on %DATE%, making 
her %NUMBER% years old at the time 
of her death. 
?[Mrs. Slough?] is the oldest liv-
ing American as of the time she 
died, L. Stephen Coles, Executive 
Director of the Gerontology Re-
search Group, said %DATE%. 
The tagging task was ill-defined enough that 
we were surprised at how high inter-rater 
agreement was (averaging 84%). The Kappa 
score of 62 is good, but low enough to be indica-
tive of the difficulty of the rating task.  We be-
lieve that with more practice and discussion 
between raters, agreement on the task could be 
improved. 
Interestingly, a series of experiments aimed 
at making the judging task more concrete re-
sulted in uniformly degraded inter-rater agree-
ment. Providing a checkbox to allow judges to 
specify that one sentence fully entailed another, 
for instance, left the raters frustrated, slowed 
down the tagging, and had a negative impact on 
agreement. Similarly, efforts to identify classes 
of syntactic alternations that would not count 
against an ?equivalent? judgment resulted, in 
most cases, in a collapse in inter-rater agreement. 
After completing hundreds of judgments, the 
raters themselves were asked for suggestions as 
to what checkboxes or instructions might im-
prove tagging speed and accuracy. In the end, 
few generalizations seemed useful in streamlin-
ing the task; each pair is sufficiently idiosyn-
cratic that that common sense has to take 
precedence over formal guidelines. 
In a few cases, firm tagging guidelines were 
found to be useful. One example was the treat-
ment of pronominal and NP anaphora. Raters 
were instructed to treat anaphors and their full 
forms as equivalent, regardless of how great the 
disparity in length or lexical content between the 
two sentences. (Often these correspondences are 
extremely interesting, and in sufficient quantity 
would provide interesting fodder for learning 
models of anaphora.) 
SCC argued that Lexmark was trying 
to shield itself from competition? 
The company also argued that Lex-
mark was trying to squash competi-
tion?
But Secretary of State Colin Powell 
brushed off this possibil-
ity %%day%%. 
Secretary of State Colin Powell 
last week ruled out a non-
aggression treaty.
Note that many of the 33% of sentence pairs 
judged to be ?not equivalent? still overlap sig-
nificantly in information content and even word-
ing. These pairs reflect a range of relationships, 
from pairs that are completely unrelated seman-
tically, to those that are partially overlapping, to 
those that are almost-but-not-quite semantically 
equivalent.  
6 Discussion
Given that MSRP reflects both the initial heuris-
tics and the SVM methodology that was em-
ployed to identify paraphrase candidates for 
human evaluation, it is also limited by that tech-
nology. The 67% ratio of positive to negative 
judgments is a reasonably reliable indicator of 
the precision of our technique--though it should 
13
be recalled that parameters were deliberately 
distorted to yield imprecise results that included 
positive and a large number of ?near-miss? 
negatives.  Coverage is hard to estimate reliably. 
we calculate that fewer than 30% of the pairs in 
a set of matched first-two sentences extracted 
from clustered news data, after application of 
simple heuristics, are paraphrases (Dolan et al, 
2004). It seems reasonable to assume that the 
reduction to 10% seen in the initial data set still 
leaves many valid paraphrase pairs uncaptured 
in the corpus. The need to limit the corpus to 
those sentences for which authorship can be 
verified, and more specifically. to no more than 
a single sentence extracted from each article. 
further constrains the coverage in ways whose 
consequences are not yet known. In addition, the 
three-shared-words heuristic further guarantees 
that an entire class of paraphrases in which no 
words are shared in common have been ex-
cluded from the data. It has been observed that 
the mean lexical overlap in the corpus is a rela-
tively high 0.7 (Weeds et al 2005), suggesting 
that more lexically divergent examples will be 
needed.  In these respects, as Wu (2005) points 
out, the corpus is far from distributionally neu-
tral. This is a matter that we hope to remedy in 
the future, since in many ways this excluded set 
of pairs is the most interesting of all.  
The above limitations, together with its rela-
tively small size, perhaps make the MRSP inap-
propriate for direct use as a training corpus. We 
show separately that the results of training a 
classifier on the present corpus may be inferior 
to other training sets, though better than crude 
string or text-based heuristics (Brockett & Dolan, 
2005). We expect that the utility of the corpus 
will stem primarily from its use as a tool for 
evaluating paraphrase recognition algorithms. It 
has already been applied in this way by Corley 
& Mihalcea (2005) and Wu (2005).  
7 A Virtual Super Corpus? 
Although larger than any other non-translation-
based labeled paraphrase corpus currently pub-
licly available, MSRP is tiny compared with the 
huge bilingual parallel corpora publicly avail-
able within the Machine Translation community, 
for example, the Canadian Hansards, the Hong 
Kong Parliamentary corpus, or the United Na-
tions documents. It is improbable that we will 
ever encounter a ?naturally occurring? para-
phrase corpus on the scale of any of these bilin-
gual corpora.  Moreover, whatever extraction 
technique is employed to identify paraphrases in 
other kinds of data will be apt to reflect the im-
plicit biases of the methodology employed.   
Here we would like to put forward a proposal.  
The paraphrase research community might be 
able to construct a ?virtual paraphrase corpus? 
that would be adequately large for both training 
and testing purposes and minimize selectional 
biases. This could be achieved in something like 
the following manner. Research groups could 
compile their own labeled paraphrase corpora, 
applying whatever learning techniques they 
choose to select their initial data. If enough in-
terested groups were to release a sufficiently 
large number of reasonably-sized corpora, it 
might be possible to achieve some sort consen-
sus, in a manner analogous to the division of the 
Penn Treebank into sections, whereby classifiers 
and other tools are conventionally trained on one 
subset of corpora, and tested against another 
subset. Though this would present issues of its 
own, it would obviate many of the problems of 
extraction bias inherent in automated extraction, 
and allow better cross comparison across sys-
tems.   
8 Future Directions 
For our part we plan to expand the MSRP, 
both by extending the number of sentence pairs, 
and also improving the balance of positive and 
negative examples. We anticipate using multiple 
classifiers to reduce inherent biases in candidate 
corpus selection, and with better author identifi-
cation to ensure proper attribution, to be able to 
draw on a larger dataset for consideration by our 
judges.  
In future releases we expect to make avail-
able more information about individual evalua-
tor judgments. Burger & Ferro (2005) have 
suggested that this data may allow researchers 
greater freedom to construct models based on 
the judgments of specific judges or combina-
tions of judges, permitting more fine-grained use 
of the corpus.  
One further issue that we will also be at-
tempting to address is the need to provide a bet-
ter metric for corpus coverage and quality. Until 
reliable metrics can be established for end-to-
14
end paraphrase tasks?these will probably need 
to be application specific?the Alignment Error 
Rate strategy that was successfully applied in 
early development of machine translation sys-
tems (Och & Ney, 2000, 2003) offers a useful 
intermediate representation of the coverage and 
precision of a corpus and extraction techniques. 
Though fullscale reliability studies have yet to 
be performed, the AER technique is already 
finding application in other fields such as sum-
marization (Daum? & Marcu, forthcoming). We 
expect to be able to provide a reasonably large 
corpus of word-aligned paraphrase sentences in 
the near future that we hope will serve as some 
sort of standard by which corpus extraction 
techniques can be measured and compared in a 
uniform fashion.   
One other path that we are concurrently ex-
ploring is collection and validation of para-
phrase data by volunteers on the web. Some 
initial efforts using game formats for elicitation 
are presented in Chklovski (2005) and Brockett 
& Dolan (2005). It is our hope that web volun-
teers will prove a useful source of colloquial 
paraphrases of written text, and?if paraphrase 
identification can be effectively embedded in the 
game?of paraphrase judgments.   
9 Conclusion
We have used heuristic techniques and a classi-
fier to automatically create a corpus of 5801 
?naturally occurring? (non-constructed) sentence 
pairs, labeled according to whether, in the judg-
ment of our evaluators, the sentences ?mean the 
same thing? or not.  To our knowledge, MSRP 
constitutes the largest currently-available broad-
domain corpus of paraphrase pairs that does not 
have its origins in translations from another lan-
guage.  We hope that others will utilize it, find it 
useful, and provide feedback when it is not.  
The methodology that we have described for 
extracting this corpus is readily adaptable by 
others, and is not limited to news clusters, but 
can be readily extended to any flat corpus con-
taining a large number of semantically similar 
sentences on which topic-based document clus-
tering is possible. We have shown that by allow-
ing a statistical learning algorithm to constrain 
the search space, it is possible to identify a man-
ageable-sized candidate corpus on the basis of 
which human judges can label sentence pairs for 
paraphrase content quickly and in a cost effec-
tive manner. We hope that others will follow our 
example.   
Acknowledgements 
We would like to thank Monica Corston-Oliver, 
Jeff Stevenson, Amy Muia and David Rojas of 
Butler Hill Group LLC for their assistance in 
annotating the Microsoft Research Paraphrase 
Corpus and in preparing the seed data used for 
training. This paper has also benefited from 
feedback from several anonymous reviewers. 
All errors and omissions are our own. 
References  
Regina Barzilay and Katherine. R. McKeown. 2001. 
Extracting Paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL.
Regina Barzilay and  Lillian Lee. 2003. Learning to 
Paraphrase; an unsupervised approach using mul-
tiple-sequence alignment. In Proceedings of 
HLT/NAACL 2003.
Chris Brockett and William B. Dolan. 2005. Support 
Vector Machines for Paraphrase Identification and 
Corpus Construction. In Proceedings of The Third 
International Workshop on Paraphrasing 
(IWP2005), Jeju, Republic of Korea. 
Chris Brockett and William B. Dolan. 2005. Echo 
Chamber: A Game for Eliciting a Colloquial Para-
phrase Corpus. AAAI 2005 Spring Symposium, 
Knowledge Collection from Volunteer Contribu-
tors (KCVC05). Stanford, CA. March 21-23, 2005. 
P. Brown, S. A. Della Pietra, V.J. Della Pietra and R. 
L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation. Computational Linguistics,
Vol. 19(2): 263-311.  
John Burger and Lisa Ferro. 2005.  Generating an 
Entailment Corpus from News Headlines.  In Pro-
ceedings of the ACL Workshop on Empirical 
Modeling of Semantic Equivalence and Entailment.
pp 49-54. 
Timothy Chklovski. 2005 1001 Paraphrases: Incent-
ing Responsible Contributions in Collecting Para-
phrases from Volunteers. AAAI 2005 Spring 
Symposium, Knowledge Collection from Volunteer 
Contributors (KCVC05). Stanford, CA. March 21-
23, 2005. 
Courtney Courley and Rada Mihalcea. 2005. Measur-
ing the Semantic Similarity of Texts.  In Proceed-
ings of the ACL Workshop on Empirical Modeling 
of Semantic Equivalence and Entailment. Pp 13-
18.
Hal Daum? III and Daniel Marcu. (forthcoming)  
Induction of Word and Phrase Alignments for 
15
Automatic Document Summarization. To appear 
in Computational Linguistics.
William. B. Dolan, Chris Quirk, and Chris Brockett. 
2004. Unsupervised Construction of Large Para-
phrase Corpora: Exploiting Massively Parallel 
News Sources. Proceedings of COLING 2004,
Geneva, Switzerland.  
Susan Dumais. 1998. Using SVMs for Text Catego-
rization. IEEE Intelligent Systems, Jul.-Aug. 1998: 
21-23 
Susan Dumais, John Platt, David Heckerman, Meh-
ran Sahami. 1998. Inductive learning algorithms 
and representations for text categorization. In Pro-
ceedings of the Seventh International Conference 
on Information and Knowledge Management. 
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press. 
Andrew Finch, Taro Watanabe, Yasuhiro Akiba and 
Eiichiro Sumita. 2004. Paraphrasing as Machine 
Translation. Journal of Natural Language Proc-
essing, 11(5), pp 87-111. 
Pascale Fung and Percy Cheung. 2004. Multi-level 
Bootstrapping for Extracting Parallel Sentences 
from a Quasi-Comparable Corpus. In Proceedings 
of Coling 2004, 1051-1057. 
Shudong Huang, David Graff, and George Dodding-
ton (eds.) 2002. Multiple-Translation Chinese 
Corpus. Linguistic Data Consortium. 
Thorsten Joachims. 2002.  Learning to Classify Text 
Using Support Vector Machines: Methods, Theory, 
and Algorithms. Kluwer Academic Publishers, 
Boston/Dordrecht/London. 
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet 
Physice-Doklady 10: 707-710. 
Microsoft Research Paraphrase Corpus. 
http://research.microsoft.com/research/downloads/
default.aspx 
Franz Joseph Och and H. Ney. 2000. Improved Sta-
tistical Alignment Models.  In Proceedings of the 
38th Annual Meeting of the ACL, Hong Kong, 
China, pp 440-447. 
Franz Joseph Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models.  Computational Linguistics, 29 (1): 
19-52. 
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. 
Syntax-based Alignment of Multiple Translations: 
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of NAACL-HLT.
John C. Platt. 1999. Fast Training of Support Vector 
Machines Using Sequential Minimal Optimization. 
In Bernhard Sch?lkopf, Christopher J. C. Burges 
and Alexander J. Smola (eds.). 1999.  Advances in 
Kernel Methods: Support Vector Learning. The 
MIT Press, Cambridge, MA. 185-208. 
Chris Quirk, Chris Brockett, and William B. Dolan. 
2004. Monolingual Machine Translation for Para-
phrase Generation, In Proceedings of the 2004 
Conference on Empirical Methods in Natural 
Language Processing, 25-26 July 2004, Barcelona 
Spain, pp. 142-149. 
Kathy Rooney (ed.) 2001. Encarta Thesaurus.
Bloomsbury Publishing. 
Satoshi Shirai, Kazuhide Yamamoto, Francis Bond & 
Hozumi Tanaka. 2002. Towards a thesaurus of 
predicates. In Proceedings of LREC 2002 (Third 
International Conference on Language Resources 
and Evaluation), (May 29-31, 2002). Vol.6, pp. 
1965-1972. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Julie Weeds, David Weir and Bill Keller. 2005. The 
Distributional Similarity of Subparses. In Pro-
ceedings of the ACL Workshop on Empirical 
Modeling of Semantic Equivalence and Entailment.
pp 7-12.  
Dekai Wu. 2005. Recognizing Paraphrases and Tex-
tual Entailment using Inversion Transduction 
Grammars. In Proceedings of the ACL Workshop 
on Empirical Modeling of Semantic Equivalence 
and Entailment. Pp 25-30. 
Yujie Zhang and Kazuhide Yamamoto.  2002 Para-
phrasing of Chinese Utterances.  Proceedings of 
Coling 2002, pp.1163-1169. 
16
Using Contextual Speller Techniques and Language Modeling for 
ESL Error Correction 
Michael Gamon*, Jianfeng Gao*, Chris Brockett*, Alexandre Klementiev+, William 
B. Dolan*, Dmitriy Belenko*, Lucy Vanderwende* 
 
*Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{mgamon,jfgao,chrisbkt,billdol, 
dmitryb,lucyv}@microsoft.com 
+Dept. of Computer Science 
University of Illinois 
Urbana, IL 61801 
klementi@uiuc.edu 
 
 
Abstract 
We present a modular system for detection 
and correction of errors made by non-
native (English as a Second Language = 
ESL) writers. We focus on two error types: 
the incorrect use of determiners and the 
choice of prepositions. We use a decision-
tree approach inspired by contextual 
spelling systems for detection and 
correction suggestions, and a large 
language model trained on the Gigaword 
corpus to provide additional information to 
filter out spurious suggestions. We show 
how this system performs on a corpus of 
non-native English text and discuss 
strategies for future enhancements. 
1 Introduction 
English is today the de facto lingua franca for 
commerce around the globe. It has been estimated 
that about 750M people use English as a second 
language, as opposed to 375M native English 
speakers (Crystal 1997), while as much as 74% of 
writing in English is done by non-native speakers. 
However, the errors typically targeted by 
commercial proofing tools represent only a subset 
of errors that a non-native speaker might make. For 
example, while many non-native speakers may 
encounter difficulty choosing among prepositions, 
this is typically not a significant problem for native 
speakers and hence remains unaddressed in 
proofing tools such as the grammar checker in 
Microsoft Word (Heidorn 2000). Plainly there is an 
opening here for automated proofing tools that are 
better geared to the non-native users.  
One challenge that automated proofing tools 
face is that writing errors often present a semantic 
dimension that renders it difficult if not impossible 
to provide a single correct suggestion. The choice 
of definite versus indefinite determiner?a 
common error type among writers with a Japanese, 
Chinese or Korean language background owing to 
the lack of overt markers for definiteness and 
indefiniteness?is highly dependent on larger 
textual context and world knowledge. It seems 
desirable, then, that proofing tools targeting such 
errors be able to offer a range of plausible 
suggestions, enhanced by presenting real-world 
examples that are intended to inform a user?s 
selection of the most appropriate wording in the 
context1. 
2 Targeted Error Types 
Our system currently targets eight different error 
types: 
1. Preposition presence and choice: 
In the other hand, ... (On the other hand ...) 
2. Definite and indefinite determiner presence 
and choice: 
 I am teacher... (am a teacher) 
3. Gerund/infinitive confusion: 
I am interesting in this book. (interested in) 
4. Auxiliary verb presence and choice: 
My teacher does is a good teacher (my teacher 
is...) 
                                                 
1 Liu et al 2000 take a similar approach, retrieving 
example sentences from a large corpus. 
449
5. Over-regularized verb inflection: 
 I writed a letter (wrote) 
6. Adjective/noun confusion: 
 This is a China book (Chinese book) 
7. Word order (adjective sequences and nominal 
compounds): 
I am a student of university (university student) 
8. Noun pluralization: 
 They have many knowledges (much knowledge) 
In this paper we will focus on the two most 
prominent and difficult errors: choice of 
determiner and prepositions. Empirical 
justification for targeting these errors comes from 
inspection of several corpora of non-native writing. 
In the NICT Japanese Learners of English (JLE) 
corpus (Izumi et al 2004), 26.6% of all errors are 
determiner related, and about 10% are preposition 
related, making these two error types the dominant 
ones in the corpus. Although the JLE corpus is 
based on transcripts of spoken language, we have 
no reason to believe that the situation in written 
English is substantially different. The Chinese 
Learners of English Corpus (CLEC, Gui and Yang 
2003) has a coarser and somewhat inconsistent 
error tagging scheme that makes it harder to isolate 
the two errors, but of the non-orthographic errors, 
more than 10% are determiner and number related. 
Roughly 2% of errors in the corpus are tagged as 
preposition-related, but other preposition errors are 
subsumed under the ?collocation error? category 
which makes up about 5% of errors. 
3 Related Work 
Models for determiner and preposition selection 
have mostly been investigated in the context of 
sentence realization and machine translation 
(Knight and Chander 1994, Gamon et al 2002,  
Bond 2005, Suzuki and Toutanova 2006, 
Toutanova and Suzuki 2007). Such approaches 
typically rely on the fact that preposition or 
determiner choice is made in otherwise native-like 
sentences. Turner and Charniak (2007), for 
example, utilize a language model based on a 
statistical parser for Penn Tree Bank data. 
Similarly, De Felice and Pulman (2007) utilize a 
set of sophisticated syntactic and semantic analysis 
features to predict 5 common English prepositions. 
Obviously, this is impractical in a setting where 
noisy non-native text is subjected to proofing. 
Meanwhile, work on automated error detection on 
non-native text focuses primarily on detection of 
errors, rather than on the more difficult task of 
supplying viable corrections (e.g., Chodorow and 
Leacock, 2000). More recently,  Han et al (2004, 
2006) use a maximum entropy classifier to propose 
article corrections in TESOL essays, while Izumi 
et al (2003) and Chodorow et al (2007) present 
techniques of automatic preposition choice 
modeling. These more recent efforts, nevertheless, 
do not attempt to integrate their methods into a 
more general proofing application designed to 
assist non-native speakers when writing English. 
Finally, Yi et al (2008) designed a system that 
uses web counts to determine correct article usage 
for a given sentence, targeting ESL users. 
4 System Description 
Our system consists of three major components: 
1. Suggestion Provider (SP) 
2. Language Model (LM) 
3. Example Provider (EP) 
The Suggestion Provider contains modules for 
each error type discussed in section 2. Sentences 
are tokenized and part-of-speech tagged before 
they are presented to these modules. Each module 
determines parts of the sentence that may contain 
an error of a specific type and one or more possible 
corrections. Four of the eight error-specific 
modules mentioned in section 2 employ machine 
learned (classification) techniques, the other four 
are based on heuristics. Gerund/infinitive 
confusion and auxiliary presence/choice each use a 
single classifier. Preposition and determiner 
modules each use two classifiers, one to determine 
whether a preposition/article should be present, 
and one for the choice of preposition/article. 
All suggestions from the Suggestion Provider 
are collected and passed through the Language 
Model. As a first step, a suggested correction has 
to have a higher language model score than the 
original sentence in order to be a candidate for 
being surfaced to the user. A second set of 
heuristic thresholds is based on a linear 
combination of class probability as assigned by the 
classifier and language model score. 
The Example Provider queries the web for 
exemplary sentences that contain the suggested 
correction. The user can choose to consult this 
information to make an informed decision about 
the correction. 
450
4.1 Suggestion Provider Modules for 
Determiners and Prepositions 
The SP modules for determiner and preposition 
choice are machine learned components. Ideally, 
one would train such modules on large data sets of 
annotated errors and corrected counterparts. Such a 
data set, however, is not currently available. As a 
substitute, we are using native English text for 
training, currently we train on the full text of the 
English Encarta encyclopedia (560k sentences) and 
a random set of 1M sentences from a Reuters news 
data set. The strategy behind these modules is 
similar to a contextual speller as described, for 
example, in (Golding and Roth 1999). For each 
potential insertion point of a determiner or 
preposition we extract context features within a 
window of six tokens to the right and to the left. 
For each token within the window we extract its 
relative position, the token string, and its part-of-
speech tag. Potential insertion sites are determined 
heuristically from the sequence of POS tags. Based 
on these features, we train a classifier for 
preposition choice and determiner choice. 
Currently we train decision tree classifiers with the 
WinMine toolkit (Chickering 2002). We also 
experimented with linear SVMs, but decision trees 
performed better overall and training and 
parameter optimization were considerably more 
efficient. Before training the classifiers, we 
perform feature ablation by imposing a count 
cutoff of 10, and by limiting the number of features 
to the top 75K features in terms of log likelihood 
ratio (Dunning 1993). 
We train two separate classifiers for both 
determiners and preposition: 
? decision whether or not a 
determiner/preposition should be present 
(presence/absence or pa classifier) 
? decision which determiner/preposition is 
the most likely choice, given that a 
determiner/preposition is present (choice 
or ch classifier) 
In the case of determiners, class values for the ch 
classifier are a/an and the. Preposition choice 
(equivalent to the ?confusion set? of a contextual 
speller) is limited to a set of 13 prepositions that 
figure prominently in the errors observed in the 
JLE corpus: about, as, at, by, for, from, in, like, of, 
on, since, to, with, than, "other" (for prepositions 
not in the list). 
The decision tree classifiers produce probability 
distributions over class values at their leaf nodes. 
For a given leaf node, the most likely 
preposition/determiner is chosen as a suggestion. If 
there are other class values with probabilities 
above heuristically determined thresholds2, those 
are also included in the list of possible suggestions. 
Consider the following example of an article-
related error: 
I am teacher from Korea. 
As explained above, the suggestion provider 
module for article errors consists of two classifiers, 
one for presence/absence of an article, the other for 
article choice. The string above is first tokenized 
and then part-of-speech tagged: 
0/I/PRP   1/am/VBP   2/teacher/NN   3/from/IN   
4/Korea/NNP   5/./.  
Based on the sequence of POS tags and 
capitalization of the nouns, a heuristic determines 
that there is one potential noun phrase that could 
contain an article: teacher. For this possible article 
position, the article presence/absence classifier 
determines the probability of the presence of an 
article, based on a feature vector of pos tags and 
surrounding lexical items: 
p(article + teacher) = 0.54 
Given that the probability of an article in this 
position is higher than the probability of not having 
an article, the second classifier is consulted to 
provide the most likely choice of article: 
p(the) = 0.04 
p(a/an) = 0.96 
Given  this probability distribution, a correction 
suggestion I am teacher from Korea -> I am a 
teacher from Korea is generated and passed on to 
evaluation by the language model component. 
4.2 The Language Model 
The language model is a 5-gram model trained 
on the English Gigaword corpus (LDC2005T12). 
In order to preserve (singleton) context information 
as much as possible, we used interpolated Kneser-
Ney smoothing (Kneser and Ney 1995) without 
count cutoff. With a 120K-word vocabulary, the 
trained language model contains 54 million 
bigrams, 338 million trigrams, 801 million 4-grams 
                                                 
2 Again, we are working on learning these thresholds 
empirically from data. 
451
and 12 billion 5-grams.  In the example from the 
previous section, the two alternative strings  of the 
original user input and the suggested correction are 
scored by the language model: 
I am teacher from Korea. score = 0.19 
I am a teacher from Korea. score = 0.60 
The score for the suggested correction is 
significantly higher than the score for the original, 
so the suggested correction is provided to the user. 
4.3 The Example Provider 
In many cases, the SP will produce several 
alternative suggestions, from which the user may 
be able to pick the appropriate correction reliably. 
In other cases, however, it may not be clear which 
suggestion is most appropriate. In this event, the 
user can choose to activate the Example Provider 
(EP) which will then perform a web search to 
retrieve relevant example sentences illustrating the 
suggested correction. For each suggestion, we 
create an exact string query including a small 
window of context to the left and to the right of the 
suggested correction. The query is issued to a 
search engine, and the retrieved results are 
separated into sentences. Those sentences that 
contain the string query are added to a list of 
example candidates.  The candidates are then 
ranked by two initially implemented criteria: 
Sentence length (shorter examples are preferred in 
order to reduce cognitive load) and context overlap 
(sentences that contain additional words from the 
user input are preferred). We have not yet 
performed a user study to evaluate the usefulness 
of the examples provided by the system. Some 
examples of usage that we retrieve are given below 
with the query string in boldface: 
Original: I am teacher from Korea. 
Suggestion: I am a teacher from Korea. 
All top 3 examples: I am a teacher.  
Original: So Smokers have to see doctor more often 
than non-smokers. 
Suggestion: So Smokers have to see a doctor more 
often than non-smokers. 
Top 3 examples: 
1. Do people going through withdrawal have 
to see a doctor? 
2. Usually, a couple should wait to see a 
doctor until after they've tried to get 
pregnant for a year. 
3. If you have had congestion for over a 
week, you should see a doctor. 
Original: I want to travel Disneyland in March. 
Suggestion: I want to travel to Disneyland in 
March. 
Top 3 examples: 
1. Timothy's wish was to travel to 
Disneyland in California. 
2. Should you travel to Disneyland in 
California or to Disney World in 
Florida? 
3. The tourists who travel to Disneyland in 
California can either choose to stay in 
Disney resorts or in the hotel for 
Disneyland vacations. 
5 Evaluation 
We perform two different types of evaluation on 
our system. Automatic evaluation is performed on 
native text, under the assumption that the native 
text does not contain any errors of the type targeted 
by our system. For example, the original choice of 
preposition made in the native text would serve as 
supervision for the evaluation of the preposition 
module. Human evaluation is performed on non-
native text, with a human rater assessing each 
suggestion provided by the system. 
5.1 Individual SP Modules 
For evaluation, we split the original training data 
discussed in section 4.1 into training and test sets 
(70%/30%). We then retrained the classifiers on 
this reduced training set and applied them to the 
held-out test set. Since there are two models, one 
for preposition/determiner presence and absence 
(pa), and one for preposition/determiner choice 
(ch), we report combined accuracy numbers of the 
two classifiers. Votes(a) stands for the counts of 
votes for class value = absence from pa, votes(p) 
stands for counts of votes for presence from pa. 
Acc(pa) is the accuracy of the pa classifier, acc(ch) 
the accuracy of the choice classifier. Combined 
accuracy is defined as in Equation 1. 
 
 
??? ?? ? ?????(?) + ??? ?? ? ??? ?? ? ?????(?)
????? ?????
 
Equation 1: Combined accuracy of the 
presence/absence and choice models 
452
The total number of cases in the test set is 
1,578,342 for article correction and 1,828,438 for 
preposition correction. 
5.1.1 Determiner choice 
Accuracy of the determiner pa and ch models 
and their combination is shown in Table 1. 
Model pa ch combined 
Accuracy 89.61% 85.97% 86.07% 
Table 1: Accuracy of the determiner pa, ch, and 
combined models. 
The baseline is 69.9% (choosing the most 
frequent class label none). The overall accuracy of 
this module is state-of-the-art compared with 
results reported in the literature (Knight and 
Chander 1994, Minnen et al 2000, Lee 2004, 
Turner and Charniak 2007). Turner and Charniak 
2007 obtained the best reported accuracy to date of 
86.74%, using a Charniak language model 
(Charniak 2001) based on a full statistical parser 
on the Penn Tree Bank. These numbers are, of 
course, not directly comparable, given the different 
corpora. On the other hand, the distribution of 
determiners is similar in the PTB (as reported in 
Minnen et al 2000) and in our data (Table 2). 
 PTB Reuters/Encarta 
mix 
no determiner 70.0% 69.9% 
the 20.6% 22.2% 
a/an 9.4% 7.8% 
Table 2: distribution of determiners in the Penn 
Tree Bank and in our Reuters/Encarta data. 
Precision and recall numbers for both models on 
our test set are shown in Table 3 and Table 4. 
Article 
pa classifier 
precision recall 
presence 84.99% 79.54% 
absence 91.43% 93.95% 
Table 3: precision and recall of the article pa 
classifier. 
Article  
ch classifier 
precision Recall 
the 88.73% 92.81% 
a/an 76.55% 66.58% 
Table 4: precision and recall of the article ch 
classifier. 
5.1.2 Preposition choice 
The preposition choice model and the combined 
model achieve lower accuracy than the 
corresponding determiner models, a result that can 
be expected given the larger choice of candidates 
and hardness of the task. Accuracy numbers are 
presented in Table 5. 
Model pa ch combined 
Accuracy 91.06%% 62.32% 86.07% 
Table 5:Accuracy of the preposition pa, ch, and 
combined models. 
The baseline in this task is 28.94% (using no 
preposition). Precision and recall numbers are 
shown in Table 6 and Table 7. From Table 7 it is 
evident that prepositions show a wide range of 
predictability. Prepositions such as than and about 
show high recall and precision, due to the lexical 
and morphosyntactic regularities that govern their 
distribution. At the low end, the semantically more 
independent prepositions since and at show much 
lower precision and recall numbers. 
 
Preposition  
pa classifier 
precision recall 
presence 90.82% 87.20% 
absence 91.22% 93.78% 
Table 6: Precision and recall of the preposition pa 
classifier. 
Preposition 
ch classifier 
precision recall 
other 53.75% 54.41% 
in 55.93% 62.93% 
for 56.18% 38.76% 
of 68.09% 85.85% 
on 46.94% 24.47% 
to 79.54% 51.72% 
with 64.86% 25.00% 
at 50.00% 29.67% 
by 42.86% 60.46% 
as 76.78% 64.18% 
from 81.13% 39.09% 
since 50.00% 10.00% 
about 93.88% 69.70% 
than 95.24% 90.91% 
Table 7: Precision and recall of the preposition ch 
classifier. 
 
453
Chodorow et al (2007) present numbers on an 
independently developed system for detection of 
preposition error in non-native English. Their 
approach is similar to ours in that they use a 
classifier with contextual feature vectors.  The 
major differences between the two systems are the 
additional use of a language model in our system 
and, from a usability perspective, in the example 
provider module we added to the correction 
process. Since both systems are evaluated on 
different data sets3, however, the numbers are not 
directly comparable. 
5.2 Language model Impact 
The language model gives us an additional piece 
of information to make a decision as to whether a 
correction is indeed valid. Initially, we used the 
language model as a simple filter: any correction 
that received a lower language model score than 
the original was filtered out. As a first approxi-
mation, this was an effective step: it reduced the 
number of preposition corrections by 66.8% and 
the determiner corrections by 50.7%, and increased 
precision dramatically. The language model alone, 
however, does not provide sufficient evidence: if 
we produce a full set of preposition suggestions for 
each potential preposition location and rank these 
suggestions by LM score alone, we only achieve 
58.36% accuracy on Reuters data. 
Given that we have multiple pieces of 
information for a correction candidate, namely the 
class probability assigned by the classifier and the 
language model score, it is more effective to 
combine these into a single score and impose a 
tunable threshold on the score to maximize 
precision. Currently, this threshold is manually set 
by analyzing the flags in a development set. 
5.3 Human Evaluation 
A complete human evaluation of our system would 
have to include a thorough user study and would 
need to assess a variety of criteria, from the 
accuracy of individual error detection and 
corrections to the general helpfulness of real web-
based example sentences. For a first human 
evaluation of our system prototype, we decided to 
                                                 
3 Chodorow et al (2007) evaluate their system on 
proprietary student essays from non-native students, 
where they achieve 77.8% precision at 30.4% recall for 
the preposition substitution task. 
simply address the question of accuracy on the 
determiner and preposition choice tasks on a 
sample of non-native text.  
For this purpose we ran the system over a 
random sample of sentences from the CLEC 
corpus (8k for the preposition evaluation and 6k 
for the determiner evaluation). An independent 
judge annotated each flag produced by the system 
as belonging to one of the following categories: 
? (1) the correction is valid and fixes the 
problem 
? (2) the error is correctly identified, but 
the suggested correction does not fix it 
? (3) the original and the rewrite are both 
equally good 
? (4) the error is at or near the suggested 
correction, but it is a different kind of 
error (not having to do with 
prepositions/determiners) 
? (5) There is a spelling error at or near 
the correction 
? (6) the correction is wrong, the original 
is correct 
Table 8 shows the results of this human 
assessment for articles and prepositions. 
 
Articles (6k 
sentences) 
Prepositions 
(8k 
sentences) 
count ratio count ratio 
(1) correction is 
valid 
240 55% 165 46% 
(2) error identified, 
suggestion does 
not fix it 
10 2% 17 5% 
(3) original and 
suggestion equally 
good 
17 4% 38 10% 
(4) misdiagnosis 65 15% 46 13% 
(5) spelling error 
near correction 
37 8% 20 6% 
(6) original correct 70 16% 76 21% 
Table 8: Article and preposition correction 
accuracy on CLEC data. 
The distribution of corrections across deletion, 
insertion and substitution operations is illustrated 
in Table 9. The most common article correction is 
insertion of a missing article. For prepositions, 
substitution is the most common correction, again 
an expected result given that the presence of a 
454
preposition is easier to determine for a non-native 
speaker than the actual choice of the correct 
preposition. 
 deletion insertion substitution 
Articles 8% 79% 13% 
Prepositions 15% 10% 76% 
Table 9: Ratio of deletion, insertion and 
substitution operations. 
6 Conclusion and Future Work 
Helping a non-native writer of English with the 
correct choice of prepositions and 
definite/indefinite determiners is a difficult 
challenge. By combining contextual speller based 
methods with language model scoring and 
providing web-based examples, we can leverage 
the combination of evidence from multiple 
sources. 
The human evaluation numbers presented in the 
previous section are encouraging. Article and 
preposition errors present the greatest difficulty for 
many learners as well as machines, but can 
nevertheless be corrected even in extremely noisy 
text with reasonable accuracy. Providing 
contextually appropriate real-life examples 
alongside with the suggested correction will, we 
believe, help the non-native user reach a more 
informed decision than just presenting a correction 
without additional evidence and information. 
The greatest challenge we are facing is the 
reduction of ?false flags?, i.e. flags where both 
error detection and suggested correction are 
incorrect. Such flags?especially for a non-native 
speaker?can be confusing, despite the fact that the 
impact is mitigated by the set of examples which 
may clarify the picture somewhat and help the 
users determine that they are dealing with an 
inappropriate correction. In the current system we 
use a set of carefully crafted heuristic thresholds 
that are geared towards minimizing false flags on a 
development set, based on detailed error analysis. 
As with all manually imposed thresholding, this is 
both a laborious and brittle process where each 
retraining of a model requires a re-tuning of the 
heuristics. We are currently investigating a learned 
ranker that combines information from language 
model and classifiers, using web counts as a 
supervision signal. 
7 Acknowledgements 
We thank Claudia Leacock (Butler Hill Group) for 
her meticulous analysis of errors and human 
evaluation of the system output, as well as for 
much invaluable feedback and discussion. 
References 
Bond, Francis. 2005.  Translating the Untranslatable: A 
Solution to the Problem of Generating English 
Determiners. CSLI Publications. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In Proceedingsof the 39th Annual 
Meeting of the Association for Computational 
Linguistics, pp 116-123. 
Chickering, David Maxwell. 2002. The WinMine 
Toolkit.  Microsoft Technical Report 2002-103. 
Chodorow, Martin, Joel R. Tetreault and Na-Rae Han. 
2007. Detection of Grammatical Errors Involving 
Prepositions. In Proceedings of the 4th ACL-SIGSEM 
Workshop on Prepositions, pp 25-30. 
Crystal, David. 1997.  Global English. Cambridge 
University Press. 
Rachele De Felice and Stephen G Pulman. 2007. 
Automatically acquiring models of preposition use. 
Proceedings of the ACL-07 Workshop on 
Prepositions. 
Dunning, Ted. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational 
Linguistics, 19:61-74. 
Gamon, Michael, Eric Ringger, and Simon Corston-
Oliver. 2002. Amalgam: A machine-learned 
generation module. Microsoft Technical Report, 
MSR-TR-2002-57. 
Golding, Andrew R. and Dan Roth. 1999. A Winnow 
Based Approach to Context-Sensitive Spelling 
Correction. Machine Learning, pp. 107-130. 
Gui, Shicun and Huizhong Yang (eds.). 2003. Zhongguo 
Xuexizhe Yingyu Yuliaohu. (Chinese Learner English 
Corpus). Shanghai Waiyu Jiaoyu Chubanshe.. 
Han, Na-Rae., Chodorow, Martin and Claudia Leacock. 
2004. Detecting errors in English article usage with a 
maximum entropy classifier trained on a large, 
diverse corpus. Proceedings of the 4th international 
conference on language resources and evaluation, 
Lisbon, Portugal. 
 
 
455
Han, Na-Rae. Chodorow, Martin., and Claudia Leacock. 
(2006). Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Heidorn, George. 2000. Intelligent Writing Assistance. 
In Robert Dale, Herman Moisl, and Harold Somers 
(eds.). Handbook of Natural Language Processing.  
Marcel Dekker.  pp 181 -207. 
Izumi, Emi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. The NICT JLE Corpus: Exploiting the 
Language Learner?s Speech Database for Research 
and Education. International Journal of the 
Computer, the Internet and Management 12:2, pp 
119 -125. 
Kneser, Reinhard. and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, volume 
1. 1995. pp. 181?184. 
Knight, Kevin and Ishwar Chander. 1994. Automatic 
Postediting of Documents. Proceedings of the 
American Association of Artificial Intelligence, pp 
779-784. 
Lee, John. 2004. Automatic Article Restoration. 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, pp. 31-
36. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. 
Proceedings of ACL 2000, pp 529-536. 
Minnen, Guido, Francis Bond and Ann Copestake. 
2000. Memory-Based Learning for Article 
Generation. Proceedings of the Fourth Conference 
on Computational Natural Language Learning and 
of the Second Learning Language in Logic 
Workshop, pp 43-48. 
Suzuki, Hisami and Kristina Toutanova. 2006. Learning 
to Predict Case Markers in Japanese. Proceedings of 
COLING-ACL, pp. 1049-1056. 
Toutanova, Kristina and Hisami Suzuki. 2007 
Generating Case Markers in Machine Translation.  
Proceedings of NAACL-HLT. 
Turner, Jenine and Eugene Charniak. 2007. Language 
Modeling for Determiner Selection. In Human 
Language Technologies 2007: The Conference of the 
North American Chapter of the Association for 
Computational Linguistics; Companion Volume, 
Short Papers, pp 177-180. 
Yi, Xing, Jianfeng Gao and William B. Dolan. 2008. 
Web-Based English Proofing System for English as a 
Second Language Users. To be presented at IJCNLP 
2008. 
456
A Web-based English Proofing System for English as a Second Language
Users
Xing Yi1, Jianfeng Gao2 and William B. Dolan2
1Center for Intelligent Information Retrieval, Department of Computer Science
University of Massachusetts, Amherst, MA 01003-4610, USA
yixing@cs.umass.edu
2Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA
{jfgao,billdol}@microsoft.com
Abstract
We describe an algorithm that relies on
web frequency counts to identify and correct
writing errors made by non-native writers of
English. Evaluation of the system on a real-
world ESL corpus showed very promising
performance on the very difficult problem of
critiquing English determiner use: 62% pre-
cision and 41% recall, with a false flag rate
of only 2% (compared to a random-guessing
baseline of 5% precision, 7% recall, and
more than 80% false flag rate). Performance
on collocation errors was less good, sug-
gesting that a web-based approach should be
combined with local linguistic resources to
achieve both effectiveness and efficiency.
1 Introduction
Proofing technology for native speakers of English
has been a focus of work for decades, and some
tools like spell checkers and grammar checkers have
become standard features of document processing
software products. However, designing an English
proofing system for English as a Second Language
(ESL) users presents a major challenge: ESL writ-
ing errors vary greatly among users with different
language backgrounds and proficiency levels. Re-
cent work by Brockett et al (2006) utilized phrasal
Statistical Machine Translation (SMT) techniques to
correct ESL writing errors and demonstrated that
this data-intensive SMT approach is very promising,
but they also pointed out SMT approach relies on the
availability of large amount of training data. The ex-
pense and difficulty of collecting large quantities of
Search Phrase Google.com Live.com Yahoo.com
English as
Second Language 306,000 52,407 386,000
English as a
Second Language 1,490,000 38,336,308 4,250,000
Table 1: Web Hits for Phrasal Usages
raw and edited ESL prose pose an obstacle to this
approach.
In this work we consider the prospect of using
the Web, with its billions of web pages, as a data
source with the potential to aid ESL writers. Our
research is motivated by the observation that ESL
users already use the Web as a corpus of good En-
glish, often using search engines to decide whether
a particular spelling, phrase, or syntactic construc-
tion is consistent with usage found on the Web. For
example, unsure whether the native-sounding phrase
includes the determiner ?a?, a user might search for
both quoted strings ?English as Second Language?
and ?English as a Second Language?. The counts
obtained for each of these phrases on three different
search engines are shown in Table 1. Note the cor-
rect version, ?English as a Second Language?, has a
much higher number of web hits.
In order to determine whether this approach holds
promise, we implemented a web-based system for
ESL writing error proofing. This pilot study was in-
tended to:
1. identify different types of ESL writing errors and
how often they occur in ESL users? writing samples,
so that the challenges and difficulties of ESL error
proofing can be understood better;
2. explore the advantages and drawbacks of a web-
619
based approach, discover useful web data features,
and identify which types of ESL errors can be reli-
ably proofed using this technique.
We first catalog some major categories of ESL
writing errors, then review related work. Section 3
describes our Web-based English Proofing System
for ESL users (called ESL-WEPS later). Section 4
presents experimental results. Section 5 concludes.
1.1 ESL Writing Errors
In order to get ESL writing samples, we employed
a third party to identify large volumes of ESL web
pages (mostly from Japanese, Korean and Chinese
ESL users? blogs), and cull 1K non-native sen-
tences. A native speaker then rewrote these ESL
sentences ? when possible ? to produce a native-
sounding version. 353 (34.9%) of the original 1012
ESL sentences were labeled ?native-like?, another
347 (34.3%) were rewritten, and the remaining 312
(30.8%) were classified as simply unintelligible.
Table 2 shows some examples from the corpus il-
lustrating some typical types of ESL writing errors
involving: (1) Verb-Noun Collocations (VNC) and
(4) Adjective-Noun Collocations (ANC); (2) incor-
rect use of the transitive verb ?attend?; (3) deter-
miner (article) usage problems; and (5) more com-
plex lexical and style problems. We analyzed all
the pre- and post-edited ESL samples and found 441
ESL errors: about 20% are determiner usage prob-
lems(missing/extra/misused); 15% are VNC errors,
1% are ANC errors; others represent complex syn-
tactic, lexical or style problems. Multiple errors can
co-occur in one sentence. These show that real-
world ESL error proofing is very challenging.
Our findings are consistent with previous research
results on ESL writing errors in two respects:
1. ESL users have significantly more problems
with determiner usage than native speakers be-
cause the use and omission of definite and
indefinite articles varies across different lan-
guages (Schneider and McCoy, 1998)(Lons-
dale and Strong-Krause, 2003).
2. Collocation errors are common among ESL
users, and collocational knowledge contributes
to the difference between native speakers and
ESL learners (Shei and Pain, 2000): in CLEC,
a real-world Chinese English Learner Corpus
(Gui and Yang, 2003), about 30% of ESL writ-
ing errors involve different types of collocation
errors.
In the remainder of the paper, we focus on proofing
determiner usage and VNC errors.
2 Related Work
Researchers have recently proposed some success-
ful learning-based approaches for the determiner se-
lection task (Minnen et al, 2000), but most of this
work has aimed only at helping native English users
correct typographical errors. Gamon et al(2008)
recently addressed the challenging task of proofing
writing errors for ESL users: they propose combin-
ing contextual speller techniques and language mod-
eling for proofing several types of ESL errors, and
demonstrate some promising results. In a departure
from this work, our system directly uses web data
for the ESL error proofing task.
There is a small body of previous work on the
use of online systems aimed at helping ESL learners
correct collocation errors. In Shei and Pain?s sys-
tem (2000), for instance, the British National Cor-
pus (BNC) is used to extract English collocations,
and an ESL learner writing corpus is then used to
build a collocation Error Library. In Jian et al?s sys-
tem (2004), the BNC is also used as a source of col-
locations, with collocation instances and translation
counterparts from the bilingual corpus identified and
shown to ESL users. In contrast to this earlier work,
our system uses the web as a corpus, with string fre-
quency counts from a search engine index used to in-
dicate whether a particular collocation is being used
correctly.
3 Web-based English Proofing System for
ESL Users (ESL-WEPS)
The architecture of ESL-WEPS, which consists of
four main components, is shown in Fig.1.
Parse ESL Sentence and Identify Check Points
ESL-WEPS first tags and chunks (Sang and Buck-
holz, 2000) the input ESL sentence1, and identi-
fies the elements of the structures in the sentence
to be checked according to certain heuristics: when
1One in-house HMM chunker trained on English Penn Tree-
bank was used.
620
ID Pre-editing version Post-editing version
1 Which team can take the champion? Which team will win the championship?
2 I attend to Pyoung Taek University. I attend Pyoung Taek University.
3 I?m a Japanese and studying Info and I?m Japanese and studying Info
Computer Science at Keio University. Computer Science at Keio University.
4 Her works are kinda erotic but they will Her works are kind of erotic, but they will
never arouse any obscene, devil thoughts which might never arouse any obscene, evil thoughts which might
destroy the soul of the designer. destroy the soul of the designer.
5 I think it is so beautiful to go the way of theology I think it is so beautiful to get into theology,
and very attractive too, especially in the area of Christianity. especially Christianity, which attracts me.
Table 2: Some pre- and post-editing ESL writing samples, Bold Italic characters show where the ESL errors
are and how they are corrected/rewritten by native English speaker.
ESL
Sentences
Pre-processing(POS Tagger and Chunk Parser) IdentifyCheck Point
I am learning economics
at university.
[NP I/PRP] [VP am/VBP  learning/
VBG economics/NNS] [PP at/IN] [NP
university/NN] ./.
[VP am/VBP learning/VBG
economics/NNS]
Generate a set of queries, in order to
search correct English usages from Web
Queries:
1.   [economics at university]  AND  [learning]
2.  [economics] AND  [at university] AND
[learning]
3.  [economics]  AND  [university]  AND
[learning]
Search
Engine
Use Web statistics to identify plausible errors, Collect Summaries, Mine collocations or
determiner usages, Generate good suggestions and provide Web example sentences
N-best suggestions:
1. studying 194
2. doing 12
3. visiting 11
Web Examples:
Why Study Economics? - For Lecturers
The design of open days, conferences and other events for school
students  studying economics  and/or thinking of  studying economics at
university . These could be held in a university, in a conference  
http://whystudyeconomics.ac.uk/lecturers/
Figure 1: System Architecture
checking VNC errors, the system searches for a
structure of the form (VP)(NP) or (VP)(PP)(NP) in
the chunked sentence; when checking determiner
usage, the system searches for (NP). Table 3 shows
some examples. For efficiency and effectiveness, the
user can specify that only one specific error type be
critiqued; otherwise it will check both error types:
first determiner usage, then collocations.
Generate Queries In order to find appropriate web
examples, ESL-WEPS generates at each check point
a set of queries. These queries involve three differ-
ent granularity levels, according to sentence?s syntax
structure:
1. Reduced Sentence Level. In order to use
more contextual information, our system pref-
erentially generates a maximal-length query
hereafter called S-Queries, by using the origi-
nal sentence. For the check point chunk, the
verb/adj. to be checked is found and extracted
based on POS tags; other chunks are simply
concatenated and used to formulate the query.
For example, for the first example in Table 3,
the S-Query is [?I have? AND ?this person for
years? AND ?recognized?].
2. Chunk Level. The system segments each ESL
sentence according to chunk tags and utilizes
chunk pairs to generate a query, hereafter re-
ferred to as a C-Query, e.g. the C-Query for the
second example in Table 3 is [?I? AND ?went?
AND ?to climb? AND ?a tall mountain? AND
?last week?]
3. Word Level. The system generates queries by
using keywords from the original string, in the
processing eliminating stopwords used in typ-
ical IR engines, hereafter referred to as a W-
Query, e.g. W-Query for the first example in
Table 3 is [?I? AND ?have? AND ?person? AND
?years? AND ?recognized?]
As queries get longer, web search engines tend to re-
turn fewer and fewer results. Therefore, ESL-WEPS
first segments the original ESL sentence by using
punctuation characters like commas and semicolons,
then generates a query from only the part which con-
tains the given check point. When checking deter-
miner usage, three different cases (a or an/the/none)
621
Parsed ESL sentence Error Type Check Points
(NP I/PRP) (VP have/VBP recognized/VBN) (NP this/DT person/NN) (PP for/IN) (NP years/NNS) ./. VNC recognized this person
(NP I/PRP) (VP went/VBD) (VP to/TO climb/VB) (NP a/DT tall/JJ mountain/NN) (NP last/JJ week/NN) ./. ANC tall mountain, last week
(NP I/PRP) (VP went/VBD) (PP to/TO) (NP coffee/NN) (NP shop/NN) (NP yesterday/NN) ./. Determiner usage coffee, shop, yesterday
(NP Someone/NN) (ADVP once/RB) (VP said/VBD) (SBAR that/IN) Determiner usage meet a right person
(ADVP when/WRB) (NP you/PRP) (VP meet/VBP) (NP a/DT right/JJ person/NN) at the wrong time
(PP at/IN) (NP the/DT wrong/JJ time/NN),/, (NP it/PRP) (VP ?s/VBZ) (NP a/DT pity/NN)./. ?s a pity
Table 3: Parsed ESL sentences and Check Points.
are considered for each check point. For instance,
given the last example in Table 3, three C-Queries
will be generated: [meet a right person],[meet the
right person] and [meet right person]. Note that a
term which has been POS-tagged as NNP (proper
noun) will be skipped and not used for generating
queries in order to obtain more web hits.
Retreive Web Statistics, Collect Snippets To col-
lect enough web examples, three levels of query sets
are submitted to the search engine in the following
order: S-Query, C-Query, and finally W-Query. For
each query, the web hits df returned by search en-
gine is recorded, and the snippets from the top 1000
hits are collected. For efficiency reasons, we follow
Dumais (2002)?s approach: the system relies only
on snippets rather than full-text of pages returned
for each hit; and does not rely on parsing or POS-
tagging for this step. However, a lexicon is used in
order to determine the possible parts-of-speech of a
word as well as its morphological variants. For ex-
ample, to find the correct VNC for a given noun ?tea?
in the returned snippets, the verb drank in the same
clause will be matched before ?tea?.
Identify Errors and Mine Correct Usages To de-
tect determiner usage errors, both the web hit dfq and
the length lq of a given query q are utilized, since
longer query phrases usually lead to fewer web hits.
DFLq, DFLMAX , qmax and Rq are defined as:
DFLq = dfq ? lq, for a given query q;
DFLMAX = max(DFLq),
qmax = argmax
q
(DFLq),
q ? {queries for a given check point};
Rq = DFLq/DFLMAX, given query q and check point.
If DFLMAX is less than a given threshold t1, this
check point will be skipped; otherwise the qmax in-
dicates the best usage. We also calculate the relative
ratio Rq for three usages (a or an/the/none). If Rq is
larger than a threshold t2 for a query q, the system
will not report that usage as an error because it is
sufficiently supported by web data.
For collocation check points, ESL-WEPS may in-
teract twice with the search engine: first, it issues
query sets to collect web examples and identify plau-
sible collocation errors; then, if errors are detected,
new query sets will be issued in the second step in
order to mine correct collocations from new web ex-
amples. For example, for the first sentence in Ta-
ble 3, the S-Query will be [?I have? AND ?this per-
son for years? AND ?recognized?]; the system an-
alyzes returned snippets and identifies ?recognized?
as a possible error. The system then issues a new
S-Query [?I have? AND ?this person for years?], and
finally mines the new set of snippets to discover that
?known? is the preferred lexical option. In contrast
to proofing determiner usages errors, mfreq:
mfreq = frequency of matched collocational verb/adj.
in the snippets for a given noun,
is utilized to both identify errors and suggest correct
VNCs/ANCs. If mfreq is larger than a threshold
t3, the system will conclude that the collocation is
plausible and skip the suggestion step.
4 Experiments
In order to evaluate the proofing algorithm described
above, we utilized the MSN search engine API and
the ESL writing sample set described in Section
1.1 to evaluate the algorithm?s performance on two
tasks: determiner usage and VNC proofing. From
a practical standpoint, we consider precision on the
proofing task to be considerably more important
than recall: false flags are annoying and highly vis-
ible to the user, while recall failures are much less
problematic.
Given the complicated nature of the ESL error
proofing task, about 60% of ESL sentences in our set
that contained determiner errors also contained other
types of ESL errors. As a result, we were forced
to slightly revise the typical precision/recall mea-
surement in order to evaluate performance. First,
622
Good Proofing Examples
Error sentence 1 In my opinion, therefore, when we describe terrorism, its crucially important that
we consider the degree of the influence (i.e., power) on the other countries.
proofing suggestion consider the degree of influence
Error sentence 2 Someone once said that when you meet a right person at the wrong time, it?s a pity.
proofing suggestion meet the right person at the wrong time
Plausible Useful Proofing Examples
Error sentence 3 The most powerful place in Beijing, and in the whole China.
native speaker suggestion in the whole of China
system suggestion in whole China
Error sentence 4 Me, I wanna keep in touch with old friends and wanna talk with anyone who has different thought, etc.
native speaker suggestion has different ideas
system suggestion has a different thought
Table 4: ESL Determiner Usage Proofing by Native Speaker and ESL-WEPS.
Good Proofing Examples
Error sentence 1 I had great time there and got many friends.
proofing suggestion made many friends
Error sentence 2 Which team can take the champion?
proofing suggestion win the champion
Plausible Useful Proofing Examples
Error sentence 3 It may sounds fun if I say my firm resolution of this year is to get a girl friend.
native speaker suggestion sound funny
system suggestion make * fun or get * fun
Table 5: ESL VNC Proofing by Native Speaker and ESL-WEPS.
we considered three cases: (1) the system correctly
identifies an error and proposes a suggestion that ex-
actly matches the native speaker?s rewrite; (2) the
system correctly identifies an error but makes a sug-
gestion that differs from the native speaker?s edit;
and (3) the system incorrectly identifies an error. In
the first case, we consider the proofing good, in the
second, plausibly useful, and in the third case it is
simply wrong. Correspondingly, we introduce the
categories Good Precision (GP), Plausibly Useful
Precision (PUP) and Error Suggestion Rate (ESR),
which were calculated by:
GP = # of Good Proofings# of System?s Proofings ;
PUP = # of Plausibly Useful Proofings# of System?s Proofings ;
ESR = # of Wrong Proofings# of System?s Proofings ;
GP + PUP + ESR = 1
Furthermore, assuming that there are overall NA er-
rors for a given type A of ESL error , the typical
recall and false alarm were calculated by:
recall = # of Good ProofingsNA ;
false alarm = # of Wrong Proofings# of Check points for ESL error A
Table 4 and Table 5 show examples of Good or
Plausibly Useful proofing for determiner usage and
collocation errors, respectively. It can be seen the
system makes plausibly useful proofing suggestions
because some errors types are out of current sys-
tem?s checking range.
The system achieved very promising performance
despite the fact that many of the test sentences con-
tained other, complex ESL errors: using appro-
priate system parameters, ESL-WEPS showed re-
call 40.7% on determiner usage errors, with 62.5%
of these proofing suggestions exactly matching the
rewrites provided by native speakers. Crucially, the
false flag rate was only 2%. Note that a random-
guessing baseline was about 5% precision, 7% re-
call, but more than 80% false flag rate.
For collocation errors, we focused on the most
common VNC proofing task. mfreq and threshold
t3 described in Section 3 are used to control false
alarm, GP and recall. A smaller t3 can reduce recall,
but can increase GP. Table 7 shows how performance
changes with different settings for t3, and Fig. 2(b)
plots the GP/recall curve. Results are not very good:
as recall increases, GP decreases too quickly, so that
at 30.7% recall, precision is only 37.3%. We at-
tribute this to the fact that most search engines only
return the top 1000 web snippets for each query and
our current system relies on this limited number of
snippets to generate and rank candidates.
623
Recall 16.3% 30.2% 40.7% 44.2% 47.7% 50.0%
GP 73.7% 70.3% 62.5% 56.7% 53.3% 52.4%
PUP 15.8% 16.2% 25.0% 29.9% 29.9% 29.3%
false alarm 0.4% 1.4% 2.0% 2.6% 3.7% 4.3%
Table 6: Proofing performance of determiner usage
changes when setting different system parameters.
Recall 11.3% 12.9% 17.8% 25.8% 29.0% 30.7%
GP 77.8% 53.3% 52.4% 43.2% 40.9% 37.3%
PUP 11.11% 33.33% 33.33% 45.10% 48.65% 50.00%
false alarm 0.28% 0.57% 0.85% 0.85% 1.13% 2.55%
Table 7: VNC Proofing performance changes when
setting different system parameters.
5 Conclusion
This paper introduced an approach to the challeng-
ing real-world ESL writing error proofing task that
uses the index of a web search engine for cor-
pus statistics. We validated ESL-WEPS on a web-
crawled ESL writing corpus and compared the sys-
tem?s proofing suggestions to those produced by na-
tive English speakers. Promising performance was
achieved for proofing determiner errors, but less
good results for VNC proofing, possibly because the
current system uses web snippets to rank and gener-
ate collocation candidates. We are currently investi-
gating a modified strategy that exploits high quality
local collocation/synonym lists to limit the number
of proposed Verb/Adj. candidates.
We are also collecting more ESL data to validate
our system and are extending our system to more
ESL error types. Recent experiments on new data
showed that ESL-WEPS can also effectively proof
incorrect choices of prepositions. Later research will
compare the web-based approach to conventional
corpus-based approaches like Gamon et al (2008),
and explore their combination to address complex
ESL errors.
Good Precision vs. Recall
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
80.0%
10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0% 80.0%
Good Precision vs. Recall
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
80.0%
90.0%
5.0% 10.0% 15.0% 20.0% 25.0% 30.0% 35.0%(a)Determiner Usage Error Proofing (b)VNC Error Proofing
Figure 2: GP/recall curves. X and Y axis denotes
GP and Recall respectively.
Acknowledgement The authors have benefited
extensively from discussions with Michael Gamon
and Chris Brockett. We also thank the Butler Hill
Group for collecting the ESL examples.
References
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal smt techniques. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 249?256, Sydney, Australia.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002.
Web question answering: is more always better? In
Proceedings of the 25th Annual International ACM SI-
GIR, pages 291?298, Tampere, Finland.
M. Gamon, J.F. Gao, C. Brockett, A. Klementiev, W.B.
Dolan, and L. Vanderwende. 2008. Using contextual
speller techniques and language modeling for ESL er-
ror correction. In Proceedings of IJCNLP 2008, Hy-
derabad, India, January.
S. Gui and H. Yang, 2003. Zhongguo Xuexizhe Yingyu
Yuliaoku. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe, Shanghai. (In Chinese).
Jia-Yan Jian, Yu-Chia Chang, and Jason S. Chang.
2004. TANGO: bilingual collocational concordancer.
In Proceedings of the ACL 2004, pages 19?23,
Barcelona, Spain.
D. Lonsdale and D. Strong-Krause. 2003. Automated
rating of ESL essays. In Proceedings of the NAACL-
HLT 03 workshop on Building educational applica-
tions using natural language processing, pages 61?67,
Edmonton, Canada.
G. Minnen, F. Bond, and A. Copestake. 2000. Memory-
based learning for article generation. In Proceedings
of the Fourth Conference on Computational Natural
Language Learning and of the Second Learning Lan-
guage in Logic Workshop, pages 43?48.
E. Tjong Kim Sang and S. Buckholz. 2000. Introduction
to the conll-2000 shared task: Chunking. In Proceed-
ings of CoNLL-2000 and LLL-2000, pages 127?132,
Lisbon, Portugal.
D. Schneider and K. F. McCoy. 1998. Recognizing syn-
tactic errors in the writing of second language learn-
ers. In Proceedings of the 17th international confer-
ence on Computational linguistics, pages 1198?1204,
Montreal, Quebec, Canada.
C.-C. Shei and H. Pain. 2000. An esl writer?s collo-
cational aid. Computer Assisted Language Learning,
13(2):167?182.
624
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 249?256,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Correcting ESL Errors Using Phrasal SMT Techniques 
 
 Chris Brockett, William B. Dolan, and Michael Gamon 
Natural Language Processing Group 
Microsoft Research  
One Microsoft Way, Redmond, WA 98005, USA  
{chrisbkt,billdol,mgamon}@microsoft.com  
 
  
 
Abstract 
This paper presents a pilot study of the 
use of phrasal Statistical Machine Trans-
lation (SMT) techniques to identify and 
correct writing errors made by learners of 
English as a Second Language (ESL). 
Using examples of mass noun errors 
found in the Chinese Learner Error Cor-
pus (CLEC) to guide creation of an engi-
neered training set, we show that applica-
tion of the SMT paradigm can capture er-
rors not well addressed by widely-used 
proofing tools designed for native speak-
ers. Our system was able to correct 
61.81% of mistakes in a set of naturally-
occurring examples of mass noun errors 
found on the World Wide Web, suggest-
ing that efforts to collect alignable cor-
pora of pre- and post-editing ESL writing 
samples offer can enable the develop-
ment of SMT-based writing assistance 
tools capable of repairing many of the 
complex syntactic and lexical problems 
found in the writing of ESL learners. 
1 Introduction 
Every day, in schools, universities and busi-
nesses around the world, in email and on blogs 
and websites, people create texts in languages 
that are not their own, most notably English. Yet, 
for writers of English as a Second Language 
(ESL), useful editorial assistance geared to their 
needs is surprisingly hard to come by. Grammar 
checkers such as that provided in Microsoft 
Word have been designed primarily with native 
speakers in mind. Moreover, despite growing 
demand for ESL proofing tools, there has been 
remarkably little progress in this area over the 
last decade. Research into computer feedback for 
ESL writers remains largely focused on small-
scale pedagogical systems implemented within 
the framework of CALL (Computer Aided Lan-
guage Learning) (Reuer 2003; Vanderventer 
Faltin, 2003), while commercial ESL grammar 
checkers remain brittle and difficult to customize 
to meet the needs of ESL writers of different 
first-language (L1) backgrounds and skill levels.  
  Some researchers have begun to apply statis-
tical techniques to identify learner errors in the 
context of essay evaluation (Chodorow & Lea-
cock, 2000; Lonsdale & Strong-Krause, 2003), to 
detect non-native text (Tomokiyo & Jones, 2001), 
and to support lexical selection by ESL learners 
through first-language translation (Liu et al, 
2000). However, none of this work appears to 
directly address the more general problem of 
how to robustly provide feedback to ESL writ-
ers?and for that matter non-native writers in 
any second language?in a way that is easily tai-
lored to different L1 backgrounds and second-
language (L2) skill levels.  
In this paper, we show that a noisy channel 
model instantiated within the paradigm of Statis-
tical Machine Translation (SMT) (Brown et al, 
1993) can successfully provide editorial assis-
tance for non-native writers. In particular, the 
SMT approach provides a natural mechanism for 
suggesting a correction, rather than simply 
stranding the user with a flag indicating that the 
text contains an error. Section 2 further motivates 
the approach and briefly describes our SMT sys-
tem. Section 3 discusses the data used in our ex-
periment, which is aimed at repairing a common 
type of ESL error that is not well-handled by cur-
rent grammar checking technology: mass/count 
noun confusions. Section 4 presents experimental 
results, along with an analysis of errors produced 
by the system. Finally we present discussion and 
some future directions for investigation.  
249
2 Error Correction as SMT 
2.1 Beyond Grammar Checking 
A major difficulty for ESL proofing is that errors 
of grammar, lexical choice, idiomaticity, and 
style rarely occur in isolation. Instead, any given 
sentence produced by an ESL learner may in-
volve a complex combination of all these error 
types. It is difficult enough to design a proofing 
tool that can reliably correct individual errors; 
the simultaneous combination of multiple errors 
is beyond the capabilities of current proofing 
tools designed for native speakers. Consider the 
following example, written by a Korean speaker 
and found on the World Wide Web, which in-
volves the misapplication of countability to a 
mass noun:  
 
And I knew many informations 
about Christmas while I was 
preparing this article. 
 
The grammar and spelling checkers in Microsoft 
Word 2003 correctly suggest many ? much 
and informations ? information. 
Accepting these proposed changes, however, 
does not render the sentence entirely native-like. 
Substituting the word much for many leaves 
the sentence stilted in a way that is probably un-
detectable to an inexperienced non-native 
speaker, while the use of the word knew repre-
sents a lexical selection error that falls well out-
side the scope of conventional proofing tools. A 
better rewrite might be: 
 
And I learned a lot of in-
formation about Christmas 
while I was preparing this 
article. 
 
or, even more colloquially: 
 
And I learned a lot about 
Christmas while I was pre-
paring this article 
 
Repairing the error in the original sentence, 
then, is not a simple matter of fixing an agree-
ment marker or substituting one determiner for 
another. Instead, wholesale replacement of the 
phrase knew many informations with 
the phrase learned a lot is needed to pro-
duce idiomatic-sounding output. Seen in these 
terms, the process of mapping from a raw, ESL-
authored string to its colloquial equivalent looks 
remarkably like translation. Our goal is to show 
that providing editorial assistance for writers 
should be viewed as a special case of translation. 
Rather than learning how strings in one language 
map to strings in another, however, ?translation? 
now involves learning how systematic patterns of 
errors in ESL learners? English map to corre-
sponding patterns in native English    
2.2 A Noisy Channel Model of ESL Errors 
If ESL error correction is seen as a translation 
task, the task can be treated as an SMT problem 
using the noisy channel model of (Brown et al, 
1993): here the L2 sentence produced by the 
learner can be regarded as having been corrupted 
by noise in the form of interference from his or 
her L1 model and incomplete language models 
internalized during language learning. The task, 
then, is to reconstruct a corresponding valid sen-
tence of L2 (target). Accordingly, we can seek to 
probabilistically identify the optimal correct tar-
get sentence(s) T* of an ESL input sentence S by 
applying the familiar SMT formula: 
 
( ){ }
{ })P()|P(maxarg
|Pmaxarg*
TTS
STT
T
T
=
=
 
In the context of this model, editorial assis-
tance becomes a matter of identifying those seg-
ments of the optimal target sentence or sentences 
that differ from the writer?s original input and 
displaying them to the user. In practice, the pat-
terns of errors produced by ESL writers of spe-
cific L1 backgrounds can be captured in the 
channel model as an emergent property of train-
ing data consisting ESL sentences aligned with 
their corrected edited counterparts. The highest 
frequency errors and infelicities should emerge 
as targets for replacement, while lesser frequency 
or idiosyncratic problems will in general not sur-
face as false flags. 
2.3 Implementation 
In this paper, we explore the use of a large-scale 
production statistical machine translation system 
to correct a class of ESL errors. A detailed de-
scription of the system can be found in (Menezes 
& Quirk 2005) and (Quirk et al, 2005). In keep-
ing with current best practices in SMT, our sys-
tem is a phrasal machine translation system that 
attempts to learn mappings between ?phrases? 
(which may not correspond to linguistic units) 
rather than individual words. What distinguishes 
250
this system from other phrasal SMT systems is 
that rather than aligning simple sequences of 
words, it maps small phrasal ?treelets? generated 
by a dependency parse to corresponding strings 
in the target. This ?Tree-To-String? model holds 
promise in that it allows us to potentially benefit 
from being able to access a certain amount of 
structural information during translation, without 
necessarily being completely tied to the need for 
a fully-well-formed linguistic analysis of the in-
put?an important consideration when it is 
sought to handle ungrammatical or otherwise ill-
formed ESL input, but also simultaneously to 
capture relationships not involving contiguous 
strings, for example determiner-noun relations.  
In our pilot study, this system was em-
ployed without modification to the system archi-
tecture. The sole adjustment made was to have 
both Source (erroneous) and Target (correct) sen-
tences tokenized using an English language to-
kenizer. N-best results for phrasal alignment and 
ordering models in the decoder were optimized 
by lambda training via Maximum Bleu, along the 
lines described in (Och, 2003).  
3 Data Development 
3.1 Identifying Mass Nouns 
In this paper, we focus on countability errors as-
sociated with mass nouns. This class of errors 
(involving nouns that cannot be counted, such as 
information, pollution, and home-
work) is characteristically encountered in ESL 
writing by native speakers of several East Asian 
languages (Dalgish, 1983; Hua & Lee, 2004).1 
We began by identifying a list of English nouns 
that are frequently involved in mass/count errors 
in by writing by Chinese ESL learners, by taking 
the intersection of words which: 
? occurred in either the Longman Dictionary 
of Contemporary English or the American 
Heritage Dictionary with a mass sense 
? were involved in n ? 2 mass/count errors in 
the Chinese Learner English Corpus 
CLEC (Gui and Yang, 2003), either tagged 
as a mass noun error or else with an adja-
cent tag indicating an article error.2  
                                                 
1
 These constructions are also problematic for hand-
crafted MT systems (Bond et al, 1994). 
2
 CLEC tagging is not comprehensive; some common 
mass noun errors (e.g., make a good progress) 
are not tagged in this corpus. 
This procedure yielded a list of 14 words: 
knowledge, food, homework, fruit, 
news, color, nutrition, equipment, 
paper, advice, haste, information, 
lunch, and tea. 3   Countability errors in-
volving these words are scattered across 46 sen-
tences in the CLEC corpus.   
For a baseline representing the level of writing 
assistance currently available to the average ESL 
writer, we submitted these sentences to the 
proofing tools in Microsoft Word 2003. The 
spelling and grammar checkers correctly identi-
fied 21 of the 46 relevant errors, proposed one 
incorrect substitution (a few advice ? a few 
advices), and failed to flag the remaining 25 
errors. With one exception, the proofing tools 
successfully detected as spelling errors incorrect 
plurals on lexical items that permit only mass 
noun interpretations (e.g., informations), 
but ignored plural forms like fruits and pa-
pers even when contextually inappropriate. The 
proofing tools in Word 2003 also detected singu-
lar determiner mismatches with obligatory plural 
forms (e.g. a news).  
3.2 Training Data 
The errors identified in these sentences provided 
an informal template for engineering the data in 
our training set, which was created by manipulat-
ing well-formed, edited English sentences. Raw 
data came from a corpus of ~484.6 million words 
of Reuters Limited newswire articles, released 
between 1995 and 1998, combined with a 
~7,175,000-word collection of articles from mul-
tiple news sources from 2004-2005. The result-
ing dataset was large enough to ensure that all 
targeted forms occurred with some frequency. 
From this dataset we culled about 346,000 
sentences containing examples of the 14 targeted 
words. We then used hand-constructed regular 
expressions to convert these sentences into 
mostly-ungrammatical strings that exhibited 
characteristics of the CLEC data, for example:  
? much ? many: much advice ? 
many advice  
? some ? a/an: some advice ? 
an advice  
? conversions to plurals: much good 
advice ? many good advices  
                                                 
3
 Terms that also had a function word sense, such as 
will, were eliminated for this experiment.  
251
? deletion of counters: piece(s)/ 
item(s)/sheet(s) of)  
? insertion of determiners  
These were produced in multiple combinations 
for broad coverage, for example: 
 
I'm not trying to give you 
legal advice. ? 
? I'm not trying to give you a 
legal advice. 
? I'm not trying to give you 
the legal advice. 
? I'm not trying to give you 
the legal advices. 
A total of 24128 sentences from the news data 
were ?lesioned? in this manner to create a set of 
65826 sentence pairs. To create a balanced train-
ing set that would not introduce too many arti-
facts of the substitution (e.g., many should not 
always be recast as much just because that is the 
only mapping observed in the training data), we 
randomly created an equivalent number of iden-
tity-mapped pairs from the 346,000 examples, 
with each sentence mapping to itself. 
Training sets of various sizes up to 45,000 
pairs were then randomly extracted from the le-
sioned and non-lesioned pairs so that data from 
both sets occurred in roughly equal proportions.  
Thus the 45K data set contains approximately 
22,500 lesioned examples. An additional 1,000 
randomly selected lesioned sentences were set 
aside for lambda training the SMT system?s or-
dering and replacement models. 
4  Evaluation 
4.1 Test Data 
The amount of tagged data in CLEC is too small 
to yield both development and test sets from the 
same data. In order to create a test set, we had a 
third party collect 150 examples of the 14 words 
from English websites in China. After minor 
cleanup to eliminate sentences irrelevant to the 
task,4 we ended up with 123 example sentences 
to use as test set. The test examples vary widely 
in style, from the highly casual to more formal 
public announcements. Thirteen examples were 
determined to contain no errors relevant to our 
experiment, but were retained in the data.5  
4.2 Results 
Table 1 shows per-sentence results of translating 
the test set on systems built with training data 
sets of various sizes (given in thousands of sen-
tence pairs). Numbers for the proofing tools in 
Word 2003 are presented by way of comparison, 
with the caveat that these tools have been inten-
tionally implemented conservatively so as not to 
potentially irritate native users with false flags. 
For our purposes, a replacement string is viewed 
as correct if, in the view of a native speaker who 
might be helping an ESL writer, the replacement 
would appear more natural and hence potentially 
useful as a suggestion in the context of that sen-
tence taken in isolation. Number disagreement 
on subject and verb were ignored for the pur-
poses of this evaluation, since these errors were 
not modeled when we introduced lesions into the 
data. A correction counted as Whole if the sys-
tem produced a contextually plausible substitu-
tion meeting two criteria: 1) number and 2) de-
terminer/quantifier selection (e.g., many in-
formations ? much information). 
Transformations involving bare singular targets 
(e.g., the fruits ? fruit) also counted 
as Whole.  Partial corrections are those where 
only one of the two criteria was met and part of 
the desired correction was missing (e.g., an 
                                                 
4
 In addition to eliminating cases that only involved 
subject-verb number agreement, we excluded a small 
amount of spam-like word salad, several instances of 
the word homework being misused to mean ?work 
done out of the home?, and one misidentified quota-
tion from Scott?s Ivanhoe. 
5
 This test set may be downloaded at 
http://research.microsoft.com/research/downloads 
Data Size Whole Partial Correctly Left New Error Missed Word Order  Error 
45K 55.28  0.81  8.13  12.20  21.14  1.63  
30K 36.59  4.07  7.32  16.26  32.52  3.25  
15K 47.15  2.44  5.69  11.38  29.27  4.07  
cf. Word 29.27  0.81  10.57  1.63  57.72  N/A 
 
 
Table 1.  Replacement percentages (per sentence basis) using different training data sets  
 
252
equipments ? an equipment versus the 
targeted bare noun equipment). Incorrect sub-
stitutions and newly injected erroneous material 
anywhere in the sentence counted as New Errors, 
even if the proposed replacement were otherwise 
correct. However, changes in upper and lower 
case and punctuation were ignored.  
The 55.28% per-sentence score for Whole 
matches in the system trained on the 45K data set 
means that it correctly proposed full corrections 
in 61.8% of locations where corrections needed 
to be made. The percentage of Missed errors, i.e., 
targeted errors that were ignored by the system, 
is correspondingly low. On the 45K training data 
set, the system performs nearly on a par with 
Word in terms of not inducing corrections on 
forms that did not require replacement, as shown 
in the Correctly Left column.  The dip in accu-
racy in the 30K sentence pair training set is an 
artifact of our extraction methodology: the rela-
tively small lexical set that we are addressing 
here appears to be oversensitive to random varia-
tion in the engineered training data. This makes 
it difficult to set a meaningful lower bound on 
the amount of training data that might be needed 
for adequate coverage. Nonetheless, it is evident 
from the table, that given sufficient data, SMT 
techniques can successfully offer corrections for 
a significant percentage of cases of the phenom-
ena in question.  
Table 2 shows some sample inputs together 
with successful corrections made by the system. 
Table 3 illustrates a case where two valid correc-
tions are found in the 5-best ranked translations; 
intervening candidates were identical with the 
top-ranked candidate.   
4.3 Error Analysis 
Table 1 also indicates that errors associated with 
the SMT system itself are encouragingly few. A 
small number of errors in word order were found, 
one of which resulted in a severely garbled sen-
tence in the 45K data set. In general, the percent-
age of this type of error declines consistently 
with growth of the training data size. Linearity of 
the training data may play a role, since the sen-
tence pairs differ by only a few words. On the 
whole, however, we expect the system?s order 
model to benefit from more training data.  
The most frequent single class of newly intro-
duced error relates to sporadic substitution of the 
word their for determiners a/the. This is 
associated with three words, lunch, tea, and 
haste, and is the principal contributor to the 
lower percentages in the Correctly Left bin, as 
compared with Word. This overgeneralization 
error reflects our attempt to engineer the discon-
tinuous mapping the X of them ? their 
X, motivated by examples like the following, 
encountered in the CLEC dataset: 
Input Shanghai residents can buy the fruits for a cheaper price than before.  
Replacement Shanghai residents can buy fruit for a cheaper price than before . 
Input Thank u for giving me so many advice. 
Replacement thank u for giving me so much advice . 
Input Acquiring the knowledge of information warfare is key to 
winning wars 
Replacement acquiring knowledge of information warfare is key to win-
ning wars 
Input Many knowledge about Li Bai can be gain through it. 
Replacement much knowledge about Li Bai can be gain through it . 
Input I especially like drinking the tea. 
Replacement i especially like drinking tea . 
Input Icons printed on a paper have been brought from Europe, 
and were pasted on boards on Taiwan. 
Replacement icons printed on paper have been brought from Europe , and 
were pasted on boards on Taiwan . 
 
Table 2.  Sample corrections, using 45K engineered training data 
253
In this equal world, lots of 
people are still concerned 
on the colors of them ? 
 
The inability of our translation system to handle 
such discontinuities in a unitary manner reflects 
the limited ability of current SMT modeling 
techniques to capture long-distance effects. Simi-
lar alternations are rife in bilingual data, e.g., 
ne?pas in French (Fox, 2002) and separable 
prefixes in German (Collins et al 2005). As 
SMT models become more adept at modeling 
long-distance effects in a principled manner, 
monolingual proofing will benefit as well. 
The Missed category is heterogeneous. The 
SMT system has an inherent bias against deletion, 
with the result that unwanted determiners tended 
not to be deleted, especially in the smaller train-
ing sets.  
Other errors related to coverage in the devel-
opment data set. Several occurrences of green-
grocer?s apostrophes (tea?s, equipment?s) 
caused correction failures: these were not antici-
pated when engineering the training data. Like-
wise, the test data presented several malformed 
quantifiers and quantifier-like phrases (plenty 
tea ? plenty of tea, a lot infor-
mation ? a lot of information, 
few information ? too little in-
formation) that had been unattested in the 
development set. Examples such as these high-
light the difficulty in obtaining complete cover-
age when using handcrafted techniques, whether 
to engineer errors, as in our case, or to handcraft 
targeted correction solutions.    
The system performed poorly on words that 
commonly present both mass and count noun 
senses in ways that are apt to confuse L2 writers. 
One problematic case was paper. The follow-
ing sentences, for example, remained uncor-
rected:  
  
He published many paper in 
provincial and national pub-
lication. 
He has published thirty-two 
pieces of papers. 
 
Large amounts of additional training data 
would doubtless be helpful in providing contex-
tual resolutions to the problems. Improved 
alignment models may also play a role here in 
capturing complex structures of the kind repre-
sented by constructions involving counters.     
5 Discussion 
The artificially-engineered training data that we 
relied on for our experiments proved surprisingly 
useful in modeling real errors made by non-
native speakers. However, this is obviously a less 
than ideal data source, since the errors introduced 
by regular expressions are homogenously dis-
tributed in a way that naturally-occurring errors 
are not, creating artifacts that undoubtedly impair 
our SMT models.  
Artificial data of this sort may be useful as 
proof of concept, but hand engineering such data 
plainly does not present a viable path to develop-
ing real world applications. In order to be able to 
handle the rich panoply of errors and error inter-
actions encountered in the text of second lan-
guage learners large quantities of naturally-
occurring ?before? and ?after? texts will need to 
be collected. By way of illustration, Table 4 
shows the output of results of ?translating? our 
test data into more natural English by hand and 
dumping the pre- and post-editing pairs to the 
45K training set.6 Although we were unable to 
exactly recover the target sentences, inspection 
showed that 25 sentences had improved, some 
significantly, as Table 4 shows. Under the right 
conditions, the SMT system can capture contex-
tual morphological alternations (nutri-
tion/nutritious), together with complex 
mappings represented by the dependencies 
learn ? knowledge ? many (ESL) and 
                                                 
6
 Since a single example of each pair was insufficient 
to override the system?s inherent bias towards uni-
gram mappings, 5 copies of each pair were appended 
to the training data. 
Input: And we can learn many knowledge or new information from TV 
Candidate 1: And we can learn much knowledge or new information from TV 
Candidate 5: And we can learn a lot of knowledge or new information from TV 
 
Table 3.  Multiple replacement candidates generated by 45K training set 
 
254
gain ? knowledge ? a lot of (Eng-
lish). In a rule-based correction system, an im-
mense amount of hand-coding would be required 
to handle even a small subset of the potential 
range of such mismatches between learner and 
native-like English. This knowledge, we believe, 
is best acquired from data.  
5.1 The Need for Data Collection 
Given a sufficiently large corpus of aligned sen-
tences containing error patterns produced by ESL 
writers of the same L1 background and their cor-
rected counterparts we expect eventually to be 
able to capture the rich complexity of non-native 
error within a noisy-channel based SMT model.  
As a practical matter, however, parallel data of 
the kind needed is far from easy to come by. This 
does not mean, however, that such data does not 
exist. The void left by commercial grammar 
checkers is filled, largely unobserved, by a num-
ber of services that provide editorial assistance, 
ranging from foreign language teachers, to lan-
guage helpdesks in multinational corporations, to 
mentoring services for conferences. Translation 
bureaus frequently offer editing services for non-
native speakers. Yet, unlike translation, the ?be-
fore? and ?after? texts are rarely recycled in a 
form that can be used to build translation models. 
Although collecting this data will involve a large 
investment in time, effort, and infrastructure, a 
serious effort along these lines is likely to prove 
fruitful in terms of making it possible to apply 
the SMT paradigm to ESL error correction.  
5.2 Feedback to SMT 
One challenge faced by the SMT model is the 
extremely high quality that will need to be at-
tained before a system might be usable. Since it 
is highly undesirable that learners should be pre-
sented with inaccurate feedback that they may 
not have the experience or knowledge to assess, 
the quality bar imposed on error correction is far 
higher than is that tolerated in machine transla-
tion. Exploration of error correction and writing 
assistance using SMT models may thus prove an 
important venue for testing new SMT models. 
5.3 Advantages of the SMT Approach 
Statistical Machine Translation has provided a 
hugely successful research paradigm within the 
field of natural language processing over the last 
decade. One of the major advantages of using 
SMT in ESL writing assistance is that it can be 
expected to benefit automatically from any pro-
gress made in SMT itself. In fact, the approach 
presented here benefits from all the advantages 
of statistical machine translation. Since the archi-
tecture is not dependent on hard-to-maintain 
rules or regular expressions, little or no linguistic 
expertise will be required in developing and 
maintain applications. As with SMT, this exper-
tise is pushed into the data component, to be 
handled by instructors and editors, who do not 
need programming or scripting skills.  
We expect it to be possible, moreover, once 
parallel data becomes available, to quickly ramp 
up new systems to accommodate the needs of 
Input sentence And we can learn many knowledge or new information from 
TV. 
45K system output and we can learn much knowledge or new information from 
TV . 
45K + translation sys-
tem output 
we can gain a lot of knowledge or new information from 
TV . 
Input sentence The following is one of the homework for last week. 
45K system output the following is one of their homework for last week . 
45K + translation sys-
tem output 
the following is one of the homework assignments for 
last week . 
Input sentence i like mushroom,its very nutrition 
45K system output i like mushroom , its very nutrition 
45K + translation sys-
tem output i like mushroom , its very nutritious 
 
Table 4.  Contextual corrections before and after adding ?translations? to 45K training data 
255
learners with different first-language back-
grounds and different skill levels and to writing 
assistance for learners of L2s other than English. 
It is also likely that this architecture may have 
applications in pedagogical environments and as 
a tool to assist editors and instructors who deal 
regularly with ESL texts, much in the manner of 
either Human Assisted Machine Translation or 
Machine Assisted Human Translation. We also 
believe that this same architecture could be ex-
tended naturally to provide grammar and style 
tools for native writers.  
6 Conclusion and Future Directions 
In this pilot study we have shown that SMT tech-
niques have potential to provide error correction 
and stylistic writing assistance to L2 learners. 
The next step will be to obtain a large dataset of 
pre- and post-editing ESL text with which to 
train a model that does not rely on engineered 
data. A major purpose of the present study has 
been to determine whether our hypothesis is ro-
bust enough to warrant the cost and effort of a 
collection or data creation effort.  
Although we anticipate that it will take a sig-
nificant lead time to assemble the necessary 
aligned data, once a sufficiently large corpus is 
in hand, we expect to begin exploring ways to 
improve our SMT system by tailoring it more 
specifically to the demands of editorial assistance. 
In particular, we expect to be looking into alter-
native word alignment models and possibly en-
hancing our system?s decoder using some of the 
richer, more structured language models that are 
beginning to emerge. 
Acknowledgements 
The authors have benefited extensively from dis-
cussions with Casey Whitelaw when he interned 
at Microsoft Research during the summer of 
2005. We also thank the Butler Hill Group for 
collecting the examples in our test set.   
References 
Bond, Francis, Kentaro Ogura and Satoru Ikehara. 
1994. Countability and Number in Japanese-to-
English Machine Translation. COLING-94. 
Peter E Brown, Stephen A. Della Pietra, Robert L. 
Mercer, and Vincent J. Della Pietra. 1993. The 
Mathematics of Statistical Machine Translation. 
Computational Linguistics, Vol. 19(2): 263-311.  
Martin Chodorow and Claudia Leacock. 2000. An 
Unsupervised Method for Detecting Grammatical 
Errors. NAACL 2000.  
Michael Collins, Philipp Koehn and Ivona Ku?erov?. 
2005. Clause Restructuring for Statistical machine 
Translation. ACL 2005, 531-540.  
Gerard M. Dalgish. 1984. Computer-Assisted ESL 
Research. CALICO Journal.  2(2): 32-33 
Heidi J. Fox.  2002. Phrasal Cohesion and Statistical 
Machine Translation. EMNLP 2002. 
Shicun Gui and Huizhong Yang (eds). 2003 Zhong-
guo Xuexizhe Yingyu Yuliaohu. (Chinese Learner 
English Corpus). Shanghai: Shanghai Waiyu 
Jiaoyu Chubanshe. (In Chinese). 
Hua Dongfan and Thomas Hun-Tak Lee. 2004.  Chi-
nese ESL Learners' Understanding of the English 
Count-Mass Distinction. In Proceedings of the 7th 
Generative Approaches to Second Language Ac-
quisition Conference (GASLA 2004). 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun, 
and Changning Huang. 2000. PENS: A Machine-
aided English Writing System for Chinese Users. 
ACL 2000.  
Deryle Lonsdale and Diane Strong-Krause. 2003.  
Automated Rating of ESL Essays. In Proceedings 
of the HLT/NAACL Workshop: Building Educa-
tional Applications Using Natural Language Proc-
essing.   
Arul Menezes, and Chris Quirk. 2005. Microsoft Re-
search Treelet Translation System: IWSLT Evalua-
tion. Proceedings of the International Workshop on 
Spoken Language Translation.  
Franz Josef Och, 2003. Minimum error rate training 
in statistical machine translation. ACL 2003. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models.  ACL 2000. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency Tree Translation: Syntactically In-
formed Phrasal SMT. ACL 2005. 
Veit Reuer. 2003. Error Recognition and Feedback 
with Lexical Functional Grammar. CALICO Jour-
nal, 20(3): 497-512. 
Laura Mayfield Tomokiyo and Rosie Jones. 2001. 
You?re not from round here, are you? Naive Bayes 
Detection of Non-Native Utterance Text. NAACL 
2001. 
Anne Vandeventer Faltin. 2003. Natural language 
processing tools for computer assisted language 
learning. Linguistik online 17, 5/03 (http:// 
www.linguistik-online.de/17_03/vandeventer.html) 
 
256
Overcoming the customization bottleneck using example-based MT
Stephen D. Richardson, William B. Dolan, Arul Menezes, Monica Corston-Oliver?
Microsoft Research ?Butler Hill Group
One Microsoft Way 4610 Wallingford Ave. N.
Redmond, WA 98052 Seattle WA 98103
{steveri, billdol, arulm}@microsoft.com moco@butlerhill.com
Abstract
We describe MSR-MT, a large-scale
hybrid machine translation system
under development for several
language pairs. This system?s ability to
acquire its primary translation
knowledge automatically by parsing a
bilingual corpus of hundreds of
thousands of sentence pairs and
aligning resulting logical forms
demonstrates true promise for
overcoming the so-called MT
customization bottleneck. Trained on
English and Spanish technical prose, a
blind evaluation shows that MSR-MT?s
integration of rule-based parsers,
example based processing, and
statistical techniques produces
translations whose quality exceeds that
of uncustomized commercial MT
systems in this domain.
1 Introduction
Commercially available machine translation
(MT) systems have long been limited in their
cost effectiveness and overall utility by the need
for domain customization. Such customization
typically includes identifying relevant
terminology (esp. multi-word collocations),
entering this terminology into system lexicons,
and making additional tweaks to handle
formatting and even some syntactic
idiosyncrasies. One of the goals of data-driven
MT research has been to overcome this
customization bottleneck through automated or
semi-automated extraction of translation
knowledge from bilingual corpora.
To address this bottleneck, a variety of
example based machine translation (EBMT)
systems have been created and described in the
literature. Some of these employ parsers to
produce dependency structures for the sentence
pairs in aligned bilingual corpora, which are
then aligned to obtain transfer rules or examples
(Meyers et al 2000; Watanabe et al 2000).
Other systems extract and use examples that are
represented as linear patterns of varying
complexity (Brown 1999; Watanabe and Takeda
1998; Turcato et al 1999).
For some EBMT systems, substantial
collections of examples are also manually
crafted or at least reviewed for correctness after
being identified automatically (Watanabe et al
2000; Brown 1999; Franz et al 2000). The
efforts that report accuracy results for fully
automatic example extraction (Meyers et al
2000; Watanabe et al 2000) do so for very
modest amounts of training data (a few thousand
sentence pairs). Previous work in this area thus
raises the possibility that manual review or
crafting is required to obtain example bases of
sufficient coverage and accuracy to be truly
useful.
Other variations of EBMT systems are
hybrids that integrate an EBMT component as
one of multiple sources of transfer knowledge
(in addition to other transfer rule or knowledge
based components) used during translation
(Frederking et al 1994; Takeda et al 1992).
To our knowledge, commercial quality MT
has so far been achieved only through years of
effort in creating hand-coded transfer rules.
Systems whose primary source of translation
knowledge comes from an automatically created
example base have not been shown capable of
matching or exceeding the quality of
commercial systems.
This paper reports on MSR-MT, an MT
system that attempts to break the customization
bottleneck by exploiting example-based (and
some statistical) techniques to automatically
acquire its primary translation knowledge from a
bilingual corpus of several million words. The
system leverages the linguistic generality of
existing rule-based parsers to enable broad
coverage and to overcome some of the
limitations on locality of context characteristic
of data-driven approaches. The ability of MSR-
MT to adapt automatically to a particular
domain, and to produce reasonable translations
for that domain, is validated through a blind
assessment by human evaluators. The quality of
MSR-MT?s output in this one domain is shown
to exceed the output quality of two highly rated
(though not domain-customized) commercially
available MT systems.
We believe that this demonstration is the first
in the literature to show that automatic training
methods can produce a commercially viable
level of translation quality.
2 MSR-MT
MSR-MT is a data-driven hybrid MT system,
combining rule-based analysis and generation
components with example-based transfer. The
automatic alignment procedure used to create
the example base relies on the same parser
employed during analysis and also makes use of
its own small set of rules for determining
permissible alignments. Moderately sized
bilingual dictionaries, containing only word
pairs and their parts of speech, provide
translation candidates for the alignment
procedure and are also used as a backup source
of translations during transfer. Statistical
techniques supply additional translation pair
candidates for alignment and identify certain
multi-word terms for parsing and transfer.
The robust, broad-coverage parsers used by
MSR-MT were created originally for
monolingual applications and have been used in
commercial grammar checkers.1 These parsers
produce a logical form (LF) representation that
is compatible across multiple languages (see
section 3 below). Parsers now exist for seven
languages (English, French, German, Spanish,
Chinese, Japanese, and Korean), and active
development continues to improve their
accuracy and coverage.
1 Parsers for English, Spanish, French, and German
provide linguistic analyses for the grammar checker
in Microsoft Word.
Figure 1. MSR-MT architecture.
Generation components are currently being
developed for English, Spanish, Chinese, and
Japanese. Given the automated learning
techniques used to create MSR-MT transfer
components, it should theoretically be possible,
provided with appropriate aligned bilingual
corpora, to create MT systems for any language
pair for which we have the necessary parsing
and generation components. In practice, we
have thus far created systems that translate into
English from all other languages and that
translate from English to Spanish, Chinese, and
Japanese. We have experimented only
preliminarily with Korean and Chinese to
Japanese.
Results from our Spanish-English and
English-Spanish systems are reported at the end
of this paper. The bilingual corpus used to
produce these systems comes from Microsoft
manuals and help text. The sentence alignment
of this corpus is the result of using a commercial
translation memory (TM) tool during the
translation process.
The architecture of MSR-MT is presented in
Figure 1. During the training phase, source and
target sentences from the aligned bilingual
corpus are parsed to produce corresponding LFs.
The normalized word forms resulting from
parsing are also fed to a statistical word
association learner (described in section 4.1),
which outputs learned single word translation
pairs as well as a special class of multi-word
pairs. The LFs are then aligned with the aid of
translations from a bilingual dictionary and the
learned single word pairs (section 4.2). Transfer
mappings that result from LF alignment, in the
form of linked source and target LF segments,
are stored in a special repository known as
MindNet (section 4.3). Additionally, the learned
multi-word pairs are added to the bilingual
dictionary for possible backup use during
translation and to the main parsing lexicon to
improve parse quality in certain cases.
At runtime, MSR-MT?s analysis parses
source sentences with the same parser used for
source text during the training phase (section
5.1). The resulting LFs then undergo a process
known as MindMeld, which matches them
against the LF transfer mappings stored in
MindNet (section 5.2). MindMeld also links
segments of source LFs with corresponding
target LF segments stored in MindNet. These
target LF segments are stitched together into a
single target LF during transfer, and any
translations for words or phrases not found
during MindMeld are searched for in the
updated bilingual dictionary and inserted in the
target LF (section 5.3). Generation receives the
target LF as input, from which it produces a
target sentence (section 5.4).
3 Logical form
MSR-MT?s broad-coverage parsers produce
conventional phrase structure analyses
augmented with grammatical relations (Heidorn
et al 2000). Syntactic analyses undergo further
processing in order to derive logical forms
(LFs), which are graph structures that describe
labeled dependencies among content words in
the original input. LFs normalize certain
syntactic alternations (e.g. active/passive) and
resolve both intrasentential anaphora and long-
distance dependencies.
MT has proven to be an excellent application
for driving the development of our LF
representation. The code that builds LFs from
syntactic analyses is shared across all seven of
the languages under development. This shared
architecture greatly simplifies the task of
aligning LF segments (section 4.2) from
different languages, since superficially distinct
constructions in two languages frequently
collapse onto similar or identical LF
representations. Even when two aligned
sentences produce divergent LFs, the alignment
and generation components can count on a
consistent interpretation of the representational
machinery used to build the two. Thus the
meaning of the relation Topic, for instance, is
consistent across all seven languages, although
its surface realizations in the various languages
vary dramatically.
4 Training MSR-MT
This section describes the two primary
mechanisms used by MSR-MT to automatically
extract translation mappings from parallel
corpora and the repository in which they are
stored.
4.1 Statistical learning of single word-
and multi-word associations
The software domain that has been our
primary research focus contains many words
and phrases that are not included in our general-
domain lexicons. Identifying translation
correspondences between these unknown words
and phrases across an aligned dataset can
provide crucial lexical anchors for the alignment
algorithm described in section 4.2.
In order to identify these associations, source
and target text are first parsed, and normalized
word forms (lemmas) are extracted. In the
multi-word case, English ?captoid? processing is
exploited to identify sequences of related,
capitalized words. Both single word and multi-
word associations are iteratively hypothesized
and scored by the algorithm under certain
constraints until a reliable set of each is
obtained.
Over the English/Spanish bilingual corpus
used for the present work, 9,563 single word and
4,884 multi-word associations not already
known to our system were identified using this
method.
Moore (2001) describes this technique in
detail, while Pinkham & Corston-Oliver (2001)
describes its integration with MSR-MT and
investigates its effect on translation quality.
4.2 Logical form alignment
As described in section 2, MSR-MT acquires
transfer mappings by aligning pairs of LFs
obtained from parsing sentence pairs in a
bilingual corpus. The LF alignment algorithm
first establishes tentative lexical
correspondences between nodes in the source
and target LFs using translation pairs from a
bilingual lexicon. Our English/Spanish lexicon
presently contains 88,500 translation pairs,
which are then augmented with single word
translations acquired using the statistical method
described in section 4.1. After establishing
possible correspondences, the algorithm uses a
small set of alignment grammar rules to align
LF nodes according to both lexical and
structural considerations and to create LF
transfer mappings. The final step is to filter the
mappings based on the frequency of their source
and target sides. Menezes & Richardson (2001)
provides further details and an evaluation of the
LF alignment algorithm.
The English/Spanish bilingual training
corpus, consisting largely of Microsoft manuals
and help text, averaged 14.1 words per English
sentence. A 2.5 million word sample of English
data contained almost 40K unique word forms.
The data was arbitrarily split in two for use in
our Spanish-English and English-Spanish
systems. The first sub-corpus contains over
208,000 sentence pairs and the second over
183,000 sentence pairs. Only pairs for which
both Spanish and English parsers produce
complete, spanning parses and LFs are currently
used for alignment. Table 1 provides the
number of pairs used and the number of transfer
mappings extracted and used in each case.
Spanish-
English
English-
Spanish
Total sentence pairs 208,730 183,110
Sentence pairs used 161,606 138,280
Transfer mappings
extracted
1,208,828 1,001,078
Unique, filtered
mappings used
58,314 47,136
Table 1. English/Spanish transfer mappings from
LF alignment
4.3 MindNet
The repository into which transfer mappings
from LF alignment are stored is known as
MindNet. Richardson et al (1998) describes
how MindNet began as a lexical knowledge
base containing LF-like structures that were
produced automatically from the definitions and
example sentences in machine-readable
dictionaries. Later, MindNet was generalized,
becoming an architecture for a class of
repositories that can store and access LFs
produced for a variety of expository texts,
including but not limited to dictionaries,
encyclopedias, and technical manuals.
For MSR-MT, MindNet serves as the
optimal example base, specifically designed to
store and retrieve the linked source and target
LF segments comprising the transfer mappings
extracted during LF alignment. As part of daily
regression testing for MSR-MT, all the sentence
pairs in the combined English/Spanish corpus
are parsed, the resulting spanning LFs are
aligned, and a separate MindNet for each of the
two directed language pairs is built from the LF
transfer mappings obtained. These MindNets
are about 7MB each in size and take roughly 6.5
hours each to create on a 550 Mhz PC.
5 Running MSR-MT
MSR-MT translates sentences in four processing
steps, which were illustrated in Figure 1 and
outlined in section 2 above. These steps are
detailed using a simple example in the following
sections.
5.1 Analysis
The input source sentence is parsed with the
same parser used on source text during MSR-
MT?s training. The parser produces an LF for
the sentence, as described in section 3. For the
example LF in Figure 2, the Spanish input
sentence is Haga clic en el bot?n de opci?n. In
English, this is literally Make click in the button
of option. In fluent, translated English, it is
Click the option button.
Figure 2. LF produced for Haga clic en el bot?n
de opci?n.
5.2 MindMeld
The source LF produced by analysis is next
matched by the MindMeld process to the source
LF segments that are part of the transfer
mappings stored in MindNet. Multiple transfer
mappings may match portions of the source LF.
MindMeld attempts to find the best set of
matching transfer mappings by first searching
for LF segments in MindNet that have matching
lemmas, parts of speech, and other feature
information. Larger (more specific) mappings
are preferred to smaller (more general)
mappings. In other words, transfers with context
will be matched preferentially, but the system
will fall back to the smaller transfers when no
matching context is found. Among mappings of
equal size, MindMeld prefers higher-frequency
mappings. Mappings are also allowed to match
overlapping portions of the source LF so long as
they do not conflict in any way.
After an optimal set of matching transfer
mappings is found, MindMeld creates Links on
nodes in the source LF to copies of the
corresponding target LF segments retrieved
from the mappings. Figure 3 shows the source
LF for the example sentence with additional
Links to target LF segments. Note that Links
for multi-word mappings are represented by
linking the root nodes (e.g., hacer and click) of
the corresponding segments, then linking an
asterisk (*) to the other source nodes
participating in the multi-word mapping (e.g.,
usted and clic). Sublinks between
corresponding individual source and target
nodes of such a mapping (not shown in the
figure) are also created for use during transfer.
Figure 3. Linked LF for Haga clic en el bot?n de
opci?n.
5.3 Transfer
The responsibility of transfer is to take a linked
LF from MindMeld and create a target LF that
will be the basis for the target translation. This
is accomplished through a top down traversal of
the linked LF in which the target LF segments
pointed to by Links on the source LF nodes are
stitched together. When stitching together LF
segments from possibly complex multi-word
mappings, the sublinks set by MindMeld
between individual nodes are used to determine
correct attachment points for modifiers, etc.
Default attachment points are used if needed.
Also, a very small set of simple, general, hand-
coded transfer rules (currently four for English
to/from Spanish) may apply to fill current (and
we hope, temporary) gaps in learned transfer
mappings.
In cases where no applicable transfer
mapping was found during MindMeld, the
nodes in the source LF and their relations are
simply copied into the target LF. Default (i.e.,
most commonly occurring) single word
translations may still be found in the MindNet
for these nodes and inserted in the target LF, but
if not, translations are obtained, if possible, from
the same bilingual dictionary used during LF
alignment.
Figure 4 shows the target LF created by
transfer from the linked LF shown in Figure 3.
Figure 4. Target LF for Click the option button.
5.4 Generation
A rule-based generation component maps
from the target LF to the target string (Aikawa
et al 2001). The generation components for the
target languages currently handled by MSR-MT
are application-independent, having been
designed to apply to a range of tasks, including
question answering, grammar checking, and
translation. In its application to translation,
generation has no information about the source
language for a given input LF, working
exclusively with the information passed to it by
the transfer component. It uses this information,
in conjunction with a monolingual (target
language) dictionary to produce its output. One
generic generation component is thus sufficient
for each language.
In some cases, transfer produces an
unmistakably ?non-native? target LF. In order to
correct some of the worst of these anomalies, a
small set of source-language independent rules
is applied prior to generation. The need for such
rules reflects deficiencies in our current data-
driven learning techniques during transfer.
6 Evaluating MSR-MT
In evaluating progress, we have found no
effective alternative to the most obvious
solution: periodic, blind human evaluations
focused on translations of single sentences. The
human raters used for these evaluations work for
an independent agency and played no
development role building the systems they test.
Each language pair under active development is
periodically subjected to the evaluation process
described in this section.
6.1 Evaluation Methodology
For each evaluation, five to seven evaluators
are asked to evaluate the same set of 200 to 250
blind test sentences. For each sentence, raters
are presented with a reference sentence in the
target language, which is a human translation of
the corresponding source sentence. In order to
maintain consistency among raters who may
have different levels of fluency in the source
language, raters are not shown the source
sentence. Instead, they are presented with two
machine-generated target translations presented
in random order: one translation by the system
to be evaluated (the experimental system), and
another translation by a comparison system (the
control system). The order of presentation of
sentences is also randomized for each rater in
order to eliminate any ordering effect.
Raters are asked to make a three-way choice.
For each sentence, raters may choose one of the
two automatically translated sentences as the
better translation of the (unseen) source
sentence, assuming that the reference sentence
represents a perfect translation, or, they may
indicate that neither of the two is better. Raters
are instructed to use their best judgment about
the relative importance of fluency/style and
accuracy/content preservation. We chose to use
this simple three-way scale in order to avoid
making any a priori judgments about the
relative importance of these parameters for
subjective judgments of quality. The three-way
scale also allows sentences to be rated on the
same scale, regardless of whether the
differences between output from system 1 and
system 2 are substantial or negligible.
The scoring system is similarly simple; each
judgment by a rater is represented as 1 (sentence
from experimental system judged better), 0
(neither sentence judged better), or -1 (sentence
from control system judged better). For each
sentence, the score is the mean of all raters?
judgments; for each comparison, the score is the
mean of the scores of all sentences.
6.2 Evaluation results
Although work on MSR-MT encompasses a
number of language pairs, we focus here on the
evaluation of just two, Spanish-English and
English-Spanish. Training data was held
constant for each of these evaluations.
6.2.1 Spanish-English over time
Spanish-English
systems
Mean preference
score (7 raters)
Sample
size
MSR-MT 9/00
vs.
MSR-MT 12/00
0.30 ? 0.09
(at 0.95)
200
sentences
MSR-MT 12/00
vs.
MSR-MT 4/01
0.28 ? 0.07
(at 0.99)
250
sentences
This table summarizes two evaluations
tracking progress in MSR-MT?s Spanish-
English (SE) translation quality over a seven
month development period. The first evaluation,
with seven raters, compared a September 2000
version of the system to a December 2000
version. The second evaluation, carried out by
six raters, examined progress between
December 2000 and April 2001.
A score of -1 would mean that raters
uniformly preferred the control system, while a
score of 1 would indicate that all raters preferred
the comparison system for all sentences. In each
of these evaluations, all raters significantly
preferred the comparison, or newer, version of
MSR-MT, as reflected in the mean preference
scores of 0.30 and 0.28. These numbers confirm
that the system made considerable progress over
a relatively short time span.
6.2.2 Spanish-English vs. alternative system
Spanish-English
systems
Mean preference
score (7 raters)
Sample
size
MSR-MT 9/00 vs.
Babelfish
-0.23 ? 0.12
(at 0.95)
200
sentences
MSR-MT 12/00
vs. Babelfish
0.11 ? 0.10
(at 0.95)
200
sentences
MSR-MT 4/01 vs.
Babelfish
0.32 ? 0.11
(at .99)
250
sentences
This table summarizes our comparison of
MSR-MT?s Spanish-English (SE) output to the
output of Babelfish (http://world.altavista.com/).
Three separate evaluations were performed, in
order to track MSR-MT?s progress over seven
months. The first two evaluations involved
seven raters, while the third involved six.
The shift in the mean preference score from
-0.23 to 0.32 shows clear progress against
Babelfish; by the second evaluation, raters very
slightly preferred MSR-MT in this domain. By
April, all six raters strongly preferred MSR-MT.
6.2.3 English-Spanish vs. alternative system
English-Spanish
systems
Mean preference
score (5 raters)
Sample
size
MSR-MT 2/01
vs. L&H
0.078 ? 0.13
(at 0.95)
250
sentences
MSR-MT 4/01
vs. L&H
0.19 ? 0.14
(at 0.99)
250
sentences
The evaluations summarized in this table
compared February and April 2001 versions of
MSR-MT?s English-Spanish (ES) output to the
output of the Lernout & Hauspie (L&H) ES
system (http://officeupdate.lhsl.com/) for 250
source sentences. Five raters participated in the
first evaluation, and six in the second.
The mean preference scores show that by
April, MSR-MT was strongly preferred over
L&H. Interestingly, though, one rater who
participated in both evaluations maintained a
slight but systematic preference for L&H?s
translations. Determining which aspects of the
translations might have caused this rater to
behave differently from the others is a topic for
future investigation.
6.3 Discussion
These results document steady progress in
the quality of MSR-MT?s output over a
relatively short time. By April 2001, both the SE
and ES versions of the system had surpassed
Babelfish in translation quality for this domain.
While these versions of MSR-MT are the most
fully developed, the other language pairs under
development are also progressing rapidly.
In interpreting our results, it is important to
keep in mind that MSR-MT has been
customized to the test domain, while the
Babelfish and Lernout & Hauspie systems have
not.2 This certainly affects our results, and
2Babelfish was chosen for these comparisons only
after we experimentally compared its output to that
of the related Systran system augmented with its
computer domain dictionary. Surprisingly, the
means that our comparisons have a certain
asymmetry. As our work progresses, we hope to
evaluate MSR-MT against a quality bar that is
perhaps more meaningful: the output of a
commercial system that has been hand-
customized for a specific domain.
The asymmetrical nature of our comparison
cuts both ways, however. Customization
produces better translations, and a system that
can be automatically customized has an inherent
advantage over one that requires laborious
manual customization. Comparing an
automatically-customized version of MSR-MT
to a commercial system which has undergone
years of hand-customization will represent a
comparison that is at least as asymmetrical as
those we have presented here.
We have another, more concrete, purpose in
regularly evaluating our system relative to the
output of systems like Babelfish and L&H: these
commercial systems serve as (nearly) static
benchmarks that allow us to track our own
progress without reference to absolute quality.
7 Conclusions and Future Work
This paper has described MSR-MT, an
EBMT system that produces MT output whose
quality in a specific domain exceeds that of
commercial MT systems, thus attacking head-on
the customization bottleneck. This work
demonstrates that automatic data-driven
methods can provide commercial-quality MT.
In future work we hope to demonstrate that
MSR-MT can be rapidly adapted to very
different semantic domains, and that it can
compete in translation quality even with
commercial systems that have been hand-
customized to a particular domain.
Acknowledgements
We would like to acknowledge the efforts of
the MSR NLP group in carrying out this work.
References
Aikawa, T., M. Melero, L. Schwartz, and A. Wu
2001 ?Multilingual natural language
generation,? Proceedings of 8th European
Workshop on Natural Language Generation,
Toulouse.
generic SE Babelfish engine produced slightly better
translations of our technical data.
Brown, R. 1999. ?Adding linguistic knowledge
to a lexical example-based translation
system,? Proceedings of TMI 99.
Franz, A., K. Horiguchi, L. Duan, D. Ecker, E.
Koontz, and K. Uchida 2000. ?An integrated
architecture for example-based machine
translation,? Proceedings of COLING2000.
Frederking, R., S. Nirenburg, D. Farwell, S.
Helmreich, E. Hovy, K. Knight, S. Beale, C.
Domashnev, D. Attardo, D. Grannes, and R.
Brown 1994. ?Integrating translations from
multiple sources within the Pangloss Mark
III machine translation system,? Proceedings
of AMTA94.
Heidorn, G., K. Jensen, S. Richardson, and A.
Viesse 2000. In R. Dale, H. Moisl and H.
Somers (eds) Handbook of Natural Language
Processing. Marcel Dekker Inc.
Meyers, A., M. Kosaka, and R. Grishman. 2000.
?Chart-based transfer rule application in
machine translation,? Proceedings of
COLING98.
Menezes, A. and S. Richardson 2001. ?A best-
first alignment algorithm for automatic
extraction of transfer mappings from
bilingual corpora,? Proceedings of the
Workshop on Data-Driven Machine
Translation, ACL 2001.
Moore, R. 2001 ?Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships Among Words,?
Proceedings of the Workshop on Data-
Driven Machine Translation, ACL 2001.
Pinkham, J and M. Corston-Oliver 2001
?Adding Domain Specificity to an MT
system,? Proceedings of the Workshop on
Data-Driven Machine Translation, ACL
2001.
Richardson, S. D., W. Dolan, and L.
Vanderwende 1998. ?MindNet: Acquiring
and Structuring Semantic Information from
Text,? Proceedings of COLING-ACL ?98,
Montreal.
Takeda, K., N. Uramoto, T. Nasukawa, and T.
Tsutsumi 1992. ?Shalt 2?a symmetric
machine translation system with conceptual
transfer,? Proceedings of COLING92.
Turcato, D., P. McFetridge, F. Popowich, and J.
Toole 1999. ?A unified example-based and
lexicalist approach to machine translation,?
Proceedings of TMI 99.
Watanabe, W. Kurohashi, S. and E. Aramaki
2000. ?Finding structural correspondences
from bilingual parsed corpus for corpus-
based translation,? Proceedings of
COLING2000.
Watanabe, H. and K. Takeda 1998. ?A pattern-
based machine translation system extended
by example-based processing,? Proceedings
of COLING98.
Monolingual Machine Translation for Paraphrase Generation 
Chris QUIRK,  Chris BROCKETT  and  William DOLAN 
Natural Language Processing Group 
Microsoft Research 
One Microsoft Way 
Redmond, WA  90852  USA 
{chrisq,chrisbkt,billdol}@microsoft.com 
 
 
Abstract 
We apply statistical machine translation 
(SMT) tools to generate novel paraphrases 
of input sentences in the same language. 
The system is trained on large volumes of 
sentence pairs automatically extracted from 
clustered news articles available on the 
World Wide Web. Alignment Error Rate 
(AER) is measured to gauge the quality of 
the resulting corpus. A monotone phrasal 
decoder generates contextual replacements. 
Human evaluation shows that this system 
outperforms baseline paraphrase generation 
techniques and, in a departure from previ-
ous work, offers better coverage and scal-
ability than the current best-of-breed 
paraphrasing approaches. 
1 Introduction 
The ability to categorize distinct word sequences 
as ?meaning the same thing? is vital to applications 
as diverse as search, summarization, dialog, and 
question answering. Recent research has treated 
paraphrase acquisition and generation as a machine 
learning problem (Barzilay & McKeown, 2001; 
Lin & Pantel, 2002; Shinyama et al 2002, Barzilay 
& Lee, 2003, Pang et al, 2003). We approach this 
problem as one of statistical machine translation 
(SMT), within the noisy channel model of Brown 
et al (1993). That is, we seek to identify the opti-
mal paraphrase T* of a sentence S by finding: 
 
( ){ }
{ })P()|P(maxarg
|Pmaxarg*
TTS
STT
T
T
=
=
 
 
T and S being sentences in the same language.  
We describe and evaluate an SMT-based para-
phrase generation system that utilizes a monotone 
phrasal decoder to generate meaning-preserving 
paraphrases across multiple domains. By adopting 
at the outset a paradigm geared toward generating 
sentences, this approach overcomes many prob-
lems encountered by task-specific approaches. In 
particular, we show that SMT techniques can be 
extended to paraphrase given sufficient monolin-
gual parallel data.1 We show that a huge corpus of 
comparable and alignable sentence pairs can be 
culled from ready-made topical/temporal clusters 
of news articles gathered on a daily basis from 
thousands of sources on the World Wide Web, 
thereby permitting the system to operate outside 
the narrow domains typical of existing systems. 
2 Related work 
Until recently, efforts in paraphrase were not 
strongly focused on generation and relied primarily 
on narrow data sources. One data source has been 
multiple translations of classic literary works (Bar-
zilay & McKeown 2001; Ibrahim 2002; Ibrahim et 
al. 2003). Pang et al (2003) obtain parallel mono-
lingual texts from a set of 100 multiply-translated 
news articles. While translation-based approaches 
to obtaining data do address the problem of how to 
identify two strings as meaning the same thing, 
they are limited in scalability owing to the diffi-
culty (and expense) of obtaining large quantities of 
multiply-translated source documents.  
Other researchers have sought to identify pat-
terns in large unannotated monolingual corpora. 
Lin & Pantel (2002) derive inference rules by pars-
ing text fragments and extracting semantically 
similar paths. Shinyama et al (2002) identify de-
pendency paths in two collections of newspaper 
articles. In each case, however, the information 
extracted is limited to a small set of patterns.  
Barzilay & Lee (2003) exploit the meta-
information implicit in dual collections of news-
                                                          
1
 Barzilay & McKeown (2001), for example, reject the 
idea owing to the noisy, comparable nature of their data. 
wire articles, but focus on learning sentence-level 
patterns that provide a basis for generation. Multi-
sequence alignment (MSA) is used to identify sen-
tences that share formal (and presumably semantic) 
properties. This yields a set of clusters, each char-
acterized by a word lattice that captures n-gram-
based structural similarities between sentences. 
Lattices are in turn mapped to templates that can 
be used to produce novel transforms of input sen-
tences. Their methodology provides striking results 
within a limited domain characterized by a high 
frequency of stereotypical sentence types. How-
ever, as we show below, the approach may be of 
limited generality, even within the training domain.   
3 Data collection 
Our training corpus, like those of Shinyama et 
al. and Barzilay & Lee, consists of different news 
stories reporting the same event. While previous 
work with comparable news corpora has been lim-
ited to just two news sources, we set out to harness 
the ongoing explosion in internet news coverage. 
Thousands of news sources worldwide are compet-
ing to cover the same stories, in real time. Despite 
different authorship, these stories cover the same 
events and therefore have significant content over-
lap, especially in reports of the basic facts. In other 
cases, news agencies introduce minor edits into a 
single original AP or Reuters story. We believe 
that our work constitutes the first to attempt to ex-
ploit these massively multiple data sources for 
paraphrase learning and generation.  
3.1 Gathering aligned sentence pairs 
We began by identifying sets of pre-clustered 
URLs that point to news articles on the Web, gath-
ered from publicly available sites such as 
http://news.yahoo.com/, http://news.google.com 
and http://uk.newsbot.msn.com. Their clustering 
algorithms appear to consider the full text of each 
news article, in addition to temporal cues, to pro-
duce sets of topically/temporally related articles. 
Story content is captured by downloading the 
HTML and isolating the textual content. A super-
vised HMM was trained to distinguish story con-
tent from surrounding advertisements, etc.2 
Over the course of about 8 months, we collected 
11,162 clusters, comprising 177,095 articles and 
averaging 15.8 articles per cluster. The quality of 
                                                          
2
 We hand-tagged 1,150 articles to indicate which por-
tions of the text were story content and which were ad-
vertisements, image captions, or other unwanted 
material.  We evaluated several classifiers on a 70/30 
test train split and found that an HMM trained on a 
handful of features was most effective in identifying 
content lines (95% F-measure). 
these clusters is generally good. Impressionisti-
cally, discrete events like sudden disasters, busi-
ness announcements, and deaths tend to yield 
tightly focused clusters, while ongoing stories like 
the SARS crisis tend to produce very large and 
unfocused clusters. 
To extract likely paraphrase sentence pairs from 
these clusters, we used edit distance (Levenshtein 
1966) over words, comparing all sentences pair-
wise within a cluster to find the minimal number of 
word insertions and deletions transforming the first 
sentence into the second. Each sentence was nor-
malized to lower case, and the pairs were filtered 
to reject:  
 
? Sentence pairs where the sentences were 
identical or differed only in punctuation;  
? Duplicate sentence pairs;  
? Sentence pairs with significantly different 
lengths (the shorter is less than two-thirds 
the length of the longer);  
? Sentence pairs where the Levenshtein dis-
tance was greater than 12.0.3  
 
A total of 139K non-identical sentence pairs were 
obtained. Mean Levenshtein distance was 5.17; 
mean sentence length was 18.6 words. 
3.2 Word alignment 
To this corpus we applied the word alignment 
algorithms available in Giza++ (Och & Ney, 
2000), a freely available implementation of IBM 
Models 1-5 (Brown, 1993) and the HMM align-
ment (Vogel et al 1996), along with various im-
provements and modifications motivated by 
experimentation by Och & Ney (2000). In order to 
capture the many-to-many alignments that identify 
correspondences between idioms and other phrasal 
chunks, we align in the forward direction and again 
in the backward direction, heuristically recombin-
ing each unidirectional word alignment into a sin-
gle bidirectional alignment (Och & Ney 2000). 
Figure 1 shows an example of a monolingual 
alignment produced by Giza++. Each line repre-
sents a uni-directional link; directionality is indi-
cated by a tick mark on the target side of the link. 
We held out a set of news clusters from our 
training data and extracted a set of 250 sentence 
pairs for blind evaluation. Randomly extracted on 
the basis of an edit distance of 5 ? n ? 20 (to allow 
a range of reasonably divergent candidate pairs 
while eliminating the most trivial substitutions), 
the gold-standard sentence pairs were checked by 
an independent human evaluator to ensure that 
                                                          
3
 Chosen on the basis of ablation experiments and opti-
mal AER (discussed in 3.2).  
they contained paraphrases before they were hand 
word-aligned. 
To evaluate the alignments, we adhered to the 
standards established in Melamed (2001) and Och 
& Ney (2000, 2003). Following Och & Ney?s 
methodology, two annotators each created an ini-
tial annotation for each dataset, subcategorizing 
alignments as either SURE (necessary) or POSSIBLE 
(allowed, but not required). Differences were high-
lighted and the annotators were asked to review 
their choices on these differences. Finally we com-
bined the two annotations into a single gold stan-
dard: if both annotators agreed that an alignment 
should be SURE, then the alignment was marked as 
SURE in the gold-standard; otherwise the alignment 
was marked as POSSIBLE. 
To compute Precision, Recall, and Alignment 
Error Rate (AER) for the twin datasets, we used 
exactly the formulae listed in Och & Ney (2003). 
Let A be the set of alignments in the comparison, S 
be the set of SURE alignments in the gold standard, 
and P be the union of the SURE and POSSIBLE 
alignments in the gold standard. Then we have:  
 
||
||precision
A
PA ?
=  ||
||
  recall
S
SA ?
=
 
 
||
||AER
SA
SAPA
+
?+?
=  
 
Measured in terms of AER4, final interrater agree-
ment between the two annotators on the 250 sen-
tences was 93.1%. 
                                                          
4
 The formula for AER given here and in Och  & Ney 
(2003) is intended to compare an automatic alignment 
against a gold standard alignment. However, when com-
paring one human against another, both comparison and 
reference distinguish between SURE and POSSIBLE links. 
Because the AER is asymmetric (though each direction 
Table 1 shows the results of evaluating align-
ment after trainng the Giza++ model. Although the 
overall AER of 11.58% is higher than the best bi-
lingual MT systems (Och & Ney, 2003), the train-
ing data is inherently noisy, having more in 
common with analogous corpora than conventional 
MT parallel corpora in that the paraphrases are not 
constrained by the source text structure. The iden-
tical word AER of 10.57% is unsurprising given 
that the domain is unrestricted and the alignment 
algorithm does not employ direct string matching 
to leverage word identity.5 The non-identical word 
AER of 20.88% may appear problematic in a sys-
tem that aims to generate paraphrases; as we shall 
see, however, this turns out not to be the case. Ab-
lation experiments, not described here, indicate 
that additional data will improve AER.  
3.3 Identifying phrasal replacements 
Recent work in SMT has shown that simple 
phrase-based MT systems can outperform more 
sophisticated word-based systems (e.g. Koehn et 
al. 2003). Therefore, we adopt a phrasal decoder 
patterned closely after that of Vogel et al (2003). 
We view the source and target sentences S and T 
as word sequences s1..sm and t1..tn. A word align-
ment A of S and T can be expressed as a function 
from each of the source and target tokens to a 
unique cept (Brown et al 1993); isomorphically, a 
cept represents an aligned subset of the source and 
target tokens. Then, for a given sentence pair and 
word alignment, we define a phrase pair as a sub-
set of the cepts in which both the source and target 
tokens are contiguous. 6  We gathered all phrase 
                                                                                           
differs by less than 5%), we have presented the average 
of the directional AERs. 
5
 However, following SMT practice of augmenting data 
with a bilingual lexicon, we did append an identity lexi-
con to the training data. 
6
 While this does preclude the usage of ?gapped? phrase 
pairs such as or ? either ? or, we found such map-
Training Data Type: L12 
Precision   87.46% 
Recall      89.52% 
AER         11.58% 
Identical word precision   89.36% 
Identical word recall      89.50% 
Identical word AER         10.57% 
Non-identical word preci-
sion   76.99% 
Non-identical word recall      90.22% 
Non-identical word AER     20.88% 
 
Table 1. AER on the Lev12 corpus 
 
Figure 1. An example Giza++ alignment 
pairs (limited to those containing no more than five 
cepts, for reasons of computational efficiency) oc-
curring in at least one aligned sentence somewhere 
in our training corpus into a single replacement 
database. This database of lexicalized phrase pairs, 
termed phrasal replacements, serves as the back-
bone of our channel model. 
As in (Vogel et al 2003), we assigned probabili-
ties to these phrasal replacements via IBM Model 
1. In more detail, we first gathered lexical transla-
tion probabilities of the form P(s | t) by running 
five iterations of Model 1 on the training corpus. 
This allows for computing the probability of a se-
quence of source words S given a sequence of tar-
get words T as the sum over all possible 
alignments of the Model 1 probabilities: 
 
( ) ( )
( )??
?
? ?
=
=
Tt Ss
A
tsP
TASPTSP
|
|,|
 
 
(Brown et al (1993) provides a more detailed deri-
vation of this identity.) Although simple, this ap-
proach has proven effective in SMT for several 
reasons. First and foremost, phrasal scoring by 
Model 1 avoids the sparsity problems associated 
with estimating each phrasal replacement probabil-
ity with MLE (Vogel et al 2003). Secondly, it ap-
pears to boost translation quality in more 
sophisticated translation systems by inducing lexi-
cal triggering (Och et al 2004). Collocations and 
other non-compositional phrases receive a higher 
probability as a whole than they would as inde-
pendent single word replacements. 
One further simplification was made. Given that 
our domain is restricted to the generation of mono-
lingual paraphrase, interesting output can be pro-
duced without tackling the difficult problem of 
inter-phrase reordering.7 Therefore, along the lines 
of Tillmann et al (1997), we rely on only mono-
tone phrasal alignments, although we do allow in-
tra-phrasal reordering. While this means certain 
common structural alternations (e.g., ac-
tive/passive) cannot be generated, we are still able 
to express a broad range of phenomena: 
 
                                                                                           
pings to be both unwieldy in practice and very often 
indicative of poor a word alignment. 
7
 Even in the realm of MT, such an assumption can pro-
duce competitive results (Vogel et al 2003).  In addi-
tion, we were hesitant to incur the exponential increase 
in running time associated with those movement models 
in the tradition of Brown el al (1993), especially since 
these offset models fail to capture important linguistic 
generalizations (e.g., phrasal coherence, headedness). 
? Synonymy: injured ? wounded 
? Phrasal replacements: Bush administration 
? White House 
? Intra-phrasal reorderings: margin of error 
? error margin 
 
Our channel model, then, is determined solely 
by the phrasal replacements involved. We first as-
sume a monotone decomposition of the sentence 
pair into phrase pairs (considering all phrasal de-
compositions equally likely), and the probability 
P(S | T) is then defined as the product of the each 
phrasal replacement probability. 
The target language model was a trigram model 
using interpolated Kneser-Ney smoothing (Kneser 
& Ney 1995), trained over all 1.4 million sentences 
(24 million words) in our news corpus. 
3.4 Generating paraphrases 
To generate paraphrases of a given input, a stan-
dard SMT decoding approach was used; this is de-
scribed in more detail below. Prior to decoding, 
however, the input sentence underwent preprocess-
ing: text was lowercased, tokenized, and a few 
classes of named-entities were identified using 
regular expressions. 
To begin the decoding process, we first con-
structed a lattice of all possible paraphrases of the 
source sentence based on our phrasal translation 
database. Figure 2 presents an example. The lattice 
was realized as a set of |S| + 1 vertices v0..v|S| and a 
set of edges between those vertices; each edge was 
labeled with a sequence of words and a real num-
ber. Thus a edge connecting vertex vi to vj labeled 
with the sequence of words w1..wk and the real 
number p indicates that the source words si+1 to sj 
can be replaced by words w1..wk with probability p. 
Our replacement database was stored as a trie with 
words as edges, hence populating the lattice takes 
worst case O(n2) time. Finally, since source and 
target languages are identical, we added an identity 
mapping for each source word si: an edge from vi-1 
to vi with label si and a uniform probability u. This 
allows for handling unseen words. A high u value 
permits more conservative paraphrases. 
We found the optimal path through the lattice as 
scored by the product of the replacement model 
and the trigram language model. This algorithm 
reduces easily to the Viterbi algorithm; such a dy-
namic programming approach guarantees an effi-
cient optimal search (worst case O(kn), where n is 
the maximal target length and k is the maximal 
number of replacements for any word). In addition, 
fast algorithms exist for computing the n-best lists 
over a lattice (Soong & Huang 1991). 
Finally the resultant paraphrases were cleaned 
up in a post-processing phase to ensure output was 
not trivially distinguishable from other systems 
during human evaluation. All generic named entity 
tokens were re-instantiated with their source val-
ues, and case was restored using a model like that 
used in Vita et al (2003). 
3.5 Alternate approaches 
Barzilay &  Lee (2003) have released a common 
dataset that provides a basis for comparing differ-
ent paraphrase generation systems. It consists of 59 
sentences regarding acts of violence in the Middle 
East. These are accompanied by paraphrases gen-
erated by their Multi-Sequence Alignment (MSA) 
system and a baseline employing WordNet (Fell-
baum 1998), along with human judgments for each 
output by 2-3 raters. 
The MSA WordNet baseline was created by se-
lecting a subset of the words in each test sen-
tence?proportional to the number of words 
replaced by MSA in the same sentence?and re-
placing each with an arbitrary word from its most 
frequent WordNet synset. 
Since our SMT approach depends quite heavily 
on a target language model, we presented an alter-
nate WordNet baseline using a target language 
model.8 In combination with the language model 
described in section 3.4, we used a very simple 
replacement model: each appropriately inflected 
member of the most frequent synset was proposed 
as a possible replacement with uniform probability. 
This was intended to isolate the contribution of the 
language model from the replacement model. 
Given that our alignments, while aggregated into 
phrases, are fundamentally word-aligned, one 
question that arises is whether the information we 
learn is different in character than that learned 
                                                          
8
 In contrast, Barzilay and Lee (2003) avoided using a 
language model for essentially the same reason: their 
MSA approach did not take advantage of such a re-
source. 
from much simpler techniques. To explore this 
hypothesis, we introduced an additional baseline 
that used statistical clustering to produce an auto-
mated, unsupervised synonym list, again with a 
trigram language model. We used standard bigram 
clustering techniques (Goodman 2002) to produce 
4,096 clusters of our 65,225 vocabulary items. 
4 Evaluation 
We have experimented with several methods for 
extracting a parallel sentence-aligned corpus from 
news clusters using word alignment error rate, or 
AER, (Och & Ney 2003) as an evaluation metric. 
A brief summary of these experiments is provided 
in Table 1. To evaluate the quality of generation, 
we followed the lead of Barzilay & Lee (2003). 
We started with the 59 sentences and correspond-
ing paraphrases from MSA and WordNet (desig-
nated as WN below). Since the size of this data set 
made it difficult to obtain statistically significant 
results, we also included 141 randomly selected 
sentences from held-out clusters. We then pro-
duced paraphrases with each of the following sys-
tems and compared them with MSA and WN: 
 
? WN+LM: WordNet with a trigram LM 
? CL: Statistical clusters with a trigram LM 
? PR: The top 5 sentence rewrites produced by 
Phrasal Replacement. 
 
For the sake of consistency, we did not use the 
judgments provided by Barzilay and Lee; instead 
we had two raters judge whether the output from 
each system was a paraphrase of the input sen-
tence. The raters were presented with an input sen-
tence and an output paraphrase from each system 
in random order to prevent bias toward any par-
ticular judgment. Since, on our first pass, we found 
inter-rater agreement to be somewhat low (84%), 
we asked the raters to make a second pass of judg-
ments on those where they disagreed; this signifi-
cantly improved agreement (96.9%). The results of 
this final evaluation are summarized in Table 2. 
 
Figure 2. A simplified generation lattice: 44 top ranked edges from a total 4,140 
5 Analysis 
Table 2 shows that PR can produce rewordings 
that are evaluated as plausible paraphrases more 
frequently than those generated by either baseline 
techniques or MSA. The WordNet baseline per-
forms quite poorly, even in combination with a 
trigram language model: the language model does 
not contribute significantly to resolving lexical 
selection. The performance of CL is likewise 
abysmal?again a language model does nothing to 
help. The poor performance of these synonym-
based techniques indicates that they have little 
value except as a baseline. 
The PR model generates plausible paraphrases 
for the overwhelming majority of test sentences, 
indicating that even the relatively high AER for 
non-identical words is not an obstacle to successful 
generation. Moreover, PR was able to generate a 
paraphrase for all 200 sentences (including the 59 
MSA examples). The correlation between accept-
ability and PR sentence rank validates both the 
ranking algorithm and the evaluation methodology.  
In Table 2, the PR model scores significantly 
better than MSA in terms of the percentage of 
paraphrase candidates accepted by raters. More-
over, PR generates at least five (and often hun-
dreds more) distinct paraphrases for each test 
sentence. Such perfect coverage on this dataset is 
perhaps fortuitous, but is nonetheless indicative of 
scalability. By contrast Barzilay & Lee (2003) re-
port being able to generate paraphrases for only 59 
out of 484 sentences in their training (test?) set, a 
total of 12%.  
One potential concern is that PR paraphrases 
usually involve simple substitutions of words and 
short phrases (a mean edit distance of 2.9 on the 
top ranked sentences), whereas MSA outputs more 
complex paraphrases (reflected in a mean edit dis-
tance of 25.8). This is reflected in Table 3, which 
provides a breakdown of four dimensions of inter-
est, as provided by one of our independent evalua-
tors. Some 47% of MSA paraphrases involve 
significant reordering, such as an active-passive 
alternation, whereas the monotone PR decoder 
precludes anything other than minor transpositions 
within phrasal replacements. 
Should these facts be interpreted to mean that 
MSA, with its more dramatic rewrites, is ulti-
mately more ambitious than PR? We believe that 
the opposite is true. A close look at MSA suggests 
that it is similar in spirit to example-based machine 
translation techniques that rely on pairing entire 
sentences in source and target languages, with the 
translation step limited to local adjustments of the 
target sentence (e.g. Sumita 2001). When an input 
sentence closely matches a template, results can be 
stunning. However, MSA achieves its richness of 
substitution at the cost of generality. Inspection 
reveals that 15 of the 59 MSA paraphrases, or 
25.4%, are based on a single high-frequency, do-
main-specific template (essentially a running tally 
of deaths in the Israeli-Palestinian conflict). Unless 
one is prepared to assume that similar templates 
can be found for most sentence types, scalability 
and domain extensibility appear beyond the reach 
of MSA. 
In addition, since MSA templates pair entire 
sentences, the technique can produce semantically 
different output when there is a mismatch in in-
formation content among template training sen-
tences. Consider the third and fourth rows of Table 
3, which indicate the extent of embellishment and 
lossiness found in MSA paraphrases and the top-
ranked PR paraphrases. Particularly noteworthy is 
the lossiness of MSA seen in row 4. Figure 3 illus-
trates a case where the MSA paraphrase yields a 
significant reduction in information, while PR is 
more conservative in its replacements.  
While the substitutions obtained by the PR 
model remain for the present relatively modest, 
they are not trivial. Changing a single content word 
is a legitimate form of paraphrase, and the ability 
to paraphrase across an arbitrarily large sentence 
set and arbitrary domains is a desideratum of para-
phrase research. We have demonstrated that the 
SMT-motivated PR method is capable of generat-
ing acceptable paraphrases for the overwhelming 
majority of sentences in a broad domain.  
Method B&L59 B&L59 + 141 
PR #1 54 / 59 = 91.5% 177 / 200 = 89.5% 
PR #2 53 / 59 = 89.8% 168 / 200 = 84.0% 
PR #3 46 / 59 = 78.0% 164 / 200 = 82.0% 
PR #4 49 / 59 = 83.1% 163 / 200 = 81.5% 
MSA 46 / 59 = 78.0% 46 /   59 = 78.0% 
PR #5 44 / 59 = 74.6% 155 / 200 = 77.5% 
WN 23 / 59 = 39.0% 25 /   59 = 37.9% 
WN+LM 30 / 59 = 50.9% 53 / 200 = 27.5% 
CL  14 / 59 = 23.7% 26 / 200 = 13.0% 
 
Table 2. Human acceptability judgments 
 
MSA PR#1 
Rearrangement 28 / 59 = 47% 0 / 100 =   0% 
Phrasal alternation 11 / 59 = 19% 3 / 100 =   3% 
Info added 19 / 59 = 32% 6 / 100 =   6% 
Info lost 43 / 59 = 73% 31 / 100 = 31% 
 
Table 3. Qualitative analysis of paraphrases 
6 Future work 
Much work obviously remains to be done. Our 
results remain constrained by data sparsity, despite 
the large initial training sets. One major agenda 
item therefore will be acquisition of larger (and 
more diverse) data sets. In addition to obtaining 
greater absolute quantities of data in the form of 
clustered articles, we also seek to extract aligned 
sentence pairs that instantiate a richer set of phe-
nomena. Relying on edit distance to identify likely 
paraphrases has the unfortunate result of excluding 
interesting sentence pairs that are similar in mean-
ing though different in form. For example: 
 
The Cassini spacecraft, which is en route to Saturn, 
is about to make a close pass of the ringed 
planet's mysterious moon Phoebe 
 
On its way to an extended mission at Saturn, the 
Cassini probe on Friday makes its closest ren-
dezvous with Saturn's dark moon Phoebe. 
 
We are currently experimenting with data extracted 
from the first two sentences in each article, which 
by journalistic convention tend to summarize con-
tent (Dolan et al 2004). While noisier than the edit 
distance data, initial results suggest that these can 
be a rich source of information about larger phrasal 
substitutions and syntactic reordering.  
Although we have not attempted to address the 
issue of paraphrase identification here, we are cur-
rently exploring machine learning techniques, 
based in part on features of document structure and 
other linguistic features that should allow us to 
bootstrap initial alignments to develop more data. 
This will we hope, eventually allow us to address 
such issues as paraphrase identification for IR.  
To exploit richer data sets, we will also seek to 
address the monotone limitation of our decoder 
that further limits the complexity of our paraphrase 
output. We will be experimenting with more so-
phisticated decoder models designed to handle re-
ordering and mappings to discontinuous elements. 
We also plan to pursue better (automated) metrics 
for paraphrase evaluation.  
7 Conclusions 
We presented a novel approach to the problem 
of generating sentence-level paraphrases in a broad 
semantic domain. We accomplished this by using 
methods from the field of SMT, which is oriented 
toward learning and generating exactly the sorts of 
alternations encountered in monolingual para-
phrase. We showed that this approach can be used 
to generate paraphrases that are preferred by hu-
mans to sentence-level paraphrases produced by 
other techniques. While the alternations our system 
produces are currently limited in character, the 
field of SMT offers a host of possible enhance-
ments?including reordering models?affording a 
natural path for future improvements.  
A second important contribution of this work is 
a method for building and tracking the quality of 
large, alignable monolingual corpora from struc-
tured news data on the Web. In the past, the lack of 
such a data source has hampered paraphrase re-
search; our approach removes this obstacle. 
Acknowledgements 
We are grateful to Mo Corston-Oliver, Jeff Ste-
venson, Amy Muia, and Orin Hargraves of the 
Butler Hill Group for their work in annotating the 
data used in the experiments. This paper has also 
benefited from discussions with Ken Church, Mark 
Johnson, and Steve Richardson. We greatly appre-
ciate the careful comments of three anonymous 
reviewers. We remain, however, solely responsible 
for this content. 
References 
R. Barzilay and K. R. McKeown. 2001. Extracting Para-
phrases from a parallel corpus. In Proceedings of the 
ACL/EACL. 
R. Barzilay and L. Lee. 2003. Learning to Paraphrase; 
an unsupervised approach using multiple-sequence 
alignment. In Proceedings of HLT/NAACL. 
P. Brown, S. A. Della Pietra, V. J. Della Pietra and R. L. 
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation. Computational Linguistics 19(2): 
263-311. 
W. Dolan, C. Quirk and C. Brockett. 2004. Unsuper-
vised Construction of Large Paraphrase Corpora: Ex-
ploiting Massively Parallel News Sources.  To appear 
in Proceedings of COLING-2004. 
C. Fellbaum, ed. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, MA. 
J. Goodman. 2002. JCLUSTER. Software available at 
http://research.microsoft.com/~joshuago/ 
A. Ibrahim. 2002. Extracting Paraphrases from Aligned 
Corpora. Master of Engineering Thesis, MIT. 
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting Struc-
tural Paraphrases from Aligned Monolingual Cor-
pora. In Proceedings of the Second International 
Workshop on Paraphrasing (IWP 2003). Sapporo, 
Japan. 
R. Kneser and H. Ney. 1995. Improved backing-off for 
N-gram language modeling. In Proc. Int. Conf. on 
Acoustics, Speech and Signal Processing: 181-184. 
Detroit, MI. 
P. Koehn, F. Och, and D. Marcu. 2003. Statistical 
Phrase-Based Translation. In Proceedings of 
HLT/NAACL. 
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet 
Physice-Doklady 10: 707-710. 
D. Lin and P. Pantel. 2001. DIRT - Discovery of Infer-
ence Rules from Text. In Proceedings of ACM 
SIGKDD Conference on Knowledge Discovery and 
Data Mining: 323-328. 
I. D. Melamed. 2001. Empirical Methods for Exploiting 
Parallel Texts. The MIT Press. 
R. Mihalcea and T. Pedersen. 2003. An Evaluation Ex-
ercise for Word Alignment. In HLT/NAACL Work-
shop: Building and Using Parallel Texts: 1-10. 
F. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the ACL: 440-447. 
Hong Kong, China. 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computa-
tional Linguistics 29(1): 19-52. 
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based 
Alignment of Multiple Translations: Extracting Para-
phrases and Generating New Sentences. Proceedings 
of HLT/NAACL.  
Y. Shinyama, S. Sekine and K. Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles. In Pro-
ceedings of NAACL-HLT. 
F. K. Soong and E. F. Huang. 1991. A tree-trellis based 
fast search for finding the n-best sentence hypotheses 
in continuous speech recognition. In Proceedings of 
the IEEE International Conference on Acoustics, 
Speech and Signal Processing 1: 705-708. Toronto, 
Canada. 
E. Sumita. 2001. Example-based machine translation 
using DP-matching between work sequences. In Pro-
ceedings of the ACL 2001 Workshop on Data-Driven 
Methods in Machine Translation: 1?8. 
C. Tillmann, S. Vogel, H. Ney, and A. Zubaiga. 1997. A 
DP Based Search Using Monotone Alignments in 
Statistical Translation. In Proceedings of the ACL. 
L. Vita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 
2003. tRuEcasing. In Proceedings of the ACL: 152-
159. Sapporo, Japan. 
S. Vogel, H. Ney and C. Tillmann. 1996. HMM-Based 
Word Alignment in Statistical Translation. In Pro-
ceedings of the ACL: 836-841. Copenhagen Den-
mark. 
S. Vogel, Y. Zhang, F. Huang, A. Venugopal, B. Zhao, 
A. Tribble, M. Eck, and A. Waibel. 2003. The CMU 
Statistical Machine Translation System. In Proceed-
ings of MT Summit IX, New Orleans, Louisiana, 
USA. 
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 1?9,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Third PASCAL Recognizing Textual Entailment Challenge 
 
Danilo Giampiccolo 
CELCT 
Via alla Cascata 56/c 
38100 POVO TN 
giampiccolo@celct.it 
Bernardo Magnini 
FBK-ITC 
Via Sommarive 18, 
38100 Povo TN 
magnini@itc.it  
Ido Dagan 
Computer Science Department 
Bar-Ilan University 
Ramat Gan 52900, Israel 
dagan@macs.biu.ac.il 
Bill Dolan 
Microsoft Research 
Redmond, WA, 98052, USA 
billdol@microsoft.com 
Abstract 
This paper presents the Third PASCAL 
Recognising Textual Entailment Chal-
lenge (RTE-3), providing an overview of 
the dataset creating methodology and the 
submitted systems. In creating this 
year?s dataset, a number of longer texts 
were introduced to make the challenge 
more oriented to realistic scenarios. Ad-
ditionally, a pool of resources was of-
fered so that the participants could share 
common tools. A pilot task was also set 
up, aimed at differentiating unknown en-
tailments from identified contradictions 
and providing justifications for overall 
system decisions. 26 participants submit-
ted 44 runs, using different approaches 
and generally presenting new entailment 
models and achieving higher scores than 
in the previous challenges. 
1.1 The RTE challenges 
 
The goal of the RTE challenges has been to cre-
ate a benchmark task dedicated to textual en-
tailment ? recognizing that the meaning of one 
text is entailed, i.e. can be inferred, by another1. 
In the recent years, this task has raised great in-
terest since applied semantic inference concerns 
many practical Natural Language Processing 
(NLP) applications, such as Question Answering 
(QA), Information Extraction (IE), Summariza-
tion, Machine Translation and Paraphrasing, and 
certain types of queries in Information Retrieval 
(IR). More specifically, the RTE challenges 
have aimed to focus research and evaluation on 
this common underlying semantic inference task 
and separate it from other problems that differ-
ent NLP applications need to handle. For exam-
ple, in addition to textual entailment, QA sys-
tems need to handle issues such as answer re-
trieval and question type recognition.  
By separating out the general problem of tex-
tual entailment from these task-specific prob-
lems, progress on semantic inference for many 
application areas can be promoted. Hopefully, 
research on textual entailment will finally lead to 
the development of entailment ?engines?, which 
can be used as a standard module in many appli-
cations (similar to the role of part-of-speech tag-
gers and syntactic parsers in current NLP appli-
cations). 
In the following sections, a detailed descrip-
tion of RTE-3 is presented. After a quick review 
                                                 
1
 The task was first defined by Dagan and Glickman 
(2004). 
1
of the previous challenges (1.2), section 2 de-
scribes the preparation of the dataset. In section 
3 the evaluation process and the results are pre-
sented, together with an analysis of the perform-
ance of the participating systems. 
1.2  The First and Second RTE Challenges 
 
The first RTE challenge2 aimed to provide the 
NLP community with a new benchmark to test 
progress in recognizing textual entailment, and 
to compare the achievements of different groups. 
This goal proved to be of great interest, and the 
community's response encouraged the gradual 
expansion of the scope of the original task. 
The Second RTE challenge3 built on the suc-
cess of the first, with 23 groups from around the 
world (as compared to 17 for the first challenge) 
submitting the results of their systems. Repre-
sentatives of participating groups presented their 
work at the PASCAL Challenges Workshop in 
April 2006 in Venice, Italy. The event was suc-
cessful and the number of participants and their 
contributions to the discussion demonstrated that 
Textual Entailment is a quickly growing field of 
NLP research. In addition, the workshops 
spawned an impressive number of publications 
in major conferences, with more work in pro-
gress. Another encouraging sign of the growing 
interest in the RTE challenge was represented by 
the increase in the number of downloads of the 
challenge datasets, with about 150 registered 
downloads for the RTE-2 development set. 
1.3 The Third Challenge 
 
RTE-3 followed the same basic structure of the 
previous campaigns, in order to facilitate the 
participation of newcomers and to allow "veter-
ans" to assess the improvements of their systems 
in a comparable test exercise. Nevertheless, 
some innovations were introduced, on the one 
hand to make the challenge more stimulating 
and, on the other, to encourage collaboration 
between system developers. In particular, a lim-
ited number of longer texts, i.e. up to a para-
graph in length, were incorporated in order to 
move toward more comprehensive scenarios, 
                                                 
2
 http://www.pascal-network.org/Challenges/RTE/. 
3
 http://www.pascal-network.org/Challenges/RTE2./ 
which incorporate the need for discourse analy-
sis. However, the majority of examples re-
mained similar to those in the previous chal-
lenges, providing pairs with relatively short 
texts.  
Another innovation was represented by a re-
source pool4, where contributors had the possi-
bility to share the resources they used. In fact, 
one of the key conclusions at the second RTE 
Challenge Workshop was that entailment model-
ing requires vast knowledge resources that cor-
respond to different types of entailment reason-
ing. Moreover, entailment systems also utilize 
general NLP tools such as POS taggers, parsers 
and named-entity recognizers, sometimes posing 
specialized requirements to such tools. In re-
sponse to these demands, the RTE Resource 
Pool was built, which may serve as a portal and 
forum for publicizing and tracking resources, 
and reporting on their use.  
In addition, an optional pilot task, called "Ex-
tending the Evaluation of Inferences from Texts" 
was set up by the US National Institute of Stan-
dards and Technology (NIST), in order to ex-
plore two other sub-tasks closely related to tex-
tual entailment: differentiating unknown entail-
ments from identified contradictions and provid-
ing justifications for system decisions. In the 
first sub-task, the idea was to drive systems to 
make more precise informational distinctions, 
taking a three-way decision between "YES", 
"NO" and "UNKNOWN?, so that a hypothesis 
being unknown on the basis of a text would be 
distinguished from a hypothesis being shown 
false/contradicted by a text. As for the other sub-
task, the goal for providing justifications for de-
cisions was to explore how eventual users of 
tools incorporating entailment can be made to 
understand how decisions were reached by a 
system, as users are unlikely to trust a system 
that gives no explanation for its decisions. The 
pilot task exploited the existing RTE-3 Chal-
lenge infrastructure and evaluation process by 
using the same test set, while utilizing human 
assessments for the new sub-tasks. 
                                                 
4 http://aclweb.org/aclwiki/index.php?title=Textual_Entail 
ment_Resource_Pool. 
2
 Table 1: Some examples taken from the Development Set. 
 
2 The RTE-3 Dataset 
2.1 Overview 
 
The textual entailment recognition task required the 
participating systems to decide, given two text 
snippets t and h, whether t entails h. Textual en-
tailment is defined as a directional relation between 
two text fragments, called text (t, the entailing 
text), and hypothesis (h, the entailed text), so that a 
human being, with common understanding of lan-
guage and common background knowledge, can 
infer that h is most likely true on the basis of the 
content of t. 
As in the previous challenges, the RTE-3 dataset 
consisted of 1600 text-hypothesis pairs, equally 
divided into a development set and a test set. While 
the length of the hypotheses (h) was  the same as in 
the past datasets, a certain number of texts (t) were 
longer than in previous datasets, up to a paragraph. 
The longer texts were marked as L, after being se-
lected automatically when exceeding 270 bytes. In 
the test set they were about 17% of the total.  
As in RTE-2, four applications ? namely IE, IR, 
QA and SUM ? were considered as settings or con-
texts for the pairs generation (see 2.2 for a detailed 
description). 200 pairs were selected for each ap-
plication in each dataset. Although the datasets 
were supposed to be perfectly balanced, the num-
ber of negative examples were slightly higher in 
both development and test sets (51.50% and 
51.25% respectively; this was unintentional). Posi-
tive entailment examples, where t entailed h, were 
annotated YES; the negative ones, where entailment 
did not hold, NO. Each pair was annotated with its 
TASK TEXT HYPOTHESIS ENTAILMENT 
IE At the same time the Italian digital rights group, Elec-
tronic Frontiers Italy, has asked the nation's government 
to investigate Sony over its use of anti-piracy software. 
Italy's govern-
ment investigates 
Sony. 
NO 
IE Parviz Davudi was representing Iran at a meeting of the 
Shanghai Co-operation Organisation (SCO), the fledg-
ling association that binds Russia, China and four for-
mer Soviet republics of central Asia together to fight 
terrorism 
China is a mem-
ber of SCO. 
YES 
IR Between March and June, scientific observers say, up to 
300,000 seals are killed. In Canada, seal-hunting means 
jobs, but opponents say it is vicious and endangers the 
species, also threatened by global warming 
Hunting endan-
gers seal species. 
YES 
IR The Italian parliament may approve a draft law allow-
ing descendants of the exiled royal family to return 
home. The family was banished after the Second World 
War because of the King's collusion with the fascist 
regime, but moves were introduced this year to allow 
their return. 
Italian royal fam-
ily returns home. 
NO 
QA Aeschylus is often called the father of Greek tragedy; 
he wrote the earliest complete plays which survive from 
ancient Greece. He is known to have written more than 
90 plays, though only seven survive. The most famous 
of these are the trilogy known as Orestia. Also well-
known are The Persians and Prometheus Bound. 
"The Persians" 
was written by 
Aeschylus. 
YES 
SUM A Pentagon committee and the congressionally char-
tered Iraq Study Group have been preparing reports for 
Bush, and Iran has asked the presidents of Iraq and 
Syria to meet in Tehran. 
Bush will meet 
the presidents of 
Iraq and Syria in 
Tehran. 
NO 
3
related task (IE/IR/QA/SUM) and entailment 
judgment (YES/NO, obviously released only in the 
development set). Table 1 shows some examples 
taken from the development set. 
The examples in the dataset were based mostly 
on outputs (both correct and incorrect) of Web-
based systems. In order to avoid copyright prob-
lems, input data was limited to either what had al-
ready been publicly released by official competi-
tions or else was drawn from freely available 
sources such as WikiNews and Wikipedia. 
In choosing the pairs, the following judgment 
criteria and guidelines were considered: 
 
? As entailment is a directional relation, the 
hypothesis must be entailed by the given 
text, but the text need not be entailed by 
the hypothesis. 
? The hypothesis must be fully entailed by 
the text. Judgment must be NO if the hy-
pothesis includes parts that cannot be in-
ferred from the text. 
? Cases in which inference is very probable 
(but not completely certain) were judged as 
YES.  
? Common world knowledge was assumed, 
e.g. the capital of a country is situated in 
that country, the prime minister of a state is 
also a citizen of that state, and so on. 
2.2 Pair Collection 
 
As in RTE-2, human annotators generated t-h pairs 
within 4 application settings.  
 
The IE task was inspired by the Information Ex-
traction (and Relation Extraction) application, 
where texts and structured templates were replaced 
by t-h pairs. As in the 2006 campaign, the pairs 
were generated using four different approaches: 
1) Hypotheses were taken from the relations 
tested in the ACE-2004 RDR task, while 
texts were extracted from the outputs of ac-
tual IE systems, which were provided with 
relevant news articles. Correctly extracted  
instances were used to generate positive 
examples and incorrect instances to gener-
ate negative examples. 
2) The same procedure was followed using 
output of IE systems on the dataset of the 
MUC-4 TST3 task, in which the events are 
acts of terrorism. 
3) The annotated MUC-4 dataset and the 
news articles were also used to manually 
generate entailment pairs based on ACE re-
lations.  
4) Hypotheses corresponding to relations not 
found in the ACE and MUC datasets  were 
used both to be given to IE systems and to 
manually generate t-h pairs from collected 
news articles. Examples of these relations, 
taken from various semantic fields, were 
?X beat Y?, ?X invented Y?, ?X steal Y? 
etc. 
 
The common aim of all these processes was to 
simulate the need of IE systems to recognize that 
the given text indeed entails the semantic relation 
that is expected to hold between the candidate tem-
plate slot fillers.  
 
In the IR (Information Retrieval) application set-
ting, the hypotheses were propositional IR queries, 
which specify some statement, e.g. ?robots are 
used to find avalanche victims?. The hypotheses 
were adapted and simplified from standard IR 
evaluation datasets (TREC and CLEF). Texts (t) 
that did or did not entail the hypotheses were se-
lected from documents retrieved by different search 
engines (e.g. Google, Yahoo and MSN) for each 
hypothesis. In this application setting it was as-
sumed that relevant documents (from an IR per-
spective) should entail the given propositional hy-
pothesis. 
 
For the QA (Question Answering) task, annotators 
used questions taken from the datasets of official 
QA competitions, such as TREC QA and 
QA@CLEF datasets, and the corresponding an-
swers extracted from the Web by actual QA sys-
tems. Then they transformed the question-answer 
pairs into t-h pairs as follows: 
 
? An answer term of the expected answer 
type was picked from the answer passage -
either a correct or an incorrect one.  
? The question was turned into an affirma-
tive sentence plugging in the answer term. 
? t-h pairs were generate, using the affirma-
tive sentences as hypotheses (h?s) and the 
original answer passages as texts (t?s).  
4
For example, given the question ?How high is 
Mount Everest?? and a text (t) ?The above men-
tioned expedition team comprising of 10 members 
was permitted to climb 8848m. high Mt. Everest 
from Normal Route for the period of 75 days from 
15 April, 2007 under the leadership of Mr. Wolf 
Herbert of Austria?, the annotator, extracting the 
piece of information ?8848m.? from the text, 
would turn the question into an the affirmative sen-
tence ?Mount Everest is 8848m high?, generating a 
positive entailment pair. This process simulated the 
need of a QA system to verify that the retrieved 
passage text actually entailed the provided answer. 
 
In the SUM (Summarization) setting, the 
entailment pairs were generated using two proce-
dures. 
In the first one, t?s and h?s were sentences taken 
from a news document cluster, a collection of news 
articles that describe the same news item. Annota-
tors were given the output of multi-document 
summarization systems -including the document 
clusters and the summary generated for each clus-
ter. Then they picked sentence pairs with high lexi-
cal overlap, preferably where at least one of the 
sentences was taken from the summary (this sen-
tence usually played the role of t). For positive ex-
amples, the hypothesis was simplified by removing 
sentence parts, until it was fully entailed by t. 
Negative examples were simplified in a similar 
manner. In alternative, ?pyramids? produced for 
the experimental evaluation mehod in DUC 2005 
(Passonneau et al 2005) were exploited. In this 
new evaluation method, humans select sub-
sentential content units (SCUs) in several manually 
produced summaries on a subject, and collocate 
them in a ?pyramid?, which has at the top the 
SCUs with the higher frequency, i.e. those which 
are present in most summaries. Each SCU is identi-
fied by a label, a sentence in natural language 
which expresses the content. Afterwards, the anno-
tators individuate the SCUs present in summaries 
generated automatically (called peers), and link 
them to the ones present in the pyramid, in order to 
assign each peer a weight. In this way, the SCUs in 
the automatic summaries linked to the SCUs in the 
higher tiers of the pyramid are assigned a heavier 
weight than those at the bottom. For the SUM set-
ting, the RTE-3 annotators selected relevant pas-
sages from the peers and used them as T?s, mean-
while the labels of the corresponding SCUs were 
used as H?s. Small adjustments were allowed, 
whenever the texts were not grammatically accept-
able. This process simulated the need of a summa-
rization system to identify information redundancy, 
which should be avoided in the summary. 
2.3 Final dataset  
 
Each pair of the dataset was judged by three anno-
tators. As in previous challenges, pairs on which 
the annotators disagreed were filtered-out.  
On the test set, the average agreement between 
each pair of annotators who shared at least 100 ex-
amples was 87.8%, with an average Kappa level of 
0.75, regarded as substantial agreement according 
to Landis and Koch (1997).  
19.2 % of the pairs in the dataset were removed 
from the test set due to disagreement. The dis-
agreement was generally due to the fact that the h 
was more specific than the t, for example because 
it contained more information, or made an absolute 
assertion where t proposed only a personal opinion. 
In addition, 9.4 % of the remaining pairs were dis-
carded, as they seemed controversial, too difficult, 
or too similar when compared to other pairs.  
As far as the texts extracted from the web are 
concerned, spelling and punctuation errors were 
sometimes fixed by the annotators, but no major 
change was allowed, so that the language could be 
grammatically and stylistically imperfect. The hy-
potheses were finally double-checked by a native 
English speaker. 
3 The RTE-3 Challenge 
3.1 Evaluation measures 
 
The evaluation of all runs submitted in RTE-3 was 
automatic. The judgments (classifications) returned 
by the system were compared to the Gold Standard 
compiled by the human assessors. The main 
evaluation measure was accuracy, i.e. the percent-
age of matching judgments. 
For systems that provided a confidence-ranked 
list of the pairs, in addition to the YES/NO judg-
ment, an Average Precision measure was also 
computed. This measure evaluates the ability of 
systems to rank all the T-H pairs in the test set ac-
cording to their entailment confidence (in decreas-
ing order from the most certain entailment to the 
least certain). Average precision is computed as the 
5
average of the system's precision values at all 
points in the ranked list in which recall increases, 
that is at all points in the ranked list for which the 
gold standard annotation is YES, or, more for-
mally:  
 
?
=
?n
i i
iUpToPairEntailmentiE
R 1
)(#)(1
          (1) 
 
where n is the number of the pairs in the test set, R 
is the total number of positive pairs in the test set, 
E(i) is 1 if the i-th pair is positive and 0 otherwise, 
and i ranges over the pairs, ordered by their rank-
ing.  
In other words, the more the system was confi-
dent that t entails h, the higher was the ranking of 
the pair. A perfect ranking would have placed all 
the positive pairs (for which the entailment holds) 
before all the negative ones, yielding an average 
precision value of 1. 
3.2 Submitted systems 
 
Twenty-six teams participated in the third chal-
lenge, three more than in previous year. Table 2 
presents the list of the results of each submitted 
runs and the components used by the systems. 
Overall, we noticed a move toward deep ap-
proaches, with a general consolidation of ap-
proaches based on the syntactic structure of Text 
and Hypothesis. There is an evident increase of 
systems using some form of logical inferences (at 
least seven systems). However, these approaches, 
with few notably exceptions, do not seem to be 
consolidated enough, as several systems show re-
sults  not still at the state of art (e.g. Natural Logic 
introduced by Chambers et al). For many systems 
an open issue is the availability and integration of 
different and complex semantic resources-  
A more extensive and fine grained use of spe-
cific semantic phenomena is also emerging. As an 
example, Tatu and Moldovan carry on a sophisti-
cated analysis of named entities, in particular Per-
son names, distinguishing first names from last 
names. Some form of relation extraction, either 
through manually built patterns (Chambers et al) 
or through the use of an information extraction sys-
tem (Hickl and Bensley) have been introduced this 
year, even if still on a small scale (i.e. few rela-
tions).  
On the other hand, RTE-3 confirmed that both 
machine learning using lexical-syntactic features 
and transformation-based approaches on depend-
ency representations are well consolidated tech-
niques to address textual entailment. The extension 
of transformation-based approaches toward prob-
abilistic settings is an interesting direction investi-
gated by some systems (e.g. Harmeling). On the 
side of ?light? approaches to textual entailment, 
Malakasiotis and Androutpoulos provide a useful 
baseline for the task (0.61%) using only POS tag-
ging and then applying string-based measures to 
estimate the similarity between Text and Hypothe-
sis. 
As far as resources are concerned, lexical data-
bases (mostly WordNet and DIRT) are still widely 
used. Extended WordNet is also a common re-
source (for instance in Iftene and Balahur-
Dobrescu) and the Extended Wordnet Knowledge 
Base has been successfully used in (Tatu and 
Moldovan). Verb-oriented resources are also 
largely present in several systems, including Fra-
menet (e.g. Burchardt et al), Verbnet (Bobrow et 
al.) and Propbank (e.g. Adams et al). It seems that 
the use of the Web as a resource is more limited 
when compared to the previous RTE workshop. 
However, as in RTE-2, the use of large semantic 
resources is still a crucial factor affecting the per-
formance of systems (see, for instance, the use of a 
large corpus of entailment examples in Hickl and 
Bensley).  
Finally, an interesting aspect is that, stimulated 
by the percentage of longer texts included this year, 
a number of participating systems addressed anaph-
ora resolution (e.g. Delmonte, Bar-Haim et al, 
Iftene and Balahur-Dobrescu). 
3.3 Results 
 
The accuracy achieved by the participating sys-
tems ranges from 49% to 80% (considering the best 
run of each group), while most of the systems ob-
tained a score in between 59% and 66%. One sub-
mission, Hickl and Bensley achieved 80% accu-
racy, scoring 8% higher than the second system 
(Tatu and Moldovan, 72%), and obtaining the best 
absolute result achieved in the three RTE chal-
lenges. 
6
 Table 2: Submission results and components of the systems.
 . 
System Components 
First Author Accuracy 
Average 
precision L
ex
ic
al
 
R
el
at
io
n
,
 
W
o
rd
N
et
 
 
n
-
gr
am
\w
o
rd
 
sim
ila
rit
y 
Sy
n
ta
ct
ic
 
M
at
ch
-
in
g\
A
lig
n
in
g 
Se
m
an
tic
 
R
o
le
 
La
be
lin
g\
 
Fr
am
en
et
\P
ro
ba
n
k,
 
V
er
bn
et
 
Lo
gi
ca
l I
n
fe
re
n
ce
 
Co
rp
u
s/ 
W
eb
-
ba
se
d 
St
at
ist
ic
s,
 
LS
A
 
M
L 
Cl
as
sif
ic
at
io
n
 
A
n
ap
ho
ra
 
re
so
lu
tio
n
 
 
En
ta
ilm
en
t 
Co
rp
o
ra
 
?
 
D
IR
T 
Ba
ck
gr
o
u
n
d 
K
n
o
w
le
dg
e 
Adams 0.6700  X X    X X   
0.6112 0.6118 X  X   X  X X Bar-Haim 
0.5837 0.6093  X  X   X  X  
Baral 0.4963 0.5364 X    X    X 
0.6050 0.5897 X  X    X   Blake 
  0.6587 0.6096 X  X    X   
0.5112 0.5720  X   X X     Bobrow 
  0.5150 0.5807 X   X X     
0.6250  X  X X      Burchardt 
0.6262           
0.5500   X    X    Burek 
0.5500 0.5514          
0.6050 0.6341 X  X  X  X X  Chambers 
  0.6362 0.6527 X  X  X  X X  
0.5088 0.4961  X   
 
 X    X Clark  
0.4725 0.4961  X    X    X 
Delmonte 0.5875 0.5830 X  X X X   X  
0.6563  X X X       Ferrandez 
0.6375           
0.6062  X X     X   Ferr?s 
0.6150  X X     X   
0.5600 0.5813 X  X    X   Harmling 
0.5775 0.5952 X  X    X   
Hickl 0.8000 0.8815 X X   X  X X X 
0.6913  X  X      X Iftene 
0.6913  X  X      X 
0.6400  X X     X   Li 
0.6488           
Litkowski   0.6125           
Malakasiotis  0.6175 0.6808  X     X   
Marsi 0.5913    X      X 
0.5888  X X X    X   Montejo-R?ez 
0.6038  X X X    X   
0.6238  X X X    X   Rodrigo 
0.6312  X X X    X   
0.6262  X X       X Roth 
0.5975    X     X  
0.6100 0.6195 X X     X   Settembre 
  0.6262 0.6274 X X     X   
0.7225 0.6942 X    X   X X Tatu 
  0.7175 0.6797 X    X   X  
0.6650    X    X   Wang  
0.6687           
0.6675 0.6674 X  X    X   Zanzotto 
  0.6575 0.6732 X  X    X   
7
As far as the per-task results are concerned, the 
trend registered in RTE-2 was confirmed, in that 
there was a marked difference in the performances 
obtained in different task settings. 
In fact, the average accuracy achieved in the QA 
setting (0.71) was 20 points higher than that 
achieved in the IE setting (0.52); the average accu-
racy in the IR and Sum settings was 0.66 and 0.58 
respectively. In RTE-2 the best results were 
achieved in SUM, while the lower score was al-
ways recorded in IE. As already pointed out by 
Bar-Haim (2006), these differences should be fur-
ther investigated, as they could lead to a sensible 
improvement of the performance. 
As for the LONG pairs, which represented a 
new element of this year?s challenge, no substan-
tial difference was noted in the systems? perform-
ances: the average accuracy over the long pairs 
was 58.72%, compared to 61.93% over the short 
ones.  
4 Conclusions and future work 
 
At its third round, the Recognizing Textual En-
tailment task has reached a noticeable level of ma-
turity, as the very high interest in the NLP commu-
nity and the continuously increasing number of 
participants in the challenges demonstrate. The 
relevance of Textual Entailment Recognition to 
different applications, such as the AVE5 track at 
QA at CLEF6, has also been acknowledged. Fur-
thermore, the debates and the numerous publica-
tions about the Textual Entailment have contrib-
uted to the better understanding the task and its 
nature.  
To keep a good balance between the consoli-
dated main task and the need for moving forward, 
longer texts were introduced in the dataset, in order 
to make the task more challenging, and a pilot task 
was proposed. The Third RTE Challenge have also 
confirmed that the methodology for the creation of 
the datasets, developed in the first two campaigns, 
is robust. Overall, the transition of the challenge 
coordination from Bar-Ilan ?which organized the 
first two challenges- to CELCT was successful, 
though some problems were encountered, espe-
cially in the preparation of the data set. The sys-
                                                 
5
 http://nlp.uned.es/QA/ave/. 
6
 http://clef-qa.itc.it/. 
tems which took part in RTE-3 showed that the 
technology applied to Entailment Recognition has 
made significant progress, confirmed by the results, 
which were generally better than last year. In par-
ticular, visible progress in defining several new 
principled scenarios for RTE was represented, such 
as Hickl?s commitment-based approach, Bar 
Haim?s proof system, Harmeling?s probabilistic 
model, and Standford?s use of Natural Logic. 
If, on the one hand, the success that RTE has 
had so far is very encouraging, on the other, it in-
cites to overcome certain current limitations, and to 
set realistic and, at the same time, stimulating goals 
for the future. First at all, theoretical refinements 
both of the task and the models applied to it need 
to be developed. In particular, more efforts are re-
quired to improve knowledge acquisition, as little 
progress has been made on this front so far. Also 
the data set generation and the evaluation method-
ology  need to be refined and extended. A major 
problem in the current setting of the data collection 
is that the distribution of the examples is arbitrary 
to a large extent, being determined by manual se-
lection. Therefore new evaluation methodologies, 
which can reflect realistic distributions should be 
investigated, as well as the possibility of evaluating 
Textual Entailment Recognition within additional 
concrete application scenarios, following the spirit 
of the QA Answer Validation Exercise.  
 
 
Acknowledgments 
 
The following sources were used in the preparation 
of the data: 
 
? PowerAnswer question answering system, from 
Language Computer Corporation, provided by Dan 
Moldovan and Marta Tatu. 
http://www.languagecomputer.com/solutions/question answer-
ing/power answer/ 
 
? Cicero Custom and Cicero Relation information 
extraction systems, from Language Computer Cor-
poration, provided by Sanda M. Harabagiu, An-
drew Hickl, John Lehmann and  and Paul Aarseth. 
http://www.languagecomputer.com/solutions/information_ext
action/cicero/index.html 
 
? Columbia NewsBlaster multi-document summa-
rization system, from the Natural Language Proc-
8
essing group at Columbia University?s Departmen-
tof Computer Science. 
http://newsblaster.cs.columbia.edu/ 
 
? NewsInEssence multi-document summarization 
system provided by Dragomir R. Radev and Jahna 
Otterbacher from the Computational Linguistics 
and Information Retrieval research group, Univer-
sity of Michigan. 
http://www.newsinessence.com 
 
? New York University?s information extraction 
system, provided by Ralph Grishman, Department 
of Computer Science, Courant Institute of Mathe-
matical Sciences, New York University. 
 
? MUC-4 information extraction dataset, from the 
National Institute of Standards and Technology 
(NIST).  
http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 
 
? ACE 2004 information extraction templates, 
from the National Institute of Standards and Tech-
nology (NIST). 
http://www.nist.gov/speech/tests/ace/ 
 
? TREC IR queries and TREC-QA question collec-
tions, from the National Institute of Standards and 
Technology (NIST). 
http://trec.nist.gov/ 
 
? CLEF IR queries and CLEF-QA question collec-
tions, from DELOS Network of Excellence  
for Digital Libraries. 
 http://www.clef-campaign.org/, http://clef-qa.itc.it/ 
 
? DUC 2005 annotated peers, from Columbia Uni-
versity, NY, provided by Ani Nenkova. 
http://www1.cs.columbia.edu/~ani/DUC2005/ 
 
We would like to thank the people and organiza-
tions that made these sources available for the 
challenge. In addition, we thank Idan Szpektor and 
Roy Bar Haim from Bar-Ilan University  for their 
assistance and advice, and Valentina Bruseghini 
from CELCT for managing the RTE-3 website. 
 
We would also like to acknowledge the people 
and organizations involved in creating and annotat-
ing the data: Pamela Forner, Errol Hayman, Cam-
eron Fordyce from CELCT and Courtenay 
Hendricks, Adam Savel and Annika Hamalainen 
from the Butler Hill Group, which was funded by 
Microsoft Research. 
 
This work was supported in part by the IST Pro-
gramme of the European Community, under the 
PASCAL Network of Excellence, IST-2002-
506778. We wish to thank the managers of the 
PASCAL challenges program, Michele Sebag and 
Florence d?Alche-Buc, for their efforts and sup-
port, which made this challenge possible. We also 
thank David Askey, who helped manage the RTE 3 
website.  
 
References 
 
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, 
Danilo Giampiccolo, Bernardo Magnini and Idan 
Szpektor. 2006. The Second PASCAL Recognizing 
Textual Entailment Challenge. In Proceedings of the 
Second PASCAL Challenges Workshop on Recog-
nizing Textual Entailment, Venice, Italy. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2006. The PASCAL Recognizing Textual Entailment 
Challenge. In Qui?onero-Candela et al, editors, 
MLCW 2005, LNAI Volume 3944, pages 177-190. 
Springer-Verlag. 
J. R. Landis and G. G. Koch. 1997. The measurements 
of observer agreement for categorical data. Biomet-
rics, 33:159?174. 
Rebecca Passonneau, Ani Nenkova., Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the 
pyramid method in DUC 2005. In Proceedings of the 
Document Understanding Conference (DUC 05), 
Vancouver, B.C., Canada. 
Ellen M. Voorhees and Donna Harman. 1999. Overview 
of the seventh text retrieval conference. In Proceed-
ings of the Seventh Text Retrieval Conference 
(TREC-7). NIST Special Publication. 
 
 
9
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583?593,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Data-Driven Response Generation in Social Media
Alan Ritter
Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
aritter@cs.washington.edu
Colin Cherry
National Research Council Canada
Ottawa, Ontario, K1A 0R6
Colin.Cherry@nrc-cnrc.gc.ca
William B. Dolan
Microsoft Research
Redmond, WA 98052
billdol@microsoft.com
Abstract
We present a data-driven approach to generat-
ing responses to Twitter status posts, based on
phrase-based Statistical Machine Translation.
We find that mapping conversational stimuli
onto responses is more difficult than translat-
ing between languages, due to the wider range
of possible responses, the larger fraction of
unaligned words/phrases, and the presence of
large phrase pairs whose alignment cannot be
further decomposed. After addressing these
challenges, we compare approaches based on
SMT and Information Retrieval in a human
evaluation. We show that SMT outperforms
IR on this task, and its output is preferred over
actual human responses in 15% of cases. As
far as we are aware, this is the first work to
investigate the use of phrase-based SMT to di-
rectly translate a linguistic stimulus into an ap-
propriate response.
1 Introduction
Recently there has been an explosion in the number
of people having informal, public conversations on
social media websites such as Facebook and Twit-
ter. This presents a unique opportunity to build
collections of naturally occurring conversations that
are orders of magnitude larger than those previously
available. These corpora, in turn, present new op-
portunities to apply data-driven techniques to con-
versational tasks.
We investigate the problem of response genera-
tion: given a conversational stimulus, generate an
appropriate response. Specifically, we employ a
large corpus of status-response pairs found on Twit-
ter to create a system that responds to Twitter status
posts. Note that we make no mention of context, in-
tent or dialogue state; our goal is to generate any re-
sponse that fits the provided stimulus; however, we
do so without employing rules or templates, with the
hope of creating a system that is both flexible and
extensible when operating in an open domain.
Success in open domain response generation
could be immediately useful to social media plat-
forms, providing a list of suggested responses to a
target status, or providing conversation-aware auto-
complete for responses in progress. These features
are especially important on hand-held devices (Has-
selgren et al, 2003). Response generation should
also be beneficial in building ?chatterbots? (Weizen-
baum, 1966) for entertainment purposes or compan-
ionship (Wilks, 2006). However, we are most ex-
cited by the future potential of data-driven response
generation when used inside larger dialogue sys-
tems, where direct consideration of the user?s utter-
ance could be combined with dialogue state (Wong
and Mooney, 2007; Langner et al, 2010) to generate
locally coherent, purposeful dialogue.
In this work, we investigate statistical machine
translation as an approach for response generation.
We are motivated by the following observation: In
naturally occurring discourse, there is often a strong
structural relationship between adjacent utterances
(Hobbs, 1985). For example, consider the stimulus-
response pair from the data:
Stimulus: I?m slowly making this soup
...... and it smells gorgeous!
583
Response: I?ll bet it looks delicious too!
Haha
Here ?it? in the response refers to ?this soup? in
the status by co-reference; however, there is also a
more subtle relationship between the ?smells? and
?looks?, as well as ?gorgeous? and ?delicious?. Par-
allelisms such as these are frequent in naturally oc-
curring conversations, leading us to ask whether it
might be possible to translate a stimulus into an ap-
propriate response. We apply SMT to this problem,
treating Twitter as our parallel corpus, with status
posts as our source language and their responses as
our target language. However, the established SMT
pipeline cannot simply be applied out of the box.
We identify two key challenges in adapting SMT
to the response generation task. First, unlike bilin-
gual text, stimulus-response pairs are not semanti-
cally equivalent, leading to a wider range of possible
responses for a given stimulus phrase. Furthermore,
both sides of our parallel text are written in the same
language. Thus, the most strongly associated word
or phrase pairs found by off-the-shelf word align-
ment and phrase extraction tools are identical pairs.
We address this issue with constraints and features to
limit lexical overlap. Secondly, in stimulus-response
pairs, there are far more unaligned words than in
bilingual pairs; it is often the case that large portions
of the stimulus are not referenced in the response
and vice versa. Also, there are more large phrase-
pairs that cannot be easily decomposed (for example
see figure 2). These difficult cases confuse the IBM
word alignment models. Instead of relying on these
alignments to extract phrase-pairs, we consider all
possible phrase-pairs in our parallel text, and apply
an association-based filter.
We compare our approach to response genera-
tion against two Information Retrieval or nearest
neighbour approaches, which use the input stimu-
lus to select a response directly from the training
data. We analyze the advantages and disadvantages
of each approach, and perform an evaluation using
human annotators from Amazon?s Mechanical Turk.
Along the way, we investigate the utility of SMT?s
BLEU evaluation metric when applied to this do-
main. We show that SMT-based solutions outper-
form IR-based solutions, and are chosen over actual
human responses in our data in 15% of cases. As far
as we are aware, this is the first work to investigate
the feasibility of SMT?s application to generating re-
sponses to open-domain linguistic stimuli.
2 Related Work
There has been a long history of ?chatterbots?
(Weizenbaum, 1966; Isbell et al, 2000; Shaikh et
al., 2010), which attempt to engage users, typically
leading the topic of conversation. They usually limit
interactions to a specific scenario (e.g. a Rogerian
psychotherapist), and use a set of template rules for
generating responses. In contrast, we focus on the
simpler task of generating an appropriate response
to a single utterance. We leverage large amounts of
conversational training data to scale to our Social
Media domain, where conversations can be on just
about any topic.
Additionally, there has been work on generat-
ing more natural utterances in goal-directed dia-
logue systems (Ratnaparkhi, 2000; Rambow et al,
2001). Currently, most dialogue systems rely on ei-
ther canned responses or templates for generation,
which can result in utterances which sound very
unnatural in context (Chambers and Allen, 2004).
Recent work has investigated the use of SMT in
translating internal dialogue state into natural lan-
guage (Langner et al, 2010). In addition to dialogue
state, we believe it may be beneficial to consider
the user?s utterance when generating responses in or-
der to generate locally coherent discourse (Barzilay
and Lapata, 2005). Data-driven generation based on
users? utterances might also be a useful way to fill in
knowledge gaps in the system (Galley et al, 2001;
Knight and Hatzivassiloglou, 1995).
Statistical machine translation has been applied to
a smo?rga?sbord of NLP problems, including question
answering (Echihabi and Marcu, 2003), semantic
parsing and generation (Wong and Mooney, 2006;
Wong and Mooney, 2007), summarization (Daume?
III and Marcu, 2009), generating bid-phrases in on-
line advertising (Ravi et al, 2010), spelling correc-
tion (Sun et al, 2010), paraphrase (Dolan et al,
2004; Quirk et al, 2004) and query expansion (Rie-
zler et al, 2007). Most relevant to our efforts is the
work by Soricut and Marcu (2006), who applied the
IBM word alignment models to a discourse order-
ing task, exploiting the same intuition investigated
584
in this paper: certain words (or phrases) tend to trig-
ger the usage of other words in subsequent discourse
units. As far as we are aware, ours is the first work
to explore the use of phrase-based translation in gen-
erating responses to open-domain linguistic stimuli,
although the analogy between translation and dia-
logue has been drawn (Leuski and Traum, 2010).
3 Data
For learning response-generation models, we use
a corpus of roughly 1.3 million conversations
scraped from Twitter (Ritter et al, 2010; Danescu-
Niculescu-Mizil et al, 2011). Twitter conversations
don?t occur in real-time as in IRC; rather as in email,
users typically take turns responding to each other.
Twitter?s 140 character limit, however, keeps con-
versations chat-like. In addition, the Twitter API
maintains a reference from each reply to the post
it responds to, so unlike IRC, there is no need for
conversation disentanglement (Elsner and Charniak,
2008; Wang and Oard, 2009). The first message of a
conversation is typically unique, not directed at any
particular user but instead broadcast to the author?s
followers (a status message). For the purposes of
this paper, we limit the data set to only the first two
utterances from each conversation. As a result of
this constraint, any system trained with this data will
be specialized for responding to Twitter status posts.
4 Response Generation as Translation
When applied to conversations, SMT models the
probability of a response r given the input status-
post s using a log-linear combination of feature
functions. Most prominent among these features
are the conditional phrase-translation probabilities
in both directions, P (s|r) and P (r|s), which ensure
r is an appropriate response to s, and the language
model P (r), which ensures r is a well-formed re-
sponse. As in translation, the response models are
estimated from counts of phrase pairs observed in
the training bitext, and the language model is built
using n-gram statistics from a large set of observed
responses. To find the best response to a given input
status-post, we employ the Moses phrase-based de-
coder (Koehn et al, 2007), which conducts a beam
search for the best response given the input, accord-
ing to the log-linear model.
what . . .  
time . . .  
u  . . . .
get .  . . .
out . .  . .
? . . . . .
i ge
t
off at 5
Figure 1: Example from the data where word alignment
is easy. There is a clear correspondence between words
in the status and the response.
4.1 Challenge: Lexical Repetition
When applied directly to conversation data, off-the-
shelf MT systems simply learn to parrot back the
input, sometimes with slight modification. For ex-
ample, directly applying Moses with default settings
to the conversation data produces a system which
yields the following (typical) output on the above
example:
Stimulus: I?m slowly making this soup
...... and it smells gorgeous!
Response: i?m slowly making this soup
...... and you smell gorgeous!
This ?paraphrasing? phenomenon occurs because
identical word pairs are frequently observed together
in the training data. Because there is a wide range
of acceptable responses to any status, these identical
pairs have the strongest associations in the data, and
therefore dominate the phrase table. In order to dis-
courage lexically similar translations, we filter out
all phrase-pairs where one phrase is a substring of
the other, and introduce a novel feature to penalize
lexical similarity:
?lex(s, t) = J(s, t)
Where J(s, t) is the Jaccard similarity between the
set of words in s and t.
4.2 Challenge: Word Alignment
Alignment is more difficult in conversational data
than bilingual data (Brown et al, 1990), or textual
entailment data (Brockett, 2006; MacCartney et al,
2008). In conversational data, there are some cases
in which there is a decomposable alignment between
585
if . . . .
anyones . . . .
still . . . .
awake . . . .
lets . . . .
play . . . .
a . . . .
game. . . . .
name    .
3    .
kevin    .
costner    .
movies    .
that    .
dont    .
suck    .
. . . . 
eas
ier
qu
est
ion
ple
ase
.
Figure 2: Example from the data where word alignment
is difficult (requires alignment between large phrases in
the status and response).
words, as seen in figure 1, and some difficult cases
where alignment between large phrases is required,
for example figure 2. These difficult sentence pairs
confuse the IBM word alignment models which have
no way to distinguish between the easy and hard
cases.
We aligned words in our parallel data using the
widely used tool GIZA++ (Och and Ney, 2003);
however, the standard growing heuristic resulted in
very noisy alignments. Precision could be improved
considerably by using the intersection of GIZA++
trained in two directions (s? r, and r ? s), but the
alignment also became extremely sparse. The aver-
age number of alignments-per status/response pair
in our data was only 1.7, as compared to a dataset
of aligned French-English sentence pairs (the WMT
08 news commentary data) where the average num-
ber of intersection alignments is 14.
Direct Phrase Pair Extraction
Because word alignment in status/response pairs is
a difficult problem, instead of relying on local align-
ments for extracting phrase pairs, we exploit infor-
mation from all occurrences of the pair in determin-
C(s, t) C(s,?t) C(s)
C(?s, t) C(?s,?t) N ? C(s)
C(t) N ? C(t) N
Figure 3: Contingency table for phrase pair (s,t). Fisher?s
Exact Test estimates the probability of seeing this event,
or one more extreme assuming s and t are independent.
ing whether its phrases form a valid mapping.
We consider all possible phrase-pairs in the train-
ing data,1 then use Fisher?s Exact Test to filter out
pairs with low correlation (Johnson et al, 2007).
Given a source and target phrase s and t, we consider
the contingency table illustrated in figure 3, which
includes co-occurrence counts for s and t, the num-
ber of sentence-pairs containing s, but not t and vice
versa, in addition to the number of pairs containing
neither s nor t. Fisher?s Exact Test provides us with
an estimate of the probability of observing this table,
or one more extreme, assuming s and t are indepen-
dent; in other words it gives us a measure of how
strongly associated they are. In contrast to statistical
tests such as ?2, or the G2 Log Likelihood Ratio,
Fisher?s Exact Test produces accurate p-values even
when the expected counts are small (as is extremely
common in our case).
In Fisher?s Exact Test, the hypergeometric proba-
bility distribution is used to compute the exact prob-
ability of a particular joint frequency assuming a
model of independence:
C(s)!C(?s)!C(t)!C(?t)!
N !C(s, t)!C(?s, t)!C(s,?t)!C(?s,?t)!
The statistic is computed by summing the prob-
ability for the joint frequency in Table 3, and ev-
ery more extreme joint frequency consistent with the
marginal frequencies. Moore (2004) illustrates sev-
eral tricks which make this computation feasible in
practice.
We found that this approach generates phrase-
table entries which appear quite reasonable upon
manual inspection. The top 20 phrase-pairs (after fil-
tering out identical source/target phrases, substrings,
1We define a possible phrase-pair as any pair of phrases
found in a sentence-pair from our training corpus, where both
phrases consist of 4 tokens or fewer. The total number of phrase
pairs in a sentence pair (s, r) is O(|s| ? |r|).
586
Source Target
rt [retweet] thanks for the
potter harry
ice cream
how are you you ?
good morning
chuck norris
watching movie
i miss miss you too
are you i ?m
my birthday happy birthday
wish me luck good luck
how was it was
miss you i miss
swine flu
i love you love you too
how are are you ?
did you i did
jackson michael
how are you i ?m good
michael mj
Table 1: Top 20 Phrase Pairs ranked by the Fisher Exact
Test statistic. Slight variations (substrings or symmetric
pairs) were removed to show more variety. See the sup-
plementary materials for the top 10k (unfiltered) pairs.
and symmetric pairs) are listed in Table 1.2 Our ex-
periments in ?6 show that using the phrase table pro-
duced by Fisher?s Exact Test outperforms one gen-
erated based on the poor quality IBM word align-
ments.
4.3 System Details
For the phrase-table used in the experiments (?6) we
used the 5M phrases with highest association ac-
cording the Fisher Exact Test statistic.3 To build
the language model, we used all of the 1.3M re-
sponses from the training data, along with roughly
1M replies collected using Twitter?s streaming API.
2See the supplementary materials for the top 10k (unfiltered)
phrase pairs.
3Note that this includes an arbitrary subset of the (1,1,1)
pairs (phrase pairs where both phrases were only observed once
in the data). Excluding these (1,1,1) pairs yields a rather small
phrase table, 201K phrase-pairs after filtering, while including
all of them led to a table which was too large for the memory of
the machine used to conduct the experiments.
We do not use any form of SMT reordering
model, as the position of the phrase in the response
does not seem to be very correlated with the corre-
sponding position in the status. Instead we let the
language model drive reordering.
We used the default feature weights provided by
Moses.4 Because automatic evaluation of response
generation is an open problem, we avoided the use of
discriminative training algorithms such as Minimum
Error-Rate Training (Och, 2003).
5 Information Retrieval
One straightforward data-driven approach to re-
sponse generation is nearest neighbour, or informa-
tion retrieval. This general approach has been ap-
plied previously by several authors (Isbell et al,
2000; Swanson and Gordon, 2008; Jafarpour and
Burges, 2010), and is used as a point of compari-
son in our experiments. Given a novel status s and a
training corpus of status/response pairs, two retrieval
strategies can be used to return a best response r?:
IR-STATUS [rargmaxi sim(s,si)] Retrieve the re-sponse ri whose associated status message si
is most similar to the user?s input s.
IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the re-sponse ri which has highest similarity when di-
rectly compared to s.
At first glance, IR-STATUS may appear to be the
most promising option; intuitively, if an input status
is very similar to a training status, we might expect
the corresponding training response to pair well with
the input. However, as we describe in ?6, it turns
out that directly retrieving the most similar response
(IR-RESPONSE) tends to return acceptable replies
more reliably, as judged by human annotators. To
implement our two IR response generators, we rely
on the default similarity measure implemented in the
Lucene5 Information Retrieval Library, which is an
IDF-weighted Vector-Space similarity.
6 Experiments
In order to compare various approaches to auto-
mated response generation, we used human evalu-
4The language model weight was set to 0.5, the translation
model weights in both directions were both set to 0.2, the lexical
similarity weight was set to -0.2.
5http://lucene.apache.org/
587
ators from Amazon?s Mechanical Turk (Snow et al,
2008). Human evaluation also provides us with data
for a preliminary investigation into the feasibility
of automatic evaluation metrics. While automated
evaluation has been investigated in the area of spo-
ken dialogue systems (Jung et al, 2009), it is unclear
how well it will correlate with human judgment in
open-domain conversations where the range of pos-
sible responses is very large.
6.1 Experimental Conditions
We performed pairwise comparisons of several
response-generation systems. Similar work on eval-
uating MT output (Bloodgood and Callison-Burch,
2010) has asked Turkers to rank more than two
choices, but in order to keep our evaluation as
straightforward as possible, we limited our experi-
ments to pairwise comparisons.
For each experiment comparing 2 systems (a and
b), we built a test set by selecting a random sam-
ple of 200 tweets which had received responses,
and which had a length between 4 and 20 words.
These tweets were selected from conversations col-
lected from a later, non-overlapping time-period
from those used in training. Each experiment used
a different random sample of 200 tweets. For each
of the 200 statuses, we generated a response using
method a and b, then showed the status and both re-
sponses to the Turkers, asking them to choose the
best response. The order of the systems used to
generate a response was randomized, and each of
the 200 HITs was submitted to 3 different Turkers.
Turkers were paid 1? per judgment.
The Turkers were instructed that an appropriate
response should be on the same topic as the sta-
tus, and should also ?make sense? in response to it.
While this is an inherently subjective task, from in-
specting the results, we found Turkers to be quite
competent in judging between two responses.
The systems used in these pairwise comparisons
are summarized in table 2, and example output gen-
erated by each system is presented in Table 3.
6.2 Results
The results of the experiments are summarized in
Table 4. For each experiment we show the fraction
of HITs where the majority of annotators agreed sys-
tem a was better. We also show the p-value from an
System Name Description
RND-BASELINE Picks randomly from the set of
responses which are observed at
least twice in the training data.
The assumption is these are
likely very general responses
IR-STATUS rargmaxi sim(s,si) as describedin ?5
IR-RESPONSE rargmaxi sim(s,ri) as describedin ?5
MT-CHAT Phrase-based translation system
as described in ?4
MT-BASELINE Exactly the same as MT-CHAT,
except using a phrase table ex-
tracted based on word align-
ments from GIZA++
HUMAN Actual responses from the test
data.
Table 2: Summary of systems compared experimentally
exact Binomial significance test; note that all dif-
ferences are significant with above 95% confidence.
Table 4 also shows the S coefficient (Bennett et al,
1954) as a measure of agreement between annotators
for each experiment. S is equivalent the commonly
used Kappa, except it assumes the uniform distribu-
tion when computing expected or chance agreement
(Artstein and Poesio, 2008). The uniform distribu-
tion is appropriate in our setup, since annotators are
not told which system generated each output, and the
order of choices is randomized. If annotators were
indeed choosing randomly, they would be expected
to agree in 50% of cases. In most comparisons the
value of S is in the range of 0.2 to 0.4, which can
be considered ?Fair agreement? (Landis and Koch,
1977).
Note that agreement between annotators is lower
than typically reported in corpus annotation tasks.
When annotating which of two automatically gen-
erated outputs is better, there is not always a clear
answer; both responses might be good or bad. We
can expect strong agreement only in cases where
one response is clearly better. Strong agreement is
not required, however, as we are using many anno-
tations to compare each pair of systems, and the hu-
man judgments are not intended to be used as train-
ing data. Similar agreement was reported in an eval-
uation of automatically generated MT output as part
588
Status MT-CHAT MT-BASELINE IR-STATUS HUMAN RND-BASELINE IR-RESPONSE
riding to the
arc on my bike.
Hopefully I won?t
melt into the
pavement.
good luck on your
bike ride arc
i ca n?t wait for it
! i ?m arc is a year
.
grab some re-
freshing drinks
... and a towel ...
lol ... then eat ice
cream and suika
later ...
make sure to ride
in the middle of
the street
lmfaoooo ! u sure the cup wo
n?t melt ?
I really hate
watching movies
in the living
room.
me too , what
movies are you
doing in room ?
me too ! i love
those ? ) in my
mouth .
:( cheer up ! you
need to invest in
one of those turbo
fans
Why? no comfy
spots or just too
open to public
tas fazendo oq ? someone was
watching in the
living room kasi e
. :) i was too lazy
to go up to my
room . :) )
Hello Did I miss
David Getting
his teen choice
award? Or did
they not show it :(
hello miss ? *
he was getting his
will probably win
.
it ?s just like what
you just did the
exact date hello ?
it did n?t show yet
-ha- -lol-
nothing yet...he
presented though
he is so darn cute
you wish . are they gon na
show teen choice
awards again ? i
missed it
I can think the im-
possible is possi-
ble at any given
moment.
i think you can
do it , anything
is given impossi-
ble dreams really
specs
i ca n?t wait to
see if there is no
point in the end
but if you do a
few hours ...
i hope you use
those powers for
evil . i mean good
! good . that ?s
what i meant .
for sure Eric, and
that makes it not
?impossible? any
longer.
no -user- ? nobody with
whom i might
apply the blue
cobalt ? rabbit ?
at the moment .
given the econ-
omy , perhaps
-url- ( any of em )
I want to enter my
parents house into
a home makeover
contest. Any help
out there?
me too ! thanks
for the win , par-
ents are so smart
, just got home
makeover
oh man , i do
n?t want to come
back to my mom
is getting that
i put troy on the
potty everyday at
lunch and b4 u
know it he was
going on his own
... only took a
week .
check TLC I?m
pretty sure it was
them who were
recently posting
about looking
for houses to be
nominated!
good job you want to do
laundry with me
at my parents
house after i get
off ? maybe get a
free meal out of it
? !
Table 3: Example responses from each system. We tried to pick examples where most (or all) systems generate
reasonable responses for illustration purposes.
System A System B Fraction A p-value Agreement
(S)
System A
BLEU
System B
BLEU
MT-CHAT? IR-STATUS 0.645 5.0e-05 0.347 1.15 0.57
MT-CHAT? IR-RESPONSE 0.593 1.0e-02 0.333 0.84 1.53
IR-STATUS IR-RESPONSE? 0.422 3.3e-02 0.330 0.40 1.59
MT-CHAT? MT-BASELINE 0.577 3.8e-02 0.160 1.23 1.14
MT-CHAT HUMAN? 0.145 2.2e-16 0.433 N/A N/A
MT-CHAT? RND-BASELINE 0.880 2.2e-16 0.383 1.17 0.10
Table 4: Results of pairwise comparisons between various response-generation methods. Each row presents a com-
parison between systems a and b on 200 randomly selected tweets. The column Fraction A lists the fraction of HITs
where the majority of annotators agreed System A?s response was better. The winning system is indicated with an
asterisk?. All differences are significant.
589
of the WMT09 shared tasks (Callison-Burch et al,
2009).6
The results of the paired evaluations provide a
clear ordering on the automatic systems: IR-STATUS
is outperformed by IR-RESPONSE, which is in turn
outperformed by MT-CHAT. These results are
somewhat surprising. We had expected that match-
ing status to status would create a more natural and
effective IR system, but in practice, it appears that
the additional level of indirection employed by IR-
STATUS created only more opportunity for confu-
sion and error. Also, we did not necessarily expect
MT-CHAT?s output to be preferred by human anno-
tators: the SMT system is the only one that generates
a completely novel response, and is therefore the
system most likely to make fluency errors. We had
expected human annotators to pick up on these flu-
ency errors, giving the the advantage to the IR sys-
tems. However, it appears that MT-CHAT?s ability
to tailor its response to the status on a fine-grained
scale overcame the disadvantage of occasionally in-
troducing fluency errors.7
Given MT-CHAT?s success over the IR systems,
we conducted further experiments to validate its out-
put. In order to test how close MT-CHAT?s responses
come to human-level abilities, we compared its out-
put to actual human responses from our dataset. In
some cases the human responses change the topic of
conversation, and completely ignore the initial sta-
tus. For instance, one frequent type of response we
noticed in the data was a greeting: ?How have you
been? I haven?t talked to you in a while.? For the
purposes of this evaluation, we manually filtered out
cases where the human response was completely off-
topic from the status, selecting 200 pairs at random
that met our criteria and using the actual responses
as the HUMAN output.
When compared to the actual human-generated
response, MT-CHAT loses. However, its output is
preferred over the human responses 15% of the time,
a fact that is particularly surprising given the very
small ? by MT standards ? amount of data used to
train the model. A few examples where MT-CHAT?s
output were selected over the human response are
6See inter annotator agreement in table 4.
7Also, as one can see from the example exchanges in Ta-
ble 3, fluency errors are rampant across all systems, including
the gold-standard human responses.
listed in Table 5.
We also evaluated the effect of filtering all possi-
ble phrase pairs using Fisher?s Exact Test, which we
did instead of conducting phrase extraction accord-
ing to the very noisy word alignments. We altered
our MT-CHAT system to use the standard Moses
phrase-extraction pipeline, creating the system de-
noted as MT-BASELINE. We compared this to the
complete MT-CHAT system. Note that both systems
account for lexical repetition as described in ?4.1.
MT-CHAT?s output is preferred 58% of the time over
MT-BASELINE, indicating that direct phrase extrac-
tion is useful in this conversational setting.
Finally, as an additional baseline, we compared
MT-CHAT?s output to random responses selected
from those observed 2 or more times in the train-
ing data. One might argue that short, common re-
sponses are very general, and that a reply like ?lol?
could be considered a good response to almost any
status. However, the human evaluation shows a clear
preference for MT-CHAT?s output: raters favour re-
sponses that are tailored to the stimulus.
6.3 Automatic Evaluation
The field of SMT has benefited greatly from
the existence of an automatic evaluation metric,
BLEU (Papineni et al, 2002), which grades an out-
put candidate according to n-gram matches to one or
more reference outputs. To evaluate whether BLEU
is an appropriate automatic evaluation measure for
response generation, we attempted to measure its
agreement with the human judgments. We calcu-
late BLEU using a single reference derived from our
parallel corpus. We show the smoothed BLEU 1-4
scores for each system on each dataset evaluated in
Table 4. Although these scores are extremely low,
the overall BLEU scores agree with overall annota-
tor judgments in all cases except when comparing
MT-CHAT and IR-RESPONSE. It would seem that
BLEU has some agreement with human judgments
on this task, but perhaps not enough to be immedi-
ately useful.
7 Conclusions
We have presented several data-driven approaches,
based on information retrial and phrase-based SMT,
to the task of automatically responding to open-
590
Status MT-CHAT HUMAN
i want to go to a bonfire right about
now.
that does sound fun bonfire i wan na
go !
did my tweet bring that upon you ?
hehe
I?m the invisible man. man , you could never be invisible i can still see your face surrounded
in blue !
put ya number in my phone ..and
ima call you up !
call me when u do that in your phone
call :) for what ?s up ?
o well hw kum u dnt eva call or txt
me :) ? ! ? ! lolz jpn hw r ya tho :)
? ! ? !
Gray Maynard with hair? I did not
approve this.
maynard with grey hair ! i take it back, the hair isn?t working
for maynard.
Table 5: Examples where MT-CHAT output was preferred over HUMAN response by Turker annotators
domain linguistic stimuli.
Our experiments show that SMT techniques are
better-suited than IR approaches on the task of re-
sponse generation. Our system, MT-CHAT, pro-
duced responses which were preferred by human an-
notators over actual human responses 15% of the
time. Although this is still far from human-level
performance, we believe there is much room for
improvement: from designing appropriate word-
alignment and decoding algorithms that account for
the selective nature of response in dialogue, to sim-
ply adding more training data.
We described the many challenges posed by
adapting phrase-based SMT to dialogue, and pre-
sented initial solutions to several, including direct
phrasal alignment, and phrase-table scores discour-
aging responses that are lexically similar to the sta-
tus. Finally, we have provided results from an initial
experiment to evaluate the BLEU metric when ap-
plied to response generation, showing that though
the metric as is does not work well, there is suffi-
cient correlation to suggest that a similar, dialogue-
focused approach may be feasible.
By generating responses to Tweets out of context,
we have demonstrated that the models underlying
phrase-based SMT are capable of guiding the con-
struction of appropriate responses. In the future, we
are excited about the role these models could po-
tentially play in guiding response construction for
conversationally-aware chat input schemes, as well
as goal-directed dialogue systems.
Acknowledgments
We would like to thank Oren Etzioni, Michael
Gamon, Jerry Hobbs, Dirk Hovy, Yun-Cheng Ju,
Kristina Toutanova, Saif Mohammad, Patrick Pan-
tel, and Luke Zettlemoyer, in addition to the anony-
mous reviewers for helpful discussions and com-
ments on a previous draft. The first author is sup-
pored by a National Defense Science and Engineer-
ing Graduate (NDSEG) Fellowship 32 CFR 168a.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput. Lin-
guist., 34:555?596, December.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05.
E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited-response question-
ing. Public Opinion Quarterly, 18(3):303?308.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, CSLDAMT
?10, pages 208?211, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Chris Brockett. 2006. Aligning the rte 2006 corpus. In
Microsoft Research Techincal report MSR-TR-2007-
77.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Comput.
Linguist., 16:79?85, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, StatMT ?09.
591
Nathanael Chambers and James Allen. 2004. Stochas-
tic language generation in a dialogue system: Toward
a domain independent generator. In Michael Strube
and Candy Sidner, editors, Proceedings of the 5th SIG-
dial Workshop on Discourse and Dialogue, pages 9?
18, Cambridge, Massachusetts, USA, April 30 - May
1. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! Linguistic
style accommodation in social media. In Proceedings
of WWW.
Hal Daume? III and Daniel Marcu. 2009. Induction of
word and phrase alignments for automatic document
summarization. CoRR, abs/0907.0804.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, COLING ?04, Morristown, NJ,
USA. Association for Computational Linguistics.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, ACL
?03, pages 16?23, Morristown, NJ, USA. Association
for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? a corpus and algorithm for conversation disen-
tanglement. In Proceedings of ACL-08: HLT, June.
Michel Galley, Eric Fosler-Lussier, and Alexandros
Potamianos. 2001. Hybrid natural language gener-
ation for spoken dialogue systems. In Proceedings
of the 7th European Conference on Speech Commu-
nication and Technology (EUROSPEECH?01), pages
1735?1738, Aalborg, Denmark, September.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. Hms: a predictive text entry
method using bigrams. In Proceedings of the 2003
EACL Workshop on Language Modeling for Text Entry
Methods, TextEntry ?03.
Jerry R. Hobbs. 1985. On the coherence and structure of
discourse.
Charles Lee Isbell, Jr., Michael J. Kearns, Dave Ko-
rmann, Satinder P. Singh, and Peter Stone. 2000.
Cobot in lambdamoo: A social statistics agent. In Pro-
ceedings of the Seventeenth National Conference on
Artificial Intelligence and Twelfth Conference on In-
novative Applications of Artificial Intelligence, pages
36?41. AAAI Press.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Comput. Speech Lang.,
23:479?509, October.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Computa-
tional Linguistics, ACL ?95.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL. The
Association for Computer Linguistics.
J R Landis and G G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics.
Brian Langner, Stephan Vogel, and Alan W. Black. 2010.
Evaluating a dialog language generation system: com-
paring the mountain system to other nlg approaches.
In INTERSPEECH.
Anton Leuski and David R. Traum. 2010. Practical
language processing for virtual humans. In Twenty-
Second Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI-10).
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 802?811, Morristown,
NJ, USA. Association for Computational Linguistics.
Robert C. Moore. 2004. On log-likelihood-ratios and the
significance of rare events. In EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
592
Owen Rambow, Srinivas Bangalore, and Marilyn Walker.
2001. Natural language generation in dialog systems.
In Proceedings of the first international conference on
Human language technology research, HLT ?01, pages
1?4, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference.
Sujith Ravi, Andrei Broder, Evgeniy Gabrilovich, Vanja
Josifovski, Sandeep Pandey, and Bo Pang. 2010. Au-
tomatic generation of bid phrases for online advertis-
ing. In Proceedings of the third ACM international
conference on Web search and data mining, WSDM
?10.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
464?471, Prague, Czech Republic, June. Association
for Computational Linguistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180,
Morristown, NJ, USA. Association for Computational
Linguistics.
Samira Shaikh, Tomek Strzalkowski, Sarah Taylor, and
Nick Webb. 2010. Vca: an experiment with a mul-
tiparty virtual chat agent. In Proceedings of the 2010
Workshop on Companionable Dialogue Systems.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, COLING-ACL ?06.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 266?274, Morristown,
NJ, USA. Association for Computational Linguistics.
Reid Swanson and Andrew S. Gordon. 2008. Say any-
thing: A massively collaborative open domain story
writing companion. In Proceedings of the 1st Joint
International Conference on Interactive Digital Story-
telling: Interactive Storytelling, ICIDS ?08, pages 32?
40, Berlin, Heidelberg. Springer-Verlag.
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In HLT-NAACL.
Joseph Weizenbaum. 1966. Eliza: a computer program
for the study of natural language communication be-
tween man and machine. Commun. ACM, 9:36?45,
January.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
Yuk Wah Wong and Raymond Mooney. 2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Main Conference.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
593
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 306?314,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
CLex: A Lexicon for Exploring Color, Concept and Emotion
Associations in Language
Svitlana Volkova
Johns Hopkins University
3400 North Charles
Baltimore, MD 21218, USA
svitlana@jhu.edu
William B. Dolan
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
billdol@microsoft.com
Theresa Wilson
HLTCOE
810 Wyman Park Drive
Baltimore, MD 21211, USA
taw@jhu.edu
Abstract
Existing concept-color-emotion lexicons
limit themselves to small sets of basic emo-
tions and colors, which cannot capture the
rich pallet of color terms that humans use
in communication. In this paper we begin
to address this problem by building a novel,
color-emotion-concept association lexicon
via crowdsourcing. This lexicon, which we
call CLEX, has over 2,300 color terms, over
3,000 affect terms and almost 2,000 con-
cepts. We investigate the relation between
color and concept, and color and emotion,
reinforcing results from previous studies, as
well as discovering new associations. We
also investigate cross-cultural differences in
color-emotion associations between US and
India-based annotators.
1 Introduction
People typically use color terms to describe the
visual characteristics of objects, and certain col-
ors often have strong associations with particu-
lar objects, e.g., blue - sky, white - snow. How-
ever, people also take advantage of color terms to
strengthen their messages and convey emotions in
natural interactions (Jacobson and Bender, 1996;
Hardin and Maffi, 1997). Colors are both indica-
tive of and have an effect on our feelings and emo-
tions. Some colors are associated with positive
emotions, e.g., joy, trust and admiration and some
with negative emotions, e.g., aggressiveness, fear,
boredom and sadness (Ortony et al 1988).
Given the importance of color and visual de-
scriptions in conveying emotion, obtaining a
deeper understanding of the associations between
colors, concepts and emotions may be helpful for
many tasks in language understanding and gener-
ation. A detailed set of color-concept-emotion as-
sociations (e.g., brown - darkness - boredom; red -
blood - anger) could be quite useful for sentiment
analysis, for example, in helping to understand
what emotion a newspaper article, a fairy tale, or
a tweet is trying to evoke (Alm et al 2005; Mo-
hammad, 2011b; Kouloumpis et al 2011). Color-
concept-emotion associations may also be useful
for textual entailment, and for machine translation
as a source of paraphrasing.
Color-concept-emotion associations also have
the potential to enhance human-computer inter-
actions in many real- and virtual-world domains,
e.g., online shopping, and avatar construction in
gaming environments. Such knowledge may al-
low for clearer and hopefully more natural de-
scriptions by users, for example searching for
a sky-blue shirt rather than blue or light blue
shirt. Our long term goal is to use color-emotion-
concept associations to enrich dialog systems
with information that will help them generate
more appropriate responses to users? different
emotional states.
This work introduces a new lexicon of color-
concept-emotion associations, created through
crowdsourcing. We call this lexicon CLEX1. It
is comparable in size to only two known lexi-
cons: WORDNET-AFFECT (Strapparava and Val-
itutti, 2004) and EMOLEX (Mohammad and Tur-
ney, 2010). In contrast to the development of
these lexicons, we do not restrict our annotators
to a particular set of emotions. This allows us to
1Available for download at:
http://research.microsoft.com/en-us/
downloads/
Questions about the data and the access process may be
sent to svitlana@jhu.edu
306
collect more linguistically rich color-concept an-
notations associated with mood, cognitive state,
behavior and attitude. We also do not have any
restrictions on color naming, which helps us to
discover a rich lexicon of color terms and collo-
cations that represent various hues, darkness, sat-
uration and other natural language collocations.
We also perform a comprehensive analysis of
the data by investigating several questions includ-
ing: What affect terms are evoked by a certain
color, e.g., positive vs. negative? What con-
cepts are frequently associated with a particular
color? What is the distribution of part-of-speech
tags over concepts and affect terms in the data col-
lected without any preselected set of affect terms
and concepts? What affect terms are strongly as-
sociated with a certain concept or a category of
concepts and is there any correlation with a se-
mantic orientation of a concept?
Finally, we share our experience collecting the
data using crowdsourcing, describe advantages
and disadvantages as well as the strategies we
used to ensure high quality annotations.
2 Related Work
Interestingly, some color-concept associations
vary by culture and are influenced by the tra-
ditions and beliefs of a society. As shown in
(Sable and Akcay, 2010) green represents danger
in Malaysia, envy in Belgium, love and happiness
in Japan; red is associated with luck in China and
Denmark, but with bad luck in Nigeria and Ger-
many and reflects ambition and desire in India.
Some expressions involving colors share the
same meaning across many languages. For in-
stance, white heat or red heat (the state of high
physical and mental tension), blue-blood (an aris-
tocrat, royalty), white-collar or blue collar (of-
fice clerks). However, there are some expres-
sions where color associations differ across lan-
guages, e.g., British or Italian black eye becomes
blue in Germany, purple in Spain and black-butter
in France; your French, Italian and English neigh-
bors are green with envy while Germans are yel-
low with envy (Bortoli and Maroto, 2001).
There has been little academic work on con-
structing color-concept and color-emotion lexi-
cons. The work most closely related to ours
collects concept-color (Mohammad, 2011c) and
concept-emotion (EMOLEX) associations, both
relying on crowdsourcing. His project involved
collecting color and emotion annotations for
10,170 word-sense pairs from Macquarie The-
saurus2. They analyzed their annotations, looking
for associations with the 11 basic color terms from
Berlin and Key (1988). The set of emotion labels
used in their annotations was restricted to the set
of 8 basic emotions proposed by Plutchik (1980).
Their annotators were restricted to the US, and
produced 4.45 annotations per word-sense pair on
average.
There is also a commercial project from Cym-
bolism3 to collect concept-color associations. It
has 561,261 annotations for a restricted set of 256
concepts, mainly nouns, adjectives and adverbs.
Other work on collecting emotional aspect
of concepts includes WordNet-Affect (WNA)
(Strapparava and Valitutti, 2004), the General En-
quirer (GI) (Stone et al 1966), Affective Forms
of English Words (Bradley and Lang, 1999) and
Elliott?s Affective Reasoner (Elliott, 1992).
The WNA lexicon is a set of affect terms from
WordNet (Miller, 1995). It contains emotions,
cognitive states, personality traits, behavior, at-
titude and feelings, e.g., joy, doubt, competitive,
cry, indifference, pain. Total of 289 affect terms
were manually extracted, but later the lexicon was
extended using WordNet semantic relationships.
WNA covers 1903 affect terms - 539 nouns, 517
adjectives, 238 verbs and 15 adverbs.
The General Enquirer covers 11,788 concepts
labeled with 182 category labels including cer-
tain affect categories (e.g., pleasure, arousal, feel-
ing, pain) in addition to positive/negative seman-
tic orientation for concepts4.
Affective Forms of English Words is a work
which describes a manually collected set of nor-
mative emotional ratings for 1K English words
that are rated in terms of emotional arousal (rang-
ing from calm to excited), affective valence (rang-
ing from pleasant to unpleasant) and dominance
(ranging from in control to dominated).
Elliott?s Affective Reasoner is a collection of
programs that is able to reason about human emo-
tions. The system covers a set of 26 emotion cat-
egories from Ortony et al1988).
Kaya (2004) and Strapparava and Ozbal (2010)
both have worked on inferring emotions associ-
ated with colors using semantic similarity. Their
2http://www.macquarieonline.com.au
3http://www.cymbolism.com/
4http://www.wjh.harvard.edu/?inquirer/
307
research found that Americans perceive red as ex-
citement, yellow as cheer, purple as dignity and
associate blue with comfort and security. Other
research includes that geared toward discovering
culture-specific color-concept associations (Gage,
1993) and color preference, for example, in chil-
dren vs. adults (Ou et al 2011).
3 Data Collection
In order to collect color-concept and color-
emotion associations, we use Amazon Mechani-
cal Turk5. It is a fast and relatively inexpensive
way to get a large amount of data from many cul-
tures all over the world.
3.1 MTurk and Data Quality
Amazon Mechanical Turk is a crowdsourcing
platform that has been extensively used for ob-
taining low-cost human annotations for various
linguistic tasks over the last few years (Callison-
Burch, 2009). The quality of the data obtained
from non-expert annotators, also referred to as
workers or turkers, was investigated by Snow et
al (2008). Their empirical results show that the
quality of non-expert annotations is comparable
to the quality of expert annotations on a variety of
natural language tasks, but the cost of the annota-
tion is much lower.
There are various quality control strategies that
can be used to ensure annotation quality. For in-
stance, one can restrict a ?crowd? by creating a
pilot task that allows only workers who passed
the task to proceed with annotations (Chen and
Dolan, 2011). In addition, new quality control
mechanisms have been recently introduced e.g.,
Masters. They are groups of workers who are
trusted for their consistent high quality annota-
tions, but to employ them costs more.
Our task required direct natural language in-
put from workers and did not include any mul-
tiple choice questions (which tend to attract more
cheating). Thus, we limited our quality control ef-
forts to (1) checking for empty input fields and (2)
blocking copy/paste functionality on a form. We
did not ask workers to complete any qualification
tasks because it is impossible to have gold stan-
dard answers for color-emotion and color-concept
associations. In addition, we limited our crowd to
5http://www.mturk.com
a set of trusted workers who had been consistently
working on similar tasks for us.
3.2 Task Design
Our task was designed to collect a linguistically
rich set of color terms, emotions, and concepts
that were associated with a large set of colors,
specifically the 152 RGB values corresponding to
facial features of cartoon human avatars. In to-
tal we had 36 colors for hair/eyebrows, 18 for
eyes, 27 for lips, 26 for eye shadows, 27 for fa-
cial mask and 18 for skin. These data is necessary
to achieve our long-term goal which is to model
natural human-computer interactions in a virtual
world domain such as the avatar editor.
We designed two MTurk tasks. For Task 1, we
showed a swatch for one RGB value and asked
50 workers to name the color, describe emotions
this color evokes and define a set of concepts as-
sociated with that color. For Task 2, we showed a
particular facial feature and a swatch in a particu-
lar color, and asked 50 workers to name the color
and describe the concepts and emotions associ-
ated with that color. Figure 1 shows what would
be presented to worker for Task 2.
Q1. How would you name this color?
Q2. What emotion does this color evoke?
Q3. What concepts do you associate with it?
Figure 1: Example of MTurk Task 2. Task 1 is the
same except that only a swatch is given.
The design that we suggested has a minor lim-
itation in that a color swatch may display differ-
ently on different monitors. However, we hope to
overcome this issue by collecting 50 annotations
per RGB value. The example color
e
? emotion
c
?
concept associations produced by different anno-
tators ai are shown below:
? [R=222, G=207, B=186] (a1) light golden
yellow
e
? purity, happiness
c
? butter cookie,
vanilla; (a2) gold
e
? cheerful, happy
c
? sun,
corn; (a3) golden
e
? sexy
c
? beach, jewelery.
? [R=218, G=97, B=212] (a4) pinkish pur-
ple
e
? peace, tranquility, stressless
c
? justin
308
bieber?s headphones, someday perfume; (a5)
pink
e
? happiness
c
? rose, bougainvillea.
In addition, we collected data about workers?
gender, age, native language, number of years of
experience with English, and color preferences.
This data is useful for investigating variance in an-
notations for color-emotion-concept associations
among workers from different cultural and lin-
guistic backgrounds.
4 Data Analysis
We collected 15,200 annotations evenly divided
between the two tasks over 12 days. In total, 915
workers (41% male, 51% female and 8% who did
not specify), mainly from India and United States,
completed our tasks as shown in Table 1. 18%
workers produced 20 or more annotations. They
spent 78 seconds on average per annotation with
an average salary rate $2.3 per hour ($0.05 per
completed task).
Country Annotations
India 7844
United States 5824
Canada 187
United Kingdom 172
Colombia 100
Table 1: Demographic information about annota-
tors: top 5 countries represented in our dataset.
In total, we collected 2,315 unique color terms,
3,397 unique affect terms, and 1,957 unique con-
cepts for the given 152 RGB values. In the
sections below we discuss our findings on color
naming, color-emotion and color-concept associ-
ations. We also give a comparison of annotated
affect terms and concepts from CLEX and other
existing lexicons.
4.1 Color Terms
Berlin and Kay (1988) state that as languages
evolve they acquire new color terms in a strict
chronological order. When a language has only
two colors they are white (light, warm) and black
(dark, cold). English is considered to have 11 ba-
sic colors: white, black, red, green, yellow, blue,
brown, pink, purple, orange and gray, which is
known as the B&K order.
In addition, colors can be distinguished along at
most three independent dimensions of hue (olive,
orange), darkness (dark, light, medium), satura-
tion (grayish, vivid), and brightness (deep, pale)
(Mojsilovic, 2002). Interestingly, we observe
these dimensions in CLEX by looking for B&K
color terms and their frequent collocations. We
present the top 10 color collocations for the B&K
colors in Table 2. As can be seen, color terms
truly are distinguished by darkness, saturation and
brightness terms e.g., light, dark, greenish, deep.
In addition, we find that color terms are also as-
sociated with color-specific collocations, e.g., sky
blue, chocolate brown, pea green, salmon pink,
carrot orange. These collocations were produced
by annotators to describe the color of particular
RGB values. We investigate these color-concept
associations in more details in Section 4.3.
In total, the CLEX has 2,315 unique color
Color Co-occurrences
?
white off, antique, half, dark, black, bone,
milky, pale, pure, silver
0.62
black light, blackish brown, brownish,
brown, jet, dark, green, off, ash,
blackish grey
0.43
red dark, light, dish brown, brick, or-
ange, brown, indian, dish, crimson,
bright
0.59
green dark, light, olive, yellow, lime, for-
est, sea, dark olive, pea, dirty
0.54
yellow light, dark, green, pale, golden,
brown, mustard, orange, deep,
bright
0.63
blue light, sky, dark, royal, navy, baby,
grey, purple, cornflower, violet
0.55
brown dark, light, chocolate, saddle, red-
dish, coffee, pale, deep, red,
medium
0.67
pink dark, light, hot, pale, salmon, baby,
deep, rose, coral, bright
0.55
purple light, dark, deep, blue, bright,
medium, pink, pinkish, bluish,
pretty
0.69
orange light, burnt, red, dark, yellow,
brown, brownish, pale, bright, car-
rot
0.68
gray dark, light, blue, brown, charcoal,
leaden, greenish, grayish blue, pale,
grayish brown
0.62
Table 2: Top 10 color term collocations for the
11 B&K colors; co-occurrences are sorted by fre-
quency from left to right in a decreasing order;
?10
1 p(? | color) is a total estimated probability
of the top 10 co-occurrences.
309
Agreement Color Term
% of overall Exact match 0.492
agreement Substring match 0.461
Free-marginal Exact match 0.458
Kappa Substring match 0.424
Table 3: Inter-annotator agreement on assigning
names to RGB values: 100 annotators, 152 RGB
values and 16 color categories including 11 B&K
colors, 4 additional colors and none of the above.
names for the set of 152 RGB values. The
inter-annotator agreement rate on color naming is
shown in Table 3. We report free-marginal Kappa
(Randolph, 2005) because we did not force an-
notators to assign certain number of RGB values
to a certain number of color terms. Additionally,
we report inter-annotator agreement for an exact
string match e.g., purple, green and a substring
match e.g., pale yellow = yellow = golden yellow.
4.2 Color-Emotion Associations
In total, the CLEX lexicon has 3,397 unique af-
fect terms representing feelings (calm, pleasure),
emotions (joy, love, anxiety), attitudes (indiffer-
ence, caution), and mood (anger, amusement).
The affect terms in CLEX include the 8 basic emo-
tions from (Plutchik, 1980): joy, sadness, anger,
fear, disgust, surprise, trust and anticipation6
CLEX is a very rich lexicon because we did not
restrict our annotators to any specific set of affect
terms. A wide range of parts-of-speech are rep-
resented, as shown in the first column in Table 4.
For instance, the term love is represented by other
semantically related terms such as: lovely, loved,
loveliness, loveless, love-able and the term joy is
represented as enjoy, enjoyable, enjoyment, joy-
ful, joyfulness, overjoyed.
POS Affect Terms, % Concepts, %
Nouns 79 52
Adjectives 12 29
Adverbs 3 5
Verbs 6 12
Table 4: Main syntactic categories for affect terms
and concepts in CLEX.
The manually constructed portion of
WORDNET-AFFECT includes 101 positive
and 188 negative affect terms (Strapparava and
6The set of 8 Plutchik?s emotions is a superset of emotions
from (Ekman, 1992).
Valitutti, 2004). Of this set, 41% appeared at
least once in CLEX. We also looked specifically
at the set of terms labeled as emotions in the
WORDNET-AFFECT hierarchy. Of these, 12 are
positive emotions and 10 are negative emotions.
We found that 9 out of 12 positive emotion
terms (except self-pride, levity and fearlessness)
and 9 out of 10 negative emotion terms (except in-
gratitude) also appear in CLEX as shown in Table
5. Thus, we can conclude that annotators do not
associate any colors with self-pride, levity, fear-
lessness and ingratitude. In addition, some emo-
tions were associated more frequently with colors
than others. For instance, positive emotions like
calmness, joy, love are more frequent in CLEX
than expectation and ingratitude; negative emo-
tions like sadness, fear are more frequent than
shame, humility and daze.
Positive Freq. Negative Freq.
calmness 1045 sadness 356
joy 527 fear 250
love 482 anxiety 55
hope 147 despair 19
affection 86 compassion 10
enthusiasm 33 dislike 8
liking 5 shame 5
expectation 3 humility 3
gratitude 3 daze 1
Table 5: WORDNET-AFFECT positive and neg-
ative emotion terms from CLEX. Emotions are
sorted by frequency in decreasing order from the
total 27,802 annotations.
Next, we analyze the color-emotion associ-
ations in CLEX in more detail and compare
them with the only other publicly-available color-
emotion lexicon, EMOLEX. Recall that EMOLEX
(Mohammad, 2011a) has 11 B&K colors associ-
ated with 8 basic positive and negative emotions
from (Plutchik, 1980). Affect terms in CLEX are
not labeled as conveying positive or negative emo-
tions. Instead, we use the overlapping 289 affect
terms between WORDNET-AFFECT and CLEX
and propagate labels from WORDNET-AFFECT to
the corresponding affect terms in CLEX. As a re-
sult we discover positive and negative affect term
associations with the 11 B&K colors. Table 6
shows the percentage of positive and negative af-
fect term associations with colors for both CLEX
and EMOLEX.
310
Positive Negative
CLEX EL CLEX EL
white 2.5 20.1 0.3 2.9
black 0.6 3.9 9.3 28.3
red 1.7 8.0 8.2 21.6
green 3.3 15.5 2.7 4.7
yellow 3.0 10.8 0.7 6.9
blue 5.9 12.0 1.6 4.1
brown 6.5 4.8 7.6 9.4
pink 5.6 7.8 1.1 1.2
purple 3.1 5.7 1.8 2.5
orange 1.6 5.4 1.7 3.8
gray 1.0 5.7 3.6 14.1
Table 6: The percentage of affect terms associated
with B&K colors in CLEX and EMOLEX (similar
color-emotion associations are shown in bold).
The percentage of color-emotion associations
in CLEX and EMOLEX differs because the set of
affect terms in CLEX consists of 289 positive and
negative affect terms compared to 8 affect terms
in EMOLEX. Nevertheless, we observe the same
pattern as (Mohammad, 2011a) for negative emo-
tions. They are associated with black, red and
gray colors, except yellow becomes a color of
positive emotions in CLEX. Moreover, we found
the associations with the color brown to be am-
biguous as it was associated with both positive
and negative emotions. In addition, we did not ob-
serve strong associations between white and pos-
itive emotions. This may be because white is the
color of grief in India. The rest of the positive
emotions follow the EMOLEX pattern and are as-
sociated with green, pink, blue and purple colors.
Next, we perform a detailed comparison be-
tween CLEX and EMOLEX color-emotion asso-
ciations for the 11 B&K colors and the 8 basic
emotions from (Plutchik, 1980) in Table 7. Recall
that annotations in EMOLEX are done by workers
from the USA only. Thus, we report two num-
bers for CLEX - annotations from workers from
the USA (CA) and all annotations (C). We take
EMOLEX results from (Mohammad, 2011c). We
observe a strong correlation between CLEX and
EMOLEX affect lexicons for some color-emotion
associations. For instance, anger has a strong as-
sociation with red and brown, anticipation with
green, fear with black, joy with pink, sadness
with black, brown and gray, surprise with yel-
low and orange, and finally, trust is associated
with blue and brown. Nonetheless, we also found
a disagreement in color-emotion associations be-
tween CLEX and EMOLEX. For instance antic-
ipation is associated with orange in CLEX com-
pared to white, red or yellow in EMOLEX. We also
found quite a few inconsistent associations with
the disgust emotion. This inconsistency may be
explained by several reasons: (a) EMOLEX asso-
ciates emotions with colors through concepts, but
CLEX has color-emotion associations obtained
directly from annotators; (b) CLEX has 3,397
affect terms compared to 8 basic emotions in
EMOLEX. Therefore, it may be introducing some
ambiguous color-emotion associations.
Finally, we investigate cross-cultural differ-
ences in color-emotion associations between the
two most representative groups of our annotators:
US-based and India-based. We consider the 8
Plutchik?s emotions and allow associations with
all possible color terms (rather than only 11 B&K
colors). We show top 5 colors associated with
emotions for two groups of annotators in Figure 2.
For example, we found that US-based annotators
associate pink with joy, dark brown with trust vs.
India-based annotators who associate yellow with
joy and blue with trust.
4.3 Color-Concept Associations
In total, workers annotated the 152 RGB values
with 37,693 concepts which is on average 2.47
concepts compared to 1.82 affect term per anno-
tation. CLEX contains 1,957 unique concepts in-
cluding 1,667 nouns, 23 verbs, 28 adjectives, and
12 adverbs. We investigate an overlap of con-
cepts by part-of-speech tag between CLEX and
other lexicons including EMOLEX (EL), Affec-
tive Norms of English Words (AN), General In-
quirer (GI). The results are shown in Table 8.
Finally, we generate concept clusters associ-
ated with yellow, white and brown colors in Fig-
ure 3. From the clusters, we observe the most
frequent k concepts associated with these colors
have a correlation with either positive or negative
emotion. For example, white is frequently associ-
ated with snow, milk, cloud and all of these con-
cepts evolve positive emotions. This observation
helps resolve the ambiguity in color-emotion as-
sociations we found in Table 7.
5 Conclusions
We have described a large-scale crowdsourcing
effort aimed at constructing a rich color-emotion-
311
white black red green yellow blue brown pink purple orange grey
anger
C - 3.6 43.4 0.3 0.3 0.3 3.3 0.6 0.3 1.5 2.1
CA - 3.8 40.6 0.8 - - 4.5 - 0.8 2.3 0.8
EA 2.1 30.7 32.4 5.0 5.0 2.4 6.6 0.5 2.3 2.5 9.9
sadness
C 0.3 24.0 0.3 0.6 0.3 4.2 11.4 0.3 2.2 0.3 10.3
CA - 22.2 - 0.6 - 5.3 9.4 - 4.1 - 12.3
EA 3.0 36.0 18.6 3.4 5.4 5.8 7.1 0.5 1.4 2.1 16.1
fear
C 0.8 43.0 8.9 2.0 1.2 0.4 6.1 0.4 0.8 0.4 2.0
CA - 29.5 10.5 3.2 1.1 - 3.2 - 1.1 1.1 4.2
EA 4.5 31.8 25.0 3.5 6.9 3.0 6.1 1.3 2.3 3.3 11.8
disgust
C - 2.3 1.1 11.2 1.1 1.1 24.7 1.1 3.4 1.1 -
CA - - - 14.8 1.8 - 33.3 - 1.8 - -
EA 2.0 33.7 24.9 4.8 5.5 1.9 9.7 1.1 1.8 3.5 10.5
joy
C 1.0 0.2 0.2 3.4 5.7 4.2 4.2 9.1 4.4 4.0 0.6
CA 0.9 - 0.3 3.3 4.5 4.8 2.7 10.6 4.2 3.9 0.6
EA 21.8 2.2 7.4 14.1 13.4 11.3 3.1 11.1 6.3 5.8 2.8
trust
C - - 1.2 3.5 1.2 17.4 8.1 1.2 1.2 5.8 1.2
CA - - 3.0 6.1 3.0 3.0 9.1 - - 3.0 3.0
EA 22.0 6.3 8.4 14.2 8.3 14.4 5.9 5.5 4.9 3.8 5.8
surprise
C - - - 3.3 6.7 6.7 3.3 3.3 6.7 13.3 3.3
CA - - - - 5.6 5.6 - 5.6 11.1 11.1 -
EA 11.0 13.4 21.0 8.3 13.5 5.2 3.4 5.2 4.1 5.6 8.8
anticipation
C - - - 5.3 5.3 - 5.3 5.3 - 15.8 5.3
CA - - - - - - - 10.0 - 10.0 10.0
EA 16.2 7.5 11.5 16.2 10.7 9.5 5.7 5.9 3.1 4.9 8.4
Table 7: The percentage of the 8 basic emotions associated with 11 B&K colors in CLEX vs. EMOLEX,
e.g., sadness is associated with black by 36% of annotators in EMOLEX(EA), 22.1% in CLEX(CA) by
US-based annotators only and 24% in CLEX(C) by all annotators; we report zero associations by ?-?.
(a) Joy - US: 331, I: 154 (b) Trust - US: 33, I: 47 (c) Surprise - US: 18, I: 12 (d) Anticipation - US: 10, I: 9
(e) Anger - US: 133, I: 160 (f) Sadness - US: 171, I: 142 (g) Fear - US: 95, I: 105 (h) Disgust - US: 54, I: 16
Figure 2: Apparent cross-cultural differences in color-emotion associations between US- and India-
based annotators. 10.6% of US workers associated joy with pink, while 7.1% India-based workers
associated joy with yellow (based on 331 joy associations from the US and from 154 India).
312
(a) Yellow (b) Brown (c) White
Figure 3: Concept clusters of color-concept associations for ambiguous colors: yellow, white, brown.
concept association lexicon, CLEX. This lexicon
links concepts, color terms and emotions to spe-
cific RGB values. This lexicon may help to dis-
ambiguate objects when modeling conversational
interactions in many domains. We have examined
the association between color terms and positive
or negative emotions.
Our work also investigated cross-cultural dif-
ferences in color-emotion associations between
India- and US-based annotators. We identified
frequent color-concept associations, which sug-
gests that concepts associated with a particular
color may express the same sentiment as the color.
Our future work includes applying statistical
inference for discovering a hidden structure of
concept-emotion associations. Moreover, auto-
matically identifying the strength of association
between a particular concept and emotions is an-
other task which is more difficult than just iden-
tifying the polarity of the word. We are also in-
terested in using a similar approach to investigate
CLEX?AN CLEX?EL CLEX?GI
Noun 287 Noun 574 Noun 708
Verb 4 Verb 13 Verb 17
Adj 28 Adj 53 Adj 66
Adv 1 Adv 2 Adv 3
320 642 794
AN\CLEX EL\CLEX GI\CLEX
712 7,445 11,101
CLEX\AN CLEX\EL CLEX\GI
1,637 1,315 1,163
Table 8: An overlap of concepts by part-of-
speech tag between CLEX and existing lexicons.
CLEX?GI stands for the intersection of sets,
CLEX\GI denotes the difference of sets.
the way that colors are associated with concepts
and emotions in languages other than English.
Acknowledgments
We are grateful to everyone in the NLP group
at Microsoft Research for helpful discussion and
feedback especially Chris Brocket, Piali Choud-
hury, and Hassan Sajjad. We thank Natalia Rud
from Tyumen State University, Center of Linguis-
tic Education for helpful comments and sugges-
tions.
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine
learning for text-based emotion prediction. In
Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 579?586,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Brent Berlin and Paul Kay. 1988. Basic Color Terms:
their Universality and Evolution. Berkley: Univer-
sity of California Press.
M. Bortoli and J. Maroto. 2001. Translating colors in
web site localisation. In In The Proceedings of Eu-
ropean Languages and the Implementation of Com-
munication and Information Technologies (Elicit).
M. Bradley and P. Lang. 1999. Affective forms for
english words (anew): Instruction manual and af-
fective ranking.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s me-
chanical turk. In EMNLP ?09: Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 286?295, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
313
David L. Chen and William B. Dolan. 2011. Building
a persistent workforce on mechanical turk for mul-
tilingual data collection. In Proceedings of The 3rd
Human Computation Workshop (HCOMP 2011),
August.
Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3):169?200.
Clark Davidson Elliott. 1992. The affective reasoner:
a process model of emotions in a multi-agent sys-
tem. Ph.D. thesis, Evanston, IL, USA. UMI Order
No. GAX92-29901.
J. Gage. 1993. Color and culture: Practice and mean-
ing from antiquity to abstraction, univ. of calif.
C. Hardin and L. Maffi. 1997. Color Categories in
Thought and Language.
N. Jacobson and W. Bender. 1996. Color as a deter-
mined communication. IBM Syst. J., 35:526?538,
September.
N. Kaya. 2004. Relationship between color and emo-
tion: a study of college students. College Student
Journal.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. ICWSM.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?
41.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 26?
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011a. Colourful language: Mea-
suring word-colour associations. In Proceedings
of the 2nd Workshop on Cognitive Modeling and
Computational Linguistics, pages 97?106, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011b. From once upon a time
to happily ever after: Tracking emotions in novels
and fairy tales. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 105?114, Portland, OR, USA, June. Associa-
tion for Computational Linguistics.
Saif M. Mohammad. 2011c. Even the abstract have
colour: consensus in word-colour associations. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 368?373, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Aleksandra Mojsilovic. 2002. A method for color
naming and description of color composition in im-
ages. In Proc. IEEE Int. Conf. Image Processing,
pages 789?792.
Andrew Ortony, Gerald L. Clore, and Allan Collins.
1988. The Cognitive Structure of Emotions. Cam-
bridge University Press, July.
Li-Chen Ou, M. Ronnier Luo, Pei-Li Sun, Neng-
Chung Hu, and Hung-Shing Chen. 2011. Age ef-
fects on colour emotion, preference, and harmony.
Color Research and Application.
R. Plutchik, 1980. A general psychoevolutionary the-
ory of emotion, pages 3?33. Academic press, New
York.
Justus J. Randolph. 2005. Author note: Free-marginal
multirater kappa: An alternative to fleiss fixed-
marginal multirater kappa.
P. Sable and O. Akcay. 2010. Color: Cross cultural
marketing perspectives as to what governs our re-
sponse to it. In In The Proceedings of ASSBS, vol-
ume 17.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Carlo Strapparava and Gozde Ozbal. 2010. The color
of emotions in text. COLING, pages 28?32.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In In: Proceed-
ings of the 4th International Conference on Lan-
guage Resources and Evaluation (LREC 2004), Lis-
bon, pages 1083?1086.
314
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 190?200,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Collecting Highly Parallel Data for Paraphrase Evaluation
David L. Chen
Department of Computer Science
The University of Texas at Austin
Austin, TX 78712, USA
dlcc@cs.utexas.edu
William B. Dolan
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
billdol@microsoft.com
Abstract
A lack of standard datasets and evaluation
metrics has prevented the field of paraphras-
ing from making the kind of rapid progress
enjoyed by the machine translation commu-
nity over the last 15 years. We address both
problems by presenting a novel data collection
framework that produces highly parallel text
data relatively inexpensively and on a large
scale. The highly parallel nature of this data
allows us to use simple n-gram comparisons to
measure both the semantic adequacy and lex-
ical dissimilarity of paraphrase candidates. In
addition to being simple and efficient to com-
pute, experiments show that these metrics cor-
relate highly with human judgments.
1 Introduction
Machine paraphrasing has many applications for
natural language processing tasks, including ma-
chine translation (MT), MT evaluation, summary
evaluation, question answering, and natural lan-
guage generation. However, a lack of standard
datasets and automatic evaluation metrics has im-
peded progress in the field. Without these resources,
researchers have resorted to developing their own
small, ad hoc datasets (Barzilay and McKeown,
2001; Shinyama et al, 2002; Barzilay and Lee,
2003; Quirk et al, 2004; Dolan et al, 2004), and
have often relied on human judgments to evaluate
their results (Barzilay and McKeown, 2001; Ibrahim
et al, 2003; Bannard and Callison-Burch, 2005).
Consequently, it is difficult to compare different sys-
tems and assess the progress of the field as a whole.
Despite the similarities between paraphrasing and
translation, several major differences have prevented
researchers from simply following standards that
have been established for machine translation. Pro-
fessional translators produce large volumes of bilin-
gual data according to a more or less consistent spec-
ification, indirectly fueling work on machine trans-
lation algorithms. In contrast, there are no ?profes-
sional paraphrasers?, with the result that there are
no readily available large corpora and no consistent
standards for what constitutes a high-quality para-
phrase. In addition to the lack of standard datasets
for training and testing, there are also no standard
metrics like BLEU (Papineni et al, 2002) for eval-
uating paraphrase systems. Paraphrase evaluation
is inherently difficult because the range of potential
paraphrases for a given input is both large and unpre-
dictable; in addition to being meaning-preserving,
an ideal paraphrase must also diverge as sharply as
possible in form from the original while still sound-
ing natural and fluent.
Our work introduces two novel contributions
which combine to address the challenges posed by
paraphrase evaluation. First, we describe a frame-
work for easily and inexpensively crowdsourcing ar-
bitrarily large training and test sets of independent,
redundant linguistic descriptions of the same seman-
tic content. Second, we define a new evaluation
metric, PINC (Paraphrase In N-gram Changes), that
relies on simple BLEU-like n-gram comparisons to
measure the degree of novelty of automatically gen-
erated paraphrases. We believe that this metric,
along with the sentence-level paraphrases provided
by our data collection approach, will make it possi-
190
ble for researchers working on paraphrasing to com-
pare system performance and exploit the kind of
automated, rapid training-test cycle that has driven
work on Statistical Machine Translation.
In addition to describing a mechanism for collect-
ing large-scale sentence-level paraphrases, we are
also making available to the research community
85K parallel English sentences as part of the Mi-
crosoft Research Video Description Corpus 1.
The rest of the paper is organized as follows. We
first review relevant work in Section 2. Section 3
then describes our data collection framework and the
resulting data. Section 4 discusses automatic evalua-
tions of paraphrases and introduces the novel metric
PINC. Section 5 presents experimental results estab-
lishing a correlation between our automatic metric
and human judgments. Sections 6 and 7 discuss pos-
sible directions for future research and conclude.
2 Related Work
Since paraphrase data are not readily available, var-
ious methods have been used to extract parallel text
from other sources. One popular approach exploits
multiple translations of the same data (Barzilay and
McKeown, 2001; Pang et al, 2003). Examples of
this kind of data include the Multiple-Translation
Chinese (MTC) Corpus 2 which consists of Chinese
news stories translated into English by 11 transla-
tion agencies, and literary works with multiple trans-
lations into English (e.g. Flaubert?s Madame Bo-
vary.) Another method for collecting monolingual
paraphrase data involves aligning semantically par-
allel sentences from different news articles describ-
ing the same event (Shinyama et al, 2002; Barzilay
and Lee, 2003; Dolan et al, 2004).
While utilizing multiple translations of literary
work or multiple news stories of the same event can
yield significant numbers of parallel sentences, this
data tend to be noisy, and reliably identifying good
paraphrases among all possible sentence pairs re-
mains an open problem. On the other hand, multiple
translations on the sentence level such as the MTC
Corpus provide good, natural paraphrases, but rela-
1Available for download at http://research.
microsoft.com/en-us/downloads/
38cf15fd-b8df-477e-a4e4-a4680caa75af/
2Linguistic Data Consortium (LDC) Catalog Number
LDC2002T01, ISBN 1-58563-217-1.
tively little data of this type exists. Finally, some ap-
proaches avoid the need for monolingual paraphrase
data altogether by using a second language as the
pivot language (Bannard and Callison-Burch, 2005;
Callison-Burch, 2008; Kok and Brockett, 2010).
Phrases that are aligned to the same phrase in the
pivot language are treated as potential paraphrases.
One limitation of this approach is that only words
and phrases are identified, not whole sentences.
While most work on evaluating paraphrase sys-
tems has relied on human judges (Barzilay and
McKeown, 2001; Ibrahim et al, 2003; Bannard and
Callison-Burch, 2005) or indirect, task-based meth-
ods (Lin and Pantel, 2001; Callison-Burch et al,
2006), there have also been a few attempts at creat-
ing automatic metrics that can be more easily repli-
cated and used to compare different systems. Para-
Metric (Callison-Burch et al, 2008) compares the
paraphrases discovered by an automatic system with
ones annotated by humans, measuring precision and
recall. This approach requires additional human an-
notations to identify the paraphrases within paral-
lel texts (Cohn et al, 2008) and does not evalu-
ate the systems at the sentence level. The more
recently proposed metric PEM (Paraphrase Evalu-
ation Metric) (Liu et al, 2010) produces a single
score that captures the semantic adequacy, fluency,
and lexical dissimilarity of candidate paraphrases,
relying on bilingual data to learn semantic equiva-
lences without using n-gram similarity between can-
didate and reference sentences. In addition, the met-
ric was shown to correlate well with human judg-
ments. However, a significant drawback of this ap-
proach is that PEM requires substantial in-domain
bilingual data to train the semantic adequacy evalu-
ator, as well as sample human judgments to train the
overall metric.
We designed our data collection framework for
use on crowdsourcing platforms such as Amazon?s
Mechanical Turk. Crowdsourcing can allow inex-
pensive and rapid data collection for various NLP
tasks (Ambati and Vogel, 2010; Bloodgood and
Callison-Burch, 2010a; Bloodgood and Callison-
Burch, 2010b; Irvine and Klementiev, 2010), includ-
ing human evaluations of NLP systems (Callison-
Burch, 2009; Denkowski and Lavie, 2010; Zaidan
and Callison-Burch, 2009). Of particular relevance
are the paraphrasing work by Buzek et al (2010)
191
and Denkowski et al (2010). Buzek et al automati-
cally identified problem regions in a translation task
and had workers attempt to paraphrase them, while
Denkowski et al asked workers to assess the validity
of automatically extracted paraphrases. Our work is
distinct from these earlier efforts both in terms of
the task ? attempting to collect linguistic descrip-
tions using a visual stimulus ? and the dramatically
larger scale of the data collected.
3 Data Collection
Since our goal was to collect large numbers of para-
phrases quickly and inexpensively using a crowd,
our framework was designed to make the tasks short,
simple, easy, accessible and somewhat fun. For each
task, we asked the annotators to watch a very short
video clip (usually less than 10 seconds long) and
describe in one sentence the main action or event
that occurred in the video clip
We deployed the task on Amazon?s Mechanical
Turk, with video segments selected from YouTube.
A screenshot of our annotation task is shown in Fig-
ure 1. On average, annotators completed each task
within 80 seconds, including the time required to
watch the video. Experienced annotators were even
faster, completing the task in only 20 to 25 seconds.
One interesting aspect of this framework is that
each annotator approaches the task from a linguisti-
cally independent perspective, unbiased by the lexi-
cal or word order choices in a pre-existing descrip-
tion. The data thus has some similarities to parallel
news descriptions of the same event, while avoiding
much of the noise inherent in news. It is also simi-
lar in spirit to the ?Pear Stories? film used by Chafe
(1997). Crucially, our approach allows us to gather
arbitrarily many of these independent descriptions
for each video, capturing nearly-exhaustive cover-
age of how native speakers are likely to summarize
a small action. It might be possible to achieve sim-
ilar effects using images or panels of images as the
stimulus (von Ahn and Dabbish, 2004; Fei-Fei et al,
2007; Rashtchian et al, 2010), but we believed that
videos would be more engaging and less ambiguous
in their focus. In addition, videos have been shown
to be more effective in prompting descriptions of
motion and contact verbs, as well as verbs that are
generally not imageable (Ma and Cook, 2009).
Watch and describe a short segment of a video
You will be shown a segment of a video clip and asked to describe the main action/event in that segment in
ONE SENTENCE.
Things to note while completing this task:
The video will play only a selected segment by default. You can choose to watch the entire clip and/or
with sound although this is not necessary.
Please only describe the action/event that occurred in the selected segment and not any other parts of
the video.
Please focus on the main person/group shown in the segment
If you do not understand what is happening in the selected segment, please skip this HIT and move
onto the next one
Write your description in one sentence
Use complete, grammatically-correct sentences
You can write the descriptions in any language you are comfortable with
Examples of good descriptions:
A woman is slicing some tomatoes.
A band is performing on a stage outside.
A dog is catching a Frisbee.
The sun is rising over a mountain landscape.
Examples of bad descriptions (With the reasons why they are bad in parentheses):
Tomato slicing 
(Incomplete sentence)
This video is shot outside at night about a band performing on a stage
(Description about the video itself instead of the action/event in the video)
I like this video because it is very cute
(Not about the action/event in the video)
The sun is rising in the distance while a group of tourists standing near some railings are taking
pictures of the sunrise and a small boy is shivering in his jacket because it is really cold
(Too much detail instead of focusing only on the main action/event)
Segment starts: 25 | ends: 30 | length: 5 seconds
Play Segment ? Play Entire Video
Please describe the main event/action in the selected segment (ONE SENTENCE):
Note: If you have a hard time typing in your native language on an English keyboard, you may find
Google's transliteration service helpful.
http://www.google.com/transliterate
Language you are typing in (e.g. English, Spanish, French, Hindi, Urdu, Mandarin Chinese, etc):
Your one-sentence description:
Please provide any comments or suggestions you may have below, we appreciate your input!
Figure 1: A screenshot of our annotation task as it was
deployed on Mechanical Turk.
3.1 Quality Control
One of the main problems with collecting data using
a crowd is quality control. While the cost is very low
compared to traditional annotation methods, work-
ers recruited over the Internet are often unqualified
for the tasks or are incentivized to cheat in order to
maximize their rewards.
To encourage native and fluent contributions, we
asked annotators to write the descriptions in the lan-
guage of their choice. The result was a significant
amount of translation data, unique in its multilingual
parallelism. While included in our data release, we
leave aside a full discussion of this multilingual data
for future work.
192
To ensure the quality of the annotations being pro-
duced, we used a two-tiered payment system. The
idea was to reward workers who had shown the abil-
ity to write quality descriptions and the willingness
to work on our tasks consistently. While everyone
had access to the Tier-1 tasks, only workers who had
been manually qualified could work on the Tier-2
tasks. The tasks were identical in the two tiers but
each Tier-1 task only paid 1 cent while each Tier-2
task paid 5 cents, giving the workers a strong incen-
tive to earn the qualification.
The qualification process was done manually by
the authors. We periodically evaluated the workers
who had submitted the most Tier-1 tasks (usually on
the order of few hundred submissions) and granted
them access to the Tier-2 tasks if they had performed
well. We assessed their work mainly on the gram-
maticality and spelling accuracy of the submitted de-
scriptions. Since we had hundreds of submissions to
base our decisions on, it was fairly easy to identify
the cheaters and people with poor English skills 3.
Workers who were rejected during this process were
still allowed to work on the Tier-1 tasks.
While this approach requires significantly more
manual effort initially than other approaches such
as using a qualification test or automatic post-
annotation filtering, it creates a much higher quality
workforce. Moreover, the initial effort is amortized
over time as these quality workers are retained over
the entire duration of the data collection. Many of
them annotated all the available videos we had.
3.2 Video Collection
To find suitable videos to annotate, we deployed a
separate task. Workers were asked to submit short
(generally 4-10 seconds) video segments depicting
single, unambiguous events by specifying links to
YouTube videos, along with the start and end times.
We again used a tiered payment system to reward
and retain workers who performed well.
Since the scope of this data collection effort ex-
tended beyond gathering English data alone, we
3Everyone who submitted descriptions in a foreign language
was granted access to the Tier-2 tasks. This was done to encour-
age more submissions in different languages and also because
we could not verify the quality of those descriptions other than
using online translation services (and some of the languages
were not available to be translated).
?? Someone	 ?is	 ?coa?ng	 ?a	 ?pork	 ?chop	 ?in	 ?a	 ?glass	 ?bowl	 ?of	 ?flour.	 ??? A	 ?person	 ?breads	 ?a	 ?pork	 ?chop.	 ??? Someone	 ?is	 ?breading	 ?a	 ?piece	 ?of	 ?meat	 ?with	 ?a	 ?white	 ?powdery	 ?substance.	 ??? A	 ?chef	 ?seasons	 ?a	 ?slice	 ?of	 ?meat.	 ??? Someone	 ?is	 ?pu?g	 ?flour	 ?on	 ?a	 ?piece	 ?of	 ?meat.	 ??? A	 ?woman	 ?is	 ?adding	 ?flour	 ?to	 ?meat.	 ??? A	 ?woman	 ?is	 ?coa?ng	 ?a	 ?piece	 ?of	 ?pork	 ?with	 ?breadcrumbs.	 ??? A	 ?man	 ?dredges	 ?meat	 ?in	 ?bread	 ?crumbs.	 ??? A	 ?person	 ?breads	 ?a	 ?piece	 ?of	 ?meat.	 ??? A	 ?woman	 ?is	 ?breading	 ?some	 ?meat.	 ??? Someone	 ?is	 ?breading	 ?meat.	 ??? A	 ?woman	 ?coats	 ?a	 ?meat	 ?cutlet	 ?in	 ?a	 ?dish.	 ??? A	 ?woman	 ?is	 ?coa?ng	 ?a	 ?pork	 ?loin	 ?in	 ?bread	 ?crumbs.	 ??? The	 ?laldy	 ?coated	 ?the	 ?meat	 ?in	 ?bread	 ?crumbs.	 ??? The	 ?woman	 ?is	 ?breading	 ?pork	 ?chop.	 ??? A	 ?woman	 ?adds	 ?a	 ?mixture	 ?to	 ?some	 ?meat.	 ??? The	 ?lady	 ?put	 ?the	 ?ba?er	 ?on	 ?the	 ?meat.	 ?
Figure 2: Examples of English descriptions collected for
a particular video segment.
tried to collect videos that could be understood
regardless of the annotator?s linguistic or cultural
background. In order to avoid biasing lexical
choices in the descriptions, we muted the audio and
excluded videos that contained either subtitles or
overlaid text. Finally, we manually filtered the sub-
mitted videos to ensure that each met our criteria and
was free of inappropriate content.
3.3 Data
We deployed our data collection framework on Me-
chanical Turk over a two-month period from July to
September in 2010, collecting 2,089 video segments
and 85,550 English descriptions. The rate of data
collection accelerated as we built up our workforce,
topping 10K descriptions a day when we ended our
data collection. Of the descriptions, 33,855 were
from Tier-2 tasks, meaning they were provided by
workers who had been manually identified as good
performers. Examples of some of the descriptions
collected are shown in Figure 2.
Overall, 688 workers submitted at least one En-
glish description. Of these workers, 113 submitted
at least 100 descriptions and 51 submitted at least
500. The largest number of descriptions submitted
by a single worker was 3496 4. Out of the 688 work-
ers, 50 were granted access to the Tier-2 tasks. The
4This number exceeds the total number of videos because
the worker completed both Tier-1 and Tier-2 tasks for the same
videos
193
Tier 1 Tier 2
pay $0.01 $0.05
# workers (English) 683 50
# workers (total) 835 94
# submitted (English) 51510 33829
# submitted (total) 68578 55682
# accepted (English) 51052 33825
# accepted (total) 67968 55658
Table 1: Statistics for the two video description tasks
success of our data collection effort was in part due
to our ability to retain these good workers, building a
reliable and efficient workforce. Table 1 shows some
statistics for the Tier-1 and Tier-2 tasks 5. Overall,
we spent under $5,000 including Amazon?s service
fees, some pilot experiments and surveys.
On average, 41 descriptions were produced for
each video, with at least 27 for over 95% of the
videos. Even limiting the set to descriptions pro-
duced from the Tier-2 tasks, there are still 16 de-
scriptions on average for each video, with at least 12
descriptions for over 95% of the videos. For most
clusters, then, we have a dozen or more high-quality
parallel descriptions that can be paired with one an-
other to create monolingual parallel training data.
4 Paraphrase Evaluation Metrics
One of the limitations to the development of ma-
chine paraphrasing is the lack of standard metrics
like BLEU, which has played a crucial role in driv-
ing progress in MT. Part of the issue is that a
good paraphrase has the additional constraint that
it should be lexically dissimilar to the source sen-
tence while preserving the meaning. These can be-
come competing goals when using n-gram overlaps
to establish semantic equivalence. Thus, researchers
have been unable to rely on BLEU or some deriva-
tive: the optimal paraphrasing engine under these
terms would be one that simply returns the input.
To combat such problems, Liu et al (2010) have
proposed PEM, which uses a second language as
pivot to establish semantic equivalence. Thus, no
n-gram overlaps are required to determine the se-
mantic adequacy of the paraphrase candidates. PEM
5The numbers for the English data are slightly underesti-
mated since the workers sometimes incorrectly filled out the
form when reporting what language they were using.
also separately measures lexical dissimilarity and
fluency. Finally, all three scores are combined us-
ing a support vector machine (SVM) trained on hu-
man ratings of paraphrase pairs. While PEM was
shown to correlate well with human judgments, it
has some limitations. It only models paraphrasing at
the phrase level and not at the sentence level. Fur-
ther, while it does not need reference sentences for
the evaluation dataset, PEM does require suitable
bilingual data to train the metric. The result is that
training a successful PEM becomes almost as chal-
lenging as the original paraphrasing problem, since
paraphrases need to be learned from bilingual data.
The highly parallel nature of our data suggests
a simpler solution to this problem. To measure
semantic equivalence, we simply use BLEU with
multiple references. The large number of reference
paraphrases capture a wide space of sentences with
equivalent meanings. While the set of reference sen-
tences can of course never be exhaustive, our data
collection method provides a natural distribution of
common phrases that might be used to describe an
action or event. A tight cluster with many simi-
lar parallel descriptions suggests there are only few
common ways to express that concept.
In addition to measuring semantic adequacy and
fluency using BLEU, we also need to measure lexi-
cal dissimilarity with the source sentence. We intro-
duce a new scoring metric PINC that measures how
many n-grams differ between the two sentences. In
essence, it is the inverse of BLEU since we want to
minimize the number of n-gram overlaps between
the two sentences. Specifically, for source sentence
s and candidate sentence c:
PINC(s, c) =
1
N
N?
n=1
1? | n-grams ? n-gramc || n-gramc |
where N is the maximum n-gram considered and n-
grams and n-gramc are the lists of n-grams in the
source and candidate sentences, respectively. We
use N = 4 in our evaluations.
The PINC score computes the percentage of n-
grams that appear in the candidate sentence but not
in the source sentence. This score is similar to the
Jaccard distance, except that it excludes n-grams that
only appear in the source sentence and not in the
candidate sentence. In other words, it rewards candi-
194
dates for introducing new n-grams but not for omit-
ting n-grams from the original sentence. The results
for each n are averaged arithmetically. PINC eval-
uates single sentences instead of entire documents
because we can reliably measure lexical dissimilar-
ity at the sentence level. Also notice that we do not
put additional constraints on sentence length: while
extremely short and extremely long sentences are
likely to score high on PINC, they still must main-
tain semantic adequacy as measured by BLEU.
We use BLEU and PINC together as a 2-
dimensional scoring metric. A good paraphrase, ac-
cording to our evaluation metric, has few n-gram
overlaps with the source sentence but many n-gram
overlaps with the reference sentences. This is con-
sistent with our requirement that a good paraphrase
should be lexically dissimilar from the source sen-
tence while preserving its semantics.
Unlike Liu et al (2010), we treat these two cri-
teria separately, since different applications might
have different preferences for each. For example,
a paraphrase suggestion tool for a word processing
software might be more concerned with semantic
adequacy, since presenting a paraphrase that does
not preserve the meaning would likely result in a
negative user experience. On the other hand, a query
expansion algorithm might be less concerned with
preserving the precise meaning so long as additional
relevant terms are added to improve search recall.
5 Experiments
To verify the usefulness of our paraphrase corpus
and the BLEU/PINC metric, we built and evaluated
several paraphrase systems and compared the auto-
matic scores to human ratings of the generated para-
phrases. We also investigated the pros and cons of
collecting paraphrases using video annotation rather
than directly eliciting them.
5.1 Building paraphrase models
We built 4 paraphrase systems by training English to
English translation models using Moses (Koehn et
al., 2007) with the default settings. Using our para-
phrase corpus to train and to test, we divided the sen-
tence clusters associated with each video into 90%
for training and 10% for testing. We restricted our
attention to sentences produced from the Tier-2 tasks
1	 ?
5	 ?
10	 ?
all	 ?68.9	 ?
69	 ?
69.1	 ?
69.2	 ?
69.3	 ?
69.4	 ?
69.5	 ?
69.6	 ?
69.7	 ?
69.8	 ?
69.9	 ?
44.5	 ? 45	 ? 45.5	 ? 46	 ? 46.5	 ? 47	 ? 47.5	 ? 48	 ? 48.5	 ?
BLEU
	 ?
PINC	 ?
Figure 3: Evaluation of paraphrase systems trained on
different numbers of parallel sentences. As more training
pairs are used, the model produces more varied sentences
(PINC) but preserves the meaning less well (BLEU)
in order to avoid excessive noise in the datasets, re-
sulting in 28,785 training sentences and 3,367 test
sentences. To construct the training examples, we
randomly paired each sentence with 1, 5, 10, or
all parallel descriptions of the same video segment.
This corresponds to 28K, 143K, 287K, and 449K
training pairs respectively. For the test set, we used
each sentence once as the source sentence with all
parallel descriptions as references (there were 16
references on average, with a minimum of 10 and a
maximum of 31.) We also included the source sen-
tence as a reference for itself.
Overall, all the trained models produce reasonable
paraphrase systems, even the model trained on just
28K single parallel sentences. Examples of the out-
puts produced by the models trained on single paral-
lel sentences and on all parallel sentences are shown
in Table 2. Some of the changes are simple word
substitutions, e.g. rabbit for bunny or gun for re-
volver, while others are phrasal, e.g. frying meat for
browning pork or made a basket for scores in a bas-
ketball game. One interesting result of using videos
as the stimulus to collect training data is that some-
times the learned paraphrases are not based on lin-
guistic closeness, but rather on visual similarity, e.g.
substituting cricket for baseball.
To evaluate the results quantitatively, we used the
BLEU/PINC metric. The performance of all the
trained models is shown in Figure 3. Unsurprisingly,
there is a tradeoff between preserving the meaning
195
Original sentence Trained on 1 parallel sentence Trained on all parallel sentences
a bunny is cleaning its paw a rabbit is licking its paw a rabbit is cleaning itself
a man fires a revolver a man is shooting targets a man is shooting a gun
a big turtle is walking a huge turtle is walking a large tortoise is walking
a guy is doing a flip over a park bench a man does a flip over a bench a man is doing stunts on a bench
milk is being poured into a mixer a man is pouring milk into a mixer a man is pouring milk into a bowl
children are practicing baseball children are doing a cricket children are playing cricket
a boy is doing karate a man is doing karate a boy is doing martial arts
a woman is browning pork in a pan a woman is browning pork in a pan a woman is frying meat in a pan
a player scores in a basketball game a player made a basketball game a player made a basket
Table 2: Examples of paraphrases generated by the trained models.
and producing more varied paraphrases. Systems
trained on fewer parallel sentences are more con-
servative and make fewer mistakes. On the other
hand, systems trained on more parallel sentences of-
ten produce very good paraphrases but are also more
likely to diverge from the original meaning. As a
comparison, evaluating each human description as
a paraphrase for the other descriptions in the same
cluster resulted in a BLEU score of 52.9 and a PINC
score of 77.2. Thus, all the systems performed very
well in terms of retaining semantic content, although
not as well in producing novel sentences.
To validate the results suggested by the automatic
metrics, we asked two fluent English speakers to
rate the generated paraphrases on the following cate-
gories: semantic, dissimilarity, and overall. Seman-
tic measures how well the paraphrase preserves the
original meaning while dissimilarity measures how
much the paraphrase differs from the source sen-
tence. Each category is rated from 1 to 4, with 4
being the best. A paraphrase identical to the source
sentence would receive a score of 4 for meaning and
1 for dissimilarity and overall. We randomly se-
lected 200 source sentences and generated 2 para-
phrases for each, representing the two extremes: one
paraphrase produced by the model trained with sin-
gle parallel sentences, and the other by the model
trained with all parallel sentences. The average
scores of the two human judges are shown in Ta-
ble 3. The results confirm our finding that the sys-
tem trained with single parallel sentences preserves
the meaning better but is also more conservative.
5.2 Correlation with human judgments
Having established rough correspondences between
BLEU/PINC scores and human judgments of se-
Semantic Dissimilarity Overall
1 3.09 2.65 2.51
All 2.91 2.89 2.43
Table 3: Average human ratings of the systems trained on
single parallel sentences and on all parallel sentences.
mantic equivalence and lexical dissimilarity, we
quantified the correlation between these automatic
metrics and human ratings using Pearson?s corre-
lation coefficient, a measure of linear dependence
between two random variables. We computed the
inter-annotator agreement as well as the correlation
between BLEU, PINC, PEM (Liu et al, 2010) and
the average human ratings on the sentence level. Re-
sults are shown in Table 4.
In order to measure correlation, we need to score
each paraphrase individually. Thus, we recomputed
BLEU on the sentence level and left the PINC scores
unchanged. While BLEU is typically not reliable at
the single sentence level, our large number of ref-
erence sentences makes BLEU more stable even at
this granularity. Empirically, BLEU correlates fairly
well with human judgments of semantic equiva-
lence, although still not as well as the inter-annotator
agreement. On the other hand, PINC correlates as
well as humans agree with each other in assessing
lexical dissimilarity. We also computed each met-
ric?s correlation with the overall ratings, although
neither should be used alone to assess the overall
quality of paraphrases.
PEM had the worst correlation with human judg-
ments of all the metrics. Since PEM was trained on
newswire data, its poor adaptation to this domain is
expected. However, given the large amount of train-
ing data needed (PEM was trained on 250K Chinese-
196
Semantic Dissimilarity Overall
Judge A vs. B 0.7135 0.6319 0.4920
BLEU vs. Human 0.5095 N/A 0.2127
PINC vs. Human N/A 0.6672 0.0775
PEM vs. Human N/A N/A 0.0654
PINC vs. Human (BLEU > threshold)
threshold = 0 N/A 0.6541 0.1817
threshold = 30 N/A 0.6493 0.1984
threshold = 60 N/A 0.6815 0.3986
threshold = 90 N/A 0.7922 0.4350
Combined BLEU and PINC vs. Human
Arithmetic Mean N/A N/A 0.3173
Geometric Mean N/A N/A 0.3003
Harmonic Mean N/A N/A 0.3036
PINC ?
Sigmoid(BLEU) N/A N/A 0.3532
Table 4: Correlation between the human judges as well
as between the automatic metrics and the human judges.
English sentence pairs and 2400 human ratings of
paraphrase pairs), it is difficult to use PEM as a gen-
eral metric. Adapting PEM to a new domain would
require sufficient in-domain bilingual data to sup-
port paraphrase extraction. In contrast, our approach
only requires monolingual data, and evaluation can
be performed using arbitrarily small, highly-parallel
datasets. Moreover, PEM requires sample human
ratings in training, thereby lessening the advantage
of having automatic metrics.
Since lexical dissimilarity is only desirable when
the semantics of the original sentence is unchanged,
we also computed correlation between PINC and the
human ratings when BLEU is above certain thresh-
olds. As we restrict our attention to the set of para-
phrases with higher BLEU scores, we see an in-
crease in correlation between PINC and the human
assessments. This confirms our intuition that PINC
is a more useful measure when semantic content has
been preserved.
Finally, while we do not believe any single score
could adequately describe the quality of a para-
phrase outside of a specific application, we experi-
mented with different ways of combining BLEU and
PINC into a single score. Almost any simple combi-
nation, such as taking the average of the two, yielded
decent correlation with the human ratings. The best
correlation was achieved by taking the product of
PINC and a sigmoid function of BLEU. This follows
the intuition that semantic preservation is closer to a
-??0.1	 ?
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ? 10	 ? 11	 ? 12	 ? All	 ?
Pears
on's	 ?
Corre
la?on
	 ?
Number	 ?of	 ?references	 ?for	 ?BLEU	 ?
BLEU	 ?with	 ?source	 ?vs.	 ?Seman?c	 ? BLEU	 ?without	 ?source	 ?vs.	 ?Seman?c	 ?BLEU	 ?with	 ?source	 ?vs.	 ?Overall	 ? BLEU	 ?without	 ?source	 ?vs.	 ?Overall	 ?
Figure 4: Correlation between BLEU and human judg-
ments as we vary the number of reference sentences.
binary decision (i.e. a paraphrase either preserves
the meaning or it does not, in which case PINC does
not matter at all) than a linear function. We used
an oracle to pick the best logistic function in our
experiment. In practice, some sample human rat-
ings would be required to tune this function. Other
more complicated methods for combining BLEU
and PINC are also possible with sample human rat-
ings, such as using a SVM as was done in PEM.
We quantified the utility of our highly parallel
data by computing the correlation between BLEU
and human ratings when different numbers of refer-
ences were available. The results are shown in Fig-
ure 4. As the number of references increases, the
correlation with human ratings also increases. The
graph also shows the effect of adding the source sen-
tence as a reference. If our goal is to assess seman-
tic equivalence only, then it is better to include the
source sentence. If we are trying to assess the overall
quality of the paraphrase, it is better to exclude the
source sentence, since otherwise the metric will tend
to favor paraphrases that introduce fewer changes.
5.3 Direct paraphrasing versus video
annotation
In addition to collecting paraphrases through video
annotations, we also experimented with the more
traditional task of presenting a sentence to an anno-
tator and explicitly asking for a paraphrase. We ran-
domly selected a thousand sentences from our data
and collected two paraphrases of each using Me-
chanical Turk. We conducted a post-annotation sur-
197
vey of workers who had completed both the video
description and the direct paraphrasing tasks, and
found that paraphrasing was considered more diffi-
cult and less enjoyable than describing videos. Of
those surveyed, 92% found video annotations more
enjoyable, and 75% found them easier. Based on
the comments, the only drawback of the video an-
notation task is the time required to load and watch
the videos. Overall, half of the workers preferred the
video annotation task while only 16% of the workers
preferred the paraphrasing task.
The data produced by the direct paraphrasing task
also diverged less, since the annotators were in-
evitably biased by lexical choices and word order
in the original sentences. On average, a direct para-
phrase had a PINC score of 70.08, while a parallel
description of the same video had a score of 78.75.
6 Discussions and Future Work
While our data collection framework yields useful
parallel data, it also has some limitations. Finding
appropriate videos is time-consuming and remains a
bottleneck in the process. Also, more abstract ac-
tions such as reducing the deficit or fighting for jus-
tice cannot be easily captured by our method. One
possible solution is to use longer video snippets or
other visual stimuli such as graphs, schemas, or il-
lustrated storybooks to convey more complicated in-
formation. However, the increased complexity is
also likely to reduce the semantic closeness of the
parallel descriptions.
Another limitation is that sentences produced by
our framework tend to be short and follow simi-
lar syntactic structures. Asking annotators to write
multiple descriptions or longer descriptions would
result in more varied data but at the cost of more
noise in the alignments. Other than descriptions, we
could also ask the annotators for more complicated
responses such as ?fill in the blanks? in a dialogue
(e.g. ?If you were this person in the video, what
would you say at this point??), their opinion of the
event shown, or the moral of the story. However, as
with the difficulty of aligning news stories, finding
paraphrases within these more complex responses
could require additional annotation efforts.
In our experiments, we only used a subset of our
corpus to avoid dealing with excessive noise. How-
ever, a significant portion of the remaining data is
useful. Thus, an automatic method for filtering those
sentences could allow us to utilize even more of the
data. For example, sentences from the Tier-2 tasks
could be used as positive examples to train a string
classifier to determine whether a noisy sentence be-
longs in the same cluster or not.
We have so far used BLEU to measure seman-
tic adequacy since it is the most common MT met-
ric. However, other more advanced MT metrics
that have shown higher correlation with human judg-
ments could also be used.
In addition to paraphrasing, our data collection
framework could also be used to produces useful
data for machine translation and computer vision.
By pairing up descriptions of the same video in dif-
ferent languages, we obtain parallel data without re-
quiring any bilingual skills. Another application for
our data is to apply it to computer vision tasks such
as video retrieval. The dataset can be readily used
to train and evaluate systems that can automatically
generate full descriptions of unseen videos. As far as
we know, there are currently no datasets that contain
whole-sentence descriptions of open-domain video
segments.
7 Conclusion
We introduced a data collection framework that pro-
duces highly parallel data by asking different an-
notators to describe the same video segments. De-
ploying the framework on Mechanical Turk over a
two-month period yielded 85K English descriptions
for 2K videos, one of the largest paraphrase data re-
sources publicly available. In addition, the highly
parallel nature of the data allows us to use standard
MT metrics such as BLEU to evaluate semantic ad-
equacy reliably. Finally, we also introduced a new
metric, PINC, to measure the lexical dissimilarity
between the source sentence and the paraphrase.
Acknowledgments
We are grateful to everyone in the NLP group at
Microsoft Research and Natural Language Learning
group at UT Austin for helpful discussions and feed-
back. We thank Chris Brockett, Raymond Mooney,
Katrin Erk, Jason Baldridge and the anonymous re-
viewers for helpful comments on a previous draft.
198
References
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL-05).
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of Human Lan-
guage Technology Conference / North American Asso-
ciation for Computational Linguistics Annual Meeting
(HLT-NAACL-2003).
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL-2001).
Michael Bloodgood and Chris Callison-Burch. 2010a.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL-2010).
Michael Bloodgood and Chris Callison-Burch. 2010b.
Using Mechanical Turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Olivia Buzek, Philip Resnik, and Benjamin B. Beder-
son. 2010. Error driven paraphrase annotation us-
ing Mechanical Turk. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings of Human
Language Technology Conference / North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL-06).
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. Parametric: An automatic evaluation met-
ric for paraphrasing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING-2008).
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-2008).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2009).
Wallace L. Chafe. 1997. The Pear Stories: Cognitive,
Cultural and Linguistic Aspects of Narrative Produc-
tion. Ablex, Norwood, NJ.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34:597?614, December.
Michael Denkowski and Alon Lavie. 2010. Explor-
ing normalization techniques for human judgments of
machine translation adequacy collected using Amazon
Mechanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Michael Denkowski, Hassan Al-Haj, and Alon Lavie.
2010. Turker-assisted paraphrasing for English-
Arabic machine translation. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech and
Language Data with Amazon?s Mechanical Turk.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING-2004).
Li Fei-Fei, Asha Iyer, Christof Koch, and Pietro Perona.
2007. What do we perceive in a glance of a real-world
scene? Journal of Vision, 7(1):1?29.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
03).
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL-07).
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of Human
Language Technologies: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT-2010).
Dekang Lin and Patrick Pantel. 2001. DIRT-discovery
of inference rules from text. In Proceedings of the
199
Seventh ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD-2001).
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
PEM: A paraphrase evaluation metric exploiting par-
allel texts. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2010).
Xiaojuan Ma and Perry R. Cook. 2009. How well do vi-
sual verbs work in daily communication for young and
old adults. In Proceedings of ACM CHI 2009 Confer-
ence on Human Factors in Computing Systems.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of Human Language Technology Confer-
ence / North American Association for Computational
Linguistics Annual Meeting (HLT-NAACL-2003).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 311?318,
Philadelphia, PA, July.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004).
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using Amazon?s Mechanical Turk. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of ACM
CHI 2004 Conference on Human Factors in Comput-
ing Systems.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009).
200
