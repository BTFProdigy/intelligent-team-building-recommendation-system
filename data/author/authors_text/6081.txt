53
54
55
56
57
58
Orthographic Disambiguation Incorporating Transliterated Probability
Eiji ARAMAKI Takeshi IMAI Kengo Miyo Kazuhiko Ohe
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8655, Japan
aramaki@hcc.h.u-tokyo.ac.jp
Abstract
Orthographic variance is a fundamental
problem for many natural language process-
ing applications. The Japanese language, in
particular, contains many orthographic vari-
ants for two main reasons: (1) transliterated
words allow many possible spelling varia-
tions, and (2) many characters in Japanese
nouns can be omitted or substituted. Pre-
vious studies have mainly focused on the
former problem; in contrast, this study has
addressed both problems using the same
framework. First, we automatically col-
lected both positive examples (sets of equiv-
alent term pairs) and negative examples (sets
of inequivalent term pairs). Then, by using
both sets of examples, a support vector ma-
chine based classifier determined whether
two terms (t1 and t2) were equivalent. To
boost accuracy, we added a transliterated
probability P (t1|s)P (t2|s), which is the
probability that both terms (t1 and t2) were
transliterated from the same source term (s),
to the machine learning features. Exper-
imental results yielded high levels of ac-
curacy, demonstrating the feasibility of the
proposed approach.
1 Introduction
Spelling variations, such as ?center? and ?centre?,
which have different spellings but identical mean-
ings, are problematic for many NLP applications
including information extraction (IE), question an-
swering (QA), and machine transliteration (MT). In
Table 1: Examples of Orthographic Variants.
spaghetti Thompson operation
* ?? indicates a pronunciation. () indicates a translation.
this paper, these variations can be termed ortho-
graphic variants.
The Japanese language, in particular, contains
many orthographic variants, for two main reasons:
1. It imports many words from other languages
using transliteration, resulting in many possible
spelling variations. For example, Masuyama et
al. (2004) found at least six different spellings
for? spaghetti?in newspaper articles (Table 1
Left).
2. Many characters in Japanese nouns can be
omitted or substituted, leading to tons of in-
sertion variations (Daille et al, 1996) (Table 1
Right).
To address these problems, this study developed a
support vector machine (SVM) based classifier that
48
can determine whether two terms are equivalent. Be-
cause a SVM-based approach requires positive and
negative examples, we also developed a method to
automatically generate both examples.
Our proposed method differs from previously de-
veloped methods in two ways.
1. Previous studies have focused solely on the for-
mer problem (transliteration); our target scope
is wider. We addressed both transliteration
and character omissions/substitutions using the
same framework.
2. Most previous studies have focused on back-
transliteration (Knight and Graehl, 1998; Goto
et al, 2004), which has the goal of generating a
source word (s) for a Japanese term (t). In con-
trast, we employed a discriminative approach,
which has the goal of determining whether two
terms (t1 and t2) are equivalent. These two
goals are related. For example, if two terms (t1
and t2) were transliterated from the same word
(s), they should be orthographic variants. To
incorporate this information, we incorporated
a transliterated-probability (P (s|t1)?P (s|t2))
into the SVM features.
Although we investigated performance using
medical terms, our proposed method does not de-
pend on a target domain1.
2 Orthographic Variance in Dictionary
Entries
Before developing our methodology, we examined
problems related to orthographic variance.
First, we investigated the amount of orthographic
variance between two dictionaries? entries (DIC1
(Ito et al, 2003), totaling 69,604 entries, and DIC2
(Nanzando, 2001), totaling 27,971 entries).
Exact matches between entries only occurred for
10,577 terms (15.1% of DIC1, and 37.8% of DIC2).
From other entries, we extracted orthographic vari-
ance as follows.
STEP 1: Extracting Term Pairs with Similar
Spelling
1The domain could affect the performance, because most of
medical terms are imported from other languages, leading to
many orthographic variants.



Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 464?467,
Prague, June 2007. c?2007 Association for Computational Linguistics
UTH: Semantic Relation Classification using Physical Sizes
Eiji ARAMAKI Takeshi IMAI Kengo MIYO Kazuhiko OHE
The University of Tokyo Hospital department
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
aramaki@hcc.h.u-tokyo.ac.jp
Abstract
Although researchers have shown increas-
ing interest in extracting/classifying seman-
tic relations, most previous studies have ba-
sically relied on lexical patterns between
terms. This paper proposes a novel way to
accomplish the task: a system that captures
a physical size of an entity. Experimental
results revealed that our proposed method is
feasible and prevents the problems inherent
in other methods.
1 Introduction
Classification of semantic relations is important to
NLP as it would benefit many NLP applications,
such as machine translation and information re-
trieval.
Researchers have already proposed various
schemes. For example, Hearst (1992) manually de-
signed lexico-syntactic patterns for extracting is-a
relations. Berland and Charniak (1999) proposed a
similar method for part-whole relations. Brin (1998)
employed a bootstrapping algorithm for more spe-
cific relations (author-book relations). Kim and
Baldwin (2006) and Moldovan et al(2004) focused
on nominal relations in compound nouns. Turney
(2005) measured relation similarity between two
words. While these methods differ, they all utilize
lexical patterns between two entities.
Within this context, our goal was to utilize infor-
mation specific to an entity. Although entities con-
tain many types of information, we focused on the
physical size of an entity. Here, physical size refers
to the typical width/height of an entity. For example,
we consider book to have a physical size of 20?25
cm, and book to have a size of 10?10 m, etc.
We chose to use physical size for the following
reasons:
1. Most entities (except abstract entities) have a
physical size.
2. Several semantic relations are sensitive to phys-
ical size. For example, a content-container rela-
tion (e1 content-container e2) naturally means
that e1 has a smaller size than e2. A book is
also smaller than its container, library. A part-
whole relation has a similar constraint.
Our next problem was how to determine physi-
cal sizes. First, we used Google to conduct Web
searches using queries such as ?book (*cm x*cm)?
and ?library (*m x*m)?. Next, we extracted numeric
expressions from the search results and used the av-
erage value as the physical size.
Experimental results revealed that our proposed
approach is feasible and prevents the problems in-
herent in other methods.
2 Corpus
We used a corpus provided by SemEval2007 Task
#4 training set. This corpus consisted of 980 anno-
tated sentences (140 sentences?7 relations). Table
1 presents an example.
Although the corpus contained a large quantity of
information such as WordNet sense keys, comments,
etc., we used only the most pertinent information:
entity1 (e1), entity2 (e2), and its relation (true/false)
464
The <e1>library</e1> contained <e2>books
</e2> of guidance on the processes.
WordNet(e1) = "library\%1:14:00::",
WordNet(e2) = "book\%1:10:00::",
Content-Container(e2, e1) = "true",
Query = "the * contained books"
Table 1: An Example of Task#4 Corpus.
Figure 1: Three types of Features.
1
. For example, we extracted a triple example (li-
brary, book, true from Table 1.
3 Method
We applied support vector machine (SVM)-based
learning (Vapnik, 1999) using three types of fea-
tures: (1) basic pattern features (Section 3.1), (2) se-
lected pattern features (Section 3.2), and (3) physical
size features (Section 3.3). Figure 1 presents some
examples of these features.
3.1 Basic Pattern Features
First, the system finds lexical patterns that co-occur
with semantic relations between two entities (e1 and
e2). It does so by conducting searches using two
queries ?e1 * e2? and ?e2 * e1?. For example, two
queries, ?library * book? and ?book * library?, are
generated from Table 1.
Then, the system extracts the word (or word se-
quences) between two entities from the snippets in
the top 1,000 search results. We considered the ex-
tracted word sequences to be basic patterns. For ex-
ample, given ?...library contains the book...?, the ba-
sic pattern is ?(e1) contains the (e2)? 2.
1Our system is classified as an A4 system, and therefore
does not use WordNet or Query.
2This operation does not handle any stop-words. Therefore,
We gathered basic patterns for each relation, and
identified if each pattern had been obtained as a
SVM feature or not (1 or 0). We refer to these fea-
tures as basic pattern features.
3.2 Selected Pattern Features
Because basic pattern features are generated only
from snippets, precise co-occurrence statistics are
not available. Therefore, the system searches again
with more specific queries, such as ?library contains
the book?. However, this second search is a heavy
burden for a search engine, requiring huge numbers
of queries (# of samples ? # of basic patterns).
We thus selected the most informative n patterns
(STEP1) and conducted specific searches (# of sam-
ples ? n basic patterns)(STEP2) as follows:
STEP1: To select the most informative patterns,
we applied a decision tree (C4.5)(Quinlan,
1987) and selected the basic patterns located in
the top n branches 3.
STEP2: Then, the system searched again us-
ing the selected patterns. We considered log
weighted hits (log
10
|hits|) to be selected pat-
tern features. For example, if ?library contains
the book? produced 120,000 hits in Google, it
yields the value log
10
(12, 000) = 5.
3.3 Physical Size Features
As noted in Section 1, we theorized that an entity?s
size could be a strong clue for some semantic rela-
tions.
We estimated entity size using the following
queries:
1. ?< entity > (* cm x * cm)?,
2. ?< entity > (* x * cm)?,
3. ?< entity > (* m x * m)?,
4. ?< entity > (* x * m)?.
In these queries, < entity > indicates a slot for
each entity, such as ?book?, ?library?, etc. Then, the
system examines the search results for the numerous
expressions located in ?*? and considers the average
value to be the size.
?(e1) contains THE (e2)? and ?(e1) contains (e2)? are different
patterns.
3In the experiments in Section 4, we set n = 10.
465
Precision Recall F
?=1
PROPOSED 0.57 (=284/497) 0.60 (=284/471) 0.58
+SEL 0.56 (=281/496) 0.59 (=281/471) 0.57
+SIZE 0.53 (=269/507) 0.57 (=269/471) 0.54
BASELINE 0.53 (=259/487) 0.54 (=259/471) 0.53
Table 2: Results.
When results of size expressions were insufficient
(numbers < 10), we considered the entity to be non-
physical, i.e., to have no size.
By applying the obtained sizes, the system gener-
ates a size feature, consisting of six flags:
1. LARGE-e1: (e1?s X > e2?s X) and (e1?s Y > e2?s Y)
2. LARGE-e2: (e1?s X < e2?s X) and (e1?s Y < e2?s Y)
3. NOSIZE-e1: only e1 has no size.
4. NOSIZE-e2: only e2 has no size.
5. NOSIZE-BOTH: Both e1 and e2 have no size.
6. OTHER: Other.
4 Experiments
4.1 Experimental Set-up
To evaluate the performance of our system, we
used a SemEval-Task No#4 training set. We com-
pared the following methods using a ten-fold cross-
validation test:
1. BASELINE: with only basic pattern features.
2. +SIZE: BASELINE with size features.
3. +SEL: BASELINE with selected pattern features.
4. PROPOSED: BASELINE with both size and selected
pattern features.
For SVM learning, we used TinySVM with a lin-
ear kernel4.
4.2 Results
Table 2 presents the results. PROPOSED was the
most accurate, demonstrating the basic feasibility of
our approach.
Table 3 presents more detailed results. +SIZE
made a contribution to some relations (REL2 and
REL4). Particularly for REL4, +SIZE significantly
boosted accuracy (using McNemar tests (Gillick and
4http://chasen.org/ taku/software/TinySVM/
Figure 2: The Size of a ?Car?.
Cox, 1989); p = 0.05). However, contrary to our ex-
pectations, size features were disappointing for part-
whole relations (REL6) and content-container rela-
tions (REL7).
The reason for this was mainly the difficulty in es-
timating size. Table 4 lists the sizes of several enti-
ties, revealing some strange results, such as a library
sized 12.1 ? 8.4 cm, a house sized 53 ? 38 cm, and
a car sized 39 ? 25 cm. These sizes are unusually
small for the following reasons:
1. Some entities (e.g.?car?) rarely appear with
their size,
2. In contrast, entities such as ?toy car? or ?mini
car? frequently appear with a size.
Figure 2 presents the size distribution of ?car.?
Few instances appeared of real cars sized approxi-
mately 500 ? 400 cm, while very small cars smaller
than 100 ? 100 cm appeared frequently. Our current
method of calculating average size is ineffective un-
der this type of situation.
In the future, using physical size as a clue for de-
termining a semantic relation will require resolving
this problem.
5 Conclusion
We briefly presented a method for obtaining the size
of an entity and proposed a method for classifying
semantic relations using entity size. Experimental
results revealed that the proposed approach yielded
slightly higher performance than a baseline, demon-
strating its feasibility. If we are able to estimate en-
466
Relation PROPOSED +SEL +SIZE BASELINE
Precision 0.60 (=50/83) 0.56 (=53/93) 0.54 (=53/98) 0.50 (=53/106)
REL1 Recall 0.68 (=50/73) 0.72 (=53/73) 0.72 (=53/73) 0.72 (=53/73)
(Cause-Effect) F
?=1
0.64 0.63 0.59 0.61
Precision 0.59 (=43/72) 0.60 (=44/73) 0.56 (=45/79) 0.55 (=44/79)
REL2 Recall 0.60 (=43/71) 0.61 (=44/71) 0.63 (=45/71) 0.61 (=44/71)
(Instrument-Agency) F
?=1
0.60 0.61 0.59 0.58
Precision 0.70 (=56/80) 0.73 (=55/75) 0.65 (=54/82) 0.68 (=51/74)
REL3 Recall 0.65 (=56/85) 0.64 (=55/85) 0.63 (=54/85) 0.60 (=51/85)
(Product-Producer) F
?=1
0.67 0.68 0.64 0.64
Precision 0.41 (=23/56) 0.35 (=18/51) 0.48 (=24/49) 0.52 (=13/25)
REL4 Recall 0.42 (=23/54) 0.33 (=18/54) 0.44 (=24/54) 0.24 (=13/54)
(Origin-Entity) F
?=1
0.41 0.34 0.46 0.32
Precision 0.62 (=40/64) 0.61 (=40/65) 0.56 (=28/50) 0.56 (=29/51)
REL5 Recall 0.68 (=40/58) 0.68 (=40/58) 0.48 (=28/58) 0.50 (=29/58)
(Theme-Tool) F
?=1
0.65 0.65 0.51 0.53
Precision 0.45 (=46/101) 0.46 (=46/100) 0.41 (=49/118) 0.43 (=53/123)
REL6 Recall 0.70 (=46/65) 0.70 (=46/65) 0.75 (=49/65) 0.81 (=53/65)
(Part-Whole) F
?=1
0.55 0.55 0.53 0.56
Precision 0.63 (26/41) 0.64 (=25/39) 0.51 (=16/31) 0.55 (=16/29)
REL7 Recall 0.40 (26/65) 0.38 (=25/65) 0.24 (=16/65) 0.24 (=16/65)
(Content-Container) F
?=1
0.49 0.48 0.33 0.34
Table 3: Detailed Results.
entity # size
library 51 12.1?8.4 m
room 204 5.4?3.5 m
man 75 1.5?0.5 m
benches 33 93?42 cm
granite 68 76?48 cm
sink 34 57?25 cm
house 86 53?38 cm
books 50 46?24 cm
car 91 39?25 cm
turtles 15 38?23 cm
food 38 35?26 cm
oats 16 24?13 cm
tumor shrinkage 6 -
habitat degradation 5 -
Table 4: Some Examples of Entity Sizes.
?#? indicates the number of obtained size expressions.
?-? indicates a ?NO-SIZE? entity.
tity sizes more precisely in the future, the system
will become much more accurate.
References
Matthew Berland and Eugene Charniak. 1999. Finding parts
in very large corpora. In Proceedings of the Annual Con-
ference of the Association for Computational Linguistics
(ACL1999), pages 57?64.
Sergey Brin. 1998. Extracting patterns and relations from the
world wide web. In WebDB Workshop at 6th International
Conference on Extending Database Technology, EDBT?98,
pages 172?183.
L. Gillick and SJ Cox. 1989. Some statistical issues in the com-
parison of speech recognition algorithms. In Proceedings of
IEEE International Conference on Acoustics, Speech, and
Signal Processing, pages 532?535.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of International Confer-
ence on Computational Linguistics (COLING1992), pages
539?545.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics. In Pro-
ceedings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 491?498.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. Proceedings of HLT/NAACL-2004 Workshop on
Computational Lexical Semantics.
J.R. Quinlan. 1987. Simplifying decision trees. International
Journal of Man-Machine Studies, 27(1):221?234.
Peter D. Turney. 2005. Measuring semantic similarity by latent
relational analysis. In Proceedings of the Nineteenth Inter-
national Joint Conference on Artificial Intelligence (IJCAI-
05), pages 1136?1141.
Vladimir Vapnik. 1999. The Nature of Statistical Learning
Theory. Springer-Verlag.
467
Proceedings of the Workshop on BioNLP, pages 185?192,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
TEXT2TABLE:  
Medical Text Summarization System based on Named Entity 
Recognition and Modality Identification 
 
 
Eiji ARAMAKI Yasuhide MIURA Masatsugu TONOIKE 
The university of Tokyo Fuji Xerox Fuji Xerox 
eiji.aramaki@gmail.com Yasuhide.Miura@fujixerox.co.jp masatsugu.tonoike@fujixerox.co.jp 
 
Tomoko OHKUMA 
 
Hiroshi MASHUICHI 
 
Kazuhiko OHE 
Fuji Xerox Fuji Xerox The university of Tokyo Hospital 
ohkuma.tomoko@fujixerox.co.jp hiroshi.masuichi@fujixerox.co.jp kohe@hcc.h.u-tokyo.ac.jp 
 
 
 
Abstract 
With the rapidly growing use of electronic 
health records, the possibility of large-scale 
clinical information extraction has drawn 
much attention. It is not, however, easy to ex-
tract information because these reports are 
written in natural language. To address this 
problem, this paper presents a system that 
converts a medical text into a table structure. 
This system?s core technologies are (1) medi-
cal event recognition modules and (2) a nega-
tive event identification module that judges 
whether an event actually occurred or not. 
Regarding the latter module, this paper also 
proposes an SVM-based classifier using syn-
tactic information. Experimental results dem-
onstrate empirically that syntactic information 
can contribute to the method?s accuracy. 
1 Introduction 
The use of electronic texts in hospitals is increas-
ing rapidly everywhere. This study specifically 
examines discharge summaries, which are reports 
generated by medical personnel at the end of a pa-
tient?s hospital stay. They include massive clinical 
information about a patient?s health, such as the 
frequency of drug usage, related side-effects, and 
correlation between a disease and a patient?s ac-
tions (e.g., smoking, drinking), which enables un-
precedented large-scale research, engendering 
promising findings. 
N
A
(1
(2
(3
                                                          
evertheless, it is not easy to extract clinical in-
formation from the reports because these reports 
are written in natural language. An example of a 
discharge summary is presented in Table 1. The 
table shows records that are full of medical jargon, 
acronyms, shorthand notation, misspellings, and 
sentence fragments (Tawanda et al, 2006). 
To address this problem, this paper presents a 
proposal of a system that extracts medical events 
and date times from a text. It then converts them 
into a table structure. We designate this system 
TEXT2TABLE, which is available from a web 
site 1 . The extraction method, which achieves a 
high accuracy extraction, is based on Conditional 
Random Fields (CRFs) (Lafferty et al, 2001). 
nother problem is posed by events that do not 
actually occur, i.e., future scheduled events, events 
that are merely intended to take place, or hypo-
thetical events. As described herein, we call such 
non-actual events negative events. Negative 
events are frequently mentioned in medical re-
cords; actually, in our corpus, 12% of medical 
events are negative. Several examples of negative 
events (in italic letters) are presented below: 
 
) no headache 
) keep appointment of radiotherapy 
) .. will have intravenous fluids 
1 http://lab0.com/  
185
(4
(4'
(5
th
(6
ac
A
B
T
T
A
) .. came for radiotherapy 
) .. came for headache 
) Every week radiation therapy and chemical 
erapy are scheduled 
) Please call Dr. Smith with worsening head-
he or back pain, or any other concern. 
 
Negative events have two characteristics. First, 
various words and phrases indicate that an event is 
negative. For this study, such a word or phrase that 
makes an event negative is called a negative trig-
ger. For instance, a negation word ?no? is a nega-
tive trigger in (1). A noun ?appointment? in (2) is a 
negative trigger. Similarly, the auxiliary ?will? in 
(3) signals negation. More complex phenomena are 
presented in (4) and (4'). For instance, ?radiother-
apy? in (4) is a negative event because the therapy 
will be held in the future. In contrast, ?headache? 
in (4') is not negative because a patient actually has 
a ?headache?. These indicate that a simple rule-
based approach (such as a list of triggers) can only 
imply classification of whether an event is negative 
or not, and that information of the event category 
(e.g., a therapy or symptom) is required. 
nother characteristic is a long scope of a nega-
tive trigger. Although negative triggers are near the 
descriptive words of events in (1)?(4), there could 
alternatively be a great distance of separation, as 
portrayed in (5) and (6). In (5), a noun coordina-
tion separates a negative trigger from the event. In 
(6), the trigger ?please? renders all events in that 
sentence negative. These indicate that neighboring 
words are insufficient to determine whether an 
event is negative or not. To deal with (5), syntactic 
information is helpful because the trigger and the 
event are neighboring in the dependency structure, 
as portrayed in Fig. 2. To deal with (6), bag-of-
word (BOW) information is desired. 
ecause of the observation described above, this 
paper presents a proposal of a classifier: whether 
an event is negative or not. The proposed classifier 
uses various information, the event category, 
neighboring words, BOW, and dependent phrases. 
he point of this paper is two-fold: (1) We pro-
pose a new type of text-summarizing system 
(TEXT2TABLE) that requires a technique for a 
negative event identification. (2) We investigate 
what kind of information is helpful for negative 
event identification. 
he experiment results revealed that, in spite of 
the risk of parsing error, syntactic information can 
contribute to performance, demonstrating the fea-
sibility of the proposed approach. 
lthough experiments described in this paper are 
related to Japanese medical reports, the proposed 
method does not depend on specific languages or 
domains. 
 
Table 1: A Health Record Sample. 
BRIEF RESUME OF HOSPITAL COURSE : 57 yo with 
NSCLCa with back pain and headache . Trans-
ferred from neurosurgery for additional mgmt 
with palliative XRT to head . Pt initially 
presented with cough and hemoptysis to his 
primary MD . On CXR he was found to have a 
upper left lobe mass . He subsequently un-
derwent bronchoscopy and bx revealed non-
small cell adeno CA. STaging revealed multi-
ple bony mets including skull, spine with 
MRI revealing mild compression of vertebral 
bodies at T9, T11, T12 . T9 with encroach-
ment of spinal cord underwent urgent XRT 
with no response so he was referred to neu-
rosurgery for intervention . MRI-rt. fron-
tal, left temporal, rt cerebellar 
hemorrhagic enhancing lesions- most likely 
extensive intracranial mets? T-spine surgery 
considered second priority and plan to radi-
ate cranially immediately with steroid and 
anticonvulsant . He underwent simulation on 
3/28 to whole brain and T3-T7 fields with 
plan for rx to both sites over 2.5 weeks. 
Over the past 2 weeks he has noted frontal 
and occipital HA with left eyelid swelling, 
ptosis, and denies CP, SOB, no sig. BM in 
past 5 days, small amt of stool after sup-
pository. Neuro?He was Dilantin loaded and a 
level should be checked on 3/31 . He is to 
continue Decadron . Onc?He is to receive XRT 
on 3/31 and daily during that week . Pain 
control?Currently under control with MS con-
tin and MSIR prn. regimen . Follow HA, LBP. 
ENDO?Glucose control monitored while on de-
cadron with SSRI coverage . Will check 
HgbA1C prior to discharge . GI?Aggressive 
bowel regimen to continue at home . Pt is 
Full Code . ADDITIONAL COMMENTS: Please call 
Dr. Xellcaugh with worsening headache or 
back pain, or any other concern . Keep ap-
pointment as scheduled with XRT . Please 
check fingerstick once a day, and record, 
call MD if greater than 200 .  
 
186
 
Figure 1: Visualization result (Left), magnified (Right). 
 
 
Figure 2: Negative Triggers and Events on a Depend-
ency Structure. 
 
Table 2: Corpora and Modalities 
CORPUS MODALITY 
ACE asserted, or other 
TIMEML must, may, should, would, or 
could 
Prasad et al, 
2006 
assertion, belief, facts or eventu-
alities 
Saur? et al, 2007 certain, probable, possible, or 
other 
Inui et al, 2008 affirm, infer, doubt, hear, intend, 
ask, recommend, hypothesize, or 
other 
THIS STUDY S/O, necessity, hope, possible, 
recommend, intend  
 
Table 3: Markup Scheme (Tags and Definitions) 
Tag Definition (Examples) 
R Remedy, Medical operation 
(e.g. radiotherapy) 
T Medical test, Medical examination 
(e.g., CT, MRI) 
D Deasese, Symptom 
(e.g., Endometrial cancer, headache) 
M Medication, administration of a drug 
(e.g., Levofloxacin, Flexeril) 
A patient action 
(e.g., admitted to a hospital) 
V Other verb 
(e.g., cancer spread to ...)  
 
2 Related Works 
2.1 Previous Markup Schemes 
In the NLP field, fact identification has not been 
studied well to date. Nevertheless, similar analyses 
can be found in studies of sentence modality. 
The Automatic Content Extraction (ACE)2 in-
formation extraction program deals with event ex-
traction, by which each event is annotated with 
temporal and modal markers. 
A
S  
A
T
                                                          
 similar effort is made in the TimeML project 
(Pustejovsky et al, 2003). This project specifically 
examines temporal expressions, but several modal 
expressions are also covered. 
Prasad et al (2006) propose four factuality clas-
sifications (certain, probable...etc.) for the Penn 
Discourse TreeBank (PDTB) 3. 
aur? et al (2007) propose three modal categories
for text entailment tasks. 
mong various markup schemes, the most recent 
one is Experience Mining (Inui et al, 2008), which 
collects personal experiences from the web. They 
also distinguish whether an experience is an actual 
one or not, which is a similar problem to that con-
fronting us. 
able 2 portrays a markup scheme adopted by 
each project. Our purpose is similar to that of Ex-
perience Mining. Consequently, we fundamentally 
adopt its markup scheme. However, we modify the 
label to suit medical mannerisms. For example, 
?doubt? is modified into ?(S/O) suspicion of?. Rare 
modalities such as ?hear? are removed. 
 
2.2 Previous Algorithms 
Negation is a traditional topic in medical fields. 
Therefore, we can find many previous studies of 
the topic in the relevant literature. 
An algorithm, NegEx4 was proposed by Chap-
man et al (Chapman et al, 2001a; Chapman et al, 
2001b). It outputs an inference of whether a term is 
positive or negative. The original algorithm is 
based on a list of negation expressions. Goldin et al 
(2003) incorporate machine learning techniques 
(Na?ve Bayes and decision trees) into the algorithm. 
The extended version (ConText) was also proposed 
(Chapman et al, 2007). 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words to iden-
2 http://projects.ldc.upenn.edu/ace/ 
3 http://www.seas.upenn.edu/~pdtb/ 
4 http://www.dbmi.pitt.edu/chapman/NegEx.html 
187
tify negated statements and their scope. Their tech-
nique was used in The MAYO Clinic Vocabulary 
Server (MCVS)5, which encodes clinical expres-
sions into medical ontology (SNOMED-CT) and 
identifies whether the event is positive or negative. 
M
H
T
A
                                                          
utalik et al (2001) earlier developed Negfinder 
to recognize negated patterns in medical texts. 
Their system uses regular expressions to identify 
words indicating negation. Then it passes them as 
special tokens to the parser, which makes use of 
the single-token look-ahead strategy. 
uang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified based 
on syntactic categories. In fact, they are located in 
parse trees. Their hybrid approach can identify ne-
gated concepts in radiology reports even when they 
are located distantly from the negative term. 
he Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor to encode clinical doc-
uments in a structured form (Friedman et al, 
1994). Negated concepts and certainty modifiers 
are also encoded within the system. 
Veronika et al (2008) published a negation 
scope corpus6 in which both negation and uncer-
tainty are addressed. 
lthough their motivations are identical to ours, 
two important differences are apparent. (1) Previ-
ous (except for Veronika et al, 2008) methods deal 
with the two-way problem (positive or negative), 
whereas the analyses proposed herein tackle more 
fine-grained modalities. (2) Previous studies (ex-
cept for Huang et al, 2007) are based on BOW 
approaches, whereas we use syntactic information. 
3 Medical Text Summarization System: 
TEXT2TABLE 
Because the core problem of this paper is to iden-
tify negative events, this section briefly presents a 
description of the entire system, which consists of 
four steps. The detailed algorithm of negative iden-
tification is explained in Section 4. 
STEP 1: Event Identification 
First, we define the event discussed in this paper. 
We deal with events of six types, as presented in 
5 http://mayoclinproc.highwire.org/content/81/6/741.figures-
only 
6 www.inf.u-szeged.hu/rgai/bioscope 
Table 3. Two of the four are Verb Phrases (base 
VPs); the others are noun phrases (base-NPs). Be-
cause this task is similar to Named Entity Recogni-
tion (NER), we use the state-of-the art NER 
method, which is based on the IOB2 representation 
and Conditional Random Fields (CRFs). In learn-
ing, we use standard features, as shown in Table 4. 
 
Table 4: Features for Event Identification 
Lexicon 
and 
Stem 
Current target word (and its stem) and its 
surrounding words (and stem). The win-
dow size is five words (-2, -1, 0, 1, 2). 
POS Part of speech of current target word and 
its surrounding words (-2, -1, 0, 1, 2). The 
part of speech is analyzed using a POS 
tagger7. 
DIC A fragment for the target word appears in 
the medical dictionary (Ito et al, 2003).  
 
STEP 2: Normalization 
As described in Section 1, a term in a record is 
sometimes an acronym: shorthand notation. Such 
abbreviations are converted into standard notation 
through (1) date time normalization or (2) event 
normalization. 
(1) Date Time Normalization 
As for date time expressions, relative date expres-
sions are converted into YYYY/MM/DD as fol-
lows. 
  On Dec Last year ? 2007/12/XX 
  10 Dec 2008        ? 2008/12/10 
These conversions are based on heuristic rules. 
(2) Event Normalization 
Medical terms are converted into standard notation 
(dictionary entry terms) using orthographic disam-
biguation (Aramaki et al, 2008). 
STEP 3: TIME?EVENT Relation Identification 
Then, each event is tied with a date time. The cur-
rent system relies on a simple rule (i.e., an event is 
tied with the latest date time). 
STEP 4: Negative Identification 
The proposed SVM classifier distinguishes nega-
tive events from other events. The detailed algo-
rithm is described in the next section. 
4 Modality Identification Algorithm 
First, we define the negative. We classify modality 
events into eight types (Table 5). These classifica-
tions are motivated by those used in previous stud-
                                                          
7 http://chasen-legacy.sourceforge.jp/ 
188
ies (Inui et al, 2008). However, we simplify their 
scheme because several categories are rare in this 
domain. 
T
U
hese classes are not exclusive. For that reason, 
they sometimes lead to multiple class events. For 
example, given ?No chemotherapy is planned?, an 
event ?chemotherapy? belongs to two classes, 
which are ?NEGATION? and ?FUTURE?. 
Training Phase 
sing a corpus with modality annotation, we train 
a SVM classifier for each category. The training 
features come from four parts: 
(1) Current phrases: words included in a current 
event. We also regard their STEMs, POSs, and the 
current event category as features. 
(2) Surrounding phrases: words included in the 
current event phrase and its surrounding two 
phrases (p1, p2, n1, n2, as depicted in Fig. 3). The 
unit of the phrase is base-NP/VP, which is pro-
duced by the Japanese parser (Kurohashi et al, 
1994). Its window size is two in the neighboring 
phrase (p1, p2, c, n1, n2). We also deal with their 
STEMs and POSs. 
(3) Dependent phrases: words included in the 
parent phrase of the current phrase (d1 in Fig. 3), 
and grandparent phrases (d2 in Fig. 3). We also 
deal with their STEMs and POSs. 
(4) Previous Event: words (with STEMs and 
POSs) included in the previous (left side) events. 
Additionally, we deal with the previous event cate-
gory and the modality class. 
(5) Bag-of-words: all words (with STEMs and 
POSs) in the sentence. 
 
TEST Phrase 
During the test, each SVM classifier runs. 
Although this task is multiclass labeling, several 
class combinations are unnatural, such as 
FUTURE and S/O. We list up possible label com-
binations (that have at least one occurrence in the 
corpora); if such a combination appears in a text, 
we adapt a high confidence label (using a marginal 
distance). 
 
5 Experiments 
We investigate what kind of information contrib-
utes to the performance in various machine learn-
ing algorithms. 
 
Table 5: Classification of Modalities 
NEGATION An event with negation words 
such as ?not? or ?no?. 
FUTURE An event that is scheduled for 
execution in the future. 
PURPOSE An event that is planed by a doc-
tor, but its time schedule is am-
biguous (just a hope/intention).  
S/O An event (usually a disease) that 
is suspected. For example, given 
?suspected microscopic tumor in 
...?, ?microscopic tumor'' is an 
S/O event.? 
NECESSITY An event (usually a remedy or 
medical test) that is required. 
INTEND An event that is hoped for by a 
patient.  
Note that if the event is hoped by 
a doctor, we regard is a 
PURPOSE or FUTURE. For ex-
ample, given ?He hoped for 
chemical therapy?, ?chemical 
therapy? is INTEND. 
POSSIBLE An event (usually remedy) that is 
possible under the current situa-
tion. 
RECOMMEND An event (usually remedy) that is 
recommended by other doctor(s). 
 
 
5.1 Corpus and Setting 
We collected 435 Japanese discharge summaries in 
which events and the modality are annotated. For 
training, we used the CRF toolkit8 with standard 
parameters. In this experiment setting, the input is 
an event with its contexts. The output is an event 
modality class (positive of negative in two-way) 
(or more detailed modality class in nine-way). 
T
 
                                                          
he core problem addressed in this paper is mo-
dality classification. Therefore, this task setting 
assumes that all events are identified correctly. 
Table 6 presents the event identification accuracy. 
Except for the rare class V (the other verb), we got 
more than 80% F-scores. It is true that the accu-
racy is not perfect. Nevertheless, most of the re-
maining problems in this step will be solved using 
a larger corpus. 
5.2 Comparable Methods 
We conducted experiments in the 10-fold cross 
validation manner. We investigated the perform-
8 http://crfpp.sourceforge.net/ 
189
ance in various feature combinations and the fol-
lowing machine learning methods. 
 
 
Figure 3: Features 
 
Table 6: Event Identification Result. Tag precision re-
call F-score.  
 # P R F 
A (ACTION) 1,556 94.63 91.04 92.80 
V (VERB) 1,047 84.64 74.89 79.47 
D (DISEASE) 3,601 85.56 80.24 82.82 
M (MEDICINE) 1,045 86.99 81.34 84.07 
R (REMEDY) 1,699 84.50 76.36 80.22 
T (TEST) 2,077 84.74 76.68 80.51 
ALL 11,025 84.74 76.68 80.51  
 
Table 7: Various Machine Learning Method 
SVM Support Vector Machine (Vapnik, 
1999). We used TinySVM9 with a 
polynomial kernel (degree=2). 
AP Averaged Perceptron (Collins, 2002) 
PA1 Passive Aggressive I (Crammer et 
al., 2006)* 
PA2 Passive Aggressive II (Crammer et 
al., 2006)* 
CW Confidence Weighted (Dredze et al, 
2008)* 
* The online learning library10 is used for AP PA1,2 
CW . 
 
5.3 Evaluation Metrics 
We adopt evaluation of two types: 
(1) Two-way: positive or negative: 
(2) Nine-way: positive or one of eight modality 
categories. 
Recall and F-measure are investigated in both for 
evaluation precision. 
 
5.4 Results 
The results are shown in Table 8 (Two-Way) and 
in Table 9 (Nine-Way). 
Current Event Category 
The results in ID0?ID1 indicate that the current 
event category (CAT) is useful. However, events 
are sometimes misestimated in real settings. We 
                                                          
In
R
A
A
H
9 http://chasen.org/ taku/software/TinySVM/ 
10 http://code.google.com/p/oll 
must check more practical performance in the fu-
ture. 
Bag-of-words (BOW) Information 
Results in ID1?ID2 indicate that BOW is impor-
tant. 
Surrounding Phrase Contribution 
The results appearing in ID2?ID9 represent the 
contribution of each feature position. From ID3, 
ID4, and ID7 results, next phrases (n1, n2) and 
parent phrases (d1) were able to boost the accuracy. 
Despite the risk of parsing errors, parent phrases 
(d1) are helpful, which is an insight of this study. 
 contrast, we can say that the following features 
had little contribution: previous phrases (p1, p2 
from ID5 and ID6), grandparent phrases (d2 from 
ID8), and previous events (e from ID9). 
egarding p1 and p2, these modalities are rarely 
expressed in the previous parts in Japanese. 
s for d2, the grandparent phrases might be too 
removed from the target events. 
s for e, because texts in health records are frag-
mented, each event might have little relation. 
owever, the above features are also helpful in 
cases with a stronger learning algorithm. 
In fact, among ID10?ID14, the SVM-based 
classifier achieved the best accuracy with all fea-
tures (ID14). 
 
Table 8: Two-way Results 
 
? indicates the used feature. c are features from the cur-
rent phrase. p1, p2, n1, n2 are features from surrounding 
phrases. e are features from a previous event. BOW is a 
bag-of-words using features from an entire sentence. 
CAT is the category of the current event. 
 
190
Learning Methods 
Regarding the learning algorithms, all online learn-
ing methods (ID7 and ID15?17) showed lower ac-
curacies than SVM (ID11), indicating that this task 
requires heavy learning. 
 
Nine-way Results 
Table 9 presents the accuracies of each class. Fun-
damentally, we can obtain high performance in the 
frequent classes (such as NEGATION, PURPOSE, 
and S/O). In contrast, the classifier suffers from 
low frequent classes (such as FUTURE). How to 
handle such examples is a subject of future study. 
 
Table 9: Two-way Results 
 # Preci-
sion 
Re-
call 
F-
measure 
NEGATION 441 84.19 77.36 80.63 
PURPOSE 346 91.35 63.87 75.17 
S/O 242 90.74 72.39 80.53 
FUTURE 97 23.31 55.96 32.91 
POSSIBLE 36 83.33 40.55 54.55 
INTEND 32 76.66 29.35 42.44 
RECOMMEND 21 95.71 38.57 54.98 
NECESSITY 4 100 0 0  
 
4.5 Future Works 
In this section, we will discuss several remaining 
problems. First, as described, the classifier suffers 
from low frequent modality classes. To give more 
examples for such classes is an important problem. 
Our final goal is to realize precise information ex-
traction from health records. Our IE systems are 
already available at the web site (http://lab0.com). 
Comprehensive evaluation of those systems is re-
quired. 
6 Conclusions 
This paper presented a classifier that identified 
whether an event has actually occurred or not. The 
proposed SVM-based classifier uses both BOW 
information and dependency parsing results. The 
experimental results demonstrated 85.8 F-
measure% accuracy and revealed that syntactic 
information can contribute to the method?s accu-
racy. In the future, a method of handling low-
frequency events is strongly desired. 
 
 
Acknowledgments 
Part of this research is supported by Grant-in-Aid 
for Scientific Research (A) of Japan Society for the 
Promotion of Science Project Number:?20680006  
F.Y.2008-20011 and the Research Collaboration 
Project with Fuji Xerox  Co. Ltd. 
References 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001a. Evalua-
tion of negation phrases in narrative clinical reports. 
In Proceedings of AMIA Symp, pages 105-109. 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001b. A sim-
ple algorithm for identifying negated findings and 
diseases in discharge summaries. Journal of Bio-
medical Informatics, 5:301-310. 
Wendy Chapman, John Dowling and David Chu. 2007. 
ConText: An algorithm for identifying contextual 
features from clinical text. Biological, translational, 
and clinical language processing (BioNLP2007), pp. 
81?88. 
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko 
Ohe: Orthographic Disambiguation Incorporating 
Transliterated Probability International Joint Confer-
ence on Natural Language Processing (IJCNLP2008), 
pp.48-55, 2008. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom, and 
Dietlind L. Wahner Roedler. A controlled trial of au-
tomated classification of negation from clinical notes. 
BMC Medical Informatics and Decision Making 
5:13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161-174. 
L. Gillick and S.J. Cox. 1989. Some statistical issues in 
the comparison of speech recognition algorithms. In 
Proceedings of IEEE International Conference on 
Acoustics, Speech, and Signal Processing, pages 532-
535. 
Ilya M. Goldin and Wendy Chapman. 2003. Learning to 
detect negation with not in medical texts. In Work-
shop at the 26th ACM SIGIR Conference. 
Yang Huang and Henry J. Lowe. 2007. A novel hybrid 
approach to automated negation detection in clinical 
radiology reports. Journal of the American Medical 
Informatics Association, 14(3):304-311. 
191
Kentaro Inui, Shuya Abe, Hiraku Morita, Megumi Egu-
chi, Asuka Sumida, Chitose Sao, Kazuo Hara, Koji 
Murakami, and Suguru Matsuyoshi. 2008. Experi-
ence mining: Building a large-scale database of per-
sonal experiences and opinions from web documents. 
In Proceedings of the 2008 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 314-
321. 
M. Ito, H. Imura, and H. Takahisa. 2003. Igaku- Shoin?s 
Medical Dictionary. Igakusyoin. 
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic 
analysis method of long Japanese sentences based on 
the detection of conjunctive structures. Computa-
tional Linguistics, 20(4). 
Pradeep G. Mutalik, Aniruddha Deshpande, and Pra-
kash M. Nadkarni. 2001. Use of general purpose ne-
gation detection to augment concept indexing of 
medical documents: A quantitative study using the 
umls. Journal of the American Medical Informatics 
Association, 8(6):598-609. 
J. Lafferty, A. McCallum, and F. Pereira: Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data, In Proceedings of the In-
ternational Conference on Machine Learning 
(ICML2001), pp.282-289, 2001. 
R. Prasad, N. Dinesh, A. Lee, A. Joshi and B. Webber: 
Annotating Attribution in the Penn Discourse Tree-
Bank, In Proceedings of the International Conference 
on Computational Linguistics and the Annual Con-
ference of the Association for Computational Lin-
guistics (COLING/ACL2006) Workshop on 
Sentiment and Subjectivity in Text, pp.31-38 (2006). 
R. Saur?, and J. Pustejovsky: Determining Modality and 
Factuality for Text Entailment, Proceedings of 
ICSC2007, pp. 509-516 (2007). 
Gaizauskas, A. Setzer, G. Katz, and D.R. Radev. 2003. 
New Directions in Question Answering: Timeml: 
Robust specification of event and temporal expres-
sions in text. AAAI Press. 
SNOMED-CT. 2002. SNOMED Clinical Terms Guide. 
College of American Pathologists.  
Sibanda Tawanda, Tian He, Peter Szolovits, and Uzuner 
Ozlem. 2006. Syntactically informed semantic cate-
gory recognizer for discharge summaries. In Proceed-
ings of the Fall Symposium of the American Medical 
Informatics Association (AMIA 2006), pages 11-15. 
Sibanda Tawanda and Uzuner Ozlem. 2006. Role of 
local context in automatic deidentification of un- 
grammatical, fragmented text. In Proceedings of the 
Human Language Technology conference and the 
North American chapter of the Association for Com-
putational Linguistics (HLT-NAACL2006), pages 
65-73. 
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The bioscope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(11). 
 
192
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 75?83,
Beijing, August 2010
Adverse?Effect Relations Extraction from  
Massive Clinical Records 
Yasuhide Miura a, Eiji Aramaki b, Tomoko Ohkuma a, Masatsugu Tonoike a,  
Daigo Sugihara a, Hiroshi Masuichi a and Kazuhiko Ohe c 
     a  Fuji Xerox Co., Ltd.
       b Center for Knowledge Structuring, University of Tokyo
         c University of Tokyo Hospital
yasuhide.miura@fujixerox.co.jp, eiji.aramaki@gmail.com, 
{ohkuma.tomoko,masatsugu.tonoike,daigo.sugihara, 
hiroshi.masuichi}@fujixerox.co.jp,  
kohe@hcc.h.u-tokyo.ac.jp 
 
Abstract 
The rapid spread of electronic health 
records raised an interest to large-scale 
information extraction from clinical 
texts. Considering such a background, 
we are developing a method that can 
extract adverse drug event and effect 
(adverse?effect) relations from massive 
clinical records. Adverse?effect rela-
tions share some features with relations 
proposed in previous relation extrac-
tion studies, but they also have unique 
characteristics. Adverse?effect rela-
tions are usually uncertain. Not even 
medical experts can usually determine 
whether a symptom that arises after a 
medication represents an adverse?
effect relation or not. We propose a 
method to extract adverse?effect rela-
tions using a machine-learning tech-
nique with dependency features. We 
performed experiments to extract ad-
verse?effect relations from 2,577 clini-
cal texts, and obtained F1-score of 
37.54 with an optimal parameters and 
F1-score of 34.90 with automatically 
tuned parameters. The results also 
show that dependency features increase 
the extraction F1-score by 3.59.  
1 Introduction  
The widespread use of electronic health rec-
ords (EHR) made clinical texts to be stored as 
computer processable data. EHRs contain im-
portant information about patients? health. 
However, extracting clinical information from 
EHRs is not easy because they are likely to be 
written in a natural language. 
We are working on a task to extract adverse 
drug event and effect relations from clinical 
records. Usually, the association between a 
drug and its adverse?effect relation is investi-
gated using numerous human resources, cost-
ing much time and money. The motivation of 
our task comes from this situation. An example 
of the task is presented in Figure 1. We defined 
an adverse?effect relation as a relation that 
holds between a drug entity and a symptom 
entity. The sentence illustrates the occurrence 
of the adverse?effect hepatic disorder by the 
Singulair medication.  
 
Figure 1. Example of an adverse?effect relation. 
A hepatic disorder found was suspected drug-induced and the Singulair was stopped.
adverse?effect relation
symptom drug
75
A salient characteristic of adverse?effect re-
lations is that they are usually uncertain. The 
sentence in the example states that the hepatic 
disorder is suspected drug-induced, which 
means the hepatic disorder is likely to present 
an adverse?effect relation. Figure 2 presents an 
example in which an adverse?effect relation is 
suspected, but words to indicate the suspicion 
are not stated. The two effects of the drug??the 
recovery of HbA1c and the appearance of the 
edema??are expressed merely as observation 
results in this sentence. The recovery of 
HbA1c is an expected effect of the drug and 
the appearance of the edema probably repre-
sents an adverse?effect case. The uncertain 
nature of adverse?effect relations often engen-
ders the statement of an adverse?effect rela-
tion as an observed fact. A sentence includ-
ing an adverse?effect relation occasionally be-
comes long to list all observations that ap-
peared after administration of a medication. 
Whether an interpretation that expresses an 
adverse?effect relation, such as drug-induced 
or suspected to be an adverse?effect, exists in a 
clinical record or not depends on a person who 
writes it. However, an adverse?effect relation 
is associated with an undesired effect of a 
medication. Its appearance would engender an 
extra action (e.g. stopped in the first example) 
or lead to an extra indication (e.g. but ? ap-
peared in the second example). Proper han-
dling of this extra information is likely to boost 
the extraction accuracy. 
The challenge of this study is to capture re-
lations with various certainties. To establish 
this goal, we used a dependency structure for 
the adverse?effect relation extraction method. 
Adverse?effect statements are assumed to 
share a dependency structure to a certain 
degree. For example, if we obtain the depend-
ency structures as shown in Figure 3, then we 
can easily determine that the structures are 
similar. Of course, obtaining such perfect pars-
ing results is not always possible. A statistical 
syntactic parser is known to perform badly if a 
text to be parsed belongs to a domain which 
differs from a domain on which the parser is 
trained (Gildea, 2001). A statistical parser will 
likely output incomplete results in these texts 
and will likely have a negative effect on rela-
tion extraction methods which depend on it. 
The specified research topic of this study is to 
investigate whether incomplete dependency 
structures are effective and how they behave in 
the extraction of uncertain relations.  
Figure 2. The example of an adverse-effect relation where the suspicion is not stated. 
Figure 3. The example of a similarity within dependency structures. 
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
A suspected drug-induced hepatic disorder found and the Singulair was stopped.
conjunct
nominal subject nominal subject
nominal subject nominal subject
conjunct
was
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
adverse-effect relation
drug symptom
76
2 Related Works 
Various studies have been done to extract se-
mantic information from texts. SemEval-2007 
Task:04 (Girju et al, 2007) is a task to extract 
semantic relations between nominals. The task 
includes ?Cause?Effect? relation extraction, 
which shares some similarity with a task that 
will be presented herein. Saeger et al (2008) 
presented a method to extract potential trou-
bles or obstacles related to the use of a given 
object. This relation can be interpreted as a 
more general relation of the adverse?effect 
relation. The protein?protein interaction (PPI) 
annotation extraction task of BioCreative II 
(Krallinger et al, 2008) is a task to extract PPI 
from PubMed abstracts. BioNLP?09 Shared 
Task on Event Extraction (Kim et al, 2009) is 
a task to extract bio-molecular events (bio-
events) from the GENIA event corpus.  
Similar characteristics to those of the ad-
verse?effect relation are described in previous 
reports in the bio-medical domain. Friedman et 
al. (1994) describes the certainty in findings of 
clinical radiology. Certainty is also known in 
scientific papers of biomedical domains as 
speculation (Light et al, 2004). Vincze et al 
(2008) are producing a freely available corpus 
including annotations of uncertainty along with 
its scope. 
Dependency structure feature which we uti-
lized to extract adverse?effect relations are 
widely used in relation extraction tasks. We 
present previous works which used syntac-
tic/dependency information as a feature of a 
statistical method. Beamer et al (2007), Giuli-
ano et al (2007), and Hendrickx et al (2007) 
all used syntactic information with machine 
learning techniques in SemEval-2007 Task:04 
and achieved good performance. Riedel et al 
(2009) used dependency path features with a 
statistical relational learning method in Bi-
oNLP?09 Shared Task on Event Extraction and 
achieved the best performance in the event en-
richment subtask. Miyao et al (2008) com-
pared syntactic information of various statisti-
cal parsers on PPI. 
3 Corpus  
We produced an annotated corpus of adverse?
effect relations to develop and test an adverse?
effect relation extraction method. This section 
presents a description of details of the corpus. 
3.1 Texts Comprising the Corpus 
We used a discharge summary among various 
documents in a hospital as the source data of 
the task. The discharge summary is a docu-
ment created by a doctor or another medical 
expert at the conclusion of a hospital stay. 
Medications performed during a stay are writ-
ten in discharge summaries. If adverse?effect 
relations were observed during the stay, they 
are likely to be expressed in free text. Texts 
written in discharge summaries tend to be writ-
ten more roughly than texts in newspaper arti-
cles or scientific papers. For example, the 
amounts of medications are often written in a 
name-value list as shown below: 
?When admitted to the hospital, Artist 6 mg1x, 
Diovan 70 mg1x, Norvasac 5 mg1x and BP 
was 145/83, but after dialysis, BP showed a 
decreasing tendency and in 5/14 Norvasac was 
reduced to 2.5 mg1x.? 
3.2 Why Adverse?Effect Relation Extrac-
tion from Discharge Summaries is 
Important 
In many countries, adverse?effects are investi-
gated through multiple phases of clinical trials, 
but unexpected adverse?effects occur in actual 
medications. One reason why this occurs is 
that drugs are often used in combination with 
others in actual medications. Clinical trials 
usually target single drug use. For that reason, 
the combinatory uses of drugs occasionally 
engender unknown effects. This situation natu-
rally motivates automatic adverse?effect rela-
tion extraction from actual patient records.  
  
77
 3.3 Corpus Size 
We collected 3,012 discharge summaries1 writ-
ten in Japanese from all departments of a hos-
pital. To reduce a cost to survey the occurrence 
of adverse?effects in the summaries, we first 
split the summaries into two sets: SET-A, 
which contains keywords related to adverse?
effects and SET-B, which do not contain the 
keywords. The keywords we used were ?stop, 
change, adverse effect?, and they were chosen 
based on a heuristic. The keyword filtering 
resulted to SET-A with 435 summaries and 
SET-B with 2,577 summaries. Regarding SET-
A, we randomly sampled 275 summaries and 
four annotators annotated adverse?effect in-
formation to these summaries to create the ad-
verse?effect relation corpus. For SET-B, the 
four annotators checked the small portion of 
the summaries. Cases of ambiguity were re-
solved through discussion, and even suspicious 
adverse?effect relations were annotated in the 
corpus as positive data. The overview of the 
summary selection is presented in Figure 4.  
                                                 
1 All private information was removed from them. 
The definition of private information was referred 
from the HIPAA guidelines. 
3.4 Quantities of Adverse?Effects in Clin-
ical Texts 
55.6% (=158/275) of the summaries in SET-A 
contained adverse?effects. 11.3% (=6/53) of 
the summaries in SET-B contained adverse?
effects. Since the ratio of SET-A:SET-B is 
14.4:85.6, we estimated that about 17.7%  
(=0.556?0.144+0.113?0.856) of the summar-
ies contain adverse?effects. Even considering 
that a summary may only include suspected 
adverse?effects, we think that discharge sum-
maries are a valuable resource to explore ad-
verse?effects. 
3.5 Annotated Information 
We annotated information of two kinds to the 
corpus: term information and relation infor-
mation. 
(1) Term Annotation  
Term annotation includes two tags: a tag to 
express a drug and a tag to express a drug ef-
fect. Table 1 presents the definition. In the 
corpus, 2,739 drugs and 12,391 effects were 
annotated. 
(2) Relation Annotation  
Adverse?effect relations are annotated as the 
?relation? attribute of the term tags. We repre-
sent the effect of a drug as a relation between a 
drug tag and a symptom tag. Table 2 presents 
Table 2. Annotation examples. 
Figure 4. The overview of the summary 
selection. 
Table 1. Markup scheme. 
The expression of a disease or 
symptom: e.g. endometrial cancer, 
headache. This tag covers not only a 
noun phrase but also a verb phrase 
such as ?<symptom>feels a pain in 
front of the head</symptom>?.
symptom
The expression of an administrated 
drug: e.g. Levofloxacin, Flexeril. 
drug
Definition and Examplestag
<drug relation=?1?>ACTOS(30)</drug> brought 
both <symptom relation=?1?>headache<symptom> 
and <symptom relation=?1?>insomnia</symptom>.
<drug relation=?1?>Ridora</drug> resumed 
because it is associated with an <symptom 
relation=?1?>eczematous rash</symptom>.
* If a drug has two or more adverse-effects, 
symptoms take a same relation ID.
3,012 
discharge 
summaries
435
summaries
w/ keywords
2,577
summaries
w/o keywords
275
summaries
53
summaries
153
summaries
w/ adverse?
effects
122
summaries
w/o adverse?
effects
6
summaries
w/ adverse?
effects
47
summaries
w/o adverse?
effects
YES NO
Contain keywords?
Random samplingRandom sampling
Contain adverse?
effects?
Contain adverse?
effects?
YES YESNO NO
SET-A (annotated corpus) SET-B
78
several examples, wherein ?relation=1? de-
notes the ID of a adverse?effect relation. In the 
corpus, 236 relations were annotated.  
4 Extraction Method 
We present a simple adverse?effect relation 
extraction method. We extract drug?symptom 
pairs from the corpus and discriminate them 
using a machine-learning technique. Features 
based on morphological analysis and depend-
ency analysis are used in discrimination. This 
approach is similar to the PPI extraction ap-
proach of Miyao et al (2008), in which we 
binary classify pairs whether they are in ad-
verse?effect relations or not. A pattern-based 
semi-supervised approach like Saeger et al 
(2008), or more generally Espresso (Pantel and 
Pennacchiotti, 2006), can also be taken, but we 
chose a pair classification approach to avoid 
the effect of seed patterns. To capture a view 
of an adverseness of a drug, a statistic of ad-
verse?effect relations is important. We do not 
want to favor certain patterns and chose a pair 
classification approach to equally treat every 
relation. Extraction steps of our method are as 
presented below. 
STEP 1: Pair Extraction   
All combinations of drug?symptom pairs that 
appear in a same sentence are extracted. Pairs 
<drug relation=?1?>Lasix</drug> for 
<symptom>hyperpiesia</symptom> has 
been suspended due to the appearance of 
a <symptom relation=?1?>headache
</symptom>.
headacheLasixpositive
hyperpiesiaLasixnegative
symptomdruglabel
ID Feature Definition and Examples
1 Character Distance The number of characters between members of a pair.
2 Morpheme Distance The number of morpheme between members of a pair.
3 Pair Order Order in which a drug and a symptom appear in a text; 
?drug?symptom? or ?symptom?drug?.
4 Symptom Type The type of symptom: ?disease name?, ?medical test name?, 
or ?medical test value?. 
5 Morpheme Chain Base?forms of morphemes that appear between a pair.
6 Dependency Chain Base?forms of morphemes included in the minimal 
dependency path of a pair.
7 Case Frame Chain Verb, case frame, and object triples that appear between a 
pair: e.g. ?examine? ??de?(case particle) ? ?inhalation?, 
?begin? ??wo?(case particle) ??medication?.
8 Case Frame 
Dependency Chain
Verb, case frame, and object triples included in the minimal 
dependency path of a pair.
Figure 6. Dependency chain example. 
 
Figure 5. Pair extraction example. 
hyperpiesia no-PP
for no-PP
Lasix wo-PP
headache no-PP 
appear niyori-PP
suspend ta-AUX
Lasix, wo-PP, headache, no-PP, 
appear, niyori-PP, suspend, ta-AUX
minimal path
Table 3. Features used in adverse-effect extraction. 
79
with the same relation ID become positive 
samples; pairs with different relation IDs be-
come negative samples. Figure 5 shows exam-
ples of positive and negative samples.  
STEP 2: Feature Extraction  
Features presented in Table 3 are extracted. 
The text in the corpus is in Japanese. Some 
features assume widely known characteristics 
of Japanese. For example, the dependency fea-
ture allows a phrase to depend on only one 
phrase that appears after a dependent phrase. 
Figure 6 portrays an example of a dependency 
chain feature. In the example, most terms were 
translated into English, excluding postpositions 
(PP) and auxiliaries (AUX), which are ex-
pressed in italic. To reduce the negative effect 
of feature sparsity, features which appeared in 
more than three summaries are used for fea-
tures with respective IDs 5?8. 
STEP 3: Machine Learning  
The support vector machine (SVM) (Vapnik, 
1995) is trained using positive/negative labels 
and features extracted in prior steps. In testing,                                          
an unlabeled pair is given a positive or nega-
tive label with the trained SVM.  
5 Experiment 
We performed two experiments to evaluate the 
extraction method. 
5.1 Experiment 1 
Experiment 1 aimed to observe the effects of 
the presented features. Five combinations of 
the features were evaluated with a five-fold 
cross validation assuming that an optimal pa-
rameter combination was obtained. The exper-
iment conditions are described below: 
A. Data  
7,690 drug?symptom pairs were extracted 
from the corpus.  Manually annotated infor-
mation was used to identify drugs and symp-
toms. Within 7,690 pairs, 149 pairs failed to 
extract the dependency chain feature. We re-
moved these 149 pairs and used the remaining 
7,541 pairs in the experiment. The 7,541 pairs 
consisted of 367 positive samples and 7,174 
negative samples.  
B. Feature Combinations  
We tested the five combinations of features in 
the experiment. Manually annotated infor-
mation was used for the symptom type feature. 
Features related to morphemes are obtained by 
processing sentences with a Japanese mor-
phology analyzer (JUMAN2 ver. 6.0). Features 
related to dependency and case are obtained by 
processing sentences using a Japanese depend-
ency parser (KNP ver. 3.0; Kurohashi and Na-
gao, 1994).  
C. Evaluations  
We evaluated the extraction method with all 
combinations of SVM parameters in certain 
                                                 
2 http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman-e.html 
E
D
C
B
A
ID
35.45
35.01
34.39
33.30
26.72
Precision
41.05
40.67
43.06
42.43
46.21
Recall
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=3.0, log(g)=-5.0, p=0.10
Parameters
37.181,2,3,4,5,6,7,8
36.781,2,3,4,5,6,8
37.541,2,3,4,5,6,7
36.641,2,3,4,5,6
33.051,2,3,4,5
F1-scoreFeature 
Combination
Table 4. Best F1-scores and their parameters. 
Figure 7. Precision?recall distribution. 
80
ranges. We used LIBSVM3 ver. 2.89 as an im-
plementation of SVM. The radial basis func-
tion (RBF) was used as the kernel function of 
SVM. The probability estimates option of 
LIBSVM was used to obtain the confidence 
value of discrimination.  
The gamma parameter of the RBF kernel 
was chosen from the range of [2-20, 20]. The C 
parameter of SVM was chosen from the range 
of [2-10, 210]. The SVM was trained and tested 
on 441 combinations of gamma and C. In test-
ing, the probability threshold parameter p be-
tween [0.05, 0.95] was also chosen, and the F1-
scores of all combination of gamma, C, and p 
were calculated with five-fold cross validation. 
The best F1-scores and their parameter values 
for each combination of features (optimal F1-
scores in this setting) are portrayed in Table 4. 
The precision?recall distribution of F1-scores 
with feature combination C is presented in 
Figure 7.  
5.2 Experiment 2 
Experiment 2 aimed to observe the perfor-
mance of our extraction method when SVM 
parameters were automatically tuned. In this 
experiment, we performed two cross valida-
tions: a cross validation to tune SVM parame-
ters and another cross validation to evaluate 
the extraction method. The experiment condi-
tions are described below:  
A. Data 
The same data as Experiment 1 were used. 
B. Feature Combination  
Feature combination C, which performed best 
in Experiment 1, was used.  
C. Evaluation  
                                                 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Two five-fold cross validations were per-
formed. The first cross validation divided the 
data to 5 sets (A, B, C, D, and E) each consist-
ing of development set and test set with the 
ratio of 4:1.  The second cross validation train 
and test all combination of SVM parameters (C, 
gamma, and p) in certain ranges and decide the 
optimal parameter combination(s) for  the de-
velopment sets of A, B, C, D, and E. The se-
cond cross validation denotes the execution of 
Experiment 1 for each development set.  For 
each optimal parameter combination of A, B, 
C, D, and E, the corresponding development 
set was trained and the trained model was test-
ed on the corresponding test set. The average 
F1-score on five test sets marked 34.90, which 
is 2.64 lower than the F1-score of Experiment 1 
with the same feature combination. 
6 Discussion 
The result of the experiment reveals the effec-
tiveness of the dependency chain feature and 
the case-frame chain feature. This section pre-
sents a description of the effects of several fea-
tures in detail. The section also mentions re-
maining problems in our extraction method.  
6.1 Effects of the Dependency Chain Fea-
ture and Case-frame Features  
A. Dependency Chain Feature  
The dependency chain features improved the 
F1-score by 3.59 (the F1-score difference be-
tween feature combination A and B). This in-
crease was obtained using 260 improved pairs 
and 127 deproved pairs. Improved pairs con-
Figure 8. Relation between the number of 
pairs and the morpheme distance. 
Figure 9. Number of dependency errors 
in the improved pairs sentences. 
25
93
23
sentence
with no error
sentence
with 1?3
errors
sentence
with 4 or
more errors
0
10
20
30
40
50
distance less 
than 40
distance larger than
or equal to 40
fre
qu
en
cy
improved 
deproved
81
tribute to the increase of a F1-score. Deproved 
pairs have the opposite effect. 
We observed that improved pairs tend to 
have longer morpheme distance compared to 
deproved pairs. Figure 8 shows the relation 
between the number of pairs and the mor-
pheme distance of improved pairs and de-
proved pairs. The ratio between the improved 
pairs and the deproved pairs is 11:1 when the 
distance is greater than 40.  In contrast, the 
ratio is 2:1 when the distance is smaller than 
40. This observation suggests that adverse?
effect relations share dependency structures to 
a certain degree.  
We also observed that in improved pairs, 
dependency errors tended to be low. Figure 9 
presents the manually counted number of de-
pendency errors in the 141 sentences in which 
the 260 improved pairs exist: 65.96 % of the 
sentences included 1?3 errors. The result sug-
gests that the dependency structure is effective 
even if it includes small errors.  
B. Case-frame Features  
The effect of the case-frame dependency chain 
feature differed with the effect of the depend-
ency chain feature. The case-frame chain fea-
ture improved the F1-score by 0.90 (the F1-
score difference between feature combination 
B and C), but the case-frame dependency chain 
feature decreased the F1-score by 0.36 (the F1-
score difference between feature combination 
C and E). One reason for the negative effect of 
the case-frame dependency feature might be 
feature sparsity, but no clear evidence of it has 
been found.  
6.2 Remaining Problems 
A. Imbalanced Data  
The adverse?effect relation pairs we used in 
the experiment were not balanced. Low values 
of optimal probability threshold parameter p 
suggest the degree of imbalance. We are con-
sidering introduction of some kind of method-
ology to reduce negative samples or to use a 
machine learning method that can accommo-
date imbalanced data well.  
B. Use of Medical Resources  
The extraction method we propose uses no 
medical resources. Girju et al (2007) indicate 
the effect of WordNet senses in the classifica-
tion of a semantic relation between nominals. 
Krallinger et al (2008) report that top scoring 
teams in the interaction pair subtask used so-
phisticated interactor protein normalization 
strategies. If medical terms in texts can be 
mapped to a medical terminology or ontology, 
it would likely improve the extraction accuracy.  
C. Fully Automated Extraction 
In the experiments, we used the manually 
annotated information to extract pairs and fea-
tures. This setting is, of course, not real if we 
consider a situation to extract adverse?effect 
relations from massive clinical records, but we 
chose it to focus on the relation extraction 
problem. We performed an event recognition 
experiment (Aramaki et al, 2009) and 
achieved F1-score of about 80. We assume that 
drug expressions and symptom expressions to 
be automatically recognized in a similar accu-
racy.  
We are planning to perform a fully automat-
ed adverse?effect relations extraction from a 
larger set of clinical texts to see the perfor-
mance of our method on a raw corpus. The 
extraction F1-score will likely to decrease, but 
we intend to observe the other aspect of the 
extraction, like the overall tendency of extract-
ed relations.  
7 Conclusion 
We presented a method to extract adverse?
effect relations from texts. One important 
characteristic of adverse?effect relations is that 
they are uncertain in most cases. We per-
formed experiments to extract adverse?effect 
relations from 2,577 clinical texts, and ob-
tained F1-score of 37.54 with optimal SVM 
parameters and F1-score of 34.90 with auto-
matically tuned SVM parameters. Results also 
show that dependency features increase the 
extraction F1-score by 3.59. We observed that 
an increased F1-score was obtained using the 
improvement of adverse?effects with long 
morpheme distance, which suggests that ad-
verse?effect relations share dependency struc-
tures to a certain degree. We also observed that 
the increase of the F1-score was obtained with 
dependency structures that include small errors, 
which suggests that the dependency structure 
is effective even if it includes small errors. 
  
82
References 
Aramaki, Eiji, Yasuhide Miura, Masatsugu Tonoike, 
Tomoko Ohkuma, Hiroshi Masuichi, and 
Kazuhiko Ohe. 2009. TEXT2TABLE: Medical 
Text Summarization System Based on Named 
Entity Recognition and Modality Identification. 
In Proceedings of the BioNLP 2009 Workshop, 
pages 185-192. 
Beamer, Brandon, Suma Bhat, Brant Chee, Andrew 
Fister, Alla Rozovskaya, and Roxana Girju. 
2007. UIUC: A Knowledge-rich Approach to 
Identifying Semantic Relations between Nomi-
nals. In Proceedings of Fourth International 
Workshop on Semantic Evaluations, pages 386-
389. 
Friedman, Carol, Philip O. Alderson, John H. M. 
Austin, James J. Cimino, and Stephen B. John-
son. 1994. A General Natural-language Text 
Processor for Clinical Radiology. Journal of the 
American Medical Informatics Association, 1(2), 
pages 161-174. 
Gildea, Daniel. 2001. Corpus Variation and Parser 
Performance. In Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1-9. 
Girju, Roxana, Preslav Nakov, Vivi Nastase,  Stan 
Szpakowicz, Peter Turney, and Deniz Yuret.  
2007. SemEval-2007 task 04: Classification of 
Semantic Relations between Nominals. In Pro-
ceedings of Fourth International Workshop on 
Semantic Evaluations, pages 13-18. 
Giuliano, Claudio, Alberto Lavelli, Daniele Pighin, 
and Lorenza Romano. 2007. FBK-IRST: Kernel 
Methods for Semantic Relation Extraction. In 
Proceedings of the 4th International Workshop 
on Semantic Evaluations, pages 141-144.  
Hendrickx , Iris, Roser Morante, Caroline Sporleder, 
and Antal van den Bosch. 2007. ILK: Machine 
learning of semantic relations with shallow fea-
tures and almost no data. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions, 187-190. 
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun?ichi Tsujii. 2009. 
Overview of  BioNLP?09 Shared Task on Event 
Extraction. In Proceedings of the BioNLP 2009 
Workshop Companion Volume for Shared Task, 
pages 1-9. 
Krallinger, Martin, Florian Leitner, Carlos  
Rodriguez-Penagos, and Alfonso Valencia. 2008.  
Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Ge-
nome Biology 2008, 9(Suppl 2):S4. 
Kurohashi, Sadao and Makoto Nagao. 1994. KN 
Parser : Japanese Dependency/Case Structure 
Analyzer. In Proceedings of The International 
Workshop on Sharable Natural Language Re-
sources, pages 22-28. Software available at 
http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/knp-e.html. 
Light, Marc, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Pro-
ceedings of HLT/NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, On-
tologies and Databases, pages 17-24. 
Miyao, Yusuke, Rune S?tre, Kenji Sagae, Takuya 
Matsuzaki, and Jun'ichi Tsujii. 2008. Task-
oriented Evaluation of Syntactic Parsers and 
Their Representations. In Proceedings of the 
46th Annual Meeting of the Association for 
Computational Linguistics: Human Language 
Technologies, pages 46-54. 
Pantel, Patrick and Marco Pennacchiotti. 2006. Es-
presso: Leveraging Generic Patterns for Auto-
matically Harvesting Semantic Relations. In 
Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 113-120. 
Riedel, Sebastian, Hong-Woo Chun, Toshihisa 
Takagi, and Jun'ichi Tsujii. 2009. A Markov 
Logic Approach to Bio-Molecular Event Extrac-
tion. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 
41-49. 
Saeger, Stijn De, Kentaro Torisawa, and Jun?ichi 
Kazama. 2008. Looking for Trouble. In Proceed-
ings of the 22nd International Conference on 
Computational Linguistics, pages 185-192. 
Vapnik, Vladimir N.. 1995. The Nature of Statisti-
cal Learning Theory. Springer-Verlag New York, 
Inc.. 
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008.  The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics 2008, 9(Suppl 11):S9.  
 
83
