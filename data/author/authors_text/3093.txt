Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432?439,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Structured Models for Fine-to-Coarse Sentiment Analysis
Ryan McDonald? Kerry Hannan Tyler Neylon Mike Wells Jeff Reynar
Google, Inc.
76 Ninth Avenue
New York, NY 10011
?Contact email: ryanmcd@google.com
Abstract
In this paper we investigate a structured
model for jointly classifying the sentiment
of text at varying levels of granularity. Infer-
ence in the model is based on standard se-
quence classification techniques using con-
strained Viterbi to ensure consistent solu-
tions. The primary advantage of such a
model is that it allows classification deci-
sions from one level in the text to influence
decisions at another. Experiments show that
this method can significantly reduce classifi-
cation error relative to models trained in iso-
lation.
1 Introduction
Extracting sentiment from text is a challenging prob-
lem with applications throughout Natural Language
Processing and Information Retrieval. Previous
work on sentiment analysis has covered a wide range
of tasks, including polarity classification (Pang et
al., 2002; Turney, 2002), opinion extraction (Pang
and Lee, 2004), and opinion source assignment
(Choi et al, 2005; Choi et al, 2006). Furthermore,
these systems have tackled the problem at differ-
ent levels of granularity, from the document level
(Pang et al, 2002), sentence level (Pang and Lee,
2004; Mao and Lebanon, 2006), phrase level (Tur-
ney, 2002; Choi et al, 2005), as well as the speaker
level in debates (Thomas et al, 2006). The abil-
ity to classify sentiment on multiple levels is impor-
tant since different applications have different needs.
For example, a summarization system for product
reviews might require polarity classification at the
sentence or phrase level; a question answering sys-
tem would most likely require the sentiment of para-
graphs; and a system that determines which articles
from an online news source are editorial in nature
would require a document level analysis.
This work focuses on models that jointly classify
sentiment on multiple levels of granularity. Consider
the following example,
This is the first Mp3 player that I have used ... I
thought it sounded great ... After only a few weeks,
it started having trouble with the earphone connec-
tion ... I won?t be buying another.
Mp3 player review from Amazon.com
This excerpt expresses an overall negative opinion of
the product being reviewed. However, not all parts
of the review are negative. The first sentence merely
provides some context on the reviewer?s experience
with such devices and the second sentence indicates
that, at least in one regard, the product performed
well. We call the problem of identifying the senti-
ment of the document and of all its subcomponents,
whether at the paragraph, sentence, phrase or word
level, fine-to-coarse sentiment analysis.
The simplest approach to fine-to-coarse sentiment
analysis would be to create a separate system for
each level of granularity. There are, however, obvi-
ous advantages to building a single model that clas-
sifies each level in tandem. Consider the sentence,
My 11 year old daughter has also been using it and
it is a lot harder than it looks.
In isolation, this sentence appears to convey negative
sentiment. However, it is part of a favorable review
432
for a piece of fitness equipment, where hard essen-
tially means good workout. In this domain, hard?s
sentiment can only be determined in context (i.e.,
hard to assemble versus a hard workout). If the clas-
sifier knew the overall sentiment of a document, then
disambiguating such cases would be easier.
Conversely, document level analysis can benefit
from finer level classification by taking advantage
of common discourse cues, such as the last sentence
being a reliable indicator for overall sentiment in re-
views. Furthermore, during training, the model will
not need to modify its parameters to explain phe-
nomena like the typically positive word great ap-
pearing in a negative text (as is the case above). The
model can also avoid overfitting to features derived
from neutral or objective sentences. In fact, it has al-
ready been established that sentence level classifica-
tion can improve document level analysis (Pang and
Lee, 2004). This line of reasoning suggests that a
cascaded approach would also be insufficient. Valu-
able information is passed in both directions, which
means any model of fine-to-coarse analysis should
account for this.
In Section 2 we describe a simple structured
model that jointly learns and infers sentiment on dif-
ferent levels of granularity. In particular, we reduce
the problem of joint sentence and document level
analysis to a sequential classification problem us-
ing constrained Viterbi inference. Extensions to the
model that move beyond just two-levels of analysis
are also presented. In Section 3 an empirical eval-
uation of the model is given that shows significant
gains in accuracy over both single level classifiers
and cascaded systems.
1.1 Related Work
The models in this work fall into the broad class of
global structured models, which are typically trained
with structured learning algorithms. Hidden Markov
models (Rabiner, 1989) are one of the earliest struc-
tured learning algorithms, which have recently been
followed by discriminative learning approaches such
as conditional random fields (CRFs) (Lafferty et al,
2001; Sutton and McCallum, 2006), the structured
perceptron (Collins, 2002) and its large-margin vari-
ants (Taskar et al, 2003; Tsochantaridis et al, 2004;
McDonald et al, 2005; Daume? III et al, 2006).
These algorithms are usually applied to sequential
labeling or chunking, but have also been applied to
parsing (Taskar et al, 2004; McDonald et al, 2005),
machine translation (Liang et al, 2006) and summa-
rization (Daume? III et al, 2006).
Structured models have previously been used for
sentiment analysis. Choi et al (2005, 2006) use
CRFs to learn a global sequence model to classify
and assign sources to opinions. Mao and Lebanon
(2006) used a sequential CRF regression model to
measure polarity on the sentence level in order to
determine the sentiment flow of authors in reviews.
Here we show that fine-to-coarse models of senti-
ment can often be reduced to the sequential case.
Cascaded models for fine-to-coarse sentiment
analysis were studied by Pang and Lee (2004). In
that work an initial model classified each sentence
as being subjective or objective using a global min-
cut inference algorithm that considered local label-
ing consistencies. The top subjective sentences are
then input into a standard document level polarity
classifier with improved results. The current work
differs from that in Pang and Lee through the use of
a single joint structured model for both sentence and
document level analysis.
Many problems in natural language processing
can be improved by learning and/or predicting mul-
tiple outputs jointly. This includes parsing and rela-
tion extraction (Miller et al, 2000), entity labeling
and relation extraction (Roth and Yih, 2004), and
part-of-speech tagging and chunking (Sutton et al,
2004). One interesting work on sentiment analysis
is that of Popescu and Etzioni (2005) which attempts
to classify the sentiment of phrases with respect to
possible product features. To do this an iterative al-
gorithm is used that attempts to globally maximize
the classification of all phrases while satisfying local
consistency constraints.
2 Structured Model
In this section we present a structured model for
fine-to-coarse sentiment analysis. We start by exam-
ining the simple case with two-levels of granularity
? the sentence and document ? and show that the
problem can be reduced to sequential classification
with constrained inference. We then discuss the fea-
ture space and give an algorithm for learning the pa-
rameters based on large-margin structured learning.
433
Extensions to the model are also examined.
2.1 A Sentence-Document Model
Let Y(d) be a discrete set of sentiment labels at
the document level and Y(s) be a discrete set of
sentiment labels at the sentence level. As input a
system is given a document containing sentences
s = s1, . . . , sn and must produce sentiment labels
for the document, yd ? Y(d), and each individ-
ual sentence, ys = ys1, . . . , y
s
n, where y
s
i ? Y(s) ?
1 ? i ? n. Define y = (yd,ys) = (yd, ys1, . . . , y
s
n)
as the joint labeling of the document and sentences.
For instance, in Pang and Lee (2004), yd would be
the polarity of the document and ysi would indicate
whether sentence si is subjective or objective. The
models presented here are compatible with arbitrary
sets of discrete output labels.
Figure 1 presents a model for jointly classifying
the sentiment of both the sentences and the docu-
ment. In this undirected graphical model, the label
of each sentence is dependent on the labels of its
neighbouring sentences plus the label of the docu-
ment. The label of the document is dependent on
the label of every sentence. Note that the edges
between the input (each sentence) and the output
labels are not solid, indicating that they are given
as input and are not being modeled. The fact that
the sentiment of sentences is dependent not only on
the local sentiment of other sentences, but also the
global document sentiment ? and vice versa ? al-
lows the model to directly capture the importance
of classification decisions across levels in fine-to-
coarse sentiment analysis. The local dependencies
between sentiment labels on sentences is similar to
the work of Pang and Lee (2004) where soft local
consistency constraints were created between every
sentence in a document and inference was solved us-
ing a min-cut algorithm. However, jointly modeling
the document label and allowing for non-binary la-
bels complicates min-cut style solutions as inference
becomes intractable.
Learning and inference in undirected graphical
models is a well studied problem in machine learn-
ing and NLP. For example, CRFs define the prob-
ability over the labels conditioned on the input us-
ing the property that the joint probability distribu-
tion over the labels factors over clique potentials in
undirected graphical models (Lafferty et al, 2001).
Figure 1: Sentence and document level model.
In this work we will use structured linear classi-
fiers (Collins, 2002). We denote the score of a la-
beling y for an input s as score(y, s) and define this
score as the sum of scores over each clique,
score(y, s) = score((yd,ys), s)
= score((yd, ys1, . . . , y
s
n), s)
=
n?
i=2
score(yd, ysi?1, y
s
i , s)
where each clique score is a linear combination of
features and their weights,
score(yd, ysi?1, y
s
i , s) = w ? f(y
d, ysi?1, y
s
i , s) (1)
and f is a high dimensional feature representation
of the clique and w a corresponding weight vector.
Note that s is included in each score since it is given
as input and can always be conditioned on.
In general, inference in undirected graphical mod-
els is intractable. However, for the common case of
sequences (a.k.a. linear-chain models) the Viterbi al-
gorithm can be used (Rabiner, 1989; Lafferty et al,
2001). Fortunately there is a simple technique that
reduces inference in the above model to sequence
classification with a constrained version of Viterbi.
2.1.1 Inference as Sequential Labeling
The inference problem is to find the highest scor-
ing labeling y for an input s, i.e.,
argmax
y
score(y, s)
If the document label yd is fixed, then inference
in the model from Figure 1 reduces to the sequen-
tial case. This is because the search space is only
over the sentence labels ysi , whose graphical struc-
ture forms a chain. Thus the problem of finding the
434
Input: s = s1, . . . , sn
1. y = null
2. for each yd ? Y(d)
3. ys = argmaxys score((y
d,ys), s)
4. y? = (yd,ys)
5. if score(y?, s) > score(y, s) or y = null
6. y = y?
7. return y
Figure 2: Inference algorithm for model in Figure 1.
The argmax in line 3 can be solved using Viterbi?s
algorithm since yd is fixed.
highest scoring sentiment labels for all sentences,
given a particular document label yd, can be solved
efficiently using Viterbi?s algorithm.
The general inference problem can then be solved
by iterating over each possible yd, finding ys max-
imizing score((yd,ys), s) and keeping the single
best y = (yd,ys). This algorithm is outlined in Fig-
ure 2 and has a runtime of O(|Y(d)||Y(s)|2n), due
to running Viterbi |Y(d)| times over a label space of
size |Y(s)|. The algorithm can be extended to pro-
duce exact k-best lists. This is achieved by using
k-best Viterbi techniques to return the k-best global
labelings for each document label in line 3. Merging
these sets will produce the final k-best list.
It is possible to view the inference algorithm in
Figure 2 as a constrained Viterbi search since it is
equivalent to flattening the model in Figure 1 to a
sequential model with sentence labels from the set
Y(s) ? Y(d). The resulting Viterbi search would
then need to be constrained to ensure consistent
solutions, i.e., the label assignments agree on the
document label over all sentences. If viewed this
way, it is also possible to run a constrained forward-
backward algorithm and learn the parameters for
CRFs as well.
2.1.2 Feature Space
In this section we define the feature representa-
tion for each clique, f(yd, ysi?1, y
s
i , s). Assume that
each sentence si is represented by a set of binary
predicates P(si). This set can contain any predicate
over the input s, but for the present purposes it will
include all the unigram, bigram and trigrams in
the sentence si conjoined with their part-of-speech
(obtained from an automatic classifier). Back-offs
of each predicate are also included where one or
more word is discarded. For instance, if P(si) con-
tains the predicate a:DT great:JJ product:NN,
then it would also have the predicates
a:DT great:JJ *:NN, a:DT *:JJ product:NN,
*:DT great:JJ product:NN, a:DT *:JJ *:NN, etc.
Each predicate, p, is then conjoined with the label
information to construct a binary feature. For exam-
ple, if the sentence label set is Y(s) = {subj, obj}
and the document set is Y(d) = {pos, neg}, then
the system might contain the following feature,
f(j)(y
d, ysi?1, y
s
i , s) =
?
?????
?????
1 if p ? P(si)
and ysi?1 = obj
and ysi = subj
and yd = neg
0 otherwise
Where f(j) is the jth dimension of the feature space.
For each feature, a set of back-off features are in-
cluded that only consider the document label yd, the
current sentence label ysi , the current sentence and
document label ysi and y
d, and the current and pre-
vious sentence labels ysi and y
s
i?1. Note that through
these back-off features the joint models feature set
will subsume the feature set of any individual level
model. Only features observed in the training data
were considered. Depending on the data set, the di-
mension of the feature vector f ranged from 350K to
500K. Though the feature vectors can be sparse, the
feature weights will be learned using large-margin
techniques that are well known to be robust to large
and sparse feature representations.
2.1.3 Training the Model
Let Y = Y(d) ? Y(s)n be the set of all valid
sentence-document labelings for an input s. The
weights, w, are set using the MIRA learning al-
gorithm, which is an inference based online large-
margin learning technique (Crammer and Singer,
2003; McDonald et al, 2005). An advantage of this
algorithm is that it relies only on inference to learn
the weight vector (see Section 2.1.1). MIRA has
been shown to provide state-of-the-art accuracy for
many language processing tasks including parsing,
chunking and entity extraction (McDonald, 2006).
The basic algorithm is outlined in Figure 3. The
algorithm works by considering a single training in-
stance during each iteration. The weight vector w is
updated in line 4 through a quadratic programming
problem. This update modifies the weight vector so
435
Training data: T = {(yt, st)}Tt=1
1. w(0) = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. w(i+1) = argminw*
?
?
?w*? w(i)
?
?
?
s.t. score(yt, st)? score(y?, s) ? L(yt,y?)
relative to w*
?y? ? C ? Y , where |C| = k
5. i = i + 1
6. return w(N?T )
Figure 3: MIRA learning algorithm.
that the score of the correct labeling is larger than
the score of every labeling in a constraint set C with
a margin proportional to the loss. The constraint set
C can be chosen arbitrarily, but it is usually taken to
be the k labelings that have the highest score under
the old weight vector w(i) (McDonald et al, 2005).
In this manner, the learning algorithm can update its
parameters relative to those labelings closest to the
decision boundary. Of all the weight vectors that sat-
isfy these constraints, MIRA chooses the one that is
as close as possible to the previous weight vector in
order to retain information about previous updates.
The loss function L(y,y?) is a positive real val-
ued function and is equal to zero when y = y?. This
function is task specific and is usually the hamming
loss for sequence classification problems (Taskar et
al., 2003). Experiments with different loss functions
for the joint sentence-document model on a develop-
ment data set indicated that the hamming loss over
sentence labels multiplied by the 0-1 loss over doc-
ument labels worked best.
An important modification that was made to the
learning algorithm deals with how the k constraints
are chosen for the optimization. Typically these con-
straints are the k highest scoring labelings under the
current weight vector. However, early experiments
showed that the model quickly learned to discard
any labeling with an incorrect document label for
the instances in the training set. As a result, the con-
straints were dominated by labelings that only dif-
fered over sentence labels. This did not allow the al-
gorithm adequate opportunity to set parameters rel-
ative to incorrect document labeling decisions. To
combat this, k was divided by the number of doc-
ument labels, to get a new value k?. For each doc-
ument label, the k? highest scoring labelings were
Figure 4: An extension to the model from Figure 1
incorporating paragraph level analysis.
extracted. Each of these sets were then combined to
produce the final constraint set. This allowed con-
straints to be equally distributed amongst different
document labels.
Based on performance on the development data
set the number of training iterations was set to N =
5 and the number of constraints to k = 10. Weight
averaging was also employed (Collins, 2002), which
helped improve performance.
2.2 Beyond Two-Level Models
To this point, we have focused solely on a model for
two-level fine-to-coarse sentiment analysis not only
for simplicity, but because the experiments in Sec-
tion 3 deal exclusively with this scenario. In this
section, we briefly discuss possible extensions for
more complex situations. For example, longer doc-
uments might benefit from an analysis on the para-
graph level as well as the sentence and document
levels. One possible model for this case is given
in Figure 4, which essentially inserts an additional
layer between the sentence and document level from
the original model. Sentence level analysis is de-
pendent on neighbouring sentences as well as the
paragraph level analysis, and the paragraph anal-
ysis is dependent on each of the sentences within
it, the neighbouring paragraphs, and the document
level analysis. This can be extended to an arbitrary
level of fine-to-coarse sentiment analysis by simply
inserting new layers in this fashion to create more
complex hierarchical models.
The advantage of using hierarchical models of
this form is that they are nested, which keeps in-
ference tractable. Observe that each pair of adja-
cent levels in the model is equivalent to the origi-
nal model from Figure 1. As a result, the scores
of the every label at each node in the graph can
be calculated with a straight-forward bottom-up dy-
namic programming algorithm. Details are omitted
436
Sentence Stats Document Stats
Pos Neg Neu Tot Pos Neg Tot
Car 472 443 264 1179 98 80 178
Fit 568 635 371 1574 92 97 189
Mp3 485 464 214 1163 98 89 187
Tot 1525 1542 849 3916 288 266 554
Table 1: Data statistics for corpus. Pos = positive
polarity, Neg = negative polarity, Neu = no polarity.
for space reasons.
Other models are possible where dependencies
occur across non-neighbouring levels, e.g., by in-
serting edges between the sentence level nodes and
the document level node. In the general case, infer-
ence is exponential in the size of each clique. Both
the models in Figure 1 and Figure 4 have maximum
clique sizes of three.
3 Experiments
3.1 Data
To test the model we compiled a corpus of 600 on-
line product reviews from three domains: car seats
for children, fitness equipment, and Mp3 players. Of
the original 600 reviews that were gathered, we dis-
carded duplicate reviews, reviews with insufficient
text, and spam. All reviews were labeled by on-
line customers as having a positive or negative polar-
ity on the document level, i.e., Y(d) = {pos, neg}.
Each review was then split into sentences and ev-
ery sentence annotated by a single annotator as ei-
ther being positive, negative or neutral, i.e., Y(s) =
{pos, neg, neu}. Data statistics for the corpus are
given in Table 1.
All sentences were annotated based on their con-
text within the document. Sentences were anno-
tated as neutral if they conveyed no sentiment or had
indeterminate sentiment from their context. Many
neutral sentences pertain to the circumstances un-
der which the product was purchased. A common
class of sentences were those containing product
features. These sentences were annotated as having
positive or negative polarity if the context supported
it. This could include punctuation such as excla-
mation points, smiley/frowny faces, question marks,
etc. The supporting evidence could also come from
another sentence, e.g., ?I love it. It has 64Mb of
memory and comes with a set of earphones?.
3.2 Results
Three baseline systems were created,
? Document-Classifier is a classifier that learns
to predict the document label only.
? Sentence-Classifier is a classifier that learns
to predict sentence labels in isolation of one
another, i.e., without consideration for either
the document or neighbouring sentences sen-
timent.
? Sentence-Structured is another sentence clas-
sifier, but this classifier uses a sequential chain
model to learn and classify sentences. The
third baseline is essentially the model from Fig-
ure 1 without the top level document node. This
baseline will help to gage the empirical gains of
the different components of the joint structured
model on sentence level classification.
The model described in Section 2 will be called
Joint-Structured. All models use the same ba-
sic predicate space: unigram, bigram, trigram con-
joined with part-of-speech, plus back-offs of these
(see Section 2.1.2 for more). However, due to the
structure of the model and its label space, the feature
space of each might be different, e.g., the document
classifier will only conjoin predicates with the doc-
ument label to create the feature set. All models are
trained using the MIRA learning algorithm.
Results for each model are given in the first four
rows of Table 2. These results were gathered using
10-fold cross validation with one fold for develop-
ment and the other nine folds for evaluation. This
table shows that classifying sentences in isolation
from one another is inferior to accounting for a more
global context. A significant increase in perfor-
mance can be obtained when labeling decisions be-
tween sentences are modeled (Sentence-Structured).
More interestingly, even further gains can be had
when document level decisions are modeled (Joint-
Structured). In many cases, these improvements are
highly statistically significant.
On the document level, performance can also be
improved by incorporating sentence level decisions
? though these improvements are not consistent.
This inconsistency may be a result of the model
overfitting on the small set of training data. We
437
suspect this because the document level error rate
on the Mp3 training set converges to zero much
more rapidly for the Joint-Structured model than the
Document-Classifier. This suggests that the Joint-
Structured model might be relying too much on
the sentence level sentiment features ? in order to
minimize its error rate ? instead of distributing the
weights across all features more evenly.
One interesting application of sentence level sen-
timent analysis is summarizing product reviews on
retail websites like Amazon.com or review aggrega-
tors like Yelp.com. In this setting the correct polar-
ity of a document is often known, but we wish to
label sentiment on the sentence or phrase level to
aid in generating a cohesive and informative sum-
mary. The joint model can be used to classify sen-
tences in this setting by constraining inference to the
known fixed document label for a review. If this is
done, then sentiment accuracy on the sentence level
increases substantially from 62.6% to 70.3%.
Finally we should note that experiments using
CRFs to train the structured models and logistic re-
gression to train the local models yielded similar re-
sults to those in Table 2.
3.2.1 Cascaded Models
Another approach to fine-to-coarse sentiment
analysis is to use a cascaded system. In such a sys-
tem, a sentence level classifier might first be run
on the data, and then the results input into a docu-
ment level classifier ? or vice-versa.1 Two cascaded
systems were built. The first uses the Sentence-
Structured classifier to classify all the sentences
from a review, then passes this information to the
document classifier as input. In particular, for ev-
ery predicate in the original document classifier, an
additional predicate that specifies the polarity of the
sentence in which this predicate occurred was cre-
ated. The second cascaded system uses the docu-
ment classifier to determine the global polarity, then
passes this information as input into the Sentence-
Structured model, constructing predicates in a simi-
lar manner.
The results for these two systems can be seen in
the last two rows of Table 2. In both cases there
1Alternatively, decisions from the sentence classifier can
guide which input is seen by the document level classifier (Pang
and Lee, 2004).
is a slight improvement in performance suggesting
that an iterative approach might be beneficial. That
is, a system could start by classifying documents,
use the document information to classify sentences,
use the sentence information to classify documents,
and repeat until convergence. However, experiments
showed that this did not improve accuracy over a sin-
gle iteration and often hurt performance.
Improvements from the cascaded models are far
less consistent than those given from the joint struc-
ture model. This is because decisions in the cas-
caded system are passed to the next layer as the
?gold? standard at test time, which results in errors
from the first classifier propagating to errors in the
second. This could be improved by passing a lattice
of possibilities from the first classifier to the second
with corresponding confidences. However, solutions
such as these are really just approximations of the
joint structured model that was presented here.
4 Future Work
One important extension to this work is to augment
the models for partially labeled data. It is realistic
to imagine a training set where many examples do
not have every level of sentiment annotated. For
example, there are thousands of online product re-
views with labeled document sentiment, but a much
smaller amount where sentences are also labeled.
Work on learning with hidden variables can be used
for both CRFs (Quattoni et al, 2004) and for in-
ference based learning algorithms like those used in
this work (Liang et al, 2006).
Another area of future work is to empirically in-
vestigate the use of these models on longer docu-
ments that require more levels of sentiment anal-
ysis than product reviews. In particular, the rela-
tive position of a phrase to a contrastive discourse
connective or a cue phrase like ?in conclusion? or
?to summarize? may lead to improved performance
since higher level classifications can learn to weigh
information passed from these lower level compo-
nents more heavily.
5 Discussion
In this paper we have investigated the use of a global
structured model that learns to predict sentiment on
different levels of granularity for a text. We de-
438
Sentence Accuracy Document Accuracy
Car Fit Mp3 Total Car Fit Mp3 Total
Document-Classifier - - - - 72.8 80.1 87.2 80.3
Sentence-Classifier 54.8 56.8 49.4 53.1 - - - -
Sentence-Structured 60.5 61.4 55.7 58.8 - - - -
Joint-Structured 63.5? 65.2?? 60.1?? 62.6?? 81.5? 81.9 85.0 82.8
Cascaded Sentence ? Document 60.5 61.4 55.7 58.8 75.9 80.7 86.1 81.1
Cascaded Document ? Sentence 59.7 61.0 58.3 59.5 72.8 80.1 87.2 80.3
Table 2: Fine-to-coarse sentiment accuracy. Significance calculated using McNemar?s test between top two
performing systems. ?Statistically significant p < 0.05. ??Statistically significant p < 0.005.
scribed a simple model for sentence-document anal-
ysis and showed that inference in it is tractable. Ex-
periments show that this model obtains higher ac-
curacy than classifiers trained in isolation as well
as cascaded systems that pass information from one
level to another at test time. Furthermore, extensions
to the sentence-document model were discussed and
it was argued that a nested hierarchical structure
would be beneficial since it would allow for efficient
inference algorithms.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In Proc. HLT/EMNLP.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint extraction of enti-
ties and relations for opinion recognition. In Proc. EMNLP.
M. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative online
algorithms for multiclass problems. JMLR.
Hal Daume? III, John Langford, and Daniel Marcu. 2006.
Search-based structured prediction. In Submission.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar. 2006. An
end-to-end discriminative approach to machine translation.
In Proc. ACL.
Y. Mao and G. Lebanon. 2006. Isotonic conditional random
fields and local sentiment flow. In Proc. NIPS.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proc. ACL.
R. McDonald. 2006. Discriminative Training and Spanning
Tree Algorithms for Dependency Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel. 2000.
A novel use of statistical parsing to extract information from
text. In Proc NAACL, pages 226?233.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proc. ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine learning techniques.
In EMNLP.
A. Popescu and O. Etzioni. 2005. Extracting product features
and opinions from reviews. In Proc. HLT/EMNLP.
A. Quattoni, M. Collins, and T. Darrell. 2004. Conditional
random fields for object recognition. In Proc. NIPS.
L. R. Rabiner. 1989. A tutorial on hidden Markov models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?285, February.
D. Roth and W. Yih. 2004. A linear programming formula-
tion for global inference in natural language tasks. In Proc.
CoNLL.
C. Sutton and A. McCallum. 2006. An introduction to con-
ditional random fields for relational learning. In L. Getoor
and B. Taskar, editors, Introduction to Statistical Relational
Learning. MIT Press.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dy-
namic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. In Proc.
ICML.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In Proc. EMNLP.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote:
Determining support or opposition from congressional floor-
debate transcripts. In Proc. EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004.
Support vector learning for interdependent and structured
output spaces. In Proc. ICML.
P. Turney. 2002. Thumbs up or thumbs down? Sentiment ori-
entation applied to unsupervised classification of reviews. In
EMNLP.
439
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 777?785,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The viability of web-derived polarity lexicons
Leonid Velikovich Sasha Blair-Goldensohn Kerry Hannan Ryan McDonald
Google Inc., New York, NY
{leonidv|sasha|khannan|ryanmcd}@google.com
Abstract
We examine the viability of building large
polarity lexicons semi-automatically from the
web. We begin by describing a graph propa-
gation framework inspired by previous work
on constructing polarity lexicons from lexi-
cal graphs (Kim and Hovy, 2004; Hu and
Liu, 2004; Esuli and Sabastiani, 2009; Blair-
Goldensohn et al, 2008; Rao and Ravichan-
dran, 2009). We then apply this technique
to build an English lexicon that is signifi-
cantly larger than those previously studied.
Crucially, this web-derived lexicon does not
require WordNet, part-of-speech taggers, or
other language-dependent resources typical of
sentiment analysis systems. As a result, the
lexicon is not limited to specific word classes
? e.g., adjectives that occur in WordNet ?
and in fact contains slang, misspellings, multi-
word expressions, etc. We evaluate a lexicon
derived from English documents, both qual-
itatively and quantitatively, and show that it
provides superior performance to previously
studied lexicons, including one derived from
WordNet.
1 Introduction
Polarity lexicons are large lists of phrases that en-
code the polarity of each phrase within it ? either
positive or negative ? often with some score rep-
resenting the magnitude of the polarity (Hatzivas-
siloglou and McKeown, 1997; Wiebe, 2000; Turney,
2002). Though classifiers built with machine learn-
ing algorithms have become commonplace in the
sentiment analysis literature, e.g., Pang et al (2002),
the core of many academic and commercial senti-
ment analysis systems remains the polarity lexicon,
which can be constructed manually (Das and Chen,
2007), through heuristics (Kim and Hovy, 2004;
Esuli and Sabastiani, 2009) or using machine learn-
ing (Turney, 2002; Rao and Ravichandran, 2009).
Often lexicons are combined with machine learning
for improved results (Wilson et al, 2005). The per-
vasiveness and sustained use of lexicons can be as-
cribed to a number of reasons, including their inter-
pretability in large-scale systems as well as the gran-
ularity of their analysis.
In this work we investigate the viability of polar-
ity lexicons that are derived solely from unlabeled
web documents. We propose a method based on
graph propagation algorithms inspired by previous
work on constructing polarity lexicons from lexical
graphs (Kim and Hovy, 2004; Hu and Liu, 2004;
Esuli and Sabastiani, 2009; Blair-Goldensohn et al,
2008; Rao and Ravichandran, 2009). Whereas past
efforts have used linguistic resources ? e.g., Word-
Net ? to construct the lexical graph over which prop-
agation runs, our lexicons are constructed using a
graph built from co-occurrence statistics from the
entire web. Thus, the method we investigate can
be seen as a combination of methods for propagat-
ing sentiment across lexical graphs and methods for
building sentiment lexicons based on distributional
characteristics of phrases in raw data (Turney, 2002).
The advantage of breaking the dependence on Word-
Net (or related resources like thesauri (Mohammad
et al, 2009)) is that it allows the lexicons to include
non-standard entries, most notably spelling mistakes
and variations, slang, and multiword expressions.
The primary goal of our study is to understand the
characteristics and practical usefulness of such a lex-
icon. Towards this end, we provide both a qualitative
and quantitative analysis for a web-derived English
777
lexicon relative to two previously published lexicons
? the lexicon used in Wilson et al (2005) and the
lexicon used in Blair-Goldensohn et al (2008). Our
experiments show that a web-derived lexicon is not
only significantly larger, but has improved accuracy
on a sentence polarity classification task, which is
an important problem in many sentiment analysis
applications, including sentiment aggregation and
summarization (Hu and Liu, 2004; Carenini et al,
2006; Lerman et al, 2009). These results hold true
both when the lexicons are used in conjunction with
string matching to classify sentences, and when they
are included within a contextual classifier frame-
work (Wilson et al, 2005).
Extracting polarity lexicons from the web has
been investigated previously by Kaji and Kitsure-
gawa (2007), who study the problem exclusively for
Japanese. In that work a set of positive/negative sen-
tences are first extracted from the web using cues
from a syntactic parser as well as the document
structure. Adjectives phrases are then extracted from
these sentences based on different statistics of their
occurrence in the positive or negative set. Our work,
on the other hand, does not rely on syntactic parsers
or restrict the set of candidate lexicon entries to spe-
cific syntactic classes, i.e., adjective phrases. As a
result, the lexicon built in our study is on a different
scale than that examined in Kaji and Kitsuregawa
(2007). Though this hypothesis is not tested here, it
also makes our techniques more amenable to adap-
tation for other languages.
2 Constructing the Lexicon
In this section we describe a method to construct po-
larity lexicons using graph propagation over a phrase
similarity graph constructed from the web.
2.1 Graph Propagation Algorithm
We construct our lexicon using graph propagation
techniques, which have previously been investigated
in the construction of polarity lexicons (Kim and
Hovy, 2004; Hu and Liu, 2004; Esuli and Sabas-
tiani, 2009; Blair-Goldensohn et al, 2008; Rao and
Ravichandran, 2009). We assume as input an undi-
rected edge weighted graph G = (V,E), where
wij ? [0, 1] is the weight of edge (vi, vj) ? E. The
node set V is the set of candidate phrases for inclu-
sion in a sentiment lexicon. In practice,G should en-
code semantic similarities between two nodes, e.g.,
for sentiment analysis one would hope that wij >
wik if vi=good, vj=great and vk=bad. We also as-
sume as input two sets of seed phrases, denoted P
for the positive seed set and N for the negative seed
set. The common property among all graph propaga-
tion algorithms is that they attempt to propagate in-
formation from the seed sets to the rest of the graph
through its edges. This can be done using machine
learning, graph algorithms or more heuristic means.
The specific algorithm used in this study is given
in Figure 1, which is distinct from common graph
propagation algorithms, e.g., label propagation (see
Section 2.3). The output is a polarity vector pol ?
R|V | such that poli is the polarity score for the i
th
candidate phrase (or the ith node inG). In particular,
we desire pol to have the following semantics:
poli =
?
??
??
> 0 ith phrase has positive polarity
< 0 ith phrase has negative polarity
= 0 ith phrase has no sentiment
Intuitively, the algorithm works by computing both
a positive and a negative polarity magnitude for
each node in the graph, call them pol+i and pol
-
i.
These values are equal to the sum over the max
weighted path from every seed word (either posi-
tive or negative) to node vi. Phrases that are con-
nected to multiple positive seed words through short
yet highly weighted paths will receive high positive
values. The final polarity of a phrase is then set to
poli = pol
+
i ? ?pol
-
i, where ? a constant meant to
account for the difference in overall mass of positive
and negative flow in the graph. Thus, after the al-
gorithm is run, if a phrase has a higher positive than
negative polarity score, then its final polarity will be
positive, and negative otherwise.
There are some implementation details worth
pointing out. First, the algorithm in Figure 1 is writ-
ten in an iterative framework, where on each itera-
tion, paths of increasing lengths are considered. The
input variable T controls the max path length con-
sidered by the algorithm. This can be set to be a
small value in practice, since the multiplicative path
weights result in long paths rarely contributing to
polarity scores. Second, the parameter ? is a thresh-
old that defines the minimum polarity magnitude a
778
Input: G = (V,E), wij ? [0, 1],
P , N , ? ? R, T ? N
Output: pol ? R|V |
Initialize: poli,pol
+
i ,pol
-
i = 0, for all i
pol+i = 1.0 for all vi ? P and
pol-i = 1.0 for all vi ? N
1. set ?ij = 0 for all i, j
2. for vi ? P
3. F = {vi}
4. for t : 1 . . . T
5. for (vk, vj) ? E such that vk ? F
6. ?ij = max{?ij , ?ik ? wkj}
F = F ? {vj}
7. for vj ? V
8. pol+j =
?
vi?P
?ij
9. Repeat steps 1-8 using N to compute pol-
10. ? =
?
i pol
+
i /
?
i pol
-
i
11. poli = pol
+
i ? ?pol
-
i, for all i
12. if |poli| < ? then poli = 0.0, for all i
Figure 1: Graph Propagation Algorithm.
phrase must have to be included in the lexicon. Both
T and ? were tuned on held-out data.
To construct the final lexicon, the remaining
nodes ? those with polarity scores above ? ? are ex-
tracted and assigned their corresponding polarity.
2.2 Building a Phrase Graph from the Web
Graph propagation algorithms rely on the existence
of graphs that encode meaningful relationships be-
tween candidate nodes. Past studies on building po-
larity lexicons have used linguistic resources like
WordNet to define the graph through synonym and
antonym relations (Kim and Hovy, 2004; Esuli and
Sabastiani, 2009; Blair-Goldensohn et al, 2008;
Rao and Ravichandran, 2009). The goal of this study
is to examine the size and quality of polarity lexi-
cons when the graph is induced automatically from
documents on the web.
Constructing a graph from web-computed lexi-
cal co-occurrence statistics is a difficult challenge
in and of itself and the research and implementa-
tion hurdles that arise are beyond the scope of this
work (Alfonseca et al, 2009; Pantel et al, 2009).
For this study, we used an English graph where the
node set V was based on all n-grams up to length
10 extracted from 4 billion web pages. This list was
filtered to 20 million candidate phrases using a num-
ber of heuristics including frequency and mutual in-
formation of word boundaries. A context vector for
each candidate phrase was then constructed based
on a window of size six aggregated over all men-
tions of the phrase in the 4 billion documents. The
edge set E was constructed by first, for each po-
tential edge (vi, vj), computing the cosine similar-
ity value between context vectors. All edges (vi, vj)
were then discarded if they were not one of the 25
highest weighted edges adjacent to either node vi or
vj . This serves to both reduce the size of the graph
and to eliminate many spurious edges for frequently
occurring phrases, while still keeping the graph rela-
tively connected. The weight of the remaining edges
was set to the corresponding cosine similarity value.
Since this graph encodes co-occurrences over a
large, but local context window, it can be noisy for
our purposes. In particular, we might see a number
of edges between positive and negative sentiment
words as well as sentiment words and non-sentiment
words, e.g., sentiment adjectives and all other adjec-
tives that are distributionally similar. Larger win-
dows theoretically alleviate this problem as they en-
code semantic as opposed to syntactic similarities.
We note, however, that the graph propagation al-
gorithm described above calculates the sentiment of
each phrase as the aggregate of all the best paths to
seed words. Thus, even if some local edges are erro-
neous in the graph, one hopes that, globally, positive
phrases will be influenced more by paths from pos-
itive seed words as opposed to negative seed words.
Section 3, and indeed this paper, aims to measure
whether this is true or not.
2.3 Why Not Label Propagation?
Previous studies on constructing polarity lexicons
from lexical graphs, e.g., Rao and Ravichandran
(2009), have used the label propagation algorithm,
which takes the form in Figure 2 (Zhu and Ghahra-
mani, 2002). Label propagation is an iterative algo-
rithm where each node takes on the weighted aver-
age of its neighbour?s values from the previous iter-
ation. The result is that nodes with many paths to
seeds get high polarities due to the influence from
their neighbours. The label propagation algorithm
is known to have many desirable properties includ-
ing convergence, a well defined objective function
779
Input: G = (V,E), wij ? [0, 1], P , N
Output: pol ? R|V |
Initialize: poli = 1.0 for all vi ? P and
poli = ?1.0 for all vi ? N and
poli = 0.0 ?vi /? P ?N
1. for : t .. T
2. poli =
P
(vi,vj)?E
wij?polj
P
(vi,vj)
wij
, ?vi ? V
3. reset poli = 1.0 ?vi ? P
reset poli = ?1.0 ?vi ? N
Figure 2: The label propagation algorithm (Zhu and
Ghahramani, 2002).
(minimize squared error between values of adjacent
nodes), and an equivalence to computing random
walks through graphs.
The primary difference between standard label
propagation and the graph propagation algorithm
given in Section 2.1, is that a node with multiple
paths to a seed will be influenced by all these paths
in the label propagation algorithm, whereas only the
single path from a seed will influence the polarity
of a node in our proposed propagation algorithm ?
namely the path with highest weight. The intuition
behind label propagation seems justified. That is, if
a node has multiple paths to a seed, it should be re-
flected in a higher score. This is certainly true when
the graph is of high quality and all paths trustwor-
thy. However, in a graph constructed from web co-
occurrence statistics, this is rarely the case.
Our graph consisted of many dense subgraphs,
each representing some semantic entity class, such
as actors, authors, tech companies, etc. Problems
arose when polarity flowed into these dense sub-
graphs with the label propagation algorithm. Ulti-
mately, this flow would amplify since the dense sub-
graph provided exponentially many paths from each
node to the source of the flow, which caused a re-
inforcement effect. As a result, the lexicon would
consist of large groups of actor names, companies,
etc. This also led to convergence issues since the
polarity is divided proportional to the size of the
dense subgraph. Additionally, negative phrases in
the graph appeared to be in more densely connected
regions, which resulted in the final lexicons being
highly skewed towards negative entries due to the
influence of multiple paths to seed words.
For best path propagation, these problems were
less acute as each node in the dense subgraph would
only get the polarity a single time from each seed,
which is decayed by the fact that edge weights are
smaller than 1. Furthermore, the fact that edge
weights are less than 1 results in most long paths
having weights near zero, which in turn results in
fast convergence.
3 Lexicon Evaluation
We ran the best path graph propagation algorithm
over a graph constructed from the web using manu-
ally constructed positive and negative seed sets of
187 and 192 words in size, respectively. These
words were generated by a set of five humans and
many are morphological variants of the same root,
e.g., excel/excels/excelled. The algorithm produced
a lexicon that contained 178,104 entries. Depending
on the threshold ? (see Figure 1), this lexicon could
be larger or smaller. As stated earlier, our selection
of ? and all hyperparameters was based on manual
inspection of the resulting lexicons and performance
on held-out data.
In the rest of this section we investigate the prop-
erties of this lexicon to understand both its general
characteristics as well as its possible utility in sen-
timent applications. To this end we compare three
different lexicons:
1. Wilson et al: Described in Wilson et al
(2005). Lexicon constructed by combining the
lexicon built in Riloff and Wiebe (2003) with
other sources1. Entries are are coarsely rated
? strong/weak positive/negative ? which we
weighted as 1.0, 0.5, -0.5, and -1.0 for our ex-
periments.
2. WordNet LP: Described in Blair-Goldensohn
et al (2008). Constructed using label propaga-
tion over a graph derived from WordNet syn-
onym and antonym links. Note that label prop-
agation is not prone to the kinds of errors dis-
cussed in Section 2.3 since the lexical graph is
derived from a high quality source.
3. Web GP: The web-derived lexicon described
in Section 2.1 and Section 2.2.
1See http://www.cs.pitt.edu/mpqa/
780
3.1 Qualitative Evaluation
Table 1 breaks down the lexicon by the number of
positive and negative entries of each lexicon, which
clearly shows that the lexicon derived from the web
is more than an order of magnitude larger than pre-
viously constructed lexicons.2 This in and of it-
self is not much of an achievement if the additional
phrases are of poor quality. However, in Section 3.2
we present an empirical evaluation that suggests that
these terms provide both additional and useful in-
formation. Table 1 also shows the recall of the each
lexicon relative to the other. Whereas the Wilson
et al (2005) and WordNet lexicon have a recall of
only 3% relative to the web lexicon, the web lexi-
con has a recall of 48% and 70% relative to the two
other lexicons, indicating that it contains a signifi-
cant amount of information from the other lexicons.
However, this overlap is still small, suggesting that
a combination of all the lexicons could provide the
best performance. In Section 3.2 we investigate this
empirically through a meta classification system.
Table 2 shows the distribution of phrases in the
web-derived lexicon relative to the number of to-
kens in each phrase. Here a token is simply defined
by whitespace and punctuation, with punctuation
counting as a token, e.g., ?half-baked? is counted as
3 tokens. For the most part, we see what one might
expect, as the number of tokens increases, the num-
ber of corresponding phrases in the lexicon also de-
creases. Longer phrases are less frequent and thus
will have both fewer and lower weighted edges to
adjacent nodes in the graph. There is a single phrase
of length 9, which is ?motion to dismiss for failure
to state a claim?. In fact, the lexicon contains quite
a number of legal and medical phrases. This should
not be surprising, since in a graph induced from the
web, a phrase like ?cancer? (or any disease) should
be distributionally similar to phrases like ?illness?,
?sick?, and ?death?, which themselves will be simi-
lar to standard sentiment phrases like ?bad? and ?ter-
rible?. These terms are predominantly negative in
the lexicon representing the broad notion that legal
and medical events are undesirable.
2This also includes the web-derived lexicon of (Kaji and Kit-
suregawa, 2007), which has 10K entries. A recent study by
Mohammad et al (2009) generated lexicons from thesauri with
76K entries.
Phrase length 1 2 3
# of phrases 37,449 108,631 27,822
Phrase length 4 5 6 7 8 9
# of phrases 3,489 598 71 29 4 1
Table 2: Number of phrases by phrase length in lexicon
built from the web.
Perhaps the most interesting characteristic of the
lexicon is that the most frequent phrase length is 2
and not 1. The primary reason for this is an abun-
dance of adjective phrases consisting of an adverb
and an adjective, such as ?more brittle? and ?less
brittle?. Almost every adjective of length 1 is fre-
quently combined in such a way on the web, so it
not surprising that we see many of these phrases
in the lexicon. Ideally we would see an order on
such phrases, e.g., ?more brittle? has a larger neg-
ative polarity than ?brittle?, which in turn has a
larger negative polarity than ?less brittle?. However,
this is rarely the case and usually the adjective has
the highest polarity magnitude. Again, this is eas-
ily explained. These phrases are necessarily more
common and will thus have more edges with larger
weights in the graph and thus a greater chance of ac-
cumulating a high sentiment score. The prominence
of such phrases suggests that a more principled treat-
ment of them should be investigated in the future.
Finally, Table 3 presents a selection of phrases
from both the positive and negative lexicons cate-
gorized into revealing verticals. For both positive
and negative phrases we present typical examples of
phrases ? usually adjectives ? that one would expect
to be in a sentiment lexicon. These are phrases not
included in the seed sets. We also present multiword
phrases for both positive and negative cases, which
displays concretely the advantage of building lexi-
cons from the web as opposed to using restricted lin-
guistic resources such as WordNet. Finally, we show
two special cases. The first is spelling variations
(and mistakes) for positive phrases, which were far
more prominent than for negative phrases. Many of
these correspond to social media text where one ex-
presses an increased level of sentiment by repeat-
ing characters. The second is vulgarity in negative
phrases, which was far more prominent than for pos-
itive phrases. Some of these are clearly appropri-
781
Recall wrt other lexicons
All Phrases Pos. Phrases Neg. Phrases Wilson et al WordNet LP Web GP
Wilson et al 7,628 2,718 4,910 100% 37% 2%
WordNet LP 12,310 5,705 6,605 21% 100% 3%
Web GP 178,104 90,337 87,767 70% 48% 100%
Table 1: Lexicon statistics. Wilson et al is the lexicon used in Wilson et al (2005), WordNet LP is the lexicon
constructed by Blair-Goldensohn et al (2008) that uses label propagation algorithms over a graph constructed through
WordNet, and Web GP is the web-derived lexicon from this study.
POSITIVE PHRASES NEGATIVE PHRASES
Typical Multiword expressions Spelling variations Typical Multiword expressions Vulgarity
cute once in a life time loveable dirty run of the mill fucking stupid
fabulous state - of - the - art nicee repulsive out of touch fucked up
cuddly fail - safe operation niice crappy over the hill complete bullshit
plucky just what the doctor ordered cooool sucky flash in the pan shitty
ravishing out of this world coooool subpar bumps in the road half assed
spunky top of the line koool horrendous foaming at the mouth jackass
enchanting melt in your mouth kewl miserable dime a dozen piece of shit
precious snug as a bug cozy lousy pie - in - the - sky son of a bitch
charming out of the box cosy abysmal sick to my stomach sonofabitch
stupendous more good than bad sikk wretched pain in my ass sonuvabitch
Table 3: Example positive and negative phrases from web lexicon.
ate, e.g., ?shitty?, but some are clearly insults and
outbursts that are most likely included due to their
co-occurrence with angry texts. There were also a
number of derogatory terms and racial slurs in the
lexicon, again most of which received negative sen-
timent due to their typical disparaging usage.
3.2 Quantitative Evaluation
To determine the practical usefulness of a polarity
lexicon derived from the web, we measured the per-
formance of the lexicon on a sentence classifica-
tion/ranking task. The input is a set of sentences and
the output is a classification of the sentences as be-
ing either positive, negative or neutral in sentiment.
Additionally, the system outputs two rankings, the
first a ranking of the sentence by positive polarity
and the second a ranking of the sentence by negative
polarity. Classifying sentences by their sentiment is
a subtask of sentiment aggregation systems (Hu and
Liu, 2004; Gamon et al, 2005). Ranking sentences
by their polarity is a critical sub-task in extractive
sentiment summarization (Carenini et al, 2006; Ler-
man et al, 2009).
To classify sentences as being positive, negative
or neutral, we used an augmented vote-flip algo-
rithm (Choi and Cardie, 2009), which is given in
Figure 3. This intuition behind this algorithm is sim-
ple. The number of matched positive and negative
phrases from the lexicon are counted and whichever
has the most votes wins. The algorithm flips the de-
cision if the number of negations is odd. Though this
algorithm appears crude, it benefits from not relying
on threshold values for neutral classification, which
is difficult due to the fact that the polarity scores in
the three lexicons are not on the same scale.
To rank sentences we defined the purity of a sen-
tence X as the normalized sum of the sentiment
scores for each phrase x in the sentence:
purity(X) =
?
x?X polx
? +
?
x?X |polx|
This is a normalized score in the range [?1, 1]. In-
tuitively, sentences with many terms of the same po-
larity will have purity scores at the extreme points of
the range. Before calculating purity, a simple nega-
tion heuristic was implemented that reversed the
sentiment scores of terms that were within the scope
of negations. The term ? helps to favor sentences
with multiple phrase matches. Purity is a common
metric used for ranking sentences for inclusion in
sentiment summaries (Lerman et al, 2009). Purity
and negative purity were used to rank sentences as
being positive and negative sentiment, respectively.
The data used in our initial English-only experi-
782
Lexicon Classifier Contextual Classifier
Positive Negative Positive Negative
P R AP P R AP P R AP P R AP
Wilson et al 56.4 61.8 60.8 58.1 39.0 59.7 74.5 70.3 76.2 80.7 70.1 81.2
WordNet LP 50.9 61.7 62.0 54.9 36.4 59.7 72.0 72.5 75.7 78.0 69.8 79.3
Web GP 57.7 65.1? 69.6? 60.3 42.9 68.5? 74.1 75.0? 79.9? 80.5 72.6? 82.9?
Meta Classifier - - - - - - 76.6? 74.7 81.2? 81.8? 72.2 84.1?
Table 4: Positive and negative precision (P), recall (R), and average precision (AP) for three lexicons using either
lexical matching or contextual classification strategies. ?Web GP is statistically significantly better than Wilson et al
and WordNet LP (p < 0.05). ?Meta Classifier is statistically significantly better than all other systems (p < 0.05).
Input: Scored lexicon pol, negation list NG,
input sentence X
Output: sentiment ? {POS, NEG, NEU}
1. set p, n, ng = 0
2. for x ? X
3. if polx > 0 then p++
4. else if polx < 0 then n++
5. else if x ? NG then ng++
6. flip = (ng % 2 == 1) //ng is odd
7. if (p > n & ?flip) ? (n > p & flip)
return POS
8. else if (p > n & flip) ? (n > p & ?flip)
return NEG
19. return NEU
Figure 3: Vote-flip algorithm (Choi and Cardie, 2009).
ments were a set of 554 consumer reviews described
in (McDonald et al, 2007). Each review was sen-
tence split and annotated by a human as being pos-
itive, negative or neutral in sentiment. This resulted
in 3,916 sentences, with 1,525, 1,542 and 849 posi-
tive, negative and neutral sentences, respectively.
The first six columns of Table 4 shows: 1) the pos-
itive/negative precision-recall of each lexicon-based
system where sentence classes were determined us-
ing the vote-flip algorithm, and 2) the average preci-
sion for each lexicon-based system where purity (or
negative purity) was used to rank sentences. Both
the Wilson et al and WordNet LP lexicons perform
at a similar level, with the former slightly better, es-
pecially in terms of precision. The web-derived lex-
icon, Web GP, outperforms the other two lexicons
across the board, in particular when looking at av-
erage precision, where the gains are near 10% ab-
solute. If we plot the precision-recall graphs using
purity to classify sentences ? as opposed to the vote-
flip algorithm, which only provides an unweighted
classification ? we can see that at almost all recall
levels the web-derived lexicon has superior preci-
sion to the other lexicons (Figure 4). Thus, even
though the web-derived lexicon is constructed from
a lexical graph that contains noise, the graph prop-
agation algorithms appear to be fairly robust to this
noise and are capable of producing large and accu-
rate polarity lexicons.
The second six columns of Table 4 shows the per-
formance of each lexicon as the core of a contextual
classifier (Wilson et al, 2005). A contextual classi-
fier is a machine learned classifier that predicts the
polarity of a sentence using features of that sentence
and its context. For our experiments, this was a max-
imum entropy classifier trained and evaluated us-
ing 10-fold cross-validation on the evaluation data.
The features included in the classifier were the pu-
rity score, the number of positive and negative lex-
icon matches, and the number of negations in the
sentence, as well as concatenations of these features
within the sentence and with the same features de-
rived from the sentences in a window of size 1.
For each sentence, the contextual classifier pre-
dicted either a positive, negative or neutral classifi-
cation based on the label with highest probability.
Additionally, all sentences were placed in the posi-
tive and negative sentence rankings by the probabil-
ity the classifier assigned to the positive and negative
classes, respectively. Mirroring the results of Wil-
son et al (2005), we see that contextual classifiers
improve results substantially over lexical matching.
More interestingly, we see that the a contextual clas-
sifier over the web-derived lexicons maintains the
performance edge over the other lexicons, though
the gap is smaller. Figure 5 plots the precision-recall
curves for the positive and negative sentence rank-
783
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et alWordNet LPWeb GP
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et alWordNet LPWeb GP
Figure 4: Lexicon classifier precision/recall curves for positive (left) and negative (right) classes.
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et al CCWordNet LP CCWeb GP CCMeta Classifier
0 0.2 0.4 0.6 0.8 1Recall
0.4
0.5
0.6
0.7
0.8
0.9
1
Prec
ision
Wilson et al CCWordNet LP CCWeb GP CCMeta Classifier
Figure 5: Contextual classifier precision/recall curves for positive (left) and negative (right) classes
ings, again showing that at almost every level of re-
call, the web-derived lexicon has higher precision.
For a final English experiment we built a meta-
classification system that is identical to the contex-
tual classifiers, except it is trained using features de-
rived from all lexicons. Results are shown in the
last row of Table 4 and precision-recall curves are
shown in Figure 5. Not surprisingly, this system has
the best performance in terms of average precision
as it has access to the largest amount of information,
though its performance is only slightly better than
the contextual classifier for the web-derived lexicon.
4 Conclusions
In this paper we examined the viability of senti-
ment lexicons learned semi-automatically from the
web, as opposed to those that rely on manual anno-
tation and/or resources such as WordNet. Our quali-
tative experiments indicate that the web derived lex-
icon can include a wide range of phrases that have
not been available to previous systems, most no-
tably spelling variations, slang, vulgarity, and multi-
word expressions. Quantitatively, we observed that
the web derived lexicon had superior performance
to previously published lexicons for English clas-
sification. Ultimately, a meta classifier that incor-
porates features from all lexicons provides the best
performance. In the future we plan to investigate the
construction of web-derived lexicons for languages
other than English, which is an active area of re-
search (Mihalcea et al, 2007; Jijkoun and Hofmann,
2009; Rao and Ravichandran, 2009). The advantage
of the web-derived lexicons studied here is that they
do not rely on language specific resources besides
unlabeled data and seed lists. A primary question is
whether such lexicons improve performance over a
translate-to-English strategy (Banea et al, 2008).
Acknowledgements: The authors thank Andrew
Hogue, Raj Krishnan and Deepak Ravichandran for
insightful discussions about this work.
784
References
E. Alfonseca, K. Hall, and S. Hartmann. 2009. Large-
scale computation of distributional similarities for
queries. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT).
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. 2008.
Multilingual subjectivity analysis using machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
S. Blair-Goldensohn, K. Hannan, R. McDonald, T. Ney-
lon, G.A. Reis, and J. Reynar. 2008. Building a senti-
ment summarizer for local service reviews. In NLP in
the Information Explosion Era.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-document
summarization of evaluative text. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Y. Choi and C. Cardie. 2009. Adapting a polarity lexicon
using integer linear programming for domain-specific
sentiment classification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
S.R. Das and M.Y. Chen. 2007. Yahoo! for Amazon:
Sentiment extraction from small talk on the web. Man-
agement Science, 53(9):1375?1388.
A Esuli and F. Sabastiani. 2009. SentiWordNet: A pub-
licly available lexical resource for opinion mining. In
Proceedings of the Language Resource and Evaluation
Conference (LREC).
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proceedings of the 6th International Symposium on
Intelligent Data Analysis (IDA).
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics (EACL).
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proceedings of the International
Conference on Knowledge Discovery and Data Min-
ing (KDD).
V.B. Jijkoun and K. Hofmann. 2009. Generating a non-
english subjectivity lexicon: Relations that matter. In
Proceedings of the European Chapter of the Associa-
tion for Computational Linguistics (EACL).
N. Kaji and M. Kitsuregawa. 2007. Building lexicon for
sentiment analysis from massive collection of HTML
documents. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL).
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of the International
Conference on Computational Linguistics (COLING).
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: Evaluat-
ing and learning user preferences. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured models for fine-to-coarse
sentiment analysis. In Proceedings of the Annual Con-
ference of the Association for Computational Linguis-
tics (ACL).
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Annual Conference of
the Association for Computational Linguistics (ACL).
S. Mohammad, B. Dorr, and C. Dunne. 2009. Generat-
ing high-coverage semantic orientation lexicons from
overtly marked words and a thesaurus. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learn-
ing techniques. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
D. Rao and D. Ravichandran. 2009. Semi-Supervised
Polarity Lexicon Induction. In Proceedings of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL).
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
P. Turney. 2002. Thumbs up or thumbs down? Sentiment
orientation applied to unsupervised classification of re-
views. In Proceedings of the Annual Conference of the
Association for Computational Linguistics (ACL).
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
report, CMU CALD tech report CMU-CALD-02.
785
