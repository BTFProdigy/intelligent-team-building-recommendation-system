Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1669?1679,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Lightly Supervised Learning of Procedural Dialog Systems
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Pallavi Choudhury, Chris Quirk, Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
pallavic,chrisq,
billdol@microsoft.com
Luke Zettlemoyer
Computer Science and Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Procedural dialog systems can help users
achieve a wide range of goals. However,
such systems are challenging to build,
currently requiring manual engineering of
substantial domain-specific task knowl-
edge and dialog management strategies. In
this paper, we demonstrate that it is pos-
sible to learn procedural dialog systems
given only light supervision, of the type
that can be provided by non-experts. We
consider domains where the required task
knowledge exists in textual form (e.g., in-
structional web pages) and where system
builders have access to statements of user
intent (e.g., search query logs or dialog
interactions). To learn from such tex-
tual resources, we describe a novel ap-
proach that first automatically extracts task
knowledge from instructions, then learns a
dialog manager over this task knowledge
to provide assistance. Evaluation in a Mi-
crosoft Office domain shows that the indi-
vidual components are highly accurate and
can be integrated into a dialog system that
provides effective help to users.
1 Introduction
Procedural dialog systems aim to assist users
with a wide range of goals. For example, they
can guide visitors through a museum (Traum et
al., 2012; Aggarwal et al, 2012), teach students
physics (Steinhauser et al, 2011; Dzikovska et
al., 2011), or enable interaction with a health care
U: ?I want to add page numbers and a title?
S: ?Top or Bottom of the page??
U: ?Top?
S: ?Please select page design from the tem-
plates? (*System shows drop down menu*)
U: *User selects from menu*
S: ?Enter header or footer content?
U: ?C.V.?
S: ?Task completed.?
Figure 1: An example dialog interaction between
a system (S) and user (U) that can be automatically
achieved by learning from instructional web page
and query click logs.
system (Morbini et al, 2012; Rizzo et al, 2011).
However, such systems are challenging to build,
currently requiring expensive, expert engineering
of significant domain-specific task knowledge and
dialog management strategies.
In this paper, we present a new approach for
learning procedural dialog systems from task-
oriented textual resources in combination with
light, non-expert supervision. Specifically, we as-
sume access to task knowledge in textual form
(e.g., instructional web pages) and examples of
user intent statements (e.g., search query logs or
dialog interactions). Such instructional resources
are available in many domains, ranging from
recipes that describe how to cook meals to soft-
ware help web pages that describe how to achieve
goals by interacting with a user interface.1
1ehow.com,wikianswers.com
1669
There are two key challenges: we must (1)
learn to convert the textual knowledge into a us-
able form and (2) learn a dialog manager that pro-
vides robust assistance given such knowledge. For
example, Figure 1 shows the type of task assis-
tance that we are targeting in the Microsoft Office
setting, where the system should learn from web
pages and search query logs. Our central contribu-
tion is to show that such systems can be built with-
out the help of knowledge engineers or domain ex-
perts. We present new approaches for both of our
core problems. First, we introduce a method for
learning to map instructions to tree representations
of the procedures they describe. Nodes in the tree
represent points of interaction with the questions
the system can ask the user, while edges represent
user responses. Next, we present an approach that
uses example user intent statements to simulate di-
alog interactions, and learns how to best map user
utterances to nodes in these induced dialog trees.
When combined, these approaches produce a com-
plete dialog system that can engage in conversa-
tions by automatically moving between the nodes
of a large collection of induced dialog trees.
Experiments in the Windows Office help do-
main demonstrate that it is possible to build an
effective end-to-end dialog system. We evaluate
the dialog tree construction and dialog manage-
ment components in isolation, demonstrating high
accuracy (in the 80-90% range). We also conduct
a small-scale user study which demonstrates that
users can interact productively with the system,
successfully completing over 80% of their tasks.
Even when the system does fail, it often does so in
a graceful way, for example by asking redundant
questions but still reaching the goal within a few
additional turns.
2 Overview of Approach
Our task-oriented dialog system understands user
utterances by mapping them to nodes in dialog
trees generated from instructional text. Figure 2
shows an example of a set of instructions and the
corresponding dialog tree. This section describes
the problems that we must solve to enable such in-
teractions, and outlines our approach for each.
Knowledge Acquisition We extract task knowl-
edge from instructional text (e.g., Figure 2, left)
that describes (1) actions to be performed, such
as clicking a button, and (2) places where input
is needed from the user, for example to enter the
contents of the footer or header they are trying to
create. We aim to convert this text into a form that
will enable a dialog system to automatically assist
with the described task. To this end, we construct
dialog trees (e.g., Figure 2, right) with nodes to
represent entire documents (labeled as topics t),
nodes to represent user goals or intents (g), and
system action nodes (a) that enable execution of
specific commands. Finally, each node has an as-
sociated system action as, which can prompt user
input (e.g., with the question ?Top or bottom of
the page??) and one or more user actions au that
represent possible responses. All nodes connect
to form a tree structure that follows the workflow
described in the document. Section 3 presents a
scalable approach for inducing dialog trees.
Dialog Management To understand user intent
and provide task assistance, we need a dialog man-
agement approach that specifies what the system
should do and say. We adopt a simple approach
that at all times maintains an index into a node in
a dialog tree. Each system utterance is then simply
the action as for that node. However, the key chal-
lenge comes in interpreting user utterances. After
each user statement, we must automatically up-
date our node index. At any point, the user can
state a general goal (e.g., ?I want to add page num-
bers?), refine their goal (e.g., ?in a footer?), or both
(e.g.,?I want to add page numbers in the footer?).
Users can also change their goals in the process of
completing the tasks.
We develop a simple classification approach
that is robust to these different types of user behav-
ior. Specifically, we learn classifiers that, given the
dialog interaction history, predict how to pick the
next tree node from the space of all nodes in the di-
alog trees that define the task knowledge. We iso-
late two specific cases, classifying initial user ut-
terances (Section 4) and classifying all subsequent
utterances (Section 5). This approach allows us to
isolate the difference in language for the two cases,
and bias the second case to prefer tree nodes near
the current one. The resulting approach allows for
significant flexibility in traversing the dialog trees.
Data and Evaluation We collected a large set of
such naturally-occurring web search queries that
resulted in a user click on a URL in the Microsoft
Office help domain.2 We found that queries longer
that 4-5 words often resembled natural language
utterances that could be used for dialog interac-
2http://office.microsoft.com
1670
Figure 2: An example instructional text paired with a section of the corresponding dialog tree.
tions, for example how do you add borders, how
can I add a footer, how to insert continuous page
numbers, and where is the header and footer.
We also collected instructional texts from the
web pages that describe how to solve 76 of the
most pressing user goals, as indicated by query
click log statistics. On average 1,000 user queries
were associated with each goal. To some extent
clickthroughs can be treated as a proxy for user
frustration; popular search targets probably repre-
sent user pain points.
3 Building Dialog Trees from
Instructions
Our first problem is to convert sets of instructions
for user goals to dialog trees, as shown in Figure
2. These goals are broadly grouped into topics
(instructional pages). In addition, we manually
associate each node in a dialog tree with a train-
ing set of 10 queries. For the 76 goals (246 in-
structions) in our data, this annotation effort took
a single annotator a total of 41 hours. Scaling this
approach to the entire Office help domain would
require a focused annotation effort. Crucially,
though, this annotation work can be carried out by
non-specialists, and could even be crowdsourced
(Bernstein et al, 2010).
Problem Definition As input, we are given in-
structional text (p1 . . . pn), comprised of topics
(t1 . . . tn) describing:
(1) high-level user intents (e.g., t1 ? ?add and for-
mat page numbers?)
(2) goals (g1, . . . , gk) that represent more spe-
cific user intents (e.g., g1 ? ?add header or
footer content to a preformatted page number
design?, g2 ? ?place the page number in the
side margin of the page?).
Given instructional text p1 . . . pn and queries
q1 . . . qm per topic ti, our goals are as follows:
Figure 3: Relationships between user queries and
OHP with goals, instructions and dialog trees.
- for every instructional page pi extract a topic
ti and a set of goals g1 . . . gk;
- for every goal gj for a topic ti, extract a set of
instructions i1 . . . il;
- from topics, goals and instructions, construct
dialog trees f1 . . . fn (one dialog tree per
topic). Classify instructions to user interac-
tion types thereby identifying system action
nodes a1s . . . als. Transitions between these
nodes are the user actions a1u . . . alu.
Figure 2 (left) presents an example of a topic
extracted from the help page, and a set of goals
and instructions annotated with user action types.
In the next few sections of the paper, we out-
line an overall system component design demon-
strating how queries and topics are mapped to the
dialog trees in Figure 3. The figure shows many-
to-one relations between queries and topics, one-
to-many relations between topics and goals, goals
and instructions, and one-to-one relations between
topics and dialog trees.
User Action Classification We aim to classify
instructional text (i1 . . . il) for every goal gj in the
decision tree into four categories: binary, selec-
tion, input or none.
Given a single instruction i with category au,
we use a log-linear model to represent the distri-
1671
bution over the space of possible user actions. Un-
der this representation, the user action distribution
is defined as:
p(au|i, ?) =
e???(au,i)?
a?u e
???(au,i) , (1)
where ?(au, i) ? Rn is an n-dimensional fea-
ture representation and ~? is a parameter vector we
aim to learn. Features are indicator functions of
properties of the instructions and a particular class.
For smoothing we use a zero mean, unit variance
Gaussian prior (0, 1) that penalizes ~? for drifting
too far from the mean, along with the following
optimization function:
log p(Au, ?|I) = log p(Au|I, ?)? log p(?) =
=
?
au,i?(Au,I)
p(au|i, ?)?
?
i
(? ? ?i)2
2?2i
+ k
(2)
We use L-BFGS (Nocedal and Wright, 2000) as
an optimizer.
Experimental Setup As described in Section 2,
our dataset consists of 76 goals grouped into 30
topics (average 2-3 goals per topic) for a total of
246 instructions (average 3 instructions per goal).
We manually label all instructions with user ac-
tion au categories. The distribution over cate-
gories is binary=14, input=23, selection=80 and
none=129. The data is skewed towards the cat-
egories none and selection. Many instruction do
not require any user input and can be done auto-
matically, e.g., ?On the Insert tab, in the Header
and Footer group, click Page Number?. The ex-
ample instructions with corresponding user action
labels are shown in Figure 2 (left) . Finally, we di-
vide the 246 instructions into 2 sets: 80% training
and 20% test, 199 and 47 instructions respectively.
Results We apply the user action type classifi-
cation model described in the Eq.1 and Eq.2 to
classify instructions from the test set into 4 cate-
gories. In Table 1 we report classification results
for 2 baselines: a majority class and heuristic-
based approach, and 2 models with different fea-
ture types: ngrams and ngrams + stems. For a
heuristic baseline, we use simple lexical clues to
classify instructions (e.g., X or Y for binary, select
Y for selection and type X, insert Y for input). Ta-
ble 1 summarizes the results of mapping instruc-
tional text to user actions.
Features # Features Accuracy
Baseline 1: Majority ? 0.53
Baseline 2: Heuristic ? 0.64
Ngrams 10,556 0.89
Ngrams + Stems 12,196 0.89
Table 1: Instruction classification results.
Building the Dialog Trees Based on the classi-
fied user action types, we identify system actions
a1s . . . als which correspond to 3 types of user ac-
tions a1s . . . als (excluding none type) for every goal
in a topic ti. This involved associating all words
from an instruction il with a system action als. Fi-
nally, for every topic we automatically construct a
dialog tree as shown in Figure 2 (right). The dia-
log tree includes a topic t1 with goals g1 . . . g4, and
actions (user actions au and system actions as).
Definition 1. A dialog tree encodes a user-system
dialog flow about a topic ti represented as a di-
rected unweighted graph fi = (V,E) where top-
ics, goals and actions are nodes of correspond-
ing types {t1 . . . tn}, {g1 . . . gk}, {a1 . . . al} ? V .
There is a hierarchical dependency between topic,
goal and action nodes. User interactions are
represented by edges ti ? {g1 . . . gk}, a1u =
(gj , a1) . . . alu = (ak?1, ak) ? E.
For example, in the dialog tree in Figure 2 there
is a relation t1 ? g4 between the topic t1 ?add
and format page numbers? and the goal g4 ?in-
clude page of page X of Y with the page number?.
Moreover, in the dialog tree, the topic level node
has one index i ? [1..n], where n is the number
of topics. Every goal node includes information
about its parent (topic) node and has double index
i.j, where j ? [1..k]. Finally, action nodes include
information about their parent (goal) and grand-
parent (topic) nodes and have triple index i.j.z,
where z ? [1..l].
4 Understanding Initial Queries
This section presents a model for classifying ini-
tial user queries to nodes in a dialog tree, which
allows for a variety of different types of queries.
They can be under-specified, including informa-
tion about a topic only (e.g., ?add or delete page
numbers?); partially specified, including informa-
tion about a goal (e.g., ?insert page number?); or
over-specified, including information about an ac-
tion ( e.g., ?page numbering at bottom page?.)
1672
Figure 4: Mapping initial user queries to the nodes
on different depth in a dialog tree.
Problem Definition Given an initial query, the
dialog system initializes to a state s0, searches for
the deepest relevant node given a query, and maps
the query to a node on a topic ti, goal gj or action
ak level in the dialog tree fi, as shown in Figure 4.
More formally, as input, we are given automati-
cally constructed dialog trees f1 . . . fn for instruc-
tional text (help pages) annotated with topic, goal
and action nodes and associated with system ac-
tions as shown in Figure 2 (right). From the query
logs, we associate queries with each node type:
topic qt, goal qg and action qa. This is shown in
Figure 2 and 4. We join these dialog trees repre-
senting different topics into a dialog network by
introducing a global root. Within the network,
we aim to find (1) an initial dialog state s0 that
maximizes the probability of state given a query
p(s0|q, ?); and (2) the deepest relevant node v ? V
on topic ti, goal gj or action ak depth in the tree.
Initial Dialog State Model We aim to predict
the best node in a dialog tree ti, gj , al ? V based
on a user query q. A query-to-node mapping is en-
coded as an initial dialog state s0 represented by a
binary vector over all nodes in the dialog network:
s0 = [t1, g1.1, g1.2, g1.2.1 . . . , tn, gn.1, gn.1.1].
We employ a log-linear model and try to maxi-
mize initial dialog state distribution over the space
of all nodes in a dialog network:
p(s0|q, ?) =
e
?
i ?i?i(s0,q)
?
s?0 e
?
i ?i?i(s?0,q)
, (3)
Optimization follows Eq. 2.
We experimented with a variety of features.
Lexical features included query ngrams (up to 3-
grams) associated with every node in a dialog tree
with removed stopwords and stemming query un-
igrams. We also used network structural features:
Accuracy
Features Topic Goal Action
Random 0.10 0.04 0.04
TFIDF 1Best 0.81 0.21 0.45
Lexical (L) 0.92 0.66 0.63
L + 10TFIDF 0.94 0.66 0.64
L + 10TFIDF + PO 0.94 0.65 0.65
L + 10TFIDF + QO 0.95 0.72 0.69
All above + QHistO 0.96 0.73 0.71
Table 2: Initial dialog state classification results
where L stands for lexical features, 10TFIDF - 10
best tf-idf scores, PO - prompt overlap, QO - query
overlap, and QHistO - query history overlap.
tf-idf scores, query ngram overlap with the topic
and goal descriptions, as well as system action
prompts, and query ngram overlap with a history
including queries from parent nodes.
Experimental Setup For each dialog tree,
nodes corresponding to single instructions were
hand-annotated with a small set of user queries,
as described in Section 3. Approximately 60% of
all action nodes have no associated queries3 For
the 76 goals, the resulting dataset consists of 972
node-query pairs, 80% training and 20% test.
Results The initial dialog state classification
model of finding a single node given an initial
query is described in Eq. 3.
We chose two simple baselines: (1) randomly
select a node in a dialog network and (2) use a tf-
idf 1-best model.4 Stemming, stopword removal
and including top 10 tf-idf results as features led
to a 19% increase in accuracy on an action node
level over baseline (2). Adding the following fea-
tures led to an overall 26% improvement: query
overlap with a system prompt (PO), query overlap
with other node queries (QO), and query overlap
with its parent queries (QHistO) .
We present more detailed results for topic, goal
and action nodes in Table 2. For nodes deeper in
the network, the task of mapping a user query to an
action becomes more challenging. Note, however,
that the action node accuracy numbers actually un-
3There are multiple possible reasons for this: the soft-
ware user interface may already make it clear how to accom-
plish this intent, the user may not understand that the software
makes this fine-grained option available to them, or their ex-
perience with search engines may lead them to state their in-
tent in a more coarse-grained way.
4We use cosine similarity to rank all nodes in a dialog
network and select the node with the highest rank.
1673
derstate the utility of the resulting dialog system.
The reason is that even incorrect node assignments
can lead to useful system performance. As long
as a misclassification results being assigned to a
too-high node within the correct dialog tree, the
user will experience a graceful failure: they may
be forced to answer some redundant questions, but
they will still be able to accomplish the task.
5 Understanding Query Refinements
We also developed a classifier model for mapping
followup queries to the nodes in a dialog network,
while maintaining a dialog state that summarizes
the history of the current interaction.
Problem Definition Similar to the problem def-
inition in Section 4, we are given a network of di-
alog trees f1 . . . fn and a query q?, but in addition
we are given the previous dialog state s, which
contains the previous user utterance q and the last
system action as. We aim to find a new dialog
state s? that pairs a node from the dialog tree with
updated history information, thereby undergoing a
dialog state update.
We learn a linear classifier that models
p(s?|q?, q, as, ?), the dialog state update distribu-
tion, where we constrain the new state s? to contain
the new utterance q? we are interpreting. This dis-
tribution models 3 transition types: append, over-
ride and reset.
Definition 2. An append action defines a dialog
state update when transitioning from a node to its
children at any depth in the same dialog tree e.g.,
ti ? gi.j (from a topic to a goal node), gi.j ?
ai.j.z (from a goal to an action node) etc.
Definition 3. An override action defines a dialog
state update when transitioning from a goal to its
sibling node. It could also be from an action node5
to another in its parent sibling node in the same di-
alog tree e.g., gi.j?1 ? gi.j (from one goal to an-
other goal in the same topic tree), ai.j.z ? ai.?j.z
(from an action node to another action node in a
different goal in the same dialog tree) etc.
Definition 4. A reset action defines a dialog state
update when transitioning from a node in a current
dialog tree to any other node at any depth in a
dialog tree other than the current dialog tree e.g.,
ti ? t?i, (from one topic node to another topic
5A transition from ai.j.z must be to a different goal or an
action node in a different goal but in the same dialog tree.
(a) Updates from topic node ti
(b) Updates from goal node gj
(c) Updates from action node al
Figure 5: Information state updates: append, reset
and override updates based on Definition 2, 3 and
4, respectively, from topic, goal and action nodes.
node) ti ? g?i.j (from a topic node to a goal node
in a different topic subtree), etc.
The append action should be selected when the
user?s intent is to clarify a previous query (e.g.,
?insert page numbers? ? ?page numbers in the
footer?). An override action is appropriate when
the user?s intent is to change a goal within the
same topic (e.g., ?insert page number? ?change
page number?). Finally, a reset action should be
used when the user?s intent is to restart the dialog
(e.g., ?insert page x of y? ? ?set default font?).
We present more examples for append, override
and reset dialog state update actions in Table 3.
1674
Previous Utterance, q User Utterance, q? Transition Update Action, a
inserting page numbers qt1 add a background ti ? t?i 2, reset-T, reset
how to number pages qt2 insert numbers on pages in margin ti ? si.j 1.4, append-G, append
page numbers qt3 set a page number in a footer ti ? ai.j.z 1.2.1, append-A, append
page number a document qt4 insert a comment ti ? g?i.j 21.1, reset-G, reset
page number qt5 add a comment ?redo? ti ? a?i.j.z 21.2.1, reset-A, reset
page x of y qg1 add a border gi.j ? t?i 6, reset-T, resetformat page x of x qg2 enter text and page numbers gi.j ? gi.?j 1.1, override-G, overrideenter page x of y qg3 page x of y in footer gi.j ? ai.j.z 1.3.1, append-A, appendinserting page x of y qg4 setting a default font gi.j ? g?i.j 6.1, reset-G, resetshowing page x of x qg5 set default font and style gi.j ? a?i.j.z 6.4.1, reset-A, resetpage numbers bottom qa1 make a degree symbol ai.j.z ? t?i 13, reset-T, reset
numbering at bottom page qa2 insert page numbers ai.j.z ? gi.?j 1.1, override-G, override
insert footer page numbers qa3 page number design ai.j.z?1 ? ai.j.z 1.2.2, append-A, append
headers page number qa4 comments in document ai.j.z ? g?i.j 21.1, reset-G, reset
page number in a footer qa5 changing initials in a comment ai.j.z ? a?i.j.z 21.2.1, reset-A, reset
Table 3: Example q and q? queries for append, override and reset dialog state updates.
Figure 5 illustrates examples of append, over-
ride and reset dialog state updates. All transitions
presented in Figure 5 are aligned with the example
q and q? queries in Table 3.
Dialog State Update Model We use a log-linear
model to maximize a dialog state distribution over
the space of all nodes in a dialog network:
p(s?|q?, q, as?) =
e
?
i ?i?i(s?,q?,as,q)
?
s?? e
?
i ?i?i(s??,q?,as,q)
, (4)
Optimization is done as described in Section 3.
Experimental Setup Ideally, dialog systems
should be evaluated relative to large volumes of
real user interaction data. Our query log data,
however, does not include dialog turns, and so we
turn to simulated user behavior to test our system.
Our approach, inspired by recent work (Schatz-
mann et al, 2006; Scheffler and Young, 2002;
Georgila et al, 2005), involves simulating dialog
turns as follows. To define a state s we sam-
ple a query q from a set of queries per node v
and get a corresponding system action as for this
node; to define a state s?, we sample a new query
q? from another node v? ? V, v 6= v? which
is sampled using a prior probability biased to-
wards append: p(append)=0.7, p(override)=0.2,
p(reset)=0.1. This prior distribution defines a dia-
log strategy where the user primarily continues the
current goal and rarely resets.
We simulate 1100 previous state and new query
pairs for training and 440 pairs for testing. The
features were lexical, including word ngrams,
stems with no stopwords; we also tested network
structure, such as:
- old q and new q? query overlap (QO);
- q? overlap with a system prompt as (PO);
- q? ngram overlap with all queries from the old
state s (SQO);
- q? ngram overlap with all queries from the
new state s? (S?QO);
- q? ngram overlap with all queries from the
new state parents (S?ParQO).
Results Table 4 reports results for dialog state
updates for topic, goal and action nodes. We also
report performance for two types of dialog updates
such as: append (App.) and override (Over.).
We found that the combination of lexical and
query overlap with the previous and new state
queries yielded the best accuracies: 0.95, 0.84 and
0.83 for topic, goal and action node level, respec-
tively. As in Section 4, the accuracy on the topic
level node was highest. Perhaps surprisingly, the
reset action was perfectly predicted (accuracy is
100% for all feature combinations, not included
in figure). The accuracies for append and override
actions are also high (append 95%, override 90%).
Features Topic Goal Action App. Over.
L 0.92 0.76 0.78 0.90 0.89
L+Q 0.93 0.80 0.80 0.92 0.83
L+P 0.93 0.80 0.79 0.91 0.85
L+Q+P 0.94 0.80 0.80 0.93 0.85
L+SQ 0.94 0.82 0.81 0.93 0.85
L+S?Q 0.93 0.80 0.80 0.91 0.90
L+S?+ParQ 0.94 0.80 0.80 0.91 0.86
L+Q+S?Q 0.94 0.81 0.81 0.91 0.88
L+SQ+S?Q 0.95 0.84 0.83 0.94 0.88
Table 4: Dialog state updates classification ac-
curacies where L stands for lexical features, Q -
query overlap, P - prompt overlap, SQ - previous
state query overlap, S?Q - new state query overlap,
S?ParQ - new state parent query overlap.
1675
6 The Complete Dialog System
Following the overall setup described in Section 2,
we integrate the learned models into a complete
dialog system. To evaluate the quality of the in-
teractions with this system, we performed a small
scale user study, as described here.
Experimental Setup We randomly chose 8
goals, across topics, from the 76 used to train our
models. For each goal, six users were shown a Mi-
crosoft Word document before and after the goal
was executed. Users were allowed to interact with
the system in an unconstrained manner providing a
total of 48 interactions. If at any point they thought
the system response was irrelevant to their intent,
they could either rephrase or start over.
User Study We objectively measured task com-
pletion and the quality of individual system utter-
ances. We also asked the users to rate the system
(on a scale of 1-5, with 5 being best) on various
measures, including whether the system asked rel-
evant questions. Table 5 summarizes the results.
In general, users were able to complete the tasks
and were reasonably satisfied.
Discussion Figure 6 shows four example di-
alogs. User utterances ranged from single key-
words to rich, sentence-like goal descriptions. For
the shorter inputs, the system was generally able
to identify a small cohort of high-probability in-
tents. For longer inputs, especially with explicitly
specified parameter settings, the system was usu-
ally able to elicit whatever additional information
was required to complete the task. The majority of
errors fell into two categories when the system:
I. missed key information in the input and
asked redundant questions ? Figure 6 (C);
II. failed to correctly identify the intent, and
asked an irrelevant question ? Figure 6 (D).
Table 5 shows the frequency of each error type.
Objective Measures Observed Rates
User achieved task 83%
System asked irrelevant questions 19%
System asked redundant questions 18%
User Satisfaction Score Std.Dev.
Overall satisfaction 3.42 0.49
Questions were relevant 4.17 0.41
Ambiguity was managed 4.00 0.89
Table 5: User study results.
U: ?I want a page number and title at the bot-
tom of the page?
S: ?Choose a page number design from the
gallery of designs?
U: *User selects design*
S: ?To insert header or footer content before the
page number, type the content, and then press
TAB to position the content?
U: ?Professional Resume?
S: ?Task Completed?
(A)
U: ?Alphabetize bullets?
S: ?Do you want to sort a list alphabetically?
U: ?Sort a list?
S: ?Select the text in a bulleted or numbered
list?
U: *User selection*
S: ?Ascending or Descending??
U: ?Ascending?
S: ?Task Completed?
(B)
U: ?I want the background color to be yellow?
S: ?Choose a color for the document?
U: ?Yellow ?
S: ?Task completed?
(C)
U: ?I want to change the font face and size?
S: ?Choose the default font?
U: ?No, reduce the size of the text?
S: *System Restarts*
(D)
Figure 6: Four example dialogs from the user
study, including cases that (A and B) complete
successfully, (C) have a redundant question, and
(D) fail to recognize the user intent.
7 Related work
To the best of our knowledge, this paper presents
the first effort to induce full procedural dialog sys-
tems from instructional text and query click logs.
Grounded Language Learning There has been
significant interest in grounded language learn-
ing. Perhaps the most closely related work
learns to understand instructions and automati-
1676
cally complete the tasks they describe (Branavan
et al, 2009; Vogel and Jurafsky, 2010; Kush-
man et al, 2009; Branavan et al, 2010; Artzi and
Zettlemoyer, 2013). However, these approaches
did not model user interaction. There are also
many related approaches for other grounded lan-
guage problems, including understanding game
strategy guides (Branavan et al, 2011), model-
ing users goals in a Windows domain (Horvitz
et al, 1998), learning from conversational inter-
action (Artzi and Zettlemoyer, 2011), learning
to sportscast (Chen and Mooney, 2011), learning
from event streams (Liang et al, 2009), and learn-
ing paraphrases from crowdsourced captions of
video snippets (Chen and Dolan, 2011).
Dialog Generation from Text Similarly to Pi-
wek?s work (2007; 2010; 2011), we study extract-
ing dialog knowledge from documents (mono-
logues or instructions). However, Piwek?s ap-
proach generates static dialogs, for example to
generate animations of virtual characters having a
conversation. There is no model of dialog man-
agement or user interaction, and the approach does
not use any machine learning. In contrast, to the
best of our knowledge, we are the first to demon-
strate it is possible to learn complete, interactive
dialog systems using instructional texts (and non-
expert annotation).
Learning from Web Query Logs Web query
logs have been extensively studied. For example,
they are widely used to represent user intents in
spoken language dialogs (Tu?r et al, 2011; Celiky-
ilmaz et al, 2011; Celikyilmaz and Hakkani-Tur,
2012). Web query logs are also used in many other
NLP tasks, including entity linking (Pantel et al,
2012) and training product and job intent classi-
fiers (Li et al, 2008).
Dialog Modeling and User Simulation Many
existing dialog systems learn dialog strategies
from user interactions (Young, 2010; Rieser and
Lemon, 2008). Moreover, dialog data is often lim-
ited and, therefore, user simulation is commonly
used (Scheffler and Young, 2002; Schatzmann et
al., 2006; Georgila et al, 2005).
Our overall approach is also related to many
other dialog management approaches, including
those that construct dialog graphs from dialog data
via clustering (Lee et al, 2009), learn information
state updates using discriminative classification
models (Hakkani-Tur et al, 2012; Mairesse et al,
2009), optimize dialog strategy using reinforce-
ment learning (RL) (Scheffler and Young, 2002;
Rieser and Lemon, 2008), or combine RL with
information state update rules (Heeman, 2007).
However, our approach is unique in the use of in-
ducing task and domain knowledge with light su-
pervision to assist the user with many goals.
8 Conclusions and Future Work
This paper presented a novel approach for au-
tomatically constructing procedural dialog sys-
tems with light supervision, given only textual re-
sources such as instructional text and search query
click logs. Evaluations demonstrated highly accu-
rate performance, on automatic benchmarks and
through a user study.
Although we showed it is possible to build com-
plete systems, more work will be required to scale
the approach to new domains, scale the complex-
ity of the dialog manager, and explore the range of
possible textual knowledge sources that could be
incorporated. We are particularly interested in sce-
narios that would enable end users to author new
goals by writing procedural instructions in natural
language.
Acknowledgments
The authors would like to thank Jason Williams
and the anonymous reviewers for their helpful
comments and suggestions.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, An-
thanasios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David R. Traum. 2012. The twins
corpus of museum visitor questions. In Proceedings
of LREC.
Yoav Artzi and Luke Zettlemoyer. 2011. Learning
to recover meaning from unannotated conversational
interactions. In NIPS Workshop In Learning Seman-
tics.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of ACM Symposium on User
Interface Software and Technology.
1677
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: learn-
ing to map high-level instructions to commands. In
Proceedings of ACL.
S. R. K. Branavan, David Silver, and Regina Barzi-
lay. 2011. Learning to win by reading manuals in
a monte-carlo framework. In Proceedings of ACL.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2012. A
joint model for discovery of aspects in utterances.
In Proceedings of ACL.
Asli Celikyilmaz, Dilek Hakkani-Tu?r, and Gokhan Tu?r.
2011. Mining search query logs for spoken language
understanding. In Proceedings of ICML.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of ACL.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of AAAI.
Myroslava Dzikovska, Amy Isard, Peter Bell, Jo-
hanna D. Moore, Natalie B. Steinhauser, Gwen-
dolyn E. Campbell, Leanne S. Taylor, Simon Caine,
and Charlie Scott. 2011. Adaptive intelligent tuto-
rial dialogue in the beetle ii system. In Proceedings
of AIED.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for infor-
mation state update dialogue systems. In Proceed-
ings of Eurospeech.
Dilek Hakkani-Tur, Gokhan Tur, Larry Heck, Ashley
Fidler, and Asli Celikyilmaz. 2012. A discrimi-
native classification-based approach to information
state updates for a multi-domain dialog system. In
Proceedings of Interspeech.
Peter Heeman. 2007. Combining Reinforcement
Learning with Information-State Update Rules. In
Proceedings of ACL.
Eric Horvitz, Jack Breese, David Heckerman, David
Hovel, and Koos Rommelse. 1998. The Lumiere
project: Bayesian user modeling for inferring the
goals and needs of software users. In Proceedings
of Uncertainty in Artificial Intelligence.
Nate Kushman, Micah Brodsky, S. R. K. Branavan,
Dina Katabi, Regina Barzilay, and Martin Rinard.
2009. WikiDo. In ACM HotNets.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2009. Automatic agenda graph
construction from human-human dialogs using clus-
tering method. In Proceedings of NAACL.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of SIGIR.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of ACL-IJCNLP.
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken lan-
guage understanding from unaligned data using dis-
criminative classification models. In Proceedings of
Acoustics, Speech and Signal Processing.
Fabrizio Morbini, Eric Forbell, David DeVault, Kenji
Sagae, David R. Traum, and Albert A. Rizzo. 2012.
A mixed-initiative conversational dialogue system
for healthcare. In Proceedings of SIGDIAL.
Jorge Nocedal and Stephen J. Wright. 2000. Numeri-
cal Optimization. Springer.
Patric Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent.
In Proceedings of ACL.
Paul Piwek and Svetlana Stoyanchev. 2010. Generat-
ing expository dialogue from monologue: Motiva-
tion, corpus and preliminary rules. In Proceedings
of NAACL.
Paul Piwek and Svetlana Stoyanchev. 2011. Data-
oriented monologue-to-dialogue generation. In Pro-
ceedings of ACL, pages 242?247.
Paul Piwek, Hugo Hernault, Helmut Prendinger, and
Mitsuru Ishizuka. 2007. T2d: Generating dialogues
between virtual agents automatically from text. In
Proceedings of Intelligent Virtual Agents.
Verena Rieser and Oliver Lemon. 2008. Learning ef-
fective multimodal dialogue strategies from wizard-
of-oz data: Bootstrapping and evaluation. In Pro-
ceedings of ACL.
A. Rizzo, Kenji Sagae, E. Forbell, J. Kim, B. Lange,
J. Buckwalter, J. Williams, T. Parsons, P. Kenny,
David R. Traum, J. Difede, and B. Rothbaum. 2011.
Simcoach: An intelligent virtual human system for
providing healthcare information and support. In
Proceedings of ITSEC.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2).
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
Human Language Technology Research.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava Dzikovska, and Johanna D. Moore. 2011.
1678
Talk like an electrician: Student dialogue mimick-
ing behavior in an intelligent tutoring system. In
Proceedings of AIED.
David R. Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William R. Swartout. 2012.
Ada and grace: Direct interaction with museum vis-
itors. In Proceedings of Intelligent Virtual Agents.
Go?khan Tu?r, Dilek Z. Hakkani-Tu?r, Dustin Hillard, and
Asli C?elikyilmaz. 2011. Towards unsupervised spo-
ken language understanding: Exploiting query click
logs for slot filling. In Proceedings of Interspeech.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.
Steve Young. 2010. Cognitive user interfaces. In IEEE
Signal Processing Magazine.
1679
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 222?227,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Neighborhoods as Hypergraphs
Chris Quirk and Pallavi Choudhury
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{chrisq,pallavic}@microsoft.com
Abstract
Ambiguity preserving representations
such as lattices are very useful in a num-
ber of NLP tasks, including paraphrase
generation, paraphrase recognition, and
machine translation evaluation. Lattices
compactly represent lexical variation, but
word order variation leads to a combina-
torial explosion of states. We advocate
hypergraphs as compact representations
for sets of utterances describing the same
event or object. We present a method
to construct hypergraphs from sets of
utterances, and evaluate this method on
a simple recognition task. Given a set of
utterances that describe a single object or
event, we construct such a hypergraph,
and demonstrate that it can recognize
novel descriptions of the same event with
high accuracy.
1 Introduction
Humans can construct a broad range of descrip-
tions for almost any object or event. In this paper,
we will refer to such objects or events as ground-
ings, in the sense of grounded semantics. Exam-
ples of groundings include pictures (Rashtchian et
al., 2010), videos (Chen and Dolan, 2011), transla-
tions of a sentence from another language (Dreyer
and Marcu, 2012), or even paraphrases of the same
sentence (Barzilay and Lee, 2003).
One crucial problem is recognizing whether
novel utterances are relevant descriptions of those
groundings. In the case of machine translation,
this is the evaluation problem; for images and
videos, this is recognition and retrieval. Generat-
ing descriptions of events is also often an interest-
ing task: we might like to find a novel paraphrase
for a given sentence, or generate a description of a
grounding that meets certain criteria (e.g., brevity,
use of a restricted vocabulary).
Much prior work has used lattices to compactly
represent a range of lexical choices (Pang et al,
2003). However, lattices cannot compactly repre-
sent alternate word orders, a common occurrence
in linguistic descriptions. Consider the following
excerpts from a video description corpus (Chen
and Dolan, 2011):
? A man is sliding a cat on the floor.
? A boy is cleaning the floor with the cat.
? A cat is being pushed across the floor by a
man.
Ideally we would like to recognize that the fol-
lowing utterance is also a valid description of that
event: A cat is being pushed across the floor by a
boy. That is difficult with lattice representations.
Consider the following context free grammar:
S ? X0 X1
| X2 X3
X0 ? a man | a boy
X1 ? is sliding X2 on X4
| is cleaning X4 with X2
X2 ? a cat | the cat
X3 ? is being pushed across X4 by X0
X4 ? the floor
This grammar compactly captures many lexical
and syntactic variants of the input set. Note how
the labels act as a kind of multiple-sequence-
alignment allowing reordering: spans of tokens
covered by the same label are, in a sense, aligned.
This hypergraph or grammar represents a seman-
tic neighborhood: a set of utterances that describe
the same entity in a semantic space.
Semantic neighborhoods are defined in terms of
a grounding. Two utterances are neighbors with
respect to some grounding (semantic event) if they
are both descriptions of that grounding. Para-
phrases, in contrast, may be defined over all pos-
sible groundings. That is, two words or phrases
222
are considered paraphrases if there exists some
grounding that they both describe. The para-
phrase relation is more permissive than the seman-
tic neighbor relation in that regard. We believe that
it is much easier to define and evaluate semantic
neighbors. Human annotators may have difficulty
separating paraphrases from unrelated or merely
related utterances, and this line may not be con-
sistent between judges. Annotating whether an ut-
terance clearly describes a grounding is a much
easier task.
This paper describes a simple method for con-
structing hypergraph-shaped Semantic Neighbor-
hoods from sets of expressions describing the
same grounding. The method is evaluated in
a paraphrase recognition task, inspired by a
CAPTCHA task (Von Ahn et al, 2003).
2 Inducing neighborhoods
Constructing a hypergraph to capture a set of utter-
ances is a variant of grammar induction. Given a
sample of positive examples, we infer a compact
and accurate description of the underlying lan-
guage. Conventional grammar induction attempts
to define the set of grammatical sentences in the
language. Here, we search for a grammar over the
fluent and adequate descriptions of a particular in-
put. Many of the same techniques still apply.
Rather than starting from scratch, we bootstrap
from an existing English parser. We begin by pars-
ing the set of input utterances. This parsed set of
utterances acts as a sort of treebank. Reading off a
grammar from this treebank produces a grammar
that can generate not only the seed sentences, but
also a broad range of nearby sentences. In the case
above with cat, man, and boy, we would be able
to generate cases legitimate variants where man
was replaced by boy as well as undesired variants
where man is replaced by cat or floor. This initial
grammar captures a large neighborhood of nearby
utterances including many such undesirable ones.
Therefore, we refine the grammar.
Refinements have been in common use in syn-
tactic parsing for years now. Inspired by the re-
sult that manual annotations of Treebank cate-
gories can substantially increase parser accuracy
(Klein and Manning, 2003), several approaches
have been introduced to automatically induce la-
tent symbols on existing trees. We use the split-
merge method commonly used in syntactic pars-
ing (Petrov et al, 2006). In its original setting,
the refinements captured details beyond that of the
original Penn Treebank symbols. Here, we cap-
ture both syntactic and semantic regularities in the
descriptions of a given grounding.
As we perform more rounds of refinement, the
grammar becomes tightly constrained to the orig-
inal sentences. Indeed, if we iterated to a fixed
point, the resulting grammar would parse only the
original sentences. This is a common dilemma in
paraphrase learning: the safest meaning preserv-
ing rewrite is to change nothing. We optimize the
number of split-merge rounds for task-accuracy;
two or three rounds works well in practice. Fig-
ure 1 illustrates the process.
2.1 Split-merge induction
We begin with a set of utterances that describe
a specific grounding. They are parsed with a
conventional Penn Treebank parser (Quirk et al,
2012) to produce a type of treebank. Unlike con-
ventional treebanks which are annotated by human
experts, the trees here are automatically created
and thus are more likely to contain errors. This
treebank is the input to the split-merge process.
Split: Given an input treebank, we propose re-
finements of the symbols in hopes of increasing
the likelihood of the data. For each original sym-
bol in the grammar such as NP, we consider two la-
tent refinements: NP0 and NP1. Each binary rule
then produces 8 possible variants, since the par-
ent, left child, and right child now have two possi-
ble refinements. The parameters of this grammar
are then optimized using EM. Although we do not
know the correct set of latent annotations, we can
search for the parameters that optimize the likeli-
hood of the given treebank. We initialize the pa-
rameters of this refined grammar with the counts
from the original grammar along with a small ran-
dom number. This randomness prevents EM from
starting on a saddle point by breaking symmetries;
Petrov et al describe this in more detail.
Merge: After EM has run to completion, we
have a new grammar with twice as many symbols
and eight times as many rules. Many of these sym-
bols may not be necessary, however. For instance,
nouns may require substantial refinement to dis-
tinguish a number of different actors and objects,
where determiners might not require much refine-
ment at all. Therefore, we discard the splits that
led to the least increase in likelihood, and then
reestimate the grammar once again.
223
(a) Input:
? the man plays the piano
? the guy plays the keyboard
(b) Parses:
? (S (NP (DT the) (NN man))
(VP (VBZ plays)
(NP (DT the) (NN piano)))
? (S (NP (DT the) (NN guy))
(VP (VBZ plays)
(NP (DT the) (NN keyboard)))
(c) Parses with latent annotations:
? (S (NP0 (DT the) (NN0 man))
(VP (VBZ plays)
(NP1 (DT the) (NN1 piano)))
? (S (NP0 (DT the) (NN0 guy))
(VP (VBZ plays)
(NP1 (DT the) (NN1 keyboard)))
(d) Refined grammar:
S ? NP0 VP
NP0 ? DT NN0
NP1 ? DT NN1
NP ? VBZ NP1
DT ? the
NN0 ? man | guy
NN1 ? piano | keyboard
VBZ ? plays
Figure 1: Example of hypergraph induction. First
a conventional Treebank parser converts input ut-
terances (a) into parse trees (b). A grammar could
be directly read from this small treebank, but it
would conflate all phrases of the same type. In-
stead we induce latent refinements of this small
treebank (c). The resulting grammar (d) can match
and generate novel variants of these inputs, such
as the man plays the keyboard and the buy plays
the piano. While this simplified example sug-
gests a single hard assignment of latent annota-
tions to symbols, in practice we maintain a dis-
tribution over these latent annotations and extract
a weighted grammar.
Iteration: We run this process in series. First
the original grammar is split, then some of the
least useful splits are discarded. This refined
grammar is then split again, with the least useful
splits discarded once again. We repeat for a num-
ber of iterations based on task accuracy.
Final grammar estimation: The EM proce-
dure used during split and merge assigns fractional
counts c(? ? ? ) to each refined symbol Xi and each
production Xi ? Yj Zk. We estimate the final
grammar using these fractional counts.
P (Xi ? Yj Zk) =
c(Xi, Yj , Zk)
c(Xi)
In Petrov et al, these latent refinements are later
discarded as the goal is to find the best parse with
the original coarse symbols. Here, we retain the
latent refinements during parsing, since they dis-
tinguish semantically related utterances from un-
related utterances. Note in Figure 1 how NN0
and NN1 refer to different objects; were we to ig-
nore that distinction, the parser would recognize
semantically different utterances such as the piano
plays the piano.
2.2 Pruning and smoothing
For both speed and accuracy, we may also prune
the resulting rules. Pruning low probability rules
increases the speed of parsing, and tends to in-
crease the precision of the matching operation at
the cost of recall. Here we only use an absolute
threshold; we vary this threshold and inspect the
impact on task accuracy. Once the fully refined
grammar has been trained, we only retain those
rules with a probability above some threshold. By
varying this threshold t we can adjust precision
and recall: as the low probability rules are re-
moved from the grammar, precision tends to in-
crease and recall tends to decrease.
Another critical issue, especially in these small
grammars, is smoothing. When parsing with a
grammar obtained from only 20 to 50 sentences,
we are very likely to encounter words that have
never been seen before. We may reasonably re-
ject such sentences under the assumption that they
are describing words not present in the training
corpus. However, this may be overly restrictive:
we might see additional adjectives, for instance.
In this work, we perform a very simple form of
smoothing. If the fractional count of a word given
a pre-terminal symbol falls below a threshold k,
then we consider that instance rare and reserve a
fraction of its probability mass for unseen words.
This accounts for lexical variation of the ground-
ing, especially in the least consistently used words.
Substantial speedups could be attained by us-
ing finite state approximations of this grammar:
matching complexity drops to cubic to linear in
the length of the input. A broad range of approxi-
mations are available (Nederhof, 2000). Since the
small grammars in our evaluation below seldom
exhibit self-embedding (latent state identification
224
tends to remove recursion), these approximations
would often be tight.
3 Experimental evaluation
We explore a task in description recognition.
Given a large set of videos and a number of de-
scriptions for each video (Chen and Dolan, 2011),
we build a system that can recognize fluent and
accurate descriptions of videos. Such a recognizer
has a number of uses. One example currently in
evaluation is a novel CAPTCHAs: to differentiate
a human from a bot, a video is presented, and the
response must be a reasonably accurate and fluent
description of this video.
We split the above data into training and test.
From the training sets, we build a set of recogniz-
ers. Then we present these recognizers with a se-
ries of inputs, some of which are from the held out
set of correct descriptions of this video, and some
of which are from descriptions of other videos.
Based on discussions with authors of CAPTCHA
systems, a ratio of actual users to spammers of 2:1
seemed reasonable, so we selected one negative
example for every two positives. This simulates
the accuracy of the system when presented with a
simple bot that supplies random, well-formed text
as CAPTCHA answers.1
As a baseline, we compare against a simple tf-
idf approach. In this baseline we first pool all
the training descriptions of the video into a sin-
gle virtual document. We gather term frequen-
cies and inverse document frequencies across the
whole corpus. An incoming utterance to be classi-
fied is scored by computing the dot product of its
counted terms with each document; it is assigned
to the document with the highest dot product (co-
sine similarity).
Table 2 demonstrates that a baseline tf-idf ap-
proach is a reasonable starting point. An oracle
selection from among the top three is the best per-
formance ? clearly this is a reasonable approach.
That said, grammar based approach shows im-
provements over the baseline tf-idf, especially in
recall. Recall is crucial in a CAPTCHA style task:
if we fail to recognize utterances provided by hu-
mans, we risk frustration or abandonment of the
service protected by the CAPTCHA. The relative
importance of false positives versus false negatives
1A bot might perform object recognition on the videos and
supply a stream of object names. We might simulate this by
classifying utterances consisting of appropriate object words
but without appropriate syntax or function words.
Total videos 2,029
Training descriptions 22,198
types 5,497
tokens 159,963
Testing descriptions 15,934
types 4,075
tokens 114,399
Table 1: Characteristics of the evaluation data.
The descriptions from the video description cor-
pus are randomly partitioned into training and test.
(a)
Algorithm S k Prec Rec F-0
tf-idf 99.9 46.6 63.6
tf-idf (top 3 oracle) 99.9 65.3 79.0
grammar 2 1 86.6 51.5 64.6
2 4 80.2 62.6 70.3
2 16 74.2 74.2 74.2
2 32 73.5 76.4 74.9
3 1 91.1 43.9 59.2
3 4 83.7 54.4 65.9
3 16 77.3 65.7 71.1
3 32 76.4 68.1 72.0
4 1 94.1 39.7 55.8
4 4 85.5 51.1 64.0
4 16 79.1 61.5 69.2
4 32 78.2 63.9 70.3
(b)
t S Prec Rec F-0
? 4.5? 10?5 2 74.8 73.9 74.4
? 4.5? 10?5 3 79.6 60.9 69.0
? 4.5? 10?5 4 82.5 53.2 64.7
? 3.1? 10?7 2 74.2 75.0 74.6
? 3.1? 10?7 3 78.1 64.6 70.7
? 3.1? 10?7 4 80.7 58.8 68.1
> 0 2 73.4 76.4 74.9
> 0 3 76.4 68.1 72.0
> 0 4 78.2 63.9 70.3
Table 2: Experimental results. (a) Comparison of
tf-idf baseline against grammar based approach,
varying several free parameters. An oracle checks
if the correct video is in the top three. For the
grammar variants, the number of splits S and the
smoothing threshold k are varied. (b) Variations
on the rule pruning threshold t and number of
split-merge rounds S. > 0 indicates that all rules
are retained. Here the smoothing threshold k is
fixed at 32.
225
(a) Input descriptions:
? A cat pops a bunch of little balloons that are on the groung.
? A dog attacks a bunch of balloons.
? A dog is biting balloons and popping them.
? A dog is playing balloons.
? A dog is playing with balloons.
? A dog is playing with balls.
? A dog is popping balloons with its teeth.
? A dog is popping balloons.
? A dog is popping balloons.
? A dog plays with a bunch of balloons.
? A small dog is attacking balloons.
? The dog enjoyed popping balloons.
? The dog popped the balloons.
(b) Top ranked yields from the resulting grammar:
+0.085 A dog is popping balloons.
+0.062 A dog is playing with balloons.
+0.038 A dog is playing balloons.
0.038 A dog is attacking balloons.
+0.023 A dog plays with a bunch of balloons.
+0.023 A dog attacks a bunch of balloons.
0.023 A dog pops a bunch of balloons.
0.023 A dog popped a bunch of balloons.
0.023 A dog enjoyed a bunch of balloons.
0.018 The dog is popping balloons.
0.015 A dog is biting balloons.
0.015 A dog is playing with them.
0.015 A dog is playing with its teeth.
Figure 2: Example yields from a small grammar. The descriptions in (a) were parsed as-is (including the
typographical error ?groung?), and a refined grammar was trained with 4 splits. The top k yields from
this grammar along with the probability of that derivation are listed in (b). A ?+? symbol indicates that
the yield was in the training set. No smoothing or pruning was performed on this grammar.
may vary depending on the underlying resource.
Adjusting the free parameters of this method al-
lows us to achieve different thresholds. We can
see that rule pruning does not have a large impact
on overall results, though it does allow yet another
means of tradiing off precision vs. recall.
4 Conclusions
We have presented a method for automatically
constructing compact representations of linguis-
tic variation. Although the initial evaluation only
explored a simple recognition task, we feel the
underlying approach is relevant to many linguis-
tic tasks including machine translation evalua-
tion, and natural language command and con-
trol systems. The induction procedure is rather
simple but effective, and addresses some of the
reordering limitations associated with prior ap-
proaches.(Barzilay and Lee, 2003) In effect, we
are performing a multiple sequence alignment that
allows reordering operations. The refined symbols
of the grammar act as a correspondence between
related inputs.
The quality of the input parser is crucial. This
method only considers one possible parse of the
input. A straightforward extension would be to
consider an n-best list or packed forest of input
parses, which would allow the method to move
past errors in the first input process. Perhaps also
this reliance on symbols from the original Tree-
bank is not ideal. We could merge away some or
all of the original distinctions, or explore different
parameterizations of the grammar that allow more
flexibility in parsing.
The handling of unseen words is very simple.
We are investigating means of including addi-
tional paraphrase resources into the training to in-
crease the effective lexical knowledge of the sys-
tem. It is inefficient to learn each grammar inde-
pendently. By sharing parameters across different
groundings, we should be able to identify Seman-
tic Neighborhoods with fewer training instances.
Acknowledgments
We would like to thank William Dolan and the
anonymous reviewers for their valuable feedback.
References
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
NAACL-HLT.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 190?200, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation eval-
uation. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 162?171, Montre?al, Canada, June.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
226
Japan, July. Association for Computational Linguis-
tics.
Mark-Jan Nederhof. 2000. Practical experiments with
regular approximation of context-free languages.
Computational Linguistics, 26(1):17?44, March.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, Colin Cherry, and Lucy Vanderwende.
2012. Msr splat, a language analysis toolkit. In
Proceedings of the Demonstration Session at the
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 21?24, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using amazon?s mechanical turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147, Los Angeles, June. Asso-
ciation for Computational Linguistics.
Luis Von Ahn, Manuel Blum, Nicholas J. Hopper, and
John Langford. 2003. Captcha: Using hard ai prob-
lems for security. In Eli Biham, editor, Advances in
Cryptology ? EUROCRYPT 2003, volume 2656 of
Lecture Notes in Computer Science, pages 294?311.
Springer Berlin Heidelberg.
227
Proceedings of BioNLP Shared Task 2011 Workshop, pages 155?163,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
MSR-NLP Entry in BioNLP Shared Task 2011 
 
 
Chris Quirk, Pallavi Choudhury, Michael Gamon, and Lucy Vanderwende 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{chrisq,pallavic,mgamon,lucyv}@microsoft.com 
 
 
Abstract 
We describe the system from the Natural 
Language Processing group at Microsoft 
Research for the BioNLP 2011 Shared 
Task. The task focuses on event extraction, 
identifying structured and potentially 
nested events from unannotated text. Our 
approach follows a pipeline, first 
decorating text with syntactic information, 
then identifying the trigger words of 
complex events, and finally identifying the 
arguments of those events. The resulting 
system depends heavily on lexical and 
syntactic features. Therefore, we explored 
methods of maintaining ambiguities and 
improving the syntactic representations, 
making the lexical information less brittle 
through clustering, and of exploring novel 
feature combinations and feature reduction. 
The system ranked 4th in the GENIA task 
with an F-measure of 51.5%, and 3rd in the 
EPI task with an F-measure of 64.9%. 
1 Introduction 
We describe a system for extracting complex 
events and their arguments as applied to the 
BioNLP-2011 shared task.  Our goal is to explore 
general methods for fine-grained information 
extraction, to which the data in this shared task is 
very well suited.  We developed our system using 
only the data provided for the GENIA task, but 
then submitted output for two of the tasks, GENIA 
and EPI, training models on each dataset 
separately, with the goal of exploring how general 
the overall system design is with respect to text 
domain and event types. We used no external 
knowledge resources except a text corpus used to 
train cluster features. We further describe several 
system variations that we explored but which did 
not contribute to the final system submitted. We 
note that the MSR-NLP system consistently is 
among those with the highest recall, but needs 
additional work to improve precision. 
2 System Description 
Our event extraction system is a pipelined 
approach, closely following the structure used by 
the best performing system in 2009 (Bj?rne et al, 
2009). Given an input sentence along with 
tokenization information and a set of parses, we 
first attempt to identify the words that trigger 
complex events using a multiclass classifier. Next 
we identify edges between triggers and proteins, or 
between triggers and other triggers. Finally, given 
a graph of proteins and triggers, we use a rule-
based post-processing component to produce 
events in the format of the shared task. 
2.1 Preprocessing and Linguistic Analysis 
We began with the articles as provided, with an 
included tokenization of the input and 
identification of the proteins in the input. However, 
we did modify the token text and the part-of-
speech tags of the annotated proteins in the input to 
be PROT after tagging and parsing, as we found 
that it led to better trigger detection. 
The next major step in preprocessing was to 
produce labeled dependency parses for the input. 
Note that the dependencies may not form a tree: 
there may be cycles and some words may not be 
connected. During feature construction, this 
parsing graph was used to find paths between 
155
words in the sentence. Since proteins may consist 
of multiple words, for paths we picked a single 
representative word for each protein to act as its 
starting point and ending point. Generally this was 
the token inside the protein that is closest to the 
root of the dependency parse. In the case of ties, 
we picked the rightmost such node. 
2.1.1 McClosky-Charniak-Stanford parses 
The organizers provide parses from a version of 
the McClosky-Charniak parser, MCCC (McClosky 
and Charniak, 2008), which is a two-stage 
parser/reranker trained on the GENIA corpus. In 
addition, we used an improved set of parsing 
models that leverage unsupervised data, MCCC-I 
(McClosky, 2010). In both cases, the Stanford 
Parser was used to convert constituency trees in the 
Penn Treebank format into labeled dependency 
parses: we used the collapsed dependency format. 
2.1.2 Dependency posteriors 
Effectively maintaining and leveraging the 
ambiguity present in the underlying parser has 
improved task accuracy in some downstream tasks 
(e.g., Mi et al 2008). McClosky-Charniak parses 
in two passes: the first pass is a generative model 
that produces a set of n-best candidates, and the 
second pass is a discriminative reranker that uses a 
rich set of features including non-local 
information. We renormalized the outputs from 
this log-linear discriminative model to get a 
posterior distribution over the 50-best parses. This 
set of parses preserved some of the syntactic 
ambiguity present in the sentence. 
The Stanford parser deterministically converts 
phrase-structure trees into labeled dependency 
graphs (de Marneffe et al, 2006). We converted 
each constituency tree into a dependency graph 
separately and retained the probability computed 
above on each graph. 
One possibility was to run feature extraction on 
each of these 50 parses, and weight the resulting 
features in some manner. However, this caused a 
significant increase in feature count. Instead, we 
gathered a posterior distribution over dependency 
edges: the posterior probability of a labeled 
dependency edge was estimated by the sum of the 
probability of all parses containing that edge. 
Gathering all such edges produced a single labeled 
graph that retained much of the ambiguity of the 
input sentence. Figure 1 demonstrates this process 
on a simple example. We applied a threshold of 0.5 
and retained all edges above that threshold, 
although there are many alternative ways to exploit 
this structure.  
 
Figure 1: Example sentence from the GENIA corpus. (a) Two of the top 50 constituency parses from the MCCC-I 
parser; the first had a total probability mass of 0.43 and the second 0.25 after renormalization. Nodes that differ 
between parses are shaded and outlined. (b) The dependency posteriors (labels omitted due to space) after 
conversion of 50-best parses. Solid lines indicate edges with posterior > 0.95; edges with posterior < 0.05 were 
omitted. Most of the ambiguity is in the attachment of ?elicited?. 
156
As above, the resulting graph is likely no longer 
a connected tree, though it now may also be cyclic 
and rather strange in structure. Most of the 
dependency features were built on shortest paths 
between words. We used the algorithm in Cormen 
et al (2002, pp.595) to find shortest paths in a 
cyclic graph with non-negative edge weights. The 
shortest path algorithm used in feature finding was 
supplied uniform positive edge weights. We could 
also weight edges by the negative log probability 
to find the shortest, most likely path. 
2.1.3 ENJU 
We also experimented with the ENJU parses 
(Miyao and Tsujii, 2008) provided by the shared 
task organizers. The distribution contained the 
output of the ENJU parser in a format consistent 
with the Stanford Typed Dependency 
representation . 
2.1.4 Multiple parsers 
We know that even the best modern parsers are 
prone to errors. Including features from multiple 
parsers helps mitigate these errors. When different 
parsers agree, they can reinforce certain 
classification decisions. The features that were 
extracted from a dependency parse have names 
that include an identifier for the parser that 
produced them. In this way, the machine learning 
algorithm can assign different weights to features 
from different parsers. For finding heads of multi-
word entities, we preferred the ENJU parser if 
present in that experimental condition, then fell 
back to MCCC parses, and finally MCCC-I. 
2.1.5 Dependency conversion rules 
We computed our set of dependency features (see 
2.2.1) from the collapsed, propagated Stanford 
Typed Dependency representation (see 
http://nlp.stanford.edu/software/dependencies_man
ual.pdf and de Marneffe et al, 2006), made 
available by the organizers.  We chose this form of 
representation since we are primarily interested in 
computing features that hold between content 
words.  Consider, for example, the noun phrase 
?phosphorylation of TRAF2?. A dependency 
representation would specify head-modifier 
relations for the tuples (phosphorylation, of) and 
(of, TRAF2). Instead of head-modifier, a typed 
dependency representation specifies PREP and 
PPOBJ as the two grammatical relations: 
PREP(phosphorylation-1, of-2) and PPOBJ(of-2, 
TRAF2-3). A collapsed representation has a single 
triplet specifying the relation between the content 
words directly, PREP_OF(phosphorylation-1, 
TRAF2-3); we considered this representation to be 
the most informative.   
We experimented with a representation that 
further normalized over syntactic variation.  The 
system submitted for the GENIA subtask does not 
use these conversion rules, while the system 
submitted for the EPI subtask does use these rules.  
See Table 2 for further details. While for some 
applications it may be useful to distinguish 
whether a given relation was expressed in the 
active or passive voice, or in a main or a relative 
clause, we believe that for this application it is 
beneficial to normalize over these types of 
syntactic variation.  Accordingly, we had a set of 
simple renaming conversion rules, followed by a 
rule for expansion; this list was our first effort and 
could likely be improved.  We modeled this 
normalized level of representation on the logical 
form, described in Jensen (1993), though we were 
unable to explore NP-or VP-anaphora 
 
Renaming conversion rules: 
1. ABBREV -> APPOS 
2. NSUBJPASS -> DOBJ 
3. AGENT -> NSUBJ 
4. XSUBJ -> NSUBJ 
5. PARTMOD(head, modifier where last 3 
characters are "ing") -> NSUBJ(modifier, head) 
6. PARTMOD(head, modifier where last 3 
characters are "ed") -> DOBJ(modifier, head) 
Expansion: 
1. For APPOS, find all edges that point to the head 
(gene-20) and duplicate those edges, but 
replacing the modifier with the modifier of the 
APPOS relation (kinase-26).  
 
Thus, in the 2nd sentence in PMC-1310901-01-
introduction, ?... leading to expression of a bcr-abl 
fusion gene, an aberrant activated tyrosine kinase, 
....?, there are two existing grammatical relations: 
 
PREP_OF(expression-15, gene-20) 
APPOS(gene-20, kinase-26) 
 
to which this rule adds: 
 
PREP_OF(expression-15, kinase-26) 
157
2.2 Trigger Detection 
We treated trigger detection as a multi-class 
classification problem: each token should be 
annotated with its trigger type or with NONE if it 
was not a trigger. When using the feature set 
detailed below, we found that an SVM 
(Tsochantaridis et al, 2004) outperformed a 
maximum entropy model by a fair margin, though 
the SVM was sensitive to its free parameters. A 
large value of C, the penalty incurred during 
training for misclassifying a data point, was 
necessary to achieve good results. 
2.2.1 Features for Trigger Detection 
Our initial feature set for trigger detection was 
strongly influenced by features that were 
successful in Bj?rne et al, (2009).  
Token Features. We included stems of single 
tokens from the Porter stemmer (Porter, 1980), 
character bigrams and trigrams, a binary indicator 
feature if the token has upper case letters, another 
indicator for the presence of punctuation, and a 
final indicator for the presence of a number. We 
gathered these features for both the current token 
as well as the three immediate neighbors on both 
the left and right hand sides. 
We constructed a gazetteer of possible trigger 
lemmas in the following manner. First we used a 
rule-based morphological analyzer (Heidorn, 2000) 
to identify the lemma of all words in the training, 
development, and test corpora. Next, for each word 
in the training and development sets, we mapped it 
to its lemma. We then computed the number of 
times that each lemma occurred as a trigger for 
each type of event (and none). Lemmas that acted 
as a trigger more than 50% of the time were added 
to the gazetteer. 
During feature extraction for a given token, we 
found the lemma of the token, and then look up 
that lemma in the gazetteer. If found, we included 
a binary feature to indicate its trigger type. 
Frequency Features. We included as features 
the number of entities in the sentence, a bag of 
words from the current sentence, and a bag of 
entities in the current sentence. 
Dependency Features. We used primarily a set 
of dependency chain features that were helpful in 
the past (Bj?rne et al, 2009); these features walk 
the Stanford Typed Dependency edges up to a 
distance of 3. 
We also found it helpful to have features about 
the path to the nearest protein, regardless of 
distance. In cases of multiple shortest paths, we 
took only one, exploring the dependency tree 
generally in left to right order. For each potential 
trigger, we looked at the dependency edge labels 
leading to that nearest protein. In addition we had a 
feature including both the dependency edge labels 
and the token text (lowercased) along that path. 
Finally, we had a feature indicating whether some 
token along that path was also in the trigger 
gazetteer. The formulation of this set of features is 
still not optimal especially for the ?binding? events 
as the training data will include paths to more than 
one protein argument.  Nevertheless, in Table 3, 
 
Key Relation Value Key Relation Value 
quantities child(left, NNS?JJ) measurable measurable child-1(left, NNS?JJ) quantities 
found child(after, VBN?NNS) hours hours child-1(after, VBN?NNS) found 
found child(after, VBN?NN) ingestion ingestion child-1(after, VBN?NN) found 
 
Figure 2: A sample PubMed sentence along with its dependency parse, and some key/relation/value triples 
extracted from that parse for computation of distributional similarity. Keys with a similar distribution of values 
under the same relation are likely semantically related. Inverse relations are indicated with a superscript -1. 
Prepositions are handled specially: we add edges labeled with the preposition from its parent to each child 
(indicated by dotted edges). 
158
we can see that this set of features contributed to 
improved precision. 
Cluster Features. Lexical and stem features 
were crucial for accuracy, but were unfortunately 
sparse and did not generalize well. To mitigate 
this, we incorporated word cluster features. In 
addition to the lexical item and the stem, we added 
another feature indicating the cluster to which each 
word belongs. To train clusters, we downloaded all 
the PubMed abstracts (http://pubmed.gov), parsed 
them with a simple dependency parser (a 
reimplementation of McDonald, 2006 trained on 
the GENIA corpus), and extracted dependency 
relations to use in clustering: words that occur in 
similar contexts should fall into the same cluster. 
An example sentence and the relations that were 
extracted for distributional similarity computation 
are presented in Figure 2. We ran a distributional 
similarity clustering algorithm (Pantel et al, 2009) 
to group words into clusters. 
Tfidf features. This set of features was intended 
to capture the salience of a term in the medical and 
?general? domain, with the aim of being able to 
distinguish domain-specific terms from more 
ambiguous terms. We calculated the tf.idf score for 
each term in the set of all PubMed abstracts and 
did the same for each term in Wikipedia. For each 
token in the input data, we then produced three 
features: (i) the tf.idf value of the token in PubMed 
abstracts, (ii) the tf.idf value of the token in 
Wikipedia, and (iii) the delta between the two 
values. Feature values were rounded to the closest 
integer. We found, however, that adding these 
features did not improve results. 
2.2.2 Feature combination and reduction 
We experimented with feature reduction and 
feature combination within the set of features 
described here. For feature reduction we tried a 
number of simple approaches that typically work 
well in text classification. The latter is similar to 
the task at hand, in that there is a very large but 
sparse feature set. We tried two feature reduction 
methods: a simple count cutoff, and selection of 
the top n features in terms of log likelihood ratio 
(Dunning, 1993) with the target values. For a count 
cutoff, we used cutoffs from 3 to 10, but we failed 
to observe any consistent gains. Only low cutoffs 
(3 and occasionally 5) would ever produce any 
small improvements on the development set. Using 
log likelihood ratio (as determined on the training 
set), we reduced the total number of features to 
between 10,000 and 75,000. None of these 
experiments improved results, however. One 
potential reason for this negative result may be that 
there were a lot of features in our set that capture 
the same phenomenon in different ways, i.e. which 
correlate highly. By retaining a subset of the 
original feature set using a count cutoff or log 
likelihood ratio we did not reduce this feature 
overlap in any way. Alternative feature reduction 
methods such as Principal Component Analysis, on 
the other hand, would target the feature overlap 
directly. For reasons of time we did not experiment 
with other feature reduction techniques but we 
believe that there may well be a gain still to be had. 
For our feature combination experiments the 
idea was to find highly predictive Boolean 
combinations of features. For example, while the 
features a and b may be weak indicators for a 
particular trigger, the cases where both a and b are 
present may be a much stronger indicator. A linear 
classifier such as the one we used in our 
experiments by definition is not able to take such 
Boolean combinations into account. Some 
classifiers such as SVMs with non-linear kernels 
do consider Boolean feature combinations, but we 
found the training times on our data prohibitive 
when using these kernels. As an alternative, we 
decided to pre-identify feature combinations that 
are predictive and then add those combination 
features to our feature inventory. In order to pre-
identify feature combinations, we trained decision 
tree classifiers on the training set, and treated each 
path from the root to a leaf through the decision 
tree classifier as a feature combination. We also 
experimented with adding all partial paths through 
the tree (as long as they started from the root) in 
addition to adding all full paths. Finally, we tried 
to increase the diversity of our combination 
features by using a ?bagging? approach, where we 
trained a multitude of decision trees on random 
subsets of the data. Again, unfortunately, we did 
not find any consistent improvements. Two 
observations that held relatively consistently across 
our experiments with combination features and 
different feature sets were: (i) only adding full 
paths as combination features sometimes helped, 
while adding partial paths did not, and (ii) bagging 
hardly ever led to improvements. 
159
2.3 Edge Detection 
This phase of the pipeline was again modeled as 
multi-class classification. There could be an edge 
originating from any trigger word and ending in 
any trigger word or protein. Looking at the set of 
all such edges, we trained a classifier to predict the 
label of this edge, or NONE if the edge was not 
present. Here we found that a maximum entropy 
classifier performed somewhat better than an SVM, 
so we used an in-house implementation of a 
maximum entropy trainer to produce the models. 
2.3.1 Features for Edge Detection 
As with trigger detection, our initial feature set for 
edge detection was strongly influenced by features 
that were successful in Bj?rne et al (2009). 
Additionally, we included the same dependency 
path features to the nearest protein that we used for 
trigger detection, described in 2.2.1. Further, for a 
prospective edge between two entities, where the 
entities are either a trigger and a protein, or a 
trigger and a second trigger, we added a feature 
that indicates (i) if the second entity is in the path 
to the nearest protein, (ii) if the head of the second 
entity is in the path to the nearest protein, (iii) the 
type of the second entity.   
2.4 Post-processing 
Given the set of edges, we used a simple 
deterministic procedure to produce a set of events. 
This step is not substantially different from that 
used in prior systems (Bj?rne et al, 2009). 
2.4.1 Balancing Precision and Recall 
As in Bj?rne et al (2009), we found that the trigger 
detector had quite low recall. Presumably this is 
due to the severe class imbalance in the training 
data: less than 5% of the input tokens are triggers. 
Thus, our classifier had a tendency to overpredict 
NONE. We tuned a single free parameter ? ? ?? 
(the ?recall booster?) to scale back the score 
associated with the NONE class before selecting 
the optimal class. The value was tuned for whole-
system F-measure; optimal values tended to fall in 
the range 0.6 to 0.8, indicating that only a small 
shift toward recall led to the best results. 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Gene_expression 749 76.37 81.46 78.83 1002 73.95 73.22 73.58 
Transcription 158 49.37 73.58 59.09 174 41.95 65.18 51.05 
Protein_catabolism 23 69.57 80.00 74.42 15 46.67 87.50 60.87 
Phosphorylation 111 73.87 84.54 78.85 185 87.57 81.41 84.37 
Localization 67 74.63 75.76 75.19 191 51.31 79.03 62.22 
=[SVT-TOTAL]= 1108 72.02 80.51 76.03 1567 68.99 74.03 71.54 
Binding 373 47.99 50.85 49.38 491 42.36 40.47 41.39 
=[EVT-TOTAL]= 1481 65.97 72.73 69.18 2058 62.63 65.46 64.02 
Regulation 292 32.53 47.05 38.62 385 24.42 42.92 31.13 
Positive_Regulation 999 38.74 51.67 44.28 1443 37.98 44.92 41.16 
Negative_Regulation 471 35.88 54.87 43.39 571 41.51 42.70 42.10 
=[REG-TOTAL]= 1762 36.95 51.79 43.13 2399 36.64 44.08 40.02 
ALL-Total 3243 50.20 62.60 55.72 4457 48.64 54.71 51.50 
Table 1: Approximate span matching/approximate recursive matching on development and test data 
sets for GENIA Shared Task -1 with our system. 
Trigger 
Detection 
Features 
Trigger 
Loss Recall Prec. F1 
B 2.14 48.44 64.08 55.18 
B + TI 2.14 48.17 62.49 54.40 
B + TI + C 2.14 50.32 60.90 55.11 
B + TI + C + PI 2.03 50.20 62.60 55.72 
B + TI + C + PI 
+D 
2.02 49.21 62.75 55.16 
Table 2: Recall/Precision/F1 on the GENIA 
development set using MCCC-I + Enju parse; 
adding different features for Trigger Detection. 
B = Base set Features, TI = Trigger inflect 
forms, 
160
3 Results 
Of the five evaluation tracks in the shared task, we 
participated in two: the GENIA core task, and the 
EPI (Epigenetics and Post-translational 
modifications) task. The systems used in each track 
were substantially similar; differences are called 
out below. Rather than building a system 
customized for a single trigger and event set, our 
goal was to build a more generalizable framework 
for event detection. 
3.1 GENIA Task 
Using F-measure performance on the development 
set as our objective function, we trained the final 
system for the GENIA task with all the features 
described in section 2, but without the conversion 
rules and without either feature combination or 
reduction. Furthermore, we trained the cluster 
features using the full set of PubMed documents 
(as of  January 2011). The results of our final 
submission are summarized in Table 1. Overall, we 
saw a substantial degradation in F-measure when 
moving from the development set to the test set, 
though this was in line with past experience from 
our and other systems.  
We compared the results for different parsers in 
Table 3. MCCC-I is not better in isolation but does 
produce higher F-measures in combination with 
other parsers. Although posteriors were not 
particularly helpful on the development set, we ran 
Parser 
SVT-Total Binding REG-Total All-Total 
Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 
MCCC 70.94 82.72 76.38 45.04 55.26 49.63 34.39 51.88 41.37 48.10 64.39 55.07 
MCCC-I 68.59 82.59 74.94 42.63 58.67 49.38 32.58 52.76 40.28 46.06 65.50 54.07 
Enju 71.66 82.18 76.56 40.75 51.01 45.31 32.24 49.39 39.01 46.69 62.70 53.52 
MCCC-I + 
Posteriors 
70.49 78.87 74.44 47.72 51.59 49.58 35.64 50.40 41.76 48.94 61.47 54.49 
MCCC + 
Enju 
71.84 82.04 76.60 44.77 53.02 48.55 34.96 53.15 42.18 48.69 64.59 55.52 
MCCC-I + 
Enju 
72.02 80.51 76.03 47.99 50.85 49.38 36.95 51.79 43.13 50.20 62.60 55.72 
Table 3: Comparison of Recall/Precision/F1 on the GENIA Task-1 development set using various 
combinations of parsers: Enju, MCCC (Mc-Closky Charniak), and MCCC-I (Mc-Closky Charniak 
Improved self-trained biomedical parsing model) with Stanford collapsed dependencies were used for 
evaluation. Results on Simple, Binding and Regulation and all events are shown. 
 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Hydroxylation 31 25.81 61.54 36.36 69 30.43 84.00 44.68 
Dehydroxylation 0 100.00 100.00 100.00 0 100.00 100.00 100.00 
Phosphorylation 32 71.88 85.19 77.97 65 72.31 85.45 78.33 
Dephosphorylation 1 0.00 0.00 0.00 4 0.00 0.00 0.00 
Ubiquitination 76 63.16 75.00 68.57 180 67.78 81.88 74.16 
Deubiquitination 8 0.00 0.00 0.00 10 0.00 0.00 0.00 
DNA_methylation 132 72.73 72.18 72.45 182 71.43 73.86 72.63 
DNA_demethylation 9 0.00 0.00 0.00 6 0.00 0.00 0.00 
Glycosylation 70 61.43 67.19 64.18 169 39.05 69.47 50.00 
Deglycosylation 7 0.00 0.00 0.00 12 0.00 0.00 0.00 
Acetylation 65 89.23 75.32 81.69 159 87.42 85.28 86.34 
Deacetylation 19 68.42 92.86 78.79 24 62.50 93.75 75.00 
Methylation 65 64.62 75.00 69.42 193 62.18 73.62 67.42 
Demethylation 7 0.00 0.00 0.00 10 0.00 0.00 0.00 
Catalysis 60 3.33 15.38 5.48 111 4.50 33.33 7.94 
====[TOTAL]==== 582 57.22 72.23 63.85 1194 55.70 77.60 64.85 
Table 4: Approximate span matching/approximate recursive matching on development and test data 
sets for EPI CORE Task with our system 
161
a system consisting of MCCC-I with posteriors 
(MCCC-I + Posteriors) on the test set after the 
final results were submitted, and found that it was 
competitive with our submitted system (MCCC-I + 
ENJU). We believe that ambiguity preservation 
has merit, and hope to explore more of this area in 
the future. Diversity is important: although the 
ENJU parser alone was not the best, combining it 
with other parsers led to consistently strong results.  
Table 2 explores feature ablation: TI appears to 
degrade performance, but clusters regain that loss. 
Protein depth information was helpful, but 
dependency rule conversion was not.  Therefore 
the B+TI+C+PI combination was our final 
submission on GENIA.  
3.2 EPI Task 
We trained the final system for the Epigenetics 
task with all the features described in section 2. 
Further, we produced the clusters for the 
Epigenetics task using only the set of GENIA 
documents provided in the shared task. 
In contrast to GENIA, we found that the 
dependency rule conversions had a positive impact 
on development set performance. Therefore, we 
included them in the final system. Otherwise the 
system was identical to the GENIA task system.  
4 Discussion 
After two rounds of the BioNLP shared task, in 
2009 and 2011, we wonder whether it might be 
possible to establish an upper-bound on recall and 
precision. There is considerable diversity among 
the participating systems, so it would be interesting 
to consider whether there are some annotations in 
the development set that cannot be predicted by 
any of the participating systems1. If this is the case, 
then those triggers and edges would present an 
interesting topic for discussion. This might result 
either in a modification of the annotation protocols, 
or an opportunity for all systems to learn more. 
After a certain amount of feature engineering, 
we found it difficult to achieve further 
improvements in F1. Perhaps we need a significant 
shift in architecture, such as a shift to joint 
inference (Poon and Vanderwende, 2010). Our 
system may be limited by the pipeline architecture. 
                                                          
1 Our system output for the 2011development set can be 
downloaded from http://research.microsoft.com/bionlp/ 
MWEs (multi-word entities) are a challenge. 
Better multi-word triggers accuracy may improve 
system performance. Multi-word proteins often led 
to incorrect part-of-speech tags and parse trees. 
Cursory inspection of the Epigenetics task 
shows that some domain-specific knowledge 
would have been beneficial. Our system had 
significant difficulties with the rare inverse event 
types, e.g. ?demethylation? (e.g., there are 319 
examples for ?methylation? in the combined 
training/development set, but only 12 examples for 
?demethylation?). Each trigger type was treated 
independently, thus we did not share information 
between an event and its related inverse event type. 
Furthermore, our system also failed to identify 
edges for these rare events. One approach would 
be to share parameters between types that differ 
only in a prefix, e.g., ?de?. In general, some 
knowledge about the hierarchy of events may let 
the learner generalize among related events. 
5 Conclusion and Future Work 
We have described a system designed for fine-
grained information extraction, which we show to 
be general enough to achieve good performance 
across different sets of event types and domains.  
The only domain-specific characteristic is the pre-
annotation of proteins as a special class of entities. 
We formulated some features based on this 
knowledge, for instance the path to the nearest 
protein.  This would likely have analogues in other 
domains, given that there is often a special class of 
target items for any Information Extraction task. 
As the various systems participating in the 
shared task mature, it will be viable to apply the 
automatic annotations in an end-user setting.  
Given a more specific application, we may have 
clearer criteria for balancing the trade-off between 
recall and precision.  We expect that fully-
automated systems coupled with reasoning 
components will need very high precision, while 
semi-automated systems, designed for information 
visualization or for assistance in curating 
knowledge bases, could benefit from high recall.  
We believe that the data provided for the shared 
tasks will support system development in either 
direction. As mentioned in our discussion, though, 
we find that improving recall continues to be a 
major challenge. We seek to better understand the 
data annotations provided. 
162
Our immediate plans to improve our system 
include semi-supervised learning and system 
combination.  We will also continue to explore 
new levels of linguistic representation to 
understand where they might provide further 
benefit.  Finally, we plan to explore models of joint 
inference to overcome the limitations of pipelining 
and deterministic post-processing. 
Acknowledgments 
We thank the shared task organizers for providing 
this interesting task and many resources, the Turku 
BioNLP group for generously providing their 
system and intermediate data output, and Patrick 
Pantel and the MSR NLP group for their help and 
support. 
References  
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola, 
Tapio Pahikkala and Tapio Salakoski. 2009. 
Extracting Complex Biological Events with Rich 
Graph-Based Feature Sets. In Proceedings of  the 
Workshop on BioNLP: Shared Task. 
Thomas Cormen, Charles Leiserson, and Ronald Rivest. 
2002. Introduction to Algorithms. MIT Press. 
Ted Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational 
Linguistics, 19(1), pp. 61-74. 
George E. Heidorn, 2000. Intelligent Writing 
Assistance. In Handbook of Natural Language 
Processing, ed. Robert Dale, Hermann Moisl, and 
Harold Somers.  Marcel Dekker Publishers. 
Karen Jensen. 1993. PEGASUS: Deriving Argument 
Structures after Syntax. In Natural Language 
Processing: the PLNLP approach, ed. Jensen, K., 
Heidorn, G.E., and Richardson, S.D. Kluwer 
Academic Publishers. 
Marie-Catherine de Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. In 
LREC 2006. 
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest 
models for probabilistic HPSG parsing. 
Computational Linguistics 34(1): 35-80. 
David McClosky and Eugene Charniak. 2008.  Self-
Training for Biomedical Parsing. In Proceedings of 
the Association for Computational Linguistics 2008. 
David McClosky. 2010. Any Domain Parsing: 
Automatic Domain Adaptation for Natural Language 
Parsing. Ph.D. thesis, Department of Computer 
Science, Brown University.  
Ryan McDonald. 2006. Discriminative training and 
spanning tree algorithms for dependency parsing. Ph. 
D. Thesis. University of Pennsylvania. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based Translation.  In Proceedings of ACL 2008, 
Columbus, OH. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009.  
Hoifung Poon and Lucy Vanderwende. 2010. Joint 
inference for knowledge extraction from biomedical 
literature. In Proceedings of NAACL-HLT 2010. 
Martin.F. Porter, 1980, An algorithm for suffix 
stripping, Program, 14(3):130?137. 
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten 
Joachims, and Yasemin Alton. 2004. Support vector 
machine learning for interdependent and structured 
output spaces. In ICML 2004. 
163
