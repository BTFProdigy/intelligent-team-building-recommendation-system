Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 62?68,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Hands-On NLP for an Interdisciplinary Audience 
 
 
Elizabeth D. Liddy and Nancy J. McCracken 
Center for Natural Language Processing 
School of Information Studies 
Syracuse University 
liddy@syr.edu, njm@ecs.syr.edu 
 
 
 
 
Abstract 
The need for a single NLP offering for a 
diverse mix of graduate students (including 
computer scientists, information scientists, 
and linguists) has motivated us to develop a 
course that provides students with a breadth 
of understanding of the scope of real world 
applications, as well as depth of knowledge 
of the computational techniques on which 
to build in later experiences. We describe 
the three hands-on tasks for the course that 
have proven successful, namely: 1) in-class 
group simulations of computational proc-
esses;  2) team posters and public presenta-
tions on state-of-the-art commercial NLP 
applications, and; 3) team projects imple-
menting various levels of human language 
processing using open-source software on 
large textual collections. Methods of 
evaluation and indicators of success are 
also described. 
1 Introduction 
This paper presents both an overview and some of 
the details regarding audience, assignments, tech-
nology, and projects in an interdisciplinary course 
on Natural Language Processing that has evolved 
over time and been successful along multiple di-
mensions ? both from the students? and the fac-
ulty?s perspective in terms of accomplishments and 
enjoyment. This success has required us to meet 
the challenges of enabling students from a range of 
disciplines and diverse experience to each gain a 
real understanding of what is entailed in Natural 
Language Processing. 
2 A Course Within Multiple Curricula 
The course is entitled Natural Language Processing 
and is taught at the 600 graduate course level in a 
School of Information Studies in a mid to large-
size private university. While NLP is not core to 
any of the three graduate degree programs in the 
Information School, it is considered an important 
area within the Information School for both profes-
sional careers and advanced research, as well as in 
the Computer Science and Linguistic Programs on 
campus. The course has been taught every 1? to 2 
years for the last 18 years. While some aspects of 
the course have changed dramatically, particularly 
in regards to the nature of the student team pro-
jects, the basic structure ? the six levels of lan-
guage processing ? has remained essentially the 
same, with updates to topics within these levels 
reflecting recent research findings and new appli-
cations. 
3 Audience 
At the moment, this is the only course offering on 
NLP within the university, but a second-level, 
seminar course, entitled Content Analysis Research 
Using Natural Language Processing, geared to-
wards PhD students doing social science research 
on large textual data sets, will be offered for the 
first time in Fall 2005. Given that the current NLP 
course is the only one taught, it cannot, by neces-
sity, have the depth that could be achieved in cur-
ricula where there are multiple courses. In a more 
extensive curriculum, courses provide a greater 
depth than is possible in our single course.  Our 
goal is to provide students with a solid, broad basis 
on which to build in later experiences, and to en-
62
able real understanding of a complex topic for 
which students realize there is a much greater 
depth of understanding that could be reached. 
The disciplinary mix of students in the course is 
usually an even mix of information science and 
computer science students, with slightly fewer lin-
guistics majors. Recently the Linguistics Depart-
ment has established a concentration in 
Information Representation and Retrieval, for 
which the NLP course is a required course. Also, 
the course is cross-listed as an elective for com-
puter science graduate students. All of the above 
facts contribute to the widely diverse mix of stu-
dents in the NLP course, and has required us to 
develop a curriculum that enables all students to be 
successful in achieving solid competency in NLP. 
4 Topics Covered 
The topics in the course include typical ones cov-
ered in most NLP courses and are organized 
around the levels of language processing and the 
specific computational techniques within each of 
these. Discussions of more general theoretic no-
tions such as statistical vs. symbolic NLP, repre-
sentation theories, and language modeling are 
interspersed. A single example of topics that are 
taught within the levels of language processing 
include: 
 
Morphology - Finite state automata 
Lexicology - Part-of-speech tagging 
Syntax - Parsing with context free grammars 
Semantics - Word sense disambiguation 
Discourse - Sublanguage analysis 
Pragmatics - Gricean Maxims 
 
Each of the topics has assigned readings, from the 
course?s textbook, Speech and Language Process-
ing: An Introduction to Natural Language Process-
ing, Computational Linguistics, and Speech 
Recognition by Daniel Jurafsky & James H. Mar-
tin, as well as from recent and seminal papers. 
5 Methods 
What really enables the students to fully grasp the 
content of the course are the three important hands-
on features of the course, namely: 
1. Small, in-class group simulations of compu-
tational processes.   
2. Team posters and public presentations re-
porting on the state-of-the-art in commer-
cial NLP applications such as 
summarization, text mining, machine 
translation, question answering, speech 
recognition, and natural language genera-
tion.  
3. Team projects implementing various levels 
of human language processing using open-
source software on large collections. 
Each of these features of the course is described in 
some detail in the following sections.  
The course is designed around group projects, 
while the membership of the teams changes for 
each assignment. This is key to enabling a diverse 
group to learn to work with students from different 
disciplines and to value divergent experience. It 
has also proven extremely successful in forming a 
class that thinks of itself as a community and in 
encouraging sharing of best practices so that eve-
ryone advances their learning significantly further 
than if working alone or with the same team 
throughout the course. The way that teams are 
formed for the three types of projects varies, and 
will be described in each of the following three 
sections. 
Furthermore, constant, frequent presentations to 
the class of the group work, no matter how brief, 
enable students to own their newly-gained under-
standings. In fact, this course no longer requires 
any written papers, but instead focuses on applica-
tion of what is learned, first at the specific level of 
language processing, then to new data for new 
purposes, and then, to understanding real-world 
NLP systems performing various applications ? 
with the group constantly reporting their findings 
back to the class. 
5.1 In-class Group Simulations of Computa-
tional Processes 
During the first third of the course, lectures on 
each level of language processing are followed by 
a 30 to 45 minute exercise that enables the students 
who work in small groups to simulate the process 
they have just learned about, i.e. morphological 
analysis, part-of-speech tagging, or parsing some 
sample sentences with a small grammar. These 
groups are formed by the professor in an ad hoc 
manner by counting off by 4 in a different pattern 
each week to ensure that students work with stu-
63
dents on the other side of the room, given that 
friends or students from the same school tend to sit 
together. After the exercise, each group has 5 min-
utes to report back to the class on how they ap-
proached the task, with visuals.  
We?ve found that the formation of these small 
groups is pedagogically sound and enables learning 
in three ways. First, the groups break down social 
barriers and as the course advances the students 
find it much easier to work together and are more 
comfortable in sharing their work. Secondly, the 
students begin to understand and value what the 
students from different disciplines bring to bear on 
NLP problems. That is, the computer scientists 
recognize the value of the deeper understanding of 
language of the linguistic students, and the linguis-
tic students learn how the computer science stu-
dents approach the task computationally. Thirdly, 
while there were concerns on our part that these 
simulations might be too easy, the students have 
affirmed in mid-term course evaluations (which are 
not required, but do provide invaluable insight into 
a class?s engagement with and assimilation of the 
material) that these simulations really help them to 
understand conceptually what the task is and how 
it might be accomplished before they have to 
automate the processes. 
5.2 Real World Applications of NLP 
This year, two semester-long team projects were 
assigned ? the usual team-based computer imple-
mentation of NLP for a particular computational 
task ? and an investigation into how NLP is util-
ized in various state-of-the-art commercial NLP 
applications. The motivation for adding this second 
semester-long team project was that a number of 
the students in the course, particularly the masters 
students in Information Management, are most 
likely to encounter NLP in their work world when 
they need to advise on particular language-based 
applications. It has become clear, however, that as 
a result of this assignment, all of the students are 
quite pleased with their own improved ability to 
understand what a language-based technology is 
actually doing. Even if a student is more research-
focused, they are intrigued by what might be done 
to improve or add to a particular technology. 
Students are given two weeks to familiarize 
themselves outside of class with the suggested ap-
plications sufficiently to select a topic of real inter-
est to them. This year?s choices included Spell 
Correction, Machine Translation, Search Engines, 
Text Mining, Summarization, Question Answer-
ing, Speech Recognition, Cross-Language Infor-
mation Retrieval, Natural Language Generation, 
and Dialogue Agents.  
Students then sign up, on a first-come basis, for 
their preferred application. The teams are kept 
small (up to four) to ensure that each student con-
tributes. At times a single student is sufficiently 
interested in a topic that a team of one is formed. 
Students arrange their own division of labor. There 
are three 10 to 20 minute report-backs by each 
team over the course of the semester, the first two 
to the class and the final one during an open invita-
tion, school-wide Poster & Reception event. There 
are guidelines for each of the three presentations, 
as well as a stated expectation that the teams ac-
tively critique and comment on the presentations, 
both in terms of the information presented as well 
as presentational factors. Five minutes are allowed 
for class comments and students are graded on how 
actively they participate and provide feedback. 
The 1st presentation is a non-technical overview 
of what the particular NLP application does and 
includes examples of publicly available systems / 
products the class might know. The 2nd presenta-
tion covers technical details of the application, 
concentrating on the computational linguistic as-
pects, particularly how such an application typi-
cally works, and the levels of NL processing that 
are involved (e.g., lexical, syntactic, etc). The 3rd 
presentation involves a poster which incorporates 
the best of their first two presentations and sugges-
tions from the class, plus a laptop demo if possible. 
As stated above, the 3rd presentation is done in 
an open school-wide Poster and Reception event 
which is attended by faculty and students, mainly 
PhD students. The Poster Receptions have proven 
very successful along multiple dimensions ? first, 
the students take great pride in the work they are 
presenting;  second, posters are better than one-
time, in-class presentations as the multiple oppor-
tunities to explain their work and get feedback im-
prove the students? ability to create the best 
presentation of their work; third, the wider expo-
sure of the field and its applications builds an audi-
ence for future semesters and instills in the student 
body a sense of the reach and importance of NLP. 
5.3 Hands-On NL Processing of Text  
64
The second of the semester-long team projects is 
the computer implementation of NLP.  The goal of 
the project is for students to gain hands-on experi-
ence in utilizing NLP software in the context of 
accomplishing analysis of a large, real-world data 
set. The project comprises two tasks, each of which 
is reported back to the class by each team. These 
presentations were not initially in the syllabus, but 
interestingly, the students requested that each team 
present after each task so that they could all learn 
from the experiences of the other teams. 
The corpus chosen was the publicly available 
Enron email data set, which consists of about 
250,000 unique emails from 150 people. With du-
plication, the data has approximately 500,000 files 
and takes up 2.75 gigabytes. The data set was pre-
pared for public release by William Cohen at CMU 
and, available at http://www-2.cs.cmu.edu/~enron/. 
This data set is useful not only as real text of the 
email genre, but it can be easily divided into 
smaller subsets suitable for student projects. (And, 
of course, there is also the human interest factor in 
that the data set is available due to its use in the 
Enron court proceedings!) 
The goal of the project is to use increasing lev-
els of NLP to characterize a selected subset of En-
ron email texts. The project is designed to be 
carried out in two parts, involving two assigned 
levels of NLP. The first level, part-of-speech tag-
ging, is accomplished as Task 1 and the second, 
phrase-bracketing or chunk-parsing, is assigned as 
Task 2. However, the overall characterization of 
the text is left open-ended, and the student teams 
chose various dimensions for their analyses.  Pro-
jects included analyzing the topics of the emails of 
different people, social network analyses based on 
people and topics mentioned in the email text, and 
analyses based on author and recipient header in-
formation about each email. 
Teams are established for these projects by the 
professor based on the capabilities and interests of 
the individual students as reported in short self-
surveys. This resulted in teams on which there is a 
mix of computer science, linguistics and informa-
tion science expertise. The teams accomplished the 
tasks of choosing a data analysis method, process-
ing data subsets, designing NL processing to ac-
complish the analysis, programming the NL 
processing, conducting the data analysis, and pre-
paring the in-class reports. 
5.3.1 Tools Used in the Project  
For preliminary processing of the Enron email 
files, programs and data made available by Profes-
sor Andr?s Corrada-Emmanuel at the University of 
Massachusetts at Amherst, and available at 
http://ciir.cs.umass.edu/~corrada/ were used. The 
emails were assigned MD5-digest numbers in or-
der to identify them uniquely, and the data con-
sisted of mappings from the digest numbers to 
files, as well as to authors and recipients of the 
email. The programs contained filters that could be 
used to remove extraneous text such as headers and 
forwarded text. The teams adapted parts of these 
programs to convert the email files to files with 
text suitable for NL processing. 
For the NL processing, the Natural Language 
Toolkit (NL Toolkit or NLTK), developed at the 
University of Pennsylvania by Loper and Bird 
(2002), and available for download from Source-
Forge at http://nltk.sourceforge.net/ was used.  The 
NL Toolkit is a set of libraries written in the Py-
thon programming language that provides core 
data types for processing natural language text, 
support for statistical processing, and a number of 
standard processing algorithms used in NLP, in-
cluding tokenization, part of speech (POS) tagging, 
chunk parsing, and syntactic parsing. The toolkit 
provides demonstration packages, tutorials, exam-
ple corpora and documentation to support its use in 
educational classes.  Experience using the Toolkit 
shows that in order to use the NL Toolkit, one 
member of each team should have at least some 
programming background in order to write Python 
programs that use the NL Toolkit libraries.  The 
use of Python as the programming language was 
successful in that the level needed to use the NL 
Toolkit was manageable by the students with only 
a little programming background and in that the 
computer science students were able to adapt to the 
Python programming style and could easily utilize 
the classes and libraries. 
At the beginning of the term project, the stu-
dents were offered a lab session and lab materials 
to get them started. Since no one knew the Python 
programming language at the outset, there was an 
initial learning curve for the Python language as 
well as for the NL Toolkit. The lab materials pro-
vided to the students consisted of installation in-
structions for Python and NL Toolkit and a number 
of example programs that combined programming 
65
snippets from the NL Toolkit tutorials to process 
text through the NLP phases of tokenization, POS 
tagging and the construction of frequency distribu-
tions over the POS tagged text. During the lab ses-
sion, some of the example programs were worked 
through as a group with the goal of enabling the 
students to become competent in Python and to 
introduce them to the NL Toolkit tutorials that had 
additional materials. The NL Toolkit tutorials are 
extensive on the lower levels of NL processing 
(e.g. lexical and syntactic) and students with some 
programming background were able to utilize 
them. 
As part of their first task, the student teams were 
asked to select a subset of the Enron emails to 
work with. The entire Enron email directories were 
placed on a server for the teams to look at in mak-
ing their selections. The teams also used informa-
tion about the Enron employees as described in a 
paper by Corrada-Emmanuel (2005). Some student 
teams elected to work with different email topic 
folders for one person, while others chose a few 
email folders each from a small number of people 
(2-5). Their selected emails first needed to be 
processed to text using programs adapted from 
Corrada-Emmanuel. For the most part, the sub-
corpora choices of the student teams worked out 
well in terms of size and content. Several hundred 
emails turned out to be a good size, providing 
enough data to experience the challenges of long 
processing times and to appreciate why NLP is 
useful in processing large amounts of data, without 
being unduly overwhelmed. Initially, one team 
chose all the emails from several people. The 
number of email files involved was several thou-
sand and it took several hours to unzip those direc-
tories, let alne process them, and they 
subsequently reduced the number of files for their 
analysis. 
The first task was to analyze the chosen emails 
based solely on lexical level information, namely 
words with POS tags. NL Toolkit provides librar-
ies for tokenization where the user can define the 
tokens through regular expressions, and the stu-
dents used these to tailor the tokenization of their 
emails. The Toolkit also provides a regular expres-
sion POS tagger as well as n-gram taggers, and the 
students used these in combination for their POS 
tagging. Students experimented with the Brown 
corpus and a part of the Penn Treebank corpus, 
provided by NL Toolkit to train the POS taggers, 
and compared the results.  
Building on the first task, the second task ex-
tended the analysis of the chosen emails to phrases 
from the text. Again, NL Toolkit provides a library 
for chunk parsing where regular expressions can be 
used to specify patterns of words with POS tags 
either to be included or excluded from phrases. 
Since chunk parsing depends on POS tagging, 
there was a need for a larger training corpus. A 
research center within the Information School has 
a license for Penn Treebank, and  provided addi-
tional Penn Treebank files for the class to use for 
that purpose. Most teams used regular expressions 
to bracket proper names, minimal noun phrases, 
and verb phrases. One team used these to group 
maximal noun phrases, and another team used 
regular expressions to find patterns of communica-
tion verbs for use in social network analysis.   
In retrospect, it was found that the chunk pars-
ing did not take the teams far enough in NLP 
analysis of text. Experience in teaching using the 
NL Toolkit suggests that use of the syntactic pars-
ing libraries to find more complex structures in the 
text would have provided more depth of analysis. 
Students also suggested that they would have liked 
to incorporate semantic level capabilities, such as 
the use of WordNet to find conceptual groupings 
via synonym recognition. The next offering of the 
course will include these improvements. 
Using the NL Toolkit for NL processing worked 
out well overall and enabled the students to ob-
serve and appreciate details of the processing steps 
without having to write a program for every algo-
rithm themselves. The tutorials are good, both at 
explaining concepts and providing programming 
examples. There were a few places where some 
data structure details did not seem to be suffi-
ciently documented, either in the tutorials or in the 
API.  This was true for  the recently added Brill 
POS tagger, and is likely due to its recency of ad-
dition to the toolkit.  However for the most part, 
the coverage of the documentation is impressive. 
6 Evaluation  
Multiple types of evaluation are associated with 
the course. First, the typical evaluation of the stu-
dents by the professor (here, 2 professors) was 
done on multiple dimensions that contributed pro-
portionately to the student?s final grade as follows: 
66
 
? In-Class group exercises 20% 
? NLToolkit Team Assignments 35% 
? NLP Application Team Poster &  
         Presentations 
35% 
? Contributions to class discussion  
         (both quality and quantity) 
10% 
 
Additionally, each team member evaluated each of 
their fellow team members as well as themselves. 
This was done for both of the teams in which a 
student participated. For each team member, the 
questions covered:  the role or tasks of the student 
on the project; an overall performance rating from 
1 for POOR to 4 for EXCELLENT; the rationale 
for this score, and finally; what the student could 
have done to improve their contribution. Knowl-
edge of this end-of-semester team self-evaluation 
tended to ensure that students were active team 
contributors. 
The professor was also evaluated by the stu-
dents. And while there are quantitative scores that 
are used by the university for comparison across 
faculty and to track individual faculty improve-
ments over time, the most useful feature of the stu-
dent evaluations is the set of open-ended questions 
concerning what worked well in the course, what 
didn?t work well, and what could be done to im-
prove the course. Over the years of teaching this 
course, these comments (plus the mid-term evalua-
tions) have been most instructive in efforts to find 
ways to improve the course. Frequently the sugges-
tions are very practical and easy to implement, 
such as showing a chart with the distribution of 
grades on each assignment when they are returned 
so that the students know where they stand relative 
to the class as grading is on a scale of 1 to 10. 
 
7. Indicators of Success 
 
Finally, how is the success of this course measured 
in the longer term?  For this, success is measured 
by:  whether students elect to do continued work in 
NLP, either in the context of further courses in 
which NLP is utilized, such as Information Re-
trieval or Text Mining;  whether the masters (and 
undergraduate) students decide to pursue an ad-
vanced degree based on the excitement engendered 
and knowledge gained from the NLP course; or 
whether PhD students elect to do continued re-
search either in the school?s Center for Natural 
Language Processing or as part of their disserta-
tion. For students in a terminal degree program, 
success is reflected by their seeking and obtaining 
jobs that utilize the NLP they have learned in the 
course and that has provided them with a solid, 
broad basis on which to build. For several of the 
undergraduate computer science students in the 
course, their NLP experience has given them an 
added dimension of specialization and competitive 
advantage in a tight hiring market.  
An additional measure of success was the re-
quest by the doctoral students in the home school 
for a PhD level seminar course to build on the NLP 
course. This course is entitled Content Analysis 
Research Using Natural Language Processing and 
will enable PhD students doing social science re-
search on large textual data sets to explore and ap-
ply the NLP tools that are developed within the 
school, as well as to understand how these NLP 
tools can be successfully interleaved with commer-
cial content analysis tools to support rich explora-
tion of their data. As is the current course, this 
seminar will be open to PhD students from all 
schools across campus and already has enrollees 
from public policy, communications, and man-
agement, as well as information science. 
 
8. Summary 
 
While it might appear that a disproportionate 
amount of thought and attention is given to the 
more human and social aspects of designing and 
conducting this course, experience shows that such 
attention is the key to the success of this diverse 
body of students in learning and understanding the 
content of the course. Furthermore, given the great 
diversity in class-level and disciplinary back-
ground of students, this attention to structuring the 
course has paid off in the multiple ways exempli-
fied above. While it is obvious that a course for 
computer-science majors alone would be designed 
quite differently, it would not provide the enriched 
understanding of the field of NLP and its applica-
tion value that is possible with the contributions by 
the variety of disciplines brought together in this 
course. 
Acknowledgements 
67
We would like to acknowledge the contributions of 
the students in all the classes over the years whose 
efforts and suggestions have continually improved 
the course. We would to especially acknowledge 
this year?s class, who were especially contributory 
of ideas for improving and building on a currently 
successful course, namely Agnieszka Kwiat-
kowska, Anatoliy Gruzd, Carol Schwartz, Cun-
Fang Cheng, Freddie Wade, Joshua Legler, Kei-
suke Inoue, Matthew Wolf, Michael Fudge, Michel 
Tinuiri, Olga Azarova, Rebecca Gilbert, Shuyuan 
Ho, Tuncer Can, Xiaozhony Liu, and Xue Xiao. 
References  
Loper, E. & Bird, S., 2002.  NLTK, the Natural 
Language Toolkit. In Proceedings of the ACL 
Workshop on Effective Tools and Methodolo-
gies for Teaching Natural Language Processing 
and Computational Linguistics. Philadelphia: 
Association for Computational Linguistics. 
 
Corrada-Emmanuel, A. McCallum, A., Smyth, P., 
Steyvers, M. & Chemudugunta, C., 2005. Social 
Network Analysis and Topic Discovery for the 
Enron Email Dataset. In Proceedings of the 
Workshop on Link Analysis, Counterterrorism 
and Security at 2005 SIAM International Con-
ference in Data Mining. 
68
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 205?208, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling using libSVM 
 
 
Necati Ercan Ozgencil Nancy McCracken 
Center for Natural Language Processing Center for Natural Language Processing 
School of Engineering and Computer Science School of Information Studies 
Syracuse University Syracuse University 
neozgenc@ecs.syr.edu njm@ecs.syr.edu 
 
 
 
 
Abstract 
We describe a system for the CoNLL-
2005 shared task of Semantic Role Label-
ing.  The system implements a two-layer 
architecture to first identify the arguments 
and then to label them for each predicate.  
The components are implemented as 
SVM classifiers using libSVM.   Features 
were adapted and tuned for the system, 
including a reduced set for the identifier 
classifier.  Experiments were conducted to 
find kernel parameters for the Radial Ba-
sis Function (RBF) kernel.  An algorithm 
was defined to combine the results of the 
argument labeling classifier according to 
the constraints of the argument labeling 
problem. 
1 Introduction and Strategy 
The Semantic Role Labeling (SRL) problem has 
been the topic of the both the CoNLL-2004 and the 
CoNLL-2005 Shared Tasks (Carreras and 
M?rquez, 2005).  The SRL system described here 
depends on a full syntactic parse from the Charniak 
parser, and investigates aspects of using Support 
Vector Machines (SVMs) as the machine learning 
technique for the SRL problem, using the libSVM 
package. 
In common with many other systems, this sys-
tem uses the two-level strategy of first identifying 
which phrases can be arguments to predicates in 
general, and then labeling the arguments according 
to that predicate.  The argument identification 
phase is a binary classifier that decides whether 
each constituent in the full syntax tree of the sen-
tence is a potential argument.  These potential ar-
guments are passed into the argument labeling 
classifier, which uses binary classifiers for each 
label to decide if that label should be given to that 
argument.  A post-processing phase picks the best 
labeling that satisfies the constraints of labeling the 
predicate arguments. 
For overall classification strategy and for 
suggestions of features, we are indebted to the 
work of Pradhan et al(2005) and to the work of 
many authors in both the CoNLL-2004 shared task 
and the similar semantic roles task of Senseval-3.  
We used the results of their experiments with 
features, and worked primarily on features for the 
identifying classifier and with the constraint 
satisfaction problem on the final argument output. 
2 System Description  
2.1 Input Data 
In this system, we chose to use full syntax trees 
from the Charniak parser, as the constituents of 
those trees more accurately represented argument 
phrases in the training data at the time of the data 
release.  Within each sentence, we first map the 
predicate to a constituent in the syntax tree.  In the 
cases that the predicate is not represented by a con-
stituent, we found that these were verb phrases of 
length two or more, where the first word was the 
main verb (carry out, gotten away, served up, etc.).  
In these cases, we used the first word constituent as 
the representation of the predicate, for purposes of 
computing other features that depended on a rela-
tive position in the syntax tree.   
205
We next identify every constituent in the tree as 
a potential argument, and label the training data 
accordingly.  Although approximately 97% of the 
arguments in the training data directly matched 
constituents in the Charniak tree, only 91.3% of the 
arguments in the development set match constitu-
ents.  Examination of the sentences with incorrect 
parses show that almost all of these are due to 
some form of incorrect attachment, e.g. preposi-
tional attachment, of the parser.  Heuristics can be 
derived to correct constituents with quotes, but this 
only affected a small fraction of a percent of the 
incorrect arguments.  Experiments with corrections 
to the punctuation in the Collins parses were also 
unsuccessful in identifying additional constituents.  
Our recall results on the development directory are 
bounded by the 91.3% alignment figure. 
We also did not use the the partial syntax, 
named entities or the verb senses in the 
development data. 
2.2 Learning Components:  SVM classifiers 
For our system, we chose to use libSVM, an open 
source SVM package (Chang and Lin, 2001).   
In the SRL problem, the features are nominal, 
and we followed the standard practice of represent-
ing a nominal feature with n discrete values as n 
binary features.  Many of the features in the SRL 
problem can take on a large number of values, for 
example, the head word of a constituent may take 
on as many values as there are different words pre-
sent in the training set, and these large number of 
features can cause substantial performance issues. 
The libSVM package has several kernel func-
tions available, and we chose to use the radial basis 
functions (RBF).  For the argument labeling prob-
lem, we used the binary classifiers in libSVM, with 
probability estimates of how well the label fits the 
distribution. These are normally combined using 
the ?one-against-one? approach into a multi-class 
classifier.  Instead, we combined the binary classi-
fiers in our own post-processing phase to get a la-
beling satisfying the constraints of the problem. 
2.3 The Identifier Classifier Features 
One aspect of our work was to use fewer features 
for the identifier classifier than the basic feature set 
from (Gildea and Jurafsky, 2002).  The intuition 
behind the reduction is that whether a constituent 
in the tree is an argument depends primarily on the 
structure and is independent of the lexical items of 
the predicate and headword.  This reduced feature 
set is: 
Phrase Type: The phrase label of the argument. 
Position:  Whether the phrase is before or after 
the predicate. 
Voice:  Whether the predicate is in active or 
passive voice.  Passive voice is recognized if a past 
participle verb is preceded by a form of the verb 
?be? within 3 words. 
Sub-categorization:  The phrase labels of the 
children of the predicate?s parent in the syntax tree. 
Short Path: The path from the parent of the 
argument position in the syntax tree to the parent 
of the predicate. 
The first four features are standard, and the short 
path feature is defined as a shorter version of the 
standard path feature that does not use the 
argument phrase type on one end of the path, nor 
the predicate type on the other end. 
The use of this reduced set of features was 
confirmed experimentally by comparing the effect 
of this reduced feature set on the F-measure of the 
identifier classifier, compared to feature sets that 
also added the predicate, the head word and the 
path features, as normally defined. 
 
 Reduced + Pred + Head + Path 
F-measure 81.51 81.31 72.60 81.19 
Table 1:  Additional features reduce F-measure for the 
identifier classifier. 
2.4 Using the Identifier Classifier for Train-
ing and Testing 
Theoretically, the input for training the identifier 
classifier is that, for each predicate, all constituents 
in the syntax tree are training instances, labeled 
true if it is any argument of that predicate, and 
false otherwise.  However, this leads to too many 
negative (false) instances for the training.  To cor-
rect this, we experimented with two filters for 
negative instances.  The first filter is simply a ran-
dom filter; we randomly select a percentage of ar-
guments for each argument label.  Experiments 
with the percentage showed that 30% yielded the 
best F-measure for the identifier classifier. 
The second filter is based on phrase labels from 
the syntax tree. The intent of this filter was to re-
move one word constituents of a phrase type that 
was never used.  We selected only those phrase 
206
labels whose frequency in the training was higher 
than a threshold.  Experiments showed that the best 
threshold was 0.01, which resulted in approxi-
mately 86% negative training instances. 
However, in the final experimentation, compari-
son of these two filters showed that the random 
filter was best for F-measure results of the identi-
fier classifier. 
The final set of experiments for the identifier 
classifier was to fine tune the RBF kernel training 
parameters, C and gamma.  Although we followed 
the standard grid strategy of finding the best pa-
rameters, unlike the built-in grid program of 
libSVM with its accuracy measure, we judged the  
results based on the more standard F-measure of 
the classifier.  The final values are that C = 2 and 
gamma = 0.125. 
The final result of the identifier classifier trained 
on the first 10 directories of the training set is: 
Precision:  78.27%         Recall:  89.01% 
(F-measure:  83.47) 
Training on more directories did not substan-
tially improve these precision and recall figures. 
2.5   Labeling Classifier Features 
The following is a list of the features used in the 
labeling classifiers.  
Predicate:  The predicate lemma from the 
training file. 
Path:  The syntactic path through the parse tree 
from the argument constituent to the predicate. 
Head Word:  The head word of the argument 
constituent, calculated in the standard way, but 
also stemmed.  Applying stemming reduces the 
number of unique values of this feature 
substantially, 62% in one directory of training data. 
Phrase Type, Position, Voice, and Sub-
categorization:  as in the identifier classifier. 
In addition, we experimented with the following 
features, but did not find that they increased the 
labeling classifier scores. 
Head Word POS:  the part of speech tag of the 
head word of the argument constituent. 
Temporal Cue Words:  These words were 
compiled by hand from ArgM-TMP phrases in the 
training data. 
Governing Category:  The phrase label of the 
parent of the argument. 
Grammatical Rule:  The generalization of the 
subcategorization feature to show the phrase labels 
of the children of the node that is the lowest parent 
of all arguments of the predicate. 
In the case of the temporal cue words, we 
noticed that using our definition of this feature 
increased the number of false positives for the 
ARGM-TMP label; we guess that our temporal cue 
words included too many words that occured in 
other labels.   Due to lack of time, we were not 
able to more fully pursue these features. 
2.6  Using the Labeling Classifier for Train-
ing and Testing 
Our strategy for using the labeling classifier is 
that in the testing, we pass only those arguments to 
the labeling classifier that have been marked as 
true by the identifier classifier.  Therefore, for 
training the labeling classifier, instances were con-
stituents that were given argument labels in the 
training set, i.e. there were no ?null? training ex-
amples. 
For the labeling classifier, we also found the 
best parameters for the RBF kernel of the classi-
fier.  For this, we used the grid program of libSVM 
that uses the multi-class classifier, using the accu-
racy measure to tune the parameters, since this 
combines the precision of the binary classifiers for 
each label.  The final values are that C = 0.5 and 
gamma = 0.5. 
In order to show the contribution of the labeling 
classifier to the entire system, a final test was done 
on the development set, but passing it the correct 
arguments.  We tested this with a labeling classi-
fier trained on 10 directories and one trained on 20 
directories, showing the final F-measure: 
10 directories:  83.27 
20 directories:  84.51 
2.7 Post-processing the classifier labels 
The final part of our system was to use the results 
of the binary classifiers for each argument label to 
produce a final labeling subject to the labeling con-
straints. 
For each predicate, the constraints are:  two con-
stituents cannot have the same argument label, a 
constituent cannot have more than one label, if two 
constituents have (different) labels, they cannot 
have any overlap, and finally, no argument can 
overlap the predicate.   
207
  Precision Recall F?=1 
Development 73.57% 71.87% 72.71 
Test WSJ 74.66% 74.21% 74.44 
Test Brown 65.52% 62.93% 64.20 
Test WSJ+Brown 73.48% 72.70% 73.09 
 
 
Test WSJ Precision Recall F?=1 
Overall 74.66% 74.21% 74.44 
A0 83.59% 85.07% 84.32 
A1 77.00% 74.35% 75.65 
A2 66.97% 66.85% 66.91 
A3 66.88% 60.69% 63.64 
A4 77.66% 71.57% 74.49 
A5 80.00% 80.00% 80.00 
AM-ADV 55.13% 50.99% 52.98 
AM-CAU 52.17% 49.32% 50.70 
AM-DIR 27.43% 56.47% 36.92 
AM-DIS 73.04% 72.81% 72.93 
AM-EXT 57.69% 46.88% 51.72 
AM-LOC 50.00% 49.59% 49.79 
AM-MNR 54.00% 54.94% 54.47 
AM-MOD 92.02% 94.19% 93.09 
AM-NEG 96.05% 95.22% 95.63 
AM-PNC 35.07% 40.87% 37.75 
AM-PRD 50.00% 20.00% 28.57 
AM-REC 0.00% 0.00% 0.00 
AM-TMP 68.69% 63.57% 66.03 
R-A0 77.61% 89.73% 83.23 
R-A1 71.95% 75.64% 73.75 
R-A2 87.50% 43.75% 58.33 
R-A3 0.00% 0.00% 0.00 
R-A4 0.00% 0.00% 0.00 
R-AM-ADV 0.00% 0.00% 0.00 
R-AM-CAU 100.00% 50.00% 66.67 
R-AM-EXT 0.00% 0.00% 0.00 
R-AM-LOC 66.67% 85.71% 75.00 
R-AM-MNR 8.33% 16.67% 11.11 
R-AM-TMP 66.67% 88.46% 76.03 
V 97.32% 97.32% 97.32 
Table 2:  Overall results (top) and detailed results on the 
WSJ test (bottom). 
 
To achieve these constraints, we used the prob-
abilities produced by libSVM for each of the bi-
nary argument label classifiers.  We produced a 
constraint satisfaction module that uses a greedy 
algorithm that uses probabilities from the matrix of 
potential labeling for each constituent and label.  
The algorithm iteratively chooses a label for a node 
with the highest probability and removes any po-
tential labeling that would violate constraints with 
that chosen label.  It continues to choose labels for 
nodes until all probabilities in the matrix are lower 
than a threshold, determined by experiments to be 
.3.  In the future, it is our intent to replace this 
greedy algorithm with a dynamic optimization al-
gorithm. 
3 Experimental Results 
3.1     Final System and Results 
 
The final system used an identifier classifier 
trained on (the first) 10 directories, in approxi-
mately 7 hours, and a labeling classifier trained on 
20 directories, in approximately 23 hours.  Testing 
took approximately 3.3 seconds per sentence. 
As a further test of the final system, we trained 
both the identifier classifier and the labeling classi-
fier on the first 10 directories and used the second 
10 directories as development tests.  Here are some 
of the results, showing the alignment and F-
measure on each directory, compared to 24. 
 
Directory: 12 14 16 18 20 24 
Alignment 95.7 96.1 95.9 96.5 95.9 91.3 
F-measure 80.4 79.6 79.0 80.5 79.7 71.1 
Table 3:  Using additional directories for testing 
 
Finally, we note that we did not correctly antici-
pate the final notation for the predicates in the test 
set for two word verbs.  Our system assumed that 
two word verbs would be given a start and an end, 
whereas the test set gives just the one word predi-
cate.   
References  
Xavier Carreras and Llu?s M?rquez, 2005.  Introduction 
to the CoNLL-2005 Shared Task:  Semantic Role 
Labeling, Proceedings of CoNLL-2005. 
Chih-Chung Chang and Chih-Jen Lin, 2001.  LIBSVM : 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Daniel Gildea and Daniel Jurafsky, 2002.  Automatic 
Labeling of Semantic Roles.  Computational Linguis-
tics 28(3):245-288. 
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, and Daniel Jurafsky, 
2005.  Support Vector Learning for Semantic Argu-
ment Classification,  To appear in Machine Learning 
journal, Special issue on Speech and Natural Lan-
guage Processing. 
208
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 17?24,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Modeling Reference Interviews as a Basis for Improving Automatic QA 
Systems 
 
Nancy J. McCracken, Anne R. Diekema, Grant Ingersoll, Sarah C. 
Harwell, Eileen E. Allen, Ozgur Yilmazel, Elizabeth D. Liddy 
Center for Natural Language Processing 
Syracuse University 
Syracuse, NY 13244 
{ njmccrac, diekemar, gsingers, scharwel, eeallen, oyilmaz, liddy}@syr.edu 
 
 
 
 
 
Abstract 
 
The automatic QA system described in 
this paper uses a reference interview 
model to allow the user to guide and 
contribute to the QA process.  A set of 
system capabilities was designed and 
implemented that defines how the user?s 
contributions can help improve the 
system.  These include tools, called the 
Query Template Builder and the 
Knowledge Base Builder, that tailor the 
document processing and QA system to 
a particular domain by allowing a 
Subject Matter Expert to contribute to 
the query representation and to the 
domain knowledge.  During the QA 
process, the system can interact with the 
user to improve query terminology by 
using Spell Checking, Answer Type 
verification, Expansions and Acronym 
Clarifications.  The system also has 
capabilities that depend upon, and 
expand the user?s history of interaction 
with the system, including a User 
Profile, Reference Resolution, and 
Question Similarity modules 
 
 
1  Introduction 
 
Reference librarians have successfully fielded 
questions of all types for years using the Reference 
Interview to clarify an unfocused question, narrow 
a broad question, and suggest further information 
that the user might not have thought to ask for.  
The reference interview tries to elicit sufficient 
information about the user?s real need to enable a 
librarian to understand the question enough to 
begin searching.  The question is clarified, made 
more specific, and contextualized with relevant 
detail.  Real questions from real users are often 
?ill-formed? with respect to the information 
system; that is, they do not match the structure of 
?expectations? of the system (Ross et al, 2002). A 
reference interview translates the user?s question 
into a representation that the librarian and the 
library systems can interpret correctly. The human 
reference interview process provides an ideal, 
well-tested model of how questioner and answerer 
work together co-operatively and, we believe, can 
be successfully applied to the digital environment.  
The findings of researchers applying this model in 
online situations (Bates, 1989, Straw, 2004) have 
enabled us to understand how a system might work 
with the user to provide accurate and relevant 
answers to complex questions. 
 Our long term goal in developing Question-
Answering (QA) systems for various user groups is 
to permit, and encourage users to positively 
contribute to the QA process, to more nearly 
mirror what occurs in the reference interview, and 
to develop an automatic QA system that provides 
fuller, more appropriate, individually tailored 
responses than has been available to date. 
 Building on our Natural Language 
Processing (NLP) experience in a range of 
information access applications, we have focused 
our QA work in two areas:  1) modeling the subject 
domain of the collections of interest to a set of 
 
 
 
17
users for whom we are developing the QA system, 
and; 2) modeling the query clarification and 
negotiation interaction between the information 
seeker and the information provider. Examples of 
these implementation environments are: 
 
1. Undergraduate aerospace engineering students 
working in collaborative teams on course 
projects designing reusable launch vehicles, 
who use a QA system in their course-related 
research. 
2. Customers of online business sites who use a 
QA system to learn more about the products or 
services provided by the company, or who 
wish to resolve issues concerning products or 
service delivery. 
 
In this paper, we describe the capabilities we 
have developed for these specific projects in order 
to explicate a more general picture of how we 
model and utilize both the domains of inquiry and 
typical interaction processes observed in these 
diverse user groups. 
 
2 Background and related research 
 
Our work in this paper is based on two premises: 
1) user questions and responsive answers need to 
be understood within a larger model of the user?s 
information needs and requirements, and, 2) a 
good interactive QA system facilitates a dialogue 
with its users to ensure it understands and satisfies 
these information needs. The first premise is based 
on the long-tested and successful model of the 
reference interview (Bates, 1997, Straw, 2004), 
which was again validated by the findings of an 
ARDA-sponsored workshop to increase the 
research community?s understanding of the 
information seeking needs and cognitive processes 
of intelligence analysts (Liddy, 2003). The second 
premise instantiates this model within the digital 
and distributed information environment. 
 Interactive QA assumes an interaction 
between the human and the computer, typically 
through a combination of a clarification dialogue 
and user modeling to capture previous interactions 
of users with the system. De Boni et al (2005) 
view the clarification dialogue mainly as the 
presence or absence of a relationship between the 
question from the user and the answer provided by 
the system. For example, a user may ask a 
question, receive an answer and ask another 
question in order to clarify the meaning, or, the 
user may ask an additional question which expands 
on the previous answer. In their research De Boni 
et al (2005) try to determine automatically 
whether or not there exists a relationship between a 
current question and preceding questions, and if 
there is a relationship, they use this additional 
information in order to determine the correct 
answer.  
 We prefer to view the clarification dialogue 
as more two-sided, where the system and the user 
actually enter a dialogue, similar to the reference 
interview as carried out by reference librarians 
(Diekema et al, 2004). The traditional reference 
interview is a cyclical process in which the 
questioner poses their question, the librarian (or the 
system) questions the questioner, then locates the 
answer based on information provided by the 
questioner, and returns an answer to the user who 
then determines whether this has satisfied their 
information need or whether further clarification or 
further questions are needed.  The HITIQA 
system?s (Small et al, 2004) view of a clarification 
system is closely related to ours?their dialogue 
aligns the understanding of the question between 
system and user. Their research describes three 
types of dialogue strategies: 1) narrowing the 
dialogue, 2) broadening the dialogue, and 3) a fact 
seeking dialogue. 
 Similar research was carried out by Hori et 
al. (2003), although their system automatically 
determines whether there is a need for a dialogue, 
not the user. The system identifies ambiguous 
questions (i.e. questions to which the system could 
not find an answer). By gathering additional 
information, the researchers believe that the system 
can find answers to these questions. Clarifying 
questions are automatically generated based on the 
ambiguous question to solicit additional 
information from the user. This process is 
completely automated and based on templates that 
generate the questions. Still, removing the 
cognitive burden from the user through automation 
is not easy to implement and can be the cause of 
error or misunderstanding. Increasing user 
involvement may help to reduce this error. 
 As described above, it can be seen that 
interactive QA systems have various levels of 
dialogue automation ranging from fully automatic 
(De Boni et al, 2004, Hori et al, 2004) to a strong 
18
user involvement (Small et al, 2004, Diekema et 
al., 2004). Some research suggests that 
clarification dialogues in open-domain systems are 
more unpredictable than those in restricted domain 
systems, the latter lending itself better to 
automation (Hori et al, 2003, J?nsson et al, 2004).  
Incorporating the user?s inherent knowledge of the 
intention of their query is quite feasible in 
restricted domain systems and should improve the 
quality of answers returned, and make the 
experience of the user a less frustrating one. While 
many of the systems described above are 
promising in terms of IQA, we believe that 
incorporating knowledge of the user in the 
question negotiation dialogue is key to developing 
a more accurate and satisfying QA system.   
 
3 System Capabilities 
 
In order to increase the contribution of users to our 
question answering system, we expanded our 
traditional domain independent QA system by 
adding new capabilities that support system-user 
interaction. 
 
3.1  Domain Independent QA 
 
Our traditional domain-independent QA capability 
functions in two stages, the first information 
retrieval stage selecting a set of candidate 
documents, the second stage doing the answer 
finding within the filtered set.  The answer finding 
process draws on models of question types and 
document-based knowledge to seek answers 
without additional feedback from the user.  Again, 
drawing on the modeling of questions as they 
interact with the domain representation, the system 
returns answers of variable lengths on the fly in 
response to the nature of the question since factoid 
questions may be answered with a short answer, 
but complex questions often require longer 
answers.  In addition, since our QA projects were 
based on closed collections, and since closed 
collections may not provide enough redundancy to 
allow for short answers to be returned, the variable 
length answer capability assists in finding answers 
to factoid questions.  The QA system provides 
answers in the form of short answers, sentences, 
and answer-providing passages, as well as links to 
the full answer-providing documents. The user can 
provide relevance feedback by selecting the full 
documents that offer the best information.  Using 
this feedback, the system can reformulate the 
question and look for a better set of documents 
from which to find an answer to the question. 
Multiple answers can be returned, giving the user a 
more complete picture of the information held 
within the collection.   
 One of our first tactics to assist in both 
question and domain modeling for specific user 
needs was to develop tools for Subject Matter 
Experts (SMEs) to tailor our QA systems to a 
particular domain.  Of particular interest to the 
interactive QA community is the Query Template 
Builder (QTB) and the Knowledge Base Builder 
(KBB).  
 Both tools allow a priori alterations to 
question and domain modeling for a community, 
but are not sensitive to particular users.  Then the 
interactive QA system permits question- and user-
specific tailoring of system behavior simply 
because it allows subject matter experts to change 
the way the system understands their need at the 
time of the search. 
 Question Template Builder (QTB) allows 
a subject matter expert to fine tune a question 
representation by adding or removing stopwords 
on a question-by-question basis, adding or masking 
expansions, or changing the answer focus.  The 
QTB displays a list of Question-Answer types, 
allows the addition of new Answer Types, and 
allows users to select the expected answer type for 
specific questions.  For example, the subject matter 
expert may want to adjust particular ?who? 
questions as to whether the expected answer type is 
?person? or ?organization?.  The QTB enables 
organizations to identify questions for which they 
want human intervention and to build specialized 
term expansion sets for terms in the collection.  
They can also adjust the stop word list, and refine 
and build the Frequently or Previously Asked 
Question (FAQ/PAQ) collection. 
 Knowledge Base Builder (KBB) is a suite 
of tools developed for both commercial and 
government customers.  It allows the users to view 
and extract terminology that resides in their 
document collections.  It provides useful statistics 
about the corpus that may indicate portions that 
require attention in customization.  It collects 
frequent / important terms with categorizations to 
enable ontology building (semi-automatic, 
permitting human review), term collocation for use 
19
in identifying which sense of a word is used in the 
collection for use in term expansion and 
categorization review.  KBB allows companies to 
tailor the QA system to the domain vocabulary and 
important concept types for their market.  Users 
are able to customize their QA applications 
through human-assisted automatic procedures.  
The Knowledge Bases built with the tools are  
 
 
IR Answer Providers
Question 
Processing
Session 
Tracking
Reference 
Resolution
User Profile
Question 
Similarity
User
Answer
Spell 
checking
Answer 
Type 
Verification
Expansion 
Clarification
Domain Modeling
QTB KBB
 
Figure 1. System overview 
 
 
primarily lexical semantic taxonomic resources.  
These are used by the system in creating frame 
representations of the text.  Using automatically 
harvested data, customers can review and alter 
categorization of names and entities and expand 
the underlying category taxonomy to the domain of 
interest.  For example, in the NASA QA system, 
experts added categories like ?material?, ?fuel?, 
?spacecraft? and ?RLV?, (Reusable Launch 
Vehicles).  They also could specify that ?RLV? is a 
subcategory of ?spacecraft? and that space shuttles 
like ?Atlantis? have category ?RLV?.  The KBB 
works in tandem with the QTB, where the user can 
find terms in either documents or example queries 
 
3.2 Interactive QA Development 
 
In our current NASA phase, developed for 
undergraduate aerospace engineering students to 
quickly find information in the course of their 
studies on reusable launch vehicles, the user can 
view immediate results, thus bypassing the 
Reference Interviewer, or they may take the 
opportunity to utilize its increased functionality 
and interact with the QA system. The capabilities 
we have developed, represented by modules added 
to the system, fall into two groups. Group One 
includes capabilities that draw on direct interaction 
with the user to clarify what is being asked and that 
address terminological issues.  It includes Spell 
Checking, Expansion Clarification, and Answer 
Type Verification. Answers change dynamically as 
the user provides more input about what was 
meant. Group Two capabilities are dependent 
upon, and expand upon the user?s history of 
interaction with the system and include User 
Profile, Session Tracking, Reference Resolution, 
Question Similarity and User Frustration 
Recognition modules.  These gather knowledge 
about the user, help provide co-reference 
resolution within an extended dialogue, and 
monitor the level of frustration a user is 
experiencing.   
20
 The capabilities are explained in greater 
detail below.  Figure 1 captures the NASA system 
process and flow.  
 
Group One: 
  
In this group of interactive capabilities, after the 
user asks a query, answers are returned as in a 
typical system.  If the answers presented aren?t 
satisfactory, the system will embark on a series of 
interactive steps (described below) in which 
alternative spelling, answertypes, clarifications and 
expansions will be suggested.   The user can 
choose from the system?s suggestions or type in 
their own.  The system will then revise the query 
and return a new set of answers.  If those answers 
aren?t satisfactory, the user can continue 
interacting with the system until appropriate 
answers are found. 
Spell checking: Terms not found in the 
index of the document collection are displayed as 
potentially misspelled words.  In this preliminary 
phase, spelling is checked and users have the 
opportunity to select correct and/or alternative 
spellings.  
 AnswerType verification: The interactive 
QA system displays the type of answer that the 
system is looking for in order to answer the 
question.  For example for the question, Who 
piloted the first space shuttle?, the answer type is 
?person?, and the system will limit the search for 
candidate short answers in the collection to those 
that are a person?s name.  The user can either 
accept the system?s understanding of the question 
or reject the type it suggests.  This is particularly 
useful in semantically ambiguous questions such as 
?Who makes Mountain Dew?? where the system 
might interpret the question as needing a person, 
but the questioner actually wants the name of a 
company.  
Expansion:  This capability allows users to 
review the possible relevant terms (synonyms and 
group members) that could enhance the question-
answering process.  The user can either select or 
deselect terms of interest which do or do not 
express the intent of the question.  For example, if 
the user asks: How will aerobraking change the 
orbit size? then the system can bring back the 
following expansions for ?aerobraking?:  By 
aerobraking do you mean the following: 1) 
aeroassist, 2) aerocapture, 3) aeromaneuvering, 4) 
interplanetary transfer orbits, or 5) transfer orbits. 
Acronym Clarification: For abbreviations 
or acronyms within a query, the full explications 
known by the system for the term can be displayed 
back to the user.  The clarifications implemented 
are a priori limited to those that are relevant to the 
domain.  In the aerospace domain for example, if 
the question was What is used for the TPS of the 
RLV?, the clarifications of TPS would be thermal 
protection system, thermal protection subsystem, 
test preparation sheet, or twisted pair shielded, and 
the clarification of RLV would be reusable launch 
vehicle.  The appropriate clarifications can be 
selected to assist in improving the search.  For a 
more generic domain, the system would offer 
broader choices.  For example, if the user types in 
the question: What educational programs does the 
AIAA offer?, then the system might return: By 
AIAA, do you mean (a) American Institute of 
Aeronautics and Astronautics (b) Australia 
Indonesia Arts Alliance or (c) Americans for 
International Aid & Adoption?   
 
Group Two: 
 
User Profile: The User Profile keeps track of more 
permanent information about the user.  The profile 
includes a small standard set of user attributes, 
such as the user?s name and / or research interests.  
In our commercially funded work, selected 
information gleaned from the question about the 
user was also captured in the profile.  For example, 
if a user asks ?How much protein should my 
husband be getting every day??, the fact that the 
user is married can be added to their profile for 
future marketing, or for a new line of dialogue to 
ask his name or age.  This information is then 
made available as context information for the QA 
system to resolve references that the user makes to 
themselves and their own attributes.  
 For the NASA question-answering 
capability, to assist students in organizing their 
questions and results, there is an area for users to 
save their searches as standing queries, along with 
the results of searching (Davidson, 2006).  This 
information, representing topics and areas of 
interest, can help to focus answer finding for new 
questions the user asks. 
Not yet implemented, but of interest, is the 
ability to save information such as a user?s 
21
preferences (format, reliability, sources), that could 
be used as filters in the answer finding process. 
 Reference Resolution:  A basic feature of 
an interactive QA system is the requirement to 
understand the user?s questions and responsive 
answers as one session. The sequence of questions 
and answers forms a natural language dialogue 
between the user and the system.  This necessitates 
NLP processing at the discourse level, a primary 
task of which is to resolve references across the 
session.  Building on previous work in this area 
done for the Context Track of TREC 2001 
(Harabagiu et al 2001) and additional work (Chai 
and Jin, 2004) suggesting discourse structures are 
needed to understand the question/answer 
sequence, we have developed session-based 
reference resolution capability. In a dialogue, the 
user naturally includes referring phrases that 
require several types of resolution. 
 The simplest case is that of referring 
pronouns, where the user is asking a follow-up 
question, for example: 
 
Q1:  When did Madonna enter the music business? 
A1:  Madonna's first album, Madonna, came out in 
1983 and since then she's had a string of hits, been 
a major influence in the music industry and 
become an international icon. 
Q2:  When did she first move to NYC? 
 
In this question sequence, the second 
question contains a pronoun, ?she?, that refers to 
the person ?Madonna? mentioned both in the 
previous question and its answer.    Reference 
resolution would transform the question into 
?When did Madonna first move to NYC?? 
Another type of referring phrase is the 
definite common noun phrase, as seen in the next 
example: 
 
Q1: If my doctor wants me to take Acyclovir, is it 
expensive?  
A1:  Glaxo-Wellcome, Inc., the company that 
makes Acyclovir, has a program to assist 
individuals that have HIV and Herpes.  
Q2:  Does this company have other assistance 
programs? 
 
The second question has a definite noun 
phrase ?this company? that refers to ?Glaxo-
Wellcome, Inc.? in the previous answer, thus 
transforming the question to ?Does Glaxo-
Wellcome, Inc. have other assistance programs?? 
Currently, we capture a log of the 
question/answer interaction, and the reference 
resolution capability will resolve any references in 
the current question that it can by using linguistic 
techniques on the discourse of the current session.  
This is almost the same as the narrative 
coreference resolution used in documents, with the 
addition of the need to understand first and second 
person pronouns from the dialogue context.  The 
coreference resolution algorithm is based on 
standard linguistic discourse processing techniques 
where referring phrases and candidate resolvents 
are analyzed along a set of features that typically 
includes gender, animacy, number, person and the 
distance between the referring phrase and the 
candidate resolvent. 
Question Similarity: Question Similarity is 
the task of identifying when two or more questions 
are related.  Previous studies (Boydell et al, 2005, 
Balfe and Smyth, 2005) on information retrieval 
have shown that using previously asked questions 
to enhance the current question is often useful for 
improving results among like-minded users.  
Identifying related questions is useful for finding 
matches to Frequently Asked Questions (FAQs) 
and Previously Asked Questions (PAQs) as well as 
detecting when a user is failing to find adequate 
answers and may be getting frustrated.  
Furthermore, similar questions can be used during 
the reference interview process to present 
questions that other users with similar information 
needs have used and any answers that they 
considered useful.   
CNLP?s question similarity capability 
comprises a suite of algorithms designed to 
identify when two or more questions are related.  
The system works by analyzing each query using 
our Language-to-Logic (L2L) module to identify 
and weight keywords in the query, provide 
expansions and clarifications, as well as determine 
the focus of the question and the type of answer the 
user is expecting (Liddy et al, 2003).  We then 
compute a series of similarity measures on two or 
more L2L queries.  Our measures adopt a variety 
of approaches, including those that are based on 
keywords in the query: cosine similarity, keyword 
string matching, expansion analysis, and spelling 
variations.  In addition, two measures are based on 
the representation of the whole query:answer type 
22
and answer frame analysis. An answer frame is our 
representation of the meaningful extractions 
contained in the query, along with metadata about 
where they occur and any other extractions that 
relate to in the query. 
Our system will then combine the weighted 
scores of two or more of these measures to 
determine a composite score for the two queries, 
giving more weight to a measure that testing has 
determined to be more useful for a particular task. 
We have utilized our question similarity 
module for two main tasks.  For FAQ/PAQ (call it 
XAQ) matching, we use question similarity to 
compare the incoming question with our database 
of XAQs.  Through empirical testing, we 
determined a threshold above which we consider 
two questions to be similar. 
Our other use of question similarity is in the 
area of frustration detection.  The goal of 
frustration detection is to identify the signs a user 
may be giving that they are not finding relevant 
answers so that the system can intervene and offer 
alternatives before the user leaves the system, such 
as similar questions from other users that have 
been successful.    
 
4 Implementations:  
 
The refinements to our Question Answering 
system and the addition of interactive elements 
have been implemented in three different, but 
related working systems, one of which is strictly an 
enhanced IR system.  None of the three 
incorporates all of these capabilities.  In our work 
for MySentient, Ltd, we developed the session-
based reference resolution capability, implemented 
the variable length and multiple answer capability, 
modified our processing to facilitate the building 
of a user profile, added FAQ/PAQ capability, and 
our Question Similarity capability for both 
FAQ/PAQ matching and frustration detection.  A 
related project, funded by Syracuse Research 
Corporation, extended the user tools capability to 
include a User Interface for the KBB and basic 
processing technology.  Our NASA project has 
seen several phases.  As the project progressed, we 
added the relevant developed capabilities for 
improved performance.  In the current phase, we 
are implementing the capabilities which draw on 
user choice.  
 
5 Conclusions and Future Work 
 
 The reference interview has been 
implemented as an interactive dialogue between 
the system and the user, and the full system is near 
completion. We are currently working on two 
types of evaluation of our interactive QA 
capabilities. One is a system-based evaluation in 
the form of unit tests, the other is a user-based 
evaluation. The unit tests are designed to verify 
whether each module is working correctly and 
whether any changes to the system adversely affect 
results or performance. Crafting unit tests for 
complex questions has proved challenging, as no 
gold standard for this type of question has yet been 
created.  As the data becomes available, this type 
of evaluation will be ongoing and part of regular 
system development. 
 As appropriate for this evolutionary work 
within specific domains for which there are not 
gold standard test sets, our evaluation of the QA 
systems has focused on qualitative assessments. 
What has been a particularly interesting outcome is 
what we have learned in elicitation from graduate 
students using the NASA QA system, namely that 
they have multiple dimensions on which they 
evaluate a QA system, not just traditional recall 
and precision (Liddy et al 2004). The high level 
dimensions identified include system performance, 
answers, database content, display, and 
expectations. Therefore the evaluation criteria we 
believe appropriate for IQA systems are centered 
around the display (UI) category as described in 
Liddy et al (2004).  We will evaluate aspects of 
the UI input subcategory, including question 
understanding, information need understanding, 
querying style, and question formulation 
assistance. Based on this user evaluation the 
system will be improved and retested.   
 
 
References 
 
Evelyn Balfe and Barry Smyth. 2005. An Analysis 
of Query Similarity in Collaborative Web 
Search. In Proceedings of the 27th European 
Conference on Information Retrieval. Santiago 
de Compostela, Spain. 
 
23
Marcia J. Bates. 1989. The Design of Browsing 
and Berrypicking Techniques for the Online 
Search Interface.  Online Review, 13: 407-424. 
 
Mary Ellen Bates. 1997. The Art of the Reference 
Interview.  Online World. September 15. 
 
Ois?n Boydell, Barry Smyth, Cathal Gurrin, and 
Alan F. Smeaton. 2005. A Study of Selection 
Noise in Collaborative Web Search. In 
Proceedings of the 19th International Joint 
Conference on Artificial Intelligence. Edinburgh, 
Scotland. 
http://www.ijcai.org/papers/post-0214.pdf 
 
Joyce Y. Chai, and Rong Jin. 2004.  Discourse 
Structure for Context Question Answering.  In 
Proceedings of the Workshp on the Pragmatics 
of Quesiton Answering, HST-NAACL, Boston. 
http://www.cse.msu.edu/~rongjin/publications/H
LTQAWorkshop04.pdf   
 
Barry D. Davidson. 2006. An Advanced Interactive 
Discovery Learning Environment for 
Engineering Education: Final Report.  
Submitted to R. E. Gillian, National Aeronautics 
and Space Administration. 
 
Marco De Boni and Suresh Manandhar. 2005. 
Implementing Clarification Dialogues in Open 
Domain Question Answering. Natural Language 
Engineering 11(4): 343-361. 
 
Anne R. Diekema, Ozgur Yilmazel, Jiangping 
Chen, Sarah Harwell, Lan He, and Elizabeth D. 
Liddy. 2004. Finding Answers to Complex 
Questions. In New Directions in Question 
Answering. (Ed.) Mark T. Maybury. The MIT 
Press, 141-152. 
 
Sanda Harabagiu, Dan Moldovan, Marius Pa?ca, 
Mihai Surdeanu, Rada Mihalcea, Roxana G?rju, 
Vasile Rus, Finley L?c?tu?u, Paul Mor?rescu, 
R?zvan Bunescu. 2001. Answering Complex, 
List and Context Questions with LCC?s 
Question-Answering Server, TREC 2001. 
 
Chiori Hori, Takaaki Hori., Hideki Isozaki, Eisaku 
Maeda, Shigeru Katagiri, and Sadaoki Furui. 
2003. Deriving Disambiguous Queries in a 
Spoken Interactive ODQA System. In ICASSP. 
Hongkong, I: 624-627. 
 
Arne J?nsson, Frida And?n, Lars Degerstedt, 
Annika Flycht-Eriksson, Magnus Merkel, and 
Sara Norberg. 2004. Experiences from 
Combining Dialogue System Development With 
Information Extraction Techniques. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 153-164. 
 
Elizabeth D. Liddy. 2003. Question Answering in 
Contexts. Invited Keynote Speaker. ARDA 
AQUAINT Annual Meeting. Washington, DC. 
Dec 2-5, 2003. 
 
Elizabeth D. Liddy, Anne R. Diekema, Jiangping 
Chen, Sarah Harwell, Ozgur Yilmazel, and Lan 
He. 2003. What do You Mean? Finding Answers 
to Complex Questions. Proceedings of New 
Directions in Question Answering. AAAI Spring 
Symposium, March 24-26. 
 
Elizabeth D. Liddy, Anne R. Diekema, and Ozgur 
Yilmazel. 2004. Context-Based Question-
Answering Evaluation. In Proceedings of the 27th 
Annual ACM-SIGIR Conference. Sheffield, 
England 
 
Catherine S. Ross, Kirsti Nilsen, and Patricia 
Dewdney. 2002. Conducting the Reference 
Interview.  Neal-Schuman, New York, NY. 
 
Sharon Small, Tomek Strzalkowski, Ting Liu, 
Nobuyuki Shimizu, and Boris Yamrom. 2004. A 
Data Driven Approach to Interactive QA. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 129-140. 
 
Joseph E. Straw. 2004. Expecting the Stars but 
Getting the Moon: Negotiating around Patron 
Expectations in the Digital Reference 
Environment. In The Virtual Reference 
Experience: Integrating Theory into Practice. 
Eds. R. David Lankes, Joseph Janes, Linda C. 
Smith, and Christina M.  Finneran. Neal-
Schuman, New York, NY. 
 
24
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 44?48,
Baltimore, Maryland, USA, June 26, 2014. c?2014 Association for Computational Linguistics
 
 
Optimizing Features in Active Machine Learning for Complex 
Qualitative Content Analysis 
 
Jasy Liew Suet Yan 
School of Information Studies 
Syracuse University, USA 
jliewsue@syr.edu 
Nancy McCracken 
School of Information Studies 
Syracuse University, USA 
njmccrac@syr.edu 
Shichun Zhou 
College of Engineering and 
Computer Science 
Syracuse University, USA 
szhou02@syr.edu 
Kevin Crowston 
National Science  
Foundation, USA 
crowston@syr.edu 
 
 
Abstract 
We propose a semi-automatic approach for 
content analysis that leverages machine learn-
ing (ML) being initially trained on a small set 
of hand-coded data to perform a first pass in 
coding, and then have human annotators cor-
rect machine annotations in order to produce 
more examples to retrain the existing model 
incrementally for better performance. In this 
?active learning? approach, it is equally im-
portant to optimize the creation of the initial 
ML model given less training data so that the 
model is able to capture most if not all posi-
tive examples, and filter out as many negative 
examples as possible for human annotators to 
correct. This paper reports our attempt to op-
timize the initial ML model through feature 
exploration in a complex content analysis 
project that uses a multidimensional coding 
scheme, and contains codes with sparse posi-
tive examples. While different codes respond 
optimally to different combinations of fea-
tures, we show that it is possible to create an 
optimal initial ML model using only a single 
combination of features for codes with at 
least 100 positive examples in the gold stand-
ard corpus. 
1 Introduction 
Content analysis, a technique for finding evi-
dence of concepts of theoretical interest through 
text, is an increasingly popular technique social 
scientists use in their research investigations. In 
the process commonly known as ?coding?, social 
scientists often have to painstakingly comb 
through large quantities of natural language cor-
pora to annotate text segments (e.g., phrase, sen-
tence, and paragraphs) with codes exhibiting the 
concepts of interest (Miles & Huberman, 1994). 
Analyzing textual data is very labor-intensive, 
time-consuming, and is often limited to the capa-
bilities of individual researchers (W. Evans, 
1996). The coding process becomes even more 
demanding as the complexity of the project in-
creases especially in the case of attempting to 
apply a multidimensional coding scheme with a 
significant number of codes (D?nmez, Ros?, 
Stegmann, Weinberger, & Fischer, 2005). 
With the proliferation and availability of dig-
ital texts, it is challenging, if not impossible, for 
human coders to manually analyze torrents of 
text to help advance social scientists? understand-
ing of the practices of different populations of 
interest through textual data. Therefore, compu-
tational methods offer significant benefits to help 
augment human capabilities to explore massive 
amounts of text in more complex ways for theory 
generation and theory testing. Content analysis 
can be framed as a text classification problem, 
where each text segment is labeled based on a 
predetermined set of categories or codes.  
Full automation of content analysis is still far 
from being perfect (Grimmer & Stewart, 2013). 
The accuracy of current automatic approaches on 
the best performing codes in social science re-
search ranges from 60-90% (Broadwell et al., 
2013; Crowston, Allen, & Heckman, 2012; M. 
Evans, McIntosh, Lin, & Cates, 2007; Ishita, 
Oard, Fleischmann, Cheng, & Templeton, 2010; 
Zhu, Kraut, Wang, & Kittur, 2011). While the 
potential of automatic content analysis is promis-
ing, computational methods should not be 
viewed as a replacement for the role of the pri-
mary researcher in the careful interpretation of 
text. Rather, the computers? pattern recognition 
capabilities can be leveraged to seek out the most 
likely examples for each code of interest, thus 
reducing the amount of texts researchers have to 
read and process.  
We propose a semi-automatic method that 
promotes a close human-computer partnership 
for content analysis. Machine learning (ML) is 
used to perform the first pass of coding on the 
unlabeled texts. Human annotators then have to 
correct only what the ML model identifies as 
positive examples of each code. The initial ML 
44
  
model needs to learn only from a small set of 
hand-coded examples (i.e., gold standard data), 
and will evolve and improve as machine annota-
tions that are verified by human annotators are 
used to incrementally retrain the model. In con-
trast to conventional machine learning, this ?ac-
tive learning? approach will significantly reduce 
the amount of training data needed upfront from 
the human annotators. However, it is still equally 
important to optimize the creation of the initial 
ML model given less training data so that the 
model is able to capture most if not all positive 
examples, and filter out as many negative exam-
ples as possible for human annotators to correct.  
To effectively implement the active learning 
approach for coding qualitative data, we have to 
first understand the nature and complexity of 
content analysis projects in social science re-
search. Our pilot case study, an investigation of 
leadership behaviors exhibited in emails from a 
FLOSS development project (Misiolek, Crow-
ston, & Seymour, 2012), reveals that it is com-
mon for researchers to use a multidimensional 
coding scheme consisting of a significant number 
of codes in their research inquiry. Previous work 
has shown that not all dimensions in a multidi-
mensional coding scheme could be applied fully 
automatically with acceptable level of accuracy 
(D?nmez et al., 2005) but little is known if it is 
possible at all to train an optimal model for all 
codes using the same combination of features. 
Also, the distribution of codes is often times un-
even with some rarely occurring codes having 
only few positive examples in the gold standard 
corpus.  
This paper presents our attempt in optimiz-
ing the initial ML model through feature explora-
tion using gold standard data created from a mul-
tidimensional coding scheme, including codes 
that suffer from sparseness of positive examples. 
Specifically, our study is guided by two research 
questions: 
a) How can features for an initial machine 
learning model be optimized for all codes in 
a text classification problem based on multi-
dimensional coding schemes? Is it possible 
to train a one-size-fits-all model for all codes 
using a single combination of features?  
b) Are certain features better suited for codes 
with sparse positive examples? 
2 Machine Learning Experiments 
To optimize the initial machine learning model, 
we systematically ran multiple experiments using 
a gold standard corpus of emails from a 
free/libre/open-source software (FLOSS) devel-
opment project coded for leadership behaviors 
(Misiolek et al., 2012). The coding scheme con-
tained six dimensions: 1) social/relationship, 2) 
task process, 3) task substance, 4) dual process 
and substance, 5) change behaviors, and 6) net-
working. The number of codes for each dimen-
sion ranged from 1 to 14. There were a total of 
35 codes in the coding scheme. Each sentence 
could be assigned more than one code. Framing 
the problem as a multi-label classification task, 
we trained a binary classification model for each 
code using support vector machine (SVM) with 
ten-fold cross-validation. This gold standard cor-
pus consisted of 3,728 hand-coded sentences 
from 408 email messages.  
For the active learning setup, we tune the ini-
tial ML model for high recall since having the 
annotators pick out positive examples that have 
been incorrectly classified by the model is pref-
erable to missing machine-annotated positive 
examples to be presented to human annotators 
for verification (Liew, McCracken, & Crowston, 
2014). Therefore, the initial ML model with low 
precision is acceptable. 
 
Category Features 
Content Unigram, bigram, pruning, 
tagging, lowercase, stop-
words, stemming, part-of-
speech (POS) tags 
Syntactic Token count 
Orthographic Capitalization of first letter of 
a word, capitalization of entire 
word 
Word list Subjectivity words 
Semantic Role of sender (software de-
veloper or not) 
 
Table 1. Features for ML model. 
 
As shown in Table 1, we have selected gen-
eral candidate features that have proven to work 
well across various text classification tasks, as 
well as one semantic feature specific to the con-
text of FLOSS development projects. For content 
features, techniques that we have incorporated to 
reduce the feature space include pruning, substi-
tuting certain tokens with more generic tags, 
converting all tokens to lowercase, excluding 
stopwords, and stemming. Using the wrapper 
approach (Kohavi & John, 1997), the same clas-
sifier is used to test the prediction performance 
of various feature combinations listed in Table 1. 
45
  
Model SINGLE MULTIPLE 
Measure Mean Recall Mean Precision Mean Recall Mean Precision 
Overall 
All (35) 0.690 0.065 0.877 0.068 
Dimension 
Change (1) 0.917 0.011 1.000 0.016 
Dual Process and 
Substance (13) 
0.675 0.069 0.852 0.067 
Networking (1) 0.546 0.010 0.843 0.020 
Process (3) 0.445 0.006 0.944 0.024 
Relationship (14) 0.742 0.083 0.872 0.089 
Substance (3) 0.735 0.061 0.919 0.051 
 
Table 2. Comparison of mean recall and mean precision between SINGLE and MULTIPLE models.  
 
 
 
Figure 1. Recall and precision for each code (grouped by dimension). 
 
3 Results and Discussion 
We ran 343 experiments with different combina-
tions of the 13 features in Table 1. We first com-
pare the performance of the best one-size-fits-all 
initial machine learning model that produces the 
highest recall using a single combination of fea-
tures for all codes (SINGLE) with an ?ensemble? 
model that uses different combinations of fea-
tures to produce the highest recall for each code 
(MULTIPLE). The SINGLE model combines 
content (unigram + bigram + POS tags + lower-
case + stopwords) with syntactic, orthographic, 
and semantic features. None of the best feature 
combination for each code in the MULTIPLE 
model coincides with the feature combination in 
the SINGLE model. For example, the best fea-
ture combination for code ?Phatics/Salutations? 
consists of only 2 out of the 13 features (unigram 
+ bigram). 
The best feature combination for each code 
in the MULTIPLE model varies with only some 
regularity noted in a few codes within the Dual 
and Substance dimensions. However, these pat-
terns are not consistent across all codes in a sin-
gle dimension indicating that the pertinent lin-
guistic features for codes belonging to the same 
dimension may differ despite their conceptual 
similarities, and even fitting an optimal model 
for all codes within a single dimension may 
prove to be difficult especially when the distribu-
tion of codes is uneven, and positive examples 
for certain codes are sparse. There are also no 
consistent feature patterns observed from the 
codes with sparse positive examples in the 
MULTIPLE model. 
0
0.2
0.4
0.6
0.8
1
E
xternal M
onitoring
Inform
ing
Issue D
irective
C
orrection
O
ffer/P
rovide A
ssistance
A
pproval
R
equest/Invite
C
om
m
it/A
ssum
e R
esponsibility
C
onfirm
/C
larify
O
bjection/D
isagreem
ent
Q
uery/Q
uestion
U
pdate
S
uggest/R
ecom
m
end
E
xplanation
N
etw
orking/B
oundary S
panning
R
em
ind
P
rocedure
S
chedule
C
riticism
P
roactive Inform
ing
A
pology
C
onsulting
H
um
or
A
ppreciation
S
elf-disclosure
V
ocative
A
greem
ent
E
m
otional E
xpression
P
hatics/S
alutations
Inclusive R
eference
O
pinion/P
reference
A
cronym
/Jargon
G
enerate N
ew
 Idea
E
valuation/F
eedback
P
rovide Inform
ation
Best Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)
Change Dual Networking Process Relationship Substance 
46
  
 
 
 
 
 
Figure 2. Recall and precision for each code (sorted by gold frequency) 
 
The comparison between the two models in 
Table 2 further demonstrates that the MULTI-
PLE model outperforms the SINGLE model both 
in the overall mean recall of all 35 codes, as well 
as the mean recall for each dimension. Figure 1 
(codes grouped by dimensions) illustrates that 
the feature combination on the SINGLE model is 
ill-suited for the Process codes, and half the Dual 
Process and Substance codes. Recall for each 
code for the SINGLE model are mostly below or 
at par with the recall for each code in the MUL-
TIPLE model. Thus, creating a one-size-fits-all 
initial model may not be optimal when training 
data is limited. Figure 2 (codes sorted based on 
gold frequency as shown beside the code names 
in the x-axis) exhibits that the SINGLE model is 
able to achieve similar recall to the MULTIPLE 
model for codes with over 100 positive examples 
in the training data. Precision for these codes are 
also higher compared to codes with sparse posi-
tive examples. This finding is promising because 
it implies that creating a one-size-fits-all initial 
ML model may be possible even for a multidi-
mensional coding scheme if there are more than 
100 positive examples for each code.  
4 Conclusion and Future Work 
We conclude that creating an optimal initial one-
size-fits-all ML model for all codes in a multi-
dimensional coding scheme using only a single 
feature combination is not possible when codes 
with sparse positive examples are present, and 
training data is limited, which may be common 
in real world content analysis projects in social 
science research. However, our findings also 
show that the potential of using a one-size-fits-all 
model increases when the size of positive exam-
ples for each code in the gold standard corpus are 
above 100. For social scientists who may not 
possess the technical skills needed for feature 
selection to optimize the initial ML model, this 
discovery confirms that we can create a ?canned? 
model using a single combination of features that 
would work well in text classification for a wide 
range of codes with the condition that research-
ers must be able to provide sufficient positive 
examples above a certain threshold to train the 
initial model. This would make the application of 
machine learning for qualitative content analysis 
more accessible to social scientists. 
The initial ML model with low precision 
means that the model is over-predicting. As a 
result, human annotators will have to correct 
more false positives in the machine annotations. 
For future work, we plan to experiment with dif-
ferent sampling strategies to pick the most ?prof-
itable? machine annotations to be corrected by 
human annotators. We will also work on design-
ing an interactive and adaptive user interface to 
promote greater understanding of machine learn-
ing outputs for our target users. 
0
0.2
0.4
0.6
0.8
1
R
em
ind (4)
G
enerate N
ew
 Idea (4)
C
riticism
 (5)
P
roactive Inform
ing (5)
Inform
ing (7)
P
rocedure (7)
E
xternal M
onitoring (8)
Issue D
irective (8)
S
chedule (8)
A
pology (8)
E
valuation/F
eedback (9)
C
orrection (11)
O
ffer/P
rovide A
ssistance (11)
A
pproval (12)
C
onsulting (13)
R
equest/Invite (14)
C
om
m
it/A
ssum
e R
esponsibility (17)
N
etw
orking/B
oundary S
panning (18)
A
cronym
/Jargon (19)
H
um
or (21)
A
ppreciation (23)
S
elf-disclosure (37)
V
ocative (41)
C
onfirm
/C
larify (45)
O
bjection/D
isagreem
ent (51)
A
greem
ent (67)
E
m
otional E
xpression (108)
Q
uery/Q
uestion (115)
P
hatics/S
alutations (116)
U
pdate (119)
S
uggest/R
ecom
m
end (138)
Inclusive R
eference (146)
O
pinion/P
reference (215)
P
rovide Inform
ation (312)
E
xplanation (327)
Best Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)
47
  
Acknowledgments 
This material is based upon work supported 
by the National Science Foundation under Grant 
No. IIS-1111107. Kevin Crowston is supported 
by the National Science Foundation. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the 
author(s) and do not necessarily reflect the views 
of the National Science Foundation. The authors 
wish to thank Janet Marsden for assisting with 
the feature testing experiments, and gratefully 
acknowledge helpful suggestions by the review-
ers. 
References 
Broadwell, G. A., Stromer-Galley, J., Strzalkowski, 
T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, 
A., Jiao, L., Webb, N. (2013). Modeling Soci-
ocultural phenomena in discourse. Natural Lan-
guage Engineering, 19(02), 213?257.  
Crowston, K., Allen, E. E., & Heckman, R. (2012). 
Using natural language processing technology for 
qualitative data analysis. International Journal of 
Social Research Methodology, 15(6), 523?543.  
D?nmez, P., Ros?, C., Stegmann, K., Weinberger, A., 
& Fischer, F. (2005). Supporting CSCL with au-
tomatic corpus analysis technology. In Proceed-
ings of 2005 Conference on Computer Support 
for Collaborative Learning (pp. 125?134).  
Evans, M., McIntosh, W., Lin, J., & Cates, C. (2007). 
Recounting the courts? Applying automated con-
tent analysis to enhance empirical legal research. 
Journal of Empirical Legal Studies, 4(4), 1007?
1039.  
Evans, W. (1996). Computer-supported content anal-
ysis: Trends, tools, and techniques. Social Sci-
ence Computer Review, 14(3), 269?279.  
Grimmer, J., & Stewart, B. M. (2013). Text as data: 
The promise and pitfalls of automatic content 
analysis methods for political texts. Political 
Analysis, 21(3), 267?297.  
Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng, 
A.-S., & Templeton, T. C. (2010). Investigating 
multi-label classification for human values. Pro-
ceedings of the American Society for Information 
Science and Technology, 47(1), 1?4.  
Liew, J. S. Y., McCracken, N., & Crowston, K. 
(2014). Semi-automatic content analysis of quali-
tative data. In iConference 2014 Proceedings (pp. 
1128?1132).  
Miles, M. B., & Huberman, A. M. (1994). Qualitative 
data analysis: An expanded sourcebook (2nd 
ed.). Sage.  
Misiolek, N., Crowston, K., & Seymour, J. (2012). 
Team dynamics in long-standing technology-
supported virtual teams. Presented at the Acade-
my of Management Annual Meeting, Organiza-
tional Behavior Division, Boston, MA. 
Zhu, H., Kraut, R. E., Wang, Y.-C., & Kittur, A. 
(2011). Identifying shared leadership in Wikipe-
dia. In Proceedings of the SIGCHI Conference on 
Human Factors in Computing Systems (pp. 
3431?3434). New York, NY, USA.  
 
 
 
48
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 59?62,
Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational Linguistics
Design of an Active Learning System with Human Correction for Content Analysis  Nancy McCracken School of Information Studies Syracuse University, USA njmccrac@syr.edu 
Jasy Liew Suet Yan School of Information Studies Syracuse University, USA jliewsue@syr.edu 
Kevin Crowston National Science Foundation Syracuse University, USA crowston@syr.edu    Abstract 
Our research investigation focuses on the role of humans in supplying corrected examples in active learning cycles, an important aspect of deploying active learning in practice.  In this paper, we dis-cuss sampling strategies and sampling sizes in set-ting up an active learning system for human ex-periments in the task of content analysis, which involves labeling concepts in large volumes of text.  The cost of conducting comprehensive hu-man subject studies to experimentally determine the effects of sampling sizes and sampling sizes is high. To reduce those costs, we first applied an active learning simulation approach to test the ef-fect of different sampling strategies and sampling sizes on machine learning (ML) performance in order to select a smaller set of parameters to be evaluated in human subject studies. 1 Introduction Social scientists often use content analysis to understand the practices of groups by analyzing texts such as transcripts of interpersonal commu-nication. Content analysis is the process of iden-tifying and labeling conceptually significant fea-tures in text, referred to as ?coding? (Miles and Huberman, 1994). For example, researchers studying leadership might look for evidence of behaviors such as ?suggesting or recommending? or ?inclusive reference? expressed in email mes-sages. However, analyzing text is very labor-intensive, as the text must be read and under-stood by a human. Consequently, important re-search questions in the qualitative social sciences may not be addressed because there is too much data for humans to analyze in a reasonable time. A few researchers have tried automatic tech-niques on content analysis problems. For exam-ple, Crowston et al. (2012) manually developed a classifier to identify codes related to group maintenance behavior in free/libre open source software (FLOSS) teams. Others have applied machine-learning (ML) techniques. For example, Ishita et al. (2010) used ML to automatically 
classify sections of text within documents on ten human values taken from the Schwartz?s Value Inventory. Broadwell et al. (2012) developed models to classify sociolinguistic behaviors to infer social roles (e.g., leadership). On the best performing codes, these approaches achieve ac-curacies from 60?80%, showing the potential of automatic qualitative content analysis. However, these studies all limited their reports to a subset of codes used by the social scientists, due in part to the need for a large volume of training data.  The state-of-the-art ML approaches for con-tent analysis require researchers to obtain a large amount of annotated data upfront, which is often costly or impractical. An active learning ap-proach which uses human correction during the steps of active learning could potentially help produce a large amount of annotated data while minimizing the cost of human annotation effort.  Unlike other text annotation tasks, the code an-notation for content analysis requires significant cognitive effort, which may limit, or even nulli-fy, the benefits of active learning.   We are building an active machine learning system to semi-automate the process of content analysis, and are planning to study the human role in such machine learning systems.   
Manually code 
documents in 
ATLAS.ti
Export gold 
standard data into 
XML
Human Annotation Machine Annotation
Learn model
Apply model to 
additional 
documents
Human Correction
Manually correct 
model coding
Save corrected 
coding as silver data
 Figure 1. Active learning for semi-automatic content analysis.  As illustrated in Figure 1, the system design in-corporates building a classifier from an initial set of hand-coded examples and iteratively improv-
59
ing the model by having human annotators cor-rect new examples identified by the system  Little is yet known about the optimal number of machine annotations to be presented to human annotators for correction, and how the sample sizes of machine annotations affect ML perfor-mance. Also, existing active learning sampling strategies to pick out the most ?beneficial? ex-amples for human correction to be used in the next round of ML training have not been tested in the context of social science data, where con-cept codes may be multi-dimensional or hierar-chical, and the problem may be multi-label (one phrase or sentence in the annotated text has mul-tiple labels). Also, concept codes tend to be very sparse in the text, resulting in a classification problem that has both imbalance?the non-annotated pieces of text (negative examples) tend to be far more frequent that annotated text?and rarity, where there may not be enough examples of some codes to achieve a good classifier.  The cost of conducting comprehensive human subject studies to experimentally determine the effects of sampling sizes and sampling sizes is high. Therefore, we first applied an active learn-ing simulation approach to test the effect of dif-ferent sampling strategies and sampling sizes on machine learning (ML) performance. This allows the human subject studies to involve a smaller set of parameters to be evaluated. 2 Related Work For active learning in our system, we are using what is sometimes called pool-based active learning, where a large number of unlabeled ex-amples are available to be the pool of the next samples. This type of active learning has been well explored for text categorization tasks (Lewis and Gale, 1994; Tong and Koller 2000; Schohn and Cohn 2000). This approach often uses the method of uncertainty sampling to pick new samples from the pool, both with probability models to give the ?uncertainty? (Lewis and Gale, 1994) and with SVM models, where the margin numbers give the ?uncertainty? (Tong and Koller 2000; Schohn and Cohn 2000). While much of the research focus has been on the sam-pling method, some has also focused on the size of the sample, e.g. in Schohn and Cohn (2000), sample sizes of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with a greater classification cost, and the authors settled on a sample size of 8. For 
additional active learning references, see the Set-tles (2009) survey of active learning literature. This type of active learning has also been used in the context of human correction. One such system is described in Mandel et al. (2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label. Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks.  While most active learning experiments focus on reduc-ing the number of examples to achieve an accu-rate model, there has been some effort to model the reduction of the cost of human time in anno-tation, where the human time is non-uniform per example.  Both the systems in Culotta and McCallum (2005) and in Clancy et al. (2012) for the task of named entity extraction, modeled hu-man cost in the context of sequential information extraction tasks.  However, one difference be-tween these systems and ours is that all of the tasks studied in these systems did not require annotators to have extensive training to annotate complex concept codes.  3 Problem We worked with a pilot project in which researchers are studying leadership in open source software groups by analyzing open source developer emails. After a year of part-time annotation by two annotators, the researchers developed a codebook that provides a definition and examples for 35 codes. The coders achieved an inter-annotator agreement (kappa) of about 80%, and annotated about 400 email threads, consisting of about 3700 sentences. We used these coded messages as the ?gold standard? data for our study. However, only 15 codes had more than 25 instances in the gold standard set. The most common code (?Explanation/Rationale/ Background?) occurred only 319 times.  In our pilot correction experiments, annota-tors tried correcting samples of sizes ranging from about 50 to about 400. Anecdotal evidence indicates that annotators liked to annotate sample sizes of about 100 in order to achieve good focus on a particular code definition at one time, but without getting stressed with too many examples.  Part of the required focus is that annotators need to refresh their memory on any particular code at the start of annotation, so switching frequently between different codes is cognitively taxing. This desired sample size contrasts with prior ac-
60
tive learning systems that employ much smaller sample sizes, in the range of 1 to 20.  We are currently in the process of setting up the human experiments to test our main research question of achieving an accurate model for con-tent analysis using a minimum of human effort. In this paper, we discuss two questions for active learning in order to have annotators cor-rect an acceptable number of machine annota-tions that are most likely to increase the perfor-mance of the ML model in each iteration. These are:  how do different sample sizes and different sampling strategies of machine annotations pre-sented to human annotators for correction in each round affect ML performance?  4 Active Learning Simulation Setup In a similar strategy to that of Clancy et al. (2012), we carried out a preliminary investiga-tion by conducting an active learning simulation on our gold standard data. The simulation starts with a small initial sample, and uses active learn-ing where we ?correct? the sample labels by tak-ing labels from the gold standard corpus. For our simulation experiments, we separated the gold standard data randomly into a training set of 90% of the examples, 3298 sentences, and a test set of 10%, 366 sentences.  In the experimental setup, we used a version of libSVM that was modified to produce num-bers of distance to the margin of the SVM classi-fication. We implemented the multi-label classi-fication by classifying each label separately where some sentences have the selected label and all others were counted as ?negative? labels. We used svm weights to handle the problem of imbalance in the negative examples. After exper-imentation with different combinations of fea-tures, we used a set of features that was best overall for the codes: unigram tokens lowercased and filtered by stop words, bigrams, orthographic features from capitalization, the token count, and the role of the sender of the email. For an initial sample, we randomly chose 3 positive and 3 negative examples from the de-velopment set to be the initial training set used for all experimental runs. We carried out experi-ments with a number of sample sizes, b, ranging over 5, 10, 20, 40, 50, 60, 80 and 100 instances. For experiments on methods used to select correction examples, we have chosen to experi-ment with sampling methods similar to those found in Lewis and Gale (1994) and Lewis (1995) using a random sampling method, where 
a new sample is chosen randomly from the re-maining examples in the development set, a rele-vance sampling method, where a new sample is chosen as the b number of most likely labeled candidates in the development set with the larg-est distance from the margin of the SVM classi-fication, and an uncertainty sampling method, where a new sample is chosen as the b number of candidates in the region of uncertainty on either side of the margin of the SVM classification. 5 Preliminary Results In this simulation experiment, the pool size is quite small (3664 examples) compared to the large amount of unlabeled data that is normally available for active learning, and would be avail-able for our system under actual use. We tested the active learning simulation on 8 codes. There was no clear winning sampling strategy out of the 3 we used in the simulation experiment but random sampling (5 out of 8 codes) appeared to be the one that most often produced the highest F?2 score in the shortest number of iterations. Figure 2 shows the F?2 score for each sampling strategy based on code ?Opinion/Preference? using sample sizes 5 and 100 respectively.  As for sampling sizes, we did not observe a large difference in the evolution of the F?2 score between the various sample sizes, and the learn-ing curves in Figure 2, shown for the sample siz-es of 5 and 100, are typical. This means that we should be able to use larger sample sizes for hu-man subject studies to achieve the same im-provements in performance as with the smaller sample sizes, and can carry out the experiments to relate the cost of human annotation with in-creases in performance. 
 
 Figure 2. Active ML performance for code Opinion/Preference. 
61
6 Conclusion and Future Work Our findings are inconclusive as we have yet to run the active learning simulations on all the codes. However, preliminary results are directing us towards using larger sample sizes and then experimenting with random and uncertainty sampling in the human subject studies.  From our experiments with the different codes, we found the performance on less fre-quent codes to be problematic as it is difficult for the active learning system to identify potential positive examples to improve the models. While the system performance may improve to handle such sparse cases, it may be better to modify the codebook instead. We plan to give the user feed-back on the performance of the codes at each iteration of the active learning and support modi-fications to the codebook, for example, the user may wish to drop some codes or collapse them according to some hierarchy. After all, if a code is not found in the text, it is hard to argue for its theoretical importance.  We are currently completing the design of the parameters of the active learning process for the human correction experiments on our pilot project with the codes about leadership in open source software groups. We will also be testing and undergoing further development of the user interface for the annotators.  Our next step will be to test the system on other projects with other researchers. We hope to gain more insight into what types of coding schemes and codes are easier to learn than oth-ers, and to be able to guide social scientists into developing coding schemes that are not only based on the social science theory but also useful in practice to develop an accurate classifier for very large amounts of digital text.  Acknowledgements:  This material is based upon work supported by the National Science Foundation under Grant No. IIS-1111107. Kevin Crowston is supported by the National Science Foundation. Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. The authors gratefully acknowledge helpful suggestions by the reviewers. 
Reference Broadwell, G. A., Stromer-Galley, J., Strzalkowski, T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, A., Jiao, L., & Webb, N. (2013). Modeling sociocul-tural phenomena in discourse. Natural Language Engineering, 19(02), 213?257.  Clancy, S., Bayer, S. and Kozierok, R. (2012)  ?Ac-tive Learning with a Human In The Loop,? Mitre Corporation.  Crowston, K., Allen, E. E., & Heckman, R. (2012). Using natural language processing technology for qualitative data analysis. International Journal of Social Research Methodology, 15(6), 523?543.  Culotta, A. and McCallum, A. (2005) ?Reducing La-beling Effort for Structured Prediction Tasks.? Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng, A.-S., & Templeton, T. C. (2010). Investigating multi-label classification for human values. Pro-ceedings of the American Society for Information Science and Technology, 47(1), 1?4.  Miles, M. B., & Huberman, A. M. (1994). Qualitative data analysis: An expanded sourcebook. Sage Pub-lications.  Lewis, D. D., & Gale, W. A. (1994). A sequential algorithm for training text classifiers. In Proceed-ings of the 17th annual international ACM SIGIR conference on Research and development in infor-mation retrieval (pp. 3-12). Lewis, D. D. (1995). A sequential algorithm for train-ing text classifiers: Corrigendum and additional da-ta. In ACM SIGIR Forum (Vol. 29, No. 2, pp. 13-19). Mandel, M. I., Poliner, G. E., & Ellis, D. P. (2006). Support vector machine active learning for music retrieval. Multimedia systems, 12(1), 3-13.  Schohn, G., & Cohn, D. (2000). Less is more: Active learning with support vector machines. In Interna-tional Conference on Machine Learning (pp. 839-846).  Settles, B. (2010). Active learning literature survey. University of Wisconsin, Madison, 52, 55-66. Settles, B. (2011). Closing the loop: Fast, interactive semi-supervised annotation with queries on fea-tures and instances. In Proceedings of the Confer-ence on Empirical Methods in Natural Language Processing (pp. 1467-1478).  Settles, B., & Zhu, X. (2012). Behavioral factors in interactive training of text classifiers. In Proceed-ings of the 2012 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies (pp. 563-567).   Tong, S., & Koller, D. (2002). Support vector ma-chine active learning with applications to text clas-sification. The Journal of Machine Learning Re-search, 2, 45-66.   
62
