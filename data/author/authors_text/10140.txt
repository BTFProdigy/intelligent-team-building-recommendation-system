Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 377?384
Manchester, August 2008
Generating Chinese Couplets using a Statistical MT Approach 
Long Jiang 
Microsoft Research Asia 
Sigma Center, No. 49, Zhichun Road 
Haidian District, Beijing 100190, PRC 
longj@microsoft.com 
Ming Zhou 
Microsoft Research Asia 
Sigma Center, No. 49, Zhichun Road 
Haidian District, Beijing 100190, PRC 
mingzhou@microsoft.com 
 
Abstract 
Part of the unique cultural heritage of 
China is the game of Chinese couplets 
(du?li?n). One person challenges the oth-
er person with a sentence (first sentence). 
The other person then replies with a sen-
tence (second sentence) equal in length 
and word segmentation, in a way that 
corresponding words in the two sentences 
match each other by obeying certain con-
straints on semantic, syntactic, and lexi-
cal relatedness. This task is viewed as a 
difficult problem in AI and has not been 
explored in the research community. 
In this paper, we regard this task as a 
kind of machine translation process. We 
present a phrase-based SMT approach to 
generate the second sentence. First, the 
system takes as input the first sentence, 
and generates as output an N-best list of 
proposed second sentences, using a 
phrase-based SMT decoder. Then, a set 
of filters is used to remove candidates vi-
olating linguistic constraints. Finally, a 
Ranking SVM is applied to rerank the 
candidates. A comprehensive evaluation, 
using both human judgments and BLEU 
scores, has been conducted, and the re-
sults demonstrate that this approach is 
very successful. 
1 Introduction 
Chinese antithetical couplets, called ?du?li?n?, 
form a special type of poetry composed of two 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
sentences. They use condensed language, but 
have deep and sometimes ambivalent meanings. 
The two sentences making up the couplet are 
called the ?first sentence? (FS) and the ?second 
sentence? (SS) respectively. 
Chinese couplets are considered an important 
cultural heritage. A couplet is often written in 
calligraphy on vertical red banners, and typically 
placed on either side of a door or in a large hall 
during special occasions such as wedding cere-
monies and the Chinese New Year. People also 
use couplets to celebrate birthdays, mark the 
openings of a business, and commemorate histor-
ical events. Chinese couplets have also been used 
effectively in teaching Chinese in China. 
An example of a Chinese couplet is ?? ? 
? ? ?; ? ? ? ? ??, where the FS is 
?? ? ? ? ?? and the SS is ?? ? ? ? 
??. It says that the sea is wide enough so that 
fish can jump at their pleasure, and the sky is 
high enough so that bird can fly unrestrictedly. 
The correspondence between individual words of 
the FS and SS is shown here: 
     
? ? ? ? ? 
sea wide allow fish  jump 
| | | | | 
? ? ? ? ? 
sky  high  permit  bird fly  
Figure 1. An Example of a Chinese Couplet. 
Generating the SS of a Chinese couplet given 
the FS can be viewed as a big challenge in AI. 
As far as we know, there is no previous work to 
tackle this problem. 
The general process of generating a SS given a 
FS is like this: for each word in the FS, find 
some words that can be used as the counterparts 
in the SS; then from the word lattice, select one 
word at each position in the SS so that the se-
377
lected words form a fluent sentence satisfying the 
constraints of Chinese couplets. This process is 
similar to translating a source language sentence 
into a target language sentence without word in-
sertion, deletion and reordering, but the target 
sentence should satisfy some linguistic con-
straints. Based on this observation, we propose a 
multi-phase statistical machine translation ap-
proach to generate the SS. First, a phrase-based 
SMT model is applied to generate an N-best list 
of SS candidates. Then, a set of filters based on 
linguistic constraints for Chinese couplets is used 
to remove low quality candidates. Finally, a 
Ranking SVM is applied to rerank the candidates. 
We implemented a web service based on our 
approach (anonymous URL). A user can input a 
FS, and our software outputs its top 10 best-
scoring SS candidates. Tens of thousands of 
people use our service every day. 
The rest of the paper is organized as follows. 
In Section 2, we explain the motivation of our 
work. Then Sections 3 and 4 detail our multi-
phase SMT approach for the SS generation. The 
experimental results and evaluation are reported 
in Section 5 and related work on computer poetry 
is summarized in Section 6. In Section 7, we 
conclude our study and point out the future work. 
2 Motivation 
Chinese couplets vary widely in length. A short 
couplet could consist of two sentences each con-
taining only one or two characters while a longer 
couplet may reach several hundred characters. 
However, the length of sentences in most Chi-
nese couplets is between 5 and 10 characters. 
There are also diverse forms of writing couplets. 
For instance, in one form, the FS and SS are sim-
ilar in meaning, while in another, they have to 
oppose in meaning.  
However, no matter which form a couplet fol-
lows, it generally must conform to the following 
constraints: 
Constraint 1: The two sentences of a couplet 
agree in length and word segmentation. For ex-
ample, if a FS contains 7 characters and the first 
two characters form a word, then the qualified SS 
should also contain 7 characters with the first 
two forming a word. 
Constraint 2: Tones are generally ?coinciding 
and harmonious?: In Chinese, every character is 
pronounced either ?Ping? (?) or ?Ze? (?). In a 
Chinese couplet, the character at the end of the 
FS should be ?Ze? (pronounced in a sharp 
downward tone); the character at the end of the 
SS should be ?Ping? (pronounced in a level tone).  
Constraint 3: Corresponding words in the two 
sentences should agree in their part of speech and 
characteristics. For instance, a noun in the SS 
should correspond to a noun at the same position 
in the FS. A named entity should correspond to a 
named entity. 
Constraint 4: The contents of the two sen-
tences should be related, but not duplicated. 
Constraint 5: The two sentences should be 
identical in their writing styles. For instance, if 
there is a repetition of words, characters, or pro-
nunciations in the FS, the SS should contain an 
identical repetition. And if there is a character 
decomposition in the FS, i.e., the FS contains a 
character and its ?component characters?, the SS 
should contain a character decomposition at the 
corresponding positions. 
Character decomposition is an interesting lan-
guage phenomenon in Chinese: some Chinese 
characters can be decomposed into other charac-
ters. For example, ??? (good) can be decom-
posed into ??? (daughter) and ??? (son). As 
illustrated in Figure 2, the left part of ??? is 
??? and the right part is ???. ??? and ??? are 
called the ?component characters? of ???. 
 
? 
? ? 
 
Figure 2. Character Decomposition. 
Compared to western couplets, which also 
consist of two sentences that usually rhyme and 
have the same number of syllables, Chinese 
couplets have much stronger constraints. Be-
cause in Chinese each character has one and only 
one syllable, the same number of syllables means 
the same number of characters. Moreover, the 
constraints of the FS and SS on consistency of 
part of speech sequence and writing style make 
Chinese couplets have more regular form. 
Given the FS, writing a good SS to match it is 
a difficult task because the SS must conform to 
constraints on syntax, rhyme and semantics, as 
described above. It also requires the writer to 
innovatively use extensive knowledge in differ-
ent disciplines. Some of the difficulties can be 
seen from the following example: 
378
? ? ? ? ? ? ? 
have daughter have son so call good 
| | | | | | | 
? ? ? ? ? ? ? 
lack fish lack mutton dare call delicious 
Figure 3. An Example of a Complicated Couplet. 
Figure 3 shows a complicated couplet of ?? 
? ? ? ? ? ?; ? ? ? ? ? ? ?? (Once 
one has a daughter and son, one?s life is com-
plete; who would dare call a meal without fish 
and mutton delicious? In China, there is an old 
saying that courses made of fish and mutton are 
most delicious). The FS contains a repeated cha-
racter ??? (have), and a character decomposi-
tion: ??? (good) and its ?component characters? 
??? (daughter) and ??? (son). So it requires 
that the qualified FS should contain identical 
character repletion and character decomposition. 
A perfect SS worked out after multiple attempts 
by many people for this FS is ?? ? ? ? ? ? 
??, which equally contains a repeated character 
??? (lack), and a character decomposition: ??? 
(fresh) and its ?component characters? ??? (fish) 
and ??? (mutton) at the corresponding positions. 
And the meanings of the two sentences are also 
parallel: they tell us what is important in life and 
what is important in cuisine, respectively. 
3 Couplet Generation Model 
In this paper, a multi-phase SMT approach is 
designed, where an SMT system generates an N-
best list of candidates and then a ranking model 
is used to determine the new ranking of the N-
best results using additional features. This ap-
proach is similar to recent reranking approaches 
of SMT (Och and Ney, 2004). In the SMT sys-
tem, a phrase-based log-linear model is applied 
where two phrase translation models, two lexical 
weights and a language model are used to score 
the output sentences, and a monotone phrase-
based decoder is employed to get the N-best re-
sults. Then a set of filters based on linguistic 
constraints of Chinese couplets are used to re-
move candidates of low quality. Finally a Rank-
ing SVM model is used to rerank the candidates 
using additional features like word associations, 
etc. 
3.1 Phrase-based SMT Model 
Given a FS denoted as },...,,{ 21 nfffF ? , our 
objective is to seek a SS denoted as 
},...,,{ 21 nsssS ? , where fi and si are Chinese 
characters, so that p(S|F) is maximized. 
Following Och and Ney (2002), we depart from 
the traditional noisy-channel approach and use a 
more general log-linear model. Then the S* that 
maximizes p(S|F) can be expressed as follows: 
?
?
?
?
M
i
ii
S
S
FSh
FSpS
1
),(logmaxarg        
)|(maxarg*
?
 
(1) 
where the hi(S,F) are feature functions and M 
is the number of feature functions. In our design, 
characters are used instead of words as transla-
tion units to form phrases. This is because Chi-
nese couplets use dense language like traditional 
Chinese and most of words contain only one cha-
racter. If we try to incorporate Chinese word 
segmentation, it may bring in unexpected errors. 
However, we will still report the comparison to 
the word-based method in Subsection 5.3. 
Among features commonly used in phrase-
based SMT, five features, listed in Table 1, were 
selected for our model. To apply phrase-based 
features, S and F are segmented into phrases 
Iss ...1  and Iff ...1 , respectively. We assume a 
uniform distribution over all possible segmenta-
tions. 
?
?
? I
i
ii sfpFSh
1
1 )|(),(
 Phrase translation 
model 
?
?
? I
i
ii fspFSh
1
2 )|(),(
 Inverted phrase 
translation model 
?
?
? I
i
iiw sfpFSh
1
3 )|(),(
 Lexical weight 
?
?
? I
i
iiw fspFSh
1
4 )|(),(
 Inverted lexical 
weight 
)(),(5 SpFSh ?  Language model 
Table 1. Features in our SMT Model. 
Phrase translation model (PTM) 
In a phrase-based SMT model, phrases can be 
any substring that may not necessarily be linguis-
tically motivated. In our implementation, we ex-
tract phrases of up to 4-character-grams. 
In a Chinese couplet, there is generally a direct 
one-to-one mapping between corresponding 
words in the FS and SS, respectively. As a result, 
the ith character/phrase in F is exactly ?trans-
lated? into the ith character/phrase in S. Based on 
this rule, the phrase translation probability 
)|( ii sfp  can be estimated by relative frequency 
on a training corpus: 
379
?
?
? m
r
ir
ii
ii
sfcount
sfcountsfp
1
),(
),()|(
 
(2) 
where m is the number of distinct phrases that 
can be mapped to the phrase is  and 
),( ii sfcount  is the number of occurrences that 
if  and is  appear at the corresponding positions 
in a couplet.  
The inverted phrase translation model 
)|( ii fsp  has been proven useful in previous 
SMT research work (Och and Ney, 2002); so we 
also include it in our phrase-based SMT model.  
Lexical weight (LW) 
Previous research work on phrase-based SMT 
has found that it is important to validate the qual-
ity of a phrase translation pair (Koehn et al, 
2003). A good way to do this is to check its lexi-
cal weight )|( iiw sfp , which indicates how well 
its words translate to each other:  
?
?
? Ni
j
jjiiw sfpsfp
1
)|()|(
 (3) 
where Ni is the number of characters in 
if  or 
is , jf  and js  are characters in if  and is  respec-
tively, and )|( jj sfp  is the character translation 
probability of js  into jf . Like in phrase transla-
tion probability estimation, )|( jj sfp  can be 
computed by relative frequency: 
?
?
? m
r
jr
jj
jj
sfcount
sfcountsfp
1
),(
),()|(
 
(4) 
where m is the number of distinct characters 
that can be mapped to the character js  and 
),( jj sfcount  is the number of occurrences that 
js  and jf  appear at the corresponding positions 
in a couplet.  
Like for the phrase translation model, we also 
use an inverted lexical weight )|( iiw fsp  in addi-
tion to the conventional lexical weight )|( iiw sfp  
in our phrase-based SMT model.  
Language model (LM) 
A character-based trigram language model with 
Katz back-off is constructed from the training 
data to estimate the language model p(S) using 
Maximum Likelihood Estimation. 
3.2 Model Training 
A Chinese couplet corpus is necessary for esti-
mating the phrase and character translation prob-
abilities. Currently, there is, however, no large-
sized Chinese couplet collection available. Based 
on our observation, there are many pages on the 
web containing classic Chinese couplets collec-
tively. So we used the method proposed by (Fan 
et al, 2007) to recursively mine those couplets 
with the help of some seed couplets. The method 
can automatically learn patterns in a page which 
contains collectively Chinese couplets and then 
apply the learned pattern to extract more Chinese 
couplets. There are also some online forums 
where Chinese couplet fans meet. When some 
people post FSs on the forums, many other 
people submit their SSs in response. Such data 
seems useful for our model training. So we 
crawled all posted FSs with all their replied SSs. 
Then from the crawled data, FSs having over 20 
unique SSs are selected as development or test-
ing set (see Subsection 5.1), and others are used 
for model training. Finally, with web mining ap-
proach, we collected 670,000 couplets.  
To enhance the couplet database crawled from 
the web, we also mined pairs of sentences of 
poetry which satisfied the constraints of couplets 
although they were not originally intended as 
couplets. For instance, in eight-sentence Tang 
poetry, the third and fourth sentences and the 
fifth and sixth sentences form pairs basically sa-
tisfying the constraints of Chinese couplets. 
Therefore, these sentence pairs can be used as 
couplets in our training algorithm. In that way we 
get additional 300,000 sentence pairs yielding a 
total of 970,000 sentence pairs of training data. 
Because the relationships between words and 
phrases in the FS and SS are usually reversible, 
to alleviate the data sparseness, we reverse the 
FS and SS in the training couplets and merge 
them with original training data for estimating 
translation probabilities.  
For the language people use in Chinese coup-
lets is same as that in Chinese poetry, for the 
purpose of smoothing the language model we 
add about 1,600,000 sentences from ancient Chi-
nese poetry to train language model, which are 
not necessarily couplets. 
To estimate the weights ?i in formula (1), we 
use Minimum Error Rate Training (MERT) algo-
rithm, which is widely used for phrase-based 
SMT model training (Och, 2003). The training 
data and criteria (BLEU) for MERT will be ex-
plained in Subsection 5.1.  
380
4 Couplet Generation 
In this section, we will detail each step of the 
generation of the second sentence. 
4.1 Decoding for N-best Candidates 
First, we use a phrase-based decoder similar to 
the one by (Koehn et al, 2003) to generate an N-
best list of SS candidates. Because there is no 
word reordering operation in the SS generation, 
our decoder is a monotonic decoder. In addition, 
the input FS is often shorter than ordinary MT 
input sentence, so our decoder is more efficient. 
4.2 Linguistic Filters 
A set of filters is used to remove candidates that 
violate linguistic constraints that well-formed 
Chinese couplets should obey.  
Repetition filter 
This filter removes candidates based on various 
rules related to word or character repetition. One 
such rule requires that if there are characters that 
are identical in the FS, then the corresponding 
characters in the SS should be identical too. For 
example, in a FS ?? ? ? ? ? ? ?? (have 
daughter have son so call good), the word ??? is 
repeating. The legal SS should also contain cor-
responding repeating words. For instance, a qual-
ified second sentence ?? ? ? ? ? ? ?? 
(lack fish lack mutton dare call delicious) would 
be legal because ??? corresponds to ??? and is 
repeating in the same way. Conversely, if there 
are no identical words in the FS, then the SS 
should have no identical words.  
Pronunciation repetition filter  
This filter works similarly to the repetition filter 
above except it checks the pronunciation of cha-
racters not the character surfaces. The pronuncia-
tion of a character can be looked up from a Chi-
nese character pronunciation dictionary. For 
simplicity, we only use the first pronunciation in 
the dictionary for polyphones. 
Character decomposition filter 
We compiled a Chinese character decomposition 
table from which one can look up what charac-
ters a Chinese character can be decomposed into. 
The decomposition information can be derived 
from the strokes of each character in a dictionary 
and then verified by human. Based on this table, 
we can easily filter out those SS candidates 
which contain different character decompositions 
at the corresponding positions from the FS. 
Phonetic harmony filter 
We filter out the SSs with improper tones at the 
end character position according to the Chinese 
character pronunciation dictionary. 
4.3 Reranking Based on Multiple Features 
In many cases, long-distance constraints are very 
helpful in selecting good SSs, however, it is dif-
ficult to incorporate them in the framework of 
dynamic programming decoding algorithm. To 
solve this issue, we designed an SVM-based re-
ranking model incorporating long-distance fea-
tures to select better candidates. 
As shown in formula (5), x?  is the feature vec-
tor of a SS candidate, and w?  is the vector of 
weights. ????,  stands for an inner product. f is the 
decision function with which we rank the candi-
dates. 
??? xwxfw ???? ,)(  (5) 
Besides the five features used in the phrase-
based SMT model, additional features for rerank-
ing are as follows: 
1. Mutual information (MI) score:  
This feature is designed to measure the semantic 
consistency of words in a SS candidate. For ex-
ample, the two candidates ?? ? ? ? ?? (sky 
high permit bird fly) and ?? ? ? ? ?? (sky 
high permit dog bark) have similar PTM, LW 
and LM scores. However, human beings recog-
nize the former as a better phrase, because ?? 
?? (sky high) and ?? ?? (dog bark) in the lat-
ter sentence do not make any sense together. MI 
can capture the associations between words, 
whether they are adjacent or not.  
Specifically, given a SS candidate 
},...,,{ 21 nsssS ? , we use the following formula to 
compute the MI score: 
? ?????
?
? ????
?? 1
1
1
1 11 )()(
),(log),()( n
i
n
i
n
ij ji
jin
ij
ji spsp
sspssISMI
 (6) 
The parameters p(si,sj), p(si) and p(sj) are esti-
mated using Maximum Likelihood Estimation on 
the same training data as for training PTM. 
2. MI-based structural similarity (MISS) score:  
In a Chinese couplet, if two words in the FS are 
strongly associated, their corresponding words in 
the SS should also be strongly associated, and 
vice versa. For example, in the couplet ?? ? ? 
? ?; ? ? ? ? ?? (sea wide allow fish jump; 
sky high permit bird fly), the word pairs ??? 
381
(sea) and ??? (wide), ??? (sea) and ??? (fish), 
??? (fish) and ??? (jump) in the FS are all 
strongly associated. Similarly, the corresponding 
word pairs ??? (sky) and ??? (high), ??? (sky) 
and ??? (bird), ??? (bird) and ??? (fly) in the 
SS are all strongly associated. To measure this 
kind of structural similarity, we develop a meas-
ure function called MI-based structural similarity 
score. Specifically, given the FS 
},...,,{ 21 nfffF ? , we first build its vector 
}.,,,..,,{ 12311312 nnnf vvvvvV ?? , where vij is the mu-
tual information of fi and fj (i.e., ),( ji ffI  in for-
mula (6)). Then we build a vector Vs for each SS 
candidate in the same way. We use a cosine 
function to compute the similarity between the 
two vectors as the MISS score: 
||||),cos(),( sf
sfsf VV
VVVVSFMISS ?
???
 (7) 
To estimate the parameter vector in the Rank-
ing SVM model, we used an existing training 
tool, SVM Light2, and a labeled training corpus. 
We selected 200 FSs with a length of 7 or 8 cha-
racters. For each of them, 50 SS candidates are 
generated using the N-best SMT decoder. Two 
operators are asked to label each SS candidate as 
positive if the candidate is acceptable and as 
negative if not. After removing 10 FSs and their 
SS candidates as they had no corresponding posi-
tive SS, we got 190 FSs with 9,500 labeled SS 
candidates (negative: 6,728; positive: 2,772) to 
train the Ranking SVM model. 
5 Experimental Results 
5.1 Evaluation Method 
Automatic evaluation is very important for 
parameter estimation and system tuning. An 
automatic evaluation needs a standard answer 
data set and a metric to show for a given input 
sentence the closeness of the system output to the 
standard answers. Since generating the SS given 
the FS can be viewed as a kind of machine 
translation process, the widely accepted 
automatic SMT evaluation methods may be 
applied to evaluate the generated SSs. 
BLEU (Papineni, et al 2002) is widely used 
for automatic evaluation of machine translation 
systems. It measures the similarity between the 
MT system output and human-made reference 
translations. The BLEU metric ranges from 0 to 
                                                 
2 http://svmlight.joachims.org/ 
1 and a higher BLEU score stands for better 
translation quality. 
)logexp(
1
?
?
?? N
n
nn pwBPBLEU
 (8) 
Some adaptation is necessary to use BLEU for 
evaluation of our couplet generator. First, pn, the 
n-gram precision, should be position-sensitive in 
the evaluation of SSs. Second, BP, the brevity 
penalty, should be removed, because all system 
outputs have the same length and it has no effect 
in evaluating SSs. Moreover, because the couplet 
sentences usually have less than 10 characters, 
we set n to 3 for the evaluation of SSs, while in 
MT evaluation n is often set to 4. 
It is important to note that the more reference 
translations we have for a testing sentence, the 
more reasonable the evaluation score is. From 
couplet forums mentioned in Subsection 3.2, we 
collected 1,051 FSs with diverse styles and each 
of them has over 20 unique SS references. After 
removing some noisy references by human, each 
of them has 24.3 references on average. The min-
imum and maximum number of references is 20 
and 40. Out of these data, 600 were selected for 
MERT training and the remaining 451 for testing. 
5.2 BLEU vs. Human Evaluation 
To justify whether BLEU is suitable for evaluat-
ing generated SSs, we compare BLEU with hu-
man evaluation. Figure 4 shows a linear regres-
sion of the human evaluation scores as a function 
of the BLEU score for our 6 systems which gen-
erate SSs given FSs. Among the 6 systems, three 
are implemented using a word-based SMT model 
with 100K, 400K, and 970K couplets for training, 
respectively, while the other three are imple-
mented using a phrase-based SMT model with 
100K, 400K, and 970K couplets for training, re-
spectively. The word-based SMT model contains 
only two features: word translation model and 
language model. The word translation model is 
trained on the corpus segmented by a Chinese 
word breaker implemented by (Gao et al, 2003). 
We selected 100 FSs from the testing data set; 
for each of them, the best SS candidate was gen-
erated using each system. Then we computed the 
BLEU score and the human score of each system.  
The human score is the average score of all SS 
candidates. Each candidate is scored 1 if it is ac-
ceptable, and 0 if not. The correlation of 0.92 
indicates that BLEU tracks human judgment well. 
382
 Figure 4: BLEU Predicts Human Judgments. 
5.3 Translation Unit Setting 
We conducted some experiments to compare the 
system performances with different translation 
unit settings: character-based, word-based and 
phrase-based. In each setting, we only use trans-
lation probability and language model as features. 
And after SMT decoder, we use the same filter-
ing but no reranking. We use all 451 testing data 
and the results are listed below: 
Translation Unit setting BLEU 
character-based 0.236 
word-based 0.261 
phrase-based 0.276 
Table 2. Different Translation Unit Setting. 
As shown in Table 2, the word-based transla-
tion model achieves 0.025 higher of BLEU score 
than the character-based model. And the phrase-
based model gets the highest score. The im-
provement shows that phrase-based model works 
better than word-based and character-based mod-
el in our task of SS generation. 
5.4 Feature Evaluation 
We also conducted some experiments incremen-
tally to evaluate the features used in our phrase-
based SMT model and reranking model. All test-
ing data are used. The results are listed below. 
 Features BLEU 
Phrase-
based 
SMT 
Model 
Phrase TM(PTM) + LM 0.276 
+ Inverted PTM 0.282 
+ Lexical Weight (LW) 0.315 
+ Inverted LW 0.348 
Ranking 
SVM 
+ Mutual information (MI) 0.356 
+ MI-based structural 
similarity 
0.361 
Table 3. Feature Evaluation. 
As shown in Table 3, with two features: the 
phrase translation model and the language model, 
the phrase-based SMT model can achieve a 
0.276 of BLEU score. When we add more fea-
tures incrementally, the BLEU score is improved 
consistently. Furthermore, with the Ranking 
SVM model, the score is improved by 0.13 per-
cent, from 0.348 to 0.361. This means our re-
ranking model is helpful.  
5.5 Overall Performance Evaluation 
In addition to the BLEU evaluation, we also car-
ried out human evaluation. We select 100 FSs 
from the log data of our couplet web service 
mentioned in Section 1. For each FS, 10 best SS 
candidates are generated using our best system. 
Then each SS candidate is labeled by human as 
acceptable or not. The evaluation is carried out 
using top-1 and top-10 results based on top-n 
inclusion rate. Top-n inclusion rate is defined as 
the percentage of the test sentences whose top-n 
outputs contain at least one acceptable SS. The 
results are listed below: 
 Top-1 Top-10 
Top-n inclusion rate 0.21 0.73 
Table 4. Overall Performance Evaluation. 
As shown in Table 4, our system can get a 
0.21 of top-1 inclusion rate and 0.73 of top-10 
inclusion rate. The numbers seem a little low, but 
remember that generating a SS given a FS is a 
quite difficult job, and even humans cannot do it 
well in limit time, for example, 5 minute per FS. 
However, what is more important is that our sys-
tem can provide users diversified SSs and many 
unacceptable SSs generated by our system can be 
easily refined by users to become acceptable.  
We also made careful analysis on the 27 FSs 
whose top-10 outputs contain no acceptable SS. 
As shown in Table 5, the errors mainly come 
from three aspects: unidentified named entity, 
complicated character decomposition and repeti-
tion. An example of complicated repetition is ??
? ?? ?? ??? (modern /scholar/all/myopic, 
modern scholars are all myopic). In this sentence, 
the pronunciations of the four words are identical 
(j?nsh?), a qualified SS must be meaningful and 
posses same repetitions, which poses a big chal-
lenge to the system. 
Mistake types # of FS 
Mistakes with named entities 6 
Complicated character decomposition 5 
Complicated repetition 4 
Mistakes of miscellaneous types 12 
Table 5. Error Analysis. 
383
6 Related Work 
To the best of our knowledge, no research has 
been published on generating the SS given the 
FS of a Chinese couplet. However, because our 
task can be viewed as generating the second line 
of a special type of poetry given the first line, we 
consider automatic poetry generation to be the 
most closely related existing research area. 
As to computer-assisted Chinese poetry gener-
ation, Luo has developed a tool3 which provides 
the rhyme templates of forms of classical Chi-
nese poetry and a dictionary in which one can 
look up the tone of a Chinese character. Both the 
rhyme templates and the dictionary were com-
piled by human efforts. 
For other languages, approaches to creating 
poetry with computers began in 1959 when Theo 
Lutz created the first examples of ?Computer 
Poetry? in Germany (Hartman, 1996). Master-
man finished a haiku producer (Manurung et al, 
2000). Other systems include RACTER and 
PROSE (Hartman, 1996). Approaches to poetry 
generation can roughly be classified into tem-
plate-based, evolutionary, and case-based reason-
ing. Typically, for the template-based approach, 
the generation process randomly chooses words 
from a hand-crafted lexicon and then fills in the 
gaps provided by a template-based grammar. In 
computer poetry systems, the starting point is a 
given message, or communicative goal, and the 
aim is to produce a string of text that conveys 
that message according to the linguistic resources 
available.  
There is a big difference between our task and 
poetry generation. When generating the SS of a 
Chinese couplet, the FS is given. The task of ge-
nerating the SS to match the FS is more well-
defined than generating all sentences of a poem. 
Furthermore, the constraints on Chinese couplets 
mentioned above will enable us to do a more ob-
jective evaluation of the generated SSs. 
7 Conclusions and Future Work 
This paper presents a novel approach to solve the 
problem of generating Chinese couplets. An 
SMT approach is proposed to generate the SSs 
for a FS of a Chinese couplet. The system is 
comprised of a phrase-based SMT model for the 
generation of an N-best list of SS candidates, a 
set of linguistic filters to remove unqualified 
candidates to meet the special constraints of Chi-
nese couplets, and a discriminative reranking 
                                                 
3 http://cls.hs.yzu.edu.tw 
model incorporating multi-dimensional features 
to get better results. The experimental results 
show that this approach is very promising. 
As a future work, it would be interesting to in-
vestigate how this approach can be used in poe-
try generation. 
References 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra and 
R. L. Mercer. 1993. The mathematics of statistical 
machine translation: parameter estimation. Compu-
tational Linguistics, 19:2, 263-311. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Proc. 
of the 43rd Meeting of the Association for Compu-
tational Linguistics, pages 263-270. 
B. D?az-Agudo, P. Gerv?s and P. Gonz?lez-Calero. 
2002. Poetry generation in COLIBRI. In Proc. of 
the 6th European Conference on Case Based Rea-
soning, Aberdeen, Scotland. 
C. Fan, L. Jiang, M. Zhou, S.-L. Wang. 2007. Mining 
Collective Pair Data from the Web. In Proc. of the 
International Conference on Machine Learning and 
Cybernetics 2007, pages 3997-4002. 
Jianfeng Gao, Mu Li and Changning Huang. 2003. 
Improved source-channel models for Chinese word 
segmentation. In Proc. of the 41st Meeting of the 
Association for Computational Linguistics. 
Charles O. Hartman. 1996. Virtual Muse: Experi-
ments in Computer Poetry. Wesleyan University 
Press. 
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical 
phrase-based translation, In HLT-NAACL 2003, 
pages 48-54. 
H. Manurung, G. Ritchie and H. Thompson. 2001. 
Towards a computational model of poetry genera-
tion. In Proc. of the AISB-00 Symposium on Crea-
tive and Cultural Aspects of AI, 2001. 
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41st Meet-
ing of the Association for Computational Linguis-
tics. 
F. J. Och and H. Ney. 2002. Discriminative training 
and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Meeting of 
the Association for Computational Linguistics. 
F. J. Och and H. Ney. 2004. The Alignment Template 
Approach to Statistical Machine Translation. Com-
putational Linguistics, 30:417-449. 
K. Papineni, S. Roukos, T. Ward and W.-J. Zhu. 2002. 
BLEU: a Method for automatic evaluation of ma-
chine translation. In Proc. of the 40th Meeting of 
the Association for Computational Linguistics. 
384
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 870?878,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Mining Bilingual Data from the Web with Adaptively Learnt Patterns 
 
 
Long Jiang1, Shiquan Yang2, Ming Zhou1, Xiaohua Liu1, Qingsheng Zhu2 
1Microsoft Research Asia 
Beijing, 100190, P.R.China 
2Chongqing University,  
Chongqing, 400044, P.R.China 
{longj,mingzhou,xiaoliu}@microsoft.com shiquany@gmail.com,qszhu@cqu.edu.cn 
 
  
 
Abstract 
 
Mining bilingual data (including bilingual sen-
tences and terms1) from the Web can benefit 
many NLP applications, such as machine 
translation and cross language information re-
trieval. In this paper, based on the observation 
that bilingual data in many web pages appear 
collectively following similar patterns, an 
adaptive pattern-based bilingual data mining 
method is proposed. Specifically, given a web 
page, the method contains four steps: 1) pre-
processing: parse the web page into a DOM 
tree and segment the inner text of each node 
into snippets; 2) seed mining: identify poten-
tial translation pairs (seeds) using a word 
based alignment model which takes both trans-
lation and transliteration into consideration; 3) 
pattern learning: learn generalized patterns 
with the identified seeds; 4) pattern based min-
ing: extract all bilingual data in the page using 
the learned patterns. Our experiments on Chi-
nese web pages produced more than 7.5 mil-
lion pairs of bilingual sentences and more than 
5 million pairs of bilingual terms, both with 
over 80% accuracy. 
1 Introduction 
Bilingual data (including bilingual sentences and 
bilingual terms) are critical resources for build-
ing many applications, such as machine transla-
tion (Brown, 1993) and cross language informa-
tion retrieval (Nie et al, 1999). However, most 
existing bilingual data sets are (i) not adequate 
for their intended uses, (ii) not up-to-date, (iii) 
apply only to limited domains. Because it?s very 
hard and expensive to create a large scale bilin-
                                                 
1 In this paper terms refer to proper nouns, technical terms, 
movie names, and so on. And bilingual terms/sentences 
mean terms/sentences and their translations. 
gual dataset with human effort, recently many 
researchers have turned to automatically mining 
them from the Web. 
If the content of a web page is written in two 
languages, we call the page a Bilingual Web 
Page. Many such pages exist in non-English web 
sites. Most of them have a primary language 
(usually a non-English language) and a second-
ary language (usually English). The content in 
the secondary language is often the translation of 
some primary language text in the page.  
Since bilingual web pages are very common in 
non-English web sites, mining bilingual data 
from them should be an important task. However, 
as far as we know, there is no publication availa-
ble on mining bilingual sentences directly from 
bilingual web pages. Most existing methods for 
mining bilingual sentences from the Web, such 
as (Nie et al, 1999; Resnik and Smith, 2003; Shi 
et al, 2006), try to mine parallel web documents 
within bilingual web sites first and then extract 
bilingual sentences from mined parallel docu-
ments using sentence alignment methods.  
As to mining term translations from bilingual 
web pages, Cao et al (2007) and Lin et al (2008) 
proposed two different methods to extract term 
translations based on the observation that authors 
of many bilingual web pages, especially those 
whose primary language is Chinese, Japanese or 
Korean, sometimes annotate terms with their 
English translations inside a pair of parentheses, 
like ?c1c2...cn(e1 e2 ... em)? (c1c2...cn is a primary 
language term and e1 e2 ... em is its English trans-
lation).  
Actually, in addition to the parenthesis pattern, 
there is another interesting phenomenon that in 
many bilingual web pages bilingual data appear 
collectively and follow similar surface patterns. 
Figure 1 shows an excerpt of a page which intro-
duces different kinds of dogs2. The page provides 
                                                 
2 http://www.chinapet.net 
870
a list of dog names in both English and Chinese. 
Note that those bilingual names do not follow the 
parenthesis pattern. However, most of them are 
identically formatted as: ?{Number}?{English 
name}{Chinese name}{EndOfLine}?. One ex-
ceptional pair (?1.Alaskan Malamute ????
??? ?) differs only slightly. Furthermore, 
there are also many pages containing consistently 
formatted bilingual sentences (see Figure 2). The 
page3 lists the (claimed) 200 most common oral 
sentences in English and their Chinese transla-
tions to facilitate English learning. 
 
Figure 1. Consistently formatted term translation 
pairs 
 
Figure 2. Consistently formatted sentence trans-
lation pairs 
People create such web pages for various rea-
sons. Some online stores list their products in 
two languages to make them understandable to 
foreigners. Some pages aim to help readers with 
foreign language learning. And in some pages 
where foreign names or technical terms are men-
tioned, the authors provide the translations for 
disambiguation. For easy reference, from now on 
we will call pages which contain many consis-
tently formatted translation pairs Collective Bi-
lingual Pages. 
According to our estimation, at least tens of 
millions of collective bilingual pages exist in 
Chinese web sites. Most importantly, each such 
page usually contains a large amount of bilingual 
                                                 
3 http://cul.beelink.com/20060205/2021119.shtml 
data. This shows the great potential of bilingual 
data mining. However, the mining task is not 
straightforward, for the following reasons: 
1) The patterns vary in different pages, so 
it?s impossible to mine the translation 
pairs using predefined templates; 
2) Some pages contain consistently format-
ted texts in two languages but they are not 
translation pairs; 
3) Not all translations in a collective bilin-
gual page necessarily follow an exactly 
consistent format. As shown in Figure 1, 
the ten translation pairs are supposed to 
follow the same pattern, however, due to 
typos, the pattern of the first pair is 
slightly different. 
Because of these difficulties, simply using a 
classifier to extract translation pairs from adja-
cent bilingual texts in a collective bilingual page 
may not achieve satisfactory results. Therefore in 
this paper, we propose a pattern-based approach: 
learning patterns adaptively from collective bi-
lingual pages instead of using the parenthesis 
pattern, then using the learned patterns to extract 
translation pairs from corresponding web pages. 
Specifically, our approach contains four steps: 
1) Preprocessing: parse the web page into a 
DOM tree and segment the inner text of 
each node into snippets; 
2) Seed mining: identify potential translation 
pairs (seeds) using an alignment model 
which takes both translation and translite-
ration into consideration; 
3) Pattern learning: learn generalized pat-
terns with the identified seeds; 
4) Pattern based mining: extract all bilingual 
data in the page using the learnt patterns.  
Let us take mining bilingual data from the text 
shown in Figure 1 as an example. Our method 
identifies ?Boxer??? and ?Eskimo Dog??
???? as two potential translation pairs based 
on a dictionary and a transliteration model (Step 
2 above). Then we learn a generalized pattern 
that both pairs follow as ?{BulletNumb-
er}{Punctuation}{English term}{Chinese 
term}{EndOfLine}?, (Step 3 above). Finally, we 
apply it to match in the entire text and get al 
translation pairs following the pattern (Step 4 
above). 
The remainder of this paper is organized as 
follows. In Section 2, we list some related work. 
The overview of our mining approach is pre-
sented in Section 3. In Section 4, we give de-
871
tailed introduction to each of the four modules in 
our mining approach. The experimental results 
are reported in Section 5 followed by our conclu-
sion and some future work in Section 6. 
Please note that in this paper we describe our 
method using example bilingual web pages in 
English and Chinese, however, the method can 
be applied to extract bilingual data from web 
pages written in any other pair of languages, 
such as Japanese and English, Korean and Eng-
lish etc. 
2 Related Work 
Mining Bilingual Data from the Web 
As far as we know, there is no publication avail-
able on mining parallel sentences directly from 
bilingual web pages. Most existing methods of 
mining bilingual sentences from the Web, such 
as (Nie et al, 1999; Resnik and Smith, 2003; Shi 
et al, 2006), mine parallel web documents within 
bilingual web sites first and then extract bilingual 
sentences from mined parallel documents using 
sentence alignment methods. However, since the 
number of bilingual web sites is quite small, 
these methods can not yield a large number of 
bilingual sentences. (Shi et al, 2006), mined a 
total of 1,069,423 pairs of English-Chinese paral-
lel sentences. In addition to mining from parallel 
documents, (Munteanu and Marcu, 2005) pro-
posed a method for discovering bilingual sen-
tences in comparable corpora. 
As to the term translation extraction from bi-
lingual web pages, (Cao et al, 2007) and (Lin et 
al., 2008) proposed two different methods utiliz-
ing the parenthesis pattern. The primary insight 
is that authors of many bilingual web pages, es-
pecially those whose primary language is Chi-
nese, Japanese or Korean sometimes annotate 
terms with their English translations inside a pair 
of parentheses. Their methods are tested on a 
large set of web pages and achieve promising 
results. However, since not all translations in 
bilingual web pages follow the parenthesis pat-
tern, these methods may miss a lot of translations 
appearing on the Web.  
Apart from mining term translations directly 
from bilingual web pages, more approaches have 
been proposed to mine term translations from 
text snippets returned by a web search engine 
(Jiang et al, 2007; Zhang and Vines, 2004; 
Cheng et al, 2004; Huang et al, 2005). In their 
methods the source language term is usually giv-
en and the goal is to find the target language 
translations from the Web. To obtain web pages 
containing the target translations, they submit the 
source term to the web search engine and collect 
returned snippets. Various techniques have been 
proposed to extract the target translations from 
the snippets. Though these methods achieve high 
accuracy, they are not suitable for compiling a 
large-scale bilingual dictionary for the following 
reasons: 1) they need a list of predefined source 
terms which is not easy to obtain; 2) the relev-
ance ranking in web search engines is almost 
entirely orthogonal to the intent of finding the 
bilingual web pages containing the target transla-
tion, so many desired bilingual web pages may 
never be returned; 3) most such methods rely 
heavily on the frequency of the target translation 
in the collected snippets which makes mining 
low-frequency translations difficult. 
Moreover, based on the assumption that anc-
hor texts in different languages referring to the 
same web page are possibly translations of each 
other, (Lu et al, 2004) propose a novel approach 
to construct a multilingual lexicon by making use 
of web anchor texts and their linking structure. 
However, since only famous web pages may 
have inner links from other pages in multiple 
languages, the number of translations that can be 
obtained with this method is limited. 
Pattern-based Relation Extraction 
Pattern-based relation extraction has also been 
studied for years. For instance, (Hearst, 1992; 
Finkelstein-Landau and Morin, 1999) proposed 
an iterative pattern learning method for extract-
ing semantic relationships between terms. (Brin, 
1998) proposed a method called DIPRE (Dual 
Iterative Pattern Relation Expansion) to extract a 
relation of books (author, title) pairs from the 
Web. Since translation can be regarded as a kind 
of relation, those ideas can be leveraged for ex-
tracting translation pairs. 
3 Overview of the Proposed Approach 
Web 
pages
Seed mining
Pattern-based 
mining
Pattern 
learning
Preprocessing
Bilingual 
dictionary
input
output
depend
Translation 
pairs
Transliteration 
model depend
 
Figure 3. The framework of our approach 
872
As illustrated in Figure 3, our mining system 
consists of four main steps: preprocessing, seed 
mining, pattern learning and pattern based min-
ing. The input is a set of web documents and the 
output is mined bilingual data. 
In the preprocessing step, the input web doc-
uments are parsed into DOM trees and the inner 
text of each tree node is segment into snippets. 
Then we select those tree nodes whose inner 
texts are likely to contain translation pairs collec-
tively with a simple rule. 
The seed mining module receives the inner 
text of each selected tree node and uses a word-
based alignment model to identify potential 
translation pairs. The alignment model can han-
dle both translation and transliteration in a uni-
fied framework.  
The pattern learning module receives identi-
fied potential translation pairs from the seed min-
ing as input, and then extracts generalized pattern 
candidates with the PAT tree algorithm. Then a 
SVM classifier is trained to select good patterns 
from all extracted pattern candidates. 
In the pattern-based mining step, the selected 
patterns were used to match within the whole 
inner text to extract all translation pairs follow-
ing the patterns. 
4 Adaptive Pattern-based Bilingual Da-
ta Mining 
In this section, we will present the details about 
the four steps in the proposed approach. 
4.1 Preprocessing 
HTML Page Parsing 
The Document Object Model (DOM) is an appli-
cation programming interface used for parsing 
HTML documents. With DOM, an HTML doc-
ument is parsed into a tree structure, where each 
node belongs to some predefined types (e.g. DIV, 
TABLE, TEXT, COMMENT, etc.). We removed 
nodes with types of ?B?, ?FONT?, ?I? and so on, 
because they are mainly used for controlling vis-
ual effect. After removal, their child nodes will 
be directly connected to their parents. 
Text Segmentation 
After an HTML document is parsed, the inner 
text of each node in the DOM tree will be seg-
mented into a list of text snippets according to 
their languages. That means each snippet will be 
labeled as either an English snippet (E) or a Chi-
nese snippet (C).  
The text segmentation was performed based 
on the Unicode values of characters 4  first and 
then guided by the following rules to decide the 
boundary of a snippet under some special situa-
tions: 
1) Open punctuations (such as ?(?) are pad-
ded into next snippet, and close punctua-
tions (such as ?)?) are padded into pre-
vious snippet; other punctuations (such as 
?;?) are padded into previous snippet; 
2) English snippets which contains only 1 or 
2 ASCII letters are merged with previous 
and next Chinese snippets (if exist). Since 
sometimes Chinese sentences or terms al-
so contain some abbreviations in English. 
Table 1 gives some examples of how the inner 
texts are segmented. 
Inner text 
China Development Bank (??) ?
????? 
Segmentation 
China Development Bank |(?? ) 
?????? 
Inner text Windows XP ?????? XP? 
Segmentation Windows XP |?????? XP? 
Table 1. Example segmentations (?|? indicates the 
separator between adjacent snippets) 
Since a node?s inner text includes all inner 
texts of its children, the segmentation to all texts 
of a DOM tree has to be performed from the leaf 
nodes up to the root in order to avoid repetitive 
work. When segmenting a node?s inner text, we 
first segment the texts immediately dominated by 
this node and then combine those results with its 
children?s segmented inner texts in sequence. 
As a result of the segmentation, the inner text 
of every node will look like ??ECECC 5EC??. 
Two adjacent snippets in different languages (in-
dicated as ?EC? or ?CE?) are considered a Bilin-
gual Snippet Pair (BSP). 
Collective Nodes Selection 
Since our goal is to mine bilingual knowledge 
from collective bilingual pages, we have to de-
cide if a page is really a collective bilingual page. 
In this paper, the criterion is that a collective 
page must contain at least one Collective Node 
which is defined as a node whose inner text con-
tains no fewer than 10 non-overlapping bilingual 
snippet pairs and which contains less than 10 
                                                 
4 For languages with the same character zone, other tech-
niques are needed to segment the text. 
5 Adjacent snippets in the same language only appear in the 
inner texts of some non-leaf nodes. 
873
percent of other snippets which do not belong to 
any bilingual snippet pairs. 
4.2 Seed Mining 
The input of this module is a collective node 
whose inner text has been segmented into conti-
nuous text snippets, such 
as ?EkChEk+1Ch+1Ch+2?. In this step, every ad-
jacent snippet pair in different languages will be 
checked by an alignment model to see if it is a 
potential translation pair. The alignment model 
combines a translation and a transliteration mod-
el to compute the likelihood of a bilingual snip-
pet pair being a translation pair. If it is, we call 
the snippet pair as a Translation Snippet Pair 
(TSP). If both of two adjacent pairs, e.g. EkCh 
and ChEk+1, are considered as TSPs, the one with 
lower translation score will be regarded as a 
NON-TSP. 
Before computing the likelihood of a bilingual 
snippet pair being a TSP, we preprocess it via the 
following steps: 
a) Isolating the English and Chinese con-
tents from their contexts in the bilingual 
snippet pair. Here, we use a very simple 
rule: in the English snippet, we regard all 
characters within (and including) the first 
and the last English letter in the snippet as 
the English content; similarly, in the Chi-
nese snippet we regard all characters 
within (and including) the first and the 
last Chinese character in the snippet as 
the Chinese content; 
b) Word segmentation of the Chinese con-
tent. Here, the Forward Maximum Match-
ing algorithm (Chen and Liu, 1992) based 
on a dictionary is adopted; 
c) Stop words filtering. We compiled a 
small list of stop words manually (for ex-
ample, ?of?, ?to?, ???, etc.) and remove 
them from the English and Chinese con-
tent; 
d) Stemming of the English content. We use 
an in-house stemming tool to get the un-
inflected form of all English words. 
After preprocessing, all English words form a 
collection E={e1,e2,?,em } and all Chinese 
words constitute a collection C={c1,c2,?,cn}, 
where ei is an English word, and ci is a Chinese 
word. We then use a linking algorithm which 
takes both translation and transliteration into 
consideration to link words across the two col-
lections. 
In our linking algorithm, there are three situa-
tions in which two words will be linked. The first 
is that the two words are considered translations 
of each other by the translation dictionary. The 
second is that the pronunciation similarity of the 
two words is above a certain threshold so that 
one can be considered the transliteration of the 
other. The third is that the two words are identic-
al (this rule is especially designed for linking 
numbers or English abbreviations in Chinese 
snippets). The dictionary is an in-house dictio-
nary and the transliteration model is adapted 
from (Jiang et al, 2007).  
After the linking, a translation score over the 
English and Chinese content is computed by cal-
culating the percentage of words which can be 
linked in the two collections. For some pairs, 
there are many conflicting links, for example, 
some words have multiple senses in the dictio-
nary. Then we select the one with highest trans-
lation score.  
For example, given the bilingual snippet pair 
of ?Little Smoky River? and ???????, its 
English part is separated as ?Little/Smoky/River?, 
and its Chinese part is separated as ??/?/?/?/
??. According to the dictionary, ?Little? can be 
linked with ???, and ?River? can be linked with 
???. However, ?Smoky? is translated as ???
?? in the dictionary which does not match any 
Chinese characters in the Chinese snippet. How-
ever the transliteration score (pronunciation simi-
larity) between ?Smoky? (IPA: s.m.o.k.i) and 
??/?/?? (Pinyin: si mo ji) is higher than the 
threshold, so the English word ?Smoky? can be 
linked to three Chinese characters ???, ??? and 
???. The result is a translation score of 1.0 for 
the pair ?Little Smoky River? and ???????. 
4.3 Pattern Learning 
The pattern learning module is critical for mining 
bilingual data from collective pages, because 
many translation pairs whose translation scores 
are not high enough may still be extracted by 
pattern based mining methods. 
In previous modules, the inner texts of all 
nodes are segmented into continuous text snip-
pets, and translation snippet pairs (TSP) are iden-
tified in all bilingual snippet pairs. Next, in the 
pattern learning module, those translation snippet 
pairs are used to find candidate patterns and then 
a SVM classifier is built to select the most useful 
patterns shared by most translation pairs in the 
whole text. 
874
Candidate Pattern Extraction 
First, as in the seed mining module, we isolate 
the English and Chinese contents from their con-
texts in a TSP and then replace the contents with 
two placeholders ?[E]? and ?[C]? respectively. 
Second, we merge the two snippets of a TSP 
into a string and add a starting tag ?[#]? and an 
ending tag ?[#]? to its start and end. Following 
(Chang and Lui, 2001), all processed strings are 
used to build a PAT tree, and we then extract all 
substrings containing ?E? and ?C? as pattern 
candidates from the PAT tree. However, pattern 
candidates which start or end with ?[E]? (or 
?[C]?) will be removed, since they cannot speci-
fy unambiguous boundaries when being matched 
in a string.  
Web page authors commonly commit format-
ting errors when authoring the content into an 
html page, as shown in Figure 1. There, the ten 
bilingual terms should have been written in the 
same pattern, however, because of the mistaken 
use of ?.? instead of ???, the first translation 
pair follows a slightly different pattern. Some 
other typical errors may include varying length 
or types of white space, adjacent punctuation 
marks instead of one punctuation mark, and so 
on. To make the patterns robust enough to handle 
such variation, we generalized all pattern candi-
dates through the following two steps: 
1) Replace characters in a pattern with their 
classes. We define three classes of cha-
racters: Punctuation (P), Number (N), and 
White Space (S). Table 2 lists the three 
classes and the corresponding regular ex-
pressions in Microsoft .Net Framework6. 
2) Merge identical adjacent classes. 
Class Corresponding regular expression 
P [\p{P}] 
N [\d] 
S [\s] 
Table 2. Character classes 
For example, from the translation snippet pair 
of ?7. Don?t worry.? and ??????, we will 
learn the following pattern candidates: 
? ?#[N][P][S][E][P][S][C][P]#?; 
?  ?[N][P][S][E][P][S][C][P]#?; 
?  ?[N][P][S][E][P][S][C][P]?; 
? ? 
?  ?[S][E][P][S][C][P]?; 
                                                 
6 In System.Text.RegularExpressions namespace 
Pattern Selection 
After all pattern candidates are extracted, a SVM 
classifier is used to select the good ones: 
??? xwxfw ???? ,)(  
where, x?  is the feature vector of a pattern 
candidate pi, and w?  is the vector of weights. 
????,  stands for an inner product. f is the decision 
function to decide which candidates are good. 
In this SVM model, each pattern candidate pi 
has the following four features: 
1) Generality: the percentage of those bi-
lingual snippet pairs which can match pi 
in all bilingual snippet pairs. This feature 
measures if the pattern is a common pat-
tern shared by many bilingual snippet 
pairs; 
2) Average translation score: the average 
translation score of all bilingual snippet 
pairs which can match pi. This feature 
helps decide if those pairs sharing the 
same pattern are really translations; 
3) Length: the length of pi. In general, long-
er patterns are more specific and can pro-
duce more accurate translations, however, 
they are likely to produce fewer matches; 
4) Irregularity: the standard deviation of 
the numbers of noisy snippets. Here noisy 
snippets mean those snippets between any 
two adjacent translation pairs which can 
match pi. If the irregularity of a pattern is 
low, we can be confident that pairs shar-
ing this pattern have a reliably similar in-
ner relationship with each other. 
To estimate the weight vector, we extracted all 
pattern candidates from 300 bilingual web pages 
and asked 2 human annotators to label each of 
the candidates as positive or negative. The anno-
tation took each of them about 20 hours. Then 
with the labeled training examples, we use SVM 
light7 to estimate the weights. 
4.4 Pattern-based Mining 
After good patterns are selected, every two adja-
cent snippets in different languages in the inner 
text will be merged as a target string. As we 
mentioned previously, we add a starting tag ?[#]? 
and an ending tag ?[#]? to the start and end of 
every target string. Then we attempt to match 
each of the selected patterns in each of the target 
strings and extract translation pairs. If the target 
                                                 
7 http://svmlight.joachims.org/ 
875
string was matched with more than one pattern, 
the matched string with highest translation score 
will be kept. 
The matching process is actually quite simple, 
since we transform the learnt patterns into stan-
dard regular expressions and then make use of 
existing regular expression matching tools (e.g., 
Microsoft .Net Framework) to extract translation 
pairs. 
However, to make our patterns more robust, 
when transforming the selected patterns into 
standard regular expressions, we allow each cha-
racter class to match more than once. That means 
?[N]?, ?[P]? and ?[S]? will be transformed into 
?[\d]+?, ?[\p{P}]+? and ?[\s]+? respectively. And 
?[E]? and ?[C]? will be transformed into 
?[^\u4e00-\u9fa5]+? (any character except Chi-
nese character) and ?.+?, respectively. 
5 Experimental Results 
In the following subsections, first, we will report 
the results of our bilingual data mining on a large 
set of Chinese web pages and compare them with 
previous work. Second, we will report some ex-
perimental results on a manually constructed test 
data set to analyze the impact of each part of our 
method. 
5.1 Evaluation on a Large Set of Pages  
With the proposed method, we performed bilin-
gual data extraction on about 3.5 billion web 
pages crawled from Chinese web sites. Out of 
them, about 20 million were determined to con-
tain bilingual collective nodes. From the inner 
texts of those nodes, we extracted 12,610,626 
unique translation pairs. If we consider those 
pairs whose English parts contain more than 5 
words as sentence translations and all others as 
term translations, we get 7,522,803 sentence 
translations and 5,087,823 term translations. We 
evaluated the quality of these mined translations 
by sampling 200 sentence translations and 200 
term translations and presenting those to human 
judges, with a resulting precision of 83.5% for 
sentence translations and 80.5% for term transla-
tions. 
As we mentioned in Section 2, (Shi et al, 
2006) reported that in total they mined 1,069,423 
pairs of English-Chinese parallel sentences from 
bilingual web sites. However, our method yields 
about 7.5 million pairs, about seven times as 
many.  
We also re-implemented the extraction method 
using the parenthesis pattern proposed by (Lin et 
al., 2008) and were able to mine 6,538,164 bilin-
gual terms from the same web pages. A sample 
of 200 terms was submitted for human judgment, 
resulting in a precision of 78.5% which is a little 
lower than that of our original result. Further 
analysis showed that fewer than 20% of the bi-
lingual terms mined with our method overlap 
with the data mined using the re-implemented 
method proposed by (Lin et al, 2008). This indi-
cates that our method can find many translations 
which are not covered by the parenthesis pattern 
and therefore can be used together with the pa-
renthesis pattern based method to build a bilin-
gual lexicon. 
Out of the term translations we mined, we 
found many which co-occur with their source 
terms only once in the Web. We check this by 
searching in Google with a Boolean query made 
of the term and its translation and then get the 
number of pages containing the query. If one 
attempts to extract this kind of low-frequency 
translation using a search engine-based method, 
the desired bilingual page which contains the 
target translation is not likely to be returned in 
the top n results when searching with the source 
term as the query. Even if the desired page is 
returned, the translation itself may be difficult to 
extract due to its low frequency. 
5.2 Evaluation on a Human Made Test Da-
ta Set 
Besides the evaluation of our method on a huge 
set of web pages, we also carried out some expe-
riments on a human-constructed test data set. We 
randomly selected 500 collective nodes from the 
huge set of Chinese web pages and asked two 
annotators to label all bilingual data in their inner 
texts. Half of the labeled data are then used as 
the development data set and the rest as the test 
data set to evaluate our systems with different 
settings. Table 3 shows the evaluation results. 
Setting Type Recall Precision F-Score 
Without 
pattern 
Exact 52.2 75.4 61.7 
Fuzzy 56.3 79.3 65.8 
Without 
PG 
Exact 69.2 78.6 73.6 
Fuzzy 74.3 82.9 78.4 
With PG 
Exact 79.3 80.5 79.9 
Fuzzy 86.7 87.9 87.3 
Table 3. Performance of different settings 
In Table 3, ?Without pattern? means that we 
simply treat those seed pairs found by the align-
ment model as final bilingual data. ?Without PG? 
and ?With PG? mean not generalizing and gene-
ralizing the learnt patterns to class based form, 
876
respectively. Evaluation type ?Exact? means the 
mined bilingual data are considered correct only 
if they are exactly same as the data labeled by 
human, while ?Fuzzy? means the mined bilin-
gual data are considered correct if they contain 
the data labeled by the human. 
As shown in Table 3, the system without pat-
tern-based extraction yields only 52.2% recall. 
However, after adding pattern-based extraction, 
recall is improved sharply, to 69.2% for ?With-
out PG? and to 79.3% for ?With PG?. Most of 
the improvement comes from those translations 
which have very low translation scores and 
therefore are discarded by the seed mining mod-
ule, however, most of them are found with the 
help of the learnt patterns. 
From Table 3, we can also see that the system 
?With PG? outperforms ?Without PG? in terms 
of both precision and recall. The reason may be 
that web writers often make mistakes when writ-
ing on web pages, such as punctuation misuse, 
punctuation loss, and extra spaces etc., so ex-
tracting with a strict surface pattern will often 
miss those translations which follow slightly dif-
ferent patterns.  
To find out the reasons why some non-
translation pairs are extracted, we checked 20 
pairs which are not translations but extracted by 
the system. Out of them, 5 are caused by wrong 
segmentations. For example, ????????
????? Double Concerto for Violin and 
Cello D??????? Symphony No.2 in D 
Major? is segmented into ??????????
????, ?Double Concerto for Violin and Cello 
D?, ?????????, and ?Symphony No.2 in 
D Major?. However, the ending letter ?D? of the 
second segment should have been padded into 
the third segment. For 9 pairs, the Chinese parts 
are explanative texts of corresponding English 
texts, but not translations. Because they contain 
the translations of the key words in the English 
text, our seed mining module failed to identify 
them as non-translation pairs. For 3 pairs, they 
follow the same pattern with some genuine trans-
lation pairs and therefore were extracted by the 
pattern based mining module. However, they are 
not translation pairs. For the other 3 pairs, the 
errors came from the pattern generalization. 
To evaluate the contribution of each feature 
used in the pattern selection module, we elimi-
nated one feature at a time in turn from the fea-
ture set to see how the performance changed in 
the absence of any single feature. The results are 
reported below. 
Eliminated feature F-Score (Exact) 
Null 79.9 
Generality 72.3 
Avg. translation score 74.3 
Length 77.5 
Irregularity 76.6 
Table 4. Contribution of every feature 
From the table above, we can see that every 
feature contributes to the final performance and 
that Generality is the most useful feature among 
all four features. 
6 Conclusions  
Bilingual web pages have shown great potential 
as a source of up-to-date bilingual 
terms/sentences which cover many domains and 
application types. Based on the observation that 
many web pages contain bilingual data collec-
tions which follow a mostly consistent but possi-
bly somewhat variable pattern, we propose a uni-
fied approach for mining bilingual sentences and 
terms from such pages. Our approach can adap-
tively learn translation patterns according to dif-
ferent formatting styles in various web pages and 
then use the learnt patterns to extract more bilin-
gual data. The patterns are generalized to minim-
ize the impact of format variation and typos. Ac-
cording to experimental results on a large set of 
web pages as well as on a manually made test 
data set, our method is quite promising. 
In the future, we would like to integrate the 
text segmentation module with the seed mining 
and pattern learning module to improve the accu-
racy of text segmentation. We also want to eva-
luate the usefulness of our mined data for ma-
chine translation or other applications. 
 
 
References  
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra and 
R. L. Mercer. 1993. The mathematics of statistical 
machine translation: parameter estimation. Compu-
tational Linguistics, 19:2, 263-311. 
Sergey Brin. 1998. Extracting patterns and relations 
from the World Wide Web. In Proc. of the 1998 In-
ternational Workshop on the Web and Databases. 
Pp: 172-183. 
G.H. Cao, J.F. Gao and J.Y. Nie. 2007. A system to 
mine large-scale bilingual dictionaries from mono-
lingual web pages. MT summit. Pp: 57-64. 
877
Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: 
Inform extract based on pattern discovery. In Proc. 
of the 10th ACM WWW conference.  
Keh-Jiann Chen, Shing-Huan Liu. 1992. Word Identi-
fication for Mandarin Chinese Sentences. In the 
Proceedings of COLING 1992. Pp:101-107. 
Cheng, P., Teng, J., Chen, R., Wang, J., Lu, W., and 
Cheng, L. 2004. Translating Unknown Queries 
with Web Corpora for Cross-Language Information 
Retrieval. In the Proceedings of SIGIR 2004, pp 
162-169.  
Michal Finkelstein-Landau, Emmanuel Morin. 1999. 
Extracting Semantic Relationships between Terms: 
Supervised vs. Unsupervised Methods. In Proceed-
ings of International Workshop on Ontological En-
gineering on the Global Information Infrastructure. 
Pp:71-80. 
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In the Proceed-
ings of COLING-92. Pp: 539-545. 
Huang, F., Zhang, Y., and Vogel, S. 2005. Mining 
Key phrase Translations from Web Corpora. In the 
Proceedings of HLT-EMNLP. 
L. Jiang, M. Zhou, L.-F. Chien, C. Niu. 2007. Named 
Entity Translation with Web Mining and Translite-
ration, Proceedings of the 20th IJCAI. Pp: 1629-
1634. 
D. Lin, S. Zhao, B. Durme and M. Pasca. 2008. Min-
ing Parenthetical Translations from the Web by 
Word Alignment. In ACL-08. pp 994-1002. 
Lu, W. and Lee, H. 2004. Anchor text mining for 
translation of Web queries: A transitive translation 
approach. ACM transactions on Information Sys-
tems, Vol.22, April 2004, pages 242-269.  
D. S. Munteanu, D. Marcu. Improving Machine 
Translation Performance by Exploiting Non-
Parallel Corpora. 2005. Computational Linguistics. 
31(4). Pp: 477-504. 
J-Y Nie, M. Simard, P. Isabelle, and R. Durand. 1999. 
Cross-Language Information Retrieval Based on 
Parallel Texts and Automatic Mining of parallel 
Text from the Web. In SIGIR 1999. Pp: 74-81. 
Philip Resnik, Noah A. Smith. 2003. The Web as a 
Parallel Corpus. Computational Linguistics. 29(3). 
Pp: 349-380. 
Li Shao and Hwee Tou Ng. 2004. Mining new word 
translations from comparable corpora. In Proc. of 
COLING 2004. Pp: 618?624. 
Lei Shi, Cheng Niu, Ming Zhou, Jianfeng Gao. 2006. 
A DOM Tree Alignment Model for Mining Paral-
lel Data from the Web. In ACL 2006. 
Jung H. Shin, Young S. Han and Key-Sun Choi. 1996. 
Bilingual knowledge acquisition from Korean-
English parallel corpus using alignment method: 
Korean-English alignment at word and phrase level. 
In Proceedings of the 16th conference on Computa-
tional linguistics, Copenhagen, Denmark. 
J.C. Wu, T. Lin and J.S. Chang. 2005. Learning 
Source-Target Surface Patterns for Web-based 
Terminology Translation. ACL Interactive Poster 
and Demonstration Sessions,. Pp 37-40, Ann Arbor. 
Zhang, Y. and Vines, P.. 2004. Using the Web for 
Automated Translation Extraction in Cross-
Language Information Retrieval. In the Proceed-
ings of SIGIR 2004. Pp: 162-169. 
878
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 295?303,
Beijing, August 2010
An Empirical Study on Learning to Rank of Tweets 
 
1Yajuan Duan* 2Long Jiang 2Tao Qin 2Ming Zhou 2Heung-Yeung Shum 
1Department of Computer Science and Technology 
University of Science and Technology of China 
2Microsoft Research Asia 
{v-yaduan,longj,taoqin,mingzhou,hshum}@microsoft.com 
 
Abstract 
Twitter, as one of the most popular 
micro-blogging services, provides large 
quantities of fresh information including 
real-time news, comments, conversation, 
pointless babble and advertisements. 
Twitter presents tweets in chronological 
order. Recently, Twitter introduced a 
new ranking strategy that considers 
popularity of tweets in terms of number 
of retweets.  This ranking method, 
however, has not taken into account 
content relevance or the twitter account. 
Therefore a large amount of pointless 
tweets inevitably flood the relevant 
tweets. This paper proposes a new 
ranking strategy which uses not only the 
content relevance of a tweet, but also the 
account authority and tweet-specific 
features such as whether a URL link is 
included in the tweet. We employ 
learning to rank algorithms to determine 
the best set of features with a series of 
experiments. It is demonstrated that 
whether a tweet contains URL or not, 
length of tweet and account authority are 
the best conjunction.1 
1 Introduction 
Twitter provides a platform to allow users to 
post text messages known as tweets to update 
their followers with their findings, thinking and 
comments on some topics (Java et al, 2007). 
                                                             
*
 The work was done when the first author was intern at 
Microsoft Research Asia 
The searched tweets are presented by Twitter in 
chronological order except the first three, which 
are ranked by considering popularity of tweets in 
terms of the number of retweets.   
This ranking method, however, has not taken 
into account the content relevance and twitter 
account; inevitably, a large amount of pointless 
tweets (Pear Analytics, 2009) may flood the 
relevant tweets. Although this ranking method 
can provide fresh information to tweet users, 
users frequently expect to search relevant tweets 
to the search queries. For example, consider 
someone researching consumer responses 
toward the iPad. He or she would like to find 
tweets with appropriate comments such as iPad 
is great or you can find many useful features of 
iPad, rather than tweets with irrelevant comment, 
even if they are most recent or popular. 
Moreover, neither Twitter?s current 
chronological order based ranking nor the 
recently introduced popularity based ranking can 
avoid spam. A developer can accumulate 
hundreds of thousands of followers in a day or 
so. At the same time, it is not difficult for 
spammers to create large quantities of retweets. 
By contrast, content relevance ranking can 
effectively prevent spammers from cheating. 
Different from ranking tweets through 
chronological order and popularity, a content 
relevance strategy considers many 
characteristics of a tweet to determine its 
ranking level. Thus it is difficult for spammers 
to break the ranking system by simple methods 
such as increasing retweet count or number of 
followers. 
In this paper, we propose a method to rank the 
tweets which outputs the matched tweets based 
on their content relevance to the query. We 
295
investigate the effects of content features and 
non-content features and produce a ranking 
system by a learning to rank approach.  
With a series of experiments, we determined 
the best set of features and analyzed the effects 
of each of individual feature. We provide 
empirical evidence supporting the following 
claims, 
? Account authority, length of tweet and 
whether a tweet contains a URL are the top 
three effective features for tweet ranking, 
where containing a URL is the most 
effective feature. 
? We find an effective representation of 
account authority: the number of times the 
author was listed by other users. We find 
through experiments that this representation 
is better than the widely adopted number of 
followers. 
2 Related Work 
2.1 Real-time Search 
At present, a number of web sites offer the 
so-called real-time search service which mainly 
returns real-time posts or shared links, videos 
and images obtained from micro-blogging 
systems or other medium according to the user?s 
query. We investigate the ranking method used 
by these web sites. From their self-introduction 
page, we find four main criteria for ranking 
real-time posts. They are posting time, account 
authority, topic popularity and content 
relevance. 
Specifically, Twitter maintains a specialized 
search engine which ranks tweets according to 
posting time and topic popularity. In addition, 
Google, Twazzup2 and Chirrps3 rank real-time 
tweets by posting time. While the last one also 
ranks tweets by popularity, which is measured 
by retweet count.  
Tweefind4 ranks search result according to 
authority of authors which depends on how 
popular, relevant, and active the author is. 
Additionally, Twitority5 rank tweets by author 
authority as well.  
                                                             
2 Twazzup: http://www.twazzup.com/ 
3 Chirrps: http://chirrps.com/ 
4 Tweefind: http://www.tweefind.com/ 
5 Twitority: http://www.twitority.com/ 
Bing and CrowdEye6 rank tweets by posting 
time or content relevance. Bing takes authors 
authority, retweet count and freshness into 
consideration while measuring the relevance. To 
determine the relevance of a tweet, CrowdEye 
considers a number of factors including content 
relevance and author influence which appears to 
rely heavily on the number of followers an 
author has. It turns out that the number of 
followers is not a very reasonable measure of the 
influence of an account according to our 
experimental results. 
2.2 Twitter Recommendation 
Besides tweet search, recently some researchers 
have focused on twitter recommendation system. 
Chen et al (2010) presented an approach to 
recommend URLs on Twitter as a means to 
better direct user attention in information 
streams. They designed the recommender taking 
three separate dimensions into consideration: 
content source, topic interest and social voting.  
Sun et al (2009) proposed a diffusion-based 
micro-blogging recommendation framework 
aiming to recommend micro-blogs during 
critical events via optimizing story coverage, 
reading effort and delay time of a story. The key 
point of this method is to construct an exact 
diffusion graph for micro-blogging, which is 
difficult due to the presence of extensive 
irrelevant personal messages and spam. 
2.3 Blog Search and Forum Search 
Another related topic is blog search and forum 
search. Recently, many approaches for blog 
search and forum search have been developed, 
which include learning to rank methods and 
link-based method.  
Learning to rank approach 
Xi et al (2004) used features from the thread 
trees of forums, authors, and lexical distribution 
within a message thread and then applied Linear 
Regression and Support Vector Machine (SVM) 
to train the ranking function. Fujimura et al 
(2005) exploited provisioning link and 
evaluation link between bloggers and blog 
entries, and scored each blog entry by weighting 
the hub and authority scores of the bloggers.  
Link-Based approach 
                                                             
6 CrowdEye: http://www.crowdeye.com/ 
296
Kritikopoulos et al (2006) introduced 
similarities among bloggers and blogs into blog 
ranking. This method enabled the assignment of 
a higher score to the blog entry published by a 
blogger who has already accepted a lot of 
attention. Xu and Ma (2006) built a topic 
hierarchy structure through content similarity. 
Liu et al (2007) presented a newsgroup 
structure-based approach PostRank which built 
posting trees according to response relationship 
between postings.  
Chen et al (2008) proposed a posting rank 
algorithm which built link graphs according to 
co-replier relationships. This kind of method 
exploits different types of structures among 
postings and improved the performance of 
traditional link-based ranking algorithm for 
forum search. However, it is difficult to rank 
postings which only have a few words simply 
based on content by using FGRank algorithm. 
And PostingRank approach relies too much on 
reply relations which are more likely to suffer 
from topic excursion. 
Although approaches proposed above perform 
effectively in forum search and blog search, they 
are not appropriate for twitter search because 
tweets are usually shorter and more informal 
than blogs. Furthermore, it does not have the 
explicit hierarchy structure of newsgroup 
messages on forums. In addition, tweets possess 
many particular characteristics that blog and 
forum do not have. 
3 Overview of Our Approach 
To generate a good ranking function which 
provides relevant search results and prevents 
spammers? cheating activities, we analyze both 
content features and authority features of tweets 
and determine effective features. We adopt 
learning to rank algorithms which have 
demonstrated excellent power in addressing 
various ranking problems of search engines. 
3.1 Learning to Rank Framework 
Learning to Rank is a data-driven approach 
which integrates a bag of features in the model 
effectively. Figure 1 shows the paradigm of 
learning for tweet ranking. 
At the first step, we prepare the training and 
test corpus as described in Section 5. Then we 
extract features from the training corpus. 
RankSVM algorithm (Joachims Thorsten, 1999) is 
used to train a ranking model from the training 
corpus. Finally, the model is evaluated by the 
test corpus. 
 
 
Figure 1. General Paradigm of Learning for 
Tweets Ranking 
 
3.2 Features for Tweets Ranking 
One of the most important tasks of a learning to 
rank system is the selection of a feature set. We 
exploit three types of features for tweet ranking.  
1) Content relevance features refer to those 
features which describe the content 
relevance between queries and tweets. 
2) Twitter specific features refer to those 
features which represent the particular 
characteristics of tweets, such as retweet 
count and URLs shared in tweet. 
3) Account authority features refer to those 
features which represent the influence of 
authors of the tweets in Twitter (Leavitt et al, 
2009).  
In the next section, we will describe these 
three types of features in detail. 
4 Feature Description  
4.1 Content Relevance Features 
We used three content relevance features, Okapi 
BM25 (Robertson et al, 1998), similarity of 
contents and length of tweet. 
Okapi BM25 score measures the content 
relevance between query Q and tweet T. The 
standard BM25 weighting function is: 
 
           
                      
                
         
         
 
    
 (1) 
 
297
where Length(T) denotes the length of T and 
          represents average length of tweet in 
corpus. IDF(  ) is Inverse Document Frequency. 
Similarity of contents estimates the 
popularity of documents in the corpus (Song et 
al., 2008). In our case, it measures how many 
tweets of the query are similar in content with 
the current tweet. We calculate a cosine 
similarity score for every pair of tweets, and the 
final similarity score for tweet     in     is 
computed by the following formula: 
 
               
 
       
 
       
           
          
  (2) 
 
Where     represents the TFIDF vector of    
and     refers to tweets collection of query   . 
Length is measured by the number of words 
that a tweet contains. Intuitively, a long sentence 
is apt to contain more information than a short 
one. We use length of tweet as a measure of the 
information richness of a tweet.  
4.2 Twitter?s Specific Features 
Tweets have many special characteristics. We 
exploit these characteristics and extract six 
twitter specific features as listed in Table 1. 
 
Feature Description 
URL Whether the tweet contains a URL 
URL Count Frequency of URLs in corpus 
Retweet 
Count  
How many times has this tweet been 
retweeted 
Hash tag 
Score 
Sum of frequencies of the top-n hash tags 
appeared in the tweet 
Reply Is the current tweet a reply tweet 
OOV Ratio of words out of vocabulary 
Table 1. Twitter Specific Features 
 
 
Figure 2. A Tweet Example 
 
URL & URL Count: Twitter allows users to 
include URL as a supplement in their tweets. 
The tweet in Figure 2 contains URL 
http://myloc.me/43tPj which leads to a map 
indicating where the publisher located. 
URL is a binary feature. It is assigned 1 when 
a tweet contains at least one URL, otherwise 0. 
URL Count estimates the number of times that 
the URL appears in the tweet corpus. 
Retweet Count: Twitter users can forward a 
tweet to his or her followers with or without 
modification on the forwarded tweets, which is 
called retweet on Twitter. A retweeted tweet 
usually includes an RT tag. Generally, sentences 
before RT are comments of the retweeter and 
sentences after RT are the original content, 
perhaps with some modifications. Here we only 
consider tweets including RT with the original 
content unmodified. Retweet count is defined as 
the number of times a tweet is retweeted. In 
Figure 2, original tweet Satu-slank 
#nowplaying !! http://myloc.me/43tPj is 
retweeted once.  
Hash Tag Score: Publishers are allowed to 
insert hash tags into their tweets to indicate the 
topic. In Figure 2, #nowplaying is a hash tag. We 
collect hash tags appearing in the tweets of every 
query and sort them in descending order 
according to frequency. Tag frequency for tweet 
   of query    is computed from normalized 
frequency of top-n tags. 
 
             
 
  
            
 
              
        
  (3) 
 
Where    is the normalization factor. 
            represents the frequent of      in 
corpus. And       denotes the tag collection 
extracted from    . 
Reply: This is a binary feature. It is 1 when 
the tweet is a reply and 0 otherwise. A tweet 
starting with a twitter account is regarded as a 
reply tweet in our experiment. Figure 3 shows an 
example. 
 
 
Figure 3. Reply Tweet 
 
OOV: This feature is used to roughly 
approximate the language quality of tweets. 
Words out of vocabulary in Twitter include 
spelling errors and named entities. According to 
a small-scale investigation, spelling errors 
account for more than 90% of OOVs excluding 
capitalized words, tags, mentions of users and 
298
URLs. We use a dictionary with 0.5 million 
entries to compute the ratio of OOVs in a tweet. 
 
           
             
         
     (4) 
 
4.3 Account Authority Features 
There are three important relations between 
users in Twitter: follow, retweet, and mention. 
Additionally, users are allowed to classify their 
followings into several lists based on topics. We 
measured the influence of users? authorities on 
tweets based on the following assumptions: 
? Users who have more followers and have 
been mentioned in more tweets, listed in 
more lists and retweeted by more important 
users are thought to be more authoritative. 
? A tweet is more likely to be an informative 
tweet rather than pointless babble if it is 
posted or retweeted by authoritative users. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. PageRank Algorithm for Calculating 
Popularity Score for Users 
 
In order to distinguish the effect of the three 
relations, we computed four scores for each user 
representing the authority independently. 
? Follower Score: number of followers a user 
has. 
? Mention Score: number of times a user is 
referred to in tweets. 
? List Score: number of lists a user appears in. 
? Popularity Score: computed by PageRank 
algorithm (Page et al, 1999) based on 
retweet relations. 
Following the retweet relationship among 
users, we construct a directed graph G (V, E). In 
our experiments, G is built from a tweet 
collection including about 1.1 million tweets. V 
denotes twitter users that appear in training 
examples. E is a set of directed edges. If author 
   published the tweet   , and author    
retweeted    after   , there exists an edge from 
   to   . We call    original author and    
retweeter. Figure 4 shows the PageRank 
algorithm for calculating popularity scores for 
twitter users. In our experiment, damping factor 
e was set to 0.8. Like Dong et al (2010) did, we 
define three subtypes for each account authority 
score. Table 2 presents features of account 
authority we use. 
 
Feature Description 
Sum_follo
wer 
Sum of follower scores of users who 
published or retweeted the tweet 
Sum_popul
arity 
Sum of popularity scores of users who 
published or retweeted the tweet 
Sum_menti
on 
Sum of mention scores of users who 
published or retweeted the tweet 
Sum_list 
Sum of list scores of users who published 
or retweeted the tweet 
First_follo
wer 
Follower score of the user who published 
the tweet 
First_popul
arity 
Popularity score of the user who published 
the tweet 
First_menti
on 
Mention score of the user who published 
the tweet 
First_list 
List score of the user who published the 
tweet 
Important_f
ollower 
The highest follower score of the user who 
published or retweeted the tweet 
Important_
popularity 
The highest popularity score of the user 
who published or retweeted the tweet 
Important_
mention 
The highest mention score of the user who 
published or retweeted the tweet 
Important_l
ist 
The highest list score of the user who 
published or retweeted the tweet 
Table 2. Account Authority Features for tweet 
5 Experiment Data and Evaluation 
We introduce the data we used in experiment 
and the evaluation metrics in this section. 
5.1 Data 
We analyze 140 hot searches on CrowdEye 
within a week. They consist of big events, 
                   
  
               
  
      
 
PageRank algorithm for calculating popularity score 
for users. 
Input: Directed Graph G of retweet relationship 
            Damping factor e. 
Output: popularity score for each user 
Procedure: 
Step 1: popularity score of all users are initialized as 
   . 
Step 2: update the popularity score for users. 
            denotes the collection of users who 
retweeted   ?s tweet. 
             is the number of times    has been 
retweeted by   . 
          is the number of users whose tweets    
has retweeted. 
Step 3: Repeat the second step until all popularity 
scores will never change. 
299
famous persons, new products, festivals, movies 
and so on. The most frequent types of hot 
searches, which account for more than 81% of 
all hot searches, are as follows:  
? News: news about public figures and 
news related to some places. 
? Products: character description, 
promotion information and comments 
about products. 
? Entertainment: mainly about movies, 
including film reviews and introductions 
about plots. 
We select 20 query terms as shown in Table 3, 
including 5 persons, 5 locations, 5 products and 
5 movie names. Specifically, Locations are 
sampled from a list of American cities. Person 
names come from the hot search and hot trends 
provided by Twitter and CrowdEye. Products 
are sampled from the popular searches of 35 
product categories on eBay. And movies are 
selected from a collection of recommended 
movies from 2005 to 2010. We crawl 162,626 
English tweets for the selected queries between 
March 25, 2010 and April 2, 2010 from Twitter 
Search. After removing the repeated ones, 
159,298 tweets remained. 
 
Query type Query terms 
Locations 
New York, Nashville, Denver, 
Raleigh, Lufkin 
Person 
Names 
Obama, Bill Clinton, James 
Cameron, Sandra Bullock, LeBron 
James 
products 
Corvette, iPad, Barbie, Harry Potter, 
Windows 7 
Movies 
The Dark Knight, up in the air, the 
hurt locker, Batman Begins, Wall E 
Table 3. 20 Query Terms 
 
Retweets are forwardings of corresponding 
original tweets, sometimes with comments of 
retweeters. They are supposed to contain no 
more information than the original tweets, 
therefore they drops out of ranking in this paper. 
We sample 500 tweets for each query from its 
original tweets collection and ask a human editor 
to label them with a relevance grade. In order to 
ensure the annotation is reasonable, we set 
multiple search intentions for each query 
referring to the topics arising in the tweets about 
the query in the corpus. Specifically, for 
Locations, tweets describing news related to the 
location are relevant. For people, what they have 
done and the comments about them are regarded 
as relevant information. For products, tweets 
including feature description, promotion and 
comments are considered relevant. And for 
movies, tweets about comment on the movies, 
show time and tickets information are preferred. 
We apply four judgment grades on query-tweet 
pairs: excellent, good, fair and bad. According to 
the statistics, about half of the tweets in the 
experiment data are labeled as bad. Table 4 
presents the distribution for all grades. 
 
Grade Excellent Good Fair Bad 
Percentage  20.9% 10.9% 16.9% 51.3% 
Min 2.4% 1.8% 4.0% 8.0% 
Max 69.8% 23.2% 54.4% 81.0% 
Table 4. Tweet Distribution of Each Grade 
5.2 Evaluation Metrics 
There are several metrics that are often used to 
measure the quality of rankings. In this paper, 
we use Normalized Discount Cumulative Gain 
(NDCG) which can handle multiple levels of 
relevance as the evaluation metrics (Jarvelin and 
Kekalainen, 2002). 
6 Results 
Five-fold cross-validation was used in our 
experiments. We choose tweets of sixteen 
queries (four from each query type) as the 
training data. The remaining tweets are divided 
into evaluation data and validation data equally. 
6.1 Learning to Rank for Tweet Ranking 
We learn a ranking model by using a RankSVM 
algorithm based on all features we extracted, 
which is denoted as RankSVM_Full. In the 
experiment, a toolkit named svmstruct 7 
implemented by Thorsten Joachims is used. 
Figure 5 shows the comparison between our 
method which integrates three types of features 
and ranking through chronological order, 
account authority, and content relevance 
individually. 
In this experiment, Content Relevance is 
measured by BM25 score. And Account 
                                                             
7 SVMstruct: http://svmlight.joachims.org/svm_struct.html 
300
Authority is approximated by the number of 
followers of the user. Figure 5 illustrates that 
ranking through content relevance is not as 
effective as other methods. This is because our 
work is essentially re-ranking on the result of 
Twitter Search. Hence almost all tweets include 
the query term which makes it difficult to 
distinguish them by BM25 score. Figure 5 also 
reveals that account authority is useful for 
ranking tweet relevance; it outperforms ranking 
through chronological order and is competitive 
to our model trained from all features. This 
agrees with the assumption we made about the 
influence of user authorities on tweets. 
 
 
Figure 5. Performance of Four Ranking Methods 
6.2 Feature Selection 
As the RankSVM_Full underperforms against 
some models trained from subsets of features, 
we use an advanced greedy feature selection 
method and find the best feature conjunction to 
improve the performance of RankSVM_full. 
Figure 6 shows the feature selection approach. 
Although greedy feature selection approach is 
commonly used in many problems, it does not 
work efficiently in addressing this problem 
partly for data sparseness. It is always blocked 
by a local optimum feature set. In order to 
resolve this problem, we first generate several 
feature sets randomly and run the greedy 
selection algorithm based the best one among 
them. Finally, we find the best feature 
conjunction composed by URL, Sum_mention, 
First_List, Length, and Important_follower, 
from which a model is learnt denoted as 
RankSVM_Best. Figure 7 illustrates that this 
model outperforms RankSVM_Full by about 
15.3% on NDCG@10. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6. Advanced Greedy Feature Selection 
Algorithm 
 
 
Figure 7. Comparison between RankSVM_Full 
and RankSVM_Best 
 
We conduct a paired t-test between 
RankSVM_Best and each of other four ranking 
methods on NDCG@10 of ten test queries. The 
results demonstrate that RankSVM_Best 
outperforms ranking through time, account 
authority and content relevance respectively 
with a significance level of 0.01, and 
RankSVM_Full with a level of 0.05. 
6.3 Feature Analysis 
We are interested in which features in particular 
are highly valued by our model for tweet ranking. 
We evaluate the importance of each feature by 
the decrement of performance when removing 
the feature measured from RankSVM_Best. 
Figure 8 reveals the importance of each feature 
in our model. 
An advanced greedy feature selection algorithm. 
Input: All features we extracted. 
Output: the best feature conjunction BFC 
Procedure: 
Step1: Randomly generate 80 feature set F. 
Step 2: Evaluate every feature set in F and select 
the best one denoted by RBF. 
Features excluded those in RBF are denoted as 
EX_RBF 
Step 3: t = 0,BFC(t)=RBF; 
  Repeat 
    Foreach feature in EX_RBF 
  If  Evaluation(BFC)  
     < Evaluation(BFC, feature) 
     BFC(t+1) = {BFC(t), feature} 
     EX_RBF(t+1) = EX_RBF(t) ? {feature} 
  While BFC(t+1) ? BFC(t) 
Note: Evaluation(BFC) refers to the performance of 
ranking function trained from features in BFC on 
validation data. 
301
  
Figure 8. Importance of Each Feature 
 
We observe from Figure 8 that URL is very 
important for our model; without it the 
performance declines seriously (with a 
significance level of 0.001). The reason may be 
that URLs shared in tweets, which provide more 
detailed information beyond the tweet?s 140 
characters, may be relevant to the query at a high 
probability.  
Another useful feature is the number of lists 
that the author of the tweet has been listed in. 
The performance of ranking decreases with a 
significance level of 0.05 when removing it from 
the best feature combination. However, other 
features do not show significant contribution. 
7 Discussion 
Our experiment in section 6.2 demonstrates that 
features such as Hash tag Score and Retweet 
Count are not as effective as expected. This may 
be due to the small size of training data. We 
present an approach to learn an effective tweets 
ranker in a small dataset through feature 
selection. However, 20 queries are not sufficient 
to train a powerful ranker for Twitter. 
In this study, to minimize the annotation 
effort, for each test query, we only annotate the 
tweets containing the query (returned by Twitter 
Search) and then used them for evaluation. With 
this kind of evaluation, it is hard to completely 
evaluate the significance of some features, such 
as content relevance features. In the future, we 
will select more queries including both hot 
searches and long tail searches, and select tweets 
for annotation directly from the twitter firehose. 
There is also an opportunity for more accurate 
retweet relation detection in our work. At 
present, we just identify the retweet whose 
original tweet has not been modified, which 
leaves out a fair amount of retweet information. 
We would need to develop a more precise 
retweet relation detection method. 
8 Conclusion 
In this paper, we study three types of tweet 
features and propose a tweet ranking strategy by 
applying learning to rank algorithm. We find a 
set of most effective features for tweet ranking. 
The results of experiments demonstrate that the 
system using Sum_mention, First_list, 
Important_follower, length and URL performs 
best. In particular, whether a tweet contains a 
URL is the most effective feature. Additionally, 
we find in the experiments that the number of 
times the account is listed by other users is an 
effective representation of account authority and 
performs better than the number of followers 
that is widely used in previous work. 
There are many aspects we would like to 
explore in the future. First, this research is based 
on the search results returned from Twitter 
which contains the input query. The tweets not 
containing the queries are not returned. We will 
explore query expansion approaches to improve 
the recall of the search results. We did not 
consider spam issues in the ranking process. 
However, spam filtering is important to all types 
of search engines. We will explore the impacts 
of spam and work out a spam filtering approach. 
References 
Chen Jilin, Rowan Nairn, Les Nelson, Michael 
Bernstein, and Ed H. Chi. 2010. Short and Tweet: 
Experiments on Recommending Content from 
Information Streams. In the Proceedings of the 
28th International conference on Human Factors 
in Computing Systems, Pages: 1185-1194. 
Chen Zhi, Li Zhang, Weihua Wang. 2008. 
PostingRank: Bringing Order to Web Forum 
Postings. In the proceedings of the 4th Asia 
Information Retrieval Symposium, Pages: 377-384. 
Dong Anlei, Ruiqiang Zhang, Pranam Kolari, Jing 
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, 
and Hongyuan Zha. 2010. Time of the essence: 
improving recency ranking using Twitter data. In 
the proceedings of the 19th International 
Conference on World Wide Web, Pages: 331-340. 
302
Fujimura Ko, Takafumi Inoue, and Masayuki 
Sugisaki. 2005. The EigenRumor Algorithm for 
Ranking Blogs. In the proceedings of the 2nd 
Annual Workshop on the Weblogging Ecosystem: 
Aggregation, Analysis and Dynamics, World Wide 
Web. 
Jarvelin Kalervo, and Jaana Kekalainen. 2002. 
Cumulated gain-based evaluation of IR techniques. 
ACM Transactions on Information Systems, 
Volume 20, Pages: 422-446. 
Java Akshay, Xiaodan Song, Tim Finin, and Belle 
Tseng. 2007. Why we twitter: Understanding 
Microblogging Usage and Communities. In the 
proceedings of the 9th International Workshop on 
Knowledge Discovery on the Web and the 1st 
International Workshop on Social Networks 
Analysis. Pages: 118-138. 
Joachims Thorsten. 1999. Making Large-Scale SVM 
Learning Practical. Advances in Kernel Methods: 
Support Vector Learning, Pages: 169-184. 
Pear Analytics. 2009. Twitter Study-August 2009. 
Kritikopoulos Apostolos, Martha Sideri, and Iraklis 
Varlamis. 2006. BlogRank: Ranking Weblogs 
Based on Connectivity and Similarity Features. In 
the proceedings of the 2nd International Workshop 
on Advanced Architectures and Algorithms for 
Internet Delivery and Applications. 
Leavitt Alex, Evan Burchard, David Fisher, and Sam 
Gilbert. 2009. The Influentials: New Approaches 
for Analyzing Influence on Twitter. A publication 
of the Web Ecology Project. 
Liu Hongbo, Jiahai Yang, Jiaxin Wang, Yu Zhang. 
2007. A Link-Based Rank of Postings in 
Newsgroup. In the proceedings of the 5th 
International Conference on Machine Learning 
and Data Mining in Pattern Recognition, Pages: 
392-403. 
Page Lawrence, Sergey Brin, Rajeev Motwani, and 
Terry Winograd. 1999. The PageRank Citation 
Ranking: Bring Order to the Web. Technical 
report, Stanford University. 
Robertson Stephen E., Steve Walker, and Micheline 
Hancock-Beaulieu. 1998. Okapi at TREC-7: 
Automatic Ad Hoc, Filtering, VLC and Interactive. 
In the Proceedings of the 7th Text Retrieval 
Conference. Pages: 199-210 
Song Young-In, Chin-Yew Lin, Yunbo Cao, and 
Hae-Chang Rim. 2008. Question Utility: A Novel 
Static Ranking of Question Search. In the 
Proceedings of the 23rd AAAI Conference on 
Artificial Intelligence. Pages: 1231-1236 
Sun Aaron R., Jiesi Cheng, and Daniel D. Zeng. 2009. 
A Novel Recommendation Framework for 
Micro-blogging based on Information Diffusion. 
In the proceedings of the 19th Workshop on 
Information Technologies and Systems. 
Xi Wensi, Jesper Lind, and Eric Brill. 2004. Learning 
effective ranking functions for newsgroup search. 
In the proceedings of the 27th Annual International 
ACM SIGIR Conference on Research and 
Development in Information Retrieval, Pages: 
394-401 
Xu Gu, and Ma Wei-Ying. 2006. Building Implicit 
Links from Content for Forum Search. In the 
proceedings of the 29th International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval. Pages: 300-307. 
303
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 698?706,
Beijing, August 2010
Semantic Role Labeling for News Tweets 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 3Zhongyang Xiong and 2Changning Huang 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
zyxiong@cqu.edu.cn 
v-cnh@microsoft.com 
 
Abstract 
News tweets that report what is happen-
ing have become an important real-time 
information source. We raise the prob-
lem of Semantic Role Labeling (SRL) 
for news tweets, which is meaningful for 
fine grained information extraction and 
retrieval. We present a self-supervised 
learning approach to train a domain spe-
cific SRL system to resolve the problem. 
A large volume of training data is auto-
matically labeled, by leveraging the ex-
isting SRL system on news domain and 
content similarity between news and 
news tweets. On a human annotated test 
set, our system achieves  state-of-the-art 
performance, outperforming the SRL 
system trained on news. 
1 Introduction 
Tweets are text messages up to 140 characters. 
Every day, more than 50 million tweets are gen-
erated by millions of Twitter users. According to 
the investigation by Pear Analytics (2009), about 
4% tweets are related to news1. 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
1 http://blog.twitter.com/2010/02/measuring-tweets.html 
We divide news related tweets into two cate-
gories: those excerpted from news articles and 
those not. The former kind of tweets, hereafter 
called news excerpt, is formally written while 
the latter, hereafter called news tweet, varies in 
style and often is not grammatically correct. To 
understand the proportion of news tweets, we 
randomly selected 1000 tweets related to news, 
and got 865 news tweets. Following is an exam-
ple of anews tweet, containing oh, yea, which 
usually appear in spoken language, and :-(, an 
emoticon. 
oh yea and Chile earthquake the earth off it's 
axis according to NASA and shorten the day 
by a wee second :-(                                     (S1) 
News tweets arean important information 
source because they keep reporting what is hap-
pening in real time. For example, the earthquake 
near Los Angeles that happened on Tuesday, 
July 29, 2008 was first reported through news 
tweets only seconds later than the outbreak of 
the quake. Official news did not emerge about 
this event until four minutes later. By then, 
"Earthquake" was trending on Twitter Search 
with thousands of updates2. 
However, it is a daunting task for people to 
find out information they are interested in from 
such a huge number of news tweets, thus moti-
vating us to conduct some kind of information 
                                                 
2 http://blog.twitter.com/2008/07/twitter-as-news-wire.html 
698
extraction such as event mining, where SRL 
plays a crucial  role (Surdeanu et al, 2003). 
Considering Sentence 1, suppose the agent 
earthquake and the patient day for the predicate 
shorten are identified. Then it is straightforward 
to output the event Chile earthquake shorten the 
day, which captures the essential information 
encoded in this tweet. 
Following M?rquez (2009), we define SRL 
for news tweets as the task of identifying the 
arguments of a given verb as predicate in a news 
tweet and assigning them semantic labels de-
scribing the roles they play for the predicate. To 
make our method applicable to general infor-
mation extraction tasks,  rather than only to 
some special scenarios such as arresting event 
extraction, we adopt general semantic roles, i.e., 
Agent(A0), Patient(A1), Location(AM-LOC), 
Temporal(AM-TMP),etc., instead of situation-
specific roles (Fillmore et al, 2004) such as 
Suspect, Authorities, and Offense in an arrest 
frame.  
Our first attempt is to directly apply the state-
of-art SRL system (Meza-Ruiz and Riedel, 2009) 
that trained on the CoNLL 08 shared task da-
taset(Surdeanu et al, 2008), hereafter called 
SRL-BS, to news tweets. Not surprisingly, we 
observe its F1 score drops sharply from 75.5% 
on news corpus to 43.3% on our human annotat-
ed news tweets, owing much to the informal 
written style of news tweets. 
Therefore, we have to build a domain specific 
SRL system for news tweets. Given the diversi-
fied styles of news tweets, building such a sys-
tem requires a larger number of annotated news 
tweets, which are not available, and are not af-
fordable for human labeling. We propose a novel 
method to automatically annotate news tweets, 
which leverages the existing resources of SRL 
for news domain, and content similarity between 
news and news tweets. We argue that the same 
event is likely to be reported by both news and 
news tweets, which results in  content similarity 
between the news and news tweet. Further, we 
argue that the news and news tweets reporting 
the same event tend to have similar predicate-
argument structures. We tested our assumptions 
on the event Chile earthquake that happened on 
Match 2nd, 2010. We got 261 news and 722 news 
tweets published on the same day that described 
this event.  Sentence 2 and 3 are two examples 
of the news excerpts and Sentence 1 is one ex-
ample of news tweets for this event.   
Chile Earthquake Shortened Earth Day    (S2) 
Chile Earthquake Shortened Day              (S3) 
Obviously Sentence 1, 2 and 3 all have predi-
FDWH ?shortened? with the same A0 and A1 ar-
guments. Our manually checking showed that in 
average each news tweet in those 993 samples 
had 2.4 news excerpts that had the same predi-
cate-argument structures.  
Our news tweet annotation approach consists 
of four steps. First, we submit hot queries to 
Twitter and for each query we obtain a list of 
tweets. Second, for each list of tweets, we single 
out news excerpts using heuristic rules and re-
move them from the list, conduct SRL on news 
excerpts using SRL-BS, and cluster them in 
terms of the similarity in content and predicate-
argument structures. Third, for each list of 
tweets, we try to merge every remaining tweet 
into one news excerpt cluster according to its 
content similarity to the cluster. Those that can 
be put into one news group are regarded as news 
tweet. Finally, semantic structures of news ex-
cerpts are passed to the news tweet in the same 
group through word alignment. 
Our domain specific SRL system is then 
trained on automatically constructed training 
data using the Conditional Random Field (CRF: 
Lafferty et al, 2001) learning framework. Our 
system is evaluated on a human labeled dataset, 
and achieves state-of-the-art performance, out-
performing the baseline SRL-BS.  
Our contributions can be summarized as fol-
lows: 
1) We propose to conduct SRL for news 
tweets for fine grained information ex-
traction and retrieval;  
2) We present a semi-supervised learning 
approach to train a domain specific SRL 
system for news tweets, which outper-
forms SRL-BS and achieves the state-of-
the-art performance on a human labeled 
dataset. 
The rest of this paper is organized as follows: 
In the next section, we review related work.  In 
Section 3 we detail key components of our ap-
proach. In Section 4, we setup experiments and 
evaluate the effectiveness of our method.  Final-
699
ly, Section 5 concludes and presents the future 
work. 
2 Related Work 
Our related work falls into two categories: SRL 
on news and domain adaption. 
As for SRL on news, most researchers used 
the pipelined approach, i.e., dividing the task 
into several phases such as argument identifica-
tion, argument classification, global inference, 
etc.,  and conquering them individually (Xue and 
Palmer, 2004; Koomen et al, 2005; Cohn and 
Blunsom, 2005; Punyakanok et al, 2008; 
Toutanova et al, 2005; Toutanova et al, 2008). 
Exceptions to the pipelined approach exist.  
M?rquez et al (2005) sequentially labeled the 
words according to their positions relative to an 
argument (i.e., inside, outside or at the beginning 
of it). Carreras et al (2004) and Surdeanu et al 
(2007) jointly labeled all the predicates. Vickrey 
and Koller(2008) simplified the input sentence 
by hand-written and machine learnt rules before 
conducting SRL. Some other approaches simul-
taneously resolved all the sub-tasks by integrat-
ing syntactic parsing and SRL into a single mod-
el (Musillo and Merlo, 2006; Merlo and Musillo, 
2008), or by using Markov Logic Networks 
(MLN, Richardson and Domingos, 2005) as the 
learning framework (Riedel and Meza-Ruiz, 
2008; Meza-Ruiz and Riedel, 2009). 
All the above approaches focus on sentences 
from news articles or other formal documents, 
and depend on human annotated corpus for 
training. To our knowledge, little study has been 
carried out on SRL for news tweets.  
As for domain adaption, some researchers re-
garded the out-of-GRPDLQ GDWD DV ?SULRU
NQRZOHGJH?DQGestimated the model parameters 
by maximizing the posterior under this prior dis-
tribution, and successfully applied their ap-
proach to language modeling (Bacchiani and 
Roark, 2003) and parsing (Roark and Bacchiani, 
2003). Daum? III and Marcu (2006) presented a 
QRYHO IUDPHZRUN E\ GHILQLQJ D ?JHQHUDO Go-
PDLQ?EHWZHHQWKH?WUXO\LQ-GRPDLQ?DQG?WUXO\
out-of-GRPDLQ?   
Unlike existing domain adaption approaches, 
our method is about adapting SRL system on 
news domain to the news tweets domain, two 
domains that differ in writing style but are linked 
through content similarity. 
3 Our Method 
Our method of SRL for news tweets is to train a 
domain specific SRL on automatically annotated 
training data as briefed in Section 1.  
In this section we present details of the five 
crucial components of our method, i.e., news 
excerpt identification, news excerpt clustering, 
news tweets identification, semantic structure 
mapping, and the domain specific SRL system 
constructing. 
3.1 News Excerpt Identification 
We use one heuristic rule to decide whether or 
not a tweet is news excerpt:  if a tweet has a link 
to a news article and its text content is included 
by the news article, it is news excerpt, otherwise 
not. 
Given a tweet, to apply this rule, we first ex-
tract the content link and expand it, if any, into 
the full link with the unshorten service3. This 
step is necessary because content link in tweet is 
usually shortened to reduce the total amount of 
characters. Next, we check if the full link points 
to any of the pre-defined news sites, which, in 
our experiments, are 57 English news websites. 
If yes, we download the web page and check if it 
exactly contains the text content of the input 
tweet. Figure 1 illustrates this process.  
Figure 1. An illustration of news excerpt identi-
fication. 
To test the precision of this approach, while 
preparing for the training data for the experi-
ments, we checked 100 tweets that were identi-
fied as news excerpt by this rule to find out they 
all are excerpted from news. 
                                                 
3 http://unshort.me 
700
3.2 News Excerpt Clustering 
Given as input a list of news excerpts concerning 
the same query and published in the same time 
scope, this component uses the hierarchical ag-
glomerative clustering algorithm (Manning et 
al., 2008) to divide news excerpts into groups in 
terms of the similarity in content and predicate-
argument structures.  
Before clustering, for every news excerpt, we 
remove the content link and other metadata such 
as author, retweet marks (starting with RT @), 
reply marks (starting with @ immediately after 
the author), hash tags (starting with #), etc., and 
keep only the text content; then it is further 
parsed into tokens, POS tags, chunks and syntac-
tic tree using the OpenNLP toolkit4.  After that,  
SRL is conducted with SRL-BS to get predicate-
argument structures. Finally, every news excerpt 
is represented as frequency a vector of terms, 
including tokens, POS tagger, chunks, predicate-
argument structures, etc. A news cluster is re-
garded as a ?macro? news excerpt and is also 
represented as a term frequency vector, i.e., the 
sum of all the term vectors in the cluster.  Noisy 
terms, such as numbers and predefined stop 
words are excluded from the frequency vector. 
To reduce data sparseness, words are stemmed 
by Porter stemmer (Martin F. Porter, 1980). 
The cosine similarity is used to measure the 
relevance between two clusters, as defined in 
Formula 1.  
   ,
'
'
'
CVCV
CVCV
CCCS
u
?               (1) 
Where C, &? denote two clusters, CV, CV? de-
note  the term frequency vectors of C and  &? 
respectively, and CS(C, &?) stands for the  co-
sine similarity between C and  &?. 
Initially, one news excerpt forms one cluster.  
Then the clustering process repeats merging the 
two most similar clusters into one till the simi-
larity between any pair of clusters is below a 
threshold, which is experimentally set to 0.7 in 
our experiments. 
During the training data preparation process, 
we randomly selected 100 clusters, each with 3.2 
pieces of news in average. For every pair of 
news excerpts in the same cluster, we checked if 
                                                 
4 http://opennlp.sourceforge.net/ 
they shared similar contents and semantic struc-
tures, and found out that 91.1% were the cases. 
3.3 News Tweets Identification 
After news excerpts are identified and removed 
from the list, every remaining tweet is checked if 
it is a news tweet. Here we group news excerpts 
and news tweets together in two steps because 1) 
news excerpts count for only a small proportion 
of all the tweets in the list, making our two-step 
clustering algorithm more efficient; and 2) one-
step clustering tends to output meaningless clus-
ters that include no news tweets. 
Intuitively, news tweet, more often than not, 
have news counterparts that report similar con-
tents. Thus we use the following rule to identify 
news tweets: if the content similarity between 
the tweet and any news excerpt cluster is greater 
than a threshold, which is experimentally set to 
0.7 in our experiments, the tweet is a news tweet, 
otherwise it is not. Furthermore, each news 
tweet is merged into the cluster with most simi-
lar content. Finally, we re-label any news tweet 
as news excerpt, which is then process by SRL-
BS, if its content similarity to the cluster exceeds 
a threshold, which is experimentally set to 0.9 in 
our experiments. 
Again, the cosine similarity is used to meas-
ure the content similarity between tweet and 
news excerpt cluster. Each tweet is repressed as 
a term frequency vector. Before extracting terms 
from tweet, tweet metadata is removed and a 
rule-based normalization process is conducted to 
restore abnormal strLQJVVD\?	DSRV?LQWRWKHLU
KXPDQ IULHQGO\ IRUP VD\ ?? ? 1H[W VWHPPLQJ
tools and OpenNLP are applied to get lemmas, 
POS tags, chunks, etc., and noisy terms are fil-
tered.  
We evaluated the performance of this ap-
proach when preparing for the training data. We 
randomly sampled 500 tweets that were identi-
fied as news tweets, to find that 93.8% were true 
news tweets. 
3.4 Semantic Structure Mapping 
Semantic structure mapping is formed as the 
task of word alignment from news excerpt to 
news tweet. A HMM alignment model is trained 
with GIZA++ (Franz and Hermann, 2000) on all 
(news excerpt, news tweet) pairs in the same 
cluster. After word alignment is done, semantic 
701
information attached to a word in a news excerpt 
is passed to the corresponding word in the news 
tweet as illustrated in Figure 2. 
 
Chile Earthquake Shortened Earth Day
A0 predicate A1
NASA and shorten the day by a wee second :-(
oh yea and Chile earthquake the earth off it's axis according to
 
Figure 2. An example of mapping semantic 
structures from news excerpts to news tweets. 
In Figure 2, shorten, earthquake and day in 
two sentences are aligned, respectively; and two 
predicate-argument structures in the first sen-
tence, i.e., (shortened, earthquake, A0), (short-
ened, day, A1), are passed to the second. 
News tweets may receive no semantic infor-
mation from related news excerpts after mapping, 
because of word alignment errors or no news 
excerpt in the cluster with similar semantic 
structures.  Such tweets are dropped. 
Mapping may also introduce cases that violate 
the following two structural constraints in SRL 
(Meza-Ruiz and Riedel, 2009): 1) one (predi-
cate, argument) pair has only one role label in 
one sentence; and 2) for each predicate, each of 
the proper arguments (A0~A5) can occur at most 
once. Those conflicts are largely owing to the 
noisy outputs of SRL trained on news and to the 
alignment errors. While preparing for the train-
ing data for our experiments, we found 38.9% of 
news tweets had such conflicts.  
A majority voting schema and the structural 
constrains are used to resolve the conflicts as 
described below.   
1) Step 1, for every cluster, each (predicate, 
argument, role) is weighted according to 
its frequency in the cluster; 
2) Step 2, for every cluster, detect conflicts 
using the structural constrains; if no con-
flicts exist, stop; otherwise go to Step 3;   
3) Step 3, for every cluster, keep the one 
with higher weight in each conflicting 
(predicate, argument, role) pair; if the 
weights are equal,  drop both; 
Here is an example to show the conflicting 
resolution process.  Consider the cluster includ-
ing Sentence 1, 2 and 3, where (shorten, earth-
quake, A0), (shorten, earthquake, A1), (shorten, 
axis, A0), and (shorten, day, A1) occur 6, 4, 1 
and 3 times, respectively.  This cluster includes 
three conflicting pairs:   
1) (shorten, earthquake, A0) vs. (shorten, 
earthquake, A1); 
2) (shorten, earthquake, A1) vs. (shorten, 
day, A1); 
3) (shorten, earthquake, A0) vs. (shorten, ax-
is, A0); 
The first pair is first resolved, causing (short-
en, earthquake, A0) to be kept and (shorten, 
earthquake, A1) removed, which leads to the 
second pair being resolved as well; then we pro-
cess the third pair resulting in (shorten, earth-
quake, A0) being kept and (shorten, axis, A0) 
dropped; finally (shorten, earthquake, A0) and 
(shorten, day, A1) stay in the cluster. 
The conflicting resolution algorithm is sensi-
tive to the order of conflict resolution in Step 3. 
Still consider the three conflicting pairs listed 
above. If the second pair is first processed, only 
(shorten, earthquake, A0) will be left. Our strat-
egy is to first handle the conflict resolving which 
leads to most conflicts resolved. 
We tested the performance of this semantic 
structure mapping strategy while preparing for 
the training data. We randomly selected 56 news 
tweets with conflicts and manually annotated 
them with SRL. After the conflict resolution 
method was done, we observed that 38 news 
tweets were resolved correctly, 9 resolved but 
incorrectly, and 9 remain unresolved, suggesting 
the high precision of this method, which fits our 
task.  We leave it to our future work to study 
more advanced approach for semantic structure 
mapping. 
3.5 SRL System for News Tweets 
Following M?rquez et al (2005), we regard SRL 
for tweets as a sequential labeling task, because 
of its joint inference ability and its openness to 
support other languages. 
We adopt conventional features for each token 
defined in M?rquez et al(2005),  such as the 
lemma/POS tag of the current/previous/next to-
ken, the lemma of predicate and its combination 
with the lemma/POS tag of the current token, the 
voice of the predicate (active/passive), the dis-
tance between the current token and the predi-
cate, the relative position of the current token to 
702
the predicate, and so on. We do not use features 
related to syntactic parsing trees, to allow our 
system not to rely on any syntactic parser, whose 
performance depends on style and language of 
text, which limits the generality of our system. 
Before extracting features, we perform a pre-
processing step to remove tweet metadata and 
normalize tweet text content, as described in 
Section 3.3. The OpenNLP toolkit is used for 
feature extraction, and the CRF++ toolkit 5  is 
used to train the model. 
4 Experiments 
In this section, we evaluate our SRL system on a 
gold-standard dataset consisting of 1,110 human 
annotated news tweets and show that our system 
achieves the state-of-the-art performance com-
pared with SRL-BS that is trained on news. Fur-
thermore, we study the contribution of automati-
cally generated training data. 
4.1 Evaluation Metric 
We adopt the widely used precision (Pre.), recall 
(Rec.) and F-score (F., the harmonic mean of 
precision and recall) as evaluation metrics.  
4.2 Baseline System 
We use SRL-BS as our baseline because of its 
state-of-art performance on news domain, and its 
readiness to use as well. 
4.3 Data Preparation 
We restrict to English news tweets to test our 
method. Our method can label news tweets of 
other languages, given that the related tools such 
as the SRL system on news domain, the word 
alignment tool, OpenNLP, etc., can support oth-
er languages.  
We build two corpora for our experiments: 
one is the training dataset of 10,000 news tweets 
with semantic roles automatically labeled; the 
other is the gold-standard dataset of 1,110 news 
tweets with semantic roles manually labeled. 
Training Dataset 
We randomly sample 80 queries from 300 
English queries extracted from the top stories of 
Bing news, Google news and Twitter trending 
topics from March 1, 2010 to March 4, 2010.  
                                                 
5 http://crfpp.sourceforge.net/ 
Submitting the 80 queries to Twitter search, 
we retrieve and download 512,000 tweets, from 
which we got 4,785 news excerpts and 11,427 
news tweets, which were automatically annotat-
ed using the method described in Section 3.   
Furthermore, 10,000 tweets are randomly se-
lected from the automatically annotated news 
tweets, forming the training dataset, while the 
other 1,427 news tweets are used to construct the 
gold-standard dataset. 
Gold-standard Dataset 
We ask two people to annotate the 1,427 news 
tweets, following the Annotation guidelines for 
PropBank6 with one exception: for phrasal ar-
guments, only the head word is labeled as the 
argument, because our system and SRL-BS con-
duct word level SRL. 
317 news tweets are dropped because of in-
consistent annotation, and the remaining 1,110 
news tweets form the gold-standard dataset.  
Quality of Training dataset 
Since the news tweets in the gold-standard da-
taset are randomly sampled from the automati-
cally labeled corpus and are labeled by both hu-
man and machine, we use them to estimate the 
quality of training data, i.e., to which degree the 
automatically generated results are similar to 
humans?.   
We find that our method achieves 75.6% F1 
score, much higher than the baseline, suggesting 
the relatively high quality of the training data. 
4.4 Result and Analysis 
Table 1 reports the experimental results of our 
system (SRL-TS) and the baseline on the gold-
standard dataset. 
 
 Precision Recall F-Score 
SRL-BS 36.0 % 54.5% 43.3% 
SRL-TS 78.0% 57.1% 66.0% 
Table 1. Performances of our system and the 
baseline on the gold-standard dataset. 
As shown in Table 1, our system performs 
much better than the baseline on the gold-
standard dataset in terms of all metrics. We ob-
serve two types of errors that are often made by 
                                                 
6 http://verbs.colorado.edu/~mpalmer/projects/ace/PB 
guidelines.pdf 
703
SRL-BS but not so often by our system, which 
largely explains the difference in performance.  
The first type of errors, which accounts for 
25.3% of the total errors made by SRL-BS, is 
caused by the informal written style, such as el-
lipsis, of news tweets. For instance, for the ex-
ample Sentence 1 listed in Section 1, the SRL-
BS incorrectly identify earth as the A0 argument 
of the predicate shorten. The other type of errors, 
which accounts for 10.2% of the total errors 
made by SRL-BS, is related to the discretionary 
combination of news snippets. For example, 
consider the following news tweet: 
The Chile earthquake shifted the earth's axis, 
"shortened the length of an Earth day by 1.26 
miliseconds".                                              (S4) 
We analyze the errors made by our system 
and find that 12.5% errors are attributed to the 
complex syntactic structures, suggesting that 
combining our system with systems on news 
domain is a promising direction. For example, 
our system cannot identify the A0 argument of 
the predicate shortened, because of its blindness 
of attributive clause; in contrast, SRL-BS works 
on this case.  
wow..the earthquake that caused the 2004 In-
dian Ocean tsunami shortened the day by al-
most 3 microseconds..what does that even 
mean?! HOW?                                           (S5) 
We also find that 32.3% of the errors made by 
our system are more or less related to the train-
ing data, which has noise and cannot fully repre-
sent the knowledge of SRL on news tweets. For 
instance, our system fails to label the following 
sentence, partially because the predicate strike 
does not occur in the training set. 
8.8-Magnitude-Earthquake-Strikes-Chile (S6) 
We further study how the size of automatical-
O\ODEHOHGWUDLQLQJGDWDDIIHFWVRXUV\VWHP?VSHr-
formance, as illustrated in Figure 3. We conduct 
two sets of experiments: in the first set, the train-
ing data is automatically labeled and the testing 
data is the gold-standard dataset; in the second 
set, half of the news tweets from the gold-
standard dataset are added to the training data, 
the remaining half forms the testing dataset. 
Curve 1 and 2 represent the experimental results 
of set 1 and 2, respectively. 
From Curve 1, we see that RXUV\VWHP?VSHr-
formance increases sharply when the training 
data size varies from 5,000 to 6,000; then in-
creases relatively slowly with more training data; 
and finally reaches the highest when all training 
data is used.  Curve 2 reveals a similar trend. 
 
 
Figure 3. Performance on training data of vary-
ing size. 
This phenomenon is largely due to the com-
peting between two forces: the noise in the train-
ing data, and the knowledge of SRL encoded in 
the training data.  
Interestingly, from Figure 3, we observe that 
the contribution of human labeled data is no 
longer significant after 6,000 automatically la-
beled training data is used, reaffirming the effec-
tiveness of the training data. 
5 Conclusions and Future Work 
We propose to conduct SRL on news tweets for 
fine grained information extraction and retrieval. 
We present a self-supervised learning approach 
to train a domain specific SRL system for news 
tweets. Leveraging the SRL system on news 
domain and content similarity between news and 
news tweets, our approach automatically labels a 
large volume of training data by mapping SRL-
BS generated results of news excerpts to news 
tweets. Experimental results show that our sys-
tem outperforms the baseline and achieves the 
state-of-the-art performance.  
In the future, we plan to enlarge training data 
size and test our system on a larger dataset; we 
also plan to further boost the performance of our 
system by incorporating tweets specific features 
such as hash tags, reply/re-tweet marks into our 
704
CRF model, and by combining our system with 
SRL systems trained on news.  
 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper-
vised language model adaptation. Proceedings of 
the 2003 International Conference on Acoustics, 
Speech and Signal Processing, volume 1, pages: 
224-227 
Carreras, Xavier, Llu?s M?rquez, and Grzegorz 
&KUXSD?D+LHUDUFKLFDOUHFRJQLWLRQRISURSo-
sitional arguments with Perceptrons. Proceedings 
of the Eighth Conference on Computational Natu-
ral Language Learning, pages: 106-109. 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labeling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Daum?, Hal III and Daniel Marcu. 2006. Domain 
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26(1), 101-126. 
Fillmore, Charles J., Josef Ruppenhofer, Collin F. 
Baker. 2004. FrameNet and Representing the Link 
between Semantic and Syntactic Relations. Com-
putational Linguistics and Beyond, Institute of 
Linguistics, Academia Sinica. 
Kelly, Ryan, ed. 2009. Twitter Study Reveals Inter-
esting Results About Usage. San Antonio, Texas: 
Pear Analytics. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
Lafferty, John D., Andrew McCallum, Fernando C. 
N. Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Labeling 
Sequence Data. Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, 
pages: 282-289. 
Manning, Christopher D., Prabhakar Raghavan and 
Hinrich Schtze. 2008. Introduction to Information 
Retrieval. Cambridge University Press, Cam-
bridge, UK. 
M?rquez, Llu?s, Jesus Gim?nez Pere Comas and 
Neus Catal?. 2005. Semantic Role Labeling as Se-
quential Tagging. Proceedings of the Ninth Con-
ference on Computational Natural Language 
Learning, pages: 193-196. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses us-
ing Markov Logic. Human Language Technolo-
gies: The 2009 Annual Conference of the North 
American Chapter of the ACL, pages: 155-163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Och, Franz Josef, Hermann Ney. Improved Statistical 
Alignment Models. Proceedings of the 38th Annu-
al Meeting of the Association for Computational 
Linguistics, pages: 440-447. 
Porter, Martin F. 1980. An algorithm for suffix strip-
ping. Program, 14(3), 130-137. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and in-
ference in semantic role labeling. Journal of Com-
putational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. Collec-
tive semantic role labelling with Markov Logic. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 193-
197. 
Roark, Brian and Michiel Bacchiani. 2003. Super-
vised and unsupervised PCFG adaptation to novel 
domains. Proceedings of the 2003 Conference of 
the North American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology, volume 1, pages: 126-133. 
Surdeanu, Mihai, Sanda Harabagiu, JohnWilliams 
and Paul Aarseth. 2003. Using predicate-argument 
structures for information extraction. Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics, volume 1, pages: 8-15. 
Surdeanu, Mihai, Llu?s M?rquez, Xavier Carreras and 
Pere R. Comas. 2007. Combination strategies for 
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29(1), 105-151. 
705
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Vickrey, David and Daphne Koller. 2008. Applying 
sentence simplification to the conll-2008 shared 
task. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, pag-
es: 268-272  
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
706
Coling 2010: Poster Volume, pages 725?729,
Beijing, August 2010
Collective Semantic Role Labeling on Open News Corpus  
by Leveraging Redundancy 
 
 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 5Daniel Tse* and 3Zhongyang Xiong 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
5School of Information Technologies 
The University of Sydney 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
dtse6695@it.usyd.edu.au 
zyxiong@cqu.edu.cn 
 
  
Abstract 
We propose a novel MLN-based method 
that collectively conducts SRL on 
groups of news sentences. Our method is 
built upon a baseline SRL, which uses 
no parsers and leverages redundancy. 
We evaluate our method on a manually 
labeled news corpus and demonstrate 
that news redundancy significantly im-
proves the performance of the baseline, 
e.g., it improves the F-score from 
64.13% to 67.66%.  * 
1 Introduction 
Semantic Role Labeling (SRL, M?rquez, 2009) 
is generally understood as the task of identifying 
the arguments of a given predicate and assigning 
them semantic labels describing the roles they 
play. For example, given a sentence The luxury 
auto maker sold 1,214 cars., the goal is to iden-
tify the arguments of sold and produce the fol-
lowing output: [A0 The luxury auto maker] [V 
sold] [A1 1,214 cars]. Here A0 represents the 
seller, and A1 represents the things sold (CoNLL 
2008 shared task, Surdeanu et al, 2008). 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
Gildea and Jurafsky (2002) first tackled SRL 
as an independent task, which is divided into 
several sub-tasks such as argument identifica-
tion, argument classification, global inference, 
etc. Some researchers (Xue and Palmer, 2004; 
Koomen et al, 2005; Cohn and Blunsom, 2005; 
Punyakanok et al, 2008; Toutanova et al, 2005; 
Toutanova et al, 2008) used a pipelined ap-
proach to attack the task. Some others resolved 
the sub-tasks simultaneously. For example, some 
work (Musillo and Merlo, 2006; Merlo and Mu-
sillo, 2008) integrated syntactic parsing and SRL 
into a single model, and another (Riedel and 
Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009) 
jointly handled all sub-tasks using Markov Log-
ic Networks (MLN, Richardson and Domingos, 
2005). 
All the above methods conduct sentence level 
SRL, and rely on parsers. Parsers have showed 
great effects on SRL performance. For example, 
Xue and Palmer (2004) reported that SRL per-
formance dropped more than 10% when they 
used syntactic features from an automatic parser 
instead of the gold standard parsing trees. Even 
worse, parsers are not robust and cannot always 
analyze any input, due to the fact that some in-
puts are not in the language described by the 
parser?s formal grammar, or adequately repre-
sented within the parser?s training data. 
725
We propose a novel MLN-based method that 
collectively conducts SRL on groups of news 
sentences to leverage the content redundancy in 
news. To isolate the negative effect of noise 
from parsers and thus focus on the study of the 
contribution of redundancy to SRL, we use no 
parsers in our approach. We built a baseline SRL, 
which depends on no parsers, and use the MLN 
framework to exploit  redundancy. Our intuition 
is that SRL on one sentence can help that on 
other differently phrased sentences with similar 
meaning. For example, consider the following 
sentence from a news article: 
A suicide bomber blew himself up Sunday in 
market in Pakistan's northwest crowded with 
shoppers ahead of a Muslim holiday, killing 
12 people, including a mayor who once sup-
ported but had turned against the Taliban, of-
ficials said. 
The state-of-art MLN-based system (Meza-Ruiz 
and Riedel, 2009), hereafter referred to as 
MLNBS for brevity, incorrectly labels northwest 
instead of bomber as A0 of killing. Now consider 
another sentence from another news article: 
Police in northwestern Pakistan say that a su-
icide bomber has killed at least 13 people and 
wounded dozens of others. 
Here MLNBS correctly identify bomber as A0 
of killing. When more sentences are observed 
where bomber as A0 of killing is correctly identi-
fied, we will be more confident that bomber 
should be labeled as A0 of killing, and that 
northwest should not be the A0 of killing accord-
ing to the constraint that one predicate has at 
most one A0. 
We manually construct a news corpus to 
evaluate our method. In the corpus, semantic 
role information is annotated and sentences with 
similar meanings are grouped together. Experi-
mental results show that news redundancy can 
significantly improve the performance of the 
baseline system. 
Our contributions can be summarized as fol-
lows: 
1. We present a novel method that conducts 
SRL on a set of sentences collectively, in-
stead of on a single sentence, by extend-
ing MLNBS to leverage redundancy. 
2. We show redundancy can significantly 
improve the performance of the baseline 
system, indicating a promising research 
direction towards open SRL. 
In the next section, we introduce news sen-
tence extraction and clustering. In Section 3, we 
describe our collective inference method. In Sec-
tion 4, we show our experimental results. Finally, 
in Section 5 we conclude our paper with a dis-
cussion of future work. 
2 Extraction and Clustering of News 
Sentences 
To construct a corpus to evaluate our method, 
we extract sentences from clustered news arti-
cles returned by news search engines such as 
Bing and Google, and divide them into groups 
so that sentences in a group have similar mean-
ing. 
News articles in the same cluster are supposed 
to report the same event. Thus we first group 
sentences according to the news cluster they 
come from. Then we split sentences in the same 
cluster into several groups according to the simi-
larity of meaning. We assume that two sentences 
are more similar in meaning if they share more 
synonymous proper nouns and verbs. The syno-
nyms of verbs, like plod and trudge, are mainly 
extracted from the Microsoft Encarta Diction-
ary1, and the proper nouns thesaurus, containing 
synonyms such as U.S. and the United States, is 
manually compiled. 
As examples, below are two sentence groups 
which are extracted from a news cluster describ-
ing Hurricane Ida. 
Group 1: 
? Hurricane Ida, the first Atlantic hurri-
cane to target the U.S. this year, plod-
ded yesterday toward the Gulf Coast? 
? Hurricane Ida trudged toward the Gulf 
Coast? 
? ? 
Group 2: 
? It could make landfall as early as Tues-
day morning, although it was forecast to 
weaken further. 
                                                 
1
http://uk.encarta.msn.com/encnet/features/dictionary/dictio
naryhome.aspx 
726
? Authorities said Ida could make landfall 
as early as Tuesday morning, although 
it was forecast to weaken by then. 
? ? 
3 Collective Inference Based on MLN 
Our method includes two core components: a 
baseline system that conducts SRL on every sen-
tence; and a collective inference system that ac-
cepts as input a group of sentences with prelimi-
nary SRL information provided by the baseline. 
We build the baseline by removing formulas 
involving syntactic parsing information from 
MLNBS (while keeping other rules) and retrain-
ing the system using the tool and scripts provid-
ed by Riedel and Meza-Ruiz (2008) on the man-
ually annotated news corpus described in Sec-
tion 4. 
A collective inference system is constructed 
to leverage redundancy in the SRL information 
from the baseline.  
We first redefine the predicate role and treat it 
as observed: 
predicate role: Int x Int x Int x Role; 
role has four parameters: the first one stands for 
the number of sentence in the input, which is 
necessary to distinguish the sentences in a group; 
the other three are taken from the arguments of 
the role predicate defined by Riedel and Meza-
Ruiz (2008), which denote the positions of the 
predicate and the argument in the sentence and 
the role of the argument, respectively. If the 
predication holds, it returns 1, otherwise 0.  
A hidden predicate final_role is defined to 
present the final output, which has the same pa-
rameters as the predicate role: 
predicate final_role: Int x Int x Int x Role; 
We introduce the following formula, which 
directly passes the semantic role from the base-
line to the final output: 
role(s, p, a, +r)=> final_role (s, p, a, +r)    (1) 
Here s is the sentence number in a group; p and 
a denote the positions of the predicate and ar-
gument in s, respectively; r stands for the role of 
the argument; the ?+? before the variable r indi-
cates that different r has different weight. 
Then we define another formula for collective 
inference: 
s1?s2^lemma(s1,p1,p_lemma)^lemma(s2,p2, 
p_lemma)^lemma(s1,a1,a_lemma)^lemma(s2,
a2,a_lemma)^role(s2,p2,a2,+r)=>final_role 
(s1,p1,a1,+r)                                                 (2) 
Here p_lemma(a_lemma) stands for the lemma 
of the predicate(argument), which is obtained 
from the lemma dictionary. This dictionary is 
extracted from the dataset of CoNLL 2008 
shared task and is normalized using synonym 
dictionary described in Section 2; lemma is an 
observed predicate that states whether or not the 
word has the lemma. 
Formula 2 encodes our basic ideas about col-
lective SRL: given several sentences expressing 
similar meaning, if one sentence has a predicate 
p with an argument a of role r, the other sen-
tences would be likely to have a predicate p? 
with an argument a? of role r, where p? and a? 
are the same or synonymous with p and a, re-
spectively, as illustrated by the example in Sec-
tion 1. 
Besides, we also apply structural constraints 
(Riedel and Meza-Ruiz, 2008) to final_role. 
To learn parameters of the collective infer-
ence system, we use  thebeast (Riedel and Meza-
Ruiz, 2008),  which is an open Markov Logic 
Engine, and train it on manually annotated news 
corpus described in Section 4. 
4 Experiments 
To train and test the collective inference system, 
we extract 1000 sentences from news clusters, 
and group them into 200 clusters using the 
method described in Section 2. For every sen-
tence, POS tagging is conducted with the 
OpenNLP toolkit (Jason Baldridge et al, 2009), 
lemma of each word is obtained through the 
normalized lemma dictionary described in Sec-
tion 3, and SRL is manually labeled. To reduce 
human labeling efforts, we retrain our baseline 
on the WSJ corpus of CoNLL 2008 shared task 
and run it on our news corpus, and then edit the 
SRL outputs by hand. 
We implement the collective inference system 
with the thebeast toolkit. Precision, recall, and 
F-score are used as evaluation metrics.  In both 
training and evaluation, we follow the CoNLL 
2008 shared task and regard only heads of 
phrases as arguments. 
727
Table 1 shows the averaged 10-fold cross val-
idation results of our systems and the baseline, 
where the third and second line report the results 
of using and not using Formula 1 in our collec-
tive inference system, respectively. 
 
Systems Pre. (%) Rec. (%) F-score (%) 
Baseline 69.87 59.26 64.13 
CI-1 62.99 72.96 67.61 
CI 67.01 68.33 67.66 
Table 1. Averaged 10-fold cross validation re-
sults (Pre.: precision; Rec.: recall). 
Experimental results show that the two collec-
tive inference engines (CI-1 and CI) perform 
significantly better than the baseline in terms of 
the recall and F-score, though a little worse in 
the precision. We observe that predicate-
argument relationships in sentences with com-
plex syntax are usually not recognized by the 
baseline, but some of them are correctly identi-
fied by the collective inference systems. This, 
we guess, explains in large part the difference in 
performance. For instance, consider the follow-
ing sentences in a group, where order and tell 
are synonyms: 
? Colombia said on Sunday it will appeal 
to the U.N. Security Council and the 
OAS after Hugo Chavez, the fiery leftist 
president of neighboring Venezuela, or-
dered his army to prepare for war in or-
der to assure peace. 
? President Hugo Chavez ordered Vene-
zuela's military to prepare for a possible 
armed conflict with Colombia, saying 
yesterday that his country's soldiers 
should be ready if the U.S. tries to pro-
voke a war between the South American 
neighbors. 
? Venezuelan President Hugo Chavez told 
his military and civil militias to prepare 
for a possible war with Colombia as ten-
sions mount over an agreement giving 
U.S. troops access to Colombian mili-
tary bases. 
The baseline cannot label (ordered, Chavez, A0) 
for the first sentence, partially owing to the syn-
tactic complexity of the sentence, but can identi-
fy the relationship for the second and third sen-
tence. In contrast, the collective inference sys-
tems can identify Chavez in the first sentence as 
A0 of order because of its occurrence in the oth-
er sentences of the same group. 
As Table 1 shows, the CI system achieves the 
highest F-score (67.66%), and a higher precision 
than the CI-1 system, indicating the effective-
ness of Formula 1. Consider the above three sen-
tences. CI-1 mislabels (ordered, Venezuela, A1) 
for the first sentence because the baseline labels 
it for the second sentence. In contrast, CI does 
not label it for the first sentence because the 
baseline does not and (ordered, Venezuela, A1) 
rarely occurs in the outputs of the baseline for 
this sentence group. 
We also find cases where the collective infer-
ence systems do not but should help. For exam-
ple, consider the following group of sentences: 
? A Brazilian university expelled a woman 
who was heckled by hundreds of fellow 
students when she wore a short, pink 
dress to class, taking out newspaper ads 
Sunday to publicly accuse her of immo-
rality.  
? The university also published newspaper 
ads accusing the student, Geisy Arruda, 
of immorality. 
The baseline has identified (published, univer-
sity, A0) for the second sentence. But neither 
the baseline nor our method labels (taking, uni-
versity, A0) for the first one.  This happens be-
cause publish is not considered as a synonym 
of take, and thus (published, university, A0) in 
the second provides no evidence for (taking, 
university, A0) in the first. We plan to develop 
a context based synonym detection component 
to address this issue in the future. 
5 Conclusions and Future Work 
We present a novel MLN-based method that col-
lectively conducts SRL on groups of sentences. 
To help build training and test corpora, we de-
sign a method to collect news sentences and to 
divide them into groups so that sentences of sim-
ilar meaning fall into the same cluster. Experi-
mental results on a manually labeled news cor-
pus show that collective inference, which lever-
ages redundancy, can effectively improve the 
performance of the baseline. 
728
In the future, we plan to evaluate our method 
on larger news corpora, and to extend our meth-
od to other genres of corpora, such as tweets. 
 
References  
Baldridge, Jason, Tom Morton, and Gann. 2009. 
OpenNLP, http://opennlp.sourceforge.net/ 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labelling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Journal of Computa-
tional Linguistics, 28(3):245?288. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses 
using Markov Logic. Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the ACL, pages: 155-
163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and 
inference in semantic role labeling. Journal of 
Computational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. 
Collective semantic role labelling with Markov 
Logic. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 193-197. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
 
729
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 430?439,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Mining Name Translations from Entity Graph Mapping?
Gae-won You? Seung-won Hwang? Young-In Song? Long Jiang? Zaiqing Nie?
?Pohang University of Science and Technology, Pohang, Republic of Korea
{gwyou,swhwang}@postech.ac.kr
?Microsoft Research Asia, Beijing, China
{yosong,longj,znie}@microsoft.com
Abstract
This paper studies the problem of mining en-
tity translation, specifically, mining English
and Chinese name pairs. Existing efforts
can be categorized into (a) a transliteration-
based approach leveraging phonetic similar-
ity and (b) a corpus-based approach exploiting
bilingual co-occurrences, each of which suf-
fers from inaccuracy and scarcity respectively.
In clear contrast, we use unleveraged re-
sources of monolingual entity co-occurrences,
crawled from entity search engines, repre-
sented as two entity-relationship graphs ex-
tracted from two language corpora respec-
tively. Our problem is then abstracted as find-
ing correct mappings across two graphs. To
achieve this goal, we propose a holistic ap-
proach, of exploiting both transliteration sim-
ilarity and monolingual co-occurrences. This
approach, building upon monolingual corpora,
complements existing corpus-based work, re-
quiring scarce resources of parallel or compa-
rable corpus, while significantly boosting the
accuracy of transliteration-based work. We
validate our proposed system using real-life
datasets.
1 Introduction
Entity translation aims at mapping the entity names
(e.g., people, locations, and organizations) in source
language into their corresponding names in target
language. While high quality entity translation is es-
sential in cross-lingual information access and trans-
?This work was done when the first two authors visited Mi-
crosoft Research Asia.
lation, it is non-trivial to achieve, due to the chal-
lenge that entity translation, though typically bear-
ing pronunciation similarity, can also be arbitrary,
e.g., Jackie Chan and ? (pronounced Cheng
Long). Existing efforts to address these challenges
can be categorized into transliteration- and corpus-
based approaches. Transliteration-based approaches
(Wan and Verspoor, 1998; Knight and Graehl, 1998)
identify translations based on pronunciation similar-
ity, while corpus-based approaches mine bilingual
co-occurrences of translation pairs obtained from
parallel (Kupiec, 1993; Feng et al, 2004) or compa-
rable (Fung and Yee, 1998) corpora, or alternatively
mined from bilingual sentences (Lin et al, 2008;
Jiang et al, 2009). These two approaches have com-
plementary strength? transliteration-based similar-
ity can be computed for any name pair but cannot
mine translations of little (or none) phonetic simi-
larity. Corpus-based similarity can support arbitrary
translations, but require highly scarce resources of
bilingual co-occurrences, obtained from parallel or
comparable bilingual corpora.
In this paper, we propose a holistic approach,
leveraging both transliteration- and corpus-based
similarity. Our key contribution is to replace the
use of scarce resources of bilingual co-occurrences
with the use of untapped and significantly larger
resources of monolingual co-occurrences for trans-
lation. In particular, we extract monolingual co-
occurrences of entities from English and Chinese
Web corpora, which are readily available from en-
tity search engines such as PeopleEntityCube1, de-
ployed by Microsoft Research Asia. Such engine
1http://people.entitycube.com
430
automatically extracts people names from text and
their co-occurrences to retrieve related entities based
on co-occurrences. To illustrate, Figure 1(a) demon-
strates the query result for ?Bill Gates,? retrieving
and visualizing the ?entity-relationship graph? of re-
lated people names that frequently co-occur with
Bill in English corpus. Similarly, entity-relationship
graphs can be built over other language corpora, as
Figure 1(b) demonstrates the corresponding results
for the same query, from Renlifang2 on ChineseWeb
corpus. From this point on, for the sake of simplic-
ity, we refer to English and Chinese graphs, simply
asGe andGc respectively. Though we illustrate with
English-Chinese pairs in the paper, our method can
be easily adapted to other language pairs.
In particular, we propose a novel approach of ab-
stracting entity translation as a graph matching prob-
lem of two graphsGe andGc in Figures 1(a) and (b).
Specifically, the similarity between two nodes ve
and vc in Ge and Gc is initialized as their transliter-
ation similarity, which is iteratively refined based on
relational similarity obtained from monolingual co-
occurrences. To illustrate this, an English news ar-
ticle mentioning ?Bill Gates? and ?Melinda Gates?
evidences a relationship between the two entities,
which can be quantified from their co-occurrences
in the entire English Web corpus. Similarly, we
can mine Chinese news articles to obtain the re-
lationships between ???? and ???H??
?. Once these two bilingual graphs of people and
their relationships are harvested, entity translation
can leverage these parallel relationships to further
evidence the mapping between translation pairs, as
Figure 1(c) illustrates.
To highlight the advantage of our proposed ap-
proach, we compare our results with commercial
machine translators (1) Engkoo3 developed in Mi-
crosoft Research Asia and (2) Google Translator4.
In particular, Figure 2 reports the precision for two
groups? ?heads? that belong to top-100 popular peo-
ple (determined by the number of hits), among ran-
domly sampled 304 people names5 from six graph
pairs of size 1,000 each, and the remaining ?tails?.
Commercial translators such as Google, leveraging
2http://renlifang.msra.cn
3http://www.engkoo.com
4http://translate.google.com
5See Section 4 for the sampling process.
Ours Google Engkoo0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Pre
cis
ion
 
 
Tail
Head
Figure 2: Comparison for Head and Tail datasets
bilingual co-occurrences that are scarce for tails,
show significantly lower precision for tails. Mean-
while, our work, depending solely on monolin-
gual co-occurrences, shows high precision, for both
heads and tails.
Our focus is to boost translation accuracy for
long tails with non-trivial Web occurrences in each
monolingual corpus, but not with much bilingual co-
occurrences, e.g., researchers publishing actively in
two languages but not famous enough to be featured
in multi-lingual Wikipedia entries or news articles.
As existing translators are already highly accurate
for popular heads, this focus well addresses the re-
maining challenges for entity translation.
To summarize, we believe that this paper has the
following contributions:
? We abstract entity translation problem as
a graph mapping between entity-relationship
graphs in two languages.
? We develop an effective matching algo-
rithm leveraging both pronunciation and co-
occurrence similarity. This holistic approach
complements existing approaches and en-
hances the translation coverage and accuracy.
? We validate the effectiveness of our approach
using various real-life datasets.
The rest of this paper is organized as follows. Sec-
tion 2 reviews existing work. Section 3 then devel-
ops our framework. Section 4 reports experimental
results and Section 5 concludes our work.
431
(a) English PeopleEntityCube Ge (b) Chinese Renlifang Gc
(c) Abstracting translation as graph mapping
Figure 1: Illustration of entity-relationship graphs
2 Related Work
In this section, we first survey related efforts, cate-
gorized into transliteration-based and corpus-based
approaches. Our approach leveraging both is com-
plementary to these efforts.
2.1 Transliteration-based Approaches
Many name translations are loosely based on
phonetic similarity, which naturally inspires
transliteration-based translation of finding the
translation with the closest pronunciation similarity,
using either rule-based (Wan and Verspoor, 1998) or
statistical (Knight and Graehl, 1998; Li et al, 2004)
approaches. However, people are free to designate
arbitrary bilingual names of little (or none) pho-
netic similarity, for which the transliteration-based
approach is not effective.
2.2 Corpus-based Approaches
Corpus-based approach can mine arbitrary transla-
tion pairs, by mining bilingual co-occurrences from
parallel and comparable bilingual corpora. Using
parallel corpora (Kupiec, 1993; Feng et al, 2004),
e.g., bilingual Wikipedia entries on the same per-
son, renders high accuracy but suffers from high
scarcity. To alleviate such scarcity, (Fung and Yee,
432
1998; Shao and Ng, 2004) explore a more vast re-
source of comparable corpora, which share no par-
allel document- or sentence-alignments as in paral-
lel corpora but describe similar contents in two lan-
guages, e.g., news articles on the same event. Al-
ternatively, (Lin et al, 2008) extracts bilingual co-
occurrences from bilingual sentences, such as an-
notating terms with their corresponding translations
in English inside parentheses. Similarly, (Jiang et
al., 2009) identifies potential translation pairs from
bilingual sentences using lexical pattern analysis.
2.3 Holistic Approaches
The complementary strength of the above two ap-
proaches naturally calls for a holistic approach,
such as recent work combining transliteration-
and corpus-based similarity mining bilingual co-
occurrences using general search engines. Specifi-
cally, (Al-Onaizan and Knight, 2002) uses translit-
eration to generate candidates and then web corpora
to identify translations. Later, (Jiang et al, 2007)
enhances to use transliteration to guide web mining.
Our work is also a holistic approach, but leverag-
ing significantly larger corpora, specifically by ex-
ploiting monolingual co-occurrences. Such expan-
sion enables to translate ?long-tail? people entities
with non-trivial Web occurrences in each monolin-
gual corpus, but not much bilingual co-occurrences.
Specifically, we initialize name pair similarity using
transliteration-based approach, and iteratively rein-
forces base similarity using relational similarity.
3 Our Framework
Given two graphsGe = (Ve, Ee) andGc = (Vc, Ec)
harvested from English and Chinese corpora respec-
tively, our goal is to find translation pairs, or a set S
of matching node pairs such that S ? Ve ? Vc. Let
R be a |Ve|-by-|Vc| matrix where each Rij denotes
the similarity between two nodes i ? Ve and j ? Vc.
Overall, with the matrix R, our approach consists
of the following three steps, as we will discuss in the
following three sections respectively:
1. Initialization: computing base translation sim-
ilarities Rij between two entity nodes using
transliteration similarity
2. Reinforcement model: reinforcing the trans-
lation similarities Rij by exploiting the mono-
lingual co-occurrences
3. Matching extraction: extracting the matching
pairs from the final translation similarities Rij
3.1 Initialization with Transliteration
We initialize the translation similarity Rij as the
transliteration similarity. This section explains how
to get the transliteration similarity between English
and Chinese names using an unsupervised approach.
Formally, let an English name Ne =
(e1, e2, ? ? ? , en) and a Chinese name Nc =
(c1, c2, ? ? ? , cm) be given, where ei is an English
word and Ne is a sequence of the words, and ci
is a Chinese character and Nc is a sequence of
the characters. Our goal is to compute a score
indicating the similarity between the pronunciations
of the two names.
We first convert Nc into its Pinyin representation
PYc = (s1, s2, ? ? ? , sm), where si is the Pinyin rep-
resentation of ci. Pinyin is the romanization rep-
resentation of pronunciation of Chinese character.
For example, the Pinyin representation of Ne =
(?Barack?, ?Obama?) is PYc =(?ba?, ?la?, ?ke?,
?ao?, ?ba?, ?ma?). The Pinyin representations of
Chinese characters can be easily obtained from Chi-
nese character pronunciation dictionary. In our ex-
periments, we use an in-house dictionary, which
contains pronunciations of 20, 774 Chinese charac-
ters. For the Chinese characters having multiple pro-
nunciations, we only use the most popular one.
Calculation of transliteration similarity between
Ne and Nc is now transformed to calculation of pro-
nunciation similarity between Ne and PYc. Because
letters in Chinese Pinyins and English strings are
pronounced similarly, we can further approximate
pronunciation similarity between Ne and PYc us-
ing their spelling similarity. In this paper, we use
Edit Distance (ED) to measure the spelling similar-
ity. Moreover, since words in Ne are transliterated
into characters in PYc independently, it is more ac-
curate to compute the ED between Ne and PYc, i.e.,
EDname(Ne, PYc), as the sum of the EDs of all
component transliteration pairs, i.e., every ei in Ne
and its corresponding transliteration (si) in PYc. In
other words, we need to first align all sj?s in PYc
with corresponding ei in Ne based on whether they
433
are translations of each other. Then based on the
alignment, we can calculate EDname(Ne, PYc) us-
ing the following formula.
EDname(Ne, PYc) =
?
i
ED(ei, esi) (1)
where esi is a string generated by concatenating all
si?s that are aligned to ei and ED(ei, esi) is the
Edit Distance between ei and esi, i.e., the mini-
mum number of edit operations (including inser-
tion, deletion and substitution) needed to transform
ei into esi. Because an English word usually con-
sists of multiple syllables but every Chinese charac-
ter consists of only one syllable, when aligning ei?s
with sj?s, we add the constraint that each ei is al-
lowed to be aligned with 0 to 4 si?s but each si can
only be aligned with 0 to 1 ei. To get the align-
ment between PYc and Ne which has the minimal
EDname(Ne, PYc), we use a Dynamic Program-
ming based algorithm as defined in the following
formula:
EDname(N1,ie , PY 1,jc ) = min(
EDname(N1,i?1e , PY 1,jc ) + Len(ei),
EDname(N1,ie , PY 1,j?1c ) + Len(sj),
EDname(N1,i?1e , PY 1,j?1c ) + ED(ei, sj),
EDname(N1,i?1e , PY 1,j?2c ) + ED(ei, PY j?1,jc ),
EDname(N1,i?1e , PY 1,j?3c ) + ED(ei, PY j?2,jc ),
EDname(N1,i?1e , PY 1,j?4c ) + ED(ei, PY j?3,jc ))
where, given a sequence X = (x1, x2, ? ? ?)
such that xi is a word, X i,j is the subsequence
(xi, xi+1, ? ? ? , xj) of X and Len(X) is the number
of letters except spaces in the sequence X . For in-
stance, the minimal Edit Distance between the En-
glish name ?Barack Obama? and the Chinese Pinyin
representation ?ba la ke ao ba ma? is 4, as the
best alignment is: ?Barack? ? ?ba la ke? (ED: 3),
?Obama?? ?ao ba ma? (ED: 1). Finally the translit-
eration similarity between Nc and Ne is calculated
using the following formula.
Simtl(Nc, Ne) = 1?
EDname(Ne, PYc)
Len(PYc) + Len(Ne)
(2)
For example, Simtl(?Barack Obama?, ??n
.???j?) is 1? 411+12 = 0.826.
3.2 Reinforcement Model
From the initial similarity, we model our problem as
an iterative approach that iteratively reinforces the
similarityRij of the nodes i and j from the matching
similarities of their neighbor nodes u and v.
The basic intuition is built on exploiting the sim-
ilarity between monolingual co-occurrences of two
different languages. In particular, we assume two
entities with strong relationship co-occur frequently
in both corpora. In order to express this intuition, we
formally define an iterative reinforcement model as
follows. Let Rtij denote the similarity of nodes i and
j at t-th iteration:
Rt+1ij = ?
?
(u,v)k?Bt(i,j,?)
Rtuv
2k
+ (1? ?)R0ij (3)
The model is expressed as a linear combination
of (a) the relational similarity
?
Rtuv/2k and (b)
transliteration similarity R0ij . (? is the coefficient
for interpolating two similarities.)
In the relational similarity, Bt(i, j, ?) is an or-
dered set of the best matching pairs between neigh-
bor nodes of i and ones of j such that ?(u, v)k ?
Bt(i, j, ?), Rtuv ? ?, where (u, v)k is the match-
ing pair with k-th highest similarity score. We con-
sider (u, v) with similarity over some threshold ?,
or Rtuv ? ?, as a matching pair. In this neighbor
matching process, if many-to-many matches exist,
we select only one with the greatest matching score.
Figure 3 describes such matching process more for-
mally. N(i) andN(j) are the sets of neighbor nodes
of i and j, respectively, and H is a priority queue
sorting pairs in the decreasing order of similarity
scores.
Meanwhile, note that, in order to express that
the confidence for matching (i, j) progressively con-
verges as the number of matched neighbors in-
creases, we empirically use decaying coefficient
1/2k for Rtuv, because
??
k=1 1/2k = 1.
3.3 Matching Extraction
After the convergence of the above model, we get
the |Ve|-by-|Vc| similarity matrix R?. From this
matrix, we extract one-to-one matches maximizing
the overall similarity.
More formally, this problem can be stated as
the maximum weighted bipartite matching (West,
434
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
Bt(i, j, ?)? {}
?u ? N(i),?v ? N(j) : H.push(u, v;Rtuv)
while H is not empty do
(u, v; s)? H.pop()
if s < ? then
break
end if
if neither u nor v are matched yet then
Bt(i, j, ?)? Bt(i, j, ?) ? {(u, v)}
end if
end while
return Bt(i, j, ?)
Figure 3: How to get the ordered set Bt(i, j, ?)
2000)? Given two groups of entities Ve and Vc from
the two graphs Ge and Gc, we can build a weighted
bipartite graph is G = (V,E), where V = Ve ? Vc
and E is a set of edges (u, v) with weight R?uv. To
filter out null alignment, we construct only the edges
with weight R?uv ? ?. From this bipartite graph,
the maximum weighted bipartite matching problem
finds a set of pairwise non-adjacent edges S ? E
such that
?
(u,v)?S R?uv is the maximum. Well-
known algorithms include Hungarian algorithm with
time complexity of O(|V |2 log |V |+ |V ||E|) (West,
2000).
In this paper, to speed up processing, we consider
a greedy alternative with the following steps? (1)
choose the pair with the highest similarity score, (2)
remove the corresponding row and column from the
matrix, and (3) repeat (1) and (2) until their match-
ing scores are over a specific threshold ?.
4 Experiments
This section reports our experimental results to eval-
uate our proposed approach. First, we report our ex-
perimental setting in Section 4.1. Second, we vali-
date the effectiveness and the scalability of our ap-
proach over a real-life dataset in Section 4.2.
4.1 Experimental Settings
This section describes (1) how we collect the En-
glish and Chinese EntityCube datasets, (2) how to
build ground-truth test datasets for evaluating our
framework, and (3) how to set up three parameters
?, ?, and ?.
First, we crawled Ge = (Ve, Ee) and Gc =
(Vc, Ec) from English and Chinese EntityCubes.
Specifically, we built a graph pairs (Ge, Gc) expand-
ing from a ?seed pair? of nodes se ? Ve and sc ? Vc
until the number of nodes for each graph becomes
1,0006. More specifically, when we build a graph
Ge by expanding from se, we use a queue Q. We
first initialize Q by pushing the seed node se. We
then iteratively pop a node ve from Q, save ve into
Ve, and push its neighbor nodes in decreasing order
of co-occurrence scores with ve. Similarly, we can
get Gc from a counterpart seed node vc. By using
this procedure, we built six graph pairs from six dif-
ferent seed pairs. In particular, the six seed nodes
are English names and its corresponding Chinese
names representing a wide range of occupation do-
mains (e.g., ?Barack Obama,? ?Bill Gates,? ?Britney
Spears,? ?Bruno Senna,? ?Chris Paul,? and ?Eminem?)
as Table 1 depicts. Meanwhile, though we demon-
strate the effectiveness of the proposed method for
mining name translations in Chinese and English
languages, the method can be easily adapted to other
language pairs.
Table 1: Summary for graphs and test datasets obtained
from each seed pair
i |Ve|, |Vc| |Ti| English Name Chinese Name
1 1,000 51 Barack Obama ?n.???j
2 1,000 52 Bill Gates ??
3 1,000 40 Britney Spears Y}????
4 1,000 53 Bruno Senna Y0L??
5 1,000 51 Chris Paul .????[
6 1,000 57 Eminem ???
Second, we manually searched for about 50
?ground-truth? matched translations for each graph
pair to build test datasets Ti, by randomly selecting
nodes within two hops7 from the seed pair (se, sc),
since nodes outside two hops may include nodes
whose neighbors are not fully crawled. More specif-
ically, due to our crawling process expanding to add
neighbors from the seed, the nodes close to the seed
have all the neighbors they would have in the full
graph, while those far from the node may not. In or-
der to pick the nodes that well represent the actual
6Note, this is just a default setting, which we later increase
for scalability evaluation in Figure 6.
7Note that the numbers of nodes within two hops in Ge and
Gc are 327 and 399 on average respectively.
435
neighbors, we built test datasets among those within
two hops. However, this crawling is used for the
evaluation sake only, and thus does not suggest the
bias in our proposed framework. Table 1 describes
the size of such test dataset for each graph pair.
Lastly, we set up the three parameters ?, ?, and
? using 6-fold cross validation with 6 test datasets
Ti?s of the graphs. More specifically, for each
dataset Ti, we decide ?i and ?i such that average
MRR for the other 5 test datasets is maximized.
(About MRR, see more details of Equation (4) in
Section 4.2.) We then decide ?i such that average
F1-score is maximized. Figure 4 shows the average
MRR for ?i and ?i with default values ? = 0.66
and ? = 0.2. Based on these results, we set ?i with
values {0.2, 0.15, 0.2, 0.15, 0.2, 0.15} that optimize
MRR in datasets T1, . . . T6, and similarly ?i with
{0.67, 0.65, 0.67, 0.67, 0.65, 0.67}. We also set ?i
with values {0.63, 0.63, 0.61, 0.61, 0.61, 0.61} opti-
mizing F1-score with the same default values ? =
0.2 and ? = 0.66. We can observe the variances
of optimal parameter setting values are low, which
suggests the robustness of our framework.
4.2 Experimental Results
This section reports our experimental results using
the evaluation datasets explained in previous sec-
tion. For each graph pair, we evaluated the ef-
fectiveness of (1) reinforcement model using MRR
measure in Section 4.2.1 and (2) overall framework
using precision, recall, and F1 measures in Sec-
tion 4.2.2. We also validated (3) scalability of our
framework over larger scale of graphs (with up to
five thousand nodes) in Section 4.2.3. (In all experi-
mental results, Bold numbers indicate the best per-
formance for each metric.)
4.2.1 Effectiveness of reinforcement model
We evaluated the reinforcement model over
MRR (Voorhees, 2001), the average of the recipro-
cal ranks of the query results as the following for-
mula:
MRR = 1
|Q|
?
q?Q
1
rankq
(4)
Each q is a ground-truth matched pair (u, v) such
that u ? Ve and v ? Vc, and rankq is the rank of the
similarity score of Ruv among all Ruk?s such that
k ? Vc. Q is a set of such queries. By comparing
MRRs for two matricesR0 andR?, we can validate
the effectiveness of the reinforcement model.
? Baseline matrix (R0): using only the translit-
eration similarity score, i.e., without reinforce-
ment
? Reinforced matrix (R?): using the reinforced
similarity score obtained from Equation (3)
Table 2: MRR of baseline and reinforced matrices
Set MRRBaseline R0 Reinforced R?
T1 0.6964 0.8377
T2 0.6213 0.7581
T3 0.7095 0.7989
T4 0.8159 0.8378
T5 0.6984 0.8158
T6 0.5982 0.8011
Average 0.6900 0.8082
We empirically observed that the iterative model
converges within 5 iterations. In all experiments, we
used 5 iterations for the reinforcement.
Table 2 summarizes our experimental results. As
these figures show, MRR scores significantly in-
crease after applying our reinforcement model ex-
cept for the set T4 (on average from 69% to 81%),
which indirectly shows the effectiveness of our rein-
forcement model.
4.2.2 Effectiveness of overall framework
Based on the reinforced matrix, we evaluated
the effectiveness of our overall matching framework
using the following three measures?(1) precision:
how accurately the method returns matching pairs,
(2) recall: how many the method returns correct
matching pairs, and (3) F1-score: the harmonic
mean of precision and recall. We compared our ap-
proach with a baseline, mapping two graphs with
only transliteration similarity.
? Baseline: in matching extraction, using R0 as
the similarity matrix by bypassing the rein-
forcement step
? Ours: using R?, the similarity matrix con-
verged by Equation (3)
436
0.1 0.15 0.2 0.25 0.30.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
?(?=0.66)
AVG
(MR
R)
 
 
?1?2?3?4?5?6
0.61 0.63 0.65 0.67 0.690.74
0.76
0.78
0.8
0.82
0.84
? (?=0.2)
AVG
(MR
R)
 
 
?1
?2
?3
?4
?5
?6
0.57 0.59 0.61 0.63 0.650.68
0.69
0.7
0.71
0.72
0.73
0.74
?(?=0.2, ?=0.66)
AVG
(F1?
scor
e)
 
 
?1?2?3?4?5?6
Figure 4: Parameter setup for ?, ?, and ?
In addition, we compared ours with the machine
translators of Engkoo and Google. Table 3 summa-
rizes our experimental results.
As this table shows, our approach results in the
highest precision (about 80% on average) without
compromising the best recall of Google, i.e., 61%
of Google vs. 63% of ours. Overall, our approach
outperforms others in all three measures.
Meanwhile, in order to validate the translation ac-
curacy over popular head and long-tail, as discussed
in Section 1, we separated the test data into two
groups and analyzed the effectiveness separately.
Figure 5 plots the number of hits returned for the
names from Google search engine. According to the
distribution, we separate the test data into top-100
popular people with the highest hits and the remain-
ing, denoted head and tail, respectively.
0 50 100 150 200 250 300 35010
4
105
106
107
108
Number of names
Nu
mb
er o
f hi
ts i
n G
oog
le
Figure 5: Distribution over number of hits
Table 4 shows the effectiveness with both
datasets, respectively. As difference of the effective-
ness between tail and head (denoted diff ) with re-
spect to three measures shows, our approach shows
stably high precision, for both heads and tails.
4.2.3 Scalability
To validate the scalability of our approach, we
evaluated the effectiveness of our approach over the
number of nodes in two graphs. We built larger six
graph pairs (Ge, Gc) by expanding them from the
seed pairs further until the number of nodes reaches
5,000. Figure 6 shows the number of matched trans-
lations according to such increase. Overall, the num-
ber of matched pairs linearly increases as the num-
ber of nodes increases, which suggests scalability.
The ratio of node overlap in two graphs is about be-
tween 7% and 9% of total node size.
1000 2000 3000 4000 500050
100
150
200
250
300
350
|Ve| and |Vc|
# m
atc
hed
 tra
nsla
tion
s
Figure 6: Matched translations over |Ve| and |Vc|
5 Conclusion
This paper abstracted name translation problem as a
matching problem of two entity-relationship graphs.
This novel approach complements existing name
translation work, by not requiring rare resources
of parallel or comparable corpus yet outperforming
the state-of-the-art. More specifically, we combine
bilingual phonetic similarity and monolingual Web
co-occurrence similarity, to compute a holistic no-
tion of entity similarity. To achieve this goal, we de-
437
Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets Ti
Set Precision Recall F1-scoreEngkoo Google Baseline Ours Engkoo Google Baseline Ours Engkoo Google Baseline Ours
T1 0.5263 0.4510 0.5263 0.8974 0.3922 0.4510 0.1961 0.6863 0.4494 0.4510 0.2857 0.7778
T2 0.7551 0.75 0.7143 0.8056 0.7115 0.75 0.2885 0.5577 0.7327 0.75 0.4110 0.6591
T3 0.5833 0.7925 0.5556 0.7949 0.5283 0.7925 0.1887 0.5849 0.5545 0.7925 0.2817 0.6739
T4 0.5 0.45 0.7368 0.7353 0.425 0.45 0.35 0.625 0.4595 0.45 0.4746 0.6757
T5 0.6111 0.3137 0.5 0.7234 0.4314 0.3137 0.1765 0.6667 0.5057 0.3137 0.2609 0.6939
T6 0.5636 0.8947 0.6 0.8605 0.5438 0.8947 0.1053 0.6491 0.5536 0.8947 0.1791 0.74
AVG 0.5899 0.6086 0.6055 0.8028 0.5054 0.6086 0.2175 0.6283 0.5426 0.6086 0.3155 0.7034
Table 4: Precision, Recall, and F1-score of Engkoo, Google, and Ours with head and tail datasets
Method Precision Recall F1-scorehead tail diff head tail diff head tail diff
Engkoo 0.6082 0.5854 0.0229 0.59 0.4706 0.1194 0.5990 0.5217 0.0772
Google 0.75 0.5588 0.1912 0.75 0.5588 0.1912 0.75 0.5588 0.1912
Ours 0.8462 0.7812 0.0649 0.66 0.6127 0.0473 0.7416 0.6868 0.0548
veloped a graph alignment algorithm that iteratively
reinforces the matching similarity exploiting rela-
tional similarity and then extracts correct matches.
Our evaluation results empirically validated the ac-
curacy of our algorithm over real-life datasets, and
showed the effectiveness on our proposed perspec-
tive.
Acknowledgments
This work was supported by Microsoft Research
Asia NLP theme funding and MKE (Ministry of
Knowledge Economy), Korea, under the ITRC (In-
formation Technology Research Center) support
program supervised by the IITA (Institute for In-
formation Technology Advancement) (IITA-2009-
C1090-0902-0045).
References
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating Named Entities Using Monolingual and Bilin-
gual Resources. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics
(ACL?02), pages 400?408. Association for Computa-
tional Linguistics.
Donghui Feng, Yajuan Lu?, and Ming Zhou. 2004.
A New Approach for English-Chinese Named En-
tity Alignment. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?04), pages 372?379. Association for Com-
putational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR Ap-
proach for Translating New Words from Nonparal-
lel,Comparable Texts. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
(COLING?98), pages 414?420. Association for Com-
putational Linguistics.
Long Jiang, Ming Zhou, Lee feng Chien, and Cheng Niu.
2007. Named Entity Translation withWebMining and
Transliteration. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI?07), pages 1629?1634. Morgan Kaufmann Pub-
lishers Inc.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and
Qingsheng Zhu. 2009. Mining Bilingual Data from
the Web with Adaptively Learnt Patterns. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL?09), pages 870?878.
Association for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine Transliteration. Computational Linguistics,
24(4):599?612.
Julian Kupiec. 1993. An Algorithm for finding Noun
Phrase Correspondences in Bilingual Corpora. In Pro-
ceedings of the 31th Annual Meeting of the Association
for Computational Linguistics (ACL?93), pages 17?22.
Association for Computational Linguistics.
Haizhou Li, Zhang Min, and Su Jian. 2004. A Joint
Source-Channel Model for Machine Transliteration.
In Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics (ACL?04), pages
159?166. Association for Computational Linguistics.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining Parenthetical Transla-
438
tions from the Web by Word Alignment. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics (ACL?08), pages 994?
1002. Association for Computational Linguistics.
Li Shao and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Computa-
tional Linguistics (COLING?04), pages 618?624. As-
sociation for Computational Linguistics.
Ellen M. Voorhees. 2001. The trec question answering
track. Natural Language Engineering, 7(4):361?378.
Stephen Wan and Cornelia Maria Verspoor. 1998. Auto-
matic English-Chinese Name Transliteration for De-
velopment of Multilingual Resources. In Proceed-
ings of the 17th International Conference on Compu-
tational Linguistics (COLING?98), pages 1352?1356.
Association for Computational Linguistics.
Douglas Brent West. 2000. Introduction to Graph The-
ory. Prentice Hall, second edition.
439
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 151?160,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Target-dependent Twitter Sentiment Classification 
 
 
Long Jiang1   Mo Yu2   Ming Zhou1   Xiaohua Liu1   Tiejun Zhao2 
1 Microsoft Research Asia 2 School of Computer Science & Technology 
Beijing, China Harbin Institute of Technology 
 Harbin, China 
{longj,mingzhou,xiaoliu}@microsoft.com {yumo,tjzhao}@mtlab.hit.edu.cn 
 
  
  
 
 
Abstract 
Sentiment analysis on Twitter data has attract-
ed much attention recently. In this paper, we 
focus on target-dependent Twitter sentiment 
classification; namely, given a query, we clas-
sify the sentiments of the tweets as positive, 
negative or neutral according to whether they 
contain positive, negative or neutral senti-
ments about that query. Here the query serves 
as the target of the sentiments. The state-of-
the-art approaches for solving this problem 
always adopt the target-independent strategy, 
which may assign irrelevant sentiments to the 
given target. Moreover, the state-of-the-art 
approaches only take the tweet to be classified 
into consideration when classifying the senti-
ment; they ignore its context (i.e., related 
tweets). However, because tweets are usually 
short and more ambiguous, sometimes it is not 
enough to consider only the current tweet for 
sentiment classification. In this paper, we pro-
pose to improve target-dependent Twitter sen-
timent classification by 1) incorporating 
target-dependent features; and 2) taking relat-
ed tweets into consideration. According to the 
experimental results, our approach greatly im-
proves the performance of target-dependent 
sentiment classification. 
1 Introduction 
Twitter, as a micro-blogging system, allows users 
to publish tweets of up to 140 characters in length 
to tell others what they are doing, what they are 
thinking, or what is happening around them. Over 
the past few years, Twitter has become very popu-
lar. According to the latest Twitter entry in Wik-
ipedia, the number of Twitter users has climbed to 
190 million and the number of tweets published on 
Twitter every day is over 65 million1.  
As a result of the rapidly increasing number of 
tweets, mining people?s sentiments expressed in 
tweets has attracted more and more attention. In 
fact, there are already many web sites built on the 
Internet providing a Twitter sentiment search ser-
vice, such as Tweetfeel2 , Twendz3 , and Twitter 
Sentiment4. In those web sites, the user can input a 
sentiment target as a query, and search for tweets 
containing positive or negative sentiments towards 
the target. The problem needing to be addressed 
can be formally named as Target-dependent Sen-
timent Classification of Tweets; namely, given a 
query, classifying the sentiments of the tweets as 
positive, negative or neutral according to whether 
they contain positive, negative or neutral senti-
ments about that query. Here the query serves as 
the target of the sentiments. 
The state-of-the-art approaches for solving this 
problem, such as (Go et al, 20095; Barbosa and 
Feng, 2010), basically follow (Pang et al, 2002), 
who utilize machine learning based classifiers for 
the sentiment classification of texts. However, their 
classifiers actually work in a target-independent 
way: all the features used in the classifiers are in-
dependent of the target, so the sentiment is decided 
no matter what the target is. Since (Pang et al, 
2002) (or later research on sentiment classification 
                                                          
1 http://en.wikipedia.org/wiki/Twitter 
2 http://www.tweetfeel.com/ 
3 http://twendz.waggeneredstrom.com/ 
4 http://twittersentiment.appspot.com/ 
5 The algorithm used in Twitter Sentiment 
151
of product reviews) aim to classify the polarities of 
movie (or product) reviews and each movie (or 
product) review is assumed to express sentiments 
only about the target movie (or product), it is rea-
sonable for them to adopt the target-independent 
approach. However, for target-dependent sentiment 
classification of tweets, it is not suitable to exactly 
adopt that approach. Because people may mention 
multiple targets in one tweet or comment on a tar-
get in a tweet while saying many other unrelated 
things in the same tweet, target-independent ap-
proaches are likely to yield unsatisfactory results:  
1. Tweets that do not express any sentiments 
to the given target but express sentiments 
to other things will be considered as being 
opinionated about the target. For example, 
the following tweet expresses no sentiment 
to Bill Gates but is very likely to be classi-
fied as positive about Bill Gates by target-
independent approaches. 
"People everywhere love Windows & vista. 
Bill Gates" 
2. The polarities of some tweets towards the 
given target are misclassified because of 
the interference from sentiments towards 
other targets in the tweets. For example, 
the following tweet expresses a positive 
sentiment to Windows 7 and a negative 
sentiment to Vista. However, with target-
independent sentiment classification, both 
of the targets would get positive polarity. 
?Windows 7 is much better than Vista!? 
In fact, it is easy to find many such cases by 
looking at the output of Twitter Sentiment or other 
Twitter sentiment analysis web sites. Based on our 
manual evaluation of Twitter Sentiment output, 
about 40% of errors are because of this (see Sec-
tion 6.1 for more details).  
In addition, tweets are usually shorter and more 
ambiguous than other sentiment data commonly 
used for sentiment analysis, such as reviews and 
blogs. Consequently, it is more difficult to classify 
the sentiment of a tweet only based on its content. 
For instance, for the following tweet, which con-
tains only three words, it is difficult for any exist-
ing approaches to classify its sentiment correctly. 
?First game: Lakers!? 
However, relations between individual tweets 
are more common than those in other sentiment 
data. We can easily find many related tweets of a 
given tweet, such as the tweets published by the 
same person, the tweets replying to or replied by 
the given tweet, and retweets of the given tweet. 
These related tweets provide rich information 
about what the given tweet expresses and should 
definitely be taken into consideration for classify-
ing the sentiment of the given tweet. 
In this paper, we propose to improve target-
dependent sentiment classification of tweets by 
using both target-dependent and context-aware 
approaches. Specifically, the target-dependent ap-
proach refers to incorporating syntactic features 
generated using words syntactically connected 
with the given target in the tweet to decide whether 
or not the sentiment is about the given target. For 
instance, in the second example, using syntactic 
parsing, we know that ?Windows 7? is connected 
to ?better? by a copula, while ?Vista? is connected 
to ?better? by a preposition. By learning from 
training data, we can probably predict that ?Win-
dows 7? should get a positive sentiment and 
?Vista? should get a negative sentiment.  
In addition, we also propose to incorporate the 
contexts of tweets into classification, which we call 
a context-aware approach. By considering the sen-
timent labels of the related tweets, we can further 
boost the performance of the sentiment classifica-
tion, especially for very short and ambiguous 
tweets. For example, in the third example we men-
tioned above, if we find that the previous and fol-
lowing tweets published by the same person are 
both positive about the Lakers, we can confidently 
classify this tweet as positive. 
The remainder of this paper is structured as fol-
lows. In Section 2, we briefly summarize related 
work. Section 3 gives an overview of our approach. 
We explain the target-dependent and context-
aware approaches in detail in Sections 4 and 5 re-
spectively. Experimental results are reported in 
Section 6 and Section 7 concludes our work. 
2 Related Work  
In recent years, sentiment analysis (SA) has be-
come a hot topic in the NLP research community. 
A lot of papers have been published on this topic. 
152
2.1 Target-independent SA 
Specifically, Turney (2002) proposes an unsuper-
vised method for classifying product or movie re-
views as positive or negative. In this method, 
sentimental phrases are first selected from the re-
views according to predefined part-of-speech pat-
terns. Then the semantic orientation score of each 
phrase is calculated according to the mutual infor-
mation values between the phrase and two prede-
fined seed words. Finally, a review is classified 
based on the average semantic orientation of the 
sentimental phrases in the review. 
In contrast, (Pang et al, 2002) treat the senti-
ment classification of movie reviews simply as a 
special case of a topic-based text categorization 
problem and investigate three classification algo-
rithms: Naive Bayes, Maximum Entropy, and Sup-
port Vector Machines. According to the 
experimental results, machine learning based clas-
sifiers outperform the unsupervised approach, 
where the best performance is achieved by the 
SVM classifier with unigram presences as features. 
2.2 Target-dependent SA 
Besides the above mentioned work for target-
independent sentiment classification, there are also 
several approaches proposed for target-dependent 
classification, such as (Nasukawa and Yi, 2003; 
Hu and Liu, 2004; Ding and Liu, 2007). (Nasuka-
wa and Yi, 2003) adopt a rule based approach, 
where rules are created by humans for adjectives, 
verbs, nouns, and so on. Given a sentiment target 
and its context, part-of-speech tagging and de-
pendency parsing are first performed on the con-
text. Then predefined rules are matched in the 
context to determine the sentiment about the target. 
In (Hu and Liu, 2004), opinions are extracted from 
product reviews, where the features of the product 
are considered opinion targets. The sentiment 
about each target in each sentence of the review is 
determined based on the dominant orientation of 
the opinion words appearing in the sentence. 
As mentioned in Section 1, target-dependent 
sentiment classification of review sentences is 
quite different from that of tweets. In reviews, if 
any sentiment is expressed in a sentence containing 
a feature, it is very likely that the sentiment is 
about the feature. However, the assumption does 
not hold in tweets. 
2.3 SA of Tweets 
As Twitter becomes more popular, sentiment anal-
ysis on Twitter data becomes more attractive. (Go 
et al, 2009; Parikh and Movassate, 2009; Barbosa 
and Feng, 2010; Davidiv et al, 2010) all follow the 
machine learning based approach for sentiment 
classification of tweets. Specifically, (Davidiv et 
al., 2010) propose to classify tweets into multiple 
sentiment types using hashtags and smileys as la-
bels. In their approach, a supervised KNN-like 
classifier is used. In contrast, (Barbosa and Feng, 
2010) propose a two-step approach to classify the 
sentiments of tweets using SVM classifiers with 
abstract features. The training data is collected 
from the outputs of three existing Twitter senti-
ment classification web sites. As mentioned above, 
these approaches work in a target-independent way, 
and so need to be adapted for target-dependent sen-
timent classification. 
3 Approach Overview  
The problem we address in this paper is target-
dependent sentiment classification of tweets. So 
the input of our task is a collection of tweets con-
taining the target and the output is labels assigned 
to each of the tweets. Inspired by (Barbosa and 
Feng, 2010; Pang and Lee, 2004), we design a 
three-step approach in this paper:  
1. Subjectivity classification as the first step 
to decide if the tweet is subjective or neu-
tral about the target;  
2. Polarity classification as the second step to 
decide if the tweet is positive or negative 
about the target if it is classified as subjec-
tive in Step 1;  
3. Graph-based optimization as the third step 
to further boost the performance by taking 
the related tweets into consideration.  
In each of the first two steps, a binary SVM 
classifier is built to perform the classification. To 
train the classifiers, we use SVM-Light 6  with a 
linear kernel; the default setting is adopted in all 
experiments. 
                                                          
6 http://svmlight.joachims.org/ 
153
3.1 Preprocessing 
In our approach, rich feature representations are 
used to distinguish between sentiments expressed 
towards different targets. In order to generate such 
features, much NLP work has to be done before-
hand, such as tweet normalization, POS tagging, 
word stemming, and syntactic parsing.  
In our experiments, POS tagging is performed 
by the OpenNLP POS tagger7. Word stemming is 
performed by using a word stem mapping table 
consisting of about 20,000 entries. We also built a 
simple rule-based model for tweet normalization 
which can correct simple spelling errors and varia-
tions into normal form, such as ?gooood? to 
?good? and ?luve? to ?love?. For syntactic parsing 
we use a Maximum Spanning Tree dependency 
parser (McDonald et al, 2005). 
3.2 Target-independent Features 
Previous work (Barbosa and Feng, 2010; Davidiv 
et al, 2010) has discovered many effective features 
for sentiment analysis of tweets, such as emoticons, 
punctuation, prior subjectivity and polarity of a 
word. In our classifiers, most of these features are 
also used. Since these features are all generated 
without considering the target, we call them target-
independent features. In both the subjectivity clas-
sifier and polarity classifier, the same target-
independent feature set is used. Specifically, we 
use two kinds of target-independent features: 
1. Content features, including words, punctu-
ation, emoticons, and hashtags (hashtags 
are provided by the author to indicate the 
topic of the tweet). 
2. Sentiment lexicon features, indicating how 
many positive or negative words are in-
cluded in the tweet according to a prede-
fined lexicon. In our experiments, we use 
the lexicon downloaded from General In-
quirer8. 
4 Target-dependent Sentiment Classifica-
tion  
Besides target-independent features, we also incor-
porate target-dependent features in both the subjec-
                                                          
7 http://opennlp.sourceforge.net/projects.html 
8 http://www.wjh.harvard.edu/~inquirer/ 
tivity classifier and polarity classifier. We will ex-
plain them in detail below. 
4.1 Extended Targets 
It is quite common that people express their senti-
ments about a target by commenting not on the 
target itself but on some related things of the target. 
For example, one may express a sentiment about a 
company by commenting on its products or tech-
nologies. To express a sentiment about a product, 
one may choose to comment on the features or 
functionalities of the product. It is assumed that 
readers or audiences can clearly infer the sentiment 
about the target based on those sentiments about 
the related things. As shown in the tweet below, 
the author expresses a positive sentiment about 
?Microsoft? by expressing a positive sentiment 
directly about ?Microsoft technologies?. 
?I am passionate about Microsoft technologies 
especially Silverlight.? 
In this paper, we define those aforementioned 
related things as Extended Targets. Tweets ex-
pressing positive or negative sentiments towards 
the extended targets are also regarded as positive 
or negative about the target. Therefore, for target-
dependent sentiment classification of tweets, the 
first thing is identifying all extended targets in the 
input tweet collection.  
In this paper, we first regard all noun phrases, 
including the target, as extended targets for sim-
plicity. However, it would be interesting to know 
under what circumstances the sentiment towards 
the target is truly consistent with that towards its 
extended targets. For example, a sentiment about 
someone?s behavior usually means a sentiment 
about the person, while a sentiment about some-
one?s colleague usually has nothing to do with the 
person. This could be a future work direction for 
target-dependent sentiment classification. 
In addition to the noun phrases including the 
target, we further expand the extended target set 
with the following three methods:  
1. Adding mentions co-referring to the target 
as new extended targets. It is common that 
people use definite or demonstrative noun 
phrases or pronouns referring to the target 
in a tweet and express sentiments directly 
on them. For instance, in ?Oh, Jon Stewart. 
How I love you so.?, the author expresses 
154
a positive sentiment to ?you? which actual-
ly refers to ?Jon Stewart?. By using a sim-
ple co-reference resolution tool adapted 
from (Soon et al, 2001), we add all the 
mentions referring to the target into the ex-
tended target set. 
2. Identifying the top K nouns and noun 
phrases which have the strongest associa-
tion with the target. Here, we use 
Pointwise Mutual Information (PMI) to 
measure the association. 
)()(
),(log),( tpwp
twptwPMI ?
 
Where p(w,t), p(w), and p(t) are probabili-
ties of w and t co-occurring, w appearing, 
and t appearing in a tweet respectively. In 
the experiments, we estimate them on a 
tweet corpus containing 20 million tweets. 
We set K = 20 in the experiments based on 
empirical observations. 
3. Extracting head nouns of all extended tar-
gets, whose PMI values with the target are 
above some predefined threshold, as new 
extended targets. For instance, suppose we 
have found ?Microsoft Technologies? as 
the extended target, we will further add 
?technologies? into the extended target set 
if the PMI value for ?technologies? and 
?Microsoft? is above the threshold. Simi-
larly, we can find ?price? as the extended 
targets for ?iPhone? from ?the price of 
iPhone? and ?LoveGame? for ?Lady Ga-
ga? from ?LoveGame by Lady Gaga?. 
4.2 Target-dependent Features 
Target-dependent sentiment classification needs to 
distinguish the expressions describing the target 
from other expressions. In this paper, we rely on 
the syntactic parse tree to satisfy this need. Specif-
ically, for any word stem wi in a tweet which has 
one of the following relations with the given target 
T or any from the extended target set, we generate 
corresponding target-dependent features with the 
following rules:  
? wi is a transitive verb and T (or any of the 
extended target) is its object; we generate a 
feature wi _arg2. ?arg? is short for ?argu-
ment?. For example, for the target iPhone 
in ?I love iPhone?, we generate 
?love_arg2? as a feature. 
? wi is a transitive verb and T (or any of the 
extended target) is its subject; we generate 
a feature wi_arg1 similar to Rule 1. 
? wi is a intransitive verb and T (or any of the 
extended target) is its subject; we generate 
a feature wi_it_arg1. 
?  wi is an adjective or noun and T (or any of 
the extended target) is its head; we gener-
ate a feature wi_arg1. 
?  wi is an adjective or noun and it (or its 
head) is connected by a copula with T (or 
any of the extended target); we generate a 
feature wi_cp_arg1. 
? wi is an adjective or intransitive verb ap-
pearing alone as a sentence and T (or any 
of the extended target) appears in the pre-
vious sentence; we generate a feature 
wi_arg. For example, in ?John did that. 
Great!?, ?Great? appears alone as a sen-
tence, so we generate ?great_arg? for the 
target ?John?. 
? wi is an adverb, and the verb it modifies 
has T (or any of the extended target) as its 
subject; we generate a feature arg1_v_wi. 
For example, for the target iPhone in the 
tweet ?iPhone works better with the Cell-
Band?, we will generate the feature 
?arg1_v_well?. 
Moreover, if any word included in the generated 
target-dependent features is modified by a nega-
tion9, then we will add a prefix ?neg-? to it in the 
generated features. For example, for the target iPh-
one in the tweet ?iPhone does not work better with 
the CellBand?, we will generate the features 
?arg1_v_neg-well? and ?neg-work_it_arg1?. 
To overcome the sparsity of target-dependent 
features mentioned above, we design a special bi-
nary feature indicating whether or not the tweet 
contains at least one of the above target-dependent 
features. Target-dependent features are binary fea-
tures, each of which corresponds to the presence of 
the feature in the tweet. If the feature is present, the 
entry will be 1; otherwise it will be 0. 
                                                          
9 Seven negations are used in the experiments: not, no, never, 
n?t, neither, seldom, hardly. 
155
5 Graph-based Sentiment Optimization  
As we mentioned in Section 1, since tweets are 
usually shorter and more ambiguous, it would be 
useful to take their contexts into consideration 
when classifying the sentiments. In this paper, we 
regard the following three kinds of related tweets 
as context for a tweet. 
1. Retweets. Retweeting in Twitter is essen-
tially the forwarding of a previous message. 
People usually do not change the content 
of the original tweet when retweeting. So 
retweets usually have the same sentiment 
as the original tweets.  
2. Tweets containing the target and published 
by the same person. Intuitively, the tweets 
published by the same person within a 
short timeframe should have a consistent 
sentiment about the same target.  
3. Tweets replying to or replied by the tweet 
to be classified.  
Based on these three kinds of relations, we can 
construct a graph using the input tweet collection 
of a given target. As illustrated in Figure 1, each 
circle in the graph indicates a tweet. The three 
kinds of edges indicate being published by the 
same person (solid line), retweeting (dash line), 
and replying relations (round dotted line) respec-
tively. 
 
 
 
Figure 1. An example graph of tweets about a target 
 
If we consider that the sentiment of a tweet only 
depends on its content and immediate neighbors, 
we can leverage a graph-based method for senti-
ment classification of tweets. Specifically, the 
probability of a tweet belonging to a specific sen-
timent class can be computed with the following 
formula: 
??
)(
))(())(|()|(),|(
dN
dNpdNcpcpGcp ??
 
Where c is the sentiment label of a tweet which 
belongs to {positive, negative, neutral}, G is the 
tweet graph, N(d) is a specific assignment of sen-
timent labels to all immediate neighbors of the 
tweet, and ? is the content of the tweet. 
We can convert the output scores of a tweet by 
the subjectivity and polarity classifiers into proba-
bilistic form and use them to approximate p(c| ?). 
Then a relaxation labeling algorithm described in 
(Angelova and Weikum, 2006) can be used on the 
graph to iteratively estimate p(c|?,G) for all tweets. 
After the iteration ends, for any tweet in the graph, 
the sentiment label that has the maximum p(c| ?,G) 
is considered the final label. 
6 Experiments  
Because there is no annotated tweet corpus public-
ly available for evaluation of target-dependent 
Twitter sentiment classification, we have to create 
our own. Since people are most interested in sen-
timents towards celebrities, companies and prod-
ucts, we selected 5 popular queries of these kinds: 
{Obama, Google, iPad, Lakers, Lady Gaga}. For 
each of those queries, we downloaded 400 English 
tweets10 containing the query using the Twitter API.  
We manually classify each tweet as positive, 
negative or neutral towards the query with which it 
is downloaded. After removing duplicate tweets, 
we finally obtain 459 positive, 268 negative and 
1,212 neutral tweets. 
Among the tweets, 100 are labeled by two hu-
man annotators for inter-annotator study. The re-
sults show that for 86% of them, both annotators 
gave identical labels. Among the 14 tweets which 
the two annotators disagree on, only 1 case is a 
positive-negative disagreement (one annotator con-
siders it positive while the other negative), and the 
other 13 are all neutral-subjective disagreement. 
This probably indicates that it is harder for humans 
to decide if a tweet is neutral or subjective than to 
decide if it is positive or negative. 
                                                          
10 In this paper, we use sentiment classification of English 
tweets as a case study; however, our approach is applicable to 
other languages as well. 
156
6.1 Error Analysis of Twitter Sentiment Out-
put 
We first analyze the output of Twitter Sentiment 
(TS) using the five test queries. For each query, we 
randomly select 20 tweets labeled as positive or 
negative by TS. We also manually classify each 
tweet as positive, negative or neutral about the cor-
responding query. Then, we analyze those tweets 
that get different labels from TS and humans. Fi-
nally we find two major types of error: 1) Tweets 
which are totally neutral (for any target) are classi-
fied as subjective by TS; 2) sentiments in some 
tweets are classified correctly but the sentiments 
are not truly about the query. The two types take 
up about 35% and 40% of the total errors, respec-
tively.  
The second type is actually what we want to re-
solve in this paper. After further checking those 
tweets of the second type, we found that most of 
them are actually neutral for the target, which 
means that the dominant error in Twitter Sentiment 
is classifying neutral tweets as subjective. Below 
are several examples of the second type where the 
bolded words are the targets. 
 ?No debate needed, heat can't beat lakers or 
celtics? (negative by TS but positive by human) 
?why am i getting spams from weird people ask-
ing me if i want to chat with lady gaga? (positive 
by TS but neutral by human) 
?Bringing iPhone and iPad apps into cars? 
http://www.speakwithme.com/ will be out soon and 
alpha is awesome in my car.? (positive by TS but 
neutral by human) 
?Here's a great article about Monte Veronese 
cheese. It's in Italian so just put the url into Google 
translate and enjoy http://ow.ly/3oQ77? (positive 
by TS but neutral by human) 
6.2 Evaluation of Subjectivity Classification 
We conduct several experiments to evaluate sub-
jectivity classifiers using different features. In the 
experiments, we consider the positive and negative 
tweets annotated by humans as subjective tweets 
(i.e., positive instances in the SVM classifiers), 
which amount to 727 tweets. Following (Pang et 
al., 2002), we balance the evaluation data set by 
randomly selecting 727 tweets from all neutral 
tweets annotated by humans and consider them as 
objective tweets (i.e., negative instances in the 
classifiers). We perform 10-fold cross-validations 
on the selected data. Following (Go et al, 2009; 
Pang et al, 2002), we use accuracy as a metric in 
our experiments. The results are listed below. 
 
Features Accuracy (%) 
Content features 61.1 
+ Sentiment lexicon features 63.8 
+ Target-dependent features 68.2 
Re-implementation of (Bar-
bosa and Feng, 2010) 
60.3 
 
Table 1. Evaluation of subjectivity classifiers. 
 
As shown in Table 1, the classifier using only 
the content features achieves an accuracy of 61.1%. 
Adding sentiment lexicon features improves the 
accuracy to 63.8%. Finally, the best performance 
(68.2%) is achieved by combining target-
dependent features and other features (t-test: p < 
0.005). This clearly shows that target-dependent 
features do help remove many sentiments not truly 
about the target. We also re-implemented the 
method proposed in (Barbosa and Feng, 2010) for 
comparison. From Table 1, we can see that all our 
systems perform better than (Barbosa and Feng, 
2010) on our data set. One possible reason is that 
(Barbosa and Feng, 2010) use only abstract fea-
tures while our systems use more lexical features. 
To further evaluate the contribution of target ex-
tension, we compare the system using the exact 
target and all extended targets with that using only 
the exact target. We also eliminate the extended 
targets generated by each of the three target exten-
sion methods and reevaluate the performances. 
 
Target Accuracy (%) 
Exact target 65.6 
+ all extended targets 68.2 
- co-references 68.0 
- targets found by PMI 67.8 
- head nouns 67.3 
 
Table 2. Evaluation of target extension methods. 
 
As shown in Table 2, without extended targets, 
the accuracy is 65.6%, which is still higher than 
those using only target-independent features. After 
adding all extended targets, the accuracy is im-
proved significantly to 68.2% (p < 0.005), which 
suggests that target extension does help find indi-
157
rectly expressed sentiments about the target. In 
addition, all of the three methods contribute to the 
overall improvement, with the head noun method 
contributing most. However, the other two meth-
ods do not contribute significantly.  
6.3 Evaluation of Polarity Classification  
Similarly, we conduct several experiments on posi-
tive and negative tweets to compare the polarity 
classifiers with different features, where we use 
268 negative and 268 randomly selected positive 
tweets. The results are listed below. 
 
Features Accuracy (%) 
Content features 78.8 
+ Sentiment lexicon features 84.2 
+ Target-dependent features 85.6 
Re-implementation of (Bar-
bosa and Feng, 2010) 
83.9 
 
Table 3. Evaluation of polarity classifiers. 
 
From Table 3, we can see that the classifier us-
ing only the content features achieves the worst 
accuracy (78.8%). Sentiment lexicon features are 
shown to be very helpful for improving the per-
formance. Similarly, we re-implemented the meth-
od proposed by (Barbosa and Feng, 2010) in this 
experiment. The results show that our system using 
both content features and sentiment lexicon fea-
tures performs slightly better than (Barbosa and 
Feng, 2010). The reason may be same as that we 
explained above. 
Again, the classifier using all features achieves 
the best performance. Both the classifiers with all 
features and with the combination of content and 
sentiment lexicon features are significantly better 
than that with only the content features (p < 0.01). 
However, the classifier with all features does not 
significantly outperform that using the combina-
tion of content and sentiment lexicon features. We 
also note that the improvement by target-dependent 
features here is not as large as that in subjectivity 
classification. Both of these indicate that target-
dependent features are more useful for improving 
subjectivity classification than for polarity classifi-
cation. This is consistent with our observation in 
Subsection 6.2 that most errors caused by incorrect 
target association are made in subjectivity classifi-
cation. We also note that all numbers in Table 3 
are much bigger than those in Table 1, which sug-
gests that subjectivity classification of tweets is 
more difficult than polarity classification. 
Similarly, we evaluated the contribution of tar-
get extension for polarity classification. According 
to the results, adding all extended targets improves 
the accuracy by about 1 point. However, the con-
tributions from the three individual methods are 
not statistically significant. 
6.4 Evaluation of Graph-based Optimization  
As seen in Figure 1, there are several tweets which 
are not connected with any other tweets. For these 
tweets, our graph-based optimization approach will 
have no effect. The following table shows the per-
centages of the tweets in our evaluation data set 
which have at least one related tweet according to 
various relation types.  
 
Relation type Percentage 
Published by the same person11 41.6 
Retweet 23.0 
Reply 21.0 
All 66.2 
 
Table 4. Percentages of tweets having at least one relat-
ed tweet according to various relation types. 
 
According to Table 4, for 66.2% of the tweets 
concerning the test queries, we can find at least one 
related tweet. That means our context-aware ap-
proach is potentially useful for most of the tweets. 
To evaluate the effectiveness of our context-
aware approach, we compared the systems with 
and without considering the context.  
 
System Accuracy 
F1-score (%) 
pos neu neg 
Target-dependent 
sentiment classifier 
66.0 57.5 70.1 66.1 
+Graph-based op-
timization 
68.3 63.5 71.0 68.5 
 
Table 5. Effectiveness of the context-aware approach. 
 
As shown in Table 5, the overall accuracy of the 
target-dependent classifiers over three classes is 
66.0%. The graph-based optimization improves the 
performance by over 2 points (p < 0.005), which 
clearly shows that the context information is very 
                                                          
11 We limit the time frame from one week before to one week 
after the post time of the current tweet. 
158
useful for classifying the sentiments of tweets. 
From the detailed improvement for each sentiment 
class, we find that the context-aware approach is 
especially helpful for positive and negative classes. 
 
Relation type Accuracy (%) 
Published by the same person 67.8 
Retweet 66.0 
Reply 67.0 
 
Table 6. Contribution comparison between relations. 
 
We further compared the three types of relations 
for context-aware sentiment classification; the re-
sults are reported in Table 6. Clearly, being pub-
lished by the same person is the most useful 
relation for sentiment classification, which is con-
sistent with the percentage distribution of the 
tweets over relation types; using retweet only does 
not help. One possible reason for this is that the 
retweets and their original tweets are nearly the 
same, so it is very likely that they have already got 
the same labels in previous classifications. 
7 Conclusions and Future Work 
Twitter sentiment analysis has attracted much at-
tention recently. In this paper, we address target-
dependent sentiment classification of tweets. Dif-
ferent from previous work using target-
independent classification, we propose to incorpo-
rate syntactic features to distinguish texts used for 
expressing sentiments towards different targets in a 
tweet. According to the experimental results, the 
classifiers incorporating target-dependent features 
significantly outperform the previous target-
independent classifiers.  
In addition, different from previous work using 
only information on the current tweet for sentiment 
classification, we propose to take the related tweets 
of the current tweet into consideration by utilizing 
graph-based optimization. According to the exper-
imental results, the graph-based optimization sig-
nificantly improves the performance. 
As mentioned in Section 4.1, in future we would 
like to explore the relations between a target and 
any of its extended targets. We are also interested 
in exploring relations between Twitter accounts for 
classifying the sentiments of the tweets published 
by them. 
Acknowledgments 
We would like to thank Matt Callcut for refining 
the language of this paper, and thank Yuki Arase 
and the anonymous reviewers for many valuable 
comments and helpful suggestions. We would also 
thank Furu Wei and Xiaolong Wang for their help 
with some of the experiments and the preparation 
of the camera-ready version of the paper. 
References  
Ralitsa Angelova, Gerhard Weikum. 2006. Graph-based 
text classification: learn from your neighbors. SIGIR 
2006: 485-492 
Luciano Barbosa and Junlan Feng. 2010. Robust Senti-
ment Detection on Twitter from Biased and Noisy 
Data. Coling 2010. 
Christopher Burges. 1998. A Tutorial on Support Vector 
Machines for Pattern Recognition. Data Mining and 
Knowledge Discovery, 2(2):121-167. 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan S. 2005. Identifying sources of opinions 
with conditional random fields and extraction pat-
terns. In Proc. of the 2005 Human Language Tech-
nology Conf. and Conf. on Empirical Methods in 
Natural Language Processing (HLT/EMNLP 2005). 
pp. 355-362 
Dmitry Davidiv, Oren Tsur and Ari Rappoport. 2010. 
Enhanced Sentiment Learning Using Twitter Hash-
tags and Smileys. Coling 2010. 
Xiaowen Ding and Bing Liu. 2007. The Utility of Lin-
guistic Rules in Opinion Mining. SIGIR-2007 (poster 
paper), 23-27 July 2007, Amsterdam.  
Alec Go, Richa Bhayani, Lei Huang. 2009. Twitter Sen-
timent Classification using Distant Supervision. 
Vasileios Hatzivassiloglou and Kathleen.R. McKeown. 
2002. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th ACL and the 8th 
Conference of the European Chapter of the ACL. 
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004, full paper), 
Seattle, Washington, USA, Aug 22-25, 2004. 
Thorsten Joachims. Making Large-scale Support Vector 
Machine Learning Practical. In B. Sch?olkopf, C. J. 
C. Burges, and A. J. Smola, editors, Advances in 
kernel methods: support vector learning, pages 169-
184. MIT Press, Cambridge, MA, USA, 1999. 
159
Soo-Min Kim and Eduard Hovy 2006. Extracting opi-
nions, opinion holders, and topics expressed in online 
news media text, In Proc. of ACL Workshop on Sen-
timent and Subjectivity in Text, pp.1-8, Sydney, Aus-
tralia.  
Ryan McDonald, F. Pereira, K. Ribarov, and J. Haji?c. 
2005. Non-projective dependency parsing using 
spanning tree algorithms. In Proc. HLT/EMNLP. 
Tetsuya Nasukawa, Jeonghee Yi. 2003. Sentiment anal-
ysis: capturing favorability using natural language 
processing. In Proceedings of K-CAP. 
Bo Pang, Lillian Lee. 2004. A Sentimental Education: 
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of 
ACL 2004. 
Bo Pang, Lillian Lee, Shivakumar Vaithyanathan. 2002. 
Thumbs up? Sentiment Classification using Machine 
Learning Techniques.  
Ravi Parikh and Matin Movassate. 2009. Sentiment 
Analysis of User-Generated Twitter Updates using 
Various Classification Techniques. 
Wee. M. Soon, Hwee. T. Ng, and Danial. C. Y. Lim. 
2001. A Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational Linguis-
tics, 27(4):521?544. 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. In proceedings of ACL 2002. 
Janyce Wiebe. 2000. Learning subjective adjectives 
from corpora. In Proceedings of AAAI-2000. 
Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005. 
Recognizing Contextual Polarity in Phrase-Level 
Sentiment Analysis. In Proceedings of NAACL 2005. 
160
