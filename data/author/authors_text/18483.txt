Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 704?714,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Sarcasm as Contrast between a Positive Sentiment and Negative Situation
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva,
Nathan Gilbert, Ruihong Huang
School Of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,asheq,alnds,ngilbert,huangrh}@cs.utah.edu, prafulla.surve@gmail.com
Abstract
A common form of sarcasm on Twitter con-
sists of a positive sentiment contrasted with a
negative situation. For example, many sarcas-
tic tweets include a positive sentiment, such as
?love? or ?enjoy?, followed by an expression
that describes an undesirable activity or state
(e.g., ?taking exams? or ?being ignored?). We
have developed a sarcasm recognizer to iden-
tify this type of sarcasm in tweets. We present
a novel bootstrapping algorithm that automati-
cally learns lists of positive sentiment phrases
and negative situation phrases from sarcastic
tweets. We show that identifying contrast-
ing contexts using the phrases learned through
bootstrapping yields improved recall for sar-
casm recognition.
1 Introduction
Sarcasm is generally characterized as ironic or satir-
ical wit that is intended to insult, mock, or amuse.
Sarcasm can be manifested in many different ways,
but recognizing sarcasm is important for natural lan-
guage processing to avoid misinterpreting sarcastic
statements as literal. For example, sentiment anal-
ysis can be easily misled by the presence of words
that have a strong polarity but are used sarcastically,
which means that the opposite polarity was intended.
Consider the following tweet on Twitter, which in-
cludes the words ?yay? and ?thrilled? but actually
expresses a negative sentiment: ?yay! it?s a holi-
day weekend and i?m on call for work! couldn?t be
more thrilled! #sarcasm.? In this case, the hashtag
#sarcasm reveals the intended sarcasm, but we don?t
always have the benefit of an explicit sarcasm label.
In the realm of Twitter, we observed that many
sarcastic tweets have a common structure that
creates a positive/negative contrast between a senti-
ment and a situation. Specifically, sarcastic tweets
often express a positive sentiment in reference to a
negative activity or state. For example, consider the
tweets below, where the positive sentiment terms
are underlined and the negative activity/state terms
are italicized.
(a) Oh how I love being ignored. #sarcasm
(b) Thoroughly enjoyed shoveling the driveway
today! :) #sarcasm
(c) Absolutely adore it when my bus is late
#sarcasm
(d) I?m so pleased mom woke me up with
vacuuming my room this morning. :) #sarcasm
The sarcasm in these tweets arises from the jux-
taposition of a positive sentiment word (e.g., love,
enjoyed, adore, pleased) with a negative activity or
state (e.g., being ignored, bus is late, shoveling, and
being woken up).
The goal of our research is to identify sarcasm
that arises from the contrast between a positive sen-
timent referring to a negative situation. A key chal-
lenge is to automatically recognize the stereotypi-
cally negative ?situations?, which are activities and
states that most people consider to be unenjoyable or
undesirable. For example, stereotypically unenjoy-
able activities include going to the dentist, taking an
exam, and having to work on holidays. Stereotypi-
cally undesirable states include being ignored, hav-
ing no friends, and feeling sick. People recognize
704
these situations as being negative through cultural
norms and stereotypes, so they are rarely accompa-
nied by an explicit negative sentiment. For example,
?I feel sick? is universally understood to be a nega-
tive situation, even without an explicit expression of
negative sentiment. Consequently, we must learn to
recognize phrases that correspond to stereotypically
negative situations.
We present a bootstrapping algorithm that auto-
matically learns phrases corresponding to positive
sentiments and phrases corresponding to negative
situations. We use tweets that contain a sarcasm
hashtag as positive instances for the learning pro-
cess. The bootstrapping algorithm begins with a sin-
gle seed word, ?love?, and a large set of sarcastic
tweets. First, we learn negative situation phrases
that follow a positive sentiment (initially, the seed
word ?love?). Second, we learn positive sentiment
phrases that occur near a negative situation phrase.
The bootstrapping process iterates, alternately learn-
ing new negative situations and new positive sen-
timent phrases. Finally, we use the learned lists
of sentiment and situation phrases to recognize sar-
casm in new tweets by identifying contexts that con-
tain a positive sentiment in close proximity to a neg-
ative situation phrase.
2 Related Work
Researchers have investigated the use of lexical
and syntactic features to recognize sarcasm in text.
Kreuz and Caucci (2007) studied the role that dif-
ferent lexical factors play, such as interjections (e.g.,
?gee? or ?gosh?) and punctuation symbols (e.g., ???)
in recognizing sarcasm in narratives. Lukin and
Walker (2013) explored the potential of a bootstrap-
ping method for sarcasm classification in social di-
alogue to learn lexical N-gram cues associated with
sarcasm (e.g., ?oh really?, ?I get it?, ?no way?, etc.)
as well as lexico-syntactic patterns.
In opinionated user posts, Carvalho et al (2009)
found oral or gestural expressions, represented us-
ing punctuation and other keyboard characters, to
be more predictive of irony1 in contrast to features
representing structured linguistic knowledge in Por-
1They adopted the term ?irony? instead of ?sarcasm? to re-
fer to the case when a word or expression with prior positive
polarity is figuratively used to express a negative opinion.
tuguese. Filatova (2012) presented a detailed de-
scription of sarcasm corpus creation with sarcasm
annotations of Amazon product reviews. Their an-
notations capture sarcasm both at the document level
and the text utterance level. Tsur et al (2010) pre-
sented a semi-supervised learning framework that
exploits syntactic and pattern based features in sar-
castic sentences of Amazon product reviews. They
observed correlated sentiment words such as ?yay!?
or ?great!? often occurring in their most useful pat-
terns.
Davidov et al (2010) used sarcastic tweets and
sarcastic Amazon product reviews to train a sarcasm
classifier with syntactic and pattern-based features.
They examined whether tweets with a sarcasm hash-
tag are reliable enough indicators of sarcasm to be
used as a gold standard for evaluation, but found that
sarcasm hashtags are noisy and possibly biased to-
wards the hardest form of sarcasm (where even hu-
mans have difficulty). Gonza?lez-Iba?n?ez et al (2011)
explored the usefulness of lexical and pragmatic fea-
tures for sarcasm detection in tweets. They used sar-
casm hashtags as gold labels. They found positive
and negative emotions in tweets, determined through
fixed word dictionaries, to have a strong correlation
with sarcasm. Liebrecht et al (2013) explored N-
gram features from 1 to 3-grams to build a classifier
to recognize sarcasm in Dutch tweets. They made an
interesting observation from their most effective N-
gram features that people tend to be more sarcastic
towards specific topics such as school, homework,
weather, returning from vacation, public transport,
the church, the dentist, etc. This observation has
some overlap with our observation that stereotypi-
cally negative situations often occur in sarcasm.
The cues for recognizing sarcasm may come from
a variety of sources. There exists a line of work
that tries to identify facial and vocal cues in speech
(e.g., (Gina M. Caucci, 2012; Rankin et al, 2009)).
Cheang and Pell (2009) and Cheang and Pell (2008)
performed studies to identify acoustic cues in sarcas-
tic utterances by analyzing speech features such as
speech rate, mean amplitude, amplitude range, etc.
Tepperman et al (2006) worked on sarcasm recog-
nition in spoken dialogue using prosodic and spec-
tral cues (e.g., average pitch, pitch slope, etc.) as
well as contextual cues (e.g., laughter or response to
questions) as features.
705
While some of the previous work has identi-
fied specific expressions that correlate with sarcasm,
none has tried to identify contrast between positive
sentiments and negative situations. The novel con-
tributions of our work include explicitly recogniz-
ing contexts that contrast a positive sentiment with a
negative activity or state, as well as a bootstrapped
learning framework to automatically acquire posi-
tive sentiment and negative situation phrases.
3 Bootstrapped Learning of Positive
Sentiments and Negative Situations
Sarcasm is often defined in terms of contrast or ?say-
ing the opposite of what you mean?. Our work fo-
cuses on one specific type of contrast that is common
on Twitter: the expression of a positive sentiment
(e.g., ?love? or ?enjoy?) in reference to a negative
activity or state (e.g., ?taking an exam? or ?being ig-
nored?). Our goal is to create a sarcasm classifier for
tweets that explicitly recognizes contexts that con-
tain a positive sentiment contrasted with a negative
situation.
Our approach learns rich phrasal lexicons of pos-
itive sentiments and negative situations using only
the seed word ?love? and a collection of sarcastic
tweets as input. A key factor that makes the algo-
rithm work is the presumption that if you find a pos-
itive sentiment or a negative situation in a sarcastic
tweet, then you have found the source of the sar-
casm. We further assume that the sarcasm probably
arises from positive/negative contrast and we exploit
syntactic structure to extract phrases that are likely
to have contrasting polarity. Another key factor is
that we focus specifically on tweets. The short na-
ture of tweets limits the search space for the source
of the sarcasm. The brevity of tweets also probably
contributes to the prevalence of this relatively com-
pact form of sarcasm.
3.1 Overview of the Learning Process
Our bootstrapping algorithm operates on the as-
sumption that many sarcastic tweets contain both a
positive sentiment and a negative situation in close
proximity, which is the source of the sarcasm.2 Al-
though sentiments and situations can be expressed
2Sarcasm can arise from a negative sentiment contrasted
with a positive situation too, but our observation is that this is
much less common, at least on Twitter.
Positive
Sentiment
Phrases
Negative
Situation
Phrases
Seed Word
"love"
Sarcastic Tweets
1 2
34
Figure 1: Bootstrapped Learning of Positive Sentiment
and Negative Situation Phrases
in numerous ways, we focus on positive sentiments
that are expressed as a verb phrase or as a predicative
expression (predicate adjective or predicate nomi-
nal), and negative activities or states that can be a
complement to a verb phrase. Ideally, we would
like to parse the text and extract verb complement
phrase structures, but tweets are often informally
written and ungrammatical. Therefore we try to rec-
ognize these syntactic structures heuristically using
only part-of-speech tags and proximity.
The learning process relies on an assumption that
a positive sentiment verb phrase usually appears to
the left of a negative situation phrase and in close
proximity (usually, but not always, adjacent). Picto-
rially, we assume that many sarcastic tweets contain
this structure:
[+ VERB PHRASE] [? SITUATION PHRASE]
This structural assumption drives our bootstrap-
ping algorithm, which is illustrated in Figure 1.
The bootstrapping process begins with a single seed
word, ?love?, which seems to be the most common
positive sentiment term in sarcastic tweets. Given
a sarcastic tweet containing the word ?love?, our
structural assumption infers that ?love? is probably
followed by an expression that refers to a negative
situation. So we harvest the n-grams that follow the
word ?love? as negative situation candidates. We se-
lect the best candidates using a scoring metric, and
add them to a list of negative situation phrases.
Next, we exploit the structural assumption in the
opposite direction. Given a sarcastic tweet that con-
tains a negative situation phrase, we infer that the
negative situation phrase is preceded by a positive
sentiment. We harvest the n-grams that precede the
negative situation phrases as positive sentiment can-
didates, score and select the best candidates, and
706
add them to a list of positive sentiment phrases.
The bootstrapping process then iterates, alternately
learning more positive sentiment phrases and more
negative situation phrases.
We also observed that positive sentiments are fre-
quently expressed as predicative phrases (i.e., pred-
icate adjectives and predicate nominals). For ex-
ample: ?I?m taking calculus. It is awesome. #sar-
casm?. Wiegand et al (2013) offered a related ob-
servation that adjectives occurring in predicate ad-
jective constructions are more likely to convey sub-
jectivity than adjectives occurring in non-predicative
structures. Therefore we also include a step in
the learning process to harvest predicative phrases
that occur in close proximity to a negative situation
phrase. In the following sections, we explain each
step of the bootstrapping process in more detail.
3.2 Bootstrapping Data
For the learning process, we used Twitter?s stream-
ing API to obtain a large set of tweets. We col-
lected 35,000 tweets that contain the hashtag #sar-
casm or #sarcastic to use as positive instances of sar-
casm. We also collected 140,000 additional tweets
from Twitter?s random daily stream. We removed
the tweets that contain a sarcasm hashtag, and con-
sidered the rest to be negative instances of sarcasm.
Of course, there will be some sarcastic tweets that do
not have a sarcasm hashtag, so the negative instances
will contain some noise. But we expect that a very
small percentage of these tweets will be sarcastic, so
the noise should not be a major issue. There will also
be noise in the positive instances because a sarcasm
hashtag does not guarantee that there is sarcasm in
the body of the tweet (e.g., the sarcastic content may
be in a linked url, or in a prior tweet). But again, we
expect the amount of noise to be relatively small.
Our tweet collection therefore contains a total of
175,000 tweets: 20% are labeled as sarcastic and
80% are labeled as not sarcastic. We applied CMU?s
part-of-speech tagger designed for tweets (Owoputi
et al, 2013) to this data set.
3.3 Seeding
The bootstrapping process begins by initializing the
positive sentiment lexicon with one seed word: love.
We chose this seed because it seems to be the most
common positive sentiment word in sarcastic tweets.
3.4 Learning Negative Situation Phrases
The first stage of bootstrapping learns new phrases
that correspond to negative situations. The learning
process consists of two steps: (1) harvesting candi-
date phrases, and (2) scoring and selecting the best
candidates.
To collect candidate phrases for negative situa-
tions, we extract n-grams that follow a positive senti-
ment phrase in a sarcastic tweet. We extract every 1-
gram, 2-gram, and 3-gram that occurs immediately
after (on the right-hand side) of a positive sentiment
phrase. As an example, consider the tweet in Figure
2, where ?love? is the positive sentiment:
I love waiting forever for the doctor #sarcasm
Figure 2: Example Sarcastic Tweet
We extract three n-grams as candidate negative situ-
ation phrases:
waiting, waiting forever, waiting forever for
Next, we apply the part-of-speech (POS) tagger
and filter the candidate list based on POS patterns so
we only keep n-grams that have a desired syntactic
structure. For negative situation phrases, our goal
is to learn possible verb phrase (VP) complements
that are themselves verb phrases because they should
represent activities and states. So we require a can-
didate phrase to be either a unigram tagged as a verb
(V) or the phrase must match one of 7 POS-based
bigram patterns or 20 POS-based trigram patterns
that we created to try to approximate the recogni-
tion of verbal complement structures. The 7 POS bi-
gram patterns are: V+V, V+ADV, ADV+V, ?to?+V,
V+NOUN, V+PRO, V+ADJ. Note that we used
a POS tagger designed for Twitter, which has a
smaller set of POS tags than more traditional POS
taggers. For example there is just a single V tag
that covers all types of verbs. The V+V pattern will
therefore capture negative situation phrases that con-
sist of a present participle verb followed by a past
participle verb, such as ?being ignored? or ?getting
hit?.3 We also allow verb particles to match a V tag
in our patterns. The remaining bigram patterns cap-
ture verb phrases that include a verb and adverb, an
3In some cases it may be more appropriate to consider the
second verb to be an adjective, but in practice they were usually
tagged as verbs.
707
infinitive form (e.g., ?to clean?), a verb and noun
phrase (e.g., ?shoveling snow?), or a verb and ad-
jective (e.g., ?being alone?). We use some simple
heuristics to try to ensure that we are at the end of an
adjective or noun phrase (e.g., if the following word
is tagged as an adjective or noun, then we assume
we are not at the end).
The 20 POS trigram patterns are similar in nature
and are designed to capture seven general types of
verb phrases: verb and adverb mixtures, an infini-
tive VP that includes an adverb, a verb phrase fol-
lowed by a noun phrase, a verb phrase followed by a
prepositional phrase, a verb followed by an adjective
phrase, or an infinitive VP followed by an adjective,
noun, or pronoun.
Returning to Figure 2, only two of the n-grams
match our POS patterns, so we are left with two can-
didate phrases for negative situations:
waiting, waiting forever
Next, we score each negative situation candidate
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase following
a positive sentiment phrase:
| follows(?candidate, +sentiment) & sarcastic |
| follows(?candidate, +sentiment) |
We compute the number of times that the negative
situation candidate immediately follows a positive
sentiment in sarcastic tweets divided by the number
of times that the candidate immediately follows a
positive sentiment in all tweets. We discard phrases
that have a frequency < 3 in the tweet collection
since they are too sparse.
Finally, we rank the candidate phrases based on
this probability, using their frequency as a secondary
key in case of ties. The top 20 phrases with a prob-
ability ? .80 are added to the negative situation
phrase list.4 When we add a phrase to the nega-
tive situation list, we immediately remove all other
candidates that are subsumed by the selected phrase.
For example, if we add the phrase ?waiting?, then
the phrase ?waiting forever? would be removed from
the candidate list because it is subsumed by ?wait-
ing?. This process reduces redundancy in the set of
4Fewer than 20 phrases will be learned if < 20 phrases pass
this threshold.
phrases that we add during each bootstrapping itera-
tion. The bootstrapping process stops when no more
candidate phrases pass the probability threshold.
3.5 Learning Positive Verb Phrases
The procedure for learning positive sentiment
phrases is analogous. First, we collect phrases that
potentially convey a positive sentiment by extract-
ing n-grams that precede a negative situation phrase
in a sarcastic tweet. To learn positive sentiment verb
phrases, we extract every 1-gram and 2-gram that
occurs immediately before (on the left-hand side of)
a negative situation phrase.
Next, we apply the POS tagger and filter the n-
grams using POS tag patterns so that we only keep
n-grams that have a desired syntactic structure. Here
our goal is to learn simple verb phrases (VPs) so we
only retain n-grams that contain at least one verb and
consist only of verbs and (optionally) adverbs. Fi-
nally, we score each candidate sentiment verb phrase
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase preceding
a negative situation phrase:
| precedes(+candidateVP,?situation) & sarcastic |
| precedes(+candidateVP,?situation) |
3.6 Learning Positive Predicative Phrases
We also use the negative situation phrases to harvest
predicative expressions (predicate adjective or pred-
icate nominal structures) that occur nearby. Based
on the same assumption that sarcasm often arises
from the contrast between a positive sentiment and
a negative situation, we identify tweets that contain
a negative situation and a predicative expression in
close proximity. We then assume that the predicative
expression is likely to convey a positive sentiment.
To learn predicative expressions, we use 24 copu-
lar verbs from Wikipedia5 and their inflections. We
extract positive sentiment candidates by extracting
1-grams, 2-grams, and 3-grams that appear immedi-
ately after a copular verb and occur within 5 words
of the negative situation phrase, on either side. This
constraint only enforces proximity because predica-
tive expressions often appear in a separate clause or
sentence (e.g., ?It is just great that my iphone was
stolen? or ?My iphone was stolen. This is great.?)
5http://en.wikipedia.org/wiki/List of English copulae
708
We then apply POS patterns to identify n-grams
that correspond to predicate adjective and predicate
nominal phrases. For predicate adjectives, we re-
tain ADJ and ADV+ADJ n-grams. We use a few
heuristics to check that the adjective is not part of a
noun phrase (e.g., we check that the following word
is not a noun). For predicate nominals, we retain
ADV+ADJ+N, DET+ADJ+N and ADJ+N n-grams.
We excluded noun phrases consisting only of nouns
because they rarely seemed to represent a sentiment.
The sentiment in predicate nominals was usually
conveyed by the adjective. We discard all candidates
with frequency < 3 as being too sparse. Finally,
we score each remaining candidate by estimating the
probability that a tweet is sarcastic given that it con-
tains the predicative expression near (within 5 words
of) a negative situation phrase:
| near(+candidatePRED,?situation) & sarcastic |
| near(+candidatePRED,?situation) |
We found that the diversity of positive senti-
ment verb phrases and predicative expressions is
much lower than the diversity of negative situation
phrases. As a result, we sort the candidates by their
probability and conservatively add only the top 5
positive verb phrases and top 5 positive predicative
expressions in each bootstrapping iteration. Both
types of sentiment phrases must pass a probability
threshold of ? .70.
3.7 The Learned Phrase Lists
The bootstrapping process alternately learns pos-
itive sentiments and negative situations until no
more phrases can be learned. In our experiments,
we learned 26 positive sentiment verb phrases, 20
predicative expressions and 239 negative situation
phrases.
Table 1 shows the first 15 positive verb phrases,
the first 15 positive predicative expressions, and the
first 40 negative situation phrases learned by the
bootstrapping algorithm. Some of the negative sit-
uation phrases are not complete expressions, but it
is clear that they will often match negative activities
and states. For example, ?getting yelled? was gener-
ated from sarcastic comments such as ?I love getting
yelled at?, ?being home? occurred in tweets about
?being home alone?, and ?being told? is often be-
ing told what to do. Shorter phrases often outranked
longer phrases because they are more general, and
will therefore match more contexts. But an avenue
for future work is to learn linguistic expressions that
more precisely characterize specific negative situa-
tions.
Positive Verb Phrases (26): missed, loves,
enjoy, cant wait, excited, wanted, can?t wait,
get, appreciate, decided, loving, really like,
looooove, just keeps, loveee, ...
Positive Predicative Expressions (20): great,
so much fun, good, so happy, better, my
favorite thing, cool, funny, nice, always fun,
fun, awesome, the best feeling, amazing,
happy, ...
Negative Situations (239): being ignored, be-
ing sick, waiting, feeling, waking up early, be-
ing woken, fighting, staying, writing, being
home, cleaning, not getting, crying, sitting at
home, being stuck, starting, being told, be-
ing left, getting ignored, being treated, doing
homework, learning, getting up early, going to
bed, getting sick, riding, being ditched, get-
ting ditched, missing, not sleeping, not talking,
trying, falling, walking home, getting yelled,
being awake, being talked, taking care, doing
nothing, wasting, ...
Table 1: Examples of Learned Phrases
4 Evaluation
4.1 Data
For evaluation purposes, we created a gold stan-
dard data set of manually annotated tweets. Even
for people, it is not always easy to identify sarcasm
in tweets because sarcasm often depends on con-
versational context that spans more than a single
tweet. Extracting conversational threads from Twit-
ter, and analyzing conversational exchanges, has its
own challenges and is beyond the scope of this re-
search. We focus on identifying sarcasm that is self-
contained in one tweet and does not depend on prior
conversational context.
We defined annotation guidelines that instructed
human annotators to read isolated tweets and label
709
a tweet as sarcastic if it contains comments judged
to be sarcastic based solely on the content of that
tweet. Tweets that do not contain sarcasm, or where
potential sarcasm is unclear without seeing the prior
conversational context, were labeled as not sarcas-
tic. For example, a tweet such as ?Yes, I meant that
sarcastically.? should be labeled as not sarcastic be-
cause the sarcastic content was (presumably) in a
previous tweet. The guidelines did not contain any
instructions that required positive/negative contrast
to be present in the tweet, so all forms of sarcasm
were considered to be positive examples.
To ensure that our evaluation data had a healthy
mix of both sarcastic and non-sarcastic tweets, we
collected 1,600 tweets with a sarcasm hashtag (#sar-
casm or #sarcastic), and 1,600 tweets without these
sarcasm hashtags from Twitter?s random streaming
API. When presenting the tweets to the annotators,
the sarcasm hashtags were removed so the annota-
tors had to judge whether a tweet was sarcastic or
not without seeing those hashtags.
To ensure that we had high-quality annotations,
three annotators were asked to annotate the same set
of 200 tweets (100 sarcastic + 100 not sarcastic).
We computed inter-annotator agreement (IAA) be-
tween each pair of annotators using Cohen?s kappa
(?). The pairwise IAA scores were ?=0.80, ?=0.81,
and ?=0.82. We then gave each annotator an addi-
tional 1,000 tweets to annotate, yielding a total of
3,200 annotated tweets. We used the first 200 tweets
as our Tuning Set, and the remaining 3000 tweets as
our Test Set.
Our annotators judged 742 of the 3,200 tweets
(23%) to be sarcastic. Only 713 of the 1,600 tweets
with sarcasm hashtags (45%) were judged to be sar-
castic based on our annotation guidelines. There are
several reasons why a tweet with a sarcasm hash-
tag might not have been judged to be sarcastic. Sar-
casm may not be apparent without prior conversa-
tional context (i.e., multiple tweets), or the sarcastic
content may be in a URL and not the tweet itself, or
the tweet?s content may not obviously be sarcastic
without seeing the sarcasm hashtag (e.g., ?The most
boring hockey game ever #sarcasm?).
Of the 1,600 tweets in our data set that were ob-
tained from the random stream and did not have a
sarcasm hashtag, 29 (1.8%) were judged to be sar-
castic based on our annotation guidelines.
4.2 Baselines
Overall, 693 of the 3,000 tweets in our Test Set
were annotated as sarcastic, so a system that classi-
fies every tweet as sarcastic will have 23% precision.
To assess the difficulty of recognizing the sarcastic
tweets in our data set, we evaluated a variety of base-
line systems.
We created two baseline systems that use n-gram
features with supervised machine learning to create
a sarcasm classifier. We used the LIBSVM (Chang
and Lin, 2011) library to train two support vector
machine (SVM) classifiers: one with just unigram
features and one with both unigrams and bigrams.
The features had binary values indicating the pres-
ence or absence of each n-gram in a tweet. The clas-
sifiers were evaluated using 10-fold cross-validation.
We used the RBF kernel, and the cost and gamma
parameters were optimized for accuracy using un-
igram features and 10-fold cross-validation on our
Tuning Set. The first two rows of Table 2 show the
results for these SVM classifiers, which achieved F
scores of 46-48%.
We also conducted experiments with existing sen-
timent and subjectivity lexicons to see whether they
could be leveraged to recognize sarcasm. We exper-
imented with three resources:
Liu05 : A positive and negative opinion lexicon
from (Liu et al, 2005). This lexicon contains
2,007 positive sentiment words and 4,783 neg-
ative sentiment words.
MPQA05 : The MPQA Subjectivity Lexicon that
is part of the OpinionFinder system (Wilson et
al., 2005a; Wilson et al, 2005b). This lexicon
contains 2,718 subjective words with positive
polarity and 4,910 subjective words with nega-
tive polarity.
AFINN11 The AFINN sentiment lexicon designed
for microblogs (Nielsen, 2011; Hansen et al,
2011) contains 2,477 manually labeled words
and phrases with integer values ranging from -5
(negativity) to 5 (positivity). We considered all
words with negative values to have negative po-
larity (1598 words), and all words with positive
values to have positive polarity (879 words).
We performed four sets of experiments with each
resource to see how beneficial existing sentiment
710
System Recall Precision F score
Supervised SVM Classifiers
1grams .35 .64 .46
1+2grams .39 .64 .48
Positive Sentiment Only
Liu05 .77 .34 .47
MPQA05 .78 .30 .43
AFINN11 .75 .32 .44
Negative Sentiment Only
Liu05 .26 .23 .24
MPQA05 .34 .24 .28
AFINN11 .24 .22 .23
Positive and Negative Sentiment, Unordered
Liu05 .19 .37 .25
MPQA05 .27 .30 .29
AFINN11 .17 .30 .22
Positive and Negative Sentiment, Ordered
Liu05 .09 .40 .14
MPQA05 .13 .30 .18
AFINN11 .09 .35 .14
Our Bootstrapped Lexicons
Positive VPs .28 .45 .35
Negative Situations .29 .38 .33
Contrast(+VPs, ?Situations), Unordered .11 .56 .18
Contrast(+VPs, ?Situations), Ordered .09 .70 .15
& Contrast(+Preds, ?Situations) .13 .63 .22
Our Bootstrapped Lexicons ? SVM Classifier
Contrast(+VPs, ?Situations), Ordered .42 .63 .50
& Contrast(+Preds, ?Situations) .44 .62 .51
Table 2: Experimental results on the test set
lexicons could be for sarcasm recognition in tweets.
Since our hypothesis is that sarcasm often arises
from the contrast between something positive and
something negative, we systematically evaluated the
positive and negative phrases individually, jointly,
and jointly in a specific order (a positive phrase fol-
lowed by a negative phrase).
First, we labeled a tweet as sarcastic if it con-
tains any positive term in each resource. The Pos-
itive Sentiment Only section of Table 2 shows that
all three sentiment lexicons achieved high recall (75-
78%) but low precision (30-34%). Second, we la-
beled a tweet as sarcastic if it contains any negative
term from each resource. The Negative Sentiment
Only section of Table 2 shows that this approach
yields much lower recall and also lower precision
of 22-24%, which is what would be expected of a
random classifier since 23% of the tweets are sar-
castic. These results suggest that explicit negative
sentiments are not generally indicative of sarcasm.
Third, we labeled a tweet as sarcastic if it contains
both a positive sentiment term and a negative senti-
ment term, in any order. The Positive and Negative
Sentiment, Unordered section of Table 2 shows that
this approach yields low recall, indicating that rela-
tively few sarcastic tweets contain both positive and
negative sentiments, and low precision as well.
Fourth, we required the contrasting sentiments to
occur in a specific order (the positive term must pre-
cede the negative term) and near each other (no more
than 5 words apart). This criteria reflects our obser-
vation that positive sentiments often closely precede
negative situations in sarcastic tweets, so we wanted
to see if the same ordering tendency holds for neg-
ative sentiments. The Positive and Negative Senti-
ment, Ordered section of Table 2 shows that this or-
dering constraint further decreases recall and only
slightly improves precision, if at all. Our hypothe-
711
sis is that when positive and negative sentiments are
expressed in the same tweet, they are referring to
different things (e.g., different aspects of a product).
Expressing positive and negative sentiments about
the same thing would usually sound contradictory
rather than sarcastic.
4.3 Evaluation of Bootstrapped Phrase Lists
The next set of experiments evaluates the effective-
ness of the positive sentiment and negative situa-
tion phrases learned by our bootstrapping algorithm.
The results are shown in the Our Bootstrapped Lex-
icons section of Table 2. For the sake of compar-
ison with other sentiment resources, we first eval-
uated our positive sentiment verb phrases and neg-
ative situation phrases independently. Our positive
verb phrases achieved much lower recall than the
positive sentiment phrases in the other resources, but
they had higher precision (45%). The low recall
is undoubtedly because our bootstrapped lexicon is
small and contains only verb phrases, while the other
resources are much larger and contain terms with
additional parts-of-speech, such as adjectives and
nouns.
Despite its relatively small size, our list of neg-
ative situation phrases achieved 29% recall, which
is comparable to the negative sentiments, but higher
precision (38%).
Next, we classified a tweet as sarcastic if it con-
tains both a positive verb phrase and a negative sit-
uation phrase from our bootstrapped lists, in any
order. This approach produced low recall (11%)
but higher precision (56%) than the sentiment lex-
icons. Finally, we enforced an ordering constraint
so a tweet is labeled as sarcastic only if it contains
a positive verb phrase that precedes a negative situa-
tion in close proximity (no more than 5 words apart).
This ordering constraint further increased precision
from 56% to 70%, with a decrease of only 2 points
in recall. This precision gain supports our claim that
this particular structure (positive verb phrase fol-
lowed by a negative situation) is strongly indicative
of sarcasm. Note that the same ordering constraint
applied to a positive verb phrase followed by a neg-
ative sentiment produced much lower precision (at
best 40% precision using the Liu05 lexicon). Con-
trasting a positive sentiment with a negative situa-
tion seems to be a key element of sarcasm.
In the last experiment, we added the positive pred-
icative expressions and also labeled a tweet as sar-
castic if a positive predicative appeared in close
proximity to (within 5 words of) a negative situa-
tion. The positive predicatives improved recall to
13%, but decreased precision to 63%, which is com-
parable to the SVM classifiers.
4.4 A Hybrid Approach
Thus far, we have used the bootstrapped lexicons
to recognize sarcasm by looking for phrases in our
lists. We will refer to our approach as the Contrast
method, which labels a tweet as sarcastic if it con-
tains a positive sentiment phrase in close proximity
to a negative situation phrase.
The Contrast method achieved 63% precision but
with low recall (13%). The SVM classifier with un-
igram and bigram features achieved 64% precision
with 39% recall. Since neither approach has high
recall, we decided to see whether they are comple-
mentary and the Contrast method is finding sarcastic
tweets that the SVM classifier overlooks.
In this hybrid approach, a tweet is labeled as sar-
castic if either the SVM classifier or the Contrast
method identifies it as sarcastic. This approach im-
proves recall from 39% to 42% using the Contrast
method with only positive verb phrases. Recall im-
proves to 44% using the Contrast method with both
positive verb phrases and predicative phrases. This
hybrid approach has only a slight drop in precision,
yielding an F score of 51%. This result shows that
our bootstrapped phrase lists are recognizing sarcas-
tic tweets that the SVM classifier misses.
Finally, we ran tests to see if the performance of
the hybrid approach (Contrast ? SVM) is statisti-
cally significantly better than the performance of the
SVM classifier alone. We used paired bootstrap sig-
nificance testing as described in Berg-Kirkpatrick
et al (2012) by drawing 106 samples with repeti-
tion from the test set. These results showed that the
Contrast ? SVM system is statistically significantly
better than the SVM classifier at the p < .01 level
(i.e., the null hypothesis was rejected with 99% con-
fidence).
4.5 Analysis
To get a better sense of the strength and limitations
of our approach, we manually inspected some of the
712
tweets that were labeled as sarcastic using our boot-
strapped phrase lists. Table 3 shows some of the sar-
castic tweets found by the Contrast method but not
by the SVM classifier.
i love fighting with the one i love
love working on my last day of summer
i enjoy tweeting [user] and not getting a reply
working during vacation is awesome .
can?t wait to wake up early to babysit !
Table 3: Five sarcastic tweets found by the Contrast
method but not the SVM
These tweets are good examples of a positive sen-
timent (love, enjoy, awesome, can?t wait) contrast-
ing with a negative situation. However, the negative
situation phrases are not always as specific as they
should be. For example, ?working? was learned as
a negative situation phrase because it is often neg-
ative when it follows a positive sentiment (?I love
working...?). But the attached prepositional phrases
(?on my last day of summer? and ?during vacation?)
should ideally have been captured as well.
We also examined tweets that were incorrectly la-
beled as sarcastic by the Contrast method. Some
false hits come from situations that are frequently
negative but not always negative (e.g., some peo-
ple genuinely like waking up early). However, most
false hits were due to overly general negative situa-
tion phrases (e.g., ?I love working there? was labeled
as sarcastic). We believe that an important direction
for future work will be to learn longer phrases that
represent more specific situations.
5 Conclusions
Sarcasm is a complex and rich linguistic phe-
nomenon. Our work identifies just one type of sar-
casm that is common in tweets: contrast between a
positive sentiment and negative situation. We pre-
sented a bootstrapped learning method to acquire
lists of positive sentiment phrases and negative ac-
tivities and states, and show that these lists can be
used to recognize sarcastic tweets.
This work has only scratched the surface of pos-
sibilities for identifying sarcasm arising from posi-
tive/negative contrast. The phrases that we learned
were limited to specific syntactic structures and we
required the contrasting phrases to appear in a highly
constrained context. We plan to explore methods for
allowing more flexibility and for learning additional
types of phrases and contrasting structures.
We also would like to explore new ways to iden-
tify stereotypically negative activities and states be-
cause we believe this type of world knowledge is
essential to recognize many instances of sarcasm.
For example, sarcasm often arises from a descrip-
tion of a negative event followed by a positive emo-
tion but in a separate clause or sentence, such as:
?Going to the dentist for a root canal this after-
noon. Yay, I can?t wait.? Recognizing the intensity
of the negativity may also be useful to distinguish
strong contrast from weak contrast. Having knowl-
edge about stereotypically undesirable activities and
states could also be important for other natural lan-
guage understanding tasks, such as text summariza-
tion and narrative plot analysis.
6 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI
/ NBC) contract number D12PC00285. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, or the U.S. Government.
References
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12, pages 995?1005.
Paula Carvalho, Lu??s Sarmento, Ma?rio J. Silva, and
Euge?nio de Oliveira. 2009. Clues for detecting irony
in user-generated contents: oh...!! it?s ?so easy? ;-). In
Proceedings of the 1st international CIKM workshop
on Topic-sentiment analysis for mass opinion, TSA
2009.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
713
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Henry S. Cheang and Marc D. Pell. 2008. The sound of
sarcasm. Speech Commun., 50(5):366?381, May.
Henry S. Cheang and Marc D. Pell. 2009. Acous-
tic markers of sarcasm in cantonese and english.
The Journal of the Acoustical Society of America,
126(3):1394?1405.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL 2010.
Elena Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12).
Roger J. Kreuz Gina M. Caucci. 2012. Social and par-
alinguistic cues to sarcasm. online 08/02/2012, 25:1?
22, February.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies.
Lars Kai Hansen, Adam Arvidsson, Finn Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news - affect and virality in twitter. In
The 2011 International Workshop on Social Comput-
ing, Network, and Services (SocialComNet 2011).
Roger Kreuz and Gina Caucci. 2007. Lexical influences
on the perception of sarcasm. In Proceedings of the
Workshop on Computational Approaches to Figurative
Language.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for detect-
ing sarcasm in tweets #not. In Proceedings of the 4th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, WASSA
2013.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of the 14th International
World Wide Web conference (WWW-2005).
Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for online
dialogue. In Proceedings of the Workshop on Lan-
guage Analysis in Social Media.
Finn Arup Nielsen. 2011. A new anew: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages (http://arxiv.org/abs/1103.2903).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In The 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL 2013).
Katherine P. Rankin, Andrea Salazar, Maria Luisa Gorno-
Tempini, Marc Sollberger, Stephen M. Wilson, Dani-
jela Pavlic, Christine M. Stanley, Shenly Glenn,
Michael W. Weiner, and Bruce L. Miller. 2009. De-
tecting sarcasm from paralinguistic cues: Anatomic
and cognitive correlates in neurodegenerative disease.
Neuroimage, 47:2005?2015.
Joseph Tepperman, David Traum, and Shrikanth
Narayanan. 2006. ?Yeah right?: Sarcasm recogni-
tion for spoken dialogue systems. In Proceedings of
the INTERSPEECH 2006 - ICSLP, Ninth International
Conference on Spoken Language Processing.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Product
Reviews. In Proceedings of the Fourth International
Conference on Weblogs and Social Media (ICWSM-
2010), ICWSM 2010.
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative adjectives: An unsuper-
vised criterion to extract subjective adjectives. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 534?
539, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A System for Subjec-
tivity Analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations, pages 34?35, Vancouver,
Canada, October.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the 2005
Human Language Technology Conference / Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
714
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 98?108,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
User Type Classification of Tweets with Implications for Event
Recognition
Lalindra De Silva and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{alnds,riloff}@cs.utah.edu
Abstract
Twitter has become one of the foremost
platforms for information sharing. Conse-
quently, it is beneficial for the consumers
of Twitter to know the origin of a tweet,
as it affects how they view and inter-
pret this information. In this paper, we
classify tweets based on their origin, ex-
ploiting only the textual content of tweets.
Specifically, using a rich, linguistic fea-
ture set and a supervised classifier frame-
work, we classify tweets into two user
types - organizations and individual per-
sons. Our user type classifier achieves an
89% F
1
-score for identifying tweets that
originate from organizations in English
and an 87% F
1
-score for Spanish. We
also demonstrate that classifying the user
type of a tweet can improve downstream
event recognition tasks. We analyze sev-
eral schemes that exploit user type infor-
mation to enhance Twitter event recogni-
tion and show that substantial improve-
ments can be achieved by training separate
models for different user types.
1 Introduction
Twitter has become one of the most widely used
social media platforms, with users (as of March
2013) posting approximately 400 million tweets
per day (Wickre, 2013). This public data serves
as a potential source for a multitude of informa-
tion needs, but the sheer volume of tweets is a bot-
tleneck in identifying relevant content (Becker et
al., 2011). De Choudhury et al. (2012) showed
that the user type of a Twitter account is an impor-
tant indicator in sifting through Twitter data. The
knowledge of a tweet?s origin has potential impli-
cations on the nature of the content to an end user
(e.g., credibility, location, etc). Also, certain types
of events are more likely to be reported by individ-
ual persons (e.g., local events) whereas organiza-
tions generally report events that are of interest to
a wider audience.
The first part of our research focuses on user
type classification in Twitter. De Choudhury et
al. (2012) addressed this problem by examining
meta-information derived from the Twitter API.
In contrast, the goal of our work is to classify
tweets, based solely on their textual content. We
highlight several reasons why this can be advanta-
geous. One reason is that people frequently share
content from other sources, but the shared con-
tent often appears in their Twitter timeline as if
it was their own. Consequently, a tweet that was
posted by an individual may have originated from
an organization. Moreover, meta-information can
sometimes be infeasible to obtain given the rate
limits
1
and there are times when profile informa-
tion for a user account is unavailable or ambigu-
ous (e.g., users often leave their profile informa-
tion blank or write vague entries). Therefore, we
believe there is value in being able to infer the
type of user who authored a tweet based solely on
its textual content. Potentially, our methods for
user type classification based on textual content
can also be combined with methods that examine
user profile data or other meta-data, since they are
complementary sources of information.
In this paper, we present a classifier that tries to
determine whether a tweet originated from an or-
ganization or a person using a rich, linguistically-
motivated feature set. We design features to rec-
ognize linguistic characteristics, including senti-
ment and emotion expressions, informal language
use, tweet style, and similarity with news head-
lines. We evaluate our classifier on both English
and Spanish Twitter data and find that the classifier
achieves an 89% F
1
-score for identifying tweets
that originate from organizations in English and a
1
https://dev.twitter.com/docs/rate-limiting/1.1/limits
98
87% F
1
-score for Spanish.
The second contribution of this paper is to
demonstrate that user type classification can im-
prove event recognition in Twitter. We conduct a
study of event recognition for civil unrest events
and disease outbreak events. Based on statistics
from manually annotated tweets, we found that
organization-tweets are much more likely to men-
tion these events than person-tweets. We then in-
vestigate several approaches to incorporate user
type information into event recognition models.
Our best results are produced by training sepa-
rate event classifiers for tweets from different user
types. We show that user type information con-
sistently improves event recognition performance
for both civil unrest events and disease outbreak
events and for both English and Spanish tweets.
2 Related Work
Our work is most closely related to that of De
Choudhury et al. (2012), which proposed methods
to classify Twitter users into three categories: 1)
Journalists/media bloggers, 2) Organizations and
3) Ordinary Individuals. They employed features
derived from social network structure, user ac-
tivity and users? social interaction behaviors, and
named entities and historical topic distributions in
tweets. In contrast, our work classifies isolated
tweets into two different user types, based on their
textual content. Consequently, our work can pro-
duce different user type labels for different tweets
by the same user, which can help identify shared
content not authored by the user.
Another body of related work tries to classify
Twitter users along other dimensions such as eth-
nicity and political orientation (Pennacchiotti and
Popescu, 2011; Cohen and Ruths, 2013). Gender
inference in Twitter has also garnered interest in
the recent past (Ciot et al., 2013; Liu and Ruths,
2013; Fink et al., 2012). Researchers have also fo-
cused on user behaviors showcased in Twitter in-
cluding the types of messages posted (Naaman et
al., 2010), social connections (Wu et al., 2011),
user responses to events (Popescu and Pennac-
chiotti, 2011) and behaviors related to demograph-
ics (Volkova et al., 2013; Mislove et al., 2011; Rao
et al., 2010).
Event recognition is another area that continues
to attract a lot of interest in social media. Previ-
ous work has investigated event identification and
extraction (Jackoway et al., 2011; Becker et al.,
2009; Becker et al., 2010; Ritter et al., 2012),
event discovery (Benson et al., 2011; Sakaki et al.,
2010; Petrovi?c et al., 2010), tracking events over
time (Kim et al., 2012; Sayyadi et al., 2009) and
event retrieval over archived Twitter data (Metzler
et al., 2012). While our work focuses on user type
classification, we show that the user type of a tweet
is an important piece of information that can be
beneficial in event recognition models.
3 Twitter User Types
Twitter user types can be analyzed in different
granularities and across different dimensions. We
follow a high-level categorization of user types
into organizations and individual persons. While
we acknowledge the existence of other user types,
such as automated bots, we focus only on the orga-
nization and individual person user types for our
research.
? Banking Commission Split Over EU Bonus
Cap http://t.co/GSSbmHAWsQ
? Apple likely to introduce smaller, cheaper
iPad mini today http://t.co/TuKBHZ3z
? Diet Coke may be the new #2, but U.S. soda
market is shrinking http://ow.ly/1bSNnh
Sample Tweets from Organizations
? @john It?s a stress fracture. Nah, no dancing
was involved!
? My gawd feels like my head?s gonna explode
? Watching The Rainmaker. It has totally
sucked me in :D #notsomuch lol
Sample Tweets from Individual Persons
Figure 1: Sample tweets from individual persons
and organizations
From a linguistic point of view, we can ob-
serve several distinguishing characteristics be-
tween organization- and person-tweets. As shown
in Figure 1, organization-tweets are often char-
acterized by headline-like language usage, struc-
tured style, a lack of conversation with the au-
dience (i.e., few reply-tweets), and hyperlinks to
original articles. In contrast, person-tweets show
significant language variability including short-
hand terms, conversational behavior, slang and
profanity, expressions of emotion, and an overall
relaxed usage of language.
99
3.1 Data Acquisition for User Types
To create our data sets, we archived tweets (us-
ing Twitter Streaming API) for six months, be-
ginning February 1
st
, 2013. We then used a lan-
guage filter (Lui and Baldwin, 2012) to separate
out the English and Spanish tweets. Also, in the
data sets we created (see below), we removed du-
plicates, retweets and any tweet with less than 5
words. Given that large-scale human annotation
is expensive, we explored several heuristics to re-
liably compile a large gold standard collection of
person- and organization-tweets.
3.1.1 Acquiring Person-tweets
To acquire person-tweets, we devised a person
heuristic, focusing on the name and the profile de-
scription fields in each user account correspond-
ing to a tweet. We first gathered lists of person
names (first names and surnames), for both En-
glish and Spanish, using census data
2
and online
resources
3
. We also compiled a list of common
organization terms (e.g., agency, institute, com-
pany, etc) in both English and Spanish.
The person heuristic labels a tweet as a person-
tweet if
[
no organization term is in the name or
the profile description fields
]
AND
[
all the words
in the name field are person names OR the profile
description field starts with either ?I?m? or ?I am?
]
4
. To assess the accuracy of the person heuris-
tic, we also performed a manual annotation task.
We employed two annotators and provided them
with guidelines of what constitutes an individual
person?s Twitter account. We defined an individ-
ual person as someone who uses Twitter in their
day-to-day life to post information about his/her
daily activities, update personal status messages,
comment about societal issues and/or interact with
close social circles. The annotators were able to
see the name, profile description, location and url
fields of the Twitter user account and were asked to
label each account as individual, not individual or
undetermined. To calculate annotator agreement
between the two annotators, we gave them 100
Twitter accounts, corresponding to English tweets
collected using the person heuristic. The inter-
annotator agreement (IAA) was .98 (raw agree-
ment) and .97 (G-Index score). We did not use
2
http://www.census.gov/genealogy/www/
data/1990surnames/names_files.html
3
http://genealogy.familyeducation.com/
browse/origin/spanish
4
Corresponding terms were used for Spanish
Cohen?s Kappa (?) as it is known to underestimate
agreement (known as Kappa Paradox) when one
category dominates. We then released another 250
accounts to each of the annotators, giving us a total
of 600 manually labeled accounts
5
.
In the distribution of labels assigned by the hu-
man annotators for these 600 accounts, 91.5% was
confirmed as belonging to individual persons. 5%
was identified as not individual whereas 3.5% was
labeled as undetermined. These numbers corrob-
orate our claim that the person heuristic is a valid
approximation for acquiring person-tweets.
However, limiting our person-tweets to those
from accounts identified with the person heuris-
tic could introduce bias (i.e., it may consider only
the people who provided more complete profile
information). To address this issue, we looked
into additional heuristics that are representative
of individual persons? Twitter accounts. We ob-
served that applications designed specifically for
hand-held devices (e.g., twitter for iphone) are fre-
quently used to author tweets and used by individ-
ual persons. Organizations, on the other hand, pri-
marily use the Twitter web tool and content man-
agement software applications to create, manage
and post content to Twitter.
To further investigate our observation, we ex-
tracted the source information (i.e., the software
applications used to author tweets) for a collection
of 1.2 million English tweets from our tweet pool,
for a random day, and identified those that were
clearly hand-held device apps and covered at least
1% of the tweets. Table 1 shows the distribution
of these hand-held device apps, which together ac-
counted for approximately 66% of all tweets.
Hand-held Device App % of Tweets
twitter for iphone 37.11
twitter for android 16.50
twitter for blackberry 5.50
twitter for ipad 2.55
mobile web (m2) 1.46
ios 1.36
echofon 1.29
ALL 65.77
Table 1: Percentage of (English) tweets authored
from hand-held device apps
To evaluate our hypothesis that a high percent-
age of these tweets are person-tweets, we carried
out another manual annotation task. We selected
5
We adjudicated the disagreements in the initial 100 Twit-
ter accounts.
100
100 English Twitter accounts whose tweets were
generated using one of the above hand-held de-
vice apps and asked the two annotators to label
them using the same guidelines. For this task, the
IAA was .84 (raw agreement) and .76 (G-Index
score). As before, we released another 250 ac-
counts to each of the annotators. In these 600 user
accounts, 87.1% was confirmed to be individual
persons. Only 1% was judged to be clearly not
individual whereas 11.9% was labeled as unde-
termined.
3.1.2 Acquiring Organization-tweets
Designing similar heuristics to identify
organization-tweets proved to be difficult.
Organizations describe themselves in numerous
ways, making it difficult to automatically identify
their names in user profiles. Furthermore, organi-
zation names often appear in individual persons?
accounts because they mention their employers
(e.g., I?m a software engineer at Microsoft Corpo-
ration). Therefore, to acquire organization-tweets,
we relied on web-based directories of organiza-
tions (e.g., www.twellow.com) and gathered
their tweets using the Twitter API. We used 58
organization accounts for English tweets and 83
accounts for Spanish.
3.1.3 Complete Data Set
We created a data set of 200,000 tweets for each
language, consisting of 90% person-tweets and
10% organization-tweets. Among the 180,000
person-tweets, 132,000 (66% of 200,000) were
tweets whose source was a hand-held device
app. To collect the remaining 48,000 (24%
of 200,000) of the person-tweets, we relied
on the person heuristic. Finally, we gathered
20,000 organization-tweets using the web directo-
ries mentioned previously. In doing so, to ensure
that we had a balanced mix of organizations, each
organization contributed with a maximum of 500
tweets.
4 User Type Classification
To automatically distinguish person-tweets from
organization-tweets, we trained a supervised clas-
sifier using N-gram features, an organization
heuristic, and a linguistic feature set categorized
into six classes. For the classification algorithm,
we employed a Support Vector Machine (SVM)
with a linear kernel, using the LIBSVM package
(Chang and Lin, 2011). For the features that rely
on part-of-speech (POS) tags, we used the English
Twitter POS tagger by Gimpel et al. (2011) and
another tagger trained on the CoNLL 2002 shared
task data for Spanish (Tjong Kim Sang, 2002) us-
ing the OpenNLP toolkit (OpenSource, 2010).
4.1 N-gram Features
We started off by introducing N-gram features to
capture the words in a tweet. Specifically, we
trained a supervised classifier using unigram and
bigram features encoded with binary values. In se-
lecting the N-gram features, we discarded any N-
gram that appears less than five times in the train-
ing data.
4.2 Organization Heuristic
Following observations by Messner et al. (2011),
we combined two simple heuristic rules to flag
tweets that are likely to be from an organization.
The first observation is that ?replies? (i.e., @user
mentions at the beginning of a tweet) are uncom-
mon in organization-tweets. Hence, if a tweet is a
reply, it is likely to be a person-tweet. The second
observation is that organization-tweets frequently
include a web link to external content.
Our organization heuristic, therefore, com-
bined these two properties. If the tweet is not a
reply AND contains a web link, we labeled it as
an organization-tweet. Otherwise, we labeled it
as a person-tweet. In Section 5, we evaluate this
heuristic as a classification rule on its own, and
also incorporate its label as a feature in our classi-
fier.
4.3 Linguistic Features
In the following sections, we describe our linguis-
tic features and the intuitions in designing them.
4.3.1 Emotion and Sentiment
Twitter is a platform where individuals often ex-
press emotion. We detected emotions using four
feature types: 1) interjections, 2) profanity, 3)
emoticons and 4) overall sentiment of the tweet.
Interjections, profanity, and emoticons are
widely used by individuals to convey emotion,
such as anger, surprise, happiness, etc. To iden-
tify these three feature types, we used a combina-
tion of POS tags in the English tagger (which con-
tains tags for interjections, emoticons, etc), com-
piled lists of interjections and profanity from the
101
web for both English
6
and Spanish
7
and regular
expression patterns for emoticons.
We also included sentiment features using the
sentiment140 API
8
(Go et al., 2009). This API
provides a sentiment label (positive, negative or
neutral) for a tweet corresponding to its overall
sentiment. We expect person-tweets to show more
positive and negative sentiment and organization-
tweets to be more neutral.
4.3.2 Similarity to News Headlines
Earlier, we observed that organization-tweets are
often similar to news headlines. To exploit this ob-
servation, we introduced four features using lan-
guage models and verb categories.
First, we collected 3 million person-tweets, for
each language, using the person heuristic de-
scribed in Section 3.1. Second, we collected an-
other 3 million news headlines from each of the
English and Spanish Gigaword corpora (Parker
et al., 2009; Mendonca et al., 2009). Using
these two data sets, we built unigram and bigram
language models (with Laplace smoothing) for
person-tweets and for news headlines. Given a
new tweet, we calculated the probability of the
tweet using both the person-tweet and headline
language models. We defined a binary feature that
indicates which unigram language model (person-
tweet model vs. headline model) produced the
highest probability. A similar feature is defined
for the bigram language models.
We also observed that certain verbs are pre-
dominantly used in news headlines and are rarely
associated with colloquial language (therefore, in
person-tweets). Similarly, we observed verbs that
are much more likely to be used by individual per-
sons. To identify the most discriminating verbs,
we ranked verbs appearing more than 5 times in
the collected news headlines and person-tweets
based on the following probabilities:
p(h|verb) =
Frequency of verb in headlines
Frequency of verb in all instances
p(pt|verb) =
Frequency of verb in person-tweets
Frequency of verb in all instances
The verbs were sorted by probability and we re-
tained two disjoint sets of verbs, 1) the verbs most
6
http://www.noswearing.com/dictionary
7
http://nawcom.com/swearing/mexican_
spanish.htm
8
http://help.sentiment140.com/api
representative of headlines (i.e., headline verbs),
selected by applying a threshold of p(h|verb) >
0.8 and 2) verbs most representative of person-
tweets (i.e., personal verbs), with a similar thresh-
old of p(pt|verb) > 0.8. We introduced two bi-
nary features that look for verbs in the tweet from
these two learned verb lists. The top-ranked verbs
for each category are displayed in Table 2. The
learned headline verbs tend to be more formal
and are often used in business or government con-
texts (e.g., revoke, granting, etc) whereas the per-
sonal verbs tend to represent personal activities,
communications, and emotions (e.g., hate, sleep,
etc). In total, we learned 687 headline verbs and
2221 personal verbs for English, and 1924 head-
line verbs and 5719 personal verbs for Spanish.
Headline verbs: aided, revoke, issued, broaden, tes-
tify, leads, postponing, forged, deepen, hijacked, raises,
granting, honoring, pledged, departing, suspending, cit-
ing, compensate, preserved, weakening, differing
Personal verbs: raining, sleep, hanging, hate, march-
ing, teaching, sway, having, risk, lurk, screaming, tag-
ging, disturb, baking, exaggerate, pinch, enjoy, shred-
ding, force, hide, wreck, saved, cooking, blur, told
Table 2: Top-ranked representative verbs learned
from headlines and person-tweets
4.3.3 1
st
and 2
nd
Person Pronouns
Person-tweets often include self-references, in
the form of first-person pronouns and their vari-
ant forms (e.g., possessive, reflexive), while
organization-tweets rarely contain self-references.
Also, organizations often address their audience
using second-person pronouns in tweets (e.g., Will
you High Five the #Bruins or #Blackhawks? Sign
up for a chance to win a trip to the Cup Final:
http://t.co/XQP8ZDOINV). To capture these char-
acteristics, we included two binary features that
look for 1
st
and 2
nd
person pronouns in a tweet.
4.3.4 NER Features
We hypothesized that organization-tweets will
carry more named entities and proper nouns. For
English tweets, we identified Persons, Organiza-
tions and Locations using the Named Entity Rec-
ognizer (NER) from Ritter et al. (2011). For
Spanish tweets, we used NER models trained on
CoNLL 2002 shared task data for Spanish. The
features were encoded as three values, represent-
ing the frequency of each entity type in a tweet.
102
English Spanish
P R F
1
P R F
1
ULM: Unigram Language Model 71.63 63.18 67.14 66.14 60.43 63.16
BLM: Bigram Language Model 81.46 49.17 61.32 80.03 51.08 62.36
NGR: SVM with N-grams 86.02 62.76 72.57 85.76 66.56 74.95
OrgH: Organization Heuristic 66.87 91.08 77.12 65.32 81.44 72.49
NGR + OrgH 82.26 86.82 84.48 83.85 85.17 84.50
NGR + OrgH + Linguistic Features 89.01 89.40 89.20
?
87.59 85.47 86.52
?
Table 3: User type classification results with Precision (%), Recall (%) and F
1
-Score (%). ? denotes
statistical significance at p < 0.01 compared to NGR + OrgH
4.3.5 Informal Language Features
Person-tweets often showcase erratic and casual
use of language, whereas organization-tweets tend
to have (relatively) more grammatical language
usage. Hence, we introduced a feature to deter-
mine the informality of a tweet. Specifically, we
check if a tweet begins with an uppercase letter or
not, and whether sentences are properly separated
with punctuation. To accomplish this, we used
regular expression patterns that look for capital-
ized characters following punctuation and white-
space characters. We also added a feature to check
if all the letters in the tweet are lowercased. Use of
elongated words (e.g., cooooooool) for emphasis,
is another property of person-tweets and we cap-
tured this property by identifying words with three
or more repetitions of the same character.
To comply with the 140 character length restric-
tion of a tweet, person-tweets often employ ad-
hoc short-hand usage of words that omit or replace
characters with a phonetic substitute (e.g., 2mrw,
good n8). We used lists of common abbreviations
found in social media
9
collected from the web and
a binary feature was set if a tweet contained an in-
stance from these lists.
4.3.6 Twitter Stylistic Features
One can also notice structural properties that are
prevalent in either user type. News organiza-
tions often append a topic descriptor to the be-
ginning of a tweet (e.g., Petraeus affair: Woman
who complained of harassing emails identified
http://t.co/hpyLQYeL). To encode this behavior,
we employed a simple heuristic that looked for a
semicolon or a hyphen within the first three words
of a tweet. Also, person-tweets employ heavy use
of hashtags so we included the frequency of hash-
tags in a tweet as a single feature. We added two
more features in the form of the length of the tweet
9
http://www.noslang.com/dictionary/
full/
and the frequency of @user mentions in the tweet.
5 Evaluation of User Type Classification
In this section, we discuss and evaluate our user
type classifier. All of the experiments were carried
out using five-fold cross-validation, using data sets
described in Section 3.1. In these experiments, we
maintained the separation of organization-tweets
at a user-account level in order to avoid tweets
from one organization appearing in both train and
test sets.
5.1 User Type Classifier Results
We first evaluated several baseline systems to as-
sess the difficulty of the user type classification
task. We report precision, recall and F
1
-score with
organization-tweets as the positive class.
To evaluate our hypothesis that organization-
tweets are similar to news headlines, we first pre-
dicted user types using only the unigram and bi-
gram language models described in Section 4.3.2.
As shown in Table 3 (ULM & BLM), unigram
models were capable of discerning organization-
tweets with 71% and 66% precision on English
and Spanish tweets, respectively. This is sub-
stantial performance given that the random chance
of labeling an organization-tweet (i.e., precision)
is merely 10%. The bigram models show ?
80% precision whereas the unigram models show
higher recall.
As another baseline, we evaluated an SVM clas-
sifier that uses only N-gram features. As Table 3
shows, the N-gram classifier (NGR) achieved very
high precision (86%) for both English and Spanish
tweets. However, it yielded relatively moderate re-
call (63% for English and 67% for Spanish).
We then evaluated the organization heuris-
tic (OrgH) all by itself. The heuristic identi-
fies two common characteristics of organization-
tweets and as expected, it achieved substantial re-
call (91% for English and 81% for Spanish) but
103
English Spanish
P R F
1
P R F
1
NGR + OrgH 82.26 86.82 84.48 83.85 85.17 84.50
+ Emotion and Sentiment Features 86.58 86.41 86.50 85.91 84.19 85.05
+ Features Derived from News Headlines 87.83 87.10 87.46 86.68 84.05 85.35
+ 1
st
and 2
nd
Person Pronouns 87.88 88.53 88.20 86.61 84.38 85.48
+ NER Features 88.05 88.75 88.40 86.71 84.69 85.69
+ Informal Language Features 88.39 89.14 88.77 86.89 85.31 86.09
+ Twitter Stylistic Features 89.01 89.40 89.20 87.59 85.47 86.52
NGR + OrgH + Linguistic Features 89.01 89.40 89.20 87.59 85.47 86.52
Table 4: Linguistic feature ablation with Precision (%), Recall (%) and F
1
-Score (%)
with mediocre precision.
These results show that the N-gram classifier
achieved high precision whereas the organization
heuristic achieved high recall. To exploit the best
of both worlds, we evaluated another model (NGR
+ OrgH) that added the organization heuristic as
an additional feature for the N-gram classifier.
This system fares better than all the previous mod-
els, achieving 82% precision with 87% recall for
English and 84% precision with 85% recall for
Spanish.
Next, we show the benefits obtained from
adding the linguistic feature set. As the final row
in Table 3 shows, having incorporated all the lin-
guistic features, our final system showed an im-
provement of 7% precision and 3% recall on En-
glish tweets for an overall F
1
-score gain of approx-
imately 5%. On Spanish tweets, the same incre-
ments were 4%, 0.3% and 2%, respectively. This
final classifier is statistically significantly better
than the model without linguistic features (NGR +
OrgH) for both languages at the p < 0.01 level,
analyzed using a paired booststrap test drawing
10
6
samples with repetition from test data, as de-
scribed in Berg-Kirkpatrick et al. (2012).
5.2 Analysis of Linguistic Features
Having observed that linguistic features improved
user type classification, we evaluated the impact
of each type of linguistic feature using an ablation
study. Table 4 shows the classifier performance
when each of the features types was added cumu-
latively over the NGR + OrgH baseline.
We immediately see a 4% and 2% precision
gain by adding emotion and sentiment features,
for English and Spanish, respectively. Adding fea-
tures derived from news headlines, we observe
that the classifier fares better, improving precision
for both languages and improving recall for En-
glish. 1
st
and 2
nd
person pronouns improved re-
call on English data but had little impact on Span-
ish data. The NER features produced very small
gains in both languages. The informal language
features increased recall from 84.69% to 85.31%
on Spanish tweets. Finally, the Twitter stylistic
features gained 0.7% more precision for both lan-
guages. Overall, the feature types that contributed
the most were the emotion/sentiment features, the
news headline features, and the Twitter stylistic
features.
6 Twitter Event Recognition
Twitter provides a facility where users can search
for tweets using keywords. However, keyword-
based queries for events can often lead to myriad
irrelevant results due to different senses of key-
words (polysemy) and figurative or metaphorical
use of keywords. For instance, a Twitter search
for civil unrest events with a few representative
keywords (e.g., strike, rally, riot, etc.) can often
lead to results referring to sports events, such as a
bowling strike or a tennis rally or where the key-
words are used figuratively (e.g., She?s a riot!). In
this section, we investigate if the user type of a
tweet can help cut through such ambiguity. Specif-
ically, we hypothesize that event keywords may be
used more consistantly and with less ambiguity in
organization-tweets, and therefore user type infor-
mation may be helpful in improving event recog-
nition.
To explore our hypothesis that the user type can
influence the event relevance of a tweet, we con-
structed a set of experiments using two types of
events - civil unrest events and disease outbreaks.
Civil unrest refers to forms of public disturbance
that affect the order of a society (e.g., strikes,
protests, etc.) whereas a disease outbreak refers to
an unusual or widespread occurrence of a disease
(e.g., a measles outbreak).
104
English Spanish
Civil Unrest Disease Outbreaks Civil Unrest Disease Outbreaks
Person-tweets 5.27% 9.52% 9.32% 5.00%
Organization-tweets 36.54% 39.34% 51.66% 44.06%
All-tweets 12.50% 20.07% 14.72% 13.22%
Table 6: Percentage of event-relevant tweets in 4000 tweets with keywords for each category
English Civil Unrest: protest, protested, protesting,
riot, rioted, rioting, rally, rallied, rallying, marched,
marching, strike, striked, striking
English Disease Outbreaks: outbreak, epidemic, in-
fluenza, h1n1, h5n1, pandemic, quarantine, cholera,
ebola, flu, malaria, dengue, hepatitis, measles
Spanish Civil Unrest: protesta, protestar, amoti-
naron, protestaron, protestaban, protestado, amotinarse,
amotinaban, marcha, huelga, amotinando, protestando,
amotinado
Spanish Disease Outbreaks: brote, epidemia, in-
fluenza, h1n1, h5n1, pandemia, cuarantena, sarampi?on,
c?olera, ebola, malaria, dengue, hepatitis, gripe
Table 5: Keywords used to query Twitter for two
types of events in English and Spanish
6.1 Data Acquisition for Event Recognition
We began by collecting tweets that contained at
least one of the keywords listed in Table 5, using
the Twitter search API, and we set up an annota-
tion task using Amazon Mechanical Turk (AMT)
annotators. First, we created guidelines to distin-
guish event-relevant tweets from irrelevant tweets
and annotated 300 tweets for each of the four cat-
egories (i.e., English Civil Unrest, Spanish Civil
Unrest, English Disease Outbreaks and Spanish
Disease Outbreaks).
We released 200 tweets in each category for
annotation to three AMT annotators
10
. We used
these 200 tweets to calculate pair-wise IAA using
Cohen?s Kappa (?) which we report in Table 7.
The IAA scores were generally good, ranging
from 0.67 to 0.89. Each annotator subsequently la-
beled 2000 tweets, yielding a total of 6000 tweets
for each category. In each of these 6000 tweet sets,
we randomly separated 2000 tweets as tuning data
and 4000 as test data.
First, we applied our user type classifier to these
tweets and analyzed the number of true event
tweets for each user type. Table 6 shows the per-
centage of true event tweets in the entire test set,
as well as the percentage of event tweets for each
10
We first released 100 tweets in each category to AMT
and enlisted 10 annotators. After calculating IAA on these
100 tweets, we retained 3 annotators who had the highest
agreement with our annotations.
English Spanish
Civil Unrest .89, .88, .77 .74, .74, .67
Disease Outbreaks .82, .73, .68 .84, .83, .80
Table 7: Pair-wise inter-annotator agreement
(IAA) measured using Cohen?s Kappa (?) on 200
tweets among the three AMT annotators for each
event type in each language
user type. Overall, the percentage of true event
tweets in each test set is ? 20%. This means that
most of the tweets (> 80%) with event keywords
do not discuss an event, confirming the unreliabil-
ity of using event keywords alone.
However, there is a substantial difference in
the density of true event tweets between the two
user types. Across both civil unrest and dis-
ease outbreaks, and for both languages, we see
a much higher percentage of organization-tweets
with event keywords mentioning an event than
person-tweets with event keywords. Table 6 shows
that, in English civil unrest category, organization-
tweets are 7 times more likely (36.54% as opposed
to 5.27%) to report an actual event than person-
tweets with the same keywords. In the English dis-
ease outbreaks category, organization-tweets are
4 times more likely to report an event (39.34%
vs. 9.52%). We notice similar observations in the
Spanish tweets too.
6.2 Event Recognition Results
In this section, we evaluate the impact of user type
information by introducing a simple baseline ex-
periment for Twitter event recognition followed
by several schemes that we devised to incorporate
user type information in more principled ways.
First, we trained a supervised classifier to pre-
dict the probability of a tweet being event-relevant
using only unigrams and bigrams as features, en-
coded with binary values. This baseline system is
agnostic to the user type. We used the SVM Platt
method implementation of LIBSVM (Chang and
Lin, 2011) and carried out experiments using five-
fold cross-validation. As Table 8 shows, this ap-
105
English Spanish
P R F
1
P R F
1
Civil Unrest Events
User type-agnostic classifier 80.97 50.20 61.98 77.51 60.37 67.88
User type included as a feature 80.00 50.40 61.84 77.19 61.56 68.50
(?
p
, ?
o
) optimized for F
1
-score 60.50 72.60 66.00 64.97 78.57 71.13
User type-specific classifier 79.34 63.61 70.61
?
79.20 81.89 80.52
?
Disease Outbreak Events
User type-agnostic classifier 83.15 55.99 66.92 80.49 56.14 66.15
User type included as a feature 83.46 55.36 66.57 80.93 59.36 68.48
(?
p
, ?
o
) optimized for F
1
-score 75.10 66.58 70.58 68.94 72.58 70.71
User type-specific classifier 80.35 66.07 72.51
?
82.20 74.26 78.03
?
Table 8: Event recognition results showing Precision (%), Recall (%) and F
1
-Score (%), for the two
event types in English and Spanish. ? denotes statistical significance at p < 0.01 compared to the
baseline (User type-agnostic classifier)
proach achieved 62% F
1
-score in English and 68%
F
1
-score in Spanish, for civil unrest events. For
disease outbreak events, the corresponding values
were 67% and 66%.
As our first attempt to incorporate user type in-
formation, we added the user type label as one ad-
ditional feature. As shown in Table 8, the added
feature yielded small gains for Spanish but made
little difference for English.
Given our initial hypothesis (and evidence in
Table 6) about events and organization-tweets,
we would prefer to be aggressive in labeling
organization-tweets as event-relevant. One way to
accomplish this with a trained probabilistic classi-
fier is to assign different probability thresholds to
person- and organization-tweets. To acquire the
optimal threshold parameters for person-tweets
(?
p
) and organization-tweets (?
o
), we performed a
grid-based threshold sweep on tuning data and op-
timized with respect to F
1
-scores. Table 8 shows
that this approach yielded substantial recall gains
for all four categories and produced the best F
1
-
scores thus far.
A more principled approach is to create two
completely different classifiers, one for each user
type. Each classifier can then model the vocabu-
lary and word associations that are most likely to
occur in tweets of that type. Using five-fold cross-
validation, we train separate models for person-
and organization-tweets. During event recogni-
tion, we first apply our user type classifier to a
tweet and then apply the appropriate event recog-
nition model. As shown in the final rows in Ta-
ble 8, this method consistently outperforms the
other approaches. Compared to the best compet-
ing method, the user type-specific classifiers pro-
duced F
1
-score gains of 4.6% and 9.4% for En-
glish and Spanish civil unrest events, and F
1
-score
gains of 2% and 7.3% for English and Spanish dis-
ease outbreak events.
7 Conclusion
In this work, we tackled the problem of classify-
ing tweets into two user types, organizations and
individual persons, based on their textual content.
We designed a rich set of features that exploit
different linguistic aspects of tweet content, and
demonstrated that our classifier achieves F
1
-scores
of 89% for English and 87% for Spanish. We also
presented results showing that organization-tweets
with event keywords have a much higher den-
sity of event mentions than person-tweets with the
same keywords and showed the benefits of incor-
porating user type information into event recog-
nition models. Our results showed that creating
separate event recognition classifiers for different
user types yields substantially better performance
than using a single event recognition model on all
tweets.
8 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D12PC00285.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
106
References
Hila Becker, Mor Naaman, and Luis Gravano. 2009.
Event identification in social media. In WebDB.
Hila Becker, Mor Naaman, and Luis Gravano. 2010.
Learning similarity metrics for event identification
in social media. In Proceedings of the Third ACM
International Conference on Web Search and Data
Mining, WSDM ?10, pages 291?300, New York,
NY, USA. ACM.
H. Becker, M. Naaman, and L. Gravano. 2011. Select-
ing quality twitter content for events. In Proceed-
ings of the Fifth International AAAI Conference on
Weblogs and Social Media (ICWSM11).
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds. In The 49th Annual
Meeting of the Association for Computational Lin-
guistics, Portland, Oregon, USA. To appear.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 995?1005. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Raviv Cohen and Derek Ruths. 2013. Classifying po-
litical orientation on twitter: Its not easy! In Seventh
International AAAI Conference on Weblogs and So-
cial Media.
M. De Choudhury, N. Diakopoulos, and M. Naaman.
2012. Unfolding the event landscape on twitter:
classification and exploration of user categories. In
Proceedings of the ACM 2012 conference on Com-
puter Supported Cooperative Work, pages 241?244.
ACM.
Clayton Fink, Jonathon Kopecky, and Maksym
Morawski. 2012. Inferring gender from the content
of tweets: A region specific example. In ICWSM.
K. Gimpel, N. Schneider, B. O?Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N.A. Smith. 2011. Part-of-speech
tagging for twitter: annotation, features, and exper-
iments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42?47. Association for Computa-
tional Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
A. Jackoway, H. Samet, and J. Sankaranarayanan.
2011. Identification of live news events using twit-
ter. In Proceedings of the 3rd ACM SIGSPATIAL
International Workshop on Location-Based Social
Networks, page 9. ACM.
M. Kim, L. Xie, and P. Christen. 2012. Event diffusion
patterns in social media. In Sixth International AAAI
Conference on Weblogs and Social Media.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter.
Marco Lui and Timothy Baldwin. 2012. langid. py:
An off-the-shelf language identification tool. In
Proceedings of the ACL 2012 System Demonstra-
tions, pages 25?30. Association for Computational
Linguistics.
Angelo Mendonca, David Andrew Graff, Denise
DiPersio, Linguistic Data Consortium, et al. 2009.
Spanish gigaword second edition. Linguistic Data
Consortium.
M. Messner, M. Linke, and A. Eford. 2011. Shov-
eling tweets: An analysis of the microblogging
engagement of traditional news organizations. In
International Symposium on Online Journalism,
UT Austin, available at: http://online. journalism.
utexas. edu/2011/papers/Messner2011. pdf (last ac-
cessed April 3, 2011).
D. Metzler, C. Cai, and E. Hovy. 2012. Structured
event retrieval over microblog archives. In Proceed-
ings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
646?655.
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, and J Niels Rosenquist. 2011.
Understanding the demographics of twitter users.
ICWSM, 11:5th.
M. Naaman, J. Boase, and C.H. Lai. 2010. Is it re-
ally about me?: message content in social aware-
ness streams. In Proceedings of the 2010 ACM con-
ference on Computer supported cooperative work,
pages 189?192. ACM.
OpenSource. 2010. Opennlp: http :
//opennlp.sourceforge.net/.
Robert Parker, Linguistic Data Consortium, et al.
2009. English gigaword fourth edition. Linguistic
Data Consortium.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to twitter user classifi-
cation.
107
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 181?189. Association for Computa-
tional Linguistics.
A.M. Popescu and M. Pennacchiotti. 2011. Dancing
with the stars, nba games, politics: An exploration of
twitter users response to events. In Proceedings of
the International AAAI Conference on Weblogs and
Social Media.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37?44. ACM.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
A. Ritter, O. Etzioni, S. Clark, et al. 2012. Open
domain event extraction from twitter. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 1104?1112. ACM.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors. In Proceedings of the 19th inter-
national conference on World wide web, pages 851?
860. ACM.
H. Sayyadi, M. Hurst, and A. Maykov. 2009. Event
detection and tracking in social streams. In Proceed-
ings of International Conference on Weblogs and So-
cial Media (ICWSM).
Erik F. Tjong Kim Sang. 2002. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning - Volume
20, COLING-02, pages 1?4, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic lan-
guage variations to improve multilingual sentiment
analysis in social media. In Proceedings of the
2013 Conference on Empirical Methods on Natural
Language Processing.
K Wickre. 2013. Celebrating #twitter7.
https://blog.twitter.com/2013/
celebrating-twitter7. Accessed:
03/20/2014.
S. Wu, J.M. Hofman, W.A. Mason, and D.J. Watts.
2011. Who says what to whom on twitter. In
Proceedings of the 20th international conference on
World wide web, pages 705?714. ACM.
108
