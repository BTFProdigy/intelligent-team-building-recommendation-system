Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224?233,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Large-Margin Training of
Syntactic and Structural Translation Features
David Chiang
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
chiang@isi.edu
Yuval Marton and Philip Resnik
Department of Linguistics and the UMIACS
Laboratory for Computational Linguistics
and Information Processing
University of Maryland
College Park, MD 20742, USA
{ymarton,resnik}@umiacs.umd.edu
Abstract
Minimum-error-rate training (MERT) is a bot-
tleneck for current development in statistical
machine translation because it is limited in
the number of weights it can reliably opti-
mize. Building on the work of Watanabe et
al., we explore the use of the MIRA algorithm
of Crammer et al as an alternative to MERT.
We first show that by parallel processing and
exploiting more of the parse forest, we can
obtain results using MIRA that match or sur-
pass MERT in terms of both translation qual-
ity and computational cost. We then test the
method on two classes of features that address
deficiencies in the Hiero hierarchical phrase-
based model: first, we simultaneously train a
large number of Marton and Resnik?s soft syn-
tactic constraints, and, second, we introduce
a novel structural distortion model. In both
cases we obtain significant improvements in
translation performance. Optimizing them in
combination, for a total of 56 feature weights,
we improve performance by 2.6 B??? on a
subset of the NIST 2006 Arabic-English eval-
uation data.
1 Introduction
Since its introduction by Och (2003), minimum er-
ror rate training (MERT) has been widely adopted
for training statistical machine translation (MT) sys-
tems. However, MERT is limited in the number of
feature weights that it can optimize reliably, with
folk estimates of the limit ranging from 15 to 30 fea-
tures.
One recent example of this limitation is a series
of experiments by Marton and Resnik (2008), in
which they added syntactic features to Hiero (Chi-
ang, 2005; Chiang, 2007), which ordinarily uses no
linguistically motivated syntactic information. Each
of their new features rewards or punishes a deriva-
tion depending on how similar or dissimilar it is
to a syntactic parse of the input sentence. They
found that in order to obtain the greatest improve-
ment, these features had to be specialized for par-
ticular syntactic categories and weighted indepen-
dently. Not being able to optimize them all at once
using MERT, they resorted to running MERT many
times in order to test different combinations of fea-
tures. But it would have been preferable to use a
training method that can optimize the features all at
once.
There has been much work on improving MERT?s
performance (Duh and Kirchoff, 2008; Smith and
Eisner, 2006; Cer et al, 2008), or on replacing
MERT wholesale (Turian et al, 2007; Blunsom et
al., 2008). This paper continues a line of research on
online discriminative training (Tillmann and Zhang,
2006; Liang et al, 2006; Arun and Koehn, 2007),
extending that of Watanabe et al (2007), who use
the Margin Infused Relaxed Algorithm (MIRA) due
to Crammer et al (2003; 2006). Our guiding princi-
ple is practicality: like Watanabe et al, we train on
a small tuning set comparable in size to that used
by MERT, but by parallel processing and exploit-
ing more of the parse forest, we obtain results us-
ing MIRA that match or surpass MERT in terms of
both translation quality and computational cost on a
large-scale translation task.
Taking this further, we test MIRA on two classes
of features that make use of syntactic information
and hierarchical structure. First, we generalize Mar-
ton and Resnik?s (2008) soft syntactic constraints by
224
training all of them simultaneously; and, second, we
introduce a novel structural distortion model. We ob-
tain significant improvements in both cases, and fur-
ther large improvements when the two feature sets
are combined.
The paper proceeds as follows. We describe our
training algorithm in section 2; our generalization
of Marton and Resnik?s soft syntactic constraints in
section 3; our novel structural distortion features in
section 4; and experimental results in section 5.
2 Learning algorithm
The translation model is a standard linear model
(Och and Ney, 2002), which we train using MIRA
(Crammer and Singer, 2003; Crammer et al, 2006),
following Watanabe et al (2007). We describe the
basic algorithm first and then progressively refine it.
2.1 Basic algorithm
Let e, by abuse of notation, stand for both output
strings and their derivations. We represent the fea-
ture vector for derivation e as h(e). Initialize the fea-
ture weights w. Then, repeatedly:
? Select a batch of input sentences f1, . . . , fm.
? Decode each fi to obtain a set of hypothesis
translations ei1, . . . , ein.
? For each i, select one of the ei j to be the oracle
translation e?i , by a criterion described below.
Let ?hi j = h(e?i ) ? h(ei j).
? For each ei j, compute the loss `i j, which is
some measure of how bad it would be to guess
ei j instead of e?i .
? Update w to the value of w? that minimizes:
1
2
?w? ? w?2 + C
m?
i=1
max
1? j?n
(`i j ? ?hi j ? w?) (1)
where we set C = 0.01. The first term means
that we want w? to be close to w, and second
term (the generalized hinge loss) means that we
want w? to score e?i higher than each ei j by a
margin at least as wide as the loss `i j.
When training is finished, the weight vectors from
all iterations are averaged together. (If multiple
passes through the training data are made, we only
average the weight vectors from the last pass.) The
technique of averaging was introduced in the con-
text of perceptrons as an approximation to taking a
vote among all the models traversed during training,
and has been shown to work well in practice (Fre-
und and Schapire, 1999; Collins, 2002). We follow
McDonald et al (2005) in applying this technique to
MIRA.
Note that the objective (1) is not the same as that
used by Watanabe et al; rather, it is the same as
that used by Crammer and Singer (2003) and related
to that of Taskar et al (2005). We solve this opti-
mization problem using a variant of sequential min-
imal optimization (Platt, 1998): for each i, initialize
?i j = C for a single value of j such that ei j = e?i ,
and initialize ?i j = 0 for all other values of j. Then,
repeatedly choose a sentence i and a pair of hypothe-
ses j, j?, and let
w? ? w? + ?(?hi j ? ?hi j?) (2)
?i j ? ?i j + ? (3)
?i j? ? ?i j? ? ? (4)
where
? = clip
[??i j,?i j? ]
(`i j ? `i j?) ? (?hi j ? ?hi j?) ? w?
??hi j ? ?hi j??2
(5)
where the function clip[x,y](z) gives the closest num-
ber to z in the interval [x, y].
2.2 Loss function
Assuming B??? as the evaluation criterion, the loss
`i j of ei j relative to e?i should be related somehow
to the difference between their B??? scores. How-
ever, B??? was not designed to be used on individ-
ual sentences; in general, the highest-B??? transla-
tion of a sentence depends on what the other sen-
tences in the test set are. Sentence-level approxi-
mations to B??? exist (Lin and Och, 2004; Liang
et al, 2006), but we found it most effective to per-
form B??? computations in the context of a set O of
previously-translated sentences, following Watan-
abe et al (2007). However, we don?t try to accu-
mulate translations for the entire dataset, but simply
maintain an exponentially-weighted moving average
of previous translations.
225
More precisely: For an input sentence f, let e be
some hypothesis translation and let {rk} be the set of
reference translations for f. Let c(e; {rk}), or simply
c(e) for short, be the vector of the following counts:
|e|, the effective reference length mink |rk|, and, for
1 ? n ? 4, the number of n-grams in e, and the num-
ber of n-gram matches between e and {rk}. These
counts are sufficient to calculate a B??? score, which
we write as B???(c(e)). The pseudo-document O is
an exponentially-weighted moving average of these
vectors. That is, for each training sentence, let e? be
the 1-best translation; after processing the sentence,
we update O, and its input length O f :
O ? 0.9(O + c(e?)) (6)
O f ? 0.9(O f + |f|) (7)
We can then calculate the B??? score of hypothe-
ses e in the context of O. But the larger O is, the
smaller the impact the current sentence will have on
the B??? score. To correct for this, and to bring the
loss function roughly into the same range as typical
margins, we scale the B??? score by the size of the
input:
B(e; f, {rk}) = (O f + |f|) ? B???(O + c(e; {rk})) (8)
which we also simply write as B(e). Finally, the loss
function is defined to be:
`i j = B(e?i ) ? B(ei j) (9)
2.3 Oracle translations
We now describe the selection of e?. We know of
three approaches in previous work. The first is to
force the decoder to output the reference sentence
exactly, and select the derivation with the highest
model score, which Liang et al (2006) call bold up-
dating. The second uses the decoder to search for
the highest-B??? translation (Tillmann and Zhang,
2006), which Arun and Koehn (2007) call max-B???
updating. Liang et al and Arun and Koehn experi-
ment with these methods and both opt for a third
method, which Liang et al call local updating: gen-
erate an n-best list of translations and select the
highest-B??? translation from it. The intuition is that
due to noise in the training data or reference transla-
tions, a high-B??? translation may actually use pe-
culiar rules which it would be undesirable to en-
courage the model to use. Hence, in local updating,
Model score
B
??
?
sc
or
e
0.4
0.5
0.6
0.7
0.8
0.9
1
-90 -85 -80 -75 -70 -65 -60
? = 0
? = 0.5
? = 1
? = ?
Figure 1: Scatter plot of 10-best unique translations of a
single sentence obtained by forest rescoring using various
values of ? in equation (11).
the search for the highest-B??? translation is limited
to the n translations with the highest model score,
where n must be determined experimentally.
Here, we introduce a new oracle-translation selec-
tion method, formulating the intuition behind local
updating as an optimization problem:
e? = arg max
e
(B(e) + h(e) ? w) (10)
Instead of choosing the highest-B??? translation
from an n-best list, we choose the translation that
maximizes a combination of (approximate) B???
and the model.
We can also interpret (10) in the following way:
we want e? to be the max-B??? translation, but we
also want to minimize (1). So we balance these two
criteria against each other:
e? = arg max
e
(B(e) ? ?(B(e) ? h(e) ? w)) (11)
where (B(e) ? h(e) ? w) is that part of (1) that de-
pends on e?, and ? is a parameter that controls how
much we are willing to allow some translations to
have higher B??? than e? if we can better minimize
(1). Setting ? = 0 would reduce to max-B??? up-
dating; setting ? = ? would never update w at all.
Setting ? = 0.5 reduces to equation (10).
Figure 1 shows the 10-best unique translations for
a single input sentence according to equation (11)
under various settings of ?. The points at far right are
the translations that are scored highest according to
226
the model. The ? = 0 points in the upper-left corner
are typical of oracle translations that would be se-
lected under the max-B??? policy: they indeed have
a very high B??? score, but are far removed from the
translations preferred by the model; thus they would
cause violent updates to w. Local updating would
select the topmost point labeled ? = 1. Our scheme
would select one of the ? = 0.5 points, which have
B??? scores almost as high as the max-B??? transla-
tions, yet are not very far from the translations pre-
ferred by the model.
2.4 Selecting hypothesis translations
What is the set {ei j} of translation hypotheses? Ide-
ally we would let it be the set of all possible transla-
tions, and let the objective function (1) take all of
them into account. This is the approach taken by
Taskar et al (2004), but their approach assumes that
the loss function can be decomposed into local loss
functions. Since our loss function cannot be so de-
composed, we select:
? the 10-best translations according to the model;
we then rescore the forest to obtain
? the 10-best translations according to equation
(11) with ? = 0.5, the first of which is the oracle
translation, and
? the 10-best translations with ? = ?, to serve as
negative examples.
The last case is what Crammer et al (2006) call
max-loss updating (where ?loss? refers to the gener-
alized hinge loss) and Taskar et al (2005) call loss-
augmented inference. The rationale here is that since
the objective (1) tries to minimize max j(`i j ? ?hi j ?
w?), we should include the translations that have the
highest (`i j ? ?hi j ? w) in order to approximate the
effect of using the whole forest.
See Figure 1 again for an illustration of the hy-
potheses selected for a single sentence. The max-
B??? points in the upper left are not included (and
would have no effect even if they were included).
The ? = ? points in the lower-right are the negative
examples: they are poor translations that are scored
too high by the model, and the learning algorithm
attempts to shift them to the left.
To perform the forest rescoring, we need to use
several approximations, since an exact search for
B???-optimal translations is NP-hard (Leusch et al,
2008). For every derivation e in the forest, we calcu-
late a vector c(e) of counts as in Section 2.2 except
using unclipped counts of n-gram matches (Dreyer
et al, 2007), that is, the number of matches for an n-
gram can be greater than the number of occurrences
of the n-gram in any reference translation. This can
be done efficiently by calculating c for every hyper-
edge (rule application) in the forest:
? the number of output words generated by the
rule
? the effective reference length scaled by the frac-
tion of the input sentence consumed by the rule
? the number of n-grams formed by the applica-
tion of the rule (1 ? n ? 4)
? the (unclipped) number of n-gram matches
formed by the application of the rule (1 ? n ?
4)
We keep track of n-grams using the same scheme
used to incorporate an n-gram language model into
the decoder (Wu, 1996; Chiang, 2007).
To find the best derivation in the forest, we tra-
verse it bottom-up as usual, and for every set of al-
ternative subtranslations, we select the one with the
highest score. But here a rough approximation lurks,
because we need to calculate B on the nodes of the
forest, but B does not have the optimal substructure
property, i.e., the optimal score of a parent node can-
not necessarily be calculated from the optimal scores
of its children. Nevertheless, we find that this rescor-
ing method is good enough for generating high-B???
oracle translations and low-B??? negative examples.
2.5 Parallelization
One convenient property of MERT is that it is em-
barrassingly parallel: we decode the entire tuning set
sending different sentences to different processors,
and during optimization of feature weights, differ-
ent random restarts can be sent to different proces-
sors. In order to make MIRA comparable in effi-
ciency to MERT, we must parallelize it. But with
an online learning algorithm, parallelization requires
a little more coordination. We run MIRA on each
227
processor simultaneously, with each maintaining its
own weight vector. A master process distributes dif-
ferent sentences from the tuning set to each of the
processors; when each processor finishes decoding
a sentence, it transmits the resulting hypotheses,
with their losses, to all the other processors and re-
ceives any hypotheses waiting from other proces-
sors. Those hypotheses were generated from differ-
ent weight vectors, but can still provide useful in-
formation. The sets of hypotheses thus collected are
then processed as one batch. When the whole train-
ing process is finished, we simply average all the
weight vectors from all the processors.
Having described our training algorithm, which
includes several practical improvements to Watan-
abe et al?s usage of MIRA, we proceed in the re-
mainder of the paper to demonstrate the utility of the
our training algorithm on models with large numbers
of structurally sensitive features.
3 Soft syntactic constraints
The first features we explore are based on a line
of research introduced by Chiang (2005) and im-
proved on by Marton and Resnik (2008). A hi-
erarchical phrase-based translation model is based
on synchronous context-free grammar, but does not
normally use any syntactic information derived from
linguistic knowledge or treebank data: it uses trans-
lation rules that span any string of words in the input
sentence, without regard for parser-defined syntac-
tic constituency boundaries. Chiang (2005) exper-
imented with a constituency feature that rewarded
rules whose source language side exactly spans a
syntactic constituent according to the output of an
external source-language parser. This feature can
be viewed as a soft syntactic constraint: it biases
the model toward translations that respect syntactic
structure, but does not force it to use them. However,
this more syntactically aware model, when tested in
Chinese-English translation, did not improve trans-
lation performance.
Recently, Marton and Resnik (2008) revisited
the idea of constituency features, and succeeded in
showing that finer-grained soft syntactic constraints
yield substantial improvements in B??? score for
both Chinese-English and Arabic-English transla-
tion. In addition to adding separate features for dif-
ferent syntactic nonterminals, they introduced a new
type of constraint that penalizes rules when the
source language side crosses the boundaries of a
source syntactic constituent, as opposed to simply
rewarding rules when they are consistent with the
source-language parse tree.
Marton and Resnik optimized their features?
weights using MERT. But since MERT does not
scale well to large numbers of feature weights, they
were forced to test individual features and manu-
ally selected feature combinations each in a sepa-
rate model. Although they showed gains in trans-
lation performance for several such models, many
larger, potentially better feature combinations re-
mained unexplored. Moreover, the best-performing
feature subset was different for the two language
pairs, suggesting that this labor-intensive feature se-
lection process would have to be repeated for each
new language pair.
Here, we use MIRA to optimize Marton and
Resnik?s finer-grained single-category features all at
once. We define below two sets of features, a coarse-
grained class that combines several constituency cat-
egories, and a fine-grained class that puts different
categories into different features. Both kinds of fea-
tures were used by Marton and Resnik, but only a
few at a time. Crucially, our training algorithm pro-
vides the ability to train all the fine-grained features,
a total of 34 feature weights, simultaneously.
Coarse-grained features As the basis for coarse-
grained syntactic features, we selected the following
nonterminal labels based on their frequency in the
tuning data, whether they frequently cover a span
of more than one word, and whether they repre-
sent linguistically relevant constituents: NP, PP, S,
VP, SBAR, ADJP, ADVP, and QP. We define two
new features, one which fires when a rule?s source
side span in the input sentence matches any of the
above-mentioned labels in the input parse, and an-
other which fires when a rule?s source side span
crosses a boundary of one of these labels (e.g., its
source side span only partially covers the words in
a VP subtree, and it also covers some or all or the
words outside the VP subtree). These two features
are equivalent to Marton and Resnik?s XP= and XP+
feature combinations, respectively.
228
Fine-grained features We selected the following
nonterminal labels that appear more than 100 times
in the tuning data: NP, PP, S, VP, SBAR, ADJP,
WHNP, PRT, ADVP, PRN, and QP. The labels that
were excluded were parts of speech, nonconstituent
labels like FRAG, or labels that occurred only two
or three times. For each of these labels X, we added
a separate feature that fires when a rule?s source side
span in the input sentence matches X, and a second
feature that fires when a span crosses a boundary of
X. These features are similar to Marton and Resnik?s
X= and X+, except that our set includes features for
WHNP, PRT, and PRN.
4 Structural distortion features
In addition to parser-based syntactic constraints,
which were introduced in prior work, we introduce
a completely new set of features aimed at improv-
ing the modeling of reordering within Hiero. Again,
the feature definition gives rise to a larger number of
features than one would expect to train successfully
using MERT.
In a phrase-based model, reordering is per-
formed both within phrase pairs and by the phrase-
reordering model. Both mechanisms are able to
learn that longer-distance reorderings are more
costly than shorter-distance reorderings: phrase
pairs, because phrases that involve more extreme re-
orderings will (presumably) have a lower count in
the data, and phrase reordering, because models are
usually explicitly dependent on distance.
By contrast, in a hierarchical model, all reordering
is performed by a single mechanism, the rules of the
grammar. In some cases, the model will be able to
learn a preference for shorter-distance reorderings,
as in a phrase-based system, but in the case of a word
being reordered across a nonterminal, or two non-
terminals being reordered, there is no dependence in
the model on the size of the nonterminal or nonter-
minals involved in reordering.
So, for example, if we have rules
X? (il dit X1, he said X1) (12)
X? (il dit X1,X1 he said) (13)
we might expect that rule (12) is more common in
general, but that rule (13) becomes more and more
?
?
?
?
?
?
?
?
Figure 2: Classifying nonterminal occurrences for the
structural distortion model.
rare as X1 gets larger. The default Hiero features
have no way to learn this.
To address this defect, we can classify every
nonterminal pair occurring on the right-hand side
of each grammar rule as ?reordered? or ?not re-
ordered?, that is, whether it intersects any other word
alignment link or nonterminal pair (see Figure 2).
We then define coarse- and fine-grained versions of
the structural distortion model.
Coarse-grained features Let R be a binary-
valued random variable that indicates whether a non-
terminal occurrence is reordered, and let S be an
integer-valued random variable that indicates how
many source words are spanned by the nonterminal
occurrence. We can estimate P(R | S ) via relative-
frequency estimation from the rules as they are ex-
tracted from the parallel text, and incorporate this
probability as a new feature of the model.
Fine-grained features A difficulty with the
coarse-grained reordering features is that the gram-
mar extraction process finds overlapping rules in the
training data and might not give a sensible proba-
bility estimate; moreover, reordering statistics from
the training data might not carry over perfectly into
the translation task (in particular, the training data
may have some very freely-reordering translations
that one might want to avoid replicating in transla-
tion). As an alternative, we introduce a fine-grained
version of our distortion model that can be trained
directly in the translation task as follows: define
229
a separate binary feature for each value of (R, S ),
where R is as above and S ? {?, 1, . . . , 9,?10} and ?
means any size. For example, if a nonterminal with
span 11 has its contents reordered, then the features
(true,?10) and (true, ?) would both fire. Grouping
all sizes of 10 or more into a single feature is de-
signed to avoid overfitting.
Again, using MIRA makes it practical to train
with the full fine-grained feature set?coincidentally
also a total of 34 features.
5 Experiment and results
We now describe our experiments to test MIRA and
our features, the soft-syntactic constraints and the
structural distortion features, on an Arabic-English
translation task. It is worth noting that this exper-
imentation is on a larger scale than Watanabe et
al.?s (2007), and considerably larger than Marton
and Resnik?s (2008).
5.1 Experimental setup
The baseline model was Hiero with the following
baseline features (Chiang, 2005; Chiang, 2007):
? two language models
? phrase translation probabilities p( f | e) and
p(e | f )
? lexical weighting in both directions (Koehn et
al., 2003)
? word penalty
? penalties for:
? automatically extracted rules
? identity rules (translating a word into it-
self)
? two classes of number/name translation
rules
? glue rules
The probability features are base-100 log-
probabilities.
The rules were extracted from all the allow-
able parallel text from the NIST 2008 evalua-
tion (152+175 million words of Arabic+English),
aligned by IBM Model 4 using GIZA++ (union of
both directions). Hierarchical rules were extracted
from the most in-domain corpora (4.2+5.4 million
words) and phrases were extracted from the remain-
der. We trained the coarse-grained distortion model
on 10,000 sentences of the training data.
Two language models were trained, one on data
similar to the English side of the parallel text and
one on 2 billion words of English. Both were 5-
gram models with modified Kneser-Ney smoothing,
lossily compressed using a perfect-hashing scheme
similar to that of Talbot and Brants (2008) but using
minimal perfect hashing (Botelho et al, 2005).
We partitioned the documents of the NIST 2004
(newswire) and 2005 Arabic-English evaluation data
into a tuning set (1178 sentences) and a develop-
ment set (1298 sentences). The test data was the
NIST 2006 Arabic-English evaluation data (NIST
part, newswire and newsgroups, 1529 sentences).
To obtain syntactic parses for this data, we tok-
enized it according to the Arabic Treebank standard
using AMIRA (Diab et al, 2004), parsed it with
the Stanford parser (Klein and Manning, 2003), and
then forced the trees back into the MT system?s tok-
enization.1
We ran both MERT and MIRA on the tuning
set using 20 parallel processors. We stopped MERT
when the score on the tuning set stopped increas-
ing, as is common practice, and for MIRA, we used
the development set to decide when to stop train-
ing.2 In our runs, MERT took an average of 9 passes
through the tuning set and MIRA took an average of
8 passes. (For comparison, Watanabe et al report de-
coding their tuning data of 663 sentences 80 times.)
5.2 Results
Table 1 shows the results of our experiments with
the training methods and features described above.
All significance testing was performed against the
first line (MERT baseline) using paired bootstrap re-
sampling (Koehn, 2004).
First of all, we find that MIRA is competitive with
MERT when both use the baseline feature set. In-
1The only notable consequence this had for our experimen-
tation is that proclitic Arabic prepositions were fused onto the
first word of their NP object, so that the PP and NP brackets
were coextensive.
2We chose this policy for MIRA to avoid overfitting. How-
ever, we could have used the tuning set for this purpose, just as
with MERT: in none of our runs would this change have made
more than a 0.2 B??? difference on the development set.
230
Dev NIST 06 (NIST part)
Train Features # nw nw ng nw+ng
MERT baseline 12 52.0 50.5 32.4 44.6
syntax (coarse) 14 52.2 50.9 33.0+ 45.0+
syntax (fine) 34 52.1 50.4 33.5++ 44.8
distortion (coarse) 13 52.3 51.3+ 34.3++ 45.8++
distortion (fine) 34 52.0 50.9 34.5++ 45.5++
MIRA baseline 12 52.0 49.8? 34.2++ 45.3++
syntax (fine) 34 53.1++ 51.3+ 34.5++ 46.4++
distortion (fine) 34 53.3++ 51.5++ 34.7++ 46.7++
distortion+syntax (fine) 56 53.6++ 52.0++ 35.0++ 47.2++
Table 1: Comparison of MERT and MIRA on various feature sets. Key: # = number of features; nw = newswire, ng =
newsgroups; + or ++ = significantly better than MERT baseline (p < 0.05 or p < 0.01, respectively), ? = significantly
worse than MERT baseline (p < 0.05).
deed, the MIRA system scores significantly higher
on the test set; but if we break the test set down by
genre, we see that the MIRA system does slightly
worse on newswire and better on newsgroups. (This
is largely attributable to the fact that the MIRA trans-
lations tend to be longer than the MERT transla-
tions, and the newsgroup references are also rela-
tively longer than the newswire references.)
When we add more features to the model, the two
training methods diverge more sharply. When train-
ing with MERT, the coarse-grained pair of syntax
features yields a small improvement, but the fine-
grained syntax features do not yield any further im-
provement. By contrast, when the fine-grained fea-
tures are trained using MIRA, they yield substan-
tial improvements. We observe similar behavior for
the structural distortion features: MERT is not able
to take advantage of the finer-grained features, but
MIRA is. Finally, using MIRA to combine both
classes of features, 56 in all, produces the largest im-
provement, 2.6 B??? points over the MERT baseline
on the full test set.
We also tested some of the differences between
our training method and Watanabe et al?s (2007); the
results are shown in Table 2. Compared with local
updating (line 2), our method of selecting the ora-
cle translation and negative examples does better by
0.5 B??? points on the development data. Using loss-
augmented inference to add negative examples to lo-
cal updating (line 3) does not appear to help. Never-
theless, the negative examples are important: for if
Setting Dev
full 53.6
local updating, no LAI 53.1?
local updating, LAI 53.0??
? = 0.5 oracle, no LAI failed
no sharing of updates 53.1??
Table 2: Effect of removing various improvements in
learning method. Key: ? or ?? = significantly worse than
full system (p < 0.05 or p < 0.01, respectively); LAI =
loss-augmented inference for additional negative exam-
ples.
we use our method for selecting the oracle transla-
tion without the additional negative examples (line
4), the algorithm fails, generating very long transla-
tions and unable to find a weight setting to shorten
them. It appears, then, that the additional negative
examples enable the algorithm to reliably learn from
the enhanced oracle translations.
Finally, we compared our parallelization method
against a simpler method in which all processors
learn independently and their weight vectors are all
averaged together (line 5). We see that sharing in-
formation among the processors makes a significant
difference.
6 Conclusions
In this paper, we have brought together two existing
lines of work: the training method of Watanabe et al
(2007), and the models of Chiang (2005) and Marton
231
and Resnik (2008). Watanabe et al?s work showed
that large-margin training with MIRA can be made
feasible for state-of-the-art MT systems by using a
manageable tuning set; we have demonstrated that
parallel processing and exploiting more of the parse
forest improves MIRA?s performance and that, even
using the same set of features, MIRA?s performance
compares favorably to MERT in terms of both trans-
lation quality and computational cost.
Marton and Resnik (2008) showed that it is pos-
sible to improve translation in a data-driven frame-
work by incorporating source-side syntactic analy-
sis in the form of soft syntactic constraints. This
work joins a growing body of work demonstrating
the utility of syntactic information in statistical MT.
In the area of source-side syntax, recent research
has continued to improve tree-to-string translation
models, soften the constraints of the input tree in
various ways (Mi et al, 2008; Zhang et al, 2008),
and extend phrase-based translation with source-
side soft syntactic constraints (Cherry, 2008). All
this work shows strong promise, but Marton and
Resnik?s soft syntactic constraint approach is par-
ticularly appealing because it can be used unobtru-
sively with any hierarchically-structured translation
model. Here, we have shown that using MIRA to
weight all the constraints at once removes the cru-
cial drawback of the approach, the problem of fea-
ture selection.
Finally, we have introduced novel structural dis-
tortion features to fill a notable gap in the hierar-
chical phrase-based approach. By capturing how re-
ordering depends on constituent length, these fea-
tures improve translation quality significantly. In
sum, we have shown that removing the bottleneck
of MERT opens the door to many possibilities for
better translation.
Acknowledgments
Thanks to Michael Bloodgood for performing ini-
tial simulations of parallelized perceptron training.
Thanks also to John DeNero, Kevin Knight, Daniel
Marcu, and Fei Sha for valuable discussions and
suggestions. This research was supported in part by
DARPA contract HR0011-06-C-0022 under subcon-
tract to BBN Technologies and HR0011-06-02-001
under subcontract to IBM.
References
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In Proc. MT
Summit XI.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical ma-
chine translation. In Proc. ACL-08: HLT.
Fabiano C. Botelho, Yoshiharu Kohayakawa, and Nivio
Ziviani. 2005. A practical minimal perfect hashing
method. In 4th International Workshop on Efficient
and Experimental Algorithms (WEA05).
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proc. Third Workshop on Statis-
tical Machine Translation.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proc. ACL-08: HLT.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models: Theory and experiments
with perceptron algorithms. In Proc. EMNLP 2002.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proc. HLT/NAACL 2004.
Companion volume.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT us-
ing efficient B??? oracle computation. In Proc. 2007
Workshop on Syntax and Structure in Statistical Trans-
lation.
Kevin Duh and Katrin Kirchoff. 2008. Beyond log-linear
models: Boosted minimum error rate training for n-
best re-ranking. In Proc. ACL-08: HLT, Short Papers.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Dan Klein and Chris D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. In Advances in Neural Information Processing
Systems 15 (NIPS 2002).
232
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proc. HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proc. EMNLP
2008. This volume.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. COLING-ACL
2006.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proc. COLING 2004.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. ACL-08: HLT.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc. ACL 2005.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL-08: HLT.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ACL 2002.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
Bernhard Scho?lkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 195?208. MIT
Press.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In
Proc. COLING/ACL 2006, Poster Sessions.
David Talbot and Thorsten Brants. 2008. Random-
ized language models via perfect hash functions. In
Proc. ACL-08: HLT.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. EMNLP 2004, pages 1?8.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proc. ICML
2005.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. COLING-ACL 2006.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learning for
natural language parsing and translation. In Advances
in Neural Information Processing Systems 19 (NIPS
2006).
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. EMNLP 2007.
Dekai Wu. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. 34th Annual
Meeting of the Association for Computational Linguis-
tics, pages 152?158.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT.
233
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 381?390,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Improved Statistical Machine Translation
Using Monolingually-Derived Paraphrases
Yuval Marton,
?
Chris Callison-Burch,
?
and Philip Resnik
?
?
Department of Linguistics and the CLIP Lab
at the Institute for Advanced Computer Studies (UMIACS)
University of Maryland College Park, MD 20742-7505, USA
{ymarton,resnik}@umiacs.umd.edu
?
Computer Science Department, Johns Hopkins University
3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218
ccb@cs.jhu.edu
Abstract
Untranslated words still constitute a ma-
jor problem for Statistical Machine Trans-
lation (SMT), and current SMT systems
are limited by the quantity of parallel
training texts. Augmenting the training
data with paraphrases generated by pivot-
ing through other languages alleviates this
problem, especially for the so-called ?low
density? languages. But pivoting requires
additional parallel texts. We address this
problem by deriving paraphrases monolin-
gually, using distributional semantic simi-
larity measures, thus providing access to
larger training resources, such as compa-
rable and unrelated monolingual corpora.
We present what is to our knowledge the
first successful integration of a colloca-
tional approach to untranslated words with
an end-to-end, state of the art SMT sys-
tem demonstrating significant translation
improvements in a low-resource setting.
1 Introduction
Phrase-based systems, flat and hierarchical alike
(Koehn et al, 2003; Koehn, 2004b; Koehn et al,
2007; Chiang, 2005; Chiang, 2007), have achieved
a much better translation coverage than word-
based ones (Brown et al, 1993), but untranslated
words remain a major problem in SMT. For ex-
ample, according to Callison-Burch et al (2006),
a SMT system with a training corpus of 10,000
words learned only 10% of the vocabulary; the
same system learned about 30% with a training
corpus of 100,000 words; and even with a large
training corpus of nearly 10,000,000 words it only
reached about 90% coverage of the source vocab-
ulary. Coverage of higher order n-gram levels is
even harder. This problem plays a major part in re-
ducing machine translation quality, as reflected by
both automatic measures such as BLEU (Papineni
et al, 2002) and human judgment tests. Improving
translation coverage accurately is therefore impor-
tant for SMT systems.
The first solution that might come to mind is
to use larger parallel training corpora. However,
current state-of-the-art SMT systems cannot learn
from non-aligned corpora, while sentence-aligned
parallel corpora (bitexts) are a limited resource
(See Section 2 for discussion of automatically-
compiled bitexts). Another direction might be
to make use of non-parallel corpora for training.
However, this requires developing techniques to
extract alignments or translations from them, and
in a sufficiently fast, memory-efficient, and scal-
able manner. One approach that can, in princi-
ple, better exploit both alignments from bitexts
and make use of non-parallel corpora is the dis-
tributional collocational approach, e.g., as used by
Fung and Yee (1998) and Rapp (1999). However,
the systems described there are not easily scalable,
and require pre-computation of a very large col-
location counts matrix. Related attempts propose
generating bitexts from comparable and ?quasi-
comparable? bilingual texts by iteratively boot-
strapping documents, sentences, and words (Fung
and Cheung, 2004), or by using a maximum
entropy classifier (Munteanu and Marcu, 2005).
Alignment accuracy remains a challenge for them.
Recent work has proposed augmenting the
training data with paraphrases generated by pivot-
ing through other languages (Callison-Burch et al,
2006; Madnani et al, 2007). This indeed allevi-
ates the vocabulary coverage problem, especially
for the so-called ?low density? languages. How-
ever, these approaches still require bitexts where
381
one side contains the original source language.
The paradigm described in this paper involves
constructing monolingual distributional profiles
(DPs; a.k.a. word association profiles, or co-
occurrence vectors) of out-of-vocabulary words
and phrases in the source language; then, gener-
ating paraphrase candidates from phrases that co-
occur in similar contexts, and assigning them sim-
ilarity scores. The highest ranking paraphrases
are used to augment the translation phrase table.
The table augmentation idea is similar to Callison-
Burch et al?s (Callison-Burch et al, 2006), but
our proposed paradigm does not require using a
limited resource such as parallel texts in order
to generate paraphrases. Moreover, our proposed
paradigm can, in principle, achieve large-scale ac-
quisition of paraphrases with high semantic simi-
larity. However, using parallel training texts in
pivoting techniques offers the potential advantage
of implicit translational knowledge, in the form
of sentence alignments, while our approach is un-
guided in this respect. Therefore, we conducted
experiments to find out how these relative advan-
tages play out. We present here, to our knowledge
for the first time, positive results of integrating dis-
tributional monolingually-derived paraphrases in
an end-to-end state-of-the-art SMT system.
In the rest of this paper we discuss related work
in Section 2, describe the distributional hypothesis
and distributional profiles in Section 3, and present
the monolingually-derived paraphrase generation
system in Section 4. We report our experiments
and results in Section 5, and conclude by dis-
cussing the implications and future research direc-
tions in Section 6.
2 Related Work
This is not the first to attempt to ameliorate the
out-of-vocabulary (OOV) words problem in sta-
tistical machine translation, and other natural lan-
guage processing tasks. This work is most closely
related to that of Callison-Burch et al (2006),
who also translate source-side paraphrases of the
OOV phrases. There, paraphrases are generated
from bitexts of various language pairs, by ?pivot-
ing?: translating the OOV phrases to an additional
language (or languages) and back to the source
language. The quality of these paraphrases is es-
timated by marginalizing translation probabilities
to and from the additional language side(s) e, as
follows: p(f
2
|f
1
) =
?
e
p(e|f
1
)p(f
2
|e). A ma-
jor disadvantage of their approach is that it relies
on the availability of parallel corpora in other lan-
guages. While this works for English and many
European languages, it is far less likely to help
when translating from other source languages, for
which bitexts are scarce or non-existent. Also,
the pivoting approach is inherently noisy (in both
the paraphrase candidates? correct sense, and their
translational likelihood), and it is likely to fare
poorly with out-of-domain translation. One ad-
vantage of the bitext-dependent pivoting approach
is the use of the additional human knowledge that
is encapsulated in the parallel sentence alignment.
However, we argue that the ability to use much
larger resources for paraphrasing should trump the
human knowledge advantage.
More recently, Callison-Burch (2008) has im-
proved performance of this pivoting technique by
imposing syntactic constraints on the paraphrases.
The limitation of such an approach is the reliance
on a good parser (in addition to reliance on bi-
texts), but a good parser is not available in all
languages, especially not in resource-poor lan-
guages. Another approach using a pivoting tech-
nique augments the human reference translation
with paraphrases, creating additional translation
?references? (Madnani et al, 2007). Both ap-
proaches have shown gains in BLEU score.
Barzilay and McKeown (2001) extract para-
phrases from a monolingual parallel corpus, con-
taining multiple translations of the same source.
In addition to the parallel corpus usage limitations
described above, this technique is further limited
by the small size of such materials, which are even
scarcer than the resources in the pivoting case.
Dolan et al (2004) explore generating para-
phrases by edit-distance and headlines of time-
and topic-clustered news articles; they do not ad-
dress the OOV problem directly, as their focus
is sentence-level paraphrases; although they use
a standard SMT measure, alignment error rate
(AER), they only report results of the alignment
quality, and not of an end-to-end SMT system.
Much of the previous research largely focused on
morphological analysis in order to reduce type
sparseness; Callison-Burch et al (2006) list some
of the influential work in that direction.
Work that relies on the distributional hypoth-
esis using bilingual comparable corpora (with-
out the need for bitexts), typically uses a seed
lexicon for ?bridging? source language phrases
382
with their target languages paraphrases (Fung and
Yee, 1998; Rapp, 1999; Diab and Finch, 2000).
This approach is sometimes viewed as, or com-
bined with, an information retrieval (IR) approach,
and normalizes strength-of-association measures
(see Section 3) with IR-related measures such as
TF/IDF (Fung and Yee, 1998). To date, reported
implementations suffer from scalability issues, as
they pre-compute and hold in memory a huge col-
location matrix; we know of no report of using this
approach in an end-to-end SMT system.
Another approach aiming to reduce OOV rate
concentrates on increasing parallel training set
size without using more dedicated human transla-
tion (Resnik and Smith, 2003; Oard et al, 2003).
3 Collocational Profiles
The distributional hypothesis and distribu-
tional profiles. Natural language processing
(NLP) applications that assume the distributional
hypothesis (Harris, 1940; Firth, 1957) typically
keep track of word co-occurrences in distribu-
tional profiles (a.k.a. collocation vectors, or con-
text vectors). Each distributional profile DP
u
(for some word u) keeps counts of co-occurrence
of u with all words within a usually fixed dis-
tance from each of its occurrences (a sliding win-
dow) in some training corpus. More advanced pro-
files keep ?strength of association? (SoA) infor-
mation between u and each of the co-occurring
words, which is calculated from the counts of u,
the counts of the other word, their co-occurrence
count, and the count of all words in the corpus
(corpus size). The information on the other words
with respect to u is typically kept in a vector whose
dimensions correspond to all words in the training
corpus. This is described in Equation (1), where
V is the training corpus vocabulary:
DP
u
= {< w
i
, SoA(u,w
i
) > |u,w
i
? V }
for all i s.t. 1 ? i ? |V |
(1)
Semantic similarity between words u and v can
be estimated by calculating the similarity (vector
distance) between their profiles. Slightly more for-
mally, the distributional hypothesis assumes that
if we had access to the hypothetical true (psycho-
linguistic) semantic similarity function over word
pairs, semsim(u, v), then
?u, v, w ? V,
[semsim(u, v) > semsim(u,w)] =?
[psim(DP
u
, DP
v
) > psim(DP
u
, DP
w
)],
(2)
where V is the language vocabulary, DP
word
is
the distributional profile of word, and psim() is
a 2-place vector similarity function (all further
described below). Paraphrasing and other NLP
applications that are based on the distributional
hypothesis assume entailment in the reverse di-
rection: the right-hand-side of Formula (2) (pro-
file/vector similarity) entails the left-hand-side
(semantic similarity).
The sliding window and word association (SoA)
measures. Some researchers count positional
collocations in a sliding window, i.e., the co-
counts and SoA measures are calculated per rel-
ative position (e.g., for some word/token u, po-
sition 1 is the token immediately after u; posi-
tion -2 is the token preceding the token that pre-
cedes u) (Rapp, 1999); other researchers use non-
positional (which we dub here flat) collocations,
meaning, they count all token occurrences within
the sliding window, regardless of their positions
in it relative to u (McDonald, 2000; Mohammad
and Hirst, 2006). We use here flat collocations
in a 6-token sliding window. Beside simple co-
occurrence counts within sliding windows, other
SoA measures include functions based on TF/IDF
(Fung and Yee, 1998), mutual information (PMI)
(Lin, 1998), conditional probabilities (Schuetze
and Pedersen, 1997), chi-square test, and the log-
likelihood ratio (Dunning, 1993).
Profile similarity measures. A profile similar-
ity function psim(DP
u
, DP
v
) is typically defined
as a two-place function, taking vectors as argu-
ments, each vector representing a distributional
profile of some word u and v, respectively, and
whose cells contain the SoA of u (or v) with each
word (?collocate?) in the known vocabulary. Sim-
ilarity can be (and have been) estimated in several
ways, e.g., the cosine coefficient, the Jaccard co-
efficient, the Dice coefficient, and the City-Block
measure. The formula for the cosine function for
similarity measure is given in Eq. (3):
383
psim(DP
u
, DP
v
)
= cos(DP
u
, DP
v
)
=
?
w
i
?V
SoA(u,w
i
)SoA(v, w
i
)
?
?
w
i
?V
SoA(u,w
i
)
2
?
?
w
i
?V
SoA(v, w
i
)
2
(3)
In principle, any SoA can be used with any
profile similarity measure. However, in practice,
only some SoA/similarity measure combinations
do well, and finding the best combination is still
more art than science. Some successful combina-
tions are cos
CP
(Schuetze and Pedersen, 1997),
Lin
PMI
(Lin, 1998), City
LL
(Rapp, 1999), and
Jensen?Shannon divergence of conditional prob-
abilities (JSD
CP
). We use here cosine of log-
likelihood vectors (McDonald, 2000).
Phrasal distributional profiles. Word DPs can
be generalized to phrasal DPs, simply by count-
ing words that co-occur within a sliding window
around the target phrase?s occurrences (i.e., count-
ing occurrences of words up to 6 words before
or after the target phrase). For example, when
building a DP for the target phrase counting words
in the previous sentence, then simply is in rela-
tive position -2, and sliding is in relative posi-
tion 5. Searching for similar phrasal DPs poses
an additional challenge over the word DP case
(see Section 4), but there is no additional diffi-
culty in building the phrasal profile itself as de-
scribed above. In preliminary experiments we
found no gain in using phrasal collocates (i.e.,
count how many times a phrase of more than one
word co-occurs in a sliding window around the tar-
get word/phrase).
4 Searching and Scoring Phrasal
Paraphrases
The system design is as follows: upon receiv-
ing OOV phrase phr, build distributional profile
DP
phr
. Next, gather contexts: for each occur-
rence of phr, keep surrounding (left and right)
context L__R. For each such context, gather para-
phrase candidates X which occur between L and
R in other locations in the training corpus, i.e.,
all X such that LXR occur in the corpus. Fi-
nally, rank all candidates X , by building distribu-
tional profile DP
X
and measuring profile similar-
ity between DP
X
and DP
phr
, for each X . Output
k-best candidates above a certain similarity score
threshold. The rest of this section describes this
system in more detail.
Build phrasal profile DP
phr
. Build a profile of
all word collocates, as described in Section 3. Use
sliding window of size MaxPos = 6. If phr
is very frequent (above some threshold of t oc-
currences), uniformly sample only t occurrences,
multiplying the gathered co-counts by factor of
count(phr)/t. We set t = 10000.
Gather context. The challenge in choosing the
relevant context is this: if it is very short and/or
very frequent (e.g., ?the __ is?), then it might not
be very informative, in the sense that many words
can appear in that context (in this example, practi-
cally any noun); however, if it is too long (too spe-
cific), then it might not occur enough times else-
where (or not at all) in the training corpus. There-
fore, to balance between these two extremes, we
use the following heuristics. Start small: Start
with setting the left part of the context L to be a
single word/token to the left of phrase phr. If it
is stoplisted, append the next word to the left (now
having a bigram left context instead of a unigram),
and repeat until the left context is not in the sto-
plist. Repeat similarly for R, the context to the
right of phr. Add the resulting L__R context to
a context list. We stoplist ?promiscuous? words,
i.e., those that have more than StoplistThreshold
collocates in the training corpus, using the above
MaxPos parameter value. We also stoplist bi-
grams which occur more than t times and com-
prise solely from stoplisted unigrams.
Gather candidates. For each gathered context
in the context list, gather all paraphrase candidate
phrases X that connect left hand side context L
with right hand side context R, i.e., gather all X
such that the sequence LXR occurs in the corpus.
In practice, to keep search complexity low, limit
X to be up to length MaxPhraseLen. Also, to
further speed up runtime, we uniformly sample the
context occurrences.
Rank candidates. For each candidate X ,
build distributional profile DP
X
, and evaluate
psim(DP
phr
, DP
X
).
Output k-best candidates. Output k-best para-
phrase candidates for phrase phr, in descending
order of similarity. We set k = 20. Filter out para-
phrases with score less than minScore.
384
5 Experiment
We examined the application of the system?s para-
phrases to handling unknown phrases when trans-
lating from English into Chinese (E2C) and from
Spanish into English (S2E). For all baselines we
used the phrase-based statistical machine transla-
tion system Moses (Koehn et al, 2007), with the
default model features, weighted in a log-linear
framework (Och and Ney, 2002). Feature weights
were set with minimum error rate training (Och,
2003) on a development set using BLEU (Papineni
et al, 2002) as the objective function. Test re-
sults were evaluated using BLEU and TER (Snover
et al, 2005). The phrase translation probabili-
ties were determined using maximum likelihood
estimation over phrases induced from word-level
alignments produced by performing Giza++ train-
ing (Och and Ney, 2000) on both source and tar-
get sides of the parallel training sets. When the
baseline system encountered unknown words in
the test set, its behavior was simply to reproduce
the foreign word in the translated output.
The paraphrase-augmented systems were iden-
tical to the corresponding baseline system, with
the exception of additional (paraphrase-based)
translation rules, and additional feature(s). Simi-
larly to Callison-Burch et al (2006), we added the
following feature:
h(e, f) =
8
>
>
>
<
>
>
>
:
psim(DP
f
?
, DP
f
) If phrase table entry (e, f)
is generated from (e, f
?
)
using monolingually-
derived paraphrases.
1 Otherwise,
(4)
Note that it is possible to construct a new trans-
lation rule from f to e via more than one pair of
source-side phrase and its paraphrase; e.g., if f
1
is a paraphrase of f , and so is f
2
, and both f
1
, f
2
translate to the same e, then both lead to the con-
struction of the new rule translating f to e, but
with potentially different feature scores.
In order to eliminate this duplicity and lever-
age over these alternate paths which can be used
to increase our confidence level in the new rule,
we did the following: For each paraphrase f
of some source-side phrases f
i
, with respec-
tive similarity scores sim(f
i
, f), we calculated
an aggregate score asim with a ?quasi-online-
updating? method as follows: asim
i
= (1 ?
asim
i?1
)sim(f
i
, f), where asim
0
= 0. The ag-
gregate score asim is updated in an ?online? fash-
ion with each pair f
i
, f as they are processed, but
only the final asim
k
score is used, after all k pairs
have been processed. Simple arithmetics can show
that this method is insensitive to the order in which
the paraphrases are processed. We only augment
the phrase table with a single rule from f to e,
and in it are the feature values of the phrase f
i
for
which the score sim(f
i
, f) was the highest.
5.1 English-to-Chinese Translation
For the English-Chinese (E2C) baseline system,
we trained on the LCD Sinorama and FBIS
tests (LCD2005T10 and LCD2003E14), and seg-
mented the Chinese side with the Stanford Seg-
menter (Tseng et al, 2005). After tokenization
and filtering, this bitext contained 231,586 lines
(6.4M + 5.1M tokens). We trained a trigram lan-
guage model on the Chinese side. We then split the
bitext to 32 even slices, and constructed a reduced
set of about 29,000 lines (sentences) by using only
every eighth slice. The purpose of creating this
subset model was to simulate a resource-poor lan-
guage. See Table 1.
Set # Tokens Source+Target
E2C 29K 0.8 + 0.6
E2C Full 6.4 + 5.1
bnc+apw 187
S2E 10K 0.3 + 0.3
S2E 20K 0.6 + 0.6
S2E 80K 2.3 + 2.3
wmt09 84
wmt09+acquis 139
wmt09+acquis+afp 402
Table 1: Training set sizes (million tokens).
For development, we used the Chinese-English
NIST MT 2005 evaluation set, taking one of the
English references as source, and the Chinese
source as a single reference translation. We tested
the system using the English-Chinese NIST MT
evaluation 2008 test set with its four reference
translations.
We augmented the E2C baseline models with
paraphrases generated as described above, train-
ing on the British National Corpus (BNC)
v3 (Burnard, 2000) and the first 3 million lines
of the English Gigaword v2 APW, totaling 187M
terms after tokenization, and number and punc-
tuation removal. We generated paraphrases for
phrases up to six tokens in length, and used an ar-
385
bitrary similarity threshold of minScore = 0.3.
We experimented with three variants: adding a
single additional feature for all paraphrases (1-
6grams); using only paraphrases of unigrams
(1grams); and adding two features, one only sen-
sitive to unigrams, and the other only to the rest
(1 + 2-6grams). All features had the same de-
sign as described in Section 5, each had an asso-
ciated weight (as all other features), and all fea-
ture weights in each system, including the base-
line, were tuned using a separate minimum error
rate training for each system.
Results are shown in Table 2. For the E2C sys-
tems, for which we had four reference translations
for the test set, we used shortest reference length,
and used the NIST-provided script to split the out-
put words to Chinese characters before evaluation.
Statistical significance for the BLEU results were
calculated using Koehn?s (Koehn, 2004) pair-wise
bootstrapping test with 95% confidence interval.
On the E2C 29,000-line subset, the augmented
system had a significant 1.7 BLEU points gain over
its baseline. On the full size model, results were
negative. Note that our E2C full size baseline
is reasonably strong: Its character-based BLEU
score is slightly higher than the JHU-UMD sys-
tem that participated in the NIST 2008 MT evalua-
tion (constrained training track), although we used
a subset of that system?s training materials, and
a smaller language model. Results there ranged
from 15.69 to 30.38 BLEU (ignoring a seeming
outlier of 3.93).
5.2 Spanish-to-English Translation
In order to to permit a more direct comparison
with the pivoting technique, we also experimented
with Spanish to English (S2E) translation, fol-
lowing Callison-Burch et al (2006). For base-
line we used the Spanish and English sides of
the Europarl multilingual parallel corpus (Koehn,
2005), with the standard training, development,
and test sets. We created training subset models
of 10,000, 20,000, and 80,000 aligned sentences,
as described in Callison-Burch et al (2006). For
better comparison with their pivoting system, we
used the same 5-gram language model, develop-
ment and test sets: For development, we used the
Europarl dev2006 Spanish and English sides, and
for testing we used the Europarl 2006 test set.
We trained the Spanish paraphrase generation
system on the Spanish corpora available from
dataset E2C model BLEU TER
29k baseline 15.21 90.354
29k 1grams 16.87*** 90.370
29k 1-6grams 16.54*** 90.376
29k 1 + 2-6grams 16.88*** 90.349
Full baseline 22.17 90.398
Full 1grams 21.64*** 90.459
Full 1-6grams 21.75 90.421
Full 1 + 2-6grams 21.39*** 90.433
Table 2: E2C Results: character-based BLEU and
TER scores. All models have one additional fea-
ture over baseline, except for the "1 + 2-6" mod-
els that have one feature for unigrams and an-
other feature for bigrams to 6-grams. Paraphrases
with score < .3 were filtered out. *** = sig-
nificance test over baseline with p < 0.0001,
using Koehn?s (2004) pair-wise bootstrap resam-
pling test for BLEU with 95% confidence interval.
Paraphrase Score
Source: deal
agreement 0.56
accord 0.53
talks 0.45
contract 0.42
peace deal 0.33
merger 0.32
agreement is 0.30
Source: fall
rise 0.87
slip 0.82
tumbled today 0.68
fell today 0.67
tumble 0.65
fall tokyo ap stock prices fell 0.56
are mixed 0.54
Source: to provide any other
to give any 0.74
to give further 0.70
to provide any 0.68
to give any other 0.62
to provide further 0.61
to provide other 0.53
to reveal any 0.52
to provide any further 0.48
to disclose any 0.47
to publicly discuss the 0.43
Source: we have a situation that
uncontroversial question about our 0.66
obviously with the developments this morning 0.65
community staffing of community centres 0.64
perhaps we are getting rather impatient 0.63
er around the inner edge 0.60
interested in going to the topics 0.60
and that is the day that 0.60
as a as a final point 0.59
left which it may still have 0.56
Table 3: English paraphrases from E2C 29K-
bitext systems.
386
the EACL 2009 Fourth Workshop on Statistical
Machine Translation:
1
the Spanish side of the
Europarl-v4, news training 2008, and news com-
mentary 2009. We also re-trained adding the JRC-
Acquis-v3 corpus
2
to the paraphrase training set,
and then adding also the LDC Spanish Gigaword
(LDC2006T12) and truncating the resulting cor-
pus after the first 150M lines. We lowercased
these training sets, tokenized and removed punc-
tuation marks and numbers, and this resulted in
training set sizes as detailed in Table 1. We gen-
erated paraphrases for phrases up to four tokens
in length, and used two arbitrary similarity thresh-
olds of minScore = 0.3 (as in the E2C experi-
ments), and 0.6, for enforcing only higher preci-
sion paraphrasing.
We experimented with these variants: a single
feature for all paraphrase (1-4grams); using only
paraphrases of unigrams (1grams); and using two
features: one only sensitive to unigrams and bi-
grams, and the other to the rest (1-2 + 3-4grams).
Results are shown in Table 4. We used BLEU
over lowercased outputs to evaluate all S2E sys-
tems, and Koehn?s significance test as above.
On the S2E 10,000-line subset, both the 1grams
and 1-4grams models achieved significant gains of
.4 BLEU points over the baseline. We concluded
from a manual evaluation of the 10,000-line mod-
els that the two major weaknesses of the baseline
system were (not surprisingly) number of untrans-
lated (OOV) words / phrases, followed by number
of superfluous words / phrases.
On the larger subset models, no system sig-
nificantly outperformed the baseline. Note that
our S2E baselines? scores are higher than those
of Callison-Burch et al (2006), since we evaluate
lowercased outputs, instead of recased ones.
6 Discussion and Future Work
We have shown that monolingually-derived para-
phrases, based on distributional semantic similar-
ity measures over a source-language corpus, can
improve the performance of statistical machine
translation (SMT) systems. Our proposed method
has the advantage of not relying on bitexts in order
to generate the paraphrases, and therefore gives
access to large amounts of monolingual training
data, for which creating bitexts of equivalent size
is generally unfeasible. We haven?t trained our
1
http://www.statmt.org/wmt09
2
http://wt.jrc.it/lt/Acquis
system on nearly as large a corpus as it can han-
dle, and indeed we see this as a natural next step.
Results support the assumption that a larger
monolingual paraphrase training set yields bet-
ter paraphrases: our S2E 1-4grams model per-
formed significantly better than baseline when us-
ing wmt09+aquis for paraphrasing, but when only
using wmt09, the model had a smaller advantage
that did not reach significance. However, for the
S2E 1grams model, there was a slight decrease in
performance when switching paraphrasing corpus
from wmt09+aquis to wmt09+aquis+afp. This ef-
fect might be due to the genre or unbalanced con-
tent of the additional corpus, or perhaps it is the
case that in this corpus size, paraphrases of higher-
level ngrams benefitted from the additional text
much more than paraphrases of unigrams did. The
two rightmost columns in Table 5 show that al-
though Spanish monolingual paraphrases for the
unigram baile improve when using the larger cor-
pus, (e.g., danza and un balie become the third and
fourth top candidates, pushing much worse candi-
dates far down the list), the two top paraphrase
candidates remained unchanged. However, for
the 4gram a favor del informe, antonymous can-
didates, which are bad and misleading for trans-
lation, are pushed down from the top first and
third spots by synonymous, better candidates. Ta-
ble 3 contains additional examples of good and
bad top paraphrase candidates, also in English.
Paraphrases of phrases seem to be of lower qual-
ity than those of unigrams, as can be seen at the
bottom of the table.
These results also show that our method is es-
pecially useful in settings involving low-density
languages or special domains: The smaller sub-
set models, emulating a resource-poor language
situation, show higher gains than larger models
(which are supersets of the smaller subset models),
when augmented with paraphrases derived from
the same paraphrase training set. This was vali-
dated in two very different language pairs: English
to Chinese, and Spanish to English. We believe
that larger monolingual training sets for paraphras-
ing can help languages with richer resources, and
we intend to explore this too.
Although the gains in the Spanish-English sub-
sets are somewhat smaller than the pivoting tech-
nique reported in Callison-Burch et al (2006),
e.g., .7 BLEU for the 10k subset, we take these
results as a proof of concept that can yield better
387
bitext mono.corp. features minScore BLEU TER
10k (baseline) ? ? 23.78 62.382
10k wmt09 1-4grams .6 23.81
10k wmt09 1-2+3-4gr .6 23.92 62.202
10k wmt09+aquis 1-4grams .6 24.13*** 61.739
10k wmt09+aquis 1grams .6 24.11 61.979
20k (baseline) ? ? 24.68 62.333
20k wmt09+aquis 1-4grams .6 24.75 61.528
80k (baseline) ? ? 27.89 57.977
80k wmt09+aquis 1-4grams .6 27.82 57.906
10k wmt09+aquis 1grams .3 24.11 61.979
10k wmt09+aquis+afp 1grams .3 23.97 61.974
20k wmt09+aquis+afp 1grams .3 24.77 61.276
80k wmt09+aquis+afp 1grams .3 27.84*** 57.781
Table 4: S2E Results: Lowercase BLEU and TER. Paraphrases with score < minScore were filtered out.
*** = significance test over baseline with p < 0.0001, using Koehn?s (2004) pair-wise bootstrap test for
BLEU with 95% confidence interval.
pivot wmt09+acquis wmt09+acquis+afp
Source: baile
danza el baile el baile
bailar baile y baile y
a de david palomar y la danza
dans viejo como quien se acomoda una un baile
empresa por juli?n estrada el tercero de teatro
coro al baile a la baloncesto el cine
Source: a favor del informe
a favor de este informe en contra del informe favor del informe
favor del informe a favor de este informe en contra del informe
el informe en contra de este informe a favor de este informe
a favor a favor de la resoluci?n en contra de este informe
por el informe a favor de esta resoluci?n en contra de la resoluci?n
al informe a favor del informe del se?or a favor del informe del sr.
su a favor del informe del sr. en contra del informe del sr.
del informe en contra de la propuesta a favor del excelente informe
de este informe contra el informe a favor del informe deprez
Table 5: Comparison of Spanish paraphrases: by pivoting, and by two monolingual corpora. Ordered
from best to worst score.
system example
source cuando escucho las distintas intervenciones , creo que quienes afirman que deber?amos analizar
nuestras prioridades y limitar el n?mero de objetivos que queremos conseguir , est?n en lo cierto .
reference when i listen to the various comments made , i find myself agreeing with those who recommend
that we take a look at our priorities and then limit the number of aims we want to achieve
baseline escucho when the various speeches, i believe that those who afirman that we should our
environmental limitar priorities and the number of objectives we want to achieve, are in this way.
pivoting (MW) when i can hear the various speeches , i believe that those people that we should look at our
priorities and to limit the number of objectives we want to achieve , are in fact .
wmt09+acquis escucho when the various speeches, i believe that those who claiming that we should environmental
.1-4grams limitar our priorities and the number of objectives we want to achieve, are on the way.
wmt09+acquis escucho when the various speeches, i believe that those who considered that we should our
.1grams environmental priorities and reducing the number of objectives we want to achieve, are on the way.
wmt09+acquis+afp escucho when the various speeches, i believe that those who say that we should our environmental
.1grams priorities and reduce the number of objectives we want to achieve, are on the way.
Table 6: S2E translation examples on 10k-bitext systems. Some translation differences are in bold.
388
gains with larger monolingual training sets. Pivot-
ing techniques (translating back and forth) rely on
limited resources (bitexts), and are subject to shifts
in meaning due to their inherent double transla-
tion step. In contrast, large monolingual resources
are relatively easy to collect, and our system in-
volves only a single translation/paraphrasing step
per target phrase. Table 5 also shows an exemplar
comparison with the pivoting paraphrases used in
Callison-Burch et al (2006). It seems that the piv-
oting paraphrases might suffer more from having
frequent function words as top candidates, which
might be a by-product of their alignment ?promis-
cuity?. However, the top antonymous candidate
problem seems to mainly plague the monolin-
gual distributional paraphrases (but improves with
larger corpora). See also Table 6.
The paraphrase quality remains an issue with
this method (as with all other paraphrasing meth-
ods). Some possible ways of improving it, be-
sides using larger corpora, are: using syntactic in-
formation (Callison-Burch, 2008), using semantic
knowledge such as thesaurus or WordNet to per-
form word sense disambiguation (WSD) (Resnik,
1999; Mohammad and Hirst, 2006), improving
the similarity measure, and refining the similarity
threshold. We would like to explore ways of incor-
porating syntactic knowledge that do not sacrifice
coverage as much as in Callison-Burch (2008); in-
corporating semantic knowledge to disambiguate
phrasal senses; using context to help sense disam-
biguation (Erk and Pad?, 2008); and optimizing
the similarity threshold for use in SMT, for exam-
ple on a held-out dataset: too high a threshold re-
duces coverage, while too low a threshold results
in bad paraphrases and translation.
The method presented here is quite general, and
therefore different similarity measures, including
other corpus-based ones, can be plugged in to gen-
erate paraphrases. We are looking into using DPs
with word-sense disambiguation: Since it has been
shown that similarity is often judged by the se-
mantic distance of the closest senses of the two
target words (Mohammad and Hirst, 2006), and
that paraphrases generated this way are likely to
be of higher quality (Marton et al, 2009), hence
it is also likely that the overall performance of an
SMT system using them will also improve further.
One potential advantage of using bitexts for
paraphrase generation is the usage of implicit hu-
man knowledge, i.e., sentence alignments. The
concern that not using this knowledge would turn
out detrimental to the performance of SMT sys-
tems augmented by paraphrases as described here
was largely put to rest, as our method improved
the tested subset SMT systems? quality.
Acknowledgments
Many thanks to Chris Dyer for his help with
the E2C set, and to Adam Lopez for his imple-
mentation of pattern matching with Suffix Ar-
ray. This research was partially supported by
the GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001 and NSF award 0838801, by the Euro-
MatrixPlus project funded by the European Com-
mission, and by the US National Science Foun-
dation under grant IIS-0713448. The views and
findings are the authors? alone.
References
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL-2001.
P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
313.
Lou Burnard. 2000. Reference Guide for the British
National Corpus. Oxford University Computing
Services, Oxford, England, world edition edition.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings NAACL-
2006.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP 2008, Waikiki, Hawai?i.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL-05, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Mona Diab and Steve Finch. 2000. A statistical word-
level translation model for comparable corpora. In
Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics of the Association for
Computational Linguistics, Geneva, Switzerland.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
389
Katrin Erk and Sebastian Pad?. 2008. A struc-
tured vector space model for word meaning in con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2086), pages 897?906, Honolulu, HI.
John R. Firth. 1957. A synopsis of linguistic theory
1930
?
U55. Studies in Linguistic Analysis, (special
volume of the Philological Society):1?32. Distribu-
tional Hypothesis.
Pascale Fung and Percy Cheung. 2004. Multi-
level bootstrapping for extracting parallel sentences
from a quasi-comparable corpus. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1051, Geneva, Switzerland. Asso-
ciation for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Proceedings of COLING-ACL98,
pages 414?420, Montreal, Canada.
Zellig S. Harris. 1940. Review of louis h. gray, foun-
dations of language (new york: Macmillan, 1939).
Language, 16(3):216
?
U231.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), demonstration
session, Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2004b. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296?304, San Francisco, CA.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parame-
ter tuning in statistical machine translation. In Pro-
ceedings of the ACL Workshop on Statistical Ma-
chine Translation.
Yuval Marton, Saif Mohammad, and Philip Resnik.
2009. Estimating semantic distance using soft se-
mantic constraints in knowledge-source / corpus hy-
brid models. In Procedings of EMNLP, Singapore.
S. McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2006), Sydney, Australia.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Doug Oard, David Doermann, Bonnie Dorr, Daqing
He, Phillip Resnik, William Byrne, Sanjeeve Khu-
danpur, David Yarowsky, Anton Leuski, Philipp
Koehn, and Kevin Knight. 2003. Desperately seek-
ing cebuano. In Proceedings of HLT-NAACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the ACL, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, John
Henderson, and Florence Reeder. 2002. Corpus-
based comprehensive and diagnostic MT evaluation:
Initial Arabic, Chinese, French, and Spanish results.
In Proceedings of the ACL Human Language Tech-
nology Conference, pages 124?127, San Diego, CA.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Confer-
ence of the Association for Computational Linguis-
tics., pages 519?525.
Philip Resnik and Noah Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Philip Resnik. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
Journal of Artificial Intelligence Research (JAIR),
11:95?130.
Hinrich Schuetze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retreival. Information Processing
and Management, 33(3):307
?
U318.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
John Makhoul, Linnea Micciulla, and Ralph
Weischedel. 2005. A study of translation error rate
with targeted human annotation. Technical Report
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
390
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 775?783,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Estimating Semantic Distance Using Soft Semantic Constraints
in Knowledge-Source?Corpus Hybrid Models
Yuval Marton
??
, Saif Mohammad
?
, and Philip Resnik
??
?
Department of Linguistics and
?
Laboratory for Computational Linguistics and Information Processing,
Institute for Advanced Computer Studies.
University of Maryland, College Park, MD 20742-7505, USA.
{ymarton,saif,resnik}@umiacs.umd.edu
Abstract
Strictly corpus-based measures of seman-
tic distance conflate co-occurrence infor-
mation pertaining to the many possible
senses of target words. We propose a
corpus?thesaurus hybrid method that uses
soft constraints to generate word-sense-
aware distributional profiles (DPs) from
coarser ?concept DPs? (derived from a
Roget-like thesaurus) and sense-unaware
traditional word DPs (derived from raw
text). Although it uses a knowledge
source, the method is not vocabulary-
limited: if the target word is not in the
thesaurus, the method falls back grace-
fully on the word?s co-occurrence infor-
mation. This allows the method to access
valuable information encoded in a lexical
resource, such as a thesaurus, while still
being able to effectively handle domain-
specific terms and named entities. Exper-
iments on word-pair ranking by semantic
distance show the new hybrid method to
be superior to others.
1 Introduction
Semantic distance is a measure of the closeness
in meaning of two concepts. People are consis-
tent judges of semantic distance. For example, we
can easily tell that the concepts of ?exercise? and
?jog? are closer in meaning than ?exercise? and
?theater?. Studies asking native speakers of a lan-
guage to rank word pairs in order of semantic dis-
tance confirm this?average inter-annotator corre-
lation on ranking word pairs in order of semantic
distance has been repeatedly shown to be around
0.9 (Rubenstein and Goodenough, 1965; Resnik,
1999).
A number of natural language tasks such as ma-
chine translation (Lopez, 2008) and word sense
disambiguation (Banerjee and Pedersen, 2003;
McCarthy, 2006), can be framed as semantic
distance problems. Thus, developing automatic
measures that are in-line with human notions of
semantic distance has received much attention.
These automatic approaches to semantic distance
rely on manually created lexical resources such as
WordNet, large amounts of text corpora, or both.
WordNet-based information content measures
have been successful (Hirst and Budanitsky,
2005), but there are significant limitations on their
applicability. They can be applied only if a Word-
Net exists for the language of interest (which is
not the case for the ?low-density? languages); and
even if there is a WordNet, a number of domain-
specific terms may not be encoded in it. On the
other hand, corpus-based distributional measures
of semantic distance, such as cosine and ?-skew
divergence (Dagan et al, 1999), rely on raw text
alone (Weeds et al, 2004; Mohammad, 2008).
However, when used to rank word pairs in order
of semantic distance or correct real-word spelling
errors, they have been shown to perform poorly
(Weeds et al, 2004; Mohammad and Hirst, 2006).
Mohammad and Hirst (2006) and Patwardhan
and Pedersen (2006) argued that word sense ambi-
guity is a key reason for the poor performance of
traditional distributional measures, and they pro-
posed hybrid approaches that are distributional in
nature, but also make use of information in lexical
resources such as published thesauri and WordNet.
However, both these approaches can be applied to
estimate the semantic distance between two terms
only if both terms exist in the lexical resource they
rely on. We know lexical resources tend to have
limited vocabulary and a large number of domain-
775
specific terms are usually not included.
It should also be noted that similarity values
from different distance measures are not compa-
rable (even after normalization to the same scale),
that is, a similarity score of .75 as per one distance
measure does not correspond to the same seman-
tic distance as a similarity score of .75 from an-
other distance measure.
1
Thus if one uses two
independent distance measures, in this case: one
resource-reliant and one only corpus-dependent,
then these two measures are not comparable (and
hence cannot be used in tandem), even if both
rely?partially or entirely?on distributional cor-
pus statistics.
We propose a hybrid semantic distance method
that inherently combines the elements of a
resource-reliant measure and a strictly corpus-
dependent measure by imposing resource-reliant
soft constraints on the corpus-dependent model.
We choose the Mohammad and Hirst (2006)
method as the resource-reliant method and not
one of the WordNet-based measures because, un-
like the WordNet-based measures, the Moham-
mad and Hirst method is distributional in nature
and so lends itself immediately for combination
with traditional distributional similarity measures.
Our new hybrid method combines concept?word
co-occurrence information (the Mohammad and
Hirst distributional profiles of thesaurus concepts
(DPC)) with word?word co-occurrence informa-
tion, to generate word-sense-biased distributional
profiles. The ?pure? corpus-based distributional
profile (a.k.a. co-occurrence vector, or word asso-
ciation vector), for some target word u, is biased
with soft constraints towards each of the concepts
c that list u in the thesaurus, to create a distribu-
tional profile that is specific to u in the sense that
is most related to the other words listed under c.
Thus, this method can make more fine-
grained distinctions than the Mohammad and Hirst
method, and yet uses word sense information.
2
Our proposed method falls back gracefully to rely
only on word-word co-occurrence information if
any of the target terms is not listed in the lexical re-
source. Experiments on the word-pair ranking task
1
All we can infer is that if w
1
and w
2
have a similarity
score of .75 and w
3
and w
4
have a score of .5 by the same
distance measure, then w
1
?w
2
are closer in meaning than
w
3
?w
4
.
2
Even though Mohammad and Hirst (2006) use thesaurus
categories as coarse concepts, their algorithm can be applied
using more finer-grained thesaurus word groupings (para-
graphs and semicolon units), as well.
on three different datasets show that the our pro-
posed hybrid measure outperforms all other com-
parable distance measures.
Mohammad and Hirst (2007) show that their
method can be used to compute semantic dis-
tance in a resource poor language L
1
by com-
bining its text with a thesaurus in a resource-rich
language L
2
using an L
1
?L
2
bilingual lexicon to
create cross-lingual distributional profiles of con-
cepts, that is, L
2
word co-occurrence profiles of
L
1
thesaurus concepts. Since our method makes
use of the Mohammad and Hirst DPCs, it can just
as well make use of their cross-lingual DPCs, to
compute semantic distance in a resource-poor lan-
guage, just as they did. We leave that for future
work.
2 Background and Related Work
Strictly speaking, semantic distance/closeness is
a property of lexical units?a combination of the
surface form and word sense.
3
Two terms are con-
sidered to be semantically close if there is a lex-
ical semantic relation between them. Such a re-
lation may be a classical relation such as hyper-
nymy, troponymy, meronymy, and antonymy, or
it may be what have been called an ad-hoc non-
classical relation, such as cause-and-effect (Mor-
ris and Hirst, 2004). If the closeness in meaning
is due to certain specific classical relations such as
hypernymy and troponymy, then the terms are said
to be semantically similar. Semantic relatedness
is the term used to describe the more general form
of semantic closeness caused by any semantic re-
lation (Hirst and Budanitsky, 2005). So the nouns
liquid and water are both semantically similar and
semantically related, whereas the nouns boat and
rudder are semantically related, but not similar.
The next three sub-sections describe three kinds
of automatic distance measures: (1) lexical-
resource-based measures that rely on a manually
created resource such as WordNet; (2) corpus-
based measures that rely only on co-occurrence
statistics from large corpora; and (3) hybrid mea-
sures that are distributional in nature, and that also
exploit the information in a lexical resource.
2.1 Lexical-resource-based measures
WordNet is a manually-created hierarchical net-
work of nodes (taxonomy), where each node in
3
The notion of semantic distance can be generalized, of
course, to larger units such as phrases, sentences, passages,
and so on (Landauer et al, 1998).
776
the network represents a fine-grained concept or
word sense. An edge between two nodes rep-
resents a lexical semantic relation such as hy-
pernymy and troponymy. WordNet-based mea-
sures consider two terms to be close if they occur
close to each other in the network (connected by
only a few arcs), if their definitions share many
terms (Banerjee and Pedersen, 2003; Patwardhan
and Pedersen, 2006), or if they share a lot of infor-
mation (Lin, 1998; Resnik, 1999). The length of
each arc/link (distance between nodes) can be as-
sumed a unit length, or can be computed from cor-
pus statistics. Within WordNet, the is-a hierarchy
is much more well-developed than that of other
lexical semantic relations. So, not surprisingly,
the best WordNet-based measures are those that
rely only on the is-a hierarchy. Therefore, they
are good at measuring semantic similarity (e.g.,
doctor?physician), but not semantic relatedness
(e.g., doctor?scalpel). Further, the measures can
only be used in languages that have a (sufficiently
developed) WordNet. WordNet sense information
has been criticized to be too fine grained (Agirre
and Lopez de Lacalle Lekuona, 2003; Navigli,
2006). See Hirst and Budanitsky (2005) for a com-
prehensive survey of WordNet-based measures.
2.2 Corpus-based measures
Strictly corpus-based measures of distributional
similarity rely on the hypothesis that words that
occur in similar context tend to be semantically
close (Firth, 1957; Harris, 1940). The set of
contexts of each target word u is represented by
its distributional profile (DP)?the set of words
that tend to co-occur with u within a certain dis-
tance, along with numeric scores signifying this
co-occurrence tendency with u. Then measures
such as cosine or ?-skew divergence are used to
determine how close the DPs of the two target
words are. See Section 3 for more details and re-
lated work. These measures are very appealing
because they rely simply on raw text, but, as de-
scribed earlier, when used to rank word pairs in
order of semantic distance, or to correct real-word
spelling errors, they perform poorly, compared
to the WordNet-based measures. See Weeds et
al. (2004), Mohammad (2008), and Curran (2004)
for detailed surveys of distributional measures.
As Mohammad and Hirst (2006) point out, the
DP of a word u conflates information about the
potentially many senses of u. For example, con-
sider the following. The noun bank has two senses
?river bank? and ?financial institution?. Assume
that bank, when used in the ?financial institu-
tion? sense, co-occurred with the noun money 100
times in a corpus. Similarly, assume that bank,
when used in the ?river bank? sense, co-occurred
with the noun boat 80 times. So the DP of bank
will have co-occurrence information with money
as well as boat:
DPW(bank):
money,100; boat,80; bond,70; fish,77; . . .
Assume that the DP of the word ATM is:
DPW(ATM):
money,120; boat,0; bond,90; fish,0; . . .
Thus the distributional distance of bank with ATM
will be some sort of an average of the seman-
tic distance between the ?financial institution? and
?ATM? senses and the semantic distance between
the ?river bank? and ?ATM? senses. However, in
various natural language tasks, we need the se-
mantic distance between the intended senses of
bank and ATM, which often also tends to be the
semantic distance between their closest senses.
2.3 Hybrid measures
Both Mohammad and Hirst (2006) and Patward-
han and Pedersen (2006) proposed measures that
are not only distributional in nature but also rely
on a lexical resource to exploit the manually en-
coded information therein as well as to overcome
the sense-conflation problem (described in sec-
tion 2.2). Since we essentially combine the Mo-
hammad and Hirst method with a ?pure? word-
based distributional measure to create our hybrid
approach, we briefly describe their method here.
Mohammad and Hirst (2006) generate separate
distributional profiles for the different senses of
a word, without using any sense-annotated data.
They use the categories in a Roget-style thesaurus
(Macquaries (Bernard, 1986)) as coarse senses or
concepts. There are about 1000 categories in a
thesaurus, and each category has on average 120
closely related words. A word may be found in
more than one category if it has multiple meaning.
They use a simple unsupervised algorithm to de-
termine the vector of words that tend to co-occur
with each concept and the corresponding strength
of association (a measure of how strong the ten-
dency to co-occur is). The target word u will be
assigned one DPC for each of the concepts that
777
list u. Below are example DPCs of the two con-
cepts pertaining to bank:
4
DPC(?fin. inst.?):
money,1000; boat,32; bond,705; fish,0; . . .
DPC(?river bank?):
money,5; boat,863; bond,0; fish,948; . . .
The distance between two words u, v is deter-
mined by calculating the closeness of each of the
DPCs of u to each of DPCs of v, and the closest
DPC-pair distance is chosen.
Mohammad and Hirst (2006) show that their ap-
proach performs better than other strictly corpus-
based approaches that they experimented with.
However, all those experiments were on word-
pairs that were listed in the thesaurus. Their ap-
proach is not applicable otherwise. In Sections 3
and 4 we show how cosine?log-likelihood-ratio
(or any comparable distributional measure) can be
combined with the Mohammad and Hirst DPCs to
form a hybrid approach that is not limited to the
vocabulary of a lexical resource.
Erk and Pad?o (2008) proposed a way of rep-
resenting a word sense in context by biasing the
target word?s DP according to the context sur-
rounding a target (specific) occurrence of the tar-
get word. They use dependency relations and se-
lectional preferences of the target word and com-
bine multiple DPs of words appearing in the con-
text of the target occurrence, in a manner so as
to give more weight to words co-occurring with
both the target word and the target occurrence?s
context words. The advantage of this approach
is that it does not rely on a thesaurus or Word-
Net. Its disadvantage is that it relies on depen-
dency relations and selectional preferences infor-
mation, and that the context information it uses in
order to determine the word sense is quite limited
(only the words surrounding a single occurrence
of the and hence the representation of that sense
might not be sufficiently accurate. Their approach
effectively assumes that each occurrence of a word
has a unique sense.
3 Distributional Measures with Soft
Semantic Constraints
Traditional distributional profiles of words (DPW)
give word?word co-occurrence frequencies. For
example, DPW(u) gives the number of times
4
The relatively large co-occurrence frequency values for
DPCs as compared to DPWs is because a concept can be ref-
ered to by many words (on average 100).
the target word u co-occurs with with all other
words:
5
DPW(u):
w
1
,f(u,w
1
); w
2
,f(u,w
2
); w
3
,f(u,w
3
); . . .
where f stands for co-occurrence frequency (and
can be generalized to stand for any strength
of association (SoA) measure such as the log-
likelihood ratio). Mohammad and Hirst create
concept?word co-occurrence vectors, ?distribu-
tional profiles of concepts? (DPCs), from non-
annotated corpus. For example, DPC(c) gives the
number of times the concept (thesaurus category)
c co-occurs with all the words in a corpus.
DPC(c):
w
1
,f(c,w
1
); w
2
,f(c,w
2
); w
3
,f(c,w
3
); . . .
A target word u that appears under thesaurus con-
cepts c
1
, ..., c
n
would be assigned to DPC(c
1
), ...,
DPC(c
n
). Therefore, if a target word v also ap-
pears under some same concept c, the DPCs of u
and v would be indistinguishable.
We propose the creation of distributional pro-
files of word senses (DPWS(u
c
)) that approximate
the SoA of the target word u, when used in sense
c, with each of the words in the corpus:
DPWS(u
c
):
w
1
,f(u
c
,w
1
); w
2
,f(u
c
,w
2
); w
3
,f(u
c
,w
3
); . . .
In order to get exact counts, one needs sense-
annotated data. However, such data is expensive
to create, and is scarce. Therefore, we propose
estimating these counts from the DPW and DPC
counts:
f(u
c
, w
i
) = p(c|w
i
)? f(u,w
i
) (1)
where the conditional probability p(c|w
i
) is calcu-
lated from the co-occurrence frequencies in DPCs;
and the co-occurrence count f(u,w
i
) is calcu-
lated from DPWs. If the target word is not in
the thesaurus?s vocabulary, then we assume uni-
form distribution over all concepts, and in prac-
tice use a single sense, and take the conditional
probability to be 1. Since the method takes sense-
proportional co-occurrence counts, we will refer
to this method as the hybrid-sense-proportional-
counts method (or, hybrid-prop for short).
5
The dimensions of the DP co-occurrence vector can be
defined arbitrarily, and do not have to correspond to the words
in the vocabulary. The most notable alternative representation
is the Latent Semantic Analysis and its variants (Landauer et
al., 1998; Finkelstein et al, 2002; Budiu et al, 2006).
778
For example, below is the DPWS of bank in
the ?financial institution? sense, calculated from
its DPW and DPCs:
DPW(bank):
money,100; boat,80; bond,70; fish,77; . . .
DPC(?fin. inst.?):
money,1000; boat,32; bond,705; fish,0; . . .
DPC(?river bank?):
money,5; boat,863; bond,0; fish,948; . . .
DPWS(bank
?fin.inst.?
):
money,(
1000
1000+5
? 100); boat,(
32
32+863
? 80);
bond,(
705
705+0
? 70); fish,(
0
0+948
? 77); . . .
Once the DPWS are calculated, any counts-
based SoA and distance measures can be ap-
plied. For example, in this work we use log-
likelihood ratio (Dunning, 1993) to determine
the SoA between a word sense and co-occurring
words, and cosine to determine the distance be-
tween two DPWS?s log likelihood vectors (Mc-
Donald, 2000). We also contrast this measure with
cosine of conditional probabilities vectors. Given
two target words, we determine the distance be-
tween each of their DPWS pairings and the closest
DPWS-pair distance is chosen.
3.1 The hybrid-sense-filtered-counts method
Since the DPCs are created in an unsupervised
manner, they are expected to be somewhat noisy.
Therefore, we also experimented with a variant of
the method proposed above, that simply makes use
of whether the conditional probability p(c|w
i
) is
greater than 0 or not:
f(u
c
, w
i
) =
{
f(u,w
i
) If p(c|w
i
) > 0
0 Otherwise
(2)
Since this method essentially filters out collocates
that are likely not relevant to the target sense c of
the target word u, we will refer to this method
as the hybrid-sense-filtered-counts method (or,
just hybrid-filt for short). Below is an example
hybrid-filtered DPWS of bank in the ?financial in-
stitution? sense:
DPWS(bank
?fin.inst.?
:
money,100); boat,80; bond,70; . . .
Note that the collocate fish is now filtered out,
whereas bank?s co-occurrence counts with money,
boat, and bond are left as is (and not sense-
proportionally attenuated).
4 Evaluation
We evaluated various methods on the task of
ranking word pairs in order of semantic dis-
tance. These methods included our sense-biased
methods as well as several baselines: the Mo-
hammad and Hirst (2006) DPC-based methods,
the traditional word-based distributional similar-
ity methods, and several Latent Semantic Analysis
(LSA)-based methods. We used three testsets and
their corresponding human judgment gold stan-
dards: (1) the Rubenstein and Goodenough (1965)
set of 65 noun pairs?denoted RG-65; (2) the
WordSimilarity-353 (Finkelstein et al, 2002) set
of 353 noun pairs (which include the RG-65
pairs) of which we discarded of one repeating
pair?denoted WS-353; and (3) the Resnik and
Diab (2000) set of 27 verb pairs?denoted RD-00.
4.1 Corpora and Pre-processing
We generated distributional profiles (DPWs
and DPCs) from the British National Corpus
(BNC) (Burnard, 2000), which is a balanced cor-
pus. We lowercased the characters, and stripped
numbers, punctuation marks, and any SGML-like
syntactic tags, but kept sentence boundary mark-
ers. The BNC contained 102,100,114 tokens of
546,299 types (vocabulary size) after tokenization.
For the verb set, we also lemmatized this corpus.
We considered two words as co-occurring if
they occurred in a window of?5 words from each
other. We stoplisted words that co-occurred with
more than 2000 word types.
4.2 Results
The Spearman rank correlations of the automatic
rankings of the RG-65, WS353, and RD-00 test-
sets with the corresponding gold-standard human
rankings is listed in Table 1.
6
The higher the
Spearman rank correlation, the more accurate is
the distance measure.
4.2.1 Results on the RG-65 testset
Baselines. We replicated the traditional word-
based distributional distance measure using co-
sine of vectors (DPs) containing conditional prob-
abilities (word-cos-cp). Its rank correlation of
.53 is close to the correlation of .54 reported in
Mohammad and Hirst (2006), hereafter MH06.
We replicated the MH06 concept-based approach
6
Certain experiments were not pursued as they were re-
dundant in supporting our claims.
779
Method RG-65 WS-353 RD-00
Baselines (replicated):
Traditional distributional measures
word-cos-cp .53 .31 .46
word-cos-ll .70 .54 .51
word-cos-pmi .62 .43 .57
Mohammad and Hirst methods and variants
concept-cos-cp .62 .38 .41
concept*-cos-cp .65 .33 .43
concept-cos-ll .60 .37 .43
concept*-cos-ll .64 .25 .27
concept*-cos-pmi .40 .19 .28
Other (LSA and variants)
LSA .56 .47 .55
GLSA-cos-pmi .18 n.p. n.p.
GLSA-cos-ll .47 n.p. .29
New methods:
hybrid-prop-cos-ll .72 .49 .53
hybrid-prop*-cos-ll .69 .46 .45
hybrid-filt-cos-ll .73 .54 .38
hybrid-filt*-cos-ll .77 .54 .39
hybrid-prop*-cos-pmi .58 .43 .71
hybrid-filt*-cos-pmi .61 .42 .64
Table 1: Spearman rank correlation on RG-65,
WS-353, and RD-00 testsets, trained on BNC.
?*? indicates the use of a smaller bootstrapped
concept?word co-occurrence matrix. ?n.p.? indi-
cates that the experiment was not pursued.
(concept-cos-cp), and its bootstrapped variant that
uses a smaller concept?word co-occurrence matrix
(concept*-cos-cp). The latter yielded a correla-
tion score .65, close to the .69 reported in MH06.
We also experimented with cosine of PMI vec-
tors (word-cos-pmi) which obtained a correlation
of .62. Log likelihood ratios (word-cos-ll) gave
best results among the baseline methods (.70), and
so we it more often in the implementations of our
hybrid method.
We conducted experiments with LSA and its
GLSA variants (Budiu et al, 2006) as additional
baselines. A limited vocabulary of the 33,000
most frequent words in the BNC and all test words
was used in these experiments. (A larger vocab-
ulary was computationally expensive and 33,000
is also the vocabulary size used by Budiu et
al. (2006) in their LSA experiments.)
New Methods: The hybrid method variants
proposed in this paper (hybrid-prop-cos-ll and
hybrid-filt-cos-ll) were the best performers on the
RG-65 test set. Particularly, they performed better
than both the traditional word-distance measures
(word-cos-ll), and our concept-based methods?
variants of the MH06 method that are used with
likelihood ratios (concept-cos-ll, concept*-cos-
ll). The -pmi methods were all poorer performers
than their -ll counterparts. The -pmi hybrid vari-
ants obtained higher scores than the concept-based
ones, but almost the same scores as the word-
based ones.
4.2.2 Results on WS-353 and RD-00 testsets
On WS-353, all our hybrid methods out-
performed their concept counterparts, and were
on par with their word-based counterparts. On
RD-00, word-cos-pmi out-performed all other
word-based methods, and the hybrid -pmi meth-
ods were best performers with scores of .64 and
.71. Our word-cos-ll, hybrid-prop-cos-ll, and
the two hybrid pmi results on RD-00 are better
than any non-WordNet results reported by Resnik
and Diab (2000), including their syntax-informed
methods?the variants of Lin (?distrib?, .43) and
Dorr (?LCS?, .39). In fact, our hybrid*-prop-cos-
pmi and hybrid*-filt-cos-pmi results reach corre-
lation levels of the WordNet-based methods re-
ported there (.66?.68). Also, on WS-353, our
hybrid sense-filtered variants and word-cos-ll ob-
tained a correlation score higher than published re-
sults using WordNet-based measures (Jarmasz and
Szpakowicz, 2003) (.33 to .35) and Wikipedia-
based methods (Ponzetto and Strube, 2006) (.19
to .48); and very close to the results obtained by
thesaurus-based (Jarmasz and Szpakowicz, 2003)
(.55) and LSA-based methods (Finkelstein et al,
2002) (.56).
The lower correlation scores of all measures on
the WS-353 test set are possibly due to it hav-
ing politically biased word pairs (examples in-
clude: Arafat?peace, Arafat?terror, Jerusalem?
Palestinian) for which BNC texts are likely to in-
duce low correlation with the human raters of WS-
353. This testset alo has disproportionately many
terms from the news domain.
The concept methods performed poorly on WS-
353 partly because many of the target words do
not exist in the thesaurus. For instance, there
were 17 such word types that occurred in 20 WS-
353 testset word pairs. When excluding these
pairs, concept-cos-cp goes up from .38 to .45, and
concept*-cos-pmi from .19 to .24. Interestingly,
results of the hybrid methods show that they were
largely unaffected by the out-of-vocabulary prob-
lem on the WS-353 dataset.
On the verbs dataset RD-00, while hybrid-prop-
cos-ll fared slightly better than word-cos-ll, using
the smaller matrix seemed to hurt performance of
780
hybrid*-prop-cos-ll compared to word-cos-ll. But
results suggest that the -pmi methods might serve
as a better measure than -ll for verbs, although this
claim should be tested more rigorously.
Human judgments of semantic distance are less
consistent on verb-pairs than on noun-pairs, as re-
flected in inter-rater agreement measures in Resnik
and Diab (2000) and others). Thus, not surpris-
ingly, the scores of almost all measures are lower
for the verb data than the RG-65 noun data.
5 Discussion
The hybrid methods proposed in this paper ob-
tained higher accuracies than all other methods on
the RG-65 testset (all of whose words were in the
published thesaurus), and on the RD-00 testset,
and their performance was at least respectable on
the WS-353 testset (many of whose words were
not in the published thesaurus). This is in con-
trast to the concept-distance methods which suf-
fer greatly when the target words are not in the
lexical resource (here, the thesaurus) they rely on,
even though these methods can make use of co-
occurrence information of words not in the the-
saurus with concepts from the thesaurus.
Amongst the two hybrid methods proposed, the
sense-filtered-counts method performed better
using the smaller bootstrapped concept?word co-
occurrence matrix whereas the sense-proportional
method performed better using the larger concept?
word co-occurrence matrix. We believe this is be-
cause the bootstrapping method proposed in Mo-
hammad and Hirst (2006) has the effect of reset-
ting to 0 the small co-occurrence counts. The
noise from these small co-occurrence counts af-
fects the sense-filtered-counts method more ad-
versely (since any non-zero value will cause the
inclusion of the corresponding collocate?s full co-
occurrence count) and so the bootstrapped matrix
is more suitable for this method.
The results also show that the cosine of log-
likelihood ratios method mostly performs better
than cosine of conditional probabilities and the
pmi methods on the noun sets. This further
supports the claim by Dunning (1993) that log-
likelihood ratio is much less sensitive than pmi
to low counts. Interestingly, on the verb set, the
pmi methods, and especially hybrid*-prop-cos-
pmi, did extremely well. Further investigation is
needed in order to determine if pmi is indeed more
suitable for verb semantic similarity, and why.
6 Conclusion
Traditional distributional similarity conflates co-
occurrence information pertaining to the many
senses of the target words. Mohammad and
Hirst (2006) show how distributional measures
can be used to compute distance between very
coarse word senses or concepts (thesaurus cat-
egories), and even obtain better results than
traditional distributional similarity. However,
their method requires that the target words be
listed in the thesaurus, which is often not the
case for domain-specific terms and named enti-
ties. In this paper, we proposed hybrid meth-
ods (hybrid-sense-filtered-counts and hybrid-
sense-proportional-counts) that combine word?
word co-occurrence information (traditional dis-
tributional similarity) with word?concept co-
occurrence information (Mohammad and Hirst,
2006), with soft constraints in such a manner
that the method makes use of information en-
coded in the thesaurus when available, and de-
grades gracefully if the target word is not listed
in the thesaurus. Our method generates word-
sense-biased distributional profiles (DPs) from
non-annotated corpus-based word-based DPs and
coarser-grained aggregated thesaurus-based ?con-
cept DPs? (DPCs). We showed that the hybrid
method correlates with human judgments of se-
mantic distance in most cases better than any of
the other methods we replicated.
We are now interested in improving seman-
tic distance measures for verb?verb, adjective?
adjective, and cross-part-of-speech pairs, by ex-
ploiting specific information pertaining to these
parts of speech in lexical resources in addition to
purely co-occurrence information.
Acknowledgments
We thank Mona Diab for her help with her verb
test set, Raluca Budiu for her help and clarifica-
tions regarding the GLSA method and its imple-
mentation details, and the anonymous reviewers
for their valuable feedback. This work was sup-
ported, in part, by the National Science Founda-
tion under Grant No. IIS-0705832, and in part, by
the Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor.
781
References
Eneko Agirre and Oier Lopez de Lacalle Lekuona.
2003. Clustering WordNet word senses. In Pro-
ceedings of the 1st International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), Borovets, Bulgaria.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 805?810, Acapulco, Mexico.
John R. L. Bernard, editor. 1986. The Macquarie The-
saurus. Macquarie Library, Sydney, Australia.
Raluca Budiu, Christiaan Royer, and Peter Pirolli.
2006. Modeling information scent: A compari-
son of LSA, PMI and GLSA similarity measures
on common tests and corpora. In Proceedings of
RIAO?07, Pittsburgh, PA.
Lou Burnard. 2000. Reference Guide for the British
National Corpus. Oxford University Computing
Services, Oxford, England, world edition edition.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, School of Informatics,
University of Edinburgh, Edinburgh, UK.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of cooccurrence probabili-
ties. Machine Learning, 34(1?3):43?69.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Katrin Erk and Sebastian Pad?o. 2008. A struc-
tured vector space model for word meaning in con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2086), pages 897?906, Honolulu, HI.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
John R. Firth. 1957. A synopsis of linguistic theory
193055. Studies in Linguistic Analysis, (special vol-
ume of the Philological Society):132. Distributional
Hypothesis.
Zellig S. Harris. 1940. Review of Louis H. Gray, foun-
dations of language (New York: Macmillan, 1939).
Language, 16(3):216?231.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. NLE, 11(1):87?111.
Mario Jarmasz and Stan Szpakowicz. 2003. Ro-
get?s Thesaurus and semantic similarity. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP-
2003), pages 212?219, Borovets, Bulgaria.
Thomas Landauer, Peter Foltz, and Darrell Laham.
1998. Introduction to latent semantic analysis. Dis-
course Processes, 25:259 ? 284.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, page
296304, San Francisco, CA.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):149.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. In Proceedings of the
European Chapter of the Association for Computa-
tional Linguistics Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 17?24, Trento, Italy.
S. McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh, Edinburgh, UK.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of EMNLP.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distribu-
tional profiles of concepts for measuring seman-
tic distance. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP/CoNLL-2007), pages 571?580,
Prague, Czech Republic.
Saif Mohammad. 2008. Measuring Semantic Distance
using Distributional Profiles of Concepts. Ph.D. the-
sis, Department of Computer Science, University of
Toronto, Toronto, Canada.
Jane Morris and Graeme Hirst. 2004. Non-classical
lexical semantic relations. In Proceedings of the
Workshop on Computational Lexical Semantics, Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 46?51, Boston, Mas-
sachusetts.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association, pages 105?
112, Sydney, Australia.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
Making Sense of Sense EACL Workshop, pages 1?8.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL-2006),
pages 192?199, New York, NY.
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In 22nd Annual Meeting of the Cognitive
Science Society (COGSCI2000), Philadelphia, PA.
Philip Resnik. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
JAIR, 11:95?130.
782
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics
(COLING-04), pages 1015?1021, Geneva, Switzer-
land.
783
Proceedings of ACL-08: HLT, pages 1003?1011,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Soft Syntactic Constraints for Hierarchical Phrased-Based Translation
Yuval Marton and Philip Resnik
Department of Linguistics
and the Laboratory for Computational Linguistics and Information Processing (CLIP)
at the Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742-7505, USA
{ymarton, resnik} @t umiacs.umd.edu
Abstract
In adding syntax to statistical MT, there is
a tradeoff between taking advantage of lin-
guistic analysis, versus allowing the model
to exploit linguistically unmotivated mappings
learned from parallel training data. A num-
ber of previous efforts have tackled this trade-
off by starting with a commitment to linguisti-
cally motivated analyses and then finding ap-
propriate ways to soften that commitment. We
present an approach that explores the trade-
off from the other direction, starting with a
context-free translation model learned directly
from aligned parallel text, and then adding soft
constituent-level constraints based on parses
of the source language. We obtain substantial
improvements in performance for translation
from Chinese and Arabic to English.
1 Introduction
The statistical revolution in machine translation, be-
ginning with (Brown et al, 1993) in the early 1990s,
replaced an earlier era of detailed language analy-
sis with automatic learning of shallow source-target
mappings from large parallel corpora. Over the last
several years, however, the pendulum has begun to
swing back in the other direction, with researchers
exploring a variety of statistical models that take ad-
vantage of source- and particularly target-language
syntactic analysis (e.g. (Cowan et al, 2006; Zoll-
mann and Venugopal, 2006; Marcu et al, 2006; Gal-
ley et al, 2006) and numerous others).
Chiang (2005) distinguishes statistical MT ap-
proaches that are ?syntactic? in a formal sense, go-
ing beyond the finite-state underpinnings of phrase-
based models, from approaches that are syntactic
in a linguistic sense, i.e. taking advantage of a
priori language knowledge in the form of annota-
tions derived from human linguistic analysis or tree-
banking.1 The two forms of syntactic modeling are
doubly dissociable: current research frameworks in-
clude systems that are finite state but informed by
linguistic annotation prior to training (e.g., (Koehn
and Hoang, 2007; Birch et al, 2007; Hassan et al,
2007)), and also include systems employing context-
free models trained on parallel text without benefit
of any prior linguistic analysis (e.g. (Chiang, 2005;
Chiang, 2007; Wu, 1997)). Over time, however,
there has been increasing movement in the direction
of systems that are syntactic in both the formal and
linguistic senses.
In any such system, there is a natural tension be-
tween taking advantage of the linguistic analysis,
versus allowing the model to use linguistically un-
motivated mappings learned from parallel training
data. The tradeoff often involves starting with a sys-
tem that exploits rich linguistic representations and
relaxing some part of it. For example, DeNeefe et
al. (2007) begin with a tree-to-string model, using
treebank-based target language analysis, and find it
useful to modify it in order to accommodate useful
?phrasal? chunks that are present in parallel train-
ing data but not licensed by linguistically motivated
parses of the target language. Similarly, Cowan et al
(2006) focus on using syntactically rich representa-
tions of source and target parse trees, but they re-
sort to phrase-based translation for modifiers within
1See (Lopez, to appear) for a comprehensive survey.
1003
clauses. Finding the right way to balance linguistic
analysis with unconstrained data-driven modeling is
clearly a key challenge.
In this paper we address this challenge from a less
explored direction. Rather than starting with a sys-
tem based on linguistically motivated parse trees, we
begin with a model that is syntactic only in the for-
mal sense. We then introduce soft constraints that
take source-language parses into account to a lim-
ited extent. Introducing syntactic constraints in this
restricted way allows us to take maximal advantage
of what can be learned from parallel training data,
while effectively factoring in key aspects of linguis-
tically motivated analysis. As a result, we obtain
substantial improvements in performance for both
Chinese-English and Arabic-English translation.
In Section 2, we briefly review the Hiero statis-
tical MT framework (Chiang, 2005, 2007), upon
which this work builds, and we discuss Chiang?s ini-
tial effort to incorporate soft source-language con-
stituency constraints for Chinese-English transla-
tion. In Section 3, we suggest that an insufficiently
fine-grained view of constituency constraints was re-
sponsible for Chiang?s lack of strong results, and
introduce finer grained constraints into the model.
Section 4 demonstrates the the value of these con-
straints via substantial improvements in Chinese-
English translation performance, and extends the ap-
proach to Arabic-English. Section 5 discusses the
results, and Section 6 considers related work. Fi-
nally we conclude in Section 7 with a summary and
potential directions for future work.
2 Hierarchical Phrase-based Translation
2.1 Hiero
Hiero (Chiang, 2005; Chiang, 2007) is a hi-
erarchical phrase-based statistical MT framework
that generalizes phrase-based models by permit-
ting phrases with gaps. Formally, Hiero?s trans-
lation model is a weighted synchronous context-
free grammar. Hiero employs a generalization of
the standard non-hierarchical phrase extraction ap-
proach in order to acquire the synchronous rules
of the grammar directly from word-aligned paral-
lel text. Rules have the form X ? ?e?, f??, where
e? and f? are phrases containing terminal symbols
(words) and possibly co-indexed instances of the
nonterminal symbol X.2 Associated with each rule
is a set of translation model features, ?i(f? , e?); forexample, one intuitively natural feature of a rule is
the phrase translation (log-)probability ?(f? , e?) =
log p(e?|f?) , directly analogous to the corresponding
feature in non-hierarchical phrase-based models like
Pharaoh (Koehn et al, 2003). In addition to this
phrase translation probability feature, Hiero?s fea-
ture set includes the inverse phrase translation prob-
ability log p(f? |e?), lexical weights lexwt(f? |e?) and
lexwt(e?|f?), which are estimates of translation qual-
ity based on word-level correspondences (Koehn et
al., 2003), and a rule penalty allowing the model to
learn a preference for longer or shorter derivations;
see (Chiang, 2007) for details.
These features are combined using a log-linear
model, with each synchronous rule contributing
?
i
?i?i(f? , e?) (1)
to the total log-probability of a derived hypothesis.
Each ?i is a weight associated with feature ?i, andthese weights are typically optimized using mini-
mum error rate training (Och, 2003).
2.2 Soft Syntactic Constraints
When looking at Hiero rules, which are acquired au-
tomatically by the model from parallel text, it is easy
to find many cases that seem to respect linguistically
motivated boundaries. For example,
X ? ?jingtian X1, X1 this year?,
seems to capture the use of jingtian/this year as
a temporal modifier when building linguistic con-
stituents such as noun phrases (the election this
year) or verb phrases (voted in the primary this
year). However, it is important to observe that noth-
ing in the Hiero framework actually requires nonter-
minal symbols to cover linguistically sensible con-
stituents, and in practice they frequently do not.3
2This is slightly simplified: Chiang?s original formulation
of Hiero, which we use, has two nonterminal symbols, X and
S. The latter is used only in two special ?glue? rules that permit
complete trees to be constructed via concatenation of subtrees
when there is no better way to combine them.
3For example, this rule could just as well be applied with X1
covering the ?phrase? submitted and to produce non-constituent
substring submitted and this year in a hypothesis like The bud-
get was submitted and this year cuts are likely.
1004
Chiang (2005) conjectured that there might be
value in allowing the Hiero model to favor hy-
potheses for which the synchronous derivation re-
spects linguistically motivated source-language con-
stituency boundaries, as identified using a parser.
He tested this conjecture by adding a soft constraint
in the form of a ?constituency feature?: if a syn-
chronous rule X ? ?e?, f?? is used in a derivation,
and the span of f? is a constituent in the source-
language parse, then a term ?c is added to the modelscore in expression (1).4 Unlike a hard constraint,
which would simply prevent the application of rules
violating syntactic boundaries, using the feature to
introduce a soft constraint allows the model to boost
the ?goodness? for a rule if it is constitent with the
source language constituency analysis, and to leave
its score unchanged otherwise. The weight ?c, likeall other ?i, is set via minimum error rate train-ing, and that optimization process determines em-
pirically the extent to which the constituency feature
should be trusted.
Figure 1 illustrates the way the constituency fea-
ture worked, treating English as the source language
for the sake of readability. In this example, ?c wouldbe added to the hypothesis score for any rule used in
the hypothesis whose source side spanned the minis-
ter, a speech, yesterday, gave a speech yesterday, or
the minister gave a speech yesterday. A rule trans-
lating, say, minister gave a as a unit would receive
no such boost.
Chiang tested the constituency feature for
Chinese-English translation, and obtained no signif-
icant improvement on the test set. The idea then
seems essentially to have been abandoned; it does
not appear in later discussions (Chiang, 2007).
3 Soft Syntactic Constraints, Revisited
On the face of it, there are any number of possi-
ble reasons Chiang?s (2005) soft constraint did not
work ? including, for example, practical issues like
the quality of the Chinese parses.5 However, we fo-
cus here on two conceptual issues underlying his use
of source language syntactic constituents.
4Formally, ?c(f? , e?) is defined as a binary feature, with
value 1 if f? spans a source constituent and 0 otherwise. In the
latter case ?c?c(f? , e?) = 0 and the score in expression (1) is
unaffected.
5In fact, this turns out not to be the issue; see Section 4.
Figure 1: Illustration of Chiang?s (2005) syntactic con-
stituency feature, which does not distinguish among con-
stituent types.
First, the constituency feature treats all syntac-
tic constituent types equally, making no distinction
among them. For any given language pair, however,
there might be some source constituents that tend to
map naturally to the target language as units, and
others that do not (Fox, 2002; Eisner, 2003). More-
over, a parser may tend to be more accurate for some
constituents than for others.
Second, the Chiang (2005) constituency feature
gives a rule additional credit when the rule?s source
side overlaps exactly with a source-side syntactic
constituent. Logically, however, it might make sense
not just to give a rule X ? ?e?, f?? extra credit when
f? matches a constituent, but to incur a cost when f?
violates a constituent boundary. Using the example
in Figure 1, we might want to penalize hypotheses
containing rules where f? is the minister gave a (and
other cases, such as minister gave, minister gave a,
and so forth).6
These observations suggest a finer-grained ap-
proach to the constituency feature idea, retaining the
idea of soft constraints, but applying them using var-
ious soft-constraint constituency features. Our first
observation argues for distinguishing among con-
stituent types (NP, VP, etc.). Our second observa-
tion argues for distinguishing the benefit of match-
6This accomplishes coverage of the logically complete set of
possibilities, which include not only f? matching a constituent
exactly or crossing its boundaries, but also f? being properly
contained within the constituent span, properly containing it,
or being outside it entirely. Whenever these latter possibilities
occur, f? will exactly match or cross the boundaries of some
other constituent.
1005
ing constituents from the cost of crossing constituent
boundaries. We therefore define a space of new fea-
tures as the cross product
{CP, IP, NP, VP, . . .} ? {=,+}.
where = and + signify matching and crossing bound-
aries, respectively. For example, ?NP= would de-note a binary feature that matches whenever the span
of f? exactly covers an NP in the source-side parse
tree, resulting in ?NP= being added to the hypoth-esis score (expression (1)). Similarly, ?VP+ woulddenote a binary feature that matches whenever the
span of f? crosses a VP boundary in the parse tree,
resulting in ?VP+ being subtracted from the hypoth-esis score.7 For readability from this point forward,
we will omit ? from the notation and refer to features
such as NP= (which one could read as ?NP match?),
VP+ (which one could read as ?VP crossing?), etc.
In addition to these individual features, we define
three more variants:
? For each constituent type, e.g. NP, we define
a feature NP_ that ties the weights of NP= and
NP+. If NP= matches a rule, the model score is
incremented by ?NP_, and if NP+ matches, themodel score is decremented by the same quan-
tity.
? For each constituent type, e.g. NP, we define a
version of the model, NP2, in which NP= and
NP+ are both included as features, with sepa-
rate weights ?NP= and ?NP+.
? We define a set of ?standard? linguistic labels
containing {CP, IP, NP, VP, PP, ADJP, ADVP,
QP, LCP, DNP} and excluding other labels such
as PRN (parentheses), FRAG (fragment), etc.8
We define feature XP= as the disjunction of
{CP=, IP=, . . ., DNP=}; i.e. its value equals 1
for a rule if the span of f? exactly covers a con-
stituent having any of the standard labels. The
7Formally, ?VP+ simply contributes to the sum in expres-sion (1), as with all features in the model, but weight optimiza-
tion using minimum error rate training should, and does, auto-
matically assign this feature a negative weight.
8We map SBAR and S labels in Arabic parses to CP and IP,
respectively, consistent with the Chinese parses. We map Chi-
nese DP labels to NP. DNP and LCP appear only in Chinese. We
ran no ADJP experiment in Chinese, because this label virtually
aways spans only one token in the Chinese parses.
definitions of XP+, XP_, and XP2 are analo-
gous.
? Similarly, since Chiang?s original constituency
feature can be viewed as a disjunctive ?all-
labels=? feature, we also defined ?all-labels+?,
?all-labels2?, and ?all-labels_? analogously.
4 Experiments
We carried out MT experiments for translation
from Chinese to English and from Arabic to En-
glish, using a descendant of Chiang?s Hiero sys-
tem. Language models were built using the SRI
Language Modeling Toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Word-level alignments were obtained
using GIZA++ (Och and Ney, 2000). The base-
line model in both languages used the feature set
described in Section 2; for the Chinese baseline we
also included a rule-based number translation fea-
ture (Chiang, 2007).
In order to compute syntactic features, we an-
alyzed source sentences using state of the art,
tree-bank trained constituency parsers ((Huang et
al., 2008) for Chinese, and the Stanford parser
v.2007-08-19 for Arabic (Klein and Manning,
2003a; Klein and Manning, 2003b)). In addition
to the baseline condition, and baseline plus Chi-
ang?s (2005) original constituency feature, experi-
mental conditions augmented the baseline with ad-
ditional features as described in Section 3.
All models were optimized and tested using the
BLEU metric (Papineni et al, 2002) with the NIST-
implemented (?shortest?) effective reference length,
on lowercased, tokenized outputs/references. Sta-
tistical significance of difference from the baseline
BLEU score was measured by using paired boot-
strap re-sampling (Koehn, 2004).9
4.1 Chinese-English
For the Chinese-English translation experiments, we
trained the translation model on the corpora in Ta-
ble 1, totalling approximately 2.1 million sentence
pairs after GIZA++ filtering for length ratio. Chi-
nese text was segmented using the Stanford seg-
menter (Tseng et al, 2005).
9Whenever we use the word ?significant?, we mean ?statis-
tically significant? (at p < .05 unless specified otherwise).
1006
LDC ID Description
LDC2002E18 Xinhua Ch/Eng Par News V1 beta
LDC2003E07 Ch/En Treebank Par Corpus
LDC2005T10 Ch/En News Mag Par Txt (Sinorama)
LDC2003E14 FBIS Multilanguage Txts
LDC2005T06 Ch News Translation Txt Pt 1
LDC2004T08 HK Par Text (only HKNews)
Table 1: Training corpora for Chinese-English translation
We trained a 5-gram language model using the
English (target) side of the training set, pruning 4-
gram and 5-gram singletons. For minimum error
rate training and development we used the NIST
MTeval MT03 set.
Table 2 presents our results. We first evaluated
translation performance using the NIST MT06 (nist-
text) set. Like Chiang (2005), we find that the orig-
inal, undifferentiated constituency feature (Chiang-
05) introduces a negligible, statistically insignificant
improvement over the baseline. However, we find
that several of the finer-grained constraints (IP=,
VP=, VP+, QP+, and NP=) achieve statistically
significant improvements over baseline (up to .74
BLEU), and the latter three also improve signifi-
cantly on the undifferentiated constituency feature.
By combining multiple finer-grained syntactic fea-
tures, we obtain significant improvements of up to
1.65 BLEU points (NP_, VP2, IP2, all-labels_, and
XP+).
We also obtained further gains using combina-
tions of features that had performed well; e.g., con-
dition IP2.VP2.NP_ augments the baseline features
with IP2 and VP2 (i.e. IP=, IP+, VP= and VP+),
and NP_ (tying weights of NP= and NP+; see Sec-
tion 3). Since component features in those combi-
nations were informed by individual-feature perfor-
mance on the test set, we tested the best perform-
ing conditions from MT06 on a new test set, NIST
MT08. NP= and VP+ yielded significant improve-
ments of up to 1.53 BLEU. Combination conditions
replicated the pattern of results from MT06, includ-
ing the same increasing order of gains, with im-
provements up to 1.11 BLEU.
4.2 Arabic-English
For Arabic-English translation, we used the train-
ing corpora in Table 3, approximately 100,000 sen-
Chinese MT06 MT08
Baseline .2624 .2064
Chiang-05 .2634 .2065
PP= .2607
DNP+ .2621
CP+ .2622
AP+ .2633
AP= .2634
DNP= .2640
IP+ .2643
PP+ .2644
LCP= .2649
LCP+ .2654
CP= .2657
NP+ .2662
QP= .2674^+ .2071
IP= .2680*+ .2061
VP= .2683* .2072
VP+ .2693**++ .2109*+
QP+ .2694**++ .2091
NP= .2698**++ .2217**++
Multiple / conflated features:
QP2 .2614
NP2 .2621
XP= .2630
XP2 .2633
all-labels+ .2633
VP_ .2637
QP_ .2641
NP.VP.IP=.QP.VP+ .2646
IP_ .2647
IP2+VP2 .2649
all-labels2 .2673*- .2070
NP_ .2690**++ .2101^+
IP2.VP2.NP_ .2699**++ .2105*+
VP2 .2722**++ .2123**++
all-labels_ .2731**++ .2125*++
IP2 .2750**++ .2132**+
XP+ .2789**++ .2175**++
Table 2: Chinese-English results. *: Significantly better
than baseline (p < .05). **: Significantly better than
baseline (p < .01). ^: Almost significantly better than
baseline (p < .075). +: Significantly better than Chiang-
05 (p < .05). ++: Significantly better than Chiang-05
(p < .01). -: Almost significantly better than Chiang-05
(p < .075).
1007
LDC ID Description
LDC2004T17 Ar News Trans Txt Pt 1
LDC2004T18 Ar/En Par News Pt 1
LDC2005E46 Ar/En Treebank En Translation
LDC2004E72 eTIRR Ar/En News Txt
Table 3: Training corpora for Arabic-English translation
tence pairs after GIZA++ length-ratio filtering. We
trained a trigram language model using the English
side of this training set, plus the English Gigaword
v2 AFP and Gigaword v1 Xinhua corpora. Devel-
opment and minimum error rate training were done
using the NIST MT02 set.
Table 4 presents our results. We first tested on
on the NIST MT03 and MT06 (nist-text) sets. On
MT03, the original, undifferentiated constituency
feature did not improve over baseline. Two individ-
ual finer-grained features (PP+ and AdvP=) yielded
statistically significant gains up to .42 BLEU points,
and feature combinations AP2, XP2 and all-labels2
yielded significant gains up to 1.03 BLEU points.
XP2 and all-labels2 also improved significantly on
the undifferentiated constituency feature, by .72 and
1.11 BLEU points, respectively.
For MT06, Chiang?s original feature improved the
baseline significantly ? this is a new result using
his feature, since he did not experiment with Ara-
bic ? as did our our IP=, PP=, and VP= condi-
tions. Adding individual features PP+ and AdvP=
yielded significant improvements up to 1.4 BLEU
points over baseline, and in fact the improvement for
individual feature AdvP= over Chiang?s undifferen-
tiated constituency feature approaches significance
(p < .075).
More important, several conditions combining
features achieved statistically significant improve-
ments over baseline of up 1.94 BLEU points: XP2,
IP2, IP, VP=.PP+.AdvP=, AP2, PP+.AdvP=, and
AdvP2. Of these, AdvP2 is also a significant im-
provement over the undifferentiated constituency
feature (Chiang-05), with p < .01. As we did
for Chinese, we tested the best-performing models
on a new test set, NIST MT08. Consistent patterns
reappeared: improvements over the baseline up to
1.69 BLEU (p < .01), with AdvP2 again in the
lead (also outperforming the undifferentiated con-
stituency feature, p < .05).
Arabic MT03 MT06 MT08
Baseline .4795 .3571 .3571
Chiang-05 .4787 .3679** .3678**
VP+ .4802 .3481
AP+ .4856 .3495
IP+ .4818 .3516
CP= .4815 .3523
NP= .4847 .3537
NP+ .4800 .3548
AP= .4797 .3569
AdvP+ .4852 .3572
CP+ .4758 .3578
IP= .4811 .3636** .3647**
PP= .4801 .3651** .3662**
VP= .4803 .3655** .3694**
PP+ .4837** .3707** .3700**
AdvP= .4823** .3711**- .3717**
Multiple / conflated features:
XP+ .4771 .3522
all-labels2 .4898**+ .3536 .3572
all-labels_ .4828 .3548
VP2 .4826 .3552
NP2 .4832 .3561
AdvP.VP.PP.IP= .4826 .3571
VP_ .4825 .3604
all-labels+ .4825 .3600
XP2 .4859**+ .3605^ .3613**
IP2 .4793 .3611* .3593
IP_ .4791 .3635* .3648**
XP= .4808 .3659** .3704**+
VP=.PP+.AdvP= .4833** .3677** .3718**
AP2 .4840** .3692** .3719**
PP+.AdvP= .4777 .3708** .3680**
AdvP2 .4803 .3765**++ .3740**+
Table 4: Arabic-English Experiments. Results are
sorted by MT06 BLEU score. *: Better than baseline
(p < .05). **: Better than baseline (p < .01). +: Bet-
ter than Chiang-05 (p < .05). ++: Better than Chiang-05
(p < .01). -: Almost significantly better than Chiang-05
(p < .075)
1008
5 Discussion
The results in Section 4 demonstrate, to our knowl-
edge for the first time, that significant and sometimes
substantial gains over baseline can be obtained by
incorporating soft syntactic constraints into Hiero?s
translation model. Within language, we also see
considerable consistency across multiple test sets, in
terms of which constraints tend to help most.
Furthermore, our results provide some insight into
why the original approach may have failed to yield a
positive outcome. For Chinese, we found that when
we defined finer-grained versions of the exact-match
features, there was value for some constituency
types in biasing the model to favor matching the
source language parse. Moreover, we found that
there was significant value in allowing the model
to be sensitive to violations (crossing boundaries)
of source parses. These results confirm that parser
quality was not the limitation in the original work
(or at least not the only limitation), since in our ex-
periments the parser was held constant.
Looking at combinations of new features, some
?double-feature? combinations (VP2, IP2) achieved
large gains, although note that more is not neces-
sarily better: combinations of more features did not
yield better scores, and some did not yield any gain
at all. No conflated feature reached significance, but
it is not the case that all conflated features are worse
than their same-constituent ?double-feature? coun-
terparts. We found no simple correlation between
finer-grained feature scores (and/or boundary con-
dition type) and combination or conflation scores.
Since some combinations seem to cancel individ-
ual contributions, we can conclude that the higher
the number of participant features (of the kinds de-
scribed here), the more likely a cancellation effect is;
therefore, a ?double-feature? combination is more
likely to yield higher gains than a combination con-
taining more features.
We also investigated whether non-canonical lin-
guistic constituency labels such as PRN, FRAG,
UCP and VSB introduce ?noise?, by means of the
XP features ? the XP= feature is, in fact, simply the
undifferentiated constituency feature, but sensitive
only to ?standard? XPs. Although performance of
XP=, XP2 and all-labels+ were similar to that of the
undifferentiated constituency feature, XP+ achieved
the highest gain. Intuitively, this seems plausible:
the feature says, at least for Chinese, that a transla-
tion hypothesis should incur a penalty if it is trans-
lating a substring as a unit when that substring is not
a canonical source constituent.
Having obtained positive results with Chinese, we
explored the extent to which the approach might
improve translation using a very different source
language. The approach on Arabic-English trans-
lation yielded large BLEU gains over baseline, as
well as significant improvements over the undiffer-
entiated constituency feature. Comparing the two
sets of experiments, we see that there are definitely
language-specific variations in the value of syntactic
constraints; for example, AdvP, the top performer in
Arabic, cannot possibly perform well for Chinese,
since in our parses the AdvP constituents rarely in-
clude more than a single word. At the same time,
some IP and VP variants seem to do generally well
in both languages. This makes sense, since ? at
least for these language pairs and perhaps more gen-
erally ? clauses and verb phrases seem to corre-
spond often on the source and target side. We found
it more surprising that no NP variant yielded much
gain in Arabic; this question will be taken up in fu-
ture work.
6 Related Work
Space limitations preclude a thorough review of
work attempting to navigate the tradeoff between us-
ing language analyzers and exploiting unconstrained
data-driven modeling, although the recent literature
is full of variety and promising approaches. We limit
ourselves here to several approaches that seem most
closely related.
Among approaches using parser-based syntactic
models, several researchers have attempted to re-
duce the strictness of syntactic constraints in or-
der to better exploit shallow correspondences in
parallel training data. Our introduction has al-
ready briefly noted Cowan et al (2006), who relax
parse-tree-based alignment to permit alignment of
non-constituent subphrases on the source side, and
translate modifiers using a separate phrase-based
model, and DeNeefe et al (2007), who modify
syntax-based extraction and binarize trees (follow-
ing (Wang et al, 2007b)) to improve phrasal cov-
1009
erage. Similarly, Marcu et al (2006) relax their
syntax-based system by rewriting target-side parse
trees on the fly in order to avoid the loss of ?non-
syntactifiable? phrase pairs. Setiawan et al (2007)
employ a ?function-word centered syntax-based ap-
proach?, with synchronous CFG and extended ITG
models for reordering phrases, and relax syntac-
tic constraints by only using a small number func-
tion words (approximated by high-frequency words)
to guide the phrase-order inversion. Zollman and
Venugopal (2006) start with a target language parser
and use it to provide constraints on the extraction of
hierarchical phrase pairs. Unlike Hiero, their trans-
lation model uses a full range of named nonterminal
symbols in the synchronous grammar. As an alter-
native way to relax strict parser-based constituency
requirements, they explore the use of phrases span-
ning generalized, categorial-style constituents in the
parse tree, e.g. type NP/NN denotes a phrase like
the great that lacks only a head noun (say, wall) in
order to comprise an NP.
In addition, various researchers have explored the
use of hard linguistic constraints on the source side,
e.g. via ?chunking? noun phrases and translating
them separately (Owczarzak et al, 2006), or by per-
forming hard reorderings of source parse trees in
order to more closely approximate target-language
word order (Wang et al, 2007a; Collins et al, 2005).
Finally, another soft-constraint approach that can
also be viewed as coming from the data-driven side,
adding syntax, is taken by Riezler and Maxwell
(2006). They use LFG dependency trees on both
source and target sides, and relax syntactic con-
straints by adding a ?fragment grammar? for un-
parsable chunks. They decode using Pharaoh, aug-
mented with their own log-linear features (such as
p(esnippet|fsnippet) and its converse), side by side to?traditional? lexical weights. Riezler and Maxwell
(2006) do not achieve higher BLEU scores, but
do score better according to human grammaticality
judgments for in-coverage cases.
7 Conclusion
When hierarchical phrase-based translation was in-
troduced by Chiang (2005), it represented a new and
successful way to incorporate syntax into statistical
MT, allowing the model to exploit non-local depen-
dencies and lexically sensitive reordering without
requiring linguistically motivated parsing of either
the source or target language. An approach to incor-
porating parser-based constituents in the model was
explored briefly, treating syntactic constituency as a
soft constraint, with negative results.
In this paper, we returned to the idea of linguis-
tically motivated soft constraints, and we demon-
strated that they can, in fact, lead to substantial
improvements in translation performance when in-
tegrated into the Hiero framework. We accom-
plished this using constraints that not only dis-
tinguish among constituent types, but which also
distinguish between the benefit of matching the
source parse bracketing, versus the cost of us-
ing phrases that cross relevant bracketing bound-
aries. We demonstrated improvements for Chinese-
English translation, and succeed in obtaining sub-
stantial gains for Arabic-English translation, as well.
Our results contribute to a growing body of work
on combining monolingually based, linguistically
motivated syntactic analysis with translation mod-
els that are closely tied to observable parallel train-
ing data. Consistent with other researchers, we find
that ?syntactic constituency? may be too coarse a no-
tion by itself; rather, there is value in taking a finer-
grained approach, and in allowing the model to de-
cide how far to trust each element of the syntactic
analysis as part of the system?s optimization process.
Acknowledgments
This work was supported in part by DARPA prime
agreement HR0011-06-2-0001. The authors would
like to thank David Chiang and Adam Lopez for
making their source code available; the Stanford
Parser team and Mary Harper for making their
parsers available; David Chiang, Amy Weinberg,
and CLIP Laboratory colleagues, particularly Chris
Dyer, Adam Lopez, and Smaranda Muresan, for dis-
cussion and invaluable assistance.
1010
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the ACL Workshop on
Statistical Machine Translation 2007.
P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?313.
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech.
Report TR-10-98, Comp. Sci. Group, Harvard U.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL-05.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. EMNLP.
S DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proceedings of EMNLP-CoNLL.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL Companion Vol.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. EMNLP 2002.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL-06.
H. Hassan, K. Sima?an, and A. Way. 2007. Integrating
supertags into phrase-based statistical machine trans-
lation. In Proc. ACL-07, pages 288?295.
Zhongqiang Huang, Denis Filimonov, and Mary Harper.
2008. Accuracy enhancements for mandarin parsing.
Tech. report, University of Maryland.
Dan Klein and Christopher D. Manning. 2003a. Accu-
rate unlexicalized parsing. In Proceedings of ACL-03,
pages 423?430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. Advances in Neural Information Pro-
cessing Systems, 15(NIPS 2002):3?10.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proc. EMNLP+CoNLL, pages 868?
876, Prague.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL, pages 127?133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Adam Lopez. (to appear). Statistical machine transla-
tion. ACM Computing Surveys. Earlier version: A
Survey of Statistical Machine Translation. U. of Mary-
land, UMIACS tech. report 2006-47. Apr 2007.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language phrases. In
Proc. EMNLP, pages 44?52.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the ACL, pages 440?447. GIZA++.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167.
K. Owczarzak, B. Mellebeek, D. Groves, J. Van Gen-
abith, and A. Way. 2006. Wrapper syntax for
example-based machine translation. In Proceedings
of the 7th Conference of the Association for Machine
Translation in the Americas, pages 148?155.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpusbased
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the Human Language Technology Confer-
ence (ACL?2002), pages 124?127, San Diego, CA.
Stefan Riezler and John Maxwell. 2006. Grammatical
machine translation. In Proc. HLT-NAACL, New York,
NY.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 712?719.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Chao Wang, Michael Collins, and Phillip Koehn. 2007a.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007b. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proc. EMNLP+CoNLL 2007.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the SMT Workshop, HLT-NAACL.
1011
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 145?149,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The University of Maryland Statistical Machine Translation System for
the Fourth Workshop on Machine Translation
Chris Dyer??, Hendra Setiawan?, Yuval Marton??, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park, MD 20742, USA
{redpony,hendra,ymarton,resnik} AT umd.edu
Abstract
This paper describes the techniques we
explored to improve the translation of
news text in the German-English and
Hungarian-English tracks of the WMT09
shared translation task. Beginning with a
convention hierarchical phrase-based sys-
tem, we found benefits for using word seg-
mentation lattices as input, explicit gen-
eration of beginning and end of sentence
markers, minimum Bayes risk decoding,
and incorporation of a feature scoring the
alignment of function words in the hy-
pothesized translation. We also explored
the use of monolingual paraphrases to im-
prove coverage, as well as co-training to
improve the quality of the segmentation
lattices used, but these did not lead to im-
provements.
1 Introduction
For the shared translation task of the Fourth Work-
shop on Machine Translation (WMT09), we fo-
cused on two tasks: German to English and Hun-
garian to English translation. Despite belonging to
different language families, German and Hungar-
ian have three features in common that complicate
translation into English:
1. productive compounding (especially of
nouns),
2. rich inflectional morphology,
3. widespread mid- to long-range word order
differences with respect to English.
Since these phenomena are poorly addressed with
conventional approaches to statistical machine
translation, we chose to work primarily toward
mitigating their negative effects when construct-
ing our systems. This paper is structured as fol-
lows. In Section 2 we describe the baseline model,
Section 3 describes the various strategies we em-
ployed to address the challenges just listed, and
Section 4 summarizes the final translation system.
2 Baseline system
Our translation system makes use of a hierarchical
phrase-based translation model (Chiang, 2007),
which we argue is a strong baseline for these
language pairs. First, such a system makes use
of lexical information when modeling reorder-
ing (Lopez, 2008), which has previously been
shown to be useful in German-to-English trans-
lation (Koehn et al, 2008). Additionally, since
the decoder is based on a CKY parser, it can con-
sider all licensed reorderings of the input in poly-
nomial time, and German and Hungarian may re-
quire quite substantial reordering. Although such
decoders and models have been common for sev-
eral years, there have been no published results for
these language pairs.
The baseline system translates lowercased and
tokenized source sentences into lowercased target
sentences. The features used were the rule transla-
tion relative frequency P (e?|f?), the ?lexical? trans-
lation probabilities Plex(e?|f?) and Plex(f? |e?), a rule
count, a target language word count, the target
(English) language model P (eI1), and a ?pass-
through? penalty for passing a source language
word to the target side.1 The rule feature values
were computed online during decoding using the
suffix array method described by Lopez (2007).
1The ?pass-through? penalty was necessary since the En-
glish language modeling data contained a large amount of
source-language text.
145
2.1 Training and development data
To construct the translation suffix arrays used to
compute the translation grammar, we used the par-
allel training data provided. The preprocessed
training data was filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) in both directions and sym-
metrized using the grow-diag-final-and
heuristic. We trained a 5-gram language model
from the provided English monolingual training
data and the non-Europarl portions of the parallel
training data using modified Kneser-Ney smooth-
ing as implemented in the SRI language modeling
toolkit (Kneser and Ney, 1995; Stolcke, 2002). We
divided the 2008 workshop ?news test? sets into
two halves of approximately 1000 sentences each
and designated one the dev set and the other the
dev-test set.
2.2 Automatic evaluation metric
Since the official evaluation criterion for WMT09
is human sentence ranking, we chose to minimize
a linear combination of two common evaluation
metrics, BLEU and TER (Papineni et al, 2002;
Snover et al, 2006), during system development
and tuning:
TER ? BLEU
2
Although we are not aware of any work demon-
strating that this combination of metrics correlates
better than either individually in sentence ranking,
Yaser Al-Onaizan (personal communication) re-
ports that it correlates well with the human evalua-
tion metric HTER. In this paper, we report uncased
TER and BLEU individually.
2.3 Forest minimum error training
To tune the feature weights of our system, we used
a variant of the minimum error training algorithm
(Och, 2003) that computes the error statistics from
the target sentences from the translation search
space (represented by a packed forest) that are ex-
actly those that are minimally discriminable by
changing the feature weights along a single vector
in the dimensions of the feature space (Macherey
et al, 2008). The loss function we used was the
linear combination of TER and BLEU described in
the previous section.
3 Experimental variations
This section describes the experimental variants
explored.
3.1 Word segmentation lattices
Both German and Hungarian have a large number
of compound words that are created by concate-
nating several morphemes to form a single ortho-
graphic token. To deal with productive compound-
ing, we employ word segmentation lattices, which
are word lattices that encode alternative possible
segmentations of compound words. Doing so en-
ables us to use possibly inaccurate approaches to
guess the segmentation of compound words, al-
lowing the decoder to decide which to use during
translation. This is a further development of our
general source-lattice approach to decoding (Dyer
et al, 2008).
To construct the segmentation lattices, we de-
fine a log-linear model of compound word seg-
mentation inspired by Koehn and Knight (2003),
making use of features including number of mor-
phemes hypothesized, frequency of the segments
as free-standing morphemes in a training corpus,
and letters in each segment. To tune the model
parameters, we selected a set of compound words
from a subset of the German development set,
manually created a linguistically plausible seg-
mentation of these words, and used this to select
the parameters of the log-linear model using a lat-
tice minimum error training algorithm to minimize
WER (Macherey et al, 2008). We reused the same
features and weights to create the Hungarian lat-
tices. For the test data, we created a lattice of ev-
ery possible segmentation of any word 6 charac-
ters or longer and used forward-backward pruning
to prune out low-probability segmentation paths
(Sixtus and Ortmanns, 1999). We then concate-
nated the lattices in each sentence.
Source Condition BLEU TER
German
baseline 20.8 60.7
lattice 21.3 59.9
Hungarian
baseline 11.0 71.1
lattice 12.3 70.4
Table 1: Impact of compound segmentation lat-
tices.
To build the translation model for lattice sys-
tem, we segmented the training data using the one-
best split predicted by the segmentation model,
146
and word aligned this with the English side. This
variant version of the training data was then con-
catenated with the baseline system?s training data.
3.1.1 Co-training of segmentation model
To avoid the necessity of manually creating seg-
mentation examples to train the segmentation
model, we attempted to generate sets of training
examples by selecting the compound splits that
were found along the path chosen by the decoder?s
one-best translation. Unfortunately, the segmen-
tation system generated in this way performed
slightly worse than the one-best baseline and so
we continued to use the parameter settings derived
from the manual segmentation.
3.2 Modeling sentence boundaries
Incorporating an n-gram language model proba-
bility into a CKY-based decoder is challenging.
When a partial hypothesis (also called an ?item?)
has been completed, it has not yet been determined
what strings will eventually occur to the left of
its first word, meaning that the exact computation
must deferred, which makes pruning a challenge.
In typical CKY decoders, the beginning and ends
of the sentence (which often have special charac-
teristics) are not conclusively determined until the
whole sentence has been translated and the proba-
bilities for the beginning and end sentence proba-
bilities can be added. However, by this point it is
often the case that a possibly better sentence be-
ginning has been pruned away. To address this,
we explicitly generate beginning and end sentence
markers as part of the translation process, as sug-
gested by Xiong et al (2008). The results of doing
this are shown in Table 2.
Source Condition BLEU TER
German
baseline 21.3 59.9
+boundary 21.6 60.1
Hungarian
baseline 12.3 70.4
+boundary 12.8 70.4
Table 2: Impact of modeling sentence boundaries.
3.3 Source language paraphrases
In order to deal with the sparsity associated with
a rich source language morphology and limited-
size parallel corpora (bitexts), we experimented
with a novel approach to paraphrasing out-of-
vocabulary (OOV) source language phrases in
our Hungarian-English system, using monolingual
contextual similarity rather than phrase-table piv-
oting (Callison-Burch et al, 2006) or monolin-
gual bitexts (Barzilay and McKeown, 2001; Dolan
et al, 2004). Distributional profiles for source
phrases were represented as context vectors over
a sliding window of size 6, with vectors defined
using log-likelihood ratios (cf. Rapp (1999), Dun-
ning (1993)) but using cosine rather than city-
block distance to measure profile similarity.
The 20 distributionally most similar source
phrases were treated as paraphrases, considering
candidate phrases up to a width of 6 tokens and fil-
tering out paraphrase candidates with cosine simi-
larity to the original of less than 0.6. The two most
likely translations for each paraphrase were added
to the grammar in order to provide mappings to
English for OOV Hungarian phrases.
This attempt at monolingually-derived source-
side paraphrasing did not yield improvements over
baseline. Preliminary analysis suggests that the
approach does well at identifying many content
words in translating extracted paraphrases of OOV
phrases (e.g., a kommunista part vezetaje ? ,
leader of the communist party or a ra tervezett?
until the planned to), but at the cost of more fre-
quently omitting target words in the output.
3.4 Dominance feature
Although our baseline hierarchical system permits
long-range reordering, it lacks a mechanism to
identify the most appropriate reordering for a spe-
cific sentence translation. For example, when the
most appropriate reordering is a long-range one,
our baseline system often also has to consider
shorter-range reorderings as well. In the worst
case, a shorter-range reordering has a high proba-
bility, causing the wrong reordering to be chosen.
Our baseline system lacks the capacity to address
such cases because all the features it employs are
independent of the phrases being moved; these are
modeled only as an unlexicalized generic nonter-
minal symbol.
To address this challenge, we included what we
call a dominance feature in the scoring of hypothe-
sis translations. Briefly, the premise of this feature
is that the function words in the sentence hold the
key reordering information, and therefore function
words are used to model the phrases being moved.
The feature assesses the quality of a reordering by
looking at the phrase alignment between pairs of
147
function words. In our experiments, we treated
the 128 most frequent words in the corpus as func-
tion words, similar to Setiawan et al (2007). Due
to space constraints, we will discuss the details in
another publication. As Table 3 reports, the use of
this feature yields positive results.
Source Condition BLEU TER
German
baseline 21.6 60.1
+dom 22.2 59.8
Hungarian
baseline 12.8 70.4
+dom 12.6 70.0
Table 3: Impact of alignment dominance feature.
3.5 Minimum Bayes risk decoding
Although during minimum error training we as-
sume a decoder that uses the maximum derivation
decision rule, we find benefits to translating using
a minimum risk decision rule on a test set (Kumar
and Byrne, 2004). This seeks the translation E of
the input lattice F that has the least expected loss,
measured by some loss function L:
E? = arg min
E?
EP (E|F)[L(E,E
?)] (1)
= arg min
E?
?
E
P (E|F)L(E,E?) (2)
We approximate the posterior distribution
P (E|F) and the set of possible candidate transla-
tions using the unique 500-best translations of a
source lattice F . If H(E,F) is the decoder?s path
weight, this is:
P (E|F) ? exp?H(E,F)
The optimal value for the free parameter ?must
be experimentally determined and depends on the
ranges of the feature functions and weights used in
the model, as well as the amount and kind of prun-
ing using during decoding.2 For our submission,
we used ? = 1. Since our goal is to minimize
TER?BLEU
2 we used this as the loss function in (2).
Table 4 shows the results on the dev-test set for
MBR decoding.
2If the free parameter ? lies in (1,?) the distribution is
sharpened, if it lies in [0, 1), the distribution is flattened.
Source Decoder BLEU TER
German
Max-D 22.2 59.8
MBR 22.6 59.4
Hungarian
Max-D 12.6 70.0
MBR 12.8 69.8
Table 4: Performance of maximum derivation vs.
MBR decoders.
4 Conclusion
Table 5 summarizes the impact on the dev-test set
of all features included in the University of Mary-
land system submission.
Condition
German Hungarian
BLEU TER BLEU TER
baseline 20.8 60.7 11.0 71.1
+lattices 21.3 59.9 12.3 70.4
+boundary 21.6 60.1 12.8 70.4
+dom 22.2 59.8 12.6 70.0
+MBR 22.6 59.4 12.8 69.8
Table 5: Summary of all features
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001, and the Army Research Laboratory.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors. Discussions with Chris Callison-Burch
were helpful in carrying out the monolingual para-
phrase work.
References
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In In
Proceedings of ACL-2001.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings NAACL-
2006.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
148
exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics of the Association for
Computational Linguistics, Geneva, Switzerland.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT. Association for Compu-
tational Linguistics, June.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
P. Koehn and K. Knight. 2003. Empirical methods
for compound splitting. In Proceedings of the EACL
2003.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In ACL Work-
shop on Statistical Machine Translation.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 976?
985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
Manchester, UK.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proceedings of EMNLP, Honolulu, HI.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the ACL, pages 311?318.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
Conference of the Association for Computational
Linguistics., pages 519?525.
Hendra Setiawan, Min-Yen Kan, and Haizhao Li.
2007. Ordering phrases with function words. In
Proceedings of ACL.
S. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Pro-
ceedings of ICASSP, Phoenix, AZ.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of Association for Machine
Translation in the Americas.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Intl. Conf. on Spoken Language
Processing.
Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun
Liu, and Shouxun Lin. 2008. Refinements in BTG-
based statistical machine translation. In Proceed-
ings of IJCNLP 2008.
149
Dependency Parsing of Modern Standard
Arabic with Lexical and Inflectional Features
Yuval Marton?
Nuance Communications
Nizar Habash??
Center for Computational Learning
Systems, Columbia University
Owen Rambow?
Center for Computational Learning
Systems, Columbia University
We explore the contribution of lexical and inflectional morphology features to dependency
parsing of Arabic, a morphologically rich language with complex agreement patterns. Using con-
trolled experiments, we contrast the contribution of different part-of-speech (POS) tag sets and
morphological features in two input conditions: machine-predicted condition (in which POS tags
and morphological feature values are automatically assigned), and gold condition (in which their
true values are known). We find that more informative (fine-grained) tag sets are useful in the
gold condition, but may be detrimental in the predicted condition, where they are outperformed
by simpler but more accurately predicted tag sets. We identify a set of features (definiteness,
person, number, gender, and undiacritized lemma) that improve parsing quality in the predicted
condition, whereas other features are more useful in gold. We are the first to show that functional
features for gender and number (e.g., ?broken plurals?), and optionally the related rationality
(?humanness?) feature, are more helpful for parsing than form-based gender and number. We
finally show that parsing quality in the predicted condition can dramatically improve by training
in a combined gold+predicted condition. We experimented with two transition-based parsers,
MaltParser and Easy-First Parser. Our findings are robust across parsers, models, and input
conditions. This suggests that the contribution of the linguistic knowledge in the tag sets and
features we identified goes beyond particular experimental settings, and may be informative for
other parsers and morphologically rich languages.
1. Introduction
For Arabic?as for other morphologically rich languages?the role of morphology is
often expected to be essential in syntactic modeling, and the role of word order is less
important than in morphologically poorer languages such as English. Morphology
? Nuance Communications, 505 First Ave. S, Suite 700, Seattle, WA 98104. E-mail: yuvalmarton@gmail.com.
?? Center for Computational Learning, Columbia University. E-mail: habash@ccls.columbia.edu.
? Center for Computational Learning, Columbia University. E-mail: rambow@ccls.columbia.edu.
Submission received: October 1, 2011; revised submission received: June 16, 2012; accepted for publication:
August 3, 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
interacts with syntax in two ways: agreement and assignment. In agreement, there is
coordination between the morphological features of two words in a sentence based
on their syntactic configuration (e.g., subject?verb or noun?adjective agreement in
GENDER and/or NUMBER). In assignment, specific morphological feature values are
assigned in certain syntactic configurations (e.g., CASE assignment for the subject or
direct object of a verb).1
Parsing model design aims to come up with features that best help parsers learn
the syntax and choose among different parses. The choice of optimal linguistic features
depends on three factors: relevance, redundancy, and accuracy. A feature has relevance
if it is useful in making an attachment (or labeling) decision. A particular feature may
or may not be relevant to parsing. For example, the GENDER feature may help parse
the Arabic phrase 



	

 /



	





 



bAb AlsyAr Aljdyd/Aljdyd (?door the-car
the-newmasc.sg/fem.sg [lit.]),
2 using syntactic agreement: if the-new is masculine (



	

),
it should attach to the masculine door, resulting in the meaning ?the car?s new door?;
if the-new is feminine ( 



	

), it should attach to the feminine the-car, resulting in ?the
door of the new car.? Conversely, the ASPECT feature does not constrain any syntactic
decision. Even if relevant, a feature may not necessarily contribute to optimal perfor-
mance because it may be redundant with other features that surpass it in relevance. For
example, as we will see, the DET and STATE features alone both help parsing because
they help identify the idafa construction, but they are redundant with each other and the
DET feature is more helpful because it also helps with adjectival modification of nouns.
Finally, the accuracy of automatically predicting the feature values (ratio of correct
predictions out of all predictions) of course affects the value of a feature on unseen text.
Even if relevant and non-redundent, a feature may be hard to predict with sufficient
accuracy by current technology, in which case it will be of little or no help for parsing,
even if helpful when its gold values are provided. As we will see, the CASE feature is
very relevant and not redundant, but it cannot be predicted with high accuracy and
overall it is not useful.
Different languages vary with respect to which features may be most helpful given
various tradeoffs among these three factors. In the past, it has been shown that if we
can recognize the relevant morphological features in assignment configurations well
enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech
improves Czech parsing (Collins et al 1999): CASE is relevant, not redundant, and can
be predicted with sufficient accuracy. It has been more difficult showing that agreement
morphology helps parsing, however, with negative results for dependency parsing in
several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin
2008; Nivre 2009).
In this article we investigate morphological features for dependency parsing of
Modern Standard Arabic (MSA). For MSA, the space of possible morphological features
is fairly large. We determine which morphological features help and why. We further
determine the upper bound for their contribution to parsing quality. Similar to previous
1 Other morphological features, such as MOOD or ASPECT, do not interact with syntax at all. Note also that
we do not commit to a specific linguistic theory with these terms; hence, other theoretical terms such as
the Minimalist feature checking may be used here just as well.
2 All Arabic transliterations are presented in the HSB transliteration scheme (Habash, Soudi, and
Buckwalter 2007): (alphabetically) Abt?jHxd?rzs?SDTD???fqklmnhwy and the additional symbols: ? , ?

,
A? 

, A?

, w? 
, y? 
,  , ? , a , u , i 

, ? Proceedings of the ACL 2010 Conference Short Papers, pages 178?183,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improving Arabic-to-English Statistical Machine Translation
by Reordering Post-verbal Subjects for Alignment
Marine Carpuat Yuval Marton Nizar Habash
Columbia University
Center for Computational Learning Systems
475 Riverside Drive, New York, NY 10115
{marine,ymarton,habash}@ccls.columbia.edu
Abstract
We study the challenges raised by Ara-
bic verb and subject detection and re-
ordering in Statistical Machine Transla-
tion (SMT). We show that post-verbal sub-
ject (VS) constructions are hard to trans-
late because they have highly ambiguous
reordering patterns when translated to En-
glish. In addition, implementing reorder-
ing is difficult because the boundaries of
VS constructions are hard to detect accu-
rately, even with a state-of-the-art Arabic
dependency parser. We therefore propose
to reorder VS constructions into SV or-
der for SMT word alignment only. This
strategy significantly improves BLEU and
TER scores, even on a strong large-scale
baseline and despite noisy parses.
1 Introduction
Modern Standard Arabic (MSA) is a morpho-
syntactically complex language, with different
phenomena from English, a fact that raises many
interesting issues for natural language processing
and Arabic-to-English statistical machine transla-
tion (SMT). While comprehensive Arabic prepro-
cessing schemes have been widely adopted for
handling Arabic morphology in SMT (e.g., Sa-
dat and Habash (2006), Zollmann et al (2006),
Lee (2004)), syntactic issues have not received
as much attention by comparison (Green et
al. (2009), Crego and Habash (2008), Habash
(2007)). Arabic verbal constructions are par-
ticularly challenging since subjects can occur in
pre-verbal (SV), post-verbal (VS) or pro-dropped
(?null subject?) constructions. As a result, training
data for learning verbal construction translations
is split between the different constructions and
their patterns; and complex reordering schemas
are needed in order to translate them into primarily
pre-verbal subject languages (SVO) such as En-
glish.
These issues are particularly problematic in
phrase-based SMT (Koehn et al, 2003). Standard
phrase-based SMT systems memorize phrasal
translation of verb and subject constructions as ob-
served in the training bitext. They do not cap-
ture any generalizations between occurrences in
VS and SV orders, even for the same verbs. In
addition, their distance-based reordering models
are not well suited to handling complex reorder-
ing operations which can include long distance
dependencies, and may vary by context. Despite
these limitations, phrase-based SMT systems have
achieved competitive results in Arabic-to-English
benchmark evaluations.1 However, error analysis
shows that verbs are still often dropped or incor-
rectly translated, and subjects are split or garbled
in translation. This suggests that better syntactic
modeling should further improve SMT.
We attempt to get a better understanding of
translation patterns for Arabic verb constructions,
particularly VS constructions, by studying their
occurrence and reordering patterns in a hand-
aligned Arabic-English parallel treebank. Our
analysis shows that VS reordering rules are not
straightforward and that SMT should therefore
benefit from direct modeling of Arabic verb sub-
ject translation. In order to detect VS construc-
tions, we use our state-of-the-art Arabic depen-
dency parser, which is essentially the CATIBEX
baseline in our subsequent parsing work in Mar-
ton et al (2010), and is further described there. We
show that VS subjects and their exact boundaries
are hard to identify accurately. Given the noise
in VS detection, existing strategies for source-side
reordering (e.g., Xia and McCord (2004), Collins
et al (2005), Wang et al (2007)) or using de-
1http://www.itl.nist.gov/iad/
mig/tests/mt/2009/ResultsRelease/
currentArabic.html
178
Table 1: How are Arabic SV and VS translated in
the manually word-aligned Arabic-English paral-
lel treebank? We check whether V and S are trans-
lated in a ?monotone? or ?inverted? order for all
VS and SV constructions. ?Overlap? represents
instances where translations of the Arabic verb
and subject have some English words in common,
and are not monotone nor inverted.
gold reordering all verbs %
SV monotone 2588 98.2
SV inverted 15 0.5
SV overlap 35 1.3
SV total 2638 100
VS monotone 1700 27.3
VS inverted 4033 64.7
VS overlap 502 8.0
VS total 6235 100
pendency parses as cohesion constraints in decod-
ing (e.g., Cherry (2008); Bach et al (2009)) are
not effective at this stage. While these approaches
have been successful for language pairs such as
German-English for which syntactic parsers are
more developed and relevant reordering patterns
might be less ambiguous, their impact potential on
Arabic-English translation is still unclear.
In this work, we focus on VS constructions
only, and propose a new strategy in order to bene-
fit from their noisy detection: for the word align-
ment stage only, we reorder phrases detected as
VS constructions into an SV order. Then, for
phrase extraction, weight optimization and decod-
ing, we use the original (non-reordered) text. This
approach significantly improves both BLEU and
TER on top of strong medium and large-scale
phrase-based SMT baselines.
2 VS reordering in gold Arabic-English
translation
We use the manually word-aligned parallel
Arabic-English Treebank (LDC2009E82) to study
how Arabic VS constructions are translated into
English by humans. Given the gold Arabic syn-
tactic parses and the manual Arabic-English word
alignments, we can determine the gold reorder-
ings for SV and VS constructions. We extract VS
representations from the gold constituent parses
by deterministic conversion to a simplified depen-
dency structure, CATiB (Habash and Roth, 2009)
(see Section 3). We then check whether the En-
glish translations of the Arabic verb and the Ara-
bic subject occur in the same order as in Arabic
(monotone) or not (inverted). Table 1 summa-
rizes the reordering patterns for each category. As
expected, 98% of Arabic SV are translated in a
monotone order in English. For VS constructions,
the picture is surprisingly more complex. The
monotone VS translations are mostly explained
by changes to passive voice or to non-verbal con-
structions (such as nominalization) in the English
translation.
In addition, Table 1 shows that verb subjects oc-
cur more frequently in VS order (70%) than in SV
order (30%). These numbers do not include pro-
dropped (?null subject?) constructions.
3 Arabic VS construction detection
Even if the SMT system had perfect knowledge
of VS reordering, it has to accurately detect VS
constructions and their spans in order to apply
the reordering correctly. For that purpose, we
use our state-of-ther-art parsing model, which is
essentially the CATIBEX baseline model in Mar-
ton et al (2010), and whose details we summa-
rize next. We train a syntactic dependency parser,
MaltParser v1.3 with the Nivre ?eager? algorithm
(Nivre, 2003; Nivre et al, 2006; Nivre, 2008) on
the training portion of the Penn Arabic Treebank
part 3 v3.1, hereafter PATB3 (Maamouri et al,
2008; Maamouri et al, 2009). The training / de-
velopment split is the same as in Zitouni et al
(2006). We convert the PATB3 representation into
the succinct CATiB format, with 8 dependency
relations and 6 POS tags, which we then extend
to a set of 44 tags using regular expressions of
the basic POS and the normalized surface word
form, similarly to Marton et al (2010), following
Habash and Roth (2009). We normalize Alif Maq-
sura to Ya, and Hamzated Alifs to bare Alif, as is
commonly done in Arabic SMT.
For analysis purposes, we evaluate our subject
and verb detection on the development part of
PATB3 using gold POS tags. There are various
ways to go about it. We argue that combined de-
tection statistics of constructions of verbs and their
subjects (VATS), for which we achieve an F-score
of 74%, are more telling for the task at hand.2
2We divert from the CATiB representation in that a non-
matrix subject of a pseudo verb (An and her sisters) is treated
as a subject of the verb that is under the same pseudo verb.
This treatment of said subjects is comparable to the PATB?s.
179
These scores take into account the spans of both
the subject and the specific verb it belongs to, and
potentially reorder with. We also provide statistics
of VS detection separately (F-score 63%), since
we only handle VS here. This low score can be
explained by the difficulty in detecting the post-
verbal subject?s end boundary, and the correct verb
the subject belongs to. The SV construction scores
are higher, presumably since the pre-verbal sub-
ject?s end is bounded by the verb it belongs to. See
Table 2.
Although not directly comparable, our VS
scores are similar to those of Green et al (2009).
Their VS detection technique with conditional
random fields (CRF) is different from ours in by-
passing full syntactic parsing, and in only detect-
ing maximal (non-nested) subjects of verb-initial
clauses. Additionally, they use a different train-
ing / test split of the PATB data (parts 1, 2 and 3).
They report 65.9% precision and 61.3% F-score.
Note that a closer score comparison should take
into account their reported verb detection accuracy
of 98.1%.
Table 2: Precision, Recall and F-scores for con-
structions of Arabic verbs and their subjects, eval-
uated on our development part of PATB3.
construction P R F
VATS (verbs & their subj.) 73.84 74.37 74.11
VS 66.62 59.41 62.81
SV 86.75 61.07 71.68
VNS (verbs w/ null subj.) 76.32 92.04 83.45
verbal subj. exc. null subj. 72.46 60.18 65.75
verbal subj. inc. null subj. 73.97 74.50 74.23
verbs with non-null subj. 91.94 76.17 83.31
SV or VS 72.19 59.95 65.50
4 Reordering Arabic VS for SMT word
alignment
Based on these analyses, we propose a new
method to help phrase-based SMT systems deal
with Arabic-English word order differences due to
VS constructions. As in related work on syntactic
reordering by preprocessing, our method attempts
to make Arabic and English word order closer to
each other by reordering Arabic VS constructions
into SV. However, unlike in previous work, the re-
ordered Arabic sentences are used only for word
alignment. Phrase translation extraction and de-
coding are performed on the original Arabic word
order. Preliminary experiments on an earlier ver-
sion of the large-scale SMT system described in
Section 6 showed that forcing reordering of all
VS constructions at training and test time does
not have a consistent impact on translation qual-
ity: for instance, on the NIST MT08-NW test set,
TER slightly improved from 44.34 to 44.03, while
BLEU score decreased from 49.21 to 49.09.
Limiting reordering to alignment allows the sys-
tem to be more robust and recover from incorrect
changes introduced either by incorrect VS detec-
tion, or by incorrect reordering of a correctly de-
tected VS. Given a parallel sentence (a, e), we
proceed as follows:
1. automatically tag VS constructions in a
2. generate new sentence a? = reorder(a) by
reordering Arabic VS into SV
3. get word alignment wa? on new sentence pair
(a?, e)
4. using mapping from a to a?, get correspond-
ing word alignment wa = unreorder(wa?)
for the original sentence pair (a, e)
5 Experiment set-up
We use the open-source Moses toolkit (Koehn et
al., 2007) to build two phrase-based SMT systems
trained on two different data conditions:
? medium-scale the bitext consists of 12M
words on the Arabic side (LDC2007E103).
The language model is trained on the English
side of the large bitext.
? large-scale the bitext consists of several
newswire LDC corpora, and has 64M words
on the Arabic side. The language model is
trained on the English side of the bitext aug-
mented with Gigaword data.
Except from this difference in training data, the
two systems are identical. They use a standard
phrase-based architecture. The parallel corpus is
word-aligned using the GIZA++ (Och and Ney,
2003), which sequentially learns word alignments
for the IBM1, HMM, IBM3 and IBM4 models.
The resulting alignments in both translation di-
rections are intersected and augmented using the
grow-diag-final-and heuristic (Koehn et al, 2007).
Phrase translations of up to 10 words are extracted
in the Moses phrase-table. We apply statistical
significance tests to prune unreliable phrase-pairs
180
and score remaining phrase-table entries (Chen et
al., 2009). We use a 5-gram language model with
modified Kneser-Ney smoothing. Feature weights
are tuned to maximize BLEU on the NIST MT06
test set.
For all systems, the English data is tokenized
using simple punctuation-based rules. The Arabic
side is segmented according to the Arabic Tree-
bank (PATB3) tokenization scheme (Maamouri et
al., 2009) using the MADA+TOKAN morpholog-
ical analyzer and tokenizer (Habash and Rambow,
2005). MADA-produced Arabic lemmas are used
for word alignment.
6 Results
We evaluate translation quality using both BLEU
(Papineni et al, 2002) and TER (Snover et al,
2006) scores on three standard evaluation test
sets from the NIST evaluations, which yield more
than 4400 test sentences with 4 reference transla-
tions. On this large data set, our VS reordering
method remarkably yields statistically significant
improvements in BLEU and TER on the medium
and large SMT systems at the 99% confidence
level (Table 3).
Results per test set are reported in Table 4. TER
scores are improved in all 10 test configurations,
and BLEU scores are improved in 8 out of the 10
configurations. Results on the MT08 test set show
that improvements are obtained both on newswire
and on web text as measured by TER (but not
BLEU score on the web section.) It is worth noting
that consistent improvements are obtained even on
the large-scale system, and that both baselines are
full-fledged systems, which include lexicalized re-
ordering and large 5-gram language models.
Analysis shows that our VS reordering tech-
nique improves word alignment coverage (yield-
ing 48k and 330k additional links on the medium
and large scale systems respectively). This results
in larger phrase-tables which improve translation
quality.
7 Related work
To the best of our knowledge, the only other ap-
proach to detecting and using Arabic verb-subject
constructions for SMT is that of Green et al
(2009) (see Section 3), which failed to improve
Arabic-English SMT. In contrast with our reorder-
ing approach, they integrate subject span informa-
tion as a log-linear model feature which encour-
Table 3: Evaluation on all test sets: on the total
of 4432 test sentences, improvements are statisti-
cally significant at the 99% level using bootstrap
resampling (Koehn, 2004)
system BLEU r4n4 (%) TER (%)
medium baseline 44.35 48.34
+ VS reordering 44.65 (+0.30) 47.78 (-0.56)
large baseline 51.45 42.45
+ VS reordering 51.70 (+0.25) 42.21 (-0.24)
ages a phrase-based SMT decoder to use phrasal
translations that do not break subject boundaries.
Syntactically motivated reordering for phrase-
based SMT has been more successful on language
pairs other than Arabic-English, perhaps due to
more accurate parsers and less ambiguous reorder-
ing patterns than for Arabic VS. For instance,
Collins et al (2005) apply six manually defined
transformations to German parse trees which im-
prove German-English translation by 0.4 BLEU
on the Europarl task. Xia and McCord (2004)
learn reordering rules for French to English trans-
lations, which arguably presents less syntactic dis-
tortion than Arabic-English. Zhang et al (2007)
limit reordering to decoding for Chinese-English
SMT using a lattice representation. Cherry (2008)
uses dependency parses as cohesion constraints in
decoding for French-English SMT.
For Arabic-English phrase-based SMT, the im-
pact of syntactic reordering as preprocessing is
less clear. Habash (2007) proposes to learn syntac-
tic reordering rules targeting Arabic-English word
order differences and integrates them as deter-
ministic preprocessing. He reports improvements
in BLEU compared to phrase-based SMT limited
to monotonic decoding, but these improvements
do not hold with distortion. Instead of apply-
ing reordering rules deterministically, Crego and
Habash (2008) use a lattice input to represent alter-
nate word orders which improves a ngram-based
SMT system. But they do not model VS construc-
tions explicitly.
Most previous syntax-aware word alignment
models were specifically designed for syntax-
based SMT systems. These models are often
bootstrapped from existing word alignments, and
could therefore benefit from our VS reordering ap-
proach. For instance, Fossum et al (2008) report
improvements ranging from 0.1 to 0.5 BLEU on
Arabic translation by learning to delete alignment
181
Table 4: VS reordering improves BLEU and TER scores in almost all test conditions on 5 test sets, 2
metrics, and 2 MT systems
BLEU r4n4 (%)
test set MT03 MT04 MT05 MT08nw MT08wb
medium baseline 45.95 44.94 48.05 44.86 32.05
+ VS reordering 46.33 (+0.38) 45.03 (+0.09) 48.69 (+0.64) 45.06 (+0.20) 31.96 (-0.09)
large baseline 52.3 52.45 54.66 52.60 39.22
+ VS reordering 52.63 (+0.33) 52.34 (-0.11) 55.29 (+0.63) 52.85 (+0.25) 39.87 (+0.65)
TER (%)
test set MT03 MT04 MT05 MT08nw MT08wb
medium baseline 48.77 46.45 45.00 47.74 58.02
+ VS reordering 48.31 (-0.46) 46.10 (-0.35) 44.29 (-0.71) 47.11 (-0.63) 57.30 (-0.72)
large baseline 43.33 40.42 39.15 41.81 52.05
+ VS reordering 42.95 (-0.38) 40.40 (-0.02) 38.75 (-0.40) 41.51 (-0.30) 51.86 (-0.19)
links if they degrade their syntax-based translation
system. Departing from commonly-used align-
ment models, Hermjakob (2009) aligns Arabic and
English content words using pointwise mutual in-
formation, and in this process indirectly uses En-
glish sentences reordered into VS order to collect
cooccurrence counts. The approach outperforms
GIZA++ on a small-scale translation task, but the
impact of reordering alone is not evaluated.
8 Conclusion and future work
We presented a novel method for improving over-
all SMT quality using a noisy syntactic parser: we
use these parses to reorder VS constructions into
SV for word alignment only. This approach in-
creases word alignment coverage and significantly
improves BLEU and TER scores on two strong
SMT baselines.
In subsequent work, we show that matrix (main-
clause) VS constructions are reordered much more
frequently than non-matrix VS, and that limit-
ing reordering to matrix VS constructions for
word alignment further improves translation qual-
ity (Carpuat et al, 2010). In the future, we plan to
improve robustness to parsing errors by using not
just one, but multiple subject boundary hypothe-
ses. We will also investigate the integration of VS
reordering in SMT decoding.
Acknowledgements
The authors would like to thank Mona Diab, Owen Ram-
bow, Ryan Roth, Kristen Parton and Joakim Nivre for help-
ful discussions and assistance. This material is based upon
work supported by the Defense Advanced Research Projects
Agency (DARPA) under GALE Contract No HR0011-08-C-
0110. Any opinions, findings and conclusions or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
References
Nguyen Bach, Stephan Vogel, and Colin Cherry. 2009. Co-
hesive constraints in a beam search phrase-based decoder.
In Proceedings of the 10th Meeting of the North American
Chapter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 1?4.
Marine Carpuat, Yuval Marton, and Nizar Habash. 2010. Re-
ordering matrix post-verbal subjects for arabic-to-english
smt. In Proceedings of the Conference Traitement Au-
tomatique des Langues Naturelles (TALN).
Boxing Chen, George Foster, and Roland Kuhn. 2009.
Phrase translation model enhanced with association based
features. In Proceedings of MT-Summit XII, Ottawa, On-
tario, September.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of the 46th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 72?80, Columbus, Ohio, June.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
Clause restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 531?540,
Ann Arbor, MI, June.
Josep M. Crego and Nizar Habash. 2008. Using shallow syn-
tax information to improve word alignment and reordering
for SMT. In Proceedings of the Third Workshop on Statis-
tical Machine Translation, pages 53?61, June.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of the
Third Workshop on Statistical Machine Translation, pages
44?52.
Spence Green, Conal Sathi, and Christopher D. Manning.
2009. NP subject detection in verb-initial Arabic clauses.
182
In Proceedings of the Third Workshop on Computational
Approaches to Arabic Script-based Languages (CAASL3).
Nizar Habash and Owen Rambow. 2005. Arabic Tokeniza-
tion, Part-of-Speech Tagging and Morphological Disam-
biguation in One Fell Swoop. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 573?580, Ann Arbor, Michigan,
June.
Nizar Habash and Ryan Roth. 2009. CATiB: The Columbia
Arabic treebank. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 221?224, Suntec, Singa-
pore, August. Association for Computational Linguistics.
Nizar Habash. 2007. Syntactic preprocessing for statisti-
cal machine translation. In Proceedings of the Machine
Translation Summit (MT-Summit), Copenhagen.
Ulf Hermjakob. 2009. Improved word alignment with statis-
tics and linguistic heuristics. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language
Processing, pages 229?237, Singapore, August.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL-2003, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Annual Meeting of the
Association for Computational Linguistics (ACL), demon-
stration session, Prague, Czech Republic, June.
Philipp Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the 2004
Conference on Empirical Methods in Natural Language
Processing (EMNLP-2004), Barcelona, Spain, July.
Young-Suk Lee. 2004. Morphological analysis for statistical
machine translation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages 57?
60, Boston, MA.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhancing the arabic treebank: a collaborative effort to-
ward new annotation guidelines. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The penn arabic treebank part 3 version
3.1. Linguistic Data Consortium LDC2008E22.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010. Im-
proving arabic dependency parsing with lexical and in-
flectional morphological features. In Proceedings of the
11th Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL) workshop
on Statistical Parsing of Morphologically Rich Languages
(SPMRL), Los Angeles.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A Data-Driven Parser-Generator for Dependency
Parsing. In Proceedings of the Conference on Language
Resources and Evaluation (LREC).
Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of the 8th Interna-
tional Conference on Parsing Technologies (IWPT), pages
149?160, Nancy, France.
Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguistics,
34(4).
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?52.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguistics.
Fatiha Sadat and Nizar Habash. 2006. Combination of arabic
preprocessing schemes for statistical machine translation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages 1?8,
Morristown, NJ, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of AMTA, pages 223?231, Boston, MA.
Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chi-
nese syntactic reordering for statistical machine transla-
tion. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a statistical
mt system with automatically learned rewrite patterns. In
Proceedings of COLING 2004, pages 508?514, Geneva,
Switzerland, August.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Chunk-
level reordering of source language sentences with auto-
matically learned rules for statistical machine translation.
In Human Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics An-
nual Meeting, Rochester, NY, April.
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya. 2006.
Maximum Entropy Based Restoration of Arabic Diacrit-
ics. In Proceedings of COLING-ACL, the joint conference
of the International Committee on Computational Linguis-
tics and the Association for Computational Linguistics,
pages 577?584, Sydney, Australia.
Andreas Zollmann, Ashish Venugopal, and Stephan Vogel.
2006. Bridging the inflection morphology gap for ara-
bic statistical machine translation. In Proceedings of the
Human Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 201?204, New
York City, USA.
183
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1586?1596,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Arabic Dependency Parsing with Form-based and Functional
Morphological Features
Yuval Marton
T.J. Watson Research Center
IBM
yymarton@us.ibm.com
Nizar Habash and Owen Rambow
Center for Computational Learning Systems
Columbia University
{habash,rambow}@ccls.columbia.edu
Abstract
We explore the contribution of morphologi-
cal features ? both lexical and inflectional ?
to dependency parsing of Arabic, a morpho-
logically rich language. Using controlled ex-
periments, we find that definiteness, person,
number, gender, and the undiacritzed lemma
are most helpful for parsing on automatically
tagged input. We further contrast the contri-
bution of form-based and functional features,
and show that functional gender and number
(e.g., ?broken plurals?) and the related ratio-
nality feature improve over form-based fea-
tures. It is the first time functional morpho-
logical features are used for Arabic NLP.
1 Introduction
Parsers need to learn the syntax of the modeled lan-
guage in order to project structure on newly seen
sentences. Parsing model design aims to come up
with features that best help parsers to learn the syn-
tax and choose among different parses. One as-
pect of syntax, which is often not explicitly mod-
eled in parsing, involves morphological constraints
on syntactic structure, such as agreement, which of-
ten plays an important role in morphologically rich
languages. In this paper, we explore the role of
morphological features in parsing Modern Standard
Arabic (MSA). For MSA, the space of possible mor-
phological features is fairly large. We determine
which morphological features help and why. We
also explore going beyond the easily detectable, reg-
ular form-based (?surface?) features, by represent-
ing functional values for some morphological fea-
tures. We expect that representing lexical abstrac-
tions and inflectional features participating in agree-
ment relations would help parsing quality, but other
inflectional features would not help. We further ex-
pect functional features to be superior to surface-
only features.
The paper is structured as follows. We first
present the corpus we use (Section 2), then rele-
vant Arabic linguistic facts (Section 3); we survey
related work (Section 4), describe our experiments
(Section 5), and conclude with an analysis of pars-
ing error types (Section 6).
2 Corpus
We use the Columbia Arabic Treebank (CATiB)
(Habash and Roth, 2009). Specifically, we use
the portion converted automatically from part 3 of
the Penn Arabic Treebank (PATB) (Maamouri et
al., 2004) to the CATiB format, which enriches the
CATiB dependency trees with full PATB morpho-
logical information. CATiB?s dependency represen-
tation is based on traditional Arabic grammar and
emphasizes syntactic case relations. It has a re-
duced POS tagset (with six tags only ? henceforth
CATIB6), but a standard set of eight dependency re-
lations: SBJ and OBJ for subject and (direct or indi-
rect) object, respectively, (whether they appear pre-
or post-verbally); IDF for the idafa (possessive) re-
lation; MOD for most other modifications; and other
less common relations that we will not discuss here.
For more information, see Habash et al (2009). The
CATiB treebank uses the word segmentation of the
PATB: it splits off several categories of orthographic
clitics, but not the definite article ?@ Al. In all of the
experiments reported in this paper, we use the gold
1586
VRB
???

K t?ml
?work?
MOD
PRT
?


	
? fy
?in?
OBJ
NOM
?P@Y?? @ AlmdArs
?the-schools?
MOD
NOM

?J
???m
?'@ AlHkwmy~
?the-governmental?
SBJ
NOM
H@YJ

	
?k HfydAt
?granddaughters?
MOD
NOM
HAJ
?
	
Y?@ Al?kyAt
?smart?
IDF
NOM
I.

KA??@ AlkAtb
?the-writer?
Figure 1: CATiB Annotation example (tree display from right
to left).

?J
???m
?'@ ?P@Y?? @ ?


	
? HAJ
?
	
Y?@ I.

KA??@ H@YJ

	
?k ???

K
t?ml HfydAt AlkAtb Al?kyAt fy AlmdArs AlHkwmy~ ?The
writer?s smart granddaughters work for public schools.?
segmentation. An example CATiB dependency tree
is shown in Figure 1.
3 Relevant Linguistic Concepts
In this section, we present the linguistic concepts rel-
evant to our discussion of Arabic parsing.
Orthography The Arabic script uses optional di-
acritics to represent short vowels, consonantal dou-
bling and the indefininteness morpheme (nunation).
For example, the word I.

J

? kataba ?he wrote? is of-
ten written as I.

J?ktb, which can be ambiguous with
other words such as I.

J

? kutubu? ?books?. In news
text, only around 1.6% of all words have any dia-
critic (Habash, 2010). As expected, the lack of dia-
critics contributes heavily to Arabic?s morphological
ambiguity. In this work, we only use undiacritized
text; however, some of our parsing features which
are derived through morphological disambiguation
include diacritics (specifically, lemmas, see below).
Morphemes Words can be described in terms of
their morphemes; in Arabic, in addition to concate-
native prefixes and suffixes, there are templatic mor-
phemes called root and pattern. For example, the
word 	??J.

KA?K
 yu+kAtib+uwn ?they correspond? has
one prefix and one suffix, in addition to a stem com-
posed of the root H.
H? k-t-b ?writing related? and
the pattern 1A2i3.1
Lexeme and features Alternatively, Arabic
words can be described in terms of lexemes and
inflectional features. The set of word forms that
only vary inflectionally among each other is called
the lexeme. A lemma is a specific word form cho-
sen to represent the lexeme word set; for example,
Arabic verb lemmas are third person masculine sin-
gular perfective. We explore using both the dia-
critized lemma and the undiacritized lemma (here-
after LMM). Just as the lemma abstracts over in-
flectional morphology, the root abstracts over both
inflectional and derivational morphology and thus
provides a deeper level of lexical abstraction, indi-
cating the ?core? meaning of the word. The pat-
tern is a generally complementary abstraction some-
times indicating sematic notions such causation and
reflexiveness. We use the pattern of the lemma, not
of the word form. We group the ROOT, PATTERN,
LEMMA and LMM in our discussion as lexical fea-
tures. Nominal lexemes can also be classified into
two groups: rational (i.e., human) or irrational (i.e.?
non-human).2 The rationality feature interacts with
syntactic agreement and other inflectional features
(discussed next); as such, we group it with those fea-
tures in this paper?s experiments.
The inflectional features define the the space of
variations of the word forms associated with a lex-
eme. PATB-tokenized words vary along nine di-
mensions: GENDER and NUMBER (for nominals and
verbs); PERSON, ASPECT, VOICE and MOOD (for
verbs); and CASE, STATE, and the attached defi-
nite article proclitic DET (for nominals). Inflectional
features abstract away from the specifics of mor-
pheme forms. Some inflectional features affect more
than one morpheme in the same word. For exam-
ple, changing the value of the ASPECT feature in
the example above from imperfective to perfective
yields the word form @?J.

KA? kAtab+uwA ?they corre-
sponded?, which differs in terms of prefix, suffix and
pattern.
1The digits in the pattern correspond to the positions root
radicals are inserted.
2Note that rationality (?human-ness? ??

?A? Q

	
?/?

?A??) is nar-
rower than animacy; its expression is wide-spead in Arabic, but
less so English, where it mainly shows in pronouns (he/she vs.
it) and relativizers (the student who... vs. the desk/bird which...).
1587
Surface vs. functional features Additionally,
some inflectional features, specifically gender and
number, are expressed using different morphemes
in different words (even within the same part-of-
speech). There are four sound gender-number suf-
fixes in Arabic:3 +? (null morpheme) for masculine
singular,

?+ +~ for feminine singular, 	??+ +wn for
masculine plural and H@+ +At for feminine plural.
Plurality can be expressed using sound plural suf-
fixes or using a pattern change together with singu-
lar suffixes. A sound plural example is the word pair
H@YJ

	
?k/

?YJ

	
?k Hafiyd+a~/Hafiyd+At ?granddaugh-
ter/granddaughters?. On the other hand, the plural of
the inflectionally and morphemically feminine sin-
gular word

??PY? madras+a~ ?school? is the word
?P@Y? madAris+? ?schools?, which is feminine and
plural inflectionally, but has a masculine singular
suffix. This irregular inflection, known as broken
plural, is similar to the English mouse/mice, but is
much more common in Arabic (over 50% of plurals
in our training data). A similar inconsistency ap-
pears in feminine nouns that are not inflected using
sound gender suffixes, e.g., the feminine form of the
masculine singular adjective

?P 	P

@ ?zraq+? ?blue? is
ZA

P? 	P zarqA?+? not

?

P? 	P

@* *?zraq+a~. To address
this inconsistency in the correspondence between in-
flectional features and morphemes, and inspired by
(Smr?, 2007), we distinguish between two types of
inflectional features: surface (or form-based)4 fea-
tures and functional features.
Most available Arabic NLP tools and resources
model morphology using surface inflectional fea-
tures and do not mark rationality; this includes the
PATB (Maamouri et al, 2004), the Buckwalter mor-
phological analyzer (BAMA) (Buckwalter, 2004)
and tools using them such as the Morphological
Analysis and Disambiguation for Arabic (MADA)
system (Habash and Rambow, 2005). The Elixir-
FM analyzer (Smr?, 2007) readily provides the func-
tional inflectional number feature, but not full func-
tional gender (only for adjectives and verbs but not
for nouns), nor rationality. Most recently, Alkuhlani
and Habash (2011) present a version of the PATB
(part 3) that is annotated for functional gender, num-
3We ignore duals, which are regular in Arabic, and case/state
variations in this discussion for simplicity.
4Smr? (2007) uses the term illusory for surface features.
ber and rationality features for Arabic. We use this
resource in modeling these features in Section 5.5.
Morpho-syntactic interactions Inflectional fea-
tures and rationality interact with syntax in two
ways. In agreement relations, two words in a spe-
cific syntactic configuration have coordinated values
for specific sets of features. MSA has standard (i.e.,
matching value) agreement for subject-verb pairs on
PERSON, GENDER, and NUMBER, and for noun-
adjective pairs on NUMBER, GENDER, CASE, and
DET. There are three very common cases of excep-
tional agreement: verbs preceding subjects are al-
ways singular, adjectives of irrational plural nouns
are always feminine singular, and verbs whose sub-
jects are irrational plural are also always feminine
singular. See the example in Figure 1: the adjective,
HAJ
?
	
Y?@ Al?kyAt ?smart?, of the feminine plural (and
rational) H@YJ

	
?k HafiydAt ?granddaughters? is fem-
inine plural; but the adjective,

?J
???m
?'@ AlHkwmy~
?the-governmental?, of the feminine plural (and irra-
tional) ?P@Y? madAris ?schools? is feminine singu-
lar. These agreement rules always refer to functional
morphology categories; they are orthogonal to the
morpheme-feature inconsistency discussed above.
MSA exhibits marking relations in CASE and
STATE marking. Different types of dependents
have different CASE, e.g., verbal subjects are al-
ways marked NOMINATIVE. CASE and STATE are
rarely explicitly manifested in undiacritized MSA.
The DET feature plays an important role in distin-
guishing between the N-N idafa (possessive) con-
struction, in which only the last noun may bear the
definite article, and the N-A modifier construction,
in which both elements generally exhibit agreement
in definiteness.
Lexical features do not constrain syntactic struc-
ture as inflectional features do. Instead, bilexical
dependencies are used to model semantic relations
which often are the only way to disambiguate among
different possible syntactic structures. Lexical ab-
straction also reduces data sparseness.
The core POS tagsets Words also have associ-
ated part-of-speech (POS) tags, e.g., ?verb?, which
further abstract over morphologically and syntac-
tically similar lexemes. Traditional Arabic gram-
mars often describe a very general three-way dis-
tinction into verbs, nominals and particles. In com-
1588
parison, the tagset of the Buckwalter Morphologi-
cal Analyzer (Buckwalter, 2004) used in the PATB
has a core POS set of 44 tags (before morphologi-
cal extension). Cross-linguistically, a core set con-
taining around 12 tags is often assumed, including:
noun, proper noun, verb, adjective, adverb, preposi-
tion, particles, connectives, and punctuation. Hence-
forth, we reduce CORE44 to such a tagset, and dub
it CORE12. The CATIB6 tagset can be viewed as
a further reduction, with the exception that CATIB6
contains a passive voice tag; however, this tag con-
stitutes only 0.5% of the tags in the training.
Extended POS tagsets The notion of ?POS
tagset? in natural language processing usually does
not refer to a core set. Instead, the Penn English
Treebank (PTB) uses a set of 46 tags, including
not only the core POS, but also the complete set
of morphological features (this tagset is still fairly
small since English is morphologically impover-
ished). In PATB-tokenized MSA, the corresponding
type of tagset (core POS extended with a complete
description of morphology) would contain upwards
of 2,000 tags, many of which are extremely rare (in
our training corpus of about 300,000 words, we en-
counter only 430 of such POS tags with complete
morphology). Therefore, researchers have proposed
tagsets for MSA whose size is similar to that of the
English PTB tagset, as this has proven to be a useful
size computationally. These tagsets are hybrids in
the sense that they are neither simply the core POS,
nor the complete morphologically enriched tagset,
but instead they selectively enrich the core POS
tagset with only certain morphological features. A
full dicussion of how these tagsets affect parsing is
presented in Marton et al (2010); we summarize the
main points here.
The following are the various tagsets we use in
this paper: (a) the core POS tagset CORE12; (b)
the CATiB treebank tagset CATIBEX, a newly in-
troduced extension of CATIB6 (Habash and Roth,
2009) by simple regular expressions of the word
form, indicating particular morphemes such as the
prefix ?@ Al+ or the suffix 	?? +wn; this tagset
is the best-performing tagset for Arabic on pre-
dicted values. (c) the PATB full tagset (BW), size
?2000+ (Buckwalter, 2004); We only discuss here
the best performing tagsets (on predicted values),
and BW for comparison.
4 Related Work
Much work has been done on the use of morpholog-
ical features for parsing of morphologically rich lan-
guages. Collins et al (1999) report that an optimal
tagset for parsing Czech consists of a basic POS tag
plus a CASE feature (when applicable). This tagset
(size 58) outperforms the basic Czech POS tagset
(size 13) and the complete tagset (size ?3000+).
They also report that the use of gender, number and
person features did not yield any improvements. We
got similar results for CASE in the gold experimen-
tal setting (Marton et al, 2010) but not when using
predicted POS tags (POS tagger output). This may
be a result of CASE tagging having a lower error rate
in Czech (5.0%) (Hajic? and Vidov?-Hladk?, 1998)
compared to Arabic (?14.0%, see Table 2). Simi-
larly, Cowan and Collins (2005) report that the use
of a subset of Spanish morphological features (num-
ber for adjectives, determiners, nouns, pronouns,
and verbs; and mode for verbs) outperforms other
combinations. Our approach is comparable to their
work in terms of its systematic exploration of the
space of morphological features. We also find that
the number feature helps for Arabic. Looking at He-
brew, a Semitic language related to Arabic, Tsarfaty
and Sima?an (2007) report that extending POS and
phrase structure tags with definiteness information
helps unlexicalized PCFG parsing.
As for work on Arabic, results have been reported
on PATB (Kulick et al, 2006; Diab, 2007; Green
and Manning, 2010), the Prague Dependency Tree-
bank (PADT) (Buchholz and Marsi, 2006; Nivre,
2008) and the Columbia Arabic Treebank (CATiB)
(Habash and Roth, 2009). Recently, Green and
Manning (2010) analyzed the PATB for annotation
consistency, and introduced an enhanced split-state
constituency grammar, including labels for short
Idafa constructions and verbal or equational clauses.
Nivre (2008) reports experiments on Arabic pars-
ing using his MaltParser (Nivre et al, 2007), trained
on the PADT. His results are not directly compara-
ble to ours because of the different treebanks? repre-
sentations, even though all the experiments reported
here were performed using MaltParser. Our results
agree with previous work on Arabic and Hebrew in
that marking the definite article is helpful for pars-
ing. However, we go beyond previous work in that
1589
we also extend this morphologically enhanced fea-
ture set to include additional lexical and inflectional
features. Previous work with MaltParser in Russian,
Turkish and Hindi showed gains with case but not
with agreement features (Nivre et al, 2008; Eryigit
et al, 2008; Nivre, 2009). Our work is the first using
MaltParser to show gains using agreement-oriented
features (Marton et al, 2010), and the first to use
functional features for this task (this paper).
5 Experiments
Throughout this section, we only report results us-
ing predicted input feature values (e.g., generated
automatically by a POS tagger). After presenting
the parser we use (Section 5.1), we examine a large
space of settings in the following order: the contri-
bution of numerous inflectional features in a con-
trolled fashion (Section 5.2);5 the contribution of
the lexical features in a similar fashion, as well as
the combination of lexical and inflectional features
(Section 5.3); an extension of the DET feature (Sec-
tion 5.4); using functional NUMBER and GENDER
feature values, as well as the RATIONALITY feature
(Section 5.5); finally, putting best feature combina-
tions to test with the best-performing POS tagset,
and on an unseen test set (Section 5.6). All results
are reported mainly in terms of labeled attachment
accuracy score (parent word and the dependency re-
lation to it, a.k.a. LAS). Unlabeled attachment ac-
curacy score (UAS) is also given. We use McNe-
mar?s statistical significance test as implemented by
Nilsson and Nivre (2008), and denote p < 0.05 and
p < 0.01 with + and ++, respectively.
5.1 Parser
For all experiments reported here we used the syn-
tactic dependency parser MaltParser v1.3 (Nivre,
2003; Nivre, 2008; K?bler et al, 2009) ? a
transition-based parser with an input buffer and a
stack, using SVM classifiers to predict the next state
in the parse derivation. All experiments were done
using the Nivre "eager" algorithm.6 For training, de-
5In this paper, we do not examine the contribution of differ-
ent POS tagsets, see Marton et al (2010) for details.
6Nivre (2008) reports that non-projective and pseudo-
projective algorithms outperform the "eager" projective algo-
rithm in MaltParser, but our training data did not contain any
non-projective dependencies. The Nivre "standard" algorithm
velopment and testing, we follow the splits used by
Roth et al (2008) for PATB part 3 (Maamouri et al,
2004). We kept the test unseen during training.
There are five default attributes, in the MaltParser
terminology, for each token in the text: word ID (or-
dinal position in the sentence), word form, POS tag,
head (parent word ID), and deprel (the dependency
relation between the current word and its parent).
There are default MaltParser features (in the ma-
chine learning sense),7 which are the values of func-
tions over these attributes, serving as input to the
MaltParser internal classifiers. The most commonly
used feature functions are the top of the input buffer
(next word to process, denoted buf[0]), or top of the
stack (denoted stk[0]); following items on buffer or
stack are also accessible (buf[1], buf[2], stk[1], etc.).
Hence MaltParser features are defined as POS tag
at stk[0], word form at buf[0], etc. K?bler et al
(2009) describe a ?typical? MaltParser model con-
figuration of attributes and features.8 Starting with
it, in a series of initial controlled experiments, we
settled on using buf[0-1] + stk[0-1] for wordforms,
and buf[0-3] + stk[0-2] for POS tags. For features of
new MaltParser-attributes (discussed later), we used
buf[0] + stk[0]. We did not change the features for
deprel. This new MaltParser configuration resulted
in gains of 0.3-1.1% in labeled attachment accuracy
(depending on the POS tagset) over the default Malt-
Parser configuration.9 All experiments reported be-
low were conducted using this new configuration.
5.2 Inflectional features
In order to explore the contribution of inflectional
and lexical information in a controlled manner, we
focused on the best performing core (?morphology-
free?) POS tagset, CORE12, as baseline; using three
is also reported to do better on Arabic, but in a preliminary ex-
perimentation, it did similarly or slightly worse than the "eager?
one, perhaps due to high percentage of right branching (left
headed structures) in our Arabic training set ? an observation
already noted in Nivre (2008).
7The terms ?feature? and ?attribute? are overloaded in the
literature. We use them in the linguistic sense, unless specifi-
cally noted otherwise, e.g., ?MaltParser feature(s)?.
8It is slightly different from the default configuration.
9We also experimented with normalizing word forms (Alif
Maqsura conversion to Ya, and hamza removal from Alif forms)
as is common in parsing and statistical machine translation lit-
erature ? but it resulted in a similar or slightly decreased perfor-
mance, so we settled on using non-normalized word forms.
1590
setup LAS LASdiff UAS
A
ll CORE12 78.68 ? 82.48
+ all inflectional features 77.91 -0.77 82.14
Se
p
+DET 79.82++ 1.14 83.18
+STATE 79.34++ 0.66 82.85
+GENDER 78.75 0.07 82.35
+PERSON 78.74 0.06 82.45
+NUMBER 78.66 -0.02 82.39
+VOICE 78.64 -0.04 82.41
+ASPECT 78.60 -0.08 82.39
+MOOD 78.54 -0.14 82.35
+CASE 75.81 -2.87 80.24
G
re
ed
y
+DET+STATE 79.42++ 0.74 82.84
+DET+GENDER 79.90++ 1.22 83.20
+DET+GENDER+PERSON 79.94++ 1.26 83.21
+DET+PNG 80.11++ 1.43 83.29
+DET+PNG+VOICE 79.96++ 1.28 83.18
+DET+PNG+ASPECT 80.01++ 1.33 83.20
+DET+PNG+MOOD 80.03++ 1.35 83.21
Table 1: CORE12 with inflectional features, predicted input.
Top: Adding all nine features to CORE12. Second part: Adding
each feature separately, comparing difference from CORE12.
Third part: Greedily adding best features from second part.
different setups, we added nine morphological fea-
tures with values predicted by MADA: DET (pres-
ence of the definite determiner), PERSON, ASPECT,
VOICE, MOOD, GENDER, NUMBER, STATE (mor-
phological marking as head of an idafa construc-
tion), and CASE. In setup All , we augmented the
baseline model with all nine MADA features (as
nine additional MaltParser attributes); in setup Sep ,
we augmented the baseline model with the MADA
features, one at a time; and in setup Greedy , we
combined them in a greedy heuristic (since the entire
feature space is too vast to exhaust): starting with the
most gainful feature from Sep, adding the next most
gainful feature, keeping it if it helped, or discarding
it otherwise, and continuing through the least gainful
feature. See Table 1.
Somewhat surprisingly, setup All hurts perfor-
mance. This can be explained if one examines the
prediction accuracy of each feature (top of Table 2).
Features which are not predicted with very high ac-
curacy, such as CASE (86.3%), can dominate the
negative contribution, even though they are top con-
tributors when provided as gold input (Marton et al,
2010); when all features are provided as gold in-
put, All actually does better than individual features,
which puts to rest a concern that its decrease here
feature acc set size
DET 99.6 3*
PERSON 99.1 4*
ASPECT 99.1 5*
VOICE 98.9 4*
MOOD 98.6 5*
GENDER 99.3 3*
NUMBER 99.5 4*
STATE 95.6 4*
CASE 86.3 5*
ROOT 98.4 9646
PATTERN 97.0 338
LEMMA (diacritized) 96.7 16837
LMM (undiacritized lemma) 98.3 15305
normalized word form (A,Y) 99.3 29737
non-normalized word form 98.9 29980
Table 2: Feature prediction accuracy and set sizes. * = The set
includes a "N/A" value.
setup LAS LASdiff UAS
A
ll CORE12 (repeated) 78.68 ? 82.48
+ all lexical features 78.85 0.17 82.46
Se
p
+LMM 78.96+ 0.28 82.54
+ROOT 78.94+ 0.26 82.64
+LEMMA 78.80 0.12 82.42
+PATTERN 78.59 -0.09 82.39
G
re
ed
y +LMM+ROOT 79.04++ 0.36 82.63
+LMM+ROOT+LEMMA 79.05++ 0.37 82.63
+LMM+ROOT+PATTERN 78.93 0.25 82.58
Table 3: Lexical features. Top part: Adding each feature
separately; difference from CORE12 (predicted). Bottom part:
Greedily adding best features from previous part.
is due to data sparseness. Here, when features are
predicted, the DET feature (determiner), followed by
the STATE (construct state, idafa) feature, are top in-
dividual contributors in setup Sep. Adding DET and
the so-called ?-features (PERSON, NUMBER, GEN-
DER, also shorthanded PNG) in the Greedy setup,
yields 1.43% gain over the CORE12 baseline.
5.3 Lexical features
Next, we experimented with adding the lexical fea-
tures, which involve semantic abstraction to some
degree: LEMMA, LMM (the undiacritized lemma),
and ROOT. We experimented with the same setups
as above: All, Sep, and Greedy. Adding all four
features yielded a minor gain in setup All. LMM
was the best single contributor, closely followed by
ROOT in Sep. CORE12+LMM+ROOT (with or with-
1591
CORE12 + . . . LAS LASdiff UAS
+DET+PNG (repeated) 80.11++ 1.43 83.29
+DET+PNG+LMM 80.23++ 1.55 83.34
+DET+PNG+LMM +ROOT 80.10++ 1.42 83.25
+DET+PNG+LMM +PATTERN 80.03++ 1.35 83.15
Table 4: Inflectional+lexical features together.
CORE12 + . . . LAS LASdiff UAS
+DET (repeated) 79.82++ ? 83.18
+DET2 80.13++ 0.31 83.49
+DET+PNG+LMM (repeated) 80.23++ ? 83.34
+DET2+PNG+LMM 80.21++ -0.02 83.39
Table 5: Extended inflectional features.
out LEMMA) was the best greedy combination in
setup Greedy. See Table 3. All lexical features are
predicted with high accuracy (bottom of Table 2).
Following the same greedy heuristic, we
augmented the best inflection-based model
CORE12+DET+PNG with lexical features, and
found that only the undiacritized lemma (LMM)
alone improved performance (80.23%). See Table 4.
5.4 Inflectional feature engineering
So far we experimented with morphological fea-
ture values as predicted by MADA. However, it is
likely that from a machine-learning perspective, rep-
resenting similar categories with the same tag may
be useful for learning. Therefore, we next exper-
imented with modifying inflectional features that
proved most useful.
As DET may help distinguish the N-N idafa con-
struction from the N-A modifier construction, we
attempted modeling also the DET values of pre-
vious and next elements (as MaltParser?s stk[1] +
buf[1], in addition to stk[0] + buf[0]). This vari-
ant, denoted DET2, indeed helps: when added to
the CORE12, DET2 improves non-gold parsing qual-
ity by more than 0.3%, compared to DET (Ta-
ble 5). This improvement unfortunately does not
carry over to our best feature combination to date,
CORE12+DET+PNG+LMM. However, in subsequent
feature combinations, we see that DET2 helps again,
or at least, doesn?t hurt: LAS goes up by 0.06% in
conjunction with features LMM+PERSON +FN*NGR
in Table 6.
CORE12 + . . . LAS LASdiff UAS
CORE12 (repeated) 78.68 ? 82.48
+PERSON (repeated) 78.74 0.06 82.45
+GENDER (repeated) 78.75 0.07 82.35
+NUMBER (repeated) 78.66 -0.02 82.39
+FN*GENDER 78.96++ 0.28 82.53
+FN*NUMBER 78.88+ 0.20 82.53
+FN*NUMDGTBIN 78.87 0.19 82.53
+FN*RATIONALITY 78.91+ 0.23 82.60
+FN*GNR 79.32++ 0.64 82.78
+PERSON+FN*GNR 79.34++ 0.66 82.82
+DET+LMM+PERSON+FN*NGR 80.47++ 1.79 83.57
+DET2+LMM+PERSON+FN*NGR 80.53++ 1.85 83.66
+DET2+LMM+PERSON+FN*NG 80.43++ 1.75 83.56
+DET2+LMM+PNG+FN*NGR 80.51++ 1.83 83.66
CATIBEX 79.74 ? 83.30
+DET2+LMM +PERSON+FN*NGR 80.83++ 1.09 84.02
BW 72.64 ? 77.91
+DET2+LMM +PERSON+FN*NGR 74.40++ 1.76 79.40
Table 6: Functional features: gender, number, rationality.
We also experimented with PERSON. We changed
the values of proper names from ?N/A? to ?3? (third
person), but it resulted in a similar or slightly de-
creased performance, so it was abandoned.
5.5 Functional feature values
The NUMBER and GENDER features we have used
so far only reflect surface (as opposed to functional)
values, e.g., broken plurals are marked as singular.
This might have a negative effect on learning gen-
eralizations over the complex agreement patterns in
MSA (see Section 3), beyond memorization of word
pairs seen together in training.
Predicting functional features To predict func-
tional GENDER, functional NUMBER and RATIO-
NALITY, we build a simple maximum likelihood es-
timate (MLE) model using these annotations in the
corpus created by Alkuhlani and Habash (2011). We
train using the same training data we use through-
out this paper. For all three features, we select the
most seen value in training associated with the triple
word-CATIBEX-lemma; we back off to CATIBEX-
lemma and then to lemma. For gender and num-
ber, we further back off to the surface values; for
rationality, we back off to the most common value
(irrational). On our predicted dev set, the over-
all accuracy baseline of predicting correct functional
gender-number-rationality using surface features is
1592
85.1% (for all POS tags). Our MLE model reduces
the error by two thirds reaching an overall accuracy
of 95.5%. The high accuracy may be a result of the
low percentage of words in the dev set that do not
appear in training (around 4.6%).
Digit tokens (e.g., ?4?) are also marked singu-
lar by default. They don?t show surface agreement,
even though the corresponding number-word token
(

??K. P@ Arb?~ ?four.fem.sing?) would. We further ob-
serve that MSA displays complex agreement pat-
terns with numbers (Dada, 2007). Therefore, we
alternatively experimented with binning the digit to-
kens? NUMBER value accordingly:
? the number 0 and numbers ending with 00
? the number 1 and numbers ending with 01
? the number 2 and numbers ending with 02
? the numbers 3-10 and those ending with 03-10
? the numbers, and numbers ending with, 11-99
? all other number tokens (e.g., 0.35 or 7/16)
and denoted these experiments with NUMDGTBIN.
Almost 1.5% of the tokens are digit tokens in the
training set, and 1.2% in the dev set.10
Results using these new features are shown in Ta-
ble 6. The first part repeats the CORE12 baseline.
The second part repeats previous experiments with
surface morphological features. The third part uses
the new functional morphological features instead.
The performance using NUMBER and GENDER in-
creases by 0.21% and 0.22%, respectively, as we re-
place surface features with functional features. (Re-
call that there is no functional PERSON.) We then see
that the change in the representation of digits does
not help; in the large space of experiments we have
performed, we saw some improvement through the
use of this alternative representation, but not in any
of the feature combinations that performed best and
that we report on in this paper. We then use just the
RATIONALITY feature, which results in an increase
over the baseline. The combination of all three func-
tional features (NUMBER, GENDER, RATIONALITY)
provides for a nice cumulative effect. Adding PER-
SON hardly improves further.
In the fourth part of the table, we include the other
features which we found previously to be helpful,
10We didn?t mark the number-words since in our training data
there were less than 30 lemmas of less than 2000 such tokens, so
presumably their agreement patterns can be more easily learned.
namely DET and LMM. Here, using DET2 instead of
DET (see Section 5.4) gives us a slight improvement,
providing our best result using the CORE12 POS
tagset: 80.53%. This is a 1.85% improvement over
using only the CORE12 POS tags (an 8.7% error re-
duction); of this improvement, 0.3% absolute (35%
relative) is due to the use of functional features. We
then use the best configuration, but without the RA-
TIONALITY feature; we see that this feature on its
own contributes 0.1% absolute, confirming its place
in Arabic syntax. In gold experiments which we
do not report here, the contribution was even higher
(0.6-0.7%). The last row in the fourth part of Table 6
shows that using both surface and functional variants
of NUMBER and GENDER does not help (hurts, in
fact); the functional morphology features carry suf-
ficient information for syntactic disambiguation.
The last part of the table revalidates the gains
achieved of the best feature combination using the
two other POS tagsets mentioned in Section 3: CAT-
IBEX (the best performing tagset with predicted val-
ues), and BW (the best POS tagset with gold val-
ues in Marton et al (2010), but results shown here
are with predicted values). The CATIBEX result of
80.83% is our overall best result. The result using
BW reconfirms that BW is not the best tagset to use
for parsing Arabic with current prediction ability.
5.6 Validating results on unseen test set
Once experiments on the development set were
done, we ran the best performing models on the pre-
viously unseen test set (Section 5.1). Table 7 shows
that the same trends hold on this set as well.
Model LAS LASdiff UAS
CATIBEX 78.46 ? 81.81
+DET2+LMM+PER+FN*NGR 79.45++ 0.99 82.56
Table 7: Results on unseen test set for models which performed
best on dev set ? predicted input.
6 Error Analysis
We analyze the attachment accuracy by attachment
type. We show the accuracy for selected attach-
ment types in Table 8. Using just CORE12, we see
that some attachments (subject, modifications) are
harder than others (objects, idafa). We see that by
1593
Features SBJ OBJ MN MP IDF Tot.
CORE12 67.9 90.4 72.0 70.3 94.5 78.7
CORE12 + LMM 68.8 90.4 72.6 70.9 94.6 79.0
CORE12 + DET2
+LMM+PNG 71.7 91.0 74.9 72.4 95.5 80.2
CORE12 + DET2
+LMM+PERS
+FN*NGR 72.3 91.0 76.0 73.3 95.4 80.5
Table 8: Error analysis: Accuracy by attachment type (se-
lected): subject, object, modification by a noun, modification
(of a verb or a noun) by a preposition, idafa, and overall results
(which match previously shown results)
adding LMM, all attachment types improve a little
bit; this is as expected, since this feature provides
a slight lexical abstraction. We then add features
designed to improve idafa and those relations sub-
ject to agreement, subject and nominal modification
(DET2, PERSON, NUMBER, GENDER). We see that
as expected, subject, nominal modification (MN),
and idafa reduce error by substantial margins (error
reduction over CORE12+LMM greater than 8%, in
the case of idafa the error reduction is 16.7%), while
object and prepositional attachment (MP) improve
to a lesser degree (error reduction of 6.2% or less).
We assume that the relations not subject to agree-
ment (object and prepositional attachment) improve
because of the overall improvement in the parse due
to the improvements in the other relations.
When we move to the functional features, we
again see a reduction in the attachments which are
subject to agreement, namely subject and nomi-
nal modification (error reductions over surface fea-
tures of 2.1% and 4.4%, respectively). Idafa de-
creases slightly (since this relation is not affected
by the functional features), while object stays the
same. Surprisingly, prepositional attachment also
improves, with an error reduction of 3.3%. Again,
we can only explain this by proposing that the im-
provement in nominal modification attachment has
the indirect effect of ruling out some bad preposi-
tional attachments as well.
In summary, we see that not only do morphologi-
cal features ? and functional morphology features in
particular ? improve parsing, but they improve pars-
ing in the way that we expect: those relations subject
to agreement improve more than those that are not.
Last, we point out that MaltParser does not model
generalized feature checking or matching directly,
i.e., it has not learned that certain syntactic relations
require identical (functional) morphological feature
values. The gains in parsing quality reflect that the
MaltParser SVM classifier has learned that the pair-
ing of specific morphological feature values ? e.g.,
fem.sing. for both the verb and its subject ? is use-
ful, with no generalization from each specific value
to other values, or to general pair-wise value match-
ing.
7 Conclusions and Future Work
We explored the contribution of different morpho-
logical (inflectional and lexical) features to depen-
dency parsing of Arabic. We find that definiteness
(DET), ?-features (PERSON, NUMBER, GENDER),
and undiacritized lemma (LMM) are most helpful for
Arabic dependency parsing on predicted input. We
further find that functional morphology features and
rationality improve over surface morphological fea-
tures, as predicted by the complex agreement rules
of Arabic. To our knowledge, this is the first result
in Arabic NLP that uses functional morphology fea-
tures, and that shows an improvement over surface
features.
In future work, we intend to improve the predic-
tion of functional morphological features in order to
improve parsing accuracy. We also intend to investi-
gate how these features can be integrated into other
parsing frameworks; we expect them to help inde-
pendently of the framework. We plan to make our
parser available to other researchers. Please contact
the authors if interested.
Acknowledgments
This work was supported by the DARPA GALE
program, contract HR0011-08-C-0110. We thank
Joakim Nivre for his useful remarks, Otakar Smr?
for his help with Elixir-FM, Ryan Roth and Sarah
Alkuhlani for their help with data, and three anony-
mous reviewers for useful comments. Part of
the work was done while the first author was at
Columbia University.
1594
References
Sarah Alkuhlani and Nizar Habash. 2011. A corpus for
modeling morpho-syntactic agreement in Arabic: gen-
der, number and rationality. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics (ACL), Portland, Oregon, USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of Computational Natural Language
Learning (CoNLL), pages 149?164.
Timothy A. Buckwalter. 2004. Buckwalter Arabic Mor-
phological Analyzer Version 2.0. Linguistic Data
Consortium, University of Pennsylvania, 2002. LDC
Cat alog No.: LDC2004L02, ISBN 1-58563-324-0.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL),
College Park, Maryland, USA, June.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of spanish. In
Proceedings of Human Language Technology (HLT)
and the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 795?802.
Ali Dada. 2007. Implementation of Arabic numerals and
their syntax in GF. In Proceedings of the Workshop
on Computational Approaches to Semitic Languages,
pages 9?16, Prague, Czech Republic.
Mona Diab. 2007. Towards an optimal pos tag set for
modern standard arabic processing. In Proceedings
of Recent Advances in Natural Language Processing
(RANLP), Borovets, Bulgaria.
G?lsen Eryigit, Joakim Nivre, and Kemal Oflazer. 2008.
Dependency parsing of turkish. Computational Lin-
guistics, 34(3):357?389.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING), pages 394?
402, Beijing, China.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 573?580, Ann Ar-
bor, Michigan.
Nizar Habash and Ryan Roth. 2009. Catib: The
columbia arabic treebank. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 221?
224, Suntec, Singapore, August.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic? and Barbora Vidov?-Hladk?. 1998. Tag-
ging Inflective Languages: Prediction of Morpholog-
ical Categories for a Rich, Structured Tagset. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING)- the Association for
Computational Linguistics (ACL), pages 483?490.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan and Claypool
Publishers.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and improve-
ments. In Proceedings of the Treebanks and Linguis-
tic Theories Conference, pages 31?42, Prague, Czech
Republic.
Mohamed Maamouri, Ann Bies, Timothy A. Buckwalter,
and Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
Proceedings of the NEMLAR Conference on Arabic
Language Resources and Tools, pages 102?109, Cairo,
Egypt.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with inflec-
tional and lexical morphological features. In Proceed-
ings of Workshop on Statistical Parsing of Morpho-
logically Rich Languages (SPMRL) at the 11th Meet-
ing of the North American Chapter of the Association
for Computational Linguistics (NAACL) - Human Lan-
guage Technology (HLT), Los Angeles, USA.
Jens Nilsson and Joakim Nivre. 2008. MaltEval: An
evaluation and visualization tool for dependency pars-
ing. In Proceedings of the sixth International Confer-
ence on Language Resources and Evaluation (LREC),
Marrakech, Morocco.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre, Igor M. Boguslavsky, and Leonid K.
Iomdin. 2008. Parsing the SynTagRus Treebank of
Russian. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING),
pages 641?648.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Conference on Parsing Technologies
(IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4).
1595
Joakim Nivre. 2009. Parsing Indian languages with
MaltParser. In Proceedings of the ICON09 NLP Tools
Contest: Indian Language Dependency Parsing, pages
12?18.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological tag-
ging, diacritization, and lemmatization using lexeme
models and feature ranking. In Proceedings of ACL-
08: HLT, Short Papers, pages 117?120, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Otakar Smr?. 2007. Functional Arabic Morphology. For-
mal System and Implementation. Ph.D. thesis, Charles
University, Prague.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167, Morristown, NJ, USA.
1596
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116?1126,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Online Relative Margin Maximization for Statistical Machine Translation
Vladimir Eidelman
Computer Science
and UMIACS
University of Maryland
College Park, MD
vlad@umiacs.umd.edu
Yuval Marton
Microsoft
City Center Plaza
Bellevue, WA
yuvalmarton@gmail.com
Philip Resnik
Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
Recent advances in large-margin learning
have shown that better generalization can
be achieved by incorporating higher order
information into the optimization, such as
the spread of the data. However, these so-
lutions are impractical in complex struc-
tured prediction problems such as statis-
tical machine translation. We present an
online gradient-based algorithm for rela-
tive margin maximization, which bounds
the spread of the projected data while max-
imizing the margin. We evaluate our op-
timizer on Chinese-English and Arabic-
English translation tasks, each with small
and large feature sets, and show that our
learner is able to achieve significant im-
provements of 1.2-2 BLEU and 1.7-4.3
TER on average over state-of-the-art opti-
mizers with the large feature set.
1 Introduction
The desire to incorporate high-dimensional sparse
feature representations into statistical machine
translation (SMT) models has driven recent re-
search away from Minimum Error Rate Training
(MERT) (Och, 2003), and toward other discrim-
inative methods that can optimize more features.
Examples include minimum risk (Smith and Eis-
ner, 2006), pairwise ranking (PRO) (Hopkins and
May, 2011), RAMPION (Gimpel and Smith, 2012),
and variations of the margin-infused relaxation al-
gorithm (MIRA) (Watanabe et al, 2007; Chiang et
al., 2008; Cherry and Foster, 2012). While the ob-
jective function and optimization method vary for
each optimizer, they can all be broadly described
as learning a linear model, or parameter vector w,
which is used to score alternative translation hy-
potheses.
In every SMT system, and in machine learn-
ing in general, the goal of learning is to find a
model that generalizes well, i.e. one that will yield
good translations for previously unseen sentences.
However, as the dimension of the feature space in-
creases, generalization becomes increasingly diffi-
cult. Since only a small portion of all (sparse) fea-
tures may be observed in a relatively small fixed
set of instances during tuning, we are prone to
overfit the training data. An alternative approach
for solving this problem is estimating discrimina-
tive feature weights directly on the training bi-
text (Tillmann and Zhang, 2006; Blunsom et al,
2008; Simianer et al, 2012), which is usually sub-
stantially larger than the tuning set, but this is com-
plementary to our goal here of better generaliza-
tion given a fixed size tuning set.
In order to achieve that goal, we need to care-
fully choose what objective to optimize, and how
to perform parameter estimation of w for this ob-
jective. We focus on large-margin methods such
as SVM (Joachims, 1998) and passive-aggressive
algorithms such as MIRA. Intuitively these seek
a w such that the separating distance in geomet-
ric space of two hypotheses is at least as large as
the cost incurred by selecting the incorrect one.
This criterion performs well in practice at find-
ing a linear separator in high-dimensional feature
spaces (Tsochantaridis et al, 2004; Crammer et
al., 2006).
Now, recent advances in machine learning have
shown that the generalization ability of these
learners can be improved by utilizing second or-
der information, as in the Second Order Percep-
tron (Cesa-Bianchi et al, 2005), Gaussian Margin
Machines (Crammer et al, 2009b), confidence-
weighted learning (Dredze and Crammer, 2008),
AROW (Crammer et al, 2009a; Chiang, 2012)
and Relative Margin Machines (RMM) (Shiv-
aswamy and Jebara, 2009b). The latter, RMM,
was introduced as an effective and less computa-
tionally expensive way to incorporate the spread
of the data ? second order information about the
1116
distance between hypotheses when projected onto
the line defined by the weight vector w.
Unfortunately, not all advances in machine
learning are easy to apply to structured prediction
problems such as SMT; the latter often involve la-
tent variables and surrogate references, resulting
in loss functions that have not been well explored
in machine learning (Mcallester and Keshet, 2011;
Gimpel and Smith, 2012). Although Shivaswamy
and Jebara extended RMM to handle sequen-
tial structured prediction (Shivaswamy and Jebara,
2009a), their batch approach to quadratic opti-
mization, using existing off-the-shelf QP solvers,
does not provide a practical solution: as Taskar et
al. (2006) observe, ?off-the-shelf QP solvers tend
to scale poorly with problem and training sam-
ple size? for structured prediction problems.. This
motivates an online gradient-based optimization
approach?an approach that is particularly attrac-
tive because its simple update is well suited for ef-
ficiently processing structured objects with sparse
features (Crammer et al, 2012).
The contributions of this paper include (1) in-
troduction of a loss function for structured RMM
in the SMT setting, with surrogate reference trans-
lations and latent variables; (2) an online gradient-
based solver, RM, with a closed-form parameter
update to optimize the relative margin loss; and
(3) an efficient implementation that integrates well
with the open source cdec SMT system (Dyer et
al., 2010).1 In addition, (4) as our solution is not
dependent on any specific QP solver, it can be
easily incorporated into practically any gradient-
based learning algorithm.
After background discussion on learning in
SMT (?2), we introduce a novel online learning al-
gorithm for relative margin maximization suitable
for SMT (?3). First, we introduce RMM (?3.1) and
propose a latent structured relative margin objec-
tive which incorporates cost-augmented hypothe-
sis selection and latent variables. Then, we de-
rive a simple closed-form online update necessary
to create a large margin solution while simulta-
neously bounding the spread of the projection of
the data (?3.2). Chinese-English translation exper-
iments show that our algorithm, RM, significantly
outperforms strong state-of-the-art optimizers, in
both a basic feature setting and high-dimensional
(sparse) feature space (?4). Additional Arabic-
English experiments further validate these results,
1https://github.com/veidel/cdec
even where previously MERT was shown to be ad-
vantageous (?5). Finally, we discuss the spread
and other key issues of RM (?6), and conclude
with discussion of future work (?7).
2 Learning in SMT
Given an input sentence in the source language
x ? X , we want to produce a translation y ? Y(x)
using a linear model parameterized by a weight
vector w:
(y?, d?) = arg max
(y,d)?Y(x),D(x)
w>f(x, y, d)
where w>f(x, y, d) is the weighted feature scor-
ing function, hereafter s(x, y, d), and Y(x) is the
space of possible translations of x. While many
derivations d ? D(x) can produce a given transla-
tion, we are only able to observe y; thus we model
d as a latent variable. Although our models are
actually defined over derivations, they are always
paired with translations, so our feature function
f(x, y, d) is defined over derivation?translation
pairs.2 The learning goal is then to estimate w.
The instability of MERT in larger feature
sets (Foster and Kuhn, 2009; Hopkins and May,
2011), has motivated many alternative tuning
methods for SMT. These include strategies based
on batch log-linear models (Tillmann and Zhang,
2006; Blunsom et al, 2008), as well as the in-
troduction of online linear models (Liang et al,
2006a; Arun and Koehn, 2007).
Recent batch optimizers, PRO and RAMPION,
and Batch-MIRA (Cherry and Foster, 2012), have
been partly motivated by existing MT infrastruc-
tures, as they iterate between decoding the entire
tuning set and optimizing the parameters. PRO
considers tuning a classification problem and em-
ploys a binary classifier to rank pairs of outputs.
RAMPION aims to address the disconnect between
MT and machine learning by optimizing a struc-
tured ramp loss with a concave-convex procedure.
2.1 Large-Margin Learning
Online large-margin algorithms, such as MIRA,
have also gained prominence in SMT, thanks to
their ability to learn models in high-dimensional
feature spaces (Watanabe et al, 2007; Chiang et
al., 2009). The usual presentation of MIRA?s opti-
mization problem is given as a quadratic program:
2We may omit d in some equations for clarity.
1117
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i
s.t. s(xi, yi, d)? s(xi, y?, d) ? ?i(y?)? ?i
(1)
where y? is the single most violated constraint, the
cost ?i(y) is computed using an external measure
of quality, such as 1-BLEU(yi, y), and a slack vari-
able ?i is introduced to allow for non-separable
instances. C acts as a regularization parameter,
trading off between margin maximization and con-
straint violations.
While solving the optimization problem relies
on computing the margin between the correct out-
put yi, and y?, in SMT our decoder is often inca-
pable of producing the reference translation, i.e.
yi /? Y(xi). We must instead resort to selecting a
surrogate reference, y+ ? Y(xi). This issue has
recently received considerable attention (Liang
et al, 2006a; Eidelman, 2012; Chiang, 2012),
with preference given to surrogate references ob-
tained through cost-diminished hypothesis selec-
tion. Thus, y+ is selected based on a combination
of model score and error metric from the k-best
list produced by our current model. A similar se-
lection is made for the cost-augmented hypothesis
y? ? Y(xi):
(y+, d+)? arg max
(y,d)?Y(xi),D(xi)
s(xi, y, d)??i(y)
(y?, d?)? arg max
(y,d)?Y(xi),D(xi)
s(xi, y, d) + ?i(y)
In this setting, the optimization problem be-
comes:
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i
s.t. ?s(xi, y+, y?) ? ?i(y?)??i(y+)? ?i
(2)
where ?s(xi, y+, y?)=s(xi, y+, d+)-s(xi, y?, d?)
This leads to a variant of the structured ramp
loss to be optimized:
` =
? max
(y+,d+)?Y(xi),D(xi)
(s(xi, y+, d+)??i(y+)
)
+ max
(y?,d?)?Y(xi),D(xi)
(s(xi, y?, d?) + ?i(y?)
)
(3)
The passive-aggressive update (Crammer et al,
2006), which is used to solve this problem, up-
dates w on each round such that the score of the
correct hypothesis y+ is greater than the score of
the incorrect y? by a margin at least as large as the
cost incurred by predicting the incorrect hypothe-
sis, while keeping the change to w small.
 
(a)
 
(b)
Figure 1: (a) RM and large margin solution comparison and
(b) the spread of the projections given by each. RM and large
margin solutions are shown with a darker dotted line and a
darker solid line, respectively.
3 The Relative Margin Machine in SMT
3.1 Relative Margin Machine
The margin, the distance between the correct
hypothesis and incorrect one, is defined by
s(xi, y+, d+) and s(xi, y?, d?). It is maxi-
mized by minimizing the norm in SVM, or
analogously, the proximity constraint in MIRA:
arg minw 12 ||w ?wt||2. However, theoretical re-sults supporting large-margin learning, such as the
VC-dimension (Vapnik, 1995) or the Rademacher
bound (Bartlett and Mendelson, 2003) consider
measures of complexity, in addition to the empir-
ical performance, when describing future predic-
tive ability. The measures of complexity usually
take the form of some value on the radius of the
data, such as the ratio of the radius of the data to
the margin (Shivaswamy and Jebara, 2009a). As
radius is a way of measuring spread in any pro-
jection direction, here we will specifically be in-
terested in the the spread of the data as measured
after the projection defined by the learned model
w.
More formally, the spread is the dis-
tance between y+, and the worst candidate
(yw, dw)? arg min(y,d)?Y(xi),D(xi) s(xi, y, d),after projecting both onto the line defined by the
weight vector w. For each y?, this projection is
conveniently given by s(xi, y?, d), thus the spread
is calculated as ?s(xi, y+, yw).
RMM was introduced as a generalization over
SVM that incorporates both the margin constraint
1118
and information regarding the spread of the data.
The relative margin is the ratio of the absolute,
or maximum margin, to the spread of the pro-
jected data. Thus, the RMM learns a large mar-
gin solution relative to the spread of the data, or
in other words, creates a max margin while si-
multaneously bounding the spread of the projected
data. As a concrete example, consider the plot
shown in Figure 1(a), with hypotheses represented
by two-dimensional feature vectors. The point
marked with a circle in the upper right represents
f(xi, y+), while all other squares represent alter-
native incorrect hypotheses f(xi, y?). The large
margin decision boundary is shown with a darker
solid line, while the relative margin solution is
shown with a darker dotted line. The lighter lines
parallel to each define the margins, with the square
at the intersection being f(xi, y?). The bottom
portion of Figure 1(b) presents an alternative view
of each solution, showing the projections of the
hypotheses given the learned model of each. No-
tice that with a large margin solution, although the
distance between y+ and y? is greater, the points
are highly spread, extending far to the left of the
decision boundary.
In contrast, with a relative margin, although
we have a smaller absolute margin, the spread is
smaller, all points being within a smaller distance 
of the decision boundary. The higher the spread of
the projection, the higher the variance of the pro-
jected points, and the greater the likelihood that
we will mislabel a new instance, since the high
variance projections may cross the learned deci-
sion boundary. In higher dimensions, accounting
for the spread becomes even more crucial, as will
be discussed in Section 6.3
Although RMM is theoretically well-founded
and improves practical performance over large-
margin learning in the settings where it was intro-
duced, it is unsuitable for most complex structured
prediction in NLP. Nonetheless, since structured
RMM is a generalization of Structured SVM,
which shares its underlying objective with MIRA,
our intuition is that SMT should be able to benefit
as well. But to take advantage of the second-order
information RMM utilizes for increased general-
izability in SMT, we need a computationally effi-
3The motivation of confidence-weighted estima-
tion (Dredze and Crammer, 2008) and AROW (Crammer
et al, 2009a) is related in spirit. They use second-order
information in the form of a distribution over weights to
change the maximum margin solution.
cient optimization procedure that does not require
batch training or an off-the-shelf QP solver.
3.2 RM Algorithm
We address the above-mentioned limitations by in-
troducing a novel online learning algorithm for
relative margin maximization, RM. The relative
margin solution is obtained by maximizing the
same margin as Equation (2), but now with re-
spect to the distance between y+, and the worst
candidate yw. Thus, the relative margin dictates
trading-off between a large margin as before, and
a small spread of the projection, in other words,
bounding the distance between y+ and yw. The
additional computation required, namely, obtain-
ing yw, is efficient to perform, and has likely al-
ready happened while obtaining the k-best deriva-
tions necessary for the margin update. The online
latent structured soft relative margin optimization
problem is then:
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i + D?i
s.t.: ?s(xi, y+, y?) ? ?i(y?)??i(y+)? ?i
?B ? ?i ? ?s(xi, y+, yw) ? B + ?i
(4)
where additional bounding constraints are added
to the usual margin constraints in order to contain
the spread by bounding the difference in projec-
tions. B is an additional parameter; it controls
the spread, trading off between margin maximiza-
tion and spread minimization. Notice that when
B ? ?, the bounding constraints disappear, and
we are left with the original problem in Equa-
tion (2). D, which plays an analogous role to C,
allows penalized violations of the bounding con-
straints.
The dual of Equation (4) can be derived as:
max
?,?,??
L =
?
y?Y(xi)
?y ?B
?
y?Y(xi)
?y ?B
?
y?Y(xi)
??y
?12
? ?
y?Y(xi)
?y?i(y+, y)?
?
y?Y(xi)
?y?i(y+, y)
+
?
y?Y(xi)
??y?i(y+, y),
?
y??Y(xj)
?y??j(y+, y?)?
?
y??Y(xj)
?y??j(y+, y?)
+
?
y??Y(xj)
??y??j(y+, y?)
?
(5)
where the ? Lagrange multiplier corresponds
to the standard margin constraint, while ? and
1119
?? each correspond to a bounding constraint,
and ?i(y+, y?) corresponds to the difference of
f(xi, y+, d+) and f(xi, y?, d?). The weight up-
date can then be obtained from the dual variables:
?
?y?i(y+, y)?
?
?y?i(y+, y) +
?
??y?i(y+, y)
(6)
The dual in Equation (5) can be optimized us-
ing a cutting plane algorithm, an effective method
for solving a relaxed optimization problem in
the dual, used in Structured SVM, MIRA, and
RMM (Tsochantaridis et al, 2004; Chiang, 2012;
Shivaswamy and Jebara, 2009a). The cutting
plane presented in Alg. 1 decomposes the overall
problem into subproblems which are solved inde-
pendently by creating working sets Sji , which cor-
respond to the largest violations of either the mar-
gin constraint, or bounding constraints, and itera-
tively satisfying the constraints in each set.
The cutting plane in Alg. 1 makes use of the
the closed-form gradient-based updates we de-
rived for RM presented in Alg. 2. The updates
amount to performing a subgradient descent step
to update w in accordance with the constraints.
Since the constraint matrix of the dual program is
not strictly decomposable across constraint types,
we are in effect solving an approximation of the
original problem.
Algorithm 1 RM Cutting Plane Algorithm
(adapted from (Shivaswamy and Jebara, 2009a))
Require: ith training example (xi, yi), weight w, margin
reg. C, bound B, bound reg. D, , B
1: S1i ?
{
y+
}, S2i ?
{
y+
}, S3i ?
{
y+
}
2: repeat
3: H(y) := ?i(y)??i(y+)? ?s(xi, y+, y)
4: y1 ? arg maxy?Y(xi) H(y)
5: y2 ? arg maxy?Y(xi) G(y) := ?s(xi, y+, y)6: y3 ? arg miny?Y(xi)?G(y)7: ? ? max {0,maxy?Si H(y)}8: V1 ? H(y1)? ? ? 
9: V2 ? G(y2)?B ? B
10: V3 ? ?G(y3)?B ? B
11: j ? argmaxj??{1,2,3} Vj?
12: if Vj > 0 then
13: Sji ? Sji ? {yj}
14: OPTIMIZE(w, S1i , S2i , S3i , C,B) . see Alg. 2
15: end if
16: until S1i , S2i , S3i do not change
Alternatively, we could utilize a passive-
aggressive updating strategy (Crammer et al,
2006), which would simply bypass the cutting
plane and select the most violated constraint for
Algorithm 2 RM update with ?, ?, ??
1: procedure OPTIMIZE(w, S1i , S2i , S3i , C,B)
2: whilew changes do
3: if ??S1i
?? > 1 then
4: UPDATEMARGIN(w, S1i , C)
5: end if
6: if ??S2i
?? > 1 then
7: UPDATEUPPERBOUND(w, S2i , B)
8: end if
9: if ??S3i
?? > 1 then
10: UPDATELOWERBOUND(w, S3i , B)
11: end if
12: end while
13: end procedure
14: procedure UPDATEMARGIN(w, S1i , C)
15: ?y ? 0 for all y ? S1i
16: ?y+i ? C17: for n? 1...MaxIter do
18: Select two constraints y, y? from S1i
19: ?? ? ?i(y?)??i(y)??s(xi, y, y?)||?(y,y?)||2
20: ?? ? max(??y,min(?y? , ??))
21: ?y ? ?y + ?? ; ??y ? ??y ? ??
22: w? w + ??(?(y, y?))
23: end for
24: end procedure
25: procedure UPDATEUPPERBOUND(w, S2i , B)
26: ?y ? 0 for all y ? S2i
27: for n? 1...MaxIter do
28: Select one constraint y from S2i
29: ?? ? max(0, B??s(xi ,y+ ,y)||?(y+,y)||2 )
30: ?y ? ?y + ??
31: w? w ? ??(?(y+, y))
32: end for
33: end procedure
34: procedure UPDATELOWERBOUND(w, S3i , B)
35: ??y ? 0 for all y ? S3i
36: for n? 1...MaxIter do
37: Select one constraint y from S3i
38: ??? ? max(0, ?B??s(xi ,y+ ,y)||?(y+,y)||2 )
39: ??y ? ??y + ???
40: w? w + ???(?(y+, y))
41: end for
42: end procedure
each set, if there is one, and perform the corre-
sponding parameter updates in Alg. 2. We re-
fer to the resulting passive-aggressive algorithm as
RM-PA, and the cutting plane version as RM-CP.
Preliminary experiments showed that RM-PA per-
forms on par with RM-CP, thus RM-PA is the one
used in the empirical evaluation below.
A graphical depiction of the passive-aggressive
RM update is presented in Figure 2. The upper
right circle represents y+, while all other squares
represent alternative hypotheses y?. As in the stan-
dard MIRA solution, we select the maximum mar-
gin constraint violator, y?, shown as the triangle,
and update such that the margin is greater than the
cost. Additionally, we select the maximum bound-
1120
 
Bounding Constraint 
 
dist 
 
cost > margin 
 
B
LE
U
  
S
co
re
 
Margin Constraint 
cost 
 
margin 
 
Model Score 
dist > B 
 
 B 
 
Figure 2: RM update with margin and bounding con-
straints. The diagonal dotted line depicts cost?margin equi-
librium. The vertical gray dotted line depicts the bound B.
White arrows indicate updates triggered by constraint viola-
tions. Squares are data points in the k-best list not selected
for update in this round.
task Corpus Sentences Tokens
En Zh/Ar
Zh-En
training 1.6M 44.4M 40.4M
tune (MT06) 1664 48k 39k
MT03 919 28k 24k
MT05 1082 35k 33k
Ar-En
training 1M 23.7M 22.8M
tune (MT06) 1797 55k 49k
MT05 1056 36k 33k
MT08 1360 51k 45k
4-gram LM 24M 600M ?
Table 1: Corpus statistics
ing constraint violator, yw, shown as the upside-
down triangle, and update so the distance from y+
is no greater than B.
4 Experiments
4.1 Setup
To evaluate the advantage of explicitly accounting
for the spread of the data, we conducted several
experiments on two Chinese-English translation
test sets, using two different feature sets in each.
For training we used the non-UN and non-HK
Hansards portions of the NIST training corpora,
which was segmented using the Stanford seg-
menter (Tseng et al, 2005). The data statistics are
summarized in the top half of Table 1. The English
data was lowercased, tokenized and aligned using
GIZA++ (Och and Ney, 2003) to obtain bidirec-
tional alignments, which were symmetrized using
the grow-diag-final-and method (Koehn
et al, 2003). We trained a 4-gram LM on the
English side of the corpus with additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We used cdec (Dyer et al, 2010) as our
hierarchical phrase-based decoder, and tuned the
parameters of the system to optimize BLEU (Pap-
ineni et al, 2002) on the NIST MT06 corpus.
We applied several competitive optimizers as
baselines: hypergraph-based MERT (Kumar et al,
2009), k-best variants of MIRA (Crammer et al,
2006; Chiang et al, 2009), PRO (Hopkins and
May, 2011), and RAMPION (Gimpel and Smith,
2012). The size of the k-best list was set to 500
for RAMPION, MIRA and RM, and 1500 for PRO,
with both PRO and RAMPION utilizing k-best ag-
gregation across iterations. RAMPION settings
were as described in (Gimpel and Smith, 2012),
and PRO settings as described in (Hopkins and
May, 2011), with PRO requiring regularization
tuning in order to be competitive with the other op-
timizers. MIRA and RM were run with 15 paral-
lel learners using iterative parameter mixing (Mc-
Donald et al, 2010). All optimizers were imple-
mented in cdec and use the same system config-
uration, thus the only independent variable is the
optimizer itself. We set C to 0.01, and MaxIter
to 100. We selected the bound step size D, based
on performance on a held-out dev set, to be 0.01
for the basic feature set and 0.1 for the sparse fea-
ture set. The bound constraintB was set to 1.4 The
approximate sentence-level BLEU cost ?i is com-
puted in a manner similar to (Chiang et al, 2009),
namely, in the context of previous 1-best transla-
tions of the tuning set. All results are averaged
over 3 runs.
4.2 Feature Sets
We experimented with a small (basic) feature set,
and a large (sparse) feature set. For the small
feature set, we use 14 features, including a lan-
guage model, 5 translation model features, penal-
ties for unknown words, the glue rule, and rule
arity. For experiments with a larger feature set,
we introduced additional lexical and non-lexical
sparse Boolean features of the form commonly
found in the literature (Chiang et al, 2009; Watan-
4We also conducted an investigation into the setting of the
B parameter. We explored alternative values for B, as well
as scaling it by the current candidate?s cost, and found that
the optimizer is fairly insensitive to these changes, resulting
in only minor differences in BLEU.
1121
Optimizer Zh Ar
MIRA 35k 37k
PRO 95k 115k
RAMPION 22k 24k
RM 30k 32k
Active+Inactive 3.4M 4.9M
Table 2: Active sparse feature templates
abe et al, 2007; Simianer et al, 2012).
Non-lexical features include structural distor-
tion, which captures the dependence between re-
ordering and the size of a filler, and rule shape,
which bins grammar rules by their sequence of
terminals and nonterminals (Chiang et al, 2008).
Lexical features on rules include rule ID, which
fires on a specific grammar rule. We also in-
troduce context-dependent lexical features for the
300 most frequent aligned word pairs (f ,e) in the
training corpus, which fire on triples (f ,e,f+1) and
(f ,e,f?1), capturing when we see f aligned to e,
with f+1 and f?1 occurring to the right or left of f ,
respectively. All other words fall into the default
?unk? feature bin. In addition, we have insertion
and deletion features for the 150 most frequently
unaligned target and source words. These feature
templates resulted in a total of 3.4 million possible
features, of which only a fraction were active for
the respective tuning set and optimizer, as shown
in Table 2.
4.3 Results
As can be seen from the results in Table 3, our
RM method was the best performer in all Chinese-
English tests according to all measures ? up to 1.9
BLEU and 6.6 TER over MIRA ? even though we
only optimized for BLEU.5 Surprisingly, it seems
that MIRA did not benefit as much from the sparse
features as RM. The results are especially notable
for the basic feature setting ? up to 1.2 BLEU and
4.6 TER improvement over MERT ? since MERT
has been shown to be competitive with small num-
bers of features compared to high-dimensional op-
timizers such as MIRA (Chiang et al, 2008).
For the tuning set, the decoder performance was
consistently the lowest with RM, compared to the
5In the small feature set RAMPION yielded similar best
BLEU scores, but worse TER. In preliminary experiments
with a smaller trigram LM, our RM method consistently
yielded the highest scores in all Chinese-English tests ? up
to 1.6 BLEU and 6.4 TER from MIRA, the second best per-
former.
other optimizers. We believe this is due to the
RM bounding constraint being more resistant to
overfitting the training data, and thus allowing for
improved generalization. Conversely, while PRO
had the second lowest tuning scores, it seemed to
display signs of underfitting in the basic and large
feature settings.
5 Additional Experiments
In order to explore the applicability of our ap-
proach to a wider range of languages, we also eval-
uated its performance on Arabic-English transla-
tion. All experimental details were the same as
above, except those noted below.
For training, we used the non-UN portion of the
NIST training corpora, which was segmented us-
ing an HMM segmenter (Lee et al, 2003). Dataset
statistics are given in the bottom part of Table 1.
The sparse feature templates resulted here in a to-
tal of 4.9 million possible features, of which again
only a fraction were active, as shown in Table 2.
As can be seen in Table 4, in the smaller feature
set, RM and MERT were the best performers, with
the exception that on MT08, MIRA yielded some-
what better (+0.7) BLEU but a somewhat worse
(-0.9) TER score than RM.
On the large feature set, RM is again the best
performer, except, perhaps, a tied BLEU score
with MIRA on MT08, but with a clear 1.8 TER
gain. In both Arabic-English feature sets, MIRA
seems to take the second place, while RAMPION
lags behind, unlike in Chinese-English (?4).6
Interestingly, RM achieved substantially higher
BLEU precision scores in all tests for both lan-
guage pairs. However, this was also usually cou-
pled had a higher brevity penalty (BP) thanMIRA,
with the BP increasing slightly when moving to
the sparse setting.
6 Discussion
The trend of the results, summarized as RM gain
over other optimizers averaged over all test sets, is
presented in Table 5. RM shows clear advantage
in both basic and sparse feature sets, over all other
state-of-the-art optimizers. The RM gains are no-
tably higher in the large feature set, which we take
6In our preliminary experiments with the smaller trigram
LM, MERT did better on MT05 in the smaller feature set, and
MIRA had a small advantage in two cases. RAMPION per-
formed similarly to RM on the smaller feature set. RM?s loss
was only up to 0.8 BLEU (0.7 TER) from MERT or MIRA,
while its gains were up to 1.7 BLEU and 2.1 TER over MIRA.
1122
Small (basic) feature set Large (sparse) feature set
Optimizer Tune MT03 MT05 Tune MT03 MT05
?BLEU ?BLEU ?TER ?BLEU ?TER ?BLEU ?BLEU ?TER ?BLEU ?TER
MERT 35.4 35.8 60.8 32.4 63.9 - - - - -
MIRA 35.5 35.8 61.1 32.1 64.6 36.6 35.9 60.6 32.1 64.1
PRO 34.1 36.0 60.2 31.7 63.4 35.7 34.8 56.1 31.4 59.1
RAMPION 35.1 36.5 58.6 33.0 61.3 36.7 36.9 57.7 33.3 60.6
RM 31.3 36.5 56.4 33.6 59.3 33.2 37.5 54.6 34.0 57.5
Table 3: Performance on Zh-En with basic (left) and sparse (right) feature sets on MT03 and MT05.
Small (basic) feature set Large (sparse) feature set
Optimizer Tune MT05 MT08 Tune MT05 MT08
?BLEU ?BLEU ?TER ?BLEU ?TER ?BLEU ?BLEU ?TER ?BLEU ?TER
MERT 43.8 53.3 40.2 41.0 50.7 - - - - -
MIRA 43.0 52.8 40.8 41.3 50.6 44.4 53.4 40.1 41.8 50.2
PRO 41.5 51.3 41.5 39.4 51.5 46.8 53.2 40.0 41.4 49.7
RAMPION 42.4 52.0 40.8 40.0 50.8 44.6 52.9 40.4 41.0 50.4
RM 38.5 53.3 39.8 40.6 49.7 43.0 55.3 37.5 41.8 48.4
Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.
Small set Large set
Optimizer BLEU TER BLEU TER
MERT 0.4 2.6 - -
MIRA 0.5 3.0 1.4 4.3
PRO 1.4 2.9 2.0 1.7
RAMPION 0.6 1.6 1.2 2.8
Table 5: RM gain over other optimizers averaged
over all test sets.
as an indication for the importance of bounding
the spread.
Spread analysis: For RM, the average spread
of the projected data in the Chinese-English small
feature set was 0.9?3.6 for all tuning iterations,
and 0.7?2.9 for the iteration with the highest de-
coder performance. In comparison, the spread of
the data for MIRA was 5.9?20.5 for the best it-
eration. In the sparse setting, RM had an aver-
age spread of 0.9?2.4 for the best iteration, while
MIRA had a spread of 14.0?31.1. Similarly,
on Arabic-English, RM had a spread of 0.7?2.4
in the small setting, and 0.82?1.4 in the sparse
setting, while MIRA?s spread was 9.4?26.8 and
11.4?22.1, for the small and sparse settings, re-
spectively. Notice that the average spread for RM
stays about the same when moving to higher di-
mensions, with the variance decreasing in both
cases. For MIRA, however, the average spread
increases in both cases, with the variance being
much higher than RM. For instance, observe that
the spread of MIRA on Chinese grows from 5.9 to
14.0 in the sparse feature setting. While bounding
the spread is useful in the low-dimensional setting
(0.7-1.5 BLEU gain with RM over MIRA as shown
in Table 3), accounting for the spread is even more
crucial with sparse features, where MIRA gains
only up to 0.1 BLEU, while RM gains 1 BLEU.
These results support the claim that our imposed
bound B indeed helps decrease the spread, and
that, in turn, lower spread yields better general-
ization performance.
Error Analysis: The inconclusive advantage
of RM over MIRA (in BLEU vs. TER scores)
on Arabic-English MT08 calls for a closer look.
Therefore we conducted a coarse error analysis
on 15 randomly selected sentences from MERT,
RMM and MIRA, with basic and sparse feature
settings for the latter two. This sample yielded
450 data points for analysis: output of the 5 con-
ditions on 15 sentences scored in 6 violation cate-
gories. The categories were: function word drop,
content word drop, syntactic error (with a reason-
able meaning), semantic error (regardless of syn-
tax), word order issues, and function word mis-
translation and ?hallucination?. The purpose of
this analysis was to get a qualitative feel for the
output of each model, and a better idea as to why
we obtained performance improvements. RM no-
1123
ticeably had more word order and excess/wrong
function word issues in the basic feature setting
than any optimizer. However, RM seemed to ben-
efit the most from the sparse features, as its bad
word order rate dropped close toMIRA, and its ex-
cess/wrong function word rate dropped below that
of MIRA with sparse features (MIRA?s rate actu-
ally doubled from its basic feature set). We con-
jecture both these issues will be ameliorated with
syntactic features such as those in Chiang et al
(2008). This correlates with our observation that
RM?s overall BLEU score is negatively impacted
by the BP, as the BLEU precision scores are no-
ticeably higher.
K-best: RM is potentially more sensitive to the
size and order of the k-best list. While MIRA is
only concerned with the margin between y+ and
y?, RM also accounts for the distance between y+
and yw. It might be the case that a larger k-best, or
revisiting previous strategies for y+ and y? selec-
tion, such as bold updating, local updating (Liang
et al, 2006b), or max-BLEU updating (Tillmann
and Zhang, 2006) might have a greater impact.
Also, we only explored several settings of B, and
there remains a continuum of RM solutions that
trade off between margin and spread in different
ways.
Active features: Perhaps contrary to expecta-
tion, we did not see evidence of a correlation be-
tween the number of active features and optimizer
performance. RAMPION, with the fewest features,
is the closest performer to RM in Chinese, while
MIRA, with a greater number, is the closest on
Arabic. We also notice that while PRO had the
lowest BLEU scores in Chinese, it was competi-
tive in Arabic with the highest number of features.
7 Conclusions and Future Work
We have introduced RM, a novel online margin-
based algorithm designed for optimizing high-
dimensional feature spaces, which introduces con-
straints into a large-margin optimizer that bound
the spread of the projection of the data while max-
imizing the margin. The closed-form online up-
date for our relative margin solution accounts for
surrogate references and latent variables.
Experimentation in statistical MT yielded sig-
nificant improvements over several other state-
of-the-art optimizers, especially in a high-
dimensional feature space (up to 2 BLEU and 4.3
TER on average). Overall, RM achieves the best or
comparable performance according to two scoring
methods in two language pairs, with two test sets
each, in small and large feature settings. More-
over, across conditions, RM always yielded the
best combined TER-BLEU score.7
These improvements are achieved using stan-
dard, relatively small tuning sets, contrasted with
improvements involving sparse features obtained
using much larger tuning sets, on the order of
hundreds of thousands of sentences (Liang et al,
2006a; Tillmann and Zhang, 2006; Blunsom et al,
2008; Simianer et al, 2012). Since our approach
is complementary to scaling up the tuning data, in
future work we intend to combine these two meth-
ods. In future work we also intend to explore using
additional sparse features that are known to be use-
ful in translation, e.g. syntactic features explored
by Chiang et al (2008).
Finally, although motivated by statistical ma-
chine translation, RM is a gradient-based method
that can easily be applied to other problems. We
plan to investigate its utility elsewhere in NLP
(e.g. for parsing) as well as in other domains in-
volving high-dimensional structured prediction.
Acknowledgments
We would like to thank Pannaga Shivaswamy for
valuable discussions, and the anonymous review-
ers for their comments. Vladimir Eidelman is sup-
ported by a National Defense Science and Engi-
neering Graduate Fellowship. This work was also
supported in part by the BOLT program of the De-
fense Advanced Research Projects Agency, Con-
tract HR0011-12-C-0015.
References
Abishek Arun and Philipp Koehn. 2007. Online learn-
ing methods for discriminative training of phrase
based statistical machine translation. In MT Summit
XI.
Peter L. Bartlett and Shahar Mendelson. 2003.
Rademacher and gaussian complexities: risk bounds
and structural results. J. Mach. Learn. Res., 3:463?
482, March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Columbus, Ohio, June.
7We and other researchers often use 12 (TER?BLEU) as acombined SMT quality metric.
1124
Nicolo` Cesa-Bianchi, Alex Conconi, and Claudio Gen-
tile. 2005. A second-order perceptron algorithm.
SIAM J. Comput., 34(3):640?668, March.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Waikiki, Honolulu, Hawaii.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, NAACL ?09, pages 218?226.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. J. Mach. Learn.
Res., 7:551?585.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2009a. Adaptive regularization of weight vectors.
In Advances in Neural Information Processing Sys-
tems 22, pages 414?422.
Koby Crammer, Mehryar Mohri, and Fernando Pereira.
2009b. Gaussian margin machines. Journal of
Machine Learning Research - Proceedings Track,
5:105?112.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification
for text categorization. J. Mach. Learn. Res.,
98888:1891?1926, June.
Mark Dredze and Koby Crammer. 2008. Confidence-
weighted linear classification. In In ICML 08: Pro-
ceedings of the 25th international conference on
Machine learning, pages 264?271. ACM.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
George Foster and Roland Kuhn. 2009. Stabilizing
minimum error rate training. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 242?249, Athens, Greece, March. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many Rel-
evant Features. In Claire Ne?dellec and Ce?line Rou-
veirol, editors, European Conference on Machine
Learning, pages 137?142, Berlin. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, Stroudsburg, PA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163?171.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, pages
399?406.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
ACL-44, pages 761?768.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006b. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 2006 International Conference on Com-
putational Linguistics (COLING) - the Association
for Computational Linguistics (ACL).
David Mcallester and Joseph Keshet. 2011. Gener-
alization bounds and consistency for latent struc-
tural probit and ramp loss. In J. Shawe-Taylor,
1125
R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 24, pages 2205?2212.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464, Los Angeles, California.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Pannagadatta Shivaswamy and Tony Jebara. 2009a.
Structured prediction with relative margin. In In
International Conference on Machine Learning and
Applications.
Pannagadatta K Shivaswamy and Tony Jebara. 2009b.
Relative margin machines. In In Advances in Neural
Information Processing Systems 21. MIT Press.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Jeju Island, Korea, July.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ben Taskar, Simon Lacoste-Julien, and Michael I. Jor-
dan. 2006. Structured prediction, dual extragradi-
ent and bregman projections. J. Mach. Learn. Res.,
7:1627?1653, December.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In Proceedings of the 2006 International Conference
on Computational Linguistics (COLING) - the Asso-
ciation for Computational Linguistics (ACL).
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005. A
conditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first in-
ternational conference on Machine learning, ICML
?04.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic, June. Association
for Computational Linguistics.
1126
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1123?1133,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Unified Model for Soft Linguistic Reordering Constraints
in Statistical Machine Translation
Junhui Li
?
Yuval Marton
?
Philip Resnik
?
Hal Daum
?
e III
?
?
UMIACS, University of Maryland, College Park, MD
{lijunhui, resnik, hal}@umiacs.umd.edu
?
Microsoft Corp., City Center Plaza, Bellevue, WA
yumarton@microsoft.com
Abstract
This paper explores a simple and effec-
tive unified framework for incorporating
soft linguistic reordering constraints into a
hierarchical phrase-based translation sys-
tem: 1) a syntactic reordering model
that explores reorderings for context free
grammar rules; and 2) a semantic re-
ordering model that focuses on the re-
ordering of predicate-argument structures.
We develop novel features based on both
models and use them as soft constraints
to guide the translation process. Ex-
periments on Chinese-English translation
show that the reordering approach can sig-
nificantly improve a state-of-the-art hier-
archical phrase-based translation system.
However, the gain achieved by the seman-
tic reordering model is limited in the pres-
ence of the syntactic reordering model,
and we therefore provide a detailed analy-
sis of the behavior differences between the
two.
1 Introduction
Reordering models in statistical machine transla-
tion (SMT) model the word order difference when
translating from one language to another. The
popular distortion or lexicalized reordering mod-
els in phrase-based SMT make good local pre-
dictions by focusing on reordering on word level,
while the synchronous context free grammars in
hierarchical phrase-based (HPB) translation mod-
els are capable of handling non-local reordering
on the translation phrase level. However, reorder-
ing, especially without any help of external knowl-
edge, remains a great challenge because an ac-
curate reordering is usually beyond these word
level or translation phrase level reordering mod-
els? ability. In addition, often these translation
models fail to respect linguistically-motivated syn-
tax and semantics. As a result, they tend to pro-
duce translations containing both syntactic and se-
mantic reordering confusions. In this paper our
goal is to take advantage of syntactic and seman-
tic parsing to improve translation quality. Rather
than introducing reordering models on either the
word level or the translation phrase level, we pro-
pose a unified approach to modeling reordering on
the linguistic unit level, e.g., syntactic constituents
and semantic roles. The reordering unit falls into
multiple granularities, from single words to more
complex constituents and semantic roles, and of-
ten crosses translation phrases. To show the ef-
fectiveness of our reordering models, we integrate
both syntactic constituent reordering models and
semantic role reordering models into a state-of-
the-art HPB system (Chiang, 2007; Dyer et al,
2010). We further contrast it with a stronger base-
line, already including fine-grained soft syntac-
tic constraint features (Marton and Resnik, 2008;
Chiang et al, 2008). The general ideas, however,
are applicable to other translation models, e.g.,
phrase-based model, as well.
Our syntactic constituent reordering model con-
siders context free grammar (CFG) rules in the
source language and predicts the reordering of
their elements on the target side, using word align-
ment information. Due to the fact that a con-
stituent, especially a long one, usually maps into
multiple discontinuous blocks in the target lan-
guage, there is more than one way to describe the
monotonicity or swapping patterns; we therefore
design two reordering models: one is based on the
leftmost aligned target word and the other based
on the rightmost target word.
While recently there has also been some encour-
aging work on incorporating semantic structure
(or, more specifically, predicate-argument struc-
ture: PAS) reordering in SMT, it is still an open
question whether semantic structure reordering
1123
strongly overlaps with syntactic structure reorder-
ing, since the semantic structure is closely tied to
syntax. To this end, we employ the same reorder-
ing framework as syntactic constituent reordering
and focus on semantic roles in a PAS. We then an-
alyze the differences between the syntactic and se-
mantic features.
The contributions of this paper include the fol-
lowing:
? We introduce novel soft reordering con-
straints, using syntactic constituents or se-
mantic roles, composed over word alignment
information in translation rules used during
decoding time;
? We introduce a unified framework to incor-
porate syntactic and semantic reordering con-
straints;
? We provide a detailed analysis providing in-
sight into why the semantic reordering model
is significantly less effective when syntactic
reordering features are also present.
The rest of the paper is organized as follows.
Section 2 provides an overview of HPB transla-
tion model. Section 3 describes the details of our
unified reordering models. Section 4 gives our ex-
perimental results and Section 5 discusses the be-
havior difference between syntactic constituent re-
ordering and semantic role reordering. Section 6
reviews related work and, finally Section 7 con-
cludes the paper.
2 HPB Translation Model: an Overview
In HPB models (Chiang, 2007), synchronous rules
take the formX ? ??, ?,??, whereX is the non-
terminal symbol, ? and ? are strings of lexical
items and non-terminals in the source and target
side, respectively, and ? indicates the one-to-one
correspondence between non-terminals in ? and ?.
Each such rule is associated with a set of transla-
tion model features {?
i
}, such as phrase transla-
tion probability p (? | ?) and its inverse p (? | ?),
the lexical translation probability p
lex
(? | ?) and
its inverse p
lex
(? | ?), and a rule penalty that af-
fects preference for longer or shorter derivations.
Two other widely used features are a target lan-
guage model feature and a target word penalty.
Given a derivation d, its translation log-
probability is estimated as:
logP (d) ?
?
i
?
i
?
i
(d)
(1)
	 ?
PAS	 ?
A0	 ?(NP)	 ? TMP	 ?(NP)	 ? Pre	 ?(VBD)	 ? A1	 ?(NP)	 ?Applicants	 ?	 ?	 ?	 ?	 ?	 ?yesterday	 ?	 ?	 ?	 ?	 ?	 ?filled	 ?	 ?	 ?	 ?	 ?	 ?the	 ?forms	 ?
Figure 1: Example of predicate-argument struc-
ture.
where ?
i
is the corresponding weight of feature ?
i
.
See (Chiang, 2007) for more details.
3 Unified Linguistic Reordering Models
As mentioned earlier, the linguistic reordering unit
is the syntactic constituent for syntactic reorder-
ing, and the semantic role for semantic reordering.
The syntactic reordering model takes a CFG rule
(e.g., VP ? VP PP PP) and models the reorder-
ing of the constituents on the left hand side by ex-
amining their translation or visit order according
to the target language. For the semantic reorder-
ing model, it takes a PAS and models its reorder-
ing on the target side. Figure 1 shows an example
of a PAS where the predicate (Pre) has two core
arguments (A0 and A1) and one adjunct (TMP).
Note that we refer all core arguments, adjuncts,
and predicates as semantic roles; thus we say the
PAS in Figure 1 has 4 roles. According to the an-
notation principles in (Chinese) PropBank (Palmer
et al, 2005; Xue and Palmer, 2009), all the roles
in a PAS map to a corresponding constituent in the
parse tree, and these constituents (e.g., NPs and
VBD in Figure 1) do not overlap with each other.
Next, we use a CFG rule to describe our syn-
tactic reordering model. Treating the two forms
of reorderings in a unified way, the semantic re-
ordering model is obtainable by regarding a PAS
as a CFG rule and considering a semantic role as a
constituent.
Because the translation of a source constituent
might result in multiple discontinuous blocks,
there can be several ways to describe or group
the reordering patterns. Therefore, we design
two general constituent reordering sub-models.
One is based on the leftmost aligned word (left-
most reordering model) and the other is based on
the rightmost aligned word (rightmost reordering
model), as follows. Figure 2 shows the model-
ing steps for the leftmost reordering model. Fig-
ure 2(a) is an example of a CFG rule in the source
1124
	 ?
XP	 ?XP1	 ? XP2	 ? XP3	 ? XP4	 ?f3	 ?	 ?f4	 ? f5	 ?	 ? f6	 ?	 ?f7	 ? f8	 ?...	 ?
...	 ?
...	 ?
...	 ?
?	 ?	 ?	 ?e2	 ?	 ?	 ?	 ?	 ?e3	 ?	 ?	 ?	 ?e4	 ?	 ?	 ?	 ?e5	 ?	 ?	 ?	 ?e6	 ?	 ?	 ?	 ?e7	 ?	 ?	 ?	 ?e8	 ?	 ?	 ?	 ?e9	 ?	 ??	 ? XP1	 ? XP2	 ? XP3	 ? XP4	 ?e2	 ? e3	 ? e5	 ?(a)	 ?a	 ?CFG	 ?rule	 ?and	 ?its	 ?alignment	 ? (b)	 ?leftmost	 ?aligned	 ?target	 ?words	 ?
XP1	 ? XP2	 ? XP3	 ? XP4	 ?1	 ? 4	 ? 2	 ? 3	 ? XP1	 ? XP2	 ? XP3	 ? XP4	 ?DM	 ? DS	 ? M	 ?(c)	 ?visit	 ?order	 ? (d)	 ?reordering	 ?types	 ?
Figure 2: Modeling process illustration for leftmost reordering model.
parse tree and its word alignment links to the target
language. Note that constituent XP
4
, which covers
word f
8
, has no alignment. Then for each XP
i
, we
find the leftmost target word which is aligned to a
source word covered by XP
i
. Figure 2(b) shows
that the leftmost target words for XP
1
, XP
2
, and
XP
3
are e
2
, e
5
, and e
3
, respectively, while XP
4
has no aligned target word. Then we get visit
order V = {v
i
} for {XP
i
} in the transformation
from Figure 2(b) to Figure 2(c), with the follow-
ing strategies for special cases:
? if the first constituent XP
1
is unaligned, we
add a NULL word at the beginning of the tar-
get side and link XP
1
to the NULL word;
? if a constituent XP
i
(i > 1) is unaligned, we
add a link to the target word which is aligned
to XP
i?1
, e.g., XP
4
will be linked to e
3
; and
? if k constituents XP
m
1
. . .XP
m
k
(m
1
<
. . . < m
k
) are linked to the same target word,
then v
m
i
= v
m
i+1
? 1, e.g., since XP
3
and
XP
4
are both linked to e
3
, then v
3
= v
4
? 1.
Finally Figure 2(d) converts the visit order V =
{v
1
, . . . v
n
} into a sequence of leftmost reordering
types LRT = {lrt
1
, . . . , lrt
n?1
}. For every two
adjacent constituents XP
i
and XP
i+1
with corre-
sponding visit order v
i
and v
i+1
, their reordering
could be one of the following:
? Monotone (M) if v
i+1
= v
i
+ 1;
? Discontinuous Monotone (DM) if v
i+1
> v
i
+ 1;
? Swap (S) if v
i+1
= v
i
? 1;
? Discontinuous Swap (DS) if v
i+1
< v
i
? 1.
Up to this point, we have generated a se-
quence of leftmost reordering types LRT =
{lrt
1
, . . . , lrt
n?1
} for a given CFG rule cfg:
XP ? XP
1
. . .XP
n
. The leftmost reordering
model takes the following form:
score
lrt
(cfg) = P
l
(lrt
1
, . . . , lrt
n?1
| ? (cfg))
(2)
where ? (cfg) indicates the surrounding context of
the CFG. By assuming that any two reordering
types in LRT = {lrt
1
, . . . , lrt
n?1
} are indepen-
dent of each other, we reformulate Eq. 2 into:
score
lrt
(cfg) =
n?1
?
i=1
P
l
(lrt
i
| ? (cfg))
(3)
Similarly, the sequence of rightmost reordering
types RRT can be decided for a CFG rule XP ?
XP
1
. . .XP
n
.
Accordingly, for a PAS pas: PAS ? R
1
. . .R
n
,
we can obtain its sequences of leftmost and right-
most reordering types by using the same way de-
scribed above.
3.1 Probability Estimation
In order to predict either the leftmost or right-
most reordering type for two adjacent constituents,
we use a maximum entropy classifier to esti-
mate the probability of the reordering type rt ?
{M,DM,S,DS} as follows:
P (rt | ? (cfg)) =
exp (
?
k
?
k
f
k
(rt, ? (cfg)))
?
rt
?
exp (
?
k
?
k
f
i
(rt
?
, ? (cfg)))
(4)
where f
k
are binary features, ?
k
are the weights of
these features. Most of our features f
k
are syntax-
based. For XP
i
and XP
i+1
in cfg, the features
1125
#Index Feature
cf1 L(XP
i
) & L(XP
i+1
) & L(XP)
cf2
for each XP
j
(j < i)
L(XP
i
) & L(XP
i+1
) & L(XP) & L(XP
j
)
cf3
for each XP
j
(j > i+ 1)
L(XP
i
) & L(XP
i+1
) & L(XP) & L(XP
j
)
cf4 L(XP
i
) & L(XP
i+1
) & P(XP
i
)
cf5 L(XP
i
) & L(XP
i+1
) &H(XP
i
)
cf6 L(XP
i
) & L(XP
i+1
) & P(XP
i+1
)
cf7 L(XP
i
) & L(XP
i+1
) &H(XP
i+1
)
cf8 L(XP
i
) & L(XP
i+1
) & S(XP
i
)
cf9 L(XP
i
) & L(XP
i+1
) & S(XP
i+1
)
cf10 L(XP
i
) & L(XP)
cf11 L(XP
i+1
) & L(XP)
Table 1: Features adopted in the syntactic leftmost
and rightmost reordering models. L (XP) returns
the syntactic category of XP, e.g., NP, VP, PP etc.;
H (XP) returns the head word of XP; P (XP) re-
turns the POS tagger of the head word; S (XP)
returns the translation status of XP on the target
language: un. if it is untranslated; cont. if it is
a continuous block; and discont. if it maps into
multiple discontinuous blocks.
are aimed to examine which of them should be
translated first. Therefore, most features share two
common components: the syntactic categories of
XP
i
and XP
i+1
. Table 1 shows the features used in
syntactic leftmost and rightmost reordering mod-
els. Note that we use the same features for both.
Although the semantic reordering model is
structured in precisely the same way, we use dif-
ferent feature sets to predict the reordering be-
tween two semantic roles. Given the two adjacent
roles R
i
and R
i+1
in a PAS pas, Table 2 shows the
features that are used in the semantic leftmost and
rightmost reordering models.
3.2 Integrating into the HPB Model
For models with syntactic reordering, we add two
new features (i.e., one for the leftmost reorder-
ing model and the other for the rightmost reorder-
ing model) into the log-linear translation model in
Eq. 1. Unlike the conventional phrase and lexi-
cal translation features, whose values are phrase
pair-determined and thus can be calculated offline,
the value of the reordering features can only be
obtained during decoding time, and requires word
alignment information as well. Before we present
the algorithm integrating the reordering models,
we define the following functions by assuming
XP
i
and XP
i+1
are the constituent pair of interest
in CFG rule cfg, H is the translation hypothesis
and a is its word alignment:
#Index Feature
rf1
R(R
i
) &R(R
i+1
) & P(pas)
R(R
i
) &R(R
i+1
)
rf2
for each R
j
(j < i)
R(R
i
) &R(R
i+1
) &R(R
j
) & P(pas)
R(R
i
) &R(R
i+1
) &R(R
j
)
rf3
for each R
j
(j > i+ 1)
R(R
i
) &R(R
i+1
) &R(R
j
) & P(pas)
R(R
i
) &R(R
i+1
) &R(R
j
)
rf4 R(R
i
) &R(R
i+1
) & P(R
i
)
rf5 R(R
i
) &R(R
i+1
) &H(R
i
)
rf6 R(R
i
) &R(R
i+1
) & L(R
i
)
rf7 R(R
i
) &R(R
i+1
) & P(R
i+1
)
rf8 R(R
i
) &R(R
i+1
) &H(R
i+1
)
rf9 R(R
i
) &R(R
i+1
) & L(R
i+1
)
rf10 R(R
i
) &R(R
i+1
) & S(R
i
)
rf11 R(R
i
) &R(R
i+1
) & S(R
i+1
)
rf12
R(R
i
) & P(pas)
R(R
i
)
rf13
R(R
i+1
) & P(pas)
R(R
i+1
)
Table 2: Features adopted in the semantic leftmost
and rightmost reordering models. P (pas) returns
the predicate content of pas;R (R) returns the role
type of R, e.g., Pred, A0, TMP, etc. For features
rf1, rf2, rf3, rf12 and rf13, we include another ver-
sion which excludes the predicate content P(pas)
for reasons of sparsity.
? F
1
(w
1
, w
2
, XP): returns true if constituent XP is
within the span from word w
1
to w
2
; otherwise returns
false.
? F
2
(H, cfg, XP
i
, XP
i+1
) returns true if the reordering
of the pair ?XP
i
, XP
i+1
? in rule cfg has not been calcu-
lated yet; otherwise returns false.
? F
3
(H, a, XP
i
, XP
i+1
) returns the leftmost and right-
most reordering types for the constituent pair ?XP
i
,
XP
i+1
?, given alignment a, according to Section 3.
? F
4
(rt, cfg, XP
i
, XP
i+1
) returns the probability of
leftmost reordering type rt for the constituent pair
?XP
i
, XP
i+1
? in rule cfg.
? F
5
(rt, cfg, XP
i
, XP
i+1
) returns the probability of
rightmost reordering type rt for the constituent pair
?XP
i
, XP
i+1
? in rule cfg.
Algorithm 1 integrates the syntactic leftmost
and rightmost reordering models into a CKY-style
decoder whenever a new hypothesis is generated.
Given a hypothesis H with its alignment a, it tra-
verses all CFG rules in the parse tree and sees if
two adjacent constituents are conditioned to trig-
ger the reordering models (lines 2-4). For each
pair of constituents, it first extracts its leftmost and
rightmost reordering types (line 6) and then gets
their respective probabilities returned by the max-
imum entropy classifiers defined in Section 3.1
1126
Algorithm 1: Integrating the syntactic reordering models
into a CKY-style decoder
Input: Sentence f in the source language
Parse tree t of f
All CFG rules {cfg} in t
Hypothesis H spanning from word w
1
to w
2
Alignment a of H
Output: Log-Probabilities of the syntactic leftmost
and rightmost reordering models
1. set l prob = r
p
rob = 0.0
2. foreach cfg in {cfg}
3. foreach pair XP
i
and XP
i+1
in cfg
4. if F
1
(w
1
, w
2
, XP
i
) = false or
F
1
(w
1
, w
2
, XP
i+1
) = false or
F
2
(H, cfg, XP
i
, XP
i+1
) = false
5. continue
6. (l type, r type) = F
3
(H, a, XP
i
, XP
i+1
)
7. l prob += logF
4
(l type, cfg,XP
i
,XP
i+1
)
8. r prob += logF
5
(r type, cfg,XP
i
,XP
i+1
)
9. return (l prob, r prob)
(lines 7-8). Then the algorithm returns two log-
probabilities of the syntactic reordering models.
Note that Function F
1
returns true if hypothesis
H fully covers, or fully contains, constituentXP
i
,
regardless of the reordering type of XP
i
. Do not
confuse any parsing tag XP
i
with the nameless
variables X
i
in Hiero or cdec rules.
For the semantic reordering models, we also
add two new features into the log-linear transla-
tion model. To get the two semantic reordering
model feature values, we simply use Algorithm 1
and its associated functions from F
1
to F
5
replac-
ing a CFG rule cfg with a PAS pas, and a con-
stituent XP
i
with a semantic role R
i
. Algorithm 1
therefore permits a unified treatment of syntactic
and PAS-based reordering, even though it is ex-
pressed in terms of syntactic reordering here for
ease of presentation.
4 Experiments
We have presented our unified approach to in-
corporating syntactic and semantic soft reorder-
ing constraints in an HPB system. In this section,
we test its effectiveness in Chinese-English trans-
lation.
4.1 Experimental Settings
For training we use 1.6M sentence pairs of the
non-UN and non-HK Hansards portions of NIST
MT training corpora, segmented with the Stan-
ford segmenter (Tseng et al, 2005). The En-
glish data is lowercased, tokenized and aligned
with GIZA++ (Och and Ney, 2000) to obtain bidi-
rectional alignments, which are symmetrized us-
ing the grow-diag-final-and method (Koehn et al,
2003). We train a 4-gram LM on the English
side of the corpus with 600M additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We use the HPB decoder cdec (Dyer et
al., 2010), with Mr. Mira (Eidelman et al, 2013),
which is a k-best variant of MIRA (Chiang et al,
2008), to tune the parameters of the system.
We use NIST MT 06 dataset (1664 sentence
pairs) for tuning, and NIST MT 03, 05, and 08
datasets (919, 1082, and 1357 sentence pairs, re-
spectively) for evaluation.
1
We use BLEU (Pap-
ineni et al, 2002) for both tuning and evaluation.
To obtain syntactic parse trees and semantic
roles on the tuning and test datasets, we first
parse the source sentences with the Berkeley
Parser (Petrov and Klein, 2007), trained on the
Chinese Treebank 7.0 (Xue et al, 2005). We
then pass the parses to a Chinese semantic role
labeler (Li et al, 2010), trained on the Chinese
PropBank 3.0 (Xue and Palmer, 2009), to anno-
tate semantic roles for all verbal predicates (part-
of-speech tag VV, VE, or VC).
Our basic baseline system employs 19 basic
features: a language model feature, 7 transla-
tion model features, word penalty, unknown word
penalty, the glue rule, date, number and 6 pass-
through features. Our stronger baseline employs,
in addition, the fine-grained syntactic soft con-
straint features of Marton and Resnik (2008), here-
after MR08. The syntactic soft constraint features
include both MR08 exact-matching and cross-
boundary constraints (denoted XP= and XP+).
Since the syntactic parses of the tuning and test
data contain 29 types of constituent labels and 35
types of POS tags, we have 29 types of XP+ fea-
tures and 64 types of XP= features.
4.2 Model Training
To train the syntactic and semantic reordering
models, we use a gold alignment dataset.
2
It con-
tains 7,870 sentences with 191,364 Chinese words
and 261,399 English words. We first run syn-
1
http://www.itl.nist.gov/iad/mig//tests/mt
2
This dataset includes LDC2006E86, and newswire
parts of LDC2012T16, LDC2012T20, LDC2012T24, and
LDC2013T05. Indeed, the reordering models can also be
trained on the MT training data with its automatic alignment.
However, our preliminary experiments showed that the re-
ordering models trained on gold alignment yielded higher im-
provement.
1127
Reordering
Type
Syntactic Semantic
l-m r-m l-m r-m
M 73.5 80.6 63.8 67.9
DM 3.9 3.3 14.0 12.0
S 19.5 13.2 13.1 10.7
DS 3.2 3.0 9.1 9.5
#instance 199,234 66,757
Table 3: Reordering type distribution over the re-
ordering model?s training data. Hereafter, l-m and
r-m are for leftmost and rightmost, respectively.
tactic parsing and semantic role labeling on the
Chinese sentences, then train the models by us-
ing MaxEnt toolkit with L1 regularizer (Tsuruoka
et al, 2009).
3
Table 3 shows the reordering type
distribution over the training data. Interestingly,
about 17% of the syntactic instances and 16% of
the semantic instances differ in their leftmost and
rightmost reordering types, indicating that the left-
most/rightmost distinction is informative. We also
see that the number of semantic instances is about
1/3 of that of syntactic instances, but the entropy
of the semantic reordering classes is higher, indi-
cating the reordering of semantic roles is harder
than that of syntactic constituents.
A deeper examination of the reordering model?s
training data reveals that some constituent pairs
and semantic role pairs have a preference for a
specific reordering type (monotone or swap). In
order to understand how well the MR08 system
respects their reordering preference, we use the
gold alignment dataset LDC2006E86, in which
the source sentences are from the Chinese Tree-
bank, and thus both the gold parse trees and gold
predicate-argument structures are available. Ta-
ble 4 presents examples comparing the reordering
distribution between gold alignment and the out-
put of the MR08 system. For example, the first
row shows that based on the gold alignment, for
?PP,VP?, 16% are in monotone and 76% are in
swap reordering. However, our MR08 system out-
puts 46% of them in monotone and and 50% in
swap reordering. Hence, the reordering accuracy
for ?PP,VP? is 54%. Table 4 also shows that the
semantic reordering between core arguments and
predicates (e.g., ?Pred,A1?, ?A0,Pred?) has a less
ambiguous pattern than that between adjuncts and
other roles (e.g., ?LOC,Pred?, ?A0,TMP?), indicat-
ing the higher reordering flexibility of adjuncts.
3
http://www.logos.ic.i.u-tokyo.ac.jp/?tsuruoka/maxent/
Const. Pair
Gold MR08 output
M S M S acc.
PP VP 16 76 46 50 54
NP LC 26 74 58 42 50
DNP NP 24 72 78 19 39
CP NP 26 67 84 10 33
NP DEG 39 61 31 69 66
... ... ...
all 81 13 79 14 80
Role Pair
Gold MR08 output
M S M S acc.
Pred A1 84 6 82 9 72
A0 Pred 82 11 79 8 75
LOC Pred 17 30 36 25 49
A0 TMP 35 25 61 6 45
TMP Pred 30 22 49 19 43
... ... ...
all 63 13 73 9 64
Table 4: Examples of the reordering distribution
(%) of gold alignment and the MR08 system out-
put. For simplicity, we only focus on (M)onotone
and (S)wap based on leftmost reordering.
4.3 Translation Experiment Results
Our first group of experiments investigates
whether the syntactic reordering models are able
to improve translation quality in terms of BLEU.
To this end, we respectively add our syntactic re-
ordering models into both the baseline and MR08
systems. The effect is shown in the rows of ?+ syn-
reorder? in Table 5. From the table, we have the
following two observations.
? Although the HPB model is capable of
handling non-local phrase reordering using
synchronous context free grammars, both
our syntactic leftmost reordering model and
rightmost model are still able to achieve im-
provement over both the baseline and MR08.
This suggests that our syntactic reordering
features interact well with the MR08 syntac-
tic soft constraints: the XP+ and XP= fea-
tures focus on a single constituent each, while
our reordering features focus on a pair of con-
stituents each.
? There is no clear indication of whether the
leftmost reordering model works better than
the other. In addition, integrating both the
leftmost and rightmost reordering models has
limited improvement over a single reordering
model.
Our second group of experiments is to vali-
date the semantic reordering models. Results are
1128
System
Tuning Test
MT06 MT03 MT05 MT08 Avg.
Baseline 34.1 36.1 32.3 27.4 31.9
+
syn-
reorder
l-m 35.2 36.9? 33.6? 28.4? 33.0
r-m 35.2 37.2? 33.7? 28.6? 33.2
both 35.6 37.1? 33.6? 28.8? 33.1
+
sem-
reorder
l-m 34.4 36.7? 33.0? 27.8? 32.5
r-m 34.5 36.7? 33.1? 27.8? 32.5
both 34.5 37.0? 33.6? 27.7? 32.8
+syn+sem 35.5 37.3? 33.7? 29.0? 33.3
MR08 35.6 37.4 34.2 28.7 33.4
+
syn-
reorder
l-m 36.0 38.2? 35.0? 29.2? 34.1
r-m 36.0 38.1? 34.8? 29.2? 34.0
both 35.9 38.2? 35.3? 29.5? 34.3
+
sem-
reorder
l-m 35.8 37.6? 34.7? 28.7 33.7
r-m 35.8 37.4 34.5? 28.8 33.6
both 35.8 37.6? 34.7? 28.8 33.7
+syn+sem 36.1 38.4? 35.2? 29.5? 34.4
Table 5: System performance in BLEU scores.
?/?: significant over baseline or MR08 at 0.01
/ 0.05, respectively, as tested by bootstrap re-
sampling (Koehn, 2004)
shown in the rows of ?+ sem-reorder? in Table 5.
Here we observe:
? The semantic reordering models also achieve
significant gain of 0.8 BLEU on average over
the baseline system, demonstrating the ef-
fectiveness of PAS-based reordering. How-
ever, the gain diminishes to 0.3 BLEU on the
MR08 system.
? The syntactic reordering models outperform
the semantic reordering models on both the
baseline and MR08 systems.
Finally, we integrate both the syntactic and se-
mantic reordering models into the final system.
The two models collectively achieve a gain of up
to 1.4 BLEU over the baseline and 1.0 BLEU over
MR08 on average, which is shown in the rows of
?+syn+sem? in Table 5.
5 Discussion
The trend of the results, summarized as perfor-
mance gain over the baseline and MR08 systems
averaged over all test sets, is presented in Table 6.
The syntactic reordering models outperform the
semantic reordering models, and the gain achieved
by the semantic reordering models is limited in the
presence of the MR08 syntactic features. In this
section, we look at MR08 system and the systems
improving it to explore the behavior differences
between the two reordering models.
Coverage analysis: Our statistics show that
syntactic reordering features (either leftmost or
System Baseline MR08
+syn-reorder 1.2 0.9
+sem-reorder 0.8 0.3
+ both 1.4 1.0
Table 6: Performance gain in BLEU over baseline
and MR08 systems averaged over all test sets.
rightmost) are called 24 times per sentence on av-
erage. This is compared to only 9 times per sen-
tence for semantic reordering features. This is not
surprising since the semantic reordering features
are exclusively attached to predicates, and the span
set of the semantic roles is a strict subset of the
span set of the syntactic constituents; only 22% of
syntactic constituents are semantic roles. On aver-
age, a sentences has 4 PASs and each PAS contains
3 semantic roles. Of all the semantic role pairs,
44% are in the same CFG rules, indicating that this
part of semantic reordering has overlap with syn-
tactic reordering. Therefore, the PAS model has
fewer opportunities to influence reordering.
Reordering accuracy analysis: The reordering
type distribution on the reordering model training
data in Table 3 suggests that semantic reordering
is more difficult than syntactic reordering. To val-
idate this conjecture on our translation test data,
we compare the reordering performance among
the MR08 system, the improved systems and the
maximum entropy classifiers. For the test set, we
have four reference translations. We run GIZA++
on the data combination of our translation train-
ing data and test data to get the alignment for the
test data and each reference translation. Once we
have the (semi-)gold alignment, we compute the
gold reordering types between two adjacent syn-
tactic constituents or semantic roles. Then we
evaluate the automatic reordering outputs gener-
ated from both our translation systems and max-
imum entropy classifiers. Table 7 shows the ac-
curacy averaged over the four gold reordering sets
(the four reference translations). It shows that 1)
as expected, our classifiers do worse on the harder
semantic reordering prediction than syntactic re-
ordering prediction; 2) thanks to the high accu-
racy obtained by the maxent classifiers, integrat-
ing either the syntactic or the semantic reorder-
ing constraints results in better reordering perfor-
mance from both syntactic and semantic perspec-
tives; 3) in terms of the mutual impact, the syn-
tactic reordering models help improving seman-
tic reordering more than the semantic reordering
1129
System
Syntactic Semantic
l-m r-m l-m r-m
MR08 75.0 78.0 66.3 68.5
+syn-reorder 78.4 80.9 69.0 70.2
+sem-reorder 76.0 78.8 70.7 72.7
+both 78.6 81.7 70.6 72.1
Maxent Classifier 80.7 85.6 70.9 73.5
Table 7: Reordering accuracy on four gold sets.
System
Syntactic Semantic
l-m r-m l-m r-m
+syn-reorder 1.2 1.2 - -
+sem-reorder - - 0.7 0.9
+both 1.2 1.0 0.5 0.4
Table 8: Reordering feature weights.
models help improving syntactic reordering; and
4) the rightmost models have a learnability advan-
tage over the leftmost models, achieving higher
accuracy across the board.
Feature weight analysis: Table 8 shows the
syntactic and semantic reordering feature weights.
It shows that the semantic feature weights de-
crease in the presence of the syntactic features, in-
dicating that the decoder learns to trust semantic
features less in the presence of the more accurate
syntactic features. This is consistent with our ob-
servation that semantic reordering is harder than
syntactic reordering, as seen in Tables 3 and 7.
Potential improvement analysis: Table 7 also
shows that our current maximum entropy classi-
fiers have room for improvement, especially for
semantic reordering. In order to explore the error
propagation from the classifiers themselves and
explore the upper bound for improvement from the
reordering models, we perform an ?oracle? study,
letting the classifiers be aware of the ?gold? re-
ordering type between two syntactic constituents
or two semantic roles, and returning a higher prob-
ability for the gold reordering type and a smaller
one for the others (i.e., we set 0.9 for the gold
System MT 03 MT 05 MT 08 Avg.
Non-
Oracle
MR08 37.4 34.2 28.7 33.4
+syn-
reorder
38.2 35.3 29.5 34.3
+sem-
reorder
37.6 34.7 28.8 33.7
+ both 38.4 35.2 29.5 34.4
Oracle
+syn-
reorder
39.2 35.9 29.6 34.9
+sem-
reorder
37.9 34.8 28.9 33.9
+ both 39.1 36.0 29.8 35.0
Table 9: Performance (BLEU score) comparison
between non-oracle and oracle experiments.
reordering type, and let the other non-gold three
types share 0.1). Again, to get the gold reorder-
ing type, we run GIZA++ to get the alignment for
tuning/test source sentences and each of four ref-
erence translations. We report the averaged per-
formance by using the gold reordering type ex-
tracted from the four reference translations. Ta-
ble 9 compares the performance between the non-
oracle and oracle settings. We clearly see that us-
ing gold syntactic reordering types significantly
improves the performance (e.g., 34.9 vs. 33.4 on
average) and there is still some room for improve-
ment by building a better maximum entropy clas-
sifiers (e.g., 34.9 vs. 34.3). To our surprise, how-
ever, the improvement achieved by gold semantic
reordering types is still small (e.g., 33.9 vs. 33.4),
suggesting that the potential improvement of se-
mantic reordering models is much more limited.
And we again see that the improvement achieved
by semantic reordering models is limited in the
presence of the syntactic reordering models.
6 Related Work
Syntax-based reordering: Some previous work
pre-ordered words in the source sentences, so that
the word order of source and target sentences is
similar. The reordering rules were either manu-
ally designed (Collins et al, 2005; Wang et al,
2007; Xu et al, 2009; Lee et al, 2010) or auto-
matically learned (Xia and McCord, 2004; Gen-
zel, 2010; Visweswariah et al, 2010; Khalilov
and Sima?an, 2011; Lerner and Petrov, 2013), us-
ing syntactic parses. Li et al (2007) focused on
finding the n-best pre-ordered source sentences by
predicting the reordering of sibling constituents,
while Yang et al (2012) obtained word order by
using a reranking approach to reposition nodes in
syntactic parse trees. Both are close to our work;
however, our model generates reordering features
that are integrated into the log-linear translation
model during decoding.
Another approach in previous work added soft
constraints as weighted features in the SMT de-
coder to reward good reorderings and penalize bad
ones. Marton and Resnik (2008) employed soft
syntactic constraints with weighted binary features
and no MaxEnt model. They did not explicitly
target reordering (beyond applying constraints on
HPB rules). Although employing linguistically
motivated labels in SCFG is capable of captur-
ing constituent reorderings (Chiang, 2010; Mylon-
1130
akis and Sima?an, 2011), the rules are sparser than
SCFG with nameless non-terminals (i.e., Xs) and
soft constraints. Ge (2010) presented a syntax-
driven maximum entropy reordering model that
predicted the source word translation order. Gao
et al (2011) employed dependency trees to predict
the translation order of a word and its head word.
Huang et al (2013) predicted the translation order
of two source words.
4
Our work, which shares this
approach, differs from their work primarily in that
our syntactic reordering models are based on the
constituent level, rather than the word level.
Semantics-based reordering: Semantics-
based reordering has also seen an increase
in activity recently. In the pre-ordering ap-
proach, Wu et al (2011) automatically learned
pre-ordering rules from PAS. In the soft con-
straint or reordering model approach, Liu and
Gildea (2010) modeled the reordering/deletion
of source-side semantic roles in a tree-to-string
translation model. Xiong et al (2012) and Li et
al. (2013) predicted the translation order between
either two arguments or an argument and its
predicate. Instead of decomposing a PAS into
individual units, Zhai et al (2013) constructed
a classifier for each source side PAS. Finally in
the post-processing approach category, Wu and
Fung (2009) performed semantic role labeling
on translation output and reordered arguments to
maximize the cross-lingual match of the semantic
frames between the source sentence and the target
translation. To our knowledge, their semantic
reordering models were PAS-specific. In contrast,
our model is universal and can be easily adopted
to model the reordering of other linguistic units
(e.g., syntactic constituents). Moreover, we
have studied the effectiveness of the semantic
reordering model in different scenarios.
Non-syntax-based reorderings in HPB: Re-
cently we have also seen work on lexicalized re-
ordering models without syntactic information in
HPB (Setiawan et al, 2009; Huck et al, 2013;
Nguyen and Vogel, 2013). The non-syntax-
based reordering approach models the reorder-
ing of translation words/phrases while the syntax-
based approach models the reordering of syn-
tactic constituents. Although there are overlaps
between translation phrases and syntactic con-
stituents, it is reasonable to think that the two re-
4
Note that they obtained the translation order of source
word pairs by predicting the reordering of adjacent con-
stituents, which was quite close to our work.
ordering approaches can work together well and
even complement each other, as the linguistic pat-
terns they capture differ substantially. Setiawan
et al (2013) modeled the orientation decisions
between anchors and two neighboring multi-unit
chunks which might cross phrase or rule bound-
aries. Last, we also note that recent work on non-
syntax-based reorderings in (flat) phrase-based
models (Cherry, 2013; Feng et al, 2013) can also
be potentially adopted to hpb models.
7 Conclusion and Future Work
In this paper, we have presented a unified reorder-
ing framework to incorporate soft linguistic con-
straints (of syntactic or semantic nature) into the
HPB translation model. The syntactic reordering
models take CFG rules and model their reordering
on the target side, while the semantic reordering
models work with PAS. Experiments on Chinese-
English translation show that the reordering ap-
proach can significantly improve a state-of-the-art
hierarchical phrase-based translation system. We
have also discussed the differences between the
two linguistic reordering models.
There are many directions in which this work
can be continued. First, the syntactic reordering
model can be extended to model reordering among
constituents that cross CFG rules. Second, al-
though we do not see obvious gain from the se-
mantic reordering model when the syntactic model
is adopted, it might be beneficial to further jointly
consider the two reordering models, focusing on
where each one does well. Third, to better exam-
ine the overlap or synergy between our approach
and the non-syntax-based reordering approach, we
will conduct direct comparisons and combinations
with the latter.
Acknowledgments
This research was supported in part by the
BOLT program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0012-
12-C-0015. Any opinions, findings, conclusions
or recommendations expressed in this paper are
those of the authors and do not necessarily re-
flect the view of DARPA. The authors would like
to thank three anonymous reviewers for providing
helpful comments, and also acknowledge Ke Wu,
Vladimir Eidelman, Hua He, Doug Oard, Yuening
Hu, Jordan Boyd-Graber, and Jyothi Vinjumur for
useful discussions.
1131
References
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of ACL 1996, pages 310?
318.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of HLT-NAACL 2013, pages 22?31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008, pages 224?233.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL 2010,
pages 1443?1452.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL 2010 System Demonstra-
tions, pages 7?12.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. mira: Open-
source large-margin structured learning on mapre-
duce. In Proceedings of ACL 2013 System Demon-
strations, pages 199?204.
Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Proceedings of ACL
2013, pages 322?332.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hier-
archical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Niyu Ge. 2010. A direct syntax-driven reordering
model for phrase-based machine translation. In Pro-
ceedings of HLT-NAACL 2010, pages 849?857.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of COLING 2010, pages 376?
384.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In Proceedings of
EMNLP 2013, pages 556?566.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for
hierarchical machine translation. In Proceedings of
WMT 2013, pages 452?463.
Maxim Khalilov and Khalil Sima?an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proceedings of IJCNLP 2011,
pages 38?46.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent reordering and syntax models for
English-to-Japanese statistical machine translation.
In Proceedings of COLING 2010, pages 626?634.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP 2013, pages 513?523.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proceedings of ACL 2007,
pages 720?727.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings of ACL 2010, pages 1108?1117.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013.
Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In Proceedings of
HLT-NAACL 2013, pages 540?549.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
COLING 2010, pages 716?724.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT 2008, pages
1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguistic
annotations. In Proceedings of ACL 2011, pages
642?652.
ThuyLinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into a chart-
based decoder for machine translation. In Proceed-
ings of ACL 2013, pages 1587?1596.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
1132
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311?318.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007, pages 404?411.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
Proceedings of ACL-IJCNLP 2009, pages 324?332.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin
Shen. 2013. Two-neighbor orientation model with
cross-boundary global contexts. In Proceedings of
ACL 2013, pages 1264?1274.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
168?171.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of ACL-IJCNLP 2009,
pages 477?485.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of COLING 2010, pages
1119?1127.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP
2007, pages 737?745.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
HLT-NAACL 2009: short papers, pages 13?16.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of IJCNLP 2011, pages 29?
37.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of COLING 2004, pages
508?514.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of ACL 2012, pages 902?
911.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of HLT-NAACL 2009, pages 245?253.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL 2012, pages 912?920.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2013. Handling ambiguities of bilingual
predicate-argument structures for statistical machine
translation. In Proceedings of ACL 2013, pages
1127?1136.
1133
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 13?21,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Arabic Dependency Parsing
with Lexical and Inflectional Morphological Features
Yuval Marton, Nizar Habash and Owen Rambow
Center for Computational Learning Systems (CCLS)
Columbia University
{ymarton,habash,rambow}@ccls.columbia.edu
Abstract
We explore the contribution of different lexi-
cal and inflectional morphological features to
dependency parsing of Arabic, a morpholog-
ically rich language. We experiment with all
leading POS tagsets for Arabic, and introduce
a few new sets. We show that training the
parser using a simple regular expressive ex-
tension of an impoverished POS tagset with
high prediction accuracy does better than us-
ing a highly informative POS tagset with only
medium prediction accuracy, although the lat-
ter performs best on gold input. Using con-
trolled experiments, we find that definiteness
(or determiner presence), the so-called phi-
features (person, number, gender), and undi-
acritzed lemma are most helpful for Arabic
parsing on predicted input, while case and
state are most helpful on gold.
1 Introduction
Parsers need to learn the syntax of the modeled lan-
guage, in order to project structure on newly seen
sentences. Parsing model design aims to come up
with features that best help parsers to learn the syn-
tax and choose among different parses. One aspect
of syntax, which is often not explicitly modeled in
parsing, involves morphological constraints on syn-
tactic structure, such as agreement. In this paper, we
explore the role of morphological features in pars-
ing Modern Standard Arabic (MSA). For MSA, the
space of possible morphological features is fairly
large. We determine which morphological features
help and why, and we determine the upper bound for
their contribution to parsing quality.
We first present the corpus we use (?2), then rel-
evant Arabic linguistic facts (?3); we survey related
work (?4), describe our experiments (?5), and con-
clude with analysis of parsing error types (?6).
2 Corpus
We use the Columbia Arabic Treebank (CATiB)
(Habash and Roth, 2009). Specifically, we use the
portion converted from part 3 of the Penn Arabic
Treebank (PATB) (Maamouri et al, 2004) to the
CATiB format, which enriches the CATiB depen-
dency trees with full PATB morphological informa-
tion. CATiB?s dependency representation is based
on traditional Arabic grammar and emphasizes syn-
tactic case relations. It has a reduced POS tagset
(with six tags only), but a standard set of eight
dependency relations: SBJ and OBJ for subject
and (direct or indirect) object, respectively, (whether
they appear pre- or post-verbally); IDF for the idafa
(possessive) relation; MOD for most other modifica-
tions; and other less common relations that we will
not discuss here. For more information, see (Habash
and Roth, 2009). The CATiB treebank uses the word
segmentation of the PATB.1 It splits off several cat-
egories of orthographic clitics, but not the definite
article ?@ Al. In all of the experiments reported in
this paper, we use the gold segmentation. An exam-
ple CATiB dependency tree is shown in Figure 1.2
3 Relevant Linguistic Concepts
Morphemes: At a shallow level, Arabic words can
be described in terms of their morphemes. In ad-
dition to concatenative prefixes and suffixes, Ara-
1Tokenization involves further decisions on the segmented
token forms, such as spelling normalization.
2All Arabic transliterations are presented in the HSB
transliteration scheme (Habash et al, 2007).
13
Figure 1: CATiB. ?J
 	J???@ ?J. J??? @ ?

	? ?J
?
	Y?@ I. KA??@
?k. ? 	P ???K
t?ml zwj~ AlkAtb Al?ky~ fy Almktb~ AlwTny~ ?The writer?s
smart wife works at the national library.? (Annotation example)
VRB
???K t?ml
?works?
SBJ
NOM?k. ? 	P zwj~
?wife?
IDF
NOM
I. KA??@ AlkAtb
?the-writer?
MOD
NOM?J
?
	Y?@ Al?ky~
?smart?
MOD
PRT
?

	? fy
?in?
OBJ
NOM?J. J??? @ Almktb~
?library?
MOD
NOM?J
 	J???@ AlwTny~
?national?
bic has templatic morphemes called root and pat-
tern. For example, the word 	??J. KA?K
 yu+kAtib+uwn
?they correspond? has one prefix and one suffix, in
addition to a stem composed of the root H. H? k-t-b
?writing related? and the pattern 1A2i3. 3
Lexeme and Features: At a deeper level, Arabic
words can be described in terms of sets of inflec-
tional and lexical morphological features. We first
discuss lexical features. The set of word forms that
only vary inflectionally among each other is called
the lexeme. A lemma is a particular word form used
to represent, or cite, the lexeme word set. For ex-
ample, verb lemmas are third person masculine sin-
gular perfective. We explore using both diacritized
lemma, and undiacritized lemma (lmm). Just as the
lemma abstracts over inflectional morphology, the
root abstracts over both inflectional and derivational
morphology and thus provides a deeper level of lex-
ical abstraction than the lemma. The pattern feature
is the pattern of the lemma of the lexeme, not of the
word form.
The inflectional morphological features4 define
the dimensions of Arabic inflectional morphology,
or the space of variations of a particular word.
PATB-tokenized words vary along nine dimensions:
3The digits in the pattern correspond to the positions root
radicals are inserted.
4The inflectional features we use in this paper are form-
based (illusory) as opposed to functional features (Smr?, 2007).
We plan to work with functional features in the future.
GENDER and NUMBER (for nominals and verbs);
PERSON, ASPECT, VOICE and MOOD (for verbs);
and CASE, STATE, and the attached definite article
proclitic DET (for nominals). The inflectional fea-
tures abstract away from the specifics of morpheme
forms, since they can affect more than one mor-
pheme in Arabic. For example, changing the value
of the aspect feature in the example above from im-
perfective to perfective yields the word form @?J. KA?
kAtab+uwA ?they corresponded?, which differs in
terms of prefix, suffix and pattern.
Inflectional features interact with syntax in two
ways. First, there are agreement features: two
words in a sentence which are in a specific syn-
tactic configuration have the same value for a spe-
cific set of features. In MSA, we have subject-
verb agreement on PERSON, GENDER, and NUMBER
(but NUMBER only if the subject precedes the verb),
and we have noun-adjective agreement in PERSON,
NUMBER, GENDER, and DET.5 Second, morphol-
ogy can show a specific syntactic configuration on
a single word. In MSA, we have CASE and STATE
marking. Different types of dependents have differ-
ent CASE; for example, verbal subjects are always
marked NOMINATIVE. CASE and STATE are rarely
explicitly manifested in undiacritized MSA.
Lexical features do not participate in syntactic
constraints on structure as inflectional features do.
Instead, bilexical dependencies are used in parsing
to model semantic relations which often are the only
way to disambiguate among different possible syn-
tactic structures; lexical features provide a way of
reducing data sparseness through lexical abstraction.
We compare the effect on parsing of different sub-
sets of lexical and inflectional features. Our hypoth-
esis is that the inflectional features involved in agree-
ment and the lexical features help parsing.
The core POS tagsets: Words also have associ-
ated part-of-speech (POS) tags, e.g., ?verb?, which
further abstract over morphologically and syntac-
tically similar lexemes. Traditional Arabic gram-
mars often describe a very general three-way dis-
tinction into verbs, nominals and particles. In com-
parison, the tagset of the Buckwalter Morphologi-
cal Analyzer (Buckwalter, 2004) used in the PATB
has a core POS set of 44 tags (before morphologi-
5We do not explicitly address here agreement phenomena
that require more complex morpho-syntactic modeling. These
include adjectival modifiers of irrational (non-human) plural
nominals, and pre-nominal number modifiers.
14
cal extension). Henceforth, we refer to this tagset
as CORE44. Cross-linguistically, a core set con-
taining around 12 tags is often assumed, including:
noun, proper noun, verb, adjective, adverb, preposi-
tion, particles, connectives, and punctuation. Hence-
forth, we reduce CORE44 to such a tagset, and dub
it CORE12. The CATIB6 tagset can be viewed as
a further reduction, with the exception that CATIB6
contains a passive voice tag; however, it constitutes
only 0.5% of the tags in the training.
Extended POS tagsets: The notion of ?POS
tagset? in natural language processing usually does
not refer to a core set. Instead, the Penn English
Treebank (PTB) uses a set of 46 tags, including not
only the core POS, but also the complete set of mor-
phological features (this tagset is still fairly small
since English is morphologically impoverished). In
modern standard Arabic (MSA), the corresponding
type of tagset (core POS extended with a complete
description of morphology) would contain upwards
of 2,000 tags, many of which are extremely rare (in
our training corpus of about 300,000 words, we en-
counter only 430 of such POS tags with complete
morphology). Therefore, researchers have proposed
tagsets for MSA whose size is similar to that of the
English PTB tagset, as this has proven to be a use-
ful size computationally. These tagsets are hybrids
in the sense that they are neither simply the core
POS, nor the complete morphological tagset, but in-
stead they choose certain morphological features to
include along with the core POS tag.
The following are the various tagsets we compare
in this paper: (a) the core POS tagsets CORE44 and
the newly introduced CORE12; (b) CATiB treebank
tagset (CATIB6) (Habash and Roth, 2009); and its
newly introduced extension, CATIBEX, by greedy
regular expressions indicating particular morphemes
such as the prefix ?@ Al+ or the suffix 	?? +wn.6
(c) the PATB full tagset (BW), size ?2000+ (Buck-
walter, 2004); and two extensions of the PATB re-
duced tagset (PENN POS, a.k.a. RTS, size 24), both
outperforming it: (d) Kulick et al (2006)?s tagset
(KULICK), size ?43, one of whose most impor-
tant extensions is the marking of the definite arti-
cle clitic, and (e) Diab and BenAjiba (2010)?s EX-
TENDED RTS tagset (ERTS), which marks gender,
number and definiteness, size ?134; Besides using
morphological information to extend POS tagsets,
6Inspired by a similar extension in Habash and Roth (2009).
we explore using it in separate features in parsing
models. Following this exploration, we also extend
CORE12, producing (f) CORE12EX (see Section 5
for details).
4 Related Work
Much work has been done on the use of morpho-
logical features for parsing of morphologically rich
languages. Collins et al (1999) report that an op-
timal tagset for parsing Czech consists of a basic
POS tag plus a CASE feature (when applicable).
This tagset (size 58) outperforms the basic Czech
POS tagset (size 13) and the complete tagset (size
?3000+). They also report that the use of gender,
number and person features did not yield any im-
provements. We get similar results for CASE in the
gold experimental setting but not when using pre-
dicted POS tags (POS tagger output). This may be
a result of CASE tagging having a lower error rate
in Czech (5.0%) (Hajic? and Vidov?-Hladk?, 1998)
compared to Arabic (?14.0%, see Table 3). Simi-
larly, Cowan and Collins (2005) report that the use
of a subset of Spanish morphological features (num-
ber for adjectives, determiners, nouns, pronouns,
and verbs; and mode for verbs) outperforms other
combinations. Our approach is comparable to their
work in terms of its systematic exploration of the
space of morphological features. We also find that
the number feature helps for Arabic. Looking at He-
brew, a Semitic language related to Arabic, Tsarfaty
and Sima?an (2007) report that extending POS and
phrase structure tags with definiteness information
helps unlexicalized PCFG parsing.
As for work on Arabic, results have been reported
on PATB (Kulick et al, 2006; Diab, 2007), the
Prague Dependency Treebank (PADT) (Buchholz
and Marsi, 2006; Nivre, 2008) and the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009).
Besides the work we describe in ?3, Nivre (2008)
reports experiments on Arabic parsing using his
MaltParser (Nivre et al, 2007), trained on the PADT.
His results are not directly comparable to ours be-
cause of the different treebanks representations and
tokenization used, even though all our experiments
reported here were performed using the MaltParser.
Our results agree with previous published work on
Arabic and Hebrew in that marking the definite ar-
ticle is helpful for parsing. However, we go beyond
previous work in that we also extend this morpho-
logically enhanced feature set to include additional
15
lexical and inflectional morphological features. Pre-
vious work with MaltParser in Russian, Turkish and
Hindi showed gains with case but not with agree-
ment features (Nivre et al, 2008; Eryigit et al, 2008;
Nivre, 2009). Our work is the first to show gains
using agreement in MaltParser and in Arabic depen-
dency parsing.
5 Experiments
5.1 Experimental Space
We examined a large space of settings including the
following: (a) the contribution of POS tagsets to the
parsing quality, as a function of the amount of in-
formation encoded in the tagset; (b) parsing perfor-
mance on gold vs. predicted POS and morphologi-
cal feature values for all models; (c) prediction accu-
racy of each POS tagset and morphological feature;
(d) the contribution of numerous morphological fea-
tures in a controlled fashion; and (e) the contribution
of certain feature and POS tagset combinations. All
results are reported mainly in terms of labeled at-
tachment accuracy (parent word and the dependency
relation to it). Unlabeled attachment accuracy and
label accuracy are also given, space permitting.
5.2 Parser
For all experiments reported here we used the syn-
tactic dependency parser MaltParser v1.3 (Nivre,
2003; Nivre, 2008; K?bler et al, 2009) ? a
transition-based parser with an input buffer and a
stack, using SVM classifiers to predict the next
state in the parse derivation. All experiments were
done using the Nivre "eager" algorithm.7 We
trained the parser on the training portion of PATB
part 3 (Maamouri et al, 2004). We used the same
split as in Zitouni et al (2006) for dev/test, and kept
the test unseen during training.
There are five default attributes, in the MaltParser
terminology, for each token in the text: word ID
(ordinal position in the sentence), word form, POS
7Nivre (2008) reports that non-projective and pseudo-
projective algorithms outperform the "eager" projective algo-
rithm in MaltParser; however, our training data did not contain
any non-projective dependencies, so there was no point in us-
ing these algorithms. The Nivre "standard" algorithm is also
reported to do better on Arabic, but in a preliminary experimen-
tation, it did slightly worse than the "eager? one. This could
be due to high percentage of right branching (left headed struc-
tures) in our Arabic training set, an observation already noted
in Nivre (2008).
tag, head (parent word ID), and deprel (the depen-
dency relation between the current word and its par-
ent). There are default MaltParser features (in the
machine learning sense),8 which are the values of
functions over these attributes, serving as input to
the MaltParser internal classifiers. The most com-
monly used feature functions are the top of the in-
put buffer (next word to process, denoted buf[0]), or
top of the stack (denoted stk[0]); following items on
buffer or stack are also accessible (buf[1], buf[2],
stk[1], etc.). Hence MaltParser features are de-
fined as POS tag at top of the stack, word form at
top of the buffer, etc. K?bler et al (2009) de-
scribe a ?typical? MaltParser model configuration
of attributes and features.9 Starting with it, in
a series of initial controlled experiments, we set-
tled on using buf[0], buf[1], stk[0], stk[1] for the
wordform, and buf[0], buf[1], buf[2], buf[3], stk[0],
stk[1], stk[2] for the POS tag. For features of all
new MaltParser-attributes (discussed later), we used
buf[0] and stk[0]. We did not change the features
for the deprel. This new MaltParser configuration
resulted in gains of 0.3-1.1% in labeled attachment
accuracy (depending on the POS tagset) over the
default MaltParser configuration. We also exper-
imented with using normalized word forms (Alif
Maqsura conversion to Ya, and hamza removal from
each Alif ) as is common in parsing and statistical
machine translation literature. This resulted in a
small decrease in performance (0.1-0.2% in labeled
attachment accuracy). We settled on using the non-
normalized word form. All experiments reported be-
low were conducted using this new configuration.
5.3 Parsing quality as a function of POS tag
richness
We turn first to the contribution of POS information
to parsing quality, as a function of the amount of in-
formation encoded in the POS tagset. A first rough
estimation for the amount of information is the ac-
tual tagset size, as it appears in the training data.
For this purpose we compared POS tagsets based
on, or closely inspired by, previously published
work. These sets are typically morphologically-
enriched (marking the existence of a determiner in
the word, person, gender, number, etc.). The num-
8The terms ?feature? and ?attribute? are over loaded in the
literature. We use them in the linguistic sense, unless specifi-
cally noted otherwise, e.g., ?MaltParser feature(s)?.
9It is slightly different from the default configuration.
16
ber of tag types occurring in the training data fol-
low each tagset in parentheses: BW (430 tags), ERTS
(134 tags), KULICK (32 tags), and the smallest POS
tagset published: CATIB6 (6 tags). In optimal con-
ditions (using gold POS tags), the richest tagset
(BW) is indeed the best performer (84.02%), and the
poorest (CATIB6) is the worst (81.04%). Mid-size
tagsets are in the high 82%, with the notable ex-
ception of KULICK, which does better than ERTS,
in spite of having 1/4 the tagset size; moreover, it is
the best performer in unlabeled attachment accuracy
(85.98%), in spite of being less than tenth the size of
BW. Our extended mid-size tagset, CATIBEX, was a
mid-level performer as expected.
In order to control the level of morphological and
lexical information in the POS tagset, we used the
above-mentioned additional tagsets: CORE44 (40
tags), and CORE12 (12 tags). Both were also
mid-size mid-level performers (in spite of contain-
ing no morphological extension), with CORE12 do-
ing slightly better. See Table 1 columns 2-4.
5.4 Predicted POS tags
So far we discussed optimal (gold) conditions. But
in practice, POS tags are annotated by automatic tag-
gers, so parsers get predicted POS tags as input, as
opposed to gold (human-annotated) tags. The more
informative the tagset, the less accurate the tag pre-
diction might be, so the effect on overall parsing
quality is unclear. Therefore, we repeated the exper-
iments above with POS tags predicted by the Mor-
phological Analysis and Disambiguation for Arabic
(MADA) toolkit (Habash and Rambow, 2005). See
Table 1, columns 5-7. It turned out that BW, the
best gold performer, with lowest POS prediction ac-
curacy (81.8%), suffered the biggest drop (11.38%)
and was the worst performer with predicted tags.
The simplest tagset, CATIB6, and its extension, CAT-
IBEX, benefited from the highest POS prediction ac-
curacy (97.7%), and their performance suffered the
least. CATIBEX was the best performer with pre-
dicted POS tags. Performance drop and POS pre-
diction accuracy are given in columns 8 and 9, re-
spectively. Next, we augmented the parsing models
with inflectional and lexical morphological features.
5.5 Inflectional features
Experimenting with inflectional morphological fea-
tures is especially important in Arabic parsing, since
Arabic is morphologically rich. In order to further
explore the contribution of inflectional and lexical
morphological information in a controlled manner,
we focused on the best performing core POS tagset,
CORE12 as baseline; using three different setups, we
added nine morphological features, extracted from
MADA: DET, PERSON, ASPECT, VOICE, MOOD,
GENDER, NUMBER, STATE, and CASE. In setup
All , we augmented the baseline model with all nine
MADA features (as nine additional MaltParser at-
tributes); in setup Sep , we augmented the baseline
model with each of the MADA features, one at a
time, separately; and in setup Greedy , we com-
bined them in a greedy heuristic (since the entire
feature space is too vast to exhaust): starting with
the most gainful feature from Sep, adding the next
most gainful feature, keeping it as additional Malt-
Parser attribute if it helped, or discarding it other-
wise, and repeating this heuristics through the least
gainful feature. We also augmented the same base-
line CORE12 model with a manually constructed list
of surface affixes (e.g., Al+, +wn, ~) as additional
MaltParser attributes (LINGNGRAMS). This list was
also in the base of the CATIBEX extension; it is lin-
guistically informed, yet represents a simple (albeit
shallow) alternative to morphological analysis. Re-
sults are given in Table 2.
Somewhat surprisingly, setup All hurts perfor-
mance on the predicted input. This can be explained
if one examines the prediction accuracy of each fea-
ture (Table 3). Features which are not predicted
with very high accuracy, such as CASE (86.3%),
can dominate the negative contribution, even though
they are principle top contributors in optimal (gold)
conditions (see discussion below). The determiner
feature (DET), followed by the STATE (construct
state, idafa) feature, were top individual contribu-
tors in setup Sep. Adding DET and all the so-called
phi-features (PERSON, NUMBER, GENDER) in the
Greedy setup, yielded 1.43% gain over the CORE12
baseline. Adding LINGNGRAMS yielded a 1.19%
gain over the CORE12 baseline.
We repeated the same setups (All, Sep, and
Greedy) with gold POS tags, to examine the contri-
bution of the morphological features in optimal con-
ditions. Here CASE, followed by STATE and DET,
were the top contributors. Performance of CASE is
the notable difference from the predicted conditions
above. Surprisingly, only CASE and STATE helped in
the Greedy setup, although one might expect that the
phi features help too. (See lower half of Table 2).
17
Table 1: Parsing performance with each POS tagset, on gold and predicted input. labeled = labeled attachment accuracy (depen-
dency + relation). unlabeled = unlabeled attachment accuracy (dependency only). label acc = relation label prediction accuracy.
labeled diff = difference between labeled attachment accuracy on gold and predicted input. POS acc = POS tag prediction accuracy.
tagset gold predicted gold-pred. POS tagsetlabeled unlabled label acc. labeled unlabled label acc. labeled diff. acc. size
CATIB6 81.04 83.66 92.59 78.31 82.03 90.55 -2.73 97.7 6
CATIBEX 82.52 84.97 93.40 79.74 83.30 91.44 -2.78 97.7 44
CORE12 82.92 85.40 93.52 78.68 82.48 90.63 -4.24 96.3 12
CORE44 82.71 85.17 93.28 78.39 82.16 90.36 -4.32 96.1 40
ERTS 82.97 85.23 93.76 78.93 82.56 90.96 -4.04 95.5 134
KULICK 83.60 85.98 94.01 79.39 83.15 91.14 -4.21 95.7 32
BW 84.02 85.77 94.83 72.64 77.91 86.46 -11.38 81.8 430
Table 2: CORE12 POS tagset with morphological features. Left half: Using predicted POS tags. In it: Top part: Adding all
nine features to CORE12. Second part: Adding each feature separately, comparing difference from CORE12+madafeats, predicted
(second part). Third part: Greedily adding best features from third part, predicted; difference from previous successful greedy step.
Bottom part: Surface affixes (leading and trailing character n-grams). Right half: Left half repeated with gold tags.
set predicted POS and features: gold POS and features:
-up CORE12+. . . labeled diff. unlabeled CORE12+. . . labeled diff. unlabeled
A
ll (baseline repeated) 78.68 ? 82.48 (baseline repeated) 82.92 ? 85.40
+madafeats 77.91 -0.77 82.14 +madafeats 85.15 2.23 86.61
Se
p
+DET 79.82 1.14 83.18 +CASE 84.61 1.69 86.30
+STATE 79.34 0.66 82.85 +STATE 84.15 1.23 86.38
+GENDER 78.75 0.07 82.35 +DET 83.96 1.04 86.21
+PERSON 78.74 0.06 82.45 +NUMBER 83.08 0.16 85.50
+NUMBER 78.66 -0.02 82.39 +PERSON 83.07 0.15 85.41
+VOICE 78.64 -0.04 82.41 +VOICE 83.05 0.13 85.42
+ASPECT 78.60 -0.08 82.39 +MOOD 83.05 0.13 85.47
+MOOD 78.54 -0.14 82.35 +ASPECT 83.01 0.09 85.43
+CASE 75.81 -2.87 80.24 +GENDER 82.96 0.04 85.24
G
re
ed
y
+DET+STATE 79.42 -0.40 82.84 +CASE+STATE 85.37 0.76 86.88
+DET+GENDER 79.90 0.08 83.20 +CASE+STATE+DET 85.18 -0.19 86.66
+DET+GENDER+PERSON 79.94 0.04 83.21 +CASE+STATE+NUMBER 85.36 -0.01 86.87
+DET+PHI 80.11 0.17 83.29 +CASE+STATE+PERSON 85.27 -0.10 86.76
+DET+PHI+VOICE 79.96 -0.15 83.18 +CASE+STATE+VOICE 85.25 -0.12 86.76
+DET+PHI+ASPECT 80.01 -0.10 83.20 +CASE+STATE+MOOD 85.23 -0.14 86.72
+DET+PHI+MOOD 80.03 -0.08 83.21 +CASE+STATE+ASPECT 85.23 -0.14 86.78
? +CASE+STATE+GENDER 85.26 -0.11 86.75
+NGRAMSLING 79.87 1.19 83.21 +NGRAMSLING 84.02 1.10 86.16
5.6 Lexical features
Next, we experimented with adding morpholog-
ical features involving semantic abstraction to
some degree: the diacritized LEMMA (abstracting
away from inflectional information, and indicat-
ing active/passive voice due to diacritization in-
formation), the undiacritized lemma (LMM), the
ROOT (further abstraction indicating ?core? pred-
icate or action), and the PATTERN (a generally
complementary abstraction, often indicating cau-
sation and reflexiveness). We experimented with
the same setups as above: All, Sep, and Greedy.
Adding all four features yielded a minor gain in
setup All. LMM was the best single contributor
(1.05%), closely followed by ROOT (1.03%) in Sep.
CORE12+LMM+ROOT+LEMMA was the best greedy
combination (79.05%) in setup Greedy. See Table 4.
5.7 Putting it all together
We further explored whether morphological data
should be added to an Arabic parsing model as
stand-alone machine learning features, or should
they be used to enhance and extend a POS tagset.
We created a new POS tagset, CORE12EX, size
81(see bottom of Table 3), by extending the CORE12
tagset with the features that most improved the
18
CORE12 baseline: DET and the phi features. But
CORE12EX did worse than its non-extended (but
feature-enhanced) counterpart, CORE12+DET+PHI.
Another variant, CORE12EX+DET+PHI, which
used both the extended tagset and the additional
DET and phi features, did not improve over
CORE12+DET+PHI either.
Following the results in Table 2, we added
the affix features NGRAMSLING (which proved
to help the CORE12 baseline) to the best aug-
mented CORE12+DET+PHI model, dubbing the new
model CORE12+DET+PHI+NGRAMSLING, but per-
formance dropped here too. We greedily augmented
CORE12+DET+PHI with lexical features, and found
that the undiacritzed lemma (LMM) improved per-
formance on predicted input (80.23%). In order to
test whether these findings hold with other tagsets,
we added the winning features (DET+PHI, with and
without LMM) to the best POS tagset in predicted
conditions, CATIBEX. Both variants yielded gains,
with CATIBEX+DET+PHI+LMM achieving 80.45%
accuracy, the best result on predicted input.
5.8 Validating Results on Unseen Test Set
Once experiments on the development set (PATB3-
DEV) were done, we ran the best performing mod-
els on a previously unseen test set ? the test split of
part 3 of the PATB (PATB3-TEST). Table 6 shows
that the same trends held on this set too, with even
greater relative gains, up to 1.77% absolute gains.
Table 3: Feature prediction accuracy and set sizes. * = The set
includes a "N/A" value.
feature acc set size
normalized word form (A,Y) 99.3 29737
non-normalized word form 98.9 29980
NGRAMSLING preffix 100.0 8
NGRAMSLING suffix 100.0 20
DET 99.6 3*
PERSON 99.1 4*
ASPECT 99.1 5*
VOICE 98.9 4*
MOOD 98.6 5*
GENDER 99.3 3*
NUMBER 99.5 4*
STATE 95.6 4*
CASE 86.3 5*
ROOT 98.4 9646
PATTERN 97.0 338
LEMMA (diacritized) 96.7 16837
LMM (undiacritized lemma) 98.3 15305
CORE12EX 96.0 81
Table 4: Lexical morpho-semantic features. Top part: Adding
each feature separately; difference from CORE12, predicted.
Bottom part: Greedily adding best features from previous part,
predicted; difference from previous successful greedy step.
POS tagset labeled diff. unlab. label
A
ll
CORE12 (repeated) 78.68 ? 82.48 90.63
CORE12+LMM+ROOT
+LEMMA+PATTERN
78.85 0.17 82.46 90.82
Se
p
CORE12+lmm 78.96 1.05 82.54 90.80
CORE12+ROOT 78.94 1.03 82.64 90.72
CORE12+LEMMA 78.80 0.89 82.42 90.71
CORE12+PATTERN 78.59 0.68 82.39 90.60
G
re
ed
y
CORE12+LMM+ROOT 79.04 0.08 82.63 90.86
CORE12+LMM+ROOT
+LEMMA
79.05 0.01 82.63 90.87
CORE12+LMM+ROOT
+PATTERN
78.93 -0.11 82.58 90.82
Table 6: Results on PATB3-TEST for models which performed
best on PATB3-DEV ? predicted input.
POS tagset labeled diff. unlab. label
CORE12 77.29 ? 81.04 90.05
CORE12+DET+PHI 78.57 1.28 81.66 91.09
CORE12+DET+PHI+LMM 79.06 1.77 82.07 91.37
6 Error Analysis
For selected feature sets, we look at the overall er-
ror reduction with respect to the CORE12 baseline,
and see what dependency relations particularly profit
from that feature combination: What dependencies
achieve error reductions greater than the average er-
ror reduction for that feature set over the whole cor-
pus. We investigate dependencies by labels, and for
MOD we also investigate by the POS label of the de-
pendent node (so MOD-P means a preposition node
attached to a governing node using a MOD arc).
DET: As expected, it particularly helps IDF and
MOD-N. The error reduction for IDF is 19.3%!
STATE: Contrary to na?ve expectations, STATE
does not help IDF, but instead increases error by
9.4%. This is presumably because the feature does
not actually predict construct state except when con-
struct state is marked explicitly, but this is rare.
DET+PHI: The phi features are the only subject-
verb agreement features, and they are additional
agreement features (in addition to definiteness) for
noun-noun modification. Indeed, relative to just
adding DET, we see the strongest increases in these
two dependencies, with an additional average in-
19
Table 5: Putting it all together
POS tagset inp.qual. labeled diff. unlabeled label Acc.
CORE12+DET+PHI (repeated) predicted 80.11 0.17 83.29 91.82
CORE12+DET+PHI gold 84.20 -0.95 86.23 94.49
CORE12EX predicted 78.89 -1.22 82.38 91.17
CORE12EX gold 83.06 0.14 85.26 93.80
CORE12EX+DET+PHI predicted 79.19 -0.92 82.52 91.39
CORE12+DET+PHI+NGRAMSLING predicted 79.77 -0.34 83.03 91.66
CORE12+DET+PHI+LMM predicted 80.23 0.12 83.34 91.94
CORE12+DET+PHI+LMM+ROOT predicted 80.10 -0.13 83.25 91.84
CORE12+DET+PHI+LMM+PATTERN predicted 80.03 -0.20 83.15 91.77
CATIBEX+DET+PHI predicted 80.00 0.26 83.29 91.81
CATIBEX+DET+PHI+LMM predicted 80.45 0.71 83.65 92.03
crease for IDF (presumably because certain N-N
modifications are rejected in favor of IDFs). All
other dependencies remain at the same level as with
only DET.
LMM, ROOT, LEMMA: These features abstract
over the word form and thus allow generalizations in
bilexical dependecies, which in parsing stand in for
semantic modeling. The strongest boost from these
features comes from MOD-N and MOD-P, which
is as expected since these dependencies are highly
ambiguous, and MOD-P is never helped by the mor-
phological features.
DET+PHI+LMM: This feature combination yields
gains on all main dependency types (SBJ, OBJ,
IDF, MOD-N, MOD-P, MOD-V). But the contri-
bution from the inflectional and lexical features are
unfortunately not additive. We also compare the im-
provement contributed just by LMM as compared to
DET and PHI. This improvement is quite small, but
we see that MOD-N does not improve (in fact, it
gets worse ? presumably because there are too many
features), while MOD-P (which is not helped by the
morphological features) does improve. Oddly, OBJ
also improves, for which we have no explanation.
When we turn to our best-performing configura-
tion, CATIBEX with the added DET, phi features
(PERSON, NUMBER, GENDER), and LMM, we see
that this configuration improves over CORE12 with
the same features for two dependency types only:
SBJ and MOD-N These are exactly the two types
for which agreement features are useful, and both
the features DET+PHI and the CATIBEX POS tagset
represent information for agreement. The question
arises why this information is not redundant. We
speculate that the fact that we are learning differ-
ent classifiers for different POS tags helps Malt-
Parser learn attachment decisions which are specific
to types of dependent node morphology.
In summary, our best performing configuration
yields an error reduction of 8.3% over the core POS
tag (CORE12). SBJ errors are reduced by 13.3%,
IDF errors by 17.7%, and MOD-N errors by 14.9%.
Error reduction for OBJ, MOD-P, and MOD-V are
all less than 4%. We note that the remaining MOD-
P errors make up 6.2% of all dependency relations,
roughly one third of remaining errors.
7 Conclusions and Future Work
We explored the contribution of different inflec-
tional and lexical features to dependency parsing of
Arabic, under gold and predicted POS conditions.
While more informative features (e.g., richer POS
tags) yield better parsing quality in gold conditions,
they are hard to predict, and as such they might not
contribute to ? and even hurt ? the parsing quality
under predicted conditions. We find that definiteness
(DET), phi-features (PERSON, NUMBER, GENDER),
and undiacritzed lemma (LMM) are most helpful for
Arabic parsing on predicted input, while CASE and
STATE are most helpful on gold.
In the future we plan to improve CASE prediction
accuracy; produce high accuracy supertag features,
modeling active and passive valency; and use other
parsers (e.g., McDonald and Pereira, 2006).
Acknowledgments
This work was supported by the DARPAGALE program,
contract HR0011-08-C-0110. We thank Joakim Nivre
for his useful remarks, and Ryan Roth for his help with
CATiB conversion and MADA.
20
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of Computational Natural Language
Learning (CoNLL), pages 149?164.
Timothy A. Buckwalter. 2004. Buckwalter Arabic Mor-
phological Analyzer Version 2.0. Linguistic Data
Consortium, University of Pennsylvania, 2002. LDC
Cat alog No.: LDC2004L02, ISBN 1-58563-324-0.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th Annual Meeting
of the the Association for Computational Linguistics
(ACL), College Park, Maryland, USA, June.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of spanish. In
Proceedings of Human Language Technology (HLT)
and the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 795?802.
Mona Diab and Yassine BenAjiba. 2010. From raw text
to base phrase chunks: The new generation of AMIRA
Tools for the processing of Modern Standard Arabic.
In (to appear). Spring LNCS, Special Jubilee edition.
Mona Diab. 2007. Towards an optimal pos tag set for
modern standard arabic processing. In Proceedings
of Recent Advances in Natural Language Processing
(RANLP), Borovets, Bulgaria.
G?lsen Eryigit, Joakim Nivre, and Kemal Oflazer. 2008.
Dependency parsing of turkish. Computational Lin-
guistics, 34(3):357?389.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings
of the 43rd Annual Meeting of the the Association for
Computational Linguistics (ACL), Ann Arbor, Michi-
gan, June.
Nizar Habash and Ryan Roth. 2009. Catib: The
columbia arabic treebank. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 221?
224, Suntec, Singapore, August.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Jan Hajic? and Barbora Vidov?-Hladk?. 1998. Tag-
ging Inflective Languages: Prediction of Morpholog-
ical Categories for a Rich, Structured Tagset. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING)- the Association for
Computational Linguistics (ACL), pages 483?490.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan and Claypool
Publishers.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and improve-
ments. In Proceedings of the Treebanks and Linguis-
tic Theories Conference, pages 31?42, Prague, Czech
Republic.
Mohamed Maamouri, Ann Bies, Timothy A. Buckwalter,
andWigdanMekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
Proceedings of the NEMLAR Conference on Arabic
Language Resources and Tools, pages 102?109, Cairo,
Egypt.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
the the European Chapter of the Association for Com-
putational Linguistics (EACL).
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre, Igor M. Boguslavsky, and Leonid K.
Iomdin. 2008. Parsing the SynTagRus Treebank of
Russian. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING),
pages 641?648.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Conference on Parsing Technologies
(IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguis-
tics, 34(4).
Joakim Nivre. 2009. Parsing Indian languages with
MaltParser. In Proceedings of the ICON09 NLP Tools
Contest: Indian Language Dependency Parsing, pages
12?18.
Otakar Smr?. 2007. Functional Arabic Morphology. For-
mal System and Implementation. Ph.D. thesis, Charles
University, Prague.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167, Morristown, NJ, USA.
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006. Maximum Entropy Based Restoration of Ara-
bic Diacritics. In Proceedings of the 21st International
Conference on Computational Linguistics (COLING)
and the 44th Annual Meeting of the the Association
for Computational Linguistics (ACL), pages 577?584,
Sydney, Australia.
21
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 237?249,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Filtering Antonymous, Trend-Contrasting, and Polarity-Dissimilar
Distributional Paraphrases for Improving Statistical Machine Translation
Yuval Marton
T.J. Watson Research Center
IBM
yymarton@us.ibm.com
Ahmed El Kholy and Nizar Habash
Center for Computational Learning Systems
Columbia University
{akholy,habash}@ccls.columbia.edu
Abstract
Paraphrases are useful for statistical machine
translation (SMT) and natural language pro-
cessing tasks. Distributional paraphrase gen-
eration is independent of parallel texts and
syntactic parses, and hence is suitable also
for resource-poor languages, but tends to erro-
neously rank antonyms, trend-contrasting, and
polarity-dissimilar candidates as good para-
phrases. We present here a novel method
for improving distributional paraphrasing by
filtering out such candidates. We evalu-
ate it in simulated low and mid-resourced
SMT tasks, translating from English to two
quite different languages. We show statisti-
cally significant gains in English-to-Chinese
translation quality, up to 1 BLEU from non-
filtered paraphrase-augmented models (1.6
BLEU from baseline). We also show that
yielding gains in translation to Arabic, a mor-
phologically rich language, is not straightfor-
ward.
1 Introduction
Paraphrase recognition and generation has proven
useful for various natural language processing
(NLP) tasks, including statistical machine transla-
tion (SMT), information retrieval, query expansion,
document summarization, and natural language gen-
eration. We concentrate here on phrase-level (as
opposed to sentence-level) paraphrasing for SMT.
Paraphrasing is useful for SMT as it increases trans-
lation coverage ? a persistent problem, even in large-
scale systems. Two common approaches are ?pivot?
and distributional paraphrasing. Pivot paraphrasing
translates phrases of interest to other languages and
back (Callison-Burch et al, 2006; Callison-Burch,
2008). It relies on parallel texts (or translation
phrase tables) in various languages, which are typ-
ically scarce, and hence limit its applicability. Dis-
tributional paraphrasing (Marton et al, 2009) gener-
ates paraphrases using a distributional semantic dis-
tance measure computed over a large monolingual
corpus.1 Monolingual corpora are relatively easy
and inexpensive to collect, but distributional seman-
tic distance measures are known to rank antonymous
and polarity-dissimilar phrasal candidates high. We
therefore attempt to identify and filter out such ill-
suited paraphrase candidates.
A phrase pair may have a varying degree of
antonymy, beyond the better-known complete op-
posites (hot / cold) and contradictions (did / did
not), e.g., weaker contrasts (hot / cool), contrast-
ing trends (covered / reduced coverage), or senti-
ment polarity (happy / sad). Information extrac-
tion, opinion mining and sentiment analysis litera-
ture has been grappling with identifying such pairs
(Pang and Lee, 2008), e.g., in order to distinguish
positive and negative reviews or comments, or to de-
tect contradictions (Marneffe et al, 2008; Voorhees,
2008). We transfer some of the insights, data and
techniques to the area of paraphrasing and SMT. We
distributionally expand a small seed set of antonyms
in an unsupervised manner, following Mohammad
et al (2008). We then present a method for fil-
tering antonymous and polarity-dissimilar distribu-
tional paraphrases using the expanded antonymous
list and a list of negators (e.g., cannot) and trend-
decreasing words (reduced). We evaluate the im-
pact of our approach in a SMT setting, where non-
1Other variants use a lexical resource in conjunction with
the monolingual corpus (Mirkin et al, 2009; Marton, 2010).
237
baseline translation models are augmented with dis-
tributional paraphrases. We show gains of up to
1 BLEU relative to non-filtered models (1.6 BLEU
from non-augmented baselines) in English-Chinese
models trained on small and medium-large size data,
but lower to no gains in English-Arabic. The small
training size simulates resource-poor languages.
The rest of this paper is organized as follows:
We describe distributional paraphrase generation in
Section 2, antonym discovery in Section 3, and
paraphrase-augmented SMT in Section 4. We then
report experimental results in Section 5, and discuss
the implications in Section 6. We survey related
work in Section 7, and conclude with future work
in Section 8.
2 Distributional Paraphrases
Our method improves on the method presented in
Marton et al (2009). Using a non-annotated mono-
lingual corpus, our method constructs distributional
profiles (DP; a.k.a. context vectors) of focal words
or phrases. Each DPphr is a vector containing log-
likelihood ratios of the focal phrase phr and each
word w in the corpus. Given a paraphrase candidate
phrase cand, the semantic distance between phr and
cand is calculated using the cosine of their respec-
tive DPs (McDonald, 2000). For details on DPs and
distributional measures, see Weeds et al (2004) and
Turney and Pantel (2010).
The search of the corpus for paraphrase candi-
dates is performed in the following manner:
1. For each focal phrase phr, build distributional
profile DPphr.
2. Gather contexts: for each occurrence of phr,
keep surrounding (left and right) context L R.
3. For each such context, gather paraphrase can-
didates cand which occur between L and R in
other locations in the training corpus, i.e., all
cand such that L cand R occur in the corpus.
4. For each candidate cand, build a profile
DPcand and measure profile similarity between
DPcand and DPphr.
5. Rank all cand according to the profile similar-
ity score.
6. Filter out every candidate cand that textually
entails phr: This is approximated by filtering
cand if its words all appear in phr in the same
order. For example, if phr is spoken softly, then
spoken very softly would be filtered out.
7. Filter out every candidate cand that is antony-
mous to phr (See Algorithm 1 below).
8. Output k-best remaining candidates above a
certain similarity score threshold t.
Most of the steps above are similar to, and have
been elaborated in, Marton et al (2009). Due to
space limitations, we concentrate on the main novel
element here, which is the antonym filtering step,
detailed below. Antonyms (largely speaking) are op-
posites, terms that contrast in meaning, such as hot /
cold. Negators are terms such as not and lost, which
often flip the meaning of the word or phrase that fol-
lows or contains them, e.g., confidence / lost confi-
dence. Details on obtaining their definitions and on
obtaining the antonymous pair list and the negator
list are given in Section 3.
Algorithm 1 Antonymous candidate filtering
Given an antonymous pair list, a negator list, and a
phrase-paraphrase candidate (phr-cand) pair list,
for all phr-cand pairs do
for all words w in phr do
if w is also in cand, and there is a negator up
to two words before it in either phr or cand
(but not both!) then
filter out this pair
if w, ant is an antonymous pair, and ant is
in cand, and there is no negator up to two
words before w and ant, or there is such a
negator before both then
filter out this pair
3 Antonyms, Trends, Sentiment Polarity
Native speakers of a language are good at deter-
mining whether two words are antonyms (hot?cold,
ascend?descend, friend?foe) or not (penguin?clown,
cold?chilly, boat?rudder) (Cruse, 1986; Lehrer and
Lehrer, 1982; Deese, 1965). Strict antonyms apart,
there are also many word pairs that exhibit some de-
gree of contrast in meaning, for example, lukewarm?
cold, ascend?slip, and fan?enemy (Mohammad et
al., 2008). Automatically identifying such con-
trasting word pairs has many uses including detect-
ing and generating paraphrases (The lion caught
the gazel / The gazel could not escape the lion)
238
and detecting contradictions (Marneffe et al, 2008;
Voorhees, 2008) (The inhabitants of Peru are well
off / the inhabitants of Peru are poor). Of course,
such ?contradictions? may be a result of differing
sentiment, new information, non-coreferent men-
tions, or genuinely contradictory statements. Iden-
tifying paraphrases and contradictions are in turn
useful in effectively re-ranking target language hy-
potheses in machine translation, and for re-ranking
query responses in information retrieval. Identifying
contrasting word pairs (or short phrase pairs) is also
useful for detecting humor (Mihalcea and Strappar-
ava, 2005), as satire and jokes tend to have contra-
dictions and oxymorons. Lastly, it is useful to know
which words contrast a focal word, even if only to
filter them out. For example, in the automatic cre-
ation of a thesaurus it is necessary to distinguish
near-synonyms from contrasting word pairs. Distri-
butional similarity measures typically fail to do so.
Instances of strong contrast are recorded to some
extent in manually created dictionaries, but hun-
dreds of thousands of other contrasting pairs are not.
Further, antonyms can be of many kinds such as
those described in Section 3.1 below. We use the
Mohammad et al (2008) method to automatically
generate a large list of contrasting word pairs, which
are used to identify false paraphrases. Their method
is briefly described in Section 3.2.
3.1 Kinds of antonyms
Antonyms can be classified into different kinds.
A detailed description of one such classification can
be found in Cruse (1986) (Chapters 9, 10, and 11),
where the author describes complementaries (open?
shut, dead?alive), gradable adjective pairs (long?
short, slow?fast) (further classified into polar, over-
lapping, and equipollent antonyms), directional op-
posites (up?down, north?south), (further classified
into antipodals, counterparts, and reversives), re-
lational opposites (husband?wife, predator?prey),
indirect converses (give?receive, buy?pay), con-
gruence variants (huge?little, doctor?patient), and
pseudo opposites (black?white). It should be
noted, however, that even though contrasting word
pairs and antonyms have long been studied by
linguists, lexicographers, and others, experts do
not always agree on the scope of antonymy and
the kinds of contrasting word pairs. Some lex-
ical relations have also received attention at the
Educational Testing Services (ETS). They clas-
sify antonyms into contradictories (alive?dead,
masculine?feminine), contraries (old?young, happy-
sad), reverses (attack?defend, buy?sell), direction-
als (front?back, left?right), incompatibles (happy?
morbid, frank?hypocritical), asymmetric contraries
(hot?cool, dry?moist), pseudoantonyms (popular?
shy, right?bad), and defectives (default?payment,
limp?walk) (Bejar et al, 1991).
As mentioned earlier, in addition to antonyms,
there are other meaning-contrasting phenomena, or
other ways to classify them, such as contrasting
trends and sentiment polarity. They all may have
varying degrees of contrast in meaning. Hereafter
we sometime broadly refer to all of these as antony-
mous phrases. The antonymous phrase pair genera-
tion algorithm that we use here does not employ any
antonym-subclass-specific techniques.
3.2 Detecting antonyms
Mohammad et al (2008) used a Roget-like the-
saurus, co-occurrence statistics, and a seed set of
antonyms to identify the degree of antonymy be-
tween two words, and generate a list of antony-
mous words. The thesaurus divides the vocabulary
into about a thousand coarse categories. Each cat-
egory has, on average, about a hundred closely re-
lated words. (A word with more than one sense,
is listed in more than one category.) Mohammad
et al first determine pairs of thesaurus categories
that are contrasting in meaning. A category pair
is said to be contrasting if it has a seed antonym
pair. A list of seed antonyms is compiled using 16
affix patterns such as X and unX (clear?unclear)
and X and disX (honest?dishonest). Once a con-
trasting category pair is identified, all the word pairs
across the two categories are considered to have con-
trasting meaning. The strength of co-occurrence
(as measured by pointwise mutual information) be-
tween two contrasting word pairs is taken to be the
degree of antonymy. This is based on the distri-
butional hypothesis of antonyms, which states that
antonymous pairs tend to co-occur in text more of-
ten than chance. Co-occurrence counts are made
from the British National Corpus (BNC) (Burnard,
2000). The approach attains more than 80% accu-
racy on GRE-style closest opposite questions.
239
3.3 Detecting negators
The General Inquirer (GI) (Stone et al, 1966) has
11,788 words labeled with 182 categories of word
tags, such as positive and negative semantic orien-
tation, pleasure, pain, and so on.2 Two of the GI
categories, NOTLW and DECREAS, contain terms
that negate the meaning of what follows (Choi and
Cardie, 2008; Kennedy and Inkpen, 2005). These
terms (with limited added inflection variation) form
our list of negators.
4 Paraphrase-Augmented SMT
Augmenting the source side of SMT phrase tables
with paraphrases of out-of-vocabulary (OOV) items
was introduced by Callison-Burch et al (2006),
and was adopted practically ?as-is? in consequent
work (Callison-Burch, 2008; Marton et al, 2009;
Marton, 2010). Given an OOV source-side phrase
f , if the translation model has a rule ?f ?, e? whose
source side is a paraphrase f ? of f , then a new rule
?f, e? is added, with an extra weighted log-linear
feature, whose value for the new rule is the similar-
ity score between f and f ? (computed as a function
of the pivot translation probabilities or the distribu-
tional semantic distance of the respective DPs). We
follow the same line here:
h(e, f) =
?
???????
???????
asim(DPf ? , If phrase table entry (e, f)
DPf ) is generated from (e, f ?)
using monolingually-
derived paraphrases.
1 Otherwise.
(1)
where the definition of asim is repeated below. As
noted in that previous work, it is possible to con-
struct a new translation rule from f to e via more
than one pair of source-side phrase and its para-
phrase; e.g., if f1 is a paraphrase of f , and so is f2,
and both f1, f2 translate to the same e, then both lead
to the construction of the new rule translating f to e,
but with potentially different feature scores. In order
to leverage on these paths and resolve feature value
conflicts, an aggregated similarity measure was ap-
plied: For each paraphrase f of source-side phrases
2http://www.wjh.harvard.edu/?inquirer
fi with similarity scores sim(fi, f),
asimi = asimi?1+(1?asimi?1) sim(fi, f) (2)
where asim0 = 0. We only augment the phrase
table with a single rule from f to e, and in it are the
feature values of the phrase fi for which sim(fi, f)
was the highest.
5 Experiment
5.1 System and Parameters
We augmented translation models with para-
phrases based on distributional semantic distance
measures, with our novel antonym-filtering, and
without it. We tested all models in English-
to-Chinese and English-to-Arabic translation, aug-
menting the models with translation rules for un-
known English phrases. We also contrasted these
models with non-augmented baseline models.
For baseline we used the phrase-based SMT sys-
tem Moses (Koehn et al, 2007), with the default
model features: 1. phrase translation probability,
2. reverse phrase translation probability, 3. lexical
translation probability, 4. reverse lexical translation
probability, 5. word penalty, 6. phrase penalty, 7. six
lexicalized reordering features, 8. distortion cost,
and 9. language model (LM) probability. We used
Giza++ (Och and Ney, 2000) for word alignment.
All features were weighted in a log-linear frame-
work (Och and Ney, 2002). Feature weights were
set with minimum error rate training (Och, 2003) on
a tuning set using BLEU (Papineni et al, 2002) as the
objective function. Test results were evaluated using
BLEU and TER (Snover et al, 2006): The higher
the BLEU score, the better the result; the lower the
TER score, the better the result. This is denoted
with BLEU? and TER? in Table 1. Statistical signif-
icance of model output differences was determined
using Koehn (2004)?s test on the objective function
(BLEU).
The paraphrase-augmented models were created
as described in Section 4. We used the same data
and parameter settings as in Marton (2010).3 We
used cosine distance over DPs of log-likelihood ra-
tios (McDonald, 2000), built with a sliding win-
3Data preprocessing and paraphrasing code slightly differ
from those used in Marton et al (2009) and Marton (2010), and
hence scores are not exactly the same across these publications.
240
dow of size ?6, a sampling threshold of 10000 oc-
currences, and a maximal paraphrase length of 6
tokens. We applied a paraphrase score threshold
t = 0.05; a dynamic context length (the short-
est non-stoplisted left context L occurring less than
512 times in the corpus, and similarly for R); para-
phrasing of OOV unigrams; filtering paraphrase can-
didates occurring less than 25 times in the corpus
(inspired by McDonald, 2000); and allowing up to
k = 100 best paraphrases per phrase. We tuned
the weights of each model (non-augmented base-
line, unigram-augmented, and unigram-augmented-
filtered) with a separate minimum error rate training.
We explored here augmenting OOV unigrams,
although our paraphrasing and antonym filtering
methods can be applied to longer n-grams with no
further modifications. However, preliminary experi-
ments showed that longer n-grams require additional
provisions in order to yield gains.
5.2 Data
In order to take advantage of the English antonym
resource, we chose English as the source language
for the translation task. We chose Chinese as
the translation target language in order to compare
with Marton (2010), and for the same reasons it was
chosen there: It is quite different from English (e.g.,
in word order), and four reference translation were
available from NIST. We chose Arabic as another
target language, because it is different from both
English and Chinese, and richer morphologically,
which introduces additional challenges.
English-Chinese: For training we used the
LDC Sinorama and FBIS tests (LDC2005T10 and
LDC2003E14), and segmented the Chinese side
with the Stanford Segmenter (Tseng et al, 2005).
After tokenization and filtering, this bitext contained
231,586 lines (6.4M + 5.1M tokens). We trained a
trigram language model on the Chinese side, with
the SRILM toolkit (Stolcke, 2002), using the mod-
ified Kneser-Ney smoothing option. We followed
the split in Marton (2010), and constructed the re-
duced set of about 29,000 sentence pairs. The pur-
pose of creating this subset model was to simulate a
resource-poor language. We trained separate trans-
lation models, using either the subset or the full-size
training dataset.
For weight tuning we used the Chinese-English
NIST MT 2005 evaluation set. In order to use it for
the reverse translation direction (English-Chinese),
we arbitrarily chose the first English reference set
as the tuning ?source?, and the Chinese source as a
single ?reference translation?. For testing we used
the English-Chinese NIST MT evaluation 2008 test
set with its four reference translations.
English-Arabic: We use an English-Arabic par-
allel corpus of about 135k sentences (4 million
words) and a subset of 30K sentences (one mil-
lion words) for the translation models? training data.
The sentences were extracted from Arabic News
(LDC2004T17), eTIRR (LDC2004E72), English
translation of Arabic Treebank (LDC2005E46),
and Ummah (LDC2004T18).4 For Arabic pre-
processing, we follow previously reported best to-
kenization scheme (TB)5 and orthographic word
normalization condition (Reduced) when translat-
ing from English to Arabic (El Kholy and Habash,
2010b). MADA (Habash and Rambow, 2005) is
used to pre-process the Arabic text for the translation
model and 5-gram language model (LM). As a post-
processing step, we jointly denormalize and deto-
kenize the text to produce the final Arabic output.
Following El Kholy and Habash (2010a), we use
their best detokenization technique, T+R+LM. The
technique crucially utilizes a lookup table (T), map-
ping tokenized forms to detokenized forms, based
on our MADA-fied LM. Alternatives are given con-
ditional probabilities, P (detokenized|tokenized).
Tokenized words absent from the tables are deto-
kenized using deterministic rules (R), as a backoff
strategy. We use a 5-gram untokenized LM and
the disambig utility in the SRILM toolkit to de-
cide among different alternatives. Word alignment
is done using GIZA++, as in English-Chinese sys-
tem. We use lemma-based alignment which consis-
tently yields superior results to surface-based align-
ment (El Kholy and Habash, 2010b). For LM, we
use 200M words from the Arabic Gigaword Corpus
(LDC2007T40) together with the Arabic side of our
training data.
All experiments were conducted using Moses
here as well. We used a maximum phrase length
4All are available from the Linguistic Data Consortium
(LDC) http://www.ldc.upenn.edu
5TB: Penn Arabic Tree Bank tokenization scheme
241
of size 8 tokens. Weight optimization was done us-
ing a set of 300 sentences from the NIST MT 2004
Arabic-English evaluation test set (MT04). The tun-
ing was based on tokenized Arabic without detok-
enization. Testing was done on the NIST Arabic-
English MT05 and MEDAR 2010 English-Arabic
four-reference evaluation sets. For both tuning on
MT04 and testing on MT05, since we need the re-
verse English-Arabic direction, we chose one En-
glish reference translation as the ?source?, and the
Arabic as a single ?reference?. We evaluated using
BLEU and TER here too.
English paraphrases: We augmented the base-
line models with paraphrases generated as described
above, using a monolingual text of over 516M to-
kens, consisting of the BNC and English Gigaword
documents from 2004 and 2008 (LDC2009T13),
pre-processed to remove punctuation and to conflate
numbers, dates, months, days of week, and alphanu-
meric tokens to their respective classes.
5.3 Results
English-Chinese: Results are given in Table 1.
Augmenting SMT phrase tables with paraphrases of
OOV unigrams resulted in gains of 0.6-0.7 BLEU
points for both subset and full models, but TER
scores were worse (higher) for the full model. Aug-
menting same models with same paraphrases filtered
for antonyms resulted in further gains of 1.6 and 1
BLEU points for both subset and full models, respec-
tively, relative to the respective baselines. The TER
scores of the antonym filtered models were also as
good or better (lower) than those of the baselines.
reduced size large size
model BLEU? TER? BLEU? TER?
baseline 15.8 69.2 21.8 63.8
aug-1gram 16.4B 68.9 22.5B 64.4
aug-1gram-ant-filt 17.4BD 68.7 22.8BD 63.7
Table 1: English-Chinese scores. B/D = statistically significant
w.r.t. (B)aseline or (D)istributional 1gram model, using Koehn
(2004)?s statistical significance.
English-Arabic: Results are given in columns 1-7
of Table 2. On the MT05 test set, the 135k-sentence
aug-1gram model outperformed its baseline in both
BLEU and TER scores. The lemmatized variants
of the scores showed higher or same gains. Since
only one entry was antonym-filtered here, we do
not provide separate scores for aug-1gram-ant-filt.
Surprisingly, for the reduced 30k models, all scores
(BLEU, TER, and even their lemmatized variants) of
the augmented 1gram model were somewhat worse
than the baseline?s, and those of the antonym-filtered
model were the worst. we also ran a 4-reference test
(Medar) to see whether the single MT05 reference
was problematic, but results were similar. We exam-
ine possible reasons for this in the next section.
6 Discussion
Filtering quality: Our filtering technique is based
on antonymous pair and negator lists that were ex-
panded distributionally from seed sets. Therefore,
they are noisy. From a small random sample (Ta-
ble 3) it seems that only about 10% of filtered cases
should not have been filtered; of the rest, 50% were
strongly antonymous, 25% mildly so, and 15% were
siblings (co-hypernyms) in a natural categorical hi-
erarchy or otherwise noisy paraphrases filtered due
to a noisy antonym pair. Negators in the unigrams?
paraphrase candidates were rare.
English-Chinese: Our paraphrase filtering tech-
nique yielded an additional 1 BLEU point gain
over the non-filtered paraphrase-augmented reduced
model (totaling 1.6 BLEU over baseline). The re-
duced and large augmented models? phrase table
size increased by about 27% and 4%, respectively ?
and antonym filtering did not change these numbers
by much (see left side of Table 4). Therefore, the dif-
ference in performance between the filtered and non-
filtered systems is unlikely to be quantitative (phrase
table size). The out of vocabulary (OOV) rate of the
29k subset model is somewhat high (see Table 4),
especially for the test set; but only after these exper-
iments were completed did we peek at the test set
for calculating these statistics, and in any case, we
should not be guided by such information in choos-
ing the test set. At first glance it may seem surpris-
ing that only 0.4% of the paraphrase candidates of
the English OOV unigrams (248 candidates) were
filtered by our procedure, and that it accounted for
as much as 1 BLEU in the reduced set. (For English-
Arabic only 0.6%, or 23 candidates, were filtered).
Leaving the estimation of antonymous phrase detec-
tion recall for the future, we note that these num-
242
BLEU Lemm. Brev. Ref/Sys TER Lemm. Unigram Lemma Match Analysis
? BLEU penal. ratio ? TER Exact Match Lemma-only Unmatchable Total
30k-sentence (1M word) training dataset models
MT05 baseline 23.6 31.3 99.2 1.008 57.6 47.3 15614 55.4% 4055 14.4% 8550 30.3% 28219
aug-1gram 23.2 30.8 99.9 1.001 58.8 48.4 15387 54.2% 4195 14.8% 8831 31.1% 28413
aug-1gram-ant-filt 23.2 30.8 99.9 1.001 58.8 48.3 15387 54.2% 4195 14.8% 8831 31.1% 28413
MEDAR baseline 13.6 18.7 93.6 1.066 67.6 61.3 4924 53.0% 1563 16.8% 2800 30.1% 9287
aug-1gram 12.9 18.3 94.2 1.060 68.9 62.3 4894 52.0% 1710 18.2% 2815 29.9% 9419
aug-1gram-ant-filt 12.9 18.3 94.2 1.060 69.0 62.3 4891 51.9% 1715 18.2% 2815 29.9% 9421
135k-sentence (4M word) training dataset models
MT05 baseline 25.8 33.5 99.2 1.008 55.7 45.3 16115 57.1% 3999 14.2% 8128 28.8% 28242
aug-1gram 26.4 34.3B 99.5 1.005 55.1 44.7 16156 57.1% 4068 14.4% 8089 28.6% 28313
aug-1gram-ant-filt 26.4 34.3B 99.5 1.005 55.0 44.6 16153 57.1% 4090 14.5% 8068 28.5% 28311
MEDAR baseline 17.1 23.1 94.7 1.054 65.1 58.6 5483 57.7% 1577 16.6% 2438 25.7% 9498
aug-1gram 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616
aug-1gram-ant-filt 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616
Table 2: English-Arabic translation scores and analysis for NIST MT05 and MEDAR test sets. B = statistically significant w.r.t.
(B)aseline using Koehn (2004)?s statistical significance test.
bers from English are not directly comparable to the
Chinese side: they relate to paraphrase candidates
and not phrase table entries; they relate to types and
not tokens; each OOV English word may translate
to one or more Chinese words, each of which may
comprise of one or more characters; and last but not
least, the BLEU score we use is character-based.
phrase ||| paraphrase ||| score comments
absence ||| occupation ||| 0.06 mild
absence ||| presence ||| 0.33 good
backwards ||| forwards ||| 0.21 good
wooden ||| plastic lawn ||| 0.12 sibling
dump ||| dispose of ||| 0.41 bad
cooler ||| warm ||| 0.45 mild
diminished ||| increased ||| 0.23 good
minor ||| serious ||| 0.42 good
relic ||| youth activist in the ||| 0.12 harmless
dive ||| rise ||| 0.15 good
argue ||| also recognize ||| 0.05 mild
bother ||| waste time ||| 0.79 bad
dive ||| climb ||| 0.17 good
moonlight ||| spring ||| 0.05 harmless
sharply ||| slightly ||| 0.60 good
substantial ||| meager ||| 0.14 good
warmer ||| cooler ||| 0.72 good
tough ||| delicate ||| 0.07 good
tiny ||| mostly muslim ||| 0.06 mild
softly ||| deep ||| 0.06 mild
Table 3: Random filtering examples
While individual unigram to 4gram scores for the
augmented models were lower than the baseline?s,
filtered model?s unigram and bigram scores were
lower or similar to the baseline?s, and their trigram
and 4gram scores were higher than the baseline?s.
We intend to further investigate the cause for this
pattern, and its effect on translation quality, with the
help of a native Chinese speaker ? and on BLEU, to-
gether with the brevity penalty ? in the future.
English-Arabic: The most striking fact is the set of
differences between the language pairs: In English-
Chinese, we see gains with distributional paraphrase
augmentation, and further gains when antonymous
and contrasting paraphrase candidates are filtered
out. But in the 30k-sentence English-Arabic models,
paraphrase augmentation actually degrades perfor-
mance, even in lemma scores. It has been observed
before that BLEU (and similarly TER) is not ideal
for evaluation of contributions of this sort (Callison-
Burch et al, 2006). Therefore we conducted both
manual and focused automatic analysis, including
OOV statistics and unigram lemma match analysis6
6Unigram lemma match analysis is a classification of all the
words in the translation hypothesis (against the translation ref-
erence) into: (a) exact match, which is equal to simple unigram
precision, (b) lemma-only match, which counts words that can
only be matched at the lemma level, and (c) unmatchable.
243
between the system output and the reference trans-
lation.
Table 4 shows that the OOV rates for English-
Arabic are lower than English-Chinese. But if they
were negligible, we would not expect to see gains
(or in fact any change) in either model size, contrary
to fact. It is interesting to point out that our trans-
lation model augmentation technique handles about
50% of the (non-digit, non-punctuation) OOV words
in all models (except for only half that in the 135k
model, which still showed gains).
Another concern is that the current maximal para-
phrase length (6 tokens) may be too far from the
paraphrasee?s length (unigram), resulting in lower
quality. However, a closer examination of the
length difference evident through the BLEU brevity
penalty and the reference:system-output length ra-
tio (columns 4-5 of Table 2), reveals that the dif-
ferences are small and inconsistent; on average, the
brevity penalty difference accounts for roughly 0.1
absolute BLEU points and 0.2 absolute lemmatized
BLEU points of the respective differences.7
Last, Modern Standard Arabic is a morphologi-
cally rich language: It has many inflected forms for
most verbs, and several inflected forms for nouns,
adjectives and other parts of speech ? and complex
syntactic agreement patterns showing these inflec-
tions. It might be the case that the inflected Arabic
LM model might not serve well the augmented mod-
els, since they include translation rules that are more
likely to be ?off? inflection-wise (e.g., showing un-
grammatical syntactic agreement or simply an ac-
ceptable choice that differs from the reference). Pre-
sumably, the smaller the training set, the larger this
problem, since there would be fewer rules and hence
smaller variety of inflected forms per similar core
meaning. The unigram lemma match analysis and
lemma scores? statistics (Table 2) support this con-
cern. In the 30k model, lemma-only match seems
to even further increase, at the expense of the exact
word-form match. Possible solutions include using
a lemma-based LM, or another LM that is adjusted
to this sort of inflection-wise ?off? text.
7These values are computed by subtracting the difference
between two BLEU scores from the difference between the same
two BLEU scores without the effect of brevity penalty (i.e., each
divided by its brevity penalty).
Error Analysis We conducted an error analysis of
our Arabic 30k system using part of the MT05 test
set. That set had 571 OOV types, out of which,
we were able to augment phrases for 196 OOV
types. The majority of OOV words were proper
nouns (67.8%), with the rest being mostly nouns, ad-
jectives and verbs (in the order of their frequency).
Among the OOVs for which we augmented phrases,
the proper noun ratio was smaller than the full set
(45.4% relative). We selected a random sample of
50 OOV words, and examined their translations in
the MT05 test set. The analysis considered all the
OOV word occurrences (96 sentences). We classi-
fied each OOV translation in the augmented system
and the augmented-filtered system as follows:
a1 correct (and in reference)
a2 correct (morphological variation)
a3 acceptable translation into a synonym
a4 acceptable translation into a hypernym
b1 wrong translation into a hypernym
b2 co-hypernym: a sibling in a psychologically
natural category hierarchy
b3 antonymous, trend-contrasting, or polarity dis-
similar meaning
c1 wrong proper-noun translation (sibling)
c2 wrong proper-noun translation (other)
d wrong translation for other reasons
Both the augmented and augmented-filtered system
had 27.1% correct cases (category a). Only one-
quarter of these were exact matches with the refer-
ence (category a1) that can be captured by BLEU.
Incorrect proper-noun translation (category c) was
the biggest error (augmented model: 33.3%, filtered
model: 37.5%); within this category, sibling mis-
translations (category c1), e.g., Buddhism is trans-
lated as Islam, were the majority (over half in aug-
mented model, and about two-thirds in the filtered
model). Proper nouns seem to be a much bigger
problem for translation into Arabic than into Chi-
nese in our sets. Category b mis-translations ap-
peared in 20.8% of the time (equally in augmented
and filtered). Almost half of these were sibling mis-
translations (category b2), e.g., diamond translated
as gold. Only two OOV translations in our sam-
ple were antonymous (category b3). It is possible,
therefore, that our Arabic sets do not give room for
our filtering method to be effective. In one case,
the verb deepen (reference translation

???

K) is mis-
244
translated as summit (

??

?). In the other case, the
adjective cool (political relations), whose reference
translation uses a figure of speech periods of tension
(QK?J? @ 	?? H@Q


	
?), is mistranslated as good (

?YJ
k. ),
which carries the opposite sentiment. The rest of
category b involve hypernyms (b1), such as trans-
lating the OOV word telecom into company (

??Q??? @).
Overall, the filtered model did not behave signifi-
cantly differently from its augmented counterpart.
Chinese-Arabic score difference: We conjecture
that another possible reason for the different score
gain patterns between the two language pairs is the
fact that in Chinese, many words that are siblings-in-
meaning share a character, which doesn?t necessar-
ily have a stand-alone meaning; therefore, character-
based BLEU was able to give credit to such para-
phrases on the Chinese side, which was not case for
the word-based BLEU on the Arabic side.
7 Related Work
This paper brings together several sub-areas:
SMT, paraphrase generation, distributional seman-
tic distance measures, and antonym-related work.
Therefore we can only briefly survey the most rel-
evant work here. Our work can be viewed as an ex-
tension of the line of research that seeks to augment
translation tables with automatically generated para-
phrases of OOV words or phrases in a fashion sim-
ilar to Section 4: Callison-Burch et al (2006) use
pivoting technique (translating to other languages
and back) in order to generate paraphrases, and the
pivot translation probability as their similarity score;
Callison-Burch (2008) filters such paraphrases using
syntactic parsing information; Marton et al (2009)
use distributional paraphrasing technique that ap-
plies distributional semantic distance measure for
the paraphrase score; Marton (2010) applies a lexi-
cal resource / corpus-based hybrid semantic distance
measure for the paraphrase score instead, approxi-
mating word senses; here, we apply a distributional
semantic distance measure that is similar to Marton
et al (2009), with the main difference being the fil-
tering of the resulting paraphrases for antonymity.
Other work on augmentating SMT: Habash and
Hu (2009) show, pivoting via a trilingual parallel
text, that using English as a pivot language be-
tween Chinese and Arabic outperforms translation
using a direct Chinese-Arabic bilingual parallel text.
Other attempts to reduce the OOV rate by augment-
ing the phrase table?s source side include Habash
(2009), providing an online tool for paraphrasing
OOV phrases by lexical and morphological expan-
sion of known phrases and dictionary terms ? and
transliteration of proper names.
Bond et al (2008) also pivot for paraphrasing.
They improve SMT coverage by using a manually
crafted monolingual HPSG grammar for generating
meaning and grammar-preserving paraphrases. This
grammar allows for certain word reordering, lexical
substitutions, contractions, and ?typo? corrections.
Onishi et al (2010), Du et al (2010), and others,
pivot-paraphrase the input, and represent the para-
phrases in a lattice format, decoding it with Moses.
Work on paraphrase generation: Barzilay and
McKeown (2001) extract paraphrases from a mono-
lingual parallel corpus, containing multiple transla-
tions of the same source. However, monolingual
parallel corpora are extremely rare and small. Dolan
et al (2004) use edit distance for paraphrasing.
Max (2009) and others take the context of the para-
phrased word?s occurrence into account. Zhao et al
(2008) apply SMT-style decoding for paraphrasing,
using several log linear weighted resources while
Zhao et al (2009) filter out paraphrase candidates
and weight paraphrase features according to the de-
sired NLP task. Chevelu et al (2009) introduce
a new paraphrase generation tool based on Monte-
Carlo sampling. Mirkin et al (2009), inter alia,
frame paraphrasing as a special, symmetrical case of
(WordNet-based) textual entailment. See Madnani
and Dorr (2010) for a good paraphrasing survey.
Work on measuring distributional semantic dis-
tance: For one survey of this rich topic, see Weeds
et al (2004) and Turney and Pantel (2010). We
use here cosine of log-likelihood ratios (McDonald,
2000). A recent paper (Kazama et al, 2010) advo-
cates a Bayesian approach, making rare terms have
lower strength of association, as a by-product of re-
lying on their probabilistic Expectation.
Work on detecting antonyms: Our work with
antonyms can be thought of as an application-based
extension of the (Mohammad et al, 2008) method.
Some of the earliest computational work in this
area is by Lin et al (2003) who used patterns
245
model e2z:29k e2z:232k e2a:30k e2a:135k
phrase table baseline vocab. (# source-side types) 13916 34825 24371 49854
phrase table entries: baseline 1996k 13045k 2606k 12344k
phrase table entries: aug-1gram 2543k 127.38% 13615k 104.37% 2635k 101.09% *12373k 100.23%
phrase table entries: aug-1gram-ant-filt 2542k 127.35% 13615k 104.37% 2635k 101.09% *12373k 100.23%
OOV types in tune (% tune types) 1097 21.58% 451 8.87% 141 7.31% 84 4.35%
OOV tokens in tune (% tune tokens) 2138 6.10% 917 2.62% 193 2.18% 115 1.30%
OOV types in test (% test types) 2473 33.59% 1227 16.66% 574 12.42% 339 7.34%
OOV tokens in test (% test tokens) 4844 10.40% 2075 4.46% 992 2.83% 544 1.55%
tune OOV token decrease in aug-1gram/ant-filt 1343 27.73% 510 24.58% 79 7.96% 28 5.15%
tune OOV type decrease in aug-1gram/ant-filt 646 58.89% 203 45.01% 60 42.55% 22 26.19%
test OOV token decrease in aug-1gram /ant-filt 2776 57.31% 996 48.00% 460 46.37% 127 23.35%
test OOV type decrease in aug-1gram/ant-filt 1394 56.37% 585 47.68% 246 42.86% 76 22.42%
Table 4: Out-of-vocabulary (OOV) word rates and phrase table sizes for all model sizes and language pairs. e2z = English-Chinese;
e2a = English-Arabic. The statistics marked with * in the top-right cell are identical, see ?5.3.
such as ?from X to Y ? and ?either X or Y ? to
distinguish between antonymous and similar word
pairs. Harabagiu et al (2006) detected antonyms
by determining if their WordNet synsets are con-
nected by the hypernymy?hyponymy links and ex-
actly one antonymy link. Turney (2008) proposed a
supervised method to solve word analogy questions
that require identifying synonyms, antonyms, hyper-
nyms, and other lexical-semantic relations between
word pairs.
8 Conclusions and Future Work
We presented here a novel method for filtering out
antonymous phrasal paraphrase candidates, adapted
from sentiment analysis literature, and tested in sim-
ulated low- and mid-resourced SMT tasks from En-
glish to two quite different languages. We used an
antonymous word pair list extracted distributionally
by extending a seed list. Then, the extended list, to-
gether with a negator list and a novel heuristic, were
used to filter out antonymous paraphrase candidates.
Finally, SMT models were augmented with the fil-
tered paraphrases, yielding English-Chinese transla-
tion improvements of up to 1 BLEU from the corre-
sponding non-filtered paraphrase-augmented model
(up to 1.6 BLEU from the corresponding baseline
model). Our method proved effective for mod-
els trained on both reduced and mid-large English-
Chinese parallel texts. The reduced models sim-
ulated ?low density? languages by limiting the
amount of the training text.
We also showed for the first time transla-
tion gains for English-Arabic with paraphrase-
augmented (non-filtered) models. However, Ara-
bic, and presumably other morphologically rich lan-
guages, may require more complex models in order
to benefit from our filtering method.
Our antonym detection and filtering method is
distributional and heuristic-based; hence it is noisy.
We suspect that OOV terms in larger models tend
to be harder to paraphrase (judging by the differ-
ence from the reduced models, and the lower OOV
rate), and also harder to filter paraphrase candidates
of (due to the lower paraphrase quality, which might
not even include sufficiently distributionally similar
candidates, antonymous or otherwise). In the future,
we intend to improve our method, so that it can be
used to improve also the quality of models trained
on even larger parallel texts.
Last, we intend to extend our method beyond un-
igrams, limit paraphrase length to the vicinity of the
paraphrasee?s length, and improve our inflected Ara-
bic generation technique, so it can handle this novel
type of augmented data well.
Acknowledgments
Part of this work was done while the first author
was at Columbia University. The second author was
funded through a Google research award. The au-
thors wish to thank Saif Mohammad for providing
his data and for useful discussion, and also thank the
anonymous reviewers for their useful feedback.
246
References
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceed-
ings of the Association for Computational Linguistics
(ACL).
Isaac I. Bejar, Roger Chaffin, and Susan Embretson.
1991. Cognitive and Psychometric Analysis of Ana-
logical Problem Solving. Springer-Verlag, New York,
NY.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving statistical machine
translation by paraphrasing the training data. In Pro-
ceedings of IWSLT, Hawai?i, USA.
Lou Burnard. 2000. Reference Guide for the British
National Corpus (World Edition). Oxford University
Computing Services.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL).
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), Waikiki,
Hawai?i.
Jonathan Chevelu, Thomas Lavergne, Yves Lepage, and
Thierry Moudenc. 2009. Introduction of a new para-
phrase generation tool based on monte-carlo sampling.
In Proceedings of the 47th Annual Meeting of the As-
sociation for Computational Linguistics (ACL) - the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (IJCNLP) Short Papers, pages
249?252, Suntec, Singapore.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP), Waikiki, Hawaii.
David A. Cruse. 1986. Lexical semantics. Cambridge
University Press.
James Deese. 1965. The structure of associations in lan-
guage and thought. The Johns Hopkins Press.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Geneva,
Switzerland.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
420?429, MIT, Massachusetts, USA.
Ahmed El Kholy and Nizar Habash. 2010a. Techniques
for Arabic Morphological Detokenization and Ortho-
graphic Denormalization. In Proceedings of the sev-
enth International Conference on Language Resources
and Evaluation (LREC), Valletta, Malta.
Ahmed El Kholy and Nizar Habash. 2010b. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In In Actes
de Traitement Automatique des Langues Naturelles
(TALN), Montreal, Canada.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese statistical machine translation using English
as pivot language. In Proceedings of the 4th EACL
Workshop on Statistical Machine Translation, pages
173?181, Athens, Greece.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 573?580, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine transla-
tion. In Proceedings of the 2nd International Con-
ference on Arabic Language Resources and Tools
(MEDAR), Cairo, Egypt.
Sanda M. Harabagiu, Andrew Hickl, and Finley Laca-
tusu. 2006. Lacatusu: Negation, contrast and contra-
diction in text processing. In Proceedings of the 23rd
National Conference on Artificial Intelligence (AAAI),
Boston, MA.
Junichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki
Murata, and Kentaro Torisawa. 2010. A Bayesian
method for robust estimation of distributional similar-
ities. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 247?256, Uppsala, Sweden.
Alistair Kennedy and Diana Inkpen. 2005. Sentiment
classification of movie and product reviews using con-
textual valence shifters. COMPUTATIONAL INTEL-
LIGENCE, pages 110?125.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
the Annual Meeting of the Association for Com-
putational Linguistics (ACL) demonstration session,
Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for
247
machine translation evaluation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Adrienne Lehrer and K. Lehrer. 1982. Antonymy. Lin-
guistics and Philosophy, 5:483?501.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI), pages 1492?1493, Acapulco, Mexico.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Columbus, OH.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation using
monolingually-derived paraphrases. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Singapore.
Yuval Marton. 2010. Improved statistical machine trans-
lation using monolingual text and a shallow lexical re-
source for hybrid phrasal paraphrase generation. In
Proceedings of the Ninth Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Denver, Colorado.
Aurelien Max. 2009. Sub-sentential paraphrasing by
contextual pivot translation. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL) - the 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing
(IJCNLP) - Workshop on Applied Textual Inference,
pages 18?26, Singapore. Suntec.
Scott McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, pages 531?538, Van-
couver, Canada.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Da-
gan, Marc Dymetman, and Idan Szpektor . 2009.
Source-language entailment modeling for translating
unknown terms. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics (ACL) - the 4th International Joint Conference
on Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (IJCNLP), pages
791?799, Suntec, Singapore.
Saif Mohammad, Bonnie Dorr, and Codie Dunn. 2008.
Computing word-pair antonymy. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 982?991, Waikiki,
Hawaii.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase lattice for statistical machine trans-
lation. In Proceedings of the Association for Computa-
tional Linguistics (ACL) Short Papers, pages 1?5, Up-
psala, Sweden.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1?2):1?135.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the ACL Human Language Technology
Conference, pages 124?127, San Diego, CA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas, pages 223?231, Cambridge, MA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Philip Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and associates. 1966. The General
Inquirer: A Computer Approach to Content Analysis.
The MIT Press.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Articial Intelligence Research, 37:141?188.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
Proceedings of the 22nd International Conference
248
on Computational Linguistics (COLING), pages 905?
912, Manchester, UK.
Ellen M Voorhees. 2008. Contradictions and jus-
tifications: Extensions to the textual entailment task.
In Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Colum-
bus, OH.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional sim-
ilarity. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING),
pages 1015?1021, Geneva, Switzerland.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008. Combining multiple resources to
improve smt-based paraphrasing model. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL)Human Language Technology (HLT), pages
1021?1029, Columbus, Ohio, USA.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) - the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing (IJCNLP), pages 834?842, Suntec,
Singapore.
249
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 86?90,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
SPMRL?13 Shared Task System:
The CADIM Arabic Dependency Parser
Yuval Marton
Microsoft Corporation
City Center Plaza
Bellevue, WA, USA
Nizar Habash, Owen Rambow
CCLS
Columbia University
New York, NY, USA
cadim@ccls.columbia.edu
Sarah Alkuhlani
CS Department
Columbia University
New York, NY, USA
Abstract
We describe the submission from the
Columbia Arabic & Dialect Modeling
group (CADIM) for the Shared Task at
the Fourth Workshop on Statistical Pars-
ing of Morphologically Rich Languages
(SPMRL?2013). We participate in the
Arabic Dependency parsing task for pre-
dicted POS tags and features. Our system
is based on Marton et al (2013).
1 Introduction
In this paper, we discuss the system that the
Columbia Arabic & Dialect Modeling group
(CADIM) submitted to the 2013 Shared Task on
Parsing Morphologically Rich Languages (Seddah
et al, 2013). We used a system for Arabic depen-
dency parsing which we had previously developed,
but retrained it on the training data splits used in this
task. We only participated in the Arabic dependency
parsing track, and in it, only optimized for predicted
(non-gold) POS tags and features.
We first summarize our previous work (Sec-
tion 2). We then discuss our submission and the re-
sults (Section 3).
2 Approach
In this section, we summarize Marton et al (2013).
We first present some background information on
Arabic morphology and then discuss our method-
ology and main results. We present our best per-
forming set of features, which we also use in our
SPMRL?2013 submission.
2.1 Background
Morphology interacts with syntax in two ways:
agreement and assignment. In agreement, there is
coordination between the morphological features of
two words in a sentence based on their syntactic
configuration (e.g., subject-verb or noun-adjective
agreement in GENDER and/or NUMBER). In as-
signment, specific morphological feature values are
assigned in certain syntactic configurations (e.g.,
CASE assignment for the subject or direct object of
a verb).
The choice of optimal linguistic features for
a parser depends on three factors: relevance,
redundancy and accuracy. A feature has rel-
evance if it is useful in making an attach-
ment (or labeling) decision. A particular fea-
ture may or may not be relevant to parsing.
For example, the GENDER feature may help
parse the Arabic phrase

?YK
Ym.
?'@/YK
Ym.?'@

?PAJ
??@ H. AK.
bAb AlsyArh? Aljdyd/Aljdydh?1 ?door the-car the-
newmasc.sg/fem.sg [lit.]? using syntactic agreement:
if the-new is masculine (Aljdyd YK
Ym.?'@), it should at-
tach to the masculine door (bAb H. AK.), resulting in
the meaning ?the car?s new door?; if the-new is fem-
inine (Aljdydh?

?YK
Ym.
?'@), it should attach to the femi-
nine the-car (AlsyArh?

?PAJ
??@), resulting in ?the door
of the new car?. In contrast, the ASPECT feature does
1Arabic orthographic transliteration is presented in the HSB
scheme (Habash et al, 2007): (in alphabetical order)
@ H.
H H h. h p X
	
XP 	P ? ? ?
	
? ?
	
? ?
	
?
	
?

? ? ? ?
	
? ? ? ?


A b t ? j H x d ? r z s ? S D T D? ? ? f q k l m n h w y
and the additional letters: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', h?

?, ? ?.
86
not constrain any syntactic decision.2 Even if rele-
vant, a feature may not necessarily contribute to op-
timal performance since it may be redundant with
other features that surpass it in relevance. For ex-
ample, the DET and STATE features alone both help
parsing because they help identify the idafa con-
struction (the modificiation of a nominal by a gen-
itive noun phrase), but they are redundant with each
other and the DET feature is more helpful since it
also helps with adjectival modification of nouns. Fi-
nally, the accuracy of automatically predicting the
feature values (ratio of correct predictions out of all
predictions) of course affects the value of a feature
on unseen text. Even if relevant and non-redundant,
a feature may be hard to predict with sufficient ac-
curacy by current technology, in which case it will
be of little or no help for parsing, even if helpful
when its gold values are provided. The CASE fea-
ture is very relevant and not redundant, but it cannot
be predicted with high accuracy and overall it is not
useful.
Different languages vary with respect to which
features may be most helpful given various tradeoffs
among these three factors. It has been shown pre-
viously that if the relevant morphological features
in assignment configurations can be recognized well
enough, then they contribute to parsing accuracy.
For example, modeling CASE in Czech improves
Czech parsing (Collins et al, 1999): CASE is rele-
vant, not redundant, and can be predicted with suf-
ficient accuracy. However, it had been more diffi-
cult showing that agreement morphology helps pars-
ing, with negative results for dependency parsing in
several languages (Nivre et al, 2008; Eryigit et al,
2008; Nivre, 2009). In contrast to these negative re-
sults, Marton et al (2013) showed positive results
for using agreement morphology for Arabic.
2.2 Methodology
In Marton et al (2013), we investigated morphologi-
cal features for dependency parsing of Modern Stan-
dard Arabic (MSA). The goal was to find a set of rel-
evant, accurate and non-redundant features. We used
both the MaltParser (Nivre, 2008) and the Easy-First
2For more information on Arabic morphology in the con-
text of natural language processing see Habash (2010). For a
detailed analysis of morpho-syntactic agreement, see Alkuhlani
and Habash (2011).
Parser (Goldberg and Elhadad, 2010). Since the
Easy-First Parser performed better, we use it in all
experiments reported in this paper.
For MSA, the space of possible morphological
features is quite large. We determined which mor-
phological features help by performing a search
through the feature space. In order to do this, we
separated part-of-speech (POS) from the morpho-
logical features. We defined a core set of 12 POS
features, and then explored combinations of mor-
phological features in addition to this POS tagset.
This core set of POS tags is similar to those pro-
posed in cross-lingual work (Rambow et al, 2006;
Petrov et al, 2012). We performed this search inde-
pendently for Gold input features and predicted in-
put features. We used our MADA+TOKAN system
(Habash and Rambow, 2005; Habash et al, 2009;
Habash et al, 2012) for the prediction. As the Easy-
First Parser predicts links separately before labels,
we first optimized for unlabeled attachment score,
and then optimized the Easy-First Parser labeler for
label score.
As had been found in previous results, assignment
features, specifically CASE and STATE, are very
helpful in MSA. However, in MSA this is true only
under gold conditions: since CASE is rarely explicit
in the typically undiacritized written MSA, it has a
dismal accuracy rate, which makes it useless when
used in machine-predicted (real, non-gold) condi-
tion. In contrast with previous results, we showed
that agreement features are quite helpful in both gold
and predicted conditions. This is likely a result of
MSA having a rich agreement system, covering both
verb-subject and noun-adjective relations.
Additionally, almost all work to date in MSA
morphological analysis and part-of-speech (POS)
tagging has concentrated on the morphemic form of
the words. However, often the functional morphol-
ogy (which is relevant to agreement, and relates to
the meaning of the word) is at odds with the ?sur-
face? (form-based) morphology; a well-known ex-
ample of this are the ?broken? (irregular) plurals
of nominals, which often have singular-form mor-
phemes but are in fact plurals and show plural agree-
ment if the referent is rational. In Marton et al
(2013), we showed that by modeling the functional
morphology rather than the form-based morphology,
we obtain a further increase in parsing performance
87
Feature Type Feature Explanation
Part-of-speech CORE12 12 tags for core parts-of-speech: verb, noun, adjective, adverb,
proper noun, pronoun, preposition, conjunction, relative pronoun,
particle, abbreviation, and punctuation
Inflectional features DET Presence of the determiner morpheme ?@ Al
PERSON 1st, 2nd, or 3rd
FN*N Functional number: singular, dual, plural
FN*G Functional gender: masculine or feminine
Lexical features FN*R Rationality: rational, irrational, ambiguous, unknown or N/A
LMM Undiacritized lemma
Table 1: Features used in the CADIM submission with the Easy-First Parser (Goldberg and Elhadad, 2010).
Training Set Test Set LAS UAS LaS
5K (SPMRL?2013) dev ? 70 81.7 84.7 92.7
All (SPMRL?2013) dev ? 70 84.8 87.4 94.2
Marton et al (2013) test (old split) ? 70 81.7 84.6 92.8
5K (SPMRL?2013) dev 81.1 84.2 92.7
All (SPMRL?2013) dev 84.0 86.6 94.1
5K (SPMRL?2013) test 80.5 83.5 92.7
All (SPMRL?2013) test 83.2 85.8 93.9
Marton et al (2013) test (old split) 81.0 84.0 92.7
Table 2: Results of our system on Shared Task test data, Gold Tokenization, Predicted Morphological Tags; and for
reference also on the data splits used in our previous work (Marton et al, 2013); ?? 70? refers to the test sentences
with 70 or fewer words.
Training Set Test Set Labeled Tedeval Score Unlabeled Tedeval Score
5K (SPMRL?2013) test ? 70 86.4 89.9
All (SPMRL?2013) test ? 70 87.8 90.8
Table 3: Results of our system on on Shared Task test data, Predicted Tokenization, Predicted Morphological Tags;
?? 70? refers to the test sentences with 70 or fewer words
(again, both when using gold and when using pre-
dicted POS and morphological features).
We also showed that for parsing with predicted
POS and morphological features, training on a com-
bination of gold and predicted POS and morpholog-
ical feature values outperforms the alternative train-
ing scenarios.
2.3 Best Performing Feature Set
The best performing set of features on non-gold in-
put, obtained in Marton et al (2013), are shown in
Table 1. The features are clustered into three types.
? First is part-of-speech, represented using a
?core? 12-tag set.
? Second are the inflectional morphological fea-
tures: determiner clitic, person and functional
gender and number.
? Third are the rationality (humanness) feature,
which participates in morphosyntactic agree-
ment in Arabic (Alkuhlani and Habash, 2011),
and a form of the lemma, which abstract over
all inflectional morphology.
For the training corpus, we use a combination of
the gold and predicted features.
88
3 Our Submission
3.1 Data Preparation
The data split used in the shared task is different
from the data split we used in (Marton et al, 2013),
so we retrained our models on the new splits (Diab
et al, 2013). The data released for the Shared Task
showed inconsistent availability of lemmas across
gold and predicted input, so we used the ALMOR
analyzer (Habash, 2007) with the SAMA databases
(Graff et al, 2009) to determine a lemma given the
word form and the provided (gold or predicted) POS
tags. In addition to the lemmas, the ALMOR an-
alyzer also provides morphological features in the
feature-value representation our approach requires.
Finally, we ran our existing converter (Alkuhlani
and Habash, 2012) over this representation to obtain
functional number and gender, as well as the ratio-
nality feature.3 For simplicity reasons, we used the
MLE:W2+CATiB model (Alkuhlani and Habash,
2012), which was the best performing model on seen
words, as opposed to the combination system that
used a syntactic component with better results on
unseen words. We did not perform Alif or Ya nor-
malization on the data.
We trained two models: one on 5,000 sentences
of training data and one on the entire training data.
3.2 Results
Our performance in the Shared Task for Arabic De-
pendency, Gold Tokenization, Predicted Tags, is
shown in Table 2. Our performance in the Shared
Task for Arabic Dependency, Predicted Tokeniza-
tion, Predicted Tags, is shown in Table 3. For
predicted tokenization, only the IMS/Szeged sys-
tem which uses system combination (Run 2) out-
performed our parser on all measures; our parser
performed better than all other single-parser sys-
tems. For gold tokenization, our system is the sec-
ond best single-parser system after the IMS/Szeged
single system (Run 1). For gold tokenization and
predicted morphology (Table 2), we also give the
performance reported in our previous work (Mar-
ton et al, 2013). The increase over the previously
3The functional feature generator of (Alkuhlani and Habash,
2012) was trained on a different training set from the parser, but
the functional feature generator was not trained on any of the
test corpus for the Shared Task.
reported work may simply be due to the different
split for training and test, but it may also be due
to improvements to the functional feature prediction
(Alkuhlani and Habash, 2012), and the predicted
features provided by the Shared Task organizers.
References
Sarah Alkuhlani and Nizar Habash. 2011. A corpus for
modeling morpho-syntactic agreement in Arabic: gen-
der, number and rationality. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics (ACL), Portland, Oregon, USA.
Sarah Alkuhlani and Nizar Habash. 2012. Identifying
broken plurals, irregular gender, and rationality in Ara-
bic text. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 675?685. Association for
Computational Linguistics.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 505?512, College Park, Maryland, USA, June.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic Treebanks and Associated
Corpora: Data Divisions Manual. Technical Report
CCLS-13-02, Center for Computational Learning Sys-
tems, Columbia University.
G?lsen Eryigit, Joakim Nivre, and Kemal Oflazer. 2008.
Dependency parsing of Turkish. Computational Lin-
guistics, 34(3):357?389.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of Human Language
Technology (HLT): the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 742?750, Los Angeles, California.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 573?580, Ann Ar-
bor, Michigan.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
89
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. The MEDAR Consortium,
April.
Nizar Habash, Owen Rambow, and Ryan Roth. 2012.
MADA+TOKAN Manual. Technical report, Techni-
cal Report CCLS-12-01, Columbia University.
Nizar Habash. 2007. Arabic Morphological Representa-
tions for Machine Translation. In Antal van den Bosch
and Abdelhadi Soudi, editors, Arabic Computational
Morphology: Knowledge-based and Empirical Meth-
ods. Kluwer/Springer.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1).
Joakim Nivre, Igor M. Boguslavsky, and Leonid K.
Iomdin. 2008. Parsing the SynTagRus Treebank of
Russian. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING),
pages 641?648.
Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguis-
tics, 34(4).
Joakim Nivre. 2009. Parsing Indian languages with
MaltParser. In Proceedings of the ICON09 NLP Tools
Contest: Indian Language Dependency Parsing, pages
12?18.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
the Conference on Language Resources and Evalua-
tion (LREC), May.
Owen Rambow, Bonnie Dorr, David Farwell, Rebecca
Green, Nizar Habash, Stephen Helmreich, Eduard
Hovy, Lori Levin, Keith J. Miller, Teruko Mitamura,
Florence Reeder, and Siddharthan Advaith. 2006. Par-
allel syntactic annotation of multiple languages. In
Proceedings of the Fifth Conference on Language Re-
sources and Evaluation (LREC), Genoa, Italy.
Djam? Seddah, Reut Tsarfaty, Sandra K?bler, Marie Can-
dito, Jinho Choi, Rich?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence
Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, Alina Wr?blewska, and Eric
Villemonte de la Cl?rgerie. 2013. Overview of the
spmrl 2013 shared task: A cross-framework evalua-
tion of parsing morphologically rich languages. In
Proceedings of the 4th Workshop on Statistical Pars-
ing of Morphologically Rich Languages: Shared Task,
Seattle, WA.
90
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
