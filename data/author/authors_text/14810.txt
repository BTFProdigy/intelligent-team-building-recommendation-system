Proceedings of BioNLP Shared Task 2011 Workshop, pages 1?6,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of BioNLP Shared Task 2011
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Sampo Pyysalo
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
smp@is.s.u-tokyo.ac.jp
Tomoko Ohta
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
okap@is.s.u-tokyo.ac.jp
Robert Bossy
National Institute for Agricultural Research
78352 Jouy en Josas, Cedex
Robert.Bossy@jouy.inra.fr
Ngan Nguyen
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
nltngan@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
5 Dan Ling Street, Haiian District, Beijing
jtsujii@microsoft.com
Abstract
The BioNLP Shared Task 2011, an informa-
tion extraction task held over 6 months up to
March 2011, met with community-wide par-
ticipation, receiving 46 final submissions from
24 teams. Five main tasks and three support-
ing tasks were arranged, and their results show
advances in the state of the art in fine-grained
biomedical domain information extraction and
demonstrate that extraction methods success-
fully generalize in various aspects.
1 Introduction
The BioNLP Shared Task (BioNLP-ST, hereafter)
series represents a community-wide move toward
fine-grained information extraction (IE), in particu-
lar biomolecular event extraction (Kim et al, 2009;
Ananiadou et al, 2010). The series is complemen-
tary to BioCreative (Hirschman et al, 2007); while
BioCreative emphasizes the short-term applicability
of introduced IE methods for tasks such as database
curation, BioNLP-ST places more emphasis on the
measurability of the state-of-the-art and traceabil-
ity of challenges in extraction through an approach
more closely tied to text.
These goals were pursued in the first event,
BioNLP-ST 2009 (Kim et al, 2009), through high
quality benchmark data provided for system devel-
opment and detailed evaluation performed to iden-
tify remaining problems hindering extraction perfor-
mance. Also, as the complexity of the task was high
and system development time limited, we encour-
aged focus on fine-grained IE by providing gold an-
notation for named entities as well as various sup-
porting resources. BioNLP-ST 2009 attracted wide
attention, with 24 teams submitting final results. The
task setup and data since have served as the basis
for numerous studies (Miwa et al, 2010b; Poon and
Vanderwende, 2010; Vlachos, 2010; Miwa et al,
2010a; Bjo?rne et al, 2010).
As the second event of the series, BioNLP-ST
2011 preserves the general design and goals of the
previous event, but adds a new focus on variabil-
ity to address a limitation of BioNLP-ST 2009: the
benchmark data sets were based on the Genia corpus
(Kim et al, 2008), restricting the community-wide
effort to resources developed by a single group for
a small subdomain of molecular biology. BioNLP-
ST 2011 is organized as a joint effort of several
groups preparing various tasks and resources, in
which variability is pursued in three primary direc-
tions: text types, event types, and subject domains.
Consequently, generalization of fine grained bio-IE
in these directions is emphasized as the main theme
of the second event.
This paper summarizes the entire BioNLP-ST
2011, covering the relationships between tasks and
similar broad issues. Each task is presented in detail
in separate overview papers and extraction systems
in papers by participants.
1
2 Main tasks
BioNLP-ST 2011 includes four main tracks (with
five tasks) representing fine-grained bio-IE.
2.1 Genia task (GE)
The GE task (Kim et al, 2011) preserves the task
definition of BioNLP-ST 2009, arranged based on
the Genia corpus (Kim et al, 2008). The data repre-
sents a focused domain of molecular biology: tran-
scription factors in human blood cells. The purpose
of the GE task is two-fold: to measure the progress
of the community since the last event, and to eval-
uate generalization of the technology to full papers.
For the second purpose, the provided data is com-
posed of two collections: the abstract collection,
identical to the BioNLP-ST 2009 data, and the new
full paper collection. Progress on the task is mea-
sured through the unchanged task definition and the
abstract collection, while generalization to full pa-
pers is measured on the full paper collection. In this
way, the GE task is intended to connect the entire
event to the previous one.
2.2 Epigenetics and post-translational
modification task (EPI)
The EPI task (Ohta et al, 2011) focuses on IE for
protein and DNA modifications, with particular em-
phasis on events of epigenetics interest. While the
basic task setup and entity definitions follow those of
the GE task, EPI extends on the extraction targets by
defining 14 new event types relevant to task topics,
including major protein modification types and their
reverse reactions. For capturing the ways in which
different entities participate in these events, the task
extends the GE argument roles with two new roles
specific to the domain, Sidechain and Contextgene.
The task design and setup are oriented toward the
needs of pathway extraction and curation for domain
databases (Wu et al, 2003; Ongenaert et al, 2008)
and are informed by previous studies on extraction
of the target events (Ohta et al, 2010b; Ohta et al,
2010c).
2.3 Infectious diseases task (ID)
The ID task (Pyysalo et al, 2011a) concerns the ex-
traction of events relevant to biomolecular mecha-
nisms of infectious diseases from full-text publica-
tions. The task follows the basic design of BioNLP-
ST 2009, and the ID entities and extraction targets
are a superset of the GE ones. The task extends
considerably on core entities, adding to PROTEIN
four new entity types, including CHEMICAL and
ORGANISM. The events extend on the GE defini-
tions in allowing arguments of the new entity types
as well as in introducing a new event category for
high-level biological processes. The task was im-
plemented in collaboration with domain experts and
informed by prior studies on domain information ex-
traction requirements (Pyysalo et al, 2010; Anani-
adou et al, 2011), including the support of systems
such as PATRIC (http://patricbrc.org).
2.4 Bacteria track
The bacteria track consists of two tasks, BB and BI.
2.4.1 Bacteria biotope task (BB)
The aim of the BB task (Bossy et al, 2011) is to ex-
tract the habitats of bacteria mentioned in textbook-
level texts written for non-experts. The texts are
Web pages about the state of the art knowledge about
bacterial species. BB targets general relations, Lo-
calization and PartOf , and is challenging in that
texts contain more coreferences than usual, habitat
references are not necessarily named entities, and,
unlike in other BioNLP-ST 2011 tasks, all entities
need to be recognized by participants. BB is the first
task to target phenotypic information and, as habi-
tats are yet to be normalized by the field community,
presents an opportunity for the BioNLP community
to contribute to the standardization effort.
2.4.2 Bacteria interaction task (BI)
The BI task (Jourde et al, 2011) is devoted to the ex-
traction of bacterial molecular interactions and reg-
ulations from publication abstracts. Mainly focused
on gene transcriptional regulation in Bacillus sub-
tilis, the BI corpus is provided to participants with
rich semantic annotation derived from a recently
proposed ontology (Manine et al, 2009) defining
ten entity types such as gene, protein and deriva-
tives as well as DNA sites/motifs. Their interactions
are described through ten relation types. The BI
corpus consists of the sentences of the LLL corpus
(Ne?dellec, 2005), provided with manually checked
linguistic annotations.
2
Task Text Focus #
GE abstracts, full papers domain (HT) 9
EPI abstracts event types 15
ID full papers domain (TCS) 10
BB web pages domain (BB) 2
BI abstracts domain (BS) 10
Table 1: Characteristics of BioNLP-ST 2011 main tasks.
?#?: number of event/relation types targeted. Domains:
HT = human transcription factors in blood cells, TCS
= two-component systems, BB = bacteria biology, BS =
Bacillus subtilis
2.5 Characteristics of main tasks
The main tasks are characterized in Table 1. From
the text type perspective, BioNLP-ST 2011 gener-
alizes from abstracts in 2009 to full papers (GE and
ID) and web pages (BB). It also includes data collec-
tions for a variety of specific subject domains (GE,
ID, BB an BI) and a task (EPI) whose scope is not
defined through a domain but rather event types. In
terms of the target event types, ID targets a superset
of GE events and EPI extends on the representation
for PHOSPHORYLATION events of GE. The two bac-
teria track tasks represent an independent perspec-
tive relatively far from other tasks in terms of their
target information.
3 Supporting tasks
BioNLP-ST 2011 includes three supporting tasks
designed to assist in primary the extraction tasks.
Other supporting resources made available to par-
ticipants are presented in (Stenetorp et al, 2011).
3.1 Protein coreference task (CO)
The CO task (Nguyen et al, 2011) concerns the
recognition of coreferences to protein references. It
is motivated from a finding from BioNLP-ST 2009
result analysis: coreference structures in biomedical
text hinder the extraction results of fine-grained IE
systems. While finding connections between event
triggers and protein references is a major part of
event extraction, it becomes much harder if one is
replaced with a coreferencing expression. The CO
task seeks to address this problem. The data sets for
the task were produced based on MedCO annotation
(Su et al, 2008) and other Genia resources (Tateisi
et al, 2005; Kim et al, 2008).
Event Date Note
Sample Data 31 Aug. 2010
Support. Tasks
Train. Data 27 Sep. 2010 7 weeks for development
Test Data 15 Nov. 2010 4 days for submission
Submission 19 Nov. 2010
Evaluation 22 Nov. 2010
Main Tasks
Train. Data 1 Dec. 2010 3 months for development
Test Data 1 Mar. 2011 9 days for submission
Submission 10 Mar. 2011 extended from 8 Mar.
Evaluation 11 Mar. 2011 extended from 10 Mar.
Table 2: Schedule of BioNLP-ST 2011
3.2 Entity relations task (REL)
The REL task (Pyysalo et al, 2011b) involves the
recognition of two binary part-of relations between
entities: PROTEIN-COMPONENT and SUBUNIT-
COMPLEX. The task is motivated by specific chal-
lenges: the identification of the components of pro-
teins in text is relevant e.g. to the recognition of
Site arguments (cf. GE, EPI and ID tasks), and re-
lations between proteins and their complexes rele-
vant to any task involving them. REL setup is in-
formed by recent semantic relation tasks (Hendrickx
et al, 2010). The task data, consisting of new anno-
tations for GE data, extends a previously introduced
resource (Pyysalo et al, 2009; Ohta et al, 2010a).
3.3 Gene renaming task (REN)
The REN task (Jourde et al, 2011) objective is to ex-
tract renaming pairs of Bacillus subtilis gene/protein
names from PubMed abstracts, motivated by dis-
crepancies between nomenclature databases that in-
terfere with search and complicate normalization.
REN relations partially overlap several concepts:
explicit renaming mentions, synonymy, and renam-
ing deduced from biological proof. While the task
is related to synonymy relation extraction (Yu and
Agichtein, 2003), it has a novel definition of renam-
ing, one name permanently replacing the other.
4 Schedule
Table 2 shows the task schedule, split into two
phases to allow the use of supporting task results in
addressing the main tasks. In recognition of their
higher complexity, a longer development period was
arranged for the main tasks (3 months vs 7 weeks).
3
Team GE EPI ID BB BI CO REL REN
UTurku 1 1 1 1 1 1 1 1
ConcordU 1 1 1 1 1 1
UMass 1 1 1
Stanford 1 1 1
FAUST 1 1 1
MSR-NLP 1 1
CCP-BTMG 1 1
Others 8 0 2 2 0 4 2 1
SUM 15 7 7 3 1 6 4 3
Table 3: Final submissions to BioNLP-ST 2011 tasks.
5 Participation
BioNLP-ST 2011 received 46 submissions from 24
teams (Table 3). While seven teams participated in
multiple tasks, only one team, UTurku, submitted fi-
nal results to all the tasks. The remaining 17 teams
participated in only single tasks. Disappointingly,
only two teams (UTurku, and ConcordU) performed
both supporting and main tasks, and neither used
supporting task analyses for the main tasks.
6 Results
Detailed evaluation results and analyses are pre-
sented in individual task papers, but interesting ob-
servations can be obtained also by comparisons over
the tasks. Table 4 summarizes best results for vari-
ous criteria (Note that the results shown for e.g. GEa,
GEf and GEp may be from different teams).
The community has made a significant improve-
ment in the repeated GE task, with an over 10%
reduction in error from ?09 to GEa. Three teams
achieved better results than M10, the best previously
reported individual result on the ?09 data. This in-
dicates a beneficial role from focused efforts like
BioNLP-ST. The GEf and ID results show that
generalization to full papers is feasible, with very
modest loss in performance compared to abstracts
(GEa). The results for PHOSPHORYLATION events
in GE and EPI are comparable (GEp vs EPIp), with
the small drop for the EPI result, suggesting that
the removal of the GE domain specificity does not
compromise extraction performance. EPIc results
indicate some challenges in generalization to simi-
lar event types, and EPIf suggest substantial further
challenges in additional argument extraction. The
complexity of ID is comparable to GE, also reflected
to their final results, which further indicate success-
Task Evaluation Results
BioNLP-ST 2009 (?09) 46.73 / 58.48 / 51.95
Miwa et al (2010b) (M10) 48.62 / 58.96 / 53.29
LLL 2005 (LLL) 53.00 / 55.60 / 54.30
GE abstracts (GEa) 50.00 / 67.53 / 57.46
GE full texts (GEf) 47.84 / 59.76 / 53.14
GE PHOSPHORYLATION (GEp) 79.26 / 86.99 / 82.95
GE LOCALIZATION (GEl) 37.88 / 77.42 / 50.87
EPI full task (EPIf) 52.69 / 53.98 / 53.33
EPI core task (EPIc) 68.51 / 69.20 / 68.86
EPI PHOSPHORYLATION (EPIp) 86.15 / 74.67 / 80.00
ID full task (IDf) 48.03 / 65.97 / 55.59
ID core task (IDc) 50.62 / 66.06 / 57.32
BB 45.00 / 45.00 / 45.00
BB PartOf (BBp) 32.00 / 83.00 / 46.00
BI 71.00 / 85.00 / 77.00
CO 22.18 / 73.26 / 34.05
REL 50.10 / 68.00 / 57.70
REN 79.60 / 95.90 / 87.00
Table 4: Best results for various (sub)tasks (recall / preci-
sion / f-score (%)). GEl: task 2 without trigger detection.
ful generalization to a new subject domain as well
as to new argument (entity) types. The BB task is
in part comparable to GEl and involves a represen-
tation similar to REL, with lower results likely in
part because BB requires entity recognition. The BI
task is comparable to LLL Challenge, though BI in-
volves more entity and event types. The BI result
is 20 points above the LLL best result, indicating a
substantial progress of the community in five years.
7 Discussion and Conclusions
Meeting with wide participation from the commu-
nity, BioNLP-ST 2011 produced a wealth of valu-
able resources for the advancement of fine-grained
IE in biology and biomedicine, and demonstrated
that event extraction methods can successfully gen-
eralize to new text types, event types, and domains.
However, the goal to observe the capacity of sup-
porting tasks to assist the main tasks was not met.
The entire shared task period was very long, more
than 6 months, and the complexity of the task was
high, which could be an excessive burden for partic-
ipants, limiting the application of novel resources.
There have been ongoing efforts since BioNLP-ST
2009 to develop IE systems based on the task re-
sources, and we hope to see continued efforts also
following BioNLP-ST 2011, especially exploring
the use of supporting task resources for main tasks.
4
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology.
Sophia Ananiadou, Dan Sullivan, William Black, Gina-
Anne Levow, Joseph J. Gillespie, Chunhong Mao,
Sampo Pyysalo, BalaKrishna Kolluru, Junichi Tsujii,
and Bruno Sobral. 2011. Named entity recognition
for bacterial type IV secretion systems. PLoS ONE,
6(3):e14780.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ?O. Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. Semeval-2010 task 8: Multi-way classi-
fication of semantic relations between pairs of nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages 33?
38, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Lynette Hirschman, Martin Krallinger, and Alfonso Va-
lencia, editors. 2007. Proceedings of the Second
BioCreative Challenge Evaluation Workshop. CNIO
Centro Nacional de Investigaciones Oncolo?gicas.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
A.P. Manine, E. Alphonse, and Bessie`res P. 2009. Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. International Journal of
Medical Informatics, 78(12):e31?38.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP?10, pages 37?45.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Ne?dellec. 2005. Learning Language in Logic ? Genic
Interaction Extraction Challenge. In Proceedings of
4th Learning Language in Logic Workshop (LLL?05),
pages 31?37.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010a. A re-evaluation of biomedical
named entity-term relations. Journal of Bioinformat-
ics and Computational Biology (JBCB), 8(5):917?928.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010b. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2010c. Event extraction for dna
methylation. In Proceedings of SMBM?10.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Mate? Ongenaert, Leander Van Neste, Tim De Meyer,
Gerben Menschaert, Sofie Bekaert, and Wim
Van Criekinge. 2008. PubMeth: a cancer methylation
database combining text-mining and expert annota-
tion. Nucleic Acids Research, 36(suppl 1):D842?846.
Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical litera-
ture. In Proceedings of NAACL-HLT?10, pages 813?
821.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
5
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9, Boulder, Colorado. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Tomoko Ohta, Han-Cheol Cho, Dan Sul-
livan, Chunhong Mao, Bruno Sobral, Jun?ichi Tsujii,
and Sophia Ananiadou. 2010. Towards event extrac-
tion from full texts on infectious diseases. In Proceed-
ings of BioNLP?10, pages 132?140.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun?ichi Tsujii. 2008. Coreference Resolution in
Biomedical Texts: a Machine Learning Approach. In
Ontologies and Text Mining for Life Sciences?08.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222?227.
Andreas Vlachos. 2010. Two strong baselines for the
bionlp 2009 event extraction task. In Proceedings of
BioNLP?10, pages 1?9.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucleic Acids Research, 31(1):345?347.
H. Yu and E. Agichtein. 2003. Extracting synony-
mous gene and protein terms from biological litera-
ture. Bioinformatics, 19(suppl 1):i340.
6
Proceedings of BioNLP Shared Task 2011 Workshop, pages 56?64,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP shared Task 2011 - Bacteria Biotope 
Robert Bossy1, Julien Jourde1, Philippe Bessi?res1, Maarten van de Guchte2,  
Claire N?dellec1 
 
1MIG UR1077 2Micalis UMR 1319  
INRA, Domaine de Vilvert 
78352 Jouy-en-Josas, France 
forename.name@jouy.inra.fr 
 
 
Abstract 
This paper presents the Bacteria Biotope 
task as part of the BioNLP Shared Tasks 
2011. The Bacteria Biotope task aims at 
extracting the location of bacteria from 
scientific Web pages. Bacteria location is a 
crucial knowledge in biology for phenotype 
studies. The paper details the corpus 
specification, the evaluation metrics, 
summarizes and discusses the participant 
results.  
1 Introduction 
The Bacteria Biotope (BB) task is one of the five 
main tasks of the BioNLP Shared Tasks 2011. The 
BB task consists of extracting bacteria location 
events from Web pages, in other words, citations 
of places where a given species lives. Bacteria 
locations range from plant or animal hosts for 
pathogenic or symbiotic bacteria, to natural 
environments like soil or water. Challenges for 
Information Extraction (IE) of relations in Biology 
are mostly devoted to the identification of bio-
molecular events in scientific papers where the 
events are described by relations between named 
entities, e.g. genic interactions (N?dellec, 2005), 
protein-protein interactions (Pyysalo et al, 2008), 
and more complex molecular events (Kim et al, 
2011). However, this far from reflects the diversity 
of the potential applications of text mining to 
biology. The objective of previous challenges has 
mostly been focused on modeling biological 
functions and processes using the information on 
elementary molecular events extracted from text. 
The BB task is the first step towards linking 
information on bacteria at the molecular level to 
ecological information. The information on 
bacterial habitats and properties of these habitats is 
very abundant in literature, in particular in 
Systematics literature (e.g. International Journal of 
Systematic and Evolutionary Microbiology), 
however it is rarely available in a structured way 
(Hirschman et al, 2008; Tamames and de Lorenzo, 
2009). The NCBI GenBank nucleotide isolation 
source field (GenBank) and the JGI Genome 
OnLine Database (GOLD) isolation site field are 
incomplete with respect to the microbial diversity 
and are expressed in natural language. The two 
critical missing steps in terms of biotope 
knowledge modeling are (1) the automatic 
population of databases with organism/location 
pairs that are extracted from text, and (2) the 
normalization of the habitat name with respect to 
biotope ontologies. The BB task mainly aims at 
solving the first information extraction issue. The 
second classification issue is handled through the 
categorization of locations into eight types. 
2 Context 
According to NCBI statistics there are nearly 900 
bacteria with complete genomes, which account 
for more than 87% of total complete genomes. 
Consequently, molecular studies in bacteriology 
are shifting from species-centered to full diversity 
investigation. The current trend in high-throughput 
experiments targets diversity related fields, 
typically phylogeny or ecology. In this context, 
adaptation properties, biotopes and biotope 
properties become critical information. Illustrative 
questions are: 
56
? Is there a phylogenetic correlation between 
species that share the same biotope? 
? What are common metabolic pathways of 
species that live in given conditions, especially 
species that survive in extreme conditions? 
? What are the molecular signaling patterns in 
host relationships or population relationships 
(e.g. in biofilms)? 
Recent metagenomic experiments produce 
molecular data associated with a habitat rather than 
a single species. This raises new challenges in 
computational biology and data integration, such 
as identifying known and new species that belong 
to a metagenome. 
Not only will these studies require 
comprehensive databases that associate bacterial 
species to their habitat, but they also require a 
formal description of habitats for property 
inference. The bacteria biotope description is 
potentially very rich since any physical object, 
from a cell to a continent, can be a bacterial 
habitat. However these relations are much simpler 
to model than with general formal spatial 
ontologies. A given place is a bacterial habitat if 
the bacteria and the habitat are physically in 
contact, while the relative position of the bacteria 
and its dissemination are not part of the BB task 
model.  
The BB Task requires the locations to be 
assigned different types (e.g. soil, water). We view 
location typing as a preliminary step of more fine-
grained modeling in location ontologies. Some 
classifications for bacteria biotopes have been 
proposed by some groups (Floyd et al, 2005; 
Hirschman et al, 2008; Field et al, 2008; 
Pignatelli et al, 2009). The Environment Ontology 
project (EnvO) is developing an ambitious detailed 
environment ontology for supporting standard 
manual annotation of environments of all types of 
organisms and biological samples (Field et al, 
2008). In a similar way, the GOLD group at JGI 
defined a standard classification for bacteria 
population metagenome projects. Developing 
methods for the association of such biotope classes 
to organisms remains an open question. EnvDB 
(Pignatelli et al, 2009) is an attempt to inventory 
isolation sources of bacteria as recorded in 
GenBank and to map them to a three level 
hierarchy of 71 biotope classes. The assignment of 
bacterial samples in one of the EnvDB classes is 
supported by a text-mining tool based on a Na?ve 
Bayes (NB) classifier applied to a bag of words 
representing the associated reference title and 
abstract. Unfortunately, the low number of paper 
references associated with the isolation source field 
(46 %) limits the scope of the method. 
The BB task has a similar goal, but directly 
applies to natural language texts thus avoiding the 
issue of database incompleteness. As opposed to 
database-based approaches, biotope information 
density is higher but the task has to include 
bacteria and location identification, as well as 
information extraction to relate them.  
The eight types of locations in the BB task 
capture high-level information for further ontology 
mappings.  The location types are Host, HostPart, 
Geographical and Environmental. Environmental 
is broadly defined to qualify locations that are not 
associated to hosts, in a similar way to what was 
described by Floyd et al (Floyd et al, 2005). In 
addition, the BB task types exclude artificially 
constructed biotopes (e.g. bacteria growing in labs 
on a specific medium) and laboratory mutant 
bacteria. The Environmental class is divided into 
Food, Medical, Soil and Water. Locations that are 
none of these subtypes are classified as 
Environmental. 
The exact geographical location (e.g. latitude 
and longitude coordinates) has less importance 
here than in eukaryote ecology because most of the 
biotope properties vary along distances smaller 
than the precision of the current positioning 
technologies. Geographical names are only useful 
in bacteria biotope studies when the physico-
chemical properties of the location can be inferred. 
For the sake of simplicity, the locations of bacteria 
host (e.g. the stall of the infected cow) are not 
taken into account despite their richness (Floyd et 
al., 2005). 
The important information conveyed by the 
locations, especially of Environment type, is the 
function of the bacterium in its ecosystem rather 
than the substance of the habitat. Indeed the final 
goal is to extract habitat properties and bacteria 
phenotypes. Beyond the identification of locations, 
their properties (e.g. temperature, pH, salinity, 
oxygen) are of high interest for phenotypes (e.g. 
thermophily, acidophily, halophily) and trophism 
studies. This information is difficult to extract, and 
is often incomplete or even not available in papers 
(Tamames and de Lorenzo., 2009). Hopefully, 
some properties can be automatically retrieved 
57
with the help of specialized databases, which give 
the physico-chemical properties of locations, such 
as hosts (plant, animal, human organs), soils (see 
WebSoilSurvey, Corine Land Cover), water, or 
chemical pollutants. 
From a linguistic point of view, the BB task 
differs from other IE molecular biology tasks while 
it raises some issues common to biomedicine and 
more general IE tasks. The documents are 
scientific Web pages intended for non-experts such 
as encyclopedia notices. The information is dense 
compared to scientific papers. Documents are 
structured as encyclopedia pages, with the main 
focus on a single species or a few species of the 
same genus or family. The frequency of anaphora 
and coreferences is unusually high. The location 
entities are denoted by complex expressions with 
semantic boundaries instead of rigid designators.  
3 Task description 
The goal of the BB task is illustrated in Figure 1.  
 
Bifidobacterium longum . This organism is found in 
adult humans  and formula fed infants  as a normal 
component of gut  flora. 
Figure 1. Example of information to be extracted 
in the BB Task. 
 
The entities to be extracted are of two main 
types: bacteria and locations. They are text-bound 
and their position has to be predicted. Relations are 
of type Localization between bacteria and 
locations, and PartOf between hosts and host parts. 
In the example in Figure 1, Bifidobacterium 
longum is a bacterium. adult humans and formula 
fed infants denote host locations for the bacteria. 
gut is also a bacteria location, part of the two hosts 
and thus of type host part.  
Coreference relations between entities denoting 
the same information represent valid alternatives 
for the relation arguments. For example, the three 
taxon names in Figure 2 are equivalent. 
 
 
 
The green sulfur bacteria  (GSB ; Phylum Chlorobi ) 
are commonly found in aquatic environments . 
Figure 2. Coreference example. 
 
The coreference relation between pairs of 
entities is binary, symmetric and transitive. 
Coreference sets are equivalence sets defined as 
the transitive closure of the binary coreference 
relation. Their annotation is provided in the 
training and development sets, but it does not have 
to be predicted in the test set. 
4 Corpus description 
The corpus sources are the following bacteria 
sequencing project Web pages: 
? Genome Projects referenced at NCBI; 
? Microbial Genomics Program at JGI; 
? Bacteria Genomes at EBI; 
? Microorganisms sequenced at Genoscope; 
? Encyclopedia pages from MicrobeWiki. 
The documents are publicly available and quite 
easy to understand by non-experts compared to 
scientific papers on similar topics. From the 2,086 
downloaded documents, 105 were randomly 
selected for the BB task. A quarter of the corpus 
was retained for test evaluation. The rest was split 
into train and development sets. Table 1 gives the 
distribution of the entities and relations per corpus. 
The distribution of the five document sources in 
the test corpus reflects the distribution of the 
training set and no other criteria. Food is therefore 
underrepresented.  
 
 Training+Dev Test 
Document 78 (65 + 13) 27 (26 %) 
Bacteria 538 121 (18 %) 
Environment 62 16 (21 %) 
Host 486 101 (17 %) 
HostPart 217 84 (28 %) 
Geographical 111 25 (18 %) 
Water 70 21 (23 %) 
Food 46 0 (0 %) 
Medical 24 2 (8 %) 
Soil 26 20 (43 %) 
Coreference 484 100 (17 %) 
Total entities 1,580 390 
58
 Training+Dev Test 
Localization 998 250 (20 %) 
Part of Host 204 78 (28 %) 
Total relations 1,202 328 
Table 1. Corpus Figures. 
5 Annotation methodology 
HTML tags and irrelevant metadata were stripped 
from the corpus. The Alvis pipeline (N?dellec et 
al., 2009) pre-annotated the species names that are 
potential bacteria and host names. A team of 7 
scientists manually annotated the entities, 
coreferences and relations using the Cadixe XML 
editor (Cadixe). Each document was processed by 
two independent annotators in a double-blind 
manner. Conflicts were automatically detected, 
resolved by annotator negotiation and irrelevant 
documents (e.g. without bacterial location) were 
removed. The remaining inconsistencies among 
documents were resolved by the two annotators 
assisted by a third person acting as an arbitrator. 
The annotator group designed the detailed 
annotation guidelines in two phases. First, they 
annotated a set of 10 documents, discussed the 
options and wrote detailed guidelines with 
representative and illustrative examples. During 
the annotation of the rest of the documents, new 
cases were discussed by email and the guidelines 
amended accordingly. 
Location types. The main issues under debate 
were the definition of location types, boundaries of 
annotations and coreferences. Additional 
annotation specifications concerned the exclusion 
of overly general locations (e.g. environment, 
zone), artificially constructed biotopes and indirect 
effects of bacteria on distant places. For instance, a 
disease symptom occurring in a given host part 
does not imply the presence of the bacteria in this 
place, whereas infection does. Boundaries of types 
were also an important point of discussion since 
the definite formalization of habitat categories was 
at stake. For instance we decided to exclude land 
environment citations (fields, deserts, savannah, 
etc.) from the type Soil, and thus enforced a strict 
definition of soil bacteria. The most controversial 
type was host parts. We decided to include fluids, 
secretions and excretions (which are not strictly 
organs). Therefore, the host parts category required 
specifications to determine at which point of 
dissociation from the original host is a habitat not a 
host part anymore (e.g. mother?s milk vs. industrial 
milk, rhizosphere as host part instead of soil). 
Boundaries. The bacteria name boundaries do 
not include any external modifiers (e.g. two A. 
baumannii strains). Irrelevant modifiers of 
locations are considered outside the annotation 
boundaries (e.g. responsible for a hospital 
epidemic). All annotations are contiguous and span 
on a single fragment in the same way as the other 
BioNLP Shared Tasks. This constraint led us to 
consider cases where several annotations occur 
side by side. The preferred approach was to have 
one distinct annotation for each different location 
(e.g. contact with infected animal products or 
through the air). In the case of head or modifier 
factorization, the annotation depends on the 
information conveyed by the factorized part. If the 
head is not relevant to determine the location type, 
then each term is annotated separately (e.g. 
tropical and temperate zones). Conversely, if the 
head is the most informative with regards to the 
location type, a single annotation spans the whole 
fragment (fresh and salt water). 
Coreferences. Two expressions are considered 
as coreferential and thus valid solution alternatives, 
if they convey the same information. For instance, 
complete taxon names and non-ambiguous 
abbreviations are valid alternatives (e.g. Borrelia 
garinii vs. B. garinii), while ambiguous anaphora 
ellipses are not (e.g. as in ?[..] infected with 
Borrelia duttonii. Borrelia then multiplies [..]?). 
The ellipsis of the omitted specific name 
(dutotonii) leaves the ambiguous generic name 
(Borrelia). 
The full guidelines document is available for 
download on the BioNLP Shared Task Bacteria 
Biotope page1. 
6 Evaluation procedure 
6.1 Campaign organization 
The training and development corpora with the 
reference annotations were made available to the 
participants by December 1st 2010 on the BioNLP 
Shared Tasks pages together with the evaluation 
software. The test corpus, which does not contain 
                                                   
1 https://sites.google.com/site/bionlpst/home/bacteria-biotopes/ 
BioNLP-ST_2011_Bacteria_Biotopes_Guidelines.pdf 
59
any annotation, was made available by March, 1st 
2011. The participants sent the predicted 
annotations to the BioNLP Shared Task organizers 
by March 10th. Each participant submitted a single 
final prediction set. The detailed evaluation results 
were computed, provided to the participants and 
published on the BioNLP website by March, 11th.  
6.2 Evaluation metrics 
The evaluation metrics are based on precision, 
recall and the F-measure. In the following section, 
the PartOf and Localization relations will both be 
referred to as events. The metrics measure the 
accuracy of the participant prediction of events 
with respect to the reference annotation of the test 
corpus. Predicted entities that are not event 
arguments are ignored and they do not penalize the 
score. Each event Er in the reference set is matched 
to the predicted event Ep that maximizes the event 
similarity function S. The recall is the sum of the S 
results divided by the number of events in the 
reference set. Each event Ep in the predicted set is 
matched to the reference event Er that maximizes 
S. The precision is the sum of the S results divided 
by the number of events in the predicted set. 
Participants were ranked by the F-score defined as 
the harmonic mean between precision and recall. 
Eab, the event similarity between a reference 
Localization event a and a predicted Localization 
event b, is defined as: 
Eab = Bab . Tab . Jab 
? Bab is the bacteria boundary component defined 
as: if the Bacterium arguments of both the 
predicted and reference events have exactly the 
same boundaries, then Bab = 1, otherwise Bab = 
0. Bacteria name boundary matching is strict 
since boundary mistakes usually yield a 
different taxon. 
? Tab is the location type prediction component 
defined as: if the Location arguments of both 
the predicted and reference events are of the 
same type, then Tab = 1, otherwise Tab = 0.5. 
Thus type errors divide the score by two. 
? Jab is the location boundary component defined 
as: if the Location arguments of the predicted 
and reference events overlap, then 
1?+=
ab
ba
ab OV
LENLENJ  
where LENa and LENb are the length of the 
Localization arguments of predicted and 
reference events, and OVab is the length of the 
overlapping segment between the Localization 
arguments of the predicted and reference 
events. If the arguments do not overlap, then Jab 
is 0. This formula is a Jaccard index applied to 
overlapping segments. Location boundary 
matching is relaxed, though the Jaccard index 
rewards predictions that approach the reference. 
For PartOf events between Hosts and HostParts, 
the matching score Pab is defined as: if the Host 
arguments of the reference and predicted events 
overlap and the Part arguments of the reference 
and predicted events overlap, then Pab = 1, 
otherwise Pab = 0. Boundary matching of PartOf 
arguments is relaxed, since boundary mistakes are 
already penalized in Eab. 
Arguments belonging to the same coreference 
set are strictly equivalent. In other words, the 
argument in the predicted event is correct if it is 
equal to the reference entity or to any item in the 
reference entity coreference set. 
7 Results  
7.1 Participating systems 
Three teams submitted predictions to the BB task. 
The first team is from the University of Turku 
(UTurku); their system is generic and produced 
predictions for every BioNLP Shared Task. This 
system uses ML intensely, especially SVMs, for 
entity recognition, entity typing and event 
extraction. UTurku adapted their system for the BB 
task by using specific NER patterns and external 
resources (Bj?rne and Salakoski, 2011). 
The second team is from the Japan Advanced 
Institute of Science and Technology (JAIST); their 
system was specifically designed for this task. 
They used CRF for entity recognition and typing, 
and classifiers for coreference resolution and event 
extraction (Nguyen and Tsuruoka, 2011). 
The third team is from Bibliome INRA; their 
system was specifically designed for this task 
(Ratkovik et al, 2011). This team has the same 
affiliation as the BB Task authors, however great 
care was taken to prevent communication on the 
subject between task participants and the test set 
annotators. 
60
The results of the three submissions according to 
the official metrics are shown in Table 2. The 
scores are micro-averaged: Localization and 
PartOf relations have the same weight. Given the 
novelty and the complexity of the task, these first 
results are quite encouraging. Almost half of the 
relations are correctly predicted. The Bibliome 
team achieved the highest F-measure with a 
balanced recall and precision (45%). 
 
 Recall Precision F-score 
Bibliome 45 45 45 
JAIST 27 42 33 
UTurku 17 52 26 
 
Table 2. Bacteria Biotope Task results. 
7.2 Systems description and result analysis 
All three systems perform the same distinct sub-
tasks: bacteria name detection, detection and 
typing of locations, coreference resolution and 
event extraction. The following description of the 
approaches used by the three systems in each 
subtask will be supported by intermediate results. 
Bacteria name detection. Interestingly the three 
participants used three different resources for the 
detection of bacteria names: the List of Prokaryotic 
Names with Standing in Nomenclature (LPNSN) 
by UTurku, names in the genomic BLAST page of 
NCBI by JAIST and the NCBI Taxonomy by 
Bibliome. 
 
Bibliome 84 
JAIST 55 
UTurku 16 
Table 3. Bacteria entity recall. 
 
Table 3 shows a disparity in the bacteria entity 
recall of participants. The merits of each resource 
cannot be deduced directly from these figures since 
they have been exploited in different manners. 
UTurku and JAIST systems injected the resource 
as features in a ML algorithm, whereas Bibliome 
directly projected the resource on the corpus with 
additional rule-based abbreviation detection. 
However there is some evidence that the 
resources have a major impact on the result. 
According to Sneath and Brenner (1992) LPNSN 
is necessarily incomplete. NCBI BLAST only 
contains names of species for which a complete 
genome has been published. The NCBI Taxonomy 
used by INRA only contains names of taxa for 
which some sequence was published. It appears 
that all the lists are incomplete. However, the 
bacteria referenced by the sequencing projects, 
which are mentioned in the corpus should all be 
recorded by the NCBI Taxonomy. 
Location detection and typing. As stated before, 
locations are not necessarily denoted by rigid 
designators. This was an interesting challenge that 
called for the use of external resources and 
linguistic analysis with a broad scope. 
UTurku and JAIST both used WordNet, a 
sensible choice since it encompasses a wide 
vocabulary and  is also structured with synsets and 
hyperonymy relations. The WordNet entries were 
injected as features in the participant ML-based 
entity recognition and typing subsystems. 
It is worth noting that JAIST also used word 
clustering based on MEMM for entity detection. 
This method has things in common with 
distributional semantics. JAIST experiments 
demonstrated a slight improvement using word 
clustering, but further exploration of this idea may 
prove to be valuable. 
Alternatively, the Bibliome system extracted 
terms from the corpus using linguistic criteria 
classified them as locations and predicted their 
type, by comparing them to classes in a habitat-
specific ontology. This prediction uses both 
linguistic analysis of terms and the hierarchical 
structure of the ontology. Bibliome also used 
additional resources for specific types: the NCBI 
Taxonomy for type Host and Agrovoc countries 
for type Geographical. 
 Bibliome JAIST UTurku 
Host 82 49 28 
Host part 72 36 28 
Geo. 29 60 53 
Environment 53 10 11 
Water 83 32 2 
Soil 86 37 34 
Table 4. Location entity recall by type. The 
number of entities of type Food and Medical in the 
test set is too low to be significant. The scores are 
computed using Tab and Jab. 
61
 
The location entity recall in Table 4 shows that 
Bibliome consistently outperformed the other 
groups for all types except for Geographical. This 
demonstrates the strength of exploiting a resource 
with strong semantics (ontology vs. lexicon) and 
with mixed semantic and linguistic rules. 
In order to evaluate the impact of Location entity 
boundaries and types, we computed the final score 
by relaxing Tab and Jab measures. We re-defined Tab 
as always equal to 1, in other words the type of the 
localization was not evaluated. We also re-defined 
Jab as: if the Location arguments overlap, then Jab = 
1, otherwise Jab = 0. This means that boundaries 
were relaxed. The relaxed scores are shown in 
Table 5. While the difference is not significant for 
JAIST and UTurku, the Bibliome results exhibit a 
9 point increase. This demonstrates that the 
Bibliome system is efficient at predicting which 
entities are locations, while the other participants 
predict more accurately the boundaries and types. 
 Recall Prec. F-score Diff. 
Bibliome 54 54 54 +9 
JAIST 29 45 35 +2 
UTurku 19 56 28 +2 
Table 5. Participants score using relaxed location 
boundaries and types. 
Coreference resolution. The corpus exhibits an 
unusual number of anaphora, especially bacteria 
coreferences since a single bacterium species is 
generally the central topic of a document. The 
Bibliome submission is the only one that 
performed bacteria coreference resolution. Their 
system is rule-based and dealt with referential ?it?, 
bi-antecedent anaphora and more importantly 
sortal anaphora. The JAIST system has a bacteria 
coreference module based on ML. However the 
submission was done without coreference 
resolution since their experiments did not show 
any performance improvement. 
 
Event extraction. Both UTurku and JAIST 
approached the event extraction as a classification 
task using ML (SVM). Bibliome exploited the co-
occurrence of arguments and the presence of 
trigger words from a predefined list. Both UTurku 
and Bibliome generate events in the scope of a 
sentence, whereas JAIST generates events in the 
scope of a paragraph. 
As shown in Table 6, UTurku achieved the best 
score for PartOf events. For all participants, the 
prediction is often correct (between 60 and 80%) 
while the recall is rather low (20 to 32%). 
 
  Recall Precis. F-score 
 Host 61 48 53 
 Host part 53 42 47 
 Geo. 13 38 19 
B. Env. 29 24 26 
 Water 60 55 57 
 Soil 69 59 63 
 Part-of 23 79 36 
 Host 30 43 36 
 Host part 18 68 28 
 Geo. 52 35 42 
J. Env. 5 0 0 
 Water 19 27 23 
 Soil 21 42 28 
 Part-of 31 61 41 
 Host 15 51 23 
 Host part 9 40 15 
 Geo. 32 40 36 
U. Env. 6 50 11 
 Water 1 7 2 
 Soil 12 21 15 
 Part-of 32 83 46 
Table 6. Event extraction results per type. 
 
Conversely, the score of the Localization relation 
by UTurku has been penalized by its low 
recognition of bacteria names (16%). This strongly 
affects the score of Localizations since the 
bacterium is the only expected agent argument. 
The good results of Bibliome are partly explained 
by its high bacteria name recall of 84%. 
The lack of coreference resolution might penalize 
the event extraction recall. To test this hypothesis, 
we computed the recall by taking only into account 
events where both arguments occur in the same 
sentence. The goal of this selection is to remove 
most events denoted through a coreference. The 
recall difference was not significant for Bibliome 
and JAIST, however UTurku recall raised by 12 
points (29%). That experiment confirms that 
UTurku low recall is explained by coreferences 
62
rather than the quality of event extraction. The 
paragraph scope chosen by JAIST probably 
compensates the lack of coreference resolution. 
As opposed to Bibliome, the precision of the 
Localization relation prediction by JAIST and 
UTurku, is high compared to the recall, with a 
noticeable exception of geographical locations. 
The difference between participants seems to be 
caused by the geographical entity recognition step 
more than the relation itself. This is shown by the 
difference between the entity and the event recall 
(Table 4 and 6 respectively).. The worst predicted 
type is Environment, which includes diverse 
locations, such as agricultural, natural and 
industrial sites and residues. This reveals 
significant room for improvement for Water, Soil 
and Environment entity recognition. 
8 Discussion 
The participant papers describe complementary 
methods for tackling BB Task?s new goals. The 
novelty of the task prevents participants from 
deeply investing in all of the issues together. 
Depending on the participants, the effort was 
focused on different issues with various 
approaches: entity recognition and anaphora 
resolution based on extensive use of background 
knowledge, and relation prediction based on 
linguistic analysis of syntactic dependencies. 
Moreover, these different approaches revealed to 
be complementary with distinct strengths and 
limitations. In the future, one may expect that the 
integration of these promising approaches will 
improve the current score. 
The corpus of BioNLP BB Task 2011 consists 
of a set of Web pages that were selected for their 
readability. However, some corpus traits make the 
IE task more difficult compared to scientific 
papers. For example, the relaxed style of some 
pages tolerates some typographic errors (e.g. 
morrow instead of marrow) and ambiguous 
anaphora. The genome sequencing project 
documents aim at justifying the sequencing of 
bacteria. This results in abundant descriptions of 
potential uses and locations that should not be 
predicted as actual locations. Their correct 
prediction requires complex analysis of modalities 
(possibility, probability, negation). Some pages 
describe the action of hosted bacteria at the 
molecular level, such as cellular infection. Terms 
related to the cell are ambiguous locations because 
they may refer to either bacteria or host cells. 
Scientific papers form a much richer source of 
bacterial location information that is exempt from 
such flaws. However, as opposed to Web pages, 
most of them are not publicly available and they 
are in PDF format. 
The typology of locations was designed 
according to the BB Task corpus with a strong bias 
towards natural environments since bioremediation 
and plant growth factor are important motivations 
for bacteria sequencing. It could be necessary to 
revise it according to a broader view of bacterial 
studies where pathogenicity and more generally 
human and animal health are central issues. 
9 Conclusion 
The Bacteria Biotope Task corpus and objectives 
differ from molecular biology text-mining of 
scientific papers. The annotation strategy and the 
analysis of the participant results contributed to the 
construction of a preliminary review of the nature 
and the richness of its linguistic specificities. The 
participant results are encouraging for the future of 
the Bacteria Biotope issue. The degree of 
sophistication of participating systems shows that 
the community has technologies, which are mature 
enough to address this crucial biology question. 
However, the results leave a large room for 
improvement. 
The Bacteria Biotope Task was an opportunity 
to extend molecular biology text-mining goals 
towards the support of bacteria biodiversity studies 
such as metagenomics, ecology and phylogeny. 
The prediction of bacterial location information is 
the very first step in this direction. The abundance 
of scientific papers dealing with this issue and 
describing location properties form a potentially 
rich source for further extensions. 
Acknowledgments 
The authors thank Valentin Loux for his valuable 
contribution to the definition of the Bacteria 
Biotope task. This work was partially supported by 
the French Quaero project. 
63
References 
Jari Bj?rne and Taio Salakoski. 2011. Generalizing 
Biomedical Event Extraction. Proceedings of the 
BioNLP 2011 Workshop Companion Volume for 
Shared Task. 
Cadixe. http://caderige.imag.fr/Articles/CADIXE-XML-
Annotation.pdf 
Corine Land Cover. 
http://www.eea.europa.eu/themes/landuse/interactive/
clc-download 
EnvDB database. http://metagenomics.uv.es/envDB/ 
EnvO Project. 
http://gensc.org/gc_wiki/index.php/EnvO_Project  
Dawn Field [et al. 2008. Towards a richer description 
of our complete collection of genomes and 
metagenomes: the ?Minimum Information about a 
Genome Sequence? (MIGS) specification. Nature 
Biotechnology. 26: 541-547. 
Melissa M. Floyd, Jane Tang, Matthew Kane and David 
Emerson. 2005. Captured Diversity in a Culture 
Collection: Case Study of the Geographic and 
Habitat Distributions of Environmental Isolates Held 
at the American Type Culture Collection. Applied 
and Environmental Microbiology. 71(6):2813-23. 
GenBank. http://www.ncbi.nlm.nih.gov/  
GOLD. http://www.genomesonline.org/cgi-
bin/GOLD/bin/gold.cgi 
Lynette Hirschman, Cheryl Clark, K. Bretonnel Cohen, 
Scott Mardis, Joanne Luciano, Renzo Kottmann, 
James Cole, Victor Markowitz, Nikos Kyrpides, 
Norman Morrison, Lynn M. Schriml, Dawn Field. 
2008. Habitat-Lite: a GSC case study based on free 
text terms for environmental metadata. Omics. 
12(2):129-136. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, Jun?ichi Tsujii. 2010. Extracting 
bio-molecular events from literature - the BioNLP?09 
shared task. Special issue of the International 
Journal of Computational Intelligence. 
MicrobeWiki. 
http://microbewiki.kenyon.edu/index.php/MicrobeWi
ki  
Microbial Genomics Program at JGI. http://genome.jgi-
psf.org/programs/bacteria-archaea/index.jsf 
Microorganisms sequenced at Genoscope. 
http://www.genoscope.cns.fr/spip/Microorganisms-
sequenced-at.html 
Claire N?dellec. 2005. Learning Language in Logic - 
Genic Interaction Extraction Challenge" in 
Proceedings of the Learning Language in Logic 
(LLL05) workshop joint to ICML'05. Cussens J. and 
N?dellec C. (eds). Bonn. 
Claire N?dellec, Adeline Nazarenko, Robert Bossy. 
2008..Information Extraction. Ontology Handbook. 
S. Staab, R. Studer (eds.), Springer Verlag, 2008. 
Nhung T. H. Nguyen and Yoshimasa Tsuruoka. 2011. 
Extracting Bacteria Biotopes with Semi-supervised 
Named Entity Recognition and Coreference 
Resolution. Proceedings of the BioNLP 2011 
Workshop Companion Volume for Shared Task. 
Miguel Pignatelli, Andr?s Moya, Javier Tamames.  
(2009). EnvDB, a database for describing the 
environmental distribution of prokaryotic taxa. 
Environmental Microbiology Reports. 1:198-207. 
Prokaryote Genome Projects at NCBI. 
http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi  
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari 
Bj?rne, Filip Ginter and Tapio Salakoski. 2008. 
Comparative analysis of five protein-protein 
interaction corpora. BMC Bioinformatics. vol 9. 
Suppl 3. S6. 
Zorana Ratkovic, Wiktoria Golik, Pierre Warnier, 
Philippe Veber, Claire N?dellec. 2011. BioNLP 2011 
Task Bacteria Biotope ? The Alvis System. 
Proceedings of the BioNLP 2011 Workshop 
Companion Volume for Shared Task. 
Peter H. A. Sneath and Don J. Brenner. 1992. ?Official? 
Nomenclature Lists. American Society for 
Microbioloy News. 58, 175. 
Javier Tamames and Victor de Lorenzo. 2010. 
EnvMine: A text-mining system for the automatic 
extraction of contextual information. BMC 
Bioinformatics. 11:294. 
Web Soil Survey. http://websoilsurvey.nrcs.usda.gov/ 
64
Proceedings of BioNLP Shared Task 2011 Workshop, pages 65?73,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011 ? Bacteria Gene Interactions and Renaming
Julien Jourde1, Alain-Pierre Manine2, Philippe Veber1, Kare?n Fort3, Robert Bossy1,
Erick Alphonse2, Philippe Bessie`res1
1Mathe?matique, Informatique et 2PredictiveDB 3LIPN ? Universite? Paris-Nord/
Ge?nome ? Institut National de la 16, rue Alexandre Parodi CNRS UMR7030 and
Recherche Agronomique F75010 Paris, France INIST CNRS UPS76 ? F54514
MIG INRA UR1077 {apmanine,alphonse} Vand?uvre-le`s-Nancy, France
F78352 Jouy-en-Josas, France @predictivedb.com karen.fort@inist.fr
forename.lastname@jouy.inra.fr
Abstract
We present two related tasks of the BioNLP
Shared Tasks 2011: Bacteria Gene Renam-
ing (Rename) and Bacteria Gene Interactions
(GI). We detail the objectives, the corpus spec-
ification, the evaluation metrics, and we sum-
marize the participants? results. Both issued
from PubMed scientific literature abstracts,
the Rename task aims at extracting gene name
synonyms, and the GI task aims at extracting
genic interaction events, mainly about gene
transcriptional regulations in bacteria.
1 Introduction
The extraction of biological events from scientific
literature is the most popular task in Information Ex-
traction (IE) challenges applied to molecular biol-
ogy, such as in LLL (Ne?dellec, 2005), BioCreative
Protein-Protein Interaction Task (Krallinger et al,
2008), or BioNLP (Demner-Fushman et al, 2008).
Since the BioNLP 2009 shared task (Kim et al,
2009), this field has evolved from the extraction of a
unique binary interaction relation between proteins
and/or genes towards a broader acceptation of bio-
logical events including localization and transforma-
tion (Kim et al, 2008). In the same way, the tasks
Bacteria Gene Interactions and Bacteria Gene Re-
naming deal with the extraction of various molecu-
lar events capturing the mechanisms relevant to gene
regulation in prokaryotes. The study of bacteria has
numerous applications for health, food and indus-
try, and overall, they are considered as organisms
of choice for the recent integrative approaches in
systems biology, because of their relative simplicity.
Compared to eukaryotes, they allow easier and more
in-depth analysis of biological functions and of their
related molecular mechanisms.
Processing literature on bacteria raises linguis-
tic and semantic specificities that impact text anal-
ysis. First of all, gene renaming is a frequent phe-
nomenon, especially for model bacteria. Hence, the
abundance of gene synonyms that are not morpho-
logical variants is high compared to eukaryotes. The
history of bacterial gene naming has led to drastic
amounts of homonyms and synonyms which are of-
ten missing (or worse, erroneous) in gene databases.
In particular, they often omit old gene names that
are no longer used in new publications, but that are
critical for exhaustive bibliography search. Poly-
semy makes the situation even worse, as old names
frequently happen to be reused to denote different
genes. A correct and complete gene synonym table
is crucial to biology studies, for instance when inte-
grating large scale experimental data using distinct
nomenclatures. Indeed this information can save a
lot of bibliographic research time. The Rename Task
is a new task in text-mining for biology that aims at
extracting explicit mentions of renaming relations.
It is a critical step in gene name normalization that
is needed for further extraction of biological events
such as genic interactions.
Regarding stylistics, gene and protein interactions
are not formulated in the same way for eukary-
otes and prokaryotes. Descriptions of interactions
and regulations in bacteria include more knowledge
about their molecular actors and mechanisms, com-
pared to the literature on eukaryotes. Typically in
bacteria literature, the genic regulations are more
65
likely expressed by direct binding of the protein,
while in eukaryote literature, non-genic agents re-
lated to environmental conditions are much more
frequent. The bacteria GI Task is based on (Manine
et al, 2010) which is a semantic re-annotation of the
LLL challenge corpus (Ne?dellec, 2005), where the
description of the GI events in a fine-grained rep-
resentation includes the distinction between expres-
sion, transcription and other action events, as well as
different transcription controls (e.g. regulon mem-
bership, promoter binding). The entities are not only
protein agent and gene target but extend to families,
complexes and DNA sites (binding sites, promoters)
in order to better capture the complexity of the reg-
ulation at a molecular level. The task consists in re-
lating the entities with the relevant relations.
2 Rename Task Description
The goal of the Rename task is illustrated by Figure
1. It consists in predicting renaming relations be-
tween text-bound gene names given as input. The
only type of event is Renaming where both argu-
ments are of type Gene. The event is directed, the
former and the new names are distinguished. Genes
and proteins were not distinguished because of the
high frequency of metonymy in renaming events.
The relation to predict between genes is a Renam-
ing of a former gene name into a new one. In the
example of Figure 1, YtaA, YvdP and YnzH are the
former names of three proteins renamed CotI, CotQ
and CotU, respectively.
Figure 1: Examples of relations to be extracted.
2.1 Rename Task corpus
The Rename Task corpus is a set of 1,836 PubMed
references of bacterial genetic and genomic studies,
including title and abstract. A first set of 23,000 doc-
uments was retrieved, identifying the presence of the
bacterium Bacillus subtilis in the text and/or in the
MeSH terms. B. subtilis documents are particularly
rich in renaming mentions. Many genes were re-
named in the middle of the nineties, so that the new
names matched those of the Escherichia coli homo-
logues. The 1,843 documents the most susceptible
to mention renaming were automatically filtered ac-
cording to two non exclusive criteria:
1. Either the document mentions at least two gene
synonyms as recorded in the fusion of seven B.
subtilis gene nomenclatures. This led to a set
of 703 documents.
2. Or the document contains a renaming expres-
sion from a list that we manually designed and
tested (e.g. rename, also known as). It is an ex-
tension of a previous work by (Weissenbacher,
2004). A total of 1,140 new documents not in-
cluded in the first set match this criteria.
About 70% of the documents (1,146) were kept in
the training data set. The rest was split into the de-
velopment and test sets, containing 246 and 252 doc-
uments respectively. Table 1 gives the distribution
of genes and renaming relations per corpus. Gene
names were automatically annotated in the docu-
ments with the nomenclature of B. subtilis. Gene
names involved in renaming acts were manually cu-
rated. Among the 21,878 gene mentions in the three
corpus, 680 unique names are involved in renaming
relations which represents 891 occurrences of genes.
Training + Dev. Test
Documents (1,146 + 246) 1,392 252 (15%)
Gene names 18,503 3,375 (15%)
Renamings 373 88 (24%)
Table 1: Rename Task corpus content.
2.2 Rename Task annotation and guidelines
Annotation procedure The corpus was annotated
in a joint effort of MIG/INRA and INIST/CNRS.
The reference annotation of the Rename Task cor-
pus was done in two steps, a first annotation step
by science information professionals of INIST with
MIG initial specifications, a second checking step by
people at MIG. Two annotators and a project man-
ager were in charge of the task at INIST. The docu-
ments were annotated using the Cadixe editor1. We
1http://caderige.imag.fr/Articles/
CADIXEXML-Annotation.pdf
66
provided to them detailed annotation guidelines that
were largely modified in the process. A subset of
100 documents from the first set of 703 was anno-
tated as a training session. This step was used to re-
fine the guidelines according to the methodology de-
scribed in (Bonneau-Maynard et al, 2005). Several
inter-annotator agreements coefficients were com-
puted to measure the discrepancy between annota-
tors (Fort et al, 2009). With a kappa and pi scores
(for more details on those, see (Artstein and Poesio,
2008)), the results can be considered satisfactory.
The manual analysis of the 18 discrepancies led to
enrich the annotation guidelines. The first hundreds
of documents of the second set did not mention any
renaming, leading to concentrate the annotation ef-
forts on the first set. These documents actually con-
tained renamings, but nearly exclusively concerning
other kinds of biological entities (protein domains,
molecules, cellular ultrastructures, etc.).
Guidelines In order to simplify the task, only
short names of gene/protein/groups in B. subtilis
were considered. Naming conventions set short
names of four letters long with an upper case let-
ter at the end for all genes (e.g. gerE) and the same
names with the upper case of the initial letter (e.g.
GerE) and long names for the proteins (e.g. Spore
germination protein gerE). But many irregular gene
names exist (e.g. tuf), which are considered as well.
It also happens that gene or protein name lists are
abbreviated by factorization to form a sequence. For
instance queCDEF is the abbreviation of the list of
gene names queC, queD, queE and queF. Such ag-
gregations are acceptable gene names as well. In any
case, these details were not needed by the task par-
ticipants since the corpus was provided with tagged
gene names.
Most renaming relations involve couples of the
same type, genes, proteins or aggregations. Only
18 relations link mixed couples of genes and pro-
teins. In case of ambiguity, annotators would consult
international gene databases and an internal INRA
database to help them determine whether a given
couple of names were actually synonyms.
Multiple occurrences of the same renaming rela-
tion were annotated independently, and had to be
predicted. The renaming pairs are directed, the for-
mer and the new forms have to be distinguished.
When the renaming order was not explicit in the
document, the rule was to annotate by default the
first member of the couple as the new form, and the
second one as the former form. Figure 2 presents the
most common forms of renaming.
Figure 2: Common types of relations to be extracted.
Revised annotations INIST annotations were
systematically checked by two experts in Bioinfor-
matics from INRA. Mainly, encoding relations (e.g.
the gene encoding sigma K (sigK)) that are not re-
naming cases were purged. Given the number of
ambiguous annotations, we designed a detailed ty-
pology in order to justify acceptance or rejection
decisions in seven different sub-cases hereafter pre-
sented. Three positive relations figure in Table 2,
where the underlined names are the former names
and the framed names are the new ones. Explicit re-
naming relations occur in 261 sentences, synonymy-
like relations in 349 sentences, biological proof-
based relations in 76 sentences.
Explicit renaming relation is the easiest positive
case to identify. In the example, the aggregation of
gene names ykvJKLM is clearly renamed by the au-
thors as queCDEF. Although the four genes are con-
Explicit renaming
PMID 15767583 : Genetic analysis of ykvJKLM mu-
tants in Acinetobacter confirmed that each was essen-
tial for queuosine biosynthesis, and the genes were re-
named queCDEF .
Implicit renaming
PMID 8002615 : Analysis of a suppressor mutation
ssb ( kinC ) of sur0B20 (spo0A) mutation in Bacil-
lus subtilis reveals that kinC encodes a histidine pro-
tein kinase.
Biological proof
PMID 1744050 : DNA sequencing established that
spoIIIF and spoVB are a single monocistronic locus
encoding a 518-amino-acid polypeptide with features
of an integral membrane protein.
Table 2: Positive examples of the Rename Task.
67
catenated, there is no evidence mentioned of them
acting as an operon. Furthermore, despite the con-
text involving mutants of Acinetobacter, the aggre-
gation belongs correctly to B. subtilis.
Implicit renaming is an asymmetric relation
since one of the synonyms is intended to replace the
other one in future uses. The example presents two
renaming relations between former names ssb and
spo0A, and new names kinC and sur0B20, respec-
tively. The renaming relation between ssb and kinC
has a different orientation due to additional informa-
tion in the reference. Like in the preceding example,
the renaming is a consequence of a genetic mutation
experiment. Mutation names represent an important
transversal issue that is discussed below.
Biological proof is a renaming relation induced
by an explicit scientific conclusion while the renam-
ing is not, as in the example where experiments re-
veal that two loci spoIIIF and spoVB are in fact the
same one and then become synonyms. Terms such
as ?allelic to? or ?identical to? usually qualify such
conclusions. Predicting biological proof-based rela-
tions requires some biological modeling.
The next three cases are negative (Table 3). Un-
derlined gene and protein names are involved in a
relation which is not a renaming relation.
Protein encoding relation occurs between a gene
and the protein it codes for. Some mentions may
look like renaming relations. The example presents
the gene yeaC coding for MoxR. No member of the
couple is expected to replace the other one.
Homology measures the similarity between gene
or protein sequences. Most of the homology men-
tions involve genes or proteins from different species
Protein encoding
PMID 8969499: The putative products of ORFs yeaB
(Czd protein), yeaC (MoxR), yebA (CNG-channel and
cGMP-channel proteins from eukaryotes),
Genetic homology
PMID 10619015 : Dynamic movement of the ParA-
like Soj protein of B. subtilis and its dual role in nu-
cleoid organization and developmental regulation.
Operon | Regulon | Family
PMID 3127379 : Three promoters direct transcription
of the sigA (rpoD) operon in Bacillus subtilis.
Table 3: Negative examples of the Rename Task.
(orthologues). The others compare known gene or
protein sequences of the same species (paralogues).
This may be misleading since the similarity men-
tion may look like biological proof-based relations,
as between ParA and Soj in Table 3.
Operon, regulon or family renaming involves
objects that may look like genes, proteins or sim-
ple aggregations of gene or protein names but that
are perceptibly different. The objects represent more
than one gene or protein and the renaming does not
necessarily affect all of them. More problematic,
their name may be the same as one of the genes or
proteins they contain, as in the example where sigA
and rpoD are operons but are also known as gene
names. Here, sigA (and so rpoD) represents at least
two different genes. For the sake of clarity, oper-
ons, regulons and families are rejected, even if all
the genes are clearly named, as in an aggregation.
The last point concerns mutation which are fre-
quent in Microbiology for revealing gene pheno-
types. They carry information about the original
gene names (e.g., rvtA11 is a mutant name created
by adding 11 to rvtA). But partial names cannot be
partially annotated, that is to say, the original part
(rvtA) should not be annotated in the mutation name
(rvtA11). Most of these names are local names, and
should not be annotated because of their restricted
scope. It may happen so that the mutation name
is registered as a synonym in several international
databases. To avoid inconsistencies, all renamings
involving a mutation referenced in a database were
accepted, and only biological proof-based and ex-
plicit renamings involving a strict non-null unrefer-
enced mutation (a null mutation corresponds to a to-
tal suppression of a gene) were accepted.
2.3 Rename Task evaluation procedure
The evaluation of the Rename task is given in terms
of recall, precision and F-score of renaming rela-
tions. Two set of scores are given: the first set is
computed by enforcing strict direction of renaming
relations, the second set is computed with relaxed
direction. Since the relaxed score takes into ac-
count renaming relations even if the arguments are
inverted, it will necessarily be greater or equal than
the strict score. The participant score is the relaxed
score, the strict score is given for information. Re-
laxed scores are informative with respect to the ap-
68
plication goal. The motivation of the Rename task
is to keep bacteria gene synonyms tables up to date.
The choice of the canonical name among synonyms
for denoting a gene is done by the bacteriology com-
munity, and it may be independent of the anteriority
or novelty of the name. The annotation of the ref-
erence corpus showed that the direction was not al-
ways decidable, even for a human reader. Thus, it
would have been unfair to evaluate systems on the
basis of unsure information.
2.4 Results of the Rename Task participants
Final submissions were received from three teams,
the University of Turku (Uturku), the University of
Concordia (Concordia) and the Bibliome team from
MIG/INRA. Their results are summarized in Table
4. The ranking order is given by the overall F-score
for relations with relaxed argument order.
Team Prec. Recall F-score
Univ. of Turku 95.9 79.6 87.0
Concordia Univ. 74.4 65.9 69.9
INRA 57.0 73.9 64.4
Table 4: Participant scores at the Rename Task.
Uturku achieved the best F-score with a very high
precision and a high recall. Concordia achieved the
second F-score with balanced precisions and recalls.
Bibliome is five points behind with a better recall
but much lower precision. Both UTurku and Con-
cordia predictions rely on dependencies (Charniak-
Johnson and Stanford respectively, using McClosky
model), whereas Bibliome predictions rely on bag of
words. This demonstrates the high value of depen-
dency parsing for this task, in particular for the pre-
cision of predictions. We notice that UTurku system
uses machine learning (SVM) and Concordia uses
rules based on trigger words. The good results of
UTurku confirms the hypothesis that gene renam-
ing citations are highly regular in scientific litera-
ture. The most frequently missed renamings belong
to the Biological Proof category (see Table 2). This
is expected because the renaming is formulated as a
reasoning where the conclusion is only implicit.
2.5 Discussion
The very high score of Uturku method leads us to
conclude that the task can be considered as solved
by a linguistic-based approach. Whereas Bib-
liome used an extensive nomenclature considered
as exhaustive and sentence filtering using a SVM,
Uturku used only two nomenclatures in synergy but
with more sophisticated linguistic-based methods,
in particular syntactic analyses. Bibliome methods
showed that a too high dependence to nomenclatures
may decrease scores if they contain compromised
data. However, the use of an extensive nomencla-
ture as done by Bibliome may complement Uturku
approach and improve recall. It is also interesting
that both systems do not manage renamings cross-
ing sentence boundaries.
The good results of the renaming task will be ex-
ploited to keep synonym gene lists up to date with
extensive bibliography mining. In particular this
will contribute to enriching SubtiWiki, a collabora-
tive annotation effort on B. subtilis (Flo?rez et al,
2009; Lammers et al, 2010).
3 Gene Interactions Task description
The goal of the Bacteria GI Task is illustrated by
Figure 3. The genes cotB and cotC are related to
their two promoters, not named here, by the rela-
tion PromoterOf. The protein GerE is related to
these promoters by the relation BindTo. As a con-
sequence, GerE is related to cotB and cotC by an In-
teraction relation. According to (Kim et al, 2008),
the need to define specialized relations replacing one
unique and general interaction relation was raised in
(Manine et al, 2009) for extracting genic interac-
tions from text. An ontology describes relations and
entities (Manine et al, 2008) catching a model of
gene transcription to which biologists implicitly re-
fer in their publications. Therefore, the ontology is
mainly oriented towards the description of a struc-
tural model of genes, with molecular mechanisms
of their transcription and associated regulations.
The corpus roughly contains three kinds of genic
Figure 3: Examples of relations to be extracted.
69
interaction mentions, namely regulations, regulon
membership and binding. The first case corresponds
to interactions the mechanism of which is not explic-
itly given in the text. The mention only tells that the
transcription of a given gene is influenced by a given
protein, either positively (activation), negatively (in-
hibition) or in an unspecified way. The second kind
of genic interaction mention (regulon membership)
basically conveys the same information, using the
regulon term/concept. The regulon of a gene is the
set of genes that it controls. In that case, the interac-
tion is expressed by saying that a gene is a member
of some regulon. The third and last kind of mention
provides with more mechanistic details on a regula-
tion, since it describes the binding of a protein near
the promoter of a target gene. This motivates the in-
troduction of Promoter and Site entities, which cor-
respond to DNA regions. It is thus possible to extract
the architecture of a regulatory DNA region, linking
a protein agent to its gene target (see Figure 3).
The set of entity types is divided into two main
groups, namely 10 genic entities and 3 kinds of ac-
tion (Table 5). Genic entities represent biological
objects like a gene, a group of genes or a gene prod-
uct. In particular, a GeneComplex annotation corre-
sponds to an operon, which is a group of genes that
are contiguous in the genome and under the control
of the same promoter. The annotation GeneFamily
is used to denote either genes involved in the same
biological function or genes with sequence homolo-
gies. More importantly, PolymeraseComplex anno-
tations correspond to the protein complex that is re-
sponsible for the transcription of genes. This com-
plex includes several subunits (components), com-
bined with a sigma factor, that recognizes specific
promoters on the DNA sequence.
The second group of entities are phrases express-
ing either molecular processes (e.g. sequestration,
dephosphorylation, etc.) or the molecular state of
the bacteria (e.g. presence, activity or level of a pro-
tein). They represent some kind of action that can
be performed on a genic entity. Note that transcrip-
tion and expression events were tagged as specific
actions, because they play a specific part in certain
relations (see below).
The annotation of entities and actions was pro-
vided to the participants, and the task consisted in
extracting the relations listed in Table 6.
Name Example
Gene cotA
GeneComplex sigX-ypuN
GeneFamily class III heat shock genes
GeneProduct yvyD gene product
Protein CotA
PolymeraseComplex SigK RNA polymerase
ProteinFamily DNA-binding protein
Site upstream site
Promoter promoter regions
Regulon regulon
Action activity | level | presence
Expression expression
Transcription transcription
Table 5: List of molecular entities and actions in GI.
Name Example
ActionTarget expression of yvyD
Interaction ComK negatively regulates
degR expression
RegulonDependence sigmaB regulon
RegulonMember yvyD is member of sigmaB
regulon
BindTo GerE adheres to the pro-
moter
SiteOf -35 sequence of the pro-
moter
PromoterOf the araE promoter
PromoterDependence GerE-controlled promoter
TranscriptionFrom transcription from the up-
stream site
TranscriptionBy transcription of cotD by
sigmaK RNA polymerase
Table 6: List of relations in GI.
The relations are binary and directed, and rely the
entities defined above. The three kinds of interac-
tions are represented with an Interaction annotation,
linking an agent to its target. The other relations
provide additional details on the regulation, like ele-
mentary components involved in the reaction (sites,
promoters) and contextual information (mainly pro-
vided by the ActionTarget relations). A formal def-
inition of relations and relation argument types can
be found on the Bacteria GI Task Web page.
3.1 Bacteria Gene Interactions corpus
The source of the Bacteria GI Task corpus is a set
of PubMed abstracts mainly dealing with the tran-
70
scription of genes in Bacillus subtilis. The semantic
annotation, derived from the ontology of (Manine et
al., 2008), contains 10 molecular entities, 3 different
actions, and 10 specialized relations. This is applied
to 162 sentences from the LLL set (Ne?dellec, 2005),
which are provided with manually checked linguis-
tic annotations (segmentation, lemmatization, syn-
tactic dependencies). The corpus was split into 105
sentences for training, 15 for development and 42
for test. Table 7 gives the distribution of the entities
and actions per corpus and Table 8 gives the distri-
bution of the relations per corpus.
3.2 Annotation procedures and guidelines
The semantic annotation scheme was developed by
two annotators through a series of independent an-
notations of the corpus, followed by reconciliation
steps, which could involve concerted modifications
(Manine et al, 2010). As a third and final stage, the
Entity or action Train. + Dev. Test
Documents (105+15) 120 42
Protein 219 85
Gene 173 56
Transcription 53 21
Promoter 49 10
Action 45 22
PolymeraseComplex 43 14
Expression 29 6
Site 22 8
GeneComplex 19 4
ProteinFamily 12 3
Regulon 11 2
GeneProduct 10 3
GeneFamily 6 5
Table 7: Distribution of entities and actions in GI.
Relation Train. + Dev. Test
Interaction 208 64
ActionTarget 173 47
PromoterOf 44 8
BindTo 39 4
PromoterDependence 36 4
TranscriptionBy 36 8
SiteOf 23 6
RegulonMember 17 2
TranscriptionFrom 14 2
RegulonDependence 12 1
Table 8: Distribution of relations in GI.
corpus was reviewed and the annotation simplified
to make it more appropriate to the contest. The final
annotation contains 748 relations distributed in nine
categories, 146 of them belonging to the test set.
The annotation scheme was generally well suited
to accurately represent the meaning of the sentences
in the corpus, with one notable exception. In the cor-
pus, there is a common phrasing telling that a pro-
tein P regulates the transcription of a gene G by a
given sigma factor S. In that case, the only anno-
tated interactions are between the couples (P, G) and
(S, G). This representation is not completely satis-
factory, and a ternary relation involving P, S and G
would have been more adequate.
Additional specific rules were needed to cope
with linguistic issues. First, when the argument of a
relation had coreferences, the relation was repeated
for each maximally precise coreference of the argu-
ment. Second, in case of a conjunction like ?sig-
maA and sigmaX holoenzymes?, there should ide-
ally be two entities (namely ?sigmaA holoenzyme?
and ?sigmaX holoenzyme?); however, this is not
easy to represent using the BioNLP format. In this
situation, we grouped the two entities into a single
one. These cases were rare and unlikely affected the
feasibility of the task, since entities were provided
in the test set.
3.3 Gene Interactions evaluation procedure
The training and development corpora with the ref-
erence annotations were made available to partici-
pants by December, 1st on the BioNLP shared Task
pages together with evaluation software. The test
corpus with the entity annotations has been made
available by March, 1st. The participants sent the
predicted annotations to the BioNLP shared Task
organizers by March, 10th. The evaluation results
were computed and provided to the participants and
on the Web site the same day. The participants are
evaluated and ranked according to two scores: F-
score for all event types together, and F-score for
the Interaction event type. In order for a predicted
event to count as a hit, both arguments must be the
same as in the reference in the right order and the
event type must be the same as in the reference.
71
3.4 Results of GI Task participants
There was only one participant, whose results are
shown in Tables 9 and 10. Some relations were
not significantly represented in the test set and thus
the corresponding results should be considered with
caution. This is the case for RegulonMember and
TranscriptionFrom, only represented two times each
in the test. The lowest recall, 17%, obtained for the
SiteOf relation is explained by its low representa-
tion in the corpus: most of the test errors come from
a difficult sentence with coreferences.
The recall of 56% for the Interaction relation cer-
tainly illustrates the heterogeneity of this category,
gathering mentions of interactions at large, as well
as precise descriptions of gene regulations. For in-
stance, Figure 4 shows a complex instance where all
of the interactions were missed. Surprisingly, we
also found false negatives in rather trivial examples
(?ykuD was transcribed by SigK RNA polymerase
from T4 of sporulation.?). Uturku used an SVM-
based approach for extraction, and it is thus delicate
to account for the false negatives in a simple and
concise way.
Event U. Turku scores
Global Precision 85
Global Recall 71
Global F-score 77
Interaction Precision 75
Interaction Recall 56
Interaction F-score 64
Table 9: University of Turku global scores.
Event Prec. Rec. F-score
Global 85 71 77
ActionTarget 94 92 93
BindTo 75 75 75
Interaction 75 56 64
PromoterDependence 100 100 100
PromoterOf 100 100 100
RegulonDependence 100 100 100
RegulonMember 100 50 67
SiteOf 100 17 29
TranscriptionBy 67 50 57
TranscriptionFrom 100 100 100
Table 10: University of Turku scores for each relation.
Figure 4: Examples of three missed interactions.
3.5 Discussion
The GI corpus was previously used in a relation
extraction work (Manine et al 2009) based on In-
ductive Logic Programming (Muggleton and Raedt,
1994). However a direct comparison of the results
is not appropriate here since the annotations were
partially revised, and the evaluation setting was dif-
ferent (leave-one-out in Manine?s work, test set in
the challenge).
Nevertheless, we note similar tendencies if we
compare relative results between relations. In partic-
ular, it was also found in Manine?s paper that SiteOf,
TranscriptionBy and Interaction are the most diffi-
cult relations to extract. It is also worth to mention
that both approaches rely on syntactic dependencies,
and use the curated dependencies provided in the
corpus. Interestingly, the approach by the University
of Turku reports a slightly lower F-measure with de-
pendencies calculated by the Charniak parser (about
1%, personal communication). This information is
especially important in order to consider a produc-
tion setting.
4 Conclusion
The quality of results for both challenges suggests
that current methods are mature enough to be used
in semi-automatic strategies for genome annotation,
where they could efficiently assist biological experts
involved in collaborative annotation efforts (Lam-
mers et al, 2010). However, the false positive rate,
notably for the Interaction relation, is still too high
for the extraction results to be used as a reliable
source of information without a curation step.
Acknowlegments
We thank Franc?oise Tisserand and Bernard Talercio
(INIST) for their work on the Rename corpus, and
the QUAERO Programme funded by OSEO (French
agency for innovation) for its support.
72
References
Artstein R., Poesio M. (2008). Inter-coder agreement
for Computational Linguistics. Computational Lin-
guistics, 34(4):555-96.
Bjo?rne J., Heimonen J., Ginter F., Airola A., Pahikkala
T., Salakoski T. (2009). Extracting complex biological
events with rich graph-based feature sets. BioNLP?09
Proc. Workshop Current Trends in Biomedical Natural
Language Processing: Shared Task, pp. 10-18.
Bonneau-Maynard H., Rosset S., Ayache C., Kuhn A.,
Mostefa D. (2005). Semantic annotation of the French
Media Dialog Corpus. Interspeech-2005, pp. 3457-60.
Demner-Fushman D., Ananiadou S., Cohen K.B., Pestian
J., Tsujii J., Webber B. (2008). Themes in biomedical
natural language processing: BioNLP08. BMC Bioin-
formatics, 9(Suppl. 11):S1.
Flo?rez L.A., Roppel S.F., Schmeisky A.G., Lammers
C.R., Stu?lke J. (2009). A community-curated con-
sensual annotation that is continuously updated: The
Bacillus subtilis centred wiki SubtiWiki. Database,
2009:bap012.
Fort K., Franc?ois C., Ghribi M. (2010). ?Evaluer des an-
notations manuelles disperse?es : les coefficients sont-
ils suffisants pour estimer l?accord inter-annotateurs ?
17e Conf. Traitement Automatique des Langues Na-
turelles (TALN 2010).
Kim J.D., Ohta T., Tsujii J. (2008) Corpus annotation for
mining biomedical events from literature. BMC Bioin-
formatics, 9:10.
Kim J.D., Ohta T., Pyysalo S., Kano Y., Tsujii J. (2009).
Overview of BioNLP?09 shared task on event ex-
traction. BioNLP?09 Proc. Workshop Current Trends
in Biomedical Natural Language Processing: Shared
Task, pp. 1-9.
Krallinger M., Leitner F., Rodriguez-Penagos C., Va-
lencia A. (2008). Overview of the protein-protein in-
teraction annotation extraction task of BioCreative II.
Genome Biology, 9(Suppl. 2):S4.
Lammers C.R., Flo?rez L.A., Schmeisky A.G., Roppel
S.F., Ma?der U., Hamoen L., Stu?lke J. (2010). Con-
necting parts with processes: SubtiWiki and Subti-
Pathways integrate gene and pathway annotation for
Bacillus subtilis. Microbiology, 156(3):849-59.
Manine A.P., Alphonse E., Bessie`res P. (2008). Informa-
tion extraction as an ontology population task and its
application to genic interactions. 20th IEEE Int. Conf.
Tools with Artificial Intelligence (ICTAI?08), pp. 74-
81.
Manine A.P., Alphonse E., Bessie`res P. (2009). Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. Int. J. Medical Informat-
ics, 78(12):e31-8.
Manine A.P., Alphonse E., Bessie`res P. (2010). Extrac-
tion of genic interactions with the recursive logical the-
ory of an ontology. Lecture Notes in Computer Sci-
ences, 6008:549-63.
Muggleton S., Raedt L.D. (1994) Inductive Logic Pro-
gramming: Theory and methods. J. Logic Program-
ming, 19-20:629-79.
Ne?dellec C. (2005). Learning Language in Logic ? Genic
Interaction Extraction Challenge. Proc. 4th Learning
Language in Logic Workshop (LLL?05), pp. 31-7.
Weissenbacher, D. (2004). La relation de synonymie en
Ge?nomique. RECITAL 2004 Conference.
73
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 1?7,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of BioNLP Shared Task 2013 
 Claire N?dellec MIG INRA UR1077  F-78352 Jouy-en-Josas cedex claire.nedellec@jouy.inra.fr  
Robert Bossy MIG INRA UR1077  F-78352 Jouy-en-Josas cedex robert.bossy@jouy.inra.fr   Jin-Dong Kim Database Center for Life Science  2-11-16 Yayoi, Bunkyo-ku, Tokyo  jdkim@dbcls.rois.ac.jp  
Jung-jae Kim Nanyang Technological University Singapore  jungjae.kim@ntu.edu.sg  Tomoko Ohta National Centre for Text Mining and School of Computer Science University of Manchester tomoko.ohta@manchester.ac.uk  
Sampo Pyysalo National Centre for Text Mining and School of Computer Science University of Manchester sampo.pyysalo@gmail.com    Pierre Zweigenbaum LIMSI-CNRS F-91403 Orsay  pz@limsi.fr 
 
    Abstract The BioNLP Shared Task 2013 is the third edition of the BioNLP Shared Task series that is a community-wide effort to address fine-grained, structural information extraction from biomedical literature. The BioNLP Shared Task 2013 was held from January to April 2013. Six main tasks were proposed. 38 final submissions were received, from 22 teams. The results show advances in the state of the art and demonstrate that extraction methods can be successfully generalized in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST hereafter) series is a community-wide effort toward fine-grained biomolecular event extraction, from scientific documents. BioNLP-ST 2013 follows the general outline and goals of the previous tasks, namely BioNLP-ST?09 (Kim  et al, 2009) and BioNLP-ST?11 (Kim et al, 
2011). BioNLP-ST aims to provide a common framework for the comparative evaluation of information extraction (IE) methods in the biomedical domain. It shares this common goal with other tasks, namely BioCreative (Critical Assessment of Information Extraction in Biology) (Arighi  et al, 2011), DDIExtraction (Extraction of Drug-Drug Interactions from biomedical texts) (Segura-Bedmar et al, 2011) and i2b2 (Informatics for Integrating Biology and the Bedside) Shared-Tasks (Sun et al, 2013).  The biological questions addressed by the BioNLP-ST series belong to the molecular biology domain and its related fields. With the three editions, the series gathers several groups that prepared various tasks and resources, which represent diverse themes in biology. As the two previous editions, this one measures the progress accomplished by the community on complex text-bound event extraction. Compared to the other initiatives, the BioNLP-ST series proposes a linguistically motivated approach to event representation that enables the evaluation of the participating methods in a unifying computer science framework. Each edition has attracted an 
1
increasing number of teams with 22 teams submitting 38 final results this year. The task setup and the data serve as a basis for numerous further studies, released event extraction systems, and published datasets.  The first event in 2009 triggered active research in the community on a specific fine-grained IE task called Genia event extraction  task. Expanding on this, the second BioNLP-ST was organized under the theme Generalization, where the participants introduced numerous systems that could be straightforwardly applied to different tasks. This time, the BioNLP-ST goes a step further and pursues the grand theme of Knowledge base construction. There were five tasks in 2011, and this year there are 6.  - [GE] Genia Event Extraction for NFkB knowledge base  - [CG] Cancer Genetics  - [PC] Pathway Curation  - [GRO] Corpus Annotation with Gene Regulation Ontology  - [GRN] Gene Regulation Network in Bacteria  - [BB] Bacteria Biotopes  The grand theme of Knowledge base construction is addressed in various ways: semantic web (GE, GRO), pathway (PC), molecular mechanism of cancer (CG), regulation network (GRN) and ontology population (GRO, BB).  In the biology domain, BioNLP-ST 2013 covers many new hot topics that reflect the evolving needs of biologists. BioNLP-ST 2013 broadens the scope of the text-mining application domains in biology by introducing new issues on cancer genetics and pathway curation. It also builds on the well-known previous datasets GENIA, LLL/BI and BB to propose tasks closer to the actual needs of biological data integration.  As in previous events, manually annotated data are provided to the participants for training, development and evaluation of the information extraction methods. According to their relevance for biological studies, the annotations are either bound to specific expressions in the text or represented as structured knowledge. Linguistic processing support was provided to the participants in the form of analyses of the dataset texts produced by state-of-the art tools. This paper summarizes the BioNLP-ST 2013 organization, the task characteristics and their relationships. It gives synthetic figures on the participants and discusses the participating system advances. 
2 Tasks The BioNLP-ST?13 includes six tasks from four groups: DBCLS, NaCTeM, NTU and INRA. As opposed to the last edition, all tasks were main extraction tasks. There were no supporting tasks designed to assist the extraction tasks. All tasks share the same event-based representation and file format, which is similar to the previous editions. This makes it easier to reuse the systems across tasks. Five kinds of annotation types are defined: ? T: text-bound annotation (entity/event trigger) ? Equiv: entity aliases ? E: event ? M: event modification ? R: relation ? N: normalization (external reference) The normalization type has been introduced this year to represent the references to external resources such as dictionaries for GRN or ontologies for GRO and BB. The annotations are stand-off: the texts of the documents are kept separate from the annotations that refer to specific spans of texts through character offsets. More detail and examples can be found on the BioNLP-ST?13 web site. 2.1     Genia Event Extraction (GE) Originally the design and implementation of the GE task was based on the Genia event corpus (Kim et al, 2008) that represents domain knowledge of NF?B proteins. It was first organized as the sole task of the initial 2009 edition of BioNLP-ST (Kim et al, 2009). While in 2009 the data sets consisted only of Medline abstracts, in its second edition in 2011 (Kim et al, 2011b), it was extended to include full text articles to measure the generalization of the technology to full text papers. For its third edition this year, the GE task is organized with the goal of making it a more ?real? task useful for knowledge base construction. The first design choice is to construct the data sets with recent full papers only, so that the extracted pieces of information could represent up-to-date knowledge of the domain. Second, the co-reference annotations are integrated into the event annotations, to encourage the use of these co-reference features in the solution of the event extraction. 
2
2.2     Cancer Genetics (CG) The CG task concerns the extraction of events relevant to cancer, covering molecular foundations, cellular, tissue, and organ-level effects, and organism-level outcomes. In addition to the domain, the task is novel in particular in extending event extraction to upper levels of biological organization. The CG task involves the extraction of 40 event types involving 18 types of entities, defined with respect to community-standard ontologies (Pyysalo et al, 2011a; Ohta et al, 2012). The newly introduced CG task corpus, prepared as an extension of a previously introduced corpus of 250 abstracts (Pyysalo et al, 2012), consists of 600 PubMed abstracts annotated for over 17,000 events. 2.3     Pathway Curation (PC) The PC task focuses on the automatic extraction of biomolecular reactions from text with the aim of supporting the development, evaluation and maintenance of biomolecular pathway models. The PC task setting and its document selection protocol account for both signaling and metabolic pathways. The 23 event types, including chemical modifications (Pyysalo et al, 2011b), are defined primarily with respect to the Systems Biology Ontology (SBO) (Ohta et al, 2011b; Ohta et al, 2011c), involving 4 SBO entity types. The PC task corpus was newly annotated for the task and consists of 525 PubMed abstracts, chosen for the relevance to specific pathway reactions selected from SBML models registered in BioModels and PANTHER DB repositories (Mi and Thomas, 2009). The corpus was manually annotated for over 12,000 events on top of close to 16,000 entities.  2.4     Gene Regulation Ontology (GRO) The GRO task aims to populate the Gene Regulation Ontology (GRO) (Beisswanger et al, 2008) with events and relations identified from text. The large size and the complex semantic representation of the underlying ontology are the main challenges of the task. Those issues, to a greater extent, should be addressed to support full-fledged semantic search over the biomedical literature, which is the ultimate goal of this work.   The corpus consists of 300 MEDLINE abstracts, prepared as an extension of (Kim et al, 2011c). The analysis of the inter-annotator agreement between the two annotators shows 
Kappa values of 43%-56%, which might indicate the difficulty of the task.  2.5     Gene Regulation Network in Bacteria           (GRN) The Gene Regulation Network task consists of the extraction of the regulatory network of a set of genes involved in the sporulation phenomenon of the model organism Bacillus subtilis. Participant system predictions are evaluated with respect to the target regulation network, rather than the text-bound relations. The aim is to assess the IE methods with regards to the needs of systems biology and predictive biology studies. The GRN corpus is a set of sentences from PubMed abstracts that extends the BioNLP-ST 2011 BI (Jourde et al, 2011) and LLL (Nedellec, 2005) corpora. The additional sentences cover a wider range of publication dates and complement the regulation network of the sporulation phenomenon. It has been thoroughly annotated with different levels of biological abstraction: entities, biochemical events, genic interactions and the corresponding regulation network. The network prediction submissions have been evaluated against the reference network using an original metric, the Slot Error Rate (Makhoul et al, 1999) that is more adapted to graph comparison than the usual Recall, Precision and F-score measures.  2.6     Bacteria Biotopes (BB) The Bacteria Biotope (BB) task concerns the extraction of locations in which bacteria live and the categorization of these habitats with concepts from OntoBiotope,1 a large ontology of 1,700 concepts and 2,000 synonyms. The association between bacteria and their habitats is essential information for environmental biology studies, metagenomics and phylogeny. In the previous edition of the BB task, participants had to recognize bacteria and habitat entities, to categorize habitat entities among eight broad types and to extract localization relations between bacteria and their habitats (Bossy et al, 2011). The BioNLP-ST 2013 edition has been split into 3 sub-tasks in order to better assess the performance of the predictive systems for each step. The novelty of this task is mainly the more comprehensive and fine-grained categorization. It addresses the critical problem of habitat normalization necessary for the                                                            1 http://bibliome.jouy.inra.fr/MEM-OntoBiotope 
3
automatic exploitation of bacteria-habitat databases.  2.7     Task characteristics Task features are given in Table 1. Three different types of text were considered: the abstracts of scientific papers taken from PubMed (CG, PC, GRO and GRN), full-text scientific papers (GE) and scientific web pages (BB).   Task Documents # types # events GE 34 Full papers  2 13 CG 600 Abstracts 18 40 PC 525 Abstracts 4 23 GRO 300 Abstracts 174  126 GRN 201 Abstracts 6 12 BB 124 Web pages 563 2 Table 1. Characteristics of the BioNLP-ST 2013 tasks. The number of relations or events targeted greatly varies with the tasks as shown in column 3. The high number of types and events reflect the increasing complexity of the biological knowledge to be extracted. The grand theme of Knowledge base construction in this edition has been translated into rich knowledge representations with the goal of integrating textual data with data from sources other than text. These figures illustrate the shared ambition of the organizers to promote fine-grained information extraction together with an increasing biological plausibility. Beyond gene and protein interactions, they include many complex biological phenomena and environmental factors. 3 BioNLP-ST?13 organization  BioNLP-ST?13 was split in three main periods. During thirteen weeks from mid-January to the first week of April, the participants prepared their systems with the training data. Supporting resources were delivered to participants during this period. Supporting resources were provided by the organizers and by three external providers after a public call for contribution. They range from tokenizers to entity detection tools, mostly focusing on syntactic parsing (Enju (Miyao and Tsujii, 2008), Stanford (Klein and Manning, 2002), McCCJ (Charniak and Johnson, 2005)). The test data were made available for 10 days before the participants had to submit their final results using on-line services. The evaluation results were 
communicated shortly after and published on the ST site. The descriptions of the tasks and representative sample data have been available since October 2012 so that the participants could become acquainted with the task goals and data formats in advance. Table 2 shows the task schedule.  Date Event 23 Oct. 2012 Release of sample data sets 17 Jan 2013 Release of the training data sets 06 Apr. 2013 Release of the test data sets 16 Apr. 2013 Result submission 17 Apr. 2013 Notification of the evaluation results Table 2: Schedule of BioNLP-ST 2013. The BioNLP-ST?13 web site and a dedicated mailing-list have kept the participant informed about the whole process.  4 Participation  GE 1-2-3 CG PC GRO GRN BB 1 - 2-3 EVEX ? ? ?    ?    TEES-2.1 ? ? ? ? ? ? ?  ? ? BioSEM ?          NCBI ?          DlutNLP ?          HDS 4NLP ?          NICTA  ?  ?        USheff ?          UZH  ?          HCMUS ?          NaCTeM     ? ?      NCBI     ?       RelAgent     ?       UET-NII     ?       ISI    ?       OSEE      ?     U. of Ljubljana       ?    K.U. Leuven       ?    IRISA-TexMex       ? ? ?  Boun        ? ?  LIPN        ?   LIMSI        ? ? ? Table 3: Participating teams per task. BioNLP-ST 2013 received 38 submissions from 22 teams (Table 3). One third, or seven teams, participated in multiple tasks. Only one team, UTurku, submitted final results with TEES-2.1 to 
4
all the tasks except one ? entity categorization. This broad participation resulted from the growing capability of the systems to be applied to various tasks without manual tuning. The remaining 15 teams participated in one single task. 5 Results  Table 4 summarizes the best results and the participating systems for each task and sub-task. They are all measured using F-scores, except when it is not relevant, in which case SER is used instead. It is noticeable that the TEES-2.1 system that participated in 9 of the 10 tasks and sub-tasks achieved the best result in 6 cases. Most of the participating systems applied a combination of machine learning algorithms and linguistic features, mainly syntactic parses, with some noticeable exceptions.   Tasks Evaluation results  GE Core event extraction TEES-2.1, EVEX, BioSEM:  0.51 GE 2 Event enrichment TEES2.1:  0.32 GE 3 Negation/Speculation TEES-2.1, EVEX:   0.25 CG TEES-2.1:  0.55 PC NaCTeM:  0.53 
GRO TEES-2.1: 0.22 (events),   0.63 (relations) 
GRN U. of Ljubljana:   0.73 (SER) BB 1 Entity detection and categorization IRISA: 0.46 (SER) BB 2 Relation extraction IRISA:  0.40 BB 3 Full event extraction TEES-2.1:  0.14 Table 4. Best results and team per task  (F-score, except when SER). Twelve teams submitted final results to the GE task. The performance of highly ranked systems shows that the event extraction technology is applicable to the most recent full papers without drop of performance. Six teams submitted final results to the CG task. The highest-performing systems achieved 
results comparable to those for established molecular level extraction tasks (Kim et al, 2011). The results indicate that event extraction methods generalize well to higher levels of biological organization and are applicable to the construction of knowledge bases on cancer. Two teams successfully completed the PC task, and the highest F-score reached 52.8%, indicating that event extraction is a promising approach to support pathway curation efforts. The GRN task attracted five participants. The best SER score was 0,73 (the higher, the worse), which shows their capability of designing regulatory network, but handling modalities remains an issue. Five teams participated to the 3 BB subtasks with 10 final submissions. Not surprisingly, the systems achieved better results in relation extraction than habitat categorization, which remains a major challenge in IE. One team participated in the GRO task, and their results were compared with those of a preliminary system prepared by the task organizers. An analysis of the evaluation results leads us to study issues such as the need to consider the ontology structure and the need for semantic analysis, which are not seriously dealt with by current approaches to event extraction. 6 Organization of the workshop The BioNLP Shared Task 2013 (BioNLP-ST) workshop was organized as part of the ACL BioNLP 2013 workshop. After submission of their system results, participants were invited to submit a paper on their systems to the workshop.  Task organizers were also invited to present overviews of each task, with analyses of the participant system features and results. The workshop was held in August 2013 in Sofia (Bulgaria). It included overview presentations on tasks, as well as oral and poster presentations by Shared Task participants.  7 Discussion and Conclusion This year, the tasks has significantly gained in complexity to face the increasing need for Systems Biology knowledge from various textual sources. The high level of participation and the quality of the results show that the maturity of the field is such that it can meet this challenge. The innovative and various solutions applied this year will without doubt be extended in the future. As for previous editions of BioNLP-ST, all tasks maintain an online evaluation service that is 
5
publicly available. This on-going challenge will contribute to the assessment of the evolving information extraction field in the biomedical domain. References  Auhors. 2013. Title. In Proceedings of the BioNLP 2013 Workshop Companion Volume for Shared Task, Sofia, Bulgaria. Association for Computational Linguistics. Arighi, C., Lu, Z., Krallinger, M., Cohen, K., Wilbur, W., Valencia, A., Hirschman, L. and Wu, C. 2011. Overview of the BioCreative III Workshop. BMC Bioinformatics, 12, S1. E Beisswanger, V Lee, JJ Kim, D Rebholz-Schuhmann, A Splendiani, O Dameron, S Schulz, U Hahn. Gene Regulation Ontology (GRO): Design principles and use cases. Studies in Health Technology and Informatics, 136:9-14, 2008. BioNLP-ST?13 web site: https://2013.bionlp-st.org Robert Bossy, Julien Jourde, Philippe Bessi?res, Maarten van de Guchte, Claire N?dellec. 2011. BioNLP shared Tasks 2011 - Bacteria Biotope. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland, USA, 2011. Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173?180. Association for Computational Linguistics. Julien Jourde, Alain-Pierre Manine, Philippe Veber, Karen Fort, Robert Bossy, Erick Alphonse, Philippe Bessi?res. 2011. BioNLP Shared Task 2011 - Bacteria Gene Interactions and Renaming. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland. Jin-Dong Kim, Tomoko Ohta and Jun'ichi Tsujii, 2008, Corpus annotation for mining biomedical events from literature, BMC Bioinformatics, 9(1): 10. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano and Jun'ichi Tsujii. 2009. Overview of BioNLP'09 Shared Task on Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1-9. Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen and Jun?ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. In Proceedings of BioNLP 2011 Workshop, pages 1-6. Association for Computational Linguistics. 
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011b. Overview of the Genia Event task in BioNLP Shared Task 2011. In Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. Association for Computational Linguistics. Jung-Jae Kim, Xu Han and Watson Wei Khong Chua. 2011c. Annotation of biomedical text with Gene Regulation Ontology: Towards Semantic Web for biomedical literature. Proceedings of LBM 2011, pp. 63-70. Dan Klein and Christopher D Manning. 2002. Fast ex act inference with a factored model for natural language parsing. Advances in neural information processing systems, 15(2003):3?10. John Makhoul, Francis Kubala, Richard Schwartz and Ralph Weischedel. 1999.  Performance measures for information extraction. In Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February. Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35?80. Huaiyu Mi and Paul Thomas. 2009. PANTHER path- way: an ontology-based pathway database coupled with data analysis tools. In Protein Networks and Pathway Analysis, pages 123?140. Springer. Claire N?dellec. 2005. Learning Language in Logic - Genic Interaction Extraction Challenge. In Proceedings of the Learning Language in Logic (LLL05) workshop joint to ICML'05. Cussens J. and Nedellec C. (eds). Bonn, August. Tomoko Ohta, Sampo Pyysalo, Sophia Ananiadou, and Jun'ichi Tsujii. 2011b. Pathway curation support as an information extraction task. Proceedings of LBM 2011. Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011c. From pathways to biomolecular events: opportunities and challenges. In Proceedings of BioNLP 2011 Workshop, pages 105?113. Association for Computational Linguistics. Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Open-domain anatomical entity mention detection. In Proceedings of DSSD 2012, pages 27?36. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Event extraction across multiple levels of biological organization. Bioinformatics, 28(18):i575-i581. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and Jun'ichi Tsujii. 2011b. Towards exhaustive event extraction for protein modifications. In Proceedings of the BioNLP 2011 Workshop, 
6
pp.114-123, Association for Computational Linguistics. Sampo Pyysalo, Tomoko Ohta, Jun'ichi Tsujii and Sophia Ananiadou. 2011a. Anatomical Entity Recognition with Open Biomedical Ontologies. In proceedings of LBM 2011. Isabel Segura-Bedmar, Paloma Martinez, and Daniel Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011 challenge task: Extraction of Drug-Drug Interactions from biomedical texts. In Proceedings of the 1st Challenge Task on Drug-Drug Interaction Extraction 2011, SEPLN 2011 satellite workshop. Huelva, Spain, September 7. Weiyi Sun, Anna Rumshisky, Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2 Challenge. J Am Med Inform Assoc.
7
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 153?160,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013 ? An overview of the Genic Regulation Network Task 
Robert Bossy, Philippe Bessi?res, Claire N?dellec  Unit? Math?matique, Informatique et G?nome Institut National de la Recherche Agronomique UR1077, F78352 Jouy-en-Josas, France forename.name@jouy.inra.fr  Abstract 
The goal of the Genic Regulation Network task (GRN) is to extract a regulation network that links and integrates a variety of molecular interactions between genes and proteins of the well-studied model bacterium Bacillus subtilis. It is an extension of the BI task of BioNLP-ST?11. The corpus is composed of sentences selected from publicly available PubMed scientific abstracts. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results. 1 Introduction  The Genic Regulation Network (GRN) task consists of (1) extracting information on molecular interactions between genes and proteins that are described in scientific literature, and (2) using this information to reconstruct a regulation network between molecular partners in a formal way. Several other types of biological networks can be defined at the molecular level, such as metabolisms, gene expressions, protein-protein interactions or signaling pathways.  All these networks are closely interconnected. For example, a gene codes for a protein that catalyzes the transformation of small molecules (metabolites), while the expression of the gene and its related regulation is controlled by other proteins. The concept of biological networks is not new. However, the development of new methods in molecular biology in the past twenty years has made them accessible at the level of an organism as a whole. These new methods allow for the design of large-scale experimental approaches with high throughput rates of data. They are then used to build static and dynamic models that represent the behavior of a cell in the field of Systems Biology (Kitano, 2002; de Jong, 2002). In this context, there has recently been a 
considerable focus on ?biological network inference?, that is to say the process of making inferences and predictions about these networks (D'haeseleer, et al, 2000). Therefore, it is expected that Information Extraction (IE) from scientific literature may play an important role in the domain, contributing to the construction of networks (Blaschke et al, 1999). IE also plays a role in the design and the validation of large-scale experiments, on the basis of detailed knowledge that has already been published. 2 Context Extracting molecular interactions from scientific literature is one of the most popular tasks in IE challenges applied to biology. The GRN task adds a supplementary level that is closer to the biological needs: the participant systems have to extract a regulation network from the text that links and integrates basic molecular interactions.  The GRN task is based on a series of previous challenges in IE that started with the LLL challenge in 2005 (N?dellec, 2005). The LLL corpus is a set of sentences of PubMed abstracts about molecular interactions of the model bacterium Bacillus subtilis. Originally, the LLL task defined a unique binary genic interaction relation between proteins and genes. Since then, it has evolved to include the description of interaction events in a fine-grained representation that includes the distinction between transcription, different types of regulations and binding events, as proposed by (Manine et al, 2009). This new schema better captures the complexity of regulations at the molecular level. Entities other than genes and proteins were introduced, such as DNA sites (e.g. transcription promoter sites, transcriptional regulator binding sites). We proposed the Genic Interaction task (Bossy et al, 2012) in the BioNLP?11 Shared Task with a full re-annotation of the LLL corpus that follows this schema. The GRN task in 
153
BioNLP-ST?13 builds on this corpus and includes annotation improvements and extensions that are detailed below. 3 Task description The BioNLP-ST 2013 GRN task consists of the automatic construction of the regulation network that can be derived from a set of sentences. As usual in relation extraction tasks, the GRN corpus includes text-bound annotations. However the extraction target is the network, which is a structure with a higher level of abstraction. GRN thus also provides an explicit procedure to derive a network from a set of text-bound annotations. The GRN annotation is stacked in four successive levels of annotation: 1. Text-bound entities represent genes, proteins and aggregates (families, complexes). Some entities directly relate to a gene and are given a unique gene identifier corresponding to a node of the network. These entities are hereby called genic named entities. 2. Biochemical events and relations are molecular-level events (e.g. transcription, binding) and detailed knowledge on relationships between entities (e.g. promoter of gene, regulon membership). 3. Interactions denote relations between entities and events and relations. Interactions are the first abstract annotations; they are the key to the construction of the network arcs. 4. Finally, the Genic Regulation Network is derived from the Interactions and from the identifiers of the named genic entities.  GerE is a DNA-binding protein that adheres    to the promoter of cotB and cotC    Figure 1. Example of annotated sentence.  Levels 1, 2 and 3 were obtained by a manual annotation of the GRN corpus sentences by a domain expert. Level 4 was automatically computed from the lower level annotations. The training corpus was provided to the participants with level 1, 2 and 3 annotations. The algorithm 
to compute the next level was described and implemented as a script and made available to the participants during the training stage of the challenge. The test corpus was provided with only  level 1 annotations (entities). The participants submitted their prediction either as a set of Interactions (level 3) or directly as a network (level 4). This setting allows the participants to train systems that work at different levels of abstraction. Submissions in the form of Interactions are translated into a Genic Regulation Network using the algorithm provided during the training stage. The evaluation of each submission is carried out by comparing the predicted network with the reference network. The reference network is itself computed from the gold level 1, 2 and 3 annotations of the test sentences. The following subsections describe the four annotation levels. The full annotation schema that specifies the constraints on event and relation arguments can be found on the task web page1. 3.1 Text-bound entity types Text-bound entities come in three kinds: event trigger words, genic entities and entity aggregates. Trigger words are of type Action, they serve as anchors for events. Genic entities represent mentions of biochemical objects of the bacteria cell. Genic entity types include Gene, mRNA, Promoter, Protein and Site. Finally aggregates denote composite objects of the bacteria cell. Aggregate types are: - GeneFamily: homologous gene families. - Operon: operons sensu prokaryotes. - PolymeraseComplex: RNA polymerase complexes, either the core complex alone, or bound to a sigma factor. - ProteinComplex: protein complexes formed by several proteins that bind together. - ProteinFamily: homologous protein families. - Regulon: regulons, sensu prokaryotes. 3.2 Biochemical events and relation types Biochemical events and relations represent the knowledge of cellular mechanisms at the molecular level. There are three types of events: - Transcription_by represents the transcription event by a specific RNA 
                                                       1 https://sites.google.com/site/bionlpst2013/tasks/gene-regulation-network 
Master of  Promoter Interaction 
Promoter of 
154
polymerase. Its agent is usually a PolymeraseComplex. - Transcription_from represents the transcription from a specific site or promoter. - Action_Target is a generic bio-molecular event. The relation types represent three major genetic regulation patterns in bacteria: promoter activation, regulons and binding to specific DNA sites. Two types of relations specifically denote mechanisms that involve promoters: - Promoter_of is a relation between a gene (or operon) and its promoter. - Master_of_Promoter relation represents the control of the transcription from a specific promoter by a proteic entity (Protein, ProteinComplex or ProteinFamily). Two other relation types represent the function of regulons: - Member_of_Regulon relation denotes the membership of a genic entity to a regulon. - Master_of_Regulon relation represents the control of the activity of an entire regulon by a protein. Finally two types are used to represent relations that are common to different regulation mechanisms: - Bind_to relation represents the binding of a proteic entity to a site on the chromosome. - Site_of relation denotes the belonging of a chromosomal site to a genic entity such as a gene or a promoter. 3.3 Interaction types Interaction relations are labeled with one of six types grouped into a small hierarchy following two axes: mechanism and effect. The hierarchical levels are figured here by the text indentations. Regulation Binding Transcription Activation Requirement Inhibition Figure 2. Types of Interaction relations 
The Binding and Transcription types specify the mechanism through which the agent regulates the target. In a Binding Interaction, the agent binds to the target; this includes Protein-DNA binding and excludes Protein-Protein binding mechanisms. In a Transcription Interaction, the agent affects the transcription of the target. The Activation, Requirement and Inhibition types specify the effect of the agent on the target. In an Activation Interaction, the agent increases the expression of the target. In a Requirement Interaction, the agent is necessary for the expression of the target. In an Inhibition Interaction, the agent reduces the expression of the target. The Regulation type is the default type: in such interactions, neither the mechanism nor the effect is specified. 3.4 Genic Regulation Network inference algorithm The genic regulation network corresponding to a corpus is inferred from the set of Interaction relations. The network presents itself as a directed labeled graph where nodes represent gene identifiers and edges represent gene interactions. The inference is done in two steps: the resolution of Interaction relations and the removal of redundant arcs. Step 1: Resolution of Interaction relations The agent and the target of an Interaction relation are not necessarily genic named entities. They can be secondary events or relations, another Interaction, or auxiliary entities (e.g. Promoter). The resolution of an Interaction aims to look for the genic named entity in order to infer the node concerned by the network edge. The resolution of Interaction arguments is performed using the rules specified below. These rules express well-known molecular mechanisms in a logical manner: 1. If the agent (or target) is a genic named entity, then the agent (or target) node is the gene identifier of the entity. If the entity does not have a gene identifier, then it is not a genic named entity and there is no node (and thus no edge). 2. If the agent (or target) is an event, then the agent (or target) node is the entity referenced by the event. 3. If the agent (or target) is a relation, then the agent (or target) of both arguments of the relation are nodes. 
Mechanism 
Effect 
155
4. If the target is a Promoter and this promoter is the argument of a Promoter_of relation, then the target node is the other argument of the Promoter_of relation. i.e. if A interacts with P, and P is a promoter of B, then A interacts with B. 5. If the agent is a Promoter and this promoter is the argument of a Master_of_Promoter relation, the agent is the other argument of the Master_of_Promoter relation. i.e. if A is the master of promoter P, and P interacts with B, then A interacts with B. The resolution of Interaction arguments consists of a traversal of the graph of annotations where these rules are applied iteratively. Event and relation arguments are walked through. Promoter entities are handled according to rules 4 and 5. If the resolution of the agent or the target yields more than one node, then the Interaction resolves to as many edges as the Cartesian product of the resolved nodes. For instance, if both the agent and the target resolve to two nodes, the Interaction relation resolves into four edges. Edges are labeled with the same set of types as the Interactions. Each edge inherits the type of the Interaction relation from which it has been inferred. Step 2: Removal of redundant arcs In this step, edges with the same agent, target and type are simplified into a single edge. This means that if the same Interaction is annotated several times in the corpus, then it will resolve into a single edge. This means that the prediction of only one of the interactions in the corpus is enough to reconstruct the edge. Moreover, Interaction types are ordered according to the hierarchy defined in the preceding section. Since the sentences are extracted from PubMed abstracts published during different periods, they may mention the same Interaction with different levels of detail, depending on the current state of knowledge. For a given edge, if there is another edge for the same node pair with a more specialized type, then it is removed. For instance, the edges (A, Regulation, B) and (A, Transcription, B) are simplified into (A, Transcription, B). Indeed the former edge conveys no additional information in comparison with the latter.  4 Corpus description The GRN corpus is a set of 201 sentences selected from PubMed abstracts, which are 
mainly about the sporulation phenomenon in Bacillus subtilis. This corpus is an extended version of the LLL and BI (BioNLP-ST?11) corpora. The additional sentences ensure a better coverage of the description of the sporulation. An expert of this phenomenon examined the regulation network derived from the annotation of the original sentences, and then manually listed the important interactions that were missing. We selected sentences from PubMed abstracts that contain occurrences of the missing pairs of genes. In this way, the genic interaction network is more complete with respect to the sporulation. Moreover, the publications from which the sentences are extracted cover a wider period, from 1996 to 2012. They represent a diverse range of writing styles and experimental methods. 42 sentences have been added, but 4 sentences were removed from the BI sentences because they described genic interactions in bacteria other than Bacillus subtilis. The distribution of the sentences among the training, development and test sets has been done in the following way: - Legacy sentences belong to the same set as in previous evaluation campaigns (LLL and BI). - Additional sentences have been randomly distributed to training, development and test sets. The random sampling has been constrained so that the proportion of different types of interactions is as much as possible the same as in the three sets. The GRN task does not include the automatic selection by the participant methods of the relevant sentences, which are provided. With regards to a real-world application, this selection step can be achieved with good performance by sentence filtering, as demonstrated by N?dellec et al (2001), by using a Naive Bayesian classifier. Moreover, the corpus contains sentences with no interaction. Tables 1 to 3 detail the distribution of the entities, relations and events in the corpus. They are balanced between the training and test sets: the test represents between a quarter and a third of the annotations. Table 1 details the entity frequency and their distributions by type. Column 5 contains the contribution of each entity type to the total. Genes and proteins represent two thirds of the entities, since they are the main actors in genic interactions. It is worth noting that the high number of promoters and polymerase complexes is specific to bacteria 
156
where the biological mechanisms are detailed at a molecular level.  Entity # Train+Dev Test Gene 199 70% 30% GeneFamily 2 50% 50% mRNA 1 100%  0% Operon 33 67% 33% PolymeraseComplex 62 71% 29% Promoter 63 73% 27% Protein 486 65% 35% ProteinComplex 7 100%  0% ProteinFamily 18 78% 22% Regulon 14 79% 21% Site 32 78% 22% Total 917 68% 32% 
Table 1. Entity distribution in the GRN corpus. Table 2 details the distribution of the biochemical events and relations (level 2). The most frequent event is Action Target. Action Target links, for instance, Transcription by and Transcription from events to the target gene.  Event/Relation # Train+dev Test Action target 226 68% 32% Bind to 9 78% 22% Master of Promoter 60 80% 20% Master of Regulon 13 85% 15% Member of Regulon 12 92% 8% Promoter of 47 72% 28% Site of 24 75% 25% Transcription by 86 71% 29% Transcription from 18 78% 22% 
Total 495 72% 28% Table 2. Distribution of the biochemical events and relations in the GRN corpus. Finally, Table 3 details the distribution of the Interaction relations (level 3). The distribution 
among Interaction relations is more uniform than among entities and molecular events. The frequency of the Transcription relation is much higher than Binding, which is not surprising since transcription is the major mechanism of regulation in bacteria, while binding is rare. Conversely, the relative frequency of relations among Effect types of relations is balanced.  Interaction  # Train+dev Test Regulation 80 65% 35% Inhibition 50 66% 34% Activation 49 67% 33% Requirement 35 66% 34% Binding 12 75% 25% Transcription 108 74% 26% Total 334 69% 31% Table 3. Distribution of the Interaction relations in the GRN corpus. 5 Annotation methodology A senior biologist, who is a specialist of Bacillus subtilis and a bioinformatician, a specialist of semantic annotation, defined the annotation schema. The biologist annotated the whole corpus, using the BI annotations as a starting point. The bioinformatician carefully checked each annotation. They both used the AlvisAE Annotation Editor (Papazian et al, 2012) that supported their productivity due to its intuitive visualization of dense semantic annotations. Subtiwiki provided the identifiers of genes and proteins (Fl?rez et al, 2009). Subtiwiki is a community effort that has become the reference resource for the gene nomenclature normalization of Bacillus subtilis. Other genic named entities, like operons, families or protein complexes, were given an identifier similar to their surface form. Several annotation iterations and regular cross-validations allowed the annotators to refine and normalize these identifiers. The consistency of the annotations was checked by applying the rules of the network inference procedure that revealed contradictions or dangling events. The biologist double-checked the inferred network against his deep expertise of sporulation in Bacillus subtilis. 
157
6 Evaluation procedure 6.1 Campaign organization The same rules and schedule were applied to GRN as the other BioNLP-ST tasks. The training and development data were provided eleven weeks before the test set. The submissions were gathered through an on-line service, which was active for ten days. We took into account the final run of each participant to compute the official scores. They were published on the BioNLP-ST web site together with the detailed scores. 6.2 Evaluation metrics The predictions of the participating teams were evaluated by comparing the reference network to the predicted network that was either submitted directly, or derived from the predicted Interactions. Since the genic named entity annotations are provided with their identifier, the network nodes are fixed. Therefore, the evaluation consists of comparing the edges of the two networks. Their discrepancy is measured using the Slot Error Rate (SER) defined by (Makhoul et al, 1999) as: SER = (S + D + I) / N where: - S is the number of substitutions (i.e. edges predicted with the wrong type) - D is the number of deletions (false negatives) - I is the number of insertions (false positives) - N is the number of arcs in the reference network. The SER has the advantage over F1, namely it uses an explicit characterization of the substitutions. (Makhoul et al, 1999) demonstrates that the implicit comprehension of substitutions in both recall and precision scores leads to the underestimation of deletions and insertions in the F score. However, we compute the Recall, Precision and F1 in order to make the interpretation of results easier: Recall = M / N Precision = M / P where: - M is the number of matches (true positives). - P is the number of edges in the predicted network. Matches, substitutions, deletions and insertions are counted for each pair of nodes. The genic regulation network is an oriented graph, thus the 
node pairs (A,B) and (B,A) are handled independently. For a given node pair (A,B), the number of exact matches (M) is the number of edges with the same type in the prediction as in the reference. The number of substitutions, deletions and insertions depends on the number of remaining edges. We name q and r, the number of remaining edges between two nodes A and B in the prediction and the reference respectively: - S = min(q, r) - if q > r, then I = q ? r, D = 0 - if q < r, then I = 0, D = r ? q In other words, edges from the prediction and the reference are paired, first by counting matches, then by maximizing substitutions. The remaining edges are counted either as insertions or deletions depending if the extra edges are in the prediction or reference, respectively. The values of S, D, I and M for the whole network are the sum of S, D, I and M on all the node pairs. 7 Results 7.1 Participating systems Five systems participated in GRN: - University of Ljubljana (Slovenia) (?itnik et al, 2013),  - K.U.Leuven (Belgium) (Provoost and Moens, 2013),  - IRISA-TexMex (INRIA, France) (Claveau, 2013), - EVEX (U. of Turku / TUCS, Finland and VIB / U. of Ghent, Belgium) (Hakala et al, 2013),  - TEES-2.1 (TUCS, Finland) (Bj?rne and Salakoski, 2013). 
 Participant SER Recall Precision U. of Ljubljana  0.73 34% 68% K.U.Leuven  0.83 23% 50% TEES-2.1  0.86 23% 54% IRISA-TexMex  0.91 41% 40% EVEX  0.92 13% 44% Table 4. Final evaluation of the GRN task. Teams are ranked by SER. S: Substitutions, D: Deletions, I: Insertions, M: Matches. 
158
Table 4 summarizes the scores by decreasing order. The scores are distributed between the best SER, 0.73 achieved by the University of Ljubljana, 20 points more than the lowest at 0.92. For all systems, the number of insertions is much lower than the number of deletions, except for IRISA-TexMex. The substitutions correspond to the edges that were predicted with the wrong type. In order to reveal the quality of the predictions with regards to the edge types, we calculated two alternate SERs. The results are displayed in Table 5.The SER Network Shape is obtained by erasing the type of all of the edges in the reference and predicted networks, as if all edges were of the Regulation type. The SER Network Shape measures the capacity of the systems to reconstruct the unlabeled shape of the regulation network. The SER Effect is obtained by erasing the mechanism types of all edges only, as if Binding and Transcription edges were of type Regulation. The Effect edges are kept unchanged. The SER Effect measures the quality of the predictions for valued networks that only contain Effect edges.  Participant SER SER Shape SER Effect U. of Ljubljana 0.73 0.60 0.74 
K.U. Leuven 0.83 0.64 0.83 
TEES-2.1 0.86 0.74 0.84 
IRISA-TexMex 0.91 0.51 0.87 
EVEX 0.92 0.79 0.91 
Table 5. Scores obtained by erasing edge types (Network Shape) or mechanism types (Effect). The SER Network Shape is significantly better for all systems, but the impact is dramatic for IRISA-TexMex and K.U. Leuven, showing that the typing of relations may be the major source of error. The SER Effect does not differ significantly from the original score. We deduce from the comparison of the three scores that the types that are the hardest to discriminate are effect types. This result is interesting because Effect labels are in fact the most valuable for systems biology and network inference studies. U. of Ljubljana and TEES-2.1 submissions contained level 2 and 3 predictions (interactions and biochemical events). IRISA provided only 
predictions at level 3 (interactions only). K.U. Leuven and EVEX directly submitted a network. The performance of the systems that use annotations of level 2 confirms our hypothesis that a significant part of the interactions can be deduced from low-level events. 7.2     Systems description and result analysis All systems applied machine-learning algorithms with linguistic features that were stems or lemmas, POS-tags and parses, most of them being provided by the BioNLP supporting resources. With the exception of K.U. Leuven, all systems used dependency paths between candidate arguments. However different ML algorithms were used, as shown in Table 6.  Participant ML algorithm U. Ljubljana Linear-chain CRF K.U.Leuven SVM (Gaussian RBF) TEES-2.1 SVMmulticlass (linear) IRISA-TexMex kNN (language model) EVEX SVM (TEES-2.1) Table 6. ML algorithms used by the participants. Beyond syntactic parses and ML algorithms, the participant systems combined many different sources of information and processing, so that no definitive conclusion on the respective potential of the methods can be drawn here. 8 Conclusion The GRN task has a strong legacy since the corpus is derived from LLL. Moreover, the GRN task has advanced a novel IE setting. We proposed to extract a formal data structure from successive abstract layers. Five different teams participated in the task with distinct strategies. In particular, we received submissions that work on all proposed abstraction levels. This shows that Information Extraction implementations have reached a state of maturity, which allow for new problems to be addressed quickly. The performances are promising, yet some specific problems have to be addressed, like the labeling of edges. Acknowledgments This work was partially supported by the Quaero programme funded by OSEO (the French agency for innovation). 
159
References Jari Bj?rne, Tapio Salakoski. 2013. TEES 2.1: Automated Annotation Scheme Learning in the BioNLP 2013 Shared Task. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Christian Blaschke, Miguel A. Andrade, Christos Ouzounis, Alfonso Valencia. 1999. Automatic Extraction of Biological Information From Scientific Text: Protein-Protein Interactions. Proceedings of the International Conference on Intelligent Systems for Molecular Biology (ISMB 1999), 60-67.  Robert Bossy, Julien Jourde, Alain-Pierre Manine, Philippe Veber, Erick Alphonse, Marteen van de Guchte, Philippe Bessi?res, Claire N?dellec. 2012. BioNLP Shared Task - The Bacteria Track. BMC Bioinformatics. 13(Suppl 11):S3. Vincent Claveau. 2013. IRISA participation to BioNLP-ST 2013: lazy-learning and information retrieval for information extraction tasks. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Patrik D'haeseleer, Shoudan Liang, Roland Somogyi. 2000. Genetic network inference: from co-expression clustering to reverse engineering. Bioinformatics. 16(8):707-726. Lope A. Fl?rez, Sebastian F Roppel, Arne G Schmeisky, Christoph R Lammers, J?rg St?lke. 2009. A community-curated consensual annotation that is continuously updated: the Bacillus subtilis centred wiki SubtiWiki. Database (Oxford), 2009:bap012. Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer and Filip Ginter. 2013. EVEX in ST?13: Application of a large-scale text mining resource to event extraction and network construction. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. 
GenBank. http://www.ncbi.nlm.nih.gov/  Hidde de Jong. 2002. Modeling and simulation of genetic regulatory systems: a literature review. J. Computational Biology, 9(1):67-103. Hiroaki Kitano. 2002. Computational systems biology. Nature, 420(6912):206-210. John Makhoul, Francis Kubala, Richard Schwartz and Ralph Weischedel. 1999.  Performance measures for information extraction. In Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February. Alain-Pierre Manine, Erick Alphonse, Philippe Bessi?res. 2009. Learning ontological rules to extract multiple relations of genic interactions from text. Int. J. Medical Informatics, 78(12):31?38. Claire N?dellec, Mohamed Ould Abdel Vetah, Philippe Bessi?res. 2001. Sentence filtering for information extraction in genomics, a classification problem. Practice of Knowledge Discovery in Databases (PKDD 2001), 326-337. Claire N?dellec. 2005. Learning Language in Logic - Genic Interaction Extraction Challenge" in Proceedings of the Learning Language in Logic (LLL05) workshop joint to ICML'05. Cussens J. and N?dellec C. (eds). Bonn. Fr?d?ric Papazian, Robert Bossy and Claire N?dellec. 2012. AlvisAE: a collaborative Web text annotation editor for knowledge acquisition. The 6th Linguistic Annotation Workshop (The LAW VI), Jeju, Korea. Thomas Provoost, Marie-Francine Moens. 2013. Detecting Relations in the Gene Regulation Network. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Slavko ?itnik, Marinka ?itnik, Bla? Zupan, Marko Bajec. 2013. Extracting Gene Regulation Networks Using Linear-Chain Conditional Random Fields and Rules. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics.  
160
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 161?169,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP shared Task 2013 ? An Overview of the  Bacteria Biotope Task 
Robert Bossy1, Wiktoria Golik1, Zorana Ratkovic1,2, Philippe Bessi?res1, Claire N?dellec1  1Unit? Math?matique, Informatique et G?nome MIG INRA UR1077 ? F-78352 Jouy-en-Josas ? France 2LaTTiCe UMR 8094 CNRS, 1 rue Maurice Arnoux, F-92120 Montrouge ? France forename.name@jouy.inra.fr  Abstract 
This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2013, which follows BioNLP-ST-11. The Bacteria Biotope task aims to extract the location of bacteria from scientific web pages and to characterize these locations with respect to the OntoBiotope ontology. Bacteria locations are crucial knowledge in biology for phenotype studies. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results.  1 Introduction The Bacteria Biotope (BB) task extends the BioNLP 2013 Shared Task molecular biology scope. It consists of extracting bacteria and their locations from web pages, and categorizing the locations with respect to the OntoBiotope1 ontology of microbe habitats. The locations denote the places where given species live. The bacteria habitat information is critical for the study of the interaction between the species and their environment, and for a better understanding of the underlying biological mechanisms at a molecular level. The information on bacteria biotopes and their properties is very abundant in scientific literature and in genomic databases and BRC (Biology Resource Center) catalogues. However, the information is highly diverse and expressed in natural language (Bossy et al, 2012). The two critical missing steps for population of biology databases and biotope knowledge modeling are (1) the automatic extraction of organism/location pairs and (2) the normalization of the habitat names with respect to biotope ontologies.                                                         1http://bibliome.jouy.inra.fr/MEM-OntoBiotope/OntoBiotope_BioNLP-ST13.obo 
The aim of the previous edition of the BB task (BioNLP-ST?11) was to solve the first information extraction step. The results obtained by the participant systems reached 45 percent F-measure. These results showed both the feasibility of the task, as well as a large room for improvement (Bossy et al, 2012).  The 2013 edition of the BB task maintains the primary objective of event extraction, and introduces the second issue of biotope normalization. It is handled through the categorization of the locations into a large set of types defined in the OntoBiotope ontology. Bacteria locations range from hosts, plant and animals, to natural environments (e.g. water, soil), including industrial environments.  BB?11 set of categories contained 7 types. This year, entity categorization has been enriched to better answer the biological needs, as well as to contribute to the general problem of automatic semantic annotation by ontologies. BB task is divided into three sub-tasks. Entity detection and event extraction are tackled by two distinct sub-tasks, so that the contribution of each method could be assessed. A third sub-task conjugates the two in order to measure the impact of the method interactions. 2 Context Biological motivation. Today, new sequencing methods allow biologists to study complex environments such as microbial ecosystems. Therefore, the sequence annotation process is facing radical changes with respect to the volume of data and the nature of the annotations to be considered. Not only do biochemical functions still need to be assigned to newly identified genes, but biologists have to take into account the conditions and the properties of the ecosystems in which microorganisms are living and are identified, as well as the interactions and relationships developed with their environment and other 
161
living organisms (Korbel et al, 2005). Metagenomic studies of ecosystems yield important information on the phylogenetic composition of the microbiota. The availability of bacteria biotope information represented in a formal language would then pave the way for many new environment-aware bioinformatic services. The development of methods that are able to extract and normalize natural language information at a large scale would allow us to rapidly obtain and summarize information that the bacterial species or genera are associated with in the literature. In turn, this will allow for the formulation of hypotheses regarding properties of the bacteria, the ecosystem, and the links between them.  The pioneering work on EnvDB (Pignatelli et al, 2009) aimed to link GenBank sequences of microbes to biotope mentions in scientific papers. However, EnvDB was affected by the incompleteness of the GenBank isolation source field, the low number of related bibliographic references, the bag-of-words extraction method and the small size of its habitat classification. Habitat categories. The most developed classifications of habitats are EnvO, the Metagenome classification supported by the Genomics Standards Consortium (GSC), and the OntoBiotope ontology developed by our group. EnvO (Environment Ontology project) targets a Minimum Information about a Genome Sequence (MIGS) specification (Field et al, 2008) of mainly Eukaryotes. This ambitious detailed environment ontology aims to support standard manual annotations  of all types of organism environments and biological samples. However, it suffers from some limitations for bacterial biotope descriptions. A large part of EnvO is devoted to environmental biotopes and extreme habitats, whilst it fails to finely account for the main trends in bacteria studies, such as their technological use for food transformation and bioremediation, and their pathogenic or symbiotic properties. Moreover, EnvO terms are often poorly suited for bacteria literature analysis (Ratkovic et al, 2012). The Metagenome Classification  from JGI of DOE (Joint Genome Institute, US Department Of Energy) is intended to classify metagenome projects and samples according to a mixed typology of habitats (e.g. environmental, host) and their physico-chemical properties (e.g. pH, salinity) (Ivanova et al, 2010). It is a valuable 
source of vocabulary for the analysis of bacteria literature, but its structure and scope are strongly biased by the indexing of metagenome projects. The OntoBiotope ontology is appropriate for the categorization of bacteria biotopes in the BB task because its scope and its organization reflect the scientific subject division and the microbial diversity. Its size (1,756 concepts) and its deep hierarchical structure are suitable for a fine-grained normalization of the habitats. Its vocabulary has been selected after a thorough terminological analysis of relevant scientific documents, papers, GOLD (Chen et al, 2010) and GenBank, which was partly automated by term extraction. Related terms are attached to the OntoBiotope concept labels (i.e. 383 synonyms), improving OntoBiotope coverage of natural language documents.  Its structure and a part of its vocabulary have been inspired by EnvO, the Metagenome classification and the small ATCC (American Type Collection Culture) classification for microbial collections (Floyd et al, 2005). Explicit references to 34 EnvO terms are given in the OntoBiotope file. Its main topics are: - ? Artificial ? environments (industrial and domestic), Agricultural habitats, Aquaculture habitats, Processed food; - Medical environments, Living organisms, Parts of living organisms, Bacteria-associated habitats; - ? Natural ? environment habitats, Habitats wrt physico-chemical property (including extreme ones); - Experimental medium (i.e. experimental biotopes designed for studying bacteria). The structure, the comprehensiveness and the detail of the habitat classification are critical factors for research in biology. Biological investigations involving the habitats of bacteria are very diverse and still unanticipated. Thus, shallow and light classifications are insufficient to tackle the full extent of the biological questions. Indexing genomic data with a hierarchical fine-grained ontology such as OntoBiotope allows us to obtain aggregated and adjusted information by selecting the right level or axis of abstraction. Bacteria Biotope Task.  The corpus is the same as BB?11. The documents are scientific web pages intended for a general audience in the form of encyclopedia notices. They focus on a single organism or a family. The habitat mentions are dense and more diverse than 
162
in PubMed abstracts. These features make the task both useful and feasible with a reduced investment in biology. Its linguistic characteristics, high frequency of anaphora, entities denoted by complex nominal expressions raised interesting question for BioNLP that have been treated for a long time in the general and the biomedical domains. 3 Task description The BB Task is split into two secondary goals: 1. The detection of entities and their categorization(s) (Sub-task 1). 2. The extraction of Localization relations given the entities (sub-task 2) Sub-task 1 involves the prediction of habitat entities and their position in the text. The participant also has to assign each entity to one or more concepts of the OntoBiotope ontology: the categorization task. For instance, in the excerpt Isolated from the water of abalone farm, the entity abalone farm should be assigned the OntoBiotope category fish farm. Sub-task 2 is a relation extraction task. The schema of this task contains three types of entities: - The Habitat type is the same as in sub-task 1. - Geographical entities represent location and organization named entities. - Bacteria entities are bacterial taxa. Additionally, there are two types of relations illustrated by Figure 1. - Localization relations link Bacteria to the place where they live (either a Habitat or a Geographical). - PartOf relations relate couples of Habitat entities, a living organism, which is a host (e.g. adult human), and a part of this living organism (e.g. gut).  Bifidobacterium longum. This organism is 
found in adult humans and formula fed infants 
as a normal component of gut flora. 
Figure 1. Example of a localization event in the BB Task. Sub-task 2 participants are provided with document texts and entities, and should predict the relations between the candidate entities. 
Sub-task 3 is the combination of these two sub-tasks. It consists of predicting both the entity positions and the relations between entities. Compared to sub-task 1, the systems have to predict Habitat entities, but also Geographical and Bacteria entities. It is similar to the BB task of BioNLP-ST?11, except that no categorization of the entities is required. 4 Corpus description The BB corpus document sources are web pages from bacteria sequencing projects, (EBI, NCBI, JGI, Genoscope) and encyclopedia pages from MicrobeWiki. The documents are publicly available. Table 1 gives the distribution of the entities and relations in the corpora per sub-task.   Training + Dev Test 1 & 3 Test 2 Document 78 27 26 Word 25,828 7,670 10,353     Bacteria 1,347 332 541 Geographical 168 38 82 Habitat 1,545 507 623 OntoBiotope cat. 1,575 522 NA Total entities 3,060 877 1,246     Localization 1,030 269 538 Part of Host 235 111 129 Total relations 1,265 328 667 Table 1. BB?13 corpus figures. The categorization of entities by a large ontology (sub-task 1) offers a novel task to the BioNLP-ST community; a close examination of the annotated corpus allowed us to anticipate the challenges for participating teams. A total of 2,052 entities have been manually annotated for sub-task 1 (training, development and test sets together). These entities have 1,036 distinct surface forms, which means that an entity surface form is repeated a little less than twice, on average. However, only a quarter of the surface forms are actually repeated; three quarters are unique in the corpus. Moreover, 60% of habitat entities have a surface form that does not match one of the synonyms of their ontology concept. This configuration suggests that methods that simply propagate surface forms and concept attributions from ontology synonyms and from training entities would be inefficient. We have developed a baseline prediction that projects the ontology synonyms and the training corpus 
Localization Localization 
Part of Part of 
163
habitat surface forms onto the test. This prediction scores a high Slot Error Rate of 0.74. We also note there are a few ambiguous forms (i.e. 112 forms) that are synonyms in several different concepts or that do not always denote a habitat, and a few entities are assigned more than one concept (i.e. 42 of them). These are difficult cases that require prediction methods capable of word sense disambiguation. The low number of ambiguous occurrences has a low impact on the participant scores, although their presence may motivate more sophisticated methods. 5 Annotation methodology The methodology of entity position and relations annotation is similar to BB Task?11. It involved seven scientists who participated in a double-blind annotation (each document was annotated twice), followed by a conflict resolution phase. They used the AlvisAE annotation editor (Papazian et al, 2012). The guidelines included some improvements that are detailed below. Boundaries. Habitat entities may be either names or adjective. In the case of adjectives, the head is included in the entity span if it denotes a location (e.g. intestinal sample) and is excluded otherwise (e.g. hospital epidemic). The entity spans may be discontinuous, which is relevant for overlapping entities like ground water and surface water in ground and surface water. The major change is the inclusion of all modifiers that describe the location in the habitat entity span. This makes the entity more informative and the entity boundaries easier to predict, and less subject to debate. For instance, in the example,  isolated from the water of an abalone farm,  the water entity extends from water to farm. Note that in sub-task 1, all entities have to be predicted, even when not involved in a relation. This led to the annotation of embedded entities as potential habitats for bacteria, such as abalone farm and abalone in the above example.  Equivalent sets of entities.  As in BB?11, there are many equivalent mentions of the same bacteria in the documents that play a similar role with respect to the Localization relation. Selecting only one of them as the gold reference would have been arbitrary. When this is the case, the reference annotation includes equivalent sets of entities that convey the same information (e.g. Borrelia garinii vs. B. garinii, but not Borrelia).  
Category assignment. The assignment of categories to habitat entities has been done in two steps: (i) an automatic pre-annotation by the method of Ratkovic et al, (2012) and (ii) a manual double-blind revision followed by a conflict resolution phase. In the manual annotation phase, the most frequent conflicts between annotators were the same as in the previous edition. They involved the assignment of entities to either the living organism category, organic matter or food. An example is the cane entity in cane cuttings. To handle these cases, the guidelines assert that a dead organism cannot be assigned to a living organism category. The high quality of the pre-annotation and its visualization and revision using the AlvisAE annotation editor notably sped-up the annotation process. Table 2 summarizes the figures of the pre-annotation. For sub-task 1, the pre-annotation consisted of assigning OntoBiotope categories to entities for the whole corpus (train+dev+test). The pre-annotation yielded very high results with an F-measure of almost 90%. The pre-annotation was also useful to assess the relevance of the OntoBiotope ontology for the BB task. For sub-task 2, the pre-annotation consisted of the detection of entities in the test set, where no categorization is needed. The second line in Table 2 shows that the recall of entity detection affects the F-score, but that it still made the prediction helpful for the annotators. Further data analysis revealed that the terminology-based approach of the pre-annotation poorly detected the correct boundaries of embedded entities, thereby decreasing the recall of the entity recognition.   Recall Precision F1 Corpus sub-task1 89.7% 90.1% 89.9% Test sub-task 2 47.3% 95.7% 63.3% Table 2. Pre-annotation scores. 6 Evaluation procedure The evaluation procedure was similar to the previous edition in terms of resources, schedule and metrics except that an original relevant metric was developed for the new problem of entity categorization in a hierarchy.  6.1 Campaign organization The training and development corpora with the reference annotations were made available to the participants eleven weeks before the release of 
164
the test sets. Participating teams then had ten days to submit their predictions. As with all BioNLP-ST tasks, each participant submitted a single final prediction for each BB sub-task. The detailed evaluation results were computed, provided to the participants and published on the BioNLP website two days after the submission deadline.  6.2 Evaluation metrics Sub-task 1. In this sub-task participants were given only the document texts. They had to predict habitat entities along with their categorization with the OntoBiotope ontology. The evaluation of sub-task 1 takes into account the accuracy of the boundaries of the predicted entities as well as of the ontology category. Entity pairing. The evaluation algorithm performs an optimal pairwise matching between the habitat entities in the reference and the predicted entities. We defined a similarity between two entities that takes into account the boundaries and the categorization. Each reference entity is paired with the predicted entity for which the similarity is the highest among non-zero similarities.  If the boundaries of a reference entity do not overlap with any predicted entity, then it is a false negative, or a deletion. Conversely, if the boundaries of a predicted entity do not overlap with any reference entity, then it is a false positive, or an insertion. If the similarity between the entities is 1, then it is a perfect match. But if the similarity is lower than 1, then it is a substitution.  Entity similarity. The similarity M between two entities is defined as: M = J . W J measures the accuracy of the boundaries between the reference and the predicted entities. It is defined as a Jaccard Index adapted to segments (Bossy et al, 2012). For a pair of entities with the exact same boundaries, J equals to 1. W measures the accuracy between the ontology concept assignment of the reference entity and the predicted concept assignment of the predicted entity. We used the semantic similarity proposed by Wang, et al (2007). This similarity compares the set of all ancestors of the concept assigned to the reference entity and the set of all ancestors of 
the concept assigned to the predicted entity. The similarity is the Jaccard Index between the two sets of ancestors; however, each ancestor is weighted with a factor equal to: dw where d is the number of steps between the attributed concept and the ancestor. w is a constant greater than zero and lower than or equal to 1. If both the reference and predicted entities are assigned the same concept, then the sets of ancestors are equal and W is equal to 1. If the pair of entities has different concept attributions, W is lower than 1 and depends on the relative depth of the lowest common ancestor. The lower the common ancestor is, the higher the value of W. The exponentiation by the w constant ensures that the weight of the ancestors decreases non-linearly. This similarity thus favors predictions in the vicinity of the reference concept. Note that since the ontology root is the ancestor of all concepts, W is always strictly greater than zero. (Wang et al, 2007) showed experimentally that a value of 0.8 for the w constant is optimal for clustering purposes. However we noticed that w high values tend to favor sibling predictions over ancestor/descendant predictions that are preferable here, whilst low w values do not penalize enough ontology root predictions. We settled w with a value of 0.65, which ensures that ancestor/descendant predictions always have a greater value than sibling predictions, while root predictions never yield a similarity greater than 0.5. As specified above, if the similarity M < 1, then the entity pair is a substitution. We define the importance of the substitution S as: S = 1 - M Prediction score. Most IE tasks measure the quality of a prediction with Precision and Recall, eventually merged into an F1. However the pairing detects false positives and false negatives, but also substitutions. In such cases, the Recall and Precision factor the substitutions twice, and thus underestimate false negatives and false positives. We therefore used the Slot Error Rate (SER) that has been devised to undertake this shortcoming (Makhoul et al, 1999): SER = (S + I + D) / N where: - S represents the number of substitutions. 
165
- I represents the total number of insertions. - D represents the total number of deletions. - N is the number of entities in the reference. The SER is a measure of errors, so the lower it is the better. A SER equal to zero means that the prediction is perfect. The SER is unbound, though a value greater than one means that there are more mistakes in the prediction than entities in the reference. We also computed the Recall, the Precision and F1 measures in order to facilitate the interpretation of results: Recall =M / N Precision = M / P where M is the sum of the similarity M for all pairs in the optimal pairing, N is the number of entities in the reference, and P the number of entities in the prediction. Sub-task 2. In sub-task 2, the participants had to predict relations between candidate arguments, which are Bacteria, Habitat and Geographical entities. This task can be viewed as a categorization task of all pairs of entities. Thus, we evaluate submissions with Recall, Precision and F1. Sub-task 3. Sub-task 3 is similar to sub-task 2, but it includes entity prediction. This is the same setting as the BB task in BioNLP-ST 2011, except for entity categorization. We used the same evaluation metrics based on Recall, Precision and F1 (Bossy et al, 2012). The highlights of this measure are: ? it is based on the pairing between reference and predicted relations that maximizes a similarity; ? the similarity of the boundaries of Habitat and Geographical entities is relaxed and defined as the Jaccard Index (in the same way as in sub-task 1); ? the boundaries of Bacteria is strict: the evaluation rejects all relations where the Bacteria has incorrect boundaries. 7 Results  7.1 Participating systems Five teams submitted ten predictions to the three BB sub-tasks. LIMSI (CNRS, France), see (Grouin, 2013) is the only team that submitted to the three sub-tasks. LIPN (U. Paris-Nord, France), (Bannour et al, 2013) only submitted to 
sub-task 1. TEES (TUCS, Finland), (Bj?rne and Salakoski, 2013) only submitted to sub-task 2. Finally, IRISA (INRIA, France), (Claveau, 2013))) and Boun (U. Bo?azi?i, Turkey), (Karadeniz and ?zg?r), submitted to sub-tasks 1 and 2. The scores of the submissions according to the official metrics are shown in decreasing rank order in Tables 3 to 6.  Participant Rank  SER  F1  IRISA 1  0.46 0.57  Boun 2  0.48 0.59  LIPN 3  0.49 0.61  LIMSI 4  0.66 0.44 Table 3. Scores for Sub-task 1 of the BB Task.   Participant Entity  detection Category  assignment  SER F1 SER  F1  IRISA  0.43 0.60  0.35 0.67  Boun  0.42 0.65  0.36 0.71  LIPN  0.46 0.64  0.38 0.72  LIMSI  0.45 0.71  0.66 0.50 Table 4. Detailed scores for Sub-task 1 of the BB Task. Participant systems to sub-task 1 obtained high scores despite the novelty of the task (0.46 SER for the 1st, IRISA). The results of the first three systems are very close despite the diversity of the methods. The decomposition of the scores of the predictions of entities with correct boundaries and their assignment to the right category are shown in Table 4. They are quite balanced with a slightly better rate for category assignment, with the exception of the LIMSI system, which is notably better in entity detection. This table also shows the dependency of the two entity detection and categorization steps. Errors in the entity boundaries affect the quality of categorization. Table 5 details the scores for sub-task 2. The prediction of location relations remains a difficult problem even with the entities being given. There are two reasons for this. First, there is high diversity of bacteria and locations. The many mentions of different bacteria and locations in the same paragraph make it a challenge to select the right pairing among candidate arguments. This is particularly true for the PartOf relation compared to the Localization relation (columns 5 and 6). All systems obtained 
166
a recall much lower than the precision, which may be interpreted training data overfitting.  Participant Rec. Prec.  F1  F1 PartOf F1 Loc.  TEES 2.1  0.28 0.82  0.42  0.22 0.49  IRISA  0.36 0.46  0.40  0.2 0.45  Boun  0.21 0.38  0.27  0.2 0.29  LIMSI  0.4 0.19  0.6  0.0 0.7 Table 5. Scores of Sub-task 2 for the BB Task. The second challenge is the high frequency of anaphora, especially with a bacteria antecedent. For BioNLP-ST 2011, we already pointed out that coreference resolution is critical in order to capture all relations that are not expressed inside a sentence. Participant Rec.  Prec.  F1   TEES 2.1 0.12 (0.41) 0.18 (0.61) 0.14  (0.49)  LIMSI 0.4 (0.9) 0.12 (0.82)   0.6  (0.15) Table 6. Scores of Sub-task 3 for the BB Task. (the relaxed scores are given in parentheses.) The results of sub-task 3 (Table 6) may appear disappointing compared to the first two sub-tasks and BB?11. Further analysis shows that the system scores were affected by their poor entity boundary detection and the PartOf relation predictions. In order to demonstrate this we computed a relaxed score that differs from the primary score by: - removing PartOf relations from the reference and the prediction; - accepting Localization relations even if the Bacteria entity boundaries  do not match; - removing the penalty for the incorrect boundaries of Habitat entities. This relaxed score is equivalent to ignoring PartOf relations and considering the boundaries of predicted entities as perfect. The result is exhibited in Table 6 between parentheses. The most determinant factor is the relaxation of Bacteria entity boundaries because errors are severely penalized. An error analysis of the submitted predictions revealed that more than half of the rejected Localization predictions had a Bacteria argument with incorrect boundaries.  7.2 Systems description and result analysis The participants deployed various assortments of methods ranging from linguistics and machine learning to hand-coded pattern-matching. Sub-
task 1 was handled in two successive steps, candidate entity detection and category assignment. Entity detection. The approaches combine  (1) the use of lexicons (IRISA and LIMSI), (2) then text analysis by chunking (IRISA), noun phrase analysis (Boun), term analysis by BioYaTeA (LIPN) and Cocoa entity detection (LIMSI),  (3) with additional rules (TextMarker by LIPN) or machine learning (CRF by LIMSI) for the adaptation to the corpus.  The LIMSI system combining Cocoa entity detection (BioNLP supporting resource) with CRF obtained the best result, 11 points over the less linguistics-based approach of IRISA as shown in Table 4.  Assignment of categories to entities. It was mainly realized using hand-coded rules (LIMSI, Boun), machine learning with Whisk (LIPN) or a similarity between ontology labels and the text entities (IRISA). It is interesting to note that although the approaches are very different, the three types of methods obtained close results ranging from 0.35 to 0.38 SER, apart one outlier. Prediction of relations. Sub-task 2 was completed by applying hand-coded rules (LIMSI, Boun), that were much less successful than the two machine-learning-based approaches, i.e. kNN by IRISA and multi-step SVM by TEES-2.1. In the case of TEES-2.1 attributes were generated by McCCJ parses, which may explain its success in the prediction of PartOf relations that is 20 point over the second method that did not use any parsing. Prediction of entities and relations. Sub-task 3 was completed by LIMSI using the successive application of its methods from sub-tasks 1 and 2. TEES-2.1 applied its multi-step SVM classification of sub-task 2 for relation prediction completed by additional SVM steps for candidate entity detection. These experiments allow for the comparison of very different state-of-the-art methods, resources and integration strategies. However the tight gap between the scores of the different systems prevents us from drawing a definitive conclusion. Additional criteria other than scores may also be taken into account: the simplicity of deployment, the ease of adaptation to new 
167
domains, the availability of relevant resources and the potential for improvement. 8 Conclusion After BioNLP-ST?11, the second edition of the Bacteria Biotope Task provides a wealth of new information on the generalization of the entity categorization methods to a large set of categories. The final submissions of the 5 teams show very promising results with a broad variety of methods. The introduction of new metrics appeared appropriate to reveal the quality of the results and to highlight relevant contrasts. The prediction of events still remains challenging in documents where the candidate arguments are very dense, and where most relations involve several sentences. A thorough analysis of the results indicates clear directions for improvement.  Acknowledgments This work has been partially supported by the Quaero program, funded by OSEO, the French state agency for innovation and the INRA OntoBiotope Network. References Sondes Bannour, Laurent Audibert, Henry Soldano. 2013. Ontology-based semantic annotation: an automatic hybrid rule-based method. Present volume. Jari Bj?rne, Tapio Salakoski. 2013. TEES 2.1: Automated Annotation Scheme Learning in the BioNLP 2013 Shared Task. Present volume. Robert Bossy, Julien Jourde, Alain-Pierre Manine A., Philippe Veber, Erick Alphonse, Maarten van de Guchte, Philippe Bessi?res, Claire N?dellec. 2012. BioNLP Shared Task - The Bacteria Track. BMC Bioinformatics 13(Suppl 11):S3, June .  Vincent Claveau. 2013. IRISA participation to BioNLP-ST 2013: lazy-learning and information retrieval for information extraction tasks. Present volume. Liolios K., Chen I.M., Mavromatis K., Tavernarakis N., Hugenholtz P., Markowitz V.M., Kyrpides N.C. (2010). The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. Nucleic Acids Res., 38(Database issue):D346-54. EnvDB database. http://metagenomics.uv.es/envDB/ EnvO Project. http://environmentontology.org 
Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metagenomes: the ?Minimum Information about a Genome Sequence? (MIGS) specification. Nature Biotechnology. 26: 541-547. Cyril Grouin. 2013. Building A Contrasting Taxa Extractor for Relation Identification from Assertions: BIOlogical Taxonomy & Ontology Phrase Extraction System. Present volume. ?lknur Karadeniz, Arzucan ?zg?r. 2013. Bacteria Biotope Detection, Ontology-based Normalization, and Relation Extraction using Syntactic Rules. Present volume. Korbel J.O., Doerks T., Jensen L.J., Perez-Iratxeta C., Kaczanowski S., Hooper S.D., Andrade M.A., Bork P. (2005). Systematic association of genes to phenotypes by genome and literature mining. PLoS Biol., 3(5):e134. Melissa M. Floyd, Jane Tang, Matthew Kane and David Emerson. 2005. Captured Diversity in a Culture Collection: Case Study of the Geographic and Habitat Distributions of Environmental Isolates Held at the American Type Culture Collection. Applied and Environmental Microbiology. 71(6):2813-23. GenBank. http://www.ncbi.nlm.nih.gov/  GOLD. http://www.genomesonline.org/cgi-bin/GOLD/bin/gold.cgi Ivanova N., Tringe S.G., Liolios K., Liu W.T., Morrison N., Hugenholtz P., Kyrpides N.C. (2010). A call for standardized classification of metagenome projects. Environ. Microbiol., 12(7):1803-5. John Makhoul, Francis Kubala, Richard Schwartz, and Ralph Weischedel. 1999. Performance measures for information extraction, in Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February.  von Mering C., Hugenholtz P., Raes J., Tringe S.G., Doerks T., Jensen L.J., Ward N., Bork P. (2007). Quantitative phylogenetic assessment of microbial communities in diverse environments. Science, 315(5815):1126-30. Metagenome Classification. /metagenomic_classification_tree.cgi MicrobeWiki. http://microbewiki.kenyon.edu/index.php/MicrobeWiki  Microbial Genomics Program at JGI. http://genome.jgi-psf.org/programs/bacteria-archaea/index.jsf Microorganisms sequenced at Genoscope. http://www.genoscope.cns.fr/spip/Microorganisms-sequenced-at.html 
168
Miguel Pignatelli, Andr?s Moya, Javier Tamames.  (2009). EnvDB, a database for describing the environmental distribution of prokaryotic taxa. Environmental Microbiology Reports. 1:198-207. Fr?d?ric Papazian, Robert Bossy and Claire N?dellec. 2012. AlvisAE: a collaborative Web text annotation editor for knowledge acquisition. The 6th Linguistic Annotation Workshop (The LAW VI), Jeju, Korea. Prokaryote Genome Projects at NCBI. http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi  Zorana Ratkovic, Wiktoria Golik, Pierre Warnier. 2012. Event extraction of bacteria biotopes: a knowledge-intensive NLP-based approach. BMC Bioinformatics 2012, 13(Suppl 11):S8, 26June. .  Javier Tamames and Victor de Lorenzo. 2010. EnvMine: A text-mining system for the automatic extraction of contextual information. BMC Bioinformatics. 11:294. James Z. Wang, Zhidian Du, Rapeeporn Payattakool, Philip S. Yu, and Chin-Fu Chen. 2007. A New Method to Measure the Semantic Similarity of GO Terms. Bioinformatics. 23: 1274-1281. 
169
