Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 497?507, Dublin, Ireland, August 23-29 2014.
Learning Sense-specific Word Embeddings By Exploiting
Bilingual Resources
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in learning word embeddings with neural network language
models (NNLM). However, the majority of previous NNLMs represent each word with a single
embedding, which fails to capture polysemy. In this paper, we address this problem by represent-
ing words with multiple and sense-specific embeddings, which are learned from bilingual parallel
data. We evaluate our embeddings using the word similarity measurement and show that our ap-
proach is significantly better in capturing the sense-level word similarities. We further feed our
embeddings as features in Chinese named entity recognition and obtain noticeable improvements
against single embeddings.
1 Introduction
Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector rep-
resentations for words. Each dimension of word embedding represents a latent feature of the word, hope-
fully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightfor-
wardly for computing word similarities, which benefits many practical applications (Socher et al., 2011;
Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al.,
2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013).
In recent years, neural network language models (NNLMs) have become popular architectures for
learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most
of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an
attempt to better capture the multiple senses or usages of a word, several multi-prototype models have
been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply
induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed
value. These models still may not capture the real senses of words, because different words may have
different number of senses.
We present a novel and simple method of learning sense-specific word embeddings by using bilingual
parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs.
We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the
source language with different senses is supposed to have different translations in the foreign language.
1
For instance,?? can be translated as investment / overpower / subdue / subjugate / uniform, etc. Among
all of these translations, subdue / overpower / subjugate express the same sense of??, whereas uniform
/ investment express a different sense. Therefore, we could effectively obtain the senses of one word by
clustering its translation words, exhibiting different senses in different clusters.
The created clusters are then projected back into the words in the source language texts, forming a
sense-labeled training data. The sense-labeled data are then trained with recurrent neural network langu-
gae model (RNNLM) (Mikolov, 2012), a kind of NNLM, to obtain sense-specific word embeddings. As
a concrete example, Figure 1 illustrates the process of learning sense-specific embeddings.
?
Email correspondence.
1
In this paper, source language refers to Chinese, whereas foreign language refers to English.
This work is licenced under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
497
? Cluster
Monolingualsense-labeled data
? ?? #2 , ?? ? ?? ?? ?1
?? ? ? ?? #1  ? ?? ?2
? ?? #2 ? ?? ?3
? ?? ? ?? ?? ?? ?  ?? #1 ?4
??5
? Project
? Extract
? RNNLM
Bilingual data(E: English, C: Chinese)
E: ? subdue , conquer or control the opponent .
C: ? ?? , ?? ? ?? ?? ?1
E: The workers wearing the factory 's uniform ? 
C: ?? ? ? ?? ? ?? ? 2
E: She overpowered the burglars .
C: ? ?? ? ?? ?3
E: They wore their priestly vestment in Church .
C: ? ?? ? ?? ?? ?? ? ?? ?4
??5
SL word ??
Translations
subdueuniformoverpowersubjugatevestment??
uniformvestment
overpowersubjugate
subdue
??#1
??#2
(clothes)
(defeat)
Sense-specific word embeddings
< v1#1, v2#1, ..., vN#1 >
< v1#2, v2#2, ..., vN#2 >
?? #1?? #2
Figure 1: An illustration of the proposed method. SL stands for source language.
To evaluate the sense-specific word embeddings we have learned, we manually construct a Chinese
polysemous word similarity dataset that contains 401 pairs of words with human-judged similarities. The
performance of our method on this dataset shows that sense-specific embeddings are significantly better
in capturing the sense-level similarities for polysemous words.
We also evaluate our embeddings by feeding them as features to the task of Chinese named entity
recognition (NER), which is a simple semi-supervised learning mechanism (Turian et al., 2010). In or-
der to use sense-specific embeddings as features, we should discriminate the word senses for the NER
data first. Therefore, we further develop a novel monolingual word sense disambiguation (WSD) algo-
rithm based on the RNNLM we have already trained previously. NER results show that sense-specific
embeddings provide noticeable improvements over traditional single embeddings.
Our contribution in this paper is twofold:
? We propose a novel approach of learning sense-specific word embeddings by utilizing bilingual
parallel data (Section 3). Evaluation on a manually constructed polysemous word similarity dataset
shows that our approach better captures word similarities (Section 5.2).
? To use the sense-specific embeddings in practical applications, we develop a novel WSD algorithm
for monolingual data based on RNNLM (Section 4). Using the algorithm, we feed the sense-specific
embeddings as additional features to NER and achieve significant improvement (Section 5.3).
2 Background: Word Embedding and RNNLM
There has been a line of research on learning word embeddings via NNLMs (Bengio et al., 2003; Mnih
and Hinton, 2008; Mikolov et al., 2013b). NNLMs are language models that exploit neural networks to
make probabilistic predictions of the next word given preceding words. By training NNLMs, we obtain
both high performance language models and word embeddings.
Following Mikolov et al. (2013b), we use the recurrent neural network as the basic framework for train-
ing NNLMs. RNNLM has achieved the state-of-the-art performance in language modeling (Mikolov,
2012) and learned effective word embeddings for several tasks (Mikolov et al., 2013b). The architecture
of RNNLM is shown in Figure 2.
The input layer of RNNLM consists of two components: w(t) and h(t ? 1). w(t) is the one-hot
representation of the word at time step t,
2
h(t ? 1) is the output of hidden layer at the last time step.
Therefore, the input encodes all previous history when predicting the next word at time step t. Compared
2
A feature vector of the same size of the vocabulary, and only one dimension is on.
498
w(t)
y(t)
U
W
V
h(t)
h(t-1)
Figure 2: The basic architecture of RNNLM.
with other feed-forward NNLMs, the RNNLM can theoretically represent longer context patterns. The
output y(t) represents the probability distribution of the next word p(w(t + 1)|w(t),h(t ? 1)). The
output values are computed as follows:
h(t) = f(Uw(t) +Wh(t? 1)) (1)
y(t) = g(Vh(t)) (2)
where f is a sigmoid function and g is a softmax function.
The RNNLM is trained by maximizing the log-likelihood of the training data using stochastic gradi-
ent descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute the
gradients. In the RNNLM, U is the embedding matrix, where each column vector represents a word.
As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon in
natural languages and induce a single embedding for each word. We address this issue and introduce an
effective approach for capturing polysemy in the next section.
3 Sense-specific Word Embedding Learning
In our approach, WSI is performed prior to the training of word embeddings. Inspired by Gale et al.
(1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examples
of WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1. First, we extract
the translations of the source language words from bilingual data (?). Since there may be multiple
translations for the same sense of a source language word, it is straightforward to cluster the translation
words, exhibiting different senses in different clusters (?).
Once word senses are effectively induced for each word, we are able to form the sense-labeled training
data of RNNLMs by tagging each word occurrence in the source language text with its associated sense
cluster (?). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a
standard manner (?).
3.1 Translation Words Extraction
Given bilingual data after word alignment, we present a way of extracting translation words for source
language words by exploiting the translation probability produced by word alignment models (Brown et
al., 1993; Och and Ney, 2003; Liang et al., 2006).
More formally, we notate the Chinese sentence as c = (c
1
, ..., c
I
) and English sentence as e =
(e
1
, ..., e
J
). The alignment models can be generally factored as:
p(c|e) =
?
a
p(a, c|e) (3)
p(a, c|e) =
?
J
j=1
p
d
(a
j
|a
j?
, j)p
t
(c
j
|e
a
j
) (4)
where a is the alignment specifying the position of an English word aligned to each Chinese word,
p
d
(a
j
|a
j?
, j) is the distortion probability, and p
t
(c
j
|e
a
j
) is the translation probability which we use.
499
SL Word Translation Words Translation Word Clusters Nearest Neighbours
?? investment, overpower,
subdue, subjugate, uniform
investment, uniform ??
dress
,??
policeman uniform
subdue, subjugate, overpower ??
defeat
,??
beat
,??
conquer
? blossom, cost, flower,
spend, take, took
flower, blossom ?
greens
,?
leaf
,??
fruit
take, cost, spend ??
cost
,??
save
,??
rest
? act, code, France,
French, law, method
France, French ?
Germany
,?
Russia
,?
Britain
law, act, code ??
ordinance
,??
bill
,??
rule
method ??
concept
,??
scheme
,??
way
??
lead, leader, leadership
leader, leadership ??
chief
,??
boss
,??
chairman
lead ??
supervise
,??
decision
,??
work
Table 1: Results of our approach on a sample of polysemous words. The second column lists the extracted
translation words of the source language word (Section 3.1). The third column lists the clustering results
using affinity propagation (Section 3.2). The last column lists the nearest neighbour words computed
using the learned sense-specific word embeddings (Section 5.2.2).
In this paper, we use the alignment model proposed by Liang et al. (2006). We utilize the bidirectional
translation probabilities for the extraction of translations, where a foreign language word w
e
is deter-
mined as a translation of source language word w
c
only if both translation probabilities p
t
(w
c
|w
e
) and
p
t
(w
e
|w
c
) exceed some threshold 0 < ? < 1.
The second column of Table 1 presents the extraction results on a sample of source language words
with the corresponding translation words.
3.2 Clustering of Translation Words
For each source language word, its translation words are then clustered so as to separate different senses.
At the clustering time, we first represent each translation word with a feature vector (point), so that
we can measure the similarities between points. Then we perform clustering on these feature vectors,
representing different senses in different clusters.
Different from Apidianaki (2008) who represents all occurrences of the translation words with their
contexts in the foreign language for clustering, we adopt the embeddings of the translation words as
the representations and directly perform clustering on the translation words,
3
rather than the contexts of
occurrences. The embedding representation is chosen for two reasons: (1) Word embeddings encode rich
lexical semantics. They can be directly used to measure word similarities. (2) Embedding representation
of the translation words leads to extremely high-efficiency clustering, because the number of translation
words is orders of magnitude less than their occurrences.
Moreover, since the number of senses of different source language words is varied, the commonly-
used k-means algorithm becomes inappropriate for this situation. Instead, we employ affinity propaga-
tion (AP) algorithm (Frey and Dueck, 2007) for clustering. In AP, each cluster is represented by one
of the samples of it, which we call an exemplar. AP finds the exemplars iteratively based on the con-
cept of ?message passing?. AP has the major advantage that the number of the resulting clusters is
dynamic, which mainly depends on the distribution of the data. Compared with other possible clustering
approaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines the
number of resulting clusters automatically without using any partition criterions.
The third column of Table 1 lists the resulting clusters of the translation words for the sampled pol-
ysemous words. We can see that the resulting clusters are meaningful: senses are well represented by
clusters of translation words.
3.3 Cross-lingual Word Sense Projection
The produced clusters are then projected back into the source language to identify word senses.
3
The publicly available word embeddings proposed by Collobert et al. (2011) are used.
500
For each occurrence w
o
of the word w in the source language corpora, we first select the aligned word
with the highest marginal edge posterior (Liang et al., 2006) as its translation. We then identify the sense
of w
o
by computing the similarities of its translation word with each exemplar of the clusters, and select
the one with the maximum similarity. When w
o
is aligned with NULL, we heuristically identify its sense
as the most frequent sense of w that appears in the bilingual dataset.
After projecting the word senses into the source language, we obtain a sense-labeled corpus, which is
used to train the sense-specific word embeddings with RNNLM. The training process is exactly the same
as single embeddings, except that the words in our training corpus has been labeled with senses.
4 Application of Sense-specific Word Embeddings
One of the attractive characteristic of word embeddings is that they can be directly used as word features
in various NLP applications, including NER, chunking, etc. Despite of the usefulness of word embed-
dings on these applications, previous work seldom concerns that words may have multiple senses, which
cannot be effectively represented with single embeddings. In this section, we address this problem by
utilizing sense-specific word embeddings.
We take the task of Chinese NER as a case study. Intuitively, word senses are important in NER. For
instance,? is likely to be an NE of Location when it refers to America. However, when it expresses the
sense of beautiful, it should not be an NE.
Using sense-specific word embedding features for NER is not as straightforward as using single em-
beddings. For each word in the NER data, we first need to determine the correct word sense of it, which
is a typical WSD problem. Then we use the embedding which corresponds to that sense as features.
Here we treat WSD as a sequence labeling problem, and solve it with a very natural algorithm based on
RNNLM we have already trained (Section 3).
4.1 RNNLM-based Word Sense Disambiguation
Given the automatically induced word sense inventories and the RNNLM which has already been trained
on the sense-labeled data of source language, we first develop a greedy decoding algorithm for the
sequential WSD, which works deterministically. Then we improve it using beam search.
Greedy. For word w, we denote the sense-labeled w as w
s
k
, where s
k
represents the k
th
sense of w.
In each step, a single decision is made and the sense of next word (w(t + 1)) which has the maximum
RNNLM output is chosen, given the current (sense-labeled) word w(t)
s
?
and the hidden layer h(t? 1)
at the last time step as input. We simply need to compute a shortlist of y(t) associated with w(t + 1),
that is, y(t)|
w(t+1)
at each step. This process is illustrated in Figure 3.
Beam search. The greedy procedure described above can be improved using a left-to-right beam
search decoding for obtaining a better sequence. The beam-search decoding algorithm keeps B different
sequences of decisions in the agenda, and the sequence with the best overall score is chosen as the final
sense sequence.
Note that the dynamic programming decoding (e.g. viterbi) is not applicable here, because of the
recurrent characteristic of RNNLM. At each step, decisions made by RNNLM depends on all previous
decisions instead of the previous state only, hence markov assumption is not satisfied.
5 Experiments
5.1 Experimental Settings
The Chinese-English parallel datasets we use include LDC03E24, LDC04E12 (1998), the IWSLT 2008
evaluation campaign dataset and the PKU 863 parallel dataset. All corpora are sentence-aligned. After
cleaning and filtering the corpus,
4
we obtain 918,681 pairs of sentences (21.7M words).
In this paper, we use BerkeleyAligner to produce word alignments over the parallel dataset.
5
Berke-
leyAligner also gives translation probabilities and marginal edge posterior probabilities. We adopt the
4
Sentences that are too long (more than 40 words) or too short (less than 10 words) are discarded.
5
code.google.com/p/berkeleyaligner/
501
h(t-1)
U
W
V
h(t)
Next word (sense-labeled)Last word(sense-labeled)
w(t)s*
w(t+1)s1
y(t)|w(t+1)
w(t+1)s2
w(t+1)sK
?
y(t)
?
w(t+1)s*
max
w(t)s* w(t+1)s*
w(t+1)s* w(t+2)s*
w(t+2)s* w(t+3)s*
h(t)
h(t+1)
h(t+2)
h(t-1)
Shortlist? 
Figure 3: Using RNNLM for WSD by sequential labeling (left). Decision at each step of the RNNLM-
based WSD algorithm (right).
scikit-learn tool (Pedregosa et al., 2011) to implement the AP clustering algorithm.
6
The AP algorithm
is not fully automatic in deciding the cluster number. There is a tunable parameter calls preference. A
preference with a larger value encourages more clusters to be produced. We set the preference at the
median value of the input similarity matrix to obtain a moderate number of clusters. The rnnlm toolkit
developed by Mikolov et al. (2011) is used to train RNNLM and obtain word embeddings.
7
We induce
both single and sense-specific embeddings with 50 dimensions. Finally, We obtain embeddings of a
vocabulary of 217K words, with a proportion of 8.4% having multiple sense clusters.
5.2 Evaluation on Word Similarity
Word embeddings can be directly used for computing similarities between words, which benefits many
practical applications. Therefore, we first evaluate our embeddings using a similarity measurement.
Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010):
MaxSim(u, v) = max
1?i?k
u
,1?j?k
v
s(u
i
, v
j
) (5)
AvgSim(u, v) =
1
k
u
?k
v
?
k
u
i=1
?
k
v
j=1
s(u
i
, v
j
) (6)
where k
u
and k
v
are the number of the induced senses for words u and v, respectively. s(?, ?) can be any
standard similarity measure. In this study, we use the cosine similarity.
Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin and
Wu, 2012) for the evaluation of general word similarity. These datasets rarely contain polysemous words,
and thus is unsuitable for our evaluation. To the best of our knowledge, no datasets for polysemous word
similarity evaluation have been published yet, either in English or Chinese. In order to fill this gap in the
research community, we manually construct a Chinese polysemous word similarity dataset.
5.2.1 Chinese Polysemous Word Similarity Dataset Construction
We adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset. HowNet is a Chinese
knowledge database that maintains comprehensive semantic definitions for each word in Chinese. The
process of the dataset construction includes three steps: (1) Commonly used polysemous words are
extracted according to their sense definitions in HowNet. (2) For each polysemous word, we select
several other words to form word pairs with it. (3) Each word pair is manually annotated with similarity.
In step (1), we mainly took advantage of HowNet for the selection of polysemous words. However,
the synsets defined in HowNet are often too fine-grained and many of them are difficult to distinguish,
6
scikit-learn.org
7
www.fit.vutbr.cz/
?
imikolov/rnnlm/
502
particularly for non-experts. Therefore, we manually discard those words with senses that are hard to
distinguish.
In step (2), for each polysemous word w selected in step 1, we sample several other words to form
word pairs withw. The sampled words can be roughly divided into two categories: related and unrelated.
The related words are sampled manually. They can be the hypernym, hyponym, sibling, (near-)synonym,
antonym, or topically related to one sense of w. The unrelated words are sampled randomly.
In step (3), we ask six graduate students who majored in computational linguistics to assign each word
pair a similarity score. Following the setting of WordSim-353, we restrict the similarity score in the range
(0.0, 10.0). To address the inconsistency of the annotations, we discard those word pairs with a standard
deviation greater than 1.0. We end up with 401 word pairs annotated with acceptable consistency. Unlike
the WordSim-353, in which most of the words are nouns, the words in our dataset are more diverse in
terms of part-of-speech tags.
Table 2 lists a sample of word pairs with annotated similarities from the dataset. The whole evaluation
dataset will be publicly available for the research community.
8
Word Paired word Category Mean.Sim Std.Dev
?? ??conquer synonym 8.60 0.29
??
key point
unrelated 0.12 0.19
? ?enter autonym 7.90 0.97
??
publish
near-synonym 7.86 0.76
? ?plant stem sibling 7.80 0.12
??
cost
topic-related 5.86 0.90
? ??
food
hypernym 6.50 0.71
Table 2: Sample word pairs of our dataset. The unrelated words are randomly sampled. Mean.Sim
represents the mean similarity of the annotations, Std.Dev represents the standard deviation.
5.2.2 Evaluation Results
Following Zou et al. (2013), we use Spearman?s ? correlation and Kendall?s ? correlation for evaluation.
The results are shown in Table 3. By utilizing sense-specific embeddings, our approach significantly
outperforms the single-version using either MaxSim or AvgSim measurement.
For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang et
al. (2012), which was first presented by Sch?utze (1998). The occurrences of a word are represented by
the average embeddings of its context words. Following Huang et al.?s settings, we use a context window
of size 10 and all occurrences of a word are clustered using the spherical k-means algorithm, where k is
tuned with a development set and finally set to 2.
System
MaxSim AvgSim
? ?100 ? ?100 ? ?100 ? ?100
Ours 55.4 40.9 49.3 35.2
SingleEmb 42.8 30.6 42.8 30.6
Multi-prototype 40.7 29.1 38.3 27.4
Table 3: Spearman?s ? correlation and Kendall?s ? correlation evaluated on the polysemous dataset.
Surprisingly, the multi-prototype method performs even slightly worse than the single-version, which
suggests that learning a fixed number of embeddings for every word may even harm the embedding.
Additionally, the clustering process of the multi-prototype approach suffers from high memory and time
cost, especially for the high-frequency words.
8
ir.hit.edu.cn/
?
jguo
503
To obtain intuitive insight into the superior performance of sense-specific embeddings, we list in the
last column of Table 1 the nearest neighborhoods of the sampled words in the evaluation dataset. The list
shows that we are able to find the different meanings of a word by using sense-specific embeddings.
5.3 Application on Chinese NER
We further apply the sense-specific embeddings as features to Chinese NER. We first perform WSD on
the NER data using the algorithm introduced in Section 4. For beam search decoding, the beam size B
is tuned on a development set and is finally set to 16.
We conduct our experiments on data from People?s Daily (Jan. and Jun. 1998).
9
The original corpus
contains seven NE types.
10
In this study, we select the three most common NE types: Person, Location,
Organization. The data from January are chosen as the training set (37,426 sentences). The first 2,000
sentences from June are chosen as the development set and the next 8,000 sentences as the test set.
CRF models are used in our NER system and are optimized by L2-regularized SGD. We use the
CRFSuite (Okazaki, 2007) because it accepts feature vectors with numerical values. The state-of-the-art
features (Che et al., 2013) are used in our baseline system. For both single and sense-specific embedding
features, we use a window size of 4 (two words before and two words after).
5.3.1 Results
Table 4 demonstrates the performance of NER on the test set. As desired, the single embedding features
improve the performance of our baseline, which were also shown in (Turian et al., 2010). Furthermore,
the sense-specific embeddings outperform the single word embeddings by nearly 1% F-score (88.56 vs.
87.58), which is statistically significant (p-value < 0.01 using one-tail t-test).
System P R F
Baseline 93.27 81.46 86.97
+SingleEmb 93.55 82.32 87.58
+SenseEmb (greedy) 93.38 83.56 88.20
+SenseEmb (beam search) 93.59 84.05 88.56
Table 4: Performance of NER on test data.
According to our hypothesis, the sense-specific embeddings should bring considerable improvements
to the NER of polysemous words. To verify this, we evaluate the per-token accuracy of the polysemous
words in the NER test data. We again adopt HowNet to determine the polysemy. Words that are defined
with multiple senses are selected as test set. Figure 4 shows that the sense-specific embeddings indeed
improve the NE recognition of the polysemous words, whereas the single embeddings even decrease the
accuracy slightly. We also obtain improvements on the NE recognition of the monosemous words, which
provide evidences that more accurate prediction of polysemous words is beneficial for the prediction of
the monosemous words through contextual influence.
6 Related Work
Previous studies have explored the NNLMs, which predict the next word given some history or future
words as context within a neural network architecture. Schwenk and Gauvain (2002), Bengio et al.
(2003), Mnih and Hinton (2007), and Collobert et al. (2011) proposed language models based on feed-
forward neural networks. Mikolov et al. (2010) studied language models based on RNN, which managed
to represent longer history information for word-predicting and demonstrated outstanding performance.
Besides, researchers have also explored the word embeddings learned by NNLMs. Collobert et al.
(2011) used word embeddings as the input of various NLP tasks, including part-of-speech tagging,
chunking, NER, and semantic role labeling. Turian et al. (2010) made a comprehensive comparison
of various types of word embeddings as features for NER and chunking. In addition, word embeddings
9
www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp
10
Person, Location, Organization, Date, Time, Number and Miscellany
504
Polysemous(2) Polysemous(3) Monosemous
per?t
oken
 acc
urac
y
0.60
0.65
0.70
0.75
0.80
0.85
0.90 Baseline +SingleEmb +SenseEmb
Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polyse-
mous(k) represents the set of words that have more than or equal to k senses defined in HowNet.
are shown to capture many relational similarities, which can be recovered by vector arithmetic in the
embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013)
learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to
constrain translational equivalence.
Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of
languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010)
and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each
word. They did this by clustering the contexts of words. These multi-prototype models simply induced
a fixed number of embeddings for every word, regardless of the real sense capacity of the specific word.
There has been a lot of work on using bilingual resources for word sense disambiguation (Gale et
al., 1992; Chan and Ng, 2005). By using aligned bilingual data along with word sense inventories such
as WordNet, training examples for WSD can be automatically gathered. We employ this idea for word
sense induction in our study, which is free of any pre-defined word sense thesaurus.
The most similar work to our sense induction method is Apidianaki (2008). They presented a method
of sense induction by clustering all occurrences of each word?s translation words. In their approach,
occurrences are represented with their contexts. We suggest that clustering contexts suffer from high
memory and time cost, as well as data sparsity. In our method, by clustering the embeddings of transla-
tion words, we induce word senses much more efficiently.
To evaluate word similarity models, researchers often apply a dataset with human-judged similarities
on word pairs, such as WordSim-353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG
(Rubenstein and Goodenough, 1965) and Jin and Wu (2012). For context-based multi-prototype mod-
els, (Huang et al., 2012) constructs a dataset with context-dependent word similarity. To the best of
our knowledge, there is no publicly available datasets for context-unaware polysemous word similarity
evaluation yet. This paper fills this gap.
7 Conclusion
This paper presents a novel and effective approach of producing sense-specific word embeddings by
exploiting bilingual parallel data. The proposed embeddings are expected to capture the multiple senses
of polysemous words. Evaluation on a manually annotated Chinese polysemous word similarity dataset
shows that the sense-specific embeddings significantly outperforms the single embeddings and the multi-
prototype approach.
Another contribution of this study is the development of a beam-search decoding algorithm based on
RNNLM for monolingual WSD. This algorithm bridges the proposed sense-specific embeddings and
practical applications, where no bilingual information is provided. Experiments on Chinese NER show
that the sense-specific embeddings indeed improve the performance, especially for the recognition of the
polysemous words.
505
Acknowledgments
We are grateful to Dr. Zhenghua Li, Yue Zhang, Shiqi Zhao, Meishan Zhang and the anonymous review-
ers for their insightful comments and suggestions. This work was supported by the National Key Basic
Research Program of China via grant 2014CB340503 and 2014CB340505, the National Natural Science
Foundation of China (NSFC) via grant 61370164.
References
Marianna Apidianaki. 2008. Translation-oriented word sense induction based on parallel corpora. In In Proceed-
ings of the 6th Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. The Journal of Machine Learning Research, 3:1137?1155.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In AAAI,
volume 5, pages 1037?1042.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zhendong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116?
131.
Brendan J Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. science,
315(5814):972?976.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierar-
chies via word embeddings. In Proceedings of the 52th Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, Baltimore MD, USA.
William A Gale, Kenneth W Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense
disambiguation methods. In Proceedings of the 4th International Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 101?112.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Peng Jin and Yunfang Wu. 2012. Semeval-2012 task 4: evaluating chinese word similarity. In Proceedings of the
First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,
pages 374?377.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating disambiguation from composi-
tion in distributional semantics. CoNLL-2013, pages 114?123.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations
of words. In Proceedings of COLING 2012, pages 1459?1474, Mumbai, India, December.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference
on Human Language Technology Conference of the North American Chapter of the Association of Computa-
tional Linguistics, pages 104?111.
Tomas Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In Proceedings of Interspeech, pages 1045?1048.
506
Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and J
?
Cernock`y. 2011. Rnnlm-recurrent neural
network language modeling toolkit. In Proc. of the 2011 ASRU Workshop, pages 196?201.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word
representations. In Proceedings of NAACL-HLT, pages 746?751.
Tomas Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Ph. D. thesis, Brno
University of Technology.
George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and
cognitive processes, 6(1):1?28.
Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th international conference on Machine learning, pages 641?648.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Advances in
neural information processing systems, pages 1081?1088.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.
chokkan. org/software/crfsuite.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117.
Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connectionist language modeling for large vocabulary continuous
speech recognition. In Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International Confer-
ence on, volume 1, pages 765?768.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing
Systems, pages 801?809.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Dianhai Yu. 2013. Compound embedding features for semi-
supervised learning. In Proceedings of NAACL-HLT, pages 563?568.
Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for
phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 1393?1398, Seattle, Washington, USA, October.
507
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110?120,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Revisiting Embedding Features for Simple Semi-supervised Learning
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in us-
ing continuous word embeddings learned
from unlabeled data as features to improve
supervised NLP systems, which is re-
garded as a simple semi-supervised learn-
ing mechanism. However, fundamen-
tal problems on effectively incorporating
the word embedding features within the
framework of linear models remain. In
this study, we investigate and analyze three
different approaches, including a new pro-
posed distributional prototype approach,
for utilizing the embedding features. The
presented approaches can be integrated
into most of the classical linear models in
NLP. Experiments on the task of named
entity recognition show that each of the
proposed approaches can better utilize the
word embedding features, among which
the distributional prototype approach per-
forms the best. Moreover, the combination
of the approaches provides additive im-
provements, outperforming the dense and
continuous embedding features by nearly
2 points of F1 score.
1 Introduction
Learning generalized representation of words is
an effective way of handling data sparsity caused
by high-dimensional lexical features in NLP sys-
tems, such as named entity recognition (NER)
and dependency parsing. As a typical low-
dimensional and generalized word representa-
tion, Brown clustering of words has been stud-
ied for a long time. For example, Liang (2005)
and Koo et al. (2008) used the Brown cluster
features for semi-supervised learning of various
NLP tasks and achieved significant improvements.
?
Email correspondence.
Recent research has focused on a special fam-
ily of word representations, named ?word embed-
dings?. Word embeddings are conventionally de-
fined as dense, continuous, and low-dimensional
vector representations of words. Word embed-
dings can be learned from large-scale unlabeled
texts through context-predicting models (e.g., neu-
ral network language models) or spectral methods
(e.g., canonical correlation analysis) in an unsu-
pervised setting.
Compared with the so-called one-hot represen-
tation where each word is represented as a sparse
vector of the same size of the vocabulary and only
one dimension is on, word embedding preserves
rich linguistic regularities of words with each di-
mension hopefully representing a latent feature.
Similar words are expected to be distributed close
to one another in the embedding space. Conse-
quently, word embeddings can be beneficial for
a variety of NLP applications in different ways,
among which the most simple and general way is
to be fed as features to enhance existing supervised
NLP systems.
Previous work has demonstrated effectiveness
of the continuous word embedding features in sev-
eral tasks such as chunking and NER using gener-
alized linear models (Turian et al., 2010).
1
How-
ever, there still remain two fundamental problems
that should be addressed:
? Are the continuous embedding features fit for
the generalized linear models that are most
widely adopted in NLP?
? How can the generalized linear models better
utilize the embedding features?
According to the results provided by Turian et
1
Generalized linear models refer to the models that de-
scribe the data as a combination of linear basis functions,
either directly in the input variables space or through some
transformation of the probability distributions (e.g., log-
linear models).
110
al. (2010), the embedding features brought signif-
icantly less improvement than Brown clustering
features. This result is actually not reasonable be-
cause the expressing power of word embeddings
is theoretically stronger than clustering-based rep-
resentations which can be regarded as a kind of
one-hot representation but over a low-dimensional
vocabulary (Bengio et al., 2013).
Wang and Manning (2013) showed that linear
architectures perform better in high-dimensional
discrete feature space than non-linear ones,
whereas non-linear architectures are more effec-
tive in low-dimensional and continuous feature
space. Hence, the previous method that directly
uses the continuous word embeddings as features
in linear models (CRF) is inappropriate. Word
embeddings may be better utilized in the linear
modeling framework by smartly transforming the
embeddings to some relatively higher dimensional
and discrete representations.
Driven by this motivation, we present three
different approaches: binarization (Section 3.2),
clustering (Section 3.3) and a new proposed distri-
butional prototype method (Section 3.4) for better
incorporating the embeddings features. In the bi-
narization approach, we directly binarize the con-
tinuous word embeddings by dimension. In the
clustering approach, we cluster words based on
their embeddings and use the resulting word clus-
ter features instead. In the distributional prototype
approach, we derive task-specific features from
word embeddings by utilizing a set of automati-
cally extracted prototypes for each target label.
We carefully compare and analyze these ap-
proaches in the task of NER. Experimental results
are promising. With each of the three approaches,
we achieve higher performance than directly using
the continuous embedding features, among which
the distributional prototype approach performs the
best. Furthermore, by putting the most effective
two of these features together, we finally outper-
form the continuous embedding features by nearly
2 points of F1 Score (86.21% vs. 88.11%).
The major contribution of this paper is twofold.
(1) We investigate various approaches that can bet-
ter utilize word embeddings for semi-supervised
learning. (2) We propose a novel distributional
prototype approach that shows the great potential
of word embedding features. All the presented ap-
proaches can be easily integrated into most of the
classical linear NLP models.
2 Semi-supervised Learning with Word
Embeddings
Statistical modeling has achieved great success
in most NLP tasks. However, there still remain
some major unsolved problems and challenges,
among which the most widely concerned is the
data sparsity problem. Data sparsity in NLP is
mainly caused by two factors, namely, the lack
of labeled training data and the Zipf distribution
of words. On the one hand, large-scale labeled
training data are typically difficult to obtain, espe-
cially for structure prediction tasks, such as syn-
tactic parsing. Therefore, the supervised mod-
els can only see limited examples and thus make
biased estimation. On the other hand, the nat-
ural language words are Zipf distributed, which
means that most of the words appear a few times
or are completely absent in our texts. For these
low-frequency words, the corresponding parame-
ters usually cannot be fully trained.
More foundationally, the reason for the above
factors lies in the high-dimensional and sparse lex-
ical feature representation, which completely ig-
nores the similarity between features, especially
word features. To overcome this weakness, an ef-
fective way is to learn more generalized represen-
tations of words by exploiting the numerous un-
labeled data, in a semi-supervised manner. After
which, the generalized word representations can
be used as extra features to facilitate the super-
vised systems.
Liang (2005) learned Brown clusters of
words (Brown et al., 1992) from unlabeled data
and use them as features to promote the supervised
NER and Chinese word segmentation. Brown
clusters of words can be seen as a generalized
word representation distributed in a discrete and
low-dimensional vocabulary space. Contextually
similar words are grouped in the same cluster. The
Brown clustering of words was also adopted in de-
pendency parsing (Koo et al., 2008) and POS tag-
ging for online conversational text (Owoputi et al.,
2013), demonstrating significant improvements.
Recently, another kind of word representation
named ?word embeddings? has been widely stud-
ied (Bengio et al., 2003; Mnih and Hinton, 2008).
Using word embeddings, we can evaluate the sim-
ilarity of two words straightforward by comput-
ing the dot-product of two numerical vectors in the
Hilbert space. Two similar words are expected to
111
be distributed close to each other.
2
Word embeddings can be useful as input to an
NLP model (mostly non-linear) or as additional
features to enhance existing systems. Collobert
et al. (2011) used word embeddings as input to a
deep neural network for multi-task learning. De-
spite of the effectiveness, such non-linear models
are hard to build and optimize. Besides, these ar-
chitectures are often specialized for a certain task
and not scalable to general tasks. A simple and
more general way is to feed word embeddings as
augmented features to an existing supervised sys-
tem, which is similar to the semi-supervised learn-
ing with Brown clusters.
As discussed in Section 1, Turian et al. (2010)
is the pioneering work on using word embedding
features for semi-supervised learning. However,
their approach cannot fully exploit the potential
of word embeddings. We revisit this problem
in this study and investigate three different ap-
proaches for better utilizing word embeddings in
semi-supervised learning.
3 Approaches for Utilizing Embedding
Features
3.1 Word Embedding Training
In this paper, we will consider a context-
predicting model, more specifically, the Skip-gram
model (Mikolov et al., 2013a; Mikolov et al.,
2013b) for learning word embeddings, since it is
much more efficient as well as memory-saving
than other approaches.
Let?s denote the embedding matrix to be learned
by C
d?N
, where N is the vocabulary size and d is
the dimension of word embeddings. Each column
of C represents the embedding of a word. The
Skip-gram model takes the current word w as in-
put, and predicts the probability distribution of its
context words within a fixed window size. Con-
cretely, w is first mapped to its embedding v
w
by
selecting the corresponding column vector of C
(or multiplying C with the one-hot vector of w).
The probability of its context word c is then com-
puted using a log-linear function:
P (c|w; ?) =
exp(v
>
c
v
w
)
?
c
?
?V
exp(v
c
?
>
v
w
)
(1)
where V is the vocabulary. The parameters ? are
v
w
i
, v
c
i
for w, c ? V and i = 1, ..., d. Then, the
2
The term similar should be viewed depending on the spe-
cific task.
log-likelihood over the entire training dataset D
can be computed as:
J(?) =
?
(w,c)?D
log p(c|w; ?) (2)
The model can be trained by maximizing J(?).
Here, we suppose that the word embeddings
have already been trained from large-scale unla-
beled texts. We will introduce various approaches
for utilizing the word embeddings as features for
semi-supervised learning. The main idea, as in-
troduced in Section 1, is to transform the continu-
ous word embeddings to some relatively higher di-
mensional and discrete representations. The direct
use of continuous embeddings as features (Turian
et al., 2010) will serve as our baseline setting.
3.2 Binarization of Embeddings
One fairly natural approach for converting the
continuous-valued word embeddings to discrete
values is binarization by dimension.
Formally, we aim to convert the continuous-
valued embedding matrixC
d?N
, to another matrix
M
d?N
which is discrete-valued. There are various
conversion functions. Here, we consider a sim-
ple one. For the i
th
dimension of the word em-
beddings, we divide the corresponding row vector
C
i
into two halves for positive (C
i+
) and nega-
tive (C
i?
), respectively. The conversion function
is then defined as follows:
M
ij
= ?(C
ij
) =
?
?
?
?
?
U
+
, if C
ij
? mean(C
i+
)
B
?
, if C
ij
? mean(C
i?
)
0, otherwise
where mean(v) is the mean value of vector v, U
+
is a string feature which turns on when the value
(C
ij
) falls into the upper part of the positive list.
Similarly, B
?
refers to the bottom part of the neg-
ative list. The insight behind ? is that we only con-
sider the features with strong opinions (i.e., posi-
tive or negative) on each dimension and omit the
values close to zero.
3.3 Clustering of Embeddings
Yu et al. (2013) introduced clustering embeddings
to overcome the disadvantage that word embed-
dings are not suitable for linear models. They sug-
gested that the high-dimensional cluster features
make samples from different classes better sepa-
rated by linear models.
112
In this study, we again investigate this ap-
proach. Concretely, each word is treated as a sin-
gle sample. The batch k-means clustering algo-
rithm (Sculley, 2010) is used,
3
and each cluster
is represented as the mean of the embeddings of
words assigned to it. Similarities between words
and clusters are measured by Euclidean distance.
Moreover, different number of clusters n con-
tain information of different granularities. There-
fore, we combine the cluster features of different
ns to better utilize the embeddings.
3.4 Distributional Prototype Features
We propose a novel kind of embedding features,
named distributional prototype features for su-
pervised models. This is mainly inspired by
prototype-driven learning (Haghighi and Klein,
2006) which was originally introduced as a pri-
marily unsupervised approach for sequence mod-
eling. In prototype-driven learning, a few pro-
totypical examples are specified for each target
label, which can be treated as an injection of
prior knowledge. This sparse prototype informa-
tion is then propagated across an unlabeled corpus
through distributional similarities.
The basic motivation of the distributional pro-
totype features is that similar words are supposed
to be tagged with the same label. This hypothesis
makes great sense in tasks such as NER and POS
tagging. For example, suppose Michael is a pro-
totype of the named entity (NE) type PER. Using
the distributional similarity, we could link similar
words to the same prototypes, so the word David
can be linked to Michael because the two words
have high similarity (exceeds a threshold). Using
this link feature, the model will push David closer
to PER.
To derive the distributional prototype features,
first, we need to construct a few canonical exam-
ples (prototypes) for each target annotation label.
We use the normalized pointwise mutual informa-
tion (NPMI) (Bouma, 2009) between the label and
word, which is a smoothing version of the standard
PMI, to decide the prototypes of each label. Given
the annotated training corpus, the NPMI between
a label and word is computed as follows:
?
n
(label, word) =
?(label, word)
? ln p(label, word)
(3)
3
code.google.com/p/sofia-ml
NE Type Prototypes
B-PER Mark, Michael, David, Paul
I-PER Akram, Ahmed, Khan, Younis
B-ORG Reuters, U.N., Ajax, PSV
I-ORG Newsroom, Inc, Corp, Party
B-LOC U.S., Germany, Britain, Australia
I-LOC States, Republic, Africa, Lanka
B-MISC Russian, German, French, British
I-MISC Cup, Open, League, OPEN
O ., ,, the, to
Table 1: Prototypes extracted from the CoNLL-
2003 NER training data using NPMI.
where,
?(label, word) = ln
p(label, word)
p(label)p(word)
(4)
is the standard PMI.
For each target label l (e.g., PER, ORG, LOC),
we compute the NPMI of l and all words in the
vocabulary, and the top m words are chosen as the
prototypes of l. We should note that the proto-
types are extracted fully automatically, without in-
troducing additional human prior knowledge.
Table 1 shows the top four prototypes extracted
from the NER training corpus of CoNLL-2003
shared task (Tjong Kim Sang and De Meul-
der, 2003), which contains four NE types, namely,
PER, ORG, LOC, and MISC. Non-NEs are denoted
by O. We convert the original annotation to the
standard BIO-style. Thus, the final corpus con-
tains nine labels in total.
Next, we introduce the prototypes as features to
our supervised model. We denote the set of pro-
totypes for all target labels by S
p
. For each proto-
type z ? S
p
, we add a predicate proto = z, which
becomes active at each w if the distributional sim-
ilarity between z and w (DistSim(z, w)) is above
some threshold. DistSim(z, w) can be efficiently
calculated through the cosine similarity of the em-
beddings of z and w. Figure 1 gives an illustra-
tion of the distributional prototype features. Un-
like previous embedding features or Brown clus-
ters, the distributional prototype features are task-
specific because the prototypes of each label are
extracted from the training data.
Moreover, each prototype word is also its own
prototype (since a word has maximum similarity
to itself). Thus, if the prototype is closely related
to a label, all the words that are distributionally
113
i -1x ix1?iy iyO B-LOC
in
/IN
Hague
/NNP
O B-LOC1( , )? ? ?i if y y
( , )
 word = Hague
    pos = NNP
 proto = Britain  B-LOC
 proto = England 
 ...
?
? ?
? ?
? ?? ?
?? ?
? ?
? ?
? ?? ?
i if x y
Figure 1: An example of distributional prototype
features for NER.
similar to that prototype are pushed towards that
label.
4 Supervised Evaluation Task
Various tasks can be considered to compare and
analyze the effectiveness of the above three ap-
proaches. In this study, we partly follow Turian
et al. (2010) and Yu et al. (2013), and take NER as
the supervised evaluation task.
NER identifies and classifies the named entities
such as the names of persons, locations, and orga-
nizations in text. The state-of-the-art systems typ-
ically treat NER as a sequence labeling problem,
where each word is tagged either as a BIO-style
NE or a non-NE category.
Here, we use the linear chain CRF model, which
is most widely used for sequence modeling in the
field of NLP. The CoNLL-2003 shared task dataset
from the Reuters, which was used by Turian et
al. (2010) and Yu et al. (2013), was chosen as
our evaluation dataset. The training set contains
14,987 sentences, the development set contains
3,466 sentences and is used for parameter tuning,
and the test set contains 3,684 sentences.
The baseline features are shown in Table 2.
4.1 Embedding Feature Templates
In this section, we introduce the embedding fea-
tures to the baseline NER system, turning the su-
pervised approach into a semi-supervised one.
Dense embedding features. The dense con-
tinuous embedding features can be fed directly to
the CRF model. These embedding features can
be seen as heterogeneous features from the exist-
ing baseline features, which are discrete. There is
no effective way for dense embedding features to
be combined internally or with other discrete fea-
tures. So we only use the unigram embedding fea-
tures following Turian et al. (2010). Concretely,
the embedding feature template is:
Baseline NER Feature Templates
00: w
i+k
,?2 ? k ? 2
01: w
i+k
? w
i+k+1
,?2 ? k ? 1
02: t
i+k
,?2 ? k ? 2
03: t
i+k
? t
i+k+1
,?2 ? k ? 1
04: chk
i+k
,?2 ? k ? 2
05: chk
i+k
? chk
i+k+1
,?2 ? k ? 1
06: Prefix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
07: Suffix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
08: Type(w
i+k
),?2 ? k ? 2
Unigram Features
y
i
? 00? 08
Bigram Features
y
i?1
? y
i
Table 2: Features used in the NER system. t is
the POS tag. chk is the chunking tag. Prefix
and Suffix are the first and last l characters of a
word. Type indicates if the word is all-capitalized,
is-capitalized, all-digits, etc.
? de
i+k
[d], ?2 ? k ? 2, d ranges over the
dimensions of the dense word embedding de.
Binarized embedding features. The binarized
embedding feature template is similar to the dense
one. The only difference is that the feature val-
ues are discrete and we omit dimensions with zero
value. Therefore, the feature template becomes:
? bi
i+k
[d], ?2 ? k ? 2, where bi
i+k
[d] 6= 0,
d ranges over the dimensions of the binarized
vector bi of word embedding.
In this way, the dimension of the binarized em-
bedding feature space becomes 2 ? d compared
with the originally d of the dense embeddings.
Compound cluster features. The advantage of
the cluster features is that they can be combined
internally or with other features to form compound
features, which can be more discriminative. Fur-
thermore, the number of resulting clusters n can
be tuned, and different ns indicate different granu-
larities. Concretely, the compound cluster feature
template for each specific n is:
? c
i+k
, ?2 ? k ? 2.
? c
i+k
? c
i+k+1
,?2 ? k ? 1.
? c
i?1
? c
i+1
.
Distributional prototype features. The set of
prototypes is again denoted by S
p
, which is de-
114
cided by selecting the topm (NPMI) words as pro-
totypes of each label, where m is tuned on the de-
velopment set. For each word w
i
in a sequence,
we compute the distributional similarity between
w
i
and each prototype in S
p
and select the proto-
types zs that DistSim(z, w) ? ?. We set ? = 0.5
without manual tuning. The distributional proto-
type feature template is then:
? {proto
i+k
=z | DistSim(w
i+k
, z) ? ? & z ?
S
p
}, ?2 ? k ? 2 .
We only use the unigram features, since the
number of active distributional prototype features
varies for different words (positions). Hence,
these features cannot be combined effectively.
4.2 Brown Clustering
Brown clustering has achieved great success in
various NLP applications. At most time, it
provides a strong baseline that is difficult to
beat (Turian et al., 2010). Consequently, in our
study, we conduct comparisons among the embed-
ding features and the Brown clustering features,
along with further investigations of their combina-
tion.
The Brown algorithm is a hierarchical cluster-
ing algorithm which optimizes a class-based bi-
gram language model defined on the word clus-
ters (Brown et al., 1992). The output of the Brown
algorithm is a binary tree, where each word is
uniquely identified by its path from the root. Thus
each word can be represented as a bit-string with
a specific length.
Following the setting of Owoputi et al. (2013),
we will use the prefix features of hierarchical clus-
ters to take advantage of the word similarity in dif-
ferent granularities. Concretely, the Brown cluster
feature template is:
? bc
i+k
, ?2 ? k ? 2.
? prefix (bc
i+k
, p), p ? {2,4,6,...,16}, ?2 ?
k ? 2. prefix takes the p-length prefix of
the Brown cluster coding bc
i+k
.
5 Experiments
5.1 Experimental Setting
We take the English Wikipedia until August 2012
as our unlabeled data to train the word embed-
dings.
4
Little pre-processing is conducted for the
4
download.wikimedia.org.
training of word embeddings. We remove para-
graphs that contain non-roman characters and all
MediaWiki markups. The resulting text is tok-
enized using the Stanford tokenizer,
5
and every
word is converted to lowercase. The final dataset
contains about 30 million sentences and 1.52 bil-
lion words. We use a dictionary that contains
212,779 most common words (frequency ? 80) in
the dataset. An efficient open-source implementa-
tion of the Skip-gram model is adopted.
6
We ap-
ply the negative sampling
7
method for optimiza-
tion, and the asynchronous stochastic gradient de-
scent algorithm (Asynchronous SGD) for parallel
weight updating. In this study, we set the dimen-
sion of the word embeddings to 50. Higher di-
mension is supposed to bring more improvements
in semi-supervised learning, but its comparison is
beyond the scope of this paper.
For the cluster features, we tune the number
of clusters n from 500 to 3000 on the develop-
ment set, and finally use the combination of n =
500, 1000, 1500, 2000, 3000, which achieves the
best results. For the distributional prototype fea-
tures, we use a fixed number of prototype words
(m) for each target label. m is tuned on the devel-
opment set and is finally set to 40.
We induce 1,000 brown clusters of words, the
setting in prior work (Koo et al., 2008; Turian et
al., 2010). The training data of brown clustering is
the same with that of training word embeddings.
5.2 Results
Table 3 shows the performances of NER on the
test dataset. Our baseline is slightly lower than
that of Turian et al. (2010), because they use
the BILOU encoding of NE types which outper-
forms BIO encoding (Ratinov and Roth, 2009).
8
Nonetheless, our conclusions hold. As we can see,
all of the three approaches we investigate in this
study achieve better performance than the direct
use of the dense continuous embedding features.
To our surprise, even the binarized embedding
features (BinarizedEmb) outperform the continu-
ous version (DenseEmb). This provides clear evi-
dence that directly using the dense continuous em-
beddings as features in CRF indeed cannot fully
5
nlp.stanford.edu/software/tokenizer.
shtml.
6
code.google.com/p/word2vec/.
7
More details are analyzed in (Goldberg and Levy, 2014).
8
We use BIO encoding here in order to compare with most
of the reported benchmarks.
115
Setting F1
Baseline 83.43
+DenseEmb? 86.21
+BinarizedEmb 86.75
+ClusterEmb 86.90
+DistPrototype 87.44
+BinarizedEmb+ClusterEmb 87.56
+BinarizedEmb+DistPrototype 87.46
+ClusterEmb+DistPrototype 88.11
+Brown 87.49
+Brown+ClusterEmb 88.17
+Brown+DistPrototype 88.04
+Brown+ClusterEmb+DistPrototype 88.58
Finkel et al. (2005) 86.86
Krishnan and Manning (2006) 87.24
Ando and Zhang (2005) 89.31
Collobert et al. (2011) 88.67
Table 3: The performance of semi-supervised
NER on the CoNLL-2003 test data, using vari-
ous embedding features. ? DenseEmb refers to the
method used by Turian et al. (2010), i.e., the direct
use of the dense and continuous embeddings.
exploit the potential of word embeddings. The
compound cluster features (ClusterEmb) also out-
perform the DenseEmb. The same result is also
shown in (Yu et al., 2013). Further, the distribu-
tional prototype features (DistPrototype) achieve
the best performance among the three approaches
(1.23% higher than DenseEmb).
We should note that the feature templates used
for BinarizedEmb and DistPrototype are merely
unigram features. However, for ClusterEmb, we
form more complex features by combining the
clusters of the context words. We also consider
different number of clusters n, to take advantage
of the different granularities. Consequently, the
dimension of the cluster features is much higher
than that of BinarizedEmb and DistPrototype.
We further combine the proposed features to see
if they are complementary to each other. As shown
in Table 3, the cluster and distributional prototype
features are the most complementary, whereas the
binarized embedding features seem to have large
overlap with the distributional prototype features.
By combining the cluster and distributional pro-
totype features, we further push the performance
to 88.11%, which is nearly two points higher than
the performance of the dense embedding features
(86.21%).
9
We also compare the proposed features with
the Brown cluster features. As shown in Table 3,
the distributional prototype features alone achieve
comparable performance with the Brown clusters.
When the cluster and distributional prototype fea-
tures are used together, we outperform the Brown
clusters. This result is inspiring because we show
that the embedding features indeed have stronger
expressing power than the Brown clusters, as de-
sired. Finally, by combining the Brown cluster
features and the proposed embedding features, the
performance can be improved further (88.58%).
The binarized embedding features are not included
in the final compound features because they are al-
most overlapped with the distributional prototype
features in performance.
We also summarize some of the reported
benchmarks that utilize unlabeled data (with no
gazetteers used), including the Stanford NER tag-
ger (Finkel et al. (2005) and Krishnan and Man-
ning (2006)) with distributional similarity fea-
tures. Ando and Zhang (2005) use unlabeled data
for constructing auxiliary problems that are ex-
pected to capture a good feature representation of
the target problem. Collobert et al. (2011) adjust
the feature embeddings according to the specific
task in a deep neural network architecture. We
can see that both Ando and Zhang (2005) and Col-
lobert et al. (2011) learn task-specific lexical fea-
tures, which is similar to the proposed distribu-
tional prototype method in our study. We suggest
this to be the main reason for the superiority of
these methods.
Another advantage of the proposed discrete fea-
tures over the dense continuous features is tag-
ging efficiency. Table 4 shows the running time
using different kinds of embedding features. We
achieve a significant reduction of the tagging time
per sentence when using the discrete features. This
is mainly due to the dense/sparse battle. Al-
though the dense embedding features are low-
dimensional, the feature vector for each word is
much denser than in the sparse and discrete feature
space. Therefore, we actually need much more
computation during decoding. Similar results can
be observed in the comparison of the DistProto-
type and ClusterEmb features, since the density of
the DistPrototype features is higher. It is possible
9
Statistical significant with p-value < 0.001 by two-tailed
t-test.
116
Setting Time (ms) / sent
Baseline 1.04
+DenseEmb 4.75
+BinarizedEmb 1.25
+ClusterEmb 1.16
+DistPrototype 2.31
Table 4: Running time of different features on a
Intel(R) Xeon(R) E5620 2.40GHz machine.
to accelerate the DistPrototype, by increasing the
threshold of DistSim(z, w). However, this is in-
deed an issue of trade-off between efficiency and
accuracy.
5.3 Analysis
In this section, we conduct analyses to show the
reasons for the improvements.
5.3.1 Rare words
As discussed by Turian et al. (2010), much of the
NER F1 is derived from decisions regarding rare
words. Therefore, in order to show that the three
proposed embedding features have stronger abil-
ity for handling rare words, we first conduct anal-
ysis for the tagging errors of words with differ-
ent frequency in the unlabeled data. We assign the
word frequencies to several buckets, and evaluate
the per-token errors that occurred in each bucket.
Results are shown in Figure 2. In most cases, all
three embedding features result in fewer errors on
rare words than the direct use of dense continuous
embedding features.
Interestingly, we find that for words that are
extremely rare (0?256), the binarized embedding
features incur significantly fewer errors than other
approaches. As we know, the embeddings for the
rare words are close to their initial value, because
they received few updates during training. Hence,
these words are not fully trained. In this case,
we would like to omit these features because their
embeddings are not even trustable. However, all
embedding features that we proposed except Bi-
narizedEmb are unable to handle this.
In order to see how much we have utilized
the embedding features in BinarizedEmb, we cal-
culate the sparsity of the binarized embedding
vectors, i.e., the ratio of zero values in each
vector (Section 3.2). As demonstrated in Fig-
ure 3, the sparsity-frequency curve has good prop-
erties: higher sparsity for very rare words and
very frequent words, while lower sparsity for mid-
frequent words. It indicates that for words that are
very rare or very frequent, BinarizedEmb just omit
most of the features. This is reasonable also for
the very frequent words, since they usually have
rich and diverse context distributions and their
embeddings cannot be well learned by our mod-
els (Huang et al., 2012).
l
l
l
l l
l
l
l
l
l
l
Frequency of word in unlabeled data
Spars
ity
256 1k 4k 16k 64k0
.50
0.55
0.60
0.65
0.70
Figure 3: Sparsity (with confidence interval) of the
binarized embedding vector w.r.t. word frequency
in the unlabeled data.
Figure 2(b) further supports our analysis. Bina-
rizedEmb also reduce much of the errors for the
highly frequent words (32k-64k).
As expected, the distributional prototype fea-
tures produce fewest errors in most cases. The
main reason is that the prototype features are task-
specific. The prototypes are extracted from the
training data and contained indicative information
of the target labels. By contrast, the other em-
bedding features are simply derived from general
word representations and are not specialized for
certain tasks, such as NER.
5.3.2 Linear Separability
Another reason for the superiority of the proposed
embedding features is that the high-dimensional
discrete features are more linear separable than
the low-dimensional continuous embeddings. To
verify the hypothesis, we further carry out experi-
ments to analyze the linear separability of the pro-
posed discrete embedding features against dense
continuous embeddings.
We formalize this problem as a binary classi-
fication task, to determine whether a word is an
NE or not (NE identification). The linear support
vector machine (SVM) is used to build the clas-
sifiers, using different embedding features respec-
117
0?256 256?512 512?1k 1k?2kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
0
50
100
150
200
250 DenseEmbBinarizedEmbClusterEmbDistPrototype
(a)
4k?8k 8k?16k 16k?32k 32k?64kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
40
60
80
100
120 DenseEmbBinarizedEmbClusterEmbDistPrototype
(b)
Figure 2: The number of per-token errors w.r.t. word frequency in the unlabeled data. (a) For rare words
(frequency ? 2k). (b) For frequent words (frequency ? 4k).
Setting Acc. #features
DenseEmb 95.46 250
BinarizedEmb 94.10 500
ClusterEmb 97.57 482,635
DistPrototype 96.09 1,700
DistPrototype-binary 96.82 4,530
Table 5: Performance of the NE/non-NE classi-
fication on the CoNLL-2003 development dataset
using different embedding features.
tively. We use the LIBLINEAR tool (Fan et al.,
2008) as our SVM implementation. The penalty
parameter C is tuned from 0.1 to 1.0 on the devel-
opment dataset. The results are shown in Table 5.
As we can see, NEs and non-NEs can be better
separated using ClusterEmb or DistPrototype fea-
tures. However, the BinarizedEmb features per-
form worse than the direct use of word embedding
features. The reason might be inferred from the
third column of Table 5. As demonstrated in Wang
and Manning (2013), linear models are more ef-
fective in high-dimensional and discrete feature
space. The dimension of the BinarizedEmb fea-
tures remains small (500), which is merely twice
the DenseEmb. By contrast, feature dimensions
are much higher for ClusterEmb and DistProto-
type, leading to better linear separability and thus
can be better utilized by linear models.
We notice that the DistPrototype features per-
form significantly worse than ClusterEmb in NE
identification. As described in Section 3.4, in
previous experiments, we automatically extracted
prototypes for each label, and propagated the in-
formation via distributional similarities. Intu-
itively, the prototypes we used should be more ef-
fective in determining fine-grained NE types than
identifying whether a word is an NE. To verify
this, we extract new prototypes considering only
two labels, namely, NE and non-NE, using the
same metric in Section 3.4. As shown in the last
row of Table 5, higher performance is achieved.
6 Related Studies
Semi-supervised learning with generalized word
representations is a simple and general way of im-
proving supervised NLP systems. One common
approach for inducing generalized word represen-
tations is to use clustering (e.g., Brown clustering)
(Miller et al., 2004; Liang, 2005; Koo et al., 2008;
Huang and Yates, 2009).
Aside from word clustering, word embeddings
have been widely studied. Bengio et al. (2003)
propose a feed-forward neural network based lan-
guage model (NNLM), which uses an embedding
layer to map each word to a dense continuous-
valued and low-dimensional vector (parameters),
and then use these vectors as the input to predict
the probability distribution of the next word. The
NNLM can be seen as a joint learning framework
for language modeling and word representations.
Alternative models for learning word embed-
dings are mostly inspired by the feed-forward
NNLM, including the Hierarchical Log-Bilinear
Model (Mnih and Hinton, 2008), the recurrent
neural network language model (Mikolov, 2012),
the C&W model (Collobert et al., 2011), the log-
linear models such as the CBOW and the Skip-
118
gram model (Mikolov et al., 2013a; Mikolov et
al., 2013b).
Aside from the NNLMs, word embeddings can
also be induced using spectral methods, such as
latent semantic analysis and canonical correlation
analysis (Dhillon et al., 2011). The spectral meth-
ods are generally faster but much more memory-
consuming than NNLMs.
There has been a plenty of work that exploits
word embeddings as features for semi-supervised
learning, most of which take the continuous fea-
tures directly in linear models (Turian et al., 2010;
Guo et al., 2014). Yu et al. (2013) propose com-
pound k-means cluster features based on word em-
beddings. They show that the high-dimensional
discrete cluster features can be better utilized by
linear models such as CRF. Wu et al. (2013) fur-
ther apply the cluster features to transition-based
dependency parsing.
7 Conclusion and Future Work
This paper revisits the problem of semi-supervised
learning with word embeddings. We present three
different approaches for a careful comparison and
analysis. Using any of the three embedding fea-
tures, we obtain higher performance than the di-
rect use of continuous embeddings, among which
the distributional prototype features perform the
best, showing the great potential of word embed-
dings. Moreover, the combination of the proposed
embedding features provides significant additive
improvements.
We give detailed analysis about the experimen-
tal results. Analysis on rare words and linear sep-
arability provides convincing explanations for the
performance of the embedding features.
For future work, we are exploring a novel and a
theoretically more sounding approach of introduc-
ing embedding kernel into the linear models.
Acknowledgments
We are grateful to Mo Yu for the fruitful discus-
sion on the implementation of the cluster-based
embedding features. We also thank Ruiji Fu,
Meishan Zhang, Sendong Zhao and the anony-
mous reviewers for their insightful comments and
suggestions. This work was supported by the
National Key Basic Research Program of China
via grant 2014CB340503 and the National Natu-
ral Science Foundation of China (NSFC) via grant
61133012 and 61370164.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 1?9. Association for Computational Lin-
guistics.
Yoshua Bengio, R. E. Jean Ducharme, Pascal Vincent,
and Christian Janvin. 2003. A neural probabilistic
language model. The Journal of Machine Learning
Research, 3(Feb):1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 35(8):1798?1828.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31?40.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L. E. On Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (almost)
from scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, volume 24 of NIPS, pages 199?
207.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Yoav Goldberg and Omer Levy. 2014. word2vec ex-
plained: deriving mikolov et al.?s negative-sampling
word-embedding method. CoRR, abs/1402.3722.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 497?507, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
119
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
320?327. Association for Computational Linguis-
tics.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
495?503.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 873?882,
Jeju Island, Korea. ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In Kathleen McKeown, Johanna D. Moore, Si-
mone Teufel, James Allan, and Sadaoki Furui, edi-
tors, Proc. of ACL-08: HLT, Proc. of ACL-08: HLT,
pages 595?603, Columbus, Ohio. ACL.
Vijay Krishnan and Christopher D Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 1121?1128. Association for Compu-
tational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master thesis, Massachusetts Institute
of Technology.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proc. of Workshop at
ICLR, Proc. of Workshop at ICLR, Arizona.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of the NIPS, Proc. of the NIPS, pages
3111?3119, Nevada. MIT Press.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph. d. thesis, Brno Uni-
versity of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
Proc. of the NIPS, Proc. of the NIPS, pages 1081?
1088, Vancouver. MIT Press.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, CoNLL ?09,
pages 147?155, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D Sculley. 2010. Combined regression and ranking.
In Proceedings of the 16th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 979?988. ACM.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Jan Hajic, San-
dra Carberry, and Stephen Clark, editors, Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 384?394, Uppsala, Sweden. ACL.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proc. of the Sixth International Joint Con-
ference on Natural Language Processing, Proc. of
the Sixth International Joint Conference on Natural
Language Processing, pages 1285?1291, Nagoya,
Japan. Asian Federation of Natural Language Pro-
cessing.
Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dian-
hai Yu, Hua Wu, and Haifeng Wang. 2013. Gener-
alization of words for chinese dependency parsing.
IWPT-2013, page 73.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Di-
anhai Yu. 2013. Compound embedding features for
semi-supervised learning. In Proc. of the NAACL-
HLT, Proc. of the NAACL-HLT, pages 563?568, At-
lanta. NAACL.
120
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199?1209,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Semantic Hierarchies via Word Embeddings
Ruiji Fu
?
, Jiang Guo
?
, Bing Qin
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{rjfu, jguo, bqin, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Semantic hierarchy construction aims to
build structures of concepts linked by
hypernym?hyponym (?is-a?) relations. A
major challenge for this task is the
automatic discovery of such relations.
This paper proposes a novel and effec-
tive method for the construction of se-
mantic hierarchies based on word em-
beddings, which can be used to mea-
sure the semantic relationship between
words. We identify whether a candidate
word pair has hypernym?hyponym rela-
tion by using the word-embedding-based
semantic projections between words and
their hypernyms. Our result, an F-score
of 73.74%, outperforms the state-of-the-
art methods on a manually labeled test
dataset. Moreover, combining our method
with a previous manually-built hierarchy
extension method can further improve F-
score to 80.29%.
1 Introduction
Semantic hierarchies are natural ways to orga-
nize knowledge. They are the main components
of ontologies or semantic thesauri (Miller, 1995;
Suchanek et al, 2008). In the WordNet hierar-
chy, senses are organized according to the ?is-a?
relations. For example, ?dog? and ?canine? are
connected by a directed edge. Here, ?canine? is
called a hypernym of ?dog.? Conversely, ?dog?
is a hyponym of ?canine.? As key sources
of knowledge, semantic thesauri and ontologies
can support many natural language processing
applications. However, these semantic resources
are limited in its scope and domain, and their
manual construction is knowledge intensive and
time consuming. Therefore, many researchers
?
Email correspondence.
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite ???medicinal plant
??medicine
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
Figure 1: An example of semantic hierarchy con-
struction.
have attempted to automatically extract semantic
relations or to construct taxonomies.
A major challenge for this task is the auto-
matic discovery of hypernym-hyponym relations.
Fu et al (2013) propose a distant supervision
method to extract hypernyms for entities from
multiple sources. The output of their model is
a list of hypernyms for a given enity (left pan-
el, Figure 1). However, there usually also exists
hypernym?hyponym relations among these hy-
pernyms. For instance, ??? (plant)? and
???? (Ranunculaceae)? are both hyper-
nyms of the entity ??? (aconit),? and ??
? (plant)? is also a hypernym of ????
(Ranunculaceae).? Given a list of hypernyms
of an entity, our goal in the present work is to
construct a semantic hierarchy of these hypernyms
(right panel, Figure 1).
1
Some previous works extend and refine
manually-built semantic hierarchies by using other
resources (e.g., Wikipedia) (Suchanek et al,
2008). However, the coverage is limited by the
scope of the resources. Several other works relied
heavily on lexical patterns, which would suffer
from deficiency because such patterns can only
cover a small proportion of complex linguistic cir-
cumstances (Hearst, 1992; Snow et al, 2005).
1
In this study, we focus on Chinese semantic hierarchy
construction. The proposed method can be easily adapted to
other languages.
1199
Besides, distributional similarity methods (Kotler-
man et al, 2010; Lenci and Benotto, 2012) are
based on the assumption that a term can only be
used in contexts where its hypernyms can be used
and that a term might be used in any contexts
where its hyponyms are used. However, it is not
always rational. Our previous method based on
web mining (Fu et al, 2013) works well for hy-
pernym extraction of entity names, but it is unsuit-
able for semantic hierarchy construction which in-
volves many words with broad semantics. More-
over, all of these methods do not use the word
semantics effectively.
This paper proposes a novel approach for se-
mantic hierarchy construction based on word em-
beddings. Word embeddings, also known as dis-
tributed word representations, typically represent
words with dense, low-dimensional and real-
valued vectors. Word embeddings have been
empirically shown to preserve linguistic regular-
ities, such as the semantic relationship between
words (Mikolov et al, 2013b). For example,
v(king) ? v(queen) ? v(man) ? v(woman),
where v(w) is the embedding of the word w. We
observe that a similar property also applies to the
hypernym?hyponym relationship (Section 3.3),
which is the main inspiration of the present study.
However, we further observe that hypernym?
hyponym relations are more complicated than a
single offset can represent. To address this chal-
lenge, we propose a more sophisticated and gen-
eral method ? learning a linear projection which
maps words to their hypernyms (Section 3.3.1).
Furthermore, we propose a piecewise linear pro-
jection method based on relation clustering to
better model hypernym?hyponym relations (Sec-
tion 3.3.2). Subsequently, we identify whether an
unknown word pair is a hypernym?hyponym re-
lation using the projections (Section 3.4). To the
best of our knowledge, we are the first to apply
word embeddings to this task.
For evaluation, we manually annotate a dataset
containing 418 Chinese entities and their hyper-
nym hierarchies, which is the first dataset for this
task as far as we know. The experimental results
show that our method achieves an F-score of
73.74% which significantly outperforms the pre-
vious state-of-the-art methods. Moreover, com-
bining our method with the manually-built hier-
archy extension method proposed by Suchanek et
al. (2008) can further improve F-score to 80.29%.
2 Background
As main components of ontologies, semantic hi-
erarchies have been studied by many researchers.
Some have established concept hierarchies based
on manually-built semantic resources such as
WordNet (Miller, 1995). Such hierarchies have
good structures and high accuracy, but their cov-
erage is limited to fine-grained concepts (e.g.,
?Ranunculaceae? is not included in Word-
Net.). We have made similar obsevation that about
a half of hypernym?hyponym relations are absent
in a Chinese semantic thesaurus. Therefore, a
broader range of resources is needed to supple-
ment the manually built resources. In the construc-
tion of the famous ontology YAGO, Suchanek et
al. (2008) link the categories in Wikipedia onto
WordNet. However, the coverage is still limited
by the scope of Wikipedia.
Several other methods are based on lexical
patterns. They use manually or automatically
constructed lexical patterns to mine hypernym?
hyponym relations from text corpora. A hierarchy
can then be built based on these pairwise relations.
The pioneer work by Hearst (1992) has found
out that linking two noun phrases (NPs) via cer-
tain lexical constructions often implies hypernym
relations. For example, NP
1
is a hypernym of NP
2
in the lexical pattern ?such NP
1
as NP
2
.? Snow et
al. (2005) propose to automatically extract large
numbers of lexico-syntactic patterns and subse-
quently detect hypernym relations from a large
newswire corpus. Their method relies on accurate
syntactic parsers, and the quality of the automat-
ically extracted patterns is difficult to guarantee.
Generally speaking, these pattern-based methods
often suffer from low recall or precision because
of the coverage or the quality of the patterns.
The distributional methods assume that the con-
texts of hypernyms are broader than the ones of
their hyponyms. For distributional similarity com-
puting, each word is represented as a semantic
vector composed of the pointwise mutual infor-
mation (PMI) with its contexts. Kotlerman et al
(2010) design a directional distributional measure
to infer hypernym?hyponym relations based on
the standard IR Average Precision evaluation mea-
sure. Lenci and Benotto (2012) propose anoth-
er measure focusing on the contexts that hyper-
nyms do not share with their hyponyms. However,
broader semantics may not always infer broader
contexts. For example, for terms ?Obama? and
1200
?American people?, it is hard to say whose
contexts are broader.
Our previous work (Fu et al, 2013) applies a
web mining method to discover the hypernyms of
Chinese entities from multiple sources. We as-
sume that the hypernyms of an entity co-occur
with it frequently. It works well for named enti-
ties. But for class names (e.g., singers in Hong
Kong, tropical fruits) with wider range of mean-
ings, this assumption may fail.
In this paper, we aim to identify hypernym?
hyponym relations using word embeddings, which
have been shown to preserve good properties for
capturing semantic relationship between words.
3 Method
In this section, we first define the task formally.
Then we elaborate on our proposed method com-
posed of three major steps, namely, word embed-
ding training, projection learning, and hypernym?
hyponym relation identification.
3.1 Task Definition
Given a list of hypernyms of an entity, our goal is
to construct a semantic hierarchy on it (Figure 1).
We represent the hierarchy as a directed graph
G, in which the nodes denote the words, and the
edges denote the hypernym?hyponym relations.
Hypernym-hyponym relations are asymmetric and
transitive when words are unambiguous:
? ?x, y ? L : x
H
??y ? ?(y
H
??x)
? ?x, y, z ? L : (x
H
??z ? z
H
??y)? x
H
??y
Here, L denotes the list of hypernyms. x, y and
z denote the hypernyms in L. We use
H
?? to
represent a hypernym?hyponym relation in this
paper. Actually, x, y and z are unambiguous as
the hypernyms of a certain entity. Therefore, G
should be a directed acyclic graph (DAG).
3.2 Word Embedding Training
Various models for learning word embeddings
have been proposed, including neural net lan-
guage models (Bengio et al, 2003; Mnih and
Hinton, 2008; Mikolov et al, 2013b) and spec-
tral models (Dhillon et al, 2011). More recent-
ly, Mikolov et al (2013a) propose two log-linear
models, namely the Skip-gram and CBOW model,
to efficiently induce word embeddings. These two
models can be trained very efficiently on a large-
scale corpus because of their low time complexity.
No. Examples
1
v(?)? v(??) ? v(?)? v(??)
v(shrimp)? v(prawn) ? v(fish)? v(gold fish)
2
v(??)? v(??) ? v(??)? v(??)
v(laborer)? v(carpenter) ? v(actor)? v(clown)
3
v(??)? v(??) 6? v(?)? v(??)
v(laborer)? v(carpenter) 6? v(fish)? v(gold fish)
Table 1: Embedding offsets on a sample of
hypernym?hyponym word pairs.
Additionally, their experiment results have shown
that the Skip-gram model performs best in identi-
fying semantic relationship among words. There-
fore, we employ the Skip-gram model for estimat-
ing word embeddings in this study.
The Skip-gram model adopts log-linear classi-
fiers to predict context words given the current
word w(t) as input. First, w(t) is projected to its
embedding. Then, log-linear classifiers are em-
ployed, taking the embedding as input and pre-
dict w(t)?s context words within a certain range,
e.g. k words in the left and k words in the
right. After maximizing the log-likelihood over
the entire dataset using stochastic gradient descent
(SGD), the embeddings are learned.
3.3 Projection Learning
Mikolov et al (2013b) observe that word em-
beddings preserve interesting linguistic regulari-
ties, capturing a considerable amount of syntac-
tic/semantic relations. Looking at the well-known
example: v(king) ? v(queen) ? v(man) ?
v(woman), it indicates that the embedding offsets
indeed represent the shared semantic relation be-
tween the two word pairs.
We observe that the same property also ap-
plies to some hypernym?hyponym relations. As
a preliminary experiment, we compute the em-
bedding offsets between some randomly sampled
hypernym?hyponym word pairs and measure their
similarities. The results are shown in Table 1.
The first two examples imply that a word can
also be mapped to its hypernym by utilizing word
embedding offsets. However, the offset from
?carpenter? to ?laborer? is distant from
the one from ?gold fish? to ?fish,? indicat-
ing that hypernym?hyponym relations should be
more complicated than a single vector offset can
represent. To verify this hypothesis, we com-
pute the embedding offsets over all hypernym?
1201
???-????sportsman - footballer ??-???staff - civil servant??-??laborer - gardener??-???seaman - navigator??-??actor - singer ??-??actor - protagonist??-??actor - clown
??-??position - headmaster
??-???actor - matador
??-???laborer - temporary worker ??-??laborer - carpenter ??-???position ? consul general
??-??staff - airline hostess??-???staff - salesclerk??-???staff - conductor?-??chicken - cock?-????sheep - small-tail Han sheep?-??sheep - ram ?-??equus - zebra ?-??shrimp - prawn
?-??dog - police dog?-???rabbit - wool rabbit
??-???dolphin - white-flag dolphin ?-??fish - shark ?-???fish - tropical fish?-??fish - gold fish
?-??crab - sea crab
?-??donkey - wild ass
Figure 2: Clusters of the vector offsets in training data. The figure shows that the vector offsets distribute
in some clusters. The left cluster shows some hypernym?hyponym relations about animals. The right
one shows some relations about people?s occupations.
hyponym word pairs in our training data and vi-
sualize them.
2
Figure 2 shows that the relations
are adequately distributed in the clusters, which
implies that hypernym?hyponym relations in-
deed can be decomposed into more fine-grained
relations. Moreover, the relations about animals
are spatially close, but separate from the relations
about people?s occupations.
To address this challenge, we propose to learn
the hypernym?hyponym relations using projection
matrices.
3.3.1 A Uniform Linear Projection
Intuitively, we assume that all words can be pro-
jected to their hypernyms based on a uniform tran-
sition matrix. That is, given a word x and its hy-
pernym y, there exists a matrix ? so that y = ?x.
For simplicity, we use the same symbols as the
words to represent the embedding vectors. Ob-
taining a consistent exact ? for the projection of
all hypernym?hyponym pairs is difficult. Instead,
we can learn an approximate ? using Equation 1
on the training data, which minimizes the mean-
squared error:
?
?
= arg min
?
1
N
?
(x,y)
? ?x? y ?
2
(1)
where N is the number of (x, y) word pairs in
the training data. This is a typical linear regres-
sion problem. The only difference is that our pre-
dictions are multi-dimensional vectors instead of
scalar values. We use SGD for optimization.
2
Principal Component Analysis (PCA) is applied for di-
mensionality reduction.
3.3.2 Piecewise Linear Projections
A uniform linear projection may still be under-
representative for fitting all of the hypernym?
hyponym word pairs, because the relations are
rather diverse, as shown in Figure 2. To better
model the various kinds of hypernym?hyponym
relations, we apply the idea of piecewise linear re-
gression (Ritzema, 1994) in this study.
Specifically, the input space is first segmented
into several regions. That is, all word pairs (x, y)
in the training data are first clustered into sever-
al groups, where word pairs in each group are
expected to exhibit similar hypernym?hyponym
relations. Each word pair (x, y) is represented
with their vector offsets: y ? x for clustering.
The reasons are twofold: (1) Mikolov?s work has
shown that the vector offsets imply a certain lev-
el of semantic relationship. (2) The vector off-
sets distribute in clusters well, and the word pairs
which are close indeed represent similar relations,
as shown in Figure 2.
Then we learn a separate projection for each
cluster, respectively (Equation 2).
?
?
k
= arg min
?
k
1
N
k
?
(x,y)?C
k
? ?
k
x? y ?
2
(2)
where N
k
is the amount of word pairs in the k
th
cluster C
k
.
We use the k-means algorithm for clustering,
where k is tuned on a development dataset.
3.3.3 Training Data
To learn the projection matrices, we extract train-
ing data from a Chinese semantic thesaurus,
Tongyi Cilin (Extended) (CilinE for short) which
1202
?
?
?
?
?
Root
L ev el 1
L ev el 2
L ev el 3
L ev el 4
L ev el 5
?  ob j ect
??  animal
??  insect
- -
??  dragonf ly
B
i
18
A
06@
?? : ?? 
( dragonf ly  :  animal)
?? : ?? 
( dragonf ly  :  insect)
C ilinEh y perny m-h y pony m pairs
S ense C ode:  B i1 8 A0 6 @
S ense C ode:  B i1 8 A
S ense C ode:  B i1 8
S ense C ode:  B i
S ense C ode:  B
?? : ?? 
( insect :  animal)
Figure 3: Hierarchy of CilinE and an Example of
Training Data Generation
contains 100,093 words (Che et al, 2010).
3
CilinE
is organized as a hierarchy of five levels, in which
the words are linked by hypernym?hyponym
relations (right panel, Figure 3). Each word in
CilinE has one or more sense codes (some words
are polysemous) that indicate its position in the hi-
erarchy.
The senses of words in the first level, such as
?? (object)? and ??? (time),? are very gen-
eral. The fourth level only has sense codes without
real words. Therefore, we extract words in the sec-
ond, third and fifth levels to constitute hypernym?
hyponym pairs (left panel, Figure 3).
Note that mapping one hyponym to multi-
ple hypernyms with the same projection (?x is
unique) is difficult. Therefore, the pairs with the
same hyponym but different hypernyms are ex-
pected to be clustered into separate groups. Fig-
ure 3 shows that the word ?dragonfly? in the
fifth level has two hypernyms: ?insect? in the
third level and ?animal? in the second level.
Hence the relations dragonfly
H
?? insect and
dragonfly
H
?? animal should fall into differ-
ent clusters.
In our implementation, we apply this constraint
by simply dividing the training data into two cat-
egories, namely, direct and indirect. Hypernym-
hyponym word pair (x, y) is classified into the di-
rect category, only if there doesn?t exist another
word z in the training data, which is a hypernym of
x and a hyponym of y. Otherwise, (x, y) is classi-
fied into the indirect category. Then, data in these
two categories are clustered separately.
3
www.ltp-cloud.com/download/
x
y
?k
? 
x'
?l
Figure 4: In this example, ?
k
x is located in the
circle with center y and radius ?. So y is consid-
ered as a hypernym of x. Conversely, y is not a
hypernym of x
?
.
x
y
z
x
y
(a) (b)
z
x
y
Figure 5: (a) If d(?
j
y, x) > d(?
k
x, y), we re-
move the path from y to x; (b) if d(?
j
y, x) >
d(?
k
x, z) and d(?
j
y, x) > d(?
i
z, y), we reverse
the path from y to x.
3.4 Hypernym-hyponym Relation
Identification
Upon obtaining the clusters of training data and
the corresponding projections, we can identify
whether two words have a hypernym?hyponym re-
lation. Given two words x and y, we find cluster
C
k
whose center is closest to the offset y ? x, and
obtain the corresponding projection ?
k
. For y to
be considered a hypernym of x, one of the two
conditions below must hold.
Condition 1: The projection ?
k
puts ?
k
x close
enough to y (Figure 4). Formally, the euclidean
distance between ?
k
x and y: d(?
k
x, y) must be
less than a threshold ?.
d(?
k
x, y) =? ?
k
x? y ?
2
< ? (3)
Condition 2: There exists another word z sat-
isfying x
H
??z and z
H
??y. In this case, we use the
transitivity of hypernym?hyponym relations.
Besides, the final hierarchy should be a DAG
as discussed in Section 3.1. However, the pro-
jection method cannot guarantee that theoretical-
ly, because the projections are learned from pair-
wise hypernym?hyponym relations without the w-
hole hierarchy structure. All pairwise hypernym?
hyponym relation identification methods would
suffer from this problem actually. It is an inter-
esting problem how to construct a globally opti-
1203
mal semantic hierarchy conforming to the form
of a DAG. But this is not the focus of this paper.
So if some conflicts occur, that is, a relation cir-
cle exists, we remove or reverse the weakest path
heuristically (Figure 5). If a circle has only two
nodes, we remove the weakest path. If a circle has
more than two nodes, we reverse the weakest path
to form an indirect hypernym?hyponym relation.
4 Experimental Setup
4.1 Experimental Data
In this work, we learn word embeddings from a
Chinese encyclopedia corpus named Baidubaike
4
,
which contains about 30 million sentences (about
780 million words). The Chinese segmentation
is provided by the open-source Chinese language
processing platform LTP
5
(Che et al, 2010).
Then, we employ the Skip-gram method (Section
3.2) to train word embeddings. Finally we obtain
the embedding vectors of 0.56 million words.
The training data for projection learning is
collected from CilinE (Section 3.3.3). We ob-
tain 15,247 word pairs of hypernym?hyponym
relations (9,288 for direct relations and 5,959 for
indirect relations).
For evaluation, we collect the hypernyms for
418 entities, which are selected randomly from
Baidubaike, following Fu et al (2013). We then
ask two annotators to manually label the seman-
tic hierarchies of the correct hypernyms. The final
data set contains 655 unique hypernyms and 1,391
hypernym?hyponym relations among them. We
randomly split the labeled data into 1/5 for de-
velopment and 4/5 for testing (Table 2). The hi-
erarchies are represented as relations of pairwise
words. We measure the inter-annotator agreement
using the kappa coefficient (Siegel and Castel-
lan Jr, 1988). The kappa value is 0.96, which indi-
cates a good strength of agreement.
4.2 Evaluation Metrics
We use precision, recall, and F-score as our met-
rics to evaluate the performances of the methods.
Since hypernym?hyponym relations and its re-
verse (hyponym?hypernym) have one-to-one cor-
respondence, their performances are equal. For
4
Baidubaike (baike.baidu.com) is one of the largest
Chinese encyclopedias containing more than 7.05 million en-
tries as of September, 2013.
5
www.ltp-cloud.com/demo/
Relation
# of word pairs
Dev. Test
hypernym?hyponym 312 1,079
hyponym?hypernym
?
312 1,079
unrelated 1,044 3,250
Total 1,668 5,408
Table 2: The evaluation data.
?
Since hypernym?
hyponym relations and hyponym?hypernym
relations have one-to-one correspondence, their
numbers are the same.
1 5 
10 15 
20 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
1 10 20 30 40 50 60 Indirect 
F1-
Sco
re 
Direct 
Figure 6: Performance on development data w.r.t.
cluster size.
simplicity, we only report the performance of the
former in the experiments.
5 Results and Analysis
5.1 Varying the Amount of Clusters
We first evaluate the effect of different number of
clusters based on the development data. We vary
the numbers of the clusters both for the direct and
indirect training word pairs.
As shown in Figure 6, the performance of clus-
tering is better than non-clustering (when the clus-
ter number is 1), thus providing evidences that
learning piecewise projections based on clustering
is reasonable. We finally set the numbers of the
clusters of direct and indirect to 20 and 5, respec-
tively, where the best performances are achieved
on the development data.
5.2 Comparison with Previous Work
In this section, we compare the proposed method
with previous methods, including manually-built
hierarchy extension, pairwise relation extraction
1204
P(%) R(%) F(%)
M
Wiki+CilinE
92.41 60.61 73.20
M
Pattern
97.47 21.41 35.11
M
Snow
60.88 25.67 36.11
M
balApinc
54.96 53.38 54.16
M
invCL
49.63 62.84 55.46
M
Fu
87.40 48.19 62.13
M
Emb
80.54 67.99 73.74
M
Emb+CilinE
80.59 72.42 76.29
M
Emb+Wiki+CilinE
79.78 80.81 80.29
Table 3: Comparison of the proposed method with
existing methods in the test set.
Pattern Translation
w?[??|??] h w is a [a kind of] h
w [?]? h w[,] and other h
h [?]?[?] w h[,] called w
h [?] [?]? w h[,] such as w
h [?]??? w h[,] especially w
Table 4: Chinese Hearst-style lexical patterns. The
contents in square brackets are omissible.
based on patterns, word distributions, and web
mining (Section 2). Results are shown in Table 3.
5.2.1 Overall Comparison
M
Wiki+CilinE
refers to the manually-built hierar-
chy extension method of Suchanek et al (2008).
In our experiment, we use the category taxonomy
of Chinese Wikipedia
6
to extend CilinE. Table 3
shows that this method achieves a high precision
but also a low recall, mainly because of the limit-
ed scope of Wikipedia.
M
Pattern
refers to the pattern-based method of
Hearst (1992). We extract hypernym?hyponym
relations in the Baidubaike corpus, which is al-
so used to train word embeddings (Section 4.1).
We use the Chinese Hearst-style patterns (Table
4) proposed by Fu et al (2013), in which w rep-
resents a word, and h represents one of its hy-
pernyms. The result shows that only a small part
of the hypernyms can be extracted based on these
patterns because only a few hypernym relations
are expressed in these fixed patterns, and many are
expressed in highly flexible manners.
In the same corpus, we apply the method
M
Snow
originally proposed by Snow et al (2005).
The same training data for projections learn-
6
dumps.wikimedia.org/zhwiki/20131205/
ing from CilinE (Section 3.3.3) is used as
seed hypernym?hyponym pairs. Lexico-syntactic
patterns are extracted from the Baidubaike corpus
by using the seeds. We then develop a logistic re-
gression classifier based on the patterns to recog-
nize hypernym?hyponym relations. This method
relies on an accurate syntactic parser, and the qual-
ity of the automatically extracted patterns is diffi-
cult to guarantee.
We re-implement two previous distribution-
al methods M
balApinc
(Kotlerman et al, 2010)
and M
invCL
(Lenci and Benotto, 2012) in the
Baidubaike corpus. Each word is represented as a
feature vector in which each dimension is the PMI
value of the word and its context words. We com-
pute a score for each word pair and apply a thresh-
old to identify whether it is a hypernym?hyponym
relation.
M
Fu
refers to our previous web mining
method (Fu et al, 2013). This method mines hy-
pernyms of a given word w from multiple sources
and returns a ranked list of the hypernyms. We
select the hypernyms with scores over a threshold
of each word in the test set for evaluation. This
method assumes that frequent co-occurrence of a
noun or noun phrase n in multiple sources with w
indicate possibility of n being a hypernym of w.
The results presented in Fu et al (2013) show that
the method works well when w is an entity, but
not when w is a word with a common semantic
concept. The main reason may be that there are
relatively more introductory pages about entities
than about common words in the Web.
M
Emb
is the proposed method based on word
embeddings. Table 3 shows that the proposed
method achieves a better recall and F-score than
all of the previous methods do. It can significantly
(p < 0.01) improve the F-score over the state-of-
the-art method M
Wiki+CilinE
.
M
Emb
and M
CilinE
can also be combined. The
combination strategy is to simply merge all pos-
itive results from the two methods together, and
then to infer new relations based on the transitiv-
ity of hypernym?hyponym relations. The F-score
is further improved from 73.74% to 76.29%. Note
that, the combined method achieves a 4.43% re-
call improvement over M
Emb
, but the precision is
almost unchanged. The reason is that the infer-
ence based on the relations identified automatical-
ly may lead to error propagation. For example, the
relation x
H
??y is incorrectly identified by M
Emb
.
1205
P(%) R(%) F(%)
M
Wiki+CilinE
80.39 19.29 31.12
M
Emb+CilinE
71.16 52.80 60.62
M
Emb+Wiki+CilinE
69.13 61.65 65.17
Table 5: Performance on the out-of-CilinE data in
the test set.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
l l lllllll
l
l
l ll
l MEmb+Wiki+Ci l inEMEmb+Ci l inEMWiki+Ci l inE
Figure 7: Precision-Recall curves on the out-of-
CilinE data in the test set.
When the relation y
H
??z from M
CilinE
is added, it
will cause a new incorrect relation x
H
??z.
Combining M
Emb
with M
Wiki+CilinE
achieves
a 7% F-score improvement over the best baseline
M
Wiki+CilinE
. Therefore, the proposed method
is complementary to the manually-built hierarchy
extension method (Suchanek et al, 2008).
5.2.2 Comparison on the Out-of-CilinE Data
We are greatly interested in the practical perfor-
mance of the proposed method on the hypernym?
hyponym relations outside of CilinE. We say a
word pair is outside of CilinE, as long as there
is one word in the pair not existing in CilinE. In
our test data, about 62% word pairs are outside
of CilinE. Table 5 shows the performances of the
best baseline method and our method on the out-
of-CilinE data. The method exploiting the tax-
onomy in Wikipedia, M
Wiki+CilinE
, achieves the
highest precision but has a low recall. By con-
trast, our method can discover more hypernym?
hyponym relations with some loss of precision,
thereby achieving a more than 29% F-score im-
provement. The combination of these two meth-
ods achieves a further 4.5% F-score improvement
over M
Emb+CilinE
. Generally speaking, the pro-
posed method greatly improves the recall but dam-
ages the precision.
Actually, we can get different precisions and re-
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( a)  C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( b )  W ik ipedia+ C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( c)  E mb edding
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( d)  E mb edding+ W ik ipedia+ C ilinE
Figure 8: An example for error analysis. The
red paths refer to the relations between the named
entity and its hypernyms extracted using the web
mining method (Fu et al, 2013). The black paths
with hollow arrows denote the relations identified
by the different methods. The boxes with dotted
borders refer to the concepts which are not linked
to correct positions.
calls by adjusting the threshold ? (Equation 3).
Figure 7 shows that M
Emb+CilinE
achieves a high-
er precision than M
Wiki+CilinE
when their recalls
are the same. When they achieve the same preci-
sion, the recall of M
Emb+CilinE
is higher.
5.3 Error Analysis and Discussion
We analyze error cases after experiments. Some
cases are shown in Figure 8. We can see that
there is only one general relation ??? (plant)?
H
?? ??? (organism)? existing in CilinE. Some
fine-grained relations exist in Wikipedia, but the
coverage is limited. Our method based on
word embeddings can discover more hypernym?
hyponym relations than the previous methods can.
When we combine the methods together, we get
the correct hierarchy.
Figure 8 shows that our method loses the
relation ???? (Aconitum)? H?? ????
(Ranunculaceae).? It is because they are
very semantically similar (their cosine similarity
is 0.9038). Their representations are so close to
each other in the embedding space that we have
not find projections suitable for these pairs. The
1206
error statistics show that when the cosine similari-
ties of word pairs are greater than 0.8, the recall is
only 9.5%. This kind of error accounted for about
10.9% among all the errors in our test set. One
possible solution may be adding more data of this
kind to the training set.
6 Related Work
In addition to the works mentioned in Section 2,
we introduce another set of related studies in this
section.
Evans (2004), Ortega-Mendoza et al (2007),
and Sang (2007) consider web data as a large cor-
pus and use search engines to identify hypernyms
based on the lexical patterns of Hearst (1992).
However, the low quality of the sentences in the
search results negatively influence the precision of
hypernym extraction.
Following the method for discovering patterns
automatically (Snow et al, 2005), McNamee et
al. (2008) apply the same method to extract hy-
pernyms of entities in order to improve the perfor-
mance of a question answering system. Ritter et al
(2009) propose a method based on patterns to find
hypernyms on arbitrary noun phrases. They use
a support vector machine classifier to identify the
correct hypernyms from the candidates that match
the patterns. As our experiments show, pattern-
based methods suffer from low recall because of
the low coverage of patterns.
Besides Kotlerman et al (2010) and Lenci and
Benotto (2012), other researchers also propose di-
rectional distributional similarity methods (Weeds
et al, 2004; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor et al, 2007; Clarke, 2009). How-
ever, their basic assumption that a hyponym can
only be used in contexts where its hypernyms can
be used and that a hypernym might be used in all
of the contexts where its hyponyms are used may
not always rational.
Snow et al (2006) provides a global optimiza-
tion scheme for extending WordNet, which is d-
ifferent from the above-mentioned pairwise rela-
tionships identification methods.
Word embeddings have been successfully ap-
plied in many applications, such as in sentiment
analysis (Socher et al, 2011b), paraphrase detec-
tion (Socher et al, 2011a), chunking, and named
entity recognition (Turian et al, 2010; Collobert
et al, 2011). These applications mainly utilize
the representing power of word embeddings to al-
leviate the problem of data sparsity. Mikolov et
al. (2013a) and Mikolov et al (2013b) further ob-
serve that the semantic relationship of words can
be induced by performing simple algebraic oper-
ations with word vectors. Their work indicates
that word embeddings preserve some interesting
linguistic regularities, which might provide sup-
port for many applications. In this paper, we
improve on their work by learning multiple lin-
ear projections in the embedding space, to model
hypernym?hyponym relationships within different
clusters.
7 Conclusion and Future Work
This paper proposes a novel method for seman-
tic hierarchy construction based on word em-
beddings, which are trained using a large-scale
corpus. Using the word embeddings, we learn
the hypernym?hyponym relationship by estimat-
ing projection matrices which map words to their
hypernyms. Further improvements are made us-
ing a cluster-based approach in order to model
the more fine-grained relations. Then we propose
a few simple criteria to identity whether a new
word pair is a hypernym?hyponym relation. Based
on the pairwise hypernym?hyponym relations, we
build semantic hierarchies automatically.
In our experiments, the proposed method signif-
icantly outperforms state-of-the-art methods and
achieves the best F1-score of 73.74% on a manual-
ly labeled test dataset. Further experiments show
that our method is complementary to the previous
manually-built hierarchy extension methods.
For future work, we aim to improve word
embedding learning under the guidance of
hypernym?hyponym relations. By including the
hypernym?hyponym relation constraints while
training word embeddings, we expect to improve
the embeddings such that they become more suit-
able for this task.
Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61273321 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Shiqi Zhao,
Zhenghua Li, Wei Song and the anonymous re-
viewers for insightful comments and suggestions.
We also thank Xinwei Geng and Hongbo Cai for
their help in the experiments.
1207
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorith-
m for learning directionality of inference rules. In
EMNLP-CoNLL, pages 161?170.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13?16, Beijing, Chi-
na, August.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, pages 112?119. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199?207.
Richard Evans. 2004. A framework for named entity
recognition in the open domain. Recent Advances in
Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267?274.
Ruiji Fu, Bing Qin, and Ting Liu. 2013. Exploiting
multiple sources for open-domain hypernym discov-
ery. In EMNLP, pages 1224?1234.
Maayan Geffet and Ido Dagan. 2005. The distribution-
al inclusion hypotheses and lexical entailment. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 107?114.
Association for Computational Linguistics.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539?545. Association for Compu-
tational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribution-
al similarity for lexical inference. Natural Language
Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 75?79. Association for
Computational Linguistics.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the
Third International Joint Conference on Natural
Language Processing, pages 799?804.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Rosa M Ortega-Mendoza, Luis Villase?nor-Pineda, and
Manuel Montes-y G?omez. 2007. Using lexical
patterns for extracting hyponyms from the web. In
MICAI 2007: Advances in Artificial Intelligence,
pages 904?911. Springer.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88?93.
HP Ritzema. 1994. Drainage principles and
applications.
Erik Tjong Kim Sang. 2007. Extracting hypernym
pairs from the web. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 165?168. Associa-
tion for Computational Linguistics.
Sidney Siegel and N John Castellan Jr. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304. MIT
Press, Cambridge, MA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computation-
al Linguistics, pages 801?808, Sydney, Australia,
July. Association for Computational Linguistics.
1208
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Ng. 2011a. Dynam-
ic pooling and unfolding recursive autoencoders for
paraphrase detection. In Advances in Neural Infor-
mation Processing Systems, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456?463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th internation-
al conference on Computational Linguistics, page
1015. Association for Computational Linguistics.
1209
A domain adaption Word Segmenter  
For Sighan Bakeoff 2010 
Guo jiang 
 Institute of Intelligent 
Information Processing, 
Beijing Information Science & 
Technology University, 
Beijing, China, 100192 
Guojiang132@gmail.com 
Su Wenjie 
 Institute of Intelligent 
Information Processing, 
Beijing Information Science & 
Technology University, 
Beijing, China, 100192 
dev.sunflower@gmail.com 
Yangsen Zhang 
Institute of Intelligent 
Information Processing, 
Beijing Information Science & 
Technology University, 
Beijing, China, 100192 
zhangyangsen@163.com 
 
Abstract 
We present a Chinese word segmentation 
system which ran on the closed track of the 
simplified Chinese Word Segmentation task 
of CIPS-SIGHAN-CLP 2010 bakeoffs. Our 
segmenter was built using a HMM. To fulfill 
the cross-domain segmentation task, we use 
semi-supervised machine learning method to 
get the HMM model. Finally we get the 
mean result of four domains: P=0.719, 
R=0.72 
1 Introduction 
The 2010 Sighan Bakeoff included two types 
of evaluations: 
(1) Closed training:  In the closed training 
evaluation, participants can only use data 
provided by organizers to train their systems 
specifically, the following data resources and 
software tools are not permitted to be used in 
the training: 
1) Unspecified corpus;  
2) Unspecified dictionary, word list or 
character list: include the dictionaries 
of named entity, character lists for 
specific type of Chinese named entities, 
idiom dictionaries, semantic lexicons, 
etc.  
3) Human-encoded rule bases;  
4) Unspecified software tools, include 
word segmenters, part-of-speech 
taggers, or parsers which are trained 
using unspecified data resources.  
The character type information to distinguish 
the following four character types can be 
used in training: Chinese characters, English 
letters, digits and punctuations. 
(2) Open training:  In the open training 
evaluation, participants can use any language 
resource, including the training data provided 
by organizers 
We prefer character-based Tagging than 
dictionary based word segmentation in closed 
training, for we can only use the provide train 
corpus and scale of the corpus is not large 
enough. If we select dictionary based method 
we will encounter the out-of-vocabulary 
problem. But in character-based Tagging 
method we can yield a better performance 
than the dictionary based method for such 
problem. 
2 Algorithm 
Ever before 2002 almost all word segment 
method is based on dictionary.  In SIGHAN 
2003 bakeoff, a character-based Tagging 
method was proposed and since then the 
character-based Tagging method became 
more and more popular. HMM (Hidden 
Markov Model) has been used extensively in 
speech recognition, pos tagging and get good 
grades. So we chose HMM as our machine 
learning method to fulfill our task. 
We formally define the elements of an HMM, 
and explain how the model generates an 
observation sequence. 
An HMM is characterized by the following: 
1) N, the number of states in the model. we 
denote the individual states as 
s={s?, s?,? , s?},and the state at time t as 
q? 
2) M, the number of distinct observation 
symbols per state. we denote the 
individual symbols as v={v?, v?, . . , v?} 
This work was supported by the national natural science foundation of China (60873013)?Beijing natural science foundation (KZ200811232019); The Open 
Project Program of the Key Laboratory of Computational Linguistics (Peking University), Ministry of Education;Funding Project for Academic Human Resources 
Devel-opment in Institutions of Higher Learning Under the Juris-diction of Beijing Municipality (PHR201007131) 
3) The state transition probability 
distribution A= { a?) } where a?) =P 
[q??? ?s)|q??s?], 1<i,j<N. 
4) The observation symbol probability 
distribution in state j, B={b)?k?}, where 
b)?k??P?v? at t|q??s)? 
5) The initial state distribution ???? 
where ?? ?P?q? ?s?? 
 
Graph1 
For convenience, we use the compact 
notation???A,B, ?) to indicate the complete 
parameter set of the model. 
There are three basic problems for HMM, for 
problem 1 we use forward-backward 
algorithm, for problem 2 we use Viterbi 
algorithm, for problem 3 we use 
Baum-Welch algorithm. 
To application HMM to our task we define 
the HMM five factors as blow: 
1) We define the whole labels set as Q={B, 
M, E, S}, B represents word?s begin, M 
represents word?s middle, E represents 
word?s end and S represents single word. 
2) We define all Unicode characters as O  
3) We define A={a?)}, where a?)=P[prior 
token=s?|posterior label =s)] 
4) We define B={ b)?k? }, where 
b)?k?=P[current character= v? |current 
label =s)] 
5) We define a sentence as a train sample. 
So ?={sentences start with s, s ? Q}. 
Through the design we transform the 
character-based tagging problem to HMM 
problem 2. So we can solve this problem 
with Viterbi algorithm.  
3  Experiment 
We use HMM to establish the Word 
Segment prototype system and make use of 
the Labeled supplied by the Chinese 
Academy of  Sciences to train the HMM 
and get the model parameters which will be 
used for the next iterative scaling. After that, 
we can get a system based on HMM model. 
Then, with the help of the gotten system, we 
process the unlabeled corpus. Once it is 
finished, we should add the processed corpus 
to the labeled corpus and get a larger corpus 
with which we can retrain the HMM. All 
these steps have been done according four 
test corpuses: literature, computer, medicine, 
finance. In the table, R indicates the recall 
rate, P indicates the precision rate, F1 
indicates the macro average, OOV R 
indicates the out-of-vocabulary (OOV) rate, 
OOV RR indicates the out-of-vocabulary 
(OOV) self repair rate, IV RR indicates the 
out-of-vocabulary (OOV) self repair rate. In 
order to more easily view data, we have 
presented the Graph2. 
From the table and graph, we can see that the 
finance corpus has a better result, the 
computer corpus don't show a good result for 
the R, P, F1. Generally speaking, this result 
is a reflection for the difference between the 
dictionary based Tagging method and 
character-based Tagging method. After 
recheck our corpus, we can find that there 
are more technical terms in the computer 
corpus than finance corpus. The explanation 
for the result is that if the system encounter a 
technical terms, the character-based Tagging 
method will have a bad performance. In such 
situation, dictionary based Tagging method 
may have a better performance. For the OOV 
R and OOV RR, the system has a not bad 
performance. Table I and Graph2 show the 
detailed experimental data.  
The results of four test corpus as follow: 
 
Type R P F1 OO
V R 
OO
V 
RR 
IV 
RR 
literatur
e 
0.69
5 
0.74
4 
0.71
9 
0.06
9 
0.38
1 
0.71
9 
Comput
er 
0.71
3 
0.64
1 
0.67
5 
0.15
2 
0.25
7 
0.79
5 
medici
ne 
0.73
5 
0.74 0.73
8 
0.11 0.37
8 
0.77
9 
finance 
0.73
6 
0.75
2 
0.74
4 
0.08
7 
0.23 0.78
4 
Table1 
 
Graph2 
4 Conclusion 
 Our system used a HMM and 
semi-supervised learning for domain 
adapting. Our final system achieved a 
P=0.719, R=0.72. There exist two ways to 
improve our system performance one is 
instead our model of CRF, the other is 
change another way to use the unlabeled data. 
Because the inherent shortage of HMM we 
could not get a precise model, and the way 
we use the unlabeled data can import err to 
labeled data. 
References 
Lawrence R. Rabiner. 1989,2. A Tutorial on 
Hidden Markov Models and Selected 
Applications in Speech Recognition. IEEE, 
VOL.77,No.2,pp:257-286. 
Huang Changning, HaoHai. 2007. Ten Years of 
Chinese word segmentation. Vol. 21, No. 3. 
JOURNAL OF CHINESE INFORMATION 
PROCESSING 
Huihsin Tseng, Pichuan Chang, Galen Andrew, 
Daniel Jurafsky, Christopher Manning.  A 
Conditional Random Field Word Segmenter for 
Sighan Bakeoff 2005. 
Blum A, MITCHELL T. Combining labeled and 
unlabeled data with co-training Proceeding of 
the 11
th
 Annual conference on Computational 
Learning Theory. 
Holmes, W., Russell, M., 1995b. Speech 
recognition using a linear dynamic segmental 
HMM. In: Internat. Conf. on Acoust. Speech 
Signal Process. 1995, Detroit, MI. 
 
 
