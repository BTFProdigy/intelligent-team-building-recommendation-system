A More Discerning and Adaptable Multilingual Transliteration Mechanism
for Indian Languages
Harshit Surana
Language Tech. Research Centre
IIIT, Hyderabad, India
surana.h@gmail.com
Anil Kumar Singh
Language Tech. Research Centre
IIIT, Hyderabad, India
anil@research.iiit.ac.in
Abstract
Transliteration is the process of transcribing
words from a source script to a target script.
These words can be content words or proper
nouns. They may be of local or foreign ori-
gin. In this paper we present a more dis-
cerning method which applies different tech-
niques based on the word origin. The tech-
niques used also take into account the prop-
erties of the scripts. Our approach does not
require training data on the target side, while
it uses more sophisticated techniques on the
source side. Fuzzy string matching is used to
compensate for lack of training on the target
side. We have evaluated on two Indian lan-
guages and have achieved substantially bet-
ter results (increase of up to 0.44 in MRR)
than the baseline and comparable to the state
of the art. Our experiments clearly show that
word origin is an important factor in achiev-
ing higher accuracy in transliteration.
1 Introduction
Transliteration is a crucial factor in Cross Lingual
Information Retrieval (CLIR). It is also important
for Machine Translation (MT), especially when the
languages do not use the same scripts. It is the pro-
cess of transforming a word written in a source lan-
guage into a word in a target language without the
aid of a resource like a bilingual dictionary. Word
pronunciation is usually preserved or is modified ac-
cording to the way the word should be pronounced
in the target language. In simple terms, it means
finding out how a source word should be written in
the script of the target languages such that it is ac-
ceptable to the readers of the target language.
One of the main reasons of the importance of
transliteration from the point of view of Natural Lan-
guage Processing (NLP) is that Out Of Vocabulary
(OOV) words are quite common since every lexi-
cal resource is very limited in practical terms. Such
words include named entities, technical terms, rarely
used or ?difficult? words and other borrowed words,
etc. The OOV words present a challenge to NLP ap-
plications like CLIR and MT. In fact, for very close
languages which use different scripts (like Hindi and
Urdu), the problem of MT is almost an extension of
transliteration.
A substantial percentage of these OOV words
are named entities (AbdulJaleel and Larkey, 2003;
Davis and Ogden, 1998). It has also been shown
that cross language retrieval performance (average
precision) reduced by more than 50% when named
entities in the queries were not transliterated (Larkey
et al, 2003).
Another emerging application of transliteration
(especially in the Indian context) is for building in-
put methods which use QWERTY keyboard for peo-
ple who are more comfortable typing in English.
The idea is that the user types Roman letters but
the input method transforms them into letters of In-
dian language (IL) scripts. This is not as simple
as it seems because there is no clear mapping be-
tween Roman letters and IL letters. Moreover, the
output word should be a valid word. Several com-
mercial efforts have been started in this direction
due to the lack of a good (and familiar) input mech-
64
anism for ILs. These efforts include the Google
Transliteration mechanism1 and Quilpad2. (Rathod
and Joshi, 2002) have also developed more intuitive
input mechanisms for phonetic scripts like Devana-
gari.
Our efforts take into account the type of the word,
the similarities among ILs and the characteristics of
the Latin and IL scripts. We use a sophisticated tech-
nique and machine learning on the source language
(English) side, while a simple and light technique on
the target (IL) side. The advantage of our approach
is that it requires no resources except unannotated
corpus (or pages crawled from the Web) on the IL
side (which is where the resources are scarce). The
method easily generalizes to ILs which use Brahmi
origin scripts. Our method has been designed such
that it can be used for more conventional applica-
tions (MT, CLIR) as well as for applications like
building an input mechanism.
Much of the work for transliteration in ILs has
been done from one Indian script to another. One
of the major work is of Punjabi machine transliter-
ation (Malik, 2006). This work tries to address the
problem of transliteration for Punjabi language from
Shahmukhi (Arabic script) to Gurmukhi using a set
of transliteration rules (character mappings and de-
pendency rules). Om transliteration scheme (Gana-
pathiraju et al, 2005) also provides a script repre-
sentation which is common for all Indian languages.
The display and input are in human readable Roman
script. Transliteration is partly phonetic. (Sinha,
2001) had used Hindi Transliteration used to handle
unknowns in MT.
naukri (A popular domain name) 722,000
nokri (domain name) 19,800
naukari 10,500
naukary (domain name) 5,490
nokari 665
naukarii 133
naukaree 102
Table 1: Variations of a Hindi Word nOkarI (job).
The numbers are pages returned when searching on
Google.
1www.google.co.in/press/pressrel/news transliteration.html
2www.quillpad.com
Aswani et. al (Aswani and Gaizauskas, 2005)
have used a transliteration similarity mechanism to
align English-Hindi parallel texts. They used char-
acter based direct correspondences between Hindi
and English to produce possible transliterations.
Then they apply edit distance based similarity to se-
lect the most probable transliteration in the English
text. However, such method can only be appropriate
for aligning parallel texts as the number of possible
candidates is quite small.
The paper is structured as follows. In Section-
2, we discuss the problem of a high degree of vari-
ation in Indian words, especially when written in
Latin script. In Section-3, we explain the idea of
using information about the word origin for improv-
ing transliteration. Then in Section-4 we describe
the method that we use for guessing the word origin.
Once the word origin is guessed, we can apply one
of the two methods for transliteration depending on
the word origin. These two methods are described in
Section-5 and Section-6, respectively. Fuzzy string
matching, which plays an important role in our ap-
proach, is described in Section-7. In Section-8 we
put together all the elements covered in the pre-
ceding sections and explain the Discerning Adapt-
able Transliteration Mechanism. Section-9 presents
the evaluation of our approach in comparison with
two baseline methods, one of which uses knowledge
about word origin. Finally, in Section-10 we present
the conclusions.
2 Variation in Indian Words in Latin
Script
Since the purpose of our work is not only to translit-
erate named entities but to be useful for applications
like input mechanisms, we had to consider some
other issues too which may not be considered di-
rectly related to transliteration. One of these is that
there is a lot of spelling variation in ILs. This vari-
ation is much more when the IL words are written
using the Latin script (Table-1). In other words,
the amount of ambiguity is very high when we try
to build a system that can be used for purposes
like designing input mechanisms, instead of just for
transliteration of NEs etc. for MT or CLIR. One
reason for very high variation in the latter case is
that unlike Romaji for Japanese (which is taught in
65
schools in Japan), there is no widely adopted translit-
eration scheme using the Latin script, although there
are a number of standard schemes, which are not
used by common users. At present the situation is
that most Indians use Indian scripts while writing in
ILs, but use the Latin script when communicating
online. ILs are rarely used for official communica-
tion, except in government offices in some states.
3 Word Origin and Two Ways of
Transliteration
Previous work for other languages has shown that
word origin plays a part in how the word should
be transliterated(Oh and Choi, 2002; May et al,
2004). Llitjos and Black (Llitjos and Black, 2001)
had shown that the knowledge of language origin
can substantially improve pronunciation generation
accuracy. This information has been used to get bet-
ter results (Oh and Choi, 2002). They first checked
whether the word origin is Greek or not before se-
lecting one of the two methods for transliteration.
This approach improved the results substantially.
However, they had used a set of prefixes and suffixes
to identify the word origin. Such an approach is not
scalable. In fact, in a large number of cases, word
origin cannot be identified by using list of affixes.
For ILs, we also define two categories of words:
words which can be roughly considered Indian and
those which can be roughly considered foreign.
Note that ?Indian? and ?foreign? are just loose labels
here. Indian words, which include proper nouns and
also common vocabulary words, are more relevant in
applications like input methods. Two different meth-
ods are used for transliterating, as explained later.
4 Disambiguating Word Origin
Previously (Llitjos and Black, 2001) used probabili-
ties of all trigrams to belong to a particular language
as an measure to disambiguate word origins. We
use a more sophisticated method that has been suc-
cessfully used for language and encoding identifica-
tion (Singh, 2006a).
We first prepare letter based 5-gram models from
the lists of two kinds of words (Indian and foreign).
Then we combine n-grams of all orders and rank
them according to their probability in descending or-
der. Only the top N n-grams are retained and the
rest are pruned. Now we have two probability dis-
tributions which can be compared by a measure of
distributional similarity. The measure used is sym-
metric cross entropy or SCE (Singh, 2006a).
Since the accuracy of identification is low if test
data is very low, which is true in our case because we
are trying to identify the class of a single word, we
had to extend the method used by Singh. One ma-
jor extension was that we add word beginning and
ending markers to all the words in training as well
as test data. This is because n-grams at beginning,
middle and end of words should be treated differ-
ently if we want to identify the ?language? (or class)
of the word.
For every given word, we get a probability about
its origin based on SCE. Based on this probability
measure, transliteration is performed using different
techniques for different classes (Indian or foreign).
In case of ambiguity, transliteration is performed us-
ing both methods and the probabilities are used to
get the final ranking of all possible transliterations.
5 Transliteration of Foreign Words
These words include named entities (George Bush)
and more common nouns (station, computer) which
are regularly used in ILs. To generate translitera-
tion candidates for such words, we first try to guess
the word pronunciation or use a lookup dictionary (if
available) to find it. Then we use some simple man-
ually created mappings, which can be used for all In-
dian languages. Note that these mappings are very
few in number (Figure-1 and Figure-2) and can be
easily created by non-linguistically trained people.
They play only a small role in the method because
other steps (like fuzzy string matching) do most of
the work.
For our experiments, we used the CMU speech
dictionary as the lookup, and also to train pronunci-
ation estimation. If a word is not in the CMU dic-
tionary, we estimate the word pronunciation, as ex-
plained later.
We directly map from English phonemes to IL let-
ters. This is based on our observation that a foreign
word is usually transliterated in almost the same way
as it is pronounced. Almost all English phonemes
can be roughly mapped to specific letters (repre-
senting phonemes, as IL scripts are phonetic in na-
66
ture) in ILs. Similar observations have been made
about Hindi by Su-Youn Yoon, Kyoung-Young Kim
and Richard Sproat (Yoon et al, 2007). We have
prepared our own mappings with help from native
speakers of the languages concerned, which is rel-
atively quite a simple task since the letters in Indic
scripts correspond closely with phonemes.
6 Transliteration of Indian Words
These words include (mainly Indian) named enti-
ties of (e.g. Taj Mahal, Manmohan Singh) and
common vocabulary words (common nouns, verbs)
which need to be transliterated. They also include
words which are spelled similar to the way Indian
words are spelled when written in Latin (e.g. Bagh-
dad, Husain). As stated earlier, this class of words
are much more relevant for an input method using a
QWERTY keyboard.
Since words of Indian origin usually have pho-
netic spellings when they are written in English
(Latin), the issue of pronunciation estimation or
lookup is not important. However, there can be
many possible vowel and consonant segments which
can be formed out of a single word. For example
?ai? can be interpreted as a single vowel with sound
AE (as in Husain), or as two vowels AA IH (as in
Rai). To perform segmentation, we have a simple
program which produces candidates for all possible
segments. This program uses a few rules defining
the possible consonant and vowel combinations.
Now we simply map these segments to their near-
est IL letters (or letter combinations). This is also
done using a simple set of mappings, which do not
contain any probabilities or contexts. This step gen-
erates transliteration candidates. These are then fil-
tered and ranked using fuzzy string matching.
7 Fuzzy String Matching
The initial steps use simpler methods to generate
transliteration candidates on the source as well as
the target side. They also use no resources on the
target (IL) side. The step of fuzzy string matching
compensates for the lack of more language specific
knowledge during the earlier phase. The transliter-
ation candidates are matched with the words in the
target language corpus (actually, words in the word
list extracted from the corpus). The fuzzy string
Figure 1: Mappings for foreign words. The three
columns are for Roman, Devanagari and Telugu
matching algorithm we use is finely tuned for Indian
Languages and performs much better than language
independent approaches like edit distance (Singh et
al., 2007). This method can be used for all the lan-
guages which use Abugida scripts, e.g. Hindi, Ben-
gali, Telugu, Amharic, Thai etc. It uses characteris-
tics of a writing system for fuzzy search and is able
to take care of spelling variation, which is very com-
mon in these languages. This method shows an im-
provement in F-measure of up to 30% over scaled
edit distance.
The method for fuzzy string matching is based
on the Computational Phonetic Model of Scripts
or CPMS (Singh, 2006b), which models scripts
(specifically Indic scripts) in terms of phonetic (ar-
ticulatory) and orthographic features. For calculat-
ing the distance between two letters it uses a Stepped
Distance Function (SDF). Each letter is represented
as a vector of features. Then, to calculate the dis-
tance between two strings, it uses an adapted ver-
sion of the Dynamic Time Warping algorithm (My-
67
Figure 2: Mappings for Indian Words
ers, 1980). In the fuzzy string matching method that
we use (Singh et al, 2007), an akshar (roughly a
syllable) is used as the unit, instead of a letter.
8 Discerning Adaptable Transliteration
Mechanism (DATM)
We use the above mentioned steps to transliterate a
given word based on its origin. In case of ambigu-
ity of word origin both methods are used, and pos-
sible transliterations are ranked. Based on the class
of the word, the possible pronunciations (for foreign
words) and the possible segmentations (for Indian
words) are generated. Then, for foreign words, En-
glish phonemes are mapped to IL segments. For In-
dian words, Latin segments are mapped to IL seg-
ments.
Now, the transliteration candidates are matched
with target language words, using the fuzzy text
search method (Singh et al, 2007). Possible translit-
erations are ranked based on three parameters: word
frequency, text search cost and the probability of
the word belonging to the class through which it
ForeignWords Indian Words
Word Class Identifier
Pronounciation
Guesser
Word
Segmentation
English Phonemes to
IL Segments Maps
Latin Segments to
IL Segments Maps
Possible
Pronounciations
Possible
Segmentations
Fuzzy String Matching
Transliteration
Candidates
Ranked
Transliterations
Figure 3: Block Diagram of the Discerning Adaptive
Transliteration Method (DATM)
is transliterated. A block diagram describing the
method is shown in Figure-3. The ranks are obtained
on the basis of a score which is calculated using the
following formula:
Tt =
log(ft) ? p(C | s)
cost(c, t) + K (1)
where Tt is the transliteration score for the tar-
get word t, ft is the frequency of t in the target lan-
guage corpus, C is the word class (foreign or In-
dian), s is the source word, c is a transliteration can-
didate which has been generated depending on the
predicted class C , p(C|s) is the probability of the
class C given s, cost(c, t) is the cost of fuzzy string
matching between c and t, and finally K is a con-
stant which determines how much weight is given to
the cost of fuzzy string matching.
9 Evaluation
We evaluate our method for two major languages of
India: Hindi and Telugu. We compare our results
with a very commonly used method (Oh and Choi,
2006) based on bilingual dictionary to learn translit-
68
Language ? English-Hindi English-Telugu
Method ? MRR Pr MRR Pr
DATM 0.87 80% 0.82 71%
DBL 0.56 47% 0.53 46%
BL 0.43 35% 0.43 37%
DATM: Discerning Adaptive Transliteration Mechanism
DBL: Discerning Baseline Method
BL: Baseline Method
MRR: Mean Reciprocal Rank
Pr: Precision
Table 2: Evaluation on English-Hindi and English-Telugu
erations. As there are no bilingual transliteration
dictionaries available for ILs, we had to create our
own resources.
9.1 Experimental Setup
We created 2000-word lists which consisted of both
foreign and Indian words written in Latin script
and their transliterations in Hindi and Telugu. This
dictionary was created by people with professional
knowledge in both English and the respective In-
dian language. We only use this list for training
the baseline method, as our method does not need
training data on the target side. The size of bilingual
word lists that we are using is less than those used
for experiments by some other researchers. But our
approach focuses on developing transliterations for
languages with resource scarcity. This setup is more
meaningful for languages with scarce resources.
Since, normal transliteration mechanisms do not
consider word origin, we train the baseline using
the set of 2000 words containing both foreign and
Indian words. Alignments from English to respec-
tive Indian languages were learned by aligning these
lists using GIZA++. The alignments obtained were
fed into a maximum entropy classifier with a con-
text window size of 2 (3 is generally considered
better window size, but because the training size
is not huge, a context window of 3 gave substan-
tially worse results). This method is similar to
the grapheme based model as described by Oh and
Choi (Oh and Choi, 2006). However, unlike in
their approach, the candidate pairs are matched with
words in the target language and are ranked based
on edit distance (BL).
For our method (DATM), we have used CMU dic-
tionary and a collection of Indian named entities
(written in Latin) extracted from web to train the
language identification module. We have consid-
ered n-grams of order 5 and pruned them by 3500
frequency. In case the foreign word is not found in
CMU Speech dictionary, we guess its pronunciation
using the method described by Oh and Choi. How-
ever, in this case, the context window size is 3.
We also use another method (DBL) to check the
validity of our assumptions about word origin. We
use the same technique as BL, but in this case we
train two models of 1000 words each, foreign and
Indian. To disambiguate which model to use, we
use the same language identification method as in
DATM.
9.2 Results
To evaluate our method we have created word lists
of size 200 which were doubly checked by two indi-
viduals. These also contain both Indian and Foreign
words. We use both precision and mean reciprocal
rank (MRR) to evaluate our method against base-
line (BL) and discerning baseline (DBL). MRR is
a measure commonly used in information retrieval
when there is precisely one correct answer (Kandor
and Vorhees, 2000). Results can be seen in Table-
2. The highest scores were obtained for Hindi using
DATM. The MRR in this case was 0.87.
One important fact that comes out from the re-
sults is that determining the class of a word and then
using an appropriate method can lead to significant
increase in performance. This is clear from the re-
sults for BL and DBL. The only difference between
69
English-Hindi
0
20
40
60
80
100
120
140
160
180
1 2 3 4 5
Rank
Nu
m
be
ro
fW
or
ds
DATM
DBL
BL
English-Telugu
0
20
40
60
80
100
120
140
160
1 2 3 4 5
Rank
Nu
m
be
ro
fW
or
ds
DATM
DBL
BL
Figure 4: Number of Correct Words vs. Rank. A significantly higher percentage of correct words occur
at rank 1 for the DATM method, as compared to BL and DBL methods. This percentage indicates a more
practical view of the accuracy transliteration algorithm.
these two was that two different models were trained
for the two classes. Then the class of the word was
identified (in DBL) and the model trained for that
class was used for transliteration.
It should be noted that Yoon et al (Yoon et al,
2007) have also reported MRR score on Hindi. They
have used a number of phonetic and pseudo features,
and trained their algorithm on a winnow classifier.
They tested their algorithm only for named entities.
They have considered a relatively limited number of
candidate words on the target language side (1,500)
which leads to 150k pairs on which they have eval-
uated their method. They have reported the results
as 0.91 and 0.89 under different test conditions. In
case of our evaluation, we do not restrict the candi-
date words on the target side except that it should
be available in the corpus. Because of this formula-
tion, there are over 1000k words for Hindi and over
1800k words from Telugu. This leads to a extremely
high number of pairs possible. But such an approach
is also necessary as we want our algorithm to be
scalable to bigger sizes and also because there are
no high quality tools (like named entity recogniz-
ers) for Indian languages. This is one of the reason
for relatively (compared to figures reported by other
researchers) low baseline scores. Despite all these
issues, our simpler approach yields similar results.
Figure-4 shows how the number of correct words
varies with the rank.
Two possible issues are the out of vocabulary
(OOV) words and misspelled or foreign words in
the IL corpus. The OOV words are not handled
right now by our method, but we plan to extend our
method to at least partially take care of such words.
The second issue is mostly resolved by our use of
fuzzy string matching, although there is scope for
improvement.
10 Conclusions and Further Work
We presented a more general and adaptable method
for transliteration which is especially suitable for In-
dian languages. This method first identifies the class
(foreign or Indian) of the word on the source side.
Based on the class, one of the two methods is used
for transliteration. Easily creatable mapping tables
and a fuzzy string matching algorithm are then used
to get the target word. Our evaluations shows that
the method performs substantially better than the
two baselines we tested against. The results are bet-
ter in terms of both MRR (up to 0.44) and precision
(45%). Our method is designed to be used for other
applications like tolerant input methods for Indian
languages and it uses no resources on the target lan-
guages side except an unannotated corpus. The re-
sults can be further improved if we consider context
information too.
We have also shown that disambiguating word
origin and applying an appropriate method could be
70
critical in getting good transliterations. Currently we
are assuming that the word to be transliterated is in
the target language corpus. We plan to extend the
method so that even those words can be transliter-
ated which are not in the target language corpus. We
are also working on using this method for building
a tolerant input method for Indian languages and on
integrating the transliteration mechanism as well as
the input method with an open source NLP friendly
editor called Sanchay Editor (Singh, 2008).
References
N. AbdulJaleel and L.S. Larkey. 2003. Statistical
transliteration for english-arabic cross language infor-
mation retrieval. Proceedings of the twelfth interna-
tional conference on Information and knowledge man-
agement, pages 139?146.
N. Aswani and R. Gaizauskas. 2005. A hybrid approach
to align sentences and words in English-Hindi paral-
lel corpora. Proceedings of the ACL Workshop on?
Building and Exploiting Parallel Texts.
M.W. Davis and W.C. Ogden. 1998. Free resources
and advanced alignment for cross-language text re-
trieval. Proceedings of the 6th Text Retrieval Confer-
ence (TREC-6), pages 385?402.
M. Ganapathiraju, M. Balakrishnan, N. Balakrishnan,
and R. Reddy. 2005. OM: One Tool for Many (In-
dian) Languages. ICUDL: International Conference
on Universal Digital Library, Hangzhou.
L. Larkey, N. AbdulJaleel, and M. Connell. 2003.
What?s in a Name? Proper Names in Arabic Cross-
Language Information Retrieval. Technical report,
CIIR Technical Report, IR-278.
A. Llitjos and A. Black. 2001. Knowledge of language
origin improves pronunciation of proper names. Pro-
ceedings of EuroSpeech-01, pages 1919?1922.
M.G.A. Malik. 2006. Punjabi Machine Transliteration.
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 1137?1144.
J. May, A. Brunstein, P. Natarajan, and R. Weischedel.
2004. Surprise! What?s in a Cebuano or Hindi Name?
ACM Transactions on Asian Language Information
Processing (TALIP), 2(3):169?180.
C. S. Myers. 1980. A Comparative Performance Study of
Several Dynamic Time Warping Algorithms for Speech
Recognition. Ph.D. thesis, M.I.T., Cambridge, MA,
Feb. http://gate.ac.uk.
J.H. Oh and K.S. Choi. 2002. An English-Korean
transliteration model using pronunciation and contex-
tual rules. Proceedings of the 19th international con-
ference on Computational linguistics-Volume 1, pages
1?7.
J.H. Oh and K.S. Choi. 2006. An ensemble of translit-
eration models for information retrieval. Information
Processing and Management: an International Jour-
nal, 42(4):980?1002.
A. Rathod and A. Joshi. 2002. A Dynamic Text Input
scheme for phonetic scripts like Devanagari. Proceed-
ings of Development by Design (DYD).
Anil Kumar Singh, Harshit Surana, and Karthik Gali.
2007. More accurate fuzzy text search for languages
using abugida scripts. In Proceedings of ACM SI-
GIR Workshop on Improving Web Retrieval for Non-
English Queries, Amsterdam, Netherlands.
Anil Kumar Singh. 2006a. Study of some distance mea-
sures for language and encoding identification. In Pro-
ceedings of ACL 2006 Workshop on Linguistic Dis-
tance, Sydney, Australia.
Anil Kumar Singh. 2006b. A computational phonetic
model for indian language scripts. In Constraints on
Spelling Changes: Fifth International Workshop on
Writing Systems, Nijmegen, The Netherlands.
Anil Kumar Singh. 2008. A mechanism to provide
language-encoding support and an nlp friendly editor.
In Proceedings of the Third International Joint Con-
ference on Natural Language Processing, Hyderabad,
India.
RMK Sinha. 2001. Dealing with unknowns in machine
translation. Systems, Man, and Cybernetics, 2001
IEEE International Conference on, 2.
S.Y. Yoon, K.Y. Kim, and R. Sproat. 2007. Multilingual
Transliteration Using Feature based Phonetic Method.
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 112?119.
71
A Mechanism to Provide Language-Encoding Support and an NLP Friendly
Editor
Anil Kumar Singh
Language Technologies Research Centre
IIIT, Hyderabad, India
anil@research.iiit.ac.in
Abstract
Many languages of the world (some with
very large numbers of native speakers) are
not yet supported on computers. In this pa-
per we first present a simple method to pro-
vide an extra layer of easily customizable
language-encoding support for less comput-
erized languages. We then describe an ed-
itor called Sanchay Editor, which uses this
method and also has many other facilities
useful for those using less computerized lan-
guages for simple text editing or for Nat-
ural Language Processing purposes, espe-
cially for annotation.
1 Introduction
A large number of languages of the world are still
not supported on computers. Some of them are spo-
ken by a tens or hundreds of millions of people, so
they will probably be supported in the near future.
However, many languages may not be, simply be-
cause the number of people using them on comput-
ers, for whatever reason, is not large. Those who
want to use these languages on computers, includ-
ing the researchers working on those languages, will
need support for these languages. A related problem
is that of support for encodings, as many of these
less computerized languages do not have one stan-
dard encoding that is used by all. Therefore, there is
a need of a simple and easily customizable method
of adding support for a new language or encoding.
Such a method should require minimum technical
knowledge from the user. In this paper, we will
present a method of providing language and encod-
ing support for less computerized languages.
Another need which we address in this paper is of
an editor that not only makes use of the above men-
tioned method of language and encoding support,
but also has many facilities useful for Natural Lan-
guage Processing (NLP) researchers and linguists.
2 Language-Encoding Support
There is no exhaustive, commonly agreed upon list
of encodings for many languages. Even the list of
languages is not without dispute (e.g., whether Bho-
jpuri is a language or not). This implies that the
conventional deterministic approach to language-
encoding support based on the assumption that the
possible languages and encodings are known in ad-
vance is not enough if we do not want to prevent the
possibility of using any language or encoding used
by a significant number of people, or even a rarely
used endangered language.
Even though with the increasing use of Unicode
based encodings, the problem has reduced for many
languages, we still require a facility that can allow
convenient use of new languages which are not cov-
ered in Unicode.
Therefore, what we need is a more customizable
language-encoding support where it is very easy for
the user or the developer to add support for some
language-encoding. For this purpose, as many of
the tasks should be automated as possible. This can
be done by using NLP techniques. Even though
many of the encodings used for less computerized
languages are based on just font mappings, i.e., sup-
porting them basically means providing an appropri-
957
Figure 1: A customizable design for language-encoding support
ate font. This seems to be very simple, but the prob-
lem is that the user may not know which font to use.
Moreover, providing basic support so that you can
type once you have selected the font is not enough.
The user might not even know what encoding some
existing text is in. Then, the user might want to save
the text in some other encoding. To provide user
friendly support for language-encodings in a situa-
tion like this requires a more intelligent design.
Figure-1 shows a design for language-encoding
support which addresses these problems. The main
elements of this design are:
? Properties files listing languages, encodings,
fonts, and their connections
? Language-encoding identification for text
? Language-encoding identification for fonts
? A language-encoding API
? Encoding converters
Currently, 15 languages and 10 encoding are sup-
ported. These are mostly all South Asian languages,
apart from English, since the users till now were
mostly from South Asia. A large number of freely
available fonts have also been included in the distri-
bution, but the user would probably like to add more
fonts, which can be done easily just by adding the
paths of the new fonts in a properties file. There is
no need to install these fonts, irrespective of the op-
erating systems. Also, more languages and encod-
ings can be added quite easily. In most cases, to add
a new language-encoding, the user just has to follow
these steps:
1. Make a new entry in the properties files for
each of these three: languages, encodings and
language-encodings.
2. Specify the paths of all the fonts for that
language-encoding in the properties file for
fonts. These fonts just have to be on the system
and their paths have to specified in the prop-
erties file. However, it may be preferable (for
convenience) that they be stored in fonts direc-
tory of Sanchay.
3. Specify the default font in the properties file for
default fonts.
4. If the new language uses a Unicode encoding,
make an entry for the Unicode block corre-
958
Figure 2: A font chooser listing fonts relevant to a specific language encoding pair
sponding to that language. This is not compul-
sory, but it will allow language specific listing
of fonts for language-encoding pairs involving
Unicode encodings.
In future, we will make this even more easy by
providing a graphic user interface based wizard to
go through these steps.
The editor can also use any input methods avail-
able from the operating system. New input meth-
ods can also be added as Java libraries. Such exter-
nal Java libraries have just to be copied to the ext-
lib directory of Sanchay. It is also possible to eas-
ily switch among input methods (Figure-4), whether
provided by the operating system or included into
(or add to) Sanchay. So, it is possible to enter text in
multiple languages.
Note that, right now, this support for language-
encodings is in the form of an extra platform inde-
pendent layer on top of the support provided by op-
erating systems. Such a layer could possibly be inte-
grated into operating systems in future. This might,
of course, require porting of the code for different
operating systems and can be in-built into the oper-
ating system.
2.1 A More Intelligent Listing of Fonts
In the design used on all operating systems so far,
when you want to view the list of fonts, what you
get is a list of all the fonts installed on the sys-
tem or at least all the fonts found by the operating
system or the user program. This is not very user
friendly for less computerized languages, because
most of the fonts listed may not be meant for the
language-encoding the user is interested in. What
the user needs is the list of fonts relevant to the
specific language-encoding she is interested in. In
our design, this is what the user will see (Figure-2),
when the user views the list of fonts. Of course, we
can also give the user the option to see all the fonts
installed on the system.
2.2 Language-Encoding Identification
Another important element of the design is a
language-encoding identification tool that is inte-
grated into the language-encoding support module
so that if the user opens a file and does not know the
language or encoding of the text, the tool can auto-
matically identify the language-encoding of the text.
The language-encoding identification tool is based
on byte based n-gram models using a distributional
similarity measures (Singh, 2006a). This tools is
computationally quite a light one as the amount of
959
data required for training is very small and it has
been found to be one of the most accurate language-
encoding systems currently available. The user can
make it even faster by removing those language-
encodings which she may not be interested in. This
will require only a change in the relevant properties
file.
2.3 Encoding Conversion
There is also a wrapper module for calling any in-
stalled or built in encoding converter for languages
which use more than one encodings. The user can
easily convert the encoding of the text depending on
her needs and the availability of a relevant encod-
ing converter. It is also possible to easily add new
encoding converters.
3 Sanchay Editor
Although numerous text editors, even free and open
source ones, are available, the simple open source
editor that we are going to describe in this section
(Figure-3) is based on the language-encoding sup-
port mentioned earlier and is also closely integrated
with Sanchay1, a collection of tools and APIs for
NLP. The editor is implemented as a customizable
GUI component that can be easily included in any
Java application. The notable features of this editor
are:
- Uses customizable language-encoding support as
described earlier.
- Can automatically identify language-encoding of
the text using a byte based n-gram
modeling (Singh, 2006a).
- The font chooser (Figure-2) shows only the fonts
applicable for the language-encoding.
- Text can be preprocessed for NLP or annotation
purposes from this editor.
- The formats used for annotation can be detected
and validated from the editor.
- Specialized annotation interfaces can be launched
to edit the annotated files (in text format) opened in
this editor.
- Since the editor is implemented in Java, it can be
used on any platform on which Java (JRE or JDK
version 1.5 or higher) is installed.
1http://ltrc.iiit.ac.in/anil/Sanchay-EILMT and http://source
forge.net/projects/nlp-sanchay
Some of the facilities are described in the follow-
ing sub-sections.
3.1 Validation of Annotation Formats
If the user is directly editing a document which is
annotated with POS tags, chunks or is syntactically
annotated, it is possible to automatically validate the
annotation format of the document. A text box be-
low the main editing panel shows the errors in for-
mat, if any. Usually, annotation is performed by us-
ing some annotation interface, but since the anno-
tated data is stored as simple text, the document can
be edited or annotated directly from a text editor.
The format validation facility has been included to
ensure that after any such editing or annotation, the
document is still in the correct format, as it is easy
for users to make format related mistakes.
3.2 Format Conversion
Sanchay annotation interfaces allow annotation at
various levels like POS tagging, chunking, syntac-
tic (treebank) annotation etc. Currently four dif-
ferent formats are recognized by the system: raw
text without annotation, POS tagged format where
each sentence is simply a sequence of word and POS
tag pairs separated by some symbol like underscore,
?bracket form? which allows POS tagged and chun-
ked data to be represented (including recursion), and
Shakti Standard Format (SSF)2. The editor allows
the user to convert the data from one format to an-
other.
3.3 Document Statistics
The user can also get a statistics about the document,
such as the number of words, the number of sen-
tences, the number of characters, and their respec-
tive frequencies etc. These statistics are according to
the format of the document, i.e., if the document is
in SSF format, then the document will be parsed and
the statistics will be about the annotated document
and the elements of the format, e.g. <Sentence>
tag will not be counted: only actual words (or POS
tags etc.) in the annotated document will be counted.
Such statistics can also be obtained for a number of
documents, i.e., a corpus, not just the current docu-
ment. This can be a very useful facility for working
on annotated corpus.
2www.elda.org/en/proj/scalla/SCALLA2004/sangalsharma.pdf
960
Figure 3: A multipurpose editor for NLP for South Asian languages
Figure 4: Input methods currently supported
961
3.4 Integration with Annotations Interfaces
The editor is built into Sanchay in such a way that
it is possible to open different views of a document,
depending on the annotation format. For example,
if the currently opened document is in SSF format,
then the same document can be opened in the San-
chay Syntactic Annotation Interface just by clicking
on a button or a context menu item. The opposite is
also possible, i.e., if a document is open in the Syn-
tactic Annotation Interface, then it can be directly
opened into the Sanchay Editor as a simple text file.
3.5 Some Other Facilities
Apart from the above mentioned facilities, Sanchay
Editor also has the usual facilities available in text
editors such as find and replace (with regular ex-
pressions and also in the batch mode), reverting to
the saved version, automatic periodic backup etc.
4 Facilities Being Integrated
Some other facilities that have already been imple-
mented and are going to be integrated into the San-
chay Editor include a better spell checker for South
Asian languages based on a Computational Phonetic
Model of Scripts or CPMS (Singh, 2006b). This
model provides a method to calculate the phonetic
and orthographic similarity (surface similarity) of
words or strings. Another facility is the identifi-
cation of languages and encoding in a multilingual
document (Singh and Gorla, 2007a). This is an ex-
tension of the language-encoding identification tools
described earlier and is the first systematic work
on the problem of identification of languages and
encoding in a multilingual document. When this
tool is integrated into the editor, the user will be
able to open a multilingual document and the sys-
tem will automatically identify the sections in dif-
ferent languages and display them accordingly, even
if the document has not been encoded using Uni-
code. Of course, identification is not 100% accu-
rate at present, but we are working on improving
it. Another already implemented facility that is go-
ing to be added is fuzzy text search (Singh et al,
2007c). It is also mainly based on the idea of cal-
culating surface similarity using the CPMS. Fuzzy
text search based on this method performs better
than the traditional methods. Yet another facility
to be added is a more discerning mechanism for
transliteration (Surana and Singh, 2008). The first
important idea in this mechanism is to use different
methods for transliteration based on the word origin
(identified using a modified version of the language-
encoding tool). The second major idea is to use
fuzzy text matching for selecting the best match.
This method also has outperformed other methods.
There is a plan to extend the editor to allow direct
annotation. We will begin by providing support for
discourse annotation and other similar annotations.
5 Conclusions
In this paper we presented a simple but effective
method of providing an easily customizable extra
layer of language-encoding support for less comput-
erized languages. We also described Sanchay Ed-
itor, which uses this method of language-encoding
support and has many other facilities that may be
useful for NLP researchers as well as those who just
need a simple text editor for language-encodings not
usually supported on computers. Sanchay Editor is
closely integrated with a collection of NLP tools and
APIs called Sanchay.
References
Anil Kumar Singh and Jagadeesh Gorla. 2007a. Identi-
fication of languages and encodings in a multilingual
document. In Proceedings of the 3rd ACL SIGWAC
Workshop on Web As Corpus, Louvain-la-Neuve, Bel-
gium.
Anil Kumar Singh, Harshit Surana, and Karthik Gali.
2007c. More accurate fuzzy text search for languages
using abugida scripts. In Proceedings of ACM SI-
GIR Workshop on Improving Web Retrieval for Non-
English Queries, Amsterdam, Netherlands.
Anil Kumar Singh. 2006a. Study of some distance mea-
sures for language and encoding identification. In Pro-
ceedings of ACL 2006 Workshop on Linguistic Dis-
tance, Sydney, Australia.
Anil Kumar Singh. 2006b. A computational phonetic
model for indian language scripts. In Constraints on
Spelling Changes: Fifth International Workshop on
Writing Systems, Nijmegen, The Netherlands.
Harshit Surana and Anil Kumar Singh. 2008. A more
discerning and adaptable multilingual transliteration
mechanism for indian languages. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (To appear), Hyderabad, India.
962
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 7?12,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Natural Language Processing for Less Privileged Languages: Where do we
come from? Where are we going?
Anil Kumar Singh
Language Technologies Research Centre
IIIT, Hyderabad, India
anil@research.iiit.ac.in
Abstract
In the context of the IJCNLP workshop
on Natural Language Processing (NLP) for
Less Privileged Languages, we discuss the
obstacles to research on such languages.
We also briefly discuss the ways to make
progress in removing these obstacles. We
mention some previous work and comment
on the papers selected for the workshop.
1 Introduction
While computing has become ubiquitous in the de-
veloped regions, its spread in other areas such as
Asia is more recent. However, despite the fact that
Asia is a dense area in terms of linguistic diversity
(or perhaps because of it), many Asian languages
are inadequately supported on computers. Even ba-
sic NLP tools are not available for these languages.
This also has a social cost.
NLP or Computational Linguistics (CL) based
technologies are now becoming important and fu-
ture intelligent systems will use more of these tech-
niques. Most of NLP/CL tools and technologies are
tailored for English or European languages. Re-
cently, there has been a rapid growth of IT indus-
try in many Asian countries. This is now the per-
fect time to reduce the linguistic, computational and
computational linguistics gap between the ?more
privileged? and ?less privileged? languages.
The IJCNLP workshop on NLP for Less Privi-
leged Language is aimed at bridging this gap. Only
when a basic infrastructure for supporting regional
languages becomes available can we hope for a more
equitable availability of opportunities made possi-
ble by the language technology. There have already
been attempts in this direction and this workshop
will hopefully take them further.
Figure-1 shows one possible view of the computa-
tional infrastructure needed for language processing
for a particular language, or more preferably, for a
set of related languages.
In this paper, we will first discuss various aspects
of the problem. We will then look back at the work
already done. After that, we will present some sug-
gestion for future work. But we will begin by ad-
dressing a minor issue: the terminology.
2 Terminology
There can be a debate about the correct term for the
languages on which this workshop focuses. There
are at least four candidates: less studied (LS) lan-
guages, resource scarce (RS) languages, less com-
puterized (LC) languages, and less privileged (LP)
languages. Out of these, two (LS and RS) are too
narrow for our purposes. LC is admittedly more ob-
jective, but it also is somewhat narrow in the sense
that it does not cover the lack of resources for cre-
ating resources (finance) and the lack of linguistic
study. We have used LP because it is more general
and covers all the aspects of the problem. However,
it might be preferable to use LC in many contexts.
As the common element among all these terms
is the adjective ?less? (?resoure scarce? can be
paraphrased as ?with less resources?), perhaps we
can avoid the terminological debate by calling the
languages covered by any such terms as the L-
languages.
7
FirstLevel Preprocessing
Second Level Preprocessing Editors and Interfaces
Models and Other
Applications
Higher Level Multilingual
NLP Applications
Text
Language-Encoding
Identification
Encoding Converters
Text Normalization
Sentence Splitting
Tokenization
Morphological Analyzer
Encoding Converter
Generator
Model of Scripts
Spell Checker
Model of Morphology
Part Of Speech Tagger
Other Specialized
Interfaces
Text Editor
Annotation Interfaces
Local Word Grouper
or Chunker
Figure 1: One view of the basic computational in-
frastructure required for Natural Language Process-
ing or Computational Linguistics. Components like
encoding converters are needed for languages with
less standardization, such as the South Asian lan-
guages. Language resources like lexicon, corpora
etc. have not been shown in this figure.
3 Problems
Not surprisingly, the terms mentioned in the previ-
ous section cover different aspects of the problems
that restrict work on and for these languages. There
is a lack of something and each of those terms covers
some part of what is lacking.
3.1 Linguistic Study
The term LS languages indicates that these are not
well studied linguistically. The sheer amount of lin-
guistic analysis available for English is so huge that
the linguistic work on even a language like Hindi,
which is spoken or understood by a billion people,
is simply not comparable. For languages (or di-
alects) like Santali or Manipuri, the situation is much
worse. And there are a large number of languages
which have been studied even less than Santali or
Manipuri. There are dozens (more accurately, hun-
dreds) of such languages in South Asia alone1. It
can be said that very little is known about the ma-
jority of languages of the world, many of which are
facing extinction.
3.2 Language Resources
Even those languages which have been studied to a
good extent, e.g. Telugu, lack language resources,
e.g. a large dictionary in machine readable form,
let alne resources like WordNet or FrameNet, al-
though efforts are being made to develop resources
for some of these languages. The term RS covers
this aspect of the problem.
3.3 Computerization
Computerization, in general, might include machine
readable language resources and NLP tools etc., but
here we will restrict the meaning of this term to the
support for languages that is provided on comput-
ers, either as part of operating systems, or in the
commonly used applications such as word proces-
sors. In the narrowest sense, computerization means
language-encoding support. Even this level of sup-
port is currently not available (or is inadequate) for
a large number of languages.
3.4 Language Processing
Proper computerization (in the restricted sense) is a
prerequisite to effective language processing. But
even without adequate computerization, attempts are
being made towards making language processing
possible for the L-languages. However, language
processing for the L-languages is still far behind that
for English. For a large number of language it is,
in fact, non-existent. This is true even for a lan-
guage like Gujarati, which is the official language
of the state of Gujarat in India and is recognized as
a scheduled language by the government of India.
And it is actually used as the first language by the
people of Gujarat, which is one of the larger states
in India. While adequate computerization may be
easy to achieve in the near future, at least theoret-
ically, language processing (and building language
resources) is going to be much more difficult task.
1Ethnologue: http://www.ethnologue.com/web.asp
8
NL
P
/C
L
Linguistic Study
Co
mp
ute
riza
tion
Less Privileged
(Finance, Human Resources,
Equipment, Socio-Political Support, etc.)
NL
P
/ C
L
Linguistic Study
Co
mp
ute
riza
tion
Other Privileges
More Privileged
Source
Destination
Figure 2: The four dimensions of the problem: The Source is where we come from and Destination is where
we are going. The problem is to go from the Source to the Destination and the solution is non-trivial.
3.5 Other Privileges
One of the major reasons why building language re-
sources and providing language processing capabil-
ities for the L-languages is going to be a very dif-
ficult task is the fact that these languages lack the
privileges which make it possible to build language
resources and NLP/CL tools. By ?privileges? we
mean the availability of finance, equipment, human
resources, and even political and social support for
reducing the lack of computing and language pro-
cessing support for the L-languages. The lack of
such ?privileges? may be the single biggest reason
which is holding back the progress towards provid-
ing computing and language processing support for
these languages.
4 Some (Partially) Successful Efforts
The problem seems to be insurmountable, but there
has been some progress. More importantly, the ur-
gency of solving this problem (even if partially) is
being realized by more and more people. Some re-
cent events or efforts which tried to address the prob-
lem and which have had some impact in improving
the situation are:
? The LREC conferences and workshops2.
? Workshop on ?Shallow Parsing in South Asian
Languages?, IJCAI-07, India.
2www.lrec-conf.org
? EMELD and the Digital Tools Summit in Lin-
guistics, 2006, USA.
? Workshop on Language Resources for Euro-
pean Minority Languages, 1998, Spain.
? Projects supported by ELRA on the Basic Lan-
guage Resource Kit (BLARK) that targets the
specifications of a minimal kits for each lan-
guage to support NLP tools development3.
? There is also a corresponding project at LDC
(the Less Commonly Taught Languages4).
? The IJCNLP Workshop on Named Entity
Recognition for South and South Asian Lan-
guages5.
This list is, of course, not exhaustive. There are
many papers relevant to the theme of this workshop
at the IJCNLP 2008 main conference6 , as at some
previous major conferences. There is also a very rel-
evant tutorial (Mihalcea, 2008) at the IJCNLP 2008
conference about building resources and tools for
languages with scarce resources.
Even the industry is realizing the importance of
providing computing support for some of the L-
languages. In the last few years there have been
many announcements about the addition of some
3http://www.elda.org/blark
4http://projects.ldc.upenn.edu/LCTL
5http://ltrc.iiit.ac.in/ner-ssea-08/
6http://ijcnlp2008.org
9
such language to a product or a service and also
of the addition of better facilities (input methods,
transliteration, search) in an existing product or ser-
vice for some L-language.
5 Towards a Solution
Since the problem is very much like the conserva-
tion of the Earth?s environment, there is no easy so-
lution. It is not even evident that a complete solution
is possible. However, we can still try for the best
possible solution. Such a solution should have some
prerequisites. As Figure-2 shows, the ?other privi-
leges? dimension of the problem has to be a major
element of the solution, but it is not something over
which researchers and developers have much con-
trol. This means that we will have to find ways to
work even with very little of these ?other privileges?.
This is the key point that we want to make in this
paper because it implies that the methods that have
been used for English (a language with almost un-
limited ?privileges?) may not be applicable for the
L-languages. Many of these methods assume the
availability of certain things which simply cannot be
assumed for the L-languages. For example, there is
no reasonable ground to assume that there will be
(in the near future) corpus even with shallow levels
of annotation for Avadhi or Dogri or Konkani, let
alone a treebank like resource. Therefore, we have
to look for methods which can work with unanno-
tated corpus. Moreover, these methods should also
not require a lot of work from trained linguists be-
cause such linguists may not be available to work on
these languages. There is one approach, however,
that can still allow us to build resources and tools
for these languages. This is the approach of adapt-
ing the resources of a linguistically close but more
privileged language. It is this area which needs to
be studied and explored more thoroughly because it
seems to be the only practical way to make the kind
of progress that is required urgently. The process
of resource adaptation will have to studied from lin-
guistic, computational, and other practical points of
view. Since ?other privileges? are a major factor as
discussed earlier, some ways of calculating the cost
of adaptation have also to be found.
Another very general but important point is that
we will have to build multilingual systems as far
as possible so that the cost per language is reduced.
This will require innovation in terms of modeling as
well as engineering.
6 Some Comments about the Workshop
The scope of the workshop included topics such as
the following:
? Archiving and creation of interoperable data
and metadata for less privileged languages
? Support for less privileged language on com-
puters. This includes input methods, dis-
play, fonts, encoding converters, spell check-
ers, more linguistically aware text editors etc.
? Basic NLP tools such as sentence marker, tok-
enizer, morphological analyzer, transliteration
tools, language and encoding identifiers etc.
? Advanced NLP tools such as POS taggers, local
word grouper, approximate string search, tools
for developing language resources.
There were a relatively large number of submis-
sions to the workshop and the overall quality was
at least above average. The most noteworthy fact is
that the variety of papers submitted (and selected)
was pleasantly surprising. The workshop includes
paper on topics as diverse as Machine Translation
(MT) from text to sign language (an L-language on
which very few people have worked) to MT from
speech to speech. And from segmentation and stem-
ming to parser adaptation. Also, from input meth-
ods, text editor and interfaces to part of speech
(POS) tagger. The variety is also remarkable in
terms of the languages covered and research loca-
tions.
In addition, the workshop includes three invited
talks: the first on building language resources by re-
source adaptation (David and Maxwell, 2008); the
second on cross-language resource sharing (Sorn-
lertlamvanich, 2008b); and the third on breaking the
Zipfian barrier in NLP (Choudhury, 2008). It can
be said that the workshop has been a moderate suc-
cess. We hope it will stimulate further work in this
direction.
10
7 An Overview of the Papers
We noted above that resource adaptation needs a
lot more study. In one of the papers at the work-
shop, Zeman and Resnik presented their work on
cross-language parser adaptation between related
languages, which can be highly relevant for the
L-languages in ?linguistic areas? (Emeneau, 1956;
Emeneau, 1980). Maxwell and David suggest a
better way to weave together a descriptive gram-
mar with a formal grammar through collaboration
between linguists and computer scientists. Alegria
et al discuss the strategies for sustainable MT
for Basque. They suggest that the main elements
of such a strategy should be incremental design,
reusability, standardization and open source devel-
opment.
Among the papers which focus more on comput-
erization and building of tools, Sornlertlamvanich
et al present a ubiquitous system called KUI for
collective intelligence development. Goonetilleke et
al. describe a predictive text input system called
SriShell Primo for Sinhala language. Veeraragha-
van and Roy describe a text editor and a framework
for working with Indic scripts. Aggarwal and Dave
present an implementation of a speech recognition
system interface for Indian languages.
Riza presents brief overview of the literature on
language endangerment, with focus on the Indone-
sian languages. Some other papers focused more
on linguistic study as applied for computational pur-
poses. Among them, Ali et al investigate the opti-
mal order of factors for the computational treatment
of personal anaphoric devices in Urdu discourse.
Muhirwe and Trosterud discuss finite state solutions
for reduplication in Kinyarwanda language. Maung
Maung and Mikami describe a rule-based syllable
segmentation of Myanmar text. In another paper
on a related domain, Sarkar and Bandyopadhyay
present a design of a rule-based stemmer for natu-
ral language text in Bengali.
Among the papers focusing more on NLP, Das-
gupta et al present a prototype machine translation
system from text to Indian Sign Language (ISL). In
another paper on MT, Ellis et al describe an Finnish
to English speech to speech machine translation sys-
tem that they have currently tried with some success
on the Bible. Doren and Bandyopadhyay present a
morphology driven Manipuri POS tagger. Another
paper on POS tagging is by Patel and Gali. They
have tried to build a tagger for Gujarati.
8 Conclusion
We discussed the problem of the lack of linguis-
tic study, language resources, NLP tools for some
languages, which we called the L-languages since
they less of something. We argued that the ?other
privileges? form another dimension of the problem
and are a crucial factor in deciding what methods
we should use to solve this problem. The techni-
cal has to take into account this non-technical factor.
We suggested that resource adaptation may be one
to move forward. Finally we made some comments
about the NLPLPL-08 workshop.
9 Acknowledgment
We would specially like to thank Samar Husain and
Harshit Surana (Language Technologies Research
Centre, IIIT, Hyderabad, India) for providing vital
help in organizing this workshop.
References
Rajesh Kumar Aggarwal and Mayank Dave. 2008. Im-
plementing a speech recognition system interface for
indian languages. In Proceedings of the IJCNLP
Workshop on NLP for Less Privileged Languages, Hy-
derabad, India.
I Alegria, Xabier Arregi, Xabier Artola, Arantza Diaz
de Ilarraza, Gorka Labaka, Mikel Lersundi, Aingeru
Mayor, and Kepa Sarasola. 2008. Strategies for
sustainable mt for basque: incremental design, reus-
ability, standardization and open-source. In Proceed-
ings of the IJCNLP Workshop on NLP for Less Privi-
leged Languages, Hyderabad, India.
Mohammad Naveed Ali, Muhammad Abid Khan, and
Muhammad Aamir Khan. 2008. An optimal order
of factors for the computational treatment of personal
anaphoric devices in urdu discourse. In Proceedings
of the IJCNLP Workshop on NLP for Less Privileged
Languages, Hyderabad, India.
Monojit Choudhury. 2008. Breaking the zipfian barrier
of nlp. Invited Talk at the IJCNLP Workshop on NLP
for Less Privileged Languages. Hyderabad, India.
Tirthankar Dasgupta, Sandipan Dandapat, and Anupam
Basu. 2008. Prototype machine translation system
from text-to-indian sign language. In Proceedings
11
of the IJCNLP Workshop on NLP for Less Privileged
Languages, Hyderabad, India.
Anne David and Michael Maxwell. 2008. Building lan-
guage resources: Ways to move forward. Invited Talk
at the IJCNLP Workshop on NLP for Less Privileged
Languages, 2008. Hyderabad, India.
Timo Honkela David Ellis, Mathias Creutz and Mikko
Kurimo. 2008. Speech to speech machine translation:
Biblical chatter from finnish to english. In Proceed-
ings of the IJCNLP Workshop on NLP for Less Privi-
leged Languages, Hyderabad, India.
M. B. Emeneau. 1956. India as a linguistic area. Lin-
guistics, 32:3-16.
M. B. Emeneau. 1980. Language and linguistic area. Es-
says by Murray B. Emeneau. Selected and introduced
by Anwar S. Dil. Stanford University Press.
Sandeva Goonetilleke, Yoshihiko Hayashi, Yuichi Itoh,
and Fumio Kishino. 2008. Srishell primo: A predic-
tive sinhala text input system. In Proceedings of the
IJCNLP Workshop on NLP for Less Privileged Lan-
guages, Hyderabad, India.
Zin Maung Maung and Yoshiki Mikami. 2008. A rule-
based syllable segmentation of myanmar text. In Pro-
ceedings of the IJCNLP Workshop on NLP for Less
Privileged Languages, Hyderabad, India.
Michael Maxwell and Anne David. 2008. Joint grammar
development by linguists and computer scientists. In
Proceedings of the IJCNLP Workshop on NLP for Less
Privileged Languages, Hyderabad, India.
Rada Mihalcea. 2008. How to add a new language on the
nlp map: Building resources and tools for languages
with scarce resources. Tutorial at the Third Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP). Hyderabad, India.
Jackson Muhirwe and Trond Trosterud. 2008. Finite
state solutions for reduplication in kinyarwanda lan-
guage. In Proceedings of the IJCNLP Workshop on
NLP for Less Privileged Languages, Hyderabad, In-
dia.
Chirag Patel and Karthik Gali. 2008. Part of speech tag-
ger for gujarati using conditional random fields. In
Proceedings of the IJCNLP Workshop on NLP for Less
Privileged Languages, Hyderabad, India.
Hammam Riza. 2008. Indigenous languages of indone-
sia: Creating language resources for language preser-
vation. In Proceedings of the IJCNLP Workshop on
NLP for Less Privileged Languages, Hyderabad, In-
dia.
Sandipan Sarkar and Sivaji Bandyopadhyay. 2008. De-
sign of a rule-based stemmer for natural language text
in bengali. In Proceedings of the IJCNLP Workshop
on NLP for Less Privileged Languages, Hyderabad,
India.
Thoudam Doren Singh and Sivaji Bandyopadhyay. 2008.
Morphology driven manipuri pos tagger. In Proceed-
ings of the IJCNLP Workshop on NLP for Less Privi-
leged Languages, Hyderabad, India.
Virach Sornlertlamvanich, Thatsanee Charoenporn,
Kergrit Robkop, and Hitoshi Isahara. 2008a. Kui:
an ubiquitous tool for collective intelligence devel-
opment. In Proceedings of the IJCNLP Workshop
on NLP for Less Privileged Languages, Hyderabad,
India.
Virach Sornlertlamvanich. 2008b. Cross language re-
source sharing. Invited Talk at the IJCNLP Workshop
on NLP for Less Privileged Languages, 2008. Hyder-
abad, India.
Krishnakumar Veeraraghavan and Indrani Roy. 2008.
Acharya - a text editor and framework for working
with indic scripts. In Proceedings of the IJCNLP
Workshop on NLP for Less Privileged Languages, Hy-
derabad, India.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of the IJCNLP Workshop on NLP for Less
Privileged Languages, Hyderabad, India.
12
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 5?16,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Named Entity Recognition for South and South East Asian Languages:
Taking Stock
Anil Kumar Singh
Language Technologies Research Centre
IIIT, Hyderabad, India
anil@research.iiit.ac.in
Abstract
In this paper we first present a brief discus-
sion of the problem of Named Entity Recog-
nition (NER) in the context of the IJCNLP
workshop on NER for South and South East
Asian (SSEA) languages1 . We also present a
short report on the development of a named
entity annotated corpus in five South Asian
language, namely Hindi, Bengali, Telugu,
Oriya and Urdu. We present some details
about a new named entity tagset used for this
corpus and describe the annotation guide-
lines. Since the corpus was used for a shared
task, we also explain the evaluation mea-
sures used for the task. We then present
the results of our experiments on a baseline
which uses a maximum entropy based ap-
proach. Finally, we give an overview of the
papers to be presented at the workshop, in-
cluding those from the shared task track. We
discuss the results obtained by teams partic-
ipating in the task and compare their results
with the baseline results.
1 Introduction
One of the motivations for organizing a workshop
(NERSSEAL-08) focused on named entities (NEs)
was that they have a special status in Natural Lan-
guage Processing (NLP) because they have some
properties which other elements of human languages
do not have, e.g. they refer to specific things or con-
cepts in the world and are not listed in the grammars
1http://ltrc.iiit.ac.in/ner-ssea-08
or the lexicons. Identifying and classifying them au-
tomatically can help us in processing text because
they form a significant portion of the types and to-
kens occurring in a corpus. Also, because of their
very nature, machine learning techniques have been
found to be very useful in identifying them. In order
to use these machine learning techniques, we need
corpus annotated with named entities. In this paper
we describe such a corpus developed for five South
Asian languages. These languages are Hindi, Ben-
gali, Oriya, Telugu and Urdu.
This paper also presents an overview of the work
done for the IJCNLP workshop on NER for SSEA
languages. The workshop included two tracks. The
first track was for regular research papers, while the
second was organized on the lines of a shared task.
Fairly mature named entity recognition systems
are now available for European languages (Sang,
2002; Sang and De Meulder, 2003), especially En-
glish, and even for East Asian languages (Sassano
and Utsuro, 2000). However, for South and South
East Asian languages, the problem of NER is still far
from being solved. Even though we can gain much
insight from the methods used for English, there are
many issues which make the nature of the problem
different for SSEA languages. For example, these
languages do not have capitalization, which is a ma-
jor feature used by NER systems for European lan-
guages.
Another characteristic of these languages is that
most of them use scripts of Brahmi origin, which
have highly phonetic characteristics that could be
utilized for multilingual NER. For some languages,
there are additional issues like word segmentation
5
(e.g. for Thai). Large gazetteers are not avail-
able for most of these languages. There is also
the problem of lack of standardization and spelling
variation. The number of frequently used words
(common nouns) which can also be used as names
(proper nouns) is very large for, unlike for Euro-
pean languages where a larger proportion of the first
names are not used as common words. For exam-
ple, ?Smith?, ?John?, ?Thomas? and ?George? etc. are
almost always used as person names, but ?Anand?,
?Vijay?, ?Kiran? and even ?Manmohan? can be (more
than often) used as common nouns. And the fre-
quency with which they can be used as common
nouns as against person names is more or less unpre-
dictable. The context might help in disambiguating,
but this issue does make the problem much harder
than for English.
Among other problems, one example is that of the
various ways of representing abbreviations. Because
of the alpha-syllabic nature of the SSEA scripts, ab-
breviation can be expressed through a sequence of
letters or syllables. In the latter case, the syllables
are often combined together to form a pseudo-word,
e.g. BAjapA (bhaajapaa) for Bharatiya Janata Party
or BJP.
But most importantly, there is a serious lack of
labeled data for machine learning. As part of this
workshop, we have tried to prepare some data but we
will need much more data for really accurate NER
systems.
Since most of the South and South East Asian lan-
guages are scarce in resources as well as tools, it is
very important that good systems for NER be avail-
able, because many problems in information extrac-
tion and machine translation (among others) are de-
pendent on accurate NER.
The need for a workshop specifically for SSEA
languages was felt because the South and South East
Asian region has many major and numerous minor
languages. In terms of the number of speakers there
are at least four in any list of top ten languages of the
world. For practical reasons, we focus only on the
major languages in the workshop (and in this paper).
Most of the major languages belong to two families:
Indo-European and Dravidian. There are a lot of dif-
ferences among these languages, but there are a lot
of similarities too, even across families (Emeneau,
1956; Emeneau, 1980). For the reasons mentioned
above, NER is perhaps more difficult for SSEA lan-
guages than for European languages. For better or
for worse, there too many languages and too few re-
sources. Moreover, these languages are also com-
paratively less studied by researchers. However, we
can benefit from the similarities across these lan-
guages to build multilingual systems so as to reduce
the overall cost and effort required.
All the issues mentioned above show that we
might need different methods for solving the NER
problem for SSEA languages. However, for com-
paring the results of these different methods, we will
need a reasonably good baseline. A mature system
tuned for English but trained on SSEA language data
can become such a baseline. We will describe such a
baseline in a later section. This baseline system has
been tested on the data provided for the shared task.
We present the results for all five languages under
the settings required for the shared task.
2 Related Work
Various techniques have been used for solving the
NER problem (Mikheev et al, 1999; Borthwick,
1999; Cucerzan and Yarowsky, 1999; Chieu and Ng,
2003; Klein et al, 2003; Kim and Woodland, 2000)
ranging from naively using gazetteers to rules based
techniques to purely statistical techniques, even hy-
brid approaches. Several workshops consisting of
shared tasks (Sang, 2002; Sang and De Meulder,
2003) have been held with specific focus on this
problem. In this section we will mention some of
techniques used previously.
Most of the approaches can be classified based on
the features they use, whether they are rule based or
machine learning based or hybrid approaches. Some
of the commonly used features are:
? Word form and part of speech (POS) tags
? Orthographic features like capitalization, deci-
mal, digits
? Word type patterns
? Conjunction of types like capitalization,
quotes, functional words etc.
? Bag of words
? Trigger words like New York City
6
Tag Name Description
NEP Person Bob Dylan, Mohandas Gandhi
NED Designation General Manager, Commissioner
NEO Organization Municipal Corporation
NEA Abbreviation NLP, B.J.P.
NEB Brand Pepsi, Nike (ambiguous)
NETP Title-Person Mahatma, Dr., Mr.
NETO Title-Object Pride and Prejudice, Othello
NEL Location New Delhi, Paris
NETI Time 3rd September, 1991 (ambiguous)
NEN Number 3.14, 4,500
NEM Measure Rs. 4,500, 5 kg
NETE Terms Maximum Entropy, Archeology
Table 1: The named entity tagset used for the shared task
? Affixes like Hyderabad, Rampur,
Mehdipatnam, Lingampally
? Gazetteer features: class in the gazetteer
? Left and right context
? Token length, e.g. the number of letters in a
word
? Previous history in the document or the corpus
? Classes of preceding NEs
The machine learning techniques tried for NER
include the following:
? Hidden Markov Models or HMM (Zhou and
Su, 2001)
? Decision Trees (Isozaki, 2001)
? Maximum Entropy (Borthwick et al, 1998)
? Support Vector Machines or SVM (Takeuchi
and Collier, 2002)
? Conditional Random Fields or CRF (Settles,
2004)
Different ways of classifying named entities have
been used, i.e., there are more than one tagsets for
NER. For example, the CoNLL 2003 shared task2
had only four tags: persons, locations, organizations
2http://www.cnts.ua.ac.be/conll2003/ner/
and miscellaneous. On the other hand, MUC-63 has
a near ontology for information extraction purposes.
In this (MUC-6) tagset, there are three4 main kinds
of NEs: ENAMEX (persons, locations and organi-
zations), TIMES (time expressions) and NUMEX
(number expresssions).
There has been some previous work on NER
for SSEA languages (McCallum and Li, 2003;
Cucerzan and Yarowsky, 1999), but most of the time
such work was an offshoot of the work done for Eu-
ropean languages. Even including the current work-
shop, the work on NER for SSEA languages is still
in the initial stages as the results reported by papers
in this workshop clearly show.
3 A New Named Entity Tagset
The tagset being used for the NERSSEAL-08 shared
task consists of more tags than the four tags used
for the CoNLL 2003 shared task. The reason we
opted for these tags was that we needed a slightly
finer tagset for machine translation (MT). The ini-
tial aim was to improve the performance of the MT
system.
As annotation progressed, we realized that there
were some problems that we had not anticipated.
Some classes were hard to distinguish in some con-
texts, making the task hard for annotators and bring-
ing in inconsistencies. For example, it was not al-
ways clear whether something should be marked as
3http://cs.nyu.edu/cs/faculty/grishman/muc6.html
4http://cs.nyu.edu/cs/faculty/grishman/NEtask20.book 6.html
7
Number or as Measure. Similarly for Time and Mea-
sure. Another difficult class was that of (technical)
terms. Is ?agriculture? a term or not? If no (as most
people would say), is ?horticulture? a term or not? In
fact, Term was the most difficult class to mark.
An option that we explored was to merge the
above mentioned confusable classes and ignore the
Term class. But we already had a relatively large
corpus marked up with these classes. If we merged
some classes and ignored the Term class (which had
a very large coverage and is definitely going to be
useful for MT), we would be throwing away a lot
of information. And we also had some corpus an-
notated by others which was based on a different
tagset. So some problems were inevitable. Finally,
we decided to keep the original tagset, with one
modification. The initial tagset had only eleven tags.
The problem was that there was one Title tag but
it had two different meanings: ?Mr.? is a Title, but
?The Seven Year Itch? is also a Title. This tag clearly
needed to be split into two: Title-Person and Title-
Object
We should mention here that we considered using
another tagset developed at AUKBC, Chennai. This
was based on ENAMEX, TIMEX and NUMEX. The
total number of tags in this tagset is more than a hun-
dred and it is meant specifically for MT and only for
certain domains (health, tourism). Moreover, this is
a tagset for entities in general, not just named enti-
ties.
The twelve tags in our tagset are briefly explained
in Table-1. In the next section we mention the con-
straints under which the annotated corpus was cre-
ated, using this tagset.
4 Annotation Constraints
The annotated corpus was created under severe con-
straints. The annotation was to be for five languages
by different teams, sometimes with very little com-
munication during the process of annotation. As a
result, there were many logistical problems.
There were other practical constraints like the fact
that this was not a funded project and all the work
was mainly voluntary. Another major constraint for
all the languages except Hindi was time. There was
not enough time for cross validation as the corpus
was required by a deadline. To keep annotation rea-
sonably consistent, annotation guidelines were cre-
ated and a common format was specified.
5 Annotation Guidelines
The annotation guidelines were of two kinds. One
was meant for preparing training data through man-
ual annotation. The other one was meant for prepar-
ing reference data as well as for automatic annota-
tion. The main guidelines for preparing the training
data are as follows:
? Specificity: The most important criterion while
deciding whether some expression is a named
entity or not is to see whether that expression
specifies something definite and identifiable as
if by a name or not. This decision will have to
be based on the context. For example, ?aanand?
(in South Asian languages, where there is no
capitalization) is not a named entity in ?saba
aanand hii aanand hai? (?There is bliss every-
where?). But it is a named entity in ?aanand
kaa yaha aakhiri saala hai? (?Anand is in the
last year (of his studies)?). Number, Measure
and Term may be seen as exceptions (see be-
low).
? Maximal Entity: Only the maximal entities
have to be annotated for training data. Struc-
ture of entities will not be annotated by the
annotators, even though it has to be learnt by
the NER systems. For example, ?One Hundred
Years of Solitude? has to be annotated as one
entity. ?One Hundred? is not to be marked as
a Number here, nor is ?One Hundred Years? to
be made marked as a Measure in this case. The
purpose of this guideline is to make the task of
annotation for several languages feasible, given
the constraints.
? Ambiguity: In cases where an entity can have
two valid tags, the more appropriate one is to
be used. The annotator has to make the deci-
sion in such cases. It is recommended that the
annotation be validated by another person, or
even more preferably, two different annotators
have to work on the same data independently
and inconsistencies have to be resolved by an
adjudicator. Abbreviation is an exception to the
Ambiguity guideline (see below).
8
Some other guidelines for specific tags are listed
below:
? Abbreviations: All abbreviations have to be
marked as Abbreviations, Even though every
abbreviation is also some other kind of named
entity. For example, APJ is an Abbreviation,
but also a Person. IBM is also an Organiza-
tion. Such ambiguity cannot be resolved from
the context because it is due to the (wrong?)
assumption that a named entity can have only
one tag. Multiple annotations were not al-
lowed. This is an exception to the third guide-
line above.
? Designation and Title-Person: An entity is a
Designation if it represents something formal
and official status with certain responsibilities.
If it is just something honorary, then it is a
Title-Object. For example, ?Event Coordina-
tor? or ?Research Assistant? is a Designation,
but ?Chakravarti? or ?Mahatma? are Titles.
? Organization and Brand: The distinction be-
tween these two has to be made based on the
context. For example, ?Pepsi? could mean an
Organization, but it is more likely to mean a
Brand.
? Time and Location: Whether something is to
be marked as Time or Location or not is to be
decided based on the Specificity guideline and
the context.
? Number, Measure and Term: These three may
not be strictly named entities in the way a per-
son name is. However, we have included them
because they are different from other words of
the language. For problems like machine trans-
lation, they can be treated like named entities.
For example, a Term is a word which can be di-
rectly translated into some language if we have
a dictionary of technical terms. Once we know
a word is a Term, there is likely to be less am-
biguity about the intended sense of the word,
unlike for other normal words.
The second set of guidelines are different from the
first set mainly in one respect: the corpus has to be
annotated with not just the maximal NEs, but with
all levels of NEs, i.e., nested NEs also have to be
marked.
Nested entities were introduced because one of
the requirements was that the corpus be useful for
building systems which can become parts of a ma-
chine translation (MT) system. Nested entities can
be useful for MT systems because, quite often, parts
of the entities can need to be translated, while the
others can just be transliterated. An example of a
nested named entity is ?Mahatma Gandhi Interna-
tional Hindi University?. This would be translated
in Hindi as mahaatmaa gaandhii antarraashtriya
hindii vishvavidyaalaya. Only ?International? and
?University? are to be translated, while the other
words are to be transliterated. The nested named en-
tities in this case are: ?Mahatma? (NETO), ?Gandhi?
(NEP), ?Mahatma Gandhi? (NEP), and ?Mahatma
Gandhi International Hindi University? (NEO).
6 Named Entity Annotated Corpus
For Hindi, Oriya and Telugu, all the annotation was
performed at IIIT, Hyderabad. For Bengali, the cor-
pus was developed at IIIT, Hyderabad and Jadavpur
University (Ekbal and Bandyopadhyay, 2008b), Cal-
cutta. For Urdu, annotation was performed at
CRULP, Lahore (Hussain, 2008) and IIIT, Allahabd.
Even though all the annotation was done by native
speakers of respective languages, named entity an-
notation was a new task for everyone involved. This
was because of practical constraints as explained in
an earlier section.
The corpus was divided into two parts, one for
training and one for testing. The testing corpus
was annotated with nested named entities, while the
training corpus was only annotated with ?maximal?
named entities.
Since different teams were working on different
languages, in some cases even the same language,
and also because most of the corpus was created on
short notice, each team made its own decisions re-
garding the kind of corpus to be annotated. As a re-
sult, the characteristics of the corpus differ widely
among the five languages. The Hindi and Ben-
gali (partly) text that was annotated was from the
multilingual comparable corpus known as the CIIL
(Central Institute of Indian Languages) corpus. The
Oriya corpus was part of the Gyan Nidhi corpus.
9
NE Hindi Bengali Oriya Telugu Urdu
Trn Tst Trn Tst Trn Tst Trn Tst Trn Tst
NEP 4025 199 1299 728 2079 698 1757 330 365 145
NED 935 61 185 11 67 216 87 77 98 41
NEO 1225 44 264 20 87 200 86 12 155 40
NEA 345 7 111 9 8 20 97 112 39 3
NEB 5 0 22 0 11 1 1 6 9 18
NETP 1 5 68 57 54 201 103 2 36 15
NETO 964 88 204 46 37 28 276 118 4 147
NEL 4089 211 634 202 525 564 258 751 1118 468
NETI 1760 50 285 46 102 122 244 982 279 59
NEN 6116 497 407 144 124 232 1444 391 310 47
NEM 1287 17 352 146 280 139 315 53 140 40
NETE 5658 843 1165 314 5 0 3498 138 30 4
NEs 26432 2022 5000 1723 3381 2421 8178 3153 2584 1027
Words 503179 32796 112845 38708 93173 27007 64026 8006 35447 12805
Sentences 19998 2242 6030 1835 1801 452 5285 337 1508 498
Trn: Training Data, Tst: Testing Data
Table 2: Statistics about the corpus: counts of various named entity classes and the size of the corpus as the
number of words and the number of sentences. Note that the values for the testing part are of nested NEs.
Also, the number of sentences, especially in the case or Oriya is not accurate because the sentences were not
correctly segmented as there was no automatic sentence splitter available for these languages and manual
splitting would have been too costly: without much benefit for the NER task.
Both of these (CIIL and Gyan Nidhi) corpora con-
sist of text from educational books written on vari-
ous topics for common readers. The Urdu text was
partly news corpus. The same was the case with Tel-
ugu, but the text for both these languages included
text from other domains too.
Admittedly, the texts selected for annotation were
not the ideal ones. For example, many documents
had very few named entities. Also, the distribution
of domains as well as the classes of NEs was not
representative. The size of the annotated corpora
for different languages is also widely varying, with
Hindi having the largest corpus and Urdu the small-
est. However, this corpus is hopefully just a starting
point for much more work in the near future.
Some statistics about the annotated corpus are
given in Table-2.
7 Shared Task
In the shared task, the contestants having their own
NER systems were given some annotated test data.
The contestants had the freedom to use any tech-
nique for NER, e.g. a purely rule based technique
or a purely statistical technique.
The contestants could build NER systems targeted
for a specific language, but they were required to re-
port results for their systems on all the languages
for which training data had been provided. This
condition was meant to provide a somewhat fair
ground for comparison of systems, since the amount
of training data is different for different languages.
The data released for the shared task has been
made accessible to all for non-profit research word,
not just for the shared task participants, with the
hope others will contribute in improving this data
and adding to it.
The task in this contest was different in one im-
portant way. The NER systems also had to identify
nested named entities. For example, in the sentence
The Lal Bahadur Shastri National Academy of Ad-
ministration is located in Mussoorie, ?Lal Bahadur
Shastri? is a Person, but ?Lal Bahadur Shastri Na-
tional Academy of Administration? is an Organiza-
tion. In this case, the NER systems had to identify
both ?Person? and ?Organization? in the given sen-
tence.
An evaluation script was also provided to evaluate
the performance of different systems in a uniform
way.
10
8 Evaluation Measures
As part of the evaluation process for the shared task,
precision, recall and F-measure had to be calcu-
lated for three cases: maximal named entities, nested
named entities and lexical matches. Thus, there
were nine measures of performance:
? Maximal Precision: Pm = cmrm
? Maximal Recall: Rm = cmtm
? Maximal F-Measure: Fm = 2?Pm?RmPm+Rm
? Nested Precision: Pn = cnrn
? Nested Recall: Rn = cntn
? Nested F-Measure: Fn = 2PnRnPn+Rn
? Lexical Precision: Pl = clrl
? Lexical Recall: Rl = cltl
? Lexical F-Measure: Fl = 2PlRlPl+Rl
where c is the number of correctly retrieved (iden-
tified) named entities, r is the total number of named
entities retrieved by the system being evaluated (cor-
rect plus incorrect) and t is the total number of
named entities in the reference data.
The participants were encouraged to report results
for specific classes of NEs. Evaluation was auto-
matic and was against the manually prepared refer-
ence data given to the participants. An evaluation
script for this purpose was also provided. This script
assumes that there are single test and reference file
and the number and order of sentences is the same in
both. The format accepted by the evaluation script
(which was also the format used for annotated data)
was explained in an online tutorial5.
9 Experiments on a Baseline
For our baseline experiments, we used an open
source implementation of maximum entropy based
Natural Languages Processing tools which are part
of the OpenNLP6 package. This package includes a
name finder tool.
5http://ltrc.iiit.ac.in/ner-ssea-08/NER-SAL-TUT.pdf
6http://opennlp.sourceforge.net/
This name finder was trained for all the twelve
classes of NEs and for all the five languages. The
test data, which was the same as that given to the
shared task participants, was run through this name
finder. Note that this NER tool is tuned for En-
glish in terms of the features used, even though it
was trained on different SSEA languages in our case.
Since the goal of the shared task was to encourage
investigation of techniques (especially features) spe-
cific to the SSEA languages, this fairly mature NER
system (for English) could be used as a baseline
against which to evaluate systems tuned (or specially
designed) for the five South Asian languages.
The overall results of the baseline experiments are
shown in Table-3. The performance on specific NE
classes is given in Table-4. It can be seen from the
tables that the results are drastically low in compar-
ison to the state of the art results reported for En-
glish. These results clearly show that even a ma-
chine learning based system cannot be directly used
for SSEA languages even when it has been trained
with annotated data for these languages.
In the next section we present a brief overview of
the papers selected for the workshop including the
shared task papers.
10 An Overview of the Papers
In all, twelve papers were selected for the workshop,
out of which four were in the shared task track. Saha
et al, who were able to achieve the best results in
the shared task, describe a hybrid system that ap-
plies maximum entropy models, language specific
rules, and gazetteers. For Hindi, the features they
utilized include orthographic features, information
about suffixes and prefixes, morphological features,
part of speech information, and information about
the surrounding words. They used rules for num-
bers, measures and time classes. For designation,
title-person and some terms (NETE), they built lists
or gazetteers. They also used gazetteers for person
and location. They did not use rules or gazetteers for
Oriya, Urdu and Telugu. To identify some kinds of
nested entities, they applied a set of rules.
Gali et al also combined machine learning with
language specific heuristics. In a separate section,
they discussed at some length the issues relevant to
NER for SSEA languages. Some of these have al-
11
Measure ? Precision Recall F-Measure
Language ? Pm Pn Pl Rm Rn Rl Fm Fn Fl
Bengali 50.00 44.90 52.20 07.14 06.90 06.97 12.50 11.97 12.30
Hindi 75.05 73.61 73.99 18.16 17.66 15.53 29.24 28.48 25.68
Oriya 29.63 27.46 48.25 09.11 07.60 12.18 13.94 11.91 19.44
Telugu 00.89 02.83 22.85 00.20 00.67 5.41 00.32 01.08 08.75
Urdu 47.14 43.50 51.72 18.35 16.94 18.94 26.41 24.39 27.73
m: Maximal, n: Nested, l: Lexical
Table 3: Results for the experiments on a baseline for the five South Asian languages
Bengali Hindi Oriya Telugu Urdu
NEP 06.62 26.23 28.48 00.00 04.39
NED 00.00 12.20 00.00 00.00 00.00
NEO 00.00 15.50 03.30 00.00 11.98
NEA 00.00 00.00 00.00 00.00 00.00
NEB NP NP 00.00 00.00 00.00
NETP 00.00 NP 11.62 00.00 00.00
NETO 00.00 05.92 04.08 00.00 00.00
NEL 03.03 44.79 25.49 00.00 40.21
NETI 34.00 47.41 22.38 01.51 38.38
NEN 62.63 62.22 10.65 03.51 09.52
NEM 13.61 24.39 08.03 00.71 07.15
NETE 00.00 00.18 00.00 00.00 00.00
NP: Not present in the reference data
Table 4: Baseline results for specific named entity classes (F-Measures for nested lexical match)
ready been mentioned, but two others are the ag-
glutinative property of these (especially Dravidian)
languages and the low accuracy of available part of
speech taggers, particularly for nouns. They used
a Conditional Random Fields (CRF) based method
for machine learning and applied heuristics to take
care of the language specific issues. They also point
out that a very high percentage of NEs in the Hindi
corpus were marked as NETE and machine learning
failed to take care of this class of NEs. This has been
validated by our results on the baseline too (Table-
4) and is understandable because terms are hard to
identify even for humans.
Ekbal et al also used an approach based on CRFs.
They also used some language specific features for
Hindi and Bengali. Srikanth and Murthy describe
the results of their experiments on NER using CRFs
for Telugu. They concentrated only on person, place
and organization names and used newspaper text
as the corpus. In this focused setting, they were
able to achieve overall F-measures between 80% and
97% in various experiments. Chaudhuri and Bhat-
tacharya also experimented on a news corpus for
Bengali using a three stage NER system. The three
stages were based on an NE dictionary, rules and
contextual co-occurrence statistics. They only tried
to identify the NEs, not classify them. For this task,
they were able to achieve an overall F-measure of
89.51%.
Praveen and Ravi Kumar present the results of
experiments (as part of the shared task) using two
approaches: Hidden Markov Models (HMM) and
CRF. Surprisingly, they obtained better results with
HMM for all the five languages. Goyal described ex-
periments using a CRF based model. He also used
part of speech information. He experimented only
on Hindi and was able to achieve results above 60%.
One notable fact about this paper is that it also de-
12
Language ? BL IK IH1 IH2 JU
Bengali 12.30 65.96 40.63 39.77 59.39
Hindi 25.68 65.13 50.06 46.84 33.12
Oriya 19.44 44.65 39.04 45.84 28.71
Telugu 08.75 18.74 40.94 46.58 04.75
Urdu 27.73 35.47 43.46 44.73 35.52
Average 18.78 45.99 42.83 44.75 32.30
BL: Baseline, IK: IIT Kharagpur
JU: Jadavpur University, Calcutta
IH1: Karthik et al, IIIT Hyderabad
IH2: Praveen and Ravi Kiran, IIIT Hyderabad
Table 5: Comparison of NER systems which participated in the NERSSEAL-08 shared task against a base-
line that uses maximum entropy based name finder tuned for English but trained on data from five South
Asian languages
scribes experiments on the CoNLL 2003 shared task
data for English, which shows that the significantly
higher results for English are mainly due to the fact
that the CoNLL 2003 data is already POS tagged and
chunked with high accuracy. Goyal was also able to
show that capitalization is a major clue for English,
either directly or indirectly (e.g., for accurate POS
tagging and chunking). He also indicated that the
characteristics of the Hindi annotated corpus were
partly responsible for the low results on Hindi.
Nayan et al mainly describe how an NER system
can benefit from approximate string matching based
on phonetic edit distance, both for a single language
(to account for spelling variations) and for cross-
lingual NER. Shishtla et al (?Experiments in Tel-
ugu NER?) experimented only on Telugu and used
the CoNLL shared task tagset. Using a CRF based
approach, they were able to achieve an F-measure
of 44.91%. Ekbal and Bandyopadhyay describe a
method based on Support Vector Machines (SVMs)
for Bengali NER. On a news corpus and with sixteen
NE classes, they were able to achieve an F-measure
of 91.8%. Vijayakrishna and Sobha describe a CRF
based system for Tamil using 106 NE classes. Their
system is a multi-level system which gave an over-
all F-measure of 80.44%. They also mention that
their system achieved this level of performance on a
domain focused corpus. Shishtla et al (?Character
n-gram Based Approach?) used a character n-gram
based method to identify NEs. They experimented
on Hindi as well as English and achieved F-measure
values up to 45.48% for Hindi and 68.46% for En-
glish.
Apart from the paper presentations, the workshop
will also have two invited talks. The first one is titled
?Named Entity Recognition: Different Approaches?
by Sobha L. and the second one is ?Multilingual
Named Entity Recognition? by Sivaji Bandyopad-
hyay.
11 Shared Task Results
Five teams participated in the shared task. However,
only four submitted papers for the workshop. All the
teams tried to combine machine learning with some
language specific heuristics, at least for one of the
languages. The results obtained by the four teams
are summarized in Table-5, which shows only the F-
measure for lexical match. It can be seen from the
table that all the teams were able to get significantly
better results than the baseline. Overall, the perfor-
mance of the IIT Kharagpur team was the best, fol-
lowed by the two teams from IIIT Hyderabad.
Even though all the teams obtained results much
better than the baseline, it is still quite evident that
the state of the art for NER for SSEA languages
leaves much to be desired. At around 46% max-
imum F-measure on lexical matching, the results
mean that the NER systems built so far for SSEA
languages are not quite practically useful. But, after
this workshop, we at least know where we stand and
how far we still have to go.
However, it may be noted that the conditions for
13
the shared task were very stringent compared to the
previous shared tasks on NER, e.g. neither the cor-
pus was tagged with parts of speech or chunks, nor
were good POS taggers or chunkers available for
the languages involved. This indicates that with
progress in building better resources and basic tools
for these languages, the accuracy of NER systems
should also increase. Already, some very high accu-
racies are being reported under less stringent condi-
tions, e.g. for domain focused NER.
12 Conclusions
We started by discussing the problem of NER for
South and South East Asian languages and the moti-
vations for organizing a workshop on this topic. We
also described a named entity annotated corpus for
five South Asian languages used for this workshop.
We presented some statistics about the corpus and
also the problems we encountered in getting the cor-
pus annotated by teams located in distant places. We
also presented a new named entity tagset that was
developed for annotation of this corpus. Then we
presented the results for our experiments on a rea-
sonable baseline. Finally we gave an overview of
the papers selected for the NERSSEAL-08 work-
shop and discussed the systems described in these
papers and the results obtained, including those for
the shared task which was one of the two tracks in
the workshop.
References
Sivaji Bandyopadhyay. 2008. Invited talk: Multilin-
gual named entity recognition. In Proceedings of the
IJCNLP-08 Workshop on NER for South and South
East Asian Languages, pages 15?17, Hyderabad, In-
dia, January.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 152?160.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York
University.
Bidyut Baran Chaudhuri and Suvankar Bhattacharya.
2008. An experiment on automatic detection of named
entities in bangla. In Proceedings of the IJCNLP-08
Workshop on NER for South and South East Asian
Languages, pages 51?58, Hyderabad, India, January.
H.L. Chieu and H.T. Ng. 2003. Named entity recognition
with a maximum entropy approach. Proceedings of
the seventh conference on Natural language learning
at HLT-NAACL 2003-Volume 4, pages 160?163.
S. Cucerzan and D. Yarowsky. 1999. Language indepen-
dent named entity recognition combining morpholog-
ical and contextual evidence. Proceedings of the Joint
SIGDAT Conference on EMNLP and VLC 1999, pages
90?99.
Asif Ekbal and Sivaji Bandyopadhyay. 2008a. Ben-
gali named entity recognition using support vector ma-
chine. In Proceedings of the IJCNLP-08 Workshop
on NER for South and South East Asian Languages,
pages 85?92, Hyderabad, India, January. Association
for Computational Linguistics.
Asif Ekbal and Sivaji Bandyopadhyay. 2008b. Devel-
opment of bengali named entity tagged corpus and its
use in ner systems. In Proceedings of the Sixth Work-
shop on Asian Language Resources, Hyderabad, India,
January.
Asif Ekbal, Rejwanul Haque, Amitava Das,
Venkateswarlu Poka, and Sivaji Bandyopadhyay.
2008. Language independent named entity recog-
nition in indian languages. In Proceedings of the
IJCNLP-08 Workshop on NER for South and South
East Asian Languages, pages 33?40, Hyderabad,
India, January.
M. B. Emeneau. 1956. India as a linguistic area. Lin-
guistics, 32:3-16.
M. B. Emeneau. 1980. Language and linguistic area. Es-
says by Murray B. Emeneau. Selected and introduced
by Anwar S. Dil. Stanford University Press.
Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth
Shishtla, and Dipti Misra Sharma. 2008. Aggregating
machine learning and rule based heuristics for named
entity recognition. In Proceedings of the IJCNLP-08
Workshop on NER for South and South East Asian
Languages, pages 25?32, Hyderabad, India, January.
Amit Goyal. 2008. Named entity recognition for south
asian languages. In Proceedings of the IJCNLP-08
Workshop on NER for South and South East Asian
Languages, pages 63?70, Hyderabad, India, January.
Sarmad Hussain. 2008. Resources for urdu language
processing. In Proceedings of the Sixth Workshop
on Asian Language Resources, Hyderabad, India, Jan-
uary.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision tree
14
learning. In Meeting of the Association for Computa-
tional Linguistics, pages 306?313.
J.H. Kim and PC Woodland. 2000. A rule-based named
entity recognition system for speech input. Proc. of
ICSLP, pages 521?524.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning. 2003.
Named entity recognition with character-level models.
Proceedings of CoNLL, 3.
Sobha L. 2008. Invited talk: Named entity recognition:
Different approaches. In Proceedings of the IJCNLP-
08 Workshop on NER for South and South East Asian
Languages, pages 13?14, Hyderabad, India, January.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. Seventh
Conference on Natural Language Learning (CoNLL).
A. Mikheev, M. Moens, and C. Grover. 1999. Named
Entity recognition without gazetteers. Proceedings of
the ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 1?8.
Animesh Nayan, B. Ravi Kiran Rao, Pawandeep Singh,
Sudip Sanyal, and Ratna Sanyal. 2008. Named entity
recognition for indian languages. In Proceedings of
the IJCNLP-08 Workshop on NER for South and South
East Asian Languages, pages 71?78, Hyderabad, In-
dia, January. Association for Computational Linguis-
tics.
Praveen P and Ravi Kiran V. 2008. Hybrid named entity
recognition system for south and south east asian lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NER for South and South East Asian Languages,
pages 59?62, Hyderabad, India, January.
Vijayakrishna R and Sobha L. 2008. Domain focused
named entity recognizer for tamil using conditional
random fields. In Proceedings of the IJCNLP-08
Workshop on NER for South and South East Asian
Languages, pages 93?100, Hyderabad, India, January.
Association for Computational Linguistics.
Sujan Kumar Saha, Sanjay Chatterji, Sandipan Dandapat,
Sudeshna Sarkar, and Pabitra Mitra. 2008. A hybrid
named entity recognition system for south and south
east asian languages. In Proceedings of the IJCNLP-
08 Workshop on NER for South and South East Asian
Languages, pages 17?24, Hyderabad, India, January.
E.F.T.K. Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 Shared Task: Language-Independent
Named Entity Recognition. Development, 922:1341.
Erik F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independentnamed entity
recognition. In Proceedings of CoNLL-2002, pages
155?158. Taipei, Taiwan.
Manabu Sassano and Takehito Utsuro. 2000. Named
entity chunking techniques in supervised learning for
japanese named entity recognition. In Proceedings
of the 18th conference on Computational linguistics,
pages 705?711, Morristown, NJ, USA. Association for
Computational Linguistics.
B. Settles. 2004. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. log, 1:1.
Praneeth M Shishtla, Karthik Gali, Prasad Pingali, and
Vasudeva Varma. 2008a. Experiments in telugu ner:
A conditional random field approach. In Proceed-
ings of the IJCNLP-08 Workshop on NER for South
and South East Asian Languages, pages 79?84, Hyder-
abad, India, January. Association for Computational
Linguistics.
Praneeth M Shishtla, Prasad Pingali, and Vasudeva
Varma. 2008b. A character n-gram based approach
for improved recall in indian language ner. In Pro-
ceedings of the IJCNLP-08 Workshop on NER for
South and South East Asian Languages, pages 101?
108, Hyderabad, India, January. Association for Com-
putational Linguistics.
P Srikanth and Kavi Narayana Murthy. 2008. Named
entity recognition for telugu. In Proceedings of the
IJCNLP-08 Workshop on NER for South and South
East Asian Languages, pages 41?50, Hyderabad, In-
dia, January.
K. Takeuchi and N. Collier. 2002. Use of support vec-
tor machines in extended named entity recognition. In
Proceedings of the sixth Conference on Natural Lan-
guage Learning (CoNLL-2002).
G.D. Zhou and J. Su. 2001. Named entity recognition
using an HMM-based chunk tagger. Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 473?480.
15
16
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 90?95,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Modeling Letter-to-Phoneme Conversion as a Phrase Based Statistical
Machine Translation Problem with Minimum Error Rate Training
Taraka Rama, Anil Kumar Singh, Sudheer Kolachina
Model oei ni gLdt rt e-i Phi Pi omgLCidvmi s
aaansBScimoTocsadc-ob
{vomowou Pvl ci dvPsod-ru mi Pi omgLsPl cLi imbwEeRku mi Pi omgL}b---vbogb-d
Abstract
Mi vvim,vt ,ELt diAi gt dKimP-t dEroSPod -AEtm,
vodvmt ri -dPiKimoroEEr-gov-t dPbavgodTi o c-1,
I gl rvvoPwTi gol Pi vLi AoEE-de 1mtA ri vvimPvt
ELt diAi PgodTi AodS,vt ,AodSb2 i Emi Pi dvo
rodel oei -dci Ei dci dvri vvim,vt ,ELt diAi gt d,
KimP-t d oEEmt ogLWL-gL -PToPi c t d vLi Et E,
l romELmoPi ToPi c 3vov-Pv-gor f ogL-di nmodP,
rov-t d vi gLd-4l i Pb nLi mi Pl rvP t 1 t l mi 5,
Eim-Ai dvPgri omrS ciAt dPvmovi vLovPl gLvi gL,
d-4l i P god Ti l Pi c i 11i gv-Ki rS 1tmri vvim,vt ,
ELt diAi gt dKimP-t db . l mmiPl rvP PLtW od
tKimorr -AEmtKiAi dv t 1 xbkp tKimvLi ToPi ,
r-di odc omi gtAEomoTri vt vLi Pvovi t 1vLi omvb
2 i orPt EmtEt Pi oAi oPl mi vt i Pv-Aovi vLi c-1,
I gl rvS riKi rt 1MyDvoPw1tmo rodel oei b
1 Introduction
Mi vvim,vt,ELt diAi fiMyD: gt dKimP-td god Ti ci I di c
oP vLi voPw t 1 Emi c-gv-de vLi Emt dl dg-ov-td t 1 o
Wtmc e-Ki d -vPtmvLt emoEL-g 1tmA fiHomvrivvi v orbs
yRRk:bnLi Emt dl dg-ov-td -P l Pl orrSmiEmi Pi dvic oP
o Pi 4l i dgi t 1 ELt diAi Pb Mi vvim,vt,ELt diAi gt d,
KimP-td PSPviAPEroS o KimS -AEtmvodvmt ri -d PEi rr
gLi gwimPfint l vodtKo odc f t tmi syRRy:sPEi i gLPSd,
vLi P-PPSPviAPfi3gLmt i vimi vorbsyRRy: odc vmodPr-v,
imov-td fi3Lim-1 odc Ft dcmows yRR+:b Mi vvim,vt,
ELt diAi gt dKimP-td PSPviAP AoS orPt Ti i 11i g,
v-Ki rSl Pi c 1tmgt edovi -ci dv-Igov-td odc vmodPr-vimo,
v-tdbnLi i 5-Pv-de gt edovi -ci dv-Igov-td PSPviAPl Pi
vLi tmvLt emoEL-g1tmA t 1oWtmc oPvLi -dEl vbHl vWi
wdtWvLovvLi gtmmiPEt dci dgi Ti vWi i d Wm-vvid odc
PEtwi d 1tmAP t 1WtmcPgod Ti 4l -vi -mmi el romoP-P
vLi goPi -d *der-PLb *Ki d -d t vLimrodel oei PW-vL
Pl EEt Pi crSmi el romPEi rr-dePs vL-P-mmi el rom-vSi 5-PvP
tW-de vt r-del -Pv-gELi dtAi do r-wi TtmmtW-de odc
rodel oei Kom-ov-tdb Mi vvim,vt,ELt diAi gt dKimP-td
PSPviAPgod 1og-r-vovi vLi voPwt 1gt edovi -ci dv-Igo,
v-td TS EmtK-c-de o rodel oei -dci Ei dci dvvmodPgm-E,
v-td 1tmodSWtmcb
6 dv-ro 1iWSi omPoet s rivvim,vt,ELt diAi gt dKim,
P-td WoPEim1tmAi c gt dP-cim-de t drS t di ,t di gtm,
mi PEt dci dgi P fiHrogw i v orbs7  k  oAEimi v orbs
yRR :bhi gi dvWtmwl Pi PAodS,vt,AodS gtmmiPEt d,
ci dgi Pfi -oAEt o Aomd i vorbsyRR+: odc miEtmvPP-e,
d-IgodvrSL-eLimoggl mogS 1tm l vgLs  i mAod odc
 mi dgLbnLi gl mmi dvPvovi t 1vLi omvPSPviAPe-Ki oP
Al gLoP Rp fi -oAEt o Aomd i vorbsyRRk: oggl mogS
1tmrodel oei Pr-wi  l vgLs  i mAododc  mi dgLbBtW,
iKims oggl mogS t 1vL-PriKi r -PSi vvt Ti ogL-iKi c 1tm
*der-PLb
hl ri,ToPi c oEEmt ogLi P vt vLi EmtTriA t 1 rivvim,
vt,ELt diAi gt dKimP-td orvLt l eLoEEi or-des omi -A,
Emogv-goroPvLi dl ATimt 1ml riP1tmo Eomv-gl romrod,
el oei god Ti KimSL-eLfiFtA-diwodc HrogwsyRR :b
 rvimdov-Ki oEEmt ogLi Pvt vL-PEmtTriA omi ToPi c t d
AogL-di riomd-de odc Aowi l Pi t 1mi Pt l mgi PPl gLoP
Emt dl dg-ov-td c-gv-tdom-iPbad vL-PEoEimsWi EmiPi dv
t di Pl gLAogL-di riomd-de ToPi c oEEmt ogLWLimi -d
Wi i dK-Poei vL-PEmtTriA oPo 3vov-Pv-gor f ogL-di
nmodProv-td fi3f n: EmtTriAb
nLi t l vr-di t 1 vLi EoEim-PoP1t rrtWPb 3i gv-td y
Emi Pi dvPo Tm-i1 Pl AAomS t 1 vLi mi rovic Wtmwct di
-d MyD gt dKimP-tdb 3i gv-td  ci Pgm-Ti Pt l mAt ci r
odc vLi vigLd-4l i PciK-Pi c 1tmtEv-A- -de vLi Eim,
1tmAodgi b3i gv-td  ci Pgm-Ti PvLi rivvim,vt,ELt diAi
or-edAi dvbnLi ci Pgm-Ev-td t 1vLi mi Pl rvPodc i 5Eim,
-Ai dvPodc o diWvigLd-4l i 1tmi Pv-Aov-de vLi c-1I ,
90
gl rvSriKi rt 1MyDvoPwLoKi Ti i d e-Ki d -d3i gv-td xb
*mmtmodorSP-P-PEmiPi dvic -d 3i gv-td  b  -dorrSWi
gt dgrlci W-vLo Pl AAomS odc Pl eei Pvc-mi gv-tdP1tm
1l vlmi Wtmwb
2 Related Work
ad vLi rivvim,vt,ELt diAi gt dKimP-td voPws o P-deri
rivvimgod AoEvt Al rv-Eri ELt diAi P  5 ? wP odc
Al rv-Eri rivvimPgod ei dimovi o P-deri ELt diAi b
rivvimgod orPt AoEvt o dl rrELt diAi  i ? ?  odc
K-gi ,KimPob nLi Pi i 5oAEriPe-Ki o er-AEPi t 1WLS
vLi voPw-PPt gtAEri5 odc o P-deri AogL-di riomd-de
vigLd-4l i AoS dt vTi i dt l eLvt Pt rKi vLi EmtTriAb
 tKimK-iWt 1vLi r-vimovlmi Pl EEtmvPvL-Pgro-Ab
ad t rcimoEEmt ogLi Ps vLi or-edAi dvTi vWi i d vLi
rivvimP odc ELt diAi P WoP vowi d vt Ti t di ,vt,
t di fiHrogw i v orbs7  k: odc vLi ELt diAi WoP
Emi c-gvic 1tm iKimS P-deri rivvimb Hl v mi gi dv
WtmwfiH-Pod- odc  i Ss yRRy  -oAEt o Aomd i v orbs
yRR+: PLtWPvLovAl rv-Eri rivvim,vt,ELt diAi or-ed,
Ai dvPEim1tmA Ti vvimvLod P-deri rivvimvt ELt diAi
or-edAi dvPbnLi EmtTriA god Ti i -vLimK-iWi c oPo
Al rv-,groPPgroPP-I imEmtTriA tmo Pvml gvlmi Emi c-g,
v-td EmtTriAb ad Pvml gvlmi Emi c-gv-tds vLi oretm-vLA
vowi P vLi EmiK-t l P ci g-P-tdP oP vLi 1i ovlmi PWL-gL
-d l i dgi vLi gl mmi dvci g-P-tdb
ad vLi groPP-I imoEEmt ogLs t drSvLi rivvimodc -vP
gt dvi5vomi vowi d oP1i ovlmi PbnLi dsi -vLimAl rv-groPP
ci g-P-tdvmi i Pfi oi riAodPodc Kodci dHt PgLs7  +:
tm-dPvodgi ToPi c riomd-de oP-dfiKod ci d Ht PgLodc
 oi riAodPs7  k: -Pl Pi c vt Emi c-gvvLi groPPsWL-gL
-d vL-P goPi -P o ELt diAi b 3tAi t 1 vLi Pi Ai vL,
t cPfiHrogwi vorbs7  k: omi dt vgtAErivirSol vtAov-g
odc di i c od -d-v-orLodcgmo1vic Pi i c-de vt Ti e-d vLi
groPP-Igov-tdb
3vml gvlmi Emi c-gv-td -P r-wi o voee-de EmtTriA
WLimi Bf f P finoSrtms yRRx: omi l Pi c vt At ci r
vLi EmtTriAb noSrtmgro-AP vLov i 5gi Ev 1tmo Emi ,
Emt gi PP-de PviEs -v -P gtAErivirS ol vtAov-gb nLi
WLt ri Emt gi PP -PEim1tmAi c -d o P-deri PviEb nLi
mi Pl rvPomi Et tmsoPmi oPt di c -dfi -oAEt o Aomd i vorbs
yRRk: cl i vt vLi i A-PP-td EmtToT-r-v-iPdt vTi -de -d,
1tmAi c TS vLi EmiK-t l P rivvim P i A-PP-td EmtToT-r,
-v-iPb Dmt dl dg-ov-td TS  dorteS fiDT : -Po covo,
cm-Ki d Ai vLt c fif omgLodc odc  oAEims yRRR: 1tm
rivvim,vt,ELt diAi gt dKimP-td WL-gL -P l Pi c oeo-d
TS  oAEimi v or fiyRR :b nLi S P-AErS l Pi od
*5Ei gvov-td,f o5-A-Pov-td fi*f : r-wi oretm-vLA 1tm
or-ed-de vLi rivvim,ELt diAi Eo-mP-do PEi i gLc-gv-t,
domSbnLi S gro-A vLovTS -dviemov-de vLi or-edAi dvP
-dcl gi c TS vLi oretm-vLA -dvt vLi DT PSPviAs vLi S
Wimi oTri vt -AEmtKi vLi oggl mogS t 1vLi Emt dl dg-,
ov-td P-ed-IgodvrSb 2 i orPt l Pi vLi AodS,vt,AodS
or-edAi dvoEEmt ogLTl v -d o c-11imi dvWoS odc t T,
vo-di c 1mtA o c-11imi dvPt l mgi b
nLi mi gi dv Wtmw t 1  -oAEt o Aomd i v or fiyRR+:
gtAT-di PTt vLt 1vLi oTtKi oEEmt ogLi P-doKimS -d,
vimi Pv-de Aoddimb avl Pi Pod *f r-wi oretm-vLA 1tm
or-ed-de vLi rivvimPodc ELt diAi PbnLi oretm-vLA or,
rtWPAodS,vt,AodS or-edAi dvPTi vWi i d rivvimPodc
ELt diAi Pb nLi d vLimi -Po rivvimgLl dw-de At cl ri
WL-gL l Pi P -dPvodgi ,ToPi c vmo-d-de vt vmo-d t d vLi
or-edAi dvPWL-gLLoKi Ti i d t Tvo-di c -d vLi EmiK-,
t l PPviEbnL-PAt cl ri -Pl Pi c vt el i PPvLi Et PP-Tri
rivvimgLl dwP-d iKimS WtmcbnLi d o rtgorELt diAi
Emi c-gvtm-Pl Pi c vt el i PP vLi ELt diAi P 1tmiKimS
rivvim-d o Wtmcb nLi P- i t 1 vLi rivvimgLl dwgt l rc
Ti i -vLimt di tmvWt b . drSt di godc-covi 1tmiKimS
Wtmc -PorrtWi cbnLi Ti PvELt diAi Pi 4l i dgi -Pt T,
vo-di c TS l P-de  -vimT- Pi omgLb
 d t dr-di At ci r f ah  fiCmoAAi modc 3-deims
yRR : WL-gLl EcoviPEomoAi vimP-Pl Pi c 1tmvLi MyD
voPw TS  -oAEt o Aomd i v or fiyRRk:b nLi ol vLtmP
l d-1S vLi PviEPt 1rivvimPi eAi dvov-tdsELt diAi Emi ,
c-gv-td odc Pi 4l i dgi At ci r-de -dvt o P-deri At c,
l rib nLi ELt diAi Emi c-gv-td odc Pi 4l i dgi At ci r,
-de omi gt dP-cimi c oPvoee-de EmtTriAP odc o Dim,
gi Evmt d Bf f fiCt rr-dPs yRRy: -P l Pi c vt At ci r
-vb nLi rivvimPi eAi dvimAt cl ri -PmiErogi c TS o
At dt vtdi ELmoPorci gt cimfi i dPodc  i Ss yRR : vt
Pi omgL1tmvLi Et PP-Tri Pl TPvm-deP-doWtmc odc t l v,
El v vLi n,Ti Pv r-Pv 1tml Ecov-de f ah  bH-Pod- odc
 i S fiyRRy: vowi vLi t -dvAl rv-emoAPt 1emoELiAi P
odc ELt diAi P oP 1i ovlmi P 1tmor-edAi dv odc rod,
el oei At ci r-de 1tmELt di v-gvmodPgm-Ev-tdEmtToT-r-,
v-iPb LSTm-c oEEmt ogLP-A-romvt vL-P-PTSfiKod ci d
Ht PgLodc Cod-P-l PsyRR :b
ad vLi di 5vPi gv-tdWi At ci rvLi EmtTriA oPo3vo,
v-Pv-gorf ogL-di nmodProv-td fi3f n: voPwb
3 Modeling the Problem
 PPl Ai vLov e-Ki d o Wtmcs miEmi Pi dvic oP o Pi ,
4l i dgi t 1rivvimPl  lJ1  l1bbbljbbblJsdi i cPvt Ti vmod,
Pgm-Ti c oPo Pi 4l i dgi t 1ELt diAi PsmiEmi Pi dvic oPf
91
 f I1  f1bbbfibbbfIbnLi EmtTriA t 1 I dc-de vLi Ti Pv
ELt diAi Pi 4l i dgi oAt de vLi godc-covi vmodProv-tdP
god Ti miEmi Pi dvic oP
fbest = argmax
f
{Pr (f | l)} fi7:
2 i At ci rvLi EmtTriA t 1rivvimvt ELt diAi gt d,
KimP-td ToPi c t d vLi dt -PS gLoddi r At ci rbhi 1tm,
Al rov-de vLi oTtKi i 4l ov-td l P-de HoSi Phl ri
fbest = argmax
f
p (l | f) p (f) fiy:
nL-P1tmAl rov-td orrtWP 1tmo ELt diAi d,emoA
At ci rp (f) odc ovmodPgm-Ev-tdAt ci rp (l | f)b -Ki d
o Pi 4l i dgi t 1 rivvimP ls vLi omeAo5 1l dgv-td -P o
Pi omgL 1l dgv-td vt t l vEl v vLi Ti Pv ELt diA-g Pi ,
4l i dgi b  l m-de vLi ci gt c-de ELoPi s vLi rivvimPi ,
4l i dgi l -PPi eAi dvic -dvt o Pi 4l i dgi t 1 Mrivvim
Pi eAi dvP?lK1 b *ogLPi eAi dv l?k -d l?
K
1 -PvmodPgm-Ti c
-dvt o ELt diAi Pi eAi dvf?kbnLl PvLi Ti PvELt diAi
Pi 4l i dgi -Pei dimovic 1mtA ri1vvt m-eLv-d vLi 1tmA
t 1 Eomv-or vmodProv-tdPb HS l P-de od o,emoA At ci r
pLM oPvLi rodel oei At ci rsWi LoKi vLi i 4l ov-tdP
fbest = argmax
f
p (l | f) pLM fi :
W-vLp (l | f)Wm-vvid oP
p(l?K1 | f?
K
1 ) =
K?
k=1
?(l?k | f?k) fi :
 mtA vLi oTtKi i 4l ov-tds vLi Ti PvELt diAi Pi ,
4l i dgi -Pt Tvo-di c ToPi c t d vLi Emt cl gvt 1vLi EmtT,
oT-r-v-iPt 1vmodPgm-Ev-td At ci rodc vLi EmtToT-r-v-iP
t 1 o rodel oei At ci r odc vLi -mmiPEi gv-Ki Wi -eLvPb
nLi Ai vLt c 1tmtTvo-d-de vLi vmodPgm-Ev-tdEmtToT-r,
-v-iP-Pci Pgm-Ti c Tm-i  S -d vLi di 5vPi gv-tdb  i vim,
A-d-de vLi Ti PvWi -eLvP-Pdi gi PPomS 1tmtTvo-d-de
vLi m-eLvELt diAi Pi 4l i dgi b nLi i Pv-Aov-td t 1vLi
At ci rP Wi -eLvPgod Ti ct di -d vLi 1t rrtW-de Aod,
dimb
nLi Et Pvim-tmEmtToT-r-vSPr (f | l) god orPt Ti
c-mi gvrSAt ci ric l P-de o rte,r-di omAt ci rb ad
vL-PAt ci rsWi LoKi o Pi v t 1M 1i ovlmi 1l dgv-tdP
hm(f, l),m = 1...M b  tmi ogL 1i ovlmi 1l dgv-td
vLimi i 5-PvPo Wi -eLvtmAt ci rEomoAi vim?m,m =
1...MbnLl PvLi Et Pvim-tmEmtToT-r-vSTi gtAi P
Pr (f | l) = p?M1
(f | l) fix:
=
exp
?
?Mm=1?mhm(f, l)
?
?
f?I1
exp
?
?Mm=1?mhm(f?
I
1 , l)
? fi :
W-vLvLi ci dtA-dovtms o dtmAor- ov-td 1ogvtmvLov
god Ti -edtmi c -dvLi Ao5-A- ov-td Emt gi PPb
nLi oTtKi At ci r-de i dvo-rPI dc-de vLi Pl -voTri
At ci rEomoAi vimPtmWi -eLvPWL-gLmi  i gvvLi EmtE,
imv-iPt 1 t l mvoPwb 2 i oct EvvLi gm-vim-td 1t rrtWi c
-d fi. gLs yRR : 1tmtEv-A-P-de vLi EomoAi vimPt 1vLi
At ci rbnLi ci vo-rPt 1vLi Pt rlv-td odc Emt t 11tmvLi
gt dKimei dgi omi e-Ki d -d . gLfiyRR :b nLi At ci rP
Wi -eLvPs l Pi c 1tmvLi MyD voPws omi t Tvo-di c 1mtA
vL-Pvmo-d-deb
4 Letter-to-Phoneme Alignment
2 i l Pi c  a    fi. gL odc  i Ss yRR :s od t Ei d
Pt l mgi vtt rw-vs 1tmor-ed-de vLi rivvimP W-vLvLi
ELt diAi P -d vLi vmo-d-de covoPi vPb ad vLi gt dvi5v
t 13f nsPoS *der-PL,3Eod-PLs vLi EomorrirgtmEl P-P
or-edi c T-c-mi gv-tdorrSvt t Tvo-dvLi vWt or-edAi dvPb
nLi aHf At ci rPe-Ki t drS t di ,vt,t di or-edAi dvP
Ti vWi i dWtmcP-do Pi dvidgi Eo-mb3t s  a    l Pi P
PtAi Li l m-Pv-gP vt mi I di vLi or-edAi dvPfi. gL odc
 i SsyRR :b
ad t l m-dEl v covos vLi Pt l mgi P-ci gt dP-PvPt 1
emoELiAi fitmrivvim: Pi 4l i dgi P odc vLi vomei v P-ci
gt dP-PvPt 1 ELt diAi Pi 4l i dgi Pb *KimS rivvimtm
emoELiAi -P vmi ovic oP o P-deri  Wtmc 1tmvLi
 a    -dEl vbnLi vmodPgm-Ev-td EmtToT-r-v-iPgod
vLi d Ti i oP-rSriomdv 1mtA vLi or-edAi dvP-dcl gi c
TS  a    s l P-de o Pgtm-de 1l dgv-td fiFt i Ld i vorbs
yRR :b  -el mi 7 PLtWPvLi or-edAi dvP-dcl gi c TS
 a    1tmvLi i 5oAEri WtmcP WL-gL omi Ai d,
v-tdi c TS  -oAEt o Aomd i v or fiyRR+:b ad vL-PI e,
l mi s Wi t drSPLtWvLi or-edAi dvP1mtA emoELiAi P
vt ELt diAi Pb
 -el mi 7 *5oAEri  r-edAi dvP1mtA  a  
92
5 Evaluation
2 i iKorlovic t l mAt ci rPt d vLi *der-PLCf 6  -gvs
 mi dgL Hml ri5s  i mAod Ci ri5 odc  l vgL Ci ri5
PEi i gLc-gv-tdom-iPbnLi Pi c-gv-tdom-iPomi oKo-roTri
1tmctWdrtoc t d vLi WiTP-vi t 1 Dh.    M3 M7
Mi vvim,vt,DLt diAi Ct dKimP-td CLorridei b noTri 7
PLtWPvLi dl ATimt 1WtmcP1tmi ogLrodel oei bnLi
covoPi vPoKo-roTri ov vLi WiTP-vi Wimi c-K-ci c -dvt
7R1t rcPbad vLi Emt gi PPt 1EmiEom-de vLi covoPi vPWi
vtt wt di Pi v1tmviPvsodt vLim1tmciKi rtE-de t l mEo,
moAi vimPodc vLi miAo-d-de k Pi vP1tmvmo-d-deb 2 i
miEtmv t l mmiPl rvP-d Wtmc oggl mogS movis ToPi c t d
7R,1t rcgmt PPKor-cov-tdsW-vLAi od odc Pvodcomc ci ,
K-ov-tdb
Model oei  ovoPi vP  l ATimt 12 tmcP
*der-PL Cf 6  -gv 77yy 7
 mi dgL Hml ri5 y+ +
 i mAod Ci ri5    y7
 l vgL Ci ri5 77 yxy
noTri 7  l ATimt 1WtmcP-d i ogL ovoPi v
2 i miAt Ki c vLi t di ,vt,t di or-edAi dvP 1mtA
vLi gtmEtmo odc -dcl gi c t l mtWd or-edAi dvPl P,
-de  a    b 2 i l Pi c A-d-Al A immtmmovi vmo-d,
-de fi. gLs yRR : odc vLi   Ti oA Pi omgL ci ,
gt cim-AEriAi dvic TS Ft i Ld fiFt i Ld i vorbsyRR :b
 rr vLi oTtKi vtt rPomi oKo-roTri oP EomvPt 1 vLi
f . 3*3 fiFt i Ld i vorbsyRR+: vtt rw-vb
5.1 Exploring the Parameters
nLi EomoAi vimPWL-gLLoKi oAot m-d l i dgi t d vLi
Eim1tmAodgi t 1 o ELmoPi ,ToPi c 3f n At ci r omi vLi
or-edAi dv Li l m-Pv-gPs vLi Ao5-Al A ELmoPi ridevL
fif Dh: odc vLi tmcimt 1vLi rodel oei At ci rfiFt i Ld
i v orbsyRR :b ad vLi gt dvi5v t 1 rivvimvt ELt diAi
gt dKimP-tds deli ng Ai odP o Pi 4l i dgi t 1 rivvimP tm
ELt diAi PAoEEi c vt i ogL t vLimW-vLPtAi EmtT,
oT-r-vSfi-bibsvLi eLdt regn-n: odc Pvtmi c -d o ELmoPi
voTrib nLi Pi h-PmP deli ng Cgovre gtmmiPEt dcPvt
vLi Ao5-Al A dl ATimt 1rivvimPtmELt diAi PvLovo
LSEt vLi P-Pgod gt dvo-dbB-eLimELmoPi ridevLgtmmi ,
PEt dcPo romeimELmoPi voTri cl m-de ci gt c-deb
2 i LoKi gt dcl gvic i 5Eim-Ai dvPvt Pi i WL-gL
gtAT-dov-td e-Ki P vLi Ti Pv t l vEl vb 2 i -d-v-orrS
vmo-di c vLi At ci r W-vLKom-t l P EomoAi vimP t d vLi
7LvvEW WWbEoPgor,di vWtmwbtmeC Lorridei PDh.   M3 M
vmo-d-de covo odc viPvic 1tmKom-t l P Korli P t 1 vLi
oTtKi EomoAi vimPb 2 i Kom-ic vLi Ao5-Al A ELmoPi
ridevL1mtA y vt +bnLi rodel oei At ci rWoPvmo-di c
l P-de 3haMf vtt rw-vfi3vtrgwi syRRy:b2 i Kom-ic vLi
tmcimt 1 rodel oei At ci r 1mtA y vt kb 2 i orPt vmo,
KimPi c vLi or-edAi dvLi l m-Pv-gPPEi gvml As 1mtA vLi
EomP-At d-t l P -orglngsr ov t di i dc t 1 vLi PEi gvml A
vLmt l eL vlt aB vlt a Sc-i vB vlt a Sc-i vSToi CBvlt a S
c-i vSToi CSi oc odc nlsrtrvrvt vLi At Pvrid-idvmo-to
ovvLi t vLimi dcb. l m-dvl-v-Ki el i PPWoPvLovvLi Ti Pv
or-edAi dvLi l m-Pv-gWt l rcTi mo-tob
2 i t TPimKi c vLovvLi Ti Pvmi Pl rvPWimi t Tvo-di c
WLi dvLi rodel oei Atc i rWoPvmo-di c t d  ,emoA odc
vLi or-edAi dvLi l m-Pv-gWoPmo-tob  t P-ed-Igodv
-AEmtKiAi dvWoPtTPimKi c -d vLi mi Pl rvPWLi d vLi
Korli t 1 f Dh WoPemi ovimvLod xb 2 i LoKi vowi d
gomi Pl gLvLovvLi or-edAi dvPomi orWoSPAt dt vtd-gb
 t vi vLov vLi oKimoei ridevLt 1 vLi ELt diAi Pi ,
4l i dgi WoPorPt  b2 i oct Evic vLi oTtKi EomoAi vim
Pi vv-deP1tmEim1tmA-de vmo-d-de t d vLi -dEl vcovob
5.2 System Comparison
2 i oct EvvLi mi Pl rvPe-Ki d -dfiyRR+: oPt l mToPi r-di b
2 i orPt gtAEomi t l mmi Pl rvPW-vLPtAi t vLimmi gi dv
vigLd-4l i PAi dv-tdi c -d vLi hi rovic 2 tmwPi gv-tdb
noTri y PLtWPvLi mi Pl rvPb  PvL-PvoTri PLtWPs t l m
oEEmt ogLS-ircPvLi Ti Pvmi Pl rvP-d vLi goPi t 1  i m,
Aod odc  l vgLb nLi Wtmc oggl mogS t Tvo-di c 1tm
vLi  i mAod Ci ri5 odc  l vgL Ci ri5 covoPi v l P-de
t l moEEmt ogL -PL-eLimvLod vLovt 1 orrvLi EmiK-t l P
oEEmt ogLi P r-Pvic -d vLi voTrib ad vLi goPi t 1 *d,
er-PLodc  mi dgLs orvLt l eLvLi ToPi r-di -PogL-iKi c
vLmt l eLt l moEEmt ogLsvLi Wtmc oggl mogS 1orrPPLtmv
t 1 Ti -de vLi Ti PvbBtWiKims -vAl PvorPt Ti dt vic
vLovvLi covoPi vvLovWi l Pi c 1tm*der-PL -PPr-eLvrS
romeimvLod vLt Pi t 1 vLi t vLimPSPviAPPLtWd -d vLi
voTrib
2 i orPt t TPimKi vLov 1tmod oKimoei ELt diAi
oggl mogS t 1  7b p s vLi oKimoei Wtmc oggl mogS -P
  bk7p s WL-gL gtmmtTtmoviPvLi gro-A TS Hrogwi v
orfiHrogwi vorbs7  k: vLovo  Rp ELt diAi oggl mogS
gtmmiPEt dcPvt  Rp Wtmc oggl mogSb
5.3 Difficulty Level and Accuracy
2 i orPt EmtEt Pi o diWrodel oei ,-dci Ei dci dvAi o,
Pl mi vLovWi gorr 2 i -eLvic 3SAAi vm-g CmtPP *d,
vmtES fi2 3C*: vt i Pv-Aovi vLi c-1I gl rvSriKi rt 1vLi
MyD voPw1tmo Eomv-gl romrodel oei b nLi a g-vergc
93
Model oei  ovoPi v HoPi r-di C hn 7,7  r-ed 7,7  C3a 7,7  Bf f f ,f  r-ed f ,f  Bf f f i h  
*der-PL Cf 6  -gv xkb ?Rb  x+bk  Rb ?Rbx  yb ?Rb x  yb7?Rbx  xb7?Rb R  xb ?Rb+y   bk7?Rb +
 i mAod Ci ri 5 k bR?Rb R k b k k b ?Rbx k+b ?Rb + k+b ?Rbx k b ?Rbx k bk?Rbx  RbyR?Rbyx
 mi dgL Hml ri 5 k b ?Rb + , k+bR?Rb k k bx?Rb k kkby?Rb   Rb ?Rbx+  Rb ?Rb x k b+7?Rbxy
 l vgL Ci ri 5 k b ? Rb  , k b ?Rb  k+bx?Rb y k+b ?Rb   7b7?Rby+  7b ?Rby  7b  ?Rby
noTri y 3SPviA CtAEom-Pt d -dvimAPt 1Wtmc oggl mog-iPbBaseline:hi Pl rvP1mtA Dh.   M3 3 WiTP-vibCART: C hn  i g-P-td
nmi i 3SPviA fiHrogwi vorbs 7  k:b 1-1 Align, M-M align, HMM: t di ,t di or-edAi dvPsAodS,AodS or-edAi dvPs Bf f W-vLrtgor
Emi c-gv-td fi -oAEt o Aomd i v orbs yRR+:b CSIF:Ct dPvmo-dv 3ov-P1ogv-td ad1imi dgi fiC3a : t 1fiKod ci d Ht PgL odc Cod-P-l Ps yRR :b
MeR+A*:. l moEEmt ogLW-vLA-d-Al A immtmmovi vmo-d-de odc   Pi omgLci gt cimb ,  mi 1imPvt dt miEtmvic mi Pl rvPb
3C* -Pci I di c oP1t rrtWP
dscewt =
?
rt (pl log (qf ) + qf log (pl)) fi+:
WLimi p odc q omi vLi EmtToT-r-v-iPt 1t ggl mmi dgi
t 1 rivvimfil: odc ELt diAi fif : Pi 4l i dgi Ps mi PEi g,
v-Ki rSb rPt srt gtmmiPEt dcPvt vLi gt dc-v-tdorEmtT,
oT-r-vSp(f | l)b nL-PvmodPgm-Ev-td EmtToT-r-vSgod
Ti t Tvo-di c 1mtA vLi ELmoPi voTriPei dimovic cl m-de
vmo-d-deb nLi Wi -eLvic i dvmtES Ai oPl mi dscewts1tm
i ogLrodel oei sWoPdtmAor-Pi c W-vLvLi vtvordl A,
Timt 1Pl gLn,emoA Eo-mPTi -de gt dP-cimi c 1tmgtA,
Eom-Pt d W-vLt vLimrodel oei Pb 2 i LoKi I 5i c vLi
Ao5-Al A tmcimt 1 l odc f n,emoAP vt Ti  b no,
Tri  PLtWPvLi c-1I gl rvSriKi rPoPgorgl rovic l P-de
2 3C* ortde W-vLvLi oggl mogS 1tmvLi rodel oei P
vLovWi viPvic t db  P -P iK-ci dv 1mtA vL-PvoTris
vLimi -Po mt l eL gtmmi rov-td Ti vWi i d vLi c-1I gl rvS
riKi r odc vLi oggl mogS t Tvo-di cs WL-gL orPt Pi i AP
-dvl-v-Ki rSKor-cse-Ki d vLi dovlmi t 1vLi Pi rodel oei P
odc vLi -mtmvLt emoEL-iPb
Model oei  ovoPi vP dscewt  ggl mogS
*der-PL Cf 6  -gv Rb R   bk7?Rb +
 mi dgL Hml ri 5 Rb 7 k b+7?Rbxy
 l vgL Ci ri 5 Rb x  7b  ?Rby
 i mAod Ci ri 5 Rb   RbyR?Rbyx
noTri   dscewt Korl i PEmi c-gvvLi oggl mogSmovi Pb
6 Error Analysis
ad vL-PPi gv-td Wi Emi Pi dv o Pl AAomS t 1 vLi immtm
odorSP-P1tmvLi t l vEl v ei dimovicb 2 i vm-ic vt t T,
PimKi -1 vLimi i 5-Pv odS EovvimdP -d vLi WtmcP vLov
Wimi vmodPgm-Ti c -dgtmmi gvrSb
nLi Aot m-vSt 1 immtmP t ggl mmi c -d vLi goPi t 1
KtWi r vmodPgm-Ev-tds odc c-ELvLt de vmodPgm-Ev-td -d
Eomv-gl romb ad vLi goPi t 1 *der-PLs vL-Pgod Ti ov,
vm-Tl vic vt vLi ELi dtAi dt d t 1 ri5-gor TtmmtW-de
1mtA o Kom-ivSt 1 Pt l mgi P oP o mi Pl rvt 1WL-gL vLi
dl ATimt 1PEomPi or-edAi dvP-PKimS L-eLbnLi PSP,
viA -PorPt l doTri vt riomd orrtELt d-g Kom-ov-td t 1
gimvo-d w-dcP t 1 gt dPt dodvor ELt diAi Ps At Pv dt ,
voTrS1m-gov-Ki Pr-wi P odc  b nL-PEmtTriA -Pi 5,
ogimTovic TS vLi -mmi el rom-vSt 1orrtELt d-gKom-ov-td
-dvLi rodel oei -vPi r1b
7 Conclusion and Future Work
ad vL-PEoEimWi LoKi vm-ic vt occmi PPvLi EmtTriA
t 1 rivvim,vt,ELt diAi gt dKimP-td TS At ci r-de -voP
od 3f n EmtTriA odc Wi LoKi l Pi c A-d-Al A immtm
movi vmo-d-de vt t Tvo-d vLi Pl -voTri At ci r EomoAi ,
vimPsWL-gL oggtmc-de vt t l mwdtWricei s -Po dtKi r
oEEmt ogLvtMyDvoPwbnLi mi Pl rvPt Tvo-di c omi gtA,
EomoTri vt vLi Pvovi t 1 vLi omvPSPviA odc t l mimmtm
odorSP-PPLtWPvLovo rtvt 1-AEmtKiAi dv-PPv-rrEt P,
P-Trib
advl-v-Ki rSs vLi Eim1tmAodgi t 1vLi PSPviA god Ti
-AEmtKi c -dovrioPvvWt omi oPb -mPv-PvLi f -d-Al A
*mmtmhovi nmo-d-de fif *hn: odc vLi Pi gt dc -PvLi
ci gt c-de ELoPi b 6 P-de ELt di v-g1i ovlmi ToPi c i c-v
c-Pvodgi tmPvm-de P-A-rom-vSoPvLi rtPP1l dgv-td -d
vLi f *hn -AEriAi dvov-td god -AEmtKi mi Pl rvPP-e,
d-IgodvrSb ad occ-v-tds -dgtmEtmov-de At mi At ci r
EomoAi vimP odc i 5vidP-Ki viPv-de t 1 vLi Pi EomoAi ,
vimPA-eLv -AEmtKi vLi mi Pl rvPt 1 vLi PSPviAb 2 i
orPt Erod vt -dvmt cl gi o ci gt c-de PgLiAi P-A-romvt
vLi Pl TPvm-de ToPi c vmodPcl gimfi3Lim-1odc Ft dcmows
yRR+: vt -AEmtKi vLi l Poei t 1rtWimtmcimrodel oei
At ci rPb
Acknowledgements
nL-P Wtmw WoP Pl EEtmvic TS aMf n emodv
77fi7R:y RR ,BCCfin aM:b
94
References
3l Pod Homvri vvs  m i etm Ft dcmows odc Ct r-d CLimmSb
yRRkb l vt Aov-gPSrroT-I gov-t dW-vLPvml gvl mi c 3 f P
1tmri vvim,vt ,ELt diAi gt dKimP-t db ad blt sggc-ovn tw
uERSk, AKR1sEoei Px k x+ sCt rl ATl Ps. L-t s  l di b
 CMb
f o5 H-Pod- odc BimAodd  i Sb yRRyb adKi Pv-eov-t dP
t d  t -dv,Al rv-emoA At ci rP1tmemoELiAi ,vt ,ELt diAi
gt dKimP-t db ad Iorgloi r-t oi CEt owglgosg t o 2dtWgo
Ri ovmi vg blt sgnn-ovs Eoei P 7Rx 7Rks  i dKims C. s
6 3 s3i EviATimb
 b2 bHrogws FbMi d t s odc  bDoei rb 7  kb aPPl i P -d
Hl -rc-de  i dimorMi vvimvt 3t l dc hl ri Pbad 1eg 1e-lc
32EufE4 E4 25 u . t lWnet d x31p. y t o 2dggse 2LoS
regn-nba3C b
f bCt rr-dPb yRRyb  -Pgm-A-dov-Ki vmo-d-de Ai vLt cP1tm
L-cci d f omwtKAt ci rP vLi tmS odc i 5Eim-Ai dvPW-vL
Eimgi Evmt d oretm-vLAPb ad blt sggc-ovntwreg uERSkD
st owglgosg t o 3fi : RbSHtCmPg Fks Eoei P 7 kb CMs
f tmm-PvtWds   s6 3 b
FbCmoAAimodc  b3-deimbyRR b 6 rvmogt dPimKov-Ki t d,
r-di oretm-vLAP1tmAl rv-groPPEmtTriAPb 1eg +tmloi C
twfi i se-og Rgi lo-ov pgngi lses    x7   7b
2 orvim f b Db  oi ri AodP odc  dvor Db  b Kod ci d
Ht PgLb 7  +b Model oei ,adci Ei dci dv  ovo,Rm-i dvi c
 moELiAi ,vt ,DLt diAi Ct dKimP-t db blt vlgnn -o
2dggse 2Loregn-nb
hbab oAEims  bf omgLodcs  b bf omPi vimPsodc  bHo -db
yRR b r-ed-de Mi vvimPodc DLt diAi P1tm3Ei i gL3Sd,
vLi P-Pb ad *-wre I2Eu . t lWnet d t o 2dggse 2Loregn-nb
a3C b
3-vv-gLo-  -oAEt  oAomds  m i etm Ft dcmows odc nomiw
3Lim-1b yRR+b  EErS-de AodS,vt ,AodS or-edAi dvP
odc L-cci d AomwtKAt ci rPvt ri vvim,vt ,ELt diAi gt d,
KimP-t dbadKR1 Dkk6A1eg Et owglgosg twreg : uuER7
blt sggc-ovntwreg fi i -o Et owglgosgs Eoei P +y  + s
ht gLi Pvims  iW tmws  Em-rb CMb
3-vv-gLo-  -oAEt  oAomds Ct r-d CLimmSs odc  m i etm
Ft dcmowb yRRkb  t -dv Emt gi PP-de odc c-Pgm-A-do,
v-Ki vmo-d-de 1tmri vvim,vt ,ELt diAi gt dKimP-t dbad blt S
sggc-ovntwuERSk, AKR1sEoei P Rx  7 sCt rl ATl Ps
. L-t s  l di b CMb
DbFt i Lds  b b . gLs odc  b f omgl b yRR b 3vov-Pv-gor
ELmoPi ,ToPi c vmodProv-t db ad blt sggc-ovn twreg Dkk
Et owglgosg twreg : uuERAKR1SHtCmPg Fs Eoei P  k
x b CMf tmm-PvtWds   s6 3 b
Db Ft i Lds Bb Bt odes  b H-mgLs Cb Corr-Pt d,Hl mgLs
f b  i cim-gt s  b Himvt rc-s Hb CtWods 2 b 3Li ds
Cbf tmodshb i dPs i vorbyRR+b f t Pi P . Ei d 3t l mgi
nt t rw-v1tm3vov-Pv-gor f ogL-di nmodProv-t db ad uERs
Kt rl Ai  xsEoei yb
 bFtA-diwodc  b2 bHrogwbyRR bMi omd-de Emt dl dg-o,
v-t d c-gv-t dom-i P rodel oei gt AEri 5-vSodc Wtmc Pi ri g,
v-t d Pvmovi e-i Pbad KR1S: uuERsEoei Py y y  b CMs
f tmm-PvtWds   s6 3 b
 b f omgLodc odc hbab  oAEimb yRRRb  f l rv-Pvmov,
i eS  EEmt ogL vt aAEmtK-de Dmt dl dg-ov-t d TS  dor,
t eSbEt Pdmri r-t oi CR-ovm-nr-snsy fiy: 7 x y7 b
 b b. gLodc Bb i SbyRR b 3SPviAov-g CtAEom-Pt d t 1
 om-t l P3vov-Pv-gor  r-edAi dvf t ci rPbEt Pdmri r-t oi C
R-ovm-nr-snsy fi7: 7  x7b
 b b. gLbyRR bf -d-Al A immtmmovi vmo-d-de -d Pvov-Pv-gor
AogL-di vmodProv-t db ad blt sggc-ovn twreg  Fnr uoS
omi Cfi ggr-ov t o uERSHtCmPgFsEoei P7 R 7 +b CMs
f tmm-PvtWds   s6 3 b
 b 3gLmt i vims  b Ct dw-i s  b 3Smcors f b Hi l vdoei rs
f b -rwos  b3vmtAs  b bF-AsBb bFodesodc  bFoE-,
rtWb yRRyb  DimPEi gv-Ki t d vLi  i 5vCLorri dei P1tm
nn3 hi Pi omgLb ad I333 DkkD . t lWnet d t o 2dggse
2Loregn-nb
nomiw3Lim-1 odc  m i etm Ft dcmowb yRR+b 3l TPvm-de,
ToPi c vmodPr-vimov-t db ad blt sggc-ovn twreg   re uoS
omi Cfi ggr-ov twreg unnt s-i r-t o t wEt Pdmri r-t oi C
R-ovm-nr-sns Eoei P      x7s Dmoel i s C i gLhi El Tr-gs
 l di b PPt g-ov-t d 1tmCtAEl vov-t dorM-del -Pv-gPb
 b3vt rgwi byRRyb3m-rA  od i 5vi dP-Tri rodel oei At ci r,
-de vt t rw-vb
DbnoSrtmb yRRxb B-cci d f omwtKf t ci rP1tm moELiAi
vt DLt diAi Ct dKimP-t db ad : -ore 3mlt dgi o Et oS
wglgosg t o 2dggse Et PPmo-si r-t o i oc 1gseotCt vLb
a3C b
Fbnt l vodtKo odc hbCbf t tmi b yRRyb Dmt dl dg-ov-t d
At ci r-de 1tm-AEmtKi c PEi rr-de gtmmi gv-t db ad blt S
sggc-ovn twreg  kre i oomi CPggr-ov twuERs Eoei P
7   7x7b
 bKod ci d Ht PgL odc 3bCod-P-l Pb yRR b aAEmtKi c
AtmELt ,ELt dt rt e-gor Pi 4l i dgi Emt gi PP-de W-vL gt d,
Pvmo-dv Pov-P1ogv-t d -d1imi dgi b ad blt sggc-ovn twreg
3-vere fi ggr-ov twreguERS2I bK4 : i rKR1S: uuERs
Eoei P 7   b
 bKod ci d Ht PgLodc 2 b oi ri AodPb7  kb  t dt v1tm,
ei v  l rrAiAtmS -d AiAtmS,ToPi c ri omd-de t 1Wtmc
Emt dl dg-ov-t db dlt sggc-ovn tw: gfi Ri d fEt : RR , s
Eoei P7 x yR b
hb  i dP odc Bb i Sb yRR b aAEmtKiAi dvP -d ELmoPi ,
ToPi c Pvov-Pv-gorAogL-di vmodProv-t db ad KR1 Et ow f
: uuERsEoei Pyx+ y  sHt Pvt dsf  s f oSb
95
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 99?106,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Comparison, Selection and Use of Sentence Alignment Algorithms for New
Language Pairs
Anil Kumar Singh
LTRC, IIIT
Gachibowli, Hyderabad
India - 500019
anil@research.iiit.net
Samar Husain
LTRC, IIIT
Gachibowli, Hyderabad
India - 500019
s amar@iiit.net
Abstract
Several algorithms are available for sen-
tence alignment, but there is a lack of
systematic evaluation and comparison of
these algorithms under different condi-
tions. In most cases, the factors which
can significantly affect the performance
of a sentence alignment algorithm have
not been considered while evaluating. We
have used a method for evaluation that
can give a better estimate about a sen-
tence alignment algorithm?s performance,
so that the best one can be selected. We
have compared four approaches using this
method. These have mostly been tried
on European language pairs. We have
evaluated manually-checked and validated
English-Hindi aligned parallel corpora un-
der different conditions. We also suggest
some guidelines on actual alignment.
1 Introduction
Aligned parallel corpora are collections of pairs of
sentences where one sentence is a translation of the
other. Sentence alignment means identifying which
sentence in the target language (TL) is a translation
of which one in the source language (SL). Such cor-
pora are useful for statistical NLP, algorithms based
on unsupervised learning, automatic creation of re-
sources, and many other applications.
Over the last fifteen years, several algorithms have
been proposed for sentence alignment. Their perfor-
mance as reported is excellent (in most cases not less
than 95%, and usually 98 to 99% and above). The
evaluation is performed in terms of precision, and
sometimes also recall. The figures are given for one
or (less frequently) more corpus sizes. While this
does give an indication of the performance of an al-
gorithm, the variation in performance under varying
conditions has not been considered in most cases.
Very little information is given about the conditions
under which evaluation was performed. This gives
the impression that the algorithm will perform with
the reported precision and recall under all condi-
tions.
We have tested several algorithms under differ-
ent conditions and our results show that the per-
formance of a sentence alignment algorithm varies
significantly, depending on the conditions of test-
ing. Based on these results, we propose a method
of evaluation that will give a better estimate of the
performance of a sentence alignment algorithm and
will allow a more meaningful comparison. Our view
is that unless this is done, it will not be possible to
pick up the best algorithm for certain set of con-
ditions. Those who want to align parallel corpora
may end up picking up a less suitable algorithm for
their purposes. We have used the proposed method
for comparing four algorithms under different con-
ditions. Finally, we also suggest some guidelines for
using these algorithms for actual alignment.
2 Sentence Alignment Methods
Sentence alignment approaches can be categorized
as based on sentence length, word correspondence,
and composite (where more than one approaches are
combined), though other techniques, such as cog-
99
nate matching (Simard et al, 1992) were also tried.
Word correspondence was used by Kay (Kay, 1991;
Kay and Roscheisen, 1993). It was based on the idea
that words which are translations of each other will
have similar distributions in the SL and TL texts.
Sentence length methods were based on the intuition
that the length of a translated sentence is likely to be
similar to that of the source sentence. Brown, Lai
and Mercer (Brown et al, 1991) used word count as
the sentence length, whereas Gale and Church (Gale
and Church, 1991) used character count. Brown, Lai
and Mercer assumed prior alignment of paragraphs.
Gale and Church relied on some previously aligned
sentences as ?anchors?. Wu (Wu, 1994) also used
lexical cues from corpus-specific bilingual lexicon
for better alignment.
Word correspondence was further developed in
IBM Model-1 (Brown et al, 1993) for statistical
machine translation. Melamed (Melamed, 1996)
also used word correspondence in a different (geo-
metric correspondence) way for sentence alignment.
Simard and Plamondon (Simard and Plamondon,
1998) used a composite method in which the first
pass does alignment at the level of characters as
in (Church, 1993) (itself based on cognate match-
ing) and the second pass uses IBM Model-1, fol-
lowing Chen (Chen, 1993). The method used by
Moore (Moore, 2002) also had two passes, the first
one being based on sentence length (word count) and
the second on IBM Model-1. Composite methods
are used so that different approaches can compli-
ment each other.
3 Factors in Performance
As stated above, the performance of a sentence
alignment algorithm depends on some identifiable
factors. We can even make predictions about
whether the performance will increase or decrease.
However, as the results given later show, the algo-
rithms don?t always behave in a predictable way. For
example, one of the algorithms did worse rather than
better on an ?easier? corpus. This variation in perfor-
mance is quite significant and it cannot be ignored
for actual alignment (table-1). Some of these factors
have been indicated in earlier papers, but these were
not taken into account while evaluating, nor were
their effects studied.
Translation of a text can be fairly literal or it can
be a recreation, with a whole range between these
two extremes. Paragraphs and/or sentences can be
dropped or added. In actual corpora, there can even
be noise (sentences which are not translations at all
and may not even be part of the actual text). This can
happen due to fact that the texts have been extracted
from some other format such as web pages. While
translating, sentences can also be merged or split.
Thus, the SL and TL corpora may differ in size.
All these factors affect the performance of an al-
gorithm in terms of, say, precision, recall and F-
measure. For example, we can expect the perfor-
mance to worsen if there is an increase in additions,
deletions, or noise. And if the texts were translated
fairly literally, statistical algorithms are likely to per-
form better. However, our results show that this does
not happen for all the algorithms.
The linguistic distance between SL and TL can
also play a role in performance. The simplest mea-
sure of this distance is in terms of the distance on
the family tree model. Other measures could be the
number of cognate words or some measure based
on syntactic features. For our purposes, it may not
be necessary to have a quantitative measure of lin-
guistic distance. The important point is that for lan-
guages that are distant, some algorithms may not
perform too well, if they rely on some closeness be-
tween languages. For example, an algorithm based
on cognates is likely to work better for English-
French or English-German than for English-Hindi,
because there are fewer cognates for English-Hindi.
It won?t be without a basis to say that Hindi is
more distant from English than is German. English
and German belong to the Indo-Germanic branch
whereas Hindi belongs to the Indo-Aryan branch.
There are many more cognates between English and
German than between English and Hindi. Similarly,
as compared to French, Hindi is also distant from
English in terms of morphology. The vibhaktis of
Hindi can adversely affect the performance of sen-
tence length (especially word count) as well as word
correspondence based algorithms. From the syntac-
tic point of view, Hindi is a comparatively free word
order language, but with a preference for the SOV
(subject-object-verb) order, whereas English is more
of a fixed word order and SVO type language. For
sentence length and IBM model-1 based sentence
100
alignment, this doesn?t matter since they don?t take
the word order into account. However, Melamed?s
algorithm (Melamed, 1996), though it allows ?non-
monotonic chains? (thus taking care of some differ-
ence in word order), is somewhat sensitive to the
word order. As Melamed states, how it will fare
with languages with more word variation than En-
glish and French is an open question.
Another aspect of the performance which may not
seem important from NLP-research point of view, is
its speed. Someone who has to use these algorithms
for actual alignment of large corpora (say, more than
1000 sentences) will have to realize the importance
of speed. Any algorithm which does worse than
O(n) is bound to create problems for large sizes. Ob-
viously, an algorithm that can align 5000 sentences
in 1 hour is preferable to the one which takes three
days, even if the latter is marginally more accurate.
Similarly, the one which takes 2 minutes for 100 sen-
tences, but 16 minutes for 200 sentences will be dif-
ficult to use for practical purposes. Actual corpora
may be as large as a million sentences. As an esti-
mate of the speed, we also give the runtimes for the
various runs of all the four algorithms tested.
Some algorithms, like those based on cognate
matching, may even be sensitive to the encoding or
notation used for the text. One of the algorithms
tested (Melamed, 1996) gave worse performance
when we used a notation called ITRANS for the
Hindi text, instead of the WX-notation.1
4 Evaluation in Previous Work
There have been attempts to systematically evaluate
and compare word alignment algorithms (Och and
Ney, 2003) but, surprisingly, there has been a lack of
such evaluation for sentence alignment algorithms.
One obvious problem is the lack of manually aligned
and checked parallel corpora.
Two cases where a systematic evaluation was per-
formed are the ARCADE project (Langlais et al,
1996) and Simard et al (Simard et al, 1992). In the
ARCADE project, six alignment systems were eval-
uated on several different text types. Simard et al
performed an evaluation on several corpus types and
1In this notation, capitalization roughly means aspiration for
consonants and longer length for vowels. In addition, ?w? rep-
resents ?t? as in French entre and ?x? means something similar
to ?d? in French de, hence the name of the notation.
corpus sizes. They, also compared the performance
of several (till then known) algorithms.
In most of the other cases, evaluation was per-
formed on only one corpus type and one corpus size.
In some cases, certain other factors were considered,
but not very systematically. In other words, there
wasn?t an attempt to study the effect of various fac-
tors described earlier on the performance. In some
cases, the size used for testing was too small. One
other detail is that size was sometimes mentioned in
terms of number of words, not number of sentences.
5 Evaluation Measures
We have used local (for each run) as well as global
(over all the runs) measures of performance of an
algorithm. These measures are:
? Precision (local and global)
? Recall (local and global)
? F-measure (local and global)
? 95% Confidence interval of F-measure (global)
? Runtime (local)
6 An Evaluation Scheme
Unless sentence alignment is correct, everything
else that uses aligned parallel corpora, such as word
alignment (for automatically creating bilingual dic-
tionaries) or statistical machine translation will be
less reliable. Therefore, it is important that the best
algorithm is selected for sentence alignment. This
requires that there should be a way to systemati-
cally evaluate and compare sentence alignment al-
gorithms.
To take into account the above mentioned factors,
we used an evaluation scheme which can give an
estimate of the performance under different condi-
tions. Under this scheme, we calculate the measures
given in the previous section along the following di-
mensions:
? Corpus type
? Corpus size
? Difference in sizes of SL and TL corpora
? Noise
101
We are also considering the corpus size as a factor
in performance because the second pass in Moore?s
algorithm is based on IBM Model-1, which needs
training. This training is provided at runtime by us-
ing the tentative alignments obtained from the first
pass (a kind of unsupervised learning). This means
that larger corpus sizes (enough training data) are
likely to make word correspondence more effective.
Even for sentence length methods, corpus size may
play a role because they are based on the distribution
of the length variable. The distribution assumption
(whether Gaussian or Poisson) is likely to be more
valid for larger corpus sizes.
The following algorithms/approaches were evalu-
ated:
? Brn: Brown?s sentence length (word count)
based method, but with Poisson distribution
? GC: Church and Gale?s sentence length (char-
acter count) based method, but with Poisson
distribution
? Mmd: Melamed?s geometric correspondence
based method
? Mre: Moore?s two-pass method (word count
plus word correspondence)
For Brn and GC we used our own implemen-
tations. For Mmd we used the GMA alignment
tool and for Mre we used Moore?s implementation.
Only 1-to-1 mappings were extracted from the out-
put for calculating precision, recall and F-measure,
since the test sets had only 1-to-1 alignments. En-
glish and Hindi stop lists and a bilingual lexicon
were also supplied to the GMA tool. The parame-
ter settings for this tool were kept the same as for
English-Malay. For Brn and GC, the search method
was based on the one used by Moore, i.e., searching
within a growing diagonal band. Using this search
method meant that no prior segmentation of the cor-
pora was needed (Moore, 2002), either in terms
of aligned paragraphs (Gale and Church, 1991), or
some aligned sentences as anchors (Brown et al,
1991).
We would have liked to study the effect of linguis-
tic distance more systematically, but we couldn?t get
equivalent manually-checked aligned parallel cor-
pora for other pairs of languages. We have to rely
on the reported results for other language pairs, but
those results, as mentioned before, do not mention
the conditions of testing which we are considering
for our evaluation and, therefore, cannot be directly
compared to our results for English-Hindi. Still, we
did an experiment on the English-French test data
(447 sentences) for the shared task in NAACL 2003
workshop on parallel texts (see table-1).
For all our experiments, the text in Hindi was in
WX-notation.
In the following sub-sections we describe the de-
tails of the data sets that were prepared to study the
variation in performance due to various factors.
6.1 Corpus Type
Three different types of corpora were used for the
same language pair (English-Hindi) and size. These
were EMILLE, ERDC and India Today. We took
2500 sentences from each of these, as this was the
size of the smallest corpus.
6.1.1 EMILLE
EMILLE corpus was constructed by the EMILLE
project (Enabling Minority Language Engineering),
Lancaster University, UK, and the Central Institute
of Indian Languages (CIIL), Mysore, India. It con-
sists of monolingual, parallel and annotated corpora
for fourteen South Asian languages. The parallel
corpus part has a text (200000 words) in English and
its translations in Hindi, Bengali, Punjabi, Gujarati
and Urdu. The text is from many different domains
like education, legal, health, social, and consumer
markets. The documents are mostly in simple, for-
mal language. The translations are quite literal and,
therefore, we expected this corpus to be the ?easiest?.
6.1.2 ERDC
The ERDC corpus was prepared by Electronic
Research and Development Centre, NOIDA, India.
It also has text in different domains but it is an un-
aligned parallel corpus. A project is going on to pre-
pare an aligned and manually checked version of this
corpus. We have used a part of it that has already
been aligned and manually checked. It was our opin-
ion that the translations in this corpus are less literal
and should be more difficult for sentence alignment
than EMILLE. We used this corpus for studying the
effect of corpus size, in addition to corpus type.
102
Table 1: Results for Various Corpus Types (Corpus Size = 2500)
Clean, Same Size Noisy, Same Size Noisy, Different Size
Type Brn GC Mmd Mre Brn GC Mmd Mre Brn GC Mmd Mre
EMILLE P 99.3 99.1 85.0 66.8 85.5 87.4 38.2 66.2 87.2 86.5 48.0 65.5
R 96.0 93.0 80.0 63.2 80.4 80.0 36.2 58.0 81.2 79.1 46.5 57.4
F 97.6 96.0 82.0 64.9 82.8 83.5 37.2 61.8 84.0 82.6 47.3 61.2
T 23 23 261 45 47 44 363 64 25 25 413 47
ERDC P 99.6 99.5 94.2 100.0 85.4 84.4 48.0 96.5 84.6 85.5 50.9 97.7
R 99.0 99.1 92.7 97.0 81.7 80.6 46.7 78.9 80.5 81.3 49.8 79.1
F 99.3 99.3 93.4 98.4 83.5 82.4 47.3 86.8 82.5 83.3 50.3 87.1
T 31 29 1024 85 92 90 2268 124 55 52 3172 101
India P 91.8 93.9 76.4 99.5 71.5 76.7 49.7 94.4 73.6 75.5 51.7 93.4
Today R 81.0 83.0 70.6 81.5 61.0 65.5 47.6 67.5 62.4 64.4 50.1 62.6
F 86.1 88.1 73.4 89.6 65.8 70.7 48.6 78.7 67.6 69.5 50.9 75.0
T 32 32 755 91 96 101 2120 159 60 68 987 134
English- P 100.0 100.0 100.0 100.0 87.4 87.5 77.2 95.2 91.2 93.3 77.7 96.6
French R 100.0 99.3 100.0 99.3 85.5 84.3 81.7 84.6 83.2 83.7 82.6 83.0
P: Precision, R: Recall, F: F-Measure, T: Runtime (seconds)
6.1.3 India Today
India Today is a magazine published in both En-
glish and Hindi. We used some parallel text col-
lected from the Internet versions of this magazine. It
consists of news reports or articles which appeared
in both languages. We expected this corpus to be the
most difficult because the translations are often more
like adaptations. They may even be rewritings of the
English reports or articles in Hindi. This corpus had
2500 sentences.
6.2 Corpus Size
To study the effect of corpus size, the sizes used
were 500, 1000, 5000 and 10000. All these data sets
were from ERDC corpus (which was expected to be
neither very easy nor very difficult).
6.3 Noise and Difference in Sizes of SL and TL
Corpora
To see the effect of noise and the difference in sizes
of SL and TL corpora, we took three cases for each
of the corpus types and sizes:
? Same size without noise
? Same size with noise
? Different size with noise
Three different data sets were prepared for each
corpus type and for each corpus size. To obtain
such data sets from the aligned, manually checked
and validated corpora, we added noise to the cor-
pora. The noise was in the form of sentences from
some other unrelated corpus. The number of such
sentences was 10% each of the corpus size in the
second case and 5% to SL and 15% to the TL in the
third case. The sentences were added at random po-
sitions in the SL and TL corpora and these positions
were recorded so that we could automatically cal-
culate precision, recall and F-measure even for data
sets with noise, as we did for other data sets. Thus,
each algorithm was tested on (3+4)(3) = 21 data sets.
7 A Limitation
One limitation of our work is that we are considering
only 1-to-1 alignments. This is partly due to prac-
tical constraints, but also because 1-to-1 alignments
are the ones that can be most easily and directly used
for linguistic analysis as well as machine learning.
Since we had to prepare a large number of data
sets of sizes up to 10000 sentences, manual check-
ing was a major constraint. We had four options.
The first was to take a raw unaligned corpus and
manually align it. This option would have allowed
consideration of 1-to-many, many-to-1, or partial
103
Table 2: Results for Various Corpus Sizes
Clean, Same Size Noisy, Same Size Noisy, Different Size
Size Brn GC Mmd Mre Brn GC Mmd Mre Brn GC Mmd Mre
500 P 99.2 99.2 93.9 99.8 75.4 78.2 57.4 94.3 83.5 87.2 45.4 92.4
R 98.8 98.8 91.8 95.0 71.0 73.4 56.8 70.0 77.0 80.8 44.8 70.8
F 99.0 99.0 92.8 97.3 73.1 75.7 57.1 80.4 80.1 83.9 45.1 80.2
T 9 9 126 14 10 10 148 13 10 10 181 14
1000 P 99.3 99.6 96.4 100.0 84.6 84.6 67.8 96.8 82.2 84.0 47.3 95.1
R 98.9 99.4 95.1 96.3 81.4 82.2 68.4 73.7 76.3 78.7 46.1 72.7
F 99.1 99.5 95.7 98.1 83.0 83.4 68.1 83.7 79.1 81.2 46.7 82.4
T 13 13 278 29 24 23 335 34 15 15 453 30
5000 P 99.8 99.8 93.2 99.9 88.5 88.6 56.1 98.5 85.9 86.6 57.6 97.8
R 99.4 99.5 91.6 98.2 83.2 83.3 54.9 86.0 81.7 81.3 56.7 86.3
F 99.6 99.7 92.4 99.1 85.7 85.9 55.4 91.8 83.7 83.9 57.2 91.7
T 54 53 3481 186 199 185 5248 274 185 174 3639 275
10000 P 99.8 99.9 93.2 100.0 88.0 88.9 59.6 98.5 86.8 88.7 57.2 98.4
R 99.4 99.6 91.4 98.6 82.9 83.7 58.9 89.9 81.3 82.8 56.2 89.2
F 99.6 99.7 92.3 99.3 85.4 86.2 59.2 94.0 84.0 85.6 56.6 94.0
T 102 96 4356 305 370 346 4477 467 345 322 4351 479
alignments. The second option was to pass the text
through an alignment tool and then manually check
the output for all kinds of alignment. The third op-
tion was to check only for 1-to-1 alignments from
this output. The fourth option was to evaluate on
much smaller sizes.
In terms of time and effort required, there is an
order of difference between the first and the second
and also between the second and the third option. It
is much easier to manually check the output of an
aligner for 1-to-1 alignments than to align a corpus
from the scratch. We couldn?t afford to use the first
two options. The fourth option was affordable, but
we decided to opt for a more thorough evaluation of
1-to-1 alignments, than for evaluation of all kinds of
alignments for smaller sizes. Thus, our starting data
sets had only 1-to-1 alignments.
In future, we might extend the evaluation to all
kinds of alignments, since the manual alignment
currently being done on ERDC corpus includes par-
tial and 1-to-2 or 2-to-1 alignments. Incidentally,
there are rarely any 2-to-1 alignments in English-
Hindi corpus since two English sentences are rarely
combined into one Hindi sentence (when translating
from English to Hindi), whereas the reverse is quite
possible.
8 Evaluation Results
The results for various corpus types are given in
table-1, for corpus sizes in table-2, and the global
measures in table-3. Among the four algorithms
tested, Moore?s (Mre) gives the best results (ex-
cept for the EMILLE corpus). This is as expected,
since Mre combines sentence length based method
with word correspondence. The results for Mmd are
the worst, but it should be noted that the results for
Mmd reported in this paper may not be the best that
can be obtained with it, because its performance de-
pends on some parameters. Perhaps with better tun-
ing for English-Hindi, it might perform better. An-
other expected outcome is that the results for GC
(character count) are better than Brn (word count).
One reason for this is that there are more of charac-
ters than words (Gale and Church, 1991).
Leaving aside the tuning aspect, the low perfor-
mance of Mmd may be due to the fact that it relies
on cognate matching, and there are fewer cognates
between Hindi and English. It might also be due to
the syntactic differences (word order) between Hindi
and English. This could, perhaps be taken care of
by increasing the maximum point dispersal thresh-
old (relaxing the linearity constraint), as suggested
by Melamed (Melamed, 1996).
104
The results of experiment on English-French
(table-1) show that Mmd performs better for this
language pair than for English-Hindi, but it still
seems to be more sensitive to noise than the other
three algorithms. Mre performed the best for
English-French too.
With respect to speed, Brn and GC are the fastest,
Mre is marginally slower, and Mmd is much slower.
The effects of the previously mentioned factors on
performance have been summarized below.
8.1 Corpus Type
Brn, GC, and Mmd performed almost equally well
for EMILLE and ERDC corpora, but not that well
for India Today. However, surprisingly, Mre per-
formed much worse for EMILLE than it did for
the other two corpora. It could be because of the
fact that the EMILLE has a lot of very short (1-3
words) sentences, and word correspondence (in the
second pass) may not be that effective for such sen-
tences. The results don?t support our assumption
that EMILLE is easier than ERDC, but India Today
does turn out to be more difficult than the other two
for all the test cases. This is understandable since
the translations in this corpus are much less literal.
8.2 Corpus Size
Only in the case of Mre, the performance almost
consistently increased with size. This is as expected
since the second pass in Mre needs training from
the results of the first pass. The corpus size has to be
large for this training to be effective. There doesn?t
seem to be a clear relationship between size and per-
formance for the other three algorithms.
8.3 Noise and Difference in Sizes of SL and TL
Corpora
As expected, introducing noise led to a decrease
in performance for all the algorithms (table-1 and
table-2). However (barring EMILLE) Mre seems to
become less sensitive to noise as the corpus size in-
creases. This again could be due to the unsupervised
learning aspect of Mre.
Making the SL and TL corpora differ in size
tended to reduce the performance in most cases, but
sometimes the performance marginally improved.
Table 3: Global Evaluation MeasuresBrn GC Mmd Mre
Clean, L 92.6 93.4 81.4 80.8
Same Size H 100.0 100.0 96.3 100.0
P 98.4 98.7 90.3 95.1
R 96.1 96.1 87.6 90.0
F 97.2 97.3 88.9 92.4
Noisy, L 73.1 75.8 44.1 72.6
Same Size H 87.5 86.4 62.4 92.3
P 82.7 84.1 53.8 92.2
R 77.4 78.4 52.8 74.9
F 79.8 81.1 53.3 82.5
Noisy, L 74.7 76.4 46.2 71.3
Different H 85.6 86.4 55.0 92.0
Size P 83.4 84.9 51.2 91.5
R 77.2 78.3 50.0 74.0
F 80.1 81.4 50.6 81.6
Overall L 81.1 82.4 55.4 80.0
H 90.4 90.8 73.1 91.0
P 88.2 89.2 65.1 92.9
R 83.6 84.3 63.5 79.6
F 85.7 86.6 64.6 85.5
L and H: Lower and higher limits of
95% confidence interval for F-measure
P, R, and F: Average precision,
recall, and F-measure
9 Some Notes on Actual Corpus Alignment
Based on the evaluation results and our experience
while manually checking alignments, we make some
observations below which could be useful to those
who are planning to create aligned parallel corpora.
Contrary to what we believed, sentence length
based algorithms turn out to be quite robust, but also
contrary to the commonly held view, there is scope
for improvement in the performance of these algo-
rithms by combining them with other techniques as
Moore has done. However, as the performance of
Mre on EMILLE shows, these additional techniques
might sometimes decrease the performance.
There is a tradeoff between precision and recall,
just as between robustness and accuracy (Simard and
Plamondon, 1998). If the corpus aligned automati-
cally is to be used without manual checking, then we
should opt for maximum precision. But if it?s going
to be manually checked before being used, then we
105
should opt for maximum recall. It depends on the
application too (Langlais et al, 1996), but if man-
ual checking is to be done, we can as well try to
get the maximum number of alignments, since some
decrease in precision is not going to make manual
checking much more difficult.
If the automatically aligned corpus is not to be
checked manually, it becomes even more important
to perform a systematic evaluation before aligning
a corpus, otherwise the parallel corpus will not be
reliable either for machine learning or for linguistic
analysis.
10 Conclusion
We used a systematic evaluation method for select-
ing a sentence alignment algorithm with English and
Hindi as the language pair. We tested four algo-
rithms for different corpus types and sizes, for the
same and different sizes of SL and TL corpora, as
well as presence and absence of noise. The evalu-
ation scheme we have described can be used for a
more meaningful comparison of sentence alignment
algorithms. The results of the evaluation show that
the performance depends on various factors. The di-
rection of this variation (increase or decrease) was as
predicted in most of the cases, but some results were
unexpected. We also presented some suggestions on
using an algorithm for actual alignment.
References
Brown Peter F., Cocke John, Della Pietra StephenA., Della Pietra Vincent J., Jelinek Frederick, Laf-ferty John D., Mercer Robert L., and Roossin Paul S.
1990. A Statistical Approach to Machine Translation.Computational Linguistics.
Brown Peter F., Della Pietra Stephen A., Della Pietra Vin-cent J., and Mercer Robert L. 1993. Mathematicsof Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Brown Peter F., Lai J. C. and Mercer Robert L. 1991.
Aligning Sentences in Parallel Corpora. Proceedingsof 29th Annual Meeting of the Association for Compu-tational Linguistics, 169?176. Berkeley, CA.
Chen Stanley F. 1993. Aligning Sentences in BilingualCorpora Using Lexical Information. Proceedings ofthe 31st Annual Meeting of the Association for Com-putational Linguistics, 9?16. Columbus, OH.
Church Kenneth W. 1993. Char align: A Program for
Aligning Parallel Texts at the Character Level. Pro-ceedings of the 31st Annual Meeting of the Associationfor Computational Linguistics, 1?8. Columbus, OH.
Church Kenneth W. and Hanks Patrick. 1993b. Aligning
Parallel Texts: Do Methods Developed for English-French Generalize to Asian Languages?. Proceedingsof Rocling.
Gale William A. and Church Kenneth W. 1991. A
Program for Aligning Sentences in Bilingual Corpora.Proceedings of 29th Annual Meeting of the Associa-tion for Computational Linguistics, 177?184. Berke-
ley, CA.
Kay Martin. 1991. Text-Translation Alignment.ACH/ALLC ?91: ?Making Connections? ConferenceHandbook. Tempe, Arizona.
Kay Martin and Roscheisen Martin. 1993. Text-
Translation Alignment. Computational Linguistics,19(1):121?142.
Langlais Phillippe, Simard Michel, and Vronis Jean.1996. Methods and Practical Issues in Evaluat-
ing Alignment Techniques. Proceedings of 16th In-ternational Conference on Computational Linguistics(COLING-96).
Melamed I. Dan. 1996. A Geometric Approach to Map-
ping Bitext Correspondence. IRCS Technical Report,University of Pennsylvania, 96?22.
Moore Robert C. 2002. Fast and Accurate SentenceAlignment of Bilingual Corpora. Proceedings ofAMTA, 135?144.
Och Franz Joseph and Ney Hermann 2003. A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19-51.
Simard Michel, Foster George F., and Isabelle Pierre.1992 Using Cognates to Align Sentences in Bilin-gual Corpora. Proceedings of the Fourth InternationalConference on Theoretical and Methodological Issuesin Machine Translation. Montreal, Canada.
Simard Michel and Plamondon Pierre. 1998 BilingualSentence Alignment: Balancing Robustness and Ac-curacy. Machine Translation, 13(1):59?80.
Wu Dekai. 1994. Aligning a Parallel English-ChineseCorpus Statistically with Lexical Criteria. Proceed-ings of 32nd Annual Meeting of the Association forComputational Linguistics, 80?87. Las Cruces, NM.
106
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
Can Corpus Based Measures be Used for Comparative Study of Languages?
Anil Kumar Singh
Language Tech. Research Centre
Int?l Inst. of Information Tech.
Hyderabad, India
anil@research.iiit.net
Harshit Surana
Language Tech. Research Centre
Int?l Inst. of Information Tech.
Hyderabad, India
surana.h@gmail.com
Abstract
Quantitative measurement of inter-language
distance is a useful technique for studying
diachronic and synchronic relations between
languages. Such measures have been used
successfully for purposes like deriving lan-
guage taxonomies and language reconstruc-
tion, but they have mostly been applied to
handcrafted word lists. Can we instead
use corpus based measures for comparative
study of languages? In this paper we try to
answer this question. We use three corpus
based measures and present the results ob-
tained from them and show how these results
relate to linguistic and historical knowledge.
We argue that the answer is yes and that such
studies can provide or validate linguistic and
computational insights.
1 Introduction
Crosslingual and multilingual processing is acquir-
ing importance in the computational linguistics
community. As a result, semi-automatic crosslin-
gual comparison of languages is also becoming
a fruitful area of study. Among the fundamen-
tal tools for crosslingual comparison are measures
of inter-language distances. In linguistics, the
study of inter-language distances, especially for lan-
guage classification, has a long history (Swadesh,
1952; Ellison and Kirby, 2006). Basically, the
work on this problem has been along linguistic,
archaeological and computational streams. Like
in other disciplines, computational methods are in-
creasingly being combined with other more conven-
tional approaches (Dyen et al, 1992; Nerbonne and
Heeringa, 1997; Kondrak, 2002; Ellison and Kirby,
2006). The work being presented in this paper be-
longs to the computational stream.
Even in the computational stream, most of the
previous work on inter-language distances had a
strong linguistic dimension. For example, most
of the quantitative measures of inter-language dis-
tance have been applied on handcrafted word
lists (Swadesh, 1952; Dyen et al, 1992). However,
with increasing use of computational techniques and
the availability of electronic data, a natural ques-
tion arises: Can languages be linguistically com-
pared based on word lists extracted from corpora.
A natural counter-question is whether such compar-
ison will be valid from linguistic and psycholinguis-
tic points of view. The aim of this paper is to exam-
ine such questions.
To calculate inter-language distances on the basis
of words in corpora, we propose two corpus based
distance measures. They internally use a more lin-
guistically grounded distance measure for compar-
ing strings. We also present the results obtained with
one purely statistical measure, just to show that even
naive corpus based measures can be useful. The
main contribution is to show that even noisy corpora
can be used for comparative study of languages. Dif-
ferent measures can give different kinds of insights.
2 Related Work
Typology or history of languages can be studied us-
ing spoken data or text. There has been work on
the former (Remmel, 1980; Kondrak, 2002), but we
40
will focus only on text. An example of a major work
on text based similarity is the paper by Kondrak and
Sherif (Kondrak and Sherif, 2006). They have evalu-
ated various phonetic similarity algorithms for align-
ing cognates. They found that learning based al-
gorithms outperform manually constructed schemes,
but only when large training data is used.
A recent work on applications of such techniques
for linguistic study is by Heeringa et al (Heeringa
et al, 2006). They performed a study on differ-
ent variations of string distance algorithms for di-
alectology and concluded that order sensitivity is
important while scaling with length is not. It may
be noted that Ellison and Kirby (Ellison and Kirby,
2006) have shown that scaling by distance does give
significantly better results. Nakleh et al (Nakleh
et al, 2005) have written about using phyloge-
netic techniques in historical linguistics as men-
tioned by Nerbonne (Nerbonne, 2005) in the review
of the book titled ?Language Classification by Num-
bers? by McMahon and McMahon (McMahon and
McMahon, 2005). All these works are about using
quantitative techniques for language typology and
classification etc.
3 Inter-Language Comparison
Inter-language comparison is more general than
measuring inter-language distance. In addition to
the overall linguistic distance, the comparison can
be of more specific characteristics like the propor-
tion of cognates derived vertically and horizontally.
Or it can be of specific phonetic features (Nerbonne,
2005; McMahon and McMahon, 2005). Quantita-
tive measures for comparing languages can first be
classified according to the form of data being com-
pared, i.e., speech, written text or electronic text.
Assuming that the text is in electronic form, the most
common measures are based on word lists. These
lists are usually prepared by linguists and they are
often in some special notation, e.g. more or less a
phonetic transcription.
The measures can be based on inter-lingual or on
intra-lingual comparison of phonetic forms (Ellison
and Kirby, 2006). They may or may not use statis-
tical techniques like measures of distributional sim-
ilarity (cross entropy, KL-divergence, etc.). These
characteristics of measures may imply some linguis-
tic or psycholinguistic assumptions. One of these is
about a common phonetic space.
4 Common Phonetic Space
Language distance can be calculated through
crosslingual as well as intra-lingual comparison.
Many earlier attempts (Nerbonne and Heeringa,
1997; Kondrak, 2002) were based on crosslingual
comparison of phonetic forms, but some researchers
have argued against the possibility of obtaining
meaningful results from crosslingual comparison of
phonetic forms. This is related to the idea of a
common phonetic space. Port and Leary (Port and
Leary, 2005) have argued against it. Ellison and
Kirby (Ellison and Kirby, 2006) argue that even if
there is a common space, language specific catego-
rization of sound often restructures this space. They
conclude that if there is no language-independent
common phonetic space with an equally common
similarity measure, there can be no principled ap-
proach to comparing forms in one language with
another. They suggest that language-internal com-
parison of forms is better and psychologically more
well-grounded.
This may be true, but should we really abandon
the approach based on crosslingual comparison? As
even Ellison and Kirby say, it is possible to argue
that there is a common phonetic space. After all,
the sounds produced by humans are determined by
human physiology. The only matter of debate is
whether common phonetic space makes sense from
the cognitive point of view. We argue that it does.
In psychology, there has been a long debate about
a similar problem which can be stated in terms of a
common chromatic space. Do humans in different
cultures see the same colors? There is still no con-
clusive answer, but many computational techniques
have been tried to solve real world problems like
classifying human faces, seemingly with the implicit
assumption that there is a common chromatic space.
Such techniques have shown some success (sheng
Chen and kai Liu, 2003).
Could it be that we are defining the notion of a
common chromatic (or phonetic) space too strictly?
Or that the way we define it is not relevant for com-
putational techniques? In our view the answer is
yes. We will give a simple, not very novel, exam-
41
ple. The phoneme t as in the English word battery is
not present in many languages of the world. When
a Thai speaker can not say battery, with the correct
t, he will say battery with t as in the French word
entre. Such substitution will be very regular. The
point is that even if phonetic space is restructured
for a particular language, we can still find which
segments or sections of two differently structured
phonetic spaces are close. Cyan may span different
ranges (on the spectrum) in different cultures, but the
ranges are likely to be near to one another. Even if
some culture has no color which can be called cyan,
one or two of the colors that it does have will be
closer to cyan than the others. The same is true
for all the other colors and also for sounds. If we
use fuzzy similarity measures to take care of such
differently structured cognitive spaces, cross-lingual
comparison may still be meaningful for certain pur-
poses. This argument is in defence of cross-lingual
comparison, not against intra-lingual comparison.
5 Common Orthographic Space
Writing systems used by languages differ very
widely. This can be taken to mean that there
is no common orthographic space for meaning-
ful crosslingual comparison of orthographic forms.
This may be true in general, but for sets of languages
using related scripts, we can assume a similar ortho-
graphic space. For example, most of the major South
Asian languages use scripts derived from Brahmi.
The similarity among these scripts is so much that
crosslingual comparison of text is possible for var-
ious purposes such as identifying cognates without
any phonetic transcription. This is in spite of the fact
that the letter shapes differ so much that they are not
mutually identifiable. Such similarity is relevant for
corpus based measures.
6 Corpus Based Measures
Since we use (non-parallel) corpora of the two lan-
guages for finding out the cognates and hence com-
paring two languages, the validity of the results de-
pends on how representative the corpora are. How-
ever, if they are of enough size, we might still be
able to make meaningful, even if limited, compar-
ison among languages. We restrict ourselves to
word list based comparison. In such a case, cor-
pus based measures can be effective if the corpora
contain a representative portion of the vocabulary,
or even of word segments. The second case (of seg-
ments) is relevant for the n-gram measure described
in section-7.
This category of measures have to incorporate
more linguistic information if they are to provide
good results. Designing such measures can be a
challenging problem as we will be mainly relying
on the corpus for our information. Knowledge about
similarities and differences of writing systems can
play an important role here. The two cognate based
measures described in sections 9 and 10 are an at-
tempt at this. But first we describe a simple n-gram
based measure.
7 Symmetric Cross Entropy (SCE)
The first measure is purely a letter n-gram based
measure similar to the one used by Singh (Singh,
2006b) for language and encoding identification. To
calculate the distance, we first prepare letter 5-gram
models from the corpora of the languages to be com-
pared. Then we combine n-grams of all orders and
rank them according to their probability in descend-
ing order. Only the top N n-grams are retained and
the rest are pruned. 1 Now we have two probability
distributions which can be compared by a measure
of distributional similarity. We have used symmetric
cross entropy as such a measure:
dsce =
?
gl=gm
(p(gl) log q(gm) + q(gm) log p(gl))
(1)
where p and q are the probability distributions for
the two languages and gl and gm are n-grams in lan-
guages l and m, respectively.
The disadvantage of this measure is that it does
not use any linguistic (e.g., phonetic) information,
but the advantage is that it can measure the similar-
ity of distributions of n-grams. Such measures have
proved to be very effective in automatically iden-
tifying languages of text, with accuracies nearing
100% for fairly small amounts of training and test
data (Adams and Resnik, 1997; Singh, 2006b).
1This is based on the results obtained by Cavnar (Cavnar and
Trenkle, 1994) and our own studies, which show that the top N
(300 according to Cavnar) n-grams have a high correlation with
the identity of the language.
42
8 Method for Cognate Identification
The other two measures are based on cognates, in-
herited as well as borrowed. Both of them use an
algorithm for identification of cognates. Many such
algorithms have been proposed. Estimates of sur-
face similarity can be used for finding cognate words
across languages for related languages. By surface
similarity we mean the orthographic, phonetic and
(possibly) morphological similarity of two words or
strings. In spite of the name, surface similarity is
deeper than string similarity as calculated by edit
distances. Ribeiro et al (Ribeiro et al, 2001) have
surveyed some of the algorithms for cognate align-
ment. However, since they studied methods based
on parallel text, we cannot use them directly.
For identifying cognates, we are using the compu-
tational model of scripts or CPMS (Singh, 2006a).
This model takes into account the characteristics of
Brahmi origin scripts and calculates surface simi-
larity in a fuzzy way. This is achieved by using
a stepped distance function (SDF) and a dynamic
programming (DP) algorithm. We have adapted the
CPMS for identifying cognates.
Different researchers have argued about the im-
portance of order sensitivity and scaling in using
string comparison algorithms (Heeringa et al, 2006;
Ellison and Kirby, 2006). The CPMS takes both
of these into account, as well as using knowledge
about the script. In general, the distance between
two strings can be defined as:
clm = fp(wl, wm) (2)
where fp is the function which calculates surface
similarity based cost between the word wl of lan-
guage l and the word wm of language m.
Those word pairs are identified as cognates which
have the least cost.
9 Cognate Coverage Distance (CCD)
The second measure used by us is a corpus based
estimate of the coverage of cognates across two lan-
guages. Cognate coverage is defined as the num-
ber of words (out of the vocabularies of the two lan-
guages) which are of the same origin. The decision
about whether two words are cognates or not is made
on the basis of surface similarity of the two words
as described in the previous section. We use (non-
parallel) corpora of the two languages for identify-
ing the cognates.
The normalized distance between two languages
is defined as:
t?lm = 1?
tlm
max(t) (3)
where tlm and tml are the number of cognates found
when comparing from language l to m and from lan-
guage m to l, respectively.
Since the CPMS based measure of surface lexical
similarity is asymmetric, we calculate the average
number of unidirectional cognates:
dccd = t
?
lm + t?ml
2 (4)
10 Phonetic Distance of Cognates (PDC)
Simply finding the coverage of cognates may in-
dicate the distance between two languages, but a
measure based solely on this information does not
take into account the variation between the cognates
themselves. To include this variation into the esti-
mate of distance, we use another measure based on
the sum of the CPMS based cost of n cognates found
between two languages:
Cpdclm =
n
?
i = 0
clm (5)
where n is the minimum of tlm for all the language
pairs compared.
The normalized distance can be defined as:
C ?lm =
Cpdclm
max(Cpdc) (6)
A symmetric version of this cost is then calcu-
lated:
dpdc =
C ?lm + C ?ml
2 (7)
11 Experimental Setup
For synchronic comparison, we selected ten lan-
guages for our experiment (table-1), mainly be-
cause sufficient corpora were available for these lan-
guages. These languages, though belonging to two
different families (Indo-Iranian and Dravidian), have
43
HI
AS
BN
OR
KN
MR
ML
TE
TA
PA
HI
AS
BN
OR
KN
MR
ML
TE
TA
PA
HI
AS
BN
OR
KN
MR
ML
TE
TA
PA
0.20
0.52 0.32
0.02
0.07 0.20
0.42
0.61
0.61
0.53
0.62
0.850.72
0.16
0.37
0.12
0.05  0.11
 0.16
0.17
 0.25
0.56
0.81
 0.31
0.17
0.45
CCD PDC
Combined
Figure 1: Graphical view of synchronic comparison among ten major South Asian languages using CCD
and PDC measures. The layout of the graph is modeled on the geographical locations of these languages.
The connections among the nodes of the graph are obtained by joining each node to its two closest neighbors
in terms of the values obtained by using the two measures.
a lot of similarities (Emeneau, 1956). The cognate
words among them are loanwords as well as inher-
ited words. In fact, the similarity among these lan-
guages is due to common origin (intra-family) as
well as contact and borrowing over thousands of
years (intra- and inter-family). Moreover, they also
use scripts derived from the same origin (Brahmi),
which allows us to use the CPMS for identifying
cognates. The corpora used for these ten languages
are all part of the CIIL (Central Institute of Indian
Languages) multilingual corpus. This corpus is a
collection of documents from different domains and
is one of best known corpora for Indian languages.
Still, the representativeness of this corpus may be a
matter of debate as it is not as large and diverse as
the BNC (British National Corpus) corpus for En-
glish.
For the cognate measures (CCD and PDC), the
only information we are extracting from the cor-
pora are the word types and their frequencies.
Thus, in a way, we are also working with word
lists, but our word lists are extracted from cor-
pora. Word lists handcrafted by linguists may be
very useful, but they are not always available for
all kinds of inter-language or inter-dialectal compar-
ison, whereas electronic corpora are more likely to
be available. Currently we are not doing any prepro-
cessing or stemming on the word lists before running
the cognate extraction algorithm. For SCE, n-gram
models are being prepared as described in section-
7. For all three measures, we calculate the distances
among all possible pairs of the languages.
For diachronic comparison, we selected modern
standard Hindi, medieval Hindi (actually, Avadhi)
and Sanskrit. The corpus for modern Hindi was the
same as that used for synchronic comparison. The
medieval Hindi we have experimented with is of two
different periods. These are the varieties used by
two great poets of that period, namely Jaayasi (1477-
1542 A.D.) and Tulsidas (1532-1623 A.D.). We took
some of their major works available in electronic
form as the corpora. For Sanskrit, we used the elec-
tronic version of Mahabharata (compiled during the
period 1000 B.C. to 500 A.D. approximately) as the
corpus. We calculate the distances among all pos-
sible pairs of the four varieties using the three mea-
sures. We also compare the ten modern languages
with Sanskrit using the same Mahabharata corpus.
For synchronic comparison, we first extract the
list of word types with frequencies from the corpus.
Then we rank them according to frequency. Top N
of these are retained. This is done because other-
wise a lot of less relevant word types like proper
nouns get included. We are interested in compar-
ing the core vocabulary of languages. The assump-
tion is that words in the core vocabulary are likely
to be more frequent. Another reason for restricting
the experiments to the top N word types is that there
44
BN HI KN ML MR OR PA TA TE
AS 0.02 0.39 0.71 0.86 0.61 0.20 0.61 0.93 0.73
0.12 0.25 0.39 0.61 0.45 0.11 0.58 0.95 0.46
0.05 0.30 0.51 0.50 0.43 0.18 0.42 0.70 0.64
BN 0.32 0.68 0.86 0.57 0.07 0.56 0.96 0.70
0.29 0.42 0.64 0.42 0.05 0.56 0.90 0.50
0.29 0.47 0.45 0.43 0.14 0.42 0.74 0.43
HI 0.61 0.81 0.42 0.40 0.20 0.93 0.61
0.17 0.56 0.16 0.27 0.16 0.87 0.38
0.43 0.46 0.16 0.33 0.20 0.74 0.34
KN 0.77 0.68 0.75 0.73 0.88 0.53
0.45 0.17 0.31 0.50 0.82 0.25
0.18 0.38 0.52 0.58 0.42 0.09
ML 0.89 0.88 0.88 0.62 0.72
0.65 0.59 0.77 0.56 0.31
0.42 0.53 0.55 0.07 0.19
MR 0.64 0.52 0.95 0.68
0.40 0.37 0.94 0.46
0.34 0.39 0.60 0.30
OR 0.63 0.98 0.74
0.45 0.89 0.44
0.65 0.83 0.64
PA 0.90 0.71
0.90 0.59
0.92 0.48
TA 0.85
0.81
0.39
Table 1: Inter-language comparison among ten ma-
jor South Asian languages using three corpus based
measures. The values have been normalized and
scaled to be somewhat comparable. Each cell con-
tains three values: by CCD, PDC and SCE.
are huge differences in sizes of corpora of different
languages. In the next step we identify the cognates
among these word lists. No language specific fea-
tures or thresholds are used. Only common thresh-
olds are used. We now branch out to using either
CCD or PDC.
The method used for diachronic comparison is
similar except that N is much smaller because the
amount of classical corpus being used (Jaayasi, Tul-
sidas) is also much smaller. Two letter codes are
used for ten languages and four varieties2.
12 Analysis of Results
The results of our experiments are shown tables 1
to 3 and figures 1 and 2. Table-1 shows the dis-
tances among pairs of languages using the three
2AS: Assamese, BN: Bengali, HI: Hindi, KN: Kannada,
ML: Malayalam, MR: Marathi, OR: Oriya, PA: Punjabi,
TA: Tamil, TE: Telugu, TL: Avadhi (Tulsidas), JY: Avadhi
(Jaayasi), MB: Sanskrit (Mahabharata)
measures. Figure-1 shows a graph showing the dis-
tances according to CCD and PDC. Figure-2 shows
the effect of the size of word lists (N ) on com-
parison for three linguistically close language pairs.
Table-2 shows the comparison of ten languages with
Sanskrit. Table-3 gives the diachronic comparison
among four historical varieties.
12.1 Synchronic Comparison
As table-1 shows, all three measures give results
which correspond well to the linguistic knowledge
about differences among these languages. Cognate
based measures give better results, but even the n-
gram based measure gives good results. However,
there are some differences among the values ob-
tained with different measures. These differences
are also in accordance with linguistic insights. For
example, the distance between Hindi and Telugu
was given as 0.61 by CCD and 0.38 by PDC. Simi-
larly, the distance between Hindi and Kannada was
given as 0.61 by CCD and 0.17 by PDC. These val-
ues, in relative terms, indicate that the number of
cognates between these languages is in the medium
range as compared to other pairs. But less PDC cost
shows that top N cognates are very similar. This
is because most cognates are tatsam words directly
borrowed from Sanskrit without any change.
The results presented in the table have been nor-
malized on all language pairs using the maximum
and minimum cost. The results would be differ-
ent and more comparable if we normalize over lan-
guage families (Indo-Iranian and Dravidian). With
such normalization, Punjabi-Oriya and Marathi-
Assamese are identified as the farthest language
pairs with costs of 0.92 and 0.90, respectively. This
corresponds well with the actual geographical and
linguistic distances.
While comparing with Sanskrit, it is clear that
different languages have different levels of cognate
coverage. However, except for Punjabi and Tamil,
all languages have very similar PDC cost with the
Mahabharata corpus. This again shows that the
closest cognates among these languages are tatsam
words. These results agree well with linguistic
knowledge, even though the Sanskrit corpus (Ma-
habharata) is highly biased.
Figure-1 makes the results clearer. It shows that
just by connecting each node to its nearest two
45
Distance AS BN HI KN ML MR OR PA TA TE
CCD 0.71 0.70 0.65 0.78 0.87 0.73 0.71 0.78 0.94 0.77
PDC 0.37 0.38 0.40 0.43 0.37 0.41 0.37 0.50 0.63 0.30
Table 2: Comparison with Sanskrit (Mahabharata)
Figure 2: Effect of the size of word lists on inter-
language comparison.
TL JY MB
HI 0.45 0.54 0.82
0.45 0.42 0.70
0.64 0.56 0.49
TL 0.01 0.84
0.02 0.72
0.16 0.91
JY 0.98
0.95
0.81
Table 3: Diachronic comparison among four histor-
ical varieties.
neighbors we can get a very good graphical repre-
sentation of the differences among languages. It also
shows that different measures capture different as-
pects. For example, CCD fails to connect Marathi
with Kannada and Kannada with Malayalam. Sim-
ilarly, PDC fails to connect Bengali with Hindi.
We get this missing information by combining the
graphs obtained with the two measures. More so-
phisticated methods for creating such graphs may
give better results. Note that the Hindi-Telugu and
Marathi-Kannada connections are valid as these lan-
guage pairs are close, even though they are not ge-
netically related. The results indicate closeness be-
tween two languages, but they do not distinguish be-
tween inheritance and borrowing.
We also experimented with several word list sizes.
In figure-2 the CCD values are plotted against word
list sizes for three close language pairs. There is
variation for Hindi-Punjabi and Malayalam-Telugu,
but not for Assamese-Bengali. The following obser-
vations can be derived from the three lines on the
plot. Malayalam-Telugu share a lot of common core
words but not less common words. Hindi-Punjabi
share a lot of less common words, but core words
are not exactly similar. Finally, Assamese-Bengali
share both core as well as less common words.
12.2 Diachronic Comparison
Table-4 shows the results. We can see that Hindi is
closer to Tulsidas than to Jaayasi by the CCD mea-
sure. PDC gives almost similar results for both. Tul-
sidas and Jaayasi are the nearest. Tulsidas is much
nearer to Mahabharata than Jaayasi, chiefly because
Tulsidas? language has more Sanskrit origin words.
Our results put Tulsidas nearest to Hindi, followed
by Jaayasi and then Sanskrit. This is historically as
well as linguistically correct.
13 Conclusions and Further Work
In this paper we first discussed the possibility and
validity of using corpus based measures for compar-
ative study of languages. We presented some ar-
guments in favor of this possibility. We then de-
scribed three corpus based measures for comparative
study of languages. The first measure was symmet-
ric cross entropy of letter n-grams. This measure
uses the least amount of linguistic information. The
second and third measures were cognate coverage
distance and phonetic distance of cognates, respec-
tively. These two are more linguistically grounded.
Using these measures, we presented a synchronic
comparison of ten major South Asian languages and
a diachronic comparison of four historical varieties.
The results of our experiments show that even these
simple measures based on crosslingual comparison
46
and on the data extracted from not very representa-
tive and noisy corpora can be used for obtaining or
validating useful linguistic insights about language
divergence, classification etc.
These measures can be tried for more languages
to see whether they have any validity for less related
languages than the languages we experimented with.
We can also try to design measures and find meth-
ods for distinguishing between borrowed and inher-
ited words. Proper combination of synchronic and
diachronic comparison might help us in doing this.
Other possible applications could be for language re-
construction, classification, dialectology etc.
Better versions of the two cognate based measures
can be defined by using the idea of confusion prob-
abilities (Ellison and Kirby, 2006) and the idea of
distributional similarity. If intra-lingual comparison
is more meaningful than inter-lingual comparison,
then these modified versions should be even more
useful for comparative study of languages.
References
Gary Adams and Philip Resnik. 1997. A language
identification application built on the Java client-server
platform. In Jill Burstein and Claudia Leacock, ed-
itors, From Research to Commercial Applications:
Making NLP Work in Practice, pages 43?47. Associa-
tion for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of SDAIR-
94, 3rd Annual Symposium on Document Analysis and
Information Retrieval, pages 161?175, Las Vegas, US.
I. Dyen, J.B. Kruskal, and P. Black. 1992. An
indo-european classification: A lexicostatistical exper-
iment. In Transactions of the American Philosophical
Society, 82:1-132.
T. Mark Ellison and Simon Kirby. 2006. Measuring lan-
guage divergence by intra-lexical comparison. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia. Association for Computational Linguistics.
M. B. Emeneau. 1956. India as a linguistic area. In
Linguistics 32:3-16.
W. Heeringa, P. Kleiweg, C. Gooskens, and J. Nerbonne.
2006. Evaluation of String Distance Algorithms for
Dialectology. In Proc. of ACL Workshop on Linguistic
Distances.
G. Kondrak and T. Sherif. 2006. Evaluation of Several
Phonetic Similarity Algorithms on the Task of Cognate
Identification. In Proc. of ACL Workshop on Linguistic
Distances.
Grzegorz Kondrak. 2002. Algorithms for language re-
construction. Ph.D. thesis. Adviser-Graeme Hirst.
April McMahon and Robert McMahon. 2005. Lan-
guage Classification by the Numbers. Oxford Univer-
sity Press, Oxford.
Luay Nakleh, Don Ringe, and Tandy Warnow. 2005.
Perfect phylogentic networks: A new methodology for
reconstructing the evolutionary history of natural lan-
guages. pages 81?2:382?420.
J. Nerbonne and W. Heeringa. 1997. Measuring dialect
distance phonetically. In Proceedings of SIGPHON-
97: 3rd Meeting of the ACL Special Interest Group in
Computational Phonology.
J. Nerbonne. 2005. Review of ?language classification
by the numbers? by april mcmahon and robert mcma-
hon.
B. Port and A. Leary. 2005. Against formal phonology.
pages 81(4):927?964.
M. Remmel. 1980. Computers in the historical phonetics
and phonology of Balto-Finnic languages: problems
and perspectives. In Communication pre?sente?e au 5th
International Finno-Ugric Congress, Turku.
A. Ribeiro, G. Dias, G. Lopes, and J. Mexia. 2001. Cog-
nates alignment. Machine Translation Summit VIII,
Machine Translation in The Information Age, pages
287?292.
Duan sheng Chen and Zheng kai Liu. 2003. A novel
approach to detect and correct highlighted face re-
gion in color image. In AVSS ?03: Proceedings of
the IEEE Conference on Advanced Video and Signal
Based Surveillance, page 7, Washington, DC, USA.
IEEE Computer Society.
Anil Kumar Singh. 2006a. A computational phonetic
model for indian language scripts. In Constraints on
Spelling Changes: Fifth International Workshop on
Writing Systems, Nijmegen, The Netherlands.
Anil Kumar Singh. 2006b. Study of some distance mea-
sures for language and encoding identification. In Pro-
ceedings of ACL 2006 Workshop on Linguistic Dis-
tance, Sydney, Australia.
M. Swadesh. 1952. Lexico-dating of prehistoric ethnic
contacts. In Proceedings of the American philosophi-
cal society, 96(4).
47
Proceedings of the Workshop on Linguistic Distances, pages 63?72,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Study of Some Distance Measures for Language and Encoding
Identication
Anil Kumar Singh
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
anil@research.iiit.net
Abstract
To determine how close two language
models (e.g., n-grams models) are, we
can use several distance measures. If we
can represent the models as distributions,
then the similarity is basically the simi-
larity of distributions. And a number of
measures are based on information theo-
retic approach. In this paper we present
some experiments on using such similar-
ity measures for an old Natural Language
Processing (NLP) problem. One of the
measures considered is perhaps a novel
one, which we have called mutual cross
entropy. Other measures are either well
known or based on well known measures,
but the results obtained with them vis-a-
vis one-another might help in gaining an
insight into how similarity measures work
in practice.
The first step in processing a text is to
identify the language and encoding of its
contents. This is a practical problem since
for many languages, there are no uni-
versally followed text encoding standards.
The method we have used in this paper
for language and encoding identification
uses pruned character n-grams, alone as
well augmented with word n-grams. This
method seems to give results comparable
to other methods.
1 Introduction
Many kinds of models in NLP can be seen as dis-
tributions of a variable. For various NLP prob-
lems, we need to calculate the similarity of such
models or distributions. One common example of
this is the n-grams model. We might have sev-
eral reference data sets and then we may want to
find out which of those matches most closely with
a test data set. The problem of language and en-
coding identification can be represented in these
terms. One of the most important questions then
is which similarity measure to use. We can expect
that the performance obtained with the similarity
measure will vary with the specific problem and
the kind of model used or some other problem spe-
cific details. Still, it will be useful to explore how
these measures relate to each other.
The measures we are going to focus on in this
paper are all very simple ones and they all try to
find the similarity of two models or distributions in
a (more or less) information theoretic way, except
the out of rank measure proposed by Cavnar and
Trenkle (Cavnar and Trenkle, 1994).
This work had started simply as an effort to
build a language and encoding identification tool
specifically for South Asian languages. During the
course of this work, we experimented with various
similarity measures and some of the results we ob-
tained were at least a bit surprising. One of the
measures we used was something we have called
mutual cross entropy and its performance for the
current problem was better than other measures.
Before the content of a Web page or of any kind
of text can be processed for computation, its lan-
guage and encoding has to be known. In many
cases this language-encoding is not known before-
hand and has to be determined automatically. For
languages like Hindi, there is no standard encod-
ing followed by everyone. There are many well
known web sites using their own proprietary en-
coding. This is one of the biggest problems in ac-
tually using the Web as a multilingual corpus and
for enabling a crawler to search the text in lan-
63
guages like Hindi. This means that the content in
these languages, limited as it is, is invisible not
just to people (which could be just due to lack of
display support or unavailability of fonts for a par-
ticular encoding) but even to crawlers.
The problem of language identification is sim-
ilar to some other problems in different fields
and the techniques used for one such problem
have been found to be effective for other prob-
lems too. Some of these problems are text cate-
gorization (Cavnar and Trenkle, 1994), cryptanal-
ysis (Beesley, 1988) and even species identifi-
cation (Dunning, 1994) from genetic sequences.
This means that if something works for one of
these problems, it is likely to work for these other
problems.
It should be noted here that the identifica-
tion problem here is that of identifying both lan-
guage and encoding. This is because (especially
for South Asian languages) the same encoding
can be used for more than one languages (ISCII
for all Indian languages which use Brahmi-origin
scripts) and one language can have many encod-
ings (ISCII, Unicode, ISFOC, typewriter, pho-
netic, and many other proprietary encodings for
Hindi).
In this paper we describe a method based
mainly on character n-grams for identifying the
language-encoding pair of a text. The method
requires some training text for each language-
encoding, but this text need not have the same con-
tent. A few pages (2500-10000 words) of text in a
particular language-encoding is enough. A pruned
character based n-grams model is created for each
language-encoding. A similar model is created for
the test data too and is compared to the training
models. The best match is found using a similar-
ity measure. A few (5-15) words of test data seems
to be enough for identification in most cases.
The method has been evaluated using various
similarity measures and for different test sizes. We
also consider two cases, one in which the pruned
character n-grams model is used alone, and the
other in which it is augmented with a word n-gram
model.
2 Previous Work
Language identification was one of the first natural
language processing (NLP) problems for which a
statistical approach was used.
Ingle (Ingle, 1976) used a list of short words
in various languages and matched the words in the
test data with this list. Such methods based on lists
of words or letters (unique strings) were meant for
human translators and couldn?t be used directly for
automatic language identification. They ignored
the text encoding, since they assumed printed text.
Even if adapted for automatic identification, they
were not very effective or scalable.
However, the earliest approaches used for au-
tomatic language identification were based on the
above idea and could be called ?translator ap-
proaches?. Newman (Newman, 1987), among oth-
ers, used lists of letters, especially accented letters
for various languages and identification was done
by matching the letters in the test data to these
lists.
Beesley?s (Beesley, 1988) automatic language
identifier for online texts was based on mathemat-
ical language models developed for breaking ci-
phers. These models basically had characteristic
letter sequences and frequencies (?orthographical
features?) for each language, making them similar
to n-grams models. The insights on which they are
based, as Beesley points out, have been known at
least since the time of Ibn ad-Duraihim who lived
in the 14th century. Beesley?s method needed 6-64
K of training data and 10-12 words of test data. It
treats language and encoding pair as one entity.
Adams and Resnik (Adams and Resnik, 1997)
describe a client-server system using Dunning?s
n-grams based algorithm (Dunning, 1994) for a
variety of tradeoffs available to NLP applications
like between the labelling accuracy and the size
and completeness of language models. Their sys-
tem dynamically adds language models. The sys-
tem uses other tools to identify the text encoding.
They use 5-grams with add-k smoothing. Training
size was 1-50 K and test size above 50 characters.
Some pruning is done, like for frequencies up to 3.
Some methods for language identification use
techniques similar to n-gram based text catego-
rization (Cavnar and Trenkle, 1994) which calcu-
lates and compares profiles of n-gram frequencies.
This is the approach nearest to ours. Such meth-
ods differ in the way they calculate the likelihood
that the test data matches with one of the profiles.
Beesley?s method simply uses word-wise proba-
bilities of ?digram? sequences by multiplying the
probabilities of sequences in the test string. Oth-
ers use some distance measure between training
and test profiles to find the best match.
64
Cavnar also mentions that top 300 or so n-grams
are almost always highly correlated with the lan-
guage, while the lower ranked n-grams give more
specific indication about the text, namely the topic.
The distance measure used by Cavnar is called
?out-of-rank? measure and it sums up the differ-
ences in rankings of the n-grams found in the test
data as compared to the training data. This is
among the measures we have tested.
The language model used by Combrinck and
Botha (Combrinck and Botha, 1994) is also based
on bigram or trigram frequencies (they call them
?transition vectors?). They select the most dis-
tinctive transition vectors by using as measure the
ratio of the maximum percentage of occurrences
to the total percentage of occurrences of a transi-
tion vector. These distinctive vectors then form the
model.
Dunning (Dunning, 1994) also used an n-grams
based method where the model selected is the one
which is most likely to have generated the test
string. Giguet (Giguet, 1995b; Giguet, 1995a) re-
lied upon grammatically correct words instead of
the most common words. He also used the knowl-
edge about the alphabet and the word morphology
via syllabation. Giguet tried this method for tag-
ging sentences in a document with the language
name, i.e., dealing with multilingual documents.
Another method (Stephen, 1993) was based on
?common words? which are characteristic of each
language. This methods assumes unique words
for each language. One major problem with this
method was that the test string might not contain
any unique words.
Cavnar?s method, combined with some heuris-
tics, was used by Kikui (Kikui, 1996) to identify
languages as well as encodings for a multilingual
text. He relied on known mappings between lan-
guages and encodings and treated East Asian lan-
guages differently from West European languages.
Kranig (Muthusamy et al, 1994) and (Simon,
2005) have reviewed and evaluated some of the
well known language identification methods. Mar-
tins and Silva (Martins and Silva, 2005) describe
a method similar to Cavnar?s but which uses a dif-
ferent similarity measure proposed by Jiang and
Conrath (Jiang and Conrath, 1997). Some heuris-
tics are also employed.
Poutsma?s (Poutsma, 2001) method is based on
Monte Carlo sampling of n-grams from the begin-
ning of the document instead of building a com-
plete model of the whole document. Sibun and
Reynar (Sibun and Reynar, 1996) use mutual in-
formation statistics or relative entropy, also called
Kullback-Leibler distance for language identifica-
tion. Souter et al(Souter et al, 1994) compared
unique character string, common word and ?tri-
graph? based approaches and found the last to be
the best.
Compression based approaches have also been
used for language identification. One example of
such an approach is called Prediction by Partial
Matching (PPM) proposed by Teahan (Teahan and
Harper, 2001). This approach uses cross entropy
of the test data with a language model and predicts
a character given the context.
3 Pruned Character N-grams
Like in Cavnar?s method, we used pruned n-grams
models of the reference or training as well as
test data. For each language-encoding pair, some
training data is provided. A character based n-
gram model is prepared from this data. N-grams
of all orders are combined and ranked according
to frequency. A certain number of them (say 1000)
with highest frequencies are retained and the rest
are dropped. This gives us the pruned charac-
ter n-grams model, which is used for language-
encoding identification.
As an attempt to increase the performance, we
also tried to augment the pruned character n-grams
model with a word n-gram model.
4 Distance Measures
Some of the measures we have experimented with
have already been mentioned in the section on pre-
vious work. The measures considered in this work
range from something as simple as log probabil-
ity difference to the one based on Jiang and Con-
rath (Jiang and Conrath, 1997) measure.
Assuming that we have two models or distribu-
tions P and Q over a variable X, the measures (sim)
are defined as below (p and q being probabilities
and r and s being ranks in models P and Q:
1. Log probability difference:
sim =
?
x
(log p(x) ? log q(x)) (1)
2. Absolute log probability difference:
sim =
?
x
(abs(log p(x)) ? abs(log q(x)))
(2)
65
3. Cross entropy:
sim =
?
x
(p(x) ? log q(x)) (3)
4. RE measure (based on relative entropy or
Kullback-Leibler distance ? see note below):
sim =
?
x
p(x) log p(x)log q(x) (4)
5. JC measure (based on Jiang and Conrath?s
measure) (Jiang and Conrath, 1997):
sim = A ? B (5)
where,
A = 2 ?
?
x
(log p(x) + log q(x)) (6)
and,
B =
?
x
log p(x) +
?
x
log q(x) (7)
6. Out of rank measure (Cavnar and Trenkle,
1994):
sim =
?
x
abs(r(x) ? s(x)) (8)
7. MRE measure (based on mutual or symmet-
ric relative entropy, the original definition of
KL-distance given by Kullback and Leibler):
sim =
?
x
p(x) log p(x)log q(x)+
?
x
q(x) log q(x)log p(x)
(9)
8. Mutual (or symmetric) cross entropy:
sim =
?
x
(p(x)?log q(x)+q(x)?log p(x))
(10)
As can be noticed, all these measures, in a way,
seem to be information theoretic in nature. How-
ever, our focus in this work is more on the pre-
senting empirical evidence rather than discussing
mathematical foundation of these measures. The
latter will of course be interesting to look into.
NOTE:
We had initiallly experimented with relative en-
tropy or KL-distance as defined below (instead of
the RE measure mentioned above):
sim =
?
x
p(x) log p(x)q(x) (11)
Another measure we tried was DL measure
(based on Dekang Lin?s measure, on which the JC
measure is based):
sim = AB (12)
where A and B are as given above.
The results for the latter measure were not very
good (below 50% in all cases) and the RE mea-
sure defined above performed better than relative
entropy. These results have not been reported in
this paper.
5 Mutual Cross Entropy
Cross entropy is a well known distance measure
used for various problems. Mutual cross entropy
can be seen as bidirectional or symmetric cross en-
tropy. It is defined simply as the sum of the cross
entropies of two distributions with each other.
Our motivation for using ?mutual? cross entropy
was that many similarity measures like cross en-
tropy and relative entropy measure how similar
one distribution is to the other. This will not neces-
sary mean the same thing as measuring how sim-
ilar two distributions are to each other. Mutual
information measures this bidirectional similarity,
but it needs joint probabilities, which means that
it can only be applied to measure similarity of
terms within one distribution. Relative entropy or
Kullback-Leibler measure is applicable, but as the
results show, it doesn?t work as well as expected.
Note that some authors treat relative entropy
and mutual information interchangeably. They are
very similar in nature except that one is applicable
for one variable in two distributions and the other
for two variables in one distribution.
Our guess was that symmetric measures may
give better results as both the models give some in-
formation about each other. This seems to be sup-
ported by the results for cross entropy, but (asym-
metric) cross entropy and RE measures also gave
good results.
6 The Algorithm
The foundation of the algorithm for identifying the
language and encoding of a text or string has al-
ready been explained earlier. Here we give a sum-
mary of the algorithm we have used. The parame-
ters for the algorithm and their values used in our
experiments reported here have also been listed.
These parameters allow the algorithm to be tuned
66
Table 1: DESCRIPTION OF DATA SETSNames Total Count
Languages Afrikaans (1), Assamese (1), Bengali (2), Bulgarian (1), Catalan (1)
Czech (1), Danish (1), Dutch (1), English (1), Esperanto (1)
Finnish (1), French (1), German (1), Gujarati (2), Hindi (8)
Icelandic (1), Iloko (1), Iroquoian (1), Italian (1), Kannada (1)
Khasi (1), Latin (1), Malayalam (1), Marathi (5), Modern Greek (1)
Nahuatl (1), Norwegian (1), Oriya (2), Polish (1), Portugues (1)
Punjabi (1), Romanian (1), Russian (1), Serbian (1), Spanish (1)
Tagalog (1), Tamil (1), Telugu (1), Welsh (1) 39
Encodings UTF8 (7), ISO-8859-1 (16), ISO-8859-2 (1), US-ASCII (4)
Windows-1251 (2), Windows-1250 (1), ISCII (10), ISFOCB (1)
ITrans (1), Shusha (1), Typewriter (1), WX (1), Gopika (1)
Govinda (1), Manjusha (1), Saamanaa (1), Subak (1)
Akruti Sarala (1), Webdunia (1) 19
Counts in parenthesis represent the extra ambiguity for that language or encoding.
For example, Hindi (8) means that 8 different encodings were tested for Hindi.
Language-Encoding Pairs: 53
Minimum training data size: 16035 characters (2495 words)
Maximum training data size: 650292 characters (102377 words)
Average training data size: 166198 characters (22643 words)
Confusable Languages: Assamese/Bengali/Oriya, Dutch/Afrikaans, Norwegian/Danish,
Spanish/Tagalog, Hindi/Marathi, Telugu/Kannada/Malayalam, Latin/Franch
Table 2: NUMBER OF TEST SETSSize Number
100 22083
200 10819
500 4091
1000 1867
2000 1524
All test data 840
or customized for best performance. Perhaps they
can even be learned by using some approach as the
EM algorithm.
1. Train the system by preparing character
based and word based (optional) n-grams
from the training data.
2. Combine n-grams of all orders (Oc for char-acters and Ow for words).
3. Sort them by rank.
4. Prune by selecting only the top Nc charac-ter n-grams and Nw word n-grams for eachlanguage-encoding pair.
5. For the given test data or string, calculate
the character n-gram based score simc withevery model for which the system has been
trained.
6. Select the t most likely language-encoding
pairs (training models) based on this charac-
ter based n-gram score.
7. For each of the t best training models, calcu-
late the score with the test model. The score
is calculated as:
score = simc + a ? simw (13)
where c and w represent character based and
word based n-grams, respectively. And a is
the weight given to the word based n-grams.
In our experiment, this weight was 1 for the
case when word n-grams were considered
and 0 when they were not.
8. Select the most likely language-encoding pair
out of the t ambiguous pairs, based on the
combined score obtained from word and
character based models.
67
Table 3: PRECISION FOR VARIOUS MEASURES AND TEST SIZESPrecision
Test Size (characters) LPD ALPD CE RE CT JC MRE MCE
100 CN 91.00 90.69 96.13 98.51 78.92 97.71 98.26 97.64
CWN 94.31 94.15 97.50 75.54 81.63 98.35 94.16 98.38
200 CN 94.46 94.37 97.72 99.35 91.24 99.05 99.24 99.05
CWN 96.52 96.52 98.85 90.54 92.79 99.21 91.13 99.39
500 CN 96.24 96.24 98.39 99.68 96.41 99.58 99.63 99.63
CWN 98.19 97.80 99.46 94.65 96.82 99.63 98.78 99.85
1000 CN 97.18 96.81 98.81 99.78 97.73 99.89 99.73 99.95
CWN 98.21 98.21 99.68 96.64 98.05 99.89 99.40 100.00
2000 CN 95.01 94.21 98.20 99.40 95.21 99.33 99.20 99.47
CWN 96.74 97.14 99.47 94.01 95.81 99.40 96.67 99.60
All available CN 82.50 88.57 98.33 99.88 94.76 99.88 99.76 100.00
test data CWN 89.88 94.64 99.88 94.76 96.55 99.88 97.86 100.00
CN: Character n-grams only, CWN: Character n-grams plus word n-grams
To summarize, the parameters in the above
method are:
1. Character based n-gram models Pc and Qc
2. Word based n-gram models Pw and Qw
3. Orders Oc and Ow of n-grams models
4. Number of retained top n-grams Nc and Nw(pruning ranks for character based and word
based n-grams, respectively)
5. Number t of character based models to be
disambiguated by word based models
6. Weight a of word based models
Parameters 3 to 6 can be used to tune the per-
formace of the identification system. The results
reported in this paper used the following values of
these parameters:
1. Oc = 4
2. Ow = 3
3. Nc = 1000
4. Nw = 500
5. t = 5
6. a = 1
There is, of course, the type of similarity score,
which can also be used to tune the performance.
Since MCE gave the best overall performance in
our experiments, we have selected it as the default
score type.
7 Implementation
The language and encoding tool has been imple-
mented as a small API in Java. This API uses an-
other API to prepare pruned character and word
n-grams which was developed as part of another
project. A graphical user interface (GUI) has also
been implemented for identifying the languages
and encodings of texts, files, or batches of files.
The GUI also allows a user to easily train the tool
for a new language-encoding pair. The tool will be
modified to work in client-server mode for docu-
ments from the Internet.
From implementation point of view, there are
some issues which can significantly affect the per-
formance of the system:
1. Whether the data should be read as text or as
a binary file.
2. The assumed encoding used for reading the
text, both for training and testing. For ex-
ample, if we read UTF8 data as ISO-8859-1,
there will be errors.
3. Whether the tranining models should be read
every time they are needed or be kept in
memory.
4. If training models are stored (even if they are
only read at the beginning and then kept in
memory), as will have to be done for practical
applications, how should they be stored: as
text or in binary files?
68
To take care of these issues, we adopted the fol-
lowing policy:
1. For preparing character based models, we
read the data as binary files and the charac-
ters are read as bytes and stored as numbers.
For word based models, the data is read as
text and the encoding is assumed to be UTF8.
This can cause errors, but it seems to be the
best (easy) option as we don?t know the ac-
tual encoding. A slightly more difficult op-
tion to implement would be to use charac-
ter based models to guess the encoding and
then build word based models using that as
the assumed encoding. The problem with this
method will be that no programming environ-
ment supports all possible encodings. Note
that since we are reading the text as bytes
rather than characters for preparing ?charac-
ter based n-grams?, technically we should say
that we are using byte based n-grams mod-
els, but since we have not tested on multi-byte
encodings, a byte in our experiments was al-
most always a character, except when the en-
coding was UTF8 and the byte represented
some meta-data like the script code. So, for
practical purposes, we can say that we are us-
ing character based n-grams.
2. Since after pruning, the size of the models
(character as well as word) is of the order of
50K, we can afford to keep the training mod-
els in memory rather than reading them every
time we have to identify the language and en-
coding of some data. This option is naturally
faster. However, for some applications where
language and encoding identification is to be
done rarely or where there is a memory con-
straint, the other option can be used.
3. It seems to be better to store the training mod-
els in binary format since we don?t know the
actual encoding and the assumed encoding
for storing may be wrong. We tried both
options and the results were worse when we
stored the models as text.
Our identification tool provides customizability
with respect to all the parameters mentioned in this
and the previous section.
8 Evaluation
Evaluation was performed for all the measures
listed earlier. These are repeated here with a code
for easy reference in table-3.
? LPD: Log probability difference
? ALPD: Absolute log probability difference
? CE: Cross entropy
? RE: RE measure based on relative entropy
? JC: JC measure (based on Jiang and Con-
rath?s measure)
? CT: Cavnar and Trenkle?s out of rank mea-
sure
? MRE: MRE measure based on mutual (sym-
metric) relative entropy
? MCE: Mutual (symmetric) cross entropy
We tested on six different sizes in terms of char-
acters, namely 100, 200, 500, 1000, 2000, and all
the available test data (which was not equal for
various language-encoding pairs). The number of
language-encoding pairs was 53 and the minimum
number of test data sets was 840 when we used
all available test data. In other cases, the number
was naturally larger as the test files were split in
fragments (see table-2).
The languages considered ranged from Es-
peranto and Modern Greek to Hindi and Telugu.
For Indian languages, especially Hindi, several en-
codings were tested. Some of the pairs had UTF8
as the encoding, but the information from UTF8
byte format was not explicitly used for identifi-
cation. The number of languages tested was 39
and number encodings was 19. Total number of
language-encoding pairs was 53 (see table-1).
The test and training data for about half of
the pairs was collected from web pages (such as
Gutenberg). For Indian languages, most (but not
all) data was from what is known as the CIIL cor-
pus.
We didn?t test on various training data sizes.
The size of the training data ranged from 2495 to
102377 words, with more on the lower side than
on the higher.
Note that we have considered the case where
both the language and the encoding are unknown,
not where one of them is known. In the latter case,
the performance can only improve. Another point
worth mentioning is that the training data was not
very clean, i.e., it had noise (such as words or sen-
tences from other languages). Error details have
been given in table-4.
69
Table 4: ERROR DETAILSLanguage-Encoding Identified As
Afrikaans::ISO-8859-1 Dutch::ISO-8859-1 (9)
Assamese::ISCII Bengali::ISCII (6), Oriya::ISCII (113)
Bengali::ISCII Hindi::ISCII (2), Oriya::ISCII (193)
Bulgarian::Windows-1251 Marathi::ISCII (6)
Catalan::ISO-8859-1 Latin::ISO-8859-1 (4)
Danish::ISO-8859-1 Norwegian::ISO-8859-1 (7)
Dutch::ISO-8859-1 Afrikaans::ISO-8859-1 (4)
English::ASCII Icelandic::UTF8 (36)
Esperanto::UTF8 Danish::ISO-8859-1 (5), Italian::ISO-8859-1 (1)
French::ISO-8859-1 Catalan::ISO-8859-1 (6)
German::ISO-8859-1 Dutch::ISO-8859-1 (4), Latin::ISO-8859-1 (3)
Hindi::ISCII English::ASCII (14), Marathi::ISCII (20)
Hindi::Isfocb Dutch::ISO-8859-1 (4), English::ASCII (6)
Hindi::Phonetic-Shusha English::ASCII (14)
Hindi::Typewriter English::ASCII (12)
Hindi::UTF8 Marathi::UTF8 (82)
Hindi::WX English::ASCII (8)
Hindi::Webdunia French::ISO-8859-1 (2), Gujarati::Gopika (9)
Icelandic::UTF8 Dutch::ISO-8859-1 (3), Latin::ISO-8859-1 (2)
Iloko::ISO-8859-1 Tagalog::ISO-8859-1 (18)
Iroquoian::ISO-8859-1 French::ISO-8859-1 (7)
Italian::ISO-8859-1 Catalan::ISO-8859-1 (2)
Kannada::ISCII Malayalam::ISCII (9)
Latin::ISO-8859-1 Catalan::ISO-8859-1 (3), Dutch::ISO-8859-1 (85)
French::ISO-8859-1 (28)
Malayalam::ISCII Tamil::ISCII (3)
Marathi::ISCII Hindi::ISCII (13)
Marathi::Manjusha English::ASCII (1)
Marathi::UTF8 Hindi::UTF8 (30)
Nahuatl::ISO-8859-1 English::ASCII (2)
Norwegian::ISO-8859-1 Danish::ISO-8859-1 (69)
Oriya::ISCII Assamese::ISCII (5), Bengali::ISCII (70), Hindi::ISCII (7)
Portugues::ISO-8859-1 Catalan::ISO-8859-1 (4)
Punjabi::ISCII Assamese::ISCII (2), Hindi::ISCII (1)
Romanian::US-ASCII Italian::ISO-8859-1 (2)
Russian::Windows-1251 Portugues::ISO-8859-1 (12)
Spanish::ISO-8859-1 Portugues::ISO-8859-1 (2), Tagalog::ISO-8859-1 (44)
Tagalog::ISO-8859-1 English::ASCII (37), Khasi::US-ASCII (15)
Telugu::ISCII Hindi::ISCII (15), Kannada::ISCII (21), Malayalam::ISCII (2)
These error were for MCE, both with and without word models for
all the test data sizes from 200 to all available data. Most of the
errors were for smaller sizes, i.e., 100 and 200 characters.
70
9 Results
The results are presented in table-3. As can be
seen almost the measures gave at least moderately
good results. The best results on the whole were
obtained with mutual cross entropy. The JC mea-
sure gave almost equally good results. Even a sim-
ple measure like log probability difference gave
surprisingly good results.
It can also be observed from table-3 that the size
of the test data is an important factor in perfor-
mance. More test data gives better results. But this
does not always happen, which too is surprising.
It means some other factors also come into play.
One of these factors seem to whether the train-
ing data for different models is of equal size or
not. Another factor seems to be noise in the data.
This seems to affect some measures more than the
others. For example, LPD gave the worst perfor-
mance when all the available test data was used.
For smaller data sets, noise is likely to get isolated
in some data sets, and therefore is less likely to
affect the results.
Using word n-grams to augment character n-
grams improved the performance in most of the
cases, but for measures like JC, RE, MRE and
MCE, there wasn?t much scope for improvement.
In fact, for smaller sizes (100 and 200 charac-
ters), word models actually reduced the perfor-
mance for these better measures. This means ei-
ther that word models are not very good for better
measures, or we have not used them in the best
possible way, even though intuitively they seem to
offer scope for improvement when character based
models don?t perform perfectly.
10 Issues and Enhancements
Although the method works very well even on lit-
tle test and training data, there are still some is-
sues and possible enhancements. One major issue
is that Web pages quite often contain text in more
than one language-encoding. An ideal language-
encoding identification tool should be able to mark
which parts of the page are in which language-
encoding.
Another possible enhancement is that in the
case of Web pages, we can also take into account
the language and encoding specified in the Web
page (HTML). Although it may not be correct for
non-standard encodings, it might still be useful for
differentiating between very close encodings like
ASCII and ISO-8859-1 which might seem identi-
cal to our tool.
If the text happens to be in Unicode, then it
might be possible to identify at least the encod-
ing (the same encoding might be used for more
than one languages, e.g., Devanagari for Hindi,
Sanskrit and Marathi) without using a statistical
method. This might be used for validating the re-
sult from the statistical method.
Since every method, even the best one, has
some limitations, it is obvious that for practical
applications we will have to combine several ap-
proaches in such a way that as much of the avail-
able information is used as possible and the var-
ious approaches complement each other. What is
left out by one approach should be taken care of by
some other approach. There will be some issues
in combining various approaches like the order in
which they have to used, their respective priorities
and their interaction (one doesn?t nullify the gains
from another).
It will be interesting to apply the same method
or its variations on text categorization or topic
identification and other related problems. The dis-
tance measures can also be tried for other prob-
lems.
11 Conclusion
We have presented the results about some dis-
tance measures which can be applied to NLP prob-
lems. We also described a method for automati-
cally identifying the language and encoding of a
text using several measures including one called
?mutual cross entropy?. All these measures are ap-
plied on character based pruned n-grams models
created from the training and the test data. There
is one such model for each of the known language-
encoding pairs. The character based models may
be augmented with word based models, which in-
creases the performance for not so good measures,
but doesn?t seem to have much effect for better
measures. Our method gives good performance on
a few words of test data and a few pages of training
data for each language-encoding pair. Out of the
measures considered, mutual cross entropy gave
the best results, but RE, MRE and JC measures
also performed almost equally well.
12 Acknowledgement
The author wishes to thank Preeti Pradhan, Nan-
dini Upasani and Anita Chaturvedi of Language
71
Technologies Research Centre, International Insti-
tute of Information Technology, Hyderabad, India
for helping in preparing the data for some of the
language-encoding pairs. The comments of re-
viewers also helped in improving the paper.
References
Gary Adams and Philip Resnik. 1997. A languageidentification application built on the Java client-server platform. In Jill Burstein and Claudia Lea-
cock, editors, From Research to Commercial Appli-cations: Making NLP Work in Practice, pages 43?47. Association for Computational Linguistics.
K. Beesley. 1988. Language identifier: A computerprogram for automatic natural-language identifica-tion on on-line text.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings ofSDAIR-94, 3rd Annual Symposium on DocumentAnalysis and Information Retrieval, pages 161?175,Las Vegas, US.
H. Combrinck and E. Botha. 1994. Automatic lan-
guage identification: Performance vs. complexity.In Proceedings of the Sixth Annual South AfricaWorkshop on Pattern Recognition.
Ted Dunning. 1994. Statistical identification of lan-guage. Technical Report CRL MCCS-94-273, Com-puting Research Lab, New Mexico State University,
March.
E. Giguet. 1995a. Categorization according to lan-
guage: A step toward combining linguistic knowl-edge and statistic learning.
Emmanuel Giguet. 1995b. Multilingual sentence cate-
gorisation according to language. In Proceedings ofthe European Chapter of the Association for Compu-tational Linguistics, SIGDAT Workshop, From Textto Tags: Issues in Multilingual Language Analysis,Dublin, Ireland.
Norman C. Ingle. 1976. A language identification ta-
ble. In The Incorporated Linguist, 15(4).
Jay J. Jiang and David W. Conrath. 1997. Semanticsimilarity based on corpus statistics and lexical tax-
onomy.
G. Kikui. 1996. Identifying the coding system andlanguage of on-line documents on the internet. InCOLING, pages 652?657.
Bruno Martins and Mario J. Silva. 2005. Languageidentification in web pages. In Proceedings of ACM-SAC-DE, the Document Engeneering Track of the20th ACM Symposium on Applied Computing.
Y. K. Muthusamy, E. Barnard, and R. A. Cole. 1994.
Reviewing automatic language identification. InIEEE Signal Processing Magazine.
Patricia Newman. 1987. Foreign language identifica-tion - first step in the translation process. In Pro-ceedings of the 28th Annual Conference of the Amer-ican Translators Association., pages 509?516.
Arjen Poutsma. 2001. Applying monte carlo tech-niques to language identification. In Proceedings ofCLIN.
P. Sibun and J. C. Reynar. 1996. Language identifi-
cation: Examining the issues. In In Proceedings ofSDAIR-96, the 5th Symposium on Document Analy-sis and Information Retrieval., pages 125?135.
Kranig Simon. 2005. Evaluation of language identifi-
cation methods. In BA Thesis. Universitt Tbingens.
C. Souter, G. Churcher, J. Hayes, J. Hughes, andS. Johnson. 1994. Natural language identificationusing corpus-based models. In Hermes Journal ofLinguistics., pages 183?203.
Johnson Stephen. 1993. Solving the problem of lan-guage recognition. In Technical Report. School ofComputer Studies, University of Leeds.
W. J. Teahan and D. J. Harper. 2001. Using compres-sion based language models for text categorization.In J. Callan, B. Croft and J. Lafferty (eds.), Work-shop on Language Modeling and Information Re-trieval., pages 83?88. ARDA, Carnegie Mellon Uni-versity.
72
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 398?404,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI Submission for the WMT?13 Quality Estimation Task: an
Experiment with n-gram Posteriors
Anil Kumar Singh
LIMSI
Orsay, France
anil@limsi.fr
Guillaume Wisniewski
Universite? Paris Sud
LIMSI
Orsay, France
wisniews@limsi.fr
Franc?ois Yvon
Universite? Paris Sud
LIMSI
Orsay, France
yvon@limsi.fr
Abstract
This paper describes the machine learning
algorithm and the features used by LIMSI
for the Quality Estimation Shared Task.
Our submission mainly aims at evaluating
the usefulness for quality estimation of n-
gram posterior probabilities that quantify
the probability for a given n-gram to be
part of the system output.
1 Introduction
The dissemination of statistical machine transla-
tion (SMT) systems in the professional translation
industry is still limited by the lack of reliability of
SMT outputs, the quality of which varies to a great
extent. In this context, a critical piece of informa-
tion would be for MT systems to assess their out-
put translations with automatically derived quality
measures. This problem is the focus of a shared
task, the aim of which is to predict the quality
of a translation without knowing any human ref-
erence(s).
To the best of our knowledge, all approaches
so far have tackled quality estimation as a super-
vised learning problem (He et al, 2010; Soricut
and Echihabi, 2010; Specia et al, 2010; Specia,
2011). A wide variety of features have been pro-
posed, most of which can be described as loosely
?linguistic? features that describe the source sen-
tence, the target sentence and the association be-
tween them (Callison-Burch et al, 2012). Sur-
prisingly enough, information used by the decoder
to choose the best translation in the search space,
such as its internal scores, have hardly been con-
sidered and never proved to be useful. Indeed, it is
well-known that these scores are hard to interpret
and to compare across hypotheses. Furthermore,
mapping scores of a linear classifier (such as the
scores estimated by MERT) into consistent prob-
abilities is a difficult task (Platt, 2000; Lin et al,
2007).
This work aims at assessing whether informa-
tion extracted from the decoder search space can
help to predict the quality of a translation. Rather
than using directly the decoder score, we propose
to consider a finer level of information, the n-gram
posterior probabilities that quantifies the probabil-
ity for a given n-gram to be part of the system
output. These probabilities can be directly inter-
preted as the confidence the system has for a given
n-gram to be part of the translation. As they are
directly derived from the number of hypotheses in
the search space that contains this n-gram, these
probabilities might be more reliable than the ones
estimated from the decoder scores.
We first quickly review, in Section 2, the n-gram
posteriors introduced by (Gispert et al, 2013) and
explain how they can be used in the QE task; we
then describe, in Section 3 the different systems
that have developed for our participation in the
WMT?13 shared task on Quality Estimation and
assess their performance in Section 4.
2 n-gram Posterior Probabilities in SMT
Our contribution to the WMT?13 shared task on
quality estimation relies on n-gram posteriors. For
the sake of completeness, we will quickly formal-
ize this notion and summarize the method pro-
posed by (Gispert et al, 2013) to efficiently com-
pute them. We will then describe preliminary ex-
periments to assess their usefulness for predicting
the quality of a translation hypothesis.
2.1 Computing n-gram Posteriors
For a given source sentence F , the n-gram pos-
terior probabilities quantifies the probability for
a given n-gram to be part of the system output.
Their computation relies on all the hypotheses
considered by a SMT system during decoding: in-
tuitively, the more hypotheses a n-gram appears
in, the more confident the system is that this n-
gram is part of the ?correct? translation, and the
398
higher its posterior probability is. Formally, the
posterior of a given n-gram u is defined as:
P (u|E) =
?
(A,E)?E
?u(E) ? P (E,A|F )
where the sum runs over the translation hypothe-
ses contained in the search space E (generally rep-
resented as a lattice); ?u(E) has the value 1 if u
occurs in the translation hypothesis E and 0 oth-
erwise and P (E,A|F ) is the probability that the
source sentence F is translated by the hypothesis
E using a derivation A. Following (Gispert et al,
2013), this probability is estimated by applying a
soft-max function to the score of the decoder:
P (A,E|F ) = exp (??H(E,A, F ))?
(A?,E?)?E exp (H(E?, A?, F ))
where the decoder score H(E,A, F ) is typically
a linear combination of a handful of features, the
weights of which are estimated by MERT (Och,
2003).
n-gram posteriors therefore aggregate two
pieces of information: first, the number of paths in
the lattice (i.e. the number of translation hypothe-
ses of the search path) the n-gram appears in; sec-
ond, the decoder scores of these paths that can be
roughly interpreted as a quality of the path.
Computing P (u|E) requires to enumerate all n-
gram contained in E and to count the number of
paths in which this n-gram appears at least once.
An efficient method to perform this computation
in a single traversal of the lattice is described
in (Gispert et al, 2013). This algorithm has been
reimplemented1 to generate the posteriors used in
this work.
2.2 Analysis of n-gram Posteriors
Figure 1 represents the distribution of n-gram pos-
teriors on the training set of the task 1-1. This dis-
tribution is similar to the ones observed for task 1-
3 and for higher n-gram orders. It appears that, the
distribution is quite irregular and has two modes.
The minor modes corresponds to n-grams that ap-
pear in almost every translation hypotheses and
have posterior probability close to 1. Further anal-
yses show that these n-grams are mainly made of
stop words and of out-of-vocabulary words. The
major mode corresponds to very small n-gram
posteriors (less than 10?1) that the system has only
1Our implementation can be downloaded from http://
perso.limsi.fr/Individu/wisniews/.
a very small confidence in producing. The num-
ber of n-grams that have such a small posterior
suggests that most n-grams occur only in a small
number of paths.
0 5 10 15 20 250
.00
0.0
5
0.1
0
0.1
5
0.2
0
? logP (u|E)
De
nsi
ty
Figure 1: Distribution of the unigram posteriors
observed on the training set of the task 1-1
Using n-gram posteriors to predict the quality
of translation raises a representation issue: the
number of n-grams contained in a sentence varies
with the sentence length (and hence with the num-
ber of posteriors) but this information needs to be
represented in a fixed-length vector describing the
sentence. Similarly to what is usually done in the
quality estimation task, we chose to represent pos-
teriors probability by their histogram: for a given
n-gram order, each posterior is mapped to a bin;
each bin is then represented by a feature equal to
the number of n-gram posteriors it contains. To
account for the irregular distribution of posteriors,
bin breaks are chosen on the training set so as to
ensure that each bin contains the same number of
examples. In our experiments, we considered a
partition of the training data into 20 bins.
3 Systems Description
LIMSI has participated to the tasks 1-1 (predic-
tion of the hTER) and 1-3 (prediction of the post-
edition time). Similar features and learning algo-
rithms have been considered for the two tasks. We
will first quickly describe them before discussing
the specific development made for task 1-3.
399
3.1 Features
In addition to the features described in the previ-
ous section, 176 ?standard? features for quality es-
timation have been considered. The full list of fea-
tures we have considered is given in (Wisniewski
et al, 2013) and the features set can be down-
loaded from our website.2 These features can be
classified into four broad categories:
? Association Features: Measures of the qual-
ity of the ?association? between the source
and the target sentences like, for instance,
features derived from the IBM model 1
scores;
? Fluency Features: Measures of the ?fluency?
or the ?grammaticality? of the target sentence
such as features based on language model
scores;
? Surface Features: Surface features extracted
mainly from the source sentence such as
the number of words, the number of out-
of-vocabulary words or words that are not
aligned;
? Syntactic Features: some simple syntactic
features like the number of nouns, modifiers,
verbs, function words, WH-words, number
words, etc., in a sentence;
These features sets differ, in several ways, from
the baseline feature set provided by the shared task
organizers. First, in addition to features derived
from a language model, it also includes several
features based on large span continuous space lan-
guage models (Le et al, 2011). Such language
models have already proved their efficiency both
for the translation task (Le et al, 2012) and the
quality estimation task (Wisniewski et al, 2013).
Second, each feature was expanded into two ?nor-
malized forms? in which their value was divided
either by the source length or the target length
and, when relevant, into a ?ratio form? in which
the feature value computed on the target sentence
is divided by its value computed in the source sen-
tence. At the end, when all possible feature expan-
sions are considered, each example is described by
395 features.
2http://perso.limsi.fr/Individu/
wisniews/
3.2 Learning Methods
The main focus of this work is to study the rel-
evance of features for quality estimation; there-
fore, only very standard learning methods were
used in our work. For this year submission
both random forests (Breiman, 2001) and elas-
tic net regression (Zou and Hastie, 2005) have
been used. The capacity of random forests to take
into account complex interactions between fea-
tures has proved to be a key element in the re-
sults achieved in our experiments with last year
campaign datasets (Zhuang et al, 2012). As we
are considering a larger features set this year and
the number of examples is comparatively quite
small, we also considered elastic regression, a lin-
ear model trained with L1 and L2 priors as regu-
larizers, hoping that training a sparse model would
reduce the risk of overfitting.
In this study, we have used the implementation
provided by scikit-learn (Pedregosa et al,
2011). As detailed in Section 4.1, cross-validation
has been used to choose the hyper-parameters of
all regressors, namely the number of estimators,
the maximal depth of a tree and the minimum
number of examples in a leaf for the random
forests and the importance of the L1 and the L2
regularizers for the elastic net regressor.
3.3 System for Task 1-3
Like task 1-1, task 1-3 is a regression task that
aims at predicting the time needed to post-edit a
translation hypothesis. From a machine learning
point of view, this task differs from task 1-1 in
three aspects. First, the distributed training set
is much smaller: it is made of only 803 exam-
ples, which increases the risk of overfitting. Sec-
ond, contrary to hTER scores, post-edition time is
not normalized and the label of this task can take
any positive value. Finally and most importantly,
as shown in Figure 2, the label distributions es-
timated on the training set has a long tail which
indicates the presence of several outliers: in the
worse case, it took more than 18 minutes to cor-
rect a single sentence made of 35 words! Such
a long post-edition time most certainly indicates
that the corrector has been distracted when post-
editing the sentence rather than a true difficulty in
the post-edition.
These outliers have a large impact on training
and on testing, as their contributions to both MAE
400
0 200 400 600 800 1000 12000.
00
0
0.0
02
0.0
04
0.0
06
0.0
08
Post-edition time (s)
De
nsi
ty
Figure 2: Kernel density estimate of the post-
edition time distribution used as label in task 1-3.
and MSE,3 directly depends on label values and
can therefore be very large in the case of outliers.
For instance, a simple ridge regression with the
baseline features provided by the shared task or-
ganizer achieves a MAE of 42.641 ? 2.126 on
the test set. When all the examples having a la-
bel higher than 300 are removed from the training
set, the MAE drops to 41.843? 4.134. When out-
liers are removed from both the training and the
test sets, the MAE further drops to 32.803?1.673.
These observations indicate that special care must
be taken when collecting the data and that, maybe,
post-edition times should be clipped to provide a
more reliable estimation of the predictor perfor-
mance.
In the following (and in our submission) only
examples for which the post-edition time was less
than 300 seconds were considered.
4 Results
4.1 Experimental Setup
We have tested different combinations of features
and learning methods using a standard metric for
regression: Mean Absolute Error (MAE) defined
by:
MAE = 1n
n?
i=1
|y?i ? yi|
3The two standard loss functions used to train and evalu-
ate a regressor
where n is the number of examples, yi and y?i
the true label and predicted label of the ith exam-
ple. MAE can be understood as the averaged error
made in predicting the quality of a translation.
Performance of both task 1-1 and task 1-34 was
also evaluated by the Spearman rank correlation
coefficient ? that assesses how well the relation-
ship between two variables can be described using
a monotonic function. While the value of the cor-
relation coefficient is harder to interpret as it not
directly related to the value to predict, it can be
used to compare the performance achieved when
predicting different measures of the post-editing
effort. Indeed, several sentence-level (or docu-
ment level) annotation types can be used to reflect
translation quality (Specia, 2011), such as the time
needed to post-edit a translation hypothesis, the
hTER, or qualitative judgments as it was the case
for the shared task of WMT 2012. Comparing di-
rectly these different settings is complicated, since
each of them requires to optimize a different loss,
and even if the losses are the same, their actual
values will depend on the actual annotation to be
predicted (refer again to the discussion in (Specia,
2011, p5)). Using a metric that relies on the pre-
dicted rank of the example rather than the actual
value predicted allows us to directly compare the
performance achieved on the two tasks.
As the labels for the different tasks were not re-
leased before the evaluation, all the reported re-
sults are obtained on an ?internal? test set, made of
20% of the data released by the shared task or-
ganizers as ?training? data. The remaining data
were used to train the regressor in a 10 folds cross-
validation setting. In order to get reliable estimate
of our methods performances, we used bootstrap
resampling (Efron and Tibshirani, 1993) to com-
pute confidence intervals of the different scores:
10 random splits of the data into a training and
sets were generated; a regressor was then trained
and tested for each of these splits and the resulting
confidence intervals at 95% computed.
4.2 Results
Table 1and Table 2 contain the results achieved by
our different conditions. We used, as a baseline,
the set of 17 features released by the shared task
organizers.
It appears that the differences in MAE between
4The Spearman ? was an official metric only for task 1-
1. For reasons explained in this paragraph, we also used it to
evaluate our results for task 1-3.
401
the different configurations are always very small
and hardly significant. However, the variation of
the Spearman ? are much larger and the difference
observed are practically significant when the inter-
pretation scale of (Landis and Koch, 1977) is used.
We will therefore mainly consider ? in our discus-
sion.
For the two tasks 1-1 and 1-3, the features we
have designed allow us to significantly improve
prediction performance in comparison to the base-
line. For instance, for task 1-1, the correlation
is almost doubled when the features described in
Section 3.1 are used. As expected, random forests
are overfitting and did not manage to outperform
a simple linear classifier. That is why we only
used the elastic net method for our official submis-
sion. Including posterior probabilities in the fea-
ture set did not improve performance much (ex-
cept when only the baseline features are consid-
ered) and sometimes even hurt performance. This
might be caused by an overfitting problem, the
training set becoming too small when new features
are added. We are conducting further experiments
to explain this paradoxical observation.
Another interesting observation that can be
made looking at the results of Table 1 and Ta-
ble 2 is that the prediction of the post-edition time
seems to be easier than the prediction of the hTER:
using the same classifiers and the same features,
the performance for the former task is always far
better than the performance for the latter.
5 Conclusion
In this paper, we described our submission to the
WMT?13 shared task on quality estimation. We
have explored the use of posteriors probability,
hoping that information about the search space
could help in predicting the quality of a transla-
tion. Even if features derived from posterior prob-
abilities have shown to have only a very limited
impact, we managed to significantly improve the
baseline with a standard learning method and sim-
ple features. Further experiments are required to
understand the reasons of this failure.
Our results also highlight the need to continue
gathering high-quality resources to train and in-
vestigate quality estimation systems: even when
considering few features, our systems were prone
to overfitting. Developing more elaborated sys-
tems will therefore only be possible if more train-
ing resource is available. Our experiments also
stress that both the choice of the quality measure
(i.e. the quantity to predict) and of the evaluation
metrics for quality estimation are still open prob-
lems.
6 Acknowledgments
This work was partly supported by ANR
projects Trace (ANR-09-CORD-023) and Tran-
sread (ANR-12-CORD-0015).
References
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
B. Efron and R. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman and Hall/CRC Mono-
graphs on Statistics and Applied Probability Series.
Chapman & Hall.
Adria` Gispert, Graeme Blackwood, Gonzalo Iglesias,
and William Byrne. 2013. N-gram posterior prob-
ability confidence measures for statistical machine
translation: an empirical study. Machine Transla-
tion, 27(2):85?114.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 622?630, Uppsala, Sweden, July.
Association for Computational Linguistics.
R. J. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
Output Layer Neural Network Language Model.
In Proceedings of IEEE International Conference
on Acoustic, Speech and Signal Processing, pages
5524?5527, Prague, Czech Republic.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
402
MAE ?
train test train test
Baseline Features
RandomForest 0.109? 0.013 0.130? 0.004 0.405? 0.008 0.314? 0.016
Elastic 0.127? 0.001 0.129? 0.003 0.336? 0.004 0.319? 0.015
?Linguistic? Features
RandomForest 0.082? 0.019 0.118? 0.003 0.689? 0.003 0.625? 0.009
Elastic 0.107? 0.004 0.115? 0.003 0.705? 0.009 0.660? 0.009
?Linguistic? Features + posteriors
RandomForest 0.088? 0.017 0.116? 0.003 0.694? 0.003 0.615? 0.014
Elastic 0.105? 0.006 0.114? 0.002 0.699? 0.007 0.662? 0.011
Table 1: Results for the task 1-1
MAE ?
train test train test
Baseline Features
RandomForest 25.145? 3.745 33.279? 1.687 0.669? 0.007 0.639? 0.017
Elastic 32.776? 0.795 33.702? 2.328 0.678? 0.006 0.657? 0.018
Baseline Features + Posteriors
RandomForest 33.707? 0.309 35.646? 0.889 0.674? 0.004 0.637? 0.017
Elastic 31.487? 0.261 32.922? 0.789 0.698? 0.004 0.681? 0.016
?Linguistic? Features
RandomForest 25.236? 4.400 33.017? 1.582 0.735? 0.007 0.666? 0.023
Elastic 28.706? 1.273 31.630? 1.612 0.760? 0.006 0.701? 0.017
?Linguistic? Features + Posteriors
RandomForest 22.951? 3.903 33.013? 1.514 0.741? 0.003 0.695? 0.013
Elastic 28.911? 1.020 31.865? 1.636 0.761? 0.008 0.710? 0.017
Table 2: Results for the task 1-3
403
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007. A note on platt?s probabilistic outputs for
support vector machines. Mach. Learn., 68(3):267?
276, October.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
John C. Platt, 2000. Probabilities for SV Machines,
pages 61?74. MIT Press.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine translation evaluation versus quality esti-
mation. Machine Translation, 24(1):39?50, March.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of the 15th conference of EAMT, pages 73?
80, Leuven, Belgium.
Guillaume Wisniewski, Anil Kumar Singh, and
Franc?ois Yvon. 2013. Quality estimation for ma-
chine translation: Some lessons learned. Machine
Translation. accepted for publication.
Yong Zhuang, Guillaume Wisniewski, and Franc?ois
Yvon. 2012. Non-linear models for confidence es-
timation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 157?162,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
404
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 66?76,
Dublin, Ireland, August 23rd 2014.
SSF: A Common Representation Scheme for Language Analysis for
Language Technology Infrastructure Development
Akshar Bharati
Akshar Bharati Group
Hyderabad
sangal@iiit.ac.in
Rajeev Sangal
IIT (BHU), Varanasi
sangal@iiit.ac.in
Dipti Sharma
IIIT, Hyderabad
dipti@iiit.ac.in
Anil Kumar Singh
IIT (BHU), Varanasi
nlprnd@gmail.com
Abstract
We describe a representation scheme and an analysis engine using that scheme, both of which
have been used to develop infrastructure for HLT. The Shakti Standard Format is a readable and
robust representation scheme for analysis frameworks and other purposes. The representation
is highly extensible. This representation scheme, based on the blackboard architectural model,
allows a very wide variety of linguistic and non-linguistic information to be stored in one place
and operated upon by any number of processing modules. We show how it has been successfully
used for building machine translation systems for several language pairs using the same architec-
ture. It has also been used for creation of language resources such as treebanks and for different
kinds of annotation interfaces. There is even a query language designed for this representation.
Easily wrappable into XML, it can be used equally well for distributed computing.
1 Introduction
Building infrastructures for human language technology is a non-trivial task. There can be numerous
issues that have to be addressed, whether linguistic or non-linguistic. Unless carefully managed, the
overall complexity can easily get out of control and seriously threaten the sustainability of the system.
This may apply to all large software systems, but the complexities associated with humans languages
(both within and across languages) only add to the problem. To make it possible to build various compo-
nents of an infrastructure that scales within and across languages for a wide variety of purposes, and to
be able to do it by re-using the representation(s) and the code, deserves to be considered an achievement.
GATE
1
(Cunningham et al., 2011; Li et al., 2009), UIMA
2
(Ferrucci and Lally, 2004; Bari et al., 2013;
Noh and Pad?o, 2013) and NLTK
3
(Bird, 2002) are well known achievements of this kind. This paper is
about one other such effort that has proved to be successful over the last decade or more.
2 Related Work
GATE is designed to be an architecture, a framework and a development environment, quite like UIMA,
although the two differ in their realization of this goal. It enables users to develop and deploy robust
language engineering components and resources. It also comes bundled with several commonly used
baseline Natural Language Processing (NLP) applications. It makes strict distinction between data, al-
gorithms, and ways of visualising them, such that algorithms + data + GUI = applications. Consequently,
it has three types of components: language resources, processing resources and visual resources.GATE
uses an annotation format with stand-off markup.
UIMA is a middleware architecture for processing unstructured information (UIM) (Ferrucci and
Lally, 2004), with special focus on NLP. Its development originated in the realization that the ability
to quickly discover each other?s results and rapidly combine different technologies and approaches ac-
celerates scientific advance. It has powerful search capabilities and a data-driven framework for the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://gate.ac.uk/
2
https://uima.apache.org/
3
http://www.nltk.org/
66
development, composition and distributed deployment of analysis engines. More than the development
of independent UIM applications, UIMA aims to enable accelerated development of integrated and ro-
bust applications, combining independent applications in diverse sub-areas of NLP, so as to accelerate
the research cycle as well as the production time. In UIMA, The original document and its analysis are
represented in a structure called the Common Analysis Structure, or CAS. Annotations in the CAS are
maintained separately from the document itself to allow greater flexibility than inline markup. There
is an XML specification for the CAS and it is possible to develop analysis engines that operate on and
output data in this XML format, which also (like GATE and NLTK) uses stand-off markup.
The Natural Language Toolkit (NLTK) is a suite of modules, data sets and tutorials (Bird, 2002). It
supports many NLP data types and can process many NLP tasks. It has a rich collection of educational
material (such as animated algorithms) for those who are learning NLP. It can also be used as a platform
for prototyping of research systems.
SSF and the Shakti Analyzer are similar to the above three but have a major difference when com-
pared with them. SSF is a ?powerful? notation for representing the NLP analysis, at all stages, whether
morphological, part-of-speech level, chunk level, or sentence level parse. The notation is so designed
that it is flexible, as well as readable. The notation can be read by human beings and can also be loaded
in memory, so that it can be used efficiently. It also allows the architecture to consist of modules which
can be configured easily under different settings. The power of the notation and the flexibility of the
resulting architecture gives enormous power to the system framework.
The readability of the format allows it to be used directly with any plain text editors, without requiring
the use of any special tools or editors. Many users prefer the data in plain text format as it allows them to
use the editors they are familiar with. Such readability and simplicity has turned out, in our experience,
to be an advantage even for experts like software developers and (computer savvy) linguists.
It would be an interesting exercise to marry SSF notation and the Shakti way of doing things with the
GATE and UIMA architecture. Our own feeling is that the resulting system/framework with a powerful
notation like SSF and the comprehensive framework like UIMA/GATE would lead to a new even more
powerful framework with a principled notation.
3 Shakti Standard Format
Shakti Standard Format (SSF) is a representation scheme (along with a corresponding format) that can be
used for most kinds of linguistically analyzed data. It allows information in a sentence to be represented
in the form of one or more trees together with a set of attribute-value pairs with nodes of the trees. The
attribute-value pairs allow features or properties to be specified with every node. Relations of different
types across nodes can also be specified using an attribute-value like representation. The representation is
specially designed to allow different levels and kinds of linguistic analyses to be stored. The developers
use APIs to store or access information regarding structure of trees and attribute-value pairs.
If a module is successful in its task, it adds a new analysis using trees and attribute values to the
representation. Thus, even though the format is fixed, it is extensible in terms of attributes or analyses.
This approach allows ready-made packages (such as, POS tagger, chunker, and parser) to be incorporated
easily using a wrapper (or a pair of converters). In order to interface such pre-existing packages to the
system, all that is required is to convert from (input) SSF to the input format required by that package and,
the output of the package to SSF. The rest of the modules of the system continue to operate seamlessly.
The format allows both in-memory representation as well as stream (or text) representation. They are
inter-convertible using a reader (stream to memory) and printer (memory to stream). The in-memory
representation is good in speed of processing, while the stream is good for portability, heterogenous
machines, and flexibility, in general.
SSF promotes the dictum: ?Simplify globally, and if unavoidable, complicate only locally.? Even if
the number of modules is large and each module does a small job, the local complexity (of individual
modules) remains under tight control for most of the modules. At worst, complexity is introduced only
locally, without affecting the global simplicity.
67
3.1 Text Level SSF
In SSF, a text or a document has a sequence of sentences with some structure such as paragraphs and
headings. It also includes meta information related to title, author, publisher, year and other information
related to the origin of the text or the document. Usually, there is also the information related to encoding,
and version number of the tagging scheme, etc. The text level SSF has two parts, header and body:
Figure 1: Document Structure in SSF
<document docid="..." docnumber="...">
<header>
...
</header>
<body>
...
</body>
The header contains meta information about the title, author, publisher, etc. as contained in the CML
(Corpus Markup Language) input
4
. The body contains sentences, each in SSF. The body of a text in SSF
contains text blocks given by the tag tb.
<body encode= ... >
<tb>
...
</tb>
...
</body>
A text block (tb) contains a sequence of sentences. Each sentence can be marked as a segment (to
indicate a heading, a partial sentence, etc.) or not a segment (to indicate a normal sentence).
3.2 Sentence Level SSF
Several formalisms have been developed for such descriptions, but the two main ones in the field of
NLP are Phrase Structure Grammar (PSG) (Chomsky, 1957) and Dependency Grammar (DG) (Tesniere,
1959). In PSG, a set of phrase structure rules are given for the grammar of a language. It is constituency
based and order of elements are a part of the grammar, and the resulting tree. DG, on the other hand, is
relational and shows relations between words or elements of a sentence. It, usually, tries to capture the
syntactico-semantic relations of the elements in a sentence. The resulting dependency tree is a tree with
nodes and edges being labelled.
The difference in the two approaches are shown below with the help of the following English example:
Ram ate the banana.
The phrase structure tree is drawn in Fig. 2 using a set of phrase structure rules. Fig. 3 shows the
dependency tree representation for this sentence. SSF can represent both these formats.
4
Thus SSF becomes a part of CML.
68
Figure 2: Phrase structure tree Figure 3: Dependency tree
Sentence level SSF is used to store the analysis of a sentence. It occurs as part of text level SSF. The
analysis of a sentence may mark any or all of the following kinds of information as appropriate: part
of speech of the words in the sentence; morphological analysis of the words including properties such
as root, gender, number, person, tense, aspect, modality; phrase-structure or dependency structure of the
sentence; and properties of units such as chunks, phrases, local word groups, bags, etc. Note that SSF
is theory neutral and allows both phrase structure as well as dependency structure to be coded, and even
mixed in well defined ways.
Though the format in SSF is fixed, it is extensible to handle new features. It also has a text represen-
tation, which makes it easy to read the output. The following example illustrates the SSF. For example,
the following English sentence,
Children are watching some programmes on television in the house. -- (1)
The representation for the above sentence is shown in SSF in Fig. 4. As shown in this figure, each line
represents a word/token or a group (except for lines with ?))? which only indicate the end of a group).
For each group, the symbol used is ?((?. Each word or group has 3 parts. The first part stores the tree
address of each word or group, and is for human readability only. The word or group is in the second
part, with part of speech tag or group/phrase category in the third part.
Address Token Category Attribute-value pairs
-----------------------------------------------
1 (( NP
1.1 children NNS <fs af=child,n,m,p,3,0,,>
))
2 (( VG
2.1 are VBP <fs af=be,v,m,p,3,0,,>
2.2 watching VBG <fs af=?watch,v,m,s,3,0,,? aspect=PROG>
))
3 (( NP
3.1 some DT <fs af=some,det,m,s,3,0,,>
3.2 programmes NNS <fs af=programme,n,m,p,3,0,,>
))
4 (( PP
4.1 on IN <fs af=on,p,m,s,3,0,,>
4.1.1 (( NP
4.1.2 television NN <fs af=television,n,m,s,3,0,,>
))
))
5 (( PP
5.1 in IN <fs af=in,p,m,s,3,0,,>
5.2 (( NP
5.2.1 the DT <fs af=the,det,m,s,3,0,,>
5.2.2 house NN <fs af=house,n,m,s,3,0,,>
))
))
-----------------------------------------------
Figure 4: Shakti Standard Format
69
The example below shows the SSF for the first noun phrase where feature information is also shown, as
the fourth part on each line. Some frequently occurring attributes (such as root, cat, gend, etc.) may be
abbreviated using a special attribute called ?af? or abbreviated attributes, as follows:
1 (( NP
1.1 children NNS <fs af=?child,n,m,p,3,0,,? >
| | | | | |
| | | | | \
root | | |pers |
| | | case
category | number
|
gender
The field for each attribute is at a fixed position, and a comma is used as a separater. Thus, in case no
value is given for a particular attribute, the field is left blank, e.g. last two fields in the above example.
Corresponding to the above SSF text stream, an in-memory data structure may be created using the
APIs. (However, note that value of the property Address is not stored in the in-memory data structure
explicitly. It is for human reference and readability only, and is computed when needed. A unique name,
however can be assigned to a node and saved in the memory, as mentioned later.)
There are two types of attributes: user defined or system defined. The convention that is used is that
a user defined attribute should not have an underscore at the end. System attribute may have a single
underscore at its end.
Values are of two types: simple and structured. Simple values are represented by alphanumeric strings,
with a possible underscore. Structured values have progressively more refined values separated by double
underscores. For example, if a value is:
vmod__varg__k1
it shows the value as ?vmod? (modifier of a verb), which is further refined as ?varg? (argument of the
verb) of type ?k1? (karta karaka).
3.3 Interlinking of Nodes
Nodes might be interlinked with each other through directed edges. Usually, these edges have nothing
to do with phrase structure tree, and are concerned with dependency structure, thematic structure, etc.
These are specified using the attribute value syntax, however, they do not specify a property for a node,
rather a relation between two nodes.
For example, if a node is karta karaka of another node named ?play1? in the dependency structure (in
other words, if there is a directed edge from the latter to the former) it can be represented as follows:
1 children NN < fs drel =
?
k1 : play1
?
>
2 played VB < fs name = play1 >
The above says that there is an edge labelled with ?k1? from ?played? to ?children? in the ?drel? tree
(dependency relation tree). The node with token ?played? is named as ?play1? using a special attribute
called ?name?.
So the syntax is as follows: if you associate an arc with a node C as follows:
<treename>=<edgelabel>:<nodename>
it means that there is an edge from < nodename > to C, and the edge is labelled with < edgelabel >.
Name of a node may be declared with the attribute ?name?:
name=<nodename>
3.4 Cross Linking across Sentences
There is a need to relate elements across sentences. A common case is that of co-reference of pronouns.
For example, in the following sentences:
Sita saw Ram in the house. He had come all by himself. -- (2)
70
the pronoun ?he? in the second sentence refers to the same person as referred to by ?Ram?. Similarly
?himself? refers to same person as ?he? refers to. This is show by means of a co-reference link from ?he?
to ?Ram?, and from ?himself? to ?he?. SSF allows such cross-links to be marked.
The above text of two sentences is shown in SSF below.
<document docid="gandhi-324" docnumber="2">
<header> ... </header>
<body>
<tb>
<sentence num=1>
...
2 Ram <fs name=R>
...
</sentence>
<sentence num=2>
1 He <fs coref="..%R" name=he>
...
6 himself <fs coref=he>
7 .
</sentence>
</tb>
Note that ?himself? in sentence 2 co-refers to ?he? in the same sentence. This is shown using attribute
?coref? and value ?he?. To show co-reference across sentences, a notation is used with ?%?. It is explained
next.
Name labels are defined at the level of a sentence: Scope of any name label is a sentence. It should be
unique within a sentence, and can be referred to within the sentence by using it directly.
To refer to a name label in another sentence in the same text block (paragraph), path has to be specified:
..%R
To refer to a name label R in a sentence in another text block numbered 3, refer to it as:
..%..%3%1%R
4 Shakti Natural Language Analyzer
Shakti Analyzer has been designed for analyzing natural languages. Originally, it was available for
analyzing English as part of the Shakti
5
English-Hindi machine translation system. It has now been
extended for analyzing a number of Indian languages as mentioned later (Section-6.1).
The Shakti Analyzer can incorporate new modules as black boxes or as open-source software. The
simplicity of the overall architecture makes it easy to do so. Different available English parsers have
been extensively adapted, and the version used by Shakti system runs using Collins parser.
Shakti analyzer combines rule-based approach with statistical approach. The SSF representation is
designed to keep both kinds of information. The rules are mostly linguistic in nature, and the statistical
approach tries to infer or use linguistic information. For example, statistical POS tagger tries to infer
linguistic (part-of-speech) tags, whereas WSD module uses grammatical relations together with statistics
to disambiguate the word sense.
The system has a number of innovative design principles which are described below.
4.1 System Organization Principles
A number of system organization principles have been used which have led to the rapid development of
the system. While the principles by themselves might not appear to be new, their application is perhaps
new.
4.1.1 Modularity
The system consists of a large number of modules, each one of which typically performs a small logical
task. This allows the overall machine translation task to be broken up into a large number of small sub-
tasks, each of which can be accomplished separately. Currently the system (as used in the Shakti system)
5
http:/shakti.iiit.ac.in
71
has 69 different modules. About 9 modules are used for analyzing the source language (English), 24
modules are used for performing bilingual tasks such as substituting target language roots and reordering
etc., and the remaining modules are used for generating target language.
4.1.2 Simplicity of Organization
The overall system architecture is kept extremely simple. All modules operate on data in SSF . They
communicate with each other via SSF.
The attribute value pairs allow features or properties to be specified with every node. Relations of
different types across nodes can also be specified using an attribute-value like representation. The repre-
sentation is specially designed to allow different levels and kinds of linguistic analyses to be stored. The
developer uses APIs to store or access information regarding structure of trees and attribute value pairs.
4.1.3 Designed to Deal with Failure
NLP analysis modules are known to have limited coverage. They are not always able to produce an out-
put. They fail to produce output either because of limits of the best known algorithms or incompleteness
of data or rules. For example, a sentential parser might fail to parse either because it does not know how
to deal with a construction or because a dictionary entry is missing. Similarly, a chunker or part of speech
tagger might fail, at times, to produce an analysis. The system is designed to deal with failure at every
step in the pipeline. This is facilitated by a common representation for the outputs of the POS tagger,
chunker and parser (all in SSF). The downstream modules continue to operate on the data stream, albeit
less effectively, when a more detailed analysis is not available. (If all modules were to fail, a default rule
of no-reordering and dictionary lookup would still be applied.)
As another example, if the word sense disambiguation (WSD) module fails to identify the sense of a
word in the input sentence, it does not put in the sense feature for the word. This only means that the
module which substitutes the target language root from the available equivalents from dictionary, will
use a default rule for selecting the sense because the detailed WSD was not successful (say, due to lack
of training data).
The SSF is designed to represent partial information, routinely. Appropriate modules know what to
do when their desired information is available and use defaults when it is not available. In fact, for many
modules, there are not just two but several levels at which they operate, depending on availability of
information corresponding to that level. Each level represents a graceful degradation of output quality.
The above flexibility is achieved by using two kinds of representation: constituent level representation
and feature-structure level representation. The former is used to store phrase level analysis (and partial
parse etc.) and the latter for outputs of many kinds of other tasks such as WSD, TAM computation, case
computation, dependency relations, etc.
4.1.4 Transparency for Developers
An extremely important characteristic for the successful development of complex software such as a ma-
chine translation system is to expose the input and output produced by every module. This transparency
becomes even more important in a research environment where new ideas are constantly being tried with
a high turnover of student developers.
In the Shakti system, unprecedented transparency is achieved by using a highly readable textual nota-
tion for the SSF, and requiring every module to produce output in this format. In fact, the textual SSF
output of a module is not only for the human consumption, but is used by the subsequent module in
the data stream as its input. This ensures that no part of the resulting analysis is left hidden in some
global variables; all analysis is represented in readable SSF (otherwise it is not processed at all by the
subsequent modules).
Experience has shown that this methodology has made debugging as well as the development of the
system convenient for programmers and linguists alike. In case an output is not as expected, one can
quickly find out which module went wrong (that is, which module did not function as expected). In fact,
linguists are using the system quite effectively to debug their linguistic data with ease.
72
5 Implementations
A considerable repository of implementations (in code) has evolved around SSF and the analyzer. In this
section we consider two of the kinds of implementations that have accumulated so far.
5.1 SSF API
Application Programming Interfaces (APIs) have been implemented in multiple programming languages
to allow programmers to transparently operate on any data stored in SSF. Of these, the better designed
APIs, such as those in Perl and Java, allow all kinds of operations to be performed on the SSF data.
These operation include basic operations such as reading, writing and modifying the data, as well as
for advanced operations such as search and bulk transformation of the data. The Java API is a part of
Sanchay
6
, which is a collection of tools and APIs for language processing, specially tailored for the
needs of Indian languages which were not (till very recently) well supported on computers and operating
systems.
The availability of decently designed APIs for SSF allow programmers to use SSF for arbitrary pur-
poses. And they have used it successfully to build natural language systems and tools as described below.
5.2 Sanchay Corpus Query Language
Trees have a quite constrained structure, whereas graphs have somewhat anarchic structure. Threaded
trees (Ait-Mokhtar et al., 2002; Larchevelque, 2002) provided a middle ground between the two. They
start with trees as the core structure, but they allow constrained links between the nodes of a tree that a
pure tree would not allow. This overlaying of constrained links over the core trees allows multiple layers
and/or types of annotation to be stored in the same structure. With a little more improvisation, we can
even have links across sentences, i.e., at the discourse level (see section-3.3). It is possible, for example,
to have a phrase structure tree (the core tree) overlaid with a dependency tree (via constrained links or
?threads?), just as it is possible to have POS tagged and chunked data to be overlaid with named entities
and discourse relations.
The Sanchay Corpus Query Language (SCQL) (Singh, 2012) is a query language designed for
threaded trees. It so turns out that SSF is also a representation that can be viewed as threaded trees.
Thus, the SCQL can work over data in SSF. This language has a simple, intuitive and concise syntax and
high expressive power. It allows not only to search for complicated patterns with short queries but also
allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks
that otherwise require writing programs, can be performed with one or more queries.
6 Applications
6.1 Sampark Machine Translation Architecture
Overcoming the language barrier in the Indian sub-continent is a very challenging task
7
. Sampark
8
is
an effort in this direction. Sampark has been developed as part of the consortium project called Indian
Language to India Language Machine translation (ILMT) funded by TDIL program of Department of In-
formation Technology, Government of India. Work on this project is contributed to by 11 major research
centres across India working on Natural Language Processing.
Sampark, or the ILMT project, has developed language technology for 9 Indian languages resulting
in MT for 18 language pairs. These are: 14 bi-directional systems between Hindi and Urdu / Punjabi /
Telugu / Bengali / Tamil / Marathi / Kannada and 4 bi-directional systems between Tamil and Malayalam
/ Telugu. Out of these, 8 pairs have been exposed via a web interface. A REST API is also available to
acess the machine translation system over the Internet.
6
http://sanchay.co.in
7
There are 22 constitutionally recognized languages in India, and many more which are not recognized. Hindi, Bengali,
Telugu, Marathi, Tamil and Urdu are among the major languages of the world in terms of number of speakers, summing up to
a total of 850 million.
8
http://sampark.org.in
73
The Sampark system uses Computational Paninian Grammar (CPG) (Bharati et al., 1995), in combina-
tion with machine learning. Thus, it is a hybrid system using both rule-based and statistical approaches.
There are 13 major modules that together form a hybrid system. The machine translation system is based
on the analyze-transfer-generate paradigm. It starts with an analysis of the source language sentence.
Then a transfer of structure and vocabulary to target language is carried out. Finally the target language
is generated. One of the benefits of this approach is that the language analyzer for a particular language
can be developed once and then be combined with generators for other languages, making it easier to
build a machine translation system for new pairs of languages.
Indian languages have a lot of similarities in grammatical structures, so only shallow parsing was found
to be adequate for the purposes of building a machine translation system. Transfer grammar component
has also been kept simple. Domain dictionaries are used to cover domain specific aspects.
At the core of the Sampark architecture is an enhanced version of the Shakti Natural Language Ana-
lyzer. The individual modules may, of course, be different for different language pairs, but the pipelined
architecture bears close resemblance to the Shakti machine translation system. And it uses the Shakti
Standard Format as the blackboard (Erman et al., 1980) on which the different modules (POS taggers,
chunkers, named entity recognzier, transfer grammar module etc.) operate, that is, read from and write
to. SSF thus becomes the glue that ties together all the modules in all the MT systems for the various
language pairs. The modules are not only written in different programming languages, some of them are
rule-based, whereas others are statistical.
The use of SSF as the underlying default representation helps to control the complexity of the overall
system. It also helps to achieve unprecedented transparency for input and output for every module.
Readability of SSF helps in development and debugging because the input and output of any module
can be easily seen and read by humans, whether linguists or programmers. Even if a module fails, SSF
helps to run the modules without any effect on normal operation of system. In such a case, the output
SSF would have unfilled value of an attribute and downstream modules continue to operate on the data
stream.
6.2 Annotation Interfaces and Other Tools
Sanchay, mentioned above, has a syntactic annotation interface that has been used for development of
treebanks for Indian languages (Begum et al., 2008). These treebanks have been one of the primary
sources of information for the development the Sampark machine translation systems, among other
things. This syntactic annotation interface provides facilities for everything that is required to be done
to transform the selected data in the raw text format to the final annotated treebank. The usual stages of
annotation include POS tagging, morphological annotation, chunking and dependency annotation. This
interface has evolved over a period of several years based on the feedback received from the annotators
and other users. There are plans to use the interface for similar annotation for even more languages.
The underlying default format used in the above interface is SSF. The advantages of using SSF for this
purpose are similar to those mentioned earlier for purposes such as building machine translation systems.
The complete process of annotation required to build a full-fledged treebank is complicated and there are
numerous issues that have to be taken care of. The blackboard-like nature of SSF allows for a smooth
shifts between different stages of annotation, even going back to an earlier stage, if necessary, to correct
mistakes. It allows all the annotation information to be situated in one contiguous place.
The interface uses the Java API for SSF, which is perhaps the most developed among the different
APIs for SSF. The API (a part of Sanchay) again allows transparency for the programmer as far as
manipulating the data is concerned. It also ensures that there are fewer bugs when new programmers
work on any part of the system where SSF data is being used. One recent addition to the interface was a
GUI to correct mistakes in treebanks (Agarwal et al., 2012).
The syntactic annotation interface is not the only interface in Sanchay that uses SSF. Some other
interfaces do that too. For example, there are sentence alignment and word alignment interfaces, which
also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF
using the Sanchay interfaces.
74
Then there are other tools in Sanchay such as the integrated tool for accessing language re-
sources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those
in SSF, to be accessed, searched and manipulated through the inter-connected annotation interfaces and
the SSF API. There is also a text editor in Sanchay that is specially tailored for Indian languages and it
can validate SSF (Singh, 2008).
The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can
be used for data in SSF is another big facilitator for anyone who wants to build new tools for language
processing and wants to operate on linguistic data.
Apart from these, a number of research projects have used SSF (the representation or the analyzer)
directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati
et al., 2009; Gadde et al., 2010; Husain et al., 2011).
7 Conclusion
We described a readable representation scheme called Shakti Standard Format (SSF). We showed how
this scheme (an instance of the blackboard architectural model), which is based on certain organizational
principles such as modularity, simplicity, robustness and transparency, can be used to create not only
a linguistic analysis engine (Shakti Natural Language Analyzer), but can be used for arbitrary other
purposes wherever linguistic analysis is one of the tasks. We briefly described the machine translation
systems (Shakti and Sampark) which use this scheme at their core level. Similarly, we described how
it can be used for creation of language resources (such as treebanks) and the annotation interfaces used
to create these resources. It has also figured in several research projects so far. We mentioned one
query language (Sanchay Corpus Query Language) that operates on this representation scheme and has
been integrated with the annotation interfaces. Overall, the representation scheme has been successful at
building infrastructure for language technology over the last more than a decade. The scheme is theory
neutral and can be used for both phrase structure grammar and for dependency grammar.
References
Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar Singh. 2012. A GUI to Detect and Correct Errors in Hindi
Dependency Treebank. In Proceedings of the Eighth International Conference on Language Resources and
Evaluation (LREC), Instanbul, Turkey. ELRA.
S. Ait-Mokhtar, J.P. Chanod, and C. Roux. 2002. Robustness beyond shallowness: incremental deep parsing.
Natural Language Engineering, 8(2-3):121144, January.
Alessandro Di Bari, Alessandro Faraotti, Carmela Gambardella, and Guido Vetere. 2013. A Model-driven ap-
proach to NLP programming with UIMA. In UIMA@GSCL, pages 2?9.
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. Depen-
dency Annotation Scheme for Indian Languages. In Proceedings of The Third International Joint Conference
on Natural Language Processing (IJCNLP), Hyderabad, India.
Ashkar Bharati, Vineet Chaitanya, and Rajeev Sangal. 1995. Natural Language Processing: A Paninian Perspec-
tive. Prentice-Hall of India Pvt. Ltd.
Akshar Bharati, Samar Husain, Phani Gadde, Bharat Ambati, Dipti M Sharma, and Rajeev Sangal. 2009. A
Modular Cascaded Approach to Complete Parsing. In Proceedings of the COLIPS International Conference on
Asian Language Processing 2009 (IALP), Singapore.
Steven Bird. 2002. NLTK: The Natural Language Toolkit. In In Proceedings of the ACL Workshop on Effective
Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadel-
phia: Association for Computational Linguistics.
Noam Chomsky. 1957. Syntactic Structures. The Hague/Paris: Mouton.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, Genevieve
Gorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood, Horacio Sag-
gion, Johann Petrak, Yaoyong Li, and Wim Peters. 2011. Text Processing with GATE (Version 6).
75
Lee D. Erman, Frederick Hayes-Roth, Victor R. Lesser, and D. Raj Reddy. 1980. The Hearsay-II Speech-
Understanding System: Integrating Knowledge to Resolve Uncertainty. ACM Comput. Surv., 12(2):213?253,
June.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural approach to unstructured information processing in the
corporate research environment. Natural Language Engineering, 10(3-4):327?348.
Phani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, and Rajeev Sangal. 2010. Improving Data
Driven Dependency Parsing using Clausal Information. In Proceedings of 11th Annual Conference of the North
American Chapter of the Association for Computational Linguistics (NAACL-HLT), Los Angeles.
Samar Husain, Phani Gadde, Joakim Nivre, and Rajeev Sangal. 2011. Clausal Parsing Helps Data-driven De-
pendency Parsing: Experiments with Hindi. In Proceedings of Fifth International Joint Conference on Natural
Language Processing (IJCNLP), Thailand.
J.M. Larchevelque. 2002. Optimal Incremental Parsing. ACM Transactions on Programing Languages and
Systems, 17(1):115, January.
Yaoyong Li, Kalina Bontcheva, and Hamish Cunningham. 2009. Adapting SVM for Data Sparseness and Imbal-
ance: A Case Study on Information Extraction. Natural Language Engineering, 15(2):241?271.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using UIMA to Structure An Open Platform for Textual Entailment. In
UIMA@GSCL, pages 26?33.
Anil Kumar Singh and Bharat Ambati. 2010. An Integrated Digital Tool for Accessing Language Resources. In
Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC), Malta.
ELRA.
Anil Kumar Singh. 2008. A Mechanism to Provide Language-Encoding Support and an NLP Friendly Editor. In
Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP), Hyder-
abad, India. AFNLP.
Anil Kumar Singh. 2012. A Concise Query Language with Search and Transform Operations for Corpora with
Multiple Levels of Annotation. In Proceedings of the Eighth International Conference on Language Resources
and Evaluation (LREC), Instanbul, Turkey. ELRA.
L. Tesniere. 1959. Elements de syntaxe structurale. Paris: Klincksieck.
76
