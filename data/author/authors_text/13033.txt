Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1024?1033, Prague, June 2007. c?2007 Association for Computational Linguistics
A Topic Model for Word Sense Disambiguation
Jordan Boyd-Graber
Computer Science
Princeton University
Princeton, NJ 08540
jbg@princeton.edu
David Blei
Computer Science
Princeton University
Princeton, NJ 08540
blei@cs.princeton.edu
Xiaojin Zhu
Computer Science
University of Wisconsin
Madison, WI 53706
jerryzhu@cs.wisc.edu
Abstract
We develop latent Dirichlet alocation with
WORDNET (LDAWN), an unsupervised
probabilistic topic model that includes word
sense as a hidden variable. We develop a
probabilistic posterior inference algorithm
for simultaneously disambiguating a corpus
and learning the domains in which to con-
sider each word. Using the WORDNET hi-
erarchy, we embed the construction of Ab-
ney and Light (1999) in the topic model and
show that automatically learned domains
improve WSD accuracy compared to alter-
native contexts.
1 Introduction
Word sense disambiguation (WSD) is the task of
determining the meaning of an ambiguous word in
its context. It is an important problem in natural
language processing (NLP) because effective WSD
can improve systems for tasks such as information
retrieval, machine translation, and summarization.
In this paper, we develop latent Dirichlet aloca-
tion with WORDNET (LDAWN), a generative prob-
abilistic topic model for WSD where the sense of
the word is a hidden random variable that is inferred
from data.
There are two central advantages to this approach.
First, with LDAWN we automatically learn the con-
text in which a word is disambiguated. Rather
than disambiguating at the sentence-level or the
document-level, our model uses the other words that
share the same hidden topic across many documents.
Second, LDAWN is a fully-fledged generative
model. Generative models are modular and can be
easily combined and composed to form more com-
plicated models. (As a canonical example, the ubiq-
uitous hidden Markov model is a series of mixture
models chained together.) Thus, developing a gen-
erative model for WSD gives other generative NLP
algorithms a natural way to take advantage of the
hidden senses of words.
In general, topic models are statistical models of
text that posit a hidden space of topics in which the
corpus is embedded (Blei et al, 2003). Given a
corpus, posterior inference in topic models amounts
to automatically discovering the underlying themes
that permeate the collection. Topic models have re-
cently been applied to information retrieval (Wei and
Croft, 2006), text classification (Blei et al, 2003),
and dialogue segmentation (Purver et al, 2006).
While topic models capture the polysemous use
of words, they do not carry the explicit notion of
sense that is necessary for WSD. LDAWN extends
the topic modeling framework to include a hidden
meaning in the word generation process. In this
case, posterior inference discovers both the topics
of the corpus and the meanings assigned to each of
its words.
After introducing a disambiguation scheme based
on probabilistic walks over the WORDNET hierar-
chy (Section 2), we embed the WORDNET-WALK
in a topic model, where each topic is associated with
walks that prefer different neighborhoods of WORD-
NET (Section 2.1). Then, we describe a Gibbs sam-
pling algorithm for approximate posterior inference
that learns the senses and topics that best explain a
corpus (Section 3). Finally, we evaluate our system
on real-world WSD data, discuss the properties of
the topics and disambiguation accuracy results, and
draw connections to other WSD algorithms from the
research literature.
1024
1740
entity 1930
3122object
20846
15024
animal 1304946
1305277
artifact male
2354808 2354559
foalcolt
3042424
colt
4040311
revolver
Synset ID
Word
six-gun
six-shooter
0.00 0.25
0.58
0.00 0.04
0.02 0.010.16
0.05
0.04
0.690.00
0.00
0.381.000.42 0.00
0.000.57
1.00
0.38
0.07
Figure 1: The possible paths to reach the word ?colt?
in WORDNET. Dashed lines represent omitted links.
All words in the synset containing ?revolver? are
shown, but only one word from other synsets is
shown. Edge labels are probabilities of transitioning
from synset i to synset j. Note how this favors fre-
quent terms, such as ?revolver,? over ones like ?six-
shooter.?
2 Topic models and WordNet
The WORDNET-WALK is a probabilistic process of
word generation that is based on the hyponomy re-
lationship in WORDNET (Miller, 1990). WORD-
NET, a lexical resource designed by psychologists
and lexicographers to mimic the semantic organiza-
tion in the human mind, links ?synsets? (short for
synonym sets) with myriad connections. The spe-
cific relation we?re interested in, hyponomy, points
from general concepts to more specific ones and is
sometimes called the ?is-a? relationship.
As first described by Abney and Light (1999), we
imagine an agent who starts at synset [entity],
which points to every noun in WORDNET 2.1 by
some sequence of hyponomy relations, and then
chooses the next node in its random walk from the
hyponyms of its current position. The agent repeats
this process until it reaches a leaf node, which corre-
sponds to a single word (each of the synset?s words
are unique leaves of a synset in our construction).
For an example of all the paths that might gener-
ate the word ?colt? see Figure 1. The WORDNET-
WALK is parameterized by a set of distributions over
children for each synset s in WORDNET, ?s.
Symbol Meaning
K number of topics
?k,s multinomial probability vector over
the successors of synset s in topic k
S scalar that, when multiplied by ?s
gives the prior for ?k,s
?s normalized vector whose ith entry,
when multiplied by S, gives the prior
probability for going from s to i
?d multinomial probability vector over
the topics that generate document d
? prior for ?
z assignment of a word to a topic
? a path assignment through
WORDNET ending at a word.
?i,j one link in a path ? going from syn-
set i to synset j.
Table 1: A summary of the notation used in the pa-
per. Bold vectors correspond to collections of vari-
ables (i.e. zu refers to a topic of a single word, but
z1:D are the topics assignments of words in docu-
ment 1 through D).
2.1 A topic model for WSD
The WORDNET-WALK has two important proper-
ties. First, it describes a random process for word
generation. Thus, it is a distribution over words
and thus can be integrated into any generative model
of text, such as topic models. Second, the synset
that produces each word is a hidden random vari-
able. Given a word assumed to be generated by a
WORDNET-WALK, we can use posterior inference
to predict which synset produced the word.
These properties allow us to develop LDAWN,
which is a fusion of these WORDNET-WALKs and
latent Dirichlet alocation (LDA) (Blei et al, 2003),
a probabilistic model of documents that is an im-
provement to pLSI (Hofmann, 1999). LDA assumes
that there are K ?topics,? multinomial distributions
over words, which describe a collection. Each docu-
ment exhibits multiple topics, and each word in each
document is associated with one of them.
Although the term ?topic? evokes a collection of
ideas that share a common theme and although the
topics derived by LDA seem to possess semantic
coherence, there is no reason to believe this would
1025
be true of the most likely multinomial distributions
that could have created the corpus given the assumed
generative model. That semantically similar words
are likely to occur together is a byproduct of how
language is actually used.
In LDAWN, we replace the multinomial topic dis-
tributions with a WORDNET-WALK, as described
above. LDAWN assumes a corpus is generated by
the following process (for an overview of the nota-
tion used in this paper, see Table 1).
1. For each topic, k ? {1, . . . ,K}
(a) For each synset s, randomly choose transition prob-
abilities ?k,s ? Dir(S?s).
2. For each document d ? {1, . . . , D}
(a) Select a topic distribution ?d ? Dir(?)
(b) For each word n ? {1, . . . , Nd}
i. Select a topic z ? Mult(1, ?d)
ii. Create a path ?d,n starting with ?0 as the root
node.
iii. From children of ?i:
A. Choose the next node in the walk ?i+1 ?
Mult(1, ?z,?i)
B. If ?i+1 is a leaf node, generate the associ-
ated word. Otherwise, repeat.
Every element of this process, including the
synsets, is hidden except for the words of the doc-
uments. Thus, given a collection of documents, our
goal is to perform posterior inference, which is the
task of determining the conditional distribution of
the hidden variables given the observations. In the
case of LDAWN, the hidden variables are the param-
eters of the K WORDNET-WALKs, the topic assign-
ments of each word in the collection, and the synset
path of each word. In a sense, posterior inference
reverses the process described above.
Specifically, given a document collection w1:D,
the full posterior is
p(?1:K ,z1:D,?1:D,?1:D |w1:D, ?, S?) ?(?K
k=1 p(?k |S?)
?D
d=1 p(?d | ?)
?Nd
n=1 p(?d,n |?1:K)p(wd,n |?d,n)
)
, (1)
where the constant of proportionality is the marginal
likelihood of the observed data.
Note that by encoding the synset paths as a hid-
den variable, we have posed the WSD problem as
a question of posterior probabilistic inference. Fur-
ther note that we have developed an unsupervised
model. No labeled data is needed to disambiguate a
corpus. Learning the posterior distribution amounts
to simultaneously decomposing a corpus into topics
and its words into their synsets.
The intuition behind LDAWN is that the words
in a topic will have similar meanings and thus share
paths within WORDNET. For example, WORDNET
has two senses for the word ?colt;? one referring to a
young male horse and the other to a type of handgun
(see Figure 1).
Although we have no a priori way of know-
ing which of the two paths to favor for a
document, we assume that similar concepts
will also appear in the document. Documents
with unambiguous nouns such as ?six-shooter?
and ?smoothbore? would make paths that pass
through the synset [firearm, piece,
small-arm] more likely than those go-
ing through [animal, animate being,
beast, brute, creature, fauna]. In
practice, we hope to see a WORDNET-WALK that
looks like Figure 2, which points to the right sense
of cancer for a medical context.
LDAWN is a Bayesian framework, as each vari-
able has a prior distribution. In particular, the
Dirichlet prior for ?s, specified by a scaling factor
S and a normalized vector ?s fulfills two functions.
First, as the overall strength of S increases, we place
a greater emphasis on the prior. This is equivalent to
the need for balancing as noted by Abney and Light
(1999).
The other function that the Dirichlet prior serves
is to enable us to encode any information we have
about how we suspect the transitions to children
nodes will be distributed. For instance, we might ex-
pect that the words associated with a synset will be
produced in a way roughly similar to the token prob-
ability in a corpus. For example, even though ?meal?
might refer to both ground cereals or food eaten at
a single sitting and ?repast? exclusively to the lat-
ter, the synset [meal, repast, food eaten
at a single sitting] still prefers to transi-
tion to ?meal? over ?repast? given the overall corpus
counts (see Figure 1, which shows prior transition
probabilities for ?revolver?).
By setting ?s,i, the prior probability of transition-
ing from synset s to node i, proportional to the to-
tal number of observed tokens in the children of i,
1026
we introduce a probabilistic variation on informa-
tion content (Resnik, 1995). As in Resnik?s defini-
tion, this value for non-word nodes is equal to the
sum of all the frequencies of hyponym words. Un-
like Resnik, we do not divide frequency among all
senses of a word; each sense of a word contributes
its full frequency to ?.
3 Posterior Inference with Gibbs Sampling
As described above, the problem of WSD corre-
sponds to posterior inference: determining the prob-
ability distribution of the hidden variables given ob-
served words and then selecting the synsets of the
most likely paths as the correct sense. Directly com-
puting this posterior distribution, however, is not
tractable because of the difficulty of calculating the
normalizing constant in Equation 1.
To approximate the posterior, we use Gibbs sam-
pling, which has proven to be a successful approx-
imate inference technique for LDA (Griffiths and
Steyvers, 2004). In Gibbs sampling, like all Markov
chain Monte Carlo methods, we repeatedly sample
from aMarkov chain whose stationary distribution is
the posterior of interest (Robert and Casella, 2004).
Even though we don?t know the full posterior, the
samples can be used to form an empirical estimate
of the target distribution. In LDAWN, the samples
contain a configuration of the latent semantic states
of the system, revealing the hidden topics and paths
that likely led to the observed data.
Gibbs sampling reproduces the posterior distri-
bution by repeatedly sampling each hidden variable
conditioned on the current state of the other hidden
variables and observations. More precisely, the state
is given by a set of assignments where each word
is assigned to a path through one of K WORDNET-
WALK topics: uth word wu has a topic assignment
zu and a path assignment ?u. We use z?u and ??u
to represent the topic and path assignments of all
words except for u, respectively.
Sampling a new topic for the word wu requires
us to consider all of the paths that wu can take in
each topic and the topics of the other words in the
document u is in. The probability of wu taking on
topic i is proportional to
p(zu = i |z?u)
?
? p(? |??u)1[wu ? ?], (2)
which is the probability of selecting z from ?d times
the probability of a path generating wu from a path
in the ith WORDNET-WALK.
The first term, the topic probability of the uth
word, is based on the assignments to the K topics
for words other than u in this document,
p(zu = i|z?u) =
n(d)?u,i + ?i
?
j n
(d)
?u,j +
?K
j=1 ?j
, (3)
where n(d)?u,j is the number of words other than u in
topic j for the document d that u appears in.
The second term in Equation 2 is a sum over the
probabilities of every path that could have generated
the word wu. In practice, this sum can be com-
puted using a dynamic program for all nodes that
have unique parent (i.e. those that can?t be reached
by more than one path). Although the probability of
a path is specific to the topic, as the transition prob-
abilities for a synset are different across topics, we
will omit the topic index in the equation,
p(?u = ?|??u, ) =
?l?1
i=1 ?
?u
?i,?i+1
. (4)
3.1 Transition Probabilities
Computing the probability of a path requires us to
take a product over our estimate of the probability
from transitioning from i to j for all nodes i and j in
the path ?. The other path assignments within this
topic, however, play an important role in shaping the
transition probabilities.
From the perspective of a single node i, only paths
that pass through that node affect the probability of
u also passing through that node. It?s convenient to
have an explicit count of all of the paths that tran-
sition from i to j in this topic?s WORDNET-WALK,
so we use T?ui,j to represent all of the paths that go
from i to j in a topic other than the path currently
assigned to u.
Given the assignment of all other words to paths,
calculating the probability of transitioning from i to
j with word u requires us to consider the prior ? and
the observations Ti,j in our estimate of the expected
value of the probability of transitioning from i to j,
??ui,j =
T?ui,j + Si?i,j
Si +
?
k T
?u
i,k
. (5)
1027
As mentioned in Section 2.1, we paramaterize the
prior for synset i as a vector ?i, which sums to one,
and a scale parameter S.
The next step, once we?ve selected a topic, is to
select a path within that topic. This requires the
computation of the path probabilities as specified in
Equation 4 for all of the paths wu can take in the
sampled topic and then sampling from the path prob-
abilities.
The Gibbs sampler is essentially a randomized
hill climbing algorithm on the posterior likelihood as
a function of the configuration of hidden variables.
The numerator of Equation 1 is proportional to that
posterior and thus allows us to track the sampler?s
progress. We assess convergence to a local mode of
the posterior by monitoring this quantity.
4 Experiments
In this section, we describe the properties of the
topics induced by running the previously described
Gibbs sampling method on corpora and how these
topics improve WSD accuracy.
Of the two data sets used during the course of
our evaluation, the primary dataset was SEMCOR
(Miller et al, 1993), which is a subset of the Brown
corpus with many nouns manually labeled with the
correct WORDNET sense. The words in this dataset
are lemmatized, and multi-word expressions that are
present in WORDNET are identified. Only the words
in SEMCOR were used in the Gibbs sampling pro-
cedure; the synset assignments were only used for
assessing the accuracy of the final predictions.
We also used the British National Corpus, which
is not lemmatized and which does not have multi-
word expressions. The text was first run through
a lemmatizer, and then sequences of words which
matched a multi-word expression in WORDNET
were joined together into a single word. We took
nouns that appeared in SEMCOR twice or in the
BNC at least 25 times and used the BNC to com-
pute the information-content analog ? for individ-
ual nouns (For example, the probabilities in Figure 1
correspond to ?).
4.1 Topics
Like the topics created by structures such as LDA,
the topics in Table 2 coalesce around reasonable
themes. The word list was compiled by summing
over all of the possible leaves that could have gen-
erated each of the words and sorting the words by
decreasing probability. In the vast majority of cases,
a single synset?s high probability is responsible for
the words? positions on the list.
Reassuringly, many of the top senses for the
present words correspond to the most frequent sense
in SEMCOR. For example, in Topic 4, the senses for
?space? and ?function? correspond to the top senses
in SEMCOR, and while the top sense for ?set? corre-
sponds to ?an abstract collection of numbers or sym-
bols? rather than ?a group of the same kind that be-
long together and are so used,? it makes sense given
the math-based words in the topic. ?Point,? however,
corresponds to the sense used in the phrase ?I got to
the point of boiling the water,? which is neither the
top SEMCOR sense nor a sense which makes sense
given the other words in the topic.
While the topics presented in Table 2 resemble
the topics one would obtain through models like
LDA (Blei et al, 2003), they are not identical. Be-
cause of the lengthy process of Gibbs sampling, we
initially thought that using LDA assignments as an
initial state would converge faster than a random ini-
tial assignment. While this was the case, it con-
verged to a state that less probable than the randomly
initialized state and no better at sense disambigua-
tion (and sometimes worse). The topics presented
in 2 represent words both that co-occur together in
a corpus and co-occur on paths through WORDNET.
Because topics created through LDA only have the
first property, they usually do worse in terms of both
total probability and disambiguation accuracy (see
Figure 3).
Another interesting property of topics in LDAWN
is that, with higher levels of smoothing, words that
don?t appear in a corpus (or appear rarely) but are
in similar parts of WORDNET might have relatively
high probability in a topic. For example, ?maturity?
in topic two in Table 2 is sandwiched between ?foot?
and ?center,? both of which occur about five times
more than ?maturity.? This might improve LDA-
based information retrieval schemes (Wei and Croft,
2006) .
1028
1740
1930
0.23 0.76
3122 0.42
0.01
2236
0.10 0.00
0.00
0.00
0.00
7626
someone
0.00
9609711
0.00
9120316
1743824
0.00
cancer
7998922 genus0.04
0.04
8564599
star_sign
0.06
8565580
0.06
cancer
0.5
9100327
cancer
1
constellation
0.01
0.01
cancer
0.5
crab
0.5
13875408
0.58 0.19
14049094 14046733
tumor
0.97
14050958
0.00
malignancy
0.06
0.94
14051451
0.90
cancer
0.96
Synset ID
Transition Prob
Word
1957888
1.0
Figure 2: The possible paths to reach the word ?cancer? in WORDNET along with transition probabilities
from the medically-themed Topic 2 in Table 2, with the most probable path highlighted. The dashed lines
represent multiple links that have been consolidated, and synsets are represented by their offsets within
WORDNET 2.1. Some words for immediate hypernyms have also been included to give context. In all other
topics, the person, animal, or constellation senses were preferred.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7
president growth material point water plant music
party age object number house change film
city treatment color value road month work
election feed form function area worker life
administration day subject set city report time
official period part square land mercer world
office head self space home requirement group
bill portion picture polynomial farm bank audience
yesterday length artist operator spring farmer play
court level art component bridge production thing
meet foot patient corner pool medium style
police maturity communication direction site petitioner year
service center movement curve interest relationship show
Table 2: The most probable words from six randomly chosen WORDNET-walks from a thirty-two topic
model trained on the words in SEMCOR. These are summed over all of the possible synsets that generate
the words. However, the vast majority of the contributions come from a single synset.
1029
 
0.275 0.28
 
0.285 0.29
 
0.295 0.3
 
0.305
 
0
 
1000
 
2000
 
3000
 
4000
 
5000
 
6000
 
7000
 
8000
 
9000
 
10000
Accuracy
Iteratio
n
Unsee
ded
Seede
d with 
LDA
-
96000
-
94000
-
92000
-
90000
-
88000
-
86000
-
84000
-
82000
-
80000
 
0
 
1000
 
2000
 
3000
 
4000
 
5000
 
6000
 
7000
 
8000
 
9000
 
10000
Model Probability
Iteratio
n
Unsee
ded
Seede
d with 
LDA
Figure 3: Topics seeded with LDA initially have
a higher disambiguation accuracy, but are quickly
matched by unseeded topics. The probability for the
seeded topics starts lower and remains lower.
4.2 Topics and the Weight of the Prior
Because the Dirichlet smoothing factor in part
determines the topics, it also affects the disam-
biguation. Figure 4 shows the modal disambigua-
tion achieved for each of the settings of S =
{0.1, 1, 5, 10, 15, 20}. Each line is one setting of K
and each point on the line is a setting of S. Each
data point is a run for the Gibbs sampler for 10,000
iterations. The disambiguation, taken at the mode,
improved with moderate settings of S, which sug-
gests that the data are still sparse for many of the
walks, although the improvement vanishes if S dom-
inates with much larger values. This makes sense,
as each walk has over 100,000 parameters, there are
fewer than 100,000 words in SEMCOR, and each
 
0.24
 
0.26
 
0.28 0.3
 
0.32
 
0.34
 
0.36
 
0.38
S=20
S=15
S=10
S=5
S=1
S=0.1
Accuracy
Smoot
hing F
actor
64 top
ics
32 top
ics
16 top
ics 8 topic
s
4 topic
s
2 topic
s
1 topic Rando
m
Figure 4: Each line represents experiments with a set
number of topics and variable amounts of smooth-
ing on the SEMCOR corpus. The random baseline
is at the bottom of the graph, and adding topics im-
proves accuracy. As smoothing increases, the prior
(based on token frequency) becomes stronger. Ac-
curacy is the percentage of correctly disambiguated
polysemous words in SEMCOR at the mode.
word only serves as evidence to at most 19 parame-
ters (the length of the longest path in WORDNET).
Generally, a greater number of topics increased
the accuracy of the mode, but after around sixteen
topics, gains became much smaller. The effect of ?
is also related to the number of topics, as a value of S
for a very large number of topics might overwhelm
the observed data, while the same value of S might
be the perfect balance for a smaller number of topics.
For comparison, the method of using a WORDNET-
WALK applied to smaller contexts such as sentences
or documents achieves an accuracy of between 26%
and 30%, depending on the level of smoothing.
5 Error Analysis
This method works well in cases where the delin-
eation can be readily determined from the over-
all topic of the document. Words such as ?kid,?
?may,? ?shear,? ?coach,? ?incident,? ?fence,? ?bee,?
and (previously used as an example) ?colt? were
all perfectly disambiguated by this method. Figure
2 shows the WORDNET-WALK corresponding to a
medical topic that correctly disambiguates ?cancer.?
Problems arose, however, with highly frequent
1030
words, such as ?man? and ?time? that have many
senses and can occur in many types of documents.
For example, ?man? can be associated with many
possible meanings: island, game equipment, ser-
vant, husband, a specific mammal, etc.
Although we know that the ?adult male? sense
should be preferred, the alternative meanings will
also be likely if they can be assigned to a topic
that shares common paths in WORDNET; the doc-
uments contain, however, many other places, jobs,
and animals which are reasonable explanations (to
LDAWN) of how ?man? was generated. Unfortu-
nately, ?man? is such a ubiquitous term that top-
ics, which are derived from the frequency of words
within an entire document, are ultimately uninfor-
mative about its usage.
While mistakes on these highly frequent terms
significantly hurt our accuracy, errors associated
with less frequent terms reveal that WORDNET?s
structure is not easily transformed into a probabilis-
tic graph. For instance, there are two senses of
the word ?quarterback,? a player in American foot-
ball. One is position itself and the other is a per-
son playing that position. While one would expect
co-occurrence in sentences such as ?quarterback is a
easy position, so our quarterback is happy,? the paths
to both terms share only the root node, thus making
it highly unlikely a topic would cover both senses.
Because of WORDNET?s breadth, rare senses
also impact disambiguation. For example, the
metonymical use of ?door? to represent a whole
building as in the phrase ?girl next door? is un-
der the same parent as sixty other synsets contain-
ing ?bridge,? ?balcony,? ?body,? ?arch,? ?floor,? and
?corner.? Surrounded by such common terms that
are also likely to co-occur with the more conven-
tional meanings of door, this very rare sense be-
comes the preferred disambiguation of ?door.?
6 Related Work
Abney and Light?s initial probabilistic WSD ap-
proach (1999) was further developed into a Bayesian
network model by Ciaramita and Johnson (2000),
who likewise used the appearance of monosemous
terms close to ambiguous ones to ?explain away? the
usage of ambiguous terms in selectional restrictions.
We have adapted these approaches and put them into
the context of a topic model.
Recently, other approaches have created ad hoc
connections between synsets in WORDNET and then
considered walks through the newly created graph.
Given the difficulties of using existing connections
in WORDNET, Mihalcea (2005) proposed creating
links between adjacent synsets that might comprise
a sentence, initially setting weights to be equal to
the Lesk overlap between the pairs, and then using
the PageRank algorithm to determine the stationary
distribution over synsets.
6.1 Topics and Domains
Yarowsky was one of the first to contend that ?there
is one sense for discourse? (1992). This has lead
to the approaches like that of Magnini (Magnini et
al., 2001) that attempt to find the category of a text,
select the most appropriate synset, and then assign
the selected sense using domain annotation attached
to WORDNET.
LDAWN is different in that the categories are not
an a priori concept that must be painstakingly anno-
tated within WORDNET and require no augmenta-
tion of WORDNET. This technique could indeed be
used with any hierarchy. Our concepts are the ones
that best partition the space of documents and do the
best job of describing the distinctions of diction that
separate documents from different domains.
6.2 Similarity Measures
Our approach gives a probabilistic method of us-
ing information content (Resnik, 1995) as a start-
ing point that can be adjusted to cluster words in
a given topic together; this is similar to the Jiang-
Conrath similarity measure (1997), which has been
used in many applications in addition to disambigua-
tion. Patwardhan (2003) offers a broad evaluation of
similarity measures for WSD.
Our technique for combining the cues of topics
and distance in WORDNET is adjusted in a way sim-
ilar in spirit to Buitelaar and Sacaleanu (2001), but
we consider the appearance of a single term to be
evidence for not just that sense and its immediate
neighbors in the hyponomy tree but for all of the
sense?s children and ancestors.
Like McCarthy (2004), our unsupervised system
acquires a single predominant sense for a domain
based on a synthesis of information derived from a
1031
textual corpus, topics, and WORDNET-derived sim-
ilarity, a probabilistic information content measure.
By adding syntactic information from a thesaurus
derived from syntactic features (taken from Lin?s au-
tomatically generated thesaurus (1998)), McCarthy
achieved 48% accuracy in a similar evaluation on
SEMCOR; LDAWN is thus substantially less effec-
tive in disambiguation compared to state-of-the-art
methods. This suggests, however, that other meth-
ods might be improved by adding topics and that our
method might be improved by using more informa-
tion than word counts.
7 Conclusion and Future Work
The LDAWN model presented here makes two con-
tributions to research in automatic word sense dis-
ambiguation. First, we demonstrate a method for au-
tomatically partitioning a document into topics that
includes explicit semantic information. Second, we
show that, at least for one simple model of WSD,
embedding a document in probabilistic latent struc-
ture, i.e., a ?topic,? can improve WSD.
There are two avenues of research with LDAWN
that we will explore. First, the statistical nature of
this approach allows LDAWN to be used as a com-
ponent in larger models for other language tasks.
Other probabilistic models of language could in-
sert the ability to query synsets or paths of WORD-
NET. Similarly, any topic based information re-
trieval scheme could employ topics that include se-
mantically relevant (but perhaps unobserved) terms.
Incorporating this model in a larger syntactically-
aware model, which could benefit from the local
context as well as the document level context, is an
important component of future research.
Second, the results presented here show a marked
improvement in accuracy as more topics are added
to the baseline model, although the final result is not
comparable to state-of-the-art techniques. As most
errors were attributable to the hyponomy structure
of WORDNET, incorporating the novel use of topic
modeling presented here with a more mature unsu-
pervised WSD algorithm to replace the underlying
WORDNET-WALK could lead to advances in state-
of-the-art unsupervised WSD accuracy.
References
Steven Abney and Marc Light. 1999. Hiding a semantic
hierarchy in a markov model. In Proceedings of the
Workshop on Unsupervised Learning in Natural Lan-
guage Processing, pages 1?8.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Ranking
and selecting synsets by domain relevance. In Pro-
ceedings of WordNet and Other Lexical Resources:
Applications, Extensions and Customizations. NAACL
2001. Association for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away ambiguity: Learning verb selectional
preference with bayesian networks. In COLING-00,
pages 187?193.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In HLT
?91: Proceedings of the workshop on Speech and Nat-
ural Language, pages 233?237. Association for Com-
putational Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, pages 5228?5235.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. Proceedings of the Twenty-Second Annual
International SIGIR Conference.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2001. Using domain information
for word sense disambiguation. In In Proceedings of
2nd International Workshop on Evaluating Word Sense
Disambiguation Systems, Toulouse, France.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In In 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 280?287.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Human Language Technology and Empirical
Methods in Natural Language Processing Conference,
pages 411?418.
1032
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In 3rd
DARPA Workshop on Human Language Technology,
pages 303?308.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using Measures of Semantic Related-
ness for Word Sense Disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics, pages
241?257.
Matthew Purver, Konrad Ko?rding, Thomas Griffiths, and
Joshua Tenenbaum. 2006. Unsupervised topic mod-
elling for multi-party spoken discourse. In Proceed-
ings of COLING-ACL.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Inter-
national Joint Conferences on Artificial Intelligence,
pages 448?453.
Christian Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer Texts in Statistics.
Springer-Verlag, New York, NY.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In Proceedings of
the Twenty-Ninth Annual International SIGIR Confer-
ence.
1033
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 272?276,
Prague, June 2007. c?2007 Association for Computational Linguistics
PU-BCD: Exponential Family Models for the Coarse- and Fine-Grained
All-Words Tasks
Jonathan Chang
Princeton University
Department of Electrical Engineering
jcone@princeton.edu
Miroslav Dud??k, David M. Blei
Princeton University
Department of Computer Science
{mdudik,blei}@cs.princeton.edu
Abstract
This paper describes an exponential family
model of word sense which captures both
occurrences and co-occurrences of words
and senses in a joint probability distribution.
This statistical framework lends itself to the
task of word sense disambiguation. We eval-
uate the performance of the model in its par-
ticipation on the SemEval-2007 coarse- and
fine-grained all-words tasks under a variety
of parameters.
1 Introduction
This paper describes an exponential family model
suited to performing word sense disambiguation.
Exponential family models are a mainstay of mod-
ern statistical modeling (Brown, 1986) and they are
widely and successfully used for example in text
classification (Berger et al, 1996). In statistical
machine learning research, a general methodology
and many algorithms were developed for undirected
graphical model representation of exponential fam-
ilies (Jordan, 2004), providing a solid basis for effi-
cient inference.
Our model differs from other probabilistic mod-
els used for word sense disambiguation in that it
captures not only word-sense co-occurrences but
also contextual sense-sense co-occurrences, thereby
breaking the na??ve Bayes assumption. Although
spare in the types of features, the model is extremely
expressive. Our model has parameters that control
for word-sense interaction and sense-sense similar-
ity, allowing us to capture many of the salient fea-
tures of word and sense use. After fitting the param-
eters of our model from a labeled corpus, the task
of word sense disambiguation immediately follows
by considering the posterior distribution of senses
given words.
We used this model to participate in SemEval-
2007 on the coarse- and fine-grained all-words tasks.
In both of these tasks, a series of sentences are
given with certain words tagged. Each competing
system must assign a sense from a sense inventory
to the tagged words. In both tasks, performance
was gauged by comparing the output of each system
to human-tagged senses. In the fine-grained task,
precision and recall were simply and directly com-
puted against the golden annotations. However, in
the coarse-grained task, the sense inventory was first
clustered semi-automatically with each cluster rep-
resenting an equivalence class over senses (Navigli,
2006). Precision and recall were computed against
equivalence classes.
This paper briefly derives the model and then
explores its properties for WSD. We show how
common algorithms, such as ?dominant sense? and
?most frequent sense,? can be expressed in the ex-
ponential family framework. We then proceed to
present an evaluation of the developed techniques on
the SemEval-2007 tasks in which we participated.
2 The model
We describe an exponential family model for word
sense disambiguation. We posit a joint distribution
over words w and senses s.
2.1 Notation
We define a document d to be a sequence of words
from some lexicon W; for the participation in this
contest, a document consists of a sentence. Associ-
ated with each word is a sense from a lexicon S. In
272
this work, our sense lexicon is the synsets of Word-
Net (Fellbaum and Miller, 2003), but our methods
easily generalize to other sense lexicons, such as
VerbNet (Kipper et al, 2000).
Formally, we denote the sequence of words in a
document d by wd = (wd,1, . . . , wd,nd) and the se-
quence of synsets by sd = (sd,1, sd,2, . . . , sd,nd),
where nd denotes the number of words in the docu-
ment. A corpus D is defined as a collection of doc-
uments. We also write w ? s if w can be used to
represent sense s.
2.2 An exponential family of words and senses
We turn our attention to an exponential family
of words and senses. The vector of parameters
? = (?,?) consists of two blocks capturing depen-
dence on word-synset co-occurrences, and synset
co-occurrences.
p?,n(s,w)
= exp
{?
i?wi,si +
?
i,j ?si,sj
}/
Z?,n .
(1)
The summations are first over all positions in the
document, 1 ? i ? n, and then over all pairs of
positions in the document, 1 ? i, j ? n. We discuss
parameters of our exponential model in turn.
Word-sense parameters ? Using parameters ?
alone, it is possible to describe an arbitrary context
independent distribution between a word and its as-
signed synset.
Sense co-occurrence parameters? Parameters?
are the only parameters that establish the depen-
dence of sense on its context. More specifically,
they capture co-occurrences of synset pairs within a
context. Larger values favor, whereas smaller values
disfavor each pair of synsets.
3 Parameter estimation
With the model in hand, we need to address two
problems in order to use it for problems such as
WSD. First, in parameter estimation, we find values
of the parameters that explain a labeled corpus, such
as SemCor (Miller et al, 1993). Once the parame-
ters are fit, we use posterior inference to compute the
posterior probability distribution of a set of senses
given a set of unlabeled words in a context, p(s |w).
This distribution is used to predict the senses of the
words.
In this section, it will be useful to introduce the
notation p?(s, w) to denote the empirical probabili-
ties of observing the word-sense pair s, w in the en-
tire corpus:
p?(s, w) =
?
d,i ?(sd,i, s)?(wd,i, w)/
?
d nd ,
where ?(x, y) = 1 if x = y and 0 otherwise.
Similarly, we will define p?(s) to denote the empiri-
cal probability of observing a sense s over the entire
corpus:
p?(s) =
?
d,i ?(sd,i, s)/
?
d nd .
3.1 Word-sense parameters ?
Fallback Let ?WNw,s = 0 if w ? s and ?
WN
w,s = ??
otherwise. This simply sets to zero the probability of
assigning a word w to a synset s when w 6? s while
making all w ? s equally likely as an assignment
to s. This forces the model to rely entirely on ?
for inference. If ? is also set to 0, this then forces
the system to fall back onto its arbitrary tie-breaking
mechanism such as choosing randomly or choosing
the first sense.
Most-frequent synset One approach to disam-
biguation is the technique of choosing the most fre-
quently occurring synset which the word may ex-
press. This can be implemented within the model by
setting ?w,s = ?MFSw,s ? ln p?(s) if w ? s and ??
otherwise.
MLE Given a labeled corpus, we would like to
find the corresponding parameters that maximize
likelihood of the data. Equivalently, we would like
to maximize the log likelihood
L(?) =
?
d
[?
i?wd,i,sd,i +
?
i,j ?sd,i,sd,j ? lnZ?,nd
]
.
(2)
In this section, we consider a simple case when it
is possible to estimate parameters maximizing the
likelihood exactly, i.e., the case where our model
depends only on word-synset co-occurrences and is
parametrized solely by ? (setting ? = 0).
Using Eq. (1), with ? = 0, we obtain
p?(sD,wD) =
exp
{?
d,i?wd,i,sd,i
}
?
d Z?,nd
.
273
Thus, p?(sD,wD) can be viewed as a multino-
mial model with
?
d nd trials and |S| outcomes,
parametrized by ?w,s. The maximum likelihood es-
timates in this model are ??w,s ? ln p?(s, w).
This setting of the parameters corresponds pre-
cisely to the dominant-sensemodel (McCarthy et al,
2004). The resulting model is thus
p?,n(s,w) =
?
i p?(si, wi) . (3)
3.2 Sense co-occurrence parameters ?
Unlike ?, it is impossible to find a closed-form so-
lution for the maximum-likelihood settings of ?.
Therefore, we turn to intuitive methods.
Observed synset co-occurrence One natural ad
hoc statistic to use to compute the parameters ? are
the empirical sense co-occurrences. In particular, we
may set
?si,sj = ?
SF
si,sj ? ln p?(si, sj) . (4)
We will observe in section 5 that the performance
of ? = ?SF actually degrades the performance of
the system, especially when combined with ? = ??.
This can be understood as a by-product of an un-
sympathetic interaction between ? and ?. In other
words, ? and ? overlap; by favoring a sense pair the
model will also implicitly favor each of the senses in
the pair.
Discounted observed synset co-occurrence As
we noted earlier, the combination ? = ??,? = ?SF
actually performs worse than ? = ??,? = 0.
In order to cancel out the aforementioned over-
lap effect, we attempt to compute the number of
co-occurrences beyond what the occurrences them-
selves would imply. To do so, we set
? = ?DSF ? ln
p?(si, sj)
p?(si)p?(sj)
, (5)
a quantity which finds an analogue in the notion of
mutual information. We will see shortly that such
a setting of ? will allow sense co-occurrence to im-
prove disambiguation performance.
4 Word Sense Disambiguation
Finally, we describe how to perform WSD using the
exponential family model. Our goal is to assign a
synset si to every word wi in an unlabeled document
d of length n. In this setting, the synsets are hidden
variables. Thus, we assign synsets according to their
posterior probability given the observed words:
s? = argmax
s?Sn
p?,n(s,w)
?
s? p?,n(s
?,w)
,
where the sum is over all possible sequences of
synsets. This combinatorial sum renders exact infer-
ence computationally intractable. We discuss how to
obtain the sense assignment using approximate in-
ference.
4.1 Variational Inference
To approximate the posterior over senses, we use
variational inference (Jordan et al, 1999). In vari-
ational inference, one first chooses a family of
distributions for which inference is computationlly
tractable. Then the distribution in that family which
best approximates the posterior distribution of inter-
est is found.
For our purposes, it is convenient to select q from
the family of factorized multinomial distributions:
q(s) =
?
i
qi(si) ,
where each qi(si) is a multinomial distribution
over all possible senses. Observe that finding s? is
much simpler using q(s): one can find the argmax
of each individual qi independently.
It can be shown that the multinomial which mini-
mizes the KL-divergence must satisfy:
qi(si) ? exp
?
?
?
?wi,si +
?
j 6=i
?
sj
qj(sj)?si,sj
?
?
?
(6)
a system of transcendental equations which can
be solved iteratively to find q. This q is then used to
efficiently perform inference and hence disambigua-
tion.
5 Evaluation
This section evaluates the performance of the model
and the techniques described in the previous sec-
tions with respect to the coarse- and fine-grained all-
words tasks at SemEval-2007.
In order to train the parameters, we trained our
model in a supervised fashion on SemCor (Miller et
274
? = ?WN ? = ?MFS ? = ??
? = 0 52.0% 45.8% 51.2%
? = ?SF 48.8% 45.3% 52.5%
? = ?DSF 47.0% 44.6% 54.2%
Table 1: Precision for the fine-grained all-words task. The results corresponding to the bolded value was
submitted to the competition.
al., 1993) with Laplace smoothing for parameter es-
timates. We utilized the POS tagging and lemma-
tization given in the coarse-grained all-words test
set. Wherever a headword was tagged differently
between the two test sets, we produced an answer
only for the coarse-grained test and not for the fine-
grained one. This led to responses on only 93.9% of
the fine-grained test words. Of the 6.1% over which
no response was given, 5.3% were tagged as ?U? in
the answer key.
In order to break ties between equally likely
senses, for the fine-grained test, the system returned
the first one returned in WordNet?s sense inventory
for that lemma. For the coarse-grained test, an arbi-
trary sense was returned in case of ties.
The precision results given in this section are over
polysemous words (of all parts of speech) for which
our system gave an answer and for which the answer
key was not tagged with ?U.?
5.1 Fine-grained results (Task 17)
The fine-grained results over all permutations of the
parameters mentioned in Section 3 are given in Ta-
ble 1. Note here that the baseline number of ? =
0,? = ?WN given in the upper-left is equivalent to
simply choosing the first WordNet sense. Notably,
such a simple configuration of the model outper-
forms all but two other of the other parameter set-
tings.
When any sort of nonzero sense co-occurrence
parameter is used with ? = ?WN, the performance
degrades dramatically, to 48.8% and 47.0% for ?SF
and ?DSF respectively. Since the discounting scheme
was devised to positively interact with ? = ??, it is
no surprise that it does poorly when ? is not set in
such a way. And as mentioned previously, na??vely
setting ? to ?SF improperly conflates ? and ?, yield-
ing a poor result.
When ? = ?MFS is used, the precision is
even lower, dropping to 45.8% when no sense co-
occurrence information is used. And similarly to
? = ?WN, any nonzero ? significantly degrades per-
formance. This seems to indicate the most-frequent
synset, as predicted by our earlier analysis, is an in-
ferior technique.
Finally, when? = ?? is used (i.e. dominant sense),
the precision is 51.2%, slightly lower than but nearly
on par with that of the baseline. When sense co-
occurrence parameters are added, the performance
increases. For ?SF, a precision of 52.5% is achieved;
a precision above the baseline. But again, because of
the interaction between ? and ?, here we expect it
to be possible to improve upon this performance.
And indeed, when ? = ?DSF, the highest value
of the entire table, 54.2% is achieved. This is a sig-
nificant improvement over the baseline and demon-
strates that our intuitively appealing mutual informa-
tion discounting mechanism allows for ? and ? to
work cooperatively.
5.2 Coarse-grained results (Task 7)
In order to perform the coarse-grained task, our sys-
tem first determined the set of sense equivalence
classes. We denote a sense equivalence class by k,
where k is some sense key member of the class. The
equivalence classes were created according to the
following constraints:
? Each sense key k may only belong to one
equivalence class k.
? All sense keys referring to the same sense s
must belong in the same class.
? All sense keys clustered together must belong
in the same class.
Once the clustering is complete, we can proceed
exactly as we did in the previous sections, while re-
placing all instances of s with k. Thus, training
in this case was performed on a SemCor where all
275
the senses were mapped back to their corresponding
sense equivalence classes.
The model fared considerably worse on the
coarse-grained all-words task. The precision of the
system as given by the scorer was 69.7% and the
recall 62.8%. These results, while naturally much
higher than those for the fine-grained test, are low by
coarse-grained standards. While the gold standard
was not available for comparison for these results,
there are two likely causes of the lower performance
on this task.
The first is that ties were not adjudicated by
choosing the first WordNet sense. Instead, an ar-
bitrary sense was chosen thereby pushing cases in
which the model is unsure from the baseline to the
much lower random precision rate. The second is the
same number of documents are mapped to a smaller
number of ?senses? (i.e. sense equivalence classes),
the number of parameters is greatly reduced. There-
fore, the expressive power of each parameter is di-
luted because it must be spread out across all senses
within the equivalence class.
We believe that both of these issues can be eas-
ily overcome and we hope to do so in future work.
Furthermore, while the model currently captures the
most salient features for word sense disambiguation,
namely word-sense occurrence and sense-sense co-
occurrence, it would be simple to extend the model
to include a larger number of features (e.g. syntactic
features).
6 Conclusion
In summary, this paper described our participation in
the the SemEval-2007 coarse- and fine-grained all-
words tasks. In particular, we described an exponen-
tial family model of word sense amenable to the task
of word sense disambiguation. The performance of
the model under a variety of parameter settings was
evaluated on both tasks and the model was shown to
be particularly effective on the fine-grained task.
7 Acknowledgments
The authors would like to thank Christiane Fell-
baum, Daniel Osherson, and the members of the
CIMPL group for their helpful contributions. This
research was supported by a grant from Google Inc.
and by NSF grant CCR-0325463.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to natural
language processing. Computational Linguistics, 22(1):39?
71.
Lawrence D. Brown. 1986. Fundamentals of Statistical Expo-
nential Families. Institute of Mathematical Statistics, Hay-
ward, CA.
Christiane Fellbaum and George A. Miller. 2003. Mor-
phosemantic links in WordNet. Traitement automatique de
langue.
Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and
Lawrence K. Saul. 1999. An introduction to varia-
tional methods for graphical models. Machine Learning,
37(2):183?233.
Michael I. Jordan. 2004. Graphical models. Statistical Science,
19(1):140?155.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-Based Construction of a Verb Lexicon. Proceedings
of the Seventeenth National Conference on Artificial Intelli-
gence and Twelfth Conference on Innovative Applications of
Artificial Intelligence table of contents, pages 691?696.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics, pages 280?287, Barcelona,
Spain.
George A. Miller, Claudia Leacock, Randee Tengi, and Ross T.
Bunker. 1993. A semantic concordance. In 3rd DARPA
Workshop on Human Language Technology.
Roberto Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In COLING-
ACL 2006, pages 105?112, July.
276
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 277?281,
Prague, June 2007. c?2007 Association for Computational Linguistics
PUTOP: Turning Predominant Senses into a Topic Model for Word Sense
Disambiguation
Jordan Boyd-Graber
Computer Science
Princeton University
Princeton, NJ 08540
jbg@princeton.edu
David Blei
Computer Science
Princeton University
Princeton, NJ 08540
blei@cs.princeton.edu
Abstract
We extend on McCarthy et al?s predom-
inant sense method to create an unsuper-
vised method of word sense disambiguation
that uses automatically derived topics us-
ing Latent Dirichlet alocation. Using topic-
specific synset similarity measures, we cre-
ate predictions for each word in each doc-
ument using only word frequency informa-
tion. It is hoped that this procedure can im-
prove upon the method for larger numbers
of topics by providing more relevant train-
ing corpora for the individual topics. This
method is evaluated on SemEval-2007 Task
1 and Task 17.
1 Generative Model of WSD
Word Sense Disambiguation (WSD) is the problem
of labeling text with the appropriate semantic labels
automatically. Although WSD is claimed to be an
essential step in information retrieval and machine
translation, it has not seen effective practical appli-
cation because the dearth of labeled data has pre-
vented the use of established supervised statistical
methods that have been successfully applied to other
natural language problems.
Unsupervised methods have been developed for
WSD, but despite modest success have not al-
ways been well understood statistically (Abney,
2004). Unsupervised methods are particularly ap-
pealing because they do not require expensive sense-
annotated data and can use the ever-increasing
amount of raw text freely available. This paper ex-
pands on an effective unsupervised method for WSD
and embeds it into a topic model, thus allowing an
algorithm trained on a single, monolithic corpora to
instead hand-pick relevant documents in choosing
a disambiguation. After developing this generative
statistical model, we present its performance on a
number of tasks.
1.1 The Intersection of Syntactic and Semantic
Similarity
McCarthy et al (2004) outlined a method for learn-
ing a word?s most-used sense given an untagged cor-
pus that ranks each sense wsi using a distributional
syntactic similarity ? and a WORDNET-derived se-
mantic similarity ?. This process for a word w uses
its distributional neighbors Nw, the possible senses
of not only the word in question, Sw, and also those
of the distributionally similar words, Snj . Thus,
P (wsi) =
?
nj?Nw
?(w, nj)
wnss(wsi, nj)
?
wsj?Sw
wnss(wsj , nj)
, (1)
where wnss(s, c) =
max
a?Sc
?(a, s). (2)
One can view finding the appropriate sense as a
search in two types of space. In determining how
good a particular synset wsi is, ? guides the search
in the semantic space and ? drives the search in the
syntactic space. We consider all of the words used
in syntactically similar contexts, which we call ?cor-
roborators,? and for each of them we find the closest
meaning to wsi using a measure of semantic sim-
ilarity ?, for instance a WORDNET-based similar-
ity measure such as Jiang-Conrath (1997). Each of
the neighboring words? contributions is weighted by
the syntactic probability, as provided by Lin?s distri-
butional similarity measure (1998), which rates two
words to be similar if they enter into similar syntac-
tic constructions.
277
Vw c
s
Figure 1: A reinterpretation of McCarthy et al?s pre-
dominant sense method as a generative model. Note
that this model has no notion of context; a synset is
assigned in an identical manner for all of the words
in a vocabulary.
One can think of this process as a generative
model, even though it was not originally posed in
such a manner. For each word w in the vocabulary,
we generate one of the neighbor corroborators ac-
cording to the Lin similarity, ?(c, w), between the
two words. We then generate a synset s for that
word proportional to the maximum semantic sim-
ilarity between s and any synset that contains the
corroborator c (see Figure 1).
Our aim in this paper is to extend the method of
McCarthy et al using topic models. It is hoped that
allowing the method to in effect ?choose? the con-
texts that it uses will improve its ability to disam-
biguate sentences.
1.2 Using Topic Models to Partition a
Document?s Words
Topic models like Latent Dirichlet alocation
(LDA) (Blei et al, 2003) assume a model of text
generation where each document has a multinomial
distribution over topics and each word comes from
one of these topics. In LDA, each topic is a multino-
mial distribution, and each document has a multino-
mial distribution over topics drawn from a Dirichlet
prior that selects the topic for each word in a docu-
ment. Previous work has shown that such a model
improves WSD over using a single corpus (Boyd-
Graber et al, 2007), and we use this insight to de-
velop an extension of McCarthy?s method for multi-
ple topics.
Although describing the statistical background
and motivations behind topic models are beyond the
scope of this paper, it suffices to note that the topics
induced from a corpus provide a statistical group-
ing of words that often occur together and a proba-
bilistic assignment of each word in a corpus to top-
ics. Thus, one topic might have terms like ?gov-
ernment,? ?president,? ?govern,? and ?regal,? while
another topic might have terms like ?finance,? ?high-
yield,? ?investor,? and ?market.? This paper assumes
that the machinery for learning these distributions
can, given a corpus and a specified number of top-
ics, return the topic distributions most likely to have
generated the corpus.
1.3 Defining the Model
While the original predominant senses method used
Lin?s thesaurus similarity method alone in generat-
ing the corroborator, we will also use the probability
of that word being part of the same topic as the word
to be disambiguated. Thus the process of choosing
the ?corroborator? is no longer identical for each
word; it is affected by its topic, which changes for
every document. This new generative process can
be thought of as a modified LDA system that, after
selecting the word generated by the topic, continues
on by generating a corroborator and a sense for the
original word:
For each document d ? {1 . . .D}:
1. Select a topic distribution ?d ? Dir(?)
2. For each word in the document n ? {1 . . . N}:
(a) Select a topic zn ? Mult(1, ?d)
(b) Select a word from that topic wn ? Mult(1, ?z)
(c) Select a ?corroborator? cn also proportional to how
important it is to the topic and its similarity to w
(d) Now, select a synset sn for that word based on a
distribution p(sn|wn, cn, zn)
The conditional dependencies for generating a
synset are shown in Figure 2. Our goal, like Mc-
Carthy et al?s, is to determine the most likely sense
for each word. This amounts to posterior inference,
which we address by marginalizing over the unob-
served variables (the topics and the corroborators),
where p(wsi) =
p(s|w) =
?
?
?
z
?
c
p(s|w, c, z)p(c|z, w)p(z|w, ?).
(3)
In order to fully specify this, we must determine the
distribution from which the corroborator is drawn
and the distribution from which the synset is drawn.
Ideally, we would want a distribution that for a
single topic would be identical to McCarthy et al?s
278
KD
N
? z
?
w c
s
Figure 2: Our generative model assumes that doc-
uments are divided into topics and that these topics
generate both the observed word and a ?corrobora-
tor,? a term similar in usage to the word. Next, a
sense that minimizes the semantic distance between
the corroborator and the word is generated.
method but would, as more topics are added, favor
corroborators in the same topic as the number of top-
ics increases. In McCarthy et al?s method, the prob-
ability of the corroborator given a word w is pro-
portional to the Lin similarity ?(w, c) between the
word and the corroborator. Here, the probability of
a corroborator c is
p(c|z, w) ?
?z,c
?0c
?(w, c), (4)
where ?z,c is the multinomial probability of word c
in the zth topic, and ?0c is the multinomial probabil-
ity of the word with a single topic (i.e. background
word probability).
Before, the corroborator was weighted simply
based on its syntactic similarity to the word w, now
we also weight that contribution by how important
(or unimportant) that word is to the topic that w has
been assigned to. This has the effect of increasing
the probability of words pertinent to the topic that
also have high syntactic similarity. Thus, whenever
the syntactic similarity captures polysemous usage,
we hope to be able to separate the different usages.
Note, however, that since for a single topic the ?
term cancels out and the procedure is equivalent to
McCarthy et al
We adapt the semantic similarity in much the
same way to make it topic specific. Because the
Jiang-Conrath similarity measure uses an underly-
ing term frequency to generate a similarity score, we
use the topic term frequency instead of the undivided
term frequency. Thus, the probability of a sense is
proportional to semantic similarity between it and
the closest sense among the senses of a corroborator
with respect to this topic-specific similarity (c.f. the
global similarity in Equation 2). The probability of
selecting a synset s given the corroborator c and a
topic z then becomes
p(s|w, c, z) ? max
s??S(c)
?z(s, s
?). (5)
This new dependence on the topic happens be-
cause we recompute the information content used by
Jiang-Conrath with the distribution over words im-
plied by each topic. We then use the similarity im-
plied by that similarity for ?z . Following the lead of
McCarthy, for notational ease, this becomes defined
as wnss in Equation 8.
1.4 Choosing a Synset
The problem of choosing a synset then is reduced to
finding the synset with the highest probability under
this model. The model is also designed so that the
task of learning the assignment of topics to words
and documents is not affected by this new machin-
ery for corroborators and senses that we?ve added
onto the model. Thus, we can use the variational in-
ference method described in (Blei et al, 2003) as a
foundation for the problem of synset inference.
Taking p(z|w) as a given (i.e. determined by run-
ning LDA on the corpus), the probability for a synset
s given a word w then becomes
p(s|w, z) =
?
z
?
c
p(s|w, c, z)p(c|z)p(z|w), (6)
whose terms have been described in the previous
section. With all of the normalization terms, we now
see that p(s|w, z) becomes
?
z
?
c
?z,c
?0c
?(w, c)
?
c?
?z,c
?0c
?(w, c?)
wnss(s, c, z)
?
s??Sw wnss(s
?, c, z)
.
(7)
and wnss(s, c, z) now becomes, for the zth topic,
max
a?S(c)
?z(a, s). (8)
Thus, we?ve now assigned a probability to each of
the possible senses a word can take in a document.
279
1.5 Intuition
For example, consider the word ?fly,? which has two
other words that have high syntactic similarity (in
our formulation, ?) with the terms ?fly ball? and ?in-
sect.? Both of these words would, given the seman-
tic similarity provided by WORDNET, point to a sin-
gle sense of ?fly;? one of them would give a higher
value, however, and thus all senses of the word ?fly?
would be assigned that sense. By separately weight-
ing these words by the topic frequencies, we would
hope to choose the sports sense in topics that have
a higher probability of the terms like ?foul ball,?
?pop fly,? and ?grounder? and the other sense in the
contexts where insect has a higher probability in the
topic.
2 Evaluations
This section describes three experiments to deter-
mine the effectiveness of this unsupervised system.
The first was used to help understand the system,
and the second two were part of the SemEval 2007
competition.
2.1 SemCor
As an initial evaluation, we learned LDA topics on
the British National corpus with paragraphs as the
underlying ?document? (this allowed for a more uni-
form document length). These documents were then
used to infer topic probabilities for each of the words
in SemCor (Miller et al, 1993), and the model de-
scribed in the previous section was run to determine
the most likely synset. The results of this procedure
are shown in Table 1. Accuracy is determined as the
percentage of words for which the most likely sense
was the one tagged in the corpus.
While the method does roughly recreate Mc-
Carthy et al?s result for a single topic, it only of-
fers a one percent improvement over McCarthy et
al. on five topics and then falls below McCarthy for
all greater numbers of topics tried. Thus, for all
subsequent experiments we used a five topic model
trained on the BNC.
2.2 SemEval-2007 Task 1: CLIR
Using IR metrics, this disambiguation scheme was
evaluated against another competing platform and
an algorithm provided by the Task 1 (Agirre et al,
Topics All Nouns
1 .393 .467
5 .397 .478
25 .387 .456
200 .359 .420
Table 1: Accuracy on disambiguating words in Sem-
Cor
Task PUTOP
Topic Expansion 0.30
Document Expansion 0.15
English Translation 0.17
SensEval 2 0.39
SensEval 3 0.33
Table 2: Performance results on Task 1
2007) organizers. Our system had the best results of
any expansion scheme considered (0.30) , although
none of the expansion schemes did better than us-
ing no expansion (0.36). Although our technique
also yielded a better score than the other competing
platform for cross-language queries (0.17), it did not
surpass the first sense-heuristic (0.26), but this is not
surprising given that our algorithm does not assume
the existence of such information. For an overview
of Task 1 results, see Table 2.
2.3 SemEval-2007 Task 17: All-Words
Task 17 (Pradhan et al, 2007) asked participants
to submit results as probability distributions over
senses. Because this is also the output of this algo-
rithm, we submitted the probabilities to the contest
before realizing that the distributions are very close
to uniform over all senses and thus yielded a pre-
cision of 0.12, very close to the random baseline.
Placing a point distribution on the argmax with our
original submission to the task, however, (consistent
with our methodology for evaluation on SemCor),
gives a precision of 0.39.
3 Conclusion
While the small improvement over the single topic
suggests that topic techniques might have traction
in determining the best sense, the addition is not ap-
preciable. In a way the failure of the technique is en-
280
couraging in that it affirms the original methodology
of McCarthy et al in finding a single predominant
sense for each word. While the syntactic similarity
measure indeed usually offers high values of similar-
ity for words related to a single sense of a word, the
similarity for words related to other senses, which
we had hoped to strengthen by using topic features,
are on par with words observed because of noise.
Thus, for a word like ?bank,? words like
?firm,? ?commercial bank,? ?company,? and ?finan-
cial institution? are the closest in terms of the syn-
tactic similarity, and this allows the financial senses
to be selected without any difficulty. Even if we had
corroborating words for another sense in some topic,
these words are absent from the syntactically simi-
lar words. If we want the meaning similar to that of
?riverbank,? the word with the most similar mean-
ing, ?side,? had a syntactic similarity on par with the
unrelated words ?individual? and ?group.? Thus, in-
terpretations other than the dominant sense as deter-
mined by the baseline method of McCarthy et al are
hard to find.
Because one topic is equivalent to McCarthy et
al.?s method, this means that we do no worse on
disambiguation. However, contrary to our hope, in-
creasing the number of topics does not lead to sig-
nificantly better sense predictions. This work has not
investigated using a topic-based procedure for deter-
mining the syntactic similarity, but we feel that this
extension could provide real improvement to the un-
supervised techniques that can make use of the co-
pious amounts of available unlabeled data.
References
Steven Abney. 2004. Understanding the yarowsky algo-
rithm. Comput. Linguist., 30(3):365?395.
Eneko Agirre, Oier Lopez de Lacalle, Arantxa Otegi,
German Rigau, and Piek Vossen. 2007. The Senseval-
2007 Task 1: Evaluating WSD on cross-language in-
formation retrieval. In Proceedings of SemEval-2007.
Association for Computational Linguistics.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Jordan L. Boyd-Graber, David M. Blei, and Jerry Zhu.
2007. Probabalistic walks in semantic hierarchies as a
topic model for WSD. In Proc. EMNLP 2007.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296?304. Morgan Kaufmann,
San Francisco, CA.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In In 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 280?287.
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In 3rd
DARPA Workshop on Human Language Technology,
pages 303?308.
Sameer Pradhan, Martha Palmer, and Edward Loper.
2007. The Senseval-2007 Task 17: English fine-
grained all-words. In Proceedings of SemEval-2007.
Association for Computational Linguistics.
281
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 227?237,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Bayesian Checking for Topic Models
David Mimno
Department of Computer Science
Princeton University Princeton, NJ 08540
mimno@cs.princeton.edu
David Blei
Department of Computer Science
Princeton University Princeton, NJ 08540
blei@cs.princeton.edu
Abstract
Real document collections do not fit the inde-
pendence assumptions asserted by most statis-
tical topic models, but how badly do they vi-
olate them? We present a Bayesian method
for measuring how well a topic model fits a
corpus. Our approach is based on posterior
predictive checking, a method for diagnosing
Bayesian models in user-defined ways. Our
method can identify where a topic model fits
the data, where it falls short, and in which di-
rections it might be improved.
1 Introduction
Probabilistic topic models are a suite of machine
learning algorithms that decompose a corpus into
a set of topics and represent each document with a
subset of those topics. The inferred topics often cor-
respond with the underlying themes of the analyzed
collection, and the topic modeling algorithm orga-
nizes the documents according to those themes.
Most topic models are evaluated by their predic-
tive performance on held out data. The idea is that
topic models are fit to maximize the likelihood (or
posterior probability) of a collection of documents,
and so a good model is one that assigns high likeli-
hood to a held out set (Blei et al, 2003; Wallach et
al., 2009).
But this evaluation is not in line with how
topic models are frequently used. Topic mod-
els seem to capture the underlying themes of a
collection?indeed the monicker ?topic model? is
retrospective?and so we expect that these themes
are useful for exploring, summarizing, and learning
about its documents (Mimno and McCallum, 2007;
Chang et al, 2009). In such exploratory data anal-
ysis, however, we are not concerned with the fit to
held out data.
In this paper, we develop and study new methods
for evaluating topic models. Our methods are based
on posterior predictive checking, which is a model
diagnosis technique from Bayesian statistics (Rubin,
1984; Gelman et al, 1996). The goal of a posterior
predictive check (PPC) is to assess the validity of a
Bayesian model without requiring a specific alterna-
tive model. Given data, we first compute a posterior
distribution over the latent variables. Then, we esti-
mate the probability of the observed data under the
data-generating distribution that is induced by the
posterior (the ?posterior predictive distribution?). A
data set that is unlikely calls the model into ques-
tion, and consequently the posterior. PPCs can show
where the model fits and doesn?t fit the observations.
They can help identify the parts of the posterior that
are worth exploring.
The key to a posterior predictive check is the dis-
crepancy function. This is a function of the data that
measures a property of the model which is impor-
tant to capture. While the model is often chosen
for computational reasons, the discrepancy function
might capture aspects of the data that are desirable
but difficult to model. In this work, we will design
a discrepancy function to measure an independence
assumption that is implicit in the modeling assump-
tions but is not enforced in the posterior. We will
embed this function in a posterior predictive check
and use it to evaluate and visualize topic models in
new ways.
227
Specifically, we develop discrepancy functions
for latent Dirichlet alocation (the simplest topic
model) that measure how well its statistical assump-
tions about the topics are matched in the observed
corpus and inferred topics. LDA assumes that each
observed word in a corpus is assigned to a topic, and
that the words assigned to the same topic are drawn
independently from the same multinomial distribu-
tion (Blei et al, 2003). For each topic, we mea-
sure the whether this assumption holds by comput-
ing the mutual information between the words as-
signed to that topic and which document each word
appeared in. If the assumptions hold, these two vari-
ables should be independent: low mutual informa-
tion indicates that the assumptions hold; high mu-
tual information indicates a mismatch to the model-
ing assumptions.
We embed this discrepancy in a PPC and study
it in several ways. First, we focus on topics that
model their observations well; this helps separate
interpretable topics from noisy topics (and ?boiler-
plate? topics, which exhibit too little noise). Sec-
ond, we focus on individual terms within topics; this
helps display a model applied to a corpus, and under-
stand which terms are modeled well. Third, we re-
place the document identity with an external variable
that might plausibly be incorporated into the model
(such as time stamp or author). This helps point the
modeler towards the most promising among more
complicated models, or save the effort in fitting one.
Finally, we validate this strategy by simulating data
from a topic model, and assessing whether the PPC
?accepts? the resulting data.
2 Probabilistic Topic Modeling
Probabilistic topic models are statistical models of
text that assume that a small number of distributions
over words, called ?topics,? are used to generate the
observed documents. One of the simplest topic mod-
els is latent Dirichlet alocation (LDA) (Blei et al,
2003). In LDA, a set of K topics describes a cor-
pus; each document exhibits the topics with different
proportions. The words are assumed exchangeable
within each document; the documents are assumed
exchangeable within the corpus.
More formally, let ?1, . . . , ?K be K topics, each
of which is a distribution over a fixed vocabulary.
For each document, LDA assumes the following
generative process
1. Choose topic proportions ?d ? Dirichlet(?).
2. For each word
(a) Choose topic assignment zd,n ? ?.
(b) Choose word wd,n ? ?zd,n .
This process articulates the statistical assumptions
behind LDA: Each document is endowed with its
own set of topic proportions ?d, but the same set of
topics ?1:K governs the whole collection.
Notice that the probability of a word is indepen-
dent of its document ?d given its topic assignment
zd,n (i.e., wd,n ? ?d | zd,n). Two documents might
have different overall probabilities of containing a
word from the ?vegetables? topic; however, all the
words in the collection (regardless of their docu-
ments) drawn from that topic will be drawn from the
same multinomial distribution.
The central computational problem for LDA is
posterior inference. Given a collection of docu-
ments, the problem is to compute the conditional
distribution of the hidden variables?the topics ?k,
topic proportions ?d, and topic assignments zd,n.
Researchers have developed many algorithms for
approximating this posterior, including sampling
methods (Griffiths and Steyvers, 2004) (used in this
paper), variational methods (Blei et al, 2003), dis-
tributed variants (Asuncion et al, 2008), and online
algorithms (Hoffman et al, 2010).
3 Checking Topic Models
Once approximated, the posterior distribution is
used for the task at hand. Topic models have been
applied to many tasks, such as classification, predic-
tion, collaborative filtering, and others. We focus
on using them as an exploratory tool, where we as-
sume that the topic model posterior provides a good
decomposition of the corpus and that the topics pro-
vide good summaries of the corpus contents.
But what is meant by ?good?? To answer this
question, we turn to Bayesian model checking (Ru-
bin, 1981; Gelman et al, 1996). The goal of
Bayesian model checking is to assess whether the
observed data matches the modeling assumptions in
the directions that are important to the analysis. The
228
Score
Ran
k
14
12
10
8
6
4
2
14
12
10
8
6
4
2
14
12
10
8
6
4
2
Topic850
weekendBroadwayTimeslisting
selective
noteworthy
critics ticketshighly
recommendeddenotesboothTicketsStreetTKTS
Topic628
IraqIraqi HusseinBaghdadSaddamShiitegovernment
al IraqisSunniKurdishforces
country
militarytroops
Topic87
Roberts GrantFortWorth BurkeHuntKravis BassKohlbergGraceRothschildBaronBordenTexasWilliam
1 2 3 4
Topic371
TicketsThroughStreetRoadSaturdaysSundaysNewFridaysJerseyHoursFreeTuesdaysMUSEUMThursdaysTHEATER
Topic178
agency
safety
reportFederalAdministrationproblemsinvestigationSafety
violationsfederalfailedinspector
reviewdepartmentgeneral
Topic750
Four FreemanSeasonsDaVinciCode ThomsonWolffLeonardoBrownThreeDanCliffHolyda
1 2 3 4
Topic760
WeekbookWarner
salesListWeeks
womanbookstoresdeathindicatesAdvicePutnamOF
reportNew
Topic632job jobs
working
officebusiness
career
worked
employeeshiredboss
managerfind
corporatehelp
experience
Topic274
LeonLevy HessBardLEVYBotsteinAtlas ShelbyPanetta Norman WieseltierHESSDavidAmerada Norma
1 2 3 4
Figure 1: Visualization of variability within topics. Nine randomly selected topics from the New York Times with
low (top row), medium (middle row) and high (bottom row) mutual information between words and documents. The
y-axis shows term rank within the topic, with size proportional to log probability. The x-axis represents divergence
from the multinomial assumption for each word: terms that are uniformly distributed across documents are towards
the left, while more specialized terms are to the right. Triangles represent real values, circles represent 20 replications
of this same plot from the posterior of the model.
229
intuition is that only when satisfied with the model
should the modeler use the posterior to learn about
her data. In complicated Bayesian models, such as
topic models, Bayesian model checking can point to
the parts of the posterior that better fit the observed
data set and are more likely to suggest something
meaningful about it.
In particular, we will develop posterior predictive
checks (PPC) for topic models. In a PPC, we spec-
ify a discrepancy function, which is a function of
the data that measures an important property that we
want the model to capture. We then assess whether
the observed value of the function is similar to val-
ues of the function drawn from the posterior, through
the distribution of the data that it induces. (This dis-
tribution of the data is called the ?posterior predic-
tive distribution.?)
An innovation in PPCs is the realized discrepancy
function (Gelman et al, 1996), which is a function
of the data and any hidden variables that are in the
model. Realized discrepancies induce a traditional
discrepancy by marginalizing out the hidden vari-
ables. But they can also be used to evaluate assump-
tions about latent variables in the posterior, espe-
cially when combined with techniques like MCMC
sampling that provide realizations of them. In topic
models, as we will see below, we use a realized dis-
crepancy to factor the observations and to check spe-
cific components of the model that are discovered by
the posterior.
3.1 A realized discrepancy for LDA
Returning to LDA, we design a discrepancy func-
tion that checks the independence assumption of
words given their topic assignments. As we men-
tioned above, given the topic assignment z the word
w should be independent of its document ?. Con-
sider a decomposition of a corpus from LDA, which
assigns every observed word wd,n to a topic zd,n.
Now restrict attention to all the words assigned to the
kth topic and form two random variables: W are the
words assigned to the topic and D are the document
indices of the words assigned to that topic. If the
LDA assumptions hold then knowing W gives no
information about D because the words are drawn
independently from the topic.
We measure this independence with the mutual
information between W and D:1
MI(W,D | k)
=
?
w
?
d
P (w, d | k) log P (w | d, k)P (d | k)P (w | k)P (d | k)
=
?
w
?
d
N(w, d, k)
N(k) log
N(w, d, k)N(k)
N(d, k)N(w, k) . (1)
Where N(w, d, k) is the number of tokens of type
w in topic k in document d, with N(w, k) =?
dN(w, d, k), N(d, k) =
?
wN(w, d, k), and
N(k) =
?
w,dN(w, d, k). This function mea-
sures the divergence between the joint distribution
over word and document index and the product of
the marginal distributions. In the limit of infinite
samples, independent random variables have mutual
information of zero, but we expect finite samples
to have non-zero values even for truly independent
variables. Notice that this is a realized discrepancy;
it depends on the latent assignments of observed
words to topics.
Eq. 1 is defined as a sum over a set of documents
and a set of words. We can rearrange this summa-
tion as a weighted sum of the instantaneous mutual
information between words and documents:
IMI(w,D | k) = H(D|k)?H(D |W = w, k).
(2)
This quantity can be understood by considering the
per-topic distribution of document labels, p(d|k).
This distribution is formed by normalizing the
counts of how many words assigned to topic k ap-
peared in each document. The first term of Eq. 2
is the entropy?some topics are evenly distributed
across many documents (high entropy); others are
concentrated in fewer documents (low entropy).
The second term conditions this distribution on
a particular word type w by normalizing the per-
document number of times w appeared in each doc-
ument (in topic k). If this distribution is close
to p(d|k) then H(D|W = w, k) will be close to
H(D|k) and IMI(w,D|k) will be low. If, on the
other hand, word w occurs many times in only a few
documents, it will have lower entropy over docu-
1There are other choices of discrepancies, such as word-
word point-wise mutual information scores (Newman et al,
2010).
230
ments than the overall distribution over documents
for the topic and IMI(w,D|k) will be high.
We illustrate this discrepancy in Figure 1, which
shows nine topics trained from the New York Times.2
Each row contains randomly selected topics from
low, middle, and high ranges of MI, respectively.
Each triangle represents a word. Its place on the y-
axis is its rank in the topic. Its place on the x-axis
is its IMI(w|k), with more uniformly distributed
words (low IMI) to the left and more specific words
(high IMI) to the right. (For now, ignore the other
points in this figure.) IMI varies between topics, but
tends to increase with rank as less frequent words
appear in fewer documents.
The discrepancy captures different kinds of struc-
ture in the topics. The top left topic represents for-
mulaic language, language that occurs verbatim in
many documents. In particular, it models the boil-
erplate text ?Here is a selective listing by critics of
The Times of new or noteworthy...? Identifying re-
peated phrases is a common phenomenon in topic
models. Most words show lower than expected IMI,
indicating that word use in this topic is less vari-
able than data drawn from a multinomial distribu-
tion. The middle-left topic is an example of a good
topic, according to this discrepancy, which is related
to Iraqi politics. The bottom-left topic is an example
of the opposite extreme from the top-left. It shows
a loosely connected series of proper names with no
overall theme.
3.2 Posterior Predictive Checks for LDA
Intuitively, the middle row of topics in Figure 1 are
the sort of topics we look for in a model, while the
top and bottom rows contain topics that are less use-
ful. Using a PPC, we can formally measure the dif-
ference between these topics. For each of the real
topics in Figure 1 we regenerated the same figure
20 times. We sampled new words for every token
from the posterior distribution of the topic, and re-
calculated the rank and IMI for each word. These
?shadow? figures are shown as gray circles. The
density of those circles creates a reference distribu-
tion indicating the expected IMI values at each rank
under the multinomial assumption.
2Details about the corpus and model fitting are in Section
4.2. Similar figures for two other corpora are in the supplement.
By themselves, IMI scores give an indication of
the distribution of a word between documents within
a topic: small numbers are better, large numbers in-
dicate greater discrepancy. These scores, however,
are based on the specific allocation of words to top-
ics. For example, lower-ranked, less frequent words
within a topic tend to have higher IMI scores than
higher-ranked, more frequent words. This difference
may be due to greater violation of multinomial as-
sumptions, but may also simply be due to smaller
sample sizes, as the entropy H(D|W = w, k) is es-
timated from fewer tokens. The reference distribu-
tions help distinguish between these two cases.
In more detail, we generate replications of the
data by considering a Gibbs sampling state. This
state assigns each observed word to a topic. We
first record the number of instances of each term as-
signed to each topic, N(w|k). Then for each word
wd,n in the corpus, we sample a new observed word
wrepd,n where P (w) ? N(w|zd,n). (We did not usesmoothing parameters.) Finally, we recalculate the
mutual information and instantenous mutual infor-
mation for each topic.
In the top-left topic, most of the words have much
lower IMI than the word at the same rank in repli-
cations, indicating lower than expected variability.
The exception is the word Broadway, which is more
variable than expected. In the middle-left topic,
IMI for the words Iraqi and Baghdad occur within
the expected range. These words fit the multino-
mial assumption: any word assigned to this topic
is equally likely to be Iraqi. Values for the words
Shiite, Sunni, and Kurdish are more specific to par-
ticular documents than we expect under the model.
In the bottom-left topic, almost all words occur with
greater variability than expected. This topic com-
bines many terms with only coincidental similarity,
such as Mets pitcher Grant Roberts and the firm
Kohlberg Kravis Roberts.
Turning to an analysis of the full mutual infor-
mation, Figure 2 shows the three left-hand topics
from Figure 1: Weekend, Iraq, and Roberts. The
histogram represents MI scores for 100 replications
of the topic, rescaled to have mean zero and unit
variance. The observed value, also rescaled, and
the mean replicated value (set to zero) are shown
with vertical lines. The formulaic Weekend topic
has significantly lower than expected MI. The Iraq
231
Deviance
cou
nt
05
1015
2025
30
05
1015
2025
30
05
1015
2025
30
Topic850
Topic628
Topic87
?20 0 20 40
Figure 2: News: Observed topic scores (vertical lines)
relative to replicated scores, rescaled so that replica-
tions have zero mean and unit variance. The Weekend
topic (top) has lower than expected MI. The Iraq (mid-
dle) and Roberts (bottom) topics both have MI greater
than expected.
and Roberts topics have significantly greater than
expected MI.
For most topics the actual discrepancy is outside
the range of any replicated discrepancies. In their
original formulation, PPCs prescribe computing a
tail probability of a replicated discrepancy being
greater than (or less than) the observed discrepancy
under the posterior predictive distribution. For ex-
ample if an observed value is greater than 70 of 100
replicated values, we report a PPC p-value of 0.7.
When the observed value is far outside the range
of any replicated values, as in Figure 2, that tail
probability will be degenerate at 0 or 1. So, we re-
port instead a deviance value, an alternative way of
comparing an observed value to a reference distri-
bution. We compute the distribution of the repli-
cated discrepancies and compute its standard devi-
ation. We then compute how many standard devia-
tions the observed discrepancy is from the mean of
the replicated discrepancies.
This score allows us to compare topics. The ob-
served value for the Weekend topic is 31.8 standard
deviations below the mean replicated value, and thus
has deviance of -31.8, which is lower than expected.
The Iraq topic has deviance of 16.8 and the Roberts
topic has deviance of 47.7. This matches our intu-
ition that the former topic is more useful than the
latter.
4 Searching for Systematic Deviations
We demonstrated that the mutual information dis-
crepancy function can detect violations of multi-
nomial assumptions, in which instances of a term
in a given topic are not independently distributed
among documents. One way to address this lack
of fit is to encode document-level extra-multinomial
variance (?burstiness?) into the model using Dirich-
let compound multinomial distributions (Doyle and
Elkan, 2009). If there is no pattern to the deviations
from multinomial word use across documents, this
method is the best we can do.
In many corpora, however, there are systematic
deviations that can be explained by additional vari-
ables. LDA is the simplest generative topic model,
and researchers have developed many variants of
LDA that account for a variety of variables that can
be found or measured with a corpus. Examples in-
clude models that account for time (Blei and Laf-
ferty, 2006), books (Mimno and McCallum, 2007),
and aspect or perspective (Mei and Zhai, 2006; Lin
et al, 2008; Paul et al, 2010). In this section, we
show how we can use the mutual information dis-
crepancy function of Equation 1 and PPCs to guide
our choice in which topic model to fit.
Greater deviance implies that a particular group-
ing better explains the variation in word use within
a topic. The discrepancy functions are large when
words appear more than expected in some groups
and less than expected in others. We know that
the individual documents show significantly more
variation than we expect from replications from the
model?s posterior distribution. If we combine docu-
ments randomly in a meaningless grouping, such de-
viance should decrease, as differences between doc-
uments are ?smoothed out.? If a grouping of docu-
ments shows equal or greater deviation, we can as-
sume that that grouping is maintaining the underly-
ing structure of the systematic deviation from the
multinomial assumption, and that further modeling
or visualization using that grouping might be useful.
4.1 PPCs for systematic discrepancy
The idea is that the words assigned to a topic should
be independent of both document and any other vari-
able that might be associated with the document. We
simply replace the document index d with another
232
Score
Ran
k
20
15
10
5
DocumentsIraqIraqi HusseinBaghdadSaddamShiitegovernmental IraqisSunniKurdishforcescountrymilitarytroopsleaderscity KurdssecuritySadr
0.0 0.5 1.0 1.5 2.0 2.5
MonthsIraqIraqiHusseinBaghdadSaddamShiitegovernmental IraqisSunniKurdishforcescountrymilitarytroopsleaderscityKurdssecurity Sadr
0.0 0.5 1.0 1.5 2.0 2.5
DesksIraqIraqiHusseinBaghdadSaddamShiitegovernmentalIraqisSunniKurdishforcescountrymilitarytroopsleaderscityKurdssecuritySadr
0.0 0.5 1.0 1.5 2.0 2.5
Figure 3: Groupings decrease MI, but values are still larger than expected. Three ways of grouping words in a
topic from the New York Times. The word leaders varies more between desks than by time, while Sadr varies more by
time than desk.
variable g in the discrepancy. For example, the New
York Times articles are each associated with a par-
ticular news desk and also associated with a time
stamp. If the topic modeling assumptions hold, the
words are independent of both these variables. If we
see a significant discrepancy relative to a grouping
defined by a metadata feature, this systematic vari-
ability suggests that we might want to take that fea-
ture into account in the model.
Let G be a set of groups and let ? ? GD be
a grouping of D documents. Let N(w, g, k) =?
dN(w, d, k)I?d=g, that is, the number of words of
typew in topic k in documents in group g, and define
the other count variables similarly. We can now sub-
stitute these group-specific counts for the document-
specific counts in the discrepancy function in Eq.
1. Note that the previous discrepancy functions are
equivalent to a trivial grouping, in which each docu-
ment is the only member of its own group. In the fol-
lowing experiments we explore groupings by pub-
lished volume, blog, preferred political candidate,
and newspaper desk, and evaluate the effect of those
groupings on the deviation between mean replicated
values and observed values of those functions.
4.2 Case studies
We analyze three corpora, each with its own meta-
data: the New York Times Annotated Corpus (1987?
2007)3, the CMU 2008 political blog corpus (Eisen-
stein and Xing, 2010), and speeches from the British
3http://www.ldc.upenn.edu
House of Commons from 1830?1891.4 Descriptive
statistics are presented in Table 1. The realization
is represented by a single Gibbs sampling state after
1000 iterations of Gibbs sampling.
Table 1: Statistics for models used as examples.
Name Docs Tokens Vocab Topics
News 1.8M 76M 121k 1000
Blogs 13k 2.2M 90k 100
Parliament 540k 55M 52k 300
New York Times articles. Figure 3 shows three
groupings of words for the middle-left topic in Fig-
ure 1: by document, by month of publication (e.g.
May of 2005), and by desk (e.g. Editorial, Foreign,
Financial). Instantaneous mutual information values
are significantly smaller for the larger groupings, but
the actual values are still larger than expected under
the model. We are interested in measuring the de-
gree to which word usage varies within topics as a
function of both time and the perspective of the ar-
ticle. For example, we may expect that word choice
may differ between opinion articles, which overtly
reflect an author?s views, and news articles, which
take a more objective, factual approach.
We summarize each grouping by plotting the dis-
tribution of deviance scores for all topics. Results
for all 1000 topics grouped by documents, months,
and desks are shown in Figure 4.
4http://www.hansard-archive.parliament.uk/
233
Deviance
Gro
upi
ng
Documents
Months
Desks
l lll l l ll ll l ll l
lll lll llll l ll ll lll lll ll
l lll lll l l lll l l ll lll l ll l
0 100 200 300 400
Figure 4: News: Lack of fit correlates best with desks.
We calculate the number of standard deviations between
the mean replicated discrepancy and the actual discrep-
ancy for each topic under three groupings. Boxes repre-
sent typical ranges, points represent outliers.
Month
Sco
re
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0e+002e?04
4e?046e?04
0.00000.0005
0.00100.0015
0.00200.0025
0e+002e?04
4e?046e?04
8e?04
?2e?040e+00
2e?044e?04
6e?04
Kurdish
Hussein
Sunni
Sadr
Maliki
Shiite
1987 1992 1997 2002 2007
Figure 5: News: Events change word distributions.
Words with the largest MI from a topic on Iraq?s gov-
ernment are shown, with individual scores grouped by
month.
Finally, we can analyze how individual words in-
teract with groupings like time or desk. Figure 5
breaks down the per-word discrepancy shown in Fig-
ure 3 by month, for the words with the largest overall
discrepancy. Kurdish is prominent during the Gulf
War and the 1996 cruise missile strikes, but is less
significant during the Iraq War. Individuals (Hus-
sein, Sadr, and Maliki) move on and off the stage.
Political blogs. The CMU 2008 political blog cor-
pus consists of six blogs, three of which supported
Barack Obama and three of which supported John
McCain. This corpus has previously been consid-
ered in the context of aspect-based topic models
(Ahmed and Xing, 2010) that assign distinct word
distributions to liberal and conservative bloggers. It
is reasonable to expect that blogs with different po-
litical leanings will use measurably different lan-
guage to describe the same themes, suggesting that
there will be systematic deviations from a multino-
mial hypothesis of exchangeability of words within
topics. Indeed, Ahmed and Xing obtained improved
results with such a model. Figure 6 shows the dis-
tribution of standard deviations from the mean repli-
cated value for a set of 150 topics grouped by doc-
ument, blog, and preferred candidate. Deviance is
greatest for blogs, followed by candidates and then
documents.
Deviance
Gro
upi
ng
Documents
Blogs
Candidates
l
lll
lll lll
0 100 200 300 400
Figure 6: Blogs: Lack of fit correlates more with blog
than preferred candidate. Grouping by preferred can-
didate has only slightly higher average deviance than by
documents, but the variance is greater.
Grouping by blogs appears to show greater de-
viance from mean replicated values than group-
ing by candidates, indicating that there is fur-
ther structure in word choice beyond a simple lib-
eral/conservative split. Are these results, however,
comparable? It may be that this difference is ex-
plained by the fact that there are six blogs and only
234
two candidates. To determine whether this particular
assignment of documents to blogs is responsible for
the difference in discrepancy functions or whether
any such split would have greater deviance, we com-
pared random groupings to the real groupings and
recalculate the PPC. We generated 10 such group-
ings by permuting document blog labels and another
10 by permuting document candidate labels, each
time holding the topics fixed. The average number
of standard deviations across topics was 6.6 ? 14.4
for permuted ?candidates? compared to 37.9? 39.2
for the real corpus, and 10.6 ? 12.9 for permuted
?blogs? compared to 44.4? 29.6 for real blogs.
British parliament proceedings. The parliament
corpus is divided into 305 volumes, each comprising
about three weeks of debates, with between 600 and
4000 speeches per session. In addition to volumes,
10 Prime Ministers were in office during this period.
Deviance
Gro
upi
ng
Documents
Volumes
PMs
ll
0 100 200 300 400
Figure 7: Parliament: Lack-of-fit correlates with time
(publication volume). Correlation with prime ministers
is not significantly better than with volume.
Grouping by prime minister shows greater av-
erage deviance than grouping by volumes, even
though there are substantially fewer divisions. Al-
though such results would need to be accompanied
by permutation experiments as in the blog corpus,
this methodology may be of interest to historians.
In order to provide insight into the nature of tem-
poral variation, we can group the terms in the sum-
mation in Equation 1 by word and rank the words by
their contribution to the discrepancy function. Fig-
ure 8 shows the most ?mismatching? words for a
topic with the most probable words ships, vessels,
admiralty, iron, ship, navy, consistent with changes
in naval technology during the Victorian era (that
is, wooden ships to ?iron clads?). Words that oc-
cur more prominently in the topic (ships, vessels)
are also variable, but more consistent across time.
Volume
Sco
re
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
iron
turret
clads
wooden
vessels
ships
1830 1835 1840 1845 1850 1855 1860 1865 1870 1875 1880 1885 1890
Figure 8: Parliament: iron-clads introduced in 1860s.
High probability words (ships, vessels) are variable, but
show less concentrated discrepancy than iron, wooden.
5 Calibration on Synthetic Data
A posterior predictive check asks ?do observations
sampled from the learned model look like the origi-
nal data?? In the previous sections, we have consid-
ered PPCs that explore variability within a topic on
a per-word basis, measure discrepancy at the topic
level, and compare deviance over all topics between
groupings of documents. Those results show that
the PPC detects deviation from multinomial assump-
tions when it exists: as expected, variability in word
choice aligns with known divisions in corpora, for
example by time and author perspective. We now
consider the opposite direction. When documents
are generated from a multinomial topic model, PPCs
should not detect systematic deviation.
We must also distinguish between lack of fit due
to model misspecification and lack of fit due to ap-
proximate inference. In this section, we present syn-
thetic data experiments where the learned model is
precisely the model used to generate documents. We
show that there is significant lack of fit introduced
by approximate inference, which can be corrected
by considering only parts of the model that are well-
estimated.
We generated 10 synthetic corpora, each consist-
ing of 100,000 100-word documents, drawn from 20
235
pco
un
t
0
10
20
30
40
All
0.0 0.2 0.4 0.6 0.8 1.0
TopDocs
0.0 0.2 0.4 0.6 0.8 1.0
TopWords
0.0 0.2 0.4 0.6 0.8 1.0
TopWordsDocs
0.0 0.2 0.4 0.6 0.8 1.0
Figure 9: Replicating only documents with large allocation in the topic leads to more uniform p-values. p-values
for 200 topics estimated from synthetic data generated from an LDA model are either uniform or skewed towards 1.0.
Overly conservative p-values would be clustered around 0.5.
topics over a vocabulary of 100 terms. Hyperpa-
rameters for both the document-topic and topic-term
Dirichlet priors were 0.1 for each dimension. We
then trained a topic model with the same hyperpa-
rameters and number of topics on each corpus, sav-
ing a Gibbs sampling state.
We can measure the fit of a PPC by examining the
distribution of empirical p-values, that is, the propor-
tion of replications wrep that result in discrepancies
less than the observed value. p-values should be uni-
formly distributed on (0, 1). Non-uniform p-values
indicate a lack of calibration. Unlike real collec-
tions, in synthetic corpora the range of discrepan-
cies from these replicated collections often includes
the real values, so p-values are meaningful. A his-
togram of p-values for 200 synthetic topics after 100
replications is shown in the left panel of Figure 9.
PPCs have been criticized for reusing training
data for model checking. For some models, the
posterior distribution is too close to the data, so all
replicated values are close to the real value, leading
to p-values clustered around 0.5 (Draper and Krn-
jajic, 2006; Bayarri and Castellanos, 2007). We
test divergence from a uniform distribution with a
Kolmogorov-Smirnov test. Our results indicate that
LDA is not overfitting, but that the distribution is not
uniform (KS p < 0.00001).
The PPC framework allows us to choose discrep-
ancy functions that reflect the relative importance
of subsets of words and documents. The second
panel in Figure 9 sums only over the 20 documents
with the largest probability of the topic, the third
sums over all documents but only over the top 10
most probable words, and the fourth sums over only
the top words and documents. This test indicates
that the distribution of p-values for the subset Top-
Words is not uniform (KS p < 0.00001), but that a
uniform distribution is a good fit for TopDocs (KS
p = 0.358) and TopWordsDocs (KS p = 0.069).
6 Conclusions
We have developed a Bayesian model checking
method for probabilistic topic models. Conditioned
on their topic assignment, the words of the docu-
ments are independently and identically distributed
by a multinomial distribution. We developed a real-
ized discrepancy function?the mutual information
between words and document indices, conditioned
on a topic?that checks this assumption. We em-
bedded this function in a posterior predictive check.
We demonstrated that we can use this posterior
predictive check to identify particular topics that fit
the data, and particular topics that misfit the data in
different ways. Moreover, our method provides a
new way to visualize topic models.
We adapted the method to corpora with external
variables. In this setting, the PPC provides a way to
guide the modeler in searching through more com-
plicated models that involve more variables.
Finally, on simulated data, we demonstrated that
PPCs with the mutual information discrepancy func-
tion can identify model fit and model misfit.
Acknowledgments
David M. Blei is supported by ONR 175-6343, NSF
CAREER 0745520, AFOSR 09NL202, the Alfred P.
Sloan foundation, and a grant from Google. David
Mimno is supported by a Digital Humanities Re-
search grant from Google. Arthur Spirling and Andy
236
Eggers suggested the use of the Hansards corpus.
References
Amr Ahmed and Eric Xing. 2010. Staying informed: Su-
pervised and semi-supervised multi-view topical anal-
ysis of ideological perspective. In EMNLP.
Arthur Asuncion, Padhraic Smyth, and Max Welling.
2008. Asynchronous distributed learning of topic
models. In NIPS.
M.J. Bayarri and M.E. Castellanos. 2007. Bayesian
checking of the second levels of hierarchical models.
Statistical Science, 22(3):322?343.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In ICML.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Ad-
vances in Neural Information Processing Systems 22,
pages 288?296.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In ICML.
David Draper and Milovan Krnjajic. 2006. Bayesian
model specification. Technical report, University of
California, Santa Cruz.
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
A. Gelman, X.L. Meng, and H.S. Stern. 1996. poste-
rior predictive assessment of model fitness via realized
discrepancies. Statistica Sinica, 6:733?807.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Matthew Hoffman, David Blei, and Francis Bach. 2010.
Online learning for latent dirichlet alocation. In NIPS.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In PKDD.
Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture
model for contextual text mining. In KDD.
David Mimno and Andrew McCallum. 2007. Organizing
the OCA: learning faceted subjects from a library of
digital books. In JCDL.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Human Language Technologies: The Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In EMNLP.
Donald B. Rubin. 1981. Estimation in parallel random-
ized experiments. Journal of Educational Statistics,
6:377?401.
D. Rubin. 1984. Bayesianly justifiable and relevant fre-
quency calculations for the applied statistician. The
Annals of Statistics, 12(4):1151?1172.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In ICML.
237
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 564?572,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Variational Inference for Adaptor Grammars
Shay B. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
scohen@cs.cmu.edu
David M. Blei
Computer Science Department
Princeton University
Princeton, NJ 08540, USA
blei@cs.princeton.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
Adaptor grammars extend probabilistic
context-free grammars to define prior dis-
tributions over trees with ?rich get richer?
dynamics. Inference for adaptor grammars
seeks to find parse trees for raw text. This
paper describes a variational inference al-
gorithm for adaptor grammars, providing
an alternative to Markov chain Monte Carlo
methods. To derive this method, we develop
a stick-breaking representation of adaptor
grammars, a representation that enables us
to define adaptor grammars with recursion.
We report experimental results on a word
segmentation task, showing that variational
inference performs comparably to MCMC.
Further, we show a significant speed-up when
parallelizing the algorithm. Finally, we report
promising results for a new application for
adaptor grammars, dependency grammar
induction.
1 Introduction
Recent research in unsupervised learning for NLP
focuses on Bayesian methods for probabilistic gram-
mars (Goldwater and Griffiths, 2007; Toutanova and
Johnson, 2007; Johnson et al, 2007). Such meth-
ods have been made more flexible with nonparamet-
ric Bayesian (NP Bayes) methods, such as Dirichlet
process mixture models (Antoniak, 1974; Pitman,
2002). One line of research uses NP Bayes meth-
ods on whole tree structures, in the form of adaptor
grammars (Johnson et al, 2006; Johnson, 2008b;
Johnson, 2008a; Johnson and Goldwater, 2009), in
order to identify recurrent subtree patterns.
Adaptor grammars provide a flexible distribu-
tion over parse trees that has more structure than
a traditional context-free grammar. Adaptor gram-
mars are used via posterior inference, the compu-
tational problem of determining the posterior distri-
bution of parse trees given a set of observed sen-
tences. Current posterior inference algorithms for
adaptor grammars are based on MCMC sampling
methods (Robert and Casella, 2005). MCMC meth-
ods are theoretically guaranteed to converge to the
true posterior, but come at great expense: they are
notoriously slow to converge, especially with com-
plex hidden structures such as syntactic trees. John-
son (2008b) comments on this, and suggests the use
of variational inference as a possible remedy.
Variational inference provides a deterministic al-
ternative to sampling. It was introduced for Dirich-
let process mixtures by Blei and Jordan (2005) and
applied to infinite grammars by Liang et al (2007).
With NP Bayes models, variational methods are
based on the stick-breaking representation (Sethu-
raman, 1994). Devising a stick-breaking represen-
tation is a central challenge to using variational in-
ference in this setting.
The rest of this paper is organized as follows. In
?2 we describe a stick-breaking representation of
adaptor grammars, which enables variational infer-
ence (?3) and a well-defined incorporation of recur-
sion into adaptor grammars. In ?4 we give an em-
pirical comparison of the algorithm to MCMC in-
ference and describe a novel application of adaptor
grammars to unsupervised dependency parsing.
2 Adaptor Grammars
We review adaptor grammars and develop a stick-
breaking representation of the tree distribution.
2.1 Definition of Adaptor Grammars
Adaptor grammars capture syntactic regularities in
sentences by placing a nonparametric prior over the
distribution of syntactic trees that underlie them.
The model exhibits ?rich get richer? dynamics: once
a tree is generated, it is more likely to reappear.
Adaptor grammars were developed by Johnson et
al. (2006). An adaptor grammar is a tuple A =
?G,M,a, b,??, which contains: (i) a context-free
grammar G = ?W,N,R, S? where W is the set of
564
terminals, N is the set of nonterminals, R is a set of
production rules, and S ? N is the start symbol?we
denote byRA the subset ofR with left-hand sideA;
(ii) a set of adapted nonterminals, M ? N; and (iii)
parameters a, b and ?, which are described below.
An adaptor grammar assumes the following gen-
erative process of trees. First, the multinomial dis-
tributions ? for a PCFG based on G are drawn
from Dirichlet distributions. Specifically, multino-
mial ?A ? Dir(?A) where? is collection of Dirich-
let parameters, indexed by A ? N.
Trees are then generated top-down starting with
S. Any non-adapted nonterminal A ? N \ M is
expanded by drawing a rule from RA. There are
two ways to expand A ?M:
1. With probability (nz ? bA)/(nA + aA) we ex-
pand A to subtree z (a tree rooted at A with a
yield in W?), where nz is the number of times
the tree z was previously generated and nA is the
total number of subtrees (tokens) previously gen-
erated root being A. We denote by a the concen-
tration parameters and b the discount parameters,
both indexed by A ? M. We have aA ? 0 and
bA ? [0, 1].
2. With probability (aA + kAbA)/(nA + aA), A is
expanded as in a PCFG by a draw from ?A over
RA, where kA is the number of subtrees (types)
previously generated with root A.
For the expansion of adapted nonterminals, this
process can be explained using the Chinese restau-
rant process (CRP) metaphor: a ?customer? (cor-
responding to a partially generated tree) enters a
?restaurant? (corresponding to a nonterminal) and
selects a ?table? (corresponding to a subtree) to at-
tach to the partially generated tree. If she is the first
customer at the table, the PCFG ?G,?? produces the
new table?s associated ?dish? (a subtree).1
When adaptor grammars are defined using the
CRP, the PCFG G has to be non-recursive with re-
1We note that our construction deviates from the strict def-
inition of adaptor grammars (Johnson et al, 2006): (i) in our
construction, we assume (as prior work does in practice) that
the adaptors in A = ?G,M,a, b,?? follow the Pitman-Yor
(PY) process (Pitman and Yor, 1997), though in general other
stochastic processes might be used; and (ii) we place a sym-
metric Dirichlet over the parameters of the PCFG, ?, whereas
Johnson et al used a fixed PCFG for the definition (though they
experimented with a Dirichlet prior).
spect to the adapted nonterminals. More precisely,
for A ? N, denote by Reachable(G, A) all the non-
terminals that can be reached from A using a partial
derivation from G. Then we restrict G such that
for all A ? M, we have A /? Reachable(G, A).
Without this restriction, we might end up in a sit-
uation where the generative process is ill-defined:
in the CRP terminology, a customer could enter a
restaurant and select a table whose dish is still in
the process of being selected.2 In the more general
form of adaptor grammars with arbitrary adaptors,
the problem amounts to mutually dependent defini-
tions of distributions which rely on the others to be
defined. We return to this problem in ?3.1.
Inference The inference problem is to compute
the posterior distribution of parse trees given ob-
served sentences x = ?x1, . . . , xn?. Typically, in-
ference with adaptor grammars is done with Gibbs
sampling. Johnson et al (2006) use an embedded
Metropolis-Hastings sampler (Robert and Casella,
2005) inside a Gibbs sampler. The proposal distribu-
tion is a PCFG, resembling a tree substitution gram-
mar (TSG; Joshi, 2003). The sampler of Johnson et
al. is based on the representation of the PY process
as a distribution over partitions of integers. This rep-
resentation is not amenable to variational inference.
2.2 Stick-Breaking Representation
To develop a variational inference algorithm for
adaptor grammars, we require an alternative repre-
sentation of the model in ?2.1. The CRP-based def-
inition implicitly marginalizes out a random distri-
bution over trees. For variational inference, we con-
struct that distribution.
We first review the Dirichlet process and its stick-
breaking representation. The Dirichlet process de-
fines a distribution over distributions. Samples from
the Dirichlet process tend to deviate from a base
distribution depending on a concentration parame-
ter. Let G ? DP(G0, a) be a distribution sampled
from the Dirichlet process with base distribution G0
2Consider the simple grammar with rules { S ? S S, S ? a
}. Assume that a customer enters the restaurant for S. She sits
at a table, and selects a dish, a subtree, which starts with the rule
S ? S S. Perhaps the first child S is expanded by S ? a. For
the second child S, it is possible to re-enter the ?S restaurant?
and choose the first table, where the ?dish? subtree is still being
generated.
565
and concentration parameter a. The distribution G
is discrete, which means it puts positive mass on a
countable number of atoms drawn from G0. Re-
peated draws from G exhibit the ?clustering prop-
erty,? which means that they will be assigned to the
same value with positive probability. Thus, they ex-
hibit a partition structure. Marginalizing out G, the
distribution of that partition structure is given by a
CRP with parameter a (Pitman, 2002).
The stick-breaking process gives a constructive
definition of G (Sethuraman, 1994). With the stick-
breaking process (for the PY process), we first sam-
ple ?stick lengths? pi ? GEM(a, b) (in the case of
Dirichlet process, we have b = 0). The GEM par-
titions the interval [0, 1] into countably many seg-
ments. First, draw vi ? Beta(1 ? b, a + ib) for
i ? {1, . . .}. Then, define pii , vi
?i?1
j=1(1 ? vj).
In addition, we also sample infinitely many ?atoms?
independently zi ? G0. Define G as:
G(z) =
??
i=1 pii?(zi, z) (1)
where ?(zi, z) is 1 if zi = z and 0 otherwise. This
random variable is drawn from a Pitman-Yor pro-
cess. Notice the discreteness of G is laid bare in the
stick-breaking construction.
With the stick-breaking representation in hand,
we turn to a constructive definition of the distri-
bution over trees given by an adaptor grammar.
Let A1, . . . , AK be an enumeration of the nonter-
minals in M which satisfies: i ? j ? Aj /?
Reachable(G, Ai). (That this exists follows from
the assumption about the lack of recursiveness of
adapted nonterminals.) Let Yield(z) be the yield of
a tree derivation z. The process that generates ob-
served sentences x = ?x1, . . . , xn? from the adaptor
grammarA = ?G,M,a, b,?? is as follows:
1. For each A ? N, draw ?A ? Dir(?A).
2. For A from A1 to AK , define GA as follows:
(a) Draw piA | aA, bA ? GEM(aA, bA).
(b) For i ? {1, . . .}, grow a tree zA,i as follows:
i. Draw A? B1 . . . Bn fromRA.
ii. zA,i = A
HHH

B1 ? ? ? Bn
iii. While Yield(zA,i) has nonterminals:
A. Choose an unexpanded nonterminal B
from yield of zA,i.
B. If B ? M, expand B according to GB
(defined on previous iterations of step 2).
C. If B ? N \M, expand B with a rule from
RB according to Mult(?B).
(c) For i ? {1, . . .}, define GA(zA,i) = piA,i
3. For i ? {1, . . . , n} draw zi as follows:
(a) If S ?M, draw zi | GS ? GS .
(b) If S /? M, draw zi as in 2(b) (omitted for
space).
4. Set xi = Yield(zi) for i ? {1, . . . , n}.
Here, there are four collections of hidden variables:
the PCFG multinomials ? = {?A | A ? N}, the
stick length proportions v = {vA | A ? M} where
vA = ?vA,1, vA,2, . . .?, the adapted nonterminals?
subtrees zA = {zA,i | A ? M; i ? {1, . . .}} and
the derivations z1:n = z1, . . . , zn. The symbol z
refers to the collection of {zA | A ? M}, and z1:n
refers to the derivations of the data x.
Note that the distribution in 2(c) is defined with
the GEM distribution, as mentioned earlier. It is a
sample from the Pitman-Yor process (or the Dirich-
let process), which is later used in 3(a) to sample
trees for an adapted non-terminal.
3 Variational Inference
Variational inference is a deterministic alternative
to MCMC, which casts posterior inference as an
optimization problem (Jordan et al, 1999; Wain-
wright and Jordan, 2008). The optimized function
is a bound on the marginal likelihood of the obser-
vations, which is expressed in terms of a so-called
?variational distribution? over the hidden variables.
When the bound is tightened, that distribution is
close to the posterior of interest. Variational meth-
ods tend to converge faster than MCMC, and can be
more easily parallelized over multiple processors in
a framework such as MapReduce (Dean and Ghe-
mawat, 2004).
The variational bound on the likelihood of the
data is:
log p(x | a,?) ? H(q) +
?
A?M
Eq[log p(vA | aA)]
+
?
A?M
Eq[log p(?A | ?A)]
+
?
A?M
Eq[log p(zA | v,?)] + Eq[log p(z | vA)]
566
Expectations are taken with respect to the variational
distribution q(v,?, z) and H(q) is its entropy.
Before tightening the bound, we define the func-
tional form of the variational distribution. We use
the mean-field distribution in which all of the hid-
den variables are independent and governed by in-
dividual variational parameters. (Note that in the
true posterior, the hidden variables are highly cou-
pled.) To account for the infinite collection of ran-
dom variables, for which we cannot define a varia-
tional distribution, we use the truncated stick distri-
bution (Blei and Jordan, 2005). Hence, we assume
that, for all A ? M, there is some value NA such
that q(vA,NA = 1) = 1. The assigned probability to
parse trees in the stick will be 0 for i > NA, so we
can ignore zA,i for i > NA. This leads to a factor-
ized variational distribution:
q(v,?, z) = (2)
?
A?M
(
q(?A)
NA?
i=1
q(vA,i)? q(zA,i)
)
?
n?
i=1
q(zi)
It is natural to define the variational distributions
over ? and v to be Dirichlet distributions with pa-
rameters ?A and Beta distributions with parameters
?A,i, respectively. The two distributions over trees,
q(zA,i) and q(zi), are more problematic. For ex-
ample, with q(zi | ?), we need to take into ac-
count different subtrees that could be generated by
the model and use them with the proper probabilities
in the variational distribution q(zi | ?). We follow
and extend the idea from Johnson et al (2006) and
use grammatons for these distributions. Gramma-
tons are ?mini-grammars,? inspired by the grammar
G.
For two strings in s, t ? W?, we use ?t ? s?
to mean that t is a substring of s. In that case, a
grammaton is defined as follows:
Definition 1. LetA = ?G,M,a, b,?? be an adap-
tor grammar with G = ?W,N,R, S?. Let s be a fi-
nite string over the alphabet ofG andA ? N. Let U
be the set of nonterminals U , Reachable(G, A) ?
(N \M). The grammaton G(A, s) is the context-
free grammar with the start symbol A and the rules
RA?
(
?
B?U
RB
)
?
?
A?B1...Bn?RA
?
i?{i|Bi?M}
{Bi ?
t | t ? s}.
Using a grammaton, we define the distributions
q(zA,i | ?A) and q(zi | ?). This requires a pre-
processing step (described in detail in ?3.3) that de-
fines, for each A ? M, a list of strings sA =
?sA,1, . . . , sA,NA?. Then, for q(zA,i | ?A) we use
the grammaton G(A, sA,i) and for q(zi | ?) we
use the grammaton G(A, xi) where xi is the ith
observed sentence. We parametrize the grammaton
with weights ?A (or ?) for each rule in the gramma-
ton. This makes the variational distributions over the
trees for strings s (and trees for x) globally normal-
ized weighted grammars. Choosing such distribu-
tions is motivated by their ability to make the varia-
tional bound tight (similar to Cohen et al, 2008, and
Cohen and Smith, 2009). In practice we do not have
to use rewrite rules for all strings t ? s in the gram-
maton. It suffices to add rewrite rules only for the
strings t = sA,i that have some grammaton attached
to them,G(A, sA,i).
The variational distribution above yields a vari-
ational inference algorithm for approximating the
posterior by estimating ?A,i, ?A, ?A and ? it-
eratively, given a fixed set of hyperparameters
a, b and ?. Let r be a PCFG rule. Let
f?(r, sB,k) = Eq(zk|?B,k)[f(r; zk)], where f(r; zk)
counts the number of times that rule r is applied in
the derivation zk. Let A ? ? denote a rule from
G. The quantity f?(r, sB,k) is computed using the
inside-outside (IO) algorithm. Fig. 1 gives the vari-
ational inference updates.
Variational EM We use variational EM to fit the
hyperparameters. Variational EM is an EM algo-
rithm where the E step is replaced by variational in-
ference (Fig. 1). The M-step optimizes the hyperpa-
rameters (a, b and ?) with respect to expected suffi-
cient statistics under the variational distribution. We
use Newton-Raphson for each (Boyd and Vanden-
berghe, 2004); Fig. 2 gives the objectives.
3.1 Note about Recursive Grammars
With recursive grammars, the stick-breaking pro-
cess representation gives probability mass to events
which are ill-defined. In step 2(iii)(c) of the stick-
breaking representation, we assign nonzero proba-
bility to an event in which we choose to expand the
current tree using a subtree with the same index that
we are currently still expanding (see footnote 2). In
567
short, with recursive grammars, we can get ?loops?
inside the trees.
We would still like to use recursion in the cases
which are not ill-defined. In the case of recur-
sive grammars, there is no problem with the stick-
breaking representation and the order by which we
enumerate the nonterminals. This is true because the
stick-breaking process separates allocating the prob-
abilities for each index in the stick and allocating the
atoms for each index in the stick.
Our variational distributions give probability 0 to
any event which is ill-defined in the sense men-
tioned above. Optimizing the variational bound in
this case is equivalent to optimizing the same vari-
ational bound with a model p? that (i) starts with p,
(ii) assigns probability 0 to ill-defined events, and
(iii) renormalizes:
Proposition 2. Let p(x, z) be a probability distri-
bution, where z ? Z, and let S ? Z. Let Q = {q |
q(z) = 0, ?z ? S}, a set of distributions. Then:
argmax
q?Q
Eq[log p(x, z)] = argmax
q
Eq[log p?(x, z)]
where p?(x, z) is a probability distribution defined
as p?(x, z) = p(x, z)/
?
z?S p(x, z) for z ? S and
0 otherwise.
For this reason, our variational approximation al-
lows the use of recursive grammars. The use of re-
cursive grammars with MCMC methods is problem-
atic, since it has no corresponding probabilistic in-
terpretation, enabled by zeroing events that are ill-
defined in the variational distribution. There is no
underlying model such as p?, and thus the inference
algorithm is invalid.
3.2 Time Complexity
The algorithm in Johnson et al (2006) works by
sampling from a PCFG containing rewrite rules that
rewrite to a whole tree fragment. This requires
a procedure that uses the inside-outside algorithm.
Despite the grammar being bigger (because of the
rewrite rules to a string), the asymptotic complexity
of the IO algorithm stays O(|N|2|xi|3 + |N|3|xi|2)
where |xi| is the length of the ith sentence.3
3This analysis is true for CNF grammars augmented with
rules rewriting to a whole string, like those used in our study.
?1A,i = 1? bA +
?
B?M
?NB
k=1 f?(A? sA,i, sB,k)
?2A,i = aA + ibA
+
?i?1
j=1
?
B?M
?NB
k=1 f?(A? sA,j , sB,k)
?A,A?? =
?
B?M
?NB
k=1 f?(A? ?, sB,k)
?A,A?sA,i = ?(?
1
A,i)??(?
1
A,i + ?
2
A,i)
+
?i?1
j=1
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
?A,A?? = ?(?A,A??)??
(?
? ?A,A??
)
Figure 1: Updates for variational inference with adaptor
grammars. ? is the digamma function.
Our algorithm requires running the IO algorithm
for each yield in the variational distribution, for each
nonterminal, and for each sentence. However, IO
runs with much smaller grammars coming from the
grammatons. The cost of running the IO algorithm
on the yields in the sticks for A ? M can be taken
into account parsing a string that appears in the cor-
pus with the full grammars. This leads to an asymp-
totic complexity of O(|N|2|xi|3 + |N|3|xi|2) for the
ith sentence in the corpus each iteration.
Asymptotically, both sampling and variational
EM behave the same. However, there are different
constants that hide in these asymptotic runtimes: the
number of iterations that the algorithm takes to con-
verge (for which variational EM generally has an ad-
vantage over sampling) and the number of additional
rewrite rules that rewrite to a string representing a
tree (for which MCMC has a relative advantage, be-
cause it does not use a fixed set of strings; instead,
the size of the grammars it uses grow as sampling
proceeds). In ?4, we see that variational EM and
sampling methods are similar in the time it takes to
complete because of a trade-off between these two
constants. Simple parallelization, however, which
is possible only with variational inference, provides
significant speed-ups.4
3.3 Heuristics for Variational Inference
For the variational approximation from ?3, we need
to decide on a set of strings, sA,i (for A ? M and
i ? {1, . . . , NA}) to define the grammatons in the
4Newman et al (2009) show how to parallelize sampling al-
gorithms, but in general, parallelizing these algorithms is more
complicated than parallelizing variational algorithms and re-
quires further approximation.
568
max?A log ?(|RA|?A)? |RA| log ?(?A) + (?A ? 1)
(?
A???RA
?(?A??)??
(?
A???RA
?A??
))
maxaA
?NA
i=1 aA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(ibA + aA)
maxbA
?NA
i=1 ibA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(1? bA)? log ?(ibA + aA)
Figure 2: Variational M-step updates. ? is the gamma function.
nonparametric stick. Any set of strings will give
a valid approximation, but to make the variational
approximation as accurate as possible, we require
that: (i) the strings in the set must be likely to be
generated using the adaptor grammar as constituents
headed by the relevant nonterminal, and (ii) strings
that are more likely to be generated should be asso-
ciated with a lower index in the stick. The reason for
the second requirement is the exponential decay of
coefficients as the index increases.
We show that a simple heuristic leads to an order
over the strings generated by the adaptor grammars
that yields an accurate variational estimation. We
begin with a weighted context-free grammar Gheur
that has the same rules as in G, only the weight for
all of its rules is 1. We then compute the quantity:
c(A, s) =
1
n
(
n?
i=1
EGheur [fi(z;A, s)]
)
? ? log |s|
(3)
where fi(z;A, s) is a function computing the count
of constituents headed by A with yield s in the tree
z for the sentence xi. This quantity can be com-
puted by using the IO algorithm onGheur. The term
? log |s| is subtracted to avoid preference for shorter
constituents, similar to Mochihashi et al (2009).
While computing c(A, s) using the IO algorithm,
we sort the set of all substrings of s according to
their expected counts (aggregated over all strings s).
Then, we use the top NA strings in the sorted list for
the grammatons of A.5
3.4 Decoding
The variational inference algorithm gives a distribu-
tions over parameters and hidden structures (through
the grammatons). We experiment with two com-
monly used decoding methods: Viterbi decoding
5The requirement to select NA in advance is strict. We ex-
perimented with dynamic expansions of the stick, in the spirit
of Kurihara et al (2006) and Wang and Blei (2009), but we did
not achieve better performance and it had an adverse effect on
runtime. For completeness, we give these results in ?4.
and minimum Bayes risk decoding (MBR; Good-
man, 1996).
To parse a string with Viterbi (or MBR) decoding,
we find the tree with highest score for the gramma-
ton which is attached to that string. For all rules
which rewrite to strings in the resulting tree, we
again perform Viterbi (or MBR) decoding recur-
sively using other grammatons.
4 Experiments
We describe experiments with variational inference
for adaptor grammars for word segmentation and de-
pendency grammar induction.
4.1 Word Segmentation
We follow the experimental setting of Johnson and
Goldwater (2009), who present state-of-the-art re-
sults for inference with adaptor grammars using
Gibbs sampling on a segmentation problem. We
use the standard Brent corpus (Brent and Cartwright,
1996), which includes 9,790 unsegmented phone-
mic representations of utterances of child-directed
speech from the Bernstein-Ratner (1987) corpus.
Johnson and Goldwater (2009) test three gram-
mars for this segmentation task. The first grammar
is a character unigram grammar (GUnigram). The
second grammar is a grammar that takes into con-
sideration collocations (GColloc) which includes the
rules { Sentence? Colloc, Sentence? Colloc Sen-
tence, Colloc ? Word+, Word ? Char+ }. The
third grammar incorporates more prior knowledge
about the syllabic structure of English (GSyllable).
GUnigram and GSyllable can be found in Johnson
and Goldwater (2009). Once an utterance is parsed,
Word constituents denote segments.
The value of ? (penalty term for string length) had
little effect on our results and was fixed at ? = ?0.2.
When NA (number of strings used in the variational
distributions) is fixed, we use NA = 15,000. We re-
port results using Viterbi and MBR decoding. John-
son and Goldwater (2009) experimented with two
569
this paper J&G 2009
grammar model Vit. MBR SA MM
GU
ni
gr
am
Dir 0.49 0.84 0.57 0.54
PY 0.49 0.84 0.81 0.75
PY+inc 0.42 0.59 - -
GC
ol
lo
c Dir 0.40 0.86 0.75 0.72
PY 0.40 0.86 0.83 0.86
PY+inc 0.43 0.60 - -
G S
yl
la
bl
e Dir 0.77 0.83 0.84 0.84
PY 0.77 0.83 0.89 0.88
PY+inc 0.75 0.76 - -
Table 1: F1 performance for word segmentation on the
Brent corpus. Dir. stands for Dirichlet Process adaptor
(b = 0), PY stands for Pitman-Yor adaptor (b optimized),
and PY+inc. stands for Pitman-Yor with iteratively in-
creasing NA for A ? M (see footnote 5). J&G 2009 are
the results adapted from Johnson and Goldwater (2009);
SA is sample average decoding, and MM is maximum
marginal decoding.
Truncated stick length
F1 sc
ore
65
70
75
80
l
l
l
l
l l l
l l l l l l
l l
l
l
l
l
l
l l
l l
l l l l l l
2000 4000 6000 8000 10000 12000 14000
Figure 3: F1 performance of GUnigram as influenced by
the length of the stick, NWord.
decoding methods, sample average (SA) and maxi-
mal marginal decoding (MM), which are closely re-
lated to Viterbi and MBR, respectively. With MM,
we marginalize the tree structure, rather than the
word segmentation induced, similar to MBR decod-
ing. With SA, we compute the probability of a whole
tree, by averaging its count in the samples, an ap-
proximation to finding the tree with highest proba-
bility, like Viterbi.
Table 1 gives the results for our experiments. No-
tice that the results for the Pitman-Yor process and
the Dirichlet process are similar. When inspecting
the learned parameters, we noticed that the discount
parameters (b) learned by the variational inference
algorithm for the Pitman-Yor process are very close
to 0. In this case, the Pitman-Yor process is reduced
to the Dirichlet process.
Similar to Johnson and Goldwater?s comparisons,
we see superior performance when using minimum
Bayes risk over Viterbi decoding. Further notice that
the variational inference algorithm obtains signifi-
cantly superior performance for simpler grammars
than Johnson et al, while performance using the syl-
lable grammar is lower. The results also suggest that
it is better to decide ahead on the set of strings avail-
able in the sticks, instead of working gradually and
increase the size of the sticks as described in foot-
note 5. We believe that the reason is that the varia-
tional inference algorithm settles in a trajectory that
uses fewer strings, then fails to exploit the strings
that are added to the stick later. Given that select-
ing NA in advance is advantageous, we may inquire
if choosing NA to be too large can lead to degraded
performance, because of fragmention of the gram-
mar. Fig. 3 suggests it is not the case, and per-
formance stays steady after NA reaches a certain
value.
One of the advantages of variational approxima-
tion over sampling methods is the ability to run
for fewer iterations. For example, with GUnigram
convergence typically takes 40 iterations with vari-
ational inference, while Johnson and Goldwater
(2009) ran their sampler for 2,000 iterations, for
which 1,000 were for burning in. The inside-outside
algorithm dominates the iteration?s runtime, both
for sampling and variational EM. Each iteration
with sampling, however, takes less time, despite the
asymptotic analysis in ?3.2, because of different im-
plementations and the different number of rules that
rewrite to a string. We now give a comparison of
clock time for GUnigram for variational inference
and sampling as described in Johnson and Goldwa-
ter (2009).6 Replicating the experiment in Johnson
and Goldwater (first row in Table 1) took 2 hours
and 11 minutes. With the variational approximation,
we had the following: (i) the preprocessing (?3.3)
step took 114 seconds; (ii) each iteration took ap-
proximately 204 seconds, with convergence after 40
iterations, leading to 8,160 seconds of pure varia-
6We used the code and data available at http://www.
cog.brown.edu/?mj/Software.htm. The machine
used for this comparison is a 64-bit machine with 2.6GHz CPU,
4MB of cache memory and 8GB of RAM.
570
tional EM processing; (iii) parsing took another 952
seconds. The total time is 2 hours and 34 minutes.
At first glance it seems that variational inference
is slower than MCMC sampling. However, note that
the cost of the grammar preprocessing step is amor-
tized over all experiments with the specific gram-
mar, and the E-step with variational inference can be
parallelized, while sampling requires an update of a
global set of parameters after each tree update. We
ran our algorithm on a cluster of 20 1.86GHz CPUs
and achieved a significant speed-up: preprocessing
took 34 seconds, each variational EM iteration took
43 seconds and parsing took 208 seconds. The total
time was 47 minutes, which is 2.8 times faster than
sampling.
4.2 Dependency Grammar Induction
We conclude our experiments with preliminary re-
sults for unsupervised syntax learning. This is a new
application of adaptor grammars, which have so far
been used in segmentation (Johnson and Goldwater,
2009) and named entity recognition (Elsner et al,
2009).
The grammar we use is the dependency model
with valence (DMV Klein and Manning, 2004) rep-
resented as a probabilistic context-free grammar,
GDMV (Smith, 2006). We note that GDMV is re-
cursive; this is not a problem (?3.1).
We used part-of-speech sequences from the Wall
Street Journal Penn Treebank (Marcus et al, 1993),
stripped of words and punctuation. We follow stan-
dard parsing conventions and train on sections 2?
21 and test on section 23 (while using sentences of
length 10 or less). Because of the unsupervised na-
ture of the problem, we report results on the training
set, in addition to the test set.
The nonterminals that we adapted correspond to
nonterminals that define noun constituents. We then
use the preprocessing step defined in ?3.3 with a uni-
form grammar and take the top 3,000 strings for each
nonterminal of a noun constituent.
The results are in Table 4.2. We report attach-
ment accuracy, the fraction of parent-child relation-
ships that the algorithm classified correctly. Notice
that the results are not very different for Viterbi and
MBR decoding, unlike the case with word segmen-
tation. It seems like the DMV grammar, applied
to this task, is more robust to changes in decod-
model Vit. MBR
tr
ai
n
non-Bayesian 48.2 48.3
Dirichlet prior 48.3 48.6
Adaptor grammar 54.0 ?53.7
te
st
non-Bayesian 45.8 46.1
Dirichlet prior 45.9 46.1
Adaptor grammar 48.3 50.2
Table 2: Attachment accuracy for different models for
dependency grammar induction. Bold marks best overall
accuracy per evaluation set, and ? marks figures that are
not significantly worse (binomial sign test, p < 0.05).
ing mechanism. Adaptor grammars improve perfor-
mance over classic EM and variational EM with a
Dirichlet prior significantly.
We note that adaptor grammars are not limited to
a selection of a Dirichlet distribution as a prior for
the grammar rules. Our variational inference algo-
rithm, for example, can be extended to use the lo-
gistic normal prior instead of the Dirichlet, shown
successful by Cohen and Smith (2009).7
5 Conclusion
We described a variational inference algorithm for
adaptor grammars based on a stick-breaking process
representation, which solves a problem with adaptor
grammars and recursive PCFGs. We tested it for a
segmentation task, and showed results which are ei-
ther comparable or an imporvement of state of the
art. We showed that significant speed-ups can be
obtained using parallelization of the algorithm. We
also tested the algorithm on a novel task for adap-
tor grammars, dependency grammar induction. We
showed that an improvement can be obtained using
adaptor grammars over non-Bayesian and paramet-
ric baselines.
Acknowledgments
The authors would like to thank the anonymous review-
ers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson,
and Chong Wang for their useful feedback and com-
ments. This work was supported by the following grants:
ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF
IIS-0836431 and IIS-0915187 to Smith.
7The performance of Cohen and Smith (2009), like the per-
formance of Headden et al (2009), is greater than what we re-
port, but those developments are orthogonal to the contributions
of this paper.
571
References
C. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
N. Bernstein-Ratner. 1987. The phonology of parent
child speech. Children?s Language, 6.
D. Blei and M. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Journal of Bayesian Anal-
ysis, 1(1):121?144.
S. Boyd and L. Vandenberghe. 2004. Convex Optimiza-
tion. Cambridge Press.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 6:93?125.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Proc. of
OSDI.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
M. Johnson and S. Goldwater. 2009. Improving nonpa-
rameteric Bayesian inference experiments on unsuper-
vised word segmentation with adaptor grammars. In
Proc. of NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2008a. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proceedings
of the Tenth Meeting of ACL Special Interest Group on
Computational Morphology and Phonology.
M. Johnson. 2008b. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proc. of ACL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183?
233.
A. Joshi. 2003. Tree adjoining grammars. In R. Mitkov,
editor, The Oxford Handbook of Computational Lin-
guistics, pages 483?501. Oxford University Press.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara, M. Welling, and N. A. Vlassis. 2006. Ac-
celerated variational Dirichlet process mixtures. In
NIPS.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2009. Distributed algorithms for topic models. Jour-
nal of Machine Learning Research, 10:1801?1828.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25(2):855?900.
J. Pitman. 2002. Combinatorial Stochastic Processes.
Lecture Notes for St. Flour Summer School. Springer-
Verlag, New York, NY.
C. P. Robert and G. Casella. 2005. Monte Carlo Statisti-
cal Methods. Springer.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proc. of NIPS.
M. J. Wainwright and M. I. Jordan. 2008. Graphi-
cal models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1:1?305.
C. Wang and D. M. Blei. 2009. Variational inference for
the nested Chinese restaurant process. In NIPS.
572
