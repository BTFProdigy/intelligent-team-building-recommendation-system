Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 174?177,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BUAP: An Unsupervised Approach to Automatic Keyphrase Extraction
from Scientific Articles
Roberto Ortiz, David Pinto, Mireya Tovar
Faculty of Computer Science, BUAP
Puebla, Mexico
korn resorte2003@hotmail.com,
{dpinto, mtovar}@cs.buap.mx
He?ctor Jime?nez-Salazar
Information Technologies Dept., UAM
DF, Mexico
hgimenezs@gmail.com
Abstract
In this paper, it is presented an unsuper-
vised approach to automatically discover
the latent keyphrases contained in scien-
tific articles. The proposed technique is
constructed on the basis of the combi-
nation of two techniques: maximal fre-
quent sequences and pageranking. We
evaluated the obtained results by using
micro-averaged precision, recall and F-
scores with respect to two different gold
standards: 1) reader?s keyphrases, and 2)
a combined set of author?s and reader?s
keyphrases. The obtained results were
also compared against three different base-
lines: one unsupervised (TF-IDF based)
and two supervised (Na??ve Bayes and
Maximum Entropy).
1 Introduction
The task of automatic keyphrase extraction has
been studied for several years. Firstly, as semantic
metadata useful for tasks such as summarization
(Barzilay and Elhadad, 1997; Lawrie et al, 2001;
DAvanzo and Magnini, 2005), but later rec-
ognizing the impact that good keyphrases
would have on the quality of various Nat-
ural Language Processing (NLP) applica-
tions (Frank et al, 1999; Witten et al, 1999;
Turney, 1999; Barker and Corrnacchia, 2000;
Medelyan and Witten, 2008). Thus, the selection
of important, topical phrases from within the
body of a document may be used in order to
improve the performance of systems dealing
with different NLP problems such as, clustering,
question-answering, named entity recognition,
information retrieval, etc.
In general, a keyphrase may be considered as
a sequence of one or more words that capture the
main topic of the document, as that keyphrase is
expected to represent one of the key ideas ex-
pressed by the document author. Following the
previously mentioned hypothesis, we may take ad-
vantage of two different techniques of text analy-
sis: maximal frequent sequences to extract a se-
quence of one or more words from a given text,
and pageranking, expecting to extract those word
sequences that represent the key ideas of the au-
thor.
The interest on extracting high quality
keyphrases from raw text has motivated forums,
such as SemEval, where different systems may
evaluate their performances. The purpose of
SemEval is to evaluate semantic analysis systems.
In particular, in this paper we are reporting the
results obtained in Task #5 of SemEval-2 2010,
which has been named: ?Automatic Keyphrase
Extraction from Scientific Articles?. We focused
this paper on the description of our approach and,
therefore, we do not describe into detail the task
nor the dataset used. For more information about
this information read the ?Task #5 Description
paper?, also published in this proceedings volume
(Nam Kim et al, 2010).
The rest of this paper is structured as follows.
Section 2 describes into detail the components of
the proposed approach. In Section 3 it is shown
the performance of the presented system. Finally,
in Section 4 a discussion of findings and further
work is given.
2 Description of the approach
The approach presented in this paper relies on the
combination of two different techniques for select-
ing the most prominent terms of a given text: max-
imal frequent sequences and pageranking. In Fig-
ure 1 we may see this two step approach, where
we are considering a sequence to be equivalent to
an n-gram. The complete description of the pro-
cedure is given as follows.
We select maximal frequent sequences which
174
we consider to be candidate keyphrases and, there-
after, we ranking them in order to determine which
ones are the most importants (according to the
pageranking algorithm). In the following subsec-
tions we give a brief description of these two tech-
niques. Afterwards, we provide an algorithm of
the presented approach.
Figure 1: Two step approach of BUAP Team at the
Task #5 of SemEval-2
2.1 Maximal Frequent Sequences
Definition: If a sequence p is a subsequence of q
and the number of elements in p is equal to n, then
the p is called an n-gram in q.
Definition: A sequence p = a
1
? ? ? a
k
is a sub-
sequence of a sequence q if all the items a
i
occur
in q and they occur in the same order as in p. If
a sequence p is a subsequence of a sequence q we
say that p occurs in q.
Definition: A sequence p is frequent in S if p is
a subsequence of at least ? documents in S where
? is a given frequency threshold. Only one oc-
currence of sequence in the document is counted.
Several occurrences within one document do not
make the sequence more frequent.
Definition: A sequence p is a maximal frequent
sequence in S if there does not exists any sequence
q in S such that p is a subsequence of q and p is
frequent in S.
2.2 PageRanking
The algorithm of PageRanking was defined by
Brin and Page in (Brin and Page, 1998). It is a
graph-based algorithm used for ranking webpages.
The algorithm considers input and output links of
each page in order to construct a graph, where
each vertex is a webpage and each edge may be
the input or output links for this webpage. They
denote as In(V
i
) the set of input links of webpage
V
i
, and Out(V
i
) their output links. The algorithm
proposed to rank each webpage based on the vot-
ing or recommendation of other webpages. The
higher the number of votes that are cast for a ver-
tex, the higher the importance of the vertex. More-
over, the importance of the vertex casting the vote
determines how important the vote itself is, and
this information is also taken into account by the
ranking model.
Although this algoritm has been initially pro-
posed for webpages ranking, it has been also used
for other NLP applications which may model their
corresponding problem in a graph structure. Eq.
(1) is the formula proposed by Brin and Page.
S(V
i
) = (1 ? d) + d ?
?
j?In(V
i
)
1
|Out(V
j
)|
S(V
j
)
(1)
where d is a damping factor that can be set be-
tween 0 and 1, which has the role of integrat-
ing into the model the probability of jumping
from a given vertex to another random vertex
in the graph. This factor is usually set to 0.85
(Brin and Page, 1998).
There are some other propossals, like the one
presented in (Mihalcea and Tarau, 2004), where a
textranking algorithm is presented. The authors
consider a weighted version of PageRank and
present some applications to NLP using unigrams.
They also construct multi-word terms by exploring
the conections among ranked words in the graph.
Our algorithm differs from textranking in that we
use MFS for feeding the PageRanking algorithm.
2.3 Algorithm
The complete algoritmic description of the pre-
sented approach is given in Algorithm 1. Read-
ers and writers keyphrases may be quite dif-
ferent. In particular, writers usually introduce
acronyms in their text, but they use the complete
or expanded representation of these acronyms
for their keyphrases. Therefore, we have in-
cluded a module (Extract Acronyms) for ex-
tracting both, acronyms with their corresponding
expanded version, which are used afterwards as
output of our system. We have preprocessed the
dataset removing stopwords and punctuation sym-
bols. Lemmatization (TreeTagger1) and stemming
(Porter Stemmer (Porter, 1980)) were also applied
in some stages of preprocessing.
The Maximal Freq Sequences module ex-
tracts maximal frequent sequences of words and
we feed the PageRaking module (PageRanking)
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
175
with all these sequences for determining the most
important ones. We use the structure of the sci-
entific articles in order to determine in and out
links of the sequences found. In fact, we use a
neighborhood criterion (a pair of MFS in the same
sentence) for determining the links between those
MFS?s. Once the ranking is calculated, we may se-
lect those sequences of a given length (unigrams,
bigrams and trigrams) as output of our system. We
also return a maximum of three acronyms, and
their associated multiterm phrases (MultiTerm),
as candidate keyphrases. Determining the length
and quantity of the sequences (n-grams) was ex-
perimentally deduced from the training corpus.
Algorithm 1: Algorithm of the Two Step ap-
proach for the Task #5 at SemEval-2
Input: A document set: D = {d
1
, d
2
, ? ? ? }
Output: A set K = {K
1
,K
2
, ? ? ? } of
keyphrases for each document d
i
:
K
i
= {k
i,1
, k
i,2
, ? ? ? }
foreach d
i
? D do1
AcronymSet = Extract Acronyms(d
i
);2
d
1
i
= Pre Processing(d
i
);3
MFS = Maximal Freq Sequences(d1
i
);4
CK = PageRanking(d1
i
, MFS);5
CU = Top Nine Unigrams(CK);6
CT = Top Three Trigrams(CK);7
K
i
= CT ;8
NU = 0;9
Acronyms = 0;10
foreach unigram ? CU do11
if unigram ? AcronymSet then12
if Acronyms < 3 then13
K
i
= K
i
?
{unigram};14
EA = MultiTerm(unigram);15
K
i
= K
i
?
{EA};16
Acronyms++;17
end18
else19
K
i
= K
i
?
{unigram};20
NU++;21
end22
end23
N = (15?(2?Acronyms+|CT |+NU));24
CB = Top N Bigrams(CK, N );25
K
i
= K
i
?
CB;26
end27
return K = {K
1
,K
2
, ? ? ? }28
In this edition of the Task #5 of SemEval-2
2010, we tested three different runs, which were
named: BUAP ? 1, BUAP ? 2 and BUAP ? 3.
Definition and differences among the three runs
are given in Table 3.
The results obtained with each run, together
with three different baselines are given in the fol-
lowing section.
3 Experimental results
In all tables, P , R, F mean micro-averaged pre-
cision, recall and F -scores. For baselines, there
were provided 1,2,-3 grams as candidates and
TFIDF as features. In Table 2, TFIDF is an
unsupervised method to rank the candidates based
on TFIDF scores. NB and ME are super-
vised methods using Na??ve Bayes and maximum
entropy in WEKA. In second column, R means
to use the reader-assigned keyword set as gold-
standard data and C means to use both author-
assigned and reader-assigned keyword sets as an-
swers.
Notice from Tables 2 and 3 that we outper-
formed all the baselines for the Top 15 candidates.
However, the Top 10 candidates were only outper-
formed by the Reader-Assigned keyphrases found.
This implies that the Writer keyphrases we ob-
tained were not of as good as the Reader ones. As
we mentioned, readers and writers assign different
keywords. The former write keyphrases based on
the lecture done, by the latter has a wider context
and their keyphrases used to be more complex. We
plan to investigate this issue in the future.
4 Conclusions
We have presented an approach based on the ex-
traction of maximal frequent sequences which are
then ranked by using the pageranking algorithm.
Three different runs were tested, modifying the
preprocessing stage and the number of bigrams
given as output. We did not see an improve-
ment when we used lemmatization of the docu-
ments. The run which obtained the best results
was ranking by the organizer according to the top
15 best keyphrases, however, we may see that our
runs need to be analysed more into detail in order
to provide a re-ranking procedure for the best 15
keyphrases found. This procedure may improve
the top 5 candidates precision.
176
Run name Description
BUAP ? 1 : This run is exactly the one described in Algorithm 1.
BUAP ? 2 : Same as BUAP ? 1 but lemmatization was applied a priori and stemming at the end.
BUAP ? 3 : Same as BUAP ? 2 but output twice the number of bigrams.
Table 1: Description of the three runs submitted to the Task #5 of SemEval-2 2010
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
TF ? IDF R 17.80% 7.39% 10.44% 13.90% 11.54% 12.61% 11.60% 14.45% 12.87%
C 22.00% 7.50% 11.19% 17.70% 12.07% 14.35% 14.93% 15.28% 15.10%
NB R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
ME R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
Table 2: Baselines
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
BUAP ? 1 R 10.40% 4.32% 6.10% 13.90% 11.54% 12.61% 14.93% 18.60% 16.56%
C 13.60% 4.64% 6.92% 17.60% 12.01% 14.28% 19.00% 19.44% 19.22%
BUAP ? 2 R 10.40% 4.32% 6.10% 13.80% 11.46% 12.52% 14.67% 18.27% 16.27%
C 14.40% 4.91% 7.32% 17.80% 12.14% 14.44% 18.73% 19.17% 18.95%
BUAP ? 3 R 10.40% 4.32% 6.10% 12.10% 10.05% 10.98% 12.33% 15.37% 13.68%
C 14.40% 4.91% 7.32% 15.60% 10.64% 12.65% 15.67% 16.03% 15.85%
Table 3: The three different runs submitted to the competition
Acknowledgments
This work has been partially supported by CONA-
CYT (Project #106625) and PROMEP (Grant
#103.5/09/4213).
References
[Barker and Corrnacchia2000] K. Barker and N. Cor-
rnacchia. 2000. Using noun phrase heads to extract
document keyphrases. In 13th Biennial Conference
of the Canadian Society on Computational Studies
of Intelligence: Advances in Artificial Intelligence.
[Barzilay and Elhadad1997] R. Barzilay and M. El-
hadad. 1997. Using lexical chains for text sum-
marization. In ACL/EACL 1997 Workshop on Intel-
ligent Scalable Text Summarization, pages 10?17.
[Brin and Page1998] S. Brin and L. Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. In COMPUTER NETWORKS AND ISDN
SYSTEMS, pages 107?117. Elsevier Science Pub-
lishers B. V.
[DAvanzo and Magnini2005] E. DAvanzo and
B. Magnini. 2005. A keyphrase-based approach
to summarization:the lake system. In Document
Understanding Conferences (DUC-2005).
[Frank et al1999] E. Frank, G.W. Paynter, I. Witten,
C. Gutwin, and C.G. Nevill-Manning. 1999. Do-
main specific keyphrase extraction. In 16th Interna-
tional Joint Conference on AI, pages 668?673.
[Lawrie et al2001] D. Lawrie, W. B. Croft, and
A. Rosenberg. 2001. Finding topic words for hi-
erarchical summarization. In SIGIR 2001.
[Medelyan and Witten2008] O. Medelyan and I. H.
Witten. 2008. Domain independent automatic
keyphrase indexing with small training sets. Jour-
nal of American Society for Information Science and
Technology, 59(7):1026?1040.
[Mihalcea and Tarau2004] R. Mihalcea and P. Tarau.
2004. Textrank: Bringing order into texts. In
EMNLP 2004, ACL, pages 404?411.
[Nam Kim et al2010] S. Nam Kim, O. Medelyan, and
M.Y. Kan. 2010. Semeval-2010 task5: Auto-
matic keyphrase extraction from scientific articles.
In Proceedings of the Fifth International Workshop
on Semantic Evaluations (SemEval-2010). Associa-
tion for Computational Linguistics.
[Porter1980] M. F. Porter. 1980. An algorithm for suf-
fix stripping. Program, 14(3).
[Turney1999] P. Turney. 1999. Learning to extract
keyphrases from text. Technical Report ERB-1057.
(NRC #41622), National Research Council, Institute
for Information Technology.
[Witten et al1999] I. Witten, G. Paynter, E. Frank,
C. Gutwin, and G. Nevill-Manning. 1999.
Kea:practical automatic key phrase extraction. In
fourth ACM conference on Digital libraries, pages
254?256.
177
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 502?505,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: A First Approximation to Relational Similarity Measuring
Mireya Tovar, J. Alejandro Reyes,
Azucena Montes
CENIDET, Department of
Computer Science
Int. Internado Palmira S/N, Col. Palmira
Cuernavaca, Morelos, Me?xico
{mtovar, alexreyes06c, amr}
@cenidet.edu.mx
Darnes Vilarin?o, David Pinto,
Saul Leo?n
B. Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto}@cs.buap.mx
saul.ls@live.com
Abstract
We describe a system proposed for measuring
the degree of relational similarity beetwen a
pair of words at the Task #2 of Semeval 2012.
The approach presented is based on a vec-
torial representation using the following fea-
tures: i) the context surrounding the words
with a windows size = 3, ii) knowledge ex-
tracted from WordNet to discover several se-
mantic relationships, such as meronymy, hy-
ponymy, hypernymy, and part-whole between
pair of words, iii) the description of the pairs
with their POS tag, morphological informa-
tion (gender, person), and iv) the average num-
ber of words separating the two words in text.
1 Introduction
The Task # 2 of Semeval 2012 focuses on measuring
the degree of relational similarity between the ref-
erence words pairs (training) and the test pairs for a
given class (Jurgens et al, 2012).
The training data set consists of 10 classes and
the testing data set consists of the 69 classes. These
datasets as well as the particularities of the task are
better described at overview paper (Jurgens et al,
2012). In this paper we report the approach submit-
ted to the competition, which is based on a vector
space model representation for each pair (Salton et
al., 1975). With respect to the type of features used,
we have observed that Fabio Celli (Celli, 2010) con-
siders that contextual information is useful, as well
the lexical and semantic information are in the ex-
traction of semantic relationships task. Additionally,
in (Chen et al, 2010) and (Negri and Kouylekov,
2010) are proposed WordNet based features with the
same purpose.
In the experiments carried out in this paper, we
use a set of lexical, semantic, WordNet-based and
contextual features which allows to construct the
vectors. Actually, we have tested a subset of the 20
contextual features proposed by Celli (Celli, 2010)
and some of those proposed by Chen (Chen et al,
2010) and Negri (Negri and Kouylekov, 2010).
The cosine similarity measure is used for deter-
mining the degree of relational similarity (Frakes
and Baeza-Yates, 1992) among the vectors.
The rest of this paper is structured as follows.
Section 2 describes the system employed. Section
3 show the obtained results. Finally, in Section 4 the
final conclusions are given.
2 System description
The approach reported in this paper measures the
relational similarity of a set of word pairs that be-
long to the same semantic relationship. Those word
pairs are represented by means of the vector space
model (Salton et al, 1975). Each value of the vec-
tor represents the average value of the correspond-
ing feature. This average is calculated using 100
samples obtained from Internet by employing the
Google search engine. The search process is car-
ried out assuming that those words co-occurring in
the same context contain some kind of semantic re-
lationship.
Let (w1, w2) be a word pair, then the vectorial
representation of this pair (~x) using semantic, con-
textual, lexical, and WordNet-based features may be
expressed as it can be seen in Eq. (1).
502
~x = (avg(f1), avg(f2), ..., avg(fn)) (1)
where avg(fk) is the average value of the feature fk.
The cardinality of the vector is 42, because we
extracted 4 lexical features, 6 semantic features, 7
WordNet-based features and 25 contextual features
(n = 42). Each word pair is then represented by
a unique vector with values associated to each fea-
ture. In Figure 1, we show the vectorial represen-
tation of the word pair (transportation, bus) using
a unique text sample (s). In this example, the num-
ber and type of features described below is followed,
i.e., the first 4 values are lexical, the following 6 are
semantic and so on.
s =?The Toyama Chih Railway is a transporta-
tion company that operates railway, tram, and
bus lines in the eastern part of the prefecture.?
~x = (6, 1, 0, 0, 27, 4, 4, 4, 4, 5, 2, 4, 5, 25, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,
4, 0, 4, 4, 4, 4)
Figure 1: Example of a feature vector for a word pair and
its corresponding sentence s.
The previous example is only illustrative, since
we have gathered 100 sentence per word pair. In
total, we collected a corpus containing 2,054,687 to-
kens, with a average class terms of 26,684 and with
an average class vocabulary of 4,006.
The features extracted are described as follows:
2.1 Lexical features
The lexical features describe morphologically and
syntactically the word pair (w1, w2). The lexical
features extracted are the following:
? Average number of words separating the two
words (w1, w2) in the text.
? The position of w1 with respect to w2 in the
text. If w1 appears before w2 then the feature
value is 1, otherwise, the value is 2.
? The Part of Speech Tag for each word in the
pair (two features). We use the FreeLing PoS-
tagger (Padro? et al, 2010) for obtaining the
grammatical category. The possible values are
the following: adjective=1; adverb=2; arti-
cle=3; noun=4; verb=5; pronoun=6; conjunc-
tion=7; preposition=8
2.2 Semantic features
The following four semantic features are boolean
values (true or false) indicating:
? If w1 and w2 are named entities (two features)1.
? If w1 and w2 are entities defined (two fea-
tures)2.
The following two semantic features indicate:
? The type of prepositional phrase in case of
existing for w1 and w2. The feature val-
ues are nominal: about=1; after=2; at=3; be-
hind=4; between=5; by=6; except=7; from=8;
into=9; near=10; of=11; over=12; through=13;
until=14; under=15; upon=16; without=17;
above=18; among=19; before=20; below=21;
beside=22; but=23; down=24; for=25; in=26;
on=27; since=28; to=29; with=30.
2.3 WordNet-based features
The semantic features are boolean values (true or
false) indicating whether or not w2 is contained in:
? the synonym set of w1
? the antonym set of w1
? the meronymy set of w1
? the hyponymy set of w1
? the hypernymy set of w1
? the part-whole set of w1
? the gloss set of w1
We used WordNet (Fellbaum, 1998) in order to de-
termine the relationship set for word w1.
1A named entity is defined by a Proper Noun Phrase, which
was detected using the module NER-Named Entity Recognition
of the FreeLing 2.1 tool.
2A defined sentence is one that begins with a definite article.
503
2.4 Contextual features
Contextual features considers values for the words
that occur in the context of w1 and w2 (in a window
size of 3). The description of those features follows.
? Nominal values indicating the Part of Speech
Tag (adjective=1; adverb=2; article=3; noun=4;
verb=5; pronoun=6; conjunction=7; preposi-
tion=8) for the three words at:
? the left context of w1 (three features).
? the right context of w1 (three features).
? the left context of w2 (three features).
? the right context of w2 (three features).
? A Nominal value indicating number of the fol-
lowing grammatical categories between w1 and
w2: verbs, adjectives and nouns (three fea-
tures).
? Nominal values indicating the frequencies of
the verbs: be, do, have, locate, know, make, use,
become, include, take between w1 and w2 (ten
features).
2.5 Feature selection
We carried out a feature selection process with the
aim of discarding irrelevant features. In this step,
we apply the attribute selection filter reported in
(Hall, 1999), that evaluates the worth of a subset
of attributes by considering the individual predic-
tive ability of each feature along with the degree of
redundancy between them and an exhaustive search
method.
The following features were obtained as relevant:
the average number of words between w1 and w2;
Named Entity of w1 and w2; phrase defined of w1
and w2; prepositional phrase type w1 and w2; part
of speech tag w1 and w2; part of speech tag of
right context of w1 with a windows size of 3; oc-
currences of verbs between w1 and w2; frequency of
verbs be, do, make, locate, take; synonym, antonym,
meronymy, hyponymy, hypernymy, part-whole and
gloss relationships between w1 and w2.
After applying the aforementioned feature selec-
tion method, we removed 17 features, and the vec-
torial representation of each word pair will be done
with only 25 values (features).
2.6 Determining the degree of similarity
We have used the features mentioned before for con-
structing a prototype vector representing a given se-
mantic class. In order to do so, we have employed
the training corpus for gathering samples from Inter-
net and, thereafter, we average the feature values in
order to construct such prototype vector.
For each word pair in the test dataset, we ob-
tained a vector using the same process explained
before. We determined the similarity for each test
feature vector with respect to the prototype of the
given class by using the cosine similarity coefficient
(Frakes and Baeza-Yates, 1992), i.e., measuring the
cosine of the angle between the two vectors.
In this way, we obtain a similarity measure of each
test word pair with respect to its corresponding class.
Finally, we may output a ranking of all the word
pairs at the test dataset by sorting these similarity
values obtained.
3 Experimental results
The approach submitted to the Task #2 of SemEval
2012 obtained very poor results. The Spearman cor-
relation coefficient, which measured the correlation
of the approach with respect to the gold standard, it
is quite low (see Table 1).
Team-Algorithm Spearman MaxDiff
UTD-NB 0.23 39.4
UTD-SVM 0.12 34.7
DULUTH-V0 0.05 32.4
DULUTH-V1 0.04 31.5
DULUTH-V2 0.04 31.1
BUAP 0.01 31.7
Random 0.02 31.2
Table 1: Spearman and MaxDiff scores obtained at the
Task #2 of Semeval 2012
Actually, it shows that the run submitted does not
correlate with the gold standard. We consider that
this behavior is derived from the nature of the sup-
port corpus used for obtaining the features set. The
number of sentences (100) used for representing the
word pairs was not enough for constructing a real
prototype of both, the semantic class and the word
pairs. A further analysis will confirm this issue.
504
Despite this limitation we note that the MaxDiff
score was 31.7% slightly above the baseline (31.2%)
and not far from the best score of the task (39.4%).
That is, we achieved an average of 31.7% of ques-
tions answered correctly.
4 Discussion and conclusion
In this paper we report the set of features used in
the approach submitted for measuring the degrees of
relational similarity between a given reference word
pair and a variety of other pairs. The results obtained
are not encouraging with a Spearman correlation co-
efficient close to zero, which mean that there are
not correlation between the run submitted and the
gold standard. A deeper analysis of the approach is
needed in order to determine if the limitation of the
system falls in the features used, the similarity mea-
sure, or the support corpus used for extracting the
features.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, VIEP #PIAD-ING11-II and
#VIAD-ING11-II.
References
Fabio Celli. 2010. Unitn: Part-of-speech counting in
relation extraction. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 198?201, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yuan Chen, Man Lan, Jian Su, Zhi Min Zhou, and
Yu Xu. 2010. Ecnu: Effective semantic relations
classification without complicated features or multi-
ple external corpora. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 226?229, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
William B. Frakes and Ricardo A. Baeza-Yates, editors.
1992. Information Retrieval: Data Structures & Algo-
rithms. Prentice-Hall.
Mark A. Hall. 1999. Correlation-based Feature Sub-
set Selection for Machine Learning. Ph.D. thesis, De-
partment of Computer Science, University of Waikato,
Hamilton, New Zealand.
David A. Jurgens, Saif M. Mohammad, Peter D. Turney,
and Keith J. Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montreal, Canada.
Matteo Negri and Milen Kouylekov. 2010. Fbk nk: A
wordnet-based system for multi-way classification of
semantic relations. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 202?205, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. European Language Re-
sources Association (ELRA).
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613?620, November.
505
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 631?634,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Three Approaches for Semantic Textual Similarity
Maya Carrillo, Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{cmaya, darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we describe the three approaches
we submitted to the Semantic Textual Similar-
ity task of SemEval 2012. The first approach
considers to calculate the semantic similar-
ity by using the Jaccard coefficient with term
expansion using synonyms. The second ap-
proach uses the semantic similarity reported
by Mihalcea in (Mihalcea et al, 2006). The
third approach employs Random Indexing and
Bag of Concepts based on context vectors. We
consider that the first and third approaches ob-
tained a comparable performance, meanwhile
the second approach got a very poor behav-
ior. The best ALL result was obtained with
the third approach, with a Pearson correlation
equal to 0.663.
1 Introduction
Finding the semantic similarity between two sen-
tences is very important in applications of natural
language processing such as information retrieval
and related areas. The problem is complex due to the
small number of terms involved in sentences which
are tipically less than 10 or 15. Additionally, it is re-
quired to ?understand? the meaning of the sentences
in order to determine the ?semantic? similarity of
texts, which is quite different of finding the lexical
similarity.
There exist different works at literature dealing
with semantic similarity, but the problem is far to
be solved because of the aforementioned issues.
In (Mihalcea et al, 2006), for instance, it is pre-
sented a method for measuring the semantic simi-
larity of texts, using corpus-based and knowledge-
based measures of similarity. The approaches pre-
sented in (Shrestha, 2011) are based on the Vector
Space Model, with the aim to capture the contex-
tual behavior, senses and correlation, of terms. The
performance of the method is better than the base-
line method that uses vector based cosine similarity
measure.
In this paper, we present three different ap-
proaches for the Textual Semantic Similarity task of
Semeval 2012 (Agirre et al, 2012). The task is de-
scribed as follows: Given two sentences s1 and s2,
the aim is to compute how similar s1 and s2 are,
returning a similarity score, and an optional confi-
dence score. The approaches should provide values
between 0 and 5 for each pair of sentences. These
values roughly correspond to the following consid-
erations, even when the system should output real
values:
5: The two sentences are completely equivalent,
as they mean the same thing.
4: The two sentences are mostly equivalent, but
some unimportant details differ.
3: The two sentences are roughly equivalent, but
some important information differs/missing.
2: The two sentences are not equivalent, but share
some details.
1: The two sentences are not equivalent, but are
on the same topic.
0: The two sentences are on different topics.
631
The description of the runs submitted to the com-
petition follows.
2 Experimentation setup
The three runs submitted to the competition use
completely different mechanisms to find the degree
of semantic similarity between two sentences. The
approaches are described as follows:
2.1 Approach BUAP-RUN-1: Term expansion
with synonyms
Let s1 = w1,1w1,2...w1,|s1| and s2 =
w2,1w2,2...w2,|s2| be two sentences. The synonyms
of a given word wi,k, expressed as synonyms(wi,k),
are obtained from online dictionaries by extracting
the synonyms of wi,k. A better matching between
the terms contained in the text fragments and the
terms at the dictionary are obtained by stemming all
the terms (using the Porter stemmer).
In order to determine the semantic similarity be-
tween any pair of terms of the two sentences (w1,i
and w2,j) we use Eq. (1).
sim(w1,i, w2,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w2,j) ||
w1,i ? synonyms(w2,j) ||
w2,j ? synonyms(w1,i)
0 otherwise
(1)
The similarity between sentences s1 and s2 is cal-
culated as shown in Eq. (2).
similarity(s1, s2) =
5 ? ?ni=1
?n
j=1 sim(w1i, w2j)
|s1 ? s2|
(2)
2.2 Approach BUAP-RUN-2
In this approach, the similarity of s1 and s2 is calcu-
lated as shown in Eq. (3) (Mihalcea et al, 2006).
similarity(s1, s2) = 12 (
?
w?{s1}
(maxSim(w,s2)?idf(w))
?
w?{s1}
idf(w)
+
?
w?{s2}
(maxSim(w,s1)?idf(w))
?
w?{s2}
idf(w) )
(3)
where idf(w) is the inverse document frequency of
the word w, and maxSim(w, s2) is the maximum
lexical similarity between the word w in sentence s2
and all the words in sentence s2 calculated by means
of the Eq. (4) reported by (Wu and Palmer, 1994).
The sentence terms are assumed to be concepts, LCS
is the depth of the least common subsumer, and the
equation is calculated using the NLTK libraries1.
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(4)
2.3 Approach BUAP-RUN-3: Random
Indexing and Bag of Concepts
The vector space model (VSM) for document rep-
resentation supporting search is probably the most
well-known IR model. The VSM assumes that term
vectors are pair-wise orthogonal. This assumption
is very restrictive because words are not indepen-
dent. There have been various attempts to build
representations for documents that are semantically
richer than only vectors based on the frequency of
terms occurrence. One example is Latent Seman-
tic Indexing (LSI), a method of word co-occurrence
analysis to compute semantic vectors (context vec-
tors) for words. LSI applies singular-value decom-
position (SVD) to the term-document matrix in or-
der to construct context vectors. As a result the di-
mension of the produced vector space will be signif-
icantly smaller; consequently the vectors that repre-
sent terms cannot be orthogonal. However, dimen-
sion reduction techniques such as SVD are expen-
sive in terms of memory and processing time. Per-
forming the SVD takes time O (nmz), where n is
the vocabulary size, m is the number of documents,
and z is the number of nonzero elements per column
in the words-by-documents matrix. As an alterna-
tive, there is a vector space methodology called Ran-
dom Indexing (RI) (Sahlgren, 2005), which presents
an efficient, scalable, and incremental method for
building context vectors. Its computational com-
plexity is O (nr) where n is as previously described
and r is the vector dimension. Particularly, we apply
RI to capture the inherent semantic structure using
Bag of Concepts representation (BoC) as proposed
by Sahlgren and Co?ster (Sahlgren and Co?ster, 2004),
where the meaning of a term is considered as the
sum of contexts in which it occurs.
1http://www.nltk.org/
632
2.3.1 Random Indexing
Random Indexing (RI) is a vector space method-
ology that accumulates context vectors for words
based on co-occurrence data. The technique can be
described as:
? First a unique random representation known as
index vector is assigned to each context (docu-
ment). Index vectors are binary vectors with a
small number of non-zero elements, which are
either +1 or -1, with equal amounts of both.
For example, if the index vectors have twenty
non-zero elements in a 1024-dimensional vec-
tor space, they have ten +1s and ten -1s. Index
vectors serve as indices or labels for documents
? Index vectors are used to produce context vec-
tors by scanning through the text and every
time a target word occurs in a context, the in-
dex vector of the context is added to the con-
text vector of the target word. Thus, at each
encounters of the target word t with a context c
the context vector of t is updated as follows: ct
+ = ic where ct is the context vector of t and ic
is the index vector of c. In this way, the context
vector of a word keeps track of the contexts in
which it occurred.
RI methodology is similar to latent semantic in-
dexing (LSI) (Deerwester et al, 1990). However,
to reduce the co-occurrence matrix no dimension re-
duction technique such as SVD is needed, since the
dimensionality d of the random index vectors is pre-
established as a parameter (implicit dimension re-
duction). Consequently d does not change once it
has been set; as a result, the dimensionality of con-
text vectors will never change with the addition of
new data.
2.3.2 Bag of Concepts
Bag of Concepts (BoC) is a recent representa-
tion scheme proposed by Sahlgren and Co?ster in
(Sahlgren and Co?ster, 2004), which is based on the
perception that the meaning of a document can be
considered as the union of the meanings of its terms.
This is accomplished by generating term context
vectors from each term within the document, and
generating a document vector as the weighted sum
of the term context vectors contained within that
document. Therefore, we use RI to represent the
meaning of a word as the sum of contexts (entire
documents) in which it occurs. Illustrating this tech-
nique, suppose you have two documents: D1: A man
with a hard hat is dancing, and D2: A man wearing
a hard hat is dancing. Let us suppose that they have
index vectors ID1 and ID2, respectively: the context
vector for hat will be the ID1 + ID2, because this
word appears in both documents. Once the context
vectors have been built by RI, they are used to repre-
sent the document as BoC. For instance, supposing
CV1, CV2, CV3, . . . and CV8, are the context vec-
tors of each word in D1, then document D1 will be
represented as the weighted sum of these eight con-
text vectors.
2.3.3 Implementation
The sentences of each file were processed to gen-
erate the BoC representations of them. BoC rep-
resentations were generated by first stemming all
words in the sentences. We then used random index-
ing to produce context vectors for each word in the
files (i.e. STS.input.MSRpar, STS.input.MSRvid,
etc.), each file was considered a different corpus and
documents were the sentences in them. The dimen-
sion of the context vectors was fixed at 2048, de-
termined by experimentation using the training set.
These context vectors were then tf ? idf -weighted,
according to the corpus, and added up for each sen-
tence, to produce BoC representations. Therefore
the similarity values were calculated by the cosine
function. Finally cosine values were multiplied by 5
to produce values between 0 and 5.
3 Experimental results
In Table 1 we show the results obtained by the
three approaches submitted to the competition. The
columns of Table 1 stand for:
? ALL: Pearson correlation with the gold stan-
dard for the five datasets, and corresponding
rank.
? ALLnrm: Pearson correlation after the system
outputs for each dataset are fitted to the gold
standard using least squares, and corresponding
rank.
633
Run ALL Rank ALL
nrm
Rank
Nrm
Mean Rank
Mean
MSR
par
MSR
vid
SMT
eur
On -
WN
SMT-
news
BUAP-
RUN-1
0.4997 63 0.7568 62 0.4892 57 0.4037 0.6532 0.4521 0.605 0.4537
BUAP-
RUN-2
-0.026 89 0.5933 89 0.0669 89 0.1109 0.0057 0.0348 0.1788 0.1964
BUAP-
RUN-3
0.663 25 0.7474 64 0.488 59 0.4018 0.6378 0.4758 0.5691 0.4057
Table 1: Results of approaches of BUAP in Task 6.
? Mean: Weighted mean across the 5 datasets,
where the weight depends on the number of
pairs in the dataset.
Followed by Pearson for individual datasets.
At this moment, we are not aware of the reasons
because the second approach obtained a very poor
performance. The way in which the idf(w) is calcu-
lated could be one of the reasons, because the corpus
used is relatively small and also from a different do-
main. With respect to the other two approaches, we
consider that they (first and third) obtained a com-
parable performance, even when the third approach
obtained the best ALL result with a Pearson correla-
tion equal to 0.663.
4 Discussion and conclusion
We have presented three different approaches for
tackling the problem of Semantic Textual Similarity.
The use of term expansion by synonyms performed
well in general and obtained a comparable behavior
than the third approach which used random index-
ing and bag of concepts. It is interesting to observe
that these two approaches performed similar when
the two term expansion mechanism are totally dif-
ferent. As further, it is important to analyze the poor
behavior of the second approach. We would like also
to introduce semantic relationships other than syn-
onyms in the process of term expansion.
Acknowledgments
This project has been partially supported by
projects CONACYT #106625, #VIAD-ING11-II,
PROMEP/103.5/11/4481 and VIEP #PIAD-ING11-
II.
References
E. Agirre, D. Cer, M. Diab, and B. Dolan. 2012.
SemEval-2012 Task 6: Semantic Textual Similarity.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In proceedings of AAAI?06,
pages 775?780.
Magnus Sahlgren and Rickard Co?ster. 2004. Using bag-
of-concepts to improve the performance of support
vector machines in text categorization. In Proceedings
of the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
M. Sahlgren. 2005. An Introduction to Random Index-
ing. Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering, TKE 2005.
Prajol Shrestha. 2011. Corpus-based methods for short
text similarity. In TALN 2011, Montpellier, France.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In 32nd. Annual Meeting of the
Association for Computational Linguistics, pages 133
?138, New Mexico State University, Las Cruces, New
Mexico.
634
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 706?709,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Lexical and Semantic Similarity for Cross-lingual Textual
Entailment
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we present a report of the two di-
fferent runs submitted to the task 8 of Semeval
2012 for the evaluation of Cross-lingual Tex-
tual Entailment in the framework of Content
Synchronization. Both approaches are based
on textual similarity, and the entailment judg-
ment (bidirectional, forward, backward or no
entailment) is given based on a set of decision
rules. The first approach uses textual simi-
larity on the translated and original versions
of the texts, whereas the second approach ex-
pands the terms by means of synonyms. The
evaluation of both approaches show a similar
behavior which is still close to the average and
median.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of the Textual Entail-
ment task (Dagan and Glickman, 2004). Given a text
(T ) and an hypothesis (H) in different languages,
the CLTE task consists of determining if the mea-
ning of H can be inferred from the meaning of T .
In this paper we present a report of the obtained
results after submitting two different runs for the
Task 8 of Semeval 2012, named ?Cross-lingual Tex-
tual Entailment for Content Synchronization? (Negri
et al, 2012). In this task, the Cross-Lingual Tex-
tual Entailment addresses textual entailment recog-
nition under a new dimension (cross-linguality), and
within a new challenging application scenario (con-
tent synchronization). The task 8 of Semeval 2012
may be formally defined as follows:
Given a pair of topically related text fragments
(T1 and T2) in different languages, the task consists
of automatically annotating it with one of the follo-
wing entailment judgments:
? Bidirectional (T1 ? T2 & T1 ? T2): the two
fragments entail each other (semantic equiva-
lence)
? Forward (T1 ? T2 & T1 ! ? T2): unidirec-
tional entailment from T1 to T2
? Backward (T1 ! ? T2 & T1 ? T2): unidirec-
tional entailment from T2 to T1
? No Entailment (T1 ! ? T2 & T1 ! ? T2): there
is no entailment between T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs. Cross-lingual datasets are avai-
lable for the following language combinations:
? Spanish/English (SPA-ENG)
? German/English (DEU-ENG)
? Italian/English (ITA-ENG)
? French/English (FRA-ENG)
The remaining of this paper is structured as fo-
llows: Section 2 describes the two different approa-
ches presented in the competition. The obtained re-
sults are shown and dicussed in Section 3. Finally,
the findings of this work are given in Section 4.
706
2 Experimental setup
For this experiment we have considered to tackle the
CLTE task by means of textual similarity and textual
length. In particular, the textual similarity is used to
determine whether some kind of entailment exists or
not. We have established the threshold of 0.5 for the
similarity function as evidence of textual entailment.
Since the two sentences to be evaluated are written
in two different languages, we have translated each
sentence to the other language, so that, we have two
sentences in English, and two sentences in the origi-
nal language (Spanish, German, Italian and French).
We have used the Google translate for this purpose
1
.
The corpora used in the experiments comes from
a cross-lingual Textual Entailment dataset presented
in (Negri et al, 2011), and provided by the task orga-
nizers. We have employed the training dataset only
for adjust some parameters of the system, but the
approach is knowledge-based and, therefore, it does
not need a training corpus. Both, the training and
test corpus contain 500 sentences for each language.
The textual length is used to determine the entail-
ment judgment (bidirectional, forward, backward,
no entailment). We have basically, assumed that the
length of a text may give some evidence of the type
of entailment. The decision rules used for determi-
ning the entailment judgment are described in Sec-
tion 2.3.
In this competition we have submitted two diffe-
rent runs which differ with respect to the type of tex-
tual similarity used (lexical vs semantic). The first
one, calculates the similarity using only the trans-
lated version of the original sentences, whereas the
second approach uses text expansion by means of
synonyms and, thereafter, it calculates the similarity
between the pair of sentences.
Let T1 be the sentence in the original language,
T2 the T1 topically related text fragment (written in
English). Let T3 be the English translation of T1,
and T4 the translation of T2 to the original language
(Spanish, German, Italian and French). The formal
description of these two approaches are given as fo-
llows.
1http://translate.google.com.mx/
2.1 Approach 1: Lexical similarity
The evidence of textual entailment between T1 and
T2 is calculated using two formulae of lexical si-
milarity. Firstly, we determine the similarity bet-
ween the two texts written in the source language
(SimS). Additionally, we calculate the lexical simi-
larity between the two sentences written in the target
language (SimT ), in this case English.
Given the limited text length of the text fragments,
we have used the Jaccard coefficient as similarity
measure. Eq. (1) shows the lexical similarity for the
two texts written in the original language, whereas,
Eq. (2) presents the Jaccard coefficient for the texts
written in English.
simS = simJaccard(T1, T4) =
|T1 ? T4|
|T1 ? T4|
(1)
simT = simJaccard(T2, T3) =
|T2 ? T3|
|T2 ? T3|
(2)
2.2 Approach 2: Semantic similarity
In this case we calculate the semantic similarity bet-
ween the two texts written in the original language
(simS), and the semantic similarity between the two
text fragments written in English (simT ). The se-
mantic level of similarity is given by considering
the synonyms of each term for each sentence (in
the original and target language). For this purpose,
we have employed five dictionaries containing syno-
nyms for the five different languages considered in
the competition (English, Spanish, German, Italian,
and French)2. In Table 1 we show the number of
terms, so as the number of synonyms in average by
term considered for each language.
Let T1 = w1,1w1,2...w1,|T1|, T2 =
w2,1w2,2...w2,|T2| be the source and target
sentences, and let T3 = w3,1w3,2...w3,|T3|,
T4 = w4,1w4,2...w4,|T4| be translated version of the
original source and target sentences, respectively.
The synonyms of a given word wi,k, expressed as
synset(wi,k), are obtained from the aforementioned
dictionaries by extracting the synonyms of wi,k. In
order to obtain a better matching between the terms
contained in the text fragments and the terms in the
2http://extensions.services.openoffice.org/en/dictionaries
707
Table 1: Dictionaries of synonyms used for term expan-
sion
Language Terms synonyms per term
(average)
English 2,764 60
Spanish 9,887 45
German 21,958 115
Italian 25,724 56
French 36,207 93
dictionary, we have stemmed all the terms using the
Porter stemmer.
In order to determine the semantic similarity bet-
ween two terms of sentences written in the source
language (w1,i and w4,j) we use Eq. (3). The se-
mantic similariy between two terms of the English
sentences are calculated as shown in Eq. (4).
sim(w1,i, w4,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w4,j) ||
w1,i ? synset(w4,j) ||
w4,j ? synset(w1,i)
0 otherwise
(3)
sim(w2,i, w3,j) =
?
?
?
?
?
?
?
?
?
1 if (w2,i == w3,j) ||
w2,i ? synset(w3,j) ||
w3,j ? synset(w2,i)
0 otherwise
(4)
Both equations consider the existence of semantic
similarity when the two words are identical, or when
the some of the two words appear in the synonym set
of the other word.
The semantic similarity of the complete text frag-
ments T1 and T4 (simS) is calculated as shown in
Eq. (5). Whereas, the semantic similarity of the
complete text fragments T2 and T3 (simT ) is cal-
culated as shown in Eq. (6).
simS(T1, T4) =
?|T1|
i=1
?|T4|
j=1 sim(w1,i,w4,j)
|T1?T4|
(5)
simT (T2, T3) =
?|T2|
i=1
?|T3|
j=1 sim(w2,i,w3,j)
|T2?T3|
(6)
2.3 Decision rules
Both approches used the same decision rules in or-
der to determine the entailment judgment for a given
pair of text fragments (T1 and T2). The following al-
gorithm shows the decision rules used.
Algorithm 1.
If |T2| < |T3| then
If (simT > 0.5 and simS > 0.5)
then forward
ElseIf |T2| > |T3| then
If (simT > 0.5 and simS > 0.5)
then backward
ElseIf (|T1| == |T4| and |T2| == |T3|) then
If (simT > 0.5 and simS > 0.5)
then bidirectional
Else no entailment
As mentioned above, the rules employed the le-
xical or semantic textual similarity, and the textual
length for determining the textual entailment.
3 Results
In Table 2 we show the overall results obtained by
the two approaches submitted to the competition.
We also show the highest, lowest, average and me-
dian overall results obtained in the competition.
SPA-
ENG
ITA-
ENG
FRA-
ENG
DEU-
ENG
Highest 0.632 0.566 0.57 0.558
Average 0.407 0.362 0.366 0.357
Median 0.346 0.336 0.336 0.336
Lowest 0.266 0.278 0.278 0.262
BUAP run1 0.35 0.336 0.334 0.33
BUAP run2 0.366 0.344 0.342 0.268
Table 2: Overall statistics obtained in the Task 8 of Se-
meval 2012
The runs submitted perform similar, but the se-
mantic approach obtained a slightly better perfor-
mance. The two results are above the median but
below the average. We consider that better results
may be obtained if the two features used (textual si-
milarity and textual length) were introduced into a
supervised classifier, so that, the decision rules were
approximated on the basis of a training dataset, ins-
tead of the empirical setting done in this work. Fu-
ture experiments will be carried out in this direction.
708
4 Discussion and conclusion
Two different approaches for the Cross-lingual Tex-
tual Entailment for Content Synchronization task of
Semeval 2012 are reported in this paper. We used
two features for determining the textual entailment
judgment between two texts T1 and T2 (written in
two different languages). The first approach pro-
posed used lexical similarity, meanwhile the second
used semantic similarity by means of term expan-
sion with synonyms.
Even if the performance of both approaches is
above the median and slighly below the average,
we consider that we may easily improve this perfor-
mance by using syntactic features of the text frag-
ments. Additionally, we are planning to integrate
some supervised techniques based on decision rules
which may be trained in a supervised dataset. Future
experiments will be executed in this direction.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, #VIAD-ING11-II and VIEP
#PIAD-ING11-II.
References
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Learning Methods for Text Un-
derstanding and Mining, January.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 321?
324, Los Angeles, California, June. Association for
Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
709
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 145?148,
Dublin, Ireland, August 23-24, 2014.
BUAP: Evaluating Compositional Distributional Semantic Models on Full
Sentences through Semantic Relatedness and Textual Entailment
Sau?l Leo?n, Darnes Vilarin?o, David Pinto, Mireya Tovar, Beatriz Beltra?n
Beneme?rita Universidad Auto?noma de Puebla
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{saul.leon,darnes,dpinto,mtovar,bbeltran}@cs.buap.mx
Abstract
The results obtained by the BUAP team at
Task 1 of SemEval 2014 are presented in this
paper. The run submitted is a supervised ver-
sion based on two classification models: 1)
We used logistic regression for determining
the semantic relatedness between a pair of
sentences, and 2) We employed support vec-
tor machines for identifying textual entailment
degree between the two sentences. The be-
haviour for the second subtask (textual entail-
ment) obtained much better performance than
the one evaluated at the first subtask (related-
ness), ranking our approach in the 7th position
of 18 teams that participated at the competi-
tion.
1 Introduction
The Compositional Distributional Semantic Models
(CDSM) applied to sentences aim to approximate
the meaning of those sentences with vectors summa-
rizing their patterns of co-occurrence in corpora. In
the Task 1 of SemEval 2014, the organizers aimed
to evaluate the performance of this kind of models
through the following two tasks: semantic related-
ness and textual entailment. Semantic relatedness
captures the degree of semantic similarity, in this
case, between a pair of sentences, whereas textual
entailment allows to determine the entailment rela-
tion holding between two sentences.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
This document is a description paper, therefore,
we focus the rest of it on the features and models we
used for carrying out the experiments. A complete
description of the task and the dataset used are given
in Marelli et al. (2014a) and in Marelli et al. (2014b),
respectively.
The remaining of this paper is structured as fol-
lows. In Section 2 we describe the general model
we used for comparing two sentences and the set of
the features used for constructing the vectorial rep-
resentation for each sentence. Section 3 shows how
we integrate the features calculated in a single vector
which fed a supervised classifier aiming to construct
a classication model that solves the two aforemen-
tioned problems: semantic relatedness and textual
entailment. In the same section we show the ob-
tained results. Finally, in Section 4 we present our
findings.
2 Description of the Distributional
Semantic Model Used
Given a sentence S = w
1
w
2
? ? ?w
|S|
, with w
i
a sen-
tence word, we have calculated different correlated
terms (t
i,j
) or a numeric vector (V
i
) for each word
w
i
as follows:
1. {t
i,j
|relation(t
i,j
, w
i
)} such as ?relation? is
one the following dependency relations: ?ob-
ject?, ?subject? or ?property?.
2. {t
i,j
|t
i,j
= c
k
? ? ? c
k+n
} with n = 2, ? ? ? , 5, and
c
k
? w
i
; these tokens are also known as n-
grams of length n.
3. {t
i,j
|t
i,j
= c
k
? ? ? c
k+((n?1)?r)
} with n =
145
2, ? ? ? , 5, r = 2, ? ? ? , 5, and c
k
? w
i
; these to-
kens are also known as skip-grams of length
n.
4. V
i
is obtained by applying the Latent Semantic
Analysis (LSA) algorithm implemented in the
R software environment for statistical comput-
ing and graphics. V
i
is basically a vector of val-
ues that represent relation of the word w
i
with
it context, calculated by using a corpus con-
structed by us, by integrating information from
Europarl, Project-Gutenberg and Open Office
Thesaurus.
3 A Classification Model for Semantic
Relatedness and Textual Entailment
based on DSM
Once each sentence has been represented by means
of a vectorial representation of patterns, we con-
structed a single vector, ??u , for each pair of sen-
tences with the aim of capturing the semantic relat-
edness on the basis of a training corpus.
The entries of this representation vector are calcu-
lated by obtaining the semantic similarity between
each pair of sentences, using each of the DSM
shown in the previous section. In order to calcu-
late each entry, we have found the maximum similar-
ity between each word of the first sentence with re-
spect to the second sentence and, thereafter, we have
added all these values, thus, ??u = {f
1
, ? ? ? , f
9
}.
Given a pair of sentences S
1
=
w
1,1
w
2,1
? ? ?w
|S
1
|,1
and S
2
= w
1,2
w
2,2
? ? ?w
|S
2
|,2
,
such as each w
i,k
is represented according to the
correlated terms or numeric vectors established
at Section 2, the entry f
i
of ??u is calculated
as: f
l
=
?
|S
1
|
i=1
max{sim(w
i,1
, w
j,2
)}, with
j = 1, ? ? ? , |S
2
|.
The specific similarity measure (sim()) and the
correlated term or numeric vector used for each f
l
is
described as follows:
1. f
1
: w
i,k
is the ?object? of w
i
(as defined
in 2), and sim() is the maximum similar-
ity obtained by using the following six Word-
Net similarity metrics offered by NLTK: Lea-
cock & Chodorow (Leacock and Chodorow,
1998), Lesk (Lesk, 1986), Wu & Palmer (Wu
and Palmer, 1994), Resnik (Resnik, 1995), Lin
(Lin, 1998), and Jiang & Conrath1 (Jiang and
Conrath, 1997).
2. f
2
: w
i,k
is the ?subject? of w
i
, and sim() is
the maximum similarity obtained by using the
same six WordNet similarity metrics.
3. f
3
: w
i,k
is the ?property? of w
i
, and sim() is
the maximum similarity obtained by using the
same six WordNet similarity metrics.
4. f
4
: w
i,k
is an n-gram containing w
i
, and sim()
is the cosine similarity measure.
5. f
5
: w
i,k
is an skip-gram containing w
i
, and
sim() is the cosine similarity measure.
6. f
6
: w
i,k
is numeric vector obtained with LSA,
and sim() is the Rada Mihalcea semantic sim-
ilarity measure (Mihalcea et al., 2006).
7. f
7
: w
i,k
is numeric vector obtained with LSA,
and sim() is the cosine similarity measure.
8. f
8
: w
i,k
is numeric vector obtained with LSA,
and sim() is the euclidean distance.
9. f
9
: w
i,k
is numeric vector obtained with LSA,
and sim() is the Chebyshev distance.
All these 9 features were introduced to a logistic
regression classifier in order to obtain a classifica-
tion model which allows us to determine the value of
relatedness between a new pair of sentences2. Here,
we use as supervised class, the value of relatedness
given to each pair of sentences on the training cor-
pus.
The obtained results for the relatedness subtask
are given in Table 1. In columns 2, 3 and 5, a large
value signals a more efficient system, but a large
MSE (column 4) means a less efficient system. As
can be seen, our run obtained the rank 12 of 17, with
values slightly below the overall average.
3.1 Textual Entailment
In order to calculate the textual entailment judgment,
we have enriched the vectorial representation previ-
ously mentioned with synonyms, antonyms and cue-
1Natural Language Toolkit of Python; http://www.nltk.org/
2We have employed the Weka tool with the default settings
for this purpose
146
Table 1: Results obtained at the substask ?Relatedness? of the Semeval 2014 Task 1
TEAM ID PEARSON SPEARMAN MSE Rank
ECNU run1 0.82795 0.76892 0.32504 1
StanfordNLP run5 0.82723 0.75594 0.32300 2
The Meaning Factory run1 0.82680 0.77219 0.32237 3
UNAL-NLP run1 0.80432 0.74582 0.35933 4
Illinois-LH run1 0.79925 0.75378 0.36915 5
CECL ALL run1 0.78044 0.73166 0.39819 6
SemantiKLUE run1 0.78019 0.73598 0.40347 7
CNGL run1 0.76391 0.68769 0.42906 8
UTexas run1 0.71455 0.67444 0.49900 9
UoW run1 0.71116 0.67870 0.51137 10
FBK-TR run3 0.70892 0.64430 0.59135 11
BUAP run1 0.69698 0.64524 0.52774 12
UANLPCourse run2 0.69327 0.60269 0.54225 13
UQeResearch run1 0.64185 0.62565 0.82252 14
ASAP run1 0.62780 0.59709 0.66208 15
Yamraj run1 0.53471 0.53561 2.66520 16
asjai run5 0.47952 0.46128 1.10372 17
overall average 0.71876 0.67159 0.63852 8-9
Our difference against the overall average -2% -3% 11% -
words (?no?, ?not?, ?nobody? and ?none?) for de-
tecting negation at the sentences3 . Thus, if some of
these new features exist on the training pair of sen-
tences, we add a boolean value of 1, otherwise we
set the feature to zero.
This new set of vectors is introduced to a support
vector machine classifier4, using as class the textual
entailment judgment given on the training corpus.
The obtained results for the textual entailment
subtask are given in Table 2. Our run obtained the
rank 7 of 18, with values above the overall average.
We consider that this improvement over the related-
ness task was a result of using other features that
are quite important for semantic relatedness, such
as lexical relations (synonyms and antonyms), and
the consideration of the negation phenomenon in the
sentences.
4 Conclusions
This paper describes the use of compositional distri-
butional semantic models for solving the problems
3Synonyms were extracted from WordNet, whereas the
antonyms were collected from Wikipedia.
4Again, we have employed the weka tool with the default
settings for this purpose.
of semantic relatedness and textual entailment. We
proposed different features and measures for that
purpose. The obtained results show a competitive
approach that may be further improved by consider-
ing more lexical relations or other type of semantic
similarity measures.
In general, we obtained the 7th place in the official
ranking list from a total of 18 teams that participated
at the textual entailment subtask. The result at the
semantic relatedness subtask could be improved if
we were considered to add the new features taken
into consideration at the textual entailment subtask,
an idea that we will implement in the future.
References
Jay J. Jiang and David W. Conrath. Semantic simi-
larity based on corpus statistics and lexical taxon-
omy. In Proc of 10th International Conference
on Research in Computational Linguistics, RO-
CLING?97, pages 19?33, 1997.
Claudia Leacock and Martin Chodorow. Combin-
ing local context and wordnet similarity for word
sense identification. In Christiane Fellfaum, edi-
tor, MIT Press, pages 265?283, 1998.
147
Table 2: Results obtained at the substask ?Textual Entailment? of the Semeval 2014 Task 1
TEAM ID ACCURACY Rank
Illinois-LH run1 84.575 1
ECNU run1 83.641 2
UNAL-NLP run1 83.053 3
SemantiKLUE run1 82.322 4
The Meaning Factory run1 81.591 5
CECL ALL run1 79.988 6
BUAP run1 79.663 7
UoW run1 78.526 8
CDT run1 77.106 9
UIO-Lien run1 77.004 10
FBK-TR run3 75.401 11
StanfordNLP run5 74.488 12
UTexas run1 73.229 13
Yamraj run1 70.753 14
asjai run5 69.758 15
haLF run2 69.413 16
CNGL run1 67.201 17
UANLPCourse run2 48.731 18
Overall average 75.358 11-12
Our difference against the overall average 4.31% -
Michael Lesk. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference
on Systems Documentation, pages 24?26. ACM,
1986.
Dekang Lin. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Inter-
national Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA,
1998. Morgan Kaufmann Publishers Inc.
Marco Marelli, Luisa Bentivogli, Marco Baroni,
Raffaella Bernardi, Stefano Menini, and Roberto
Zamparelli. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on
full sentences through semantic relatedness and
textual entailment. In Proceedings of the 8th
International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, 2014a.
Marco Marelli, Stefano Menini, Marco Baroni,
Luisa Bentivogli, Raffaella Bernardi, and Roberto
Zamparelli. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014, Reykjavik, Iceland,
2014b.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial In-
telligence, pages 775?780, 2006.
Philip Resnik. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, IJCAI?95, pages 448?453,
San Francisco, CA, USA, 1995.
Zhibiao Wu and Martha Stone Palmer. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, 1994.
148
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 149?153,
Dublin, Ireland, August 23-24, 2014.
BUAP: Evaluating Features for Multilingual and Cross-Level Semantic
Textual Similarity
Darnes Vilarin?o, David Pinto, Sau?l Leo?n, Mireya Tovar,Beatriz Beltra?n
Beneme?rita Universidad Auto?noma de Puebla
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes,dpinto,saul.leon,mtovar,bbeltran}@cs.buap.mx
Abstract
In this paper we present the evaluation of
different features for multiligual and cross-
level semantic textual similarity. Three dif-
ferent types of features were used: lexical,
knowledge-based and corpus-based. The re-
sults obtained at the Semeval competition rank
our approaches above the average of the rest
of the teams highlighting the usefulness of the
features presented in this paper.
1 Introduction
Semantic textual similarity aims to capture whether
the meaning of two texts are similar. This concept
is somehow different from the textual similarity def-
inition itself, because in the latter we are only in-
terested in measuring the number of lexical com-
ponents that the two texts share. Therefore, tex-
tual similarity can range from exact semantic equiv-
alence to a complete unrelatedness pair of texts.
Finding the semantic similarity between a pair
of texts has become a big challenge for specialists
in Natural Language Processing (NLP), because it
has applications in some NLP task such as machine
translation, automatic construction of summaries,
authorship attribution, machine reading comprehen-
sion, information retrieval, among others, which
usually need a manner to calculate degrees of simi-
larity between two given texts.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Semantic textual similarity can be calculated us-
ing texts of different sizes, for example between, a
paragraph and a sentence, or a sentence and a phrase,
or a phrase and a word, or even a word and a sense.
When we consider this difference, we say the task is
called ?Cross-Level Semantic Similarity?, but when
this distinction is not considered, then we call the
task just as ?Semantic Textual Similarity?.
In this paper, we evaluate different features for de-
termining those that obtain the best performances for
calculating both, cross-level semantic similarity and
multilingual semantic textual similarity.
The remaining of this paper is structured as fol-
lows. Section 2 presents the features used in both
experiments. Section 3 shows the manner we used
the features for determining the degree of seman-
tic textual similarity. Section 4, on the other hand,
shows the experiments we have carried out for de-
termining cross-level semantic similarity. Finally, in
Section 5 the conclusions and findings are given.
2 Description of Features
In this section we describe the different features used
for evaluation semantic textual similarity. Basically,
we have used three different types of features: lex-
ical, knowledge-based and corpus-based. The first
one, counts the frequency of ocurrence of lexical
features which include n-grams of characters, skip-
grams1, words and some lexical relationships such
as synonymy or hypernymy. Additionally, we have
used two other features: the Jaccard coefficient be-
tween the two text, expanding each term with a set of
1They are also known as disperse n-grams because they con-
sider to ?skip? a certain number of characters.
149
synonyms taken from WordReference Carrillo et al.
(2012), and the cosine between the two texts repre-
sented each by a bag of character n-grams and skip-
grams. In this case, we did not applied any word
sense disambiguation system before expanding with
synonyms, a procedure that may be performed in a
further work.
The second set of features considers the following
six word similarity metrics offered by NLTK: Lea-
cock & Chodorow (Leacock and Chodorow, 1998),
Lesk (Lesk, 1986), Wu & Palmer (Wu and Palmer,
1994), Resnik (Resnik, 1995), Lin (Lin, 1998), and
Jiang & Conrath2 (Jiang and Conrath, 1997). In
this case, we determine the similarity between two
texts as the maximum possible pair of words similar-
ity. The third set of features considers two corpus-
based measures, both based on Rada Mihalcea?s tex-
tual semantic similarity (Mihalcea et al., 2006). The
first one uses Pointwise Mutual Information (PMI)
(Turney, 2001) for calculating the similarity between
pairs of words, whereas the second one uses Latent
Semantic Analysis (LSA) (Landauer et al., 1998)
(implemented in the R software environment for sta-
tistical computing) for that purpose. In particular,
the PMI and LSA values were obtained using a cor-
pus built on the basis of Europarl, Project-Gutenberg
and Open Office Thesaurus. A summary of these
features can be seen in Table 1.
3 Multilingual Semantic Textual Similarity
This task aims to find the semantic textual similar-
ity between two texts written in the same language.
Two different languages were considered: English
and Spanish. The degree of semantic similarity
ranges from 0 to 5; the bigger this value, the best se-
mantic match between the two texts. For the experi-
ments we have used the training datasets provided at
2012, 2013 and 2014 Semeval competitions. These
datasets are completely described at the task descrip-
tion papers of these Semeval editions Agirre et al.
(2013, 2014).
In order to calculate the semantic textual simi-
larity for the English language, we have used all
the features mentioned at Section 2. We have con-
structed a single vector for each pair of texts of the
training corpus, thus resulting 6,627 vectors in total.
2Natural Language Toolkit of Python; http://www.nltk.org/
The resulting set of vectors fed a supervised classi-
fier, in particular, a logistic regression model3. This
approach has been named as BUAP-EN-run1. The
most representative results obtained at the competi-
tion for the English language can be seen in Table 2.
As can be seen, we outperformed the average result
in all the cases, except on the case that the OnWN
corpus was used.
In order to calculate the semantic textual similar-
ity for the Spanish language, we have submitted two
runs, the first one is a supervised approach which
constructs a regression model, similar that the one
constructed for the English language, but consider-
ing only the following features: character n-grams,
character skip-grams, and the cosine similarity of
bag of character n-grams and skip-grams. This ap-
proach was named BUAP-run1. Given that the num-
ber of Spanish samples was so small, we decided
to investigate the behaviour of training with English
and testing with Spanish language. It is quite inter-
esting that this approach obtained a relevant ranking
(17 from 22 runs), even if the type of features used
were na??ve.
The second approach submitted for determining
the semantic textual similarity for the Spanish lan-
guage is an unsupervised one. It uses the same fea-
tures of the supervised approach for Spanish, but
these features were used to create a representation
vector for each text (independently), so that we may
be able to calculate the similarity by means of the
cosine measure between the two vectors. The ap-
proach was named BUAP-run2.
The most representative results obtained at the
competition for the Spanish language can be seen
in Table 3. There we can see that our unsupervised
approach slightly outperformed the overall average,
but the supervised approach was below the overall
average, a fact that is expected since we have trained
using the English corpus and testing with the Span-
ish language. Despite this, it is quite interesting that
the result obtained with this supervised approach is
not so bad.
Due to space constraints, we did not reported the
complete set of results of the competition, however,
these results can be seen at the task 10 description
3We used the version of the logistic classifier implemented
in the the Weka toolkit
150
Table 1: Features used for calculating semantic textual similarity
Feature Type
n-grams of characters (n = 2, ? ? ? , 5) Lexical
skip-grams of characters (skip = 2, ? ? ? , 5) Lexical
Number of words shared Lexical
Number of synonyms shared Lexical
Number of hypernyms shared Lexical
Jaccard coefficient with synonyms expansion Lexical
Cosine of bag of character n-grams and skip-grams Lexical
Leacock & Chodorow?s word similarity Knowledge-based
Lesk?s word similarity Knowledge-based
Wu & Palmer?s word similarity Knowledge-based
Resnik?s word similarity Knowledge-based
Lin?s word similarity Knowledge-based
Jiang & Conrath?s word similarity Knowledge-based
Rada Mihalcea?s metric using PMI Corpus-based
Rada Mihalcea?s metric using LSA Corpus-based
Table 2: Results obtained at the Task 10 of the Semeval competition for the English language
Team Name deft-forum deft-news headlines images OnWN tweet-news Weighted mean Rank
DLS@CU-run2 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639 0.7610 1
Meerkat Mafia-pairingWords 0.4711 0.7628 0.7597 0.8013 0.8745 0.7793 0.7605 2
NTNU-run3 0.5305 0.7813 0.7837 0.8343 0.8502 0.6755 0.7549 3
BUAP-EN-run1 0.4557 0.6855 0.6888 0.6966 0.6539 0.7706 0.6715 19
Overall average 0.3607 0.6198 0.5885 0.6760 0.6786 0.6001 0.6015 27-28
Bielefeld SC-run2 0.2108 0.4307 0.3112 0.3558 0.3607 0.4087 0.3470 36
UNED-run22 p np 0.1043 0.3148 0.0374 0.3243 0.5086 0.4898 0.3097 37
LIPN-run2 0.0843 - - - - - 0.0101 38
Our difference against the average 9% 7% 10% 2% -2% 17% 7% -
Table 3: Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symbol
denotes a system that used Wikipedia to build its model for the Wikipedia test dataset)
Team Name System type Wikipedia News Weighted correlation Rank
UMCC DLSI-run2 supervised 0.7802 0.8254 0.8072 1
Meerkat Mafia-run2 unsupervised 0.7431 0.8454 0.8042 2
UNAL-NLP-run1 weakly supervised 0.7804 0.8154 0.8013 3
BUAP-run2 unsupervised 0.6396 0.7637 0.7137 14
Overall average - 0.6193 0.7504 0.6976 14-15
BUAP-run1 supervised 0.5504 0.6785 0.6269 17
RTM-DCU-run2 supervised 0.3689 0.6253 0.5219 20
Bielefeld SC-run2 unsupervised* 0.2646 0.5546 0.4377 21
Bielefeld SC-run1 unsupervised* 0.2632 0.5545 0.4371 22
Difference between our run1 and the overall average - -7% -7% -7% -
Difference between our run2 and the overall average - 2% 1% 2% -
paper (Agirre et al., 2014) of Semeval 2014.
4 Cross-Level Semantic Similarity
This task aims to find semantic similarity between
a pair of texts of different length written in En-
glish language, actually each text belong to a dif-
ferent level of representation of language (para-
graph, sentence, phrase, word, and sense). Thus,
the pair of levels that were required to be compared
in order to determine their semantic similarity were:
paragraph-to-sentence, sentence-to-phrase, phrase-
to-word, and word-to-sense.
The task cross level similarity judgments are
based on five rating levels which goes from 0 to
151
4. The first (0) implies that the two items do not
mean the same thing and are not on the same topic,
whereas the last one (4) implies that the two items
have very similar meanings and the most important
ideas, concepts, or actions in the larger text are rep-
resented in the smaller text. The remaining rating
levels imply something in the middle.
For word-to-sense comparison, a sense is paired
with a word and the perceived meaning of the word
is modulated by virtue of the comparison with the
paired sense?s definition. For the experiments pre-
sented at the competition, a corpus of 2,000 pairs
of texts were provided for training and other 2,000
pairs for testing. This dataset considered 500 pairs
for each type of level of semantic similarity. The
complete description of this task together with the
dataset employed is given in the task description pa-
per Jurgens et al. (2014).
We submitted two supervised approaches, to this
task employing all the features presented at Section
2. The first approach simply constructs a single vec-
tor for each pair of training texts using the afore-
mentioned features. These vectors are introduced in
Weka for constructing a classification model based
on logistic regression. This approach was named
BUAP-run1.
We have observed that when comparing texts of
different length, there may be a high discrepancy
between those texts because a very small length in
the texts may difficult the process of determining the
semantic similarity. Therefore, we have proposed
to expand small text with the aim of having more
term useful in the process of calculating the degree
of semantic similarity. In particular, we have ex-
panded words for the phrase-to-word and word-to-
sense cases. The expansion has been done as fol-
lows. When we calculated the similarity between
phrases and words, we expanded the word compo-
nent with those related terms obtained by means of
the Related-Tags Service of Flickr. When we cal-
culated the semantic similarity between words and
senses, we expanded the word component with their
WordNet Synsets (none word sense disambiguation
method was employed). This second approach was
named BUAP-run2.
The most representative results for the cross-level
semantic similarity task (which include our results)
are shown in Table 4. There we can see that the fea-
tures obtained a good performance when we com-
puted the semantic similarity between paragraphs
and sentences, and when we calculated the simili-
raty between sentences to phrases. Actually, both
runs obtained exactly the same result, because the
main difference between these two runs is that the
second one expands the word/sense using the Re-
lated Tags of Flickr. However, the set of expansion
words did not work properly, in particular when cal-
culating the semantic similarity between phrases and
words. We consider that this behaviour is due to
the domain of the expansion set do not match with
the domain of the dataset to be evaluated. In the
case of expanding words for calculating the similar-
ity between words and senses, we obtained a slightly
better performance, but again, this values are not
sufficient to highly outperform the overall average.
As future work we consider to implement a self-
expansion technique for obtaining a set of related
terms by means of the same training corpus. This
technique has proved to be useful when the expan-
sion process is needed in restricted domains Pinto
et al. (2011).
5 Conclusions
This paper presents the results obtained by the
BUAP team at the Task 3 and 10 of SemEval 2014.
In both task we have used a set of similar features,
due to the aim of these two task are quite similar:
determining semantic similarity. Some special mod-
ifications has been done according to each task in
order to tackle some issues like the language or the
text length.
In general, the features evaluated performed well
over the two approaches, however, some issues arise
that let us know that we need to tune the approaches
presented here. For example, a better expansion set
is required in the case of the Task 3, and a great num-
ber of samples for the spanish samples of Task 10
will be required.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor
Gonzalez-Agirre, and Weiwei Guo. *sem 2013
shared task: Semantic textual similarity. In 2nd
Joint Conference on Lexical and Computational
152
Table 4: Results obtained at Task 3 of Semeval 2014
Team System Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-Word Word-to-Sense Rank
SimCompass run1 0.811 0.742 0.415 0.356 1
ECNU run1 0.834 0.771 0.315 0.269 2
UNAL-NLP run2 0.837 0.738 0.274 0.256 3
BUAP run1 0.805 0.714 0.162 0.201 9
BUAP run2 0.805 0.714 0.142 0.194 10
Overall average - 0.728 0.651 0.198 0.192 11-12
Our run1 - Overall average 8% 6% -4% 1% -
Our run2 - Overall average 8% 6% -6% 0% -
Semantics (*SEM), pages 32?43, Atlanta, Geor-
gia, USA, 2013.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. Semeval-2014 task 10: Multilingual se-
mantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evalua-
tion (SemEval-2014), Dublin, Ireland, 2014.
Maya Carrillo, Darnes Vilarin?o, David Pinto,
Mireya Tovar, Saul Leo?n, and Esteban Castillo.
Fcc: Three approaches for semantic textual sim-
ilarity. In Proceedings of the 1st Joint Con-
ference on Lexical and Computational Seman-
tics (SemEval 2012), pages 631?634, Montre?al,
Canada, 2012.
Jay J. Jiang and David W. Conrath. Semantic simi-
larity based on corpus statistics and lexical taxon-
omy. In Proc of 10th International Conference
on Research in Computational Linguistics, RO-
CLING?97, pages 19?33, 1997.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. Semeval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evalua-
tion (SemEval-2014), Dublin, Ireland, 2014.
Thomas K. Landauer, Peter W. Foltz, and Darrell
Laham. An Introduction to Latent Semantic Anal-
ysis. Discourse Processes, (25):259?284, 1998.
Claudia Leacock and Martin Chodorow. Combin-
ing local context and wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, MIT Press, pages 265?283, 1998.
Michael Lesk. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference
on Systems Documentation, pages 24?26. ACM,
1986.
Dekang Lin. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Inter-
national Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA,
1998. Morgan Kaufmann Publishers Inc.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial In-
telligence, pages 775?780, 2006.
David Pinto, Paolo Rosso, and He?ctor Jime?nez-
Salazar. A self-enriching methodology for clus-
tering narrow domain short texts. Computer Jour-
nal, 54(7):1148?1165, 2011.
Philip Resnik. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, IJCAI?95, pages 448?453,
San Francisco, CA, USA, 1995.
Peter D. Turney. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the
12th European Conference on Machine Learning,
pages 491?502. Springer-Verlag, 2001.
Zhibiao Wu and Martha Stone Palmer. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, 1994.
153
