Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1149?1155,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Multilingual Dependency Analysis System using Online
Passive-Aggressive Learning
Le-Minh Nguyen, Akira Shimazu, and Phuong-Thai Nguyen
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{nguyenml,shimazu,thai}@jaist.ac.jp
Xuan-Hieu Phan
Tohoku University
Aobayama 6-3-09, Sendai, 980-8579, Japan
hieuxuan@ecei.tohoku.ac.jp
Abstract
This paper presents an online algorithm for
dependency parsing problems. We propose
an adaptation of the passive and aggressive
online learning algorithm to the dependency
parsing domain. We evaluate the proposed
algorithms on the 2007 CONLL Shared
Task, and report errors analysis. Experimen-
tal results show that the system score is bet-
ter than the average score among the partici-
pating systems.
1 Introduction
Research on dependency parsing is mainly based
on machine learning methods, which can be called
history-based (Yamada and Matsumoto, 2003; Nivre
et al, 2006) and discriminative learning methods
(McDonald et al, 2005a; Corston-Oliver et al,
2006). The learning methods using in discrimina-
tive parsing are Perceptron (Collins, 2002) and on-
line large-margin learning (MIRA) (Crammer and
Singer, 2003).
The difference of MIRA-based parsing in com-
parison with history-based methods is that the
MIRA-based parser were trained to maximize the
accuracy of the overall tree. The MIRA based
parsing is close to maximum-margin parsing as in
Taskar et al (2004) and Tsochantaridis et al (2005)
for parsing. However, unlike maximum-margin
parsing, it is not limited to parsing sentences of 15
words or less due to computation time. The perfor-
mance of MIRA based parsing achieves the state-of-
the-art performance in English data (McDonald et
al., 2005a; McDonald et al, 2006).
In this paper, we propose a new adaptation of on-
line larger-margin learning to the problem of depen-
dency parsing. Unlike the MIRA parser, our method
does not need an optimization procedure in each
learning update, but users only an update equation.
This might lead to faster training time and easier im-
plementation.
The contributions of this paper are two-fold: First,
we present a training algorithm called PA learning
for dependency parsing, which is as easy to im-
plement as Perceptron, yet competitive with large
margin methods. This algorithm has implications
for anyone interested in implementing discrimina-
tive training methods for any application. Second,
we evaluate the proposed algorithm on the multilin-
gual data task as well as the domain adaptation task
(Nivre et al, 2007).
The remaining parts of the paper are organized as
follows: Section 2 proposes our dependency pars-
ing with Passive-Aggressive learning. Section 3
discusses some experimental results and Section 4
gives conclusions and plans for future work.
2 Dependency Parsing with
Passive-Aggressive Learning
This section presents the modification of Passive-
Aggressive Learning (PA) (Crammer et al, 2006)
for dependency parsing. We modify the PA algo-
rithm to deal with structured prediction, in which
our problem is to learn a discriminant function that
maps an input sentence x to a dependency tree y.
Figure 1 shows an example of dependency parsing
which depicts the relation of each word to another
word within a sentence. There are some algorithms
1149
Figure 1: This is an example of dependency tree
to determine these relations of each word to another
words, for instance, the modified CKY algorithm
(Eisner, 1996) is used to define these relations for
a given sentence.
2.1 Parsing Algorithm
Dependency-tree parsing as the search for the maxi-
mum spanning tree (MST) in a graph was proposed
by McDonald et al (2005b). In this subsection,
we briefly describe the parsing algorithms based on
the first-order MST parsing. Due to the limitation
of participation time, we only applied the first-order
decoding parsing algorithm in CONLL-2007. How-
ever, our algorithm can be used for the second order
parsing.
Let the generic sentence be denoted by x ; the
ith word of x is denoted by wi. The generic de-
pendency tree is denoted by y. If y is a dependency
tree for sentence x, we write (i, j) ? y to indicate
that there is a directed edge from word xwi to word
xwj in the tree, that is, xwi is the parent of xwj .
T = {(xt, yt)}nt=1 denotes the training data. We fol-
low the edge based factorization method of Eisner
(Eisner, 1996) and define the score of a dependency
tree as the sum of the score of all edges in the tree,
s(x, y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? ?(i, j) (1)
where ?(i, j) is a high-dimensional binary fea-
ture representation of the edge from xwi to xwj .
For example in Figure 1, we can present an example
?(i, j) as follows;
?(i, j) =
{
1 if xwi =? hit? andxwj =? ball?
0 otherwise
The basic question must be answered for models
of this form: how to find the dependency tree y with
the highest score for sentence x? The two algorithms
we employed in our dependency parsing model are
the Eisner parsing (Eisner, 1996) and Chu-Liu?s al-
gorithm (Chu and Liu, 1965). The algorithms are
commonly used in other online-learning dependency
parsing, such as in (McDonald et al, 2005a).
In the next subsection we will address the problem
of how to estimate the weight wi associated with a
feature ?i in the training data using an online PA
learning algorithm.
2.2 Online PA Learning
This section presents a modification of PA algo-
rithm for structured prediction, and its use in de-
pendency parsing. The Perceptron style for natural
language processing problems as initially proposed
by (Collins, 2002) can provide state of the art re-
sults on various domains including text chunking,
syntactic parsing, etc. The main drawback of the
Perceptron style algorithm is that it does not have a
mechanism for attaining the maximize margin of the
training data. It may be difficult to obtain high accu-
racy in dealing with hard learning data. The struc-
tured support vector machine (Tsochantaridis et al,
2005) and the maximize margin model (Taskar et al,
2004) can gain a maximize margin value for given
training data by solving an optimization problem (i.e
quadratic programming). It is obvious that using
such an optimization algorithm requires much com-
putational time. For dependency parsing domain,
McDonald et al(2005a) modified the MIRA learn-
ing algorithm (McDonald et al, 2005a) for struc-
tured domains in which the optimization problem
can be solved by using Hidreth?s algorithm (Censor
and Zenios, 1997), which is faster than the quadratic
programming technique. In contrast to the previous
method, this paper presents an online algorithm for
dependency parsing in which we can attain the max-
imize margin of the training data without using opti-
mization techniques. It is thus much faster and eas-
ier to implement. The details of PA algorithm for
dependency parsing are presented below.
Assume that we are given a set of sentences xi
and their dependency trees yi where i = 1, ..., n. Let
the feature mapping between a sentence x and a tree
y be: ?(x, y) = ?1(x, y),?2(x, y), ...,?d(x, y)
where each feature mapping ?j maps (x, y) to a real
value. We assume that each feature ?(x, y) is asso-
1150
ciated with a weight value. The goal of PA learning
for dependency parsing is to obtain a parameter w
that minimizes the hinge-loss function and the mar-
gin of learning data.
Input:S = {(xi; yi), i = 1, 2, ..., n} in which1
xi is the sentence and yi is a dependency tree
Aggressive parameter C2
Output: the PA learning model3
Initialize: w1 = (0, 0, ..., 0)4
for t=1, 2... do5
Receive an sentence xt6
Predict y?t = argmaxy?Y (wt ? ?(xt, yt))7
Suffer loss: lt =8
wt ??(xt, y?t )?wt ??(xt, yt) +
??(yt, y?t )
Set:9
PA: ?t = lt||?(xt,y?t )??(xt,yt)||2
PA-I: ?t = min{C, lt||?(xt,y?t )??(xt,yt)||2 }
PA-II: ?t = lt||?(xt,y?t )??(xt,yt)||2+ 12C
Update:
wt+1 = wt + ?t(?(xt, yt)? ?(xt, y?t ))
end10
Algorithm 1: The Passive-Aggressive algo-
rithm for dependency parsing.
Algorithm 1 shows the PA learning algorithm for
dependency parsing in which its three variants are
different only in the update formulas. In Algorithm
1, we employ two kinds of argmax algorithms: The
first is the decoding algorithm for projective lan-
guage data and the second one is for non-projective
language data. Algorithm 1 shows (line 8) p(y, yt)
is a real-valued loss for the tree yt relative to the
correct tree y. We define the loss of a dependency
tree as the number of words which have an incorrect
parent. Thus, the largest loss a dependency tree can
have is the length of the sentence. The similar loss
function is designed for the dependency tree with la-
beled. Algorithm 1 returns an averaged weight vec-
tor: an auxiliary weight vector v is maintained that
accumulates the values of w after each iteration, and
the returned weight vector is the average of all the
weight vectors throughout training. Averaging has
been shown to help reduce overfitting (McDonald et
al., 2005a; Collins, 2002). It is easy to see that the
main difference between the PA algorithms and the
Perceptron algorithm (PC) (Collins, 2002) as well as
the MIRA algorithm (McDonald et al, 2005a) is in
line 9. As we can see in the PC algorithm, we do
not need the value ?t and in the MIRA algorithm we
need an optimization algorithm to compute ?t. We
also have three updated formulations for obtaining
?t in Line 9. In the scope of this paper, we only
focus on using the second update formulation (PA-I
method) for training dependency parsing data.
2.3 Feature Set
We denote p-word: word of parent node in depen-
dency tree. c-word: word of child node. p-pos: POS
of parent node. c-pos: POS of child node. p-pos+1:
POS to the right of parent in sentence. p-pos-1: POS
to the left of parent. c-pos+1: POS to the right of
child. c-pos-1: POS to the left of child. b-pos: POS
of a word in between parent and child nodes. The
p-word,p-pos
p-word
p-pos
c-word, c-pos
c-word
c-pos
Table 1: Feature Set 1: Basic Unit-gram features
p-word, p-pos, c-word, c-pos
p-pos, c-word, c-pos
p-word, c-word, c-pos
p-word, p-pos, c-pos
p-word, p-pos, c-word
p-word, c-word
p-pos, c-pos
Table 2: Feature Set 2: Basic bi-gram features
p-pos, b-pos, c-pos
p-pos, p-pos+1, c-pos-1, c-pos
p-pos-1, p-pos, c-pos-1, c-pos
p-pos, p-pos+1, c-pos, c-pos+1
p-pos-1, p-pos, c-pos, c-pos+1
Table 3: Feature Set 3: In Between POS Features
and Surrounding Word POS Features
features used in our system are described below.
? Tables 1 and 2 show our basic features. These
1151
features are added for entire words as well as
for the 5-gram prefix if the word is longer than
5 characters.
? In addition to these features shown in Table 1,
the morphological information for each pair of
words p-word and c-word are represented. In
addition, we also add the conjunction morpho-
logical information of p-word and c-word. We
do not use the LEMMA and CPOSTAG infor-
mation in our set features. The morphological
information are obtained from FEAT informa-
tion.
? Table 3 shows our Feature set 3 which take the
form of a POS trigram: the POS of the par-
ent, of the child, and of a word in between,
for all words linearly between the parent and
the child. This feature was particularly helpful
for nouns identifying their parent (McDonald
et al, 2005a).
? Table 3 also depicts these features taken the
form of a POS 4-gram: The POS of the par-
ent, child, word before/after parent and word
before/after child. The system also used back-
off features with various trigrams where one of
the local context POS tags was removed.
? All features are also conjoined with the direc-
tion of attachment, as well as the distance be-
tween the two words being attached.
3 Experimental Results and Discussion
We test our parsing models on the CONLL-2007
(Hajic? et al, 2004; Aduriz et al, 2003; Mart?? et
al., 2007; Chen et al, 2003; Bo?hmova? et al, 2003;
Marcus et al, 1993; Johansson and Nugues, 2007;
Prokopidis et al, 2005; Csendes et al, 2005; Mon-
temagni et al, 2003; Oflazer et al, 2003) data set on
various languages including Arabic, Basque, Cata-
lan, Chinese, English, Italian, Hungarian, and Turk-
ish. Each word is attached by POS tags for each sen-
tence in both the training and the testing data. Table
4 shows the number of training and testing sentences
for these languages. The table shows that the sen-
tence length in Arabic data is largest and its size of
training data is smallest. These factors might be af-
fected to the accuracy of our proposed algorithm as
we will discuss later.
The training and testing were conducted on a pen-
tium IV at 4.3 GHz. The detailed information about
the data are shown in the CONLL-2007 shared task.
We applied non-projective and projective parsing
along with PA learning for the data in CONLL-2007.
Table 5 reports experimental results by using the
first order decoding method in which an MST pars-
ing algorithm (McDonald et al, 2005b) is applied
for non-projective parsing and the Eisner?s method
is used for projective language data. In fact, in our
method we applied non-projective parsing for the
Italian data, the Turkish data, and the Greek data.
This was because we did not have enough time to
train all training data using both projective and non-
projective parsing. This is the problem of discrimi-
native learning methods when performing on a large
set of training data. In addition, to save time in train-
ing we set the number of best trees k to 1 and the
parameter C is set to 0.05.
Table 5 shows the comparison of the proposed
method with the average, and three top systems on
the CONLL-2007. As a result, our method yields
results above the average score on the CONLL-2007
shared task (Nivre et al, 2007).
Table 5 also indicates that the Basque results ob-
tained a lower score than other data. We obtained
69.11 UA score and 58.16 LA score, respectively.
These are far from the results of the Top3 scores
(81.13 and 75.49). We checked the outputs of the
Basque data to understand the main reason for the
errors. We see that the errors in our methods are
usually mismatched with the gold data at the labels
?ncmod? and ?ncsubj?. The main reason might be
that the application of projective parsing for this data
in both training and testing is not suitable. This was
because the number of sentences with at least 1 non
projective relation in the data is large (26.1).
The Arabic score is lower than the scores of other
data because of some difficulties in our method as
follows. Morphological and sentence length prob-
lems are the main factors which affect the accuracy
of parsing Arabic data. In addition, the training size
in the Arabic is also a problem for obtaining a good
result. Furthermore, since our tasks was focused on
improving the accuracy of English data, it might be
unsuitable for other languages. This is an imbalance
1152
Languages Training size Tokens size tokens-per-sent % of NPR % of-sentence AL-1-NPR
Arabic 2,900 112,000 38.3 0.4 10.1
Basque 3,200 51,000 15.8 2.9 26.2
Catalan 15,000 431,000 28.8 0.1 2.9
Chinese 57,000 337,000 5.9 0.0 0.0
Czech 25,400 432,000 17.0 1.9 23.2
English 18,600 447,000 24.0 0.3 6.7
Greek 2,700 65,000 24.2 1.1 20.3
Hungarian 6,000 132,000 21.8 2.9 26.4
Italian 3,100 71,000 22.9 0.5 7.4
Turkish 5,600 65,000 11.6 0.5 33.3
Table 4: The data used in the multilingual track (Nivre et al, 2007). NPR means non-projective-relations.
AL-1-NPR means at-least-least 1 non-projective relation.
problem in our method. Table 5 also shows the com-
parison of our system to the average score and the
Top3 scores. It depicts that our system is accurate
in English data, while it has low accuracy in Basque
and Arabic data.
We also evaluate our models in the domain adap-
tation tasks. This task is to adapt our model trained
on PennBank data to the test data in the Biomedical
domain. The pchemtb-closed shared task (Marcus
et al, 1993; Johansson and Nugues, 2007; Kulick
et al, 2004) is used to illustrate our models. We do
not use any additional unlabeled data in the Biomed-
ical domain. Only the training data in the PennBank
is used to train our model. Afterward, we selected
carefully a suitable parameter using the development
test set. We set the parameter C to 0.01 and se-
lect the non projective parsing for testing to obtain
the highest result in the development data after per-
forming several experiments. After that, the trained
model was used to test the data in Biomedical do-
main. The score (UA=82.04; LA=79.50) shows that
our method yields results above the average score
(UA=76.42; LA=73.03). In addition, it is officially
coming in 4th place out of 12 teams and within 1.5%
of the top systems.
The good result of performing our model in an-
other domain suggested that the PA learning seems
sensitive to noise. We hope that this problem is
solved in future work.
4 Conclusions
This paper presents an online algorithm for depen-
dency parsing problem which have tested on various
language data in CONLL-2007 shared task. The per-
formance in English data is close to the Top3 score.
We also perform our algorithm on the domain adap-
tation task, in which we only focus on the training of
the source data and select a suitable parameter using
the development set. The result is very good as it
is close to the Top3 score of participating systems.
Future work will also be focused on extending our
method to a version of using semi-supervised learn-
ing that can efficiently be learnt by using labeled and
unlabeled data. We hope that the application of the
PA algorithm to other NLP problems such as seman-
tic parsing will be explored in future work.
Acknowledgments
We would like to thank D. Yuret for his helps in
checking errors of my parser?s outputs. We would
like to thank Vinh-Van Nguyen his helps during the
revision process and Mary Ann Mooradian for cor-
recting the paper.
We would like to thank to anonymous review-
ers for helpful discussions and comments on the
manuscript. Thank also to Sebastian Riedel for
checking the issues raised in the reviews.
The work on this paper was supported by a Mon-
bukagakusho 21st COE Program.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
1153
Languages Unlabled Accuracy Labeled Accuracy NTeams
PA-I Average Top3 Top2 Top1 PA-I Average Top3 Top2 Top1
Arabic 73.46 78.84 84.21 85.81 86.09 68.34 74.75 83.0 75.08 76.52 20
Basque 69.11 75.15 81.13 81.93 81.13 58.16 68.06 75.49 75.73 76.92 20
Catalan 88.12 87.98 93.12 93.34 93.40 83.23 79.85 87.90 88.16 88.70 20
Chinese 84.05 81.98 87.91 88.88 88.94 79.77 76.59 83.51 83.84 84.69 21
Czech 80.91 77.56 84.19 85.16 86.28 72.54 70.12 77.98 78.60 80.19 20
English 88.01 82.67 89.87 90.13 90.63 86.73 80.95 88.41 89.01 89.61 23
Greek 77.56 77.78 81.37 82.04 84.08 70.42 70.22 74.42 74.65 76.31 20
Hungarian 78.13 76.34 82.49 83.51 83.55 68.12 71.49 78.09 79.53 80.27 21
Italian 80.40 82.45 87.68 87.77 87.91 75.06 78.06 78.09 79.53 80.27 20
Turkish 80.19 73.19 85.77 85.77 86.22 67.63 73.19 79.24 79.79 79.81 20
Multilingual-average 79.99 71.13 85.62 85.71 86.55 72.52 65.77 79.90 80.28 80.32 23
pchemtb-closed 82.04 76.42 83.08 83.38 83.42 79.50 73.03 80.22 80.40 81.06 8
Table 5: Dependency accuracy in the CONLL-2007 shared task.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
Y. Censor and S.A. Zenios. 1997. Parallel optimization:
theory, algorithms, and applications. In Oxford Uni-
versity Press.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
S. Corston-Oliver, A. Aue, K. Duh, , and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of HLT/NAACL.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, J. Keshet, S.Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:581?585.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING 1996, pages 340?345.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald, K. Cramer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
R. McDonald, K. Crammer, and F. Pereira. 2006. Multi-
lingual dependency parsing with a two-stage discrim-
inative parser. In Conference on Natural Language
Learning.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
1154
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
B. Taskar, D. Klein, M. Collins, D. Koller, and C.D. Man-
ning. 2004. Max-margin parsing. In proceedings of
EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for interde-
pendent and structured output spaces. In proceedings
ICML 2004.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
1155
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 182?185,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Building a Large Syntactically-Annotated Corpus of Vietnamese 
 
 
Phuong-Thai Nguyen 
College of Technology, VNU 
thainp@vnu.edu.vn 
Xuan-Luong Vu 
Vietnam Lexicography Cen-
tre 
vuluong@vietlex.vn 
Thi-Minh-Huyen Nguyen 
University of Natural 
Sciences, VNU 
huyenntm@vnu.edu.vn 
Van-Hiep Nguyen 
University of Social Sciences and 
Humanities, VNU 
hiepnv@vnu.edu.vn 
Hong-Phuong Le 
LORIA/INRIA Lorraine 
lehong@loria.fr 
 
 
Abstract 
Treebank is an important resource for 
both research and application of natural 
language processing. For Vietnamese, we 
still lack such kind of corpora. This paper 
presents up-to-date results of a project for 
Vietnamese treebank construction. Since 
Vietnamese is an isolating language and 
has no word delimiter, there are many 
ambiguities in sentence analysis. We sys-
tematically applied a lot of linguistic 
techniques to handle such ambiguities. 
Annotators are supported by automatic-
labeling tools and a tree-editor tool. Raw 
texts are extracted from Tuoi Tre 
(Youth), an online Vietnamese daily 
newspaper. The current annotation 
agreement is around 90 percent.  
1 Introduction 
Treebanks are used for training syntactic parsers, 
part-of-speech taggers, and word segmenters. 
These systems then can be used for applications 
such as information extraction, machine transla-
tion, question answering, and text summariza-
tion. Treebanks are also useful for linguistic stu-
dies, for example the extraction of syntactic pat-
terns or the investigation of linguistic phenome-
na. Recently, treebanks and other large corpora 
have become more important since the develop-
ment of powerful machine learning methods. 
As mentioned above, Vietnamese is an isolat-
ing language. There is no word delimiter in Viet-
namese. The smallest unit in the construction of 
words is syllables. Words can be single or com-
pound. Vietnamese script is invented based on 
Latin alphabet in which the expansion includes 
accent characters and stressed accents.  
Since Vietnamese word order is quite fixed, 
we choose to use constituency representation of 
syntactic structures. For languages with freer 
word order such as Japanese or Czech, depen-
dency representation is more suitable. We apply 
annotation scheme proposed by Marcus et al 
(1993). This approach has been successfully ap-
plied to a number of languages such as English, 
Chinese, Arabic, etc. 
For Vietnamese, there are three annotation le-
vels including word segmentation, POS tagging, 
and syntactic labeling. Word segmentation iden-
tifies word boundary in sentences. POS tagging 
assigns correct POS tags to words. Syntactic 
labeling recognizes both phrase-structure tags 
and functional tags. Our main target is to build a 
corpus of 10,000 syntactically-annotated sen-
tences (trees) and an additional POS tagged data 
set of 10,000 sentences. Treebank construction is 
a very complicated task including major phases: 
investigation, guideline preparation, building 
tools, raw text collection, and annotation. This is 
a repeated process involving especially three 
phases: annotation, guideline revision, and tool 
upgrade. Raw texts are collected from a newspa-
per source, the Youth online daily newspaper, 
with a number of topics including social and pol-
itics. We completed about 9,500 trees and 10,000 
POS tagged sentences. 
In order to deal with ambiguities occurring at 
various levels of annotation, we systematically 
applied linguistic analysis techniques such as 
deletion, insertion, substitution, questioning, 
transformation, etc. Notions for analysis tech-
niques are described in guideline. These tech-
niques are originated in literatures or proposed 
182
by our group. They are described with examples, 
arguments, and alternatives. For automatic labe-
ling tools, we used advanced machine learning 
methods such as CRFs for POS tagging or 
LPCFGs for syntactic parsing. These tools 
helped us speed up labeling process. Besides, 
tree editor was also very helpful. 
Our treebank project is a branch project of a 
national project which aims to develop basic re-
sources and tools for Vietnamese language and 
speech processing. This national project is called 
VLSP 1 . In addition to treebank, other text-
processing resources and tools include: Vietnam-
ese machine readable dictionary, English-
Vietnamese parallel corpus, word segmenter, 
POS tagger, chunker, and parser. Treebank and 
tools are closely related. Tools are trained using 
treebank data, and then they can be used in tree-
bank construction. 
The rest of this paper is organized as follow: 
First, we present issues in Vietnamese word 
segmentation problem. Second, POS tagging and 
syntactic parsing are described. Third, tools and 
annotation process are represented. Fourth, we 
present annotation agreement evaluation. And 
last, some conclusion is drawn.   
2 Word Segmentation 
There are many approaches to word definition, 
for example based on morphology, based on syn-
tax, based on semantics, or linguistic compari-
son. We consider words as syntactic atoms 
(Sciullo and Williams, 1987) according to the 
sense that it is impossible to analyze word struc-
ture using syntactic rules, or that words are the 
smallest unit which is syntactically independent. 
We choose this criterion partly because the first 
application of word segmentation is for syntactic 
analysis (build trees).  
According to application view, machine trans-
lation researchers may argue that Vietnamese 
words and foreign words should match each oth-
er. The problem is that there are so many possi-
ble foreign languages which are different in vo-
cabulary. Dictionary editors may want to extract 
phrases from text which need to be explained in 
meaning. For this application, syntactic parsers 
can be used as tool for editors. Parsers can ex-
tract candidates for phrase/word entry. 
The following word types are considered in 
word segmentation phase: single words, com-
pound words, repeated words, idioms, proper 
                                               
1 Vietnamese Language and Speech Processing 
names, date/time, number expressions, foreign 
words, abbreviations. 
Word segmentation ambiguity is the major 
problem annotators have to deal with. Suppose 
that three words ?nh? c?a?, ?s?c ??p?, and ?hi?u 
s?ch? are being considered. Annotators need to 
identify these combinations as words in: 
a. Nh? c?a b? b?n qu?  
b. C? ?y gi? g?n s?c ??p.  
c. Ngo?i hi?u s?ch c? b?n cu?n n?y  
And not words in: 
a. ? nh? c?a ng? ch?ng ??ng g? c?.  
b. B?c n?y m?u s?c ??p h?n.  
c. Ngo?i c?a hi?u s?ch b?o b?y la li?t.  
We used dictionaries as a reference. In prac-
tice, we consider dictionary words as candidate 
for word segmentation and make decision using 
context. 
3 POS Tagging and Syntactic Annota-
tion Guidelines 
3.1 POS Tag Set 
For European languages, word classes closely 
relate to morphological aspects such as gender, 
number, case, etc. For Vietnamese, words are 
often classified based on their combination abili-
ty, their syntactic functions, and their general 
meaning. We choose first two criteria, combina-
tion ability and syntactic function, for POS tag 
set design. Therefore our POS tag set will not 
contain morphological information (number, as-
pect, tense, etc.), sub-categorization information 
(transitive/intransitive verbs, verbs followed by 
clauses, etc.), and semantic information.  
3.2 Syntactic Tag Set  
Our tag set contains three tag types: constituency 
tags, functional tags, and null-element tags. We 
use the tag H to label phrase head. If a phrase has 
more than one head, connected by coordination 
conjunctions or commas, then all heads are la-
beled with H tag. Other treebanks often does not 
use head tag. Therefore researchers on syntactic 
parsing (Collins, 1999) used heuristic rules to 
determine CFG rules? head. Machine learning 
methods also can be used (Chiang and Bikel, 
2002). Null elements are often used for adjective 
clauses, ellipsis, passive voice, and topic. 
3.3 Sentence and Phrase Analysis Tech-
niques 
Annotation of real text requires various tech-
niques to be applied. Ambiguity may occur in 
many steps of analysis such as determining 
183
phrase?s head, discriminating between possible 
complements, discriminating between adjuncts 
and other sentence elements, etc. Sentence analy-
sis techniques include deletion, substitution, in-
sertion, transformation, questioning. These tech-
niques exploit contextual information, word 
combination, word order, and functional words 
to disambiguation between possible structures.  
3.4 Linguistics Issues 
The problem of treebank construction can be 
considered as an application of linguistic theories 
though treebanks can also be used for linguistic 
studies. However, there are still disagreements 
among linguists as to solutions for many linguis-
tic issues. For example, that the classifier noun is 
noun phrase?s head or pre-modifier is controver-
sial. Another example, Vietnamese sentence 
structure is subject-predicate or topic-comment is 
also controversial. Our treebank relies more on 
subject-predicate structure. Moreover, we choose 
linguistic solutions most appropriate to our de-
sign.  
4 Tools 
We designed a tool for supporting annotators in 
most all phases of the annotation process. Main 
functions of our editor are as follows: 
- Edit and view trees in both text mode and 
graphical mode 
- View log files, highlight modifications 
- Search by words or syntactic patterns 
- Predict errors (edit, spell, or syntax) 
- Compute annotation agreement and high-
light differences 
- Compute several kinds of statistics  
For encoding the treebank, we have developed 
an exchange format named vnSynAF, a syntactic 
annotation framework which is conformed to the 
standard framework SynAF of ISO. The frame-
work SynAF is built on top of an XML-based 
annotation scheme which is recommended by 
ISO for the encoding of treebanks2. Our tool also 
supports bracketing representation (or Lisp style) 
of Penn English Treebank. These formats can be 
converted into each other.   
For the task of word segmentation, we used 
vnTokenizer, a highly accurate segmenter which 
uses a hybrid approach to automatically tokenize 
Vietnamese text. The approach combines both 
finite-state automata technique, regular expres-
                                               
2 ISO/CD/24615, Language Resource Management- 
Syntactic Annotation Framework (SynAF) TC37/SC 4 
N421, 22th Aug 2007, http://tc37sc4.org/documents 
sion parsing, and the maximal-matching strategy 
which is augmented by statistical methods to re-
solve ambiguities of segmentation (Phuong et al, 
2008). 
We used JVnTagger, a POS tagger based on 
Conditional Random Fields (Lafferty et al, 
2001) and Maximum Entropy (Berger et al, 
1996). This tagger is also developed under sup-
ported of VLSP project. Training data size is 
10,000 sentences. Experiments with 5-fold cross 
validation showed that F1 scores for CRFs and 
Maxent are 90.40% and 91.03% respectively.   
A syntactic parser based on Lexicalized Prob-
abilistic Context-free Grammars (LPCFGs) is 
another tool we used. Another group in VLSP 
customized Bikel?s parser3 for parsing Vietnam-
ese text. This parser is a well designed and easy 
to adapt to new languages. The group imple-
mented a Vietnamese language package which 
handles treebank, training, finding head of CFG 
rules, and word features. This parser can output 
text with constituent tags only or both constituent 
tags and functional tags. 
5 Annotation Process and Agreement 
There are three annotation levels: word segmen-
tation, POS tagging, and syntactic labeling. Since 
the word segmentation tool had been available 
before the start of our project, it was used for the 
first annotation level (word segmentation) im-
mediately. As to the other annotation levels (POS 
tagging and syntactic parsing), first several thou-
sand sentences were labeled manually. After that 
a POS tagger and a parser are trained bimonthly, 
then the annotation task becomes semi-
automatic. According to our annotation process, 
each sentence is annotated and revised by at least 
two annotators. The first annotator labels raw 
sentences or revises automatically-analyzed sen-
tences. Then the second annotator revises the 
output of the first annotator. In addition, we also 
check corpus by syntactic phenomena, for exam-
ple direction words, questions, etc. This process 
is supported by tool. So there are many sentences 
which are revised more than twice.  
Table 2 shows a number of important corpus 
statistics such as sentence count, word count, and 
syllable count for two data sets. We completed 
the POS tagged data set and will complete the 
syntactically-labeled data set soon. The average 
sentence length is about 21.6 words.  
 
                                               
3 http://www.cis.upenn.edu/~dbikel/software.html 
184
Data set Sentences Words Syllables 
POS tagged 10,368 210,393 255,237 
Syntactically 
labeled 
9,633 208,406 251,696 
Table 1. Corpus statistics 
Annotation agreement measures how similar 
two texts annotated independently by different 
annotators are. Since this problem is similar to 
parsing evaluation, we use parseval measure. 
First, syntactic constituents in the form (i, j, la-
bel) are extracted from syntactic trees. Then tree 
comparison problem is transformed into consti-
tuent comparison. We can compute three kinds 
of measurement: constituent and function simi-
larity, constituent similarity, and bracket simi-
larity. By using this method, we can evaluate 
both overall agreement and constituency agree-
ment.      
Annotation agreement A between two annota-
tors can be computed as follows: 
21
2
CC
C
A
?
?
?  
where C1 is the number of constituents in the 
first annotator?s data set, C2 is the number of 
constituents in the second annotator?s data set, 
and C is the number of identical constituents. 
Table 3 shows an example of constituent extrac-
tion from trees. From Table 3, we can compute: 
C1=6; C2=7; C=6; A=12/13=0.92 . 
 
1st annotator 2nd annotator 
(S (NP (Np H?ng)) 
     (VP (V ng?m)  
            (NP (N m?a))  
            (PP (E trong)  
                   (NP (N c?ng 
vi?n)))) 
     (. .)) 
(S (NP (Np H?ng)) 
     (VP (V ng?m)  
            (NP (NP (N m?a))  
                   (PP (E trong)  
                          (NP (N 
c?ng vi?n))))) 
    (. .)) 
(1,6,S); (1,1,NP); (2,5,VP); 
(3,3,NP); (4,5, PP); (5,5,NP) 
(1,6,S); (1,1,NP); (2,5,VP); 
(3,3,NP); (3,5,NP); (4,5, 
PP); (5,5,NP) 
Table 2. Constituent extraction from trees 
 
We carried out an experiment involving 3 an-
notators. They annotated 100 sentences and the 
result is shown in Table 4.  
 
Test A1-A2 A2-A3 A3-A1 
Full tags 90.32% 91.26% 90.71% 
Constituent 
tags 
92.40% 93.57% 91.92% 
No tags 95.24% 96.33% 95.48% 
Table 3. Annotation agreement 
 
6 Conclusions 
In this paper, we presented our most up-to-date 
results on Vietnamese treebank construction. 
This project is coming to final stage. We contin-
ue to annotate more text, revise data by syntactic 
phenomenon and feedback from users. We also 
use statistical techniques to analyze treebank data 
to find out errors and fix them. We intend to pub-
lish these data on LDC this year.  
 
Acknowledgments 
This paper is supported by a national project 
named Building Basic Resources and Tools for 
Vietnamese Language and Speech Processing, 
KC01.01/06-10.  
Reference 
Di?p Quang Ban. 2005. Ng? ph?p ti?ng Vi?t (2 t?p). 
NXB Gi?o d?c. 
Cao Xu?n H?o. 2006. Ti?ng Vi?t s? th?o ng? ph?p 
ch?c n?ng. NXB Khoa h?c X? h?i. 
Nguy?n Minh Thuy?t v? Nguy?n V?n Hi?p. 1999. 
Th?nh ph?n c?u ti?ng Vi?t. NXB ?HQG H? N?i. 
?y ban Khoa h?c X? h?i Vi?t Nam. 1983. Ng? ph?p 
ti?ng Vi?t. NXB Khoa h?c X? h?i. 
Adam Berger, Stephen D. Pietra, and Vincent D. Pie-
tra. 1996. A maximum entropy approach to natural 
language processing. Computational Linguistics, 
(22-1). 
David Chiang and Daniel M. Bikel. 2002. Recovering 
Latent Information in Treebanks. COLING. 
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. PhD thesis, Uni-
versity of Pennsylvania. 
John Lafferty, Andrew McCallum, and Fernando Pe-
reira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence 
data. ICML. 
Mitchell P. Marcus et al Building a Large Annotated 
Corpus of English: The Penn Treebank. 
1993. Computational Linguistics. 
L. H. Phuong, N. T. M. Huyen, R. Azim, H. T. Vinh. 
A hybrid approach to word segmentation of Viet-
namese texts. Proceedings of the 2nd International 
Conference on Language and Automata Theory 
and Applications, Springer LNCS 5196, Tarragona, 
Spain, 2008. 
Anna M.D. Sciullo and Edwin Williams. 1987. On the 
definition of word. The MIT Press.  
Fei Xia et al Developing Guidelines and Ensuring 
Consistency for Chinese Text Annotation. 2000. 
COLING.  
185
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 9?16,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
An Empirical Study of Vietnamese Noun Phrase Chunking with
Discriminative Sequence Models
Le Minh Nguyen
School of Information Science, JAIST
nguyenml@jaist.ac.jp
Huong Thao Nguyen and Phuong Thai Nguyen
College of Technology, VNU
{thaonth, thainp}@vnu.edu.vn
Tu Bao Ho and Akira Shimazu
Japan Advanced Institute of Science and Technology
{bao,shimazu}@jaist.ac.jp
Abstract
This paper presents an empirical work
for Vietnamese NP chunking task. We
show how to build an annotation corpus of
NP chunking and how discriminative se-
quence models are trained using the cor-
pus. Experiment results using 5 fold cross
validation test show that discriminative se-
quence learning are well suitable for Viet-
namese chunking. In addition, by em-
pirical experiments we show that the part
of speech information contribute signifi-
cantly to the performance of there learning
models.
1 Introduction
Many Natural Language Processing applications
(i.e machine translation) require syntactic infor-
mation and tools for syntactic analysis. However,
these linguistic resources are only available for
some languages(i.e English, Japanese, Chines). In
the case of Vietnamese, currently most researchers
have focused on word segmentation and part of
speech tagging. For example, Nghiem et al
(Nghiem, Dinh, Nguyen, 2008) has developed a
Vietnamese POS tagging. Tu (Tu, Phan, Nguyen,
Ha, 2006) (Nguyen, Romary, Rossignol, Vu,
2006)(Dien, Thuy, 2006) have developed Viet-
namese word segmentation.
The processing of building tools and annotated
data for other fundamental tasks such as chunk-
ing and syntactic parsing are currently developed.
This can be viewed as a bottleneck for develop-
ing NLP applications that require a deeper under-
standing of the language. The requirement of de-
veloping such tools motives us to develop a Viet-
namese chunking tool. For this goal, we have
been looking for an annotation corpus for conduct-
ing a Vietnamese chunking using machine learn-
ing methods. Unfortunately, at the moment, there
is still no common standard annotated corpus for
evaluation and comparison regarding Vietnamese
chunking.
In this paper, we aim at discussing on how
we can build annotated data for Vietnamese text
chunking and how to apply discriminative se-
quence learning for Vietnamese text chunking. We
choose discriminative sequence models for Viet-
namese text chunking because they have shown
very suitable methods for several languages(i.e
English, Japanese, Chinese) (Sha and Pereira,
2005)(Chen, Zhang, and Ishihara, 2006) (Kudo
and Matsumoto, 2001). These presentative dis-
criminative models which we choose for conduct-
ing empirical experiments including: Conditional
Random Fields (Lafferty, McCallum, and Pereira,
2001), Support Vector Machine (Vapnik, 1995)
and Online Prediction (Crammer et al 2006). In
other words, because Noun Phrase chunks appear
most frequently in sentences. So, in this paper
we focus mainly on empirical experiments for the
tasks of Vietnamese NP chunking.
We plan to answer several major questions by
using empirical experiments as follows.
? Whether or not the discriminative learning
models are suitable for Vietnamese chunking
problem?
? We want to know the difference of SVM,
Online Learning, and Conditional Random
Fields for Vietnamese chunking task.
? Which features are suitable for discriminative
learning models and how they contribute to
the performance of Vietnamese text chunk-
ing?
The rest of this paper is organized as follows:
Section 2 describes Vietnamese text chunking with
discriminative sequence learning models. Section
3 shows experimental results and Section 4 dis-
9
cusses the advantage of our method and describes
future work.
2 Vietnamese NP Chunking with
Discriminative Sequence Learning
Noun Phrase chunking is considered as the task
of grouping a consecutive sequence of words into
a NP chunk lablel. For example: ?[NP Anh Ay
(He)] [VP thich(likes)] [NP mot chiec oto(a car)]
?
Before describing NP chunking tasks, we
summarize the characteristic of Vietnamese lan-
guage and the background of Conditional Ran-
dom Fields, Support Vector Machine, and Online
Learning. Then, we present how to build the an-
notated corpus for the NP chunking task.
2.1 The characteristic of Vietnamese Words
Vietnamese syllables are elementary units that
have one way of pronunciation. In documents,
they are usually delimited by white-space. Be-
ing the elementary units, Vietnamese syllables are
not undivided elements but a structure. Generally,
each Vietnamese syllable has all five parts: first
consonant, secondary vowel, main vowel, last con-
sonant and a tone mark. For instance, the sylla-
ble tu.n (week) has a tone mark (grave accent), a
first consonant (t), a secondary vowel (u), a main
vowel () and a last consonant (n). However, except
for main vowel that is required for all syllables,
the other parts may be not present in some cases.
For example, the syllable anh (brother) has no tone
mark, no secondary vowel and no first consonant.
In other case, the syllable hoa (flower) has a sec-
ondary vowel (o) but no last consonant.
Words in Vietnamese are made of one or more
syllables which are combined in different ways.
Based on the way of constructing words from syl-
lables, we can classify them into three categories:
single words, complex words and reduplicative
words (Mai,Vu, Hoang, 1997).
The past of speechs (Pos) of each word in Viet-
namese are mainly sketched as follows.
A Noun Phrase (NP) in Vietnamese consists of
three main parts as follows: the noun center, the
prefix part, and the post fix part. The prefix and
postfix are used to support the meaning of the NP.
For example in the NP ?ba sinh vien nay?, the
noun center is ?sinh vien?, and the prefix is ?ba
(three)?, the postfix is ?nay?.
Vietnamese Tag Equivalent to English Tag
CC Coordinating conjunction)
CD Cardinal number)
DT Determiner)
V Verb
P Preposition
A Adjective
LS List item marker
MD Modal
N Noun
Table 1: Part of Speeches in Vietnamese
2.2 The Corpus
We have collected more than 9,000 sentences from
several web-sites through the internet. After that,
we then applied the segmentation tool (Tu, Phan,
Nguyen, Ha, 2006) to segment each sentences
into a sequence of tokens. Each sequence of
tokens are then represented using the format of
CONLL 2000. The details are sketched as follows.
Each line in the annotated data consists of
three columns: the token (a word or a punc-
tuation mark), the part-of-speech tag of the to-
ken, and the phrase type label (label for short)
of the token. The label of each token indicates
whether the token is outside a phrase (O), starts
a phrase (B-?PhraseType?), or continues a phrase
(I-?PhraseType?).
In order to save time for building annotated
data, we made a set of simple rules for automat-
ically generating the chunking data as follows. If
a word is not a ?noun?, ?adjective?, or ?article? it
should be assigned the label ?O?. The consecu-
tive words are NP if they is one of type as follows:
?noun noun?; ?article noun?, ?article noun adjec-
tive?. After generating such as data, we ask an
expert about Vietnamese linguistic to correct the
data. Finally, we got more than 9,000 sentences
which are annotated with NP chunking labels.
Figure 1 shows an example of the Vietnamese
chunking corpus.
2.3 Discriminative Sequence Learning
In this section, we briefly introduce three dis-
criminative sequence learning models for chunk-
ing problems.
2.3.1 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty,
McCallum, and Pereira, 2001) are undirected
graphical models used to calculate the conditional
10
Figure 1: An Example of the Vietnamese chunk-
ing corpus
probability of values on designated output nodes,
given values assigned to other designated input
nodes for data sequences. CRFs make a first-order
Markov independence assumption among output
nodes, and thus correspond to finite state machine
(FSMs).
Let o = (o1, o2, . . . , oT ) be some observed in-
put data sequence, such as a sequence of words in
a text (values on T input nodes of the graphical
model). Let S be a finite set of FSM states, each is
associated with a label l such as a clause start po-
sition. Let s = (s1, s2, . . . , sT ) be some sequences
of states (values on T output nodes). CRFs de-
fine the conditional probability of a state sequence
given an input sequence to be
P?(s|o) = 1Zo exp
( T?
t=1
F (s, o, t)
)
(1)
where Zo =
?
s exp
(?T
t=1 F (s, o, t)
)
is a nor-
malization factor over all state sequences. We de-
note ? to be the Kronecker-?. Let F (s, o, t) be the
sum of CRFs features at time position t:
?
i
?ifi(st?1, st, t) +
?
j
?jgj(o, st, t) (2)
where fi(st?1, st, t) = ?(st?1, l?)?(st, l) is a
transition feature function which represents se-
quential dependencies by combining the label l?
of the previous state st?1 and the label l of the
current state st, such as the previous label l? =
AV (adverb) and the current label l = JJ (adjec-
tive). gj(o, st, t) = ?(st, l)xk(o, t) is a per-state
feature function which combines the label l of cur-
rent state st and a context predicate, i.e., the binary
function xk(o, t) that captures a particular prop-
erty of the observation sequence o at time position
t. For instance, the current label is JJ and the cur-
rent word is ?conditional?.
Training CRFs is commonly performed by max-
imizing the likelihood function with respect to
the training data using advanced convex optimiza-
tion techniques like L-BFGS. Recently, there are
several works apply Stochastic Gradient Descent
(SGD) for training CRFs models. SGD has been
historically associated with back-propagation al-
gorithms in multilayer neural networks.
And inference in CRFs, i.e., searching the most
likely output label sequence of an input observa-
tion sequence, can be done using Viterbi algo-
rithm.
2.3.2 Support Vector Machines
Support vector machine (SVM)(Vapnik, 1995)
is a technique of machine learning based on sta-
tistical learning theory. The main idea behind
this method can be summarized as follows. Sup-
pose that we are given l training examples (xi, yi),
(1 ? i ? l), where xi is a feature vector in n di-
mensional feature space, and yi is the class label
{-1, +1 } of xi.
SVM finds a hyperplane w.x+b = 0 which cor-
rectly separates training examples and has maxi-
mum margin which is the distance between two
hyperplanes w ? x + b ? 1 and w ? x + b ? ?1.
Finally, the optimal hyperplane is formulated as
follows:
f(x) = sign
( l?
1
?iyiK(xi, x) + b
)
(3)
where ?i is the Lagrange multiple, and K(x?, x??)
is called a kernel function, which calculates sim-
ilarity between two arguments x? and x??. For in-
stance, the Polynomial kernel function is formu-
lated as follows:
K(x?, x??) = (x? ? x??)p (4)
SVMs estimate the label of an unknown example
x whether the sign of f(x) is positive or not.
Basically, SVMs are binary classifier, thus we
must extend SVMs to multi-class classifier in or-
11
der to classify three or more classes. The pair-
wise classifier is one of the most popular meth-
ods to extend the binary classification task to that
of K classes. Though, we leave the details to
(Kudo and Matsumoto, 2001), the idea of pairwise
classification is to build K.(K-1)/2 classifiers con-
sidering all pairs of classes, and final decision is
given by their weighted voting. The implementa-
tion of Vietnamese text chunking is based on Yam-
cha (V0.33)1.
2.3.3 Online Passive-Aggressive Learning
Online Passive-Aggressive Learning (PA) was
proposed by Crammer (Crammer et al 2006) as
an alternative learning algorithm to the maximize
margin algorithm. The Perceptron style for nat-
ural language processing problems as initially pro-
posed by (Collins, 2002) can provide to state of
the art results on various domains including text
segmentation, syntactic parsing, and dependency
parsing. The main drawback of the Perceptron
style algorithm is that it does not have a mech-
anism for attaining the maximize margin of the
training data. It may be difficult to obtain high
accuracy in dealing with hard learning data. The
online algorithm for chunking parsing in which
we can attain the maximize margin of the training
data without using an optimization technique. It
is thus much faster and easier to implement. The
details of PA algorithm for chunking parsing are
presented as follows.
Assume that we are given a set of sentences
xi and their chunks yi where i = 1, ..., n. Let
the feature mapping between a sentence x and
a sequence of chunk labels y be: ?(x, y) =
?1(x, y),?2(x, y), ...,?d(x, y) where each fea-
ture mapping ?j maps (x, y) to a real value. We
assume that each feature ?(x, y) is associated with
a weight value. The goal of PA learning for chunk-
ing parsing is to obtain a parameter w that min-
imizes the hinge-loss function and the margin of
learning data.
Algorithm 1 shows briefly the Online Learning
for chunking problem. The detail about this al-
gorithm can be referred to the work of (Crammer
et al 2006). In Line 7, the argmax value is com-
puted by using the Viterbi algorithm which is sim-
ilar to the one described in (Collins, 2002). Algo-
rithm 1 is terminated after T round.
1Yamcha is available at
http://chasen.org/ taku/software/yamcha/
Input: S = (xi; yi), i = 1, 2, ..., n in which1
xi is the sentence and yi is a sequence of
chunks
Aggressive parameter C2
Output: the model3
Initialize: w1 = (0, 0, ..., 0)4
for t=1, 2... do5
Receive an sentence xt6
Predict y?t = argmaxy?Y (wt.?(xt, yt))7
Suffer loss: lt =
wt.?(xt, y?t )? wt.?(xt, yt) +
??(yt, y?t )
Set:?t = lt||?(xt,y?t )??(xt,yt)||28
Update:9
wt+1 = wt + ?t(?(xt, yt)? ?(xt, y?t ))
end10
Algorithm 1: The Passive-Aggressive algo-
rithm for NP chunking.
2.3.4 Feature Set
Feature set is designed through features template
which is shown in Table 2. All edge features obey
the first-order Markov dependency that the label
(l) of the current state depends on the label (l?)
of the previous state (e.g., ?l = I-NP? and ?l? =
B-NP?). Each observation feature expresses how
much influence a statistic (x(o, i)) observed sur-
rounding the current position i has on the label
(l) of the current state. A statistic captures a par-
ticular property of the observation sequence. For
instance, the observation feature ?l = I-NP? and
?word?1 is the? indicates that the label of the cur-
rent state should be I-NP (i.e., continue a noun
phrase) if the previous word is the. Table 2 de-
scribes both edge and observation feature tem-
plates. Statistics for observation features are iden-
tities of words, POS tags surrounding the current
position, such as words and POS tags at ?2, ?1,
1, 2.
We also employ 2-order conjunctions of the cur-
rent word with the previous (w?1w0) or the next
word (w0w1), and 2-order and 3-order conjunc-
tions of two or three consecutive POS tags within
the current window to make use of the mutual de-
pendencies among singleton properties. With the
feature templates shown in Table 2 and the feature
rare threshold of 1 (i.e., only features with occur-
rence frequency larger than 1 are included into the
discriminative models)
12
Edge feature templates
Current state: si Previous state: si?1
l l?
Observation feature templates
Current state: si Statistic (or context predicate) templates: x(o, i)
l w?2; w?1; w0; w1; w2; w?1w0; w0w1;
t?2; t?1; t0; t1; t2;
t?2t?1; t?1t0; t0t1; t1t2; t?2t?1t0;
t?1t0t1; t0t1t2
Table 2: Feature templates for phrase chunking
3 Experimental Results
We evaluate the performance of using several se-
quence learning models for the Vietnamese NP
chunking problem. The data of more than 9,000
sentences is evaluated using an empirical experi-
ment with 5 fold cross validation test. It means
we used 1,800 and 7,200 sentences for testing
and training the discriminative sequence learning
models, respectively. Note that the evaluation
method is used the same as CONLL2000 did. We
used Precision, Recall, and F-Measure in which
Precision measures how many chunks found by
the algorithm are correct and the recall is per-
centage of chunks defined in the corpus that were
found by the chunking program.
Precision = #correct?chunk#numberofchunks
Recall = #correct?chunks#numerofchunksinthecorpus
F?measure =2? Precision? RecallPrecision + Recall
To compute the scores in our experiments, we
utilized the evaluation tool (conlleval.pl) which is
available in CONLL 2000 (Sang and Buchholz,
2000, ).
Figure 2 shows the precision scores of three
methods using 5 Folds cross validation test. It
reports that the CRF-LBFGS attain the highest
score. The SVMs and CRF-SGD are comparable
to CRF-LBFGS. The Online Learning achieved
the lowest score.
Figure 3 shows the recall scores of three CRFs-
LBFGS, CRFs-SGD, SVM, and Online Learning.
The results show that CRFs-SGD achieved the
highest score while the Online Learning obtained
the lowest score in comparison with others.
Figure 4 and Figure 5 show the F-measure and
accuracy scores using 5 Folds Cross-validation
Figure 2: Precision results in 5 Fold cross valida-
tion test
Test. Similar to these results of Precision and Re-
call, CRFs-LBFGS was superior to the other ones
while the Online Learning method obtained the
lowest result.
Table 3 shows the comparison of three discrim-
inative learning methods for Vietnamese Noun
Phrase chunking. We compared the three se-
quence learning methods including: CRFs using
the LBFGS method, CRFs with SGD, and On-
line Learning. Experiment results show that the
CRFs-LBFGS is the best in comparison with oth-
ers. However, the computational times when train-
ing the data is slower than either SGD or Online
Learning. The SGD is faster than CRF-LBFS ap-
proximately 6 times. The SVM model obtained a
comparable results with CRFs models and it was
superior to Online Learning. It yields results that
were 0.712% than Online Learning. However, the
SVM?s training process take slower than CRFs
and Online Learning. According to our empirical
investigation, it takes approximately slower than
CRF-SGF, CRF-LBFGS as well as Online Learn-
ing.
13
Figure 3: Recall result in 5 Fold cross validation
test
Figure 4: The F-measure results of 5 Folds Cross-
validation Test
Note that we used FlexCRFs (Phan, Nguyen,
Tu , 2005) for Conditional Random Fields us-
ing LBFGS, and for Stochastic Gradient Descent
(SGD) we used SGD1.3 which is developed by
Leon Bottou 2.
Methods Precision Recall F1
CRF-LBGS 80.85 81.034 80.86
CRF-SGD 80.74 80.66 80.58
Online-PA 80.034 80.13 79.89
SVM 80.412 80.982 80.638
Table 3: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, SVM, Online-PA)
In order to investigate which features are ma-
jor effect on the discriminative learning models for
Vietnamese Chunking problems, we conduct three
experiments as follows.
2http://leon.bottou.org/projects/sgd
Figure 5: The accuracy scores of four methods
with 5 Folds Cross-validation Test
? Cross validation test for three modes without
considering the edge features
? Cross validation test for three models without
using POS features
? Cross validation test for three models without
using lexical features
? Cross validation test for three models without
using ?edge features template? features
Note that the computational time of training
SVMs model is slow, so we skip considering fea-
ture selection for SVMs. We only consider feature
selection for CRFs and Online Learning.
Feature Set LBFGS SGD Online
Full-Features 80.86 80.58 79.89
Without-Edge 80.91 78.66 80.13
Without-Pos 62.264 62.626 59.572
Without-Lex 77.204 77.712 75.576
Table 4: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, Online-PA)
Table 4 shows that the Edge features have an
impact to the CRF-SGD model while it do not
affect to the performance of CRFs-LBFGS and
Online-PA learning. Table 4 also indicates that
the POS features are severed as important features
regarding to the performance of all discrimina-
tive sequence learning models. As we can see,
if one do not use POS features the F1-score of
each model is decreased more than 20%. We also
remark that the lexical features contribute an im-
portant role to the performance of Vietnamese text
14
Figure 6: F-measures of three methods with different feature set
chunking. If we do not use lexical features the
F1-score of each model is decreased till approxi-
mately 3%. In conclusion, the POS features signif-
icantly effect on the performance of the discrimi-
native sequence models. This is similar to the note
of (Chen, Zhang, and Ishihara, 2006).
Figure 6 reports the F-Measures of using dif-
ferent feature set for each discriminative models.
Note that WPos, WLex, and WEdge mean without
using Pos features, without using lexical features,
and without using edge features, respectively. As
we can see, the CRF-LBFGs always achieved the
best scores in comparison with the other ones and
the Online Learning achieved the lowest scores.
4 Conclusions
In this paper, we report an investigation of devel-
oping a Vietnamese Chunking tool. We have con-
structed an annotation corpus of more than 9,000
sentences and exploiting discriminative learning
models for the NP chunking task. Experimen-
tal results using 5 Folds cross-validation test have
showed that the discriminative models are well
suitable for Vietnamese phrase chunking. Con-
ditional random fields show a better performance
in comparison with other methods. The part of
speech features are known as the most influence
features regarding to the performances of discrim-
inative models on Vietnamese phrases chunking.
What our contribution is expected to be useful
for the development of Vietnamese Natural Lan-
guage Processing. Our results and corpus can be
severed as a very good baseline for Natural Lan-
guage Processing community to develop the Viet-
namese chunking task.
There are still room for improving the perfor-
mance of Vietnamese chunking models. For ex-
ample, more attention on features selection is nec-
essary. We would like to solve this in future work.
Acknowledgments
The constructive comments and helpful sugges-
tions from three anonymous reviewers are greatly
appreciated. This paper is supported by JAIST
Grant for Research Associates and a part from a
national project named Building Basic Resources
and Tools for Vietnamese Language and Speech
Processing, KC01.01/06-10.
References
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002.
K. Crammer et al 2006. Online Passive-Aggressive
Algorithm. Journal of Machine Learning Research,
2006
W. Chen, Y. Zhang, and H. Ishihara 2006. An em-
pirical study of Chinese chunking. In Proceedings
COLING/ACL 2006
Dinh Dien, Vu Thuy 2006. A maximum entropy
approach for vietnamese word segmentation. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2006: 248-253
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In the proceed-
15
ings of International Conference on Machine Learn-
ing (ICML), pp.282-289, 2001
N.C. Mai, D.N. Vu, T.P. Hoang. 1997. Foundations
of linguistics and Vietnamese. Education Publisher
(1997) 142. 152
Thi Minh Huyen Nguyen, Laurent Romary, Mathias
Rossignol, Xuan Luong Vu. 2006. A lexicon
for Vietnamese language processing. Language Re-
seourse Evaluation (2006) 40:291-309.
Minh Nghiem, Dien Dinh, Mai Nguyen. 2008. Im-
proving Vietnamese POS tagging by integrating a
rich feature set and Support Vector Machines. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2008: 128?133.
X.H. Phan, M.L. Nguyen, C.T. Nguyen. Flex-
CRFs: Flexible Conditional Random Field Toolkit.
http://flexcrfs.sourceforge.net, 2005
T. Kudo and Y. Matsumoto. 2001. Chunking with
Support Vector Machines. The Second Meeting of
the North American Chapter of the Association for
Computational Linguistics (2001)
F. Sha and F. Pereira. 2005. Shallow Parsing with
Conditional Random Fields. Proceedings of HLT-
NAACL 2003 213-220 (2003)
C.T. Nguyen, T.K. Nguyen, X.H. Phan, L.M. Viet-
namese Word Segmentation with CRFs and SVMs:
An Investigation. 2006. The 20th Pacific Asia Con-
ference on Language, Information, and Computation
(PACLIC), 1-3 November, 2006, Wuhan, China
Tjong Kim Sang and Sabine Buchholz. 2000. Intro-
duction to the CoNLL-2000 Shared Task: Chunk-
ing. Proceedings of CoNLL-2000 , Lisbon, Portugal,
2000.
V. Vapnik. 1995. The Natural of Statistical Learning
Theory. New York: Springer-Verlag, 1995.
16
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 143?150
Manchester, August 2008
A Tree-to-String Phrase-based Model for Statistical Machine Translation
Thai Phuong Nguyen
College of Technology
Vietnam National University, Hanoi
thainp@vnu.edu.vn
Akira Shimazu1, Tu-Bao Ho2, Minh Le Nguyen1, and Vinh Van Nguyen1
1School of Information Science
2School of Knowledge Science
Japan Advanced Institute of Science and Technology
{shimazu,bao,nguyenml,vinhnv}@jaist.ac.jp
Abstract
Though phrase-based SMT has achieved high
translation quality, it still lacks of generaliza-
tion ability to capture word order differences
between languages. In this paper we describe
a general method for tree-to-string phrase-
based SMT. We study how syntactic trans-
formation is incorporated into phrase-based
SMT and its effectiveness. We design syntac-
tic transformation models using unlexicalized
form of synchronous context-free grammars.
These models can be learned from source-
parsed bitext. Our system can naturally make
use of both constituent and non-constituent
phrasal translations in the decoding phase. We
considered various levels of syntactic analy-
sis ranging from chunking to full parsing.
Our experimental results of English-Japanese
and English-Vietnamese translation showed
a significant improvement over two baseline
phrase-based SMT systems.
1 Introduction
Based on the kind of linguistic information which
is made use of, syntactic SMT can be divided into
four types: tree-to-string, string-to-tree, tree-to-tree,
and hierarchical phrase-based. The tree-to-string ap-
proach (Collins et al, 2005; Nguyen and Shimazu,
2006; Liu et al, 2006 and 2007) supposes that syn-
tax of the source language is known. This approach
can be applied when a source language parser is
available. The string-to-tree approach (Yamada and
Knight, 2001; Galley et al, 2006) focuses on syntactic
modelling of the target language in cases it has syn-
tactic resources such as treebanks and parsers. The
tree-to-tree approach models the syntax of both lan-
guages, therefore extra cost is required. The fourth
approach (Chiang, 2005) constraints phrases under
context-free grammar structure without any require-
ment of linguistic annotation.
In this paper, we present a tree-to-string phrase-
based method which is based on synchronous CFGs.
This method has two important properties: syntactic
transformation is used in the decoding phase includ-
ing a word-to-phrase tree transformation model and
a phrase reordering model; phrases are the basic unit
of translation. Since we design syntactic transforma-
tion models using un-lexicalized synchronous CFGs,
the number of rules is small1. Previous studies on
tree-to-string SMT are different from ours. Collins
et al Collins et al (2005) used hand crafted rules to
carry out word reordering in the preprocessing phase
but not decoding phase. Nguyen and Shimazu (2006)
presented a more general method in which lexicalized
syntactic reordering models based on PCFGs can be
learned from source-parsed bitext and then applied in
the preprocessing phase. Liu et al (2006) changed the
translation unit from phrases to tree-to-string align-
ment templates (TATs) while we do not. TATs was
represented as xRs rules while we use synchronous
CFG rules. In order to overcome the limitation that
TATs can not capture non-constituent phrasal transla-
tions, Liu et al (2007) proposed forest-to-string rules
while our system can naturally make use of such kind
of phrasal translation by word-to-phrase tree transfor-
mation.
We carried out experiments with two language
pairs English-Japanese and English-Vietnamese. Our
system achieved significant improvements over
Pharaoh, a state-of-the-art phrase-based SMT system.
We also analyzed the dependence of translation qual-
ity on the level of syntactic analysis (shallow or deep).
Figure 1 shows the architecture of our system. The
input of this system is a source-language tree and the
output is a target-language string. This system uses
all features of conventional phrase-based SMT as in
(Koehn et al, 2003). There are two new features in-
cluding a word-to-phrase tree transformation model
and a phrase reordering model. The decoding algo-
1See Section 6.2.
143
rithm is a tree-based search algorithm.
Figure 1: A syntax-directed phrase-based SMT archi-
tecture.
2 Translation Model
We use an example of English-Vietnamese translation
to demonstrate the translation process as in Figure 2.
Now we describe a tree-to-string SMT model based
on synchronous CFGs. The translation process is:
Figure 2: The translation process.
T
1
? T
2
? T
3
? T
4
(1)
where T
1
is a source tree, T
2
is a source phrase tree,
T
3
is a reordered source phrase tree, and T
4
is a target
phrase tree.
Using the first order chain rule, the join probability
over variables (trees) in graphical representation 1 is
approximately calculated by:
P (T
1
, T
2
, T
3
, T
4
) = P (T
1
)?P (T
2
|T
1
)?P (T
3
|T
2
)?P (T
4
|T
3
)
(2)
P (T
1
) can be omitted since only one syntactic tree
is used. P (T
2
|T
1
) is a word-to-phrase tree transfor-
mation model we describe later. P (T
3
|T
2
) is a re-
ordering model. P (T
4
|T
3
) can be calculated using a
phrase translation model and a language model. This
is the fundamental equation of our study represented
in this paper. In the next section, we will describe how
to transform a word-based CFG tree into a phrase-
based CFG tree.
3 Word-to-Phrase Tree Transformation
3.1 Penn Treebank?s Tree Structure
According to this formalism, a tree is represented by
phrase structure. If we extract a CFG from a tree or
set of trees, there will be two possible rule forms:
? A ? ? where ? is a sequence of nonterminals
(syntactic categories).
? B ? ? where ? is a terminal symbol (or a word
in this case).
We consider an example of a syntactic tree and a
simple CFG extracted from that tree.
Sentence: ?I am a student?
Syntactic tree: (S (NP (NN I)) (VP (VBP am) (NP (DT a) (NN
student))))
Rule set: S ? NP VP; VP ? VBP NP; NP ? NN | DT NN; NN
? I | student;
VBP ? am; DT ? a
However, we are considering phrase-based transla-
tion. Therefore the right hand side of the second rule
form must be a sequence of terminal symbols (or a
phrase) but not a single symbol (a word). Suppose
that the phrase table contains a phrase ?am a student?
which leads to the following possible tree structure:
Phrase segmentation: ?I | am a student?
Syntactic tree: (S (NP (NN I)) (VP (VBP am a student)))
Rule set: S ? NP VP; VP ? VBP; NP ? NN; NN ? I; VBP ?
am a student
We have to find out some way to transform a CFG
tree into a tree with phrases at leaves. In the next sub-
section we propose such an algorithm.
3.2 An Algorithm for Word-to-Phrase Tree
Transformation
Table 1 represents our algorithm to transform a CFG
tree to a phrase CFG tree. When designing this algo-
rithm, our criterion is to preserve the original struc-
ture as much as possible. This algorithm includes two
steps. There are a number of notions concerning this
algorithm:
? A CFG rule has a head symbol on the right hand
side. Using this information, head child of a
node on a syntactic tree can be determined.
144
+ Input: A CFG tree, a phrase segmentation
+ Output: A phrase CFG tree
+ Step 1: Allocate phrases to leaf nodes in a top-down manner: A phrase is allocated to head word of a node if the
phrase contains the head word. This head word is then considered as the phrase head.
+ Step 2: Transform the syntactic tree by replacing leaf nodes by their allocated phrase and removing all nodes whose
span is a substring of phrases.
Table 1: An algorithm to transform a CFG tree to a phrase CFG tree.
? If a node is a pre-terminal node (containing POS
tag), its head word is itself. If a node is an in-
ner node (containing syntactic constituent tag),
its head word is retrieved through the head child.
? Word span of a node is a string of its leaves. For
instance, word span of subtree (NP (PRP$ your)
(NN class)) is ?your class?.
Now we consider an example depicted in Figure 3
and 4. Head children are tagged with functional label
H. There are two phrases: ?is a? and ?in your class?.
After the Step 1, the phrase ?is a? is attached to (VBZ
is). The phrase ?in your class? is attached to (IN in).
In Step 2, the node (V is) is replaced by (V ?is a?) and
(DT a) is removed from its father NP. Similarly, (IN
in) is replaced by (IN ?in your class?) and the subtree
NP on the right is removed.
S
[is]
NP
[Fred]
VP-H
[is]
VBZ-H NP[student]NNP-H
is NP-H[student]
DT NN-H
PP
[in]
IN-H NP[class]
PRP$ NN-H
Fred
a student in
your class
{is a}
{in your class}
Figure 3: Tree transformation - step 1. Solid arrows
show the allocation process of ?is a?. Dotted arrows
demonstrate the allocation process of ?in your class?
The proposed algorithm has some properties. We
state these properties without presenting proof2.
? Uniqueness: Given a CFG tree and a phrase seg-
mentation, by applying Algorithm 1, one and
only one phrase tree is generated.
2Proofs are simple.
Figure 4: Tree transformation - step 2.
? Constituent subgraph: A phrase CFG tree is
a connected subgraph of input tree if leaves are
ignored.
? Flatness: A phrase CFG tree is flatter than input
tree.
? Outside head: The head of a phrase is always a
word whose head outside the phrase. If there is
more than one word satisfying this condition, the
word at the highest level is chosen.
? Dependency subgraph: Dependency graph of a
phrase CFG tree is a connected subgraph of in-
put tree?s dependency graph if there exist no de-
tached nodes.
The meaning of uniqueness property is that our al-
gorithm is a deterministic procedure. The constituent-
subgraph property will be employed in the next sec-
tion for an efficient decoding algorithm. When a syn-
tactic tree is transformed, a number of subtrees are
replaced by phrases. The head word of a phrase is the
contact point of that phrase with the remaining part
of a sentence. From the dependency point of view, a
head word should depend on an outer word rather than
an inner word. About dependency-subgraph property,
when there is a detached node, an indirect dependency
will become a direct one. In any cases, there is no
145
change in dependency direction. We can observe de-
pendency trees in Figure 5. The first two trees are
source dependency tree and phrase dependency tree
of the previous example. The last one corresponds to
the case in which a detached node exists.
Fred is
ROOT
student in your classa
Fred is a
ROOT
student in your class
Fred is a student
ROOT
in your class
Figure 5: Dependency trees. The third tree corre-
sponds with phrase segmentation: ?Fred | is a student
| in your class?
3.3 Probabilistic Word-to-Phrase Tree
Transformation
We have proposed an algorithm to create a phrase
CFG tree from a pair of CFG tree and phrase seg-
mentation. Two questions naturally arise: ?is there
a way to evaluate how good a phrase tree is?? and ?is
such an evaluation valuable?? Note that phrase trees
are the means to reorder the source sentence repre-
sented as phrase segmentations. Therefore a phrase
tree is surely not good if no right order can be gen-
erated. Now the answer to the second question is
clear. We need an evaluation method to prevent our
program from generating bad phrase trees. In other
words, good phrase trees should be given a higher pri-
ority.
We define the phrase tree probability as the product
of its rule probability given the original CFG rules:
P (T
?
) =
?
i
P (LHS
i
? RHS
?
i
|LHS
i
? RHS
i
)
(3)
where T ? is a phrase tree whose CFG rules are
LHS
i
? RHS
?
i
. LHS
i
? RHS
i
are origi-
nal CFG rules. RHS?
i
are subsequences of RHS
i
.
Since phrase tree rules should capture changes made
by the transformation from word to phrase, we use
?+? to represent an expansion and ?-? to show an
overlap. These symbol will be added to a nonter-
minal on the side having a change. In the previ-
ous example, since a head noun in the word tree
has been expanded on the right, the correspond-
ing symbol in phrase tree is NN-H+. A nonter-
minal X can become one of the following symbols
X,?X,+X,X?, X+,?X?,?X+,+X?,+X+.
Conditional probabilities are computed in a sepa-
rate training phase using a source-parsed and word-
aligned bitext. First, all phrase pairs consistent with
the word alignment are collected. Then using this
phrase segmentation and syntactic trees we can gener-
ate phrase trees by word-to-phrase tree transformation
and extract rules.
4 Phrase Reordering Model
Reordering rules are represented as SCFG rules
which can be un-lexicalized or source-side lexicalized
(Nguyen and Shimazu, 2006). In this paper, we used
un-lexicalized rules. We used a learning algorithm
as in (Nguyen and Shimazu, 2006) to learn weighted
SCFGs. The training requirements include a bilingual
corpus, a word alignment tool, and a broad coverage
parser of the source language. The parser is a con-
stituency analyzer which can produce parse tree in
Penn Tree-bank?s style. The model is applicable to
language pairs in which the target language is poor
in resources. We used phrase reorder rules whose ?+?
and ?-? symbols are removed.
5 Decoding
A source sentence can have many possible phrase seg-
mentations. Each segmentation in combination with a
source tree corresponds to a phrase tree. A phrase-tree
forest is a set of those trees. A naive decoding algo-
rithm is that for each segmentation, a phrase tree is
generated and then the sentence is translated. This al-
gorithm is very slow or even intractable. Based on
the constituent-subgraph property of the tree trans-
formation algorithm, the forest of phrase trees will
be packed into a tree-structure container whose back-
bone is the original CFG tree.
5.1 Translation Options
A translation option encodes a possibility to translate
a source phrase (at a leaf node of a phrase tree) to
another phrase in target language. Since our decoder
uses a log-linear translation model, it can exploit var-
ious features of translation options. We use the same
features as (Koehn et al, 2003). Basic information of
a translation option includes:
? source phrase
? target phrase
? phrase translation score (2)
146
? lexical translation score (2)
? word penalty
Translation options of an input sentence are col-
lected before any decoding takes place. This allows a
faster lookup than consulting the whole phrase trans-
lation table during decoding. Note that the entire
phrase translation table may be too big to fit into
memory.
5.2 Translation Hypotheses
A translation hypothesis represents a partial or full
translation of an input sentence. Initial hypotheses
correspond to translation options. Each translation
hypothesis is associated with a phrase-tree node. In
other words, a phrase-tree node has a collection of
translation hypotheses. Now we consider basic infor-
mation contained in a translation hypothesis:
? the cost so far
? list of child hypotheses
? left language model state and right language
model state
5.3 Decoding Algorithm
First we consider structure of a syntactic tree. A tree
node contains fields such as syntactic category, child
list, and head child index. A leaf node has an ad-
ditional field of word string. In order to extend this
structure to store translation hypotheses, a new field
of hypothesis collection is appended. A hypothe-
sis collection contains translation hypotheses whose
word spans are the same. Actually, it corresponds to
a phrase-tree node. A hypothesis collection whose
word span is [i
1
, i
2
] at a node whose tag is X ex-
presses that:
? There is a phrase-tree node (X, i
1
, i
2
).
? There exist a phrase [i
1
, i
2
] or
? There exist a subsequence of X?s child list:
(Y
1
, j
0
, j
1
), (Y
2
, j
1
+1, j
2
), ..., (Y
n
, j
n?1
+1, j
n
)
where j
0
= i
1
and j
n
= i
2
? Suppose that [i, j] is X?s span, then [i
1
, i
2
] is a
valid phrase node?s span if and only if: i
1
<= i
or i < i
1
<= j and there exist a phrase [i
0
, i
1
?
1] overlapping X?s span at [i, i
1
? 1]. A similar
condition is required of j.
Table 2 shows our decoding algorithm. Step 1 dis-
tributes translation options to leaf nodes using a pro-
cedure similar to Step 1 of algorithm in Table 1. Step
Corpus Size Training Development Testing
Conversation 16,809 15,734 403 672
Reuters 57,778 55,757 1,000 1,021
Table 3: Corpora and data sets.
English Vietnamese
Sentences 16,809
Average sent. len. 8.5 8.0
Words 143,373 130,043
Vocabulary 9,314 9,557
English Japanese
Sentences 57,778
Average sent. len. 26.7 33.5
Words 1,548,572 1,927,952
Vocabulary 31,702 29,406
Table 4: Corpus statistics of translation tasks.
2 helps check valid subsequences in Step 3 fast. Step
3 is a bottom-up procedure, a node is translated if all
of its child nodes have been translated. Step 3.1 calls
syntactic transformation models. After reordered in
Step 3.2, a subsequence will be translated in Step 3.3
using a simple monotonic decoding procedure result-
ing in new translation hypotheses. We used a beam
pruning technique to reduce the memory cost and to
accelerate the computation.
6 Experimental Results
6.1 Experimental Settings
We used Reuters3, an English-Japanese bilingual cor-
pus, and Conversation, an English-Vietnamese corpus
(Table 4). These corpora were split into data sets as
shown in Table 3. Japanese sentences were analyzed
by ChaSen4, a word-segmentation tool.
A number of tools were used in our experiments.
Vietnamese sentences were segmented using a word-
segmentation program (Nguyen et al, 2003). For
learning phrase translations and decoding, we used
Pharaoh (Koehn, 2004), a state-of-the-art phrase-
based SMT system which is available for research
purpose. For word alignment, we used the GIZA++
tool (Och and Ney, 2000). For learning language
models, we used SRILM toolkit (Stolcke, 2002). For
MT evaluation, we used BLEU measure (Papineni et
al., 2001) calculated by the NIST script version 11b.
For the parsing task, we used Charniak?s parser (Char-
niak, 2000). For experiments with chunking (or shal-
low parsing), we used a CRFs-based chunking tool 5
to split a source sentence into syntactic chunks. Then
a pseudo CFG rule over chunks is built to generate a
two-level syntactic tree. This tree can be used in the
3http://www2.nict.go.jp/x/x161/members/mutiyama/index.html
4http://chasen.aist-nara.ac.jp/chasen/distribution.html.en
5http://crfpp.sourceforge.net/
147
+ Input: A source CFG tree, a translation-option collection
+ Output: The best target sentence
+ Step 1: Allocate translation options to hypothesis collections at leaf nodes.
+ Step 2: Compute overlap vector for all nodes.
+ Step 3: For each node, if all of its children have been translated, then for each valid
sub-sequence of child list, carry out the following steps:
+ Step 3.1: Retrieve transformation rules
+ Step 3.2: Reorder the sub-sequence
+ Step 3.3: Translate the reordered sub-sequence and update corresponding
hypothesis collections
Table 2: A bottom-up dynamic-programming decoding algorithm.
Corpus CFG PhraseCFG W2PTT Reorder
Conversation 2,784 2,684 8,862 2,999
Reuters 7,668 5,479 13,458 7,855
Table 5: Rule induction statistics.
Corpus Pharaoh PB system SD system SD system
(chunking) (full-parsing)
Conversation 35.47 35.66 36.85 37.42
Reuters 24.41 24.20 20.60 25.53
Table 6: BLEU score comparison between phrase-
based SMT and syntax-directed SMT. PB=phrase-
based; SD=syntax-directed
same way as trees produced by Charniak?s parser.
We built a SMT system for phrase-based log-linear
translation models. This system has two decoders:
beam search and syntax-based. We implemented the
algorithm in Section 5 for the syntax-based decoder.
We also implemented a rule induction module and a
module for minimum error rate training. We used the
system for our experiments reported later.
6.2 Rule Induction
In Table 5, we report statistics of CFG rules,
phrase CFG rules, word-to-phrase tree transformation
(W2PTT) rules, and reordering rules. All counted
rules were in un-lexicalized form. Those numbers are
very small in comparison with the number of phrasal
translations (up to hundreds of thousands on our cor-
pora). There were a number of ?un-seen? CFG rules
which did not have a corresponding reordering rule.
A reason is that those rules appeared once or several
times in the training corpus; however, their hierarchi-
cal alignments did not satisfy the conditions for in-
ducing a reordering rule since word alignment is not
perfect (Nguyen and Shimazu, 2006). Another reason
is that there were CFG rules which required nonlocal
reordering. This may be an issue for future research:
a Markovization technique for SCFGs.
6.3 BLEU Scores
Table 6 shows a comparison of BLEU scores be-
tween Pharaoh, our phrase-based SMT system, and
our syntax-directed (SD) SMT system with chunking
and full parsing respectively. On both Conversation
corpus and Reuters corpus: The BLEU score of our
phrase-based SMT system is comparable to that of
Pharaoh; The BLEU score of our SD system with full
parsing is higher than that of our phrase-based sys-
tem. On Conversation corpus, our SD system with
chunking has a higher performance in terms of BLEU
score than our phrase-based system. Using sign test
(Lehmann, 1986), we verified the improvements are
statistically significant. However, on Reuters corpus,
performance of the SD system with chunking is much
lower than the phrase-based system?s. The reason is
that in English-Japanese translation, chunk is a too
shallow syntactic structure to capture word order in-
formation. For example, a prepositional chunk of-
ten includes only preposition and adverb, therefore
such information does not help reordering preposi-
tional phrases.
6.4 The Effectiveness of the W2PTT Model
Without this feature, BLEU scores decreased around
0.5 on both corpora. We now consider a linguistically
motivated example of English-Vietnamese translation
to show that phrase segmentation can be evaluated
through phrase tree scoring. This example was ex-
tracted from Conversation test set.
English sentence: for my wife ?s mother
Vietnamese word order: for mother ?s wife my
Phrase segmentation 1: for my wife | ?s | mother
P1=P(PP?IN+ -NP | PP?IN NP)xP(-NP?-NP NN | NP?NP
NN)xP(-NP?POS | NP?PRP$ NN
POS)=log(0.00001)+log(0.14)+log(0.048)=-5-0.85-1.32=-7.17
Phrase segmentation 2: for | my wife ?s | mother
P2=P(PP?IN NP | PP?IN NP)xP(NP?NP NN | NP?NP
NN) xP(NP?POS | NP?PRP$ NN POS)
=log(0.32)+log(0.57)+log(0.048)=-0.5-0.24-1.32=-2.06
The first phrase segmentation is bad (or even un-
acceptable) since the right word order can not be
achieved from this segmentation by phrase reorder-
ing and word reordering within phrases. The second
phrase segmentation is much better. Source syntax
tree and phrase trees are shown in Figure 6. The first
phrase tree has a much smaller probability (P1=-7.17)
than the second (P2=-2.06).
148
Figure 6: Two phrase trees.
Corpus Level-1 Level-2 Level-3 Level-4 Full
Conversation 36.85 36.91 37.11 37.23 37.42
Reuters 20.60 22.76 24.49 25.12 25.53
Table 7: BLEU score with different syntactic levels.
Level-i means syntactic transformation was applied to
tree nodes whose level smaller than or equal to i. The
level of a pre-terminal node (POS tag) is 0. The level
of an inner node is the maximum of its children?s lev-
els.
6.5 Levels of Syntactic Analysis
Since in practice, chunking and full parsing are often
used, in Table 6, we showed translation quality of the
two cases. It is interesting if we can find how syn-
tactic analysis can affect BLEU score at more inter-
mediate levels (Table 7). On the Conversation corpus,
using syntax trees of level-1 is effective in comparison
with baseline. The increase of syntactic level makes a
steady improvement in translation quality. Note that
when we carried out experiments with chunking (con-
sidered as level-1 syntax) the translation speed (in-
cluding chunking) of our tree-to-string system was
much faster than baseline systems?. This is an option
for developing applications which require high speed
such as web translation.
7 Related Works
7.1 A Comparison of Syntactic SMT Methods
To advance the state of the art, SMT system design-
ers have experimented with tree-structured transla-
tion models. The underlying computational models
were synchronous context-free grammars and finite-
state tree transducers which conceptually have a bet-
ter expressive power than finite-state transducers. We
create Tables 8 and 9 in order to compare syntac-
tic SMT methods including ours. The first row is a
baseline phrasal SMT approach. The second column
in Table 8 only describes input types because the out-
put is often string. Syntactic SMT methods are dif-
ferent in many aspects. Methods which make use of
phrases (in either explicit or implicit way) can beat
the baseline approach (Table 8) in terms of BLEU
metric. Two main problems these models aim to deal
with are word order and word choice. In order to ac-
complish this purpose, the underlying formal gram-
mars (including synchronous context-free grammars
and tree transducers) can be fully lexicalized or un-
lexicalized (Table 9).
7.2 Non-constituent Phrasal Translations
Liu et al (2007) proposed forest-to-string rules to
capture non-constituent phrasal translation while our
system can naturally make use of such kind of phrasal
translation by using word-to-phrase tree transforma-
tion. Liu et al (2007) also discussed about how
the phenomenon of non-syntactic bilingual phrases
is dealt with in other SMT methods. Galley et al
(2006) handled non-constituent phrasal translation by
traversing the tree upwards until reaches a node that
subsumes the phrase. Marcu et al (2006) reported
that approximately 28% of bilingual phrases are non-
syntactic on their English-Chinese corpus. They pro-
posed using a pseudo nonterminal symbol that sub-
sumes the phrase and corresponding multi-headed
syntactic structure. One new xRs rule is required to
explain how the new nonterminal symbol can be com-
bined with others. This technique brought a signif-
icant improvement in performance to their string-to-
tree noisy channel SMT system.
8 Conclusions
We have presented a general tree-to-string phrase-
based method. This method employs a syntax-based
reordering model in the decoding phase. By word-
to-phrase tree transformation, all possible phrases
are considered in translation. Our method does
not suppose a uniform distribution over all possible
phrase segmentations as (Koehn et al, 2003) since
each phrase tree has a probability. We believe that
other kinds of translation unit such as n-gram (Jos
et al, 2006), factored phrasal translation (Koehn and
Hoang, 2007), or treelet (Quirk et al, 2005) can be
used in this method. We would like to consider this
problem as a future study. Moreover we would like to
use n-best trees as the input of our system. A number
149
Method Input Theoretical Decoding style Linguistic Phrase Performance
model information usage
Koehn et al (2003) string FSTs beam search no yes baseline
Yamada and Knight (2001) string SCFGs parsing target no not better
Melamed (2003) string SCFGs parsing both sides no not better
Chiang (2005) string SCFGs parsing no yes better
Quirk et al (2005) dep. tree TTs parsing source yes better
Galley et al (2006) string TTs parsing target yes better
Liu et al (2006) tree TTs tree transf. source yes better
Our work tree SCFGs tree transf. source yes better
Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous
Context-Free Grammar; TT=Tree Transducer.
Method Rule form Rule function Rule lexicalization level
Koehn et al (2003) no no no
Yamada and Knight (2001) SCFG rule reorder and function-word ins./del. unlexicalized
Melamed (2003) SCFG rule reorder and word choice full
Chiang (2005) SCFG rule reorder and word choice full
Quirk et al (2005) Treelet pair word choice full
Galley et al (2006) xRs rule reorder and word choice full
Liu et al (2006) xRs rule reorder and word choice full
Our work SCFG rule reorder unlexicalized
Table 9: A comparison of syntactic SMT methods (part 2). xRs is a kind of rule which maps a syntactic pattern
to a string, for example VP(AUX(does), RB(not),x
0
:VB) ? ne, x
0
, pas. In the column Rule lexicalization
level: full=lexicalization using vocabularies of both source language and target language.
of non-local reordering phenomena such as adjunct
attachment should be handled in the future.
References
Charniak, E. 2000. A maximum entropy inspired parser.
In Proceedings of HLT-NAACL.
Galley, M., Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, Ignacio Thayer 2006. Scal-
able Inference and Training of Context-Rich Syntactic
Translation Models. In Proceedings of ACL.
Jos B. Mario, Rafael E. Banchs, Josep M. Crego, Adri de
Gispert, Patrik Lambert, Jos A. R. Fonollosa, Marta R.
Costa-juss. 2006. N-gram-based Machine Translation.
Computational Linguistics, 32(4): 527?549.
Koehn, P. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of AMTA.
Koehn, P. and Hieu Hoang. 2007. Factored Translation
Models. In Proceedings of EMNLP.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
Lehmann, E. L. 1986. Testing Statistical Hypotheses (Sec-
ond Edition). Springer-Verlag.
Liu, Y., Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL.
Liu, Y., Yun Huang, Qun Liu, and Shouxun Lin 2007.
Forest-to-String Statistical Translation Rules. In Pro-
ceedings of ACL.
Marcu, D., Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phrases. In Proceed-
ings of EMNLP.
Melamed, I. D. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
Nguyen, Thai Phuong and Akira Shimazu. 2006. Improv-
ing Phrase-Based Statistical Machine Translation with
Morphosyntactic Transformation. Machine Translation,
20(3): 147?166.
Nguyen, Thai Phuong, Nguyen Van Vinh and Le Anh
Cuong. 2003. Vietnamese Word Segmentation Using
Hidden Markov Model. In Proceedings of International
Workshop for Computer, Information, and Communica-
tion Technologies in Korea and Vietnam.
Och, F. J. and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL.
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Report.
Quirk, C., A. Menezes, and C. Cherry. 2005. Dependency
treelet translation: Syntactically informed phrasal SMT.
In Proceedings of ACL.
Stolcke, A. 2002. SRILM - An Extensible Language Mod-
eling Toolkit. In Proc. Intl. Conf. Spoken Language
Processing.
Yamada, K. and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proceedings of ACL.
150
