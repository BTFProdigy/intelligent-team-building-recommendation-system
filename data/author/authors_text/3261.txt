Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 18?25,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
What?s in a Message?
Stergos D. Afantenos and Nicolas Hernandez
LINA, (UMR CNRS 6241)
Universit? de Nantes, France
stergos.afantenos@univ-nantes.fr
nicolas.hernandez@univ-nantes.fr
Abstract
In this paper we present the first step in a larger
series of experiments for the induction of pred-
icate/argument structures. The structures that
we are inducing are very similar to the con-
ceptual structures that are used in Frame Se-
mantics (such as FrameNet). Those structures
are called messages and they were previously
used in the context of a multi-document sum-
marization system of evolving events. The se-
ries of experiments that we are proposing are
essentially composed from two stages. In the
first stage we are trying to extract a represen-
tative vocabulary of words. This vocabulary
is later used in the second stage, during which
we apply to it various clustering approaches in
order to identify the clusters of predicates and
arguments?or frames and semantic roles, to
use the jargon of Frame Semantics. This paper
presents in detail and evaluates the first stage.
1 Introduction
Take a sentence, any sentence for that matter; step back
for a while and try to perceive that sentence in its most
abstract form. What you will notice is that once you
try to abstract away sentences, several regularities be-
tween them will start to emerge. To start with, there is
almost always an action that is performed.1 Then, there
is most of the times an agent that is performing this ac-
tion and a patient or a benefactor that is receiving this
action, and it could be the case that this action is per-
formed with the aid of a certain instrument. In other
words, within a sentence?and in respect to its action-
denoting word, or predicate in linguistic terms?there
will be several entities that are associated with the pred-
icate, playing each time a specific semantic role.
The notion of semantic roles can be traced back to
Fillmore?s (1976) theory of Frame Semantics. Accord-
ing to this theory then, a frame is a conceptual structure
which tries to describe a stereotypical situation, event
or object along with its participants and props. Each
frame takes a name (e.g. COMMERCIAL TRANSAC-
TION) and contains a list of Lexical Units (LUs) which
1In linguistic terms, an action-denoting word is also
known as a predicate.
actually evoke this frame. An LU is nothing else than
a specific word or a specific meaning of a word in the
case of polysemous words. To continue the previous
example, some LUs that evoke the frame of COMMER-
CIAL TRANSACTION could be the verbs buy, sell,
etc. Finally, the frames contain several frame elements
or semantic roles which actually denote the abstract
conceptual entities that are involved with the particu-
lar frame.
Research in semantic roles can be distinguished into
two major branches. The first branch of research con-
sists in defining an ontology of semantic roles, the
frames in which the semantic roles are found as well as
defining the LUs that evoke those frames. The second
branch of research, on the other hand, stipulates the
existence of a set of frames, including semantic roles
and LUs; its goal then, is the creation of an algorithm
that given such a set of frames containing the semantic
roles, will be able to label the appropriate portions of
a sentence with the corresponding semantic roles. This
second branch of research is known as semantic role
labeling.
Most of the research concerning the definition of the
semantic roles has been carried out by linguists who are
manually examining a certain amount of frames before
finally defining the semantic roles and the frames that
contain those semantic roles. Two such projects that
are widely known are the FrameNet (Baker et al, 1998;
Ruppenhofer et al, 2006) and PropBank/NomBank 2
(Palmer et al, 2005; Meyers et al, 2004). Due to the
fact that the aforementioned projects are accompanied
by a large amount of annotated data, computer scien-
tists have started creating algorithms, mostly based on
statistics (Gildea and Jurafsky, 2002; Xue, 2008) in or-
der to automatically label the semantic roles in a sen-
tence. Those algorithms take as input the frame that
2We would like to note here that although the two ap-
proaches (FrameNet and PropBank/NomBank) share many
common elements, they have several differences as well.
Two major differences, for example, are the fact that the
Linguistic Units (FrameNet) are referred to as Relations
(PropBank/NomBank), and that for the definition of the se-
mantic roles in the case of PropBank/NomBank there is no
reference ontology. A detailed analysis of the differences be-
tween FrameNet and PropBank/NomBank would be out of
the scope of this paper.
18
contains the roles as well as the predicate3 of the sen-
tence.
Despite the fact that during the last years we have
seen an increasing interest concerning semantic role
labeling,4 we have not seen many advancements con-
cerning the issue of automatically inducing semantic
roles from raw textual corpora. Such a process of in-
duction would involve, firstly the identification of the
words that would serve as predicates and secondly the
creation of the appropriate clusters of word sequences,
within the limits of a sentence, that behave similarly
in relation to the given predicates. Although those
clusters of word sequences could not actually be said
to serve in themselves as the semantic roles,5 they
can nevertheless be viewed as containing characteris-
tic word sequences of specific semantic roles. The last
point has the implication that if one is looking for a
human intuitive naming of the semantic role that is im-
plied by the cluster then one should look elsewhere.
This is actually reminiscent of the approach that is car-
ried out by PropBank/NomBank in which each seman-
tic role is labeled as Arg1 through Arg5 with the se-
mantics given aside in a human readable natural lan-
guage sentence.
Our goal in this paper is to contribute to the research
problem of frame induction, that is of the creation of
frames, including their associated semantic roles, given
as input only a set of textual documents. More specifi-
cally we propose a general methodology to accomplish
this task, and we test its first stage which includes the
use of corpus statistics for the creation of a subset of
words, from the initial universe of initial words that are
present in the corpus. This subset will later be used
for the identification of the predicates as well as the
semantic roles. Knowing that the problem of frame in-
duction is very difficult in the general case, we limit
ourselves to a specific genre and domain trying to ex-
ploit the characteristics that exist in that domain. The
domain that we have chosen is that of the terroristic in-
cidents which involve hostages. Nevertheless, the same
methodology could be applied to other domains.
The rest of the paper is structured as follows. In sec-
tion 2 we describe the data on which we have applied
our methodology, which itself is described in detail in
section 3. Section 4 describes the actual experiments
that we have performed and the results obtained, while
a discussion of those results follows in section 5. Fi-
nally, section 6 contains a description of the related
work while we present our future work and conclusions
in section 7.
3In the case of FrameNet the predicate corresponds to a
?Linguistic Unit?, while in the case of PropBank/NomBank
it corresponds to what is named ?Relation?.
4Cf, for example, the August 2008 issue of the journal
Computational Linguistics (34:2).
5At least as the notion of semantic roles is proposed and
used by FrameNet.
2 The Annotated Data
The annotated data that we have used in order to
perform our experiments come from a previous work
on automatic multi-document summarization of events
that evolve through time (Afantenos et al, 2008; Afan-
tenos et al, 2005; Afantenos et al, 2004). The method-
ology that is followed is based on the identification of
similarities and differences?between documents that
describe the evolution of an event?synchronically as
well as diachronically. In order to do so, the notion of
Synchronic and Diachronic cross document Relations
(SDRs),6 was introduced. SDRs connect not the doc-
uments themselves but some semantic structures that
were called messages. The connection of the messages
with the SDRs resulted in the creation of a semantic
graph that was then fed to a Natural Language Gener-
ation (NLG) system in order to produce the final sum-
mary. Although the notion of messages was originally
inspired by the notion of messages as used in the area of
NLG, for example during the stage of Content Determi-
nation as described in (Reiter and Dale, 1997), and in
general they do follow the spirit of the initial definition
by Reiter & Dale, in the following section we would
like to make it clear what the notion of messages rep-
resents for us. In the rest of the paper, when we refer to
the notion of messages, it will be in the context of the
discussion that follows.
2.1 Messages
The intuition behind messages, is the fact that during
the evolution of an event we have several activities that
take place and each activity is further decomposed into
a series of actions. Messages were created in order to
capture this abstract notion of actions. Of course, ac-
tions usually implicate several entities. In this case, en-
tities were represented with the aid of a domain ontol-
ogy. Thus, in more formal terms a message m can be
defined as follows:
m = message_type (arg1, . . . , argn)
where argi ? Topic Ontology, i ? {1, . . . , n}
In order to give a simple example, let us take for in-
stance the case of the hijacking of an airplane by ter-
rorists. In such a case, we are interested in knowing
if the airplane has arrived to its destination, or even to
another place. This action can be captured by a mes-
sage of type arrive whose arguments can be the en-
tity that arrives (the airplane in our case, or a vehicle,
in general) and the location that it arrives. The specifi-
cations of such a message can be expressed as follows:
6Although a full analysis of the notion of Synchronic and
Diachronic Relations is out of the scope of this paper, we
would like simply to mention that the premises on which
those relations are defined are similar to the ones which gov-
ern the notion of Rhetorical Structure Relations in Rhetorical
Structure Theory (RST) (Taboada and Mann, 2006), with the
difference that in the case of SDRs the relations hold across
documents, while in the case of RSTs the relation hold inside
a document.
19
arrive (what, place)
what : Vehicle
place : Location
The concepts Vehicle and Location belong to the
ontology of the topic; the concept Airplane is a sub-
concept of the Vehicle. A sentence that might in-
stantiate this message is the following:
The Boeing 747 arrived at the airport of
Stanstend.
The above sentence instantiates the following message:
arrive ("Boeing 747", "airport of
Stanstend")
The domain which was chosen was that of terroris-
tic incidents that involve hostages. An empirical study,
by three people, of 163 journalistic articles?written in
Greek?that fell in the above category, resulted in the
definition of 48 different message types that represent
the most important information in the domain. At this
point we would like to stress that what we mean by
?most important information? is the information that
one would normally expect to see in a typical summary
of such kinds of events. Some of the messages that
have been created are shown in Table 1; figure 1 pro-
vides full specifications for two messages.
free explode
kill kidnap
enter arrest
negotiate encircle
escape_from block_the_way
give_deadline
Table 1: Some of the message types defined.
negotiate (who, with_whom, about)
who : Person
with_whom : Person
about : Activity
free (who, whom, from)
who : Person
whom : Person
from : Place ? Vehicle
Figure 1: An example of message specifications
Although in an abstract way the notion of messages,
as presented in this paper approaches the notion of
frame semantics?after all, both messages and frame
semantics are concerned with ?who did what, to whom,
when, where and how??it is our hope that our ap-
proach could ultimately be used for the problem of
frame induction. Nevertheless, the two structures have
several points in which they differ. In the following
section we would like to clarify those points in which
the two differ.
2.2 How Messages differ from Frame Semantics
As it might have been evident until now, the notions
of messages and frame semantics are quite similar, at
least from an abstract point of view. In practical terms
though, the two notions exhibit several differences.
To start with, the notion of messages has been used
until now only in the context of automatic text summa-
rization of multiple documents. Thus, the aim of mes-
sages is to capture the essential information that one
would expect to see in a typical summary of this do-
main.7 In contrast, semantic roles and the frames in
which they exist do not have this limitation.
Another differentiating characteristic of frame se-
mantics and messages is the fact that semantic roles al-
ways get instantiated within the boundaries of the sen-
tence in which the predicate exists. By contrast, in mes-
sages although in the vast majority of the cases there is
a one-to-one mapping from sentences to messages, in
some of the cases the arguments of a message, which
correspond to the semantic roles, are found in neigh-
boring sentences. The overwhelming majority of those
cases (which in any case were but a few) concern re-
ferring expressions. Due to the nature of the machine
learning experiments that were performed, the actual
entities were annotated as arguments of the messages,
instead of the referring expressions that might exist in
the sentence in which the message?s predicate resided.
A final difference that exists between messages and
frame semantics is the fact that messages were meant
to exist within a certain domain, while the definition of
semantic roles is usually independent of a domain.8
3 The Approach Followed
A schematic representation of our approach is shown
in Figure 2. As it can be seen from this figure, our ap-
proach comprises two stages. The first stage concerns
the creation of a lexicon which will contain as most as
possible?and, of course, as accurately as possible?
candidates that are characteristic either of the predi-
cates (message types) or of the semantic roles (argu-
ments of the messages). This stage can be thought of
as a filtering stage. The second stage involves the use
of unsupervised clustering techniques in order to create
the final clusters of words that are characteristic either
of the predicates or of the semantic roles that are asso-
7In this sense then, the notion of messages is reminiscent
of Schank & Abelson?s (1977) notion of scripts, with the dif-
ference that messages are not meant to exist inside a struc-
ture similar to Schank & Abelson?s ?scenario?. We would
like also to note that the notion of messages shares certain
similarities with the notion of templates of Information Ex-
traction, as those structures are used in conferences such as
MUC. Incidentally, it is not by chance that the ?M? in MUC
stands for Message (Understanding Conference).
8We would like to note at this point that this does not ex-
clude of course the fact that the notion of messages could be
used in a more general, domain independent way. Neverthe-
less, the notion of messages has for the moment been applied
in two specific domains (Afantenos et al, 2008).
20
ciated with those predicates. The focus of this paper is
on the first stage.
As we have said, our aim in this paper is the use
of statistical measures in order to extract from a given
corpus a set of words that are most characteristic of
the messages that exist in this corpus. In the context
of this paper, a word will be considered as being char-
acteristic of a message if this word is employed in a
sentence that has been annotated with that message. If
a particular word does not appear in any message an-
notated sentence, then this word will not be considered
as being characteristic of this message. In more formal
terms then, we can define our task as follows. If by U
we designate the set of all the words that exist in our
corpus, then we are looking for a setM such that:
M? U ?
w ?M? m appears at least once
in a message instance (1)
In order to extract the set M we have employed the
following four statistical measures:
Collection Frequency: The set that results from the
union of the n% most frequent words that appear
in the corpus.
Document Frequency: The set that results from the
union of the n% most frequent words of each doc-
ument in the corpus.
tf.idf: For each word in the corpus we calculate its
tf.idf . Then we create a set which is the union of
words with the highest n% tf.idf score in each
document.
Inter-document Frequency: A word has inter-docu-
ment frequency n if it appears in at least n docu-
ments in the corpus. The set with inter-document
frequency n is the set that results from the union
of all the words that have inter-document fre-
quency n.
As we have previously said in this paper, our goal is
the exploitation of the characteristic vocabulary that
exists in a specific genre and domain in order to ulti-
mately achieve our goal of message induction, some-
thing which justifies the use of the above statistical
measures. The first three measures are known to be
used in context of Information retrieval to capture top-
ical informations. The latter measure has been pro-
posed by (Hernandez and Grau, 2003) in order to ex-
tract rhetorical indicator phrases from a genre depen-
dant corpus.
In order to calculate the aforementioned statistics,
and create the appropriate set of words, we ignored all
the stop-words. In addition we worked only with the
verbs and nouns. The intuition behind this decision lies
in the fact that the created set will later be used for the
identification of the predicates and the induction of the
semantic roles. As Gildea & Jurafsky (2002)?among
others?have mentioned, predicates, or action denot-
ing words, are mostly represented by verbs or nouns.9
Thus, in this series of experiments we are mostly focus-
ing in the extraction of a set of words that approaches
the set that is obtained by the union of all the verbs and
nouns found in the annotated sentences.
4 Experiments and Results
The corpus that we have consists of 163 journalistic
articles which describe the evolution of five different
terroristic incidents that involved hostages. The cor-
pus was initially used in the context of training a multi-
document summarization system. Out of the 3,027 sen-
tences that the corpus contains, about one third (1,017
sentences) were annotated with the 48 message types
that were mentioned in section 2.1.
Number of Documents: 163
Number of Token: 71,888
Number of Sentences: 3,027
Annotated Sentences (messages): 1,017
Distinct Verbs and Nouns in the Corpus: 7,185
Distinct Verbs and Nouns in the Messages: 2,426
Table 2: Corpus Statistics.
The corpus contained 7,185 distinct verbs and nouns,
which actually constitute the U of the formula (1)
above. Out of those 7,185 distinct verbs and nouns
2,426 appear in the sentences that have been annotated
with the messages. Our goal was to create this set that
approached as much as possible to the set of 2,426 dis-
tinct verbs and nouns that are found in the messages.
Using the four different statistical measures pre-
sented in the previous section, we tried to reconstruct
that set. In order to understand how the statistical mea-
sures behaved, we varied for each one of them the value
of the threshold used. For each statistical measure used,
the threshold represents something different. For the
Collection Frequency measure the threshold represents
the n% most frequent words that appear in the cor-
pus. For the Document Frequency it represents the n%
most frequent words that appear in each document sep-
arately. For tf.idf it represents the words with the high-
est n% tf.idf score in each document. Finally for the
Inter-document Frequency the threshold represents the
verbs and nouns that appear in at least n documents.
Since for the first three measures the threshold repre-
sents a percentage, we varied it from 1 to 100 in order
to study how this measure behaves. For the case of
the Inter-document Frequency, we varied the threshold
from 1 to 73 which represents the maximum number of
documents in which a word appeared.
In order to measure the performance of the statistical
measures employed, we used four different evaluation
measures, often employed in the information retrieval
9In some rare cases predicates can be represented by ad-
jectives as well.
21
Lexicon Extraction(initial predicate filtering) Unsupervised Clustering
Clusters of predicates and semantic roles
Figure 2: Two different stages in the process of predicate clustering
field. Those measures are the Precision, Recall, F-
measure and Fallout. Precision represents the percent-
age of the correctly obtained verbs and nouns over the
total number of obtained verbs and nouns. Recall rep-
resents the percentage of the obtained verbs and nouns
over the target set of verbs and nouns. The F-measure
is the harmonic mean of Precision and Recall. Finally,
fallout represents the number of verbs and nouns that
were wrongly classified by the statistical measures as
belonging to a message, over the total number of verbs
and nouns that do not belong to a message. In an ideal
situation one expects a very high precision and recall
(and by consequence F-measure) and a very low Fall-
out.
The obtained graphs that combine the evaluation re-
sults for the four statistical measures presented in sec-
tion 3 are shown in Figures 3 through 6. A first remark
that we can make in respect to those graphs is that con-
cerning the collection frequency, document frequency
and tf.idf measures, for small threshold numbers we
have more or less high precision values while the recall
and fallout values are low. This implies that for smaller
threshold values the obtained sets are rather small, in
relation toM (and by consequence to U as well). As
the threshold increases we have the opposite situation,
that is the precision falls while the recall and the fall-
out increases, implying that we get much bigger sets of
verbs and nouns.
In terms of absolute numbers now, the best F-
measure is given by the Collection Frequency measure
with a threshold value of 46%. In other words, the
best results?in terms of F-measure?is given by the
union of the 46% most frequent verbs and nouns that
appear in the corpus. For this threshold the Precision
is 54.14%, the Recall is 72.18% and the F-measure is
61,87%. This high F-measure though comes at a cer-
tain cost since the Fallout is at 31.16%. This implies
that although we get a rather satisfying score in terms
of precision and recall, the number of false positives
that we get is rather high in relation to our universe.
As we have earlier said, a motivating factor of this pa-
per is the automatic induction of the structures that we
have called messages; the extracted lexicon of verbs
and messages will later be used by an unsupervised
clustering algorithm in order to create the classes of
words which will correspond to the message types. For
this reason, although we prefer to have an F-measure as
high as possible, we also want to have a fallout measure
as low as possible, so that the number of false positives
will not perturb the clustering algorithm.
If, on the other hand, we examine the relation be-
tween the F-measure and Fallout, we notice that for the
Inter-document Frequency with a threshold value of 4
we obtain a Precision of 71.60%, a recall of 43.86%
and an F-measure of 54.40%. Most importantly though
we get a fallout measure of 8.86% which implies that
the percentage of wrongly classified verbs and nouns
compose a small percentage of the total universe of
verbs and nouns. This combination of high F-measure
and very low Fallout is very important for later stages
during the process of message induction.
5 Discussion
As we have claimed in the introduction of this paper,
although we have applied our series of experiments in
a single domain, that of terroristic incidents which in-
volve hostages, we believe that the proposed procedure
can be viewed as a ?general? one. In the section we
would like to clarify what exactly we mean by this
statement.
In order to proceed, we would like to suggest that
one can view two different kinds of generalization for
the proposed procedure:
1. The proposed procedure is a general one in the
sense that it can be applied in a large corpus of het-
erogeneous documents incorporating various do-
mains and genres, in order to yield ?general?, i.e.
domain-independent, frames that can later be used
for any kind of domain.
2. The proposed procedure is a general one in the
sense that it can be used in any kind of domain
without any modifications. In contrast with the
first point, in this case the documents to which
the proposed procedure will be applied ought to
be homogeneous and rather representative of the
domain. The induced frames will not be general
ones, but instead will be domain dependent ones.
22
L e x i c LLLeLxLi LcoLoeoxoi oceLeeexei ecnLnenxni ncxLxexxxi xc L  e  x  i  c iL ie ix ii ic ELEeExEi EccLcecxci cc
trtta
otrtta
ntrtta
 trtta
Etrtta
Lttrtta
Lotrtta
(lpdfgf)U
spduvvCmpug?lp
Cuvv)??
Figure 3: Collection Frequency statistics
L e x i c LLLeLxLi LcoLoeoxoi oceLeeexei ecnLnenxni ncxLxexxxi xc L  e  x  i  c iL ie ix ii ic ELEeExEi EccLcecxci cc
trtta
otrtta
ntrtta
 trtta
Etrtta
Lttrtta
Lotrtta
(lpdfgf)U
spduvv
Cmpug?lp
Cuvv)??
Figure 4: Document Frequency statistics
Given the above two definitions of generality, we
could say that the procedure proposed in this paper
falls rather in the second category than in the first
one. Ignoring for the moment the second stage of the
procedure?clustering of word sequences characteris-
tic of specific semantic roles?and focusing on the ac-
tual work described in this paper, that is the use of
statistical methods for the identification of candidate
predicates, it becomes clear that the use of an hetero-
geneous, non-balanced corpus is prone to skewing the
results. By consequence, we believe that the proposed
procedure is general in the sense that we can use it for
any kind of domain which is described by an homoge-
neous corpus of documents.
6 Related Work
Teufel and Moens (2002) and Saggion and Lapalme
(2002) have shown that templates based on domain
concepts and relations descriptions can be used for the
task of automatic text summarization. The drawback
of their work is that they rely on manual acquisition
of lexical resources and semantic classes? definition.
Consequently, they do not avoid the time-consuming
task of elaborating linguistic resources. It is actually
for this kind of reason?that is, the laborious manual
work?that automatic induction of various structures is
a recurrent theme in different research areas of Natural
Language Processing.
An example of an inductive Information Extraction
algorithm is the one presented by Fabio Ciravegna
(2001). The algorithm is called (LP)2. The goal of the
algorithm is to induce several symbolic rules given as
input previous SGML tagged information by the user.
The induced rules will later be applied in new texts in
order to tag it with the appropriate SGML tags. The
induced rules by (LP)2 fall into two distinct categories.
In the first we have a bottom up procedure which gen-
eralizes the tag instances found in the training corpus
which uses shallow NLP knowledge. A second set of
rules is also created which have a corrective character;
that is, the application of this second set of rules aims
at correcting several of the mistakes that are performed
by the first set of rules.
On the other hand several researchers have pioneered
the automatic acquisition of lexical and semantic re-
sources (such as verb classes). Some approaches are
based on Harris?s (1951) distribution hypothesis: syn-
tactic structures with high occurrences can be used for
identifying word clusters with common contexts (Lin
and Pantel, 2001). Some others perform analysis from
semantic networks (Green et al, 2004). Poibeau and
Dutoit (2002) showed that both can be used in a com-
plementary way.
Currently, our approach follows the first trend.
Based on Hernandez and Grau (2003; 2004)?s proposal,
we aim at explicitly using corpus characteristics such as
its genre and domain features to reduce the quantity of
considered data. In this paper we have explored various
statistical measures which could be used as a filter for
improving results obtained by the previous mentioned
works.
23
L e x i c LLLeLxLi LcoLoeoxoi oceLeeexei ecnLnenxni ncxLxexxxi xc  L  e  x  i  c iL ie ix ii ic ELEeExEi EccLcecxci cc
trtta
otrtta
ntrtta
 trtta
Etrtta
Lttrtta
Lotrtta
(lpdfgf)U
spduvvCm?pug?lp
Cuvv)??
Figure 5: Tf.idf statistics
L ex i c on  ELtLLLeLxLiLcLoLnL LEeteLeeexeieceoene eExtxLxexxxixcxoxnx xEit iLieixii icioini iEctcLcecxcicccocnc cEotoLoeoxoiocooono oEntnLnenx
trtta
etrtta
itrtta
otrtta
 trtta
Lttrtta
Letrtta
(lpdfgf)UspduvvCmpug?lpCuvv)??
Figure 6: Inter-document frequency statistics
7 Conclusions and Future Work
In this paper we have presented a statistical approach
for the extraction of a lexicon which contains the verbs
and nouns that can be considered as candidates for use
as predicates for the induction of predicate/argument
structures that we call messages. Actually, the research
presented here can be considered as the first step in a
two-stages approach. The next step involves the use
of clustering algorithms on the extracted lexicon which
will provide the final clusters that will contain the pred-
icates and arguments for the messages. This process
is itself part of a larger process for the induction of
predicate/argument structures. Apart from messages,
such structures could as well be the structures that are
associated with frame semantics, that is the frames
and their associated semantic roles. Despite the great
resemblances that messages and frames have, one of
their great differences is the fact that messages were
firstly introduced in the context of automatic multi-
document summarization. By consequence they are
meant to capture the most important information in a
domain. Frames and semantic roles on the other hand,
do not have this restriction and thus are more general.
Nonetheless, it is our hope that the current research
could ultimately be useful for the induction of frame se-
mantics. In fact it is in our plans for the immediate fu-
ture work to apply the same procedure in FrameNet an-
notated data10 in order to extract a vocabulary of verbs
10See http://framenet.icsi.berkeley.edu/
index.php?option=com_wrapper&Itemid=84
and nouns which will be characteristic of the different
Linguistic Units (LUs) for the frames of FrameNet.
The proposed statistical measures are meant to be a
first step towards a fully automated process of mes-
sage induction. The immediate next step in the pro-
cess involves the application of various unsupervised
clustering techniques on the obtained lexicon in order
to create the 48 different classes each one of which
will represent a distinct vocabulary for the 48 differ-
ent message types. We are currently experimenting
with several algorithms such K-means, Expectation-
Minimization (EM), Cobweb and Farthest First. In ad-
dition to those clustering algorithms, we are also exam-
ining the use of various lexical association measures
such as Mutual Information, Dice coefficient, ?2, etc.
Although this approach will provide us with clusters of
predicates and candidate arguments, still the problem
of linking the predicates with their arguments remains.
Undoubtedly, the use of more linguistically oriented
techniques, such as syntactic analysis, is inevitable. We
are currently experimenting with the use of a shallow
parser (chunker) in order to identify the chunks that
behave similarly in respect to a given cluster of pred-
icates.
Concerning the evaluation of our approach, the high-
est F-measure score (61,87%) was given by the Col-
lection Frequency statistical measure with a threshold
value of 46%. This high F-measure though came at the
cost of a high Fallout score (31.16%). Since the ex-
tracted lexicon will later be used as an input to a clus-
tering algorithm, we would like to minimize as much as
24
possible the false positives. By consequence we have
opted in using the Inter-document Frequency measure
which presents an F-measure of 54.40% and a much
more limited Fallout of 8.86%.
Acknowledgments
The authors would like to thank Konstantina Liontou and
Maria Salapata for their help on the annotation of the mes-
sages, as well as the anonymous reviewers for their insightful
and constructive comments.
References
Stergos D. Afantenos, Irene Doura, Eleni Kapel-
lou, and Vangelis Karkaletsis. 2004. Exploit-
ing cross-document relations for multi-document
evolving summarization. In G. A. Vouros and
T. Panayiotopoulos, editors, Methods and Applica-
tions of Artificial Intelligence: Third Hellenic Con-
ference on AI, SETN 2004, volume 3025 of Lecture
Notes in Computer Science, pages 410?419, Samos,
Greece, May. Springer-Verlag Heidelberg.
Stergos D. Afantenos, Konstantina Liontou, Maria
Salapata, and Vangelis Karkaletsis. 2005. An in-
troduction to the summarization of evolving events:
Linear and non-linear evolution. In Bernadette
Sharp, editor, Proceedings of the 2nd International
Workshop on Natural Language Understanding and
Cognitive Science, NLUCS 2005, pages 91?99, Ma-
iami, Florida, USA, May. INSTICC Press.
Stergos D. Afantenos, Vangelis Karkaletsis, Panagio-
tis Stamatopoulos, and Constantin Halatsis. 2008.
Using synchronic and diachronic relations for sum-
marizing multiple documents describing evolving
events. Journal of Intelligent Information Systems,
30(3):183?226, June.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the COLING-ACL, Montreal, Canada.
Fabio Ciravegna. 2001. Adaptive information extrac-
tion from text by rule induction and generalisation.
In 17th International Joint Conference on Artificial
Intelligence (IJCAI 2001), pages 1251?1256, Seat-
tle, USA, August.
C. J. Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences: Conference on the Origin and Develop-
ment of Language and Speech, 280:20?32.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing frame semantic verb classes from
wordnet and ldoce. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 375?382,
Barcelona, Spain, July.
Zelig Harris. 1951. Structural Linguistics. University
of Chicago Press.
Nicolas Hernandez and Brigitte Grau. 2003. Auto-
matic extraction of meta-descriptors for text descrip-
tion. In International Conference on Recent Ad-
vances In Natural Language Processing (RANLP),
Borovets, Bulgaria, 10-12 September.
Nicolas Hernandez. 2004. D?tection et Description
Automatique de Structures de Texte. Ph.D. thesis,
Universit? de Paris-Sud XI.
Dekang Lin and Patrick Pantel. 2001. Induction of
semantic classes from natural language text. In Pro-
ceedings of ACM Conference on Knowledge Discov-
ery and Data Mining (KDD-01), pages 317?322, San
Francisco, CA.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Adam Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24?31, Boston, Massachusetts, USA,
May. Association for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Thierry Poibeau and Dominique Dutoit. 2002. In-
ferring knowledge from a large semantic network.
In Proceeding of the Semantic networks workshop,
during the Computational Linguistics Conference
(COLING 2002), Taipei, Taiwan.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Natural Lan-
guage Engineering, 3(1):57?87.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. Framenet ii: Extended theory and
practice. Unpublished manuscript; accessible at
http://framenet.icsi.berkeley.edu.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with sumum.
Computational Linguistics, 28(4):497?526.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: an Inquiry into
Human Knowledge Structures. L. Erlbaum, Hills-
dale, NJ.
Maite Taboada and William C. Mann. 2006. Rhetor-
ical structure theory: Looking back and moving
ahead. Discourse Studies, 8(3):423?459, June.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28:409?445.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255, June.
25
Recognizing Textual Parallelisms with edit distance and similarity degree
Marie Gue?gan and Nicolas Hernandez
LIMSI-CNRS
Universite? de Paris-Sud, France
guegan@aist.enst.fr | hernandez@limsi.fr
Abstract
Detection of discourse structure is crucial
in many text-based applications. This pa-
per presents an original framework for de-
scribing textual parallelism which allows
us to generalize various discourse phe-
nomena and to propose a unique method
to recognize them. With this prospect, we
discuss several methods in order to iden-
tify the most appropriate one for the prob-
lem, and evaluate them based on a manu-
ally annotated corpus.
1 Introduction
Detection of discourse structure is crucial in many
text-based applications such as Information Re-
trieval, Question-Answering, Text Browsing, etc.
Thanks to a discourse structure one can precisely
point out an information, provide it a local context,
situate it globally, link it to others.
The context of our research is to improve au-
tomatic discourse analysis. A key feature of the
most popular discourse theories (RST (Mann and
Thompson, 1987), SDRT (Asher, 1993), etc.) is
the distinction between two sorts of discourse re-
lations or rhetorical functions: the subordinating
and the coordinating relations (some parts of a
text play a subordinate role relative to other parts,
while some others have equal importance).
In this paper, we focus our attention on a dis-
course feature we assume supporting coordination
relations, namely the Textual Parallelism. Based
on psycholinguistics studies (Dubey et al, 2005),
our intuition is that similarities concerning the sur-
face, the content and the structure of textual units
can be a way for authors to explicit their intention
to consider these units with the same rhetorical im-
portance.
Parallelism can be encountered in many specific
discourse structures such as continuity in infor-
mation structure (Kruijff-Korbayova? and Kruijff,
1996), frame structures (Charolles, 1997), VP el-
lipses (Hobbs and Kehler, 1997), headings (Sum-
mers, 1998), enumerations (Luc et al, 1999), etc.
These phenomena are usually treated mostly inde-
pendently within individual systems with ad-hoc
resource developments.
In this work, we argue that, depending on de-
scription granularity we can proceed, computing
syntagmatic (succession axis of linguistic units)
and paradigmatic (substitution axis) similarities
between units can allow us to generically handle
such discourse structural phenomena. Section 2
introduces the discourse parallelism phenomenon.
Section 3 develops three methods we implemented
to detect it: a similarity degree measure, a string
editing distance (Wagner and Fischer, 1974) and a
tree editing distance1 (Zhang and Shasha, 1989).
Section 4 discusses and evaluates these methods
and their relevance. The final section reviews re-
lated work.
2 Textual parallelism
Our notion of parallelism is based on similarities
between syntagmatic and paradigmatic represen-
tations of (constituents of) textual units. These
similarities concern various dimensions from shal-
low to deeper description: layout, typography,
morphology, lexicon, syntax, and semantics. This
account is not limited to the semantic dimension
as defined by (Hobbs and Kehler, 1997) who con-
sider text fragments as parallel if the same predi-
cate can be inferred from them with coreferential
or similar pairs of arguments.
1For all measures, elementary units considered are syn-
tactic tags and word tokens.
281
We observe parallelism at various structural lev-
els of text: among heading structures, VP ellipses
and others, enumerations of noun phrases in a
sentence, enumerations with or without markers
such as frame introducers (e.g. ?In France, . . . In
Italy, . . . ?) or typographical and layout markers.
The underlying assumption is that parallelism be-
tween some textual units accounts for a rhetorical
coordination relation. It means that these units can
be regarded as equally important.
By describing textual units in a two-tier frame-
work composed of a paradigmatic level and syn-
tagmatic level, we argue that, depending on the
description granularity we consider (potentially at
the character level for item numbering), we can
detect a wide variety of parallelism phenomena.
Among parallelism properties, we note that the
parallelism of a given number of textual units is
based on the parallelism of their constituents. We
also note that certain semantic classes of con-
stituents, such as item numbering, are more effec-
tive in marking parallelism than others.
2.1 An example of parallelism
The following example is extracted from our cor-
pus (see section 4.1). In this case, we have an enu-
meration without explicit markers.
For the purposes of chaining, each type of link
between WordNet synsets is assigned a direction
of up, down, or horizontal.
Upward links correspond to generalization: for
example, an upward link from apple to fruit indi-
cates that fruit is more general than apple.
Downward links correspond to specialization:
for example, a link from fruit to apple would have
a downward direction.
Horizontal links are very specific specializations.
The parallelism pattern of the first two items is de-
scribed as follows:
[JJ + suff =ward] links correspond to [NN + suff
= alization] : for example , X link from Y to Z .
This pattern indicates that several item con-
stituents can be concerned by parallelism and that
similarities can be observed at the typographic,
lexical and syntactic description levels. Tokens
(words or punctuation marks) having identical
shallow descriptions are written in italics. The
X, Y and Z variables stand for matching any non-
parallel text areas between contiguous parallel tex-
tual units. Some words are parallel based on
their syntactic category (?JJ? / adjectives, ?NN? /
nouns) or suffix specifications (?suff? attribute).
The third item is similar to the first two items but
with a simpler pattern:
JJ links U [NN + suff =alization] W .
Parallelism is distinguished by these types of sim-
ilarities between sentences.
3 Methods
Three methods were used in this study. Given a
pair of sentences, they all produce a score of sim-
ilarity between these sentences. We first present
the preprocessing to be performed on the texts.
3.1 Prior processing applied on the texts
The texts were automatically cut into sentences.
The first two steps hereinafter have been applied
for all the methods. The last third was not applied
for the tree editing distance (see 3.3). Punctua-
tion marks and syntactic labels were henceforward
considered as words.
1. Text homogenization: lemmatization together
with a semantic standardization. Lexical chains
are built using WordNet relations, then words are
replaced by their most representative synonym:
Horizontal links are specific specializations.
horizontal connection be specific specialization .
2. Syntactic analysis by (Charniak, 1997)?s parser:
( S1 ( S ( NP ( JJ Horizontal ) (NNS links ) ( VP
( AUX are) ( NP ( ADJP ( JJ specific ) ( NNS
specializations ) ( SENT .)))))))
3. Syntactic structure flattening:
S1 S NP JJ Horizontal NNS links VP AUX are
NP ADJP JJ specific NNS specializations SENT.
3.2 Wagner & Fischer?s string edit distance
This method is based on Wagner & Fischer?s
string edit distance algorithm (Wagner and Fis-
cher, 1974), applied to sentences viewed as strings
of words. It computes a sentence edit distance, us-
ing edit operations on these elementary entities.
The idea is to use edit operations to transform
sentence S1 into S2. Similarly to (Wagner and Fis-
cher, 1974), we considered three edit operations:
1. replacing word x ? S1 by y ? S2: (x ? y)
2. deleting word x ? S1: (x ? ?)
3. inserting word y ? S2 into S1: (? ? y)
By definition, the cost of a sequence of edit op-
erations is the sum of the costs2 of the elementary
2We used unitary costs in this study
282
operations, and the distance between S1 and S2 is
the cost of the least cost transformation of S1 into
S2. Wagner & Fischer?s method provides a simple
and effective way (O(|S1||S2|)) to compute it. To
reduce size effects, we normalized by |S1|+|S2|2 .
3.3 Zhang & Shasha?s algorithm
Zhang & Shasha?s method (Zhang and Shasha,
1989; Dulucq and Tichit, 2003) generalizes Wag-
ner & Fischer?s edit distance to trees: given two
trees T1 and T2, it computes the least-cost se-
quence of edit operations that transforms T1 into
T2. Elementary operations have unitary costs and
apply to nodes (labels and words in the syntactic
trees). These operations are depicted below: sub-
stitution of node c by node g (top figure), inser-
tion of node d (middle fig.), and deletion of node
d (bottom fig.), each read from left to right.
Tree edit distance d(T1, T2) is determined after
a series of intermediate calculations involving spe-
cial subtrees of T1 and T2, rooted in keyroots.
3.3.1 Keyroots, special subtrees and forests
Given a certain node x, L(x) denotes its left-
most leaf descendant. L is an equivalence rela-
tion over nodes and keyroots (KR) are by definition
the equivalence relation representatives of high-
est postfix index. Special subtrees (SST) are the
subtrees rooted in these keyroots. Consider a tree
T postfix indexed (left figure hereinafter) and its
three SSTs (right figure).
SST(k1) rooted in k1 is denoted:
T [L(k1), L(k1) + 1, . . . , k1]. E.g: SST(3) =
T [1, 2, 3] is the subtree containing nodes a, b, d.
A forest of SST(k1) is defined as:
T [L(k1), L(k1) + 1, . . . , x], where x is a
node of SST(k1). E.g: SST(3) has 3 forests :
T [1] (node a), T [1, 2] (nodes a and b) and itself.
Forests are ordered sequences of subtrees.
3.3.2 An idea of how it works
The algorithm computes the distance between all
pairs of SSTs taken in T1 and T2, rooted in
increasingly-indexed keyroots. In the end, the last
SSTs being the full trees, we have d(T1, T2).
In the main routine, an N1 ? N2 array called
TREEDIST is progressively filled with values
TREEDIST(i, j) equal to the distance between the
subtree rooted in T1?s ith node and the subtree
rooted in T2?s jth node. The bottom right-hand
cell of TREEDIST is therefore equal to d(T1, T2).
Each step of the algorithm determines the edit
distance between two SSTs rooted in keyroots
(k1, k2) ? (T1 ? T2). An array FDIST is ini-
tialized for this step and contains as many lines
and columns as the two given SSTs have nodes.
The array is progressively filled with the distances
between increasing forests of these SSTs, simi-
larly to Wagner & Fischer?s method. The bot-
tom right-hand value of FDIST contains the dis-
tance between the SSTs, which is then stored in
TREEDIST in the appropriate cell. Calculations
in FDIST and TREEDIST rely on the double re-
currence formula depicted below:
The first formula is used to compute the dis-
tance between two forests (a white one and a black
one), each of which is composed of several trees.
The small circles stand for the nodes of highest
postfix index. Distance between two forests is de-
fined as the minimum cost operation between three
possibilities: replacing the rightmost white tree by
the rightmost black tree, deleting the white node,
or inserting the black node.
The second formula is analogous to the first one,
in the special case where the forests are reduced to
a single tree. The distance is defined as the mini-
mum cost operation between: replacing the white
node with the black node, deleting the white node,
or inserting the black node.
283
It is important to notice that the first formula
takes the left context of the considered subtrees
into account3 : ancestor and left sibling orders are
preserved. It is not possible to replace the white
node with the black node directly, the whole sub-
tree rooted in the white node has to be replaced.
The good thing is, the cost of this operation has
already been computed and stored in TREEDIST.
Let?s see why all the computations required at a
given step of the recurrence formula have already
been calculated. Let two SSTs of T1 and T2 be
rooted in pos1 and pos2. Considering the symme-
try of the problem, let?s only consider what hap-
pens with T1. When filling FDIST(pos1, pos2),
all nodes belonging to SST(pos1) are run through,
according to increasing postfix indexes. Consider
x ? T [L(pos1), . . . , pos1]:
If L(x) = L(pos1), then x belongs to the left-
most branch of T [L(pos1), . . . , pos1] and forest
T [L(pos1), . . . , x] is reduced to a single tree. By
construction, all FDIST(T [L(pos1), . . . , y],?) for
y ? x ? 1 have already been computed. If things
are the same for the current node in SST(pos2),
then TREEDIST(T [L(pos1), . . . , x],?) can be
calculated directly, using the appropriate FDIST
values previously computed.
If L(x) 6= L(pos1), then x does not belong
to the leftmost branch of T [L(pos1), . . . , pos1]
and therefore x has a non-empty left context
T [L(pos1), . . . , L(x)?1]. Let?s see why comput-
ing FDIST(T [L(pos1), . . . , x],?) requires values
which have been previously obtained.
? If x is a keyroot, since the algorithm
runs through keyroots by increasing order,
TREEDIST(T [L(x), . . . , x],?) has already
been computed.
? If x is not a keyroot, then there exists a node
z such that : x < z < pos1, z is a keyroot
and L(z) = L(x). Therefore x belongs to
the leftmost branch of T [L(z), . . . , z], which
means TREEDIST(T [L(z), . . . , x],?) has
already been computed.
Complexity for this algorithm is :
O(|T1| ? |T2| ? min(p(T1), f(T1)) ? min(p(T2), f(T2)))
where d(Ti) is the depth Ti and f(Ti) is the num-
ber of terminal nodes of Ti.
3The 2nd formula does too, since left context is empty.
3.4 Our proposal: a degree of similarity
This final method computes a degree of similar-
ity between two sentences, considered as lists of
syntactic (labels) and lexical (words) constituents.
Because some constituents are more likely to in-
dicate parallelism than others (e.g: the list item
marker is more pertinent than the determiner ?a?),
a crescent weight function p(x) ? [0, 1] w.r.t.
pertinence is assigned to all lexical and syntac-
tic constituents x. A set of special subsentences
is then generated: the greatest common divisor of
S1 and S2, gcd(S1, S2), is defined as the longest
list of words common to S1 and S2. Then for
each sentence Si, the set of special subsentences
is computed using the words of gcd(S1, S2) ac-
cording to their order of appearance in Si. For
example, if S1 = cabcad and S2 = acbae,
gcd(S1, S2) = {c, a, b, a}. The set of subsen-
tences for S1 is {caba, abca} and the set for S2 is
reduced to {acba}. Note that any generated sub-
sentence is exactly the size of gcd(S1, S2).
For any two subsentences s1 and s2, we define
a degree of similarity D(s1, s2), inspired from
string edit distances:
D(s1, s2) =
n
X
i=1
?
dmax ? d(xi)
dmax
? p(xi)
?
8
>
>
>
>
<
>
>
>
>
>
:
n size of all subsentences
xi ith constituent of s1
dmax max possible dist. between any xi ? s1 and its
parallel constituent in s2, i.e. dmax = n ? 1
d(xi) distance between current constituent xi
in s1 and its parallel constituent in s2
p(xi) parallelism weight of xi
The further a constituent from s1 is from its
symmetric occurrence in s2, the more similar
the compared subsentences are. Eventually, the
degree of similarity between sentences S1 and S2
is defined as:
D(S1, S2) =
2
|S1| + |S2|
? max
s1,s2
D(s1, s2)
Example
Consider S1 = cabcad and S2 = acbae, along
with their subsentences s1 = caba and s?1 = abca
for S1, and s2 = acba for S2. The degrees of
parallelism between s1 and s2, and between s?1
and s2 are computed. The mapping between the
parallel constituents is shown below.
284
For example:
D(s1, s2) =
4
X
i=1
?3 ? d(xi)
3 ? p(xi)
?
= 2/3p(c) + 2/3p(a) + p(b) + p(a)
Assume p(b) = p(c) = 12 and p(a) = 1. Then
D(s1, s2) = 2.5 and, similarly D(s?1, s2) ' 2.67.
Therefore the normalized degree of parallelism is
D(S1, S2) = 25+6 ? 2.67, which is about 0.48.
4 Evaluation
This section describes the methodology employed
to evaluate performances. Then, after a prelimi-
nary study of our corpus, results are presented suc-
cessively for each method. Finally, the behavior of
the methods is analyzed at sentence level.
4.1 Methodology
Our parallelism detection is an unsupervised clus-
tering application: given a set of pairs of sen-
tences, it automatically classifies them into the
class of the parallelisms and the remainders
class. Pairs were extracted from 5 scientific ar-
ticles written in English, each containing about
200 sentences: Green (ACL?98), Kan (Kan et
al. WVLC?98), Mitkov (Coling-ACL?98), Oakes
(IRSG?99) and Sand (Sanderson et al SIGIR?99).
The idea was to compute for each pair a paral-
lelism score indicating the similarity between the
sentences. Then the choice of a threshold deter-
mined which pairs showed a score high enough to
be classified as parallel.
Evaluation was based on a manual annotation
we proceeded over the texts. In order to reduce
computational complexity, we only considered the
parallelism occurring between consecutive sen-
tences. For each sentence, we indicated the index
of its parallel sentence. We assumed transitivity of
parallelism : if S1//S2 and S2//S3, then S1//S3.
It was thus considered sufficient to indicate the in-
dex of S1 for S2 and the index of S2 for S3 to
account for a parallelism between S1, S2 and S3.
We annotated pairs of sentences where textual
parallelism led us to rhetorically coordinate them.
The decision was sometimes hard to make. Yet
we annotated it each time to get more data and to
study the behavior of the methods on these exam-
ples, possibly penalizing our applications. In the
end, 103 pairs were annotated.
We used the notions of precision (correctness)
and recall (completeness). Because efforts in im-
proving one often result in degrading the other,
the F-measure (harmonic mean) combines them
into a unique parameter, which simplifies compar-
isons of results. Let P be the set of the annotated
parallelisms and Q the set of the pairs automati-
cally classified in the parallelisms after the use of
a threshold. Then the associated precision p, recall
r and F-measure f are defined as:
p = |P ?Q||Q| r =
|P ?Q|
|P | f =
2
1/p + 1/q
As we said, the unique task of the implemented
methods was to assign parallelism scores to pairs
of sentences, which are collected in a list. We
manually applied various thresholds to the list
and computed their corresponding F-measure. We
kept as a performance indicator the best F-measure
found. This was performed for each method and
on each text, as well as on the texts all gathered
together.
4.2 Preliminary corpus study
This paragraph underlines some of the character-
istics of the corpus, in particular the distribution of
the annotated parallelisms in the texts for adjacent
sentences. The following table gives the percent-
age of parallelisms for each text:
Parallelisms Nb of pairs
Green 39 (14.4 %) 270
Kan 12 (6 %) 200
Mitkov 13 (8.4 %) 168
Oakes 22 (13.7 %) 161
Sand 17 (7.7 %) 239
All gathered 103 (9.9 %) 1038
Green and Oakes show significantly more paral-
lelisms than the other texts. Therefore, if we con-
sider a lazy method that would put all pairs in the
class of parallelisms, Green and Oakes will yield
a priori better results. Precision is indeed directly
related to the percentage of parallelisms in the text.
In this case, it is exactly this percentage, and it
gives us a minimum value of the F-measure our
methods should at least reach:
Precision Recall F-measure
Green 14.4 100 25.1
Kan 6 100 11.3
Mitkov 8.4 100 15.5
Oakes 13.7 100 24.1
Sand 7.7 100 14.3
All 9.9 100 18.0
4.3 A baseline: counting words in common
We first present the results of a very simple and
thus very fast method. This baseline counts the
285
words sentences S1 and S2 have in common, and
normalizes the result by |S1|+|S2|2 in order to re-
duce size effects. No syntactic analysis nor lexical
homogenization was performed on the texts.
Results for this method are summarized in the fol-
lowing table. The last column shows the loss (%)
in F-measure after applying a generic threshold
(the optimal threshold found when all texts are
gathered together) on each text.
F-meas. Prec. Recall Thres. Loss
Green 45 34 67 0.4 2
Kan 24 40 17 0.9 10
Mitkov 22 13 77 0.0 8
Oakes 45 78 32 0.8 7
Sand 23 17 35 0.5 1
All 30 23 42 0.5 -
We first note that results are twice as good as
with the lazy approach, with Green and Oakes
far above the rest. Yet this is not sufficient for a
real application. Furthermore, the optimal thresh-
old is very different from one text to another,
which makes the learning of a generic threshold
able to detect parallelisms for any text impossible.
The only advantage here is the simplicity of the
method: no prior treatment was performed on the
texts before the search, and the counting itself was
very fast.
4.4 String edit distance
We present the results for the 1st method below:
F-meas. Prec. Recall Thres. Loss
Green 52 79 38 0.69 0
Kan 44 67 33 0.64 2
Mitkov 38 50 31 0.69 0
Oakes 82 94 73 0.68 0
Sand 47 54 42 0.72 9
All 54 73 43 0.69 -
Green and Oakes still yield the best results, but
the other texts have almost doubled theirs. Results
for Oakes are especially good: an F-measure of
82% guaranties high precision and recall.
In addition, the use of a generic threshold on
each text had little influence on the value of the
F-measure. The greatest loss is for Sand and only
corresponds to the adjunction of four pairs of sen-
tences in the class of parallelisms. The selection of
a unique generic threshold to predict parallelisms
should therefore be possible.
4.5 Tree edit distance
The algorithm was applied using unitary edit
costs. Since it did not seem natural to establish
mappings between different levels of the sentence,
edit operations between two constituents of dif-
ferent nature (e.g: substitution of a lexical by a
syntactic element) were forbidden by a prohibitive
cost (1000). However, this banning only improved
the results shyly, unfortunately.
F-meas. Prec. Recall Thres. Loss
Green 46 92 31 0.72 3
Kan 44 67 33 0.75 0
Mitkov 43 40 46 0.87 11
Oakes 81 100 68 0.73 0
Sand 52 100 35 0.73 2
All 51 73 39 0.75 -
As illustrated in the table above, results are
comparable to those previously found. We note an
especially good F-measure for Sand: 52%, against
47% for the string edit distance. Optimal thresh-
olds were quite similar from one text to another.
4.6 Degree of similarity
Because of the high complexity of this method, a
heuristic was applied. The generation of the sub-
sentences is indeed in
?Ckini , ki being the number
of occurrences of the constituent xi in gcd, and
ni the number of xi in the sentence. We chose
to limit the generation to a fixed amount of sub-
sentences. The constituents that have a great Ckini
bring too much complexity: we chose to eliminate
their (ni ? ki) last occurrences and to keep their
ki first occurrences only to generate subsequences.
An experiment was conducted in order to
determine the maximum amount of subsentences
that could be generated in a reasonable amount of
time without significant performance loss and 30
was a sufficient number. In another experiment,
different parallelism weights were assigned to
lexical constituents and syntactic labels. The aim
was to understand their relative importance for
parallelisms detection. Results show that lexical
constituents have a significant role, but conclu-
sions are more difficult to draw for syntactic
labels. It was decided that, from now on, the lex-
ical weight should be given the maximum value, 1.
Finally, we assigned different weights to the
syntactic labels. Weights were chosen after count-
ing the occurrences of the labels in the corpus. In
fact, we counted for each label the percentage of
occurrences that appeared in the gcd of the paral-
lelisms with respect to those appearing in the gcd
of the other pairs. Percentages were then rescaled
from 0 to 1, in order to emphasize differences
286
between labels. The obtained parallelism values
measured the role of the labels in the detection of
parallelism. Results for this experiment appear in
the table below.
F-meas. Prec. Recall Thres. Loss
Green 55 59 51 0.329 2
Kan 47 80 33 0.354 5
Mitkov 35 40 31 0.355 0
Oakes 76 80 73 0.324 4
Sand 29 20 59 0.271 0
All 50 59 43 0.335 -
The optimal F-measures were comparable to
those obtained in 4.4 and the corresponding
thresholds were similar from one text to another.
This section showed how the three proposed
methods outperformed the baseline. Each of them
yielded comparable results.
The next section presents the results at sentence
level, together with a comparison of these three
methods.
4.7 Analysis at sentence level
The different methods often agreed but sometimes
reacted quite differently.
Well retrieved parallelisms
Some parallelisms were found by each method
with no difficulty: they were given a high degree
of parallelism by each method. Typically, such
sentences presented a strong lexical and syntactic
similarity, as in the example in section 2.
Parallelisms hard to find
Other parallelisms received very low scores
from each method. This happened when the an-
notated parallelism was lexically and syntactically
poor and needed either contextual information or
external semantic knowledge to find keywords
(e.g: ?first?, ?second?, . . . ), paraphrases or pat-
terns (e.g: ?X:Y? in the following example (Kan)):
Rear: a paragraph in which a link just stopped
occurring the paragraph before.
No link: any remaining paragraphs.
Different methods, different results
Eventually, we present some parallelisms that
obtained very different scores, depending on the
method.
First, it seems that a different ordering of the
parallel constituents in the sentences alter the per-
formances of the edit distance algorithms (3.2;
3.3). The following example (Green) received a
low score with both methods:
When we consider AnsV as our dependent vari-
able, the model for the High Web group is still
not significant, and there is still a high probabil-
ity that the coefficient of LI is 0.
For our Low Web group, who followed signif-
icantly more intra-article links than the High
Web group, the model that results is significant
and has the following equation: <EQN/>.
This is due to the fact that both algorithms do not
allow the inversion of two constituents and thus
are unable to find all the links from the first sen-
tence to the other. The parallelism measure is ro-
bust to inversion.
Sometimes, the syntactic parser gave different
analyses for the same expression, which made
mapping between the sentences containing this ex-
pression more difficult, especially for the tree edit
distance. The syntactic structure has less impor-
tance for the other methods, which are thus more
insensitive to an incorrect analysis.
Finally, the parallelism measure seems more
adapted to a diffuse distribution of the parallel
constituents in the sentences, whereas edit dis-
tances seem more appropriate when parallel con-
stituents are concentrated in a certain part of the
sentences, in similar syntactic structures. The fol-
lowing example (Green) obtained very high scores
with the edit distances only:
Strong relations are also said to exist between
words that have synsets connected by a single
horizontal link or words that have synsets con-
nected by a single IS-A or INCLUDES relation.
A regular relation is said to exist between two
words when there is at least one allowable path
between a synset containing the first word and a
synset containing the second word in the Word-
Net database.
5 Related work
Experimental work in psycholinguistics has
shown the importance of the parallelism effect in
human language processing. Due to some kind
of priming (syntactic, phonetic, lexical, etc.), the
comprehension and the production of a parallel ut-
terance is made faster (Dubey et al, 2005).
So far, most of the works were led in order to
acquire resources and to build systems to retrieve
specific parallelism phenomena. In the field of in-
formation structure theories, (Kruijff-Korbayova?
and Kruijff, 1996) implemented an ad-hoc system
287
to identify thematic continuity (lexical relation be-
tween the subject parts of consecutive sentences).
(Luc et al, 1999) described and classified markers
(lexical clues, layout and typography) occurring in
enumeration structures. (Summers, 1998) also de-
scribed the markers required for retrieving head-
ing structures. (Charolles, 1997) was involved in
the description of frame introducers.
Integration of specialized resources dedicated
to parallelism detection could be an improvement
to our approach. Let us not forget that our fi-
nal aim remains the detection of discourse struc-
tures. Parallelism should be considered as an ad-
ditional feature which among other discourse fea-
tures (e.g. connectors).
Regarding the use of parallelism, (Hernandez
and Grau, 2005) proposed an algorithm to parse
the discourse structure and to select pairs of sen-
tences to compare.
Confronted to the problem of determining tex-
tual entailment4 (the fact that the meaning of
one expression can be inferred from another)
(Kouylekov and Magnini, 2005) applied the
(Zhang and Shasha, 1989)?s algorithm on the de-
pendency trees of pairs of sentences (they did not
consider syntactic tags as nodes but only words).
They encountered problems similar to ours due to
pre-treatment limits. Indeed, the syntactic parser
sometimes represents in a different way occur-
rences of similar expressions, making it harder to
apply edit transformations. A drawback concern-
ing the tree-edit distance approach is that it is not
able to observe the whole tree, but only the subtree
of the processed node.
6 Conclusion
Textual parallelism plays an important role among
discourse features when detecting discourse struc-
tures. So far, only occurrences of this phenomenon
have been treated individually and often in an ad-
hoc manner. Our contribution is a unifying frame-
work which can be used for automatic processing
with much less specific knowledge than dedicated
techniques.
In addition, we discussed and evaluated several
methods to retrieve them generically. We showed
that simple methods such as (Wagner and Fis-
cher, 1974) can compete with more complex ap-
proaches, such as our degree of similarity and the
4Compared to entailment, the parallelism relation is bi-
directional and not restricted to semantic similarities.
(Zhang and Shasha, 1989)?s algorithm.
Among future works, it seems that variations
such as the editing cost of transformation for edit
distance methods and the weight of parallel units
(depending their semantic and syntactic charac-
teristics) can be implemented to enhance perfor-
mances. Combining methods also seems an inter-
esting track to follow.
References
Nicholas Asher. 1993. Reference to abstract objects in
discourse. Kluwer, Dordrecht.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI.
M. Charolles. 1997. L?encadrement du discours -
univers, champs, domaines et espaces. Cahier de
recherche linguistique, 6.
Amit Dubey, Patrick Sturt, and Frank Keller. 2005.
Parallelism in coordination as an instance of syntac-
tic priming: Evidence from corpus-based modeling.
In HLTC and CEMNLP, Vancouver.
S. Dulucq and L. Tichit. 2003. RNA Secondary
Structure Comparison: Exact Analysis of the Zhang-
Shasha Tree Edit Algorithm. Theoretical Computer
Science, 306(1-3):471?484.
N. Hernandez and B. Grau. 2005. De?tection au-
tomatique de structures fines du discours. In TALN,
France.
J. R. Hobbs and A. Kehler. 1997. A theory of paral-
lelism and the case of vp ellipsis. In ACL.
M. Kouylekov and B. Magnini. 2005. Recognizing
Textual Entailment with Tree Edit Distance Algo-
rithms. PASCAL Challenges on RTE.
I. Kruijff-Korbayova? and G.-J. M. Kruijff. 1996. Iden-
tification of topic-focus chains. In DAARC, vol-
ume 8, pages 165?179. University of Lancaster, UK.
C. Luc, M. Mojahid, J. Virbel, Cl. Garcia-Debanc, and
M.-P. Pe?ry-Woodley. 1999. A linguistic approach
to some parameters of layout: A study of enumera-
tions. In AAAI, North Falmouth, Massachusets.
W. C. Mann and S. A. Thompson. 1987. Rhetori-
cal structure theory: A theory of text organisation.
Technical report isi/rs-87-190.
K. M. Summers. 1998. Automatic Discovery of Logi-
cal Document Structure. Ph.D. thesis, U. of Cornell.
R.A. Wagner and M.J. Fischer. 1974. The String-to-
String Correction Problem. Journal of the ACM,
21(1):168?173.
K. Zhang and D. Shasha. 1989. Simple fast algo-
rithms for the editing distance between trees and
related problems. SIAM Journal on Computing,
18(6):1245?1262.
288
JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 61?68,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Participation du LINA ? DEFT 2012
Florian Boudin Amir Hazem Nicolas Hernandez Prajol Shrestha
Universit? de Nantes
pr?nom.nom@univ-nantes.fr
R?SUM?
Cet article pr?sente la participation de l??quipe TALN du LINA au d?fi fouille de textes (DEFT)
2012. D?velopp? sp?cifiquement pour la seconde piste du d?fi, notre syst?me combine les sorties
de trois diff?rentes m?thodes d?extraction de mots cl?s. Notre syst?me s?est class? ? la 2i e`me place
sur un total de 9 syst?mes avec une f-mesure de 21,3%.
ABSTRACT
LINA at DEFT 2012
This article presents the participation of the TALN group at LINA to the d?fi fouille de textes
(DEFT) 2012. Developed specifically for the second task, our system combines the outputs of
three different keyword extraction methods. Our system ranked 2nd out of 9 systems with a
f-measure of 21,3%.
MOTS-CL?S : extraction de mots cl?s, deft 2012, combinaison de m?thodes.
KEYWORDS: keyword extraction, deft 2012, combining methods.
1 Introduction
L?indexation automatique consiste ? identifier un ensemble de mots cl?s (e.g. mots, termes) qui
d?crit le contenu d?un document. Les mots cl?s peuvent ensuite ?tre utilis?s, entre autres, pour
faciliter la recherche d?information ou la navigation dans les collections de documents. L??dition
2012 du d?fi fouille de textes (DEFT) porte sur l?extraction automatique de mots cl?s ? partir
d?articles scientifiques parus dans le domaine des Sciences Humaines et Sociales (SHS).
L?objectif du d?fi est de retrouver, ? partir du contenu des documents (i.e. articles scientifiques),
les mots cl?s qui ont pu ?tre choisis par les auteurs. Deux diff?rentes pistes ont ?t? propos?es.
La premi?re piste consiste ? identifier dans une terminologie, les mots cl?s qui ont ?t? assign?s
aux documents. Cette terminologie regroupe l?ensemble des mots cl?s utilis?s dans la collection.
La seconde piste, de prime abord plus complexe, consiste ? extraire les mots cl?s directement ?
partir du contenu des documents. Cet article d?crit notre participation ? la seconde piste du d?fi.
Le reste de cet article est organis? comme suit. La section 2 d?crit l?ensemble de donn?es utilis?
pour la campagne d??valuation. La section 3 pr?sente les diff?rentes m?thodes que nous avons
d?velopp?es sp?cifiquement pour la seconde piste du d?fi. Nous d?crivons ensuite en section
4 nos r?sultats exp?rimentaux avant de pr?senter les m?thodes que nous avons test?es et qui
61
qui ont eu un impact nul ou n?gatif sur les r?sultats. La section 6 conclut cet article et donne
quelques perspectives de travaux futurs.
2 Description de la campagne DEFT 2012
L?ensemble de documents utilis? pour le d?fi 2012 est constitu? de 234 articles scientifiques parus
dans le domaine des SHS. Ces articles ont ?t? publi?s entre 2001 et 2008 dans quatre revues
diff?rentes. L?ensemble d?apprentissage contient 60% des documents (soit 141 articles), et celui
de test contient les 40% restants (soit 93 articles). La r?partition des quatre diff?rentes revues
dans les deux ensembles est uniforme.
Du point de vue technique, les articles sont au format XML. Ils sont structur?s en deux parties : le
r?sum? et le corps de l?article. Chaque article contient ?galement le nombre de mots cl?s indexant
son contenu. Les mots cl?s assign?s ? chaque article sont disponibles pour chacun des articles de
l?ensemble d?entra?nement.
Les syst?mes participant au d?fi sont ?valu?s ? l?aide des mesures classiques de pr?cision, rappel
et f-mesure. Pour chaque article, les mots cl?s g?n?r?s par les syst?mes sont compar?s aux mots
cl?s de r?f?rence (assign?s par les auteurs). Afin de limiter les probl?mes li?s aux diff?rentes
variations orthographiques, plusieurs traitements de normalisation (i.e. normalisation de la casse
et lemmatisation) sont appliqu?s au pr?alable aux mots cl?s. Chaque participant peut soumettre
jusqu?? trois ex?cutions par piste.
La liste ci-dessous pr?sente quelques unes des difficult?s que nous avons identifi?es dans les
articles de l?ensemble d?entra?nement.
? Articles diff?rents ayant le m?me r?sum?, e.g. les articles as_2002_007048ar et as_2002_
007053ar.
? Contenu des articles dans des langues diff?rentes et/ou m?lang?es, e.g. fran?ais et anglais
dans ttr_2008_037494ar, espagnol dans meta_2005_019927ar.
? Contenu des articles tr?s bruit? avec des probl?mes de ponctuation, de caract?res unicodes et
de segmentation en paragraphes, e.g. ci-dessous un extrait de l?article meta_2005_019840ar.
<p>Ce langage est au c.ur des pr?occupations des juristes , qui
nous rappellent r?guli?rement </p>
<p>que le droit est affaire de mots. Et cela dans tout l.univers
du droit , vers quelque c?t? que l.on se</p>
<p>tourne , dans le monde juridique anglophone - o?, pour
Mellinkoff (1963& amp;#x00A0 ;: vii), &amp;#x00A0;The law is a</p>
<p></p>
<p>dans son ensemble , la technique juridique aboutit , pour la
plus grande part , ? une question de</p>
<p>terminologie&amp;# x00A0;. Chacun pourra le v?rifier par la
consultation d.ouvrages parmi les plus r?cents et</p>
62
3 Approches
Les diff?rentes m?thodes que nous avons d?velopp?es utilisent le mot comme unit? principale.
Nous avons donc appliqu? un ensemble commun de pr?-traitements aux documents : segmenta-
tion en phrases, d?coupage en mots et ?tiquetage morpho-syntaxique. L?information structurelle
pr?sente dans chacun des documents (i.e. r?sum?, corps de l?article et paragraphes) est pr?serv?e.
Chaque paragraphe est segment? en phrases en utilisant la m?thode PUNKT de d?tection de
changement de phrases (Kiss et Strunk, 2006) mise en ?uvre dans la bo?te ? outils NLTK (Bird et
Loper, 2004). La tokenisation des phrases est effectu?e avec un outil d?velopp? en interne utili-
sant le lexique des formes fl?chies du fran?ais (lefff)1 pour l?indentification des unit?s lexicales
complexes (e.g. mots compos?s). L??tiquetage morpho-syntaxique est obtenu ? l?aide du Stanford
POS Tagger (Toutanova et al, 2003)2 entrain?e sur le French Treebank (Abeill? et al, 2003).
3.1 Syst?me 1
Ce syst?me est bas? sur du T F? IDF et trois r?gles issues du corpus d?apprentissage. La principale
question qui se pose ici est : qu?est ce qu?un mot cl? ? ou autrement dit, qu?est ce qui fait qu?un
terme a plus de chances d??tre un mot cl? qu?un autre ?
En analysant les documents du corpus d?apprentissage, nous avons relev? trois particularit?s li?es
aux mots cl?s. La premi?re concerne leur localisation dans les documents. Chaque document ?tant
divis? en deux parties qui sont : le r?sum? (ABSTRACT) et le corps du document (BODY), nous
nous sommes donc int?ress?s ? la position des mots cl?s par rapport ? ce d?coupage. Nous avons
pu constater qu?un terme apparaissant ? la fois dans le r?sum? et dans le corps du document avait
plus de chances d??tre un mot cl?. Ainsi, nous avons utilis? cette information comme premi?re
r?gle de notre syst?me (nous appellerons cette r?gle : R1). Deux strat?gies utilisant cette r?gle
ont ?t? adopt?es, la premi?re consiste ? ne s?lectionner que des termes qui apparaissent ? la
fois dans le r?sum? et dans le corps du document (nous appellerons cette strat?gie : S1), la
deuxi?me consiste ? donner la priorit? aux termes respectant la strat?gie S1 en utilisant une
pond?ration par un param?tre ? fix? empiriquement (nous appellerons cette strat?gie : S2).
Les diff?rents tests conduits ont montr? que l?utilisation de la strat?gie S1 donnait de meilleurs
r?sultats que l?utilisation de la strat?gie S2. Intuitivement, nous aurions tendance ? penser le
contraire (la strat?gie S2 devrait ?tre meilleur que S1), car ?liminer des termes n?apparaissant
que dans le r?sum? ou que dans le corps du document nous ferait sans doute perdre des mots
cl?s. L?explication et que la strat?gie S1 corrige sans doute les faiblesses de notre syst?me qui
renverrait plus de faux positifs que de vrais n?gatifs.
La deuxi?me particularit? relative aux n-grammes, d?coule de la question suivante : est ce qu?un
terme simple (1-gramme) a plus de chances d??tre un mot cl? qu?un terme compos? (n-grammes
avec n > 1) ? De part le corpus d?apprentissage nous avons pu constater qu?il y avait 70% de
termes simples et 30% de termes compos?s. Ainsi, nous avons voulu donner une plus grande
importance aux termes simples extraits par notre syst?me. De la m?me mani?re que pour la
strat?gie S2, nous avons introduit un param?tre de pond?ration ? afin de prioriser les termes
simples (nous appellerons cette r?gle R2).
1
http://www.labri.fr/perso/clement/lefff/2Nous utilison la version 3.1.0 avec les param?tres par d?faut.
63
La troisi?me particularit? rel?ve de la simple observation que la quasi totalit? des mots cl?s
?taient soit des noms, soit des adjectifs. ? partir de cette constatation, nous avons introduit une
troisi?me r?gle (R3) qui filtre les verbes.
3.2 Syst?me 2
Ce syst?me repose sur l?exploitation d?un existant ? savoir l?approche KEA (Keyphrase Extraction
Algorithm) de (Witten et al, 1999). KEA permet d?une part de mod?liser les expressions significa-
tives (compos?es d?un ou plusieurs mots) du contenu de textes ? l?aide de textes et d?expressions
cl?s associ?es et d?autre part d?extraire les expressions cl?s d?une collection de textes ? l?aide
d?une mod?lisation construite a priori. L?approche utilise un classifieur bay?sien na?f pour calculer
un score de probabilit? de chaque expression cl? candidate. La construction requiert un ensemble
d?expressions cl?s class?es positivement pour chaque texte du corpus d?apprentissage. L?extraction
se r?alise sur un corpus de domaine similaire au domaine du corpus d?apprentissage.
Les phases de mod?lisation ou d?extraction des expressions cl?s fonctionnent toutes deux ? la
suite de deux phases ?l?mentaires : l?extraction de candidats et le calcul de traits descriptifs
des candidats. Les candidats s?obtiennent par extraction de n-grammes de taille pr?d?finie ne
d?butant pas et ne finissant pas par un mot outil.
Les traits utilis?s pour d?crire chaque candidat au sein d?un document sont les suivants : le
T F ? IDF (mesure de sp?cificit? du candidat pour le document), la position de la premi?re
occurrence (pourcentage du texte pr?c?dent l?occurrence), le nombre de mots qui compose le
candidat. Les candidats ayant un haut T F ? IDF , apparaissant au d?but d?un texte et comptant
le plus de mots sont ainsi consid?r?s comme ?tant de bons descripteurs du contenu d?un texte.
Les derni?res ?volutions de KEA permettent d?exploiter des lexiques contr?l?s de type th?saurus
dans la construction de la mod?lisation (Medelyan et Witten, 2006).
Nous n?avons pas exploit? de ressources ext?rieures de type lexiques contr?l?s dans la construction
de notre mod?lisation. Nous avons utilis? la version 5.0 de l?impl?mentation de KEA3 disponible
sous licence GNU ; en pratique nous avons utilis? les fonctionnalit?s d?extraction ?libre? pr?sentes
d?s la version 3.0. Les fonctionnalit?s d?velopp?es ult?rieurement concernent l?exploitation de
lexiques contr?l?s. Nos candidats ?taient au maximum de taille 5. Nous avons exploit? le corpus
d?apprentissage fourni pour la seconde piste pour construire notre mod?lisation. Chaque texte
(r?sum? et corps) a ?t? consid?r? comme une unit? documentaire.
L?approche KEA est facilement portable ? diff?rentes langues du fait qu?elle n?cessite peu de
ressources. En particulier elle ne requiert pas une pr?-analyse syntaxique pour s?lectionner des
candidats. Nous avons n?anmoins port? une certaine attention ? nos traitements pr?liminaires
et nous avons constat? qu?une pr?-segmentation en token mots ainsi que l?utilisation d?une liste
multilingue de mots outils augmentaient la qualit? de l?extraction des expressions cl?s lorsque
nous ?valuions l?approche par validation crois?e sur le corpus d?apprentissage. Concernant la
liste des mots outils, nous avons fusionn? les listes fournies par KEA pour le fran?ais, l?anglais et
l?espagnol. Nous l?avons compl?t?e des formes des mots outils pouvant subir une ?lision du e final
en fran?ais (e.g. ?le? s?est vu compl?t? de la forme ?l??, de m?me pour ?lorsque? avec ?lorsqu??. . .).
Ces formes ?taient en effet reconnues par notre segmenteur en mots.
3
http://www.nzdl.org/Kea
64
3.3 Syst?me 3
Ce syst?me est bas? sur une approche par classification supervis?e. La t?che d?extraction de mots
cl?s est ici consid?r?e comme une t?che de classification binaire. La premi?re ?tape consiste ?
g?n?rer tous les mots cl?s candidats ? partir du document. Pour ce faire, nous commen?ons par
extraire tous les n-grammes de mots jusqu?? n = 4. Des contraintes syntaxiques sont ensuite
utilis?es pour filtrer les candidats. Ainsi, seuls les n-grammes compos?s uniquement de noms,
d?adjectifs et de mots outils (except? en premier/dernier mot du n-gramme) sont gard?s.
Pour chaque candidat, nous calculons les traits suivants :
? Poids T F ? IDF
? Nombre de mots du n-gramme
? Patron syntaxique du n-gramme (e.g. ?Nom Adjectif?)
? Position relative de la premi?re occurence dans le document
? Section(s) o? apparait le n-gramme (r?sum?, corps ou les deux)
? Nombre de documents de la collection dans lesquels le n-gramme apparait
? Score de saillance dans l?arbre de d?pendances de coh?sion lexicale du texte (voir ci-dessous)
Nous construisons ce que nous appelons un ?arbre de d?pendances de coh?sion lexicale? selon
une approche d?crite par Choi ? la section 6.3.1. de sa th?se (Choi, 2002). Une d?pendance
est pr?suppos?e exister entre deux phrases cons?cutives si celles-ci ont des mots en commun ;
l?hypoth?se est de consid?rer la seconde phrase comme une ?laboration de la premi?re. En
pratique, notre algorithme ne reconna?t pas syst?matiquement une relation de d?pendance
entre deux phrases cons?cutives qui partagent des mots en commun. En effet notre algorithme
recherche, pour chaque phrase du texte, la phrase la plus haute dans la cha?ne de d?pendance
de la phrase pr?c?dente avec laquelle elle partage des mots en commun. L?arbre est construit
en prenant le texte dans son ensemble (r?sum? et corps) pr?alablement lemmatis?. Un score de
saillance est calcul? pour chaque phrase en fonction du nombre de ses d?pendances (directes
et transitives) normalis? par le nombre de d?pendances maximal qu?une phrase peut avoir sur
le texte donn?. Chaque expression candidate h?rite alors du score de la phrase o? apparait sa
premi?re occurence.
Nous utilisons la combinaison par vote de trois algorithmes de classification disponibles dans la
boite ? outils Weka (Hall et al, 2009) : NaiveBayes, J48 et RandomForest. Les mots-cl?s candidats
sont ensuite tri?s selon leurs scores de pr?diction.
3.4 Combinaison des syst?mes
Les trois syst?mes que nous avons d?velopp?s utilisent diff?rentes m?thodes pour capturer
l?importance d?un mot cl? par rapport ? un document. Une combinaison des sorties de ces
derniers est donc pertinente.
Nous disposons pour chaque document, de trois listes pond?r?es de mots cl?s. La m?thode la
plus simple consisterait ? utiliser la somme des scores des trois syst?mes. Cependant, les scores
calcul?s par chacun des syst?mes ne sont pas directement comparables. ? la place du score, nous
utilisons pour chaque mot cl? candidat, l?inverse de son rang dans la liste ordonn?e.
Deux strat?gies de combinaison ont ?t? utilis?es. La premi?re, COMBI1 consiste ? assigner la
65
somme de l?inverse des rangs d?un mot cl? dans les listes ordonn?es des trois syst?mes. Pour la
seconde strat?gie, COMBI2, nous ne consid?rons que les mots cl?s apparaissant dans les sorties
des trois syst?mes. L?id?e est de filtrer les mots cl?s consid?r?s comme important par seulement
un ou deux des trois syst?mes.
4 R?sultats
Nous pr?sentons dans cette section les r?sultats officiels de la campagne DEFT 2012. Nous
avons soumis trois ex?cutions pour chacune des deux pistes. Pour la premi?re piste, nous avons
simplement utilis? le Syst?me 1 (d?crit dans la section 3.1) et filtr? les mots cl?s candidats ? l?aide
de la terminologie. Le nombre de mots cl?s retourn?s est fix? ? 7 pour la premi?re ex?cution et ?
6 pour les deux autres. Les trois configurations utilisent la r?gle R3.
La premi?re ex?cution utilise la r?gle R2 (? = 0,6). La seconde ex?cution utilise la r?gle R2
(? = 0, 65). La troisi?me ex?cution utilise la r?gle R1 avec la strat?gie S2 (?= 0, 65) et la r?gle
R2 (? = 0,65). Pour la seconde piste, nous avons soumis les ex?cutions de deux combinaisons
(COMBI1 et COMBI2) ainsi que du syst?me 3 (d?crit dans la section 3.3). Le nombre de mots cl?s
retourn?s est fix? ? 130% du nombre de mots cl?s de r?f?rence pour COMBI1 et COMBI2 et ?
110% pour le syst?me 3. Ces nombres permettent d?obtenir les meilleurs r?sultats sur l?ensemble
d?entra?nement.
La table 1 pr?sente les r?sultats de nos trois ex?cutions pour la premi?re piste. Les r?sultats
obtenus par les trois ex?cutions sont moins bons que ceux obtenus sur l?ensemble d?entrainement
(f-mesure=0,44 pour la premi?re ex?cution). Nous constatons que la variation du rappel sur les
trois ex?cutions est faible. La chute de la pr?cision pour la troisi?me ex?cution s?explique par
l?application de la r?gle R1 qui limite le nombre de candidats possibles.
Syst?me Pr?cision Rappel f-mesure
1 0,3812 0,4004 0,3906
2 0,3759 0,3948 0,3851
3 0,3343 0,4097 0,3682
TAB. 1 ? R?sultats de nos trois ex?cutions pour la premi?re piste.
La table 2 montre les r?sultats de nos trois ex?cutions pour la seconde piste. Nous pouvons voir
que la performance de COMBI2 est largement en dessous de COMBI1. Nous avions constat? le
ph?nom?ne inverse sur les donn?es d?entra?nement. Ceci est du au fait que le nombre de mots
cl?s retourn?s par COMBI2 peut dans certains cas ?tre inf?rieur au seuil que nous avons fix?. En
effet, l?intersection des listes des 100 meilleurs mots cl?s candidats de chaque syst?me est tr?s
restrainte pour quelque uns des documents de l?ensemble de test. Nous constatons que les scores
du syst?me 3, ayant obtenu les meilleurs r?sultats sur l?ensemble d?entra?nement parmis nos trois
syst?mes, sont faibles en comparaison des deux combinaisons. Ce r?sultat semble indiquer un
probl?me de sur-entra?nement et illustre bien l?utilit? de la combinaison.
La table 3 pr?sente, pour chacune des deux pistes, le classement des diff?rentes ?quipes sur la
base de la meilleure soumission. Notre soumission est class?e au rang 5 sur 10 pour la premi?re
66
Syst?me Pr?cision Rappel f-mesure
COMBI1 0,1949 0,2355 0,2133
COMBI2 0,1788 0,2128 0,1943
Syst?me 3 0,1643 0,1880 0,1753
TAB. 2 ? R?sultats de nos trois ex?cutions pour la seconde piste.
piste et au rang 2 sur 9 pour la seconde piste. Les r?sultats obtenus par l??quipe 16 sont bien
au dessus de toutes les autres ?quipes et montrent qu?une marge de progression importante est
possible pour notre syst?me.
Rang Piste 1 Piste 2
1 ?quipe 16 (0,9488) ?quipe 16 (0,5874)
2 ?quipe 05 (0,7475) ?quipe 06 (0,2133)
3 ?quipe 04 (0,4417) ?quipe 05 (0,2087)
4 ?quipe 02 (0,3985) ?quipe 02 (0,1921)
5 ?quipe 06 (0,3906) ?quipe 01 (0,1901)
6 ?quipe 01 (0,2737) ?quipe 13 (0,1632)
7 ?quipe 13 (0,1378) ?quipe 04 (0,1270)
8 ?quipe 17 (0,1079) ?quipe 17 (0,0895)
9 ?quipe 03 (0,0857) ?quipe 03 (0,0785)
10 ?quipe 18 (0,0428) -
TAB. 3 ? Classement de DEFT 2012 sur la base de la meilleure soumission de chaque ?quipe pour
chacune des deux pistes. Notre classement est indiqu? en gras (?quipe 06).
5 Ce qui n?a pas march?
Nous d?crivons ici les m?thodes qui ont eu un impact nul ou n?gatif sur les r?sultats.
Traits ayant un impact n?gatif sur la performance du syst?me 3 : la dispersion d?un mot cl?
dans le document, mots appartenant ? des phrases contenant des citations, noms des auteurs les
plus cit?s dans le document (sp?cifique aux articles commen?ant par ?as?).
Suppression de la redondance : nous avons constat? un niveau de redondance important
des mots cl?s dans les sorties de nos syst?mes. Par exemple, les mots cl?s ?jardins collectifs?,
?jardins? et ?collectifs? sont tous les trois pr?sents dans le top 10, ce qui fait baisser le rappel.
Plusieurs strat?gies ont ?t? exp?riment?es pour supprimer cette redondance (e.g. suppression
d?un n-gramme si tous les mots qui le composent sont ?galement pr?sents parmi les 10 meilleurs
candidats). Une d?gradation des r?sultats est cependant observ?e indiquant que la strat?gie ?
adopter est d?pendante des documents.
Mod?le de pond?ration ? base de graphe : nous avons impl?ment? l?approche propos?e
dans (Mihalcea et Tarau, 2004). Il s?agit de repr?senter chaque document sous la forme d?un
67
graphe de mots connect?s par des relations de co-occurrences. Des algorithmes de centralit? sont
ensuite appliqu?s pour extraire les mots les plus caract?ristiques. Les r?sultats obtenus par cette
m?thode sont inf?rieurs ? ceux obtenus ? l?aide d?une pond?ration par la mesure T F ? IDF .
6 Conclusions
Nous avons d?crit la participation du LINA ? DEFT 2012. Notre syst?me est le r?sultat de la
combinaison des sorties de trois diff?rentes m?thodes d?extraction de mots cl?s. Les r?sultats
obtenus par ce dernier sont toujours meilleurs que ceux obtenus par chacune des trois m?thodes
individuellement. Pour la seconde piste, notre syst?me s?est class? ? la 2i e`me place sur un total de
9 syst?mes avec une f-mesure de 21,3%.
La strat?gie que nous avons employ?e pour combiner les sorties des diff?rentes m?thodes n?est
cependant pas optimale. Nous envisageons d??tendre ce travail en proposant d?autres strat?gies
comme par exemple l?utilisation d?un meta-classifieur.
R?f?rences
ABEILL?, A., CL?MENT, L. et TOUSSENEL, F. (2003). Building a treebank for French. Treebanks :
building and using parsed corpora, pages 165?188.
BIRD, S. et LOPER, E. (2004). NLTK : The natural language toolkit. In ACL, Barcelone, Espagne.
CHOI, F. Y. Y. (2002). Content-based Text Navigation. Th?se de doctorat, Department of Computer
Science, University of Manchester.
HALL, M., FRANK, E., HOLMES, G., PFAHRINGER, B., REUTEMANN, P. et WITTEN, I. (2009). The weka
data mining software : an update. ACM SIGKDD Explorations Newsletter, 11(1):10?18.
KISS, T. et STRUNK, J. (2006). Unsupervised multilingual sentence boundary detection. Compu-
tational Linguistics, 32(4):485?525.
MEDELYAN, O. et WITTEN, I. H. (2006). Thesaurus based automatic keyphrase indexing. In
Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, JCDL ?06, pages 296?297,
New York, NY, USA. ACM.
MIHALCEA, R. et TARAU, P. (2004). Textrank : Bringing order into texts. In LIN, D. et WU,
D., ?diteurs : Proceedings of EMNLP 2004, pages 404?411, Barcelona, Spain. Association for
Computational Linguistics.
TOUTANOVA, K., KLEIN, D., MANNING, C. et SINGER, Y. (2003). Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the 3rd Conference of the North American
Chapter of the ACL (NAACL 2003), pages 173?180. Association for Computational Linguistics.
WITTEN, I. H., PAYNTER, G. W., FRANK, E., GUTWIN, C. et NEVILL-MANNING, C. G. (1999). Kea :
Practical automatic keyphrase extraction. CoRR, cs.DL/9902007.
68
LAW VIII - The 8th Linguistic Annotation Workshop, pages 110?119,
Dublin, Ireland, August 23-24 2014.
Exploiting the human computational effort dedicated to message reply
formatting for training discursive email segmenters
Nicolas Hernandez Soufian Salim
LINA UMR 6241 Laboratory
University of Nantes (France)
firstname.lastname@univ-nantes.fr
Abstract
In the context of multi-domain and multimodal online asynchronous discussion analysis, we
propose an innovative strategy for manual annotation of dialog act (DA) segments. The process
aims at supporting the analysis of messages in terms of DA. Our objective is to train a sequence
labelling system to detect the segment boundaries. The originality of the proposed approach
is to avoid manually annotating the training data and instead exploit the human computational
efforts dedicated to message reply formatting when the writer replies to a message by inserting
his response just after the quoted text appropriate to his intervention. We describe the approach,
propose a new electronic mail corpus and report the evaluation of segmentation models we built.
1 Introduction
Automatic processing of online conversations (forum, emails) is a highly important issue for the indus-
trial and the scientific communities which care to improve existing question/answering systems, identify
emotions or intentions in customer requests or reviews, detect messages containing requests for action
or unsolved severe problems. . .
In most works, conversation interactions between the participants are modelled in terms of dialogue
acts (DA) (Austin, 1962). The DAs describe the communicative function conveyed by each text utterance
(e.g. question, answer, greeting,. . . ). In this paper, we address the problem of rhetorically segmenting
the new content parts of messages in online asynchronous discussions. The process aims at supporting
the analysis of messages in terms of DA. We pay special attention to the processing of electronic mails.
The main trend in automatic DA recognition consists in using supervised learning algorithms to predict
the DA conveyed by a sentence or a message (Tavafi et al., 2013). The hypothesized message segmenta-
tion results from the global analysis of these individual predictions over each sentence. A first remark on
this paradigm is that it is not realistic to use in the context of multi-domain and multimodal processing
because it requires the building of training data which is a very substantial and time-consuming task. A
second remark is that the model does not have a fine-grained representation of the message structure or
the relations between messages. Considering such characteristics could drastically improve the systems
to allow to focus on specific text parts or to filter out less relevant ones. Indeed, apart from the closing
formula, a message may for example be made of several distinct information requests, the description of
an unsuccessful procedure, the quote of third-party messages. . .
So far, few works address the problem of message segmentation. (Lampert et al., 2009a) propose to
segment emails in prototypical zones such as the author?s contribution, quotes of original messages, the
signature, the opening and closing formulas. In comparison, we focus on the segmentation of the author?s
contribution (what we call the new content part). (Joty et al., 2013) identifies clusters of topically related
sentences through the multiple messages of a thread, without distinguishing email and forum messages.
Apart from the topical aspect, our problem differs because we are only interested in the cohesion between
sentences in nearby fragments and not on distant sentences.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
110
[Hi!]
S1
[I got my ubuntu cds today and i?m really impressed.]
S2
[My
friends like them and my teachers too (i?m a student).]
S3
[It?s really funny to see, how people like ubuntu and start feeling geek
and blaming microsoft when they use it.]
S4
[Unfortunately everyone wants an ubuntu cd, so can i download the cd
covers anywhere or an ?official document? which i can attach to
self-burned cds?]
S5
[I searched the entire web site but found nothing.]
S6
[Thanks in
advance.]
S7
[John]
S8
(a) Original message.
[On Sun, 04 Dec 2005, John Doe <john@doe.com> wrote:]
R1
> [I got my ubuntu cds today and i?m really impressed.]
R2
[My
> friends like them and my teachers too (i?m a student).]
R3
> [It?s really funny to see, how people like ubuntu and start feeling geek
> and blaming microsoft when they use it.]
R4
[Rock!]
R5
> [Unfortunately everyone wants an ubuntu cd, so can i download the cd
> covers anywhere or an ?official document? which i can attach to
> self-burned cds?]
R6
[We don?t have any for the warty release, but we will have them for
hoary, because quite a few people have asked. :-)]
R7
[Bob.]
R8
(b) Reply message.
Figure 1: An original message and its reply (ubuntu-users email
archive). Sentences have been tagged to facilitate the discussion.
Original Reply Label
S1
R1
S2 > R2 Start
S3 > R3 Inside
S4 > R4 End
R5
S5 > R6 Start&End
R7
[...]
S6
[...]
Figure 2: Alignment of the sen-
tences from the original and
reply messages shown in Fig-
ure 1 and labels inferred from
the re-use of the original mes-
sage text. Labels are associated
to the original sentences.
Despite the drawbacks mentioned above, a supervised approach remains the most efficient and reliable
method to solve classification problems in Natural Language Processing. Our aim is to train a system to
detect the segment boundaries, i.e. to determine, through a classification approach, if a given sentence
starts, ends or continues a segment.
The originality of the proposed approach is to avoid manually annotating the training data and instead
to exploit the human computational efforts dedicated to a similar task in a different context of produc-
tion (von Ahn, 2006). As recommended by the Netiquette
1
, when replying to a message (email or forum
post), the writer should ?summarize the original message at the top of its reply, or include (or "quote")
just enough text of the original to give a context, in order to make sure readers understand when they
start to read the response
2
.? As a corollary, the writer should ?edit out all the irrelevant material.? Our
idea is to use this effort, in particular when the writer replies to a message by inserting his response or
comment just after the quoted text appropriate to his intervention. This posting style is called interleaved
or inline replying. The so built segmentation model should be usable for any posting styles by applying it
only on new content parts. Figure 1a shows an example of an original message and, Figure 1b, one of its
reply. We can see that the reply message re-uses only four selected sentences from the original message;
namely S2, S3, S4 and S5 which respectively correspond to sentences R2, R3, R4 and R6 in the reply
message. The author of the reply message deliberately discarded the remaining of the original message.
The segment build up by sentences S2, S3, S4 and the one by the single sentence S5 can respectively
be associated with two acts : a comment and a question.
In Section 2, we explain our approach for building an annotated corpus of segmented online messages
at no cost. In Section 3, we describe the system and the features we use to model the segmentation. After
1
Set of guidelines for Network Etiquette (Netiquette) when using network communication or information services RFC1855.
2
It is true that some email software clients do not conform to the recommendations of Netiquette and that some online
participants are less sensitive to arguments about posting style (many writers reply above the original message). We assume
that there are enough messages with inline replying available to build our training data.
111
presenting our experimental framework in Section 4, we report some evaluations for the segmentation
task in Section 5. Finally, we discuss our approach in comparison to other works in Section 6.
2 Building annotated corpora of segmented online discussions at no cost
We present the assumptions and the detailed steps of our approach.
2.1 Annotation scheme
The basic idea is to interpret the operation performed by a discussion participant on the message he
replies as an annotation operation. Assumptions about the kind of annotations depend on the operation
that has been performed. Deletion or re-use of the original text material can give hints about the relevance
of the content: discarded material is probably less relevant than re-used one.
We assume that by replying inside a message and by only including some specific parts, the partic-
ipant performs some cognitive operations to identify homogeneous self-contained text segments. Con-
sequently, we make some assumptions about the role played by the sentences in the original message
information structure. A sentence in a segment plays one of the following roles: starting and ending
(SE) a segment when there is only one sentence in the segment, starting (S) a segment if there are at
least two sentences in the segment and it is the first one, ending (E) a segment if there are at least two
sentences in the segment and it is the last one, inside (I) a segment in any other cases.
Figure 2 illustrates the scheme by showing how sentences from Figure 1 can be aligned and the labels
inferred from it. It is similar to the BIO scheme except it is not at the token level but at the sentence level
(Ratinov and Roth, 2009).
2.2 Annotation generation procedure
Before being able to predict labels of the original message sentences, it is necessary to identify those that
are re-used in a reply message. Identification of the quoted lines in a reply message is not sufficient for
various reasons. First, the segmenter is intended to work on non-noisy data (i.e. the new content parts
in the messages) while a quoted message is an altered version of the original one. Indeed, some email
software clients involved in the discussion are not always standards-compliant and totally compatible
3
.
In particular, the quoted parts can be wrongly re-encoded at each exchange step due to the absence of
dedicated header information. In addition, the client programs can integrate their own mechanisms for
quoting the previous messages when including them as well as for wrapping too long lines
4
. Second,
accessing the original message may allow taking some contextual features into consideration (like the
visual layout for example). Third, to go further, the original context of the extracted text also conveys
some segmentation information. For instance, a sentence from the original message, not present in the
reply, but following an aligned sentence, can be considered as starting a segment.
So in addition to identifying the quoted lines, we deploy an alignment procedure to get the original
version of the quoted text. In this paper, we do not consider the contextual features from the original
message and focus only on sentences that have been aligned.
The generation procedure is intended to "automatically" annotate sentences from the original messages
with segmentation information. The procedure follows the following steps:
1. Messages posted in the interleaved replying style are identified
2. For each pair of original and reply messages:
(a) Both messages are tokenized at sentence and at word levels
(b) Quoted lines in the reply message are identified
(c) Sentences which are part of the quoted text in the reply message are identified
3
The Request for Comments (RFC) are guidelines and protocols proposed by working groups involved in the Internet
Standardization https://tools.ietf.org/html, the message contents suffer from encoding and decoding problems.
Some of the RFC are dedicated to email format and encoding specifications (See RFC 2822 and 5335 as starting points). There
have been several propositions with updates and consequently obsoleted versions which may explain some alteration issues.
4
Feature for making the text readable without any horizontal scrolling by splitting lines into pieces of about 80 characters.
112
(d) Sentences in the original message are aligned with quoted text in the reply message
5
(e) Aligned original sentences are labelled in terms of position in segment
(f) The sequence of labelled sentences is added to the training data
Messages with inline replying are recognized thanks to the presence of at least two consecutive quoted
lines separated by new content lines. Pairs of original and reply messages are constituted based on the
in-reply-to field present in the email headers. As declared in the RFC 3676
6
, we consider as quoted
lines, the lines beginning with the ">" (greater than) sign. Lines which are not quoted lines are considered
to be new content lines. The word tokens are used to index the quoted lines and the sentences.
Labelling of aligned sentence (sentence from the original message re-used in the reply message) uses
this simple rule-based algorithm:
For each aligned original sentence:
if the sentence is surrounded by new content in the reply message, the label is Start&End
else if the sentence is preceded by a new content, the label is Start
else if the sentence is followed by a new content, the label is End
else, the label is Inside
2.3 Alignment module
For finding alignments between two given text messages, we use a dynamic programming (DP) string
alignment algorithm (Sankoff and Kruskal, 1983). In the context of speech recognition, the algorithm
is also known as the NIST align/scoring algorithm. Indeed, it is widely used to evaluate the output of
speech recognition systems by comparing the hypothesized text output by the speech recognizer to the
correct, or reference text. The algorithm works by ?performing a global minimization of a Levenshtein
distance function which weights the cost of correct words, insertions, deletions and substitutions as 0,
75, 75 and 100 respectively. The computational complexity of DP is O(MN).?
The Carnegie Mellon University provides an implementation of the algorithm in its speech recognition
toolkit
7
. We use an adaptation of it which allows working on lists of strings
8
rather than directly on
strings (as sequences of characters).
3 Building the segmenter
Each email is processed as a sequence of sentences. We choose to define the segmentation problem as a
sequence labelling task whose aim is to assign the globally best set of labels for the entire sequence at
once. The underlying idea is that the choice of the optimal label for a given sentence is dependent on
the choices of nearby sentences. Our email segmenter is built around a linear-chain Conditional Random
Field (CRF), as implemented in the sequence labelling toolkit Wapiti (Lavergne et al., 2010).
Training the classifier to recognize the different labels of the previously defined annotation scheme can
be problematic. It has indeed some disadvantages that can undermine the effectiveness of the classifier.
In particular, sentences annotated SE will, by definition, share important characteristics with sentences
bearing the annotation S and E. So we chose to transform these annotations into a binary scheme and
merely differentiate sentences that starts a new segment (True), or "boundary sentences", from those that
do not (False). The conversion process is trivial, and can easily be reversed
9
.
We distinguish four sets of features: n-gram features, information structure based features, thematic
features and miscellaneous features. All the features are domain-independent. Almost all features are
language-independent as well, save for a few that can be easily translated. For our experiments, the
CRF window size is set at 5, i.e. the classification algorithm takes into account features of the next and
previous two sentences as well as the current one.
5
Section 2.3 details how alignment is performed.
6
http://www.ietf.org/rfc/rfc3676.txt
7
Sphinx 4 edu.cmu.sphinx.util.NISTAlign http://cmusphinx.sourceforge.net
8
https://github.com/romanows/WordSequenceAligner
9
Sentences labelled with SE or S are turned into True, the other ones into False. To reverse the process, a True is turned into
SE if the next sentence is also a boundary (i.e. a True) and into S otherwise. While a False is turned into E if the next sentence
is a boundary (i.e. a True) and into I otherwise.
113
n-gram features We select the case-insensitive word bi-grams and tri-grams with the highest docu-
ment frequency in the training data (empirically we select the top 1,000 n-grams), and check for their
presence in each sentence. Since the probability of having multiple occurrences of the same n-gram in
one sentence are extremely low, we do not record the number of occurrences but merely a boolean value.
Information structure based features This feature set is inspired by the information structure theory
(Kruijff-Korbayov? and Kruijff, 1996) which describes the information imparted by the sentence in terms
of the way it is related to prior context. The theory relates these functions with particular syntactic
constructions (e.g. topicalization) and word order constraints in the sentence.
We focus on the first and last three significant tokens in the sentence. A token is considered as sig-
nificant if its occurrence frequency is higher than 1/2,000
10
. As features we use n-grams of the surface
form, lemma and part-of-speech tag of each triplet (36 features).
Thematic feature The only feature we use to account for thematic shift recognition is the output of
the TextTiling algorithm (Hearst, 1997). TextTiling is one of the most commonly used algorithms for
automatic text segmentation. If the algorithm detects a rupture in the lexical cohesion of the text (between
two consecutive blocks), it will place a boundary to indicate a thematic change. Due to the short size of
the messages, we define a block size to equate the sum of three times the sentence average size in our
corpus. We set the step-size (overlap size of the rolling window) to the average size of a sentence.
Miscellaneous features This feature set includes stylistic and semantic features. 24 features, several
of them borrowed from related work in speech act classification (Qadir and Riloff, 2011) and email
segmentation (Lampert et al., 2009b), are in the set: Stylistic features capture information about the
visual structure and composition of the message: the position of the sentence in the email, the average
length of a token, the total number of tokens and characters, the proportion of upper-case, alphabetic and
numeric characters, the number of greater-than signs (?>?); whether the sentence ends with or contains
a question mark, a colon or a semicolon; whether the sentence contains any punctuation within the first
three tokens (this is meant to recognize greetings (Qadir and Riloff, 2011)).
Semantic features check for meaningful words and phrases: whether the sentence begins with or con-
tains a ?wh*? question word or a phrase suggesting an incoming interrogation (e.g. ?is it?, ?are there?);
whether the sentence contains a modal; whether any plan phrases (e.g. ?i will?, ?we are going to?)
are present; whether the sentence contains first person (e.g. ?we?, ?my?) second person or third person
words; the first personal pronoun found in the sentence; the first verbal form found.
4 Experimental framework
We describe the data, the preprocessing and the evaluation protocol we use for our experiments.
4.1 Corpus
The current work takes place in a project dealing with multilingual and multimodal discussion process-
ing, mainly in interrogative technical domains. For these reasons we did not consider the Enron Corpus
(30,000 threads) (Klimt and Yang, 2004) (which is from a corporate environment), neither the W3C
Corpus (despite its technical consistence) or its subset, the British Columbia Conversation Corpus (BC3)
(Ulrich et al., 2008).
We rather use the ubuntu-users email archive
11
as our primary corpus. It offers a number of advan-
tages. It is free, and distributed under an unrestrictive license. It increases continuously, and therefore
is representative of modern emailing in both content and formatting. Additionally, many alternatives
archives are available, in a number of different languages, including some very resource-poor languages.
Ubuntu also offers a forum and a FAQ which are interesting in the context of multimodal studies.
We use a copy of December 2013. The corpus contains a total of 272,380 messages (47,044 threads).
33,915 of them are posted in the inline replying style that we are interested in. These messages are made
10
This value was set up empirically on our data. More experimentation needs to be done to generalize it.
11
Ubuntu mailing lists archives (See ubuntu-users): https://lists.ubuntu.com/archives/
114
of 418,858 sentences, themselves constituted of 76,326 unique tokens (5,139,123 total). 87,950 of these
lines (21%) are automatically labelled by our system as the start of a new segment (either SE or S).
4.2 Evaluation protocol
In order to evaluate the efficiency of the segmenter, we perform a 10-fold cross-validation on the Ubuntu
corpus, and compare its performance to two different baselines. The first one, the ?regular? baseline,
is computed by segmenting the test set into regular segments of the same length as the average training
set segment length, rounded up. The second one is the TextTiling algorithm we described in section 3.
While it is used as a feature in the proposed approach in the previous section, the direct output of the
TextTiling algorithm is used for the baseline.
The results are measured with a panel of metrics used in text segmentation and Information Retrieval
(IR). Precision (P ) and Recall (R) are provided for all results. P is the percentage of boundaries iden-
tified by the classifier that are indeed true boundaries. R is the percentage of true boundaries that are
identified by the classifier. We also provide the harmonic mean of precision and recall: F
1
= 2 ?
P ?R
P+R
However, automatic evaluation of speech segmentation through these metrics is problematic as pre-
dicted segment boundaries seldom align precisely. Therefore, we also provide an array of metrics rele-
vant to the field of text segmentation : P
k
, WindowDiff and the Generalized Hamming Distance (GHD).
The P
k
metric is a probabilistically motivated error metric for the assessment of segmentation algo-
rithms (Beeferman et al., 1999). WindowDiff compares the number of segment boundaries found within
a fixed-sized window to the number of boundaries found in the same window of text for the reference
segmentation (Pevzner and Hearst, 2002). The GHD is an extension of the Hamming distance
12
that
gives partial credit for near misses (Bookstein et al., 2002).
4.3 Preprocessing
To reduce noise in the corpus we filter out undesirable emails based on several criteria, the first of which is
encoding. Messages that are not UTF-8 encoded are removed from the selection. The second criterion is
MIME type: we keep single-part plain text messages only, and remove those with HTML or other special
contents. In addition, we choose to consider only replies to thread starters. This choice is based on the
assumption that the alignment module would have more difficulty in recognizing properly sentences that
were repeatedly transformed in successive replies. Indeed, these replies - that would contain quoted text
from other messages - would be more likely to be poorly labelled through automatic annotation. The last
criterion is length. The dataset being built from a mailing list that can cover very technical discussions,
users sometimes send very lengthy messages containing many lines of copied-and-pasted code, software
logs, bash command outputs, etc. The number of these messages is marginal, but their lengths being
disproportionately high, they can have a negative impact on the segmenter?s performance. We therefore
exclude messages longer than the average message length plus the standard length deviation. After
filtering, the dataset is left with 6,821 messages out of 33,915 (20%).
For building the segmenter features, we use the Stanford Part-Of-Speech Tagger for morpho-syntactic
tagging (Toutanova et al., 2003), and the WordNet lexical database for lemmatization (Miller, 1995).
5 Experiments
Table 1 shows the summary of all obtained results. On the left side are shown results about segmentation
metrics, on the right side results about information retrieval metrics. First, we examine baseline scores,
and display them in the top section. Second, in the middle section, we show results for segmenters based
on individual feature sets (with A standing for n-grams, B for information structure, C for TextTiling
and D for miscellaneous features). Finally, in the lower section, we show results based on feature sets
combinations.
12
Wikipedia article on the Hamming distance: http://en.wikipedia.org/wiki/Hamming_distance
115
Segmentation metrics Information Retrieval metrics
WD P
k
GHD P R F
1
regular baseline .59 .25 .60 .31 .49 .38
TextTiling baseline .41 .07 .38 .75 .44 .56
?(A) with A = n-grams .38 .05 .39 1 .39 .56
?(B) with B = info. structure .43 .11 .38 .60 .68 .64
?(C) with C = TextTiling .39 .05 .38 .94 .40 .56
?(D) with D = misc. features .41 .09 .38 .69 .49 .57
?(A+B + C +D) .38 .05 .39 1 .39 .56
?(?(A) + ?(B) + ?(C) + ?(D)) .38 .06 .36 .81 .47 .59
?(A) ? ?(B + C +D) .45 .12 .40 .58 .69 .63
?(A) ? ?(?(B + C +D)) .36 .06 .34 .80 .53 .64
Table 1: Comparative results between baselines and tested segmenters. All displayed results show Win-
dowDiff (WD), P
k
and GHD as error rates, therefore a lower score is desirable for these metrics. This
contrasts with the three IR scores, for which a low value denotes poor performance. Best scores are
shown in bold.
5.1 Baseline segmenters
The first section of Table 1 shows the results obtained by both of our baselines. Unsurprisingly, TextTil-
ing performs much better than the basic regular segmentation algorithm across all metrics save recall.
5.2 Segmenters based on individual feature sets
The second section of Table 1 shows the results for four different classifiers, each trained with a distinct
subset of the feature set. The ? function is the classification function, its parameters are features, and
its output a prediction. While all classifiers easily beat the regular baseline, and match the TextTiling
baseline when it comes to IR metrics, only the thematic and the n-grams segmenters manage to surpass
TextTiling when performance is measured by segmentation metrics. In terms of IR scores, the n-grams
classifier in particular stands out as it manages to achieve an outstanding 100% precision, although this
result is mitigated by a meager 39% recall. It is also interesting to see that the thematic classifier, based
only on contextual information about TextTiling output, performs better than the TextTiling baseline.
5.3 Segmenters based on feature sets combinations
The last section of Table 1 shows the results of four different segmenters. The first one, ?(A+B+C+D),
is a simple classifier that takes all available features into account. Its results are exactly identical to that
of the n-grams classifier, most certainly due to the fact that other features are filtered out due to the
sheer number of lexical features. The second one, ?(?(A) + ?(B) + ?(C) + ?(D)), uses as features
the outputs of the four classifiers trained on each individual feature set. Results show this approach isn?t
significantly better. The third one, ?(A) ? ?(B + C + D), segments according to the union of the
boundaries detected by a classifier trained on n-grams features and those identified by a classifier trained
on all other features. This idea is motivated by the fact that we know all boundaries found by the n-grams
classifier to be accurate (P = 1). Doing this allows the segmenter to obtain the best possible recall
(R = .69), but at the expense of precision (P = .58). The last one, ?(A) ? ?(?(B +C +D)), attempts
to increase the n-grams classifier?s recall without sacrificing too much precision by being more selective
about boundaries. The ? function is the "cherry picking" function, which filters out boundaries predicted
without sufficient confidence. Only those identified by the n-grams classifier and those classified as
boundaries with a confidence score of at least .99 by a classifier trained on the other feature sets are
considered. This system outperforms all others both in terms of segmentation scores and F
1
, however it
is still relatively conservative and the segmentation ratio (the number of guessed boundaries divided by
the number of true boundaries) remains significantly lower than expected, at 0.67. Tuning the minimum
116
confidence score (c) allows to adjust P from .58 (c = 0) to 1 (c = 1) and R from .39 (c = 1) to .69 (c = 0).
6 Related work
Three research areas are directly related to our study: a) collaborative approaches for acquiring annotated
corpora, b) detection of email structure, and c) sentence alignment. In the (Wang et al., 2013)?s taxonomy
of the collaborative approaches for acquiring annotated corpora, our approach could be related to the
Wisdom of the Crowds (WotC) genre where motivators are altruism or prestige to collaborate for the
building of a public resource. As a major difference, we did not initiate the annotation process and
consequently we did not define annotation guidelines, design tasks or develop tools for annotating which
are always problematic questions. We have just rerouted a posteriori the result of an existing task which
was performed in a distinct context. In our case the burning issue is to determine the adequacy of our
segmentation task. Our work is motivated by the need to identify important snippets of information in
messages for applications such as being able to determine whether all the aspects of a customer request
were fully considered. We argue that even if it is not always obvious to tag topically or rhetorically a
segment, the fact that it was a human who actually segmented the message ensures its quality. We think
that our approach can also be used for determining the relevance of the segments, however it has some
limits, and we do not know how labelling segments with dialogue acts may help us do so.
Detecting the structure of a thread is a hot topic. As mentioned in Section 1, very little works have been
done on email segmentation. We are aware of recent works in linear text segmentation such as (Kazant-
seva and Szpakowicz, 2011) who addresses the problem by modelling the text as a graph of sentences
and by performing clustering and/or cut methods. Due to the size of the messages (and consequently
the available lexical material), it is not always possible to exploit this kind of method. However, our
results tend to indicate that we should investigate in this direction nonetheless. By detecting sub-units of
information within the message, our work may complement the works of (Wang et al., 2011; Kim et al.,
2010) who propose solutions for detecting links between messages. We may extend these approaches by
considering the possibility of pointing from/to multiple message sources/targets.
Concerning the alignment process, our task can be compared to the detection of monolingual text
derivation (otherwise called plagiarism, near?duplication, revision). (Poulard et al., 2011) compare, for
instance, the use of n?grams overlap with the use of text hapax. In contrast, we already know that a text
(the reply message) derives from another (the original message). Sentence alignment has also been a very
active field of research in statistical machine translation for building parallel corpora. Some methods are
based on sentence length comparison (Gale and Church, 1991), some methods rely on the overlap of rare
words (cognates and named entities) (Enright and Kondrak, 2007). In comparison, in our task, despite
some noise, the compared text includes large parts of material identical to the original text. The kinds of
edit operation in presence (no inversion
13
only deletion, insertion and substitution) lead us to consider
the Levenshtein distance as a serious option.
7 Future work
The main contribution of this work is to exploit the human effort dedicated to reply formatting for training
discursive email segmenters. We have implemented and tested various segmenter models. There is still
room for improvement, but our results indicate that the approach merits more thorough examination.
Our segmentation approach remains relatively simple and can be easily extended. One way would be
to consider contextual features in order to characterize the sentences in the original message structure.
As future works, we plan to complete our current experiments with two new approaches for evaluation.
The first one will consists in comparing the automatic segmentation with those performed by human
annotators. This task remains tedious since it will then be necessary to define an annotation protocol,
write guidelines and build other resources. The second evaluation we plan to perform is an extrinsic
evaluation. The idea will be to measure the contribution of the segmentation in the process of detecting
the dialogue acts, i.e. to check if existing sentence-level classification systems would perform better with
such contextual information.
13
When computing the Levenshtein distance, the inversion edit operation is the most costly operation.
117
References
John L. Austin. 1962. How to do Things with Words: The William James Lectures delivered at Harvard University
in 1955. Oxford: Clarendon Press.
Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine
learning, 34(1-3):177?210.
Abraham Bookstein, Vladimir A Kulyukin, and Timo Raita. 2002. Generalized hamming distance. Information
Retrieval, 5(4):353?375.
Jessica Enright and Grzegorz Kondrak. 2007. A fast method for parallel document identification. In Human
Language Technologies 2007: The Conference of the North American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Papers, pages 29?32, Rochester, New York, April. Association
for Computational Linguistics.
William A. Gale and Kenneth Ward Church. 1991. A program for aligning sentences in bilingual corpora. In
Proceedings of the 29th Annual Meeting of the Association of Computational Linguistics (ACL).
Marti A Hearst. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational lin-
guistics, 23(1):33?64.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2013. Topic segmentation and labeling in asynchronous
conversations. Journal of AI Research (JAIR), 47:521?573.
Anna Kazantseva and Stan Szpakowicz. 2011. Linear text segmentation using affinity propagation. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages 284?293,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010. Tagging and linking web forum posts. In Proceedings
of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ?10, pages 192?202,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Bryan Klimt and Yiming Yang. 2004. The enron corpus: A new dataset for email classification research. In
Jean-Fran?ois Boulicaut, Floriana Esposito, Fosca Giannotti, and Dino Pedreschi, editors, ECML, volume 3201
of Lecture Notes in Computer Science, pages 217?226. Springer.
Ivana Kruijff-Korbayov? and Geert-Jan M. Kruijff. 1996. Identification of topic-focus chains. In S. Botley,
J. Glass, T. McEnery, and A. Wilson, editors, Approaches to Discourse Anaphora: Proceedings of the Discourse
Anaphora and Anaphora Resolution Colloquium (DAARC96), volume 8, pages 165?179. University Centre for
Computer Corpus Research on Language, University of Lancaster, UK, July 17-18.
Andrew Lampert, Robert Dale, and C?cile Paris. 2009a. Segmenting email message text into zones. In Proceed-
ings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2,
EMNLP ?09, pages 919?928, Stroudsburg, PA, USA. Association for Computational Linguistics.
Andrew Lampert, Robert Dale, and C?cile Paris. 2009b. Segmenting email message text into zones. In Proceed-
ings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages
919?928. Association for Computational Linguistics.
Thomas Lavergne, Olivier Capp?, and Fran?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513. Association for
Computational Linguistics, July.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Lev Pevzner and Marti A Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation.
Computational Linguistics, 28(1):19?36.
Fabien Poulard, Nicolas Hernandez, and B?atrice Daille. 2011. Detecting derivatives using specific and invariant
descriptors. Polibits, (43):7?13.
Ashequl Qadir and Ellen Riloff. 2011. Classifying sentences as speech acts in message board posts. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, pages 748?758. Association for
Computational Linguistics.
L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, 6.
118
D Sankoff and J B Kruskal. 1983. Time Warps, String Edits, and Macromolecules: The Theory and Practice
of Sequence Comparison. Addison-Wesley Publishing Company, Inc., Reading, Massachusetts. ISBN 0-201-
07809-0.
Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2013. Dialogue act recogni-
tion in synchronous and asynchronous conversations. In Proceedings of the 14th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIGDIAL 2013), SIGDIAL?13.
Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages
173?180. Association for Computational Linguistics.
J. Ulrich, G. Murray, and G. Carenini. 2008. A publicly available annotated corpus for supervised email summa-
rization. In AAAI08 EMAIL Workshop, Chicago, USA. AAAI.
L. von Ahn. 2006. Games with a purpose. Computer, 39(6):92?94.
Li Wang, Diana Mccarthy, and Timothy Baldwin. 2011. Predicting thread linking structure by lexical chaining.
In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 76?85, Canberra,
Australia, December.
Aobo Wang, CongDuyVu Hoang, and Min-Yen Kan. 2013. Perspectives on crowdsourcing annotations for natural
language processing. Language Resources and Evaluation, 47(1):9?31.
119
