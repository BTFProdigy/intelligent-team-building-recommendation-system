 Using Knowledge to Facilitate Factoid Answer Pinpointing 
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran  
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
USA 
{hovy,ulf,cyl,ravichan}@isi.edu 
 
Abstract 
In order to answer factoid questions, the 
Webclopedia QA system employs a 
range of knowledge resources.  These 
include a QA Typology with answer 
patterns, WordNet, information about 
typical numerical answer ranges, and 
semantic relations identified by a robust 
parser, to filter out likely-looking but 
wrong candidate answers.  This paper 
describes the knowledge resources and 
their impact on system performance. 
1.   Introduction 
The TREC evaluations of QA systems 
(Voorhees, 1999) require answers to be drawn 
from a given source corpus.  Early QA systems 
used a simple filtering technique, question word 
density within a fixed n-word window, to 
pinpoint answers.  Robust though this may be, 
the window method is not accurate enough.  In 
response, factoid question answering systems 
have evolved into two types:  
? Use-Knowledge: extract query words from 
the input question, perform IR against the 
source corpus, possibly segment resulting 
documents, identify a set of segments 
containing likely answers, apply a set of 
heuristics that each consults a different 
source of knowledge to score each 
candidate, rank them, and select the best 
(Harabagiu et al, 2001; Hovy et al, 2001; 
Srihari and Li, 2000; Abney et al, 2000).  
? Use-the-Web: extract query words from the 
question, perform IR against the web, 
extract likely answer-bearing sentences, 
canonicalize the results, and select the most 
frequent answer(s).  Then, for justification, 
locate examples of the answers in the source 
corpus (Brill et al, 2001; Buchholz, 2001).  
Of course, these techniques can be combined: 
the popularity ratings from Use-the-Web can 
also be applied as a filtering criterion (Clarke et 
al., 2001), or the knowledge resource heuristics 
can filter the web results.  However, simply 
going to the web without using further 
knowledge (Brill et al, 2001) may return the 
web?s majority opinions on astrology, the killers 
of JFK, the cancerous effects of microwave 
ovens, etc.?fun but not altogether trustworthy.   
In this paper we describe the range of 
filtering techniques our system Webclopedia 
applies, from simplest to most sophisticated, and 
indicate their impact on the system.   
2.   Webclopedia Architecture  
As shown in Figure 1, Webclopedia adopts the 
Use-Knowledge architecture. Its modules are 
described in more detail in (Hovy et al, 2001; 
Hovy et al, 1999):  
? Question parsing: Using BBN?s 
IdentiFinder (Bikel et al, 1999), the 
CONTEX parser (Hermjakob, 1997) 
produces a syntactic-semantic analysis of 
the question and determines the QA type.   
? Query formation: Single- and multi-word 
units (content words) are extracted from the 
analysis, and WordNet synsets (Fellbaum, 
1998) are used for query expansion.  A 
series of Boolean queries of decreasing 
specificity is formed.  
? IR: The publicly available IR engine MG 
(Witten et al, 1994) returns the top-ranked 
N documents.  
 ? Selecting and ranking sentences: For each 
document, the most promising K sentences 
are located and scored using a formula that  
 rewards word and phrase overlap with the 
question and its expanded query words.  Results 
are ranked.   
? Parsing candidates: CONTEX parses the 
top-ranked 300 sentences.   
? Pinpointing: As described in Section 3, a 
number of knowledge resources are used to 
perform filtering/pinpointing operations.   
? Ranking of answers: The candidate 
answers? scores are compared and the 
winner(s) are output. 
3. Knowledge Used for Pinpointing 
3.1   Type 1: Question Word Matching 
Unlike (Prager et al, 1999), we do not first 
annotate the source corpus, but perform IR 
directly on the source text, using MG (Witten et 
al., 1994).  To determine goodness, we assign an 
initial base score to each retrieved sentence.  We 
then compare the sentence to the question and 
adapt this score as follows:  
? exact matches of proper names double the 
base score. 
? matching an upper-cased term adds a 60% 
bonus of the base score for multi-words 
terms and 30% for single words (matching 
?United States? is better than just ?United?).  
? matching a WordNet synonym of a term 
discounts by 10% (lower case) and 50% 
(upper case).  (When ?Cage? matches 
?cage?, the former may be the last name of a 
person and the latter an object; the case 
mismatch signals less reliability.)  
? lower-case term matches after Porter 
stemming are discounted 30%; upper-case 
matches 70% (Porter stemming is more 
aggressive than WordNet stemming).  
? Porter stemmer matches of both question 
and sentence words with lower case are 
discounted 60%; with upper case, 80%.  
? if CONTEX indicates a term as being 
qsubsumed (see Section 3.9) the term is 
discouned 90% (in ?Which country 
manufactures weapons of mass 
destruction??, ?country? will be marked as 
qsubsumed).   
The top-scoring 300 sentences are passed on for 
further filtering.   
3.2  Type 2: Qtargets, the QA Typology, 
and the Semantic Ontology 
We classify desired answers by their semantic 
type, which have been taxonomized in the 
Webclopedia QA Typology (Hovy et al, 2002), 
Candidate answer parsing
? Steps: parse sentences
? Engines: CONTEX
Matching
? Steps: match general constraint patterns against parse trees
             match desired semantic type against parse tree elements
             assign score to words in sliding window
? Engine: Matcher
Ranking and answer extraction
? Steps: rank candidate answers
             extract and format them
? Engine: Answer ranker/formatter
QA typology
? QA types, categorized in taxonomy
Constraint patterns
? Identify likely answers in relation to
   other parts of the sentence
Create query
Retrieve documents
Select & rank sentences
Parse top sentences
Parse question
Input question
Perform additional inference
Rank and prepare answers
Output answers
Question parsing
? Steps: parse question
             find desired semantic type
? Engines:  IdentiFinder  (BBN)
                 CONTEX
Match sentences against answers
Query creation
?  Steps: extract, combine important words
 expand query words using WordNet
 create queries, order by specificity
?  Engines: Query creator
IR
?  Steps: retrieve top 1000 documents
?  Engines: MG (RMIT Melbourne)
Sentence selection and ranking
?  Steps: score each sentence in each document
 rank sentences and pass top 300 along
?  Engines:Ranker
Figure 1. Webclopedia architecture. 
 http://www.isi.edu/natural-language/projects/we 
bclopedia/Taxonomy/taxonomy_toplevel.html). 
The currently approx. 180 classes,  which we 
call qtargets, were developed after an analysis of 
over 17,000 questions (downloaded in 1999 
from answers.com) and later enhancements to 
Webclopedia.  They are of several types:  
? common semantic classes such as PROPER-
PERSON, EMAIL-ADDRESS, LOCATION, 
PROPER-ORGANIZATION;  
? classes particular to QA such as YES:NO, 
ABBREVIATION-EXPANSION, and WHY-
FAMOUS;  
? syntactic classes such as NP and NOUN, 
when no semnatic type can be determined 
(e.g., ?What does Peugeot manufacture??);  
? roles and slots, such as REASON and TITLE-
P respectively, to indicate a desired relation 
with an anchoring concept.    
Given a question, the CONTEX parser uses a 
set of 276 hand-built rules to identify its most 
likely qtarget(s), and records them in a backoff 
scheme (allowing more general qtarget nodes to 
apply when more specific ones fail to find a 
match).  The generalizations are captured in a 
typical concept ontology, a 10,000-node extract 
of WordNet.   
The recursive part of pattern matching is 
driven mostly by interrogative phrases.  For 
example, the rule that determines the 
applicability of the qtarget WHY-FAMOUS 
requires the question word ?who?, followed by 
the copula, followed by a proper name.  When 
there is no match at the current level, the system 
examines any interrogative constituent, or words 
in special relations to it.  For example, the 
qtarget TEMPERATURE-QUANTITY (as in 
?What is the melting point of X?? requires as 
syntactic object something that in the ontology is 
subordinate to TEMP-QUANTIFIABLE-ABS-
TRACT with, as well, the word ?how? paired 
with ?warm?, ?cold?, ?hot?, etc., or the phrase  
?how many degrees? and a TEMPERATURE-
UNIT (as defined in the ontology).   
3.3 Type 3: Surface Pattern Matching 
Often qtarget answers are expressed using rather 
stereotypical words or phrases.  For example, the 
year of birth of a person is typically expressed 
using one of these phrases:  
<name> was born in <birthyear> 
<name> (<birthyear>?<deathyear>) 
We have developed a method to learn such 
patterns automatically from text on the web 
(Ravichandran and Hovy, 2002).  We have 
added into the QA Typology the patterns for 
appropriate qtargets (qtargets with closed-list 
answers, such as PLANETS, require no patterns).  
Where some QA systems use such patterns 
exclusively (Soubbotin and Soubbotin, 2001) or 
partially (Wang et al, 2001; Lee et al, 2001), 
we employ them as an additional source of 
evidence for the answer.  Preliminary results on 
for a range of qtargets, using the TREC-10 
questions and the TREC corpus, are:  
Question type 
(qtarget) 
Number of 
questions 
MRR on 
TREC docs 
BIRTHYEAR 8 0.47875 
INVENTORS 6 0.16667 
DISCOVERERS 4 0.1250 
DEFINITIONS 102 0.3445 
WHY-FAMOUS 3 0.6666 
LOCATIONS 16 0.75 
3.4  Type 4: Expected Numerical Ranges  
Quantity-targeting questions are often 
underspecified and rely on culturally shared  
cooperativeness rules and/or world knowledge: 
Q: How many people live in Chile?  
S1: ?From our correspondent comes good 
news about the nine people living in  Chile?? 
A1: nine  
While certainly nine people do live in Chile, 
we know what the questioner intends.  We have 
hand-implemented a rule that provides default 
range assumptions for POPULATION questions 
and biases quantity questions accordingly.  
3.5 Type 5: Abbreviation Expansion  
Abbreviations often follow a pattern: 
Q: What does NAFTA stand for? 
S1: ?This range of topics includes the North 
American Free Trade Agreement, NAFTA, 
and the world trade agreement GATT.?  
S2: ?The interview now changed to the subject 
of trade and pending economic issues, such as 
the issue of opening the rice market, NAFTA, 
and the issue of Russia repaying economic 
cooperation funds.?  
After Webclopedia identifies the qtarget as 
ABBREVIATION-EXPANSION, it extracts 
 possible answer candidates, including ?North 
American Free Trade Agreement? from S1 and 
?the rice market? from S2.  Rules for acronym 
matching easily prefer the former.  
3.6 Type 6: Semantic Type Matching  
Phone numbers, zip codes, email addresses, 
URLs, and different types of quantities obey 
lexicographic patterns that can be exploited for 
matching, as in  
Q: What is the zip code for Fremont, CA?  
S1: ??from Everex Systems Inc., 48431 
Milmont Drive, Fremont, CA 94538.?  
and  
Q: How hot is the core of the earth?  
S1. ?The temperature of Earth?s inner core 
may be as high as 9,000 degrees Fahrenheit 
(5,000 degrees Celsius).?  
Webclopedia identifies the qtargets respectively 
as ZIP-CODE and TEMPERATURE-QUANTITY.  
Approx. 30 heuristics (cascaded) apply to the 
input before parsing to mark up numbers and 
other orthographically recognizable units of all 
kinds, including (likely) zip codes, quotations, 
year ranges, phone numbers, dates, times, 
scores, cardinal and ordinal numbers, etc.  
Similar work is reported in (Kwok et al, 2001).  
3.7 Type 7: Definitions from WordNet  
We have found a 10% increase in accuracy in 
answering definition questions by using external 
glosses obtained from WordNet.  For  
Q: What is the Milky Way?  
Webclopedia identified two leading answer 
candidates:   
A1: outer regions  
A2: the galaxy that contains the Earth  
Comparing these with the WordNet gloss:  
WordNet: ?Milky Way?the galaxy containing 
the solar system?  
allows Webclopedia to straightforwardly match 
the candidate with the greater word overlap.   
Curiously, the system also needs to use 
WordNet to answer questions involving 
common knowledge, as in:  
Q: What is the capital of the United States?  
because authors of the TREC collection do not 
find it necessary to explain what Washington is:  
Ex: ?Later in the day, the president returned to 
Washington, the capital of the United States.?  
While WordNet?s definition  
Wordnet: ?Washington?the capital of the 
United States?  
directly provides the answer to the matcher, it 
also allows the IR module to focus its search on 
passages containing ?Washington?, ?capital?, 
and ?United States?, and the matcher to pick a 
good motivating passage in the source corpus.   
Clearly, this capability can be extended to 
include (definitional and other) information 
provided by other sources, including 
encyclopedias and the web (Lin 2002). 
3.8 Type 8: Semantic Relation Matching  
So far, we have considered individual words and 
groups of words.  But often this is insufficient to 
accurately score an answer.  As also noted in 
(Buchholz, 2001), pinpointing can be improved 
significantly by matching semantic relations 
among constituents:  
Q: Who killed Lee Harvey Oswald?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?Belli?s clients have included Jack Ruby, 
who killed John F. Kennedy assassin Lee 
Harvey Oswald, and Jim and Tammy Bakker.?  
S2: ?On Nov. 22, 1963, the building gained 
national notoriety when Lee Harvey Oswald 
allegedly shot and killed President John F. 
Kennedy from a sixth floor window as the 
presidential motorcade passed.?  
The CONTEX parser (Hermjakob, 1997; 
2001) provides the semantic relations.  The 
parser uses machine learning techniques to build 
a robust grammar that produces semantically 
annotated syntax parses of English (and Korean 
and Chinese) sentences at approx. 90% accuracy 
(Hermjakob, 1999).   
The matcher compares the parse trees of S1 
and S2 to that of the question.  Both S1 and S2 
receive credit for matching question words ?Lee 
Harvey Oswald? and ?kill? (underlined), as well 
as for finding an answer (bold) of the proper 
qtarget type (PROPER-PERSON).  However, is 
the answer ?Jack Ruby? or ?President John F. 
Kennedy??  The only way to determine this is to 
consider the semantic relationship between these 
 candidates and the verb ?kill? (parse trees 
simplified, and only portions shown here):   
 
[1] Who killed Lee Harvey Oswald?  [S-SNT] 
    (SUBJ) [2] Who  [S-INTERR-NP] 
        (PRED) [3] Who  [S-INTERR-PRON] 
    (PRED) [4] killed  [S-TR-VERB] 
    (OBJ) [5] Lee Harvey Oswald  [S-NP] 
        (PRED) [6] Lee?Oswald  [S-PROPER-NAME] 
            (MOD) [7] Lee  [S-PROPER-NAME] 
            (MOD) [8] Harvey  [S-PROPER-NAME] 
            (PRED) [9] Oswald  [S-PROPER-NAME] 
    (DUMMY) [10] ?  [D-QUESTION-MARK] 
 
[1] Jack Ruby, who killed John F. Kennedy assassin  
  Lee Harvey Oswald  [S-NP] 
   (PRED) [2] <Jack Ruby>1  [S-NP] 
   (DUMMY) [6] ,  [D-COMMA] 
   (MOD) [7] who killed John F. Kennedy assassin  
                 Lee Harvey Oswald  [S-REL-CLAUSE] 
     (SUBJ) [8] who<1>  [S-INTERR-NP] 
     (PRED) [10] killed  [S-TR-VERB] 
     (OBJ) [11] JFK assassin?Oswald  [S-NP] 
         (PRED) [12] JFK?Oswald [S-PROP-NAME] 
             (MOD) [13] JFK  [S-PROPER-NAME] 
             (MOD) [19] assassin  [S-NOUN] 
             (PRED) [20] ?Oswald [S-PROPER-NAME] 
Although the PREDs of both S1 and S2 
match that of the question ?killed?, only S1 
matches ?Lee Harvey Oswald? as the head of 
the logical OBJect.  Thus for S1, the matcher 
awards additional credit to node [2] (Jack Ruby) 
for being the logical SUBJect of the killing 
(using anaphora resolution). In S2, the parse tree 
correctly records that node [13] (?John F. 
Kennedy?) is not the object of the killing.  Thus 
despite its being closer to ?killed?, the candidate 
in S2 receives no extra credit from semantic 
relation matching.   
It is important to note that the matcher 
awards extra credit for each matching semantic 
relationship between two constituents, not only 
when everything matches.  This granularity 
improves robustness in the case of partial 
matches.   
Semantic relation matching applies not only 
to logical subjects and objects, but also to all 
other roles such as location, time, reason, etc. 
(for additional examples see http://www.isi.edu/ 
natural-language/projects/webclopedia/sem-rel-
examples.html).  It also applies at not only the 
sentential level, but at all levels, such as post-
modifying prepositional and pre-modifying 
determiner phrases  
Additionally, Webclopedia uses 10 lists of 
word variations with a total of 4029 entries for 
semantically related concepts such as ?to 
invent?, ?invention? and ?inventor?, and rules 
for handling them.  For example, via coercing 
?invention? to ?invent?, the system can give 
?Johan Vaaler? extra credit for being a likely 
logical subject of ?invention?:  
Q: Who invented the paper clip?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?The paper clip, weighing a desk-crushing 
1,320 pounds, is a faithful copy of Norwegian 
Johan Vaaler?s 1899 invention, said Per 
Langaker of the Norwegian School of 
Management.?  
while ?David? actually loses points for being 
outside of the clausal scope of the inventing:  
S2: ??Like the guy who invented the safety pin, 
or the guy who invented the paper clip,? David 
added.?  
3.9 Type 9: Word Window Scoring  
Webclopedia also includes a typical window-
based scoring module that moves a window over 
the text and assigns a score to each window 
position depending on a variety of criteria (Hovy 
et al, 1999).  Unlike (Clarke et al, 2001; Lee et 
al., 2001; Chen et al, 2001), we have not 
developed a very sophisticated scoring function, 
preferring to focus on the modules that employ 
information deeper than the word level.  
This method is applied only when no other 
method provides a sufficiently high-scoring 
answer.  The window scoring function is  
S  = (500/(500+w))*(1/r) * ?[(?I1.5*q*e*b*u)1.5] 
Factors: 
w: window width (modulated by gaps of 
various lengths: ?white house? ? ?white car and 
house?), 
r: rank of qtarget in list returned by 
CONTEX, 
I: window word information content (inverse 
log frequency score of each word), summed,  
q: # different question words matched, plus 
specific rewards (bonus q=3.0),  
e: penalty if word matches one of question 
word?s WordNet synset items (e=0.8),  
 b: bonus for matching main verb, proper 
names, certain target words (b=2.0),  
u: (value 0 or 1) indicates whether a word has 
been qsubsumed (?subsumed? by the qtarget) 
and should not contribute (again) to the score.  
For example, ?In what year did Columbus 
discover America?? the qsubsumed words are 
?what? and ?year?. 
4. Performance Evaluation  
In TREC-10?s QA track, Webclopedia received 
an overall Mean Reciprocal Rank (MRR) score 
of 0.435, which put it among the top 4 
performers of the 68 entrants (the average MRR 
score for the main QA task was about 0.234).  
The pinpointing heuristics are fairly accurate: 
when Webclopedia finds answers, it usually 
ranks them in the first place (1st place: 35.5%; 
2nd: 8.94%; 3rd: 5.69%; 4th: 3.05%; 5th: 5.28%; 
not found: 41.87%).  
We determined the impact of each 
knowledge source on system performance, using 
the TREC-10 test corpus using the standard 
MRR scoring.  We applied the system to the 
questions of each knowledge type separately, 
with and without its specific knowledge 
source/algorithm.  Results are shown in Table 1, 
columns A (without) and B (with).  To indicate 
overall effect, we also show (in columns C and 
D) the percentage of questions in TREC-10 and 
-9 respecively of each knowledge type.   
5. Conclusions 
It is tempting to search for a single technique 
that will solve the whole problem (for example, 
Ittycheriah et al (2001) focus on the subset of 
factoid questions answerable by NPs, and train a 
statistical model to perform NP-oriented answer 
pinpointing).  Our experience, however, is that 
even factoid QA is varied enough to require 
various special-purpose techniques and 
knowledge.  The theoretical limits of the various 
techniques are not known, though Light et al?s 
(2001) interesting work begins to study this.   
Column A: % questions of the knowledge type  
     answered correctly without using knowlege 
Column B: % questions, now using knowledge 
Column C: % questions of type in TREC-10  
Column D: % questions of type in TREC-9  
 A B C D 
Abbreviation exp. 20.0 70.0  1.0 2.3 
Number ranges 50.0 50.0  1.2 1.8 
WordNet (def Qs) 48.3 67.5 20.9 5.1 
Semantic types     
- locator types N/A N/A  0.0 0.4 
- quantity types 22.5 48.7 10.8 5.5 
- date/year types 45.0 57.3  9.2 10.2 
Patterns      
- definitions ? 34.4 20.9 5.1 
- why-famous  ? 66.7 0.6 ? 
- locations ? 75.0 3.2 ? 
- birthyear ? 47.9 1.6 ? 
Semantic relations 39.4 46.5 72.2 85.7 
Table 1. Performance of knowledge sources. 
Semantic relation scores measured only on 
questions in which they could logically apply.   
We conclude that factoid QA performance 
can be significantly improved by the use of 
knowledge attuned to specific question types 
and specific information characteristics.  Most of 
the techniques for exploiting this knowledge 
require learning to ensure robustness.  To 
improve performance beyond this, we believe a 
combination of going to the web and turning to 
deeper world knowledge and automated 
inference (Harabagiu et al, 2001) to be the 
answer.  It remains an open question how much 
work these techniques would require, and what 
their payoff limits are.   
References  
Abney, S., M. Collins, and A. Singhal. 2000. Answer 
Extraction. Proceedings of the Applied Natural 
Language Processing Conference (ANLP-
NAACL-00), Seattle, WA, 296?301.  
Bikel, D., R. Schwartz, and R. Weischedel.  1999.  
An Algorithm that Learns What?s in a Name.  
Machine Learning?Special Issue on NL 
Learning, 34, 1?3. 
Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng. 
2001. Data-Intensive Question Answering.  
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 183?189.  
Buchholz, S. 2001. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. Proceedings of the 
TREC-10 Conference. NIST, 496?503.  
Chen, J., A.R. Diekema, M.D. Taffet, N. McCracken, 
N. Ercan Ozgencil, O. Yilmazel, and E.D. Liddy. 
 2001. CNLP at TREC-10 QA Track. Proceedings 
of the TREC-10 Conference. NIST, 480?490. 
Clarke, C.L.A., G.V. Cormack, T.R. Lynam, C.M. Li, 
and G.L. McLearn. 2001. Web Reinforced 
Question Answering. Proceedings of the TREC-
10 Conference. NIST, 620?626.  
Clarke, C.L.A., G.V. Cormack, and T.R. Lynam. 
2001. Exploiting Redundancy in Question 
Answering. Proceedings of the SIGIR 
Conference. New Orleans, LA, 358?365.  
Fellbaum, Ch. (ed). 1998. WordNet: An Electronic 
Lexical Database. Cambridge: MIT Press. 
Harabagiu, S., D. Moldovan, M. Pasca, R. Mihalcea, 
M. Surdeanu, R. Buneascu, R. G?rju, V. Rus and 
P. Morarescu. 2001. FALCON: Boosting 
Knowledge for Answer Engines. Proceedings of 
the 9th Text Retrieval Conference (TREC-9), 
NIST, 479?488.  
Hermjakob, U. 1997. Learning Parse and 
Translation Decisions from Examples with Rich 
Context.  Ph.D. dissertation, University of Texas 
Austin. file://ftp.cs.utexas.edu/pub/mooney/paper 
s/hermjakob-dissertation 97.ps.gz.  
Hermjakob, U. 2001. Parsing and Question 
Classification for Question Answering. 
Proceedings of the Workshop on Question 
Answering at ACL-2001.  Toulouse, France.  
Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and 
C.-Y. Lin. 1999. Question Answering in 
Webclopedia.  Proceedings of the TREC-9 
Conference.  NIST. Gaithersburg, MD, 655?673. 
Hovy, E.H., U. Hermjakob, and D. Ravichandran. 
2002. A Question/Answer Typology with Surface 
Text Patterns.  Poster in Proceedings of the 
DARPA Human Language Technology 
Conference (HLT).  San Diego, CA, 234?238.   
Hovy, E.H., U. Hermjakob, and C.-Y. Lin. 2001. The 
Use of External Knowledge in Factoid QA.   
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 166?174.  
Ittycheriah, A., M. Franz, and S. Roukos. 2001. 
IBM?s Statistical Question Answering System. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 317?323.  
Kwok, K.L., L. Grunfeld, N. Dinstl, and M. Chan. 
2001. TREC2001 Question-Answer, Web and 
Cross Language experiments using PIRCS. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 447?451.  
Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C. Lee, 
B-K. Kwak, J, Cha, D. Kim, J-H. An, H. Kim, 
and K. Kim. 2001. SiteQ: Engineering High 
Performance QA System Using Lexico=Semantic 
Pattern Matching and Shallow NLP. Proceedings 
of the TREC-10 Conference. NIST, Gaithersburg, 
MD, 437?446.  
Light, M., G.S. Mann, E. Riloff, and E. Breck. 2001. 
Analyses for Elucidating Current Question 
Answering Technology. Natural Language 
Engineering, 7:4, 325?342.  
Lin, C.-Y. 2002. The Effectiveness of Dictionary and 
Web-Based Answer Reranking.  Proceedings of 
the 19th International Conference on 
Computational Linguistics (COLING 2002), 
Taipei, Taiwan.  
Oh, JH., KS. Lee, DS. Chang, CW. Seo, and KS. 
Choi. 2001. TREC-10 Experiments at KAIST: 
Batch Filtering and Question Answering. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 354?361. 
Prager, J., E. Brown, D.R. Radev, and K. Czuba. 
1999. One Search Engine or Two for Question 
Answering. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 235?240. 
Ravichandran, D. and E.H. Hovy. 2002. Learning 
Surface Text Patterns for a Question Answering 
System. Proceedings of the ACL conference. 
Philadelphia, PA.  
Soubbotin, M.M. and S.M. Soubbotin. 2001. Patterns 
of Potential Answer Expressions as Clues to the 
Right Answer. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 175?182.   
Srihari, R. and W. Li. 2000. A Question Answering 
System Supported by Information Extraction. 
Proceedings of the 1st Meeting of the North 
American Chapter of the Association for 
Computational Linguistics (ANLP-NAACL-00), 
Seattle, WA, 166?172. 
Voorhees, E. 1999. Overview of the Question 
Answering Track. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 71?81.  
Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D. Bu, 
and S. Bai. 2001. TREC-10 Experiments at CAS-
ICT: Filtering, Web, and QA. Proceedings of the 
TREC-10 Conference. NIST, 229?241.  
Witten, I.H., A. Moffat, and T.C. Bell. 1994. 
Managing Gigabytes: Compressing and Indexing 
Documents and Images. New York: Van 
Nostrand Reinhold. 
 
Towards Terascale Knowledge Acquisition 
Patrick Pantel, Deepak Ravichandran and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{pantel,ravichan,hovy}@isi.edu 
 
Abstract 
Although vast amounts of textual data are freely 
available, many NLP algorithms exploit only a 
minute percentage of it. In this paper, we study the 
challenges of working at the terascale. We present 
an algorithm, designed for the terascale, for mining  
is-a relations that achieves similar performance to a 
state-of-the-art linguistically-rich method. We fo-
cus on the accuracy of these two systems as a func-
tion of processing time and corpus size. 
1 Introduction 
The Natural Language Processing (NLP) com-
munity has recently seen a growth in corpus-based 
methods. Algorithms light in linguistic theories but 
rich in available training data have been success-
fully applied to several applications such as ma-
chine translation (Och and Ney 2002), information 
extraction (Etzioni et al 2004), and question an-
swering (Brill et al 2001). 
In the last decade, we have seen an explosion in 
the amount of available digital text resources. It is 
estimated that the Internet contains hundreds of 
terabytes of text data, most of which is in an 
unstructured format. Yet, many NLP algorithms 
tap into only megabytes or gigabytes of this 
information. 
In this paper, we make a step towards acquiring 
semantic knowledge from terabytes of data. We 
present an algorithm for extracting is-a relations, 
designed for the terascale, and compare it to a state 
of the art method that employs deep analysis of 
text (Pantel and Ravichandran 2004). We show 
that by simply utilizing more data on this task, we 
can achieve similar performance to a linguistically-
rich approach. The current state of the art co-
occurrence model requires an estimated 10 years 
just to parse a 1TB corpus (see Table 1). Instead of 
using a syntactically motivated co-occurrence ap-
proach as above, our system uses lexico-syntactic 
rules. In particular, it finds lexico-POS patterns by 
making modifications to the basic edit distance 
algorithm. Once these patterns have been learnt, 
the algorithm for finding new is-a relations runs in 
O(n), where n is the number of sentences. 
In semantic hierarchies such as WordNet (Miller 
1990), an is-a relation between two words x and y 
represents a subordinate relationship (i.e. x is more 
specific than y). Many algorithms have recently 
been proposed to automatically mine is-a (hypo-
nym/hypernym) relations between words. Here, we 
focus on is-a relations that are characterized by the 
questions ?What/Who is X?? For example, Table 2 
shows a sample of 10 is-a relations discovered by 
the algorithms presented in this paper. In this table, 
we call azalea, tiramisu, and Winona Ryder in-
stances of the respective concepts flower, dessert 
and actress. These kinds of is-a relations would be 
useful for various purposes such as ontology con-
struction, semantic information retrieval, question 
answering, etc. 
The main contribution of this paper is a compari-
son of the quality of our pattern-based and co-
occurrence models as a function of processing time 
and corpus size. Also, the paper lays a foundation 
for terascale acquisition of knowledge. We will 
show that, for very small or very large corpora or 
for situations where recall is valued over precision, 
the pattern-based approach is best. 
2 Relevant Work 
Previous approaches to extracting is-a relations 
fall under two categories: pattern-based and co-
occurrence-based approaches. 
2.1 Pattern-based approaches 
Marti Hearst (1992) was the first to use a pat-
tern-based approach to extract hyponym relations 
from a raw corpus. She used an iterative process to 
semi-automatically learn patterns. However, a 
corpus of 20MB words yielded only 400 examples. 
Our pattern-based algorithm is very similar to the 
one used by Hearst. She uses seed examples to 
manually discover her patterns whearas we use a 
minimal edit distance algorithm to automatically 
discover the patterns. 
771
Riloff and Shepherd (1997) used a semi-
automatic method for discovering similar words 
using a few seed examples by using pattern-based 
techniques and human supervision. Berland and 
Charniak (1999) used similar pattern-based tech-
niques and other heuristics to extract meronymy 
(part-whole) relations. They reported an accuracy 
of about 55% precision on a corpus of 100,000 
words. Girju et al (2003) improved upon Berland 
and Charniak's work using a machine learning 
filter. Mann (2002) and Fleischman et al (2003) 
used part of speech patterns to extract a subset of 
hyponym relations involving proper nouns. 
Our pattern-based algorithm differs from these 
approaches in two ways. We learn lexico-POS 
patterns in an automatic way. Also, the patterns are 
learned with the specific goal of scaling to the 
terascale (see Table 2). 
2.2 Co-occurrence-based approaches 
The second class of algorithms uses co-
occurrence statistics (Hindle 1990, Lin 1998). 
These systems mostly employ clustering algo-
rithms to group words according to their meanings 
in text. Assuming the distributional hypothesis 
(Harris 1985), words that occur in similar gram-
matical contexts are similar in meaning. Curran 
and Moens (2002) experimented with corpus size 
and complexity of proximity features in building 
automatic thesauri. CBC (Clustering by Commit-
tee) proposed by Pantel and Lin (2002) achieves 
high recall and precision in generating similarity 
lists of words discriminated by their meaning and 
senses. However, such clustering algorithms fail to 
name their classes. 
Caraballo (1999) was the first to use clustering 
for labeling is-a relations using conjunction and 
apposition features to build noun clusters. Re-
cently, Pantel and Ravichandran (2004) extended 
this approach by making use of all syntactic de-
pendency features for each noun. 
3 Syntactical co-occurrence approach 
Much of the research discussed above takes a 
similar approach of searching text for simple sur-
face or lexico-syntactic patterns in a bottom-up 
approach. Our co-occurrence model (Pantel and 
Ravichandran 2004) makes use of semantic classes 
like those generated by CBC. Hyponyms are gen-
erated in a top-down approach by naming each 
group of words and assigning that name as a hypo-
nym of each word in the group (i.e., one hyponym 
per instance/group label pair). 
The input to the extraction algorithm is a list of 
semantic classes, in the form of clusters of words, 
which may be generated from any source. For ex-
ample, following are two semantic classes discov-
ered by CBC: 
(A) peach, pear, pineapple, apricot, 
mango, raspberry, lemon, cherry, 
strawberry, melon, blueberry, fig, apple, 
plum, nectarine, avocado, grapefruit, 
papaya, banana, cantaloupe, cranberry, 
blackberry, lime, orange, tangerine, ... 
(B) Phil Donahue, Pat Sajak, Arsenio 
Hall, Geraldo Rivera, Don Imus, Larry King, 
David Letterman, Conan O'Brien, Rosie 
O'Donnell, Jenny Jones, Sally Jessy Raph-
ael, Oprah Winfrey, Jerry Springer, Howard 
Stern, Jay Leno, Johnny Carson, ... 
The extraction algorithm first labels concepts 
(A) and (B) with fruit and host respectively. Then, 
is-a relationships are extracted, such as: apple is a 
fruit, pear is a fruit, and David Letterman is a host. 
An instance such as pear is assigned a hypernym 
fruit not because it necessarily occurs in any par-
ticular syntactic relationship with the word fruit, 
but because it belongs to the class of instances that 
does. The labeling of semantic classes is performed 
in three phases, as outlined below. 
3.1 Phase I 
In the first phase of the algorithm, feature vec-
tors are extracted for each word that occurs in a 
semantic class. Each feature corresponds to a 
grammatical context in which the word occurs. For 
example, ?catch __? is a verb-object context. If the 
word wave occurred in this context, then the con-
text is a feature of wave. 
We then construct a mutual information vector 
MI(e) = (mie1, mie2, ?, miem) for each word e, 
where mief is the pointwise mutual information 
between word e and context f, which is defined as: 
 
N
c
N
c
N
c
ef m
j
ej
n
i
if
ef
mi
??
== ?
=
11
log  
Table 2. Sample of 10 is-a relationships discovered by 
our co-occurrence and pattern-based systems.  
CO-OCCURRENCE SYSTEM PATTERN-BASED SYSTEM 
Word Hypernym Word Hypernym 
azalea flower American airline 
bipolar disorder disease Bobby Bonds coach 
Bordeaux wine radiation therapy cancer 
treatment 
Flintstones television show tiramisu dessert 
salmon fish Winona Ryder actress 
 
Table 1. Approximate processing time on a single 
Pentium-4 2.5 GHz machine. 
TOOL 15 GB ORPUS 1 TB CORPUS 
POS Tagger 2 days 125 days 
NP Chunker 3 days 214 days 
Dependency Parser 56 days 10.2 years 
Syntactic Parser 5.8 years 388.4 years 
 
772
where n is the number of elements to be clustered, 
cef is the frequency count of word e in grammatical 
context f, and N is the total frequency count of all 
features of all words. 
3.2 Phase II 
Following (Pantel and Lin 2002), a committee 
for each semantic class is constructed. A commit-
tee is a set of representative elements that unambi-
guously describe the members of a possible class. 
For example, in one of our experiments, the com-
mittees for semantic classes (A) and (B) from Sec-
tion 3 were: 
A) peach, pear, pineapple, apricot, mango, 
raspberry, lemon, blueberry 
B) Phil Donahue, Pat Sajak, Arsenio Hall, 
Geraldo Rivera, Don Imus, Larry King, 
David Letterman 
3.3 Phase III 
By averaging the feature vectors of the commit-
tee members of a particular semantic class, we 
obtain a grammatical template, or signature, for 
that class. For example, Figure 1 shows an excerpt 
of the grammatical signature for semantic class 
(B). The vector is obtained by averaging the fea-
ture vectors of the words in the committee of this 
class. The ?V:subj:N:joke? feature indicates a sub-
ject-verb relationship between the class and the 
verb joke while ?N:appo:N:host? indicates an ap-
position relationship between the class and the 
noun host. The two columns of numbers indicate 
the frequency and mutual information scores. 
To name a class, we search its signature for cer-
tain relationships known to identify class labels. 
These relationships, automatically learned in 
(Pantel and Ravichandran 2004), include apposi-
tions, nominal subjects, such as relationships, and 
like relationships. We sum up the mutual informa-
tion scores for each term that occurs in these rela-
tionships with a committee of a class. The highest 
scoring term is the name of the class. 
The syntactical co-occurrence approach has 
worst-case time complexity O(n2k), where n is the 
number of words in the corpus and k is the feature-
space (Pantel and Ravichandran 2004). Just to 
parse a 1 TB corpus, this approach requires ap-
proximately 10.2 years (see Table 2). 
4 Scalable pattern-based approach 
We propose an algorithm for learning highly 
scalable lexico-POS patterns. Given two sentences 
with their surface form and part of speech tags, the 
algorithm finds the optimal lexico-POS alignment. 
For example, consider the following 2 sentences: 
1) Platinum is a precious metal. 
2) Molybdenum is a metal. 
Applying a POS tagger (Brill 1995) gives the 
following output: 
 
Surface Platinum is a precious metal . 
POS NNP VBZ DT JJ NN . 
 
Surface Molybdenum is a metal . 
POS NNP VBZ DT NN . 
 
A very good pattern to generalize from the 
alignment of these two strings would be 
 
Surface  is a  metal . 
POS NNP     . 
 
We use the following notation to denote this 
alignment: ?_NNP is a (*s*) metal.?, where  
?_NNP represents the POS tag NNP?. 
To perform such alignments we introduce two 
wildcard operators, skip (*s*) and wildcard (*g*). 
The skip operator represents 0 or 1 instance of any 
word (similar to the \w* pattern in Perl), while the 
wildcard operator represents exactly 1 instance of 
any word (similar to the \w+ pattern in Perl). 
4.1 Algorithm 
We present an algorithm for learning patterns at 
multiple levels. Multilevel representation is de-
fined as the different levels of a sentence such as 
the lexical level and POS level. Consider two 
strings a(1, n) and b(1, m) of lengths n and m re-
spectively. Let a1(1, n) and a2(1, n) be the level 1 
(lexical level) and level 2 (POS level) representa-
tions for the string a(1, n). Similarly, let b1(1, m) 
and b2(1, m) be the level 1 and level 2 representa-
tions for the string b(1, m). The algorithm consists 
of two parts: calculation of the minimal edit dis-
tance and retrieval of an optimal pattern. The 
minimal edit distance algorithm calculates the 
number of edit operations (insertions, deletions and 
replacements) required to change one string to 
another string. The optimal pattern is retrieved by 
{Phil Donahue,Pat Sajak,Arsenio Hall} 
 N:gen:N  
  talk show 93 11.77 
  television show 24 11.30 
  TV show 25 10.45 
  show 255 9.98 
  audience 23 7.80 
  joke 5 7.37 
 V:subj:N  
  joke 39 7.11 
  tape 10 7.09 
  poke 15 6.87 
  host 40 6.47 
  co-host 4 6.14 
  banter 3 6.00 
  interview 20 5.89 
 N:appo:N  
  host 127 12.46 
  comedian 12 11.02 
  King 13 9.49 
  star 6 7.47 
Figure 1. Excerpt of the grammatical signature for the 
television host class. 
 
773
keeping track of the edit operations (which is the 
second part of the algorithm). 
Algorithm for calculating the minimal edit distance 
between two strings 
D[0,0]=0 
for i = 1 to n do  D[i,0] = D[i-1,0] + cost(insertion) 
for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) 
for i = 1 to n do 
 for j = 1 to m do 
  D[i,j] = min( D[i-1,j-1] + cost(substitution), 
        D[i-1,j] + cost(insertion), 
        D[i,j-1] + cost(deletion)) 
Print (D[n,m]) 
Algorithm for optimal pattern retrieval 
i = n, j = m; 
while i ? 0 and j ? 0 
 if D[i,j] = D[i-1,j] + cost(insertion) 
  print (*s*), i = i-1 
 else if D[i,j] = D[i,j-1] + cost(deletion) 
  print(*s*), j = j-1 
 else if a1i = b1j 
  print (a1i), i = i -1, j = j =1 
 else if a2i = b2j 
  print (a2i), i = i -1, j = j =1 
 else 
  print (*g*), i = i -1, j = j =1 
We experimentally set (by trial and error): 
cost(insertion)  = 3 
cost(deletion)  = 3 
cost(substitution) = 0 if a1i=b1j 
  = 1 if a1i?b1j, a2i=b2j 
  = 2 if a1i?b1j, a2i?b2j 
4.2 Implementation and filtering 
The above algorithm takes O(y2) time for every 
pair of strings of length at most y. Hence, if there 
are x strings in the collection, each string having at 
most length y, the algorithm has time complexity 
O(x2y2) to extract all the patterns in the collection. 
Applying the above algorithm on a corpus of 
3GB  with 50 is-a relationship seeds, we obtain a 
set of 600 lexico-POS. Following are two of them: 
1) X_JJ#NN|JJ#NN#NN|NN _CC Y_JJ#JJ#NN|JJ 
|NNS|NN|JJ#NNS|NN#NN|JJ#NN|JJ#NN#NN 
e.g. ?caldera or lava lake? 
2) X_NNP#NNP|NNP#NNP#NNP#NNP#NNP#CC#NNP 
|NNP|VBN|NN#NN|VBG#NN|NN ,_, _DT 
Y_NN#IN#NN|JJ#JJ#NN|JJ|NN|NN#IN#NNP 
|NNP#NNP|NN#NN|JJ#NN|JJ#NN#NN 
e.g. ?leukemia, the cancer of ... 
Note that we store different POS variations of 
the anchors X and Y. As shown in example 1, the 
POS variations of the anchor X are (JJ NN, JJ NN 
NN, NN). The variations for anchor Y are (JJ JJ 
NN, JJ, etc.). The reason is quite straightforward: 
we need to determine the boundary of the anchors 
X and Y and a reasonable way to delimit them 
would be to use POS information.  All the patterns 
produced by the multi-level pattern learning algo-
rithm were generated from positive examples. 
From amongst these patterns, we need to find the 
most important ones. This is a critical step because 
frequently occurring patterns have low precision 
whereas rarely occurring patterns have high preci-
sion. From the Information Extraction point of 
view neither of these patterns is very useful. We 
need to find patterns with relatively high occur-
rence and high precision. We apply the log likeli-
hood principle (Dunning 1993) to compute this 
score. The top 15 patterns according to this metric 
are listed in Table 3 (we omit the POS variations 
for visibility). Some of these patterns are similar to 
the ones discovered by Hearst (1992) while other 
patterns are similar to the ones used by Fleischman 
et al (2003). 
4.3 Time complexity 
To extract hyponym relations, we use a fixed 
number of patterns across a corpus. Since we treat 
each sentences independently from others, the 
algorithm runs in linear time O(n) over the corpus 
size, where n is number of sentences in the corpus. 
5 Experimental Results 
In this section, we empirically compare the pat-
tern-based and co-occurrence-based models pre-
sented in Section 3 and Section 4. The focus is on 
the precision and recall of the systems as a func-
tion of the corpus size. 
5.1 Experimental Setup 
We use a 15GB newspaper corpus consisting of 
TREC9, TREC 2002, Yahoo! News ~0.5GB, AP 
newswire ~2GB, New York Times ~2GB, Reuters 
~0.8GB, Wall Street Journal ~1.2GB, and various 
online news website ~1.5GB. For our experiments, 
we extract from this corpus six data sets of differ-
ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB 
and 15GB. 
For the co-occurrence model, we used Minipar 
(Lin 1994), a broad coverage parser, to parse each 
data set. We collected the frequency counts of the 
grammatical relationships (contexts) output by 
Minipar and used them to compute the pointwise 
mutual information vectors described in Section 
3.1. For the pattern-based approach, we use Brill?s 
POS tagger (1995) to tag each data set. 
5.2 Precision 
We performed a manual evaluation to estimate 
the precision of both systems on each dataset. For 
each dataset, both systems extracted a set of is-a 
Table 3. Top 15 lexico-syntactic patterns discovered 
by our system. 
X, or Y X, _DT Y _(WDT|IN) Y like X and 
X, (a|an) Y X, _RB known as Y _NN, X and other Y 
X, Y X ( Y ) Y, including X, 
Y, or X Y such as X Y, such as X 
X is a Y X, _RB called Y Y, especially X 
 
774
relationships. Six sets were extracted for the pat-
tern-based approach and five sets for the co-
occurrence approach (the 15GB corpus was too 
large to process using the co-occurrence model ? 
see dependency parsing time estimates in Table 2). 
From each resulting set, we then randomly se-
lected 50 words along with their top 3 highest 
ranking is-a relationships. For example, Table 4 
shows three randomly selected names for the pat-
tern-based system on the 15GB dataset. For each 
word, we added to the list of hypernyms a human 
generated hypernym (obtained from an annotator 
looking at the word without any system or Word-
Net hyponym). We also appended the WordNet 
hypernyms for each word (only for the top 3 
senses). Each of the 11 random samples contained 
a maximum of 350 is-a relationships to manually 
evaluate (50 random words with top 3 system, top 
3 WordNet, and human generated relationship). 
We presented each of the 11 random samples to 
two human judges. The 50 randomly selected 
words, together with the system, human, and 
WordNet generated is-a relationships, were ran-
domly ordered. That way, there was no way for a 
judge to know the source of a relationship nor each 
system?s ranking of the relationships. For each 
relationship, we asked the judges to assign a score 
of correct, partially correct, or incorrect. We then 
computed the average precision of the system, 
human, and WordNet on each dataset. We also 
computed the percentage of times a correct rela-
tionship was found in the top 3 is-a relationships of 
a word and the mean reciprocal rank (MRR). For 
each word, a system receives an MRR score of 1 / 
M, where M is the rank of the first name judged 
correct. Table 5 shows the results comparing the 
two automatic systems. Table 6 shows similar 
results for a more lenient evaluation where both 
correct and partially correct are judged correct. 
For small datasets (below 150MB), the pattern-
based method achieves higher precision since the 
co-occurrence method requires a certain critical 
mass of statistics before it can extract useful class 
signatures (see Section 3). On the other hand, the 
pattern-based approach has relatively constant 
precision since most of the is-a relationships se-
lected by it are fired by a single pattern. Once the 
co-occurrence system reaches its critical mass (at 
150MB), it generates much more precise hypo-
nyms. The Kappa statistics for our experiments 
were all in the range 0.78 ? 0.85. 
Table 7 and Table 8 compare the precision of the 
pattern-based and co-occurrence-based methods 
with the human and WordNet hyponyms. The 
variation between the human and WordNet scores 
across both systems is mostly due to the relative 
cleanliness of the tokens in the co-occurrence-
based system (due to the parser used in the ap-
proach). WordNet consistently generated higher 
precision relationships although both algorithms 
approach WordNet quality on 6GB (the pattern-
based algorithm even surpasses WordNet precision 
on 15GB). Furthermore, WordNet only generated a 
hyponym 40% of the time. This is mostly due to 
the lack of proper noun coverage in WordNet. 
On the 6 GB corpus, the co-occurrence approach 
took approximately 47 single Pentium-4 2.5 GHz 
processor days to complete, whereas it took the 
pattern-based approach only four days to complete 
on 6 GB and 10 days on 15 GB. 
5.3 Recall 
The co-occurrence model has higher precision 
than the pattern-based algorithm on most datasets. 
Table 4. Is-a relationships assigned to three randomly selected words (using pattern-based system on 15GB dataset). 
RANDOM WORD HUMAN WORDNET PATTERN-BASED SYSTEM (RANKED) 
Sanwa Bank bank none subsidiary / lender / bank 
MCI Worldcom Inc. telecommunications company none phone company / competitor / company 
cappuccino beverage none item / food / beverage 
 
Table 5. Average precision, top-3 precision, and MRR 
for both systems on each dataset. 
 PATTERN SYSTEM CO-OCCURRENCE SYSTEM 
 
Prec Top-3 MRR Prec Top-3 MRR 
1.5MB
 
38.7% 41.0% 41.0% 4.3% 8.0% 7.3% 
15MB 39.1% 43.0% 41.5% 14.6% 32.0% 24.3% 
150MB 40.6% 46.0% 45.5% 51.1% 73.0% 67.0% 
1.5GB 40.4% 39.0% 39.0% 56.7% 88.0% 77.7% 
6GB 46.3% 52.0% 49.7% 64.9% 90.0% 78.8% 
15GB 55.9% 54.0% 52.0% Too large to process 
 
Table 6. Lenient average precision, top-3 precision, 
and MRR for both systems on each dataset. 
 PATTERN SYSTEM CO-OCCURRENCE SYSTEM 
 
Prec Top-3 MRR Prec Top-3 MRR 
1.5MB
 
56.6% 60.0% 60.0% 12.4% 20.0% 15.2% 
15MB 57.3% 63.0% 61.0% 23.2% 50.0% 37.3% 
150MB 50.7% 56.0% 55.0% 60.6% 78.0% 73.2% 
1.5GB 52.6% 51.0% 51.0% 69.7% 93.0% 85.8% 
6GB 61.8% 69.0% 67.5% 78.7% 92.0% 86.2% 
15GB 67.8% 67.0% 65.0% Too large to process 
 
775
However, Figure 2 shows that the pattern-based 
approach extracts many more relationships. 
Semantic extraction tasks are notoriously diffi-
cult to evaluate for recall. To approximate recall, 
we defined a relative recall measure and conducted 
a question answering (QA) task of answering defi-
nition questions. 
5.3.1 Relative recall 
Although it is impossible to know the number of 
is-a relationships in any non-trivial corpus, it is 
possible to compute the recall of a system relative 
to another system?s recall. The recall of a system 
A, RA, is given by the following formula: 
 
C
C
R AA =  
where CA is the number of correct is-a relation-
ships extracted by A and C is the total number of 
correct is-a relationships in the corpus. We define 
relative recall of system A given system B, RA,B, as: 
 
B
A
B
A
BA C
C
R
R
R ==
,
 
Using the precision estimates, PA, from the pre-
vious section, we can estimate CA ? PA ? |A|, where 
A is the total number of is-a relationships discov-
ered by system A. Hence, 
 
BP
AP
R
B
A
BA
?
?
=
,
 
Figure 3 shows the relative recall of A = pattern-
based approach relative to B = co-occurrence 
model. Because of sparse data, the pattern-based 
approach has much higher precision and recall (six 
times) than the co-occurrence approach on the 
small 15MB dataset. In fact, only on the 150MB 
dataset did the co-occurrence system have higher 
recall. With datasets larger than 150MB, the co-
occurrence algorithm reduces its running time by 
filtering out grammatical relationships for words 
that occurred fewer than k = 40 times and hence 
recall is affected (in contrast, the pattern-based 
approach may generate a hyponym for a word that 
it only sees once). 
5.3.2 Definition questions 
Following Fleischman et al (2003), we select 
the 50 definition questions from the TREC2003 
(Voorhees 2003) question set. These questions are 
of the form ?Who is X?? and ?What is X?? For 
each question (e.g., ?Who is Niels Bohr??, ?What 
is feng shui??) we extract its respective instance 
(e.g., ?Neils Bohr? and ?feng shui?), look up their 
corresponding hyponyms from our is-a table, and 
present the corresponding hyponym as the answer. 
We compare the results of both our systems with 
WordNet. We extract at most the top 5 hyponyms 
provided by each system. We manually evaluate 
the three systems and assign 3 classes ?Correct 
(C)?, ?Partially Correct (P)? or ?Incorrect (I)? to 
each answer. 
This evaluation is different from the evaluation 
performed by the TREC organizers for definition 
questions. However, by being consistent across all 
Total Number of Is-A Relationships vs. Dataset
0
200000
400000
600000
800000
1000000
1200000
1400000
1.5MB 15MB 150MB 1.5GB 6GB 15GB
Datasets
To
ta
l I
s-
A 
Re
la
tio
n
s
hi
ps
s
Pattern-based System
Co-occurrence-based System
Figure 2. Number of is-a relationships extracted by 
the pattern-based and co-occurrence-based approaches. 
 
Table 7. Average precision of the pattern-based sys-
tem vs. WordNet and human hyponyms. 
 PRECISION MRR 
 
Pat. WNet Human Pat. WNet Human 
1.5MB
 
38.7% 45.8% 83.0% 41.0% 84.4% 83.0% 
15MB 39.1% 52.4% 81.0% 41.5% 95.0% 91.0% 
150MB 40.6% 49.4% 84.0% 45.5% 88.9% 94.0% 
1.5GB 40.4% 43.4% 79.0% 39.0% 93.3% 89.0% 
6GB 46.3% 46.5% 76.0% 49.7% 75.0% 76.0% 
15GB 55.9% 45.6% 79.0% 52.0% 78.0% 79.0% 
 
Table 8. Average precision of the co-occurrence-
based system vs. WordNet and human hyponyms. 
 PRECISION MRR 
 
Co-occ WNet Human Co-occ WNet Human 
1.5MB
 
4.3% 42.7% 52.7% 7.3% 87.7% 95.0% 
15MB 14.6% 38.1% 48.7% 24.3% 86.6% 95.0% 
150MB 51.1% 57.5% 65.8% 67.0% 85.1% 98.0% 
1.5GB 56.7% 62.8% 70.3% 77.7% 93.0% 98.0% 
6GB 64.9% 68.9% 75.2% 78.8% 94.3% 98.0% 
 
Relative Recall (Pattern-based vs. Co-occurrence-based)
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
1.5MB 15MB 150MB 1.5GB 6GB 15GB
(projected)
Datesets
Re
la
tiv
e 
Re
ca
ll
Figure 3. Relative recall of the pattern-based approach 
relative to the co-occurrence approach. 
 
 
776
systems during the process, these evaluations give 
an indication of the recall of the knowledge base. 
We measure the performance on the top 1 and the 
top 5 answers returned by each system. Table 9 
and Table 10 show the results. 
The corresponding scores for WordNet are 38% 
accuracy in both the top-1 and top-5 categories (for 
both strict and lenient). As seen in this experiment, 
the results for both the pattern-based and co-
occurrence-based systems report very poor per-
formance for data sets up to 150 MB. However, 
there is an increase in performance for both sys-
tems on the 1.5 GB and larger datasets. The per-
formance of the system in the top 5 category is 
much better than that of WordNet (38%). There is 
promise for increasing our system accuracy by re-
ranking the outputs of the top-5 hypernyms. 
6 Conclusions 
There is a long standing need for higher quality 
performance in NLP systems. It is possible that 
semantic resources richer than WordNet will en-
able them to break the current quality ceilings. 
Both statistical and symbolic NLP systems can 
make use of such semantic knowledge. With the 
increased size of the Web, more and more training 
data is becoming available, and as Banko and Brill 
(2001) showed, even rather simple learning algo-
rithms can perform well when given enough data. 
In this light, we see an interesting need to de-
velop fast, robust, and scalable methods to mine 
semantic information from the Web. This paper 
compares and contrasts two methods for extracting 
is-a relations from corpora. We presented a novel 
pattern-based algorithm, scalable to the terascale, 
which outperforms its more informed syntactical 
co-occurrence counterpart on very small and very 
large data. 
Albeit possible to successfully apply linguisti-
cally-light but data-rich approaches to some NLP 
applications, merely reporting these results often 
fails to yield insights into the underlying theories 
of language at play. Our biggest challenge as we 
venture to the terascale is to use our new found 
wealth not only to build better systems, but to im-
prove our understanding of language. 
References  
Banko, M. and Brill, E. 2001. Mitigating the paucity of data problem.  
In Proceedings of HLT-2001. San Diego, CA. 
Berland, M. and E. Charniak, 1999. Finding parts in very large 
corpora. In ACL-1999. pp. 57?64. College Park, MD. 
Brill, E., 1995. Transformation-based error-driven learning and 
natural language processing: A case study in part of speech 
tagging. Computational Linguistics, 21(4):543?566. 
Brill, E.; Lin, J.; Banko, M.; Dumais, S.; and Ng, A. 2001. Data-
intensive question answering. In Proceedings of the TREC-10 
Conference, pp 183?189. Gaithersburg, MD. 
Caraballo, S. 1999. Automatic acquisition of a hypernym-labeled 
noun hierarchy from text. In Proceedings of ACL-99. pp 120?126, 
Baltimore, MD. 
Curran, J. and Moens, M. 2002. Scaling context space. In Proceedings 
of ACL-02. pp 231?238, Philadelphia, PA. 
Dunning, T. 1993. Accurate methods for the statistics of surprise and 
coincidence. Computational Linguistics 191 (1993), 61?74. 
Etzioni, O.; Cafarella, M.; Downey, D.; Kok, S.; Popescu, A.M.; 
Shaked, T.; Soderland, S.; Weld, D. S.; and Yates, A. 2004. Web-
scale information extraction in Know-It All (Preliminary Results). 
To appear in the Conference on WWW. 
Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies 
for online question answering: Answering questions before they are 
asked. In Proceedings of ACL-03. pp. 1?7. Sapporo, Japan. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic 
constraints for the automatic discovery of part-whole relations. In 
Proceedings of HLT/NAACL-03. pp. 80?87. Edmonton, Canada. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. (ed.) The 
Philosophy of Linguistics. New York: Oxford University Press. pp. 
26?47. 
Hearst, M. 1992. Automatic acquisition of hyponyms from large text 
corpora. In COLING-92. pp. 539?545. Nantes, France. 
Hindle, D. 1990. Noun classification from predicate-argument 
structures. In Proceedings of ACL-90. pp. 268?275. Pittsburgh, PA. 
Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based 
parser. Proceedings of COLING-94. pp. 42?48. Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of similar words. In 
Proceedings of COLING/ACL-98. pp. 768?774. Montreal, Canada. 
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question 
Answering. SemaNet? 02: Building and Using Semantic Networks, 
Taipei, Taiwan. 
Miller, G. 1990. WordNet: An online lexical database. International 
Journal of Lexicography, 3(4). 
Och, F.J. and Ney, H. 2002. Discriminative training and maximum 
entropy models for statistical machine translation. In Proceedings 
of ACL. pp. 295?302. Philadelphia, PA. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of SIGKDD-02. pp. 613?619. Edmonton, Canada. 
Pantel, P. and Ravichandran, D. 2004. Automatically labeling seman-
tic classes. In Proceedings of HLT/NAACL-04. pp. 321?328. Bos-
ton, MA. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach for 
building semantic lexicons. In Proceedings of EMNLP-1997. 
Voorhees, E. 2003. Overview of the question answering track. In 
Proceedings of TREC-12 Conference. NIST, Gaithersburg, MD. 
Table 9. QA definitional evaluations for pattern-based 
system. 
 TOP-1 TOP5 
 
Strict Lenient Strict Lenient 
1.5MB
 
0% 0% 0% 0% 
15MB 0% 0% 0% 0% 
150MB 2.0% 2.0% 2.0% 2.0% 
1.5GB 16.0% 22.0% 20.0% 22.0% 
6GB 38.0% 52.0% 56.0% 62.0% 
15GB 38.0% 52.0% 70.0% 74.0% 
 
Table 10. QA definitional evaluations for co-
occurrence-based system. 
 TOP-1 TOP5 
 
Strict Lenient Strict Lenient 
1.5MB
 
0% 0% 0% 0% 
15MB 0% 0% 0% 0% 
150MB 0% 0% 0% 0% 
1.5GB 6.0% 8.0% 6.0% 8.0% 
6GB 36.0% 44.0% 60.0% 62.0% 
 
777
Toward Semantics-Based Answer Pinpointing
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292-6695
USA
tel: +1-310-448-8731
{hovy,gerber,ulf,cyl,ravichan}@isi.edu
ABSTRACT
We describe the treatment of questions (Question-Answer
Typology, question parsing, and results) in the Weblcopedia
question answering system.  
1. INTRODUCTION
Several research projects have recently investigated the
problem of automatically answering simple questions that have
brief phrasal answers (?factoids?), by identifying and extracting
the answer from a large collection of text.  
The systems built in these projects exhibit a fairly standard
structure: they create a query from the user?s question, perform
IR with the query to locate (segments of) documents likely to
contain an answer, and then pinpoint the most likely answer
passage within the candidate documents.  The most common
difference lies in the pinpointing. Many projects employ a
window-based word scoring method that rewards desirable
words in the window.  They move the window across the
candidate answers texts/segments and return the window at the
position giving the highest total score.  A word is desirable if
it is a content word and it is either contained in the question, or
is a variant of a word contained in the question, or if it matches
the words of the expected answer.  Many variations of this
method are possible?of the scores, of the treatment of multi-
word phrases and gaps between desirable words, of the range of
variations allowed, and of the computation of the expected
answer words.  
Although it works to some degree (giving results of up to 30%
in independent evaluations), the window-based method has
several quite serious limitations:
? it cannot pinpoint answer boundaries precisely (e.g., an
exact name or noun phrase),
? it relies solely on information at the word level, and
hence cannot recognize information of the desired type
(such as Person or Location),
? it cannot locate and compose parts of answers that are
distributed over areas wider than the window.
Window-based pinpointing is therefore not satisfactory in the
long run, even for factoid QA.  In this paper we describe work
in our Webclopedia project on semantics-based answer
pinpointing. Initially, though, recognizing the simplicity and
power of the window-based technique for getting started, we
implemented a version of it as a fallback method.  We then
implemented two more sophisticated methods: syntactic-
semantic question analysis and QA pattern matching.  This
involves classification of QA types to facilitate recognition of
desired answer types, a robust syntactic-semantic parser to
analyze the question and candidate answers, and a matcher that
combines word- and parse-tree-level information to identify
answer passages more precisely.  We expect that the two
methods will really show their power when more complex non-
factoid answers are sought.  In this paper we describe how well
the three methods did relative to each other.  Section 2 outlines
the Webclopedia system.  Sections 3, 4, and 5 describe the
semantics-based components: a QA Typology, question and
answer parsing, and matching.  Finally, we outline current
work on automatically learning QA patterns using the Noisy
Channel Model.  
2. WEBCLOPEDIA
Webclopedia?s architecture (Figure 1) follows the pattern
outlined above:
Question parsing: Using BBN?s IdentiFinder [1], our
parser CONTEX (Section 4) produces a syntactic-semantic
analysis of the question and determines the QA type (Section
3).  
Query formation : Single- and multi-word units (content
words) are extracted from the analysis, and WordNet synsets are
used for query expansion.  A Boolean query is formed. See [9].
IR: The IR engine MG [12] returns the top-ranked 1000
documents.
Segmentat ion : To decrease the amount of text to be
processed, the documents are broken into semantically
coherent segments.  Two text segmenter?TexTiling [5] and
C99 [2]?were tried; the first is used; see [9].
Ranking segments : For each segment, each sentence i s
scored using a formula that rewards word and phrase overlap
with the question and its expanded query words.  Segments are
ranked.  See [9]
Parsing segments : CONTEX parses each sentence of the
top-ranked 100 segments (Section 4).  
Pinpointing: For each sentence, three steps of matching are
performed (Section 5); two compare the analyses of the
question and the sentence; the third uses the window method to
compute a goodness score.  
Ranking of answers : The candidate answers? scores are
compared and the winner(s) are output.
3. THE QA TYPOLOGY
In order to perform pinpointing deeper than the word level, the
system has to produce a representation of what the user i s
asking.  Some previous work in automated question answering
has categorized questions by question word or by a mixture of
question word and the semantic class of the answer [11, 10].  To
ensure full coverage of all forms of simple question and answer,
and to be able to factor in deviations and special requirements,
we are developing a QA Typology.  
We motivate the Typology (a taxonomy of QA types) as
follows.  
There are many ways to ask the same thing: What is the age o f
the Queen of Holland?  How old is the Netherlands? queen?  How
long has the ruler of Holland been alive?  Likewise, there are
many ways of delivering the same answer: about 60; 63 years
old; since January 1938.  Such variations form a sort of
semantic equivalence class of both questions and answers.
Since the user may employ any version of his or her question,
and the source documents may contain any version(s) of the
answer, an efficient system should group together equivalent
question types and answer types.  Any specific question can
then be indexed into its type, from which all equivalent forms
of the answer can be ascertained.  These QA equivalence types
can help with both query expansion and answer pinpointing.
However, the equivalence is fuzzy; even slight variations
introduce exceptions: who invented the gas laser? can be
answered by both Ali Javan and a scientist at MIT, while what
is the name of the person who invented the gas laser? requires
the former only.  This inexactness suggests that the QA types
be organized in an inheritance hierarchy, allowing the answer
requirements satisfying more general questions to be
overridden by more specific ones ?lower down?.  
These considerations help structure the Webclopedia QA
Typology.  Instead of focusing on question word or semantic
type of the answer, our classes attempt to represent the user?s
intention, including for example the classes Why-Famous (for
Who was Christopher Columbus? but not Who discovered
IR
? Steps: create query from question (WordNet-expand)
             retrieve top 1000 documents
? Engines: MG (Sydney)?(Lin)
                  AT&T (TREC)?(Lin)
Segmentation
? Steps:segment each document into topical segments
? Engines: fixed-length (not used)
                 TexTiling (Hearst 94)?(Lin)
                 C99 (Choi 00)?(Lin)
                 MAXNET (Lin 00, not used)
Ranking
? Steps: score each sentence in each segment,
                              using WordNet expansion
             rank segments
? Engines: FastFinder (Junk)
Matching
? Steps: match general constraint patterns against parse trees
            match desired semantic type against parse tree elements
            match desired words against words in sentences
? Engines: matcher (Junk)
Ranking and answer extraction
? Steps: rank candidate answers
            extract and format them
? Engines: part of matcher (Junk)
Question parsing
? Steps: parse question
            find desired semantic type
? Engines: IdentiFinder (BBN)
                CONTEX (Hermjakob)
QA typology
? Categorize QA types in taxonomy (Gerber)
Constraint patterns
? Identify likely answers in relation to other
   parts of the sentence (Gerber)
Retrieve documents
Segment documents
Rank segments
Parse top segments
Parse question
Input question
Match segments against question
Rank and prepare answers
Create query
Output answers
Segment Parsing
? Steps: parse segment sentences
? Engines: CONTEX (Hermjakob)
Figure 1. Webclopedia architecture.
America?, which is the QA type Proper-Person) and
Abbreviation-Expansion (for What does HLT stand for?).  In
addition, the QA Typology becomes increasingly specific as
one moves from the root downward.
To create the QA Typology, we analyzed 17,384 questions and
their answers (downloaded from answers.com); see (Gerber, in
prep.).  The Typology (Figure 2) contains 72 nodes, whose leaf
nodes capture QA variations that can in many cases be further
differentiated.
Each Typology node has been annotated with examples and
typical patterns of expression of both Question and Answer,
using a simple template notation that expressed configurations
of words and parse tree annotations (Figure 3).  Question
pattern information (specifically, the semantic type of the
answer required, which we call a Qtarget) is produced by the
CONTEX parser (Section 4) when analyzing the question,
enabling it to output its guess(s) for the QA type.  Answer
pattern information is used by the Matcher (Section 5) to
pinpoint likely answer(s) in the parse trees of candidate answer
sentences.
Question examples and question templates
Who was Johnny Mathis' high school track coach?
Who was Lincoln's Secretary of State?
who be <entity>'s <role>
Who was President of Turkmenistan in 1994?
Who is the composer of Eugene Onegin?
Who is the chairman of GE?
who be <role> of <entity>
Answer templates and actual answers
<person>, <role> of  <entity>
Lou Vasquez, track coach of?and Johnny Mathis
<person> <role-title*> of <entity>
Signed Saparmurad Turkmenbachy [Niyazov],
president of Turkmenistan
<entity>?s <role> <person>
...Turkmenistan?s President Saparmurad Niyazov
<person>'s <entity>
...in Tchaikovsky's Eugene Onegin...
<role-title> <person> ... <entity> <role>
Mr. Jack Welch, GE chairman...
<subject>|<psv object> of related role-verb
       ...Chairman John Welch said ...GE's
Figure 3. Some QA Typology node annotations for
Proper-Person.
At the time of the TREC-9 Q&A evaluation, we had produced
approx. 500 patterns by simply cross-combining approx. 20
Question patterns with approx. 25 Answer patterns.  To our
disappointment (Section 6), these patterns were both too
specific and too few to identify answers frequently?when they
applied, they were quite accurate, but they applied too seldom.
We therefore started work on automatically learning QA
patterns in parse trees (Section 7).  On the other hand, the
semantic class of the answer (the Qtarget) is used to good effect
(Sections 4 and 6).
4. PARSING
CONTEX is a deterministic machine-learning based grammar
learner/parser that was originally built for MT [6].  For
English, parses of unseen sentences measured 87.6% labeled
precision and 88.4% labeled recall, trained on 2048 sentences
from the Penn Treebank. Over the past few years it has been
extended to Japanese and Korean [7].
4.1 Parsing Questions
Accuracy is particularly important for question parsing,
because for only one question there may be several answers in a
large document collection.  In particular, it is important to
identify as specific a Qtarget as possible.  But grammar rules
ERACITY YES:NO
TRUE:FALSE
NTIT Y A GENT NAME LAST-NAME
FIRST-NAME
ORGANIZATION
GROUP-OF-PEOPLE
A NIMAL
PERSON OCCUPATION-PERSON
GEOGRAPHICA L-PERSON
PROPER-NAMED-ENTITY PROPER-PERSON
PROPER-ORGANIZATION
PROPER-PLACE CITY
COUNTRY
STATE-DISTRICT
QUANTITY NU MERICAL-QUANTI TY
MONETARY-QUANTITY
TEMPORAL-QUANTITY
MASS-QUANTI TY
SPATIAL-QUANTITY DISTANCE-QUANTITY
AREA-QUANTITY
VOLUME-QUANTI TY
TEMP-LOC DATE
DATE-RANGE
LOCATOR ADDRESS
EMAIL-ADDRESS
PHONE-NUMBER
URL
TANGIBLE-OBJECT HU MAN-FOOD
SUBS TANCE LIQUID
BODY-PART
INSTRUMENT
GARMENT
TITLED-WORK
ABSTRACT SHAPE
ADJECTIVE COLOR
DISEASE
TEXT
NARRATIVE GENERAL-INFO DEFINITION USE
EXPRESSION-ORIGIN
HISTORY WHY-FAMOUS BIO
ANTECEDENT
INFLUENCE CONSEQUENT
CAUSE-EFFECT METHOD-MEANS
CIRCUMSTANCE-MEANS REASON
EVALUATION PRO-CON
CONTRAST
RATING
COUNSEL-ADVICE
Figure 2. Portion of Webclopedia QA Typology.
for declarative sentences do not apply well to questions, which
although typically shorter than declaratives, exhibit markedly
different word order, preposition stranding (?What university
was Woodrow Wilson President of??), etc.  
Unfortunately for CONTEX, questions to train on were not
initially easily available; the Wall Street Journal sentences
contain a few questions, often from quotes, but not enough and
not representative enough to result in an acceptable level of
question parse accuracy.  By collecting and treebanking,
however, we increased the number of questions in the training
data from 250 (for our TREC-9 evaluation version of
Webclopedia) to 400 on Oct 16 to 975 on Dec 9.  The effect i s
shown in Table 1.  In the first test run (?[trained] without
[additional questions]?), CONTEX was trained mostly on
declarative sentences (2000 Wall Street Journal sentences,
namely the enriched Penn Treebank, plus a few other non-
question sentences such as imperatives and short phrases).  In
later runs (?[trained] with [add. questions]?), the system was
trained on the same examples plus a subset of the 1153
questions we have treebanked at ISI (38 questions from the pre-
TREC-8 test set, all 200 from TREC-8 and 693 TREC-9, and
222 others).
The TREC-8 and TREC-9 questions were divided into 5 subsets,
used in a five-fold cross validation test in which the system was
trained on all but the test questions, and then evaluated on the
test questions.   
Reasons for the improvement include (1) significantly more
training data; (2) a few additional features, some more treebank
cleaning, a bit more background knowledge etc.; and (3) the
251 test questions on Oct. 16 were probably a little bit harder
on average, because a few of the TREC-9 questions initially
treebanked (and included in the October figures) were selected
for early treebanking because they represented particular
challenges, hurting subsequent Qtarget processing.
4.2 Parsing Potential Answers
The semantic type ontology in CONTEX was extended to
include 115 Qtarget types, plus some combined types; more
details in [8].  Beside the Qtargets that refer to concepts in
CONTEX?s concept ontology (see first example below),
Qtargets can also refer to part of speech labels (first example),
to constituent roles or slots of parse trees (second and third
examples), and to more abstract nodes in the QA Typology
(later examples). For questions with the Qtargets Q-WHY-
FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and
others, the parser also provides Qargs?information helpful for
matching (final examples).
Semantic ontology types (I-EN-CITY)
and part of speech labels (S-PROPER-NAME):
What is the capital of Uganda?
QTARGET: (((I-EN-CITY S-PROPER-NAME))
((EQ I-EN-PROPER-PLACE)))
Parse tree roles:
Why can't ostriches fly?
      QTARGET: (((ROLE REASON)))
Name a film in which Jude Law acted.
      QTARGET: (((SLOT TITLE-P TRUE)))
QA Typology nodes:
What are the Black Hills known for?
     Q-WHY-FAMOUS
What is Occam's Razor?
     Q-DEFINITION
What is another name for nearsightedness?
     Q-SYNONYM
Should you exercise when you're sick?
     Q-YES-NO-QUESTION
Qargs for additional information:
Who was Betsy Ross?
     QTARGET: (((Q-WHY-FAMOUS-PERSON)))  
     QARGS: (("Betsy Ross"))
How is "Pacific Bell" abbreviated?
     QTARGET: (((Q-ABBREVIATION)))
     QARGS: (("Pacific Bell"))
What are geckos?
     QTARGET: (((Q-DEFINITION)))
     QARGS: (("geckos" "gecko") ("animal"))
These Qtargets are determined during parsing using 276 hand-
written rules.  Still, for approx. 10% of the TREC-8&9
questions there is no easily determinable Qtarget (?What does
the Peugeot company manufacture??; ?What is caliente in
English??).  Strategies for dealing with this are under
investigation.  More details appear in (Hermjakob, 2001).  The
current accuracy of the parser on questions and resulting
Qtargets sentences is shown in Table 2.
5. ANSWER MATCHING
The Matcher performs three independent matches, in order:
? match QA patterns in the parse tree,
? match Qtargets and Qwords in the parse tree,
? match over the answer text using a word window.
Details appear in [9].
Table 1. Improvement in parsing of questions.
Labeled Labeled Tagging Crossing
Precision Recall Precision Recall Accuracy Brackets
Without, Oct 16 90.74% 90.72% 84.62% 83.48% 94.95% 0.6
With, Oct 16 94.19% 94.86% 91.63% 91.91% 98.00% 0.48
With, Dec 9 97.33% 97.13% 95.40% 95.13% 98.64% 0.19
Table 1.  Improvement in parsing of questions.
6. RESULTS
We entered the TREC-9 short form QA track, and received an
overall Mean Reciprocal Rank score of 0.318, which put
Webclopedia in essentially tied second place with two others.
(The best system far outperformed those in second place.)  
In order to determine the relative performance of the modules,
we counted how many correct answers their output contained,
working on our training corpus.  Table 3 shows the evolution
of the system over a sample one-month period, reflecting the
amount of work put into different modules.  The modules QA
pattern, Qtarget, Qword, and Window were all run in parallel
from the same Ranker output.  
The same pattern, albeit with lower scores, occurred in the
TREC test (Table 4).  The QA patterns made only a small
contribution, the Qtarget made by far the largest contribution,
and, interestingly, the word-level window match lay
somewhere in between.
Table 4. TREC-9 test: correct answers
attributable to each module.
IR hits QA pattern Qtarget Window Total
78.1 5.5 26.2 10.4 30.3
We are pleased with the performance of the Qtarget match.  This
shows that CONTEX is able to identify to some degree the
semantic type of the desired answer, and able to pinpoint these
types also in candidate answers.  The fact that it outperforms
the window match indicates the desirability of looking deeper
than the surface level.  As discussed in Section 4, we are
strengthening the parser?s ability to identify Qtargets.  
We are disappointed in the performance of the 500 QA patterns.
Analysis suggests that we had too few patterns, and the ones we
had were too specific.  When patterns matched, they were rather
accurate, both in finding correct answers and more precisely
pinpointing the boundaries of answers.  However, they were
too sensitive to variations in phrasing.  Furthermore, it was
difficult to construct robust and accurate question and answer
phraseology patterns manually, for several reasons.  First,
manual construction relies on the inventiveness of the pattern
builder to foresee variations of phrasing, for both question and
answer.  It is however nearly impossible to think of all
possible variations when building patterns.  
Second, it is not always clear at what level of representation to
formulate the pattern: when should one specify using words?
Parts of speech? Other parse tree nodes? Semantic classes?  The
patterns in Figure 3 include only a few of these alternatives.
Specifying the wrong elements can result in non-optimal
coverage.  Third, the work is simply tedious.  We therefore
decided to try to learn QA patterns automatically.  
7. TOWARD LEARNING QA PATTERNS
AUTOMATICALLY
To learn corresponding question and answer expressions, we
pair up the parse trees of a question and (each one of) its
answer(s).  We then apply a set of matching criteria to identify
potential corresponding portions of the trees.  We then use the
EM algorithm to learn the strengths of correspondence
combinations at various levels of representation.  This work i s
still in progress.  
In order to learn this information we observe the truism that
there are many more answers than questions. This holds for the
two QA corpora we have access to?TREC and an FAQ website
(since discontinued).  We therefore use the familiar version of
the Noisy Channel Model and Bayes? Rule.   For each basic QA
type (Location, Why-Famous, etc.):
Table 2. Question parse tree and Qtarget accuracies.
# Penn # Question Crossing Qtarget Qtarget
Treebank sentences Labeled Labele d Tagging brackets accuracy accuracy
sentences added Precision Recall Accuracy (/ sent) (strict) (lenient)
2000 0 83.47% 82.49% 94.65% 0.34 63.00% 65.50%
3000 0 84.74% 84.16% 94.51% 0.35 65.30% 67.40%
2000 38 91.20% 89.37% 97.63% 0.26 85.90% 87.20%
3000 38 91.52% 90.09% 97.29% 0.26 86.40% 87.80%
2000 975 95.71% 95.45% 98.83% 0.17 96.10% 97.30%
Date Number
Qs
IR
hits
Ranker
hits
QA
pattern
Qtgt
match
Qword
fallback
Window
fallback
Total
2-Jul 52 1.00 0.61 0.12 0.49 0.15 0.19 0.62
8-Jul 38 0.89 0.40 0.28 0.40 0.12 n/a 0.53
13-Jul 52 1.00 0.61 0.04 0.48 0.15 0.22 0.53
3-Aug 55 n/a n/a 0.04 0.32 0.15 0.19 0.41
Table 3. Relative performance of Webclopedia modules on training corpus.
P(A|Q)  =  argmax P(Q|A) . P(A)
P(A)  =   ?all trees (# nodes that may express a true A) 
/  (number of nodes in tree)
P(Q|A)  =  ?all QA tree pairs (number of covarying nodes 
in Q and A trees)
/ (number of nodes in A tree)
As usual, many variations are possible, including how to
determine likelihood of expressing a true answer; whether to
consider all nodes or just certain major syntactic ones (N, NP,
VP, etc.); which information within each node to consider
(syntactic? semantic? lexical?); how to define ?covarying
information??node identity? individual slot value equality?;
what to do about the actual answer node in the A trees; if (and
how) to represent the relationships among A nodes that have
been found to be important; etc.  Figure 4 provides an answer
parse tree that indicates likely Location nodes, determined by
appropriate syntactic class, semantic type, and syntactic role
in the sentence.  
Our initial model focuses on bags of corresponding QA parse
tree nodes, and will help to indicate for a given question what
type of node(s) will contain the answer.  We plan to extend this
model to capture structured configurations of nodes that, when
matched to a question, will help indicate where in the parse tree
of a potential answer sentence the answer actually lies.  Such
bags or structures of nodes correspond, at the surface level, to
important phrases or words.  However, by using CONTEX
output we abstract away from the surface level, and learn to
include whatever syntactic and/or semantic information is best
suited for predicting likely answers.
8. REFERENCES
[1] Bikel, D., R. Schwartz, and R. Weischedel.  1999.  An
Algorithm that Learns What s in a Name.  Machine
Learning Special Issue on NL Learning, 34, 1?3.
[2] Choi, F.Y.Y. 2000. Advances in independent linear text
segmentation. Proceedings of the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-00), 26?33.
[3] Fellbaum, Ch. (ed). 1998. WordNet: An Electronic Lexical
Database. Cambridge: MIT Press.
[4] Gerber, L. 2001.  A QA Typology for Webclopedia. In prep.
[5] Hearst, M.A. 1994. Multi-Paragraph Segmentation of
Expository Text.  Proceedings of the Annual Conference
of the Association for Computational Linguistics (ACL-
94).
[6] Hermjakob, U. 1997. Learning Parse and Translation
Decisions from Examples with Rich Context.  Ph.D.
dissertation, University of Texas at Austin.
file://ftp.cs.utexas.edu/pub/ mooney/papers/hermjakob-
dissertation-97.ps.gz.
[7] Hermjakob, U.  2000. Rapid Parser Development: A
Machine Learning Approach for Korean. Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-2000).
http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz.
[8] Hermjakob, U. 2001. Parsing and Question Classification
for Question Answering. In prep.
[9] Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.
Lin. 2000. Question Answering in Webclopedia.
Proceedings of the TREC-9 Conference.  NIST.
Gaithersburg, MD.
[10] Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,, R.
Girju, R. Goodrum, and V. Rus. 2000. The Structure and
Performance of an Open-Domain Question Answering
System. Proceedings of the Conference of the Association
for Computational Linguistics (ACL-2000), 563?570.
[11] Srihari, R. and W. Li. 2000. A Question Answering System
Supported by Information Extraction. In Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-00), 166?172.
[12] Witten, I.H., A. Moffat, and T.C. Bell. 1994. Managing
Gigabytes: Compressing and Indexing Documents and
Images. New York: Van Nostrand Reinhold.
SU
RF
  L
ux
or
 is
 fa
m
ed
 fo
r i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
.  
CA
T 
S-
SN
T 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(S
UB
J) 
SC
O
RE
 4
 
SU
RF
  i
s  
CA
T 
S-
A
U
X
 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
JP
 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(C
OM
PL
) 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 0
 
SU
RF
  f
or
 it
s V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  .
  
CA
T 
D
-P
ER
IO
D
 
LE
X
  .
  
R
O
LE
S 
(D
UM
M
Y)
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
J 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(P
RE
D)
 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 1
 
SU
RF
  f
or
  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-F
O
R 
CL
A
SS
ES
 (I
-E
P-
FO
R)
 
LE
X
  f
or
  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  a
nd
  
CA
T 
S-
CO
O
RD
-C
O
N
J 
CL
A
SS
 I-
EC
-A
N
D
 
CL
A
SS
ES
 (I
-E
C-
AN
D)
 
LE
X
  a
nd
  
R
O
LE
S 
(C
ON
J) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(C
OO
RD
) 
SC
O
RE
 2
 
SU
RF
  i
ts 
 
CA
T 
S-
PO
SS
-P
RO
N
 
CL
A
SS
 I-
EN
-P
O
SS
-P
RO
N
O
U
N
 
CL
A
SS
ES
 (I
-E
N-
PO
SS
-P
RO
NO
UN
) 
LE
X
  P
O
SS
-P
RO
N
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
RO
PE
R-
O
RG
A
N
IZ
A
TI
O
N
 
CL
A
SS
ES
 (I
-E
N-
PR
OP
ER
-O
RG
AN
IZ
AT
IO
N 
I-E
N-
OR
GA
NI
ZA
TI
ON
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  V
al
le
y 
of
 th
e 
K
in
gs
  
R
O
LE
S 
(M
OD
) 
N
A
M
ED
-E
N
TI
TY
-U
N
IT
-P
 T
RU
E 
SC
O
RE
 4
 
SU
RF
  P
ha
ra
on
ic
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
H
A
RA
O
N
IC
 
CL
A
SS
ES
 (I
-E
N-
PH
AR
AO
NI
C)
 
LE
X
  P
ha
ra
on
ic
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  o
f t
he
 K
in
gs
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  o
f  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-O
F 
CL
A
SS
ES
 (I
-E
P-
OF
) 
LE
X
  o
f  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
in
gs
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  c
om
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-K
A
RN
A
K
 
CL
A
SS
ES
 (I
-E
N-
KA
RN
AK
) 
LE
X
  k
ar
na
k 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  t
em
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
Figure 4. Candidate answer tree showing likely Location answers.
Automatic Derivation of Surface Text Patterns for a Maximum Entropy
Based Question Answering System
Deepak Ravichandran
 
USC Information Sciences Institute
4676, Admiralty Way
Marina del Rey, CA, 90292
ravichan@isi.edu
Abraham Ittycheriah and Salim Roukos
IBM TJ Watson Research Center
Yorktown Heights, NY, 10598

abei,roukos  @us.ibm.com
Abstract
In this paper we investigate the use of surface
text patterns for a Maximum Entropy based
Question Answering (QA) system. These text
patterns are collected automatically in an un-
supervised fashion using a collection of trivia
question and answer pairs as seeds. These pat-
terns are used to generate features for a statis-
tical question answering system. We report our
results on the TREC-10 question set.
1 Introduction
Several QA systems have investigated the use of text pat-
terns for QA (Soubbotin and Soubbotin, 2001), (Soub-
botin and Soubbotin, 2002), (Ravichandran and Hovy,
2002). For example, for questions like ?When was
Gandhi born??, typical answers are ?Gandhi was born in
1869? and ?Gandhi (1869-1948)?. These examples sug-
gest that the text patterns such as ?  NAME  was born in
 BIRTHDATE  ? and ?  NAME  (  BIRTHDATE  -
 DEATHYEAR  )? when formulated as regular expres-
sions, can be used to select the answer phrase to ques-
tions. Another approach to a QA system is learning cor-
respondences between question and answer pairs. IBM?s
Statistical QA (Ittycheriah et al, 2001a) system uses a
probabilistic model trainable from Question-Answer sen-
tence pairs. The training is performed under a Maximum
Entropy model, using bag of words, syntactic and name
entity features. This QA system does not employ the use
of patterns. In this paper, we explore the inclusion of
surface text patterns into the framework of a statistical
question answering system.
2 KM Corpus
A corpus of question-answer pairs was obtained from
Knowledge Master (1999). We refer to this corpus as the
1Work done while the author was an intern at IBM TJ Wat-
son Research Center during Summer 2002.
KM database. Each of the pairs in KM represents a trivia
question and its corresponding answer, such as the ones
used in the trivia card game. The question-answer pairs
in KM were filtered to retain only questions that look
similar to the ones presented in the TREC task2. Some
examples of QA pairs in KM:
1. Which country was invaded by the Libyan troops in
1983? - Chad
2. Who led the 1930 Salt March in India? - Mohandas
Gandhi
3 Unsupervised Construction of Training
Set for Pattern Extraction
We use an unsupervised technique that uses the QA in
KM as seeds to learn patterns. This method was first de-
scribed in Ravichandran and Hovy (2002). However, in
this work we have enriched the pattern format by induc-
ing specific semantic types of QTerms, and have learned
many more patterns using the KM.
3.1 Algorithm for sentence construction
1. For every question, we run a Named Entity Tagger
HMMNE 3 and identify chunks of words, that sig-
nify entities. Each such entity obtained from the
Question is defined as a Question term (QTerm).
The Answer Term (ATerm) is the Answer given by
the KM corpus.
2. Each of the question-answer pairs is submitted as
query to a popular Internet search engine4. We use
the top 50 relevant documents after stripping off the
HTML tags. The text is then tokenized to smoothen
white space variations and chopped to individual
sentences.
3. For every sentence obtained from Step (3) apply
2This was done by retaining only those questions that had
10 words or less, and were not multiple choice.
3In these experiments we use HMMNE, a named entity tag-
ger similar to the BBN?s Identifinder HMM Tagger (Bikel et al,
1999).
4Alta Vista http://www.altavista.com
HMMNE and retain only those sentences that con-
tains at least one of the QTerms plus the ATerm.
For example, we obtain the following sentences for the
QA pair ?Which country was invaded by the Libyan
troops in 1983? - Chad?:
1. More than 7,000 Libyan troops entered Chad.
2. An OUA peacekeeping force of 3,500 troops replaced the
Libyan forces in the remainder of Chad.
3. In the summer of 1983, GUNT forces launched an offen-
sive against government positions in northern and eastern
Chad.
The underlined words indicate the QTerms and the
ATerms that helped to select the sentence as a potential
way of answering the Question. The algorithm described
above was applied to each of the 16,228 QA pairs in our
KM database. A total of more than 250K sentences was
obtained.
3.2 Sentence Canonicalization
Every sentence obtained from the sentence construction
algorithm is canonicalized. Canonicalization of a sen-
tence is performed on the basis of the information pro-
vided by HMMNE, the QTerms and the ATerm. Canon-
icalization in this context may be defined as the general-
ization of a sentence based on the following process:
1. Apply HMMNE to each sentence obtained from the
sentence construction algorithm.
2. Identify the QTerms and ATerm in the answer sen-
tence.
3. Replace the ATerm by the tag ?  ANSWER  ?.
4. Replace each identified Named Entity by the class
of entity it represents.
5. If a given Named Entity is also a QTerm, indicate it
by the tag ?QT?.
The following example illustrates canonicalization.
Consider the sentence:
More than 7,000 Libyan troops entered Chad.
The application of HMMNE results in:
More than  NUMEX TYPE=CARDINAL  7,000
 /NUMEX   HUMAN TYPE=PEOPLE  Libyan
 /HUMAN  troops entered  ENAMEX TYPE=
COUNTRY  Chad  /ENAMEX  .
The canonicalization step gives the sentence:
More than  CARDINAL  PEOPLE QT  troops en-
tered  ANSWER  .
3.3 Pattern Extraction
Pattern extraction algorithm.
1. Every sentence obtained from sentence canon-
icalization algorithm is delimited by the tags
?  START  ? and ?  END  ? and then passed
through a Suffix Tree. The Suffix Tree algorithm
obtains the counts of all sub-strings of the sentence.
2. From the Suffix Tree we obtain only those sub-
strings that are at least a trigram, contain both the
?  ANSWER  ? and the ?  QT  ? tag and have at
least a count of 3 occurrences.
Source Number of Questions
Trec8 200
Trec9 500
KM 4200
Table 1: Training source and sizes.
Some examples of patterns obtained from the Suffix Tree
algorithm are as follows:
1. son of  PERSON QT  and  ANSWER 
2. of the  ANSWER  DISEASE QT 
3. of  ANSWER  at  LOCATION QT 
4.  ANSWER  was the  ORDINAL 	 OCCUPATION
QT  to
5.  ANSWER  was elected  OCCUPATION QT  of the
 LOCATION QT 
6.  ANSWER  was a prolific  OCCUPATION QT 
7.  LOCATION QT  ,  ANSWER 
8.  ANSWER  ,  LOCATION QT 
9.  START 
 ANSWER  served as  OCCUPATION
QT  from  DATE 
10.  START  ANSWER  is the  PEOPLE QT 
name for
A set of 22,353 such patterns were obtained by the ap-
plication of the pattern extraction algorithm from more
than 250,000 sentences. Some patterns are very general
and applicable to many questions, such as the ones in ex-
amples (7) and (8) while others are more specific to a
few questions, such as examples (9) and (10). Having
obtained these patterns we now can learn the appropriate
?weights? to use these patterns in a Question Answering
System.
4 Maximum Entropy Training
For these experiments we use the Maximum Entropy for-
mulation (Della Pietra et al, 1995) and model the distri-
bution (Ittycheriah, 2001b),
 ffAutomatically Labeling Semantic Classes 
 
 
Patrick Pantel and Deepak Ravichandran 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{pantel,ravichan}@isi.edu 
 
 
Abstract 
Systems that automatically discover semantic 
classes have emerged in part to address the 
limitations of broad-coverage lexical re-
sources such as WordNet and Cyc. The cur-
rent state of the art discovers many semantic 
classes but fails to label their concepts. We 
propose an algorithm labeling semantic 
classes and for leveraging them to extract is-a 
relationships using a top-down approach. 
1 Introduction 
The natural language literature is rich in theories of se-
mantics (Barwise and Perry 1985; Schank and Abelson 
1977). However, WordNet (Miller 1990) and Cyc (Le-
nat 1995) aside, the community has had little success in 
actually building large semantic repositories. Such 
broad-coverage lexical resources are extremely useful in 
applications such as word sense disambiguation (Lea-
cock, Chodorow and Miller 1998) and question answer-
ing (Pasca and Harabagiu 2001). 
Current manually constructed ontologies such as 
WordNet and Cyc have important limitations. First, they 
often contain rare senses. For example, WordNet in-
cludes a rare sense of computer that means ?the person 
who computes?. Using WordNet to expand queries to an 
information retrieval system, the expansion of computer 
will include words like estimator and reckoner. Also, 
the words dog, computer and company all have a sense 
that is a hyponym of person. Such rare senses make it 
difficult for a coreference resolution system to use 
WordNet to enforce the constraint that personal pro-
nouns (e.g. he or she) must refer to a person. The second 
problem with these lexicons is that they miss many do-
main specific senses. For example, WordNet misses the 
user-interface-object sense of the word dialog (as often 
used in software manuals). WordNet alo contains a 
very poor coverage of proper nouns. 
There is a need for (semi-) automatic approaches to 
building and extending ontologies as well as for validat-
ing the structure and content of existing ones. With the 
advent of the Web, we have access to enormous 
amounts of text. The future of ontology growing lies in 
leveraging this data by harvesting it for concepts and 
semantic relationships. Moreover, once such knowledge 
is discovered, mechanisms must be in place to enrich 
current ontologies with this new knowledge. 
To address some of the coverage and specificity 
problems in WordNet and Cyc, Pantel and Lin (2002) 
proposed and algorithm, called CBC, for automatically 
extracting semantic classes. Their classes consist of 
clustered instances like the three shown below: 
(A) multiple sclerosis, diabetes, 
osteoporosis, cardiovascular disease, 
Parkinson's, rheumatoid arthritis, heart 
disease, asthma, cancer, hypertension, 
lupus, high blood pressure, arthritis, 
emphysema, epilepsy, cystic fibrosis, 
leukemia, hemophilia, Alzheimer, myeloma, 
glaucoma, schizophrenia, ... 
(B) Mike Richter, Tommy Salo, John 
Vanbiesbrouck, Curtis Joseph, Chris Osgood, 
Steve Shields, Tom Barrasso, Guy Hebert, 
Arturs Irbe, Byron Dafoe, Patrick Roy, Bill 
Ranford, Ed Belfour, Grant Fuhr, Dominik 
Hasek, Martin Brodeur, Mike Vernon, Ron 
Tugnutt, Sean Burke, Zach Thornton, Jocelyn 
Thibault, Kevin Hartman, Felix Potvin, ... 
(C) pink, red, turquoise, blue, purple, 
green, yellow, beige, orange, taupe, white, 
lavender, fuchsia, brown, gray, black, 
mauve, royal blue, violet, chartreuse, 
teal, gold, burgundy, lilac, crimson, 
garnet, coral, grey, silver, olive green, 
cobalt blue, scarlet, tan, amber, ... 
A limitation of these concepts is that CBC does not 
discover their actual names. That is, CBC discovers a 
semantic class of Canadian provinces such as Manitoba, 
Alberta, and Ontario, but stops short of labeling the 
concept as Canadian Provinces. Some applications such 
as question answering would benefit from class labels. 
For example, given the concept list (B) and a label 
goalie/goaltender, a QA system could look for answers 
to the question ?Which goaltender won the most Hart 
Trophys?? in the concept. 
In this paper, we propose an algorithm for automati-
cally inducing names for semantic classes and for find-
ing instance/concept (is-a) relationships. Using concept 
signatures (templates describing the prototypical syntac-
tic behavior of instances of a concept), we extract con-
cept names by searching for simple syntactic patterns 
such as ?concept apposition-of instance?. Searching 
concept signatures is more robust than searching the 
syntactic features of individual instances since many 
instances suffer from sparse features or multiple senses. 
Once labels are assigned to concepts, we can extract 
a hyponym relationship between each instance of a con-
cept and its label. For example, once our system labels 
list (C) as color, we may extract relationships such as: 
pink is a color, red is a color, turquoise is a color, etc. 
Our results show that of the 159,000 hyponyms we ex-
tract using this simple method, 68% are correct. Of the 
65,000 proper name hyponyms we discover, 81.5% are 
correct. 
The remainder of this paper is organized as follows. 
In the next section, we review previous algorithms for 
extracting semantic classes and hyponym relationships. 
Section 3 describes our algorithm for labeling concepts 
and for extracting hyponym relationships. Experimental 
results are presented in Section 4 and finally, we con-
clude with a discussion and future work. 
2 Previous Work 
There have been several approaches to automatically 
discovering lexico-semantic information from text 
(Hearst 1992; Riloff and Shepherd 1997; Riloff and 
Jones 1999; Berland and Charniak 1999; Pantel and Lin 
2002; Fleischman et al 2003; Girju et al 2003). One 
approach constructs automatic thesauri by computing 
the similarity between words based on their distribution 
in a corpus (Hindle 1990; Lin 1998). The output of 
these programs is a ranked list of similar words to each 
word. For example, Lin?s approach outputs the follow-
ing top-20 similar words of orange:  
(D) peach, grapefruit, yellow, lemon, pink, 
avocado, tangerine, banana, purple, Santa 
Ana, strawberry, tomato, red, pineapple, 
pear, Apricot, apple, green, citrus, mango 
A common problem of such lists is that they do not 
discriminate between the senses of polysemous words. 
For example, in (D), the color and fruit senses of orange 
are mixed up. 
Lin and Pantel (2001) proposed a clustering algo-
rithm, UNICON, which generates similar lists but 
discriminates between senses of words. Later, Pantel 
and Lin (2002) improved the precision and recall of 
UNICON clusters with CBC (Clustering by Commit-
tee). Using sets of representative elements called com-
mittees, CBC discovers cluster centroids that 
unambiguously describe the members of a possible 
class. The algorithm initially discovers committees that 
are well scattered in the similarity space. It then pro-
ceeds by assigning elements to their most similar com-
mittees. After assigning an element to a cluster, CBC 
removes their overlapping features from the element 
before assigning it to another cluster. This allows CBC 
to discover the less frequent senses of a word and to 
avoid discovering duplicate senses. 
CBC discovered both the color sense of orange, as 
shown in list (C) of Section 1, and the fruit sense shown 
below: 
(E) peach, pear, apricot, strawberry, ba-
nana, mango, melon, apple, pineapple, 
cherry, plum, lemon, grapefruit, orange, 
berry, raspberry, blueberry, kiwi, ... 
There have also been several approaches to discov-
ering hyponym (is-a) relationships from text. Hearst 
(1992) used seven lexico-syntactic patterns, for example 
?such NP as {NP,}*{(or|and)} NP? and ?NP {, NP}*{,} 
or other NP?. Berland and Charniak (1999) used similar 
pattern-based techniques and other heuristics to extract 
meronymy (part-whole) relations. They reported an 
accuracy of about 55% precision on a corpus of 100,000 
words. Girju, Badulescu and Moldovan (2003) 
improved upon this work by using a machine learning 
filter. Mann (2002) and Fleischman et al (2003) used 
part of speech patterns to extract a subset of hyponym 
relations involing proper nouns. 
3 Labeling Classes 
The research discussed above on discovering hyponym 
relationships all take a bottom up approach. That is, 
they use patterns to independently discover semantic 
relationships of words. However, for infrequent words, 
these patterns do not match or, worse yet, generate in-
correct relationships. 
Ours is a top down approach. We make use of co-
occurrence statistics of semantic classes discovered by 
algorithms like CBC to label their concepts. Hyponym 
relationships may then be extracted easily: one hypo-
nym per instance/concept label pair. For example, if we 
labeled concept (A) from Section 1 with disease, then 
we could extract is-a relationships such as: diabetes is a 
disease, cancer is a disease, and lupus is a disease. A 
concept instance such as lupus is assigned a hypernym 
disease not because it necessarily occurs in any particu-
lar syntactic relationship with disease, but because it 
belongs to the class of instances that does. 
The input to our labeling algorithm is a list of se-
mantic classes, in the form of clusters of words, which 
may be generated from any source. In our experiments, 
we used the clustering outputs of CBC (Pantel and Lin 
2002). The output of the system is a ranked list of con-
cept names for each semantic class. 
In the first phase of the algorithm, we extract feature 
vectors for each word that occurs in a semantic class. 
Phase II then uses these features to compute grammati-
cal signatures of concepts using the CBC algorithm. 
Finally, we use simple syntactic patterns to discover 
class names from each class? signature. Below, we de-
scribe these phases in detail. 
3.1 Phase I 
We represent each word (concept instance) by a feature 
vector. Each feature corresponds to a context in which 
the word occurs. For example, ?catch __? is a verb-
object context. If the word wave occurred in this con-
text, then the context is a feature of wave. 
We first construct a frequency count vector C(e) = 
(ce1, ce2, ?, cem), where m is the total number of features 
and cef is the frequency count of feature f occurring in 
word e. Here, cef is the number of times word e occurred 
in a grammatical context f. For example, if the word 
wave occurred 217 times as the object of the verb catch, 
then the feature vector for wave will have value 217 for 
its ?object-of catch? feature. In Section 4.1, we describe 
how we obtain these features. 
We then construct a mutual information vector 
MI(e) = (mie1, mie2, ?, miem) for each word e, where mief 
is the pointwise mutual information between word e and 
feature f, which is defined as: 
 
N
c
N
c
N
c
ef m
j
ej
n
i
if
ef
mi
?? == ?
=
11
log  (1) 
where n is the number of words and N = ? ?
= =
n
i
m
j
ijc
1 1
is the 
total frequency count of all features of all words. 
Mutual information is commonly used to measure 
the association strength between two words (Church and 
Hanks 1989). A well-known problem is that mutual 
information is biased towards infrequent ele-
ments/features. We therefore multiply mief with the fol-
lowing discounting factor: 
 
1,min
,min
1
11
11
+???
?
???
?
???
?
???
?
?+ ??
??
==
==
m
j
jf
n
i
ei
m
j
jf
n
i
ei
ef
ef
cc
cc
c
c  (2) 
3.2 Phase II 
Following (Pantel and Lin 2002), we construct a com-
mittee for each semantic class. A committee is a set of 
representative elements that unambiguously describe the 
members of a possible class. 
For each class c, we construct a matrix containing 
the similarity between each pair of words ei and ej in c 
using the cosine coefficient of their mutual information 
vectors (Salton and McGill 1983): 
 ( ) ??
?
?
?
=
f
fe
f
fe
f
fefe
ji
ji
ji
mimi
mimi
eesim
22
,  (3) 
For each word e, we then cluster its most similar in-
stances using group-average clustering (Han and Kam-
ber 2001) and we store as a candidate committee the 
highest scoring cluster c'  according to the following 
metric: 
 | c'| ? avgsim(c') (4) 
where |c'| is the number of elements in c' and avgsim(c') 
is the average pairwise similarity between words in c'. 
The assumption is that the best representative for a con-
cept is a large set of very similar instances. The commit-
tee for class c is then the highest scoring candidate 
committee containing only words from c. For example, 
below are the committee members discovered for the 
semantic classes (A), (B), and (C) from Section 1: 
1) cardiovascular disease, diabetes, 
multiple sclerosis, osteoporosis, 
Parkinson's, rheumatoid arthritis 
2) Curtis Joseph, John Vanbiesbrouck, Mike 
Richter, Tommy Salo 
3) blue, pink, red, yellow 
3.3 Phase III 
By averaging the feature vectors of the committee 
members of a particular semantic class, we obtain a 
grammatical template, or signature, for that class. For 
example, Figure 1 shows an excerpt of the grammatical 
signature for concept (B) in Section 1. The vector is 
obtained by averaging the feature vectors for the words 
Curtis Joseph, John Vanbiesbrouck, Mike Richter, and 
Tommy Salo (the committee of this concept). The  
?-V:subj:N:sprawl? feature indicates a subject-verb re-
lationship between the concept and the verb sprawl 
while ?N:appo:N:goaltender? indicates an apposition 
relationship between the concept and the noun goal-
tender. The (-) in a relationship means that the right 
hand side of the relationship is the head (e.g. sprawl is 
the head of the subject-verb relationship). The two col-
umns of numbers indicate the frequency and mutual 
information score for each feature respectively. 
In order to discover the characteristics of human 
naming conventions, we manually named 50 concepts 
discovered by CBC. For each concept, we extracted the 
relationships between the concept committee and the 
assigned label. We then added the mutual information 
scores for each extracted relationship among the 50 
concepts. The top-4 highest scoring relationships are: 
? Apposition (N:appo:N) 
e.g. ... Oracle, a company known 
for its progressive employment 
policies, ... 
? Nominal subject (-N:subj:N) 
e.g. ... Apple was a hot young com-
pany, with Steve Jobs in charge. 
? Such as (-N:such as:N) 
e.g. ... companies such as IBM must 
be weary ... 
? Like (-N:like:N) 
e.g. ... companies like Sun Micro-
systems do no shy away from such 
challenges, ... 
To name a class, we simply search for these syntac-
tic relationships in the signature of a concept. We sum 
up the mutual information scores for each term that oc-
curs in these relationships with a committee of a class. 
The highest scoring term is the name of the class. For 
example, the top-5 scoring terms that occurred in these 
relationships with the signature of the concept repre-
sented by the committee {Curtis Joseph, John 
Vanbiesbrouck, Mike Richter, Tommy Salo} are: 
1)      goalie 40.37 
2)      goaltender 33.64 
3)      goalkeeper 19.22 
4)      player 14.55 
5)      backup 9.40 
The numbers are the total mutual information scores 
of each name in the four syntactic relationships. 
4 Evaluation 
In this section, we present an evaluation of the class 
labeling algorithm and of the hyponym relationships 
discovered by our system. 
4.1 Experimental Setup 
We used Minipar (Lin 1994), a broad coverage parser, 
to parse 3GB of newspaper text from the Aquaint 
(TREC-9) collection. We collected the frequency counts 
of the grammatical relationships (contexts) output by 
Minipar and used them to compute the pointwise mutual 
information vectors described in Section 3.1. 
We used the 1432 noun clusters extracted by CBC1 
as the list of concepts to name. For each concept, we 
then used our algorithm described in Section 3 to extract 
the top-20 names for each concept. 
                                                          
1 Available at http://www.isi.edu/~pantel/demos.htm 
{Curtis Joseph, John Vanbiesbrouck, 
 Mike Richter, Tommy Salo} 
 -N:gen:N  
  pad 57 11.19 
  backup 29 9.95 
  crease 7 9.69 
  glove 52 9.57 
  stick 20 9.15 
  shutout 17 8.80 
 -N:conj:N  
  Hasek 15 12.36 
  Martin Brodeur 12 12.26 
  Belfour 13 12.22 
  Patrick Roy 10 11.90 
  Dominik Hasek 7 11.20 
  Roy 6 10.01 
 -V:subj:N  
  sprawl 11 6.69 
  misplay 6 6.55 
  smother 10 6.54 
  skate 28 6.43 
  turn back 10 6.28 
  stop 453 6.19 
 N:appo:N  
  goaltender 449 10.79 
  goalie 1641 10.76 
  netminder 57 10.39 
  goalkeeper 487 9.69 
 N:conj:N  
  Martin Brodeur 11 12.49 
  Dominik Hasek 11 12.33 
  Ed Belfour 10 12.04 
  Curtis Joseph 7 11.46 
  Tom Barrasso 5 10.85 
  Byron Dafoe 5 10.80 
  Chris Osgood 4 10.25 
Figure 1. Excerpt of the grammatical signature for the 
goalie/goaltender concept. 
4.2 Labeling Precision 
Out of the 1432 noun concepts, we were unable to name 
21 (1.5%) of them. This occurs when a concept?s com-
mittee members do not occur in any of the four syntactic 
relationships described in Section 0. We performed a 
manual evaluation of the remaining 1411 concepts. 
We randomly selected 125 concepts and their top-5 
highest ranking names according to our algorithm. Ta-
ble 1 shows the first 10 randomly selected concepts 
(each concept is represented by three of its committee 
members). 
For each concept, we added to the list of names a 
human generated name (obtained from an annotator 
looking at only the concept instances). We also ap-
pended concept names extracted from WordNet. For 
each concept that contains at least five instances in the 
WordNet hierarchy, we named the concept with the 
most frequent common ancestor of each pair of in-
stances. Up to five names were generated by WordNet 
for each concept. Because of the low coverage of proper 
nouns in WordNet, only 33 of the 125 concepts we 
evaluated had WordNet generated labels. 
We presented to three human judges the 125 ran-
domly selected concepts together with the system, hu-
man, and WordNet generated names randomly ordered. 
That way, there was no way for a judge to know the 
source of a label nor the system?s ranking of the labels. 
For each name, we asked the judges to assign a score of 
correct, partially correct, or incorrect. We then com-
puted the mean reciprocal rank (MRR) of the system, 
human, and WordNet labels. For each concept, a nam-
ing scheme receives a score of 1 / M where M is the 
rank of the first name judged correct. Table 2 shows the 
results. Table 3 shows similar results for a more lenient 
evaluation where M is the rank of the first name judged 
correct or partially correct. 
Our system achieved an overall MRR score of 
77.1%. We performed much better than the baseline 
WordNet (19.9%) because of the lack of coverage 
(mostly proper nouns) in the hierarchy. For the 33 con-
cepts that WordNet named, it achieved a score of 75.3% 
and a lenient score of 82.7%, which is high considering 
the simple algorithm we used to extract labels using 
WordNet. 
The Kappa statistic (Siegel and Castellan Jr. 1988) 
measures the agreements between a set of judges? as-
sessments correcting for chance agreements: 
 ( ) ( )( )EP
EPAPK ?
?=
1
 (5) 
where P(A) is the probability of agreement between the 
judges and P(E) is the probability that the judges agree 
Table 1. Labels assigned to 10 randomly selected concepts (each represented by three committee members.
CBC CONCEPT HUMAN LABEL WORDNET LABELS SYSTEM LABELS (RANKED) 
BMG, EMI, Sony record label none label / company / album / 
machine / studio 
Preakness Stakes, Preakness, Belmont 
Stakes 
horse race none race / event / run / victory / 
start 
Olympia Snowe, Susan Collins, James 
Jeffords 
US senator none republican / senator / chair-
man / supporter / conservative 
Eldoret, Kisumu, Mombasa African city none city / port / cut off / town / 
southeast 
Bronze Star, Silver Star, Purple Heart medal decoration / laurel 
wreath / medal / medal-
lion / palm 
distinction / set / honor / sym-
bol 
Mike Richter, Tommy Salo, John 
Vanbiesbrouck 
NHL goalie none goalie / goaltender / goal-
keeper / player / backup 
Dodoma, Mwanza, Mbeya African city none facilitator / town 
fresco, wall painting, Mural art painting / picture painting / world / piece / floor 
/ symbol 
Qinghua University, Fudan University, 
Beijing University 
university none university / institution / stock-
holder / college / school 
Federal Bureau of Investigation, Drug 
Enforcement Administration, FBI 
governmental depart-
ment 
law enforcement agency agency / police / investigation 
/ department / FBI 
 
by chance on an assessment. An experiment with K ? 
0.8 is generally viewed as reliable and 0.67 < K < 0.8 
allows tentative conclusions. The Kappa statistic for our 
experiment is K = 0.72. 
The human labeling is at a disadvantage since only 
one label was generated per concept. Therefore, the 
human scores either 1 or 0 for each concept. Our sys-
tem?s highest ranking name was correct 72% of the 
time. Table 4 shows the percentage of semantic classes 
with a correct label in the top 1-5 ranks returned by our 
system. 
Overall, 41.8% of the top-5 names extracted by our 
system were judged correct. The overall accuracy for 
the top-4, top-3, top-2, and top-1 names are 44.4%, 
48.8%, 58.5%, and 72% respectively. Hence, the name 
ranking of our algorithm is effective. 
4.3 Hyponym Precision 
The 1432 CBC concepts contain 18,000 unique words. 
For each concept to which a word belongs, we extracted 
up to 3 hyponyms, one for each of the top-3 labels for 
the concept. The result was 159,000 hyponym relation-
ships. 24 are shown in the Appendix. 
Two judges annotated two random samples of 100 
relationships: one from all 159,000 hyponyms and one 
from the subset of 65,000 proper nouns. For each in-
stance, the judges were asked to decide whether the 
hyponym relationship was correct, partially correct or 
incorrect. Table 5 shows the results. The strict measure 
counts a score of 1 for each correctly judged instance 
and 0 otherwise. The lenient measure also gives a score 
of 0.5 for each instance judged partially correct. 
Many of the CBC concepts contain noise. For ex-
ample, the wine cluster: 
Zinfandel, merlot, Pinot noir, Chardonnay, 
Cabernet Sauvignon, cabernet, riesling, 
Sauvignon blanc, Chenin blanc, sangiovese, 
syrah, Grape, Chianti ... 
contains some incorrect instances such as grape, appe-
lation, and milk chocolate. Each of these instances will 
generate incorrect hyponyms such as grape is wine and 
milk chocolate is wine. This hyponym extraction task 
would likely serve well for evaluating the accuracy of 
lists of semantic classes. 
Table 5 shows that the hyponyms involving proper 
nouns are much more reliable than common nouns. 
Since WordNet contains poor coverage of proper nouns, 
these relationships could be useful to enrich it. 
4.4 Recall 
Semantic extraction tasks are notoriously difficult to 
evaluate for recall. To approximate recall, we conducted 
two question answering (QA) tasks: answering 
definition questions and performing QA information 
retrieval. 
Table 2. MRR scores for the human evaluation of naming 125 
random concepts. 
JUDGE HUMAN 
LABELS 
WordNet 
Labels 
System 
Labels 
1 100% 18.1% 74.4% 
2 91.2% 20.0% 78.1% 
3 89.6% 21.6% 78.8% 
Combined 93.6% 19.9% 77.1% 
 
Table 3. Lenient MRR scores for the human evaluation of 
naming 125 random concepts. 
JUDGE HUMAN 
LABELS 
WordNet 
Labels 
System 
Labels 
1 100% 22.8% 85.0% 
2 96.0% 20.8% 86.5% 
3 92.0% 21.8% 85.2% 
Combined 96.0% 21.8% 85.6% 
 
Table 4. Percentage of concepts with a correct name in the 
top-5 ranks returned by our system. 
JUDGE TOP-1 TOP-2 TOP-3 TOP-4 TOP-5 
1 68.8% 75.2% 78.4% 83.2% 84.0% 
2 73.6% 80.0% 81.6% 83.2% 84.8% 
3 73.6% 80.0% 82.4% 84.0% 88.8% 
Combined 72.0% 78.4% 80.8% 83.5% 85.6% 
 
Table 5. Accuracy of 159,000 extracted hyponyms and a sub-
set of 65,000 proper noun hyponyms. 
JUDGE All Nouns Proper Nouns 
 Strict Lenient Strict Lenient 
1 62.0% 68.0% 79.0% 82.0% 
2 74.0% 76.5% 84.0% 85.5% 
Combined 68.0% 72.2% 81.5% 83.8% 
 
Definition Questions 
We chose the 50 definition questions that appeared in 
the QA track of TREC2003 (Voorhees, 2003). For ex-
ample: ?Who is Aaron Copland?? and ?What is the 
Kama Sutra?? For each question we looked for at most 
five corresponding concepts in our hyponym list. For 
example, for Aaron Copland, we found the following 
hypernyms: composer, music, and gift. We compared 
our system with the concepts in WordNet and Fleisch-
man et al?s instance/concept relations (Fleischman et al 
2003). Table 6 shows the percentage of correct answers 
in the top-1 and top-5 returned answers from each sys-
tem. All systems seem to have similar performance on 
the top-1 answers, but our system has many more an-
swers in the top-5. This shows that our system has com-
paratively higher recall for this task. 
Information (Passage) Retrieval 
Passage retrieval is used in QA to supply relevant in-
formation to an answer pinpointing module. The higher 
the performance of the passage retrieval module, the 
higher will be the performance of the answer pinpoint-
ing module. 
The passage retrieval module can make use of the 
hyponym relationships that are discovered by our sys-
tem. Given a question such as ?What color ??, the like-
lihood of a correct answer being present in a retrieved 
passage is greatly increased if we know the set of all 
possible colors and index them in the document collec-
tion appropriately. 
We used the hyponym relations learned by our sys-
tem to perform semantic indexing on a QA passage re-
trieval task. We selected the 179 questions from the QA 
track of TREC-2003 that had an explicit semantic an-
swer type (e.g. ?What band was Jerry Garcia with?? 
and ?What color is the top stripe on the U.S. flag??). 
For each expected semantic answer type corresponding 
to a given question (e.g. band and color), we indexed 
the entire TREC-2002 IR collection with our system?s 
hyponyms. 
We compared the passages returned by the passage 
retrieval module with and without the semantic index-
ing. We counted how many of the 179 questions had a 
correct answer returned in the top-1 and top-100 pas-
sages. Table 7 shows the results. 
Our system shows small gains in the performance of 
the IR output. In the top-1 category, the performance 
improved by 20%. This may lead to better answer selec-
tions. 
5 Conclusions and Future Work 
Current state of the art concept discovery algorithms 
generate lists of instances of semantic classes but stop 
short of labeling the classes with concept names. Class 
labels would serve useful in applications such as ques-
tion answering to map a question concept into a seman-
tic class and then search for answers within that class. 
We propose here an algorithm for automatically label-
ing concepts that searches for syntactic patterns within a 
grammatical template for a class. Of the 1432 noun con-
cepts discovered by CBC, our system labelled 98.5% of 
them with an MRR score of 77.1% in a human evalua-
tion. 
Hyponym relationships were then easily extracted, 
one for each instance/concept label pair. We extracted 
159,000 hyponyms and achieved a precision of 68%. On 
a subset of 65,000 proper names, our performance was 
81.5%. 
This work forms an important attempt to building 
large-scale semantic knowledge bases. Without being 
able to automatically name a cluster and extract hypo-
nym/hypernym relationships, the utility of automatically 
generated clusters or manually compiled lists of terms is 
limited. Of course, it is a serious open question how 
many names each cluster (concept) should have, and 
how good each name is. Our method begins to address 
this thorny issue by quantifying the name assigned to a 
class and by simultaneously assigning a number that can 
be interpreted to reflect the strength of membership of 
each element to the class. This is potentially a signifi-
cant step away from traditional all-or-nothing seman-
tic/ontology representations to a concept representation 
Table 6. Percentage of correct answers in the Top-1 and 
Top-5 returned answers on 50 definition questions. 
SYSTEM Top-1 Top-5 
 Strict Lenient Strict Lenient 
WordNet 38% 38% 38% 38% 
Fleischman 36% 40% 42% 44% 
Our System 36% 44% 60% 62% 
 
Table 7. Percentage of questions where the passage retrieval 
module returns a correct answer in the Top-1 and Top-100 
ranked passages (with and without semantic indexing). 
 CORRECT TOP-1 Correct Top-100 
With semantic 
indexing 
43 / 179 134 / 179 
Without semantic 
indexing 
36 / 179 131 / 179 
 
scheme that is more nuanced and admits multiple names 
and graded set memberships. 
Acknowledgements 
The authors wish to thank the reviewers for their helpful 
comments. This research was partly supported by NSF 
grant #EIA-0205111. 
References 
Barwise, J. and Perry, J. 1985. Semantic innocence and un-
compromising situations. In: Martinich, A. P. (ed.) The 
Philosophy of Language. New York: Oxford University 
Press. pp. 401?413. 
Berland, M. and E. Charniak, 1999. Finding parts in very large 
corpora. In ACL-1999. pp. 57?64. College Park, MD. 
Church, K. and Hanks, P. 1989. Word association norms, mu-
tual information, and lexicography. In Proceedings of ACL-
89. pp. 76?83. Vancouver, Canada. 
Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline 
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of ACL-03. pp. 
1?7. Sapporo, Japan. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning 
semantic constraints for the automatic discovery of part-
whole relations. In Proceedings of HLT/NAACL-03. pp. 
80?87. Edmonton, Canada. 
Han, J. and Kamber, M. 2001. Data Mining ? Concepts and 
Techniques. Morgan Kaufmann. 
Hearst, M. 1992. Automatic acquisition of hyponyms from 
large text corpora. In COLING-92. pp. 539?545. Nantes, 
France. 
Hindle, D. 1990. Noun classification from predicate-argument 
structures. In Proceedings of ACL-90. pp. 268?275. Pitts-
burgh, PA. 
Leacock, C.; Chodorow, M.; and Miller; G. A. 1998. Using 
corpus statistics and WordNet relations for sense identifica-
tion. Computational Linguistics, 24(1):147?165. 
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33?38. 
Lin, D. 1994. Principar - an efficient, broad-coverage, princi-
ple-based parser. Proceedings of COLING-94. pp. 42?48. 
Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of similar 
words. In Proceedings of COLING/ACL-98. pp. 768?774. 
Montreal, Canada. 
Lin, D. and Pantel, P. 2001. Induction of semantic classes 
from natural language text. In Proceedings of SIGKDD-01. 
pp. 317?322. San Francisco, CA. 
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies 
for Question Answering. SemaNet? 02: Building and 
Using Semantic Networks, Taipei, Taiwan. 
Miller, G. 1990. WordNet: An online lexical database. Inter-
national Journal of Lexicography, 3(4). 
Pasca, M. and Harabagiu, S. 2001. The informative role of 
WordNet in Open-Domain Question Answering. In Pro-
ceedings of NAACL-01 Workshop on WordNet and Other 
Lexical Resources. pp. 138?143. Pittsburgh, PA. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from 
Text. In Proceedings of SIGKDD-02. pp. 613?619. Edmon-
ton, Canada. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach for 
building semantic lexicons. In Proceedings of EMNLP-
1997. 
Riloff, E. and Jones, R. 1999. Learning dictionaries for infor-
mation extraction by multi-level bootstrapping. In Proceed-
ings of AAAI-99. pp. 474?479. Orlando, FL. 
Salton, G. and McGill, M. J. 1983. Introduction to Modern 
Information Retrieval. McGraw Hill 
Schank, R. and Abelson, R. 1977. Scripts, Plans, Goals and 
Understanding: An Inquiry into Human Knowledge Struc-
tures. Lawrence Erlbaum Associates. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statis-
tics for the Behavioral Sciences. McGraw-Hill. 
Voorhees, E. 2003. Overview of the question answering track. 
To appear in Proceedings of TREC-12 Conference. NIST, 
Gaithersburg, MD. 
Appendix. Sample hyponyms discovered by our system.
INSTANCE CONCEPT INSTANCE CONCEPT 
actor hero price support benefit 
Ameritrade brokerage republican politician 
Arthur 
Rhodes 
pitcher Royal Air 
Force 
force 
bebop MUSIC Rwanda city 
Buccaneer team Santa Ana city 
Congressional  
Research 
Service 
agency shot-blocker player 
Cuba country slavery issue 
Dan Petrescu midfielder spa facility 
Hercules aircraft taxi vehicle 
Moscow city Terrence 
Malick 
director 
Nokia COMPANY verbena tree 
nominee candidate Wagner composer 
 
 Learning Surface Text Patterns 
for a Question Answering System 
Deepak Ravichandran and Eduard Hovy  
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
USA 
{ravichan,hovy}@isi.edu  
 
 
Abstract 
In this paper we explore the power of 
surface text patterns for open-d main 
question answering systems.  In order to 
obtain an optimal set of patterns, we have 
developed a method for learning such 
patterns automatically. A tagged corpus 
is built from the Internet in a 
bootstrapping process by providing a few 
hand-crafted examples of each question 
type to Altavista. Patterns are then 
automatically extracted from the returned 
documents and standardized. We 
calculate the precision of each pattern, 
and the average precision for each 
question type. These patterns are then 
applied to find answers to new questions.  
Using the TREC-10 question set, we 
report results for two cases: answers 
determined from the TREC-10 corpus 
and from the web. 
 
1 Introduction 
Most of the recent open domain question-
answering systems use external knowledge 
and tools for answer pinpointing. These may 
include named entity taggers, WordNet, 
parsers, hand-tagged corpora, and ontology 
lists (Srihari and Li, 00; Harabagiu et al, 01; 
Hovy et al, 01; Prager et al, 01). However, at 
the recent TREC-10 QA evaluation 
(Voorhees, 01), the winning system used just 
one resource: a fairly extensive list of surface 
patterns (Soubbotin and Soubbotin, 01). The 
apparent power of such patterns surprised 
many. We therefore decided to investigate 
their potential by acquiring patterns 
automatically and to measure their accuracy. 
It has been noted in several QA systems 
that certain types of answer are expressed 
using characteristic phrases (Lee et al, 01; 
Wang et al, 01). For example, for 
BIRTHDATEs (with questions like ?When 
was X born??), typical answers are  
?Mozart was born in 1756.? 
?Gandhi (1869? 1948)??  
These examples suggest that phrases like  
?<NAME> was born in <BIRTHDATE>? 
?<NAME> (<BIRTHDATE>? ?  
when formulated as regular expressions, can 
be used to locate the correct answer.  
In this paper we present an approach for 
automatically learning such regular 
expressions (along with determining their 
precision) from the web, for given types of 
questions.  Our method uses the machine 
learning technique of bootstrapping to build a 
large tagged corpus starting with only a few 
examples of QA pairs.  Similar techniques 
have been investigated extensively in the field 
of information extraction (Riloff, 96).  These 
techniques are greatly aided by the fact that 
there is no need to hand-tag a corpus, while 
the abundance of data on the web makes it 
easier to determine reliable statistical 
estimates. 
Our system assumes each sentence to be a 
simple sequence of words and searche  for 
repeated word orderings as evidence for 
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 41-47.
                         Proceedings of the 40th Annual Meeting of the Association for
 useful answer phrases.  We use suffix trees 
for extracting substrings of optimal length.  
We borrow the idea of suffix trees from 
computational biology (Gusfield, 97) where it 
is primarily used for detecting DNA 
sequences. Suffix trees can be processed in 
time linear on the size of the corpus and, more 
importantly, they do not restrict the length of 
substrings.  We then test the patterns learned 
by our system on new unseen questions from 
the TREC-10 set and evaluate their results to 
determine the precision of the patterns. 
 
2 Learning of Patterns 
We describe the pattern-lear ing algorithm 
with an example.  A table of patterns is 
constructed for each individual question type 
by the following procedure (Algorithm 1).  
1. Select an example for a given question 
type. Thus for BIRTHYEAR questions we 
select ?Mozart 1756? (we refer to 
?Mozart? as the question term and ?1756? 
as the answer term). 
2. Submit the question and the answer term 
as queries to a search engine.  Thus, we 
give the query +?Mozart? +?1756? to 
AltaVista (http://www.altavista.com). 
3. Download the top 1000 web documents 
provided by the search engine. 
4. Apply a sentence breaker to the 
documents. 
5. Retain only those sentences that contain 
both the question and the answer term.  
Tokenize the input text, smooth variations 
in white space characters, and remove html 
and other extraneous tags, to allow simple 
regular expression matching tools such as 
egrep to be used. 
6. Pass each retained sentence through a 
suffix tree construc or.  This finds all 
substrings, of all lengths, along with their 
counts. For example consider the  
sentences ?The great composer Mozart 
(1756? 1791) achieved fame at a young 
age? ?Mozart (1756? 1791) was a genius?, 
and ?The whole world would always be 
indebted to the great music of Mozart 
(1756? 1791)?. The longest matching 
substring for all 3 sentences is ?Mozart 
(1756? 1791)?, which the suffix tree would 
extract as one of the outputs along with the 
score of 3. 
7. Pass each phrase in the suffix tree through 
a filter to retain only those phrases that 
co tain both the question and the answer 
term. For the example, we extract only 
those phrases from the suffix tree that 
contain the words ?Mozart? and ?1756?.  
8. Replace the word for the question term by 
the tag ?<NAME>? and the word for the 
answer term by the term ?<ANSWER>?.   
 
This procedure is repeated for different 
examples of the same question type.  For 
BIRTHDATE we also use ?Gandhi 1869?, 
?Newton 1642?, etc. 
For BIRTHDATE, the above steps 
produce the following output: 
a. born in <ANSWER> , <NAME>   
b. <NAME> was born on <ANSWER> ,  
c. <NAME> ( <ANSWER> -   
d. <NAME> ( <ANSWER -  )  
...  
These are some of the most common 
substrings of the extracted sentences that 
contain both <NAME> and <ANSWER>.  
Since the suffix tree records all substrings, 
partly overlapping strings such as c and d are 
separately saved, which allows us to obtain 
separate counts of their occurrence 
fr quencies.  As will be seen later, this allows 
us to differentiate patterns such as d (which 
records a still living person, and is quite 
precise) from its more general substring c 
(which is less precise).   
 
Algorithm 2: Calculating the precision of each 
pattern. 
1. Query the search engine by using only the 
question term (in the example, only 
?Mozart?). 
2. Download the top 1000 web documents 
provided by the search engine. 
3. As before, segment these documents into 
individual sentences. 
4. Retain only those sentences that contain 
the question term. 
5. For each pattern obtained from Algorithm 
1, check the presence of each pattern in the 
 sentence obtained from above for two 
instances: 
i) Presence of the pattern with 
<ANSWER> tag matched by any 
word. 
ii) Presence of the pattern in the sentence 
with <ANSWER> tag matched by the 
correct answer term. 
In our example, for the pattern ?<NAME> 
was born in <ANSWER>? we check the 
presence of the following strings in the 
answer sentence 
i) Mozart was born in <ANY_WORD> 
ii) Mozart was born in 1756 
Calculate the precision of each pattern by 
the formula P = Ca / o where  
Ca = total number of patterns with the 
answer term present  
Co = total number of patterns present 
with answer term replaced by any word 
6. Retain only the patterns matching a 
sufficient number of examples (we choose 
the number of examples > 5). 
 
We obtain a table of regular expression 
patterns for a given question type, along with 
the precision of each pattern.  This precision 
is the probability of each pattern containing 
the answer and follows directly from the 
principle of maximum likelihood estimation. 
For BIRTHDATE the following table is 
obtained: 
1.0  <NAME>( <ANSWER> -  )  
0.85  <NAME> was born on <ANSWER>, 
0.6  <NAME> was born in <ANSWER> 
0.59  <NAME> was born <ANSWER>  
0.53  <ANSWER> <NAME> was born 
0.50  ?  <NAME> ( <ANSWER> 
0.36 <NAME> ( <ANSWER> -  
For a given question type a good range of 
patterns was obtained by giving the system as 
few as 10 examples.  The rather long list of 
patterns obtained would have been very 
difficult for any human to come up with 
manually.   
The question term could appear in the 
documents obtained from the web in various
ways.  Thus ?Mozart? could be written as 
?Wolfgang Amadeus Mozart?, ?Mozart, 
Wolfgang Amadeus?, ?Amadeus Mozart? or 
?Mozart?.  To learn from such variations, in 
step 1 of Algorithm 1 we specify the various 
ways in which the question term could be 
specified in the text.  The presence of any of 
these names would cause it to be tagged as the 
original question term ?Mozart?.  
The same arrangement is also done for the 
answer term so that presence of any variant of 
the answer term would cause it to be treated 
exactly like the original answer term.  While 
easy to do for BIRTHDATE, this step can be 
problematic for question types such as 
DEFINITION, which may contain various 
acceptable answers.  In general the input 
example terms have to be carefully selected 
so that the questions they represent do not 
have a long list of possible answers, as this 
would affect the confidence of the precision 
scores for each pattern.  All the answers need 
to be enlisted to ensure a high confidence in 
the precision score of each pttern, in the 
present framework.   
 The precision of the patterns obtained 
from one QA-pair example in algorithm 1 is 
calculated from the documents obtained in 
algorithm 2 for other examples of the same 
question type.  In other words, the precision 
scores are calculated by cross-checking the 
patterns across various examples of the same 
typ .  This step proves to be very significant 
as it helps to eliminate dubious patterns, 
which may appear because the contents of 
two or more websites may be the same, or the 
same web document reappears in the search 
engine output for algorithms 1 and 2. 
Algorithm 1 does not explicitly specify 
any particular question type.  Judicious choice 
of the QA example pair therefore allows it to 
be used for many question types without
change.    
 
3 Finding Answers 
Using the patterns to answer a new question 
we employ the following algorithm:  
1. Determine the question type of the new 
question.  We use our existing QA system 
(Hovy et al, 2002b; 2001) to do so.   
2. The question term in the question is 
identified, also using our existing system.
 3. Create a query from the question term and 
perform IR (by using a given answer 
document corpus such as the TREC-10 
collection or web search otherwise).   
4. Segment the documents obtained into 
sentences and smooth out white space 
variations and html and other tags, as 
before. 
5. Replace the question term in each sentence 
by the question tag (?<NAME>?, in the 
case of BIRTHYEAR).  
6. Using the pattern table developed for that 
particular question type, search for the 
presence of each pattern.  Select words 
matching the tag ?<ANSWER>? as the 
answer. 
7. Sort these answers by their pattern?s 
precision scores.  Discard duplicates (by 
elementary string comparisons).  Return 
the top 5 answers. 
 
4 Experiments 
From our Webclopedia QA Typology 
(Hovy et al, 2002a) we selected 6 different 
question types: BIRTHDATE, LOCATION, 
INVENTOR, DISCOVERER, DEFINITION, 
WHY-FAMOUS.  The pattern table for each 
of these question types was constructed using 
Algorithm 1.  
Some of the patterns obtained long with 
their precision are as follows 
 
BIRTHYEAR  
1.0 <NAME> ( <ANSWER> - ) 
0.85 <NAME> was born on <ANSWER> , 
0.6 <NAME> was born in <ANSWER> 
0.59 <NAME> was born <ANSWER> 
0.53 <ANSWER> <NAME> was born 
0.5 - <NAME> ( <ANSWER> 
0.36 <NAME> ( <ANSWER> - 
0.32 <NAME> ( <ANSWER> ) , 
0.28 born in <ANSWER> , <NAME> 
0.2 of <NAME> ( <ANSWER> 
 
INVENTOR 
1.0 <ANSWER> invents <NAME> 
1.0 the <NAME> was invented by 
<ANSWER> 
1.0 <ANSWER> invented the <NAME> in 
1.0 <ANSWER> ' s invention of the 
<NAME> 
1.0 <ANSWER> invents the <NAME> . 
1.0 <ANSWER> ' s <NAME> was 
1.0 <NAME> , invented by <ANSWER> 
1.0 <ANSWER> ' s <NAME> and
1.0 that <ANSWER> ' s <NAME> 
1.0 <NAME> was invented by <ANSWER> , 
 
DISCOVERER 
1.0 when <ANSWER> discovered 
<NAME> 
1.0 <ANSWER> ' s discovery of <NAME> 
1.0 <ANSWER> , the discoverer of 
<NAME> 
1.0 <ANSWER> discovers <NAME> . 
1.0 <ANSWER> discover <NAME> 
1.0 <ANSWER> discovered <NAME> , the 
1.0 discovery of <NAME> by <ANSWER>. 
0.95 <NAME> was discovered by 
<ANSWER> 
0.91 of <ANSWER> ' s <NAME> 
0.9 <NAME> was discovered by 
<ANSWER> in 
 
DEFINITION 
1.0 <NAME> and related <ANSWER>s 
1.0 <ANSWER> ( <NAME> , 
1.0 <ANSWER> , <NAME> . 
1.0 , a <NAME> <ANSWER> , 
1.0 ( <NAME> <ANSWER> ) , 
1.0 form of <ANSWER> , <NAME> 
1.0 for <NAME> , <ANSWER> and 
1.0 cell <ANSWER> , <NAME> 
1.0 and <ANSWER> > <ANSWER> > 
<NAME> 
0.94 as <NAME> , <ANSWER> and
 
WHY-FAMOUS 
1.0 <ANSWER> <NAME> called 
1.0 laureate <ANSWER> <NAME> 
1.0 by the <ANSWER> , <NAME> , 
1.0 <NAME> - the <ANSWER> of 
1.0 <NAME> was the <ANSWER> of 
0.84 by the <ANSWER> <NAME> , 
0.8 the famous <ANSWER> <NAME> , 
0.73 the famous <ANSWER> <NAME> 
0.72 <ANSWER> > <NAME> 
0.71 <NAME> is the <ANSWER> of 
 
LOCATION 
1.0 <ANSWER> ' s <NAME> . 
 1.0 regional : <ANSWER> : <NAME> 
1.0 to <ANSWER> ' s <NAME> , 
1.0 <ANSWER> ' s <NAME> in 
1.0 in <ANSWER> ' s <NAME> , 
1.0 of <ANSWER> ' s <NAME> , 
1.0 at the <NAME> in <ANSWER> 
0.96 the <NAME> in <ANSWER> , 
0.92 from <ANSWER> ' s <NAME> 
0.92 near <NAME> in <ANSWER> 
 
For each question type, we extracted the 
corresponding questions from the TREC-10 
set.  These questions were run through the 
testing phase of the algorithm.  Two sets of 
experiments were performed.  In the first 
case, the TREC corpus was used as the input 
source and IR was performed by the IR 
component of our QA system (Lin, 2002).  In 
the second case, the web was the input source 
and the IR was performed by the AltaVista 
search engine.  
Results of the experiments, measured by 
Mean Reciprocal Rank (MRR) score 
(Voorhees, 01), are:  
 
TREC Corpus 
Question type Number of 
questions 
MRR on 
TREC docs 
BIRTHYEAR 8 0.48 
INVENTOR 6 0.17 
DISCOVERER 4 0.13 
DEFINITION 102 0.34 
WHY-FAMOUS 3 0.33 
LOCATION 16 0.75 
 
Web 
Question type Number of 
questions 
MRR on the 
Web 
BIRTHYEAR 8 0.69 
INVENTOR 6 0.58 
DISCOVERER 4 0.88 
DEFINITION 102 0.39 
WHY-FAMOUS 3 0.00 
LOCATION 16 0.86 
 
The results indicate that the system 
performs better on the Web data than on the 
TREC corpus.  The abundance of data on the 
web makes it easier for the system to locate 
answers with high precision scores (the 
system finds many examples of correct 
answers among the top 20 when using the 
Web as the input source).  A similar result for 
QA was obtained by Brill et al (2001).  The 
TREC corpus does not have enough candidate 
answers with high precision score and has to 
settle for answers extracted from sentences 
matched by low precision patterns.  The 
WHY-FAMOUS question type is an 
exception and may be due to the fact that the 
system was tested on a small number of 
questions.   
 
5 Shortcoming and Extensions 
No external knowledge has been added to 
these patterns.  We frequently observe the 
need for matching part of speech and/or 
semantic types, however.  For example, the 
question: ?Where are the Rocky Mountains 
located?? is answered by ?Denver?s new 
airport, topped with white fiberglass cones in 
imitation of the Rocky Mountains in the 
background, continues to lie empty?, because 
the system picked the answer ?the 
background? using the pattern ?the <NAME> 
in <ANSWER>,?. Using a named entity 
tagger and/or an ontology would enable the 
system to use the knowledge that 
?background? is not a location. 
DEFINITION questions pose a related 
problem.  Frequently the system?s patterns 
match a term that is too general, though 
correct technically.  For ?what is nepotism?? 
the pattern ?<ANSWER>, <NAME>? 
matches ??in the form of widespread 
bureaucratic abuses: graft, nepotism??; for 
?what is sonar?? the pattern ?<NAME> and 
related <ANSWER>s? matches ??while its 
sonar and related underseas systems are 
built??.  
The patterns cannot handle long-distance 
dependencies.  For example, for ?Where is 
London?? the system cannot locate the answer 
in ?London, which has one of the most busiest 
airports in the world, lies on the banks of the 
river Thames? due to the explosive danger of 
unrestricted wildcard matching, as would be 
required in the pattern ?<QUESTION>, 
(<any_word>)*, lies on <ANSWER>?.  This 
is one of the reasons why the system performs 
 very well on certain types of questions from 
the web but performs poorly with documents 
obtained from the TREC corpus.  The 
abundance and variation of data on the 
Internet alows the system to find an instance 
of its patterns without losing answers to long-
term dependencies.  The TREC corpus, on the 
other hand, typically contains fewer candidate 
answers for a given question and many of the 
answers present may match only long-term 
dependency patterns. 
More information needs to be added to the 
text patterns regarding the length of the 
answer phrase to be expected.  The system 
searches in the range of 50 bytes of the 
answer phrase to capture the pattern. It fails to 
perform under certain conditions as 
exemplified by the question ?When was 
Lyndon B. Johnson born??.  The system 
selects the sentence ?Tower gained national 
attention in 1960 when he lost to democratic 
Sen. Lyndon B. Johnson, who ran for both re-
election and the vice presidency? using the 
pattern ?<NAME> <ANSWER> ? ?.  The 
system lacks the information that the 
<ANSWER> tag should be replaced exactly 
by one word.  Simple extensions could be 
made to the system so that instead of 
searching in the range of 50 bytes for the 
answer phrase it could search for the answer 
in the range of 1? 2 chunks (basic phrases in 
English such as simple NP, VP, PP, etc.). 
A more serious limitat on is that the 
present framework can handle only one 
anchor point (the question term) in the 
candidate answer sentence.  It cannot work for 
types of question that require multiple words 
from the question to be in the answer 
sentence, possibly apart from each other.  For 
example, in ?Which county does the city of 
Long Beach lie??, the answer ?Long Beach is 
situated in Los Angeles County? requires the 
pattern. ?<QUESTION_TERM_1> situated in 
<ANSWER> <QUESTION_TERM_2>?, 
where <QUESTION_TERM_1> and 
<QUESTION_TERM_2> represent the terms 
?Long Beach? and ?county? respectively.  
The performance of the system depends 
significantly on there being only one anchor 
word, which allows a single word match 
between the question and the candidate 
answer sentence.  The presence of multiple 
anchor words would help to eliminate many 
of the candidate answers by simply using the 
condition that all the anchor words from the 
question must be present in the candidate 
answer sentence. 
The system does not classify or make any 
distinction between upper and lower case 
let ers.  For example, ?What is micron?? is 
answered by ?In Boise, Idaho, a spokesman 
for Micron, a maker of semiconductors, said 
Simms are ? a very high volume product for 
us ?? ?.  The answer returned by the system 
would have been perfect if the word ?micron? 
had been capitalized in the question. 
Canonicalization of words is also an issue.  
While giving examples in the bootstrapping 
procedure, say, for BIRTHDATE questions, 
the answer term could be written in many 
ways (for example, Gandhi?s birth date can be 
wr tten as ?1869?, ?Oct. 2, 1869?, ?2nd 
October 1869?,  ?October 2 1869?, and so 
on). Instead of enlisting all the possibilities a 
date tagger could be used to cluster all the 
variations and tag them with the same term.  
The same idea could also be extended for 
smoothing out the variations in the question 
term for names of persons (Gandhi could be 
written as ?Mahatma Gandhi?, ?Mohandas 
Karamchand Gandhi?, etc.). 
 
6 Conclusion 
The web results easily outperform the 
TREC results.  This suggests that there is a 
need to integrate the outputs of the Web and 
the TREC corpus.  Since the output from the 
Web contains many correct answers among 
the top ones, a simple word count could help 
in eliminating many unlikely answers.  This 
would work well for question types like 
BIRTHDATE or LOCATION but is not clear 
for question types like DEFINITION. 
The simplicity of this method makes it 
perfect for multilingual QA.  Many tools 
required by sophisticated QA systems (named 
entity taggers, parsers, ontologies, etc.) are 
language specific and require significant 
effort to adapt to a new language.  Since the 
answer patterns used in this method are 
 learned using only a small number of manual 
training terms, one can rapidly learn patterns 
for new languages, assuming the web search 
engine is appropriately switched. 
Acknowledgements 
This work was supported by the Advanced 
Research and Development Activity 
(ARDA)'s Advanced Question Answering for 
Intelligence (AQUAINT) Program under 
contract number MDA908-02-C-0007. 
 
References  
Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng. 
2001. Data-Intensive Question Answering. 
Proceedings of the TREC-10 Conference. 
NIST, Gaithersburg, MD, 183? 9. 
Gusfield, D. 1997. Algorithms on Strings, Trees 
and Sequences: Computer Science and 
Computational Biology. Chapter 6: Linear 
Time construction of Suffix trees, 94? 121. 
Harabagiu, S., D. Moldovan, M. Pasca, R. 
Mihalcea, M. Surdeanu, R. Buneascu, R. G?rju, 
V. Rus and P. Morarescu. 2001. FALCON: 
Boosting Knowledge for Answer Engines. 
Proceedings of the 9th Text Retrieval 
Conference (TREC-9), NIST, 479? 488.  
Hovy, E.H., U. Hermjakob, and C.-Y. Lin. 2001. 
The Use of External Knowledge in Factoid 
QA.   Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 166?
174.  
Hovy, E.H., U. Hermjakob, and D. Ravichandran. 
2002a. A Question/Answer Typology with 
Surface Text Patterns. Proceedings of the 
Human Language Technology (HLT) 
conference.  San Diego, CA.  
Hovy, E.H., U. Hermjakob, C.-Y. Lin, and D. 
Ravichandran. 2002b. Using Knowledge to 
Facilitate Pinpointing of Factoid Answers. 
Proceedings of the COLING-2002 conference. 
Taipei, Taiwan.  
Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C. 
Lee, B-K. Kwak, J, Cha, D. Kim, J-H. An, H. 
Kim, and K. Kim. 2001. SiteQ: Engineering 
High Performance QA System Using Lexico-
Semantic Pattern Matching and Shallow NLP. 
Proceedings of the TREC-10 Conference. 
NIST, Gaithersburg, MD, 437? 46.  
Lin, C-Y. 2002. The Effectiveness of Dictionary 
and Web-Based Answer Reranking.  
Proceedings of the COLING-2002 conference. 
Taipei, Taiwan.  
Prager, J. and J. Chu-Carroll. 2001. Use of 
WordNet Hypernyms for Answering What-Is 
Questions. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 309?
316. 
Riloff, E. 1996. Automatically Generating 
Extraction Patterns from Untagged Text.  
Proceedings of the Thirteenth National 
Conference on Artificial Intelligence (AAAI-
96), 1044? 1049. 
Soubbotin, M.M. and S.M. Soubbotin. 2001. 
Patterns of Potential Answer Expressions as 
Clues to the Right Answer. Proceedings of the 
TREC-10 Conference. NIST, Gaithersburg, 
MD, 175? 182.  
Srihari, R. and W. Li. 2000. A Question 
Answering System Supported by Information 
Extraction. Proceedings of the 1st Meeting of 
the North American Chapter of the Association 
for Computational Linguistics (ANLP-
NAACL-00), Seattle, WA, 166? 172. 
Voorhees, E. 2001. Overview of the Question 
Answering Track. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 157?
165.  
Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D. 
Bu, and S. Bai. 2001. TREC-10 Experiments at 
CAS-ICT: Filtering, Web, and QA. 
Proceedings of the TREC-10 Conference. 
NIST, Gaithersburg, MD, 229? 41.  
Proceedings of the 43rd Annual Meeting of the ACL, pages 622?629,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Randomized Algorithms and NLP: Using Locality Sensitive Hash Function
for High Speed Noun Clustering
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292.
{ravichan, pantel, hovy}@ISI.EDU
Abstract
In this paper, we explore the power of
randomized algorithm to address the chal-
lenge of working with very large amounts
of data. We apply these algorithms to gen-
erate noun similarity lists from 70 million
pages. We reduce the running time from
quadratic to practically linear in the num-
ber of elements to be computed.
1 Introduction
In the last decade, the field of Natural Language Pro-
cessing (NLP), has seen a surge in the use of cor-
pus motivated techniques. Several NLP systems are
modeled based on empirical data and have had vary-
ing degrees of success. Of late, however, corpus-
based techniques seem to have reached a plateau
in performance. Three possible areas for future re-
search investigation to overcoming this plateau in-
clude:
1. Working with large amounts of data (Banko and
Brill, 2001)
2. Improving semi-supervised and unsupervised al-
gorithms.
3. Using more sophisticated feature functions.
The above listing may not be exhaustive, but it is
probably not a bad bet to work in one of the above
directions. In this paper, we investigate the first two
avenues. Handling terabytes of data requires more
efficient algorithms than are currently used in NLP.
We propose a web scalable solution to clustering
nouns, which employs randomized algorithms. In
doing so, we are going to explore the literature and
techniques of randomized algorithms. All cluster-
ing algorithms make use of some distance similar-
ity (e.g., cosine similarity) to measure pair wise dis-
tance between sets of vectors. Assume that we are
given n points to cluster with a maximum of k fea-
tures. Calculating the full similarity matrix would
take time complexity n2k. With large amounts of
data, say n in the order of millions or even billions,
having an n2k algorithm would be very infeasible.
To be scalable, we ideally want our algorithm to be
proportional to nk.
Fortunately, we can borrow some ideas from the
Math and Theoretical Computer Science community
to tackle this problem. The crux of our solution lies
in defining Locality Sensitive Hash (LSH) functions.
LSH functions involve the creation of short signa-
tures (fingerprints) for each vector in space such that
those vectors that are closer to each other are more
likely to have similar fingerprints. LSH functions
are generally based on randomized algorithms and
are probabilistic. We present LSH algorithms that
can help reduce the time complexity of calculating
our distance similarity atrix to nk.
Rabin (1981) proposed the use of hash func-
tions from random irreducible polynomials to cre-
ate short fingerprint representations for very large
strings. These hash function had the nice property
that the fingerprint of two identical strings had the
same fingerprints, while dissimilar strings had dif-
ferent fingerprints with a very small probability of
collision. Broder (1997) first introduced LSH. He
proposed the use of Min-wise independent functions
to create fingerprints that preserved the Jaccard sim-
622
ilarity between every pair of vectors. These tech-
niques are used today, for example, to eliminate du-
plicate web pages. Charikar (2002) proposed the
use of random hyperplanes to generate an LSH func-
tion that preserves the cosine similarity between ev-
ery pair of vectors. Interestingly, cosine similarity is
widely used in NLP for various applications such as
clustering.
In this paper, we perform high speed similarity
list creation for nouns collected from a huge web
corpus. We linearize this step by using the LSH
proposed by Charikar (2002). This reduction in
complexity of similarity computation makes it pos-
sible to address vastly larger datasets, at the cost,
as shown in Section 5, of only little reduction in
accuracy. In our experiments, we generate a simi-
larity list for each noun extracted from 70 million
page web corpus. Although the NLP community
has begun experimenting with the web, we know
of no work in published literature that has applied
complex language analysis beyond IR and simple
surface-level pattern matching.
2 Theory
The core theory behind the implementation of fast
cosine similarity calculation can be divided into two
parts: 1. Developing LSH functions to create sig-
natures; 2. Using fast search algorithm to find near-
est neighbors. We describe these two components in
greater detail in the next subsections.
2.1 LSH Function Preserving Cosine Similarity
We first begin with the formal definition of cosine
similarity.
Definition: Let u and v be two vectors in a k
dimensional hyperplane. Cosine similarity is de-
fined as the cosine of the angle between them:
cos(?(u, v)). We can calculate cos(?(u, v)) by the
following formula:
cos(?(u, v)) =
|u.v|
|u||v|
(1)
Here ?(u, v) is the angle between the vectors u
and v measured in radians. |u.v| is the scalar (dot)
product of u and v, and |u| and |v| represent the
length of vectors u and v respectively.
The LSH function for cosine similarity as pro-
posed by Charikar (2002) is given by the following
theorem:
Theorem: Suppose we are given a collection of
vectors in a k dimensional vector space (as written as
Rk). Choose a family of hash functions as follows:
Generate a spherically symmetric random vector r
of unit length from this k dimensional space. We
define a hash function, hr, as:
hr(u) =
{
1 : r.u ? 0
0 : r.u < 0
(2)
Then for vectors u and v,
Pr[hr(u) = hr(v)] = 1?
?(u, v)
pi
(3)
Proof of the above theorem is given by Goemans
and Williamson (1995). We rewrite the proof here
for clarity. The above theorem states that the prob-
ability that a random hyperplane separates two vec-
tors is directly proportional to the angle between the
two vectors (i,e., ?(u, v)). By symmetry, we have
Pr[hr(u) 6= hr(v)] = 2Pr[u.r ? 0, v.r < 0]. This
corresponds to the intersection of two half spaces,
the dihedral angle between which is ?. Thus, we
have Pr[u.r ? 0, v.r < 0] = ?(u, v)/2pi. Proceed-
ing we have Pr[hr(u) 6= hr(v)] = ?(u, v)/pi and
Pr[hr(u) = hr(v)] = 1 ? ?(u, v)/pi. This com-
pletes the proof.
Hence from equation 3 we have,
cos(?(u, v)) = cos((1? Pr[hr(u) = hr(v)])pi)
(4)
This equation gives us an alternate method for
finding cosine similarity. Note that the above equa-
tion is probabilistic in nature. Hence, we generate a
large (d) number of random vectors to achieve the
process. Having calculated hr(u) with d random
vectors for each of the vectors u, we apply equation
4 to find the cosine distance between two vectors.
As we generate more number of random vectors, we
can estimate the cosine similarity between two vec-
tors more accurately. However, in practice, the num-
ber (d) of random vectors required is highly domain
dependent, i.e., it depends on the value of the total
number of vectors (n), features (k) and the way the
vectors are distributed. Using d random vectors, we
623
can represent each vector by a bit stream of length
d.
Carefully looking at equation 4, we can ob-
serve that Pr[hr(u) = hr(v)] = 1 ?
(hamming distance)/d1 . Thus, the above theo-
rem, converts the problem of finding cosine distance
between two vectors to the problem of finding ham-
ming distance between their bit streams (as given by
equation 4). Finding hamming distance between two
bit streams is faster and highly memory efficient.
Also worth noting is that this step could be consid-
ered as dimensionality reduction wherein we reduce
a vector in k dimensions to that of d bits while still
preserving the cosine distance between them.
2.2 Fast Search Algorithm
To calculate the fast hamming distance, we use the
search algorithm PLEB (Point Location in Equal
Balls) first proposed by Indyk and Motwani (1998).
This algorithm was further improved by Charikar
(2002). This algorithm involves random permuta-
tions of the bit streams and their sorting to find the
vector with the closest hamming distance. The algo-
rithm given in Charikar (2002) is described to find
the nearest neighbor for a given vector. We mod-
ify it so that we are able to find the top B closest
neighbor for each vector. We omit the math of this
algorithm but we sketch its procedural details in the
next section. Interested readers are further encour-
aged to read Theorem 2 from Charikar (2002) and
Section 3 from Indyk and Motwani (1998).
3 Algorithmic Implementation
In the previous section, we introduced the theory for
calculation of fast cosine similarity. We implement
it as follows:
1. Initially we are given n vectors in a huge k di-
mensional space. Our goal is to find all pairs of
vectors whose cosine similarity is greater than
a particular threshold.
2. Choose d number of (d << k) unit random
vectors {r0, r1, ......, rd} each of k dimensions.
A k dimensional unit random vector, in gen-
eral, is generated by independently sampling a
1Hamming distance is the number of bits which differ be-
tween two binary strings.
Gaussian function with mean 0 and variance 1,
k number of times. Each of the k samples is
used to assign one dimension to the random
vector. We generate a random number from
a Gaussian distribution by using Box-Muller
transformation (Box and Muller, 1958).
3. For every vector u, we determine its signature
by using the function hr(u) (as given by equa-
tion 4). We can represent the signature of vec-
tor u as: u? = {hr1(u), hr2(u), ......., hrd(u)}.
Each vector is thus represented by a set of a bit
streams of length d. Steps 2 and 3 takes O(nk)
time (We can assume d to be a constant since
d << k).
4. The previous step gives n vectors, each of them
represented by d bits. For calculation of fast
hamming distance, we take the original bit in-
dex of all vectors and randomly permute them
(see Appendix A for more details on random
permutation functions). A random permutation
can be considered as random jumbling of the
bits of each vector2. A random permutation
function can be approximated by the following
function:
pi(x) = (ax + b)mod p (5)
where, p is prime and 0 < a < p , 0 ? b < p,
and a and b are chosen at random.
We apply q different random permutation for
every vector (by choosing random values for a
and b, q number of times). Thus for every vec-
tor we have q different bit permutations for the
original bit stream.
5. For each permutation function pi, we lexico-
graphically sort the list of n vectors (whose bit
streams are permuted by the function pi) to ob-
tain a sorted list. This step takes O(nlogn)
time. (We can assume q to be a constant).
6. For each sorted list (performed after applying
the random permutation function pi), we calcu-
late the hamming distance of every vector with
2The jumbling is performed by a mapping of the bit index
as directed by the random permutation function. For a given
permutation, we reorder the bit indexes of all vectors in similar
fashion. This process could be considered as column reording
of bit vectors.
624
B of its closest neighbors in the sorted list. If
the hamming distance is below a certain prede-
termined threshold, we output the pair of vec-
tors with their cosine similarity (as calculated
by equation 4). Thus, B is the beam parameter
of the search. This step takes O(n), since we
can assume B, q, d to be a constant.
Why does the fast hamming distance algorithm
work? The intuition is that the number of bit
streams, d, for each vector is generally smaller than
the number of vectors n (ie. d << n). Thus, sort-
ing the vectors lexicographically after jumbling the
bits will likely bring vectors with lower hamming
distance closer to each other in the sorted lists.
Overall, the algorithm takes O(nk+nlogn) time.
However, for noun clustering, we generally have the
number of nouns, n, smaller than the number of fea-
tures, k. (i.e., n < k). This implies logn << k and
nlogn << nk. Hence the time complexity of our
algorithm is O(nk + nlogn) ? O(nk). This is a
huge saving from the original O(n2k) algorithm. In
the next section, we proceed to apply this technique
for generating noun similarity lists.
4 Building Noun Similarity Lists
A lot of work has been done in the NLP community
on clustering words according to their meaning in
text (Hindle, 1990; Lin, 1998). The basic intuition
is that words that are similar to each other tend to
occur in similar contexts, thus linking the semantics
of words with their lexical usage in text. One may
ask why is clustering of words necessary in the first
place? There may be several reasons for clustering,
but generally it boils down to one basic reason: if the
words that occur rarely in a corpus are found to be
distributionally similar to more frequently occurring
words, then one may be able to make better infer-
ences on rare words.
However, to unleash the real power of clustering
one has to work with large amounts of text. The
NLP community has started working on noun clus-
tering on a few gigabytes of newspaper text. But
with the rapidly growing amount of raw text avail-
able on the web, one could improve clustering per-
formance by carefully harnessing its power. A core
component of most clustering algorithms used in the
NLP community is the creation of a similarity ma-
trix. These algorithms are of complexity O(n2k),
where n is the number of unique nouns and k is the
feature set length. These algorithms are thus not
readily scalable, and limit the size of corpus man-
ageable in practice to a few gigabytes. Clustering al-
gorithms for words generally use the cosine distance
for their similarity calculation (Salton and McGill,
1983). Hence instead of using the usual naive cosine
distance calculation between every pair of words we
can use the algorithm described in Section 3 to make
noun clustering web scalable.
To test our algorithm we conduct similarity based
experiments on 2 different types of corpus: 1. Web
Corpus (70 million web pages, 138GB), 2. Newspa-
per Corpus (6 GB newspaper corpus)
4.1 Web Corpus
We set up a spider to download roughly 70 million
web pages from the Internet. Initially, we use the
links from Open Directory project3 as seed links for
our spider. Each webpage is stripped of HTML tags,
tokenized, and sentence segmented. Each docu-
ment is language identified by the software TextCat4
which implements the paper by Cavnar and Trenkle
(1994). We retain only English documents. The web
contains a lot of duplicate or near-duplicate docu-
ments. Eliminating them is critical for obtaining bet-
ter representation statistics from our collection. The
problem of identifying near duplicate documents in
linear time is not trivial. We eliminate duplicate and
near duplicate documents by using the algorithm de-
scribed by Kolcz et al (2004). This process of dupli-
cate elimination is carried out in linear time and in-
volves the creation of signatures for each document.
Signatures are designed so that duplicate and near
duplicate documents have the same signature. This
algorithm is remarkably fast and has high accuracy.
This entire process of removing non English docu-
ments and duplicate (and near-duplicate) documents
reduces our document set from 70 million web pages
to roughly 31 million web pages. This represents
roughly 138GB of uncompressed text.
We identify all the nouns in the corpus by us-
ing a noun phrase identifier. For each noun phrase,
we identify the context words surrounding it. Our
context window length is restricted to two words to
3http://www.dmoz.org/
4http://odur.let.rug.nl/?vannoord/TextCat/
625
Table 1: Corpus description
Corpus Newspaper Web
Corpus Size 6GB 138GB
Unique Nouns 65,547 655,495
Feature size 940,154 1,306,482
the left and right of each noun. We use the context
words as features of the noun vector.
4.2 Newspaper Corpus
We parse a 6 GB newspaper (TREC9 and
TREC2002 collection) corpus using the dependency
parser Minipar (Lin, 1994). We identify all nouns.
For each noun we take the grammatical context of
the noun as identified by Minipar5. We do not use
grammatical features in the web corpus since pars-
ing is generally not easily web scalable. This kind of
feature set does not seem to affect our results. Cur-
ran and Moens (2002) also report comparable results
for Minipar features and simple word based proxim-
ity features. Table 1 gives the characteristics of both
corpora. Since we use grammatical context, the fea-
ture set is considerably larger than the simple word
based proximity feature set for the newspaper cor-
pus.
4.3 Calculating Feature Vectors
Having collected all nouns and their features, we
now proceed to construct feature vectors (and
values) for nouns from both corpora using mu-
tual information (Church and Hanks, 1989). We
first construct a frequency count vector C(e) =
(ce1, ce2, ..., cek), where k is the total number of
features and cef is the frequency count of feature
f occurring in word e. Here, cef is the number
of times word e occurred in context f . We then
construct a mutual information vector MI(e) =
(mie1,mie2, ...,miek) for each word e, where mief
is the pointwise mutual information between word e
and feature f , which is defined as:
mief = log
cef
N
?n
i=1
cif
N ?
?k
j=1
cej
N
(6)
where n is the number of words and N =
5We perform this operation so that we can compare the per-
formance of our system to that of Pantel and Lin (2002).
?n
i=1
?m
j=1 cij is the total frequency count of all
features of all words.
Having thus obtained the feature representation of
each noun we can apply the algorithm described in
Section 3 to discover similarity lists. We report re-
sults in the next section for both corpora.
5 Evaluation
Evaluating clustering systems is generally consid-
ered to be quite difficult. However, we are mainly
concerned with evaluating the quality and speed of
our high speed randomized algorithm. The web cor-
pus is used to show that our framework is web-
scalable, while the newspaper corpus is used to com-
pare the output of our system with the similarity lists
output by an existing system, which are calculated
using the traditional formula as given in equation
1. For this base comparison system we use the one
built by Pantel and Lin (2002). We perform 3 kinds
of evaluation: 1. Performance of Locality Sensitive
Hash Function; 2. Performance of fast Hamming
distance search algorithm; 3. Quality of final simi-
larity lists.
5.1 Evaluation of Locality sensitive Hash
function
To perform this evaluation, we randomly choose 100
nouns (vectors) from the web collection. For each
noun, we calculate the cosine distance using the
traditional slow method (as given by equation 1),
with all other nouns in the collection. This process
creates similarity lists for each of the 100 vectors.
These similarity lists are cut off at a threshold of
0.15. These lists are considered to be the gold stan-
dard test set for our evaluation.
For the above 100 chosen vectors, we also calcu-
late the cosine similarity using the randomized ap-
proach as given by equation 4 and calculate the mean
squared error with the gold standard test set using
the following formula:
errorav =
?
?
i
(CSreal,i ? CScalc,i)
2/total
(7)
where CSreal,i and CScalc,i are the cosine simi-
larity scores calculated using the traditional (equa-
tion 1) and randomized (equation 4) technique re-
626
Table 2: Error in cosine similarity
Number of ran-
dom vectors d
Average error in
cosine similarity
Time (in hours)
1 1.0000 0.4
10 0.4432 0.5
100 0.1516 3
1000 0.0493 24
3000 0.0273 72
10000 0.0156 241
spectively. i is the index over all pairs of elements
that have CSreal,i >= 0.15
We calculate the error (errorav) for various val-
ues of d, the total number of unit random vectors r
used in the process. The results are reported in Table
26. As we generate more random vectors, the error
rate decreases. For example, generating 10 random
vectors gives us a cosine error of 0.4432 (which is a
large number since cosine similarity ranges from 0
to 1.) However, generation of more random vectors
leads to reduction in error rate as seen by the val-
ues for 1000 (0.0493) and 10000 (0.0156). But as
we generate more random vectors the time taken by
the algorithm also increases. We choose d = 3000
random vectors as our optimal (time-accuracy) cut
off. It is also very interesting to note that by using
only 3000 bits for each of the 655,495 nouns, we
are able to measure cosine similarity between every
pair of them to within an average error margin of
0.027. This algorithm is also highly memory effi-
cient since we can represent every vector by only a
few thousand bits. Also the randomization process
makes the the algorithm easily parallelizable since
each processor can independently contribute a few
bits for every vector.
5.2 Evaluation of Fast Hamming Distance
Search Algorithm
We initially obtain a list of bit streams for all the
vectors (nouns) from our web corpus using the ran-
domized algorithm described in Section 3 (Steps 1
to 3). The next step involves the calculation of ham-
ming distance. To evaluate the quality of this search
algorithm we again randomly choose 100 vectors
(nouns) from our collection. For each of these 100
vectors we manually calculate the hamming distance
6The time is calculated for running the algorithm on a single
Pentium IV processor with 4GB of memory
with all other vectors in the collection. We only re-
tain those pairs of vectors whose cosine distance (as
manually calculated) is above 0.15. This similarity
list is used as the gold standard test set for evaluating
our fast hamming search.
We then apply the fast hamming distance search
algorithm as described in Section 3. In particular, it
involves steps 3 to 6 of the algorithm. We evaluate
the hamming distance with respect to two criteria: 1.
Number of bit index random permutations functions
q; 2. Beam search parameter B.
For each vector in the test collection, we take the
top N elements from the gold standard similarity list
and calculate how many of these elements are actu-
ally discovered by the fast hamming distance algo-
rithm. We report the results in Table 3 and Table 4
with beam parameters of (B = 25) and (B = 100)
respectively. For each beam, we experiment with
various values for q, the number of random permu-
tation function used. In general, by increasing the
value for beam B and number of random permu-
tation q , the accuracy of the search algorithm in-
creases. For example in Table 4 by using a beam
B = 100 and using 1000 random bit permutations,
we are able to discover 72.8% of the elements of the
Top 100 list. However, increasing the values of q and
B also increases search time. With a beam (B) of
100 and the number of random permutations equal
to 100 (i.e., q = 1000) it takes 570 hours of process-
ing time on a single Pentium IV machine, whereas
with B = 25 and q = 1000, reduces processing time
by more than 50% to 240 hours.
We could not calculate the total time taken to
build noun similarity list using the traditional tech-
nique on the entire corpus. However, we estimate
that its time taken would be at least 50,000 hours
(and perhaps even more) with a few of Terabytes of
disk space needed. This is a very rough estimate.
The experiment was infeasible. This estimate as-
sumes the widely used reverse indexing technique,
where in one compares only those vector pairs that
have at least one feature in common.
5.3 Quality of Final Similarity Lists
For evaluating the quality of our final similarity lists,
we use the system developed by Pantel and Lin
(2002) as gold standard on a much smaller data set.
We use the same 6GB corpus that was used for train-
627
Table 3: Hamming search accuracy (Beam B = 25)
Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
25 6.1% 4.9% 4.2% 3.1% 2.4% 1.9%
50 6.1% 5.1% 4.3% 3.2% 2.5% 1.9%
100 11.3% 9.7% 8.2% 6.2% 5.7% 5.1%
500 44.3% 33.5% 30.4% 25.8% 23.0% 20.4%
1000 58.7% 50.6% 48.8% 45.0% 41.0% 37.2%
Table 4: Hamming search accuracy (Beam B = 100)
Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
25 9.2% 9.5% 7.9% 6.4% 5.8% 4.7%
50 15.4% 17.7% 14.6% 12.0% 10.9% 9.0%
100 27.8% 27.2% 23.5% 19.4% 17.9% 16.3%
500 73.1% 67.0% 60.7% 55.2% 53.0% 50.5%
1000 87.6% 84.4% 82.1% 78.9% 75.8% 72.8%
ing by Pantel and Lin (2002) so that the results are
comparable. We randomly choose 100 nouns and
calculate the top N elements closest to each noun in
the similarity lists using the randomized algorithm
described in Section 3. We then compare this output
to the one provided by the system of Pantel and Lin
(2002). For every noun in the top N list generated
by our system we calculate the percentage overlap
with the gold standard list. Results are reported in
Table 5. The results shows that we are able to re-
trieve roughly 70% of the gold standard similarity
list. In Table 6, we list the top 10 most similar words
for some nouns, as examples, from the web corpus.
6 Conclusion
NLP researchers have just begun leveraging the vast
amount of knowledge available on the web. By
searching IR engines for simple surface patterns,
many applications ranging from word sense disam-
biguation, question answering, and mining seman-
tic resources have already benefited. However, most
language analysis tools are too infeasible to run on
the scale of the web. A case in point is generat-
ing noun similarity lists using co-occurrence statis-
tics, which has quadratic running time on the input
size. In this paper, we solve this problem by pre-
senting a randomized algorithm that linearizes this
task and limits memory requirements. Experiments
show that our method generates cosine similarities
between pairs of nouns within a score of 0.03.
In many applications, researchers have shown that
more data equals better performance (Banko and
Brill, 2001; Curran and Moens, 2002). Moreover,
at the web-scale, we are no longer limited to a snap-
shot in time, which allows broader knowledge to be
learned and processed. Randomized algorithms pro-
vide the necessary speed and memory requirements
to tap into terascale text sources. We hope that ran-
domized algorithms will make other NLP tools fea-
sible at the terascale and we believe that many al-
gorithms will benefit from the vast coverage of our
newly created noun similarity list.
Acknowledgement
We wish to thank USC Center for High Performance
Computing and Communications (HPCC) for help-
ing us use their cluster computers.
References
Banko, M. and Brill, E. 2001. Mitigating the paucity of dat-
aproblem. In Proceedings of HLT. 2001. San Diego, CA.
Box, G. E. P. and M. E. Muller 1958. Ann. Math. Stat. 29,
610?611.
Broder, Andrei 1997. On the Resemblance and Containment of
Documents. Proceedings of the Compression and Complex-
ity of Sequences.
Cavnar, W. B. and J. M. Trenkle 1994. N-Gram-Based Text
Categorization. In Proceedings of Third Annual Symposium
on Document Analysis and Information Retrieval, Las Ve-
gas, NV, UNLV Publications/Reprographics, 161?175.
628
Table 5: Final Quality of Similarity Lists
Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
Accuracy 70.7% 71.9% 72.2% 71.7% 71.2% 71.1%
Table 6: Sample Top 10 Similarity Lists
JUST DO IT computer science TSUNAMI Louis Vuitton PILATES
HAVE A NICE DAY mechanical engineering tidal wave PRADA Tai Chi
FAIR AND BALANCED electrical engineering LANDSLIDE Fendi Cardio
POWER TO THE PEOPLE chemical engineering EARTHQUAKE Kate Spade SHIATSU
NEVER AGAIN Civil Engineering volcanic eruption VUITTON Calisthenics
NO BLOOD FOR OIL ECONOMICS HAILSTORM BURBERRY Ayurveda
KINGDOM OF HEAVEN ENGINEERING Typhoon GUCCI Acupressure
If Texas Wasn?t Biology Mudslide Chanel Qigong
BODY OF CHRIST environmental science windstorm Dior FELDENKRAIS
WE CAN PHYSICS HURRICANE Ferragamo THERAPEUTIC TOUCH
Weld with your mouse information science DISASTER Ralph Lauren Reflexology
Charikar, Moses 2002. Similarity Estimation Techniques from
Rounding Algorithms In Proceedings of the 34th Annual
ACM Symposium on Theory of Computing.
Church, K. and Hanks, P. 1989. Word association norms, mu-
tual information, and lexicography. In Proceedings of ACL-
89. pp. 76?83. Vancouver, Canada.
Curran, J. and Moens, M. 2002. Scaling context space. In
Proceedings of ACL-02 pp 231?238, Philadelphia, PA.
Goemans, M. X. and D. P. Williamson 1995. Improved Ap-
proximation Algorithms for Maximum Cut and Satisfiability
Problems Using Semidefinite Programming. JACM 42(6):
1115?1145.
Hindle, D. 1990. Noun classification from predicate-argument
structures. In Proceedings of ACL-90. pp. 268?275. Pitts-
burgh, PA.
Lin, D. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of COLING/ACL-98. pp. 768?774.
Montreal, Canada.
Indyk, P., Motwani, R. 1998. Approximate nearest neighbors:
towards removing the curse of dimensionality Proceedings
of 30th STOC, 604?613.
A. Kolcz, A. Chowdhury, J. Alspector 2004. Improved ro-
bustness of signature-based near-replica detection via lexi-
con randomization. Proceedings of ACM-SIGKDD (2004).
Lin, D. 1994 Principar - an efficient, broad-coverage,
principle-based parser. Proceedings of COLING-94, pp. 42?
48. Kyoto, Japan.
Pantel, Patrick and Dekang Lin 2002. Discovering Word
Senses from Text. In Proceedings of SIGKDD-02, pp. 613?
619. Edmonton, Canada
Rabin, M. O. 1981. Fingerprinting by random polynomials.
Center for research in Computing technology , Harvard Uni-
versity, Report TR-15-81.
Salton, G. and McGill, M. J. 1983. Introduction to Modern
Information Retrieval. McGraw Hill.
Appendix A. Random Permutation
Functions
We define [n] = {0, 1, 2, ..., n? 1}.
[n] can thus be considered as a set of integers from
0 to n? 1.
Let pi : [n] ? [n] be a permutation function chosen
at random from the set of all such permutation func-
tions.
Consider pi : [4] ? [4].
A permutation function pi is a one to one mapping
from the set of [4] to the set of [4].
Thus, one possible mapping is:
pi : {0, 1, 2, 3} ? {3, 2, 1, 0}
Here it means: pi(0) = 3, pi(1) = 2, pi(2) = 1,
pi(3) = 0
Another possible mapping would be:
pi : {0, 1, 2, 3} ? {3, 0, 1, 2}
Here it means: pi(0) = 3, pi(1) = 0, pi(2) = 1,
pi(3) = 2
Thus for the set [4] there would be 4! = 4?3?2 =
24 possibilities. In general, for a set [n] there would
be n! unique permutation functions. Choosing a ran-
dom permutation function amounts to choosing one
of n! such functions at random.
629
Statistical QA - Classifier vs. Re-ranker: What?s the difference? 
Deepak Ravichandran, Eduard Hovy, and Franz Josef Och 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{ravichan,hovy,och}@isi.edu 
Abstract 
In this paper, we show that we can ob-
tain a good baseline performance for 
Question Answering (QA) by using 
only 4 simple features. Using these fea-
tures, we contrast two approaches used 
for a Maximum Entropy based QA sys-
tem. We view the QA problem as a 
classification problem and as a re-
ranking problem. Our results indicate 
that the QA system viewed as a re-
ranker clearly outperforms the QA sys-
tem used as a classifier. Both systems 
are trained using the same data.  
1 Introduction 
Open-Domain factoid Question Answering (QA) 
is defined as the task of answering fact-based 
questions phrased in Natural Language. Exam-
ples of some question and answers that fall in 
the fact-based category are: 
 
1. What is the capital of Japan? - Tokyo 
2. What is acetaminophen? - Non-aspirin pain killer 
3. Where is the Eiffel Tower? - Paris 
 
The architecture of most of QA systems consists 
of two basic modules: the information retrieval 
(IR) module and the answer pinpointing module. 
These two modules are used in a typical pipeline 
architecture. 
For a given question, the IR module finds a 
set of relevant segments. Each segment typically 
consists of at most R sentences1 . The answer 
pinpointing module processes each of these seg-
ments and finds the appropriate answer phrase. 
                                                          
1
 In our experiments we use R=1  
phrase. Evaluation of a QA system is judged on 
the basis on the final output answer and the cor-
responding evidence provided by the segment. 
This paper focuses on the answer pinpointing 
module. Typical QA systems perform re-ranking 
of candidate answers as an important step in 
pinpointing. The goal is to rank the most likely 
answer first by using either symbolic or statisti-
cal methods. Some QA systems make use of 
statistical answer pinpointing (Xu et. al, 2002; 
Ittycheriah, 2001; Ittycheriah and Salim, 2002) 
by treating it as a classification problem. In this 
paper, we cast the pinpointing problem in a sta-
tistical framework and compare two approaches, 
classification and re-ranking. 
2 Statistical Answer Pinpointing 
2.1 Answer Modeling 
The answer-pinpointing module gets as input a 
question q and a set of possible answer candi-
dates }...{ 21 Aaaa . It outputs one of the answer 
}...{ 21 Aaaaa ?  from the candidate answer set. 
We consider two ways of modeling this prob-
lem.  
One approach is the traditional classification 
view (Ittycheriah, 2001) where we present each 
Question-Answer pair to the classifier which 
classifies it as either correct answer (true) or in-
correct answer (false), based on some evidence 
(features).  
In this case, we model )},...{,|( 21 qaaaacP A . 
Here, 
 false}{true,c = signifies the correctness 
of the answer a  with respect to the question q . 
The probability )},...{,|( 21 qaaaacP A  for each QA 
pair is modeled independently of other such 
pairs. Thus, for the same question, many QA 
pairs are presented to the classifier as independ-
ent events (histories). If the training corpus con-
tains Q questions with A answers for each ques-
tion, the total number of events (histories) would 
be equal to Q?A with two classes (futures) (cor-
rect or incorrect answer) for each event. Once 
the probabilities )},...{,|( 21 qaaaacP A  have been 
computed, the system has to return the best an-
swer. The following decision rule is used: 
)]},...{,|([maxarg 21 qaaaatruePa A
a
=
?
 
Another way of viewing the problem is as a 
re-ranking task. This is possible because the QA 
task requires the identification of only one cor-
rect answer, instead of identifying all the correct 
answer in the collection. In this case, we model 
)},...{|( 21 qaaaaP A . If the training corpus contains 
Q questions with A answers for each question, 
the total number of events (histories) would be 
equal to Q, with A classes (futures). This view 
requires the following decision-rule to identify 
the answer that seems most promising: 
)]},...{|([maxarg 21 qaaaaPa A
a
=
?
 
In summary, 
 Classifier Re-ranker 
#Events (Histo-
ries) 
Q?A Q 
#Classes (Futures) 
per event 
2 A 
 
where, 
Q = total number of questions. 
A = total number of answer chunks considered 
for each question. 
2.2 Maximum Entropy formulation 
We use Maximum Entropy to model the given 
problem both as a classifier and a re-ranker. We 
define M feature functions, 
Mmqaaaaf Am ,....,1),},...{,( 21 = , that may be useful 
in characterizing the task. Della Pietra et. al 
(1995) contains good description of Maximum 
Entropy models. 
We model the classifier as follows: 
? ?
?
=
=
=
?
1
21?,
1
21,
21
)]},...{,(exp[
)]},...{,(exp[
)},...{,|(
c
M
m
Amcm
M
m
Amcm
A
qaaaaf
qaaaaf
qaaaacP
?
?
 
where, 
},{;,....,1;
,
falsetruecMmcm ==? are the model 
parameters. 
The decision rule for choosing the best an-
swer is: 
])},...{,([maxarg
)]},...{,|([maxarg
1
21,
21
?
=
?
=
=
M
m
Amtruem
a
A
a
qaaaaf
qaaaatruePa
?
 
The above decision rule requires comparison of 
different probabilities of the 
form )},...{,|( 21 qaaaatrueP A . However, these 
probabilities are modeled as independent events 
(histories) in the classifier and hence the training 
criterion does not make them directly compara-
ble. 
For the re-ranker, we model the probability 
as: 
( )
? ?
?
=
=
=
?
1
21
1
21
21
)]},...{,?(exp[
)]},...{,(exp[
},...{|
a
M
m
Amm
M
m
Amm
A
qaaaaf
qaaaaf
qaaaaP
?
?
 
where, 
Mmm ,....,1; =?  are the model parameters. 
Note that for the classifier the model parameters 
are cm,?  , whereas for the re-ranker they are m? . 
This is because for the classifier, each feature 
function has different weights associated with 
each class (future). Hence, the classifier has 
twice the model parameters as compared to the 
re-ranker. 
The decision rule for the re-ranker is given by: 
.
?
=
?
=
=
M
m
Amm
a
A
a
qaaaaf
qaaaaPa
1
21
21
)]},...{,([maxarg
)]},...{|([maxarg
?
  
The re-ranker makes the probabilities 
)},...{|( 21 qaaaaP A , considered for the decision 
rule, directly comparable against each other, by 
incorporating them into the training criterion 
itself. Table 1 summarizes the differences of the 
two models. 
  
2.3 Feature Functions 
Using above formulation to model the probabil-
ity distribution we need to come up with features 
fj. We use only four basic feature functions for 
our system. 
1. Frequency: It has been observed that the 
correct answer has a higher frequency 
(Magnini et al; 2002) in the collection of 
answer chunks (C). Hence we count the 
number of time a potential answer occurs in 
the IR output and use its logarithm as a fea-
ture. This is a positive continuous valued 
feature. 
2. Expected Answer Class: Most of the current 
QA systems employ some type of Answer 
Class Identification module. Thus questions 
like ?When did Bill Clinton go to college??, 
would be identified as a question asking 
about a time (or a time period), ?Where is 
the sea of tranquility?? would be identified 
as a question asking for a location. If the an-
swer class matches the expected answer 
class (derived from the question by the an-
swer identification module) this feature fires 
(i.e., it has a value of 1). Details of this mod-
ule are explained in Hovy et al (2002). This 
is a binary-valued feature. 
3. Question Word Absent: Usually a correct 
answer sentence contains a few of the ques-
tion words. This feature fires if the candidate 
answer does not contain any of the question 
words. This is also a binary valued feature. 
4. Word Match: It is the sum of ITF2 values for 
the words in the question that matches iden-
tically with the words in the answer sen-
tence. This is a positive continuous valued 
feature. 
2.4 Training 
We train our Maximum Entropy model using 
Generalized Iterative scaling (Darroch and 
Ratcliff, 1972) approach by using YASMET3 . 
3 Evaluation Metric 
The performance of the QA system is highly 
dependent on the performance of the two indi-
vidual modules IR and answer-pinpointing. The 
system would have excellent performance if 
both have good accuracy. Hence, we need a 
good evaluation metric to evaluate each of these 
components individually. One standard metric 
for IR is recall and precision. We can modify 
this metric for QA as follows: 
                                                          
2
 ITF = Inverse Term Frequency. We take a large inde-
pendent corpus & estimate ITF(W) =1/(count(W)), where 
W = Word. 
3
 YASMET. (Yet Another Small Maximum Entropy 
Toolkit) http://www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/software/YASMET.html 
 
 Classifier Re-Ranker 
Mode
ling 
Equa-
tion ? ?
?
=
=
=
?
1
21?,
1
21,
21
)]},...{,(exp[
)]},...{,(exp[
)},...{,|(
c
M
m
Amcm
M
m
Amcm
A
qaaaaf
qaaaaf
qaaaacP
?
?
 
? ?
?
=
=
=
?
1
21
1
21
21
)]},...{,?(exp[
)]},...{,(exp[
)},...{|(
a
M
m
Amm
M
m
Amm
A
qaaaaf
qaaaaf
qaaaaP
?
?
 
 
Deci-
sion 
Rule  
])},...{,([maxarg
)}},...{,|({maxarg
1
21,
21
?
=
?
=
=
M
m
Atruemm
a
A
a
qaaaaf
qaaaatruePa
?
 
 
.
?
=
?
=
=
M
m
Amm
a
A
a
qaaaaf
qaaaaPa
1
21
21
)]},...{,([maxarg
)]},...{|([maxarg
?
 
 
Table 1 : Model comparison between a Classifier and Re-ranker 
 segmentsanswer relevant  Total #
returnedsegment  answer relevant  #
  Recall =  
returned segments Total #
returned segmentsanswer relevant  #
 Precision =  
It is almost impossible to measure recall be-
cause the IR collection is typically large and in-
volves several hundreds of thousands of 
documents. Hence, we evaluate our IR by only 
the precision measure at top N segments. This 
method is actually a rather sloppy approximation 
to the original recall and precision measure. 
Questions with fewer correct answers in the col-
lection would have a lower precision score as 
compared to questions with many answers. 
Similarly, it is unclear how one would evaluate 
answer questions with No Answer (NIL) in the 
collection using this metric. All these questions 
would have zero precision from the IR collec-
tion. 
The answer-pinpointing module is evaluated 
by checking if the answer returned by the system 
as the top ranked (#1) answer is correct/incorrect 
with respect to the IR collection and the true 
answer. Hence, if the IR system fails to return 
even a single sentence that contains the correct 
answer for the given question, we do not penal-
ize the answer-pinpointing module. It is again 
unclear how to evaluate questions with No an-
swer (NIL). (Here, for our experiments we at-
tribute the error to the IR module.) 
Finally, the combined system is evaluated by 
using the standard technique, wherein the An-
swer (ranked #1) returned by the system is 
judged to be either correct or incorrect and then 
the average is taken. 
Question: 
1395 Who is Tom Cruise married to ? 
 
IR Output: 
1 Tom Cruise is married to actress Nicole Kidman and they have two adopted children . 
2 Tom Cruise is married to Nicole Kidman . 
. 
. 
 
Output of Chunker: (The number to the left of each chunk records the IR sentence from 
which that particular chunk came) 
1  Tom Cruise 
1  Tom 
1  Cruise 
1  is married 
1  married 
1  actress Nicole Kidman and they 
1  actress Nicole Kidman 
1  actress 
1  Nicole Kidman 
1  Nicole 
1  Kidman 
1  they 
1  two adopted children 
1  two 
1  adopted 
1  children 
2  Tom Cruise 
2  Tom 
2  Cruise 
2  is married 
2  married 
2  Nicole Kidman 
2  Nicole 
2  Kidman 
. 
. 
 
Figure 1 : Candidate answer extraction for a question. 
4 Experiments 
4.1 Framework 
Information Retrieval 
For our experiments, we use the Web search 
engine AltaVista. For every question, we re-
move stop-words and present all other question 
words as query to the Web search engine. The 
top relevant documents are downloaded. We 
apply a sentence segmentor, and identify those 
sentences that have high ITF overlapping words 
with the given question. The sentences are then 
re-ranked accordingly and only the top K sen-
tences (segments) are presented as output of the 
IR system. 
Candidate Answer Extraction 
For a given question, the IR returns top K 
segments. For our experiments a segment con-
sists of one sentence. We parse each of the sen-
tences and obtain a set of chunks, where each 
chunk is a node of the parse tree. Each chunk is 
viewed as a potential answer. For our experi-
ments we restrict the number of potential an-
swers to be at most 5000. We illustrate this 
process in Figure 1. 
Training/Test Data 
Table 2 : Training size and sources. 
 
 Training + 
Validation 
Test 
Question collec-
tion 
TREC 9 + 
TREC 10 
TREC11 
Total questions 1192 500 
 
We use the TREC 9 and TREC 10 data sets 
for training and the TREC 11 data set for testing. 
We initially apply the IR step as described above 
and obtain a set of at most 5000 answers. For 
each such answer we use the pattern file sup-
plied by NIST to tag answer chunks as either 
correct (1) or incorrect (0). This is a very noisy 
way of tagging data. In some cases, even though 
the answer chunk may be tagged as correct it 
may not be supported by the accompanying sen-
tence, while in other cases, a correct chunk may 
be graded as incorrect, since the pattern file list 
did not represent a exhaustive list of answers. 
We set aside 20% of the training data for valida-
tion. 
4.2 Classifier vs. Re-Ranker 
We evaluate the performance of the QA system 
viewed as a classifier (with a post-processing 
step) and as a re-ranker. In order to do a fair 
evaluation of the system we test the performance 
of the QA system under varying conditions of 
the output of the IR system. The results are 
shown in Table 3. 
The results should be read in the following 
way: We use the same IR system. However, dur-
ing each run of the experiment we consider only 
the top K sentences returned by the IR system 
K={1,10,50,100,150,200}. The column ? cor-
rect?  represents the number of questions the en-
tire QA (IR + re-ranker) system answered 
correctly. ? IR Loss?  represents the average 
number of questions for which the IR failed 
completely (i.e., the IR did not return even a sin-
gle sentence that contains the correct answer). 
The IR precision is the precision of the IR sys-
tem for the number of sentences considered. An-
swer-pinpointing performance is based on the 
metric described above. Finally, the overall 
score is the score of the entire QA system. (i.e., 
precision at rank#1). 
The ? Overall Precision" column indicates 
that the re-ranker clearly outperforms the classi-
fier. However, it is also very interesting to com-
pare the performance of the re-ranker ? Overall 
Precision?  with the ? Answer-Pinpointing preci-
sion? . For example, in the last row, for the re-
ranker the ? Answer-Pinpointing Precision?  is 
0.5182 whereas the ? Overall Precision?  is only 
0.34. The difference is due to the performance of 
the poor performance of the IR system (? IR 
Loss?  = 0.344). 
4.3 Oracle IR system 
In order to determine the performance of the 
answer pinpointing module alone, we perform 
the so-called oracle IR experiment. Here, we 
present to the answer pinpointing module only 
those sentences from IR that contain an answer4. 
The task of the answer pinpointing module is to 
pick out of the correct answer from the given 
collection. We report results in Table 4. In these 
results too the re-ranker has better performance 
as compared to the classifier. However, as we 
see from the results, there is a lot of room for 
improvement for the re-ranker system, even with 
a perfect IR system. 
5 Discussion 
Our experiments clearly indicate that the QA 
system viewed as a re-ranker outperforms the 
QA system viewed as a classifier. The difference 
stem from the following reasons: 
1. The classification training criteria work on a 
more difficult objective function of trying to 
find whether each candidate answer answers 
the given question, as opposed to trying to 
find the best answer for the given question. 
Hence, the same feature set that works for 
the re-ranker need not work for the classi-
fier. The feature set used in this problem is 
not good enough to help the classifier dis-
tinguish between correct and incorrect an-
                                                          
4
 This was performed by extracting all the sentences that 
were judged to have the correct answer by human evalua-
tors during the TREC 2002 evaluations. 
swers for the given question (even though it 
is good for the re-ranker to come up with the 
best answer). 
2. The comparison of probabilities across dif-
ferent events (histories) for the classifier, 
during the decision rule process, is problem-
atic. This is because the probabilities, which 
we obtain after the classification approach, 
are only a poor estimate of the true probabil-
ity. The re-ranker, however, directly allows 
these probabilities to be comparable by in-
corporating them into the model itself. 
3. The QA system viewed as a classifier suf-
fers from the problem of a highly unbal-
anced data set. We have less than 1% 
positive examples and more than 99% nega-
tive examples (we had almost 4 million 
training data events) in the problem. Ittyche-
riah (2001), and Ittycheriah and Roukos 
(2002), use a more controlled environment 
for training their system. They have 23% 
positive examples and 77% negative exam-
ples. They prune out most of the incorrect 
answer initially, using a pre-processing step 
by using either a rule-based system (Ittyche-
riah, 2001) or a statistical system (Ittyche-
riah et al, 2002); and hence obtain a much 
more manageable distribution in the training 
phase of the Maximum Entropy model. 
Answer-Pinpointing 
Precision Number Correct Overall Precision IR Sen-
tences 
Total 
ques-
tions IR Precision IR Loss Classifier Re-ranker Classifier Re-ranker Classifier Re-ranker 
1 500 0.266 0.742 0.0027 0.3565 29 46 0.058 0.092 
10 500 0.2018 0.48 0.0016 0.4269 7 111 0.014 0.222 
50 500 0.1155 0.386 0.0015 0.4885 6 150 0.012 0.3 
100 500 0.0878 0.362 0.0015 0.5015 5 160 0.01 0.32 
150 500 0.0763 0.35 0.0015 0.5138 5 167 0.01 0.334 
200 500 0.0703 0.344 0.0015 0.5182 3 170 0.01 0.34 
Table 3 : Results for Classifier and Re-ranker under varying conditions of IR. 
IR Sentences = Total IR sentences considered for every question 
IR Precision = Precision @ (IR Sentences) 
IR Loss = (Number of Questions for which the IR did not produce a single answer)/(Total Questions) 
Overall Precision = (Number Correct)/(Total Questions) 
  
 
 
 
 
6 Conclusion 
The re-ranker system is very robust in handling 
large amounts of data and still produces reason-
able results. There is no need for a major pre-
processing step (for eliminating undesirable in-
correct answers from the training) or the post-
processing step (for selecting the most promis-
ing answer.) 
We also consider it significant that a QA sys-
tem with just 4 features (viz. Frequency, 
Expected Answer Type, Question word absent, 
and ITF word match) is a good baseline system 
and performs better than the median perform-
ance of all the QA systems in the TREC 2002 
evaluations5.  
Ittycheriah (2001), and Ittycheriah and Rou-
kos (2002) have shown good results by using a 
range of features for Maximum Entropy QA sys-
tems. Also, the results indicate that there is 
scope for research in IR for QA systems. The 
QA system has an upper ceiling on performance 
due to the quality of the IR system. The QA 
community has yet to address these problems in 
a principled way, and the IR details of most of 
the system are hidden behind the complicated 
system architecture. 
The re-ranking model basically changes the 
objective function for training and the system is 
directly optimized on the evaluation function 
criteria (though still using Maximum Likelihood 
training). Also this approach seems to be very 
robust to noisy training data and is highly scal-
able. 
Acknowledgements. 
This work was supported by the Advance Re-
search and Development Activity (ARDA)?s 
Advanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 
                                                          
5
 However, since the IR system used here was from the 
Web, our results are not directly comparable with the 
TREC systems. 
MDA908-02-C-007. The authors wish to ex-
press particular gratitude to Dr. Abraham It-
tycheriah, both for his supervision and education 
of the first author during his summer visit to 
IBM TJ Watson Research Center in 2002 and 
for his thoughtful comments on this paper, 
which was inspired by his work. 
References 
Darroch, J. N., and D. Ratcliff. 1972. Generalized 
iterative scaling for log-linear models. Annals of 
Mathematical Statistics, 43:1470?1480. 
Hermjakob, U. 1997. Learning Parse and Translation 
Decisions from Examples with Rich Context. 
Ph.D. Dissertation, University of Texas at Austin, 
Austin, TX.  
Hovy, E.H., U. Hermjakob, D. Ravichandran. 2002. 
A Question/Answer Typology with Surface Text 
Patterns. Proceedings of the DARPA Human Lan-
guage Technology Conferenc,. San Diego, CA, 
247?250. 
Ittycheriah, A. 2001. Trainable Question Answering 
System. Ph.D. Dissertation, Rutgers, The State 
University of New Jersey, New Brunswick, NJ. 
Ittycheriah., A., and S. Roukos. 2002. IBM?S Ques-
tion Answering System-TREC-11. Proceedings of 
TREC 2002, NIST, MD, 394?401. 
Magnini, B, M. Negri, R. Prevete, and H. Tanev. 
2002. Is it the Right Answer? Exploiting Web Re-
dundancy for Answer Validation. Proceedings of 
the 40th Meeting of the Association of Computa-
tional Linguistics, Philadelphia, PA, 425?432. 
Della Pietra, S., V. Della Pietra, and J. Lafferty. 
1995. Inducing Features of Random Fields, Tech-
nical Report Department of Computer Science, 
Carnegie-Mellon University, CMU?CS-95?144. 
Xu, J., A. J. Licuanan, S. May, R. Miller, and R. 
Weischedel. 2002. TREC2002QA at BBN: An-
swer Selection and Confidence Estimation. Pro-
ceedings of TREC 2002. NIST MD. 290?295 
Answer-Pinpointing 
Precision Total ques-
tions IR precision Classifier Re-ranker 
429 1.0 0.156 0.578 
Table 4 : Performance with a perfect IR system 
 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582?590,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Labeled Class Instances using Graph
Random Walks
Partha Pratim Talukdar?
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Joseph Reisinger?
University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Deepak Ravichandran
Google Inc.
Mountain View, CA 94043
deepakr@google.com
Rahul Bhagat?
USC Information Sciences Institute
Marina Del Rey, CA 90292
rahul@isi.edu
Fernando Pereira
Google Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
We present a graph-based semi-supervised la-
bel propagation algorithm for acquiring open-
domain labeled classes and their instances
from a combination of unstructured and struc-
tured text sources. This acquisition method
significantly improves coverage compared to
a previous set of labeled classes and instances
derived from free text, while achieving com-
parable precision.
1 Introduction
1.1 Motivation
Users of large document collections can readily ac-
quire information about the instances, classes, and
relationships described in the documents. Such rela-
tions play an important role in both natural language
understanding andWeb search, as illustrated by their
prominence in both Web documents and among the
search queries submitted most frequently by Web
users (Jansen et al, 2000). These observations moti-
vate our work on algorithms to extract instance-class
information from Web documents.
While work on named-entity recognition tradi-
tionally focuses on the acquisition and identifica-
tion of instances within a small set of coarse-grained
classes, the distribution of instances within query
logs indicates that Web search users are interested
in a wider range of more fine-grained classes. De-
pending on prior knowledge, personal interests and
immediate needs, users submit for example medi-
cal queries about the symptoms of leptospirosis or
?Contributions made during internships at Google.
the treatment of monkeypox, both of which are in-
stances of zoonotic diseases, or the risks and benefits
of surgical procedures such as PRK and angioplasty.
Other users may be more interested in African coun-
tries such as Uganda and Angola, or active volca-
noes like Etna and Kilauea. Note that zoonotic dis-
eases, surgical procedures, African countries and
active volcanoes serve as useful class labels that cap-
ture the semantics of the associated sets of class in-
stances. Such interest in a wide variety of specific
domains highlights the utility of constructing large
collections of fine-grained classes.
Comprehensive and accurate class-instance in-
formation is useful not only in search but also
in a variety of other text processing tasks includ-
ing co-reference resolution (McCarthy and Lehn-
ert, 1995), named entity recognition (Stevenson and
Gaizauskas, 2000) and seed-based information ex-
traction (Riloff and Jones, 1999).
1.2 Contributions
We study the acquisition of open-domain, labeled
classes and their instances from both structured
and unstructured textual data sources by combin-
ing and ranking individual extractions in a princi-
pled way with the Adsorption label-propagation al-
gorithm (Baluja et al, 2008), reviewed in Section 3
below.
A collection of labeled classes acquired from
text (Van Durme and Pas?ca, 2008) is extended in two
ways:
1. Class label coverage is increased by identify-
ing additional class labels (such as public agen-
cies and governmental agencies) for existing
582
instances such as Office of War Information),
2. The overall instance coverage is increased by
extracting additional instances (such as Addi-
son Wesley and Zebra Books) for existing class
labels (book publishers).
The WebTables database constructed by Cafarella
et al (2008) is used as the source of additional
instances. Evaluations on gold-standard labeled
classes and instances from existing linguistic re-
sources (Fellbaum, 1998) indicate coverage im-
provements relative to that of Van Durme and Pas?ca
(2008), while retaining similar precision levels.
2 First Phase Extractors
To show Adsorption?s ability to uniformly combine
extractions from multiple sources and methods, we
apply it to: 1) high-precision open-domain extrac-
tions from free Web text (Van Durme and Pas?ca,
2008), and 2) high-recall extractions from WebTa-
bles, a large database of HTML tables mined from
the Web (Cafarella et al, 2008). These two meth-
ods were chosen to be representative of two broad
classes of extraction sources: free text and structured
Web documents.
2.1 Extraction from Free Text
Van Durme and Pas?ca (2008) produce an open-
domain set of instance clusters C ? C that parti-
tions a given set of instances I using distributional
similarity (Lin and Pantel, 2002), and labels using
is-a patterns (Hearst, 1992). By filtering the class
labels using distributional similarity, a large number
of high-precision labeled clusters are extracted. The
algorithm proceeds iteratively: at each step, all clus-
ters are tested for label coherence and all coherent
labels are tested for high cluster specificity. Label
L is coherent if it is shared by at least J% of the
instances in cluster C, and it is specific if the total
number of other clusters C ? ? C, C ? 6= C containing
instances with label L is less thanK. When a cluster
is found to match these criteria, it is removed from
C and added to an output set. The procedure termi-
nates when no new clusters can be removed from C.
Table 1 shows a few randomly chosen classes and
representative instances obtained by this procedure.
2.2 Extraction from Structured Text
To expand the instance sets extracted from free
text, we use a table-based extraction method that
mines structured Web data in the form of HTML
tables. A significant fraction of the HTML ta-
bles in Web pages is assumed to contain coherent
lists of instances suitable for extraction. Identifying
such tables from scratch is hard, but seed instance
lists can be used to identify potentially coherent ta-
ble columns. In this paper we use the WebTables
database of around 154 million tables as our struc-
tured data source (Cafarella et al, 2008).
We employ a simple ranking scheme for candi-
date instances in the WebTables corpus T . Each ta-
ble T ? T consists of one or more columns. Each
column g ? T consists of a set of candidate in-
stances i ? g corresponding to row elements. We
define the set of unique seed matches in g relative to
semantic class C ? C as
MC(g)
def
= {i ? I(C) : i ? g}
where I(C) denotes the set of instances in seed class
C. For each column g, we define its ?-unique class
coverage, that is, the set of classes that have at least
? unique seeds in g,
Q(g;?)
def
= {C ? C : |MC(g)| ? ?}.
Using M and Q we define a method for scoring
columns relative to each class. Intuitively, such a
score should take into account not only the number
of matches from class C, but also the total num-
ber of classes that contribute to Q and their relative
overlap. Towards this end, we introduce the scoring
function
score(C, g;?)
def
= |MC(g)|
? ?? ?
seed matches
?
class coherence
? ?? ?
|MC(g)|
|
?
C??Q(g;?) I(C
?)|
which is the simplest scoring function combining
the number of seed matches with the coherence of
the table column. Coherence is a critical notion
in WebTables extraction, as some tables contain in-
stances across many diverse seed classes, contribut-
ing to extraction noise. The class coherence intro-
duced here also takes into account class overlap; that
583
Class Size Examples of Instances
Book Publishers 70 crown publishing, kluwer academic, prentice hall, puffin
Federal Agencies 161 catsa, dhs, dod, ex-im bank, fsis, iema, mema, nipc, nmfs, tdh, usdot
Mammals 956 armadillo, elephant shrews, long-tailed weasel, river otter, weddell seals, wild goat
NFL Players 180 aikman, deion sanders, fred taylor, jamal lewis, raghib ismail, troy vincent
Scientific Journals 265 biometrika, european economic review, nature genetics, neuroscience
Social Issues 210 gender inequality, lack of education, substandard housing, welfare dependency
Writers 5089 bronte sisters, hemingway, kipling, proust, torquato tasso, ungaretti, yeats
Table 1: A sample of the open-domain classes and associated instances from (Van Durme and Pas?ca, 2008).
is, a column containing many semantically similar
classes is penalized less than one containing diverse
classes.1 Finally, an extracted instance i is assigned
a score relative to class C equal to the sum of all its
column scores,
score(i, C;?)
def
=
1
ZC
?
g?T,T?T
score(C, g;?)
where ZC is a normalizing constant set to the max-
imum score of any instance in class C. This scor-
ing function assigns high rank to instances that oc-
cur frequently in columns with many seed matches
and high class specificity.
The ranked list of extracted instances is post-
filtered by removing all instances that occur in less
than d unique Internet domains.
3 Graph-Based Extraction
To combine the extractions from both free and struc-
tured text, we need a representation capable of en-
coding efficiently all the available information. We
chose a graph representation for the following rea-
sons:
? Graphs can represent complicated relationships
between classes and instances. For example,
an ambiguous instance such as Michael Jor-
dan could belong to the class of both Profes-
sors and NBA players. Similarly, an instance
may belong to multiple nodes in the hierarchy
of classes. For example, Blue Whales could be-
long to both classes Vertebrates and Mammals,
because Mammals are a subset of Vertebrates.
1Note that this scoring function does not take into account
class containment: if all seeds are both wind Instruments and
instruments, then the column should assign higher score to the
more specific class.
? Extractions frommultiple sources, such asWeb
queries, Web tables, and text patterns can be
represented in a single graph.
? Graphs make explicit the potential paths of in-
formation propagation that are implicit in the
more common local heuristics used for weakly-
supervised information extraction. For exam-
ple, if we know that the instance Bill Clinton
belongs to both classes President and Politician
then this should be treated as evidence that the
class of President and Politician are related.
Each instance-class pair (i, C) extracted in the
first phase (Section 2) is represented as a weighted
edge in a graph G = (V,E,W ), where V is the set
of nodes, E is the set of edges and W : E ? R+
is the weight function which assigns positive weight
to each edge. In particular, for each (i, C,w) triple
from the set of base extractions, i and C are added
to V and (i, C) is added to E, 2 with W (i, C) = w.
The weight w represents the total score of all extrac-
tions with that instance and class. Figure 1 illustrates
a portion of a sample graph. This simple graph rep-
resentation could be refined with additional types of
nodes and edges, as we discuss in Section 7.
In what follows, all nodes are treated in the same
way, regardless of whether they represent instances
or classes. In particular, all nodes can be assigned
class labels. For an instance node, that means that
the instance is hypothesized to belong to the class;
for a class node, that means that the node?s class is
hypothesized to be semantically similar to the label?s
class (Section 5).
We now formulate the task of assigning labels to
nodes as graph label propagation. We are given a
2In practice, we use two directed edges, from i to C and
from C to i, both with weight w.
584
bob dylan
musician
0.95
johnny cash
0.87
singer
0.73
billy joel
0.82
0.75
Figure 1: Section of a graph used as input into Adsorp-
tion. Though the nodes do not have any type associated
with them, for readability, instance nodes are marked in
pink while class nodes are shown in green.
set of instances I and a set of classes C represented
as nodes in the graph, with connecting edges as de-
scribed above. We annotate a few instance nodes
with labels drawn from C. That is, classes are used
both as nodes in the graph and as labels for nodes.
There is no necessary alignment between a class
node and any of the (class) labels, as the final labels
will be assigned by the Adsorption algorithm.
The Adsorption label propagation algo-
rithm (Baluja et al, 2008) is now applied to
the given graph. Adsorption is a general framework
for label propagation, consisting of a few nodes
annotated with labels and a rich graph structure
containing the universe of all labeled and unlabeled
nodes. Adsorption proceeds to label all nodes
based on the graph structure, ultimately producing a
probability distribution over labels for each node.
More specifically, Adsorption works on a graph
G = (V,E,W ) and computes for each node v a la-
bel distribution Lv that represents which labels are
more or less appropriate for that node. Several in-
terpretations of Adsorption-type algorithms have ap-
peared in various fields (Azran, 2007; Zhu et al,
2003; Szummer and Jaakkola, 2002; Indyk and Ma-
tousek, 2004). For details, the reader is referred to
(Baluja et al, 2008). We use two interpretations
here:
Adsorption through Random Walks: Let Gr =
(V,Er,Wr) be the edge-reversed version of the
original graph G = (V,E,W ) where (a, b) ?
Er iff (b, a) ? E; and Wr(a, b) = W (b, a).
Now, choose a node of interest q ? V . To es-
timate Lq for q, we perform a random walk on
Gr starting from q to generate values for a ran-
dom label variable L. After reaching a node v
during the walk, we have three choices:
1. With probability pcontv , continue the ran-
dom walk to a neighbor of v.
2. With probability pabndv , abandon the ran-
dom walk. This abandonment proba-
bility makes the random walk stay rela-
tively close to its source when the graph
has high-degree nodes. When the ran-
dom walk passes through such a node,
it is likely that further transitions will be
into regions of the graph unrelated to the
source. The abandonment probability mit-
igates that effect.
3. With probability pinjv , stop the random
walk and emit a label L from Iv.
Lq is set to the expectation of all labels L emit-
ted from random walks initiated from node q.
Adsorption through Averaging: For this interpre-
tation we make some changes to the original
graph structure and label set. We extend the la-
bel distributions Lv to assign a probability not
only to each label in C but also to the dummy
label ?, which represents lack of information
about the actual label(s). We represent the ini-
tial knowledge we have about some node labels
in an augmented graph G? = (V ?, E?,W ?) as
follows. For each v ? V , we define an ini-
tial distribution Iv = L?, where L? is the
dummy distribution with L?(?) = 1, repre-
senting lack of label information for v. In addi-
tion, let Vs ? V be the set of nodes for which
we have some actual label knowledge, and let
V ? = V ? {v? : v ? Vs}, E? = E ? {(v?, v) :
v ? Vs}, and W ?(v?, v) = 1 for v ? Vs,
W ?(u, v) = W (u, v) for u, v ? V . Finally,
let Iv? (seed labels) specify the knowledge about
possible labels for v ? Vs. Less formally, the
v? nodes in G? serve to inject into the graph the
prior label distributions for each v ? Vs.
The algorithm proceeds as follows: For each
node use a fixed-point computation to find label
585
distributions that are weighted averages of the
label distributions for all their neighbors. This
causes the non-dummy initial distribution of Vs
nodes to be propagated across the graph.
Baluja et al (2008) show that those two views are
equivalent. Algorithm 1 combines the two views:
instead of a random walk, for each node v, it itera-
tively computes the weighted average of label distri-
butions from neighboring nodes, and then uses the
random walk probabilities to estimate a new label
distribution for v.
For the experiments reported in Section 4, we
used the following heuristics from Baluja et al
(2008) to set the random walk probabilities:
? Let cv =
log ?
log(? + expH(v)) where H(v) =
?
?
u puv ? log(puv) with puv =
W (u,v)
P
u
? W (u
? ,v)
.
H(v) can be interpreted as the entropy of v?s
neighborhood. Thus, cv is lower if v has many
neighbors. We set ? = 2.
? jv = (1 ? cv) ?
?
H(v) if Iv 6= L> and 0
otherwise.
? Then let
zv = max(cv + jv, 1)
pcontv = cv/zv
pinjv = jv/zv
pabndv = 1? p
cont
v ? p
abnd
v
Thus, abandonment occurs only when the con-
tinuation and injection probabilities are low
enough.
The algorithm is run until convergence which is
achieved when the label distribution on each node
ceases to change within some tolerance value. Alter-
natively, the algorithm can be run for a fixed number
of iterations which is what we used in practice3.
Finally, since Adsorption is memoryless, it eas-
ily scales to tens of millions of nodes with dense
edges and can be easily parallelized, as described
by Baluja et al (2008).
3The number of iterations was set to 10 in the experiments
reported in this paper.
Algorithm 1 Adsorption Algorithm.
Input: G? = (V
?
, E
?
,W ?), Iv (?v ? V ?).
Output: Distributions {Lv : v ? V }.
1: Lv = Iv ?v ? V
?
2:
3: repeat
4: Nv =
?
u W (u, v)
5: Dv = 1Nv
?
u W (u, v)Lu ?v ? V
?
6: for all v ? V
?
do
7: Lv = pcontv ?Dv +p
inj
v ? Iv +pabndv ?L
>
8: end for
9: until convergence
4 Experiments
4.1 Data
As mentioned in Section 3, one of the benefits of
using Adsorption is that we can combine extrac-
tions by different methods from diverse sources into
a single framework. To demonstrate this capabil-
ity, we combine extractions from free-text patterns
and from Web tables. To the best of our knowl-
edge, this is one of the first attempts in the area of
minimally-supervised extraction algorithms where
unstructured and structured text are used in a prin-
cipled way within a single system.
Open-domain (instance, class) pairs were ex-
tracted by applying the method described by Van
Durme and Pas?ca (2008) on a corpus of over 100M
English web documents. A total of 924K (instance,
class) pairs were extracted, containing 263K unique
instances in 9081 classes. We refer to this dataset as
A8.
Using A8, an additional 74M unique (in-
stance,class) pairs are extracted from a random 10%
of the WebTables data, using the method outlined in
Section 2.2. For maximum coverage we set ? = 2
and d = 2, resulting in a large, but somewhat noisy
collection. We refer to this data set as WT.
4.2 Graph Creation
We applied the graph construction scheme described
in Section 3 on the A8 and WT data combined, re-
sulting in a graph with 1.4M nodes and 75M edges.
Since extractions in A8 are not scored, weight of all
586
Seed Class Seed Instances
Book Publishers millbrook press, academic press, springer verlag, chronicle books, shambhala publications
Federal Agencies dod, nsf, office of war information, tsa, fema
Mammals african wild dog, hyaena, hippopotamus, sperm whale, tiger
NFL Players ike hilliard, isaac bruce, torry holt, jon kitna, jamal lewis
Scientific Journals american journal of roentgenology, pnas, journal of bacteriology, american economic review,
ibm systems journal
Table 2: Classes and seeds used to initialize Adsorption.
edges originating from A8 were set at 14. This graph
is used in all subsequent experiments.
5 Evaluation
We evaluated the Adsorption algorithm under two
experimental settings. First, we evaluate Adsorp-
tion?s extraction precision on (instance, class) pairs
obtained by Adsorption but not present in A8 (Sec-
tion 5.1). This measures whether Adsorption can
add to the A8 extractions at fairly high precision.
Second, we measured Adsorption?s ability to assign
labels to a fixed set of gold instances drawn from
various classes (Section 5.2).
Book Publishers Federal Agencies NFL Players Scientific Journals Mammals20
40
60
80
100
 
 Adsorption A8
Book
Publishers
Federal
Agencies
NFL
Players
Scientific
Journals
Mammals
A8 Adsorption
Figure 2: Precision at 100 comparisons for A8 and Ad-
sorption.
5.1 Instance Precision
First we manually evaluated precision across five
randomly selected classes from A8: Book Publish-
ers, Federal Agencies, NFL Players, Scientific Jour-
nals and Mammals. For each class, 5 seed in-
stances were chosen manually to initialize Adsorp-
tion. These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the
4A8 extractions are assumed to be high-precision and hence
we assign them the highest possible weight.
resulting ranked extractions were manually evalu-
ated.
Since the A8 system does not produce ranked lists
of instances, we chose 100 random instances from
the A8 results to compare to the top 100 instances
produced by Adsorption. Each of the resulting 500
instance-class pairs (i, C) was presented to two hu-
man evaluators, who were asked to evaluate whether
the relation ?i is a C? was correct or incorrect. The
user was also presented with Web search link to ver-
ify the results against actual documents. Results
from these experiments are presented in Figure 2
and Table 4. The results in Figure 2 show that the
A8 system has higher precision than the Adsorption
system. This is not surprising since the A8 system is
tuned for high precision. When considering individ-
ual evaluation classes, changes in precision scores
between the A8 system and the Adsorption system
vary from a small increase from 87% to 89% for the
class Book Publishers, to a significant decrease from
52% to 34% for the class Federal Agencies, with a
decrease of 10% as an average over the 5 evaluation
classes.
Class Precision at 100
(non-A8 extractions)
Book Publishers 87.36
Federal Agencies 29.89
NFL Players 94.95
Scientific Journals 90.82
Mammal Species 84.27
Table 4: Precision of top 100 Adsorption extractions (for
five classes) which were not present in A8.
Table 4 shows the precision of the Adsorption sys-
tem for instances not extracted by the A8 system.
587
Seed Class Non-Seed Class Labels Discovered by Adsorption
Book Publishers small presses, journal publishers, educational publishers, academic publishers,
commercial publishers
Federal Agencies public agencies, governmental agencies, modulation schemes, private sources,
technical societies
NFL Players sports figures, football greats, football players, backs, quarterbacks
Scientific Journals prestigious journals, peer-reviewed journals, refereed journals, scholarly journals,
academic journals
Mammal Species marine mammal species, whale species, larger mammals, common animals, sea mammals
Table 3: Top class labels ranked by their similarity to a given seed class in Adsorption.
Seed Class Sample of Top Ranked Instances Discovered by Adsorption
Book Publishers small night shade books, house of anansi press, highwater books,
distributed art publishers, copper canyon press
NFL Players tony gonzales, thabiti davis, taylor stubblefield, ron dixon, rodney hannah
Scientific Journals journal of physics, nature structural and molecular biology,
sciences sociales et sante?, kidney and blood pressure research,
american journal of physiology?cell physiology
Table 5: Random examples of top ranked extractions (for three classes) found by Adsorption which were not present
in A8.
Such an evaluation is important as one of the main
motivations of the current work is to increase cov-
erage (recall) of existing high-precision extractors
without significantly affecting precision. Results in
Table 4 show that Adsorption is indeed able to ex-
traction with high precision (in 4 out of 5 cases)
new instance-class pairs which were not extracted
by the original high-precision extraction set (in this
case A8). Examples of a few such pairs are shown
in Table 5. This is promising as almost all state-
of-the-art extraction methods are high-precision and
low-recall. The proposed method shows a way to
overcome that limitation.
As noted in Section 3, Adsorption ignores node
type and hence the final ranked extraction may also
contain classes along with instances. Thus, in ad-
dition to finding new instances for classes, it also
finds additional class labels similar to the seed class
labels with which Adsorption was run, at no extra
cost. Some of the top ranked class labels extracted
by Adsorption for the corresponding seed class la-
bels are shown in Table 3. To the best of our knowl-
edge, there are no other systems which perform both
tasks simultaneously.
5.2 Class Label Recall
Next we evaluated each extraction method on its rel-
ative ability to assign labels to class instances. For
each test instance, the five most probably class la-
bels are collected using each method and the Mean
Reciprocal Rank (MRR) is computed relative to a
gold standard target set. This target set, WN-gold,
consists of the 38 classes in Wordnet containing 100
or more instances.
In order to extract meaningful output from Ad-
sorption, it is provided with a number of labeled seed
instances (1, 5, 10 or 25) from each of the 38 test
classes. Regardless of the actual number of seeds
used as input, all 25 seed instances from each class
are removed from the output set from all methods,
in order to ensure fair comparison.
The results from this evaluation are summarized
in Table 6; AD x refers to the adsorption run with x
seed instances. Overall, Adsorption exhibits higher
MRR than either of the baseline methods, with MRR
increasing as the amount of supervision is increased.
Due to its high coverage, WT assigns labels to
a larger number of the instance in WN-gold than
any other method. However, the average rank of
the correct class assignment is lower, resulting is
588
MRR MRR # found
Method (full) (found only)
A8 0.16 0.47 2718
WT 0.15 0.21 5747
AD 1 0.26 0.45 4687
AD 5 0.29 0.48 4687
AD 10 0.30 0.51 4687
AD 25 0.32 0.55 4687
Table 6: Mean-Reciprocal Rank scores of instance class
labels over 38 Wordnet classes (WN-gold). MRR (full)
refers to evaluation across the entire gold instance set.
MRR (found only) computes MRR only on recalled in-
stances.
lower MRR scores compared to Adsorption. This
result highlights Adsorption?s ability to effectively
combine high-precision, low-recall (A8) extractions
with low-precision, high-recall extractions (WT) in
a manner that improves both precision and coverage.
6 Related Work
Graph based algorithms for minimally supervised
information extraction methods have recently been
proposed. For example, Wang and Cohen (2007)
use a random walk on a graph built from entities and
relations extracted from semi-structured text. Our
work differs both conceptually, in terms of its focus
on open-domain extraction, as well as methodologi-
cally, as we incorporate both unstructured and struc-
tured text. The re-ranking algorithm of Bellare et al
(2007) also constructs a graph whose nodes are in-
stances and attributes, as opposed to instances and
classes here. Adsorption can be seen as a general-
ization of the method proposed in that paper.
7 Conclusion
The field of open-domain information extraction has
been driven by the growth of Web-accessible data.
We have staggering amounts of data from various
structured and unstructured sources such as general
Web text, online encyclopedias, query logs, web ta-
bles, or link anchor texts. Any proposed algorithm
to extract information needs to harness several data
sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards
that goal. In doing so, we achieved the following:
1. Improved coverage relative to a high accuracy
instance-class extraction system while main-
taining adequate precision.
2. Combined information from two different
sources: free text and web tables.
3. Demonstrated a graph-based label propagation
algorithm that given as little as five seeds per
class achieved good results on a graph with
more than a million nodes and 70 million
edges.
In this paper, we started off with a simple graph.
For future work, we plan to proceed along the fol-
lowing lines:
1. Encode richer relationships between nodes,
for example instance-instance associations and
other types of nodes.
2. Combine information from more data sources
to answer the question of whether more data or
diverse sources are more effective in increasing
precision and coverage.
3. Apply similar ideas to other information extrac-
tion tasks such as relation extraction.
Acknowledgments
We would like to thank D. Sivakumar for useful dis-
cussions and the anonymous reviewers for helpful
comments.
References
A. Azran. 2007. The rendezvous algorithm: multiclass
semi-supervised learning with markov random walks.
Proceedings of the 24th international conference on
Machine learning, pages 49?56.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random
walks through the view graph.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. NIPS 2007Workshop
on Machine Learning for Web Search.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. Webtables: Exploring the power of tables on the
web. VLDB.
589
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
P. Indyk and J. Matousek. 2004. Low-distortion embed-
dings of finite metric spaces. Handbook of Discrete
and Computational Geometry.
B. Jansen, A. Spink, and T. Saracevic. 2000. Real life,
real users, and real needs: a study and analysis of user
queries on the Web. Information Processing and Man-
agement, 36(2):207?227.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
K. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence (IJCAI-95), pages 1050?1055, Montreal, Que-
bec.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-
derived name lists for named entity recognition. In
Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), Seattle, Wash-
ington.
M. Szummer and T. Jaakkola. 2002. Partially labeled
classification with markov random walks. Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2002 NIPS Conference.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. Twenty-Third AAAI Conference on Artificial In-
telligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using theWeb. Data
Mining, 2007. ICDM 2007. Seventh IEEE Interna-
tional Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
590
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 675?682,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Semi-Supervised Polarity Lexicon Induction
Delip Rao?
Department of Computer Science
Johns Hopkins University
Baltimore, MD
delip@cs.jhu.edu
Deepak Ravichandran
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA
deepakr@google.com
Abstract
We present an extensive study on the prob-
lem of detecting polarity of words. We
consider the polarity of a word to be ei-
ther positive or negative. For example,
words such as good, beautiful , and won-
derful are considered as positive words;
whereas words such as bad, ugly, and sad
are considered negative words. We treat
polarity detection as a semi-supervised la-
bel propagation problem in a graph. In
the graph, each node represents a word
whose polarity is to be determined. Each
weighted edge encodes a relation that ex-
ists between two words. Each node (word)
can have two labels: positive or negative.
We study this framework in two differ-
ent resource availability scenarios using
WordNet and OpenOffice thesaurus when
WordNet is not available. We report our
results on three different languages: En-
glish, French, and Hindi. Our results in-
dicate that label propagation improves sig-
nificantly over the baseline and other semi-
supervised learning methods like Mincuts
and Randomized Mincuts for this task.
1 Introduction
Opinionated texts are characterized by words or
phrases that communicate positive or negative sen-
timent. Consider the following example of two
movie reviews1 shown in Figure 1. The posi-
tive review is peppered with words such as enjoy-
able, likeable, decent, breathtakingly and the negative
?Work done as a summer intern at Google Inc.
1Source: Live Free or Die Hard,
rottentomatoes.com
Figure 1: Movie Reviews with positive (left) and
negative (right) sentiment.
comment uses words like ear-shattering, humorless,
unbearable. These terms and prior knowledge of
their polarity could be used as features in a su-
pervised classification framework to determine the
sentiment of the opinionated text (E.g., (Esuli and
Sebastiani, 2006)). Thus lexicons indicating po-
larity of such words are indispensable resources
not only in automatic sentiment analysis but also
in other natural language understanding tasks like
textual entailment. This motivation was seen in
the General Enquirer effort by Stone et al (1966)
and several others who manually construct such
lexicons for the English language.2 While it is
possible to manually build these resources for a
language, the ensuing effort is onerous. This mo-
tivates the need for automatic language-agnostic
methods for building sentiment lexicons. The im-
portance of this problem has warranted several ef-
forts in the past, some of which will be reviewed
here.
We demonstrate the application of graph-based
semi-supervised learning for induction of polar-
ity lexicons. We try several graph-based semi-
2The General Inquirer tries to classify English words
along several dimensions, including polarity.
675
supervised learning methods like Mincuts, Ran-
domized Mincuts, and Label Propagation. In par-
ticular, we define a graph with nodes consisting
of the words or phrases to be classified either as
positive or negative. The edges between the nodes
encode some notion of similarity. In a transduc-
tive fashion, a few of these nodes are labeled us-
ing seed examples and the labels for the remaining
nodes are derived using these seeds. We explore
natural word-graph sources like WordNet and ex-
ploit different relations within WordNet like syn-
onymy and hypernymy. Our method is not just
confined to WordNet; any source listing synonyms
could be used. To demonstrate this, we show
the use of OpenOffice thesaurus ? a free resource
available in several languages.3
We begin by discussing some related work in
Section 2 and briefly describe the learning meth-
ods we use, in Section 3. Section 4 details our
evaluation methodology along with detailed ex-
periments for English. In Section 5 we demon-
strate results in French and Hindi, as an example
of how the method could be easily applied to other
languages as well.
2 Related Work
The literature on sentiment polarity lexicon induc-
tion can be broadly classified into two categories,
those based on corpora and the ones using Word-
Net.
2.1 Corpora based approaches
One of the earliest work on learning polarity
of terms was by Hatzivassiloglou and McKeown
(1997) who deduce polarity by exploiting con-
straints on conjoined adjectives in the Wall Street
Journal corpus. For example, the conjunction
?and? links adjectives of the same polarity while
?but? links adjectives of opposite polarity. How-
ever the applicability of this method for other im-
portant classes of sentiment terms like nouns and
verbs is yet to be demonstrated. Further they as-
sume linguistic features specific to English.
Wiebe (2000) uses Lin (1998a) style distribu-
tionally similar adjectives in a cluster-and-label
process to generate sentiment lexicon of adjec-
tives.
In a different work, Riloff et al (2003) use man-
ually derived pattern templates to extract subjec-
tive nouns by bootstrapping.
3http://www.openoffice.org
Another corpora based method due to Turney
and Littman (2003) tries to measure the semantic
orientation O(t) for a term t by
O(t) =
?
ti?S+
PMI(t, ti) ?
?
tj?S?
PMI(t, tj)
where S+ and S? are minimal sets of polar terms
that contain prototypical positive and negative
terms respectively, and PMI(t, ti) is the point-
wise mutual information (Lin, 1998b) between
the terms t and ti. While this method is general
enough to be applied to several languages our aim
was to develop methods that exploit more struc-
tured sources like WordNet to leverage benefits
from the rich network structure.
Kaji and Kitsuregawa (2007) outline a method
of building sentiment lexicons for Japanese us-
ing structural cues from HTML documents. Apart
from being very specific to Japanese, excessive de-
pendence on HTML structure makes their method
brittle.
2.2 WordNet based approaches
These approaches use lexical relations defined in
WordNet to derive sentiment lexicons. A sim-
ple but high-precision method proposed by Kim
and Hovy (2006) is to add all synonyms of a po-
lar word with the same polarity and its antonyms
with reverse polarity. As demonstrated later, the
method suffers from low recall and is unsuitable in
situations when the seed polar words are too few ?
not uncommon in low resource languages.
In line with Turney?s work, Kamps et. al. (2004)
try to determine sentiments of adjectives in Word-
Net by measuring relative distance of the term
from exemplars, such as ?good? and ?bad?. The
polarity orientation of a term t is measured as fol-
lows
O(t) = d(t, good) ? d(t, bad)d(good, bad)
where d(.) is a WordNet based relatedness mea-
sure (Pedersen et al, 2004). Again they report re-
sults for adjectives alone.
Another relevant example is the recent work by
Mihalcea et. al. (2007) on multilingual sentiment
analysis using cross-lingual projections. This is
achieved by using bridge resources like dictionar-
ies and parallel corpora to build sentence subjec-
tivity classifiers for the target language (Roma-
nian). An interesting result from their work is that
676
only a small fraction of the lexicon entries pre-
serve their polarities under translation.
The primary contributions of this paper are :
? An application of graph-based semi-
supervised learning methods for inducing
sentiment lexicons from WordNet and other
thesauri. The label propagation method
naturally allows combining several relations
from WordNet.
? Our approach works on all classes of words
and not just adjectives
? Though we report results for English, Hindi,
and French, our methods can be easily repli-
cated for other languages where WordNet is
available.4 In the absence of WordNet, any
thesaurus listing synonyms could be used.
We present one such result using the OpenOf-
fice thesaurus ? a freely available multilin-
gual resource scarcely used in NLP literature.
3 Graph based semi-supervised learning
Most natural language data has some structure that
could be exploited even in the absence of fully an-
notated data. For instance, documents are simi-
lar in the terms they contain, words could be syn-
onyms of each other, and so on. Such informa-
tion can be readily encoded as a graph where the
presence of an edge between two nodes would in-
dicate a relationship between the two nodes and,
optionally, the weight on the edge could encode
strength of the relationship. This additional infor-
mation aids learning when very few annotated ex-
amples are present. We review three well known
graph based semi-supervised learning methods ?
mincuts, randomized mincuts, and label propaga-
tion ? that we use in induction of polarity lexicons.
3.1 Mincuts
A mincut of a weighted graph G(V,E) is a par-
titioning the vertices V into V1 and V2 such that
sum of the edge weights of all edges between V1
and V2 is minimal (Figure 2).
Mincuts for semi-supervised learning proposed
by Blum and Chawla (2001) tries to classify data-
points by partitioning the similarity graph such
that it minimizes the number of similar points be-
ing labeled differently. Mincuts have been used
4As of this writing, WordNet is available for more than 40
world languages (http://www.globalwordnet.org)
Figure 2: Semi-supervised classification using
mincuts
in semi-supervised learning for various tasks, in-
cluding document level sentiment analysis (Pang
and Lee, 2004). We explore the use of mincuts for
the task of sentiment lexicon learning.
3.2 Randomized Mincuts
An improvement to the basic mincut algorithm
was proposed by Blum et. al. (2004). The deter-
ministic mincut algorithm, solved using max-flow,
produces only one of the several possible mincuts.
Some of these cuts could be skewed thereby nega-
tively effecting the results. As an extreme example
consider the graph in Figure 3a. Let the nodes with
degree one be labeled as positive and negative re-
spectively, and for the purpose of illustration let
all edges be of the same weight. The graph in Fig-
ure 3a. can be partitioned in four equal cost cuts ?
two of which are shown in (b) and (c). The min-
Figure 3: Problem with mincuts
cut algorithm, depending on the implementation,
will return only one of the extreme cuts (as in (b))
while the desired classification might be as shown
in Figure 3c.
The randomized mincut approach tries to ad-
dress this problem by randomly perturbing the ad-
jacency matrix by adding random noise.5 Mincut
is then performed on this perturbed graph. This is
5We use a Gaussian noise N (0, 1).
677
repeated several times and unbalanced partitions
are discarded. Finally the remaining partitions are
used to deduce the final classification by majority
voting. In the unlikely event of the voting result-
ing in a tie, we refrain from making a decision thus
favoring precision over recall.
3.3 Label propagation
Another semi-supervised learning method we use
is label propagation by Zhu and Ghahramani
(2002). The label propagation algorithm is a trans-
ductive learning framework which uses a few ex-
amples, or seeds, to label a large number of un-
labeled examples. In addition to the seed exam-
ples, the algorithm also uses a relation between the
examples. This relation should have two require-
ments:
1. It should be transitive.
2. It should encode some notion of relatedness
between the examples.
To name a few, examples of such relations in-
clude, synonymy, hypernymy, and similarity in
some metric space. This relation between the ex-
amples can be easily encoded as a graph. Thus ev-
ery node in the graph is an example and the edge
represents the relation. Also associated with each
node, is a probability distribution over the labels
for the node. For the seed nodes, this distribution
is known and kept fixed. The aim is to derive the
distributions for the remaining nodes.
Consider a graph G(V,E,W ) with vertices V ,
edges E, and an n ? n edge weight matrix W =
[wij ], where n = |V |. The label propagation algo-
rithm minimizes a quadratic energy function
E = 12
?
(i, j) ? E
wij(yi ? yj)2
where yi and yj are the labels assigned to the
nodes i and j respectively.6 Thus, to derive the
labels at yi, we set ??yiE = 0 to obtain the follow-
ing update equation
yi =
?
(i,j)?E
wijyj
?
(i,j)?E
wij
In practice, we use the following iterative algo-
rithm as noted by Zhu and Ghahramani (2002). A
6For binary classification yk ? {?1, +1}.
n ? n stochastic transition matrix T is derived by
row-normalizing W as follows:
Tij = P (j ? i) =
wij
?n
k=1 wkj
where Tij can be viewed as the transition probabil-
ity from node j to node i. The algorithm proceeds
as follows:
1. Assign a n ? C matrix Y with the initial as-
signment of labels, where C is the number of
classes.
2. Propagate labels for all nodes by computing
Y = TY
3. Row-normalize Y such that each row adds up
to one.
4. Clamp the seed examples in Y to their origi-
nal values
5. Repeat 2-5 until Y converges.
There are several points to be noted. First, we add
a special label ?DEFAULT? to existing set of la-
bels and set P (DEFAULT | node = u) = 1 for all
unlabeled nodes u. For all the seed nodes s with
class label Lwe define P (L | node = s) = 1. This
ensures nodes that cannot be labeled at all7 will re-
tain P (DEFAULT) = 1 thereby leading to a quick
convergence. Second, the algorithm produces a
probability distribution over the labels for all un-
labeled points. This makes this method specially
suitable for classifier combination approaches. For
this paper, we simply select the most likely label
as the predicted label for the point. Third, the al-
gorithm eventually converges. For details on the
proof for convergence we refer the reader to Zhu
and Ghahramani (2002).
4 Evaluation and Experiments
We use the General Inquirer (GI)8 data for eval-
uation. General Inquirer is lexicon of English
words hand-labeled with categorical information
along several dimensions. One such dimension is
called valence, with 1915 words labeled ?Positiv?
(sic) and 2291 words labeled ?Negativ? for words
with positive and negative sentiments respectively.
Since we want to evaluate the performance of the
7As an example of such a situation, consider a discon-
nected component of unlabeled nodes with no seed in it.
8http://www.wjh.harvard.edu/?inquirer/
678
algorithms alone and not the recall issues in us-
ing WordNet, we only consider words from GI that
also occur in WordNet. This leaves us the distri-
bution of words as enumerated in Table 1.
PoS type No. of Positives No. of Negatives
Nouns 517 579
Verbs 319 562
Adjectives 547 438
Table 1: English evaluation data from General In-
quirer
All experiments reported in Sections 4.1 to 4.5
use the data described above with a 50-50 split
so that the first half is used as seeds and the sec-
ond half is used for test. Note that all the exper-
iments described below did not involve any pa-
rameter tuning thus obviating the need for a sepa-
rate development test set. The effect of number of
seeds on learning is described in Section 4.6.
4.1 Kim-Hovy method and improvements
Kim and Hovy (2006) enrich their sentiment lexi-
con from WordNet as follows. Synonyms of a pos-
itive word are positive while antonyms are treated
as negative. This basic version suffers from a very
poor recall as shown in the Figure 4 for adjectives
(see iteration 1). The recall can be improved for a
slight trade-off in precision if we re-run the above
algorithm on the output produced at the previous
level. This could be repeated iteratively until there
is no noticeable change in precision/recall. We
consider this as the best possible F1-score pro-
duced by the Kim-Hovy method. The classwise
F1 for this method is shown in Table 2. We use
these scores as our baseline.
Figure 4: Kim-Hovy method
PoS type P R F1
Nouns 92.59 21.43 34.80
Verbs 87.89 38.31 53.36
Adjectives 92.95 31.71 47.28
Table 2: Precision/Recall/F1-scores for Kim-
Hovy method
4.2 Using prototypes
We now consider measuring semantic orientation
from WordNet using prototypical examples such
as ?good? and ?bad? similar to Kamps et al
(2004). Kamps et. al., report results only for
adjectives though their method could be used for
other part-of-speech types. The results for us-
ing prototypes are listed in Table 3. Note that
the seed data was fully unused except for the ex-
amples ?good? and ?bad?. We still test on the
same test data as earlier for comparing results.
Also note that the recall need not be 100 in this
case as we refrain from making a decision when
d(t,good) = d(t,bad).
PoS type P R F1
Nouns 48.03 99.82 64.86
Verbs 58.12 100.00 73.51
Adjectives 57.35 99.59 72.78
Table 3: Precision/Recall/F1-scores for prototype
method
4.3 Using mincuts and randomized mincuts
We now report results for mincuts and random-
ized mincuts algorithm using the WordNet syn-
onym graph. As seen in Table 4, we only observed
a marginal improvement (for verbs) over mincuts
by using randomized mincuts.
But the overall improvement of using graph-
based semi-supervised learning methods over the
Kim-Hovy and Prototype methods is quite signifi-
cant.
4.4 Using label propagation
We extract the synonym graph from WordNet with
an edge between two nodes being defined iff one
is a synonym of the other. When label propaga-
tion is performed on this graph results in Table
5 are observed. The results presented in Tables
2-5 need deeper inspection. The iterated Kim-
Hovy method suffers from poor recall. However
both mincut methods and the prototype method by
679
P R F1
Nouns
Mincut 68.25 100.00 81.13
RandMincut 68.32 99.09 80.08
Verbs
Mincut 72.34 100.00 83.95
RandMincut 73.06 99.02 84.19
Adjectives
Mincut 73.78 100.00 84.91
RandMincut 73.58 100.00 84.78
Table 4: Precision/Recall/F1-scores using mincuts
and randomized mincuts
PoS type P R F1
Nouns 82.55 58.58 58.53
Verbs 81.00 85.94 83.40
Adjectives 84.76 64.02 72.95
Table 5: Precision/Recall/F1-scores for Label Pro-
pogation
Kamps et. al., have high recall as they end up
classifying every node as either positive or nega-
tive. Note that the recall for randomized mincut
is not 100 as we do not make a classification de-
cision when there is a tie in majority voting (refer
Section 3.2). Observe that the label propagation
method performs significantly better than previ-
ous graph based methods in precision. The rea-
son for lower recall is attributed to the lack of con-
nectivity between plausibly related nodes, thereby
not facilitating the ?spread? of labels from the la-
beled seed nodes to the unlabeled nodes. We ad-
dress this problem by adding additional edges to
the synonym graph in the next section.
4.5 Incorporating hypernyms
The main reason for low recall in label propaga-
tion is that the WordNet synonym graph is highly
disconnected. Even nodes which are logically re-
lated have paths missing between them. For exam-
ple the positive nouns compliment and laud belong
to different synonym subgraphs without a path
between them. But incorporating the hypernym
edges the two are connected by the noun praise.
So, we incorporated hypernyms of every node to
improve connectivity. Performing label propaga-
tion on this combined graph gives much better re-
sults (Table 6) with much higher recall and even
slightly better precision. In Table 6., we do not
report results for adjectives as WordNet does not
define hypernyms for adjectives. A natural ques-
PoS type P R F1
Nouns 83.88 99.64 91.08
Verbs 85.49 100.00 92.18
Adjectives N/A N/A N/A
Table 6: Effect of adding hypernyms
tion to ask is if we can use other WordNet relations
too. We will defer this until section 6.
4.6 Effect of number of seeds
The results reported in Sections 4.1 to 4.5 fixed
the number of seeds. We now investigate the per-
formance of the various methods on the number
of seeds used. In particular, we are interested in
performance under conditions when the number of
seeds are few ? which is the motivation for using
semi-supervised learning in the first place. Fig-
ure 5 presents our results for English. Observe that
Label Propagation performs much better than our
baseline even when the number of seeds is as low
as ten. Thus label propagation is especially suited
when annotation data is extremely sparse.
One reason for mincuts performing badly with
few seeds is because they generate degenrate cuts.
5 Adapting to other languages
In order to demonstrate the ease of adaptability of
our method for other languages, we used the Hindi
WordNet9 to derive the adjective synonym graph.
We selected 489 adjectives at random from a list
of 10656 adjectives and this list was annotated by
two native speakers of the language. The anno-
tated list was then split 50-50 into seed and test
sets. Label propagation was performed using the
seed list and evaluated on the test list. The results
are listed in Table 7.
Hindi P R F1
90.99 95.10 93.00
Table 7: Evaluation on Hindi dataset
WordNet might not be freely available for all
languages or may not exist. In such cases build-
ing graph from an existing thesaurus might also
suffice. As an example, we consider French. Al-
though the French WordNet is available10 , we
9http://www.cfilt.iitb.ac.in/wordnet/webhwn/
10http://www.illc.uva.nl/EuroWordNet/consortium-
ewn.html
680
Figure 5: Effect of number of seeds on the F-score for Nouns, Verbs, and Adjectives. The X-axis is
number of seeds and the Y-axis is the F-score.
found the cost prohibitive to obtain it. Observe
that if we are using only the synonymy relation in
WordNet then any thesaurus can be used instead.
To demonstrate this, we consider the OpenOffice
thesaurus for French, that is freely available. The
synonym graph of French adjectives has 9707 ver-
tices and 1.6M edges. We manually annotated a
list of 316 adjectives and derived seed and test sets
using a 50-50 split. The results of label propaga-
tion on such a graph is shown in Table 8.
French P R F1
73.65 93.67 82.46
Table 8: Evaluation on French dataset
The reason for better results in Hindi compared
to French can be attributed to (1) higher inter-
annotator agreement (? = 0.7) in Hindi compared
that in French (? = 0.55).11 (2) The Hindi ex-
periment, like English, used WordNet while the
French experiment was performed on graphs de-
rived from the OpenOffice thesaurus due lack of
freely available French WordNet.
11We do not have ? scores for English dataset derived from
the Harvard Inquirer project.
6 Conclusions and Future Work
This paper demonstrated the utility of graph-based
semi-supervised learning framework for building
sentiment lexicons in a variety of resource avail-
ability situations. We explored how the struc-
ture of WordNet could be leveraged to derive
polarity lexicons. The paper combines, for the
first time, relationships like synonymy and hyper-
nymy to improve label propagation results. All
of our methods are independent of language as
shown in the French and Hindi cases. We demon-
strated applicability of our approach on alterna-
tive thesaurus-derived graphs when WordNet is
not freely available, as in the case of French.
Although our current work uses WordNet and
other thesauri, in resource poor situations when
only monolingual raw text is available we can per-
form label propagation on nearest neighbor graphs
derived directly from raw text using distributional
similarity methods. This is work in progress.
We are also currently working on the possibil-
ity of including WordNet relations other than syn-
onymy and hypernymy. One relation that is in-
teresting and useful is antonymy. Antonym edges
cannot be added in a straight-forward way to the
681
graph for label propagation as antonymy encodes
negative similarity (or dissimilarity) and the dis-
similarity relation is not transitive.
References
[Blum and Chawla2001] Avrim Blum and Shuchi
Chawla. 2001. Learning from labeled and un-
labeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages
19?26.
[Blum et al2004] Blum, Lafferty, Rwebangira, and
Reddy. 2004. Semi-supervised learning using ran-
domized mincuts. In Proceedings of the ICML.
[Esuli and Sebastiani2006] Andrea Esuli and Fabrizio
Sebastiani. 2006. Determining term subjectivity
and term orientation for opinion mining. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 193?200.
[Hatzivassiloglou and McKeown1997] Vasileios Hatzi-
vassiloglou and Kathleen McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of the ACL, pages 174?181.
[Kaji and Kitsuregawa2007] Nobuhiro Kaji and Masaru
Kitsuregawa. 2007. Building lexicon for sentiment
analysis from massive collection of HTML docu-
ments. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1075?1083.
[Kamps et al2004] Jaap Kamps, Maarten Marx, R. ort.
Mokken, and Maarten de Rijke. 2004. Using
WordNet to measure semantic orientation of adjec-
tives. In Proceedings of LREC-04, 4th International
Conference on Language Resources and Evaluation,
volume IV.
[Kim and Hovy2006] Soo-Min Kim and Eduard H.
Hovy. 2006. Identifying and analyzing judgment
opinions. In Proceedings of the HLT-NAACL.
[Lin1998a] Dekang Lin. 1998a. Automatic retrieval
and clustering of similar words. In Proceedings of
COLING, pages 768?774.
[Lin1998b] Dekang Lin. 1998b. An information-
theoretic definition of similarity. In Proceedings
of the 15th International Conference in Machine
Learning, pages 296?304.
[Mihalcea et al2007] Rada Mihalcea, Carmen Banea,
and Janyce Wiebe. 2007. Learning multilingual
subjective language via cross-lingual projections. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 976?
983.
[Pang and Lee2004] Bo Pang and Lillian Lee. 2004.
A sentimental education: Sentiment analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of the ACL, pages 271?278.
[Pedersen et al2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. Word-
net::similarity - measuring the relatedness of con-
cepts. In Proceeding of the HLT-NAACL.
[Riloff et al2003] Ellen Riloff, Janyce Wiebe, and
Theresa Wilson. 2003. Learning subjective nouns
using extraction pattern bootstrapping. In Proceed-
ings of the 7th Conference on Natural Language
Learning, pages 25?32.
[Stone et al1966] Philip J. Stone, Dexter C. Dunphy,
Marshall S. Smith, and Daniel M. Ogilvie. 1966.
The General Inquirer: A Computer Approach to
Content Analysis. MIT Press.
[Turney and Littman2003] Peter D. Turney and
Michael L. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from
association. ACM Transactions on Information
Systems, 21(4):315?346.
[Wiebe2000] Janyce M. Wiebe. 2000. Learning sub-
jective adjectives from corpora. In Proceedings of
the 2000 National Conference on Artificial Intelli-
gence. AAAI.
[Zhu and Ghahramani2002] Xiaojin Zhu and Zoubin
Ghahramani. 2002. Learning from labeled and un-
labeled data with label propagation. Technical Re-
port CMU-CALD-02-107, Carnegie Mellon Univer-
sity.
682
Proceedings of ACL-08: HLT, pages 674?682,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Large Scale Acquisition of Paraphrases for Learning Surface Patterns
Rahul Bhagat?
Information Sciences Institute
University of Southern California
Marina del Rey, CA
rahul@isi.edu
Deepak Ravichandran
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA
deepakr@google.com
Abstract
Paraphrases have proved to be useful in many
applications, including Machine Translation,
Question Answering, Summarization, and In-
formation Retrieval. Paraphrase acquisition
methods that use a single monolingual corpus
often produce only syntactic paraphrases. We
present a method for obtaining surface para-
phrases, using a 150GB (25 billion words)
monolingual corpus. Our method achieves an
accuracy of around 70% on the paraphrase ac-
quisition task. We further show that we can
use these paraphrases to generate surface pat-
terns for relation extraction. Our patterns are
much more precise than those obtained by us-
ing a state of the art baseline and can extract
relations with more than 80% precision for
each of the test relations.
1 Introduction
Paraphrases are textual expressions that convey the
same meaning using different surface words. For ex-
ample consider the following sentences:
Google acquired YouTube. (1)
Google completed the acquisition of YouTube. (2)
Since they convey the same meaning, sentences
(1) and (2) are sentence level paraphrases, and the
phrases ?acquired? and ?completed the acquisition
of ? in (1) and (2) respectively are phrasal para-
phrases.
Paraphrases provide a way to capture the vari-
ability of language and hence play an important
?Work done during an internship at Google Inc.
role in many natural language processing (NLP) ap-
plications. For example, in question answering,
paraphrases have been used to find multiple pat-
terns that pinpoint the same answer (Ravichandran
and Hovy, 2002); in statistical machine transla-
tion, they have been used to find translations for
unseen source language phrases (Callison-Burch et
al., 2006); in multi-document summarization, they
have been used to identify phrases from different
sentences that express the same information (Barzi-
lay et al, 1999); in information retrieval they have
been used for query expansion (Anick and Tipirneni,
1999).
Learning paraphrases requires one to ensure iden-
tity of meaning. Since there are no adequate se-
mantic interpretation systems available today, para-
phrase acquisition techniques use some other mech-
anism as a kind of ?pivot? to (help) ensure semantic
identity. Each pivot mechanism selects phrases with
similar meaning in a different characteristic way. A
popular method, the so-called distributional simi-
larity, is based on the dictum of Zelig Harris ?you
shall know the words by the company they keep?:
given highly discriminating left and right contexts,
only words with very similar meaning will be found
to fit in between them. For paraphrasing, this has
been often used to find syntactic transformations in
parse trees that preserve (semantic) meaning. An-
other method is to use a bilingual dictionary or trans-
lation table as pivot mechanism: all source language
words or phrases that translate to a given foreign
word/phrase are deemed to be paraphrases of one
another. In this paper we call the paraphrases that
contain only words as surface paraphrases and those
674
that contain paths in a syntax tree as syntactic para-
phrases.
We here, present a method to acquire surface
paraphrases from a single monolingual corpus. We
use a large corpus (about 150GB) to overcome the
data sparseness problem. To overcome the scalabil-
ity problem, we pre-process the text with a simple
parts-of-speech (POS) tagger and then apply locality
sensitive hashing (LSH) (Charikar, 2002; Ravichan-
dran et al, 2005) to speed up the remaining compu-
tation for paraphrase acquisition. Our experiments
show results to verify the following main claim:
Claim 1: Highly precise surface paraphrases can be
obtained from a very large monolingual corpus.
With this result, we further show that these para-
phrases can be used to obtain high precision surface
patterns that enable the discovery of relations in a
minimally supervised way. Surface patterns are tem-
plates for extracting information from text. For ex-
ample, if one wanted to extract a list of company ac-
quisitions, ??ACQUIRER? acquired ?ACQUIREE??
would be one surface pattern with ??ACQUIRER??
and ??ACQUIREE?? as the slots to be extracted.
Thus we can claim:
Claim 2: These paraphrases can then be used for
generating high precision surface patterns for rela-
tion extraction.
2 Related Work
Most recent work in paraphrase acquisition is based
on automatic acquisition. Barzilay and McKeown
(2001) used a monolingual parallel corpus to obtain
paraphrases. Bannard and Callison-Burch (2005)
and Zhou et al (2006) both employed a bilingual
parallel corpus in which each foreign language word
or phrase was a pivot to obtain source language para-
phrases. Dolan et al (2004) and Barzilay and Lee
(2003) used comparable news articles to obtain sen-
tence level paraphrases. All these approaches rely
on the presence of parallel or comparable corpora
and are thus limited by their availability and size.
Lin and Pantel (2001) and Szpektor et al (2004)
proposed methods to obtain entailment templates by
using a single monolingual resource. While both dif-
fer in their approaches, they both end up finding syn-
tactic paraphrases. Their methods cannot be used if
we cannot parse the data (either because of scale or
data quality). Our approach on the other hand, finds
surface paraphrases; it is more scalable and robust
due to the use of simple POS tagging. Also, our use
of locality sensitive hashing makes finding similar
phrases in a large corpus feasible.
Another task related to our work is relation extrac-
tion. Its aim is to extract instances of a given rela-
tion. Hearst (1992) the pioneering paper in the field
used a small number of hand selected patterns to ex-
tract instances of hyponymy relation. Berland and
Charniak (1999) used a similar method for extract-
ing instances of meronymy relation. Ravichandran
and Hovy (2002) used seed instances of a relation
to automatically obtain surface patterns by querying
the web. But their method often finds patterns that
are too general (e.g., X and Y), resulting in low pre-
cision extractions. Rosenfeld and Feldman (2006)
present a somewhat similar web based method that
uses a combination of seed instances and seed pat-
terns to learn good quality surface patterns. Both
these methods differ from ours in that they learn
relation patterns on the fly (from the web). Our
method however, pre-computes paraphrases for a
large set of surface patterns using distributional sim-
ilarity over a large corpus and then obtains patterns
for a relation by simply finding paraphrases (offline)
for a few seed patterns. Using distributional simi-
larity avoids the problem of obtaining overly gen-
eral patterns and the pre-computation of paraphrases
means that we can obtain the set of patterns for any
relation instantaneously.
Romano et al (2006) and Sekine (2006) used syn-
tactic paraphrases to obtain patterns for extracting
relations. While procedurally different, both meth-
ods depend heavily on the performance of the syntax
parser and require complex syntax tree matching to
extract the relation instances. Our method on the
other hand acquires surface patterns and thus avoids
the dependence on a parser and syntactic matching.
This also makes the extraction process scalable.
3 Acquiring Paraphrases
This section describes our model for acquiring para-
phrases from text.
675
3.1 Distributional Similarity
Harris?s distributional hypothesis (Harris, 1954) has
played an important role in lexical semantics. It
states that words that appear in similar contexts tend
to have similar meanings. In this paper, we apply
the distributional hypothesis to phrases i.e. word n-
grams.
For example, consider the phrase ?acquired? of
the form ?X acquired Y ?. Considering the con-
text of this phrase, we might find {Google, eBay,
Yahoo,...} in position X and {YouTube, Skype,
Overture,...} in position Y . Now consider another
phrase ?completed the acquisition of ?, again of the
form ?X completed the acquisition of Y ?. For this
phrase, we might find {Google, eBay, Hilton Hotel
corp.,...} in position X and {YouTube, Skype, Bally
Entertainment Corp.,...} in position Y . Since the
contexts of the two phrases are similar, our exten-
sion of the distributional hypothesis would assume
that ?acquired? and ?completed the acquisition of ?
have similar meanings.
3.2 Paraphrase Learning Model
Let p be a phrase (n-gram) of the form X p Y ,
where X and Y are the placeholders for words oc-
curring on either side of p. Our first task is to
find the set of phrases that are similar in meaning
to p. Let P = {p1, p2, p3, ..., pl} be the set of all
phrases of the form X pi Y where pi ? P . Let
Si,X be the set of words that occur in position X of
pi and Si,Y be the set of words that occur in posi-
tion Y of pi. Let Vi be the vector representing pi
such that Vi = Si,X ? Si,Y . Each word f ? Vi
has an associated score that measures the strength
of the association of the word f with phrase pi; as
do many others, we employ pointwise mutual infor-
mation (Cover and Thomas, 1991) to measure this
strength of association.
pmi(pi; f) = log P (pi,f)P (pi)P (f) (1)
The probabilities in equation (1) are calculated by
using the maximum likelihood estimate over our
corpus.
Once we have the vectors for each phrase pi ? P ,
we can find the paraphrases for each pi by finding its
nearest neighbors. We use cosine similarity, which
is a commonly used measure for finding similarity
between two vectors.
If we have two phrases pi ? P and pj ? P with
the corresponding vectors Vi and Vj constructed
as described above, the similarity between the two
phrases is calculated as:
sim(pi; pj) = Vi!Vj|Vi|?|Vj | (2)
Each word in Vi (and Vj) has with it an associated
flag which indicates weather the word came from
Si,X or Si,Y . Hence for each phrase pi of the form
X pi Y , we have a corresponding phrase ?pi that
has the form Y pi X. This is important to find cer-
tain kinds of paraphrases. The following example
will illustrate. Consider the sentences:
Google acquired YouTube. (3)
YouTube was bought by Google. (4)
From sentence (3), we obtain two phrases:
1. pi = acquired which has the form ?X acquired Y ?
where ?X = Google? and ?Y = YouTube?
2. ?pi = ?acquired which has the form ?Y acquired
X? where ?X = YouTube? and ?Y = Google?
Similarly, from sentence (4) we obtain two phrases:
1. pj = was bought by which has the form ?X was
bought by Y ? where ?X = YouTube? and ?Y =
Google?
2. ?pj = ?was bought by which has the form ?Y
was bought by X? where ?X = Google? and ?Y
= YouTube?
The switching of X and Y positions in (3) and (4)
ensures that ?acquired? and ??was bought by? are
found to be paraphrases by the algorithm.
3.3 Locality Sensitive Hashing
As described in Section 3.2, we find paraphrases of
a phrase pi by finding its nearest neighbors based
on cosine similarity between the feature vector of
pi and other phrases. To do this for all the phrases
in the corpus, we?ll have to compute the similarity
between all vector pairs. If n is the number of vec-
tors and d is the dimensionality of the vector space,
finding cosine similarity between each pair of vec-
tors has time complexity O(n2d). This computation
is infeasible for our corpus, since both n and d are
large.
676
To solve this problem, we make use of Local-
ity Sensitive Hashing (LSH). The basic idea behind
LSH is that a LSH function creates a fingerprint
for each vector such that if two vectors are simi-
lar, they are likely to have similar fingerprints. The
LSH function we use here was proposed by Charikar
(2002). It represents a d dimensional vector by a
stream of b bits (b & d) and has the property of pre-
serving the cosine similarity between vectors, which
is exactly what we want. Ravichandran et al (2005)
have shown that by using the LSH nearest neighbors
calculation can be done in O(nd) time.1.
4 Learning Surface Patterns
Let r be a target relation. Our task is to find a set of
surface patterns S = {s1, s2, ..., sn} that express the
target relation. For example, consider the relation r
= ?acquisition?. We want to find the set of patterns
S that express this relation:
S = {?ACQUIRER? acquired ?ACQUIREE?,
?ACQUIRER? bought ?ACQUIREE?, ?ACQUIREE?
was bought by ?ACQUIRER?,...}.
The remainder of the section describes our model
for learning surface patterns for target relations.
4.1 Model Assumption
Paraphrases express the same meaning using differ-
ent surface forms. So if one knew a pattern that ex-
presses a target relation, one could build more pat-
terns for that relation by finding paraphrases for the
surface phrase(s) in that pattern. This is the basic
assumption of our model.
For example, consider the seed pattern
??ACQUIRER? acquired ?ACQUIREE?? for
the target relation ?acquisition?. The surface phrase
in the seed pattern is ?acquired?. Our model then
assumes that we can obtain more surface patterns
for ?acquisition? by replacing ?acquired? in the
seed pattern with its paraphrases i.e. {bought, ?was
bought by2,...}. The resulting surface patterns are:
1The details of the algorithm are omitted, but interested
readers are encouraged to read Charikar (2002) and Ravichan-
dran et al (2005)
2The ??? in ??was bought by? indicates that the
?ACQUIRER? and ?ACQUIREE? arguments of the input
phrase ?acquired? need to be switched for the phrase ?was
bought by?.
{?ACQUIRER? bought ?ACQUIREE?, ?ACQUIREE?
was bought by ?ACQUIRER?,...}
4.2 Surface Pattern Model
Let r be a target relation. Let SEED = {seed1,
seed2,..., seedn} be the set of seed patterns that ex-
press the target relation. For each seedi ? SEED,
we obtain the corresponding set of new patterns
PATi in two steps:
1. We find the surface phrase, pi, using a seed
and find the corresponding set of paraphrases,
Pi = {pi,1, pi,2, ..., pi,m}. Each paraphrase,
pi,j ? Pi, has with it an associated score which
is similarity between pi and pi,j .
2. In seed pattern, seedi, we replace the sur-
face phrase, pi, with its paraphrases and
obtain the set of new patterns PATi =
{pati,1, pati,2, ..., pati,m}. Each pattern has
with it an associated score, which is the same as
the score of the paraphrase from which it was
obtained3 . The patterns are ranked in the de-
creasing order of their scores.
After we obtain PATi for each seedi ? SEED,
we obtain the complete set of patterns, PAT , for
the target relation r as the union of all the individual
pattern sets, i.e., PAT = PAT1 ? PAT2 ? ... ?
PATn.
5 Experimental Methodology
In this section, we describe experiments to validate
the main claims of the paper. We first describe para-
phrase acquisition, we then summarize our method
for learning surface patterns, and finally describe the
use of patterns for extracting relation instances.
5.1 Paraphrases
Finding surface variations in text requires a large
corpus. The corpus needs to be orders of magnitude
larger than that required for learning syntactic varia-
tions, since surface phrases are sparser than syntac-
tic phrases.
For our experiments, we used a corpus of about
150GB (25 billion words) obtained from Google
News4 . It consists of few years worth of news data.
3If a pattern is generated from more than one seed, we assign
it its average score.
4The corpus was cleaned to remove duplicate articles.
677
We POS tagged the corpus using Tnt tagger (Brants,
2000) and collected all phrases (n-grams) in the cor-
pus that contained at least one verb, and had a noun
or a noun-noun compound on either side. We re-
stricted the phrase length to at most five words.
We build a vector for each phrase as described in
Section 3. Tomitigate the problem of sparseness and
co-reference to a certain extent, whenever we have a
noun-noun compound in the X or Y positions, we
treat it as bag of words. For example, in the sen-
tence ?Google Inc. acquired YouTube?, ?Google?
and ?Inc.? will be treated as separate features in the
vector5.
Once we have constructed all the vectors, we find
the paraphrases for every phrase by finding its near-
est neighbors as described in Section 3. For our ex-
periments, we set the number of random bits in the
LSH function to 3000, and the similarity cut-off be-
tween vectors to 0.15. We eventually end up with
a resource containing over 2.5 million phrases such
that each phrase is connected to its paraphrases.
5.2 Surface Patterns
One claim of this paper is that we can find good sur-
face patterns for a target relation by starting with a
seed pattern. To verify this, we study two target re-
lations6:
1. Acquisition: We define this as the relation be-
tween two companies such that one company
acquired the other.
2. Birthplace: We define this as the relation be-
tween a person and his/her birthplace.
For ?acquisition? relation, we start with the sur-
face patterns containing only the words buy and ac-
quire:
1. ??ACQUIRER? bought ?ACQUIREE?? (and its
variants, i.e. buy, buys and buying)
2. ??ACQUIRER? acquired ?ACQUIREE?? (and its
variants, i.e. acquire, acquires and acquiring)
5This adds some noise in the vectors, but we found that this
results in better paraphrases.
6Since we have to do all the annotations for evaluations on
our own, we restricted our experiments to only two commonly
used relations.
This results in a total of eight seed patterns.
For ?birthplace? relation, we start with two seed
patterns:
1. ??PERSON? was born in ?LOCATION??
2. ??PERSON? was born at ?LOCATION??.
We find other surface patterns for each of these
relations by replacing the surface words in the seed
patterns by their paraphrases, as described in Sec-
tion 4.
5.3 Relation Extraction
The purpose of learning surface patterns for a rela-
tion is to extract instances of that relation. We use
the surface patterns obtained for the relations ?ac-
quisition? and ?birthplace? to extract instances of
these relations from the LDC North American News
Corpus. This helps us to extrinsically evaluate the
quality of the surface patterns.
6 Experimental Results
In this section, we present the results of the experi-
ments and analyze them.
6.1 Baselines
It is hard to construct a baseline for comparing the
quality of paraphrases, as there isn?t much work in
extracting surface level paraphrases using a mono-
lingual corpus. To overcome this, we show the effect
of reduction in corpus size on the quality of para-
phrases, and compare the results informally to the
other methods that produce syntactic paraphrases.
To compare the quality of the extraction patterns,
and relation instances, we use the method presented
by Ravichandran and Hovy (2002) as the baseline.
For each of the given relations, ?acquisition? and
?birthplace?, we use 10 seed instances, download
the top 1000 results from the Google search engine
for each instance, extract the sentences that contain
the instances, and learn the set of baseline patterns
for each relation. We then apply these patterns to
the test corpus and extract the corresponding base-
line instances.
6.2 Evaluation Criteria
Here we present the evaluation criteria we used to
evaluate the performance on the different tasks.
678
Paraphrases
We estimate the quality of paraphrases by annotating
a random sample as correct/incorrect and calculating
the accuracy. However, estimating the recall is diffi-
cult given that we do not have a complete set of para-
phrases for the input phrases. Following Szpektor et
al. (2004), instead of measuring recall, we calculate
the average number of correct paraphrases per input
phrase.
Surface Patterns
We can calculate the precision (P ) of learned pat-
terns for each relation by annotating the extracted
patterns as correct/incorrect. However calculating
the recall is a problem for the same reason as above.
But we can calculate the relative recall (RR) of the
system against the baseline and vice versa. The rela-
tive recallRRS|B of system S with respect to system
B can be calculated as:
RRS|B = CS?CBCB
where CS is the number of correct patterns found by
our system and CB is the number of correct patterns
found by the baseline. RRB|S can be found in a sim-
ilar way.
Relation Extraction
We estimate the precision (P ) of the extracted in-
stances by annotating a random sample of instances
as correct/incorrect. While calculating the true re-
call here is not possible, even calculating the true
relative recall of the system against the baseline is
not possible as we can annotate only a small sam-
ple. However, following Pantel et al (2004), we as-
sume that the recall of the baseline is 1 and estimate
the relative recall RRS|B of the system S with re-
spect to the baseline B using their respective pre-
cision scores PS and PB and number of instances
extracted by them |S| and |B| as:
RRS|B = PS?|S|PB?|B|
6.3 Gold Standard
In this section, we describe the creation of gold stan-
dard for the different tasks.
Paraphrases
We created the gold standard paraphrase test set by
randomly selecting 50 phrases and their correspond-
ing paraphrases from our collection of 2.5 million
phrases. For each test phrase, we asked two annota-
tors to annotate its paraphrases as correct/incorrect.
The annotators were instructed to look for strict
paraphrases i.e. equivalent phrases that can be sub-
stituted for each other.
To obtain the inter-annotator agreement, the two
annotators annotated the test set separately. The
kappa statistic (Siegal and Castellan Jr., 1988) was
? = 0.63. The interesting thing is that the anno-
tators got this respectable kappa score without any
prior training, which is hard to achieve when one
annotates for a similar task like textual entailment.
Surface Patterns
For the target relations, we asked two annotators to
annotate the patterns for each relation as either ?pre-
cise? or ?vague?. The annotators annotated the sys-
tem as well as the baseline outputs. We consider the
?precise? patterns as correct and the ?vague? as in-
correct. The intuition is that applying the vague pat-
terns for extracting target relation instances might
find some good instances, but will also find many
bad ones. For example, consider the following two
patterns for the ?acquisition? relation:
?ACQUIRER? acquired ?ACQUIREE? (5)
?ACQUIRER? and ?ACQUIREE? (6)
Example (5) is a precise pattern as it clearly identi-
fies the ?acquisition? relation while example (6) is
a vague pattern because it is too general and says
nothing about the ?acquisition? relation. The kappa
statistic between the two annotators for this task was
? = 0.72.
Relation Extraction
We randomly sampled 50 instances of the ?acquisi-
tion? and ?birthplace? relations from the system and
the baseline outputs. We asked two annotators to an-
notate the instances as correct/incorrect. The anno-
tators marked an instance as correct only if both the
entities and the relation between them were correct.
To make their task easier, the annotators were pro-
vided the context for each instance, and were free
to use any resources at their disposal (including a
web search engine), to verify the correctness of the
instances. The annotators found that the annotation
for this task was much easier than the previous two;
the few disagreements they had were due to ambigu-
ity of some of the instances. The kappa statistic for
this task was ? = 0.91.
679
Annotator Accuracy
Average # correct
paraphrases
Annotator 1 67.31% 4.2
Annotator 2 74.27% 4.28
Table 1: Quality of paraphrases
are being distributed to approved a revision to the
have been distributed to unanimously approved a new
are being handed out to approved an annual
were distributed to will consider adopting a
?are handing out approved a revised
will be distributed to all approved a new
Table 2: Example paraphrases
6.4 Result Summary
Table 1 shows the results of annotating the para-
phrases test set. We do not have a baseline
to compare against but we can analyze them in
light of numbers reported previously for syntac-
tic paraphrases. DIRT (Lin and Pantel, 2001) and
TEASE (Szpektor et al, 2004) report accuracies of
50.1% and 44.3% respectively compared to our av-
erage accuracy across two annotators of 70.79%.
The average number of paraphrases per phrase is
however 10.1 and 5.5 for DIRT and TEASE respec-
tively compared to our 4.2. One reason why this
number is lower is that our test set contains com-
pletely random phrases from our set (2.5 million
phrases): some of these phrases are rare and have
very few paraphrases. Table 2 shows some para-
phrases generated by our system for the phrases ?are
being distributed to? and ?approved a revision to
the?.
Table 3 shows the results on the quality of surface
patterns for the two relations. It can be observed
that our method outperforms the baseline by a wide
margin in both precision and relative recall. Table 4
shows some example patterns learned by our system.
Table 5 shows the results of the quality of ex-
tracted instances. Our system obtains very high pre-
cision scores but suffers in relative recall given that
the baseline with its very general patterns is likely
to find a huge number of instances (though a very
small portion of them are correct). Table 6 shows
some example instances we extracted.
acquisition birthplace
X agreed to buy Y X , who was born in Y
X , which acquired Y X , was born in Y
X completed its acquisition
of Y
X was raised in Y
X has acquired Y X was born in NNNNa in Y
X purchased Y X , born in Y
a
Each ?N? here is a placeholder for a number from 0 to 9.
Table 4: Example extraction templates
acquisition birthplace
1. Huntington Bancshares
Inc. agreed to acquire Re-
liance Bank
1. Cyril Andrew Ponnam-
peruma was born in Galle
2. Sony bought Columbia
Pictures
2. Cook was born in NNNN
in Devonshire
3. Hanson Industries buys
Kidde Inc.
3. Tansey was born in
Cincinnati
4. Casino America inc.
agreed to buy Grand Palais
4. Tsoi was born in NNNN in
Uzbekistan
5. Tidewater inc. acquired
Hornbeck Offshore Services
Inc.
5. Mrs. Totenberg was born
in San Francisco
Table 6: Example instances
6.5 Discussion and Error Analysis
We studied the effect of the decrease in size of the
available raw corpus on the quality of the acquired
paraphrases. We used about 10% of our original cor-
pus to learn the surface paraphrases and evaluated
them. The precision, and the average number of
correct paraphrases are calculated on the same test
set, as described in Section 6.2. The performance
drop on using 10% of the original corpus is signif-
icant (11.41% precision and on an average 1 cor-
rect paraphrase per phrase), which shows that we in-
deed need a large amount of data to learn good qual-
ity surface paraphrases. One reason for this drop
is also that when we use only 10% of the original
data, for some of the phrases from the test set, we do
not find any paraphrases (thus resulting in 0% accu-
racy for them). This is not unexpected, as the larger
resource would have a much larger recall, which
again points at the advantage of using a large data
set. Another reason for this performance drop could
be the parameter settings: We found that the qual-
ity of learned paraphrases depended greatly on the
various cut-offs used. While we adjusted our model
680
Relation Method # Patterns
Annotator 1 Annotator 2
P RR P RR
Acquisition
Baseline 160 55% 13.02% 60% 11.16%
Paraphrase Method 231 83.11% 28.40% 93.07% 25%
Birthplace
Baseline 16 31.35% 15.38% 31.25% 15.38%
Paraphrase Method 16 81.25% 40% 81.25% 40%
Table 3: Quality of Extraction Patterns
Relation Method # Patterns
Annotator 1 Annotator 2
P RR P RR
Acquisition
Baseline 1, 261, 986 6% 100% 2% 100%
Paraphrase Method 3875 88% 4.5% 82% 12.59%
Birthplace
Baseline 979, 607 4% 100% 2% 100%
Paraphrase Method 1811 98% 4.53% 98% 9.06%
Table 5: Quality of instances
parameters for working with smaller sized data, it is
conceivable that we did not find the ideal setting for
them. So we consider these numbers to be a lower
bound. But even then, these numbers clearly indi-
cate the advantage of using more data.
We also manually inspected our paraphrases. We
found that the problem of ?antonyms? was some-
what less pronounced due to our use of a large cor-
pus, but they still were the major source of error.
For example, our system finds the phrase ?sell? as
a paraphrase for ?buy?. We need to deal with this
problem separately in the future (may be as a post-
processing step using a list of antonyms).
Moving to the task of relation extraction, we see
from table 5 that our system has a much lower rel-
ative recall compared to the baseline. This was ex-
pected as the baseline method learns some very gen-
eral patterns, which are likely to extract some good
instances, even though they result in a huge hit to
its precision. However, our system was able to ob-
tain this performance using very few seeds. So an
increase in the number of input seeds, is likely to in-
crease the relative recall of the resource. The ques-
tion however remains as to what good seeds might
be. It is clear that it is much harder to come up with
good seed patterns (that our system needs), than seed
instances (that the baseline needs). But there are
some obvious ways to overcome this problem. One
way is to bootstrap. We can look at the paraphrases
of the seed patterns and use them to obtain more pat-
terns. Our initial experiments with this method using
handpicked seeds showed good promise. However,
we need to investigate automating this approach.
Another method is to use the good patterns from the
baseline system and use them as seeds for our sys-
tem. We plan to investigate this approach as well.
One reason, why we have seen good preliminary re-
sults using these approaches (for improving recall),
we believe, is that the precision of the paraphrases is
good. So either a seed doesn?t produce any new pat-
terns or it produces good patterns, thus keeping the
precision of the system high while increasing rela-
tive recall.
7 Conclusion
Paraphrases are an important technique to handle
variations in language. Given their utility in many
NLP tasks, it is desirable that we come up with
methods that produce good quality paraphrases. We
believe that the paraphrase acquisition method pre-
sented here is a step towards this very goal. We have
shown that high precision surface paraphrases can be
obtained by using distributional similarity on a large
corpus. We made use of some recent advances in
theoretical computer science to make this task scal-
able. We have also shown that these paraphrases
can be used to obtain high precision extraction pat-
terns for information extraction. While we believe
that more work needs to be done to improve the sys-
tem recall (some of which we are investigating), this
seems to be a good first step towards developing a
minimally supervised, easy to implement, and scal-
able relation extraction system.
681
References
P. G. Anick and S. Tipirneni. 1999. The paraphrase
search assistant: terminological feedback for iterative
information seeking. In ACM SIGIR, pages 153?159.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Association for
Computational Linguistics, pages 597?604.
R. Barzilay and L. Lee. 2003. Learning to paraphrase: an
unsupervised approach using multiple-sequence align-
ment. In In Proceedings North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 16?23.
R. Barzilay and K. R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In In Proceedings of
Association for Computational Linguistics, pages 50?
57.
R. Barzilay, K. R. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. InAssociation for Computational Lin-
guistics, pages 550?557.
M. Berland and E. Charniak. 1999. Finding parts in very
large corpora. In In Proceedings of Association for
Computational Linguistics, pages 57?64.
T. Brants. 2000. Tnt ? a statistical part-of-speech tag-
ger. In In Proceedings of the Applied NLP Conference
(ANLP).
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 17?24.
M. S. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In In Proceedings of the
thiry-fourth annual ACM symposium on Theory of
computing, pages 380?388.
T.M. Cover and J.A. Thomas. 1991. Elements of Infor-
mation Theory. John Wiley & Sons.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: ex-
ploiting massively parallel news sources. In In Pro-
ceedings of the conference on Computational Linguis-
tics (COLING), pages 350?357.
Z. Harris. 1954. Distributional structure. Word, pages
10(23):146?162.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the confer-
ence on Computational linguistics, pages 539?545.
D. Lin and P. Pantel. 2001. Dirt: Discovery of infer-
ence rules from text. In ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 323?328.
P. Pantel, D. Ravichandran, and E.H. Hovy. 2004. To-
wards terascale knowledge acquisition. In In Proceed-
ings of the conference on Computational Linguistics
(COLING), pages 771?778.
D. Ravichandran and E.H. Hovy. 2002. Learning sur-
face text for a question answering system. In Associ-
ation for Computational Linguistics (ACL), Philadel-
phia, PA.
D. Ravichandran, P. Pantel, and E.H. Hovy. 2005. Ran-
domized algorithms and nlp: using locality sensitive
hash function for high speed noun clustering. In In
Proceedings of Association for Computational Lin-
guistics, pages 622?629.
L. Romano, M. Kouylekov, I. Szpektor, I. Dagan, and
A. Lavelli. 2006. Investigating a generic paraphrase-
based approach for relation extraction. In In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics (EACL).
B. Rosenfeld and R. Feldman. 2006. Ures: an unsuper-
vised web relation extraction system. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, pages 667?674.
S. Sekine. 2006. On-demand information extraction. In
In Proceedings of COLING/ACL, pages 731?738.
S. Siegal and N.J. Castellan Jr. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment relations.
In In Proceedings of Empirical Methods in Natural
Language Processing, pages 41?48.
L. Zhou, C.Y. Lin, D. Munteanu, and E.H. Hovy. 2006.
Paraeval: using paraphrases to evaluate summaries au-
tomatically. In In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 447?454.
682
