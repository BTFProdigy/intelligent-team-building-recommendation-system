Proceedings of ACL-08: HLT, pages 550?558,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Robustness and Generalization of Role Sets: PropBank vs. VerbNet
Ben?at Zapirain and Eneko Agirre
IXA NLP Group
University of the Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical University of Catalonia
lluism@lsi.upc.edu
Abstract
This paper presents an empirical study on the
robustness and generalization of two alterna-
tive role sets for semantic role labeling: Prop-
Bank numbered roles and VerbNet thematic
roles. By testing a state?of?the?art SRL sys-
tem with the two alternative role annotations,
we show that the PropBank role set is more
robust to the lack of verb?specific semantic
information and generalizes better to infre-
quent and unseen predicates. Keeping in mind
that thematic roles are better for application
needs, we also tested the best way to generate
VerbNet annotation. We conclude that tagging
first PropBank roles and mapping into Verb-
Net roles is as effective as training and tagging
directly on VerbNet, and more robust for do-
main shifts.
1 Introduction
Semantic Role Labeling is the problem of analyzing
clause predicates in open text by identifying argu-
ments and tagging them with semantic labels indi-
cating the role they play with respect to the verb.
Such sentence?level semantic analysis allows to de-
termine ?who? did ?what? to ?whom?, ?when? and
?where?, and, thus, characterize the participants and
properties of the events established by the predi-
cates. This kind of semantic analysis is very inter-
esting for a broad spectrum of NLP applications (in-
formation extraction, summarization, question an-
swering, machine translation, etc.), since it opens
the door to exploit the semantic relations among lin-
guistic constituents.
The properties of the semantically annotated cor-
pora available have conditioned the type of research
and systems that have been developed so far. Prop-
Bank (Palmer et al, 2005) is the most widely used
corpus for training SRL systems, probably because
it contains running text from the Penn Treebank cor-
pus with annotations on all verbal predicates. Also,
a few evaluation exercises on SRL have been con-
ducted on this corpus in the CoNLL-2004 and 2005
conferences. However, a serious criticisms to the
PropBank corpus refers to the role set it uses, which
consists of a set of numbered core arguments, whose
semantic translation is verb-dependent. While Arg0
and Arg1 are intended to indicate the general roles
of Agent and Theme, other argument numbers do
not generalize across verbs and do not correspond
to general semantic roles. This fact might compro-
mise generalization and portability of SRL systems,
especially when the training corpus is small.
More recently, a mapping from PropBank num-
bered arguments into VerbNet thematic roles has
been developed and a version of the PropBank cor-
pus with thematic roles has been released (Loper et
al., 2007). Thematic roles represent a compact set of
verb-independent general roles widely used in lin-
guistic theory (e.g., Agent, Theme, Patient, Recipi-
ent, Cause, etc.). We foresee two advantages of us-
ing such thematic roles. On the one hand, statisti-
cal SRL systems trained from them could generalize
better and, therefore, be more robust and portable,
as suggested in (Yi et al, 2007). On the other hand,
roles in a paradigm like VerbNet would allow for in-
ferences over the assigned roles, which is only pos-
sible in a more limited way with PropBank.
In a previous paper (Zapirain et al, 2008), we pre-
sented a first comparison between the two previous
role sets on the SemEval-2007 Task 17 corpus (Prad-
han et al, 2007). The SemEval-2007 corpus only
550
comprised examples about 50 different verbs. The
results of that paper were, thus, considered prelim-
inary, as they could depend on the small amount of
data (both in training data and number of verbs) or
the specific set of verbs being used. Now, we ex-
tend those experiments to the entire PropBank cor-
pus, and we include two extra experiments on do-
main shifts (using the Brown corpus as test set) and
on grouping VerbNet labels. More concretely, this
paper explores two aspects of the problem. First,
having in mind the claim that general thematic roles
should be more robust to changing domains and
unseen predicates, we study the performance of a
state-of-the-art SRL system trained on either codi-
fication of roles and some specific settings, i.e. in-
cluding/excluding verb-specific information, label-
ing unseen verb predicates, or domain shifts. Sec-
ond, assuming that application scenarios would pre-
fer dealing with general thematic role labels, we ex-
plore the best way to label a text with thematic roles,
namely, by training directly on VerbNet roles or by
using the PropBank SRL system and perform a pos-
terior mapping into thematic roles.
The results confirm our preliminary findings (Za-
pirain et al, 2008). We observe that the PropBank
roles are more robust in all tested experimental con-
ditions, i.e., the performance decrease is more se-
vere for VerbNet. Besides, tagging first PropBank
roles and then mapping into VerbNet roles is as ef-
fective as training and tagging directly on VerbNet,
and more robust for domain shifts.
The rest of the paper is organized as follows: Sec-
tion 2 contains some background on PropBank and
VerbNet role sets. Section 3 presents the experimen-
tal setting and the base SRL system used for the role
set comparisons. In Section 4 the main compara-
tive experiments on robustness are described. Sec-
tion 5 is devoted to analyze the posterior mapping of
PropBank outputs into VerbNet thematic roles, and
includes results on domain?shift experiments using
Brown as test set. Finally, Sections 6 and 7 contain
a discussion of the results.
2 Corpora and Semantic Role Sets
The PropBank corpus is the result of adding a se-
mantic layer to the syntactic structures of Penn Tree-
bank II (Palmer et al, 2005). Specifically, it pro-
vides information about predicate-argument struc-
tures to all verbal predicates of the Wall Street Jour-
nal section of the treebank. The role set is theory?
neutral and consists of a set of numbered core ar-
guments (Arg0, Arg1, ..., Arg5). Each verb has a
frameset listing its allowed role labels and mapping
each numbered role to an English-language descrip-
tion of its semantics.
Different senses for a polysemous verb have dif-
ferent framesets, but the argument labels are seman-
tically consistent in all syntactic alternations of the
same verb?sense. For instance in ?Kevin broke [the
window]Arg1 ? and in ?[The door]Arg1 broke into a
million pieces?, for the verb broke.01, both Arg1 ar-
guments have the same semantic meaning, that is
?broken entity?. Nevertheless, argument labels are
not necessarily consistent across different verbs (or
verb senses). For instance, the same Arg2 label is
used to identify the Destination argument of a propo-
sition governed by the verb send and the Beneficiary
argument of the verb compose. This fact might com-
promise generalization of systems trained on Prop-
Bank, which might be focusing too much on verb?
specific knowledge. It is worth noting that the two
most frequent arguments, Arg0 and Arg1, are in-
tended to indicate the general roles of Agent and
Theme and are usually consistent across different
verbs. However, this correspondence is not total.
According to the study by (Yi et al, 2007), Arg0
corresponds to Agent 85.4% of the time, but also
to Experiencer (7.2%), Theme (2.1%), and Cause
(1.9%). Similarly, Arg1 corresponds to Theme in
47.0% of the occurrences but also to Topic (23.0%),
Patient (10.8%), and Product (2.9%), among others.
Contrary to core arguments, adjuncts (Temporal and
Location markers, etc.) are annotated with a closed
set of general and verb-independent labels.
VerbNet (Kipper et al, 2000) is a computational
verb lexicon in which verbs are organized hier-
archically into classes depending on their syntac-
tic/semantic linking behavior. The classes are based
on Levin?s verb classes (Levin, 1993) and each con-
tains a list of member verbs and a correspondence
between the shared syntactic frames and the se-
mantic information, such as thematic roles and se-
lectional constraints. There are 23 thematic roles
(Agent, Patient, Theme, Experiencer, Source, Ben-
eficiary, Instrument, etc.) which, unlike the Prop-
551
Bank numbered arguments, are considered as gen-
eral verb-independent roles.
This level of abstraction makes them, in princi-
ple, better suited (compared to PropBank numbered
arguments) for being directly exploited by general
NLP applications. But, VerbNet by itself is not an
appropriate resource to train SRL systems. As op-
posed to PropBank, the number of tagged examples
is far more limited in VerbNet. Fortunately, in the
last years a twofold effort has been made in order
to generate a large corpus fully annotated with the-
matic roles. Firstly, the SemLink1 resource (Loper
et al, 2007) established a mapping between Prop-
Bank framesets and VerbNet thematic roles. Sec-
ondly, the SemLink mapping was applied to a repre-
sentative portion of the PropBank corpus and man-
ually disambiguated (Loper et al, 2007). The re-
sulting corpus is currently available for the research
community and makes possible comparative studies
between role sets.
3 Experimental Setting
3.1 Datasets
The data used in this work is the benchmark corpus
provided by the SRL shared task of CoNLL-2005
(Carreras and Ma`rquez, 2005). The dataset, of over
1 million tokens, comprises PropBank sections 02?
21 for training, and sections 24 and 23 for develop-
ment and test, respectively. From the input informa-
tion, we used part of speech tags and full parse trees
(generated using Charniak?s parser) and discarded
named entities. Also, we used the publicly avail-
able SemLink mapping from PropBank into Verb-
Net roles (Loper et al, 2007) to generate a replicate
of the CoNLL-2005 corpus containing also the Verb-
Net annotation of roles.
Unfortunately, SemLink version 1.0 does not
cover all propositions and arguments in the Prop-
Bank corpus. In order to have an homogeneous cor-
pus and not to bias experimental evaluation, we de-
cided to discard all incomplete examples and keep
only those propositions that were 100% mapped into
VerbNet roles. The resulting corpus contains 56% of
the original propositions, that is, over 50,000 propo-
sitions in the training set. This subcorpus is much
larger than the SemEval-2007 Task 17 dataset used
1http://verbs.colorado.edu/semlink/
in our previous experimental work (Zapirain et al,
2008). The difference is especially noticeable in
the diversity of predicates represented. In this case,
there are 1,709 different verbs (1,505 lemmas) com-
pared to the 50 verbs of the SemEval corpus. We
believe that the size and richness of this corpus is
enough to test and extract reliable conclusions on
the robustness and generalization across verbs of the
role sets under study.
In order to study the behavior of both role sets
in out?of?domain data, we made use of the Prop-
Banked Brown corpus (Marcus et al, 1994) for test-
ing, as it is also mapped into VerbNet thematic roles
in the SemLink resource. Again, we discarded those
propositions that were not entirely mapped into the-
matic roles (45%).
3.2 SRL System
Our basic Semantic Role Labeling system represents
the tagging problem as a Maximum Entropy Markov
Model (MEMM). The system uses full syntactic
information to select a sequence of constituents
from the input text and tags these tokens with Be-
gin/Inside/Outside (BIO) labels, using state-of-the-
art classifiers and features. The system achieves very
good performance in the CoNLL-2005 shared task
dataset and in the SRL subtask of the SemEval-2007
English lexical sample task (Zapirain et al, 2007).
Check this paper for a complete description of the
system.
When searching for the most likely state se-
quence, the following constraints are observed2:
1. No duplicate argument classes for Arg0?Arg5
PropBank (or VerbNet) roles are allowed.
2. If there is a R-X argument (reference), then
there has to be a X argument before (referent).
3. If there is a C-X argument (continuation), then
there has to be a X argument before.
4. Before a I-X token, there has to be a B-X or I-X
token.
5. Given a predicate, only the arguments de-
scribed in its PropBank (or VerbNet) lexical en-
try (i.e., the verbal frameset) are allowed.
2Note that some of the constraints are dependent of the role
set used, i.e., PropBank or VerbNet
552
Regarding the last constraint, the lexical entries
of the verbs were constructed from the training data
itself. For instance, the verb build appears with
four different PropBank core roles (Arg0?3) and five
VerbNet roles (Product, Material, Asset, Attribute,
Theme), which are the only ones allowed for that
verb at test time. Note that in the cases where the
verb sense was known we could constraint the pos-
sible arguments to those that appear in the lexical en-
try of that sense, as opposed of using the arguments
that appear in all senses.
4 On the Generalization of Role Sets
We first seek a basic reference of the comparative
performance of the classifier on each role set. We
devised two settings based on our dataset. In the
first setting (?SemEval?) we use all the available in-
formation provided in the corpus, including the verb
senses in PropBank and VerbNet. This information
was available both in the training and test, and was
thus used as an additional feature by the classifier
and to constrain further the possible arguments when
searching for the most probable Viterbi path. We call
this setting ?SemEval? because the SemEval-2007
competition (Pradhan et al, 2007) was performed
using this configuration.
Being aware that, in a real scenario, the sense in-
formation will not be available, we devised the sec-
ond setting (?CoNLL?), where the hand-annotated
verb sense information was discarded. This is the
setting used in the CoNLL 2005 shared task (Car-
reras and Ma`rquez, 2005).
The results for the first setting are shown in the
?SemEval setting? rows of Table 1. The correct,
excess, missed, precision, recall and F1 measures
are reported, as customary. The significance inter-
vals for F1 are also reported. They have been ob-
tained with bootstrap resampling (Noreen, 1989).
F1 scores outside of these intervals are assumed to
be significantly different from the related F1 score
(p < 0.05). The results for PropBank are slightly
better, which is reasonable, as the number of labels
that the classifier has to learn in the case of VerbNet
should make the task harder. In fact, given the small
difference, one could think that VerbNet labels, be-
ing more numerous, are easier to learn, perhaps be-
cause they are more consistent across verbs.
In the second setting (?CoNLL setting? row in
the same table) the PropBank classifier degrades
slightly, but the difference is not statistically signif-
icant. On the contrary, the drop of 1.6 points for
VerbNet is significant, and shows greater sensitivity
to the absence of the sense information for verbs.
One possible reason could be that the VerbNet clas-
sifier is more dependant on the argument filter (i.e.,
the 5th constraint in Section 3.2, which only allows
roles that occur in the verbal frameset) used in the
Viterbi search, and lacking the sense information
makes the filter less useful. In fact, we have attested
that the 5th constrain discard more than 60% of the
possible candidates for VerbNet, making the task of
the classifier easier.
In order to test this hypothesis, we run the CoNLL
setting with the 5th constraint disabled (that is, al-
lowing any argument). The results in the ?CoNLL
setting (no 5th)? rows of Table 1 show that the drop
for PropBank is negligible and not significant, while
the drop for VerbNet is more important, and statisti-
cally significant.
Another view of the data is obtained if we com-
pute the F1 scores for core arguments and adjuncts
separately (last two columns in Table 1). The per-
formance drop for PropBank in the first three rows
is equally distributed on both core arguments and ad-
juncts. On the contrary, the drop for VerbNet roles
is more acute in core arguments (3.7 points), while
adjuncts with the 5th constraint disabled get results
close to the SemEval setting. These results confirm
that the information in the verbal frameset is more
important in VerbNet than in PropBank, as only core
arguments are constrained in the verbal framesets.
The explanation could stem from the fact that cur-
rent SRL systems rely more on syntactic information
than pure semantic knowledge. While PropBank ar-
guments Arg0?5 are easier to distinguish on syntac-
tic grounds alone, it seems quite difficult to distin-
guish among roles like Theme and Topic unless we
have access to the specific verbal frameset. This cor-
responds nicely with the performance drop for Verb-
Net when there is less information about the verb in
the algorithm (i.e., sense or frameset).
We further analyzed the results by looking at each
of the individual core arguments and adjuncts. Ta-
ble 2 shows these results on the CoNLL setting. The
performance for the most frequent roles is similar
553
PropBank
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 6,022 1,378 1,722 81.38 77.76 79.53 ?0.9 82.25 72.48
CoNLL setting 5,977 1,424 1,767 80.76 77.18 78.93 ?0.9 81.64 71.90
CoNLL setting (no 5th) 5,972 1,434 1,772 80.64 77.12 78.84 ?0.9 81.49 71.50
No verbal features 5,557 1,828 2,187 75.25 71.76 73.46 ?1.0 74.87 70.11
Unseen verbs 267 89 106 75.00 71.58 73.25 ?4.0 76.21 64.92
VerbNet
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 5,927 1,409 1,817 80.79 76.54 78.61 ?0.9 81.28 71.83
CoNLL setting 5,816 1,548 1,928 78.98 75.10 76.99 ?0.9 79.44 70.20
CoNLL setting (no 5th) 5,746 1,669 1,998 77.49 74.20 75.81 ?0.9 77.60 71.67
No verbal features 4,679 2,724 3,065 63.20 60.42 61.78 ?0.9 59.19 69.95
Unseen verbs 207 136 166 60.35 55.50 57.82 ?4.3 55.04 63.41
Table 1: Basic results using PropBank (top) and VerbNet (bottom) role sets on different settings.
for both. Arg0 gets 88.49, while Agent and Expe-
riencer get 87.31 and 87.76 respectively. Arg2 gets
79.91, but there is more variation on Theme, Topic
and Patient (which get 75.46, 85.70 and 78.64 re-
spectively).
Finally, we grouped the results according to the
frequency of the verbs in the training data. Table 3
shows that both PropBank and VerbNet get decreas-
ing results for less frequent verbs. PropBank gets
better results in all frequency ranges, except for the
most frequent, which contains a single verb (say).
Overall, the results on this section point out at the
weaknesses of the VerbNet role set regarding robust-
ness and generalization. The next sections examine
further its behavior.
4.1 Generalization to Unseen Predicates
In principle, the PropBank core roles (Arg0?4) get
a different interpretation depending of the verb, that
is, the meaning of each of the roles is described sepa-
rately for each verb in the PropBank framesets. Still,
the annotation criteria used with PropBank tried to
make the two main roles (Arg0 and Arg1, which ac-
count for most of the occurrences) consistent across
verbs. On the contrary, in VerbNet al roles are com-
pletely independent of the verb, in the sense that the
interpretation of the role does not vary across verbs.
But, at the same time, each verbal entry lists the pos-
sible roles it accepts, and the combinations allowed.
This experiment tests the sensitivity of the two ap-
proaches when the SRL system encounters a verb
which does not occur in the training data. In prin-
ciple, we would expect the VerbNet semantic la-
bels, which are more independent across verbs, to be
more robust at tagging new predicates. It is worth
noting that this is a realistic scenario, even for the
verb-specific PropBank labels. Predicates which do
not occur in the training data, but do have a Prop-
Bank lexicon entry, could appear quite often in the
text to be analyzed.
For this experiment, we artificially created a test
set for unseen verbs. We chose 50 verbs at random,
and split them into 40 verbs for training and 10 for
testing (yielding 13,146 occurrences for training and
2,723 occurrences for testing; see Table 4).
The results obtained after training and testing the
classifier are shown in the last rows in Table 1. Note
that they are not directly comparable to the other re-
sults mentioned so far, as the train and test sets are
smaller. Figures indicate that the performance of the
PropBank argument classifier is considerably higher
than the VerbNet classifier, with a ?15 point gap.
This experiment shows that lacking any informa-
tion about verbal head, the classifier has a hard time
to distinguish among VerbNet roles. In order to con-
firm this, we performed the following experiment.
4.2 Sensitivity to Verb-dependent Features
In this experiment we want to test the sensitivity of
the role sets when the classifier does not have any in-
formation of the verb predicate. We removed from
the training and testing data all the features which
make any reference to the verb, including, among
others: the surface form, lemma and POS of the
verb, and all the combined features that include the
verb form (please, refer to (Zapirain et al, 2007) for
a complete description of the feature set).
The results are shown in the ?No verbal features?
554
CoNLL setting No verb features
PBank VNet PBank VNet
corr. F1 corr. F1 F1 F1
Overall 5977 78.93 5816 76.99 73.46 61.78
Arg0 1919 88.49 84.02
Arg1 2240 79.81 73.29
Arg2 303 65.44 48.58
Arg3 10 52.63 14.29
Actor1 44 85.44 0.00
Actor2 10 71.43 25.00
Agent 1603 87.31 77.21
Attribut. 25 71.43 50.79
Cause 51 62.20 5.61
Experien. 215 87.76 86.69
Location 31 64.58 25.00
Patient1 38 67.86 5.71
Patient 208 78.64 25.06
Patient2 21 67.74 43.33
Predicate 83 62.88 28.69
Product 44 61.97 2.44
Recipient 85 79.81 62.73
Source 29 60.42 30.95
Stimulus 39 63.93 13.70
Theme 1021 75.46 52.14
Theme1 20 57.14 4.44
Theme2 21 70.00 23.53
Topic 683 85.70 73.58
ADV 132 53.44 129 52.12 52.67 53.31
CAU 13 53.06 13 52.00 53.06 45.83
DIR 22 53.01 27 56.84 40.00 46.34
DIS 133 77.78 137 79.42 77.25 78.34
LOC 126 61.76 126 61.02 59.56 57.34
MNR 109 58.29 111 54.81 52.99 51.49
MOD 249 96.14 248 95.75 96.12 95.57
NEG 124 98.41 124 98.80 98.41 98.01
PNC 26 44.07 29 44.62 38.33 41.79
TMP 453 75.00 450 73.71 73.06 73.89
Table 2: Detailed results on the CoNLL setting. Refer-
ence arguments and verbs have been omitted for brevity,
as well as those with less than 10 occ. The last two
columns refer to the results on the CoNLL setting with
no verb features.
Freq. PBank VNet Freq. PBank VNet
0-50 74,21 71,11 500-900 77,97 75,77
50-100 74,79 71,83 > 900 91,83 92,23
100-500 77,16 75,41
Table 3: F1 results split according to the frequency of the
verb in the training data.
Train affect, announce, ask, attempt, avoid, believe, build, care,
cause, claim, complain, complete, contribute, describe,
disclose, enjoy, estimate, examine, exist, explain, express,
feel, fix, grant, hope, join, maintain, negotiate, occur,
prepare, promise, propose, purchase, recall, receive,
regard, remember, remove, replace, say
Test allow, approve, buy, find, improve, kill, produce, prove,
report, rush
Table 4: Verbs used in the unseen verb experiment
rows of Table 1. The performance drops more than
5 points in PropBank, but the drop for VerbNet is
dramatic, with more than 15 points.
A closer look at the detailed role-by-role perfor-
mances can be done if we compare the F1 rows in the
CoNLL setting and in the ?no verb features? setting
in Table 2. Those results show that both Arg0 and
Arg1 are quite robust to the lack of target verb in-
formation, while Arg2 and Arg3 get more affected.
Given the relatively low number of Arg2 and Arg3
arguments, their performance drop does not affect
so much the overall PropBank performance. In the
case of VerbNet, the picture is very different. Focus-
ing on the most frequent roles first, while the perfor-
mance drop for Experiencer, Agent and Topic is of
1, 10 and 12 points respectively, the other roles get
very heavy losses (e.g. Theme and Patient drop 23
and 50 points), and the rest of roles are barely found.
It is worth noting that the adjunct labels get very
similar performances in both PropBank and Verb-
Net cases. In fact, Table 1 in the last two rows shows
very clearly that the performance drop is caused by
the core arguments.
The better robustness of the PropBank roles can
be explained by the fact that, when creating Prop-
Bank, the human PropBank annotators tried to be
consistent when tagging Arg0 and Arg1 across
verbs. We also think that both Arg0 and Arg1 can
be detected quite well relying on unlexicalized syn-
tactic features only, that is, not knowing which are
the verbal and nominal heads. On the other hand,
distinguishing between Arg2?4 is more dependant
on the subcategorization frame of the verb, and thus
more sensitive to the lack of verbal information.
In the case of VerbNet, the more fine-grained dis-
tinction among roles seems to depend more on the
meaning of the predicate. For instance, distinguish-
ing between Agent?Experiencer, or Theme?Topic?
Patient. The lack of the verbal head makes it much
more difficult to distinguish among those roles. The
same phenomena can be observed among the roles
not typically realized as Subject or Object such as
Recipient, Source, Product, or Stimulus.
5 Mapping into VerbNet Thematic Roles
As mentioned in the introduction, the interpretation
of PropBank roles depends on the verb, and that
555
Test on WSJ all core adj.
PropBank to VerbNet (hand) 79.17 ?0.9 81.77 72.50
VerbNet (SemEval setting) 78.61 ?0.9 81.28 71.84
PropBank to VerbNet (MF) 77.15 ?0.9 79.09 71.90
VerbNet (CoNLL setting) 76.99 ?0.9 79.44 70.88
Test on Brown
PropBank to VerbNet (MF) 64.79 ?1.0 68.93 55.94
VerbNet (CoNLL setting) 62.87 ?1.0 67.07 54.69
Table 5: Results on VerbNet roles using two different
strategies. Topmost 4 rows for the usual test set (WSJ),
and the 2 rows below for the Brown test set.
makes them less suitable for NLP applications. On
the other hand, VerbNet roles have a direct inter-
pretation. In this section, we test the performance
of two different approaches to tag input sentences
with VerbNet roles: (1) train on corpora tagged with
VerbNet, and tag the input directly; (2) train on cor-
pora tagged with PropBank, tag the input with Prop-
Bank roles, and use a PropBank to VerbNet mapping
to output VerbNet roles.
The results for the first approach are already avail-
able (cf. Table 1). For the second approach, we
just need to map PropBank roles into VerbNet roles
using SemLink (Loper et al, 2007). We devised
two experiments. In the first one we use the hand-
annotated verb class in the test set. For each predi-
cate we translate PropBank roles into VerbNet roles
making use of the SemLink mapping information
corresponding to that verb lemma and its verbal
class.
For instance, consider an occurrence of allow in a
test sentence. If the occurrence has been manually
annotated with the VerbNet class 29.5, we can use
the following entry in SemLink to add the VerbNet
role Predicate to the argument labeled with Arg1,
and Agent to the Arg0 argument.
<predicate lemma="allow">
<argmap pb-roleset="allow.01" vn-class="29.5">
<role pb-arg="1" vn-theta="Predicate" />
<role pb-arg="0" vn-theta="Agent" />
</argmap>
</predicate>
The results obtained using the hand-annotated
VerbNet classes (and the SemEval setting for Prop-
Bank), are shown in the first row of Table 5. If we
compare these results to those obtained by VerbNet
in the SemEval setting (second row of Table 5), they
are 0.5 points better, but the difference is not statis-
tically significant.
experiment corr. F1
Grouped (CoNLL Setting) 5,951 78.11?0.9
PropBank to VerbNet to Grouped 5,970 78.21?0.9
Table 6: Results for VerbNet grouping experiments.
In a second experiment, we discarded the sense
annotations from the dataset, and tried to predict the
VerbNet class of the target verb using the most fre-
quent class for the verb in the training data. Sur-
prisingly, the accuracy of choosing the most fre-
quent class is 97%. In the case of allow the most
frequent class is 29.5, so we would use the same
SemLink entry as above. The third row in Table 5
shows the results using the most frequent VerbNet
class (and the CoNLL setting for PropBank). The
performance drop compared to the use of the hand-
annotated VerbNet class is of 2 points and statisti-
cally significant, and 0.2 points above the results ob-
tained using VerbNet directly on the same conditions
(fourth row of the same Table).
The last two rows in table 5 show the results when
testing on the the Brown Corpus. In this case, the
difference is larger, 1.9 points, and statistically sig-
nificant in favor of the mapping approach. These
results show that VerbNet roles are less robust to
domain shifts. The performance drop when mov-
ing to an out?of?domain corpus is consistent with
previously published results (Carreras and Ma`rquez,
2005).
5.1 Grouping experiments
VerbNet roles are more numerous than PropBank
roles, and that, in itself, could cause a drop in per-
formance. Motivated by the results in (Yi et al,
2007), we grouped the 23 VerbNet roles in 7 coarser
role groups. Note that their groupings are focused
on the roles which map to PropBank Arg2. In our
case we are interested in a more general grouping
which covers all VerbNet roles, so we added two
additional groups (Agent-Experiencer and Theme-
Topic-Patient). We re-tagged the roles in the datasets
with those groups, and then trained and tested our
SRL system on those grouped labels. The results
are shown in the first row of Table 6. In order to
judge if our groupings are easier to learn, we can
see that he performance gain with respect to the un-
grouped roles (fourth row of Table 5) is small (76.99
556
vs. 78.11) but significant. But if we compare them
to the results of the PropBank to VerbNet mapping,
where we simply substitute the fine-grained roles by
their corresponding groups, we see that they still lag
behind (second row in Table 6).
Although one could argue that better motivated
groupings could be proposed, these results indicate
that the larger number of VerbNet roles does not ex-
plain in itself the performance difference when com-
pared to PropBank.
6 Related Work
As far as we know, there are only two other works
performing comparisons of alternative role sets on
a common test data. Gildea and Jurafsky (2002)
mapped FrameNet frame elements into a set of ab-
stract thematic roles (i.e., more general roles such as
Agent, Theme, Location), and concluded that their
system could use these thematic roles without degra-
dation in performance.
(Yi et al, 2007) is a closely related work. They
also compare PropBank and VerbNet role sets, but
they focus on the performance of Arg2. They show
that splitting Arg2 instances into subgroups based on
VerbNet thematic roles improves the performance of
the PropBank-based classifier. Their claim is that
since VerbNet uses argument labels that are more
consistent across verbs, they would provide more
consistent training instances which would general-
ize better, especially to new verbs and genres. In fact
they get small improvements in PropBank (WSJ)
and a large improvement when testing on Brown.
An important remark is that Yi et al use a com-
bination of grouped VerbNet roles (for Arg2) and
PropBank roles (for the rest of arguments). In con-
trast, our study compares both role sets as they stand,
without modifications or mixing. Another difference
is that they compare the systems based on the Prop-
Bank roles ?by mapping the output VerbNet labels
back to PropBank Arg2? while in our case we de-
cided to do just the contrary (i.e., mapping PropBank
output into VerbNet labels and compare there). As
we already said, we think that VerbNet?based labels
can be more useful for NLP applications, so our tar-
get is to have a SRL system that provides VerbNet
annotations. While not in direct contradiction, both
studies show different angles of the complex relation
between the two role sets.
7 Conclusion and Future work
In this paper we have presented a study of the per-
formance of a state-of-the-art SRL system trained
on two alternative codifications of roles (PropBank
and VerbNet) and some particular settings, e.g., in-
cluding/excluding verb?specific information in fea-
tures, labeling of infrequent and unseen verb pred-
icates, and domain shifts. We observed that Prop-
Bank labeling is more robust in all previous experi-
mental conditions, showing less performance drops
than VerbNet labels.
Assuming that application-based scenarios would
prefer dealing with general thematic role labels, we
explore the best way to label a text with VerbNet
thematic roles, namely, by training directly on Verb-
Net roles or by using the PropBank SRL system
and performing a posterior mapping into thematic
roles. While results are similar and not statistically
significant in the WSJ test set, when testing on the
Brown out?of?domain test set the difference in favor
of PropBank plus mapping step is statistically signif-
icant. We also tried to map the fine-grained VerbNet
roles into coarser roles, but it did not yield better re-
sults than the mapping from PropBank roles. As a
side-product, we show that a simple most frequent
sense disambiguation strategy for verbs is sufficient
to provide excellent results in the PropBank to Verb-
Net mapping.
Regarding future work, we would like to explore
ways to improve the performance on VerbNet roles,
perhaps using selectional preferences. We also want
to work on the adaptation to new domains of both
roles sets.
Acknowledgements
We are grateful to Martha Palmer and Edward Loper
for kindly providing us with the SemLink map-
pings. This work has been partially funded by
the Basque Government (IT-397-07) and by the
Ministry of Education (KNOW TIN2006-15049,
OpenMT TIN2006-15307-C03-02). Ben?at is sup-
ported by a PhD grant from the University of the
Basque Country.
557
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Ido Dagan and Daniel Gildea, editors, Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), pages 152?
164, Ann Arbor, Michigan, USA, June. Association
for Computational Linguistics.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class based construction of a verb lexicon. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence (AAAI-2000), Austin, TX, July.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press, Chicago.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: annotating predicate argument structure. In
HLT ?94: Proceedings of the workshop on Human
Language Technology, pages 114?119, Morristown,
NJ, USA. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
105.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 87?92, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Szu-Ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In Pro-
ceedings of the Human Language Technology Con-
ferences/North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT/NAACL-2007).
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2007.
Sequential SRL Using Selectional Preferences. An
Approach with Maximum Entropy Markov Models. In
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 354?357.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2008.
A Preliminary Study on the Robustness and General-
ization of Role Sets for Semantic Role Labeling. In
Proceedings of the 9th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing-2008), pages 219?230, Haifa, Israel,
February.
558
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 73?76,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Generalizing over Lexical Features:
Selectional Preferences for Semantic Role Classification
Be
?
nat Zapirain, Eneko Agirre
Ixa Taldea
University of the Basque Country
Donostia, Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s M
`
arquez
TALP Research Center
Technical University of Catalonia
Barcelona, Catalonia
lluism@lsi.upc.edu
Abstract
This paper explores methods to allevi-
ate the effect of lexical sparseness in the
classification of verbal arguments. We
show how automatically generated selec-
tional preferences are able to generalize
and perform better than lexical features in
a large dataset for semantic role classifi-
cation. The best results are obtained with
a novel second-order distributional simi-
larity measure, and the positive effect is
specially relevant for out-of-domain data.
Our findings suggest that selectional pref-
erences have potential for improving a full
system for Semantic Role Labeling.
1 Introduction
Semantic Role Labeling (SRL) systems usually
approach the problem as a sequence of two sub-
tasks: argument identification and classification.
While the former is mostly a syntactic task, the
latter requires semantic knowledge to be taken
into account. Current systems capture semantics
through lexicalized features on the predicate and
the head word of the argument to be classified.
Since lexical features tend to be sparse (especially
when the training corpus is small) SRL systems
are prone to overfit the training data and general-
ize poorly to new corpora.
This work explores the usefulness of selectional
preferences to alleviate the lexical dependence of
SRL systems. Selectional preferences introduce
semantic generalizations on the type of arguments
preferred by the predicates. Therefore, they are
expected to improve generalization on infrequent
and unknown words, and increase the discrimina-
tive power of the argument classifiers.
For instance, consider these two sentences:
JFK was assassinated (in Dallas)
Location
JFK was assassinated (in November)
Temporal
Both share syntactic and argument structure, so
the lexical features (i.e., the words ?Dallas? and
?November?) represent the most important knowl-
edge to discriminate between the two different ad-
junct roles. The problem is that, in new text,
one may encounter similar expressions with new
words like Texas or Autumn.
We propose a concrete classification problem as
our main evaluation setting for the acquired selec-
tional preferences: given a verb occurrence and
a nominal head word of a constituent dependant
on that verb, assign the most plausible role to the
head word according to the selectional preference
model. This problem is directly connected to ar-
gument classification in SRL, but we have iso-
lated the evaluation from the complete SRL task.
This first step allows us to analyze the potential
of selectional preferences as a source of seman-
tic knowledge for discriminating among different
role labels. Ongoing work is devoted to the inte-
gration of selectional preference?derived features
in a complete SRL system.
2 Related Work
Automatic acquisition of selectional preferences
is a relatively old topic, and will mention the
most relevant references. Resnik (1993) proposed
to model selectional preferences using semantic
classes from WordNet in order to tackle ambiguity
issues in syntax (noun-compounds, coordination,
PP-attachment).
Brockman and Lapata (2003) compared sev-
eral class-based models (including Resnik?s se-
lectional preferences) on a syntactic plausibility
judgement task for German. The models re-
turn weights for (verb, syntactic function, noun)
triples, and the correlation with human plausibil-
ity judgement is used for evaluation. Resnik?s
selectional preference scored best among class-
based methods, but it performed equal to a simple,
purely lexical, conditional probability model.
73
Distributional similarity has also been used to
tackle syntactic ambiguity. Pantel and Lin (2000)
obtained very good results using the distributional
similarity measure defined by Lin (1998).
The application of selectional preferences to se-
mantic roles (as opposed to syntactic functions)
is more recent. Gildea and Jurafsky (2002) is
the only one applying selectional preferences in
a real SRL task. They used distributional clus-
tering and WordNet-based techniques on a SRL
task on FrameNet roles. They report a very small
improvement of the overall performance when us-
ing distributional clustering techniques. In this pa-
per we present complementary experiments, with
a different role set and annotated corpus (Prop-
Bank), a wider range of selectional preference
models, and the analysis of out-of-domain results.
Other papers applying semantic preferences
in the context of semantic roles, rely on the
evaluation on pseudo tasks or human plausibil-
ity judgments. In (Erk, 2007) a distributional
similarity?based model for selectional preferences
is introduced, reminiscent of that of Pantel and
Lin (2000). The results over 100 frame-specific
roles showed that distributional similarities get
smaller error rates than Resnik and EM, with Lin?s
formula having the smallest error rate. Moreover,
coverage of distributional similarities and Resnik
are rather low. Our distributional model for selec-
tional preferences follows her formalization.
Currently, there are several models of distri-
butional similarity that could be used for selec-
tional preferences. More recently, Pad?o and Lap-
ata (2007) presented a study of several parameters
that define a broad family of distributional similar-
ity models, including publicly available software.
Our paper tests similar techniques to those pre-
sented above, but we evaluate selectional prefer-
ence models in a setting directly related to SR
classification, i.e., given a selectional preference
model for a verb we find the role which fits best
for a given head word. The problem is indeed
qualitatively different: we do not have to choose
among the head words competing for a role (as
in the papers above) but among selectional prefer-
ences competing for a head word.
3 Selectional Preference Models
In this section we present all the variants for ac-
quiring selectional preferences used in our study,
and how we apply them to the SR classification.
WordNet-based SP models: we use Resnik?s se-
lectional preference model.
Distributional SP models: Given the availabil-
ity of publicly available resources for distribu-
tional similarity, we used 1) a ready-made the-
saurus (Lin, 1998), and 2) software (Pad?o and La-
pata, 2007) which we run on the British National
Corpus (BNC).
In the first case, Lin constructed his thesaurus
based on his own similarity formula run over a
large parsed corpus comprising journalism texts.
The thesaurus lists, for each word, the most sim-
ilar words, with their weight. In order to get the
similarity for two words, we could check the entry
in the thesaurus for either word. But given that
the thesaurus is not symmetric, we take the av-
erage of both similarities. We will refer to this
similarity measure as sim
th
lin
. Another option is
to use second-order similarity, where we compute
the similarity of two words using the entries in the
thesaurus, either using the cosine or Jaccard mea-
sures. We will refer to these similarity measures
as sim
th2
jac
and sim
th2
cos
hereinafter.
For the second case, we tried the optimal pa-
rameters as described in (Pad?o and Lapata, 2007,
p. 179): word-based space, medium context, log-
likelihood association, and 2,000 basis elements.
We tested Jaccard, cosine and Lin?s measure (Lin,
1998) for similarity, yielding sim
jac
, sim
cos
and
sim
lin
, respectively.
3.1 Role Classification with SP Models
Given a target sentence where a predicate and sev-
eral potential argument and adjunct head words
occur, the goal is to assign a role label to each of
the head words. The classification of candidate
head words is performed independently of each
other.
Since we want to evaluate the ability of selec-
tional preference models to discriminate among
different roles, this is the only knowledge that will
be used to perform classification (avoiding the in-
clusion of any other feature commonly used in
SRL). Thus, for each head word, we will simply
select the role (r) of the predicate (p) which fits
best the head word (w). This selection rule is for-
malized as:
R(p, w) = argmax
r?Roles(p)
S(p, r, w)
being S(p, r, w) the prediction of the selectional
preference model, which can be instantiated with
all the variants mentioned above.
74
For the sake of comparison we also define a lex-
ical baseline model, which will determine the con-
tribution of lexical features in argument classifica-
tion. For a test pair (p, w) the model returns the
role under which the head word occurred most of-
ten in the training data given the predicate.
4 Experimental Setting
The data used in this work is the benchmark cor-
pus provided by the CoNLL-2005 shared task on
SRL (Carreras and M`arquez, 2005). The dataset,
of over 1 million tokens, comprises PropBank sec-
tions 02-21 for training, and sections 24 and 23 for
development and test, respectively. In these ex-
periments, NEG, DIS and MOD arguments have
been discarded because, apart from not being con-
sidered ?pure? adjunct roles, the selectional pref-
erences implemented in this study are not able to
deal with non-nominal argument heads.
The predicate?rol?head (p, r, w) triples for gen-
eralizing the selectional preferences are extracted
from the arguments of the training set, yield-
ing 71,240 triples, from which 5,587 different
predicate-role selectional preferences (p, r) are
derived by instantiating the different models in
Section 3.
Selectional preferences are then used, to predict
the corresponding roles of the (p, w) pairs from
the test corpora. The test set contains 4,134 pairs
(covering 505 different predicates) to be classified
into the appropriate role label. In order to study
the behavior on out-of-domain data, we also tested
on the PropBanked part of the Brown corpus. This
corpus contains 2,932 (p, w) pairs covering 491
different predicates.
The performance of each selectional preference
model is evaluated by calculating the standard pre-
cision, recall and F
1
measures. It is worth men-
tioning that none of the models is able to predict
the role when facing an unknown head word. This
happens more often with WordNet based models,
which have a lower word coverage compared to
distributional similarity?based models.
5 Results and Discussion
The results are presented in Table 1. The lexi-
cal row corresponds to the baseline lexical match
method. The following row corresponds to the
WordNet-based selectional preference model. The
distributional models follow, including the results
obtained by the three similarity formulas on the
prec. rec. F
1
prec. recall F
1
lexical .779 .349 .482 .663 .059 .108
res .589 .495 .537 .505 .379 .433
sim
Jac
.573 .564 .569 .481 .452 .466
sim
cos
.607 .598 .602 .507 .476 .491
sim
Lin
.580 .560 .570 .500 .470 .485
sim
th
Lin
.635 .625 .630 .494 .464 .478
sim
th2
Jac
.657 .646 .651 .531 .499 .515
sim
th2
cos
.654 .644 .649 .531 .499 .515
Table 1: Results for WSJ test (left), and Brown
test (right)
co-occurrences extracted from the BNC (sim
Jac
,
sim
cos
sim
Lin
), and the results obtained when
using Lin?s thesaurus directly (sim
th
Lin
) and as a
second-order vector (sim
th2
Jac
and sim
th2
cos
).
As expected, the lexical baseline attains very
high precision in all datasets, which underscores
the importance of the lexical head word features
in argument classification. The recall is quite
low, specially in Brown, confirming and extend-
ing (Pradhan et al, 2008), which also reports sim-
ilar performance drops when doing argument clas-
sification on out-of-domain data.
One of the main goals of our experiments is to
overcome the data sparseness of lexical features
both on in-domain and out-of-domain data. All
our selectional preference models improve over
the lexical matching baseline in recall, up to 30
absolute percentage points in the WSJ test dataset
and 44 absolute percentage points in the Brown
corpus. This comes at the cost of reduced preci-
sion, but the overall F-score shows that all selec-
tional preference models improve over the base-
line, with up to 17 absolute percentage points
on the WSJ datasets and 41 absolute percentage
points on the Brown dataset. The results, thus,
show that selectional preferences are indeed alle-
viating the lexical sparseness problem.
As an example, consider the following head
words of potential arguments of the verb wear
found in the test set: doctor, men, tie, shoe. None
of these nouns occurred as heads of arguments of
wear in the training data, and thus the lexical fea-
ture would be unable to predict any role for them.
Using selectional preferences, we successfully as-
signed the Arg0 role to doctor and men, and the
Arg1 role to tie and shoe.
Regarding the selectional preference variants,
WordNet-based and first-order distributional sim-
ilarity models attain similar levels of precision,
but the former are clearly worse on recall and F
1
.
75
The performance loss on recall can be explained
by the worse lexical coverage of WordNet when
compared to automatically generated thesauri. Ex-
amples of words missing in WordNet include ab-
breviations (e.g., Inc., Corp.) and brand names
(e.g., Texaco, Sony). The second-order distribu-
tional similarity measures perform best overall,
both in precision and recall. As far as we know,
it is the first time that these models are applied to
selectional preference modeling, and they prove to
be a strong alternative to first-order models. The
relative performance of the methods is consistent
across the two datasets, stressing the robustness of
all methods used.
Regarding the use of similarity software (Pad?o
and Lapata, 2007) on the BNC vs. the use of
Lin?s ready-made thesaurus, both seem to perform
similarly, as exemplified by the similar results of
sim
Lin
and sim
th
Lin
. The fact that the former per-
formed better on the Brown data, and worse on the
WSJ data could be related to the different corpora
used to compute the co-occurrence, balanced cor-
pus and journalism texts respectively. This could
be an indication of the potential of distributional
thesauri to adapt to the target domain.
Regarding the similarity metrics, the cosine
seems to perform consistently better for first-order
distributional similarity, while Jaccard provided
slightly better results for second-order similarity.
The best overall performance was for second-
order similarity, also using the cosine. Given
the computational complexity involved in build-
ing a complete thesaurus based on the similarity
software, we used the ready-made thesaurus of
Lin, but could not try the second-order version on
BNC.
6 Conclusions and Future Work
We have empirically shown how automatically
generated selectional preferences, using WordNet
and distributional similarity measures, are able to
effectively generalize lexical features and, thus,
improve classification performance in a large-
scale argument classification task on the CoNLL-
2005 dataset. The experiments show substantial
gains on recall and F
1
compared to lexical match-
ing, both on the in-domain WSJ test and, espe-
cially, on the out-of-domain Brown test.
Alternative selectional models were studied and
compared. WordNet-based models attain good
levels of precision but lower recall than distribu-
tional similarity methods. A new second-order
similarity method proposed in this paper attains
the best results overall in all datasets.
The evidence gathered in this paper suggests
that using semantic knowledge in the form of se-
lectional preferences has a high potential for im-
proving the results of a full system for SRL, spe-
cially when training data is scarce or when applied
to out-of-domain corpora.
Current efforts are devoted to study the integra-
tion of the selectional preference models presented
in this paper in a in-house SRL system. We are
particularly interested in domain adaptation, and
whether distributional similarities can profit from
domain corpora for better performance.
Acknowledgments
This work has been partially funded by the EU Commis-
sion (project KYOTO ICT-2007-211423) and Spanish Re-
search Department (project KNOW TIN2006-15049-C03-
01). Be?nat enjoys a PhD grant from the University of the
Basque Country.
References
Carsten Brockmann and Mirella Lapata. 2003. Evaluating
and combining approaches to selectional preference ac-
quisition. In Proceedings of the 10th Conference of the
European Chapter of the ACL, pages 27?34.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages 152?
164, Ann Arbor, MI, USA.
Katrin Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguistics,
pages 216?223, Prague, Czech Republic.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL, pages 768?774.
Sebastian Pad?o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199, June.
Patrick Pantel and Dekang Lin. 2000. An unsupervised ap-
proach to prepositional phrase attachment using contex-
tually similar words. In Proceedings of the 38th Annual
Conference of the ACL, pages 101?108.
S. Pradhan, W. Ward, and J. H. Martin. 2008. Towards robust
semantic role labeling. Computational Linguistics, 34(2).
Philip Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proceedings of the workshop on Human Language
Technology, pages 278?283, Morristown, NJ, USA.
76
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354?357,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-UPC: Sequential SRL Using Selectional Preferences.
An aproach with Maximum Entropy Markov Models
Ben?at Zapirain, Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical University of Catalonia
Barcelona, Catalonia
lluism@lsi.upc.edu
Abstract
We present a sequential Semantic Role La-
beling system that describes the tagging
problem as a Maximum Entropy Markov
Model. The system uses full syntactic in-
formation to select BIO-tokens from input
data, and classifies them sequentially us-
ing state-of-the-art features, with the addi-
tion of Selectional Preference features. The
system presented achieves competitive per-
formance in the CoNLL-2005 shared task
dataset and it ranks first in the SRL subtask
of the Semeval-2007 task 17.
1 Introduction
In Semantic Role Labeling (SRL) the goal is to iden-
tify word sequences or arguments accompanying the
predicate and assign them labels depending on their
semantic relation. In this task we disambiguate ar-
gument structures in two ways: predicting VerbNet
(Kipper et al, 2000) thematic roles and PropBank
(Palmer et al, 2005) numbered arguments, as well
as adjunct arguments.
In this paper we describe our system for the SRL
subtask of the Semeval2007 task 17. It is based on
the architecture and features of the system named
?model 2? of (Surdeanu et al, forthcoming), but it
introduces two changes: we use Maximum Entropy
for learning instead of AdaBoost and we enlarge the
feature set with combined features and other seman-
tic features.
Traditionally, most of the features used in SRL
are extracted from automatically generated syntac-
tic and lexical annotations. In this task, we also ex-
periment with provided hand labeled semantic infor-
mation for each verb occurrence such as the Prop-
Bank predicate sense and the Levin class. In addi-
tion, we use automatically learnt Selectional Prefer-
ences based on WordNet to generate a new kind of
semantic based features.
We participated in both the ?close? and the ?open?
tracks of Semeval2007 with the same system, mak-
ing use, in the second case, of the larger CoNLL-
2005 training set.
2 System Description
2.1 Data Representation
In order to make learning and labeling easier, we
change the input data representation by navigating
through provided syntactic structures and by extract-
ing BIO-tokens from each of the propositions to be
annotated as shown in (Surdeanu et al, forthcom-
ing). These sequential tokens are selected by ex-
ploring the sentence spans or regions defined by the
clause boundaries, and they are labeled with BIO
tags depending on the location of the token: at the
beginning, inside, or outside of a verb argument. Af-
ter this data pre-processing step, we obtain a more
compact and easier to process data representation,
making also impossible overlapping and embedded
argument predictions.
2.2 Feature Representation
Apart from Selectional Preferences (cf. Section 3)
and those extracted from provided semantic infor-
mation, most of the features we used are borrowed
from the existing literature (Gildea and Jurafsky,
2002; Xue and Palmer, 2004; Surdeanu et al, forth-
coming).
354
On the verb predicate:
? Form; Lemma; POS tag; Chunk type and Type
of verb phrase; Verb voice; Binary flag indicat-
ing if the verb is a start/end of a clause.
? Subcategorization, i.e., the phrase structure rule
expanding the verb parent node.
? VerbNet class of the verb (in the ?close? track
only).
On the focus constituent:
? Type; Head;
? First and last words and POS tags of the con-
stituent.
? POS sequence.
? Bag-of-words of nouns, adjectives, and adverbs
in the constituent.
? TOP sequence: right-hand side of the rule ex-
panding the constituent node; 2/3/4-grams of
the TOP sequence.
? Governing category as described in (Gildea
and Jurafsky, 2002).
Context of the focus constituent:
? Previous and following words and POS tags of
the constituent.
? The same features characterizing focus con-
stituents are extracted for the two previous and
following tokens, provided they are inside the
clause boundaries of the codified region.
Relation between predicate and constituent:
? Relative position; Distance in words and
chunks; Level of embedding with respect to the
constituent: in number of clauses.
? Binary position; if the argument is after or be-
fore the predicate.
? Constituent path as described in (Gildea and
Jurafsky, 2002); All 3/4/5-grams of path con-
stituents beginning at the verb predicate or end-
ing at the constituent.
? Partial parsing path as described in (Carreras
et al, 2004)); All 3/4/5-grams of path elements
beginning at the verb predicate or ending at the
constituent.
? Syntactic frame as described by Xue and
Palmer (2004)
Combination Features
? Predicate and Phrase Type
? Predicate and binary position
? Head Word and Predicate
? Predicate and PropBank frame sense
? Predicate, PropBank frame sense, VerbNet
class (in the ?close? track only)
2.3 Maximum Entropy Markov Models
Maximum Entropy Markov Models are a discrimi-
native model for sequential tagging that models the
local probability P (sn | sn?1, o), where o is the
context of the observation.
Given a MEMM, the most likely state sequence is
the one that maximizes the following
S = argmax
n?
i=1
P (si | si?1, o)
Translating the problem to SRL, we have
role/argument labels connected to each state in the
sequence (or proposition), and the observations are
the features extracted in these points (token fea-
tures). We get the most likely label sequence finding
out the most likely state sequence (Viterbi).
All the conditional probabilities are given by the
Maximum Entropy classifier with a tunable Gaus-
sian prior from the Mallet Toolkit1.
Some restrictions are considered when we search
the most likely sequence2:
1. No duplicate argument classes for A0-A5 and
thematic roles.
2. If there is a R-X argument (reference), then
there has to be a X argument before (refer-
enced).
3. If there is a C-X argument (continuation), then
there has to be a X argument before.
4. Before a I-X token, there has to be a B-X or I-X
token (because of the BIO encoding).
5. Given a predicate and its PropBank sense, only
some arguments are allowed (e.g. not all the
verbs support A2 argument).
6. Given a predicate and its Verbnet class, only
some thematic roles are allowed.
3 Including Selectional Preferences
Selectional Preferences (SP) try to capture the fact
that linguistic elements prefer arguments of a cer-
tain semantic class, e.g. a verb like ?eat? prefers as
subject edible things, and as subject animate entities,
as in ?She was eating an apple? They can be learned
from corpora, generalizing from the observed argu-
ment heads (e.g. ?apple?, ?biscuit?, etc.) into ab-
stract classes (e.g. edible things). In our case we
1http://mallet.cs.umass.edu
2Restriction 5 applies to PropBank output. Restriction 6 ap-
plies to VerbNet output
355
follow (Agirre and Martinez, 2001) and use Word-
Net (Fellbaum, 1998) as the generalization classes
(the concept <food,nutrient>).
The aim of using Selectional Preferences (SP) in
SRL is to generalize from the argument heads in
the training instances into general word classes. In
theory, using word classes might overcome the data
sparseness problem for the head-based features, but
at the cost of introducing some noise.
More specifically, given a verb, we study the oc-
currences of the target verb in a training corpus (e.g.
the PropBank corpus), and learn a set of SPs for
each argument and adjunct of that verb. For in-
stance, given the verb ?kill? we would have 2 SPs
for each argument type, and 4 SPs for some of the
observed adjuncts: kill A0, kill A1, kill AM-
LOC, kill AM-MNR, kill AM-PNC and kill AM-
TMP.
Rather than coding the SPs directly as features,
we code the predictions instead, i.e. for each propo-
sition in the training and testing set, we check the
SPs for all the argument (and adjunct) headwords,
and the SP which best fits the headword (see below)
is the one that is selected. We codify the predicted
argument (or adjunct) label as features, and we insert
them among the corresponding argument features.
For instance, let?s assume that the word ?railway?
appears as the headword of a candidate argument of
?kill?. WordNet 1.6 yields the following hypernyms
for ?railway? (from most general to most specific, we
include the WordNet 1.6 concept numbers preceded
by their specifity level);
1 00001740 1 00017954
2 00009457 2 05962976
3 00011937 3 05997592
4 03600463 4 06004580
5 03243979 5 06008236
6 03526208 6 06005839
7 03208595 7 02927599
8 03209020
Note that we do not care about the sense ambigu-
ity and the explosion of concepts that it carries. Our
algorithm will check each of the hypernyms of rail-
way and match them with the concepts in the SPs of
?kill?, giving preference to the most specific concept.
In case that equally specific concepts match different
SPs, we will choose the SP that has the concept that
ranks highest in the SP, and code the SP feature with
the label of the SP where the match succeeds. In the
example, these are the most specific matches:
AM-LOC Con:03243979 Level:5 Ranking:32
A0 Con:06008236 Level:5 Ranking:209
There is a tie in the level, so we choose the one
with the highest rank. All in all, this means that ac-
cording to the learnt SPs we would predict that ?rail-
way? is a location feature for ?kill?, and we would
therefore insert the ?SP:AM-LOC? feature among
the argument features.
If ?railway? appears as the headword of other
verbs, the predicted argument might be different.
See for instance, the following verbs:
destroy:A1 Con:03243979 Level:5 Ranking:43
go:A0 Con:02927599 Level:7 Ranking:131
go:A2 Con:02927599 Level:7 Ranking:721
build:A1 Con:03209020 Level:8 Ranking:294
Note that our training examples did not contain
?railway? as an argument of any of these verbs, but
due to the SPs we are able to code into a feature that
?railway? belongs to a concrete semantic class which
contains conceptually similar headwords.
We decided to code the prediction of the SPs,
rather than the SPs themselves, in order to be more
robust to noise.
There is a further subtlety with our SP system. In
order to label training and testing sets in similar con-
ditions and avoid overfitting problems as much as
possible, we split the training set into five folds and
tagged each one with SPs learnt from the other four.
For extracting SP features from test set examples,
we use SPs learnt in the whole training set.
4 Experiments and Results
We participated in the ?close? and the ?open? tracks
with the same classification model, but using dif-
ferent training sets in each one. In the close track
we only use the provided training set, and in the
open, the CoNLL-2005 training set (without Verb-
Net classes or thematic roles).
Before our participation, we tested the system in
the CoNLL-2005 close track setting and it achieved
competitive performance in comparison to the state-
of-the-art results published in that challenge.
4.1 Semeval2007 setting
The data provided in the close track consists of the
propositions of 50 different verb lemmas from Prop-
Bank (sections 02-21). The data for the CoNLL-
2005 is also a subset of the PropBank data, but it
356
Track Label rank prec. rec. F1
Close VerbNet 1st 85.31 82.08 83.66
Close PropBank 1st 85.04 82.07 83.52
Open PropBank 1st 84.51 82.24 83.36
Table 1: Results in the SRL subtask of SemEval-
2007 task 17
includes all the propositions in sections 02-21 and
no VerbNet classes nor thematic roles for learning.
There is a total of 21 argument types for Prop-
Bank and 47 roles for VerbNet, which amounts to
21 ? 2 + 1 = 43 BIO-labels for PropBank predic-
tions and 47 ? 2 + 1 = 95 for VerbNet. We filtered
the less frequent (<5).
We trained the Maximum Entropy classifiers with
114,380 examples for the close track, and with
828,811 for the open track. We tuned the classifier
by setting the Exponential Gaussian prior in 0.1
4.2 Results
In the close track we trained two classifiers, one
to label PropBank numbered arguments and a sec-
ond to label VerbNet thematic roles. Due to lack
of time, we only trained the PropBank labels in the
open track. Table 1 shows the results obtained in the
SRL subtask. We ranked first in all of them, out of
two participants.
4.3 Discussion
The results indicate that in the close track the system
performs similarly on both PropBank arguments and
VerbNet roles. The absence of VerbNet class-based
features in the CoNLL-2005 training data could
cause the loss of performance in the open track. We
plan to perform the experiment on VerbNet roles for
the open track to check the ability of the classifier to
generalize across verbs.
Regarding the use of SP features, nowadays, we
have not obtained relevant improvements in the pre-
dictions of the classifiers. It is our first approach to
these kind of semantic features and there are more
sophisticated but evident extraction variants which
we are exploring.
Although the general performance is very simi-
lar without SP features, using them our system ob-
tains better results in ARG3 core arguments and in
the most frequent adjuncts such as location (LOC),
general-purpose (ADV) and temporal (TMP).
We reproduced this improvements in experiments
realized with CoNLL-2005 larger test sets. In that
case, we improved ARG3-ARG4 core arguments as
well as the mentioned adjuncts. There were more
examples to be classified and we get better overall
performance, but we need further experiments to be
more conclusive.
5 Conclusions
We have presented a sequential semantic role la-
beling system for the Semeval-2007 task 17 (SRL).
Based on Maximum Entropy Markov Models, it ob-
tains competitive and promising results. We also
have introduced semantic features extracted from
Selectional Restrictions but we only have prelimi-
nary evidence of their usefulness.
Acknowledgements
We thank David Martinez for kindly providing the
software that learnt the selectional preferences. This
work has been partially funded by the Spanish ed-
ucation ministry (KNOW). Ben?at is supported by a
PhD grant from the University of the Basque Coun-
try.
References
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL-
2001, Toulouse, France.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hi-
erarchical recognition of propositional arguments with
perceptrons. In Proceedings of CoNLL 2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics , 28(3).
K. Kipper, Hoa Trang Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon. In Pro-
ceedings of AAAI-2000 Seventeenth National Confer-
ence on Artificial Intellingence, Austin, TX .
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics , 31(1).
M. Surdeanu, L. Ma`rquez, X. Carreras, and P. Comas.
(forthcoming). Combination strategies for semantic
role labeling. In Journal of Artificial Intelligence Re-
search.
N. Xue and M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of EMNLP-2004 .
357
Selectional Preferences for
Semantic Role Classification
Ben?at Zapirain?
University of the Basque Country
Eneko Agirre??
University of the Basque Country
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Mihai Surdeanu?
University of Arizona
This paper focuses on a well-known open issue in Semantic Role Classification (SRC) research:
the limited influence and sparseness of lexical features. We mitigate this problem using models
that integrate automatically learned selectional preferences (SP). We explore a range of models
based on WordNet and distributional-similarity SPs. Furthermore, we demonstrate that the SRC
task is better modeled by SP models centered on both verbs and prepositions, rather than verbs
alone. Our experiments with SP-based models in isolation indicate that they outperform a lexical
baseline with 20 F1 points in domain and almost 40 F1 points out of domain. Furthermore, we
show that a state-of-the-art SRC system extended with features based on selectional preferences
performs significantly better, both in domain (17% error reduction) and out of domain (13%
error reduction). Finally, we show that in an end-to-end semantic role labeling system we obtain
small but statistically significant improvements, even though our modified SRC model affects
only approximately 4% of the argument candidates. Our post hoc error analysis indicates that
the SP-based features help mostly in situations where syntactic information is either incorrect or
insufficient to disambiguate the correct role.
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: benat.zapirain@ehu.es.
?? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: e.agirre@ehu.es.
? UPC Campus Nord (Omega building), Jordi Girona 1?3, 08034 Barcelona, Catalonia.
E-mail: lluism@lsi.upc.edu.
? 1040 E. 4th Street, Tucson, AZ 85721. E-mail: msurdeanu@arizona.edu.
Submission received: 14 November 2011; revised submission received: 31 May 2012; accepted for publication:
15 August 2012.
doi:10.1162/COLI a 00145
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Semantic Role Labeling (SRL) is the problem of analyzing clause predicates in text by
identifying arguments and tagging them with semantic labels indicating the role they
play with respect to the predicate. Such sentence-level semantic analysis allows the
determination of who did what to whom, when and where, and thus characterizes the
participants and properties of the events established by the predicates. For instance,
consider the following sentence, in which the arguments of the predicate to send have
been annotated with their respective semantic roles.1
(1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal.
Recognizing these event structures has been shown to be important for a broad
spectrum of NLP applications. Information extraction, summarization, question
answering, machine translation, among others, can benefit from this shallow semantic
analysis at sentence level, which opens the door for exploiting the semantic relations
among arguments (Boas 2002; Surdeanu et al 2003; Narayanan and Harabagiu 2004;
Melli et al 2005; Moschitti et al 2007; Higashinaka and Isozaki 2008; Surdeanu,
Ciaramita, and Zaragoza 2011). In Ma`rquez et al (2008) the reader can find a broad
introduction to SRL, covering several historical and definitional aspects of the problem,
including also references to the main resources and systems.
State-of-the-art systems leverage existing hand-tagged corpora (Fillmore,
Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised
machine learning systems, and typically perform SRL in two sequential steps:
argument identification and argument classification. Whereas the former is mostly a
syntactic recognition task, the latter usually requires semantic knowledge to be taken
into account. The semantic knowledge that most current systems capture from text is
basically limited to the predicates and the lexical units contained in their arguments,
including the argument head. These ?lexical features? tend to be sparse, especially
when the training corpus is small, and thus SRL systems are prone to overfit the
training data and generalize poorly to new corpora (Pradhan, Ward, and Martin 2008).
As a simplified example of the effect of sparsity, consider the following sentences
occurring in an imaginary training data set for SRL:
(2) [JFK]Patient was assassinated [in Dallas]Location
(3) [John Lennon]Patient was assassinated [in New York]Location
(4) [JFK]Patient was assassinated [in November]Temporal
(5) [John Lennon]Patient was assassinated [in winter]Temporal
All four sentences share the same syntactic structure, so the lexical features (i.e., the
words Dallas, New York, November, and winter) represent the most relevant knowledge
for discriminating between the Location and Temporal adjunct labels in learning.
1 For simplicity, in this paper we talk about arguments in the most general sense. Unless noted otherwise,
argument will refer to both core-arguments (Agent, Patient, Instrument, etc.) and adjuncts (Manner,
Temporal, Location, etc.).
632
Zapirain et al Selectional Preferences for Semantic Role Classification
The problem is that, as in the following sentences, for the same predicate, one may
encounter similar expressions with new words like Texas or December, which the
classifiers cannot match with the lexical features seen during training, and thus become
useless for classification:
(6) [Smith] was assassinated [in Texas]
(7) [Smith] was assassinated [in December]
This problem is exacerbated when SRL systems are applied to texts coming from
new domains where the number of new predicates and argument heads increases
considerably. The CoNLL-2004 and 2005 evaluation exercises on semantic role labeling
(Carreras and Ma`rquez 2004, 2005) reported a significant performance degradation
of around 10 F1 points when applied to out-of-domain texts from the Brown corpus.
Pradhan, Ward, and Martin (2008) showed that this performance degradation is
essentially caused by the argument classification subtask, and suggested the lexical
data sparseness as one of the main reasons.
In this work, we will focus on Semantic Role Classification (SRC), and we will show
that selectional preferences (SP) are useful for generalizing lexical features, helping
fight sparseness and domain shifts, and improving SRC results. Selectional preferences
try to model the kind of words that can fill a specific argument of a predicate, and
have been widely used in computational linguistics since the early days (Wilks 1975).
Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and
distributional similarity based on corpora (Pantel and Lin 2000) have been successfully
used for acquiring selectional preferences, and in this work we have used several of
those models.
The contributions of this work to the field of SRL are the following:
1. We formalize and implement a method that applies several selectional
preference models to Semantic Role Classification, introducing for the first
time the use of selectional preferences for prepositions, in addition to
selectional preferences for verbs.
2. We show that the selectional preference models are able to generalize
lexical features and improve role classification performance in a controlled
experiment disconnected from a complete SRL system. The positive effect
is consistently observed in all variants of WordNet and distributional
similarity measures and is especially relevant for out-of-domain data. The
separate learning of SPs for verbs and prepositions contributes
significantly to the improvement of the results.
3. We integrate the information of several SP models in a state-of-the-art SRL
system (SwiRL)2 and obtain significant improvements in semantic role
classification and, as a consequence, in the end-to-end SRL task. The key
for the improvement lies in the combination of the predictions provided
by SwiRL and the several role classification models based on selectional
preferences.
2 http://surdeanu.info/mihai/swirl/.
633
Computational Linguistics Volume 39, Number 3
4. We present a manual analysis of the output of the combined role
classification system. By observing a set of real examples, we categorized
and quantified the situations in which SP models tend to help role
classification. By inspecting also a set of negative cases, this analysis also
sheds light on the limitations of the current approach and identifies
opportunities for further improvements.
The use of selectional preferences for improving role classification was first pre-
sented in Zapirain, Agirre, and Ma`rquez (2009), and later extended in Zapirain et al
(2010) to a full-fledged SRC system. In the current paper, we provide more detailed
background information and details of the selectional preference models, as well as
complementary experiments on the integration in a full-fledged system. More impor-
tantly, we incorporate a detailed analysis of the output of the system, comparing it with
that of a state-of-the-art SRC system not using SPs.
The rest of the paper is organized as follows. Section 2 provides background on the
automatic acquisition of selectional preference, and its recent relation to the semantic
role labeling problem. In Section 3, the SP models investigated in this paper are ex-
plained in all their variants. The results of the SP models in laboratory conditions are
presented in Section 4. Section 5 describes the method for integrating the SP models in a
state-of-the-art SRL system and discusses the results obtained. In Section 6 the qualita-
tive analysis of the system output is presented, including a detailed discussion of several
examples. Finally, Section 7 concludes and outlines some directions for future research.
2. Background
The simplest model for generating selectional preferences would be to collect all heads
filling each role of the target predicate. This is akin to the lexical features used by current
SRL systems, and we refer to this model as the lexical model. More concretely, the
lexical model for verb-role selectional preferences consists of the list of words appearing
as heads of the role arguments of the predicate verb. This model can be extracted
automatically from the SRL training corpus using straightforward techniques. When
using this model for role classification, it suffices to check whether the head word of
the argument matches any of the words in the lexical model. The lexical model is the
baseline for our other SP models, all of which build on that model.
In order to generalize the lexical model, semantic classes can be used. Although in
principle any lexical resource listing semantic classes for nouns could be applied, most
of the literature has focused on the use of WordNet (Resnik 1993b). In the WordNet-
based model, the words occurring in the lexical model are projected over the semantic
hierarchy of WordNet, and the semantic classes which represent best those words are
selected. Given a new example, the SRC system has to check whether the new word
matches any of those semantic classes. For instance, in example sentences (2)?(5), the
semantic class <time period> covers both training examples for Temporal (i.e., November
and winter), and <geographical area> covers the examples for Location. When test
words Texas and December occur in Examples (6) and (7), the semantic classes to which
they belong can be used to tag the first as Location and the second as Temporal.
As an alternative to the use of WordNet, one can also apply automatically acquired
distributional similarity thesauri. Distributional similarity methods analyze the co-
occurrence patterns of words and are able to capture, for instance, that December is more
closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is
typically used on-line (i.e., given a pair of words, their similarity is computed on the go),
634
Zapirain et al Selectional Preferences for Semantic Role Classification
but, in order to speed up its use, it has also been used to produce off-line a full thesauri,
storing, for every word, the weighted list of all outstanding similar words (Lin 1998).
In the Distributional similarity model, when test item Texas in Example (6) is to be
labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity
to November and winter, would be used to label the argument with the Location role.
The automatic acquisition of selectional preferences is a well-studied topic in NLP.
Many methods using semantic classes and selectional preferences have been proposed
and applied to a variety of syntactic?semantic ambiguity problems, including syntactic
parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez
2008; Koo, Carreras, and Collins 2008; Agirre et al 2011), word sense disambiguation
(Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun res-
olution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and
Roth 2009). In addition, selectional preferences have been shown to be effective to
improve the quality of inference and information extraction rules (Pantel et al 2007;
Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not
mention selectional preferences, but all of them use some notion of preferring certain
semantic types over others in order to accomplish their respective task.
In fact, one could use different notions of semantic types. In one extreme, we would
have a small set of coarse semantic classes. For instance, some authors have used the
26 so-called ?semantic fields? used to classify all nouns in WordNet (Agirre, Baldwin,
and Martinez 2008; Agirre et al 2011). The classification could be more fine-grained, as
defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy
and Carroll 2003), and other lexical resources could be used as well. Other authors have
used automatically induced hierarchical word classes, clustered according to occurrence
information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009).
On the other extreme, each word would be its own semantic class, as in the lexical
model, but one could also model selectional preference using distributional similarity
(Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel
2008). In this paper we will focus on WordNet-based models that use the whole hierarchy
and on distributional similarity models, and we will use the lexical model as baseline.
2.1 WordNet-Based Models
Resnik (1993b) proposed the modeling of selectional preferences using semantic classes
from WordNet and applied the model to tackle some ambiguity issues in syntax, such
as noun-compounds, coordination, and prepositional phrase attachment. Given two
alternative structures, Resnik used selectional preferences to choose the attachment
maximizing the fitness of the head to the selectional preferences of the attachment
points. This is similar to our task, but in our case we compare the target head to the selec-
tional preference models for each possible role label (i.e., given a verb and the head of an
argument, we need to find the role with the selectional preference that fits the head best).
In Resnik?s model, he first characterizes the restrictiveness of the selectional pref-
erence of an argument position r of a governing predicate p, noted as R(p, r). For that,
given a set of classes C from the WordNet nominal hierarchies, he takes the relative en-
tropy or Kullback-Leibler distance between the prior distribution P(C) and the posterior
distribution P(C|p, r):
R(p, r) =
?
c?C
P(c|p, r)logP(c|p, r)
P(c)
(1)
635
Computational Linguistics Volume 39, Number 3
The priors can be computed from any corpora, computing frequencies of classes
and using maximum likelihood estimates. The frequencies for classes cannot be directly
observed, but they can be estimated from the lexical frequencies of the nouns under
the class, as in Equation (2). Note that in WordNet, hypernyms (?hyp? for short)
correspond to superclass relations, and therefore hyp(n) returns all superclasses of
noun n.
freq(c) =
?
{n|c?hyp(n)}
freq(n) (2)
A complication arises because of the polysemy of nouns. If each occurrence of a
noun counted once in all classes that its senses belong to, polysemous nouns would
account for more probability mass than monosemous nouns, even if they occurred the
same number of times. As a solution, the frequency of polysemous nouns is split among
its senses uniformly. For instance, the probability of the class <time period> can be
estimated according to the frequencies of nouns like November, spring, and the rest of
nouns under it. November has a single sense, so every occurrence counts as 1, but spring
has six different senses, so each occurrence should only count as 0.16. Note that with
this method we are implicitly dealing with the word sense ambiguity problem. When
encountering a polysemous noun as an argument of a verb, we record the occurrence
of all of its senses. Given enough occurrences of nouns, the classes generalizing the
intended sense of the nouns will gather more counts than competing classes. In the
example, <time period> would have 1.16 compared with 0.16 <tool> (i.e., for the metal
elastic device meaning of spring). Researchers have used this fact to perform Word Sense
Disambiguation using selectional preferences (Resnik 1993a; Agirre and Martinez 2001;
McCarthy and Carroll 2003).
The posterior probability can be computed similarly, but it takes into account occur-
rences of the nouns in the required argument position of the predicate, and thus requires
a corpus annotated with roles.
The selectional preference of a predicate p and role r for a head w0 of any potential
argument, noted as SPRes(p, r, w0), is formulated as follows:3
SPRes(p, r, w0) = max
c0?hyp(w0 )
P(c0|p, r)log P(c0|p,r)P(c0)
R(p, r)
(3)
The numerator formalizes the goodness of fit for the best semantic class c0 that
contains w0. The hypernym (i.e., superclass) of w0 yielding the maximum value is
chosen. The denominator models how restrictive the selectional preference is for p and
r, as modeled in Equation (1).
Variations of Resnik?s idea to find a suitable level of generalization have been
explored in later years. Li and Abe (1998) applied the minimum-description length
principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a
class should be preferred rather than its children.
Brockmann and Lapata (2003) compared several class-based models (including
Resnik?s selectional preferences) on a syntactic plausibility judgment task for German.
3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented
in this paper.
636
Zapirain et al Selectional Preferences for Semantic Role Classification
The models return weights for (verb, syntactic function, noun) triples, and correla-
tion with human plausibility judgment is used for evaluation. Resnik?s selectional
preference scored best among WordNet-based methods (Li and Abe 1998; Clark and
Weir 2002). Despite its earlier publication, Resnik?s method is still the most popular
representative among WordNet-based methods (Pado?, Pado?, and Erk 2007; Erk, Pado?,
and Pado? 2010; Baroni and Lenci 2010). We also chose to use Resnik?s model in this
paper.
One of the disadvantages of the WordNet-based models, compared with the distri-
butional similarity models, is that they require that the heads are present in WordNet.
This limitation can negatively influence the coverage of the model, and also its general-
ization ability.
2.2 Distributional Similarity Models
Distributional similarity models assume that a word is characterized by the words it
co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size
context window. Each word w would be represented by the set of words that co-occur
with it, T(w). In a more elaborate model, each word w would be represented as a vector
of words T(w) with weights, where Ti(w) corresponds to the weight of the ith word in
the vector. The weights can be calculated following a simple frequency of co-occurrence,
or using some other formula.
Then, given two words w and w0, their similarity can be computed using any simi-
larity measure between their co-occurrence sets or vectors. For instance, early work by
Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0)
(cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions,
including Jaccard and the cosine between two vectors T(w) and T(w0) (cf. Equation (5)
in Figure 1).
In the context of lexical semantics, the similarity measure defined by Lin (1998)
has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account
syntactic dependencies (d) in its co-occurrence model. In this case, the set T(w) of co-
occurrences of w contains pairs (d,v) of dependencies and words, representing the fact
simJac(w, w0) =
|T(w) ? T(w0)|
|T(w) ? T(w0)|
(4)
simcos(w, w0) =
?n
i=1
Ti(w)Ti(w0)
?
?n
i=1
Ti(w)2
?
?n
i=1
Ti(w0)2
(5)
simLin(w, w0) =
?
(d,v)?T(w)?T(w0 )(I(w, d, v) + I(w0, d, v))
?
(d,v)?T(w) I(w, d, v) +
?
(d,v)?T(w0 ) I(w0, d, v)
(6)
Figure 1
Similarity measures used in the paper. Jac and cos stand for Jaccard and cosine similarity metrics.
T(w) is the set of words co-occurring with w, Ti(w) is the weight of the ith element of the vector
of words co-occurring with w, and I(w, d, v) is the mutual information between w and d, v.
637
Computational Linguistics Volume 39, Number 3
that the corpus contains an occurrence of w having dependency d with v. For instance,
if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the set
T for John. The measure uses information-theoretic principles, and I(w, d, v) represents
the information content of the triple (Lin 1998).
Although the use of co-occurrence vectors for words to compute similarity has been
standard practice, some authors have argued for more complex uses. Schu?tze (1998)
builds vectors for each context of occurrence of a word, combining the co-occurrence
vectors for each word in the context. The vectors for contexts were used to induce
senses and to improve information retrieval results. Edmonds (1997) built a lexical co-
occurrence network, and applied it to a lexical choice task. Chakraborti et al (2007)
used transitivity over co-occurrence relations, with good results on several classification
tasks. Note that all these works use second order and higher order to refer to their method.
In this paper, we will also use second order to refer to a new method which goes beyond
the usual co-occurrence vectors (cf. Section 3.3).
A full review of distributional models is out of the scope of this paper, as we are in-
terested in showing that some of those models can be used successfully to improve SRC.
Pado? and Lapata (2007) present a review of distributional models for word similarity,
and a study of several parameters that define a broad family of distributional similarity
models, including Jaccard and Lin. They provide publicly available software,4 which
we have used in this paper, as explained in the next section. Baroni and Lenci (2010)
present a framework for extracting distributional information from corpora that can be
used to build models for different tasks.
Distributional similarity models were first used to tackle syntactic ambiguity. For
instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the
distributional similarity measure defined by Lin (1998). Distributional similarity was
used to overcome sparsity problems: Alongside the counts in the training data of the
target words, the counts of words similar to the target ones were also used. Although
not made explicit, Lin was actually using a distributional similarity model of selectional
preferences.
The application of distributional selectional preferences to semantic roles (as op-
posed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones
applying selectional preferences in a real SRL task. They used distributional clustering
and WordNet-based techniques on a SRL task on FrameNet roles. They report a very
small improvement of the overall performance when using distributional clustering
techniques. In this paper we present complementary experiments, with a different role
set and annotated corpus (PropBank), a wider range of selectional preference models,
and the analysis of out-of-domain results.
Other papers applying semantic preferences in the context of semantic roles rely on
the evaluation of artificial tasks or human plausibility judgments. Erk (2007) introduced
a distributional similarity?based model for selectional preferences, reminiscent of that
of Pantel and Lin (2000). Her approach models the selectional preference SPsim(p, r, w0)
of an argument position r of governing predicate p for a possible head-word w0 as
follows:
SPsim(p, r, w0) =
?
w?Seen(p,r)
sim(w0, w) ? weight(p, r, w) (7)
4 http://www.coli.uni-saarland.de/?pado/dv/dv.html.
638
Zapirain et al Selectional Preferences for Semantic Role Classification
where sim(w0, w) is the similarity between the seen and potential heads, Seen(p, r) is the
set of heads of role r for predicate p seen in the training data set (as in the lexical model),
and weight(p, r, w) is the weight of the seen head word w. Our distributional model for
selectional preferences follows her formalization.
Erk instantiated the basic model with several corpus-based distributional similarity
measures, including Lin?s similarity, Jaccard, and cosine (Figure 1) among others, and
several implementations of the weight function such as the frequency. The quality of
each model instantiation, alongside Resnik?s model and an expectation maximization
(EM)-based clustering model, was tested in a pseudo-disambiguation task where the
goal was to distinguish an attested filler of the role and a randomly chosen word. The
results over 100 frame-specific roles showed that distributional similarities attain similar
error rates to Resnik?s model but better than EM-based clustering, with Lin?s formula
having the smallest error rate. Moreover, the coverage of distributional similarity mea-
sures was much better than Resnik?s. In a more recent paper, Erk, Pado?, and Pado? (2010)
extend the aforementioned work, including evaluation to human plausibility judgments
and a model for inverse selectional preferences.
In this paper we test similar techniques to those presented here, but we evaluate
selectional preference models in a setting directly related to semantic role classification,
namely, given a selectional preference model for a verb we find the role which fits
best the given head word. The problem is indeed qualitatively different from previous
work in that we do not have to choose among the head words competing for a role but
among selectional preferences of roles competing for a head word.
More recent work on distributional selectional preference has explored the use of
discriminative models (Bergsma, Lin, and Goebel 2008) and topical models (O? Se?aghdha
2010; Ritter, Mausam, and Etzioni 2010). These models would be a nice addition to those
implemented in this paper, and if effective, they would improve further our results with
respect to the baselines which don?t use selectional preferences.
Contrary to WordNet-based models, distributional preferences do not rely on a
hand-built resource. Their coverage and generalization ability depend on the corpus
from which the distributional similarity model was computed. This fact makes this
approach more versatile in domain adaptation scenarios, as more specific and test-set
focused generalization corpora could be used to modify, enrich, or even replace the
original corpus.
2.3 PropBank
In this work we use the semantic roles defined in PropBank. The Proposition Bank
(Palmer, Gildea, and Kingsbury 2005) emerged as a primary resource for research in
SRL. It provides semantic role annotation for all verbs in the Penn Treebank corpus.
PropBank takes a ?theory-neutral? approach to the designation of core semantic roles.
Each verb has a frameset listing its allowed role labelings in which the arguments are
designated by number (starting from 0). Each numbered argument is provided with an
English language description specific to that verb. The most frequent roles are Arg0 and
Arg1 and, generally, Arg0 stands for the prototypical agent and Arg1 corresponds to the
prototypical patient or theme of the proposition. The rest of arguments (Arg2 to Arg5)
do not generalize across verbs, that is, they have verb specific interpretations.
Apart from the core numbered roles, there are 13 labels to designate adjuncts:
AM-ADV (general-purpose), AM-CAU (cause), AM-DIR (direction), AM-DIS (dis-
course marker), AM-EXT (extent), AM-LOC (location), AM-MNR (manner), AM-MOD
639
Computational Linguistics Volume 39, Number 3
Table 1
Example of verb-role lexical SP models for write, listed in alphabetical order. Number of heads
indicates the number of head words attested, Unique heads indicates the number of distinct
head words attested, and Examples lists some of the heads in alphabetical order.
Verb-role Number of Unique Examples
heads heads
write-Arg0 98 84 Angrist anyone baker ball bank Barlow Bates ...
write-Arg1 97 69 abstract act analysis article asset bill book ...
write-Arg2 7 7 bank commander hundred jaguar Kemp member ...
write-AM-LOC 2 2 paper space
write-AM-TMP 1 1 month
(modal verb), AM-NEG (negation marker), AM-PNC (purpose), AM-PRD (predication),
AM-REC (reciprocal), and AM-TMP (temporal).
3. Selectional Preference Models for Argument Classification
Our approach for applying selectional preferences to semantic role classification is
discriminative. That is, the SP-based models provide a score for every possible role
label given a verb (or preposition), the head word of the argument, and the selectional
preferences for the verb (or preposition). These scores can be used to directly assign the
most probable role or to codify new features to train enriched semantic role classifiers.
In this section we first present all the variants for acquiring selectional preferences
used in our study, and then present the method to apply them to semantic role classifi-
cation. We selected several variants that have been successful in some previous works.
3.1 Lexical SP Model
In order to implement the lexical model we gathered all heads w of arguments filling
a role r of a predicate p and obtained freq(p, r, w) from the corresponding training data
(cf. Section 4.1). Table 1 shows a sample of the heads of arguments attested in the
corpus for the verb write. The lexical SP model can be simply formalized as follows:
SPlex(p, r, w0) = freq(p, r, w0) (8)
3.2 WordNet-Based SP Models
We instantiated the model based on (Resnik 1993b) presented in the previous sec-
tion (SPRes, cf. Equation (3)) using the implementation of Agirre and Martinez (2001).
Tables 2 and 3 show the synsets5 that generalize best the head words in Table 1
for write-Arg0 and write-Arg1, according to the weight assigned to those synsets by
Equation (1). According to this model, and following basic intuition, the words attested
as being Arg0s of write are best generalized by semantic classes such as living things,
5 The WordNet terminology for concepts is synset. In this paper we use concept, synset, and semantic class
interchangeably.
640
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 2
Excerpt from the selectional preferences for write-Arg0 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00002086 5.875 life form organism being living thing any living entity
n#00001740 5.737 entity something anything having existence (living or nonliving)
n#00009457 4.782 object physical object a physical (tangible and visible) entity;
n#00004123 4.351 person individual someone somebody mortal human soul
a human being;
Table 3
Excerpt from the selectional preferences for write-Arg1 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00019671 7.956 communication something that is communicated between people
or groups
n#04949838 4.257 message content subject matter substance what a communication
that . . .
n#00018916 3.848 relation an abstraction belonging to or characteristic of two entities
n#00013018 3.574 abstraction a concept formed by extracting common features
from examples
entities, physical objects, and human beings, whereas Arg1s by communication, mes-
sage, relation, and abstraction.
Resnik?s method performs well among Wordnet-based methods, but we realized
that it tends to overgeneralize. For instance, in Table 2, the concept for ?entity? (one of
the unique beginners of the WordNet hierarchy) has a high weight. This means that a
head like ?grant? would be assigned Arg0. In fact, any noun which is under concept
n#00001740 (entity) but not under n#04949838 (message) would be assigned Arg0. This
observation led us to speculate on an alternative method which would try to generalize
as little as possible.
Our intuition is that general synsets can fit several selectional preferences at the
same time. For instance, the <entity> class, as a superclass of most words, would be a
correct generalization for the selectional preferences of all agent, patient, and instrument
roles of a predicate like break. On the contrary, specific concepts are usually more useful
for characterizing selectional preferences, as in the <tool> class for the instrument role
of break. The priority of using specific synsets over more general ones is, thus, justified
in the sense that they may better represent the most relevant semantic characteristics of
the selectional preferences.
The alternative method (SPwn) is based on the depth of the concepts in the WordNet
hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model
the specificity of concepts (the deeper the more specific) is not new (Rada et al 1989;
Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect
to generalization: When we check which SP is a better fit for a given target head, we
always prefer the SP that contains the most specific generalization for the target head
(the lowest synset which is a hypernym of the target word).
641
Computational Linguistics Volume 39, Number 3
Table 4
Excerpt from the selectional preferences for write-Arg0 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#01967203 9 1 humanoid human being any living or extinct member of the . . .
n#07603319 8 1 spy undercover agent a secret agent hired by a state to . . .
n#07151308 8 1 woman a human female who does housework
n#06183656 8 1 Federal Reserve the central bank of the US
Table 5
Excerpt from the selectional preferences for write-Arg1 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#05403815 13 1 information formal accusation of a crime
n#05401516 12 1 accusation accusal a formal charge of wrongdoing brought . . .
n#04925620 11 1 charge complaint a pleading describing some wrong or offense
n#04891230 11 1 memoir an account of the author?s personal experiences
More concretely, we model selectional preferences as a multiset6 of synsets, storing
all hypernyms of the heads seen in the training data for a certain role of a given
predicate, that is:
Smul(p, r) =
?
w?Seen(p,r)
hyp(w) (9)
where Seen(p, r) are all the argument heads for predicate p and role r, and hyp(w) returns
all the synsets and hypernyms of w, including hypernyms of hypernyms recursively up
to the top synsets.
For any given synset s, let d(s) be the depth of the synset in the WordNet hierarchy,
and let 1Smul(p,r)(s) be the multiplicity function which returns how many times s is con-
tained in the multiset Smul(p, r). We define a partial order among synsets a, b ? Smul(p, r)
as follows: ord(a) > ord(b) iff d(a) > d(b) or d(a) = d(b) ? 1Smul(p,r)(a) > 1Smul(p,r)(b).
Tables 4 and 5 show the most specific synsets (according to their depth) for write-Arg0
and write-Arg1.
We can then measure the goodness of fit of the selectional preference for a word as
the rank in the partial order of the first hypernym of the head that is also present in the
selectional preference. For that, we introduce SPwn(p, r, w), which following the previous
notation is defined as:
SPwn(p, r, w) = arg max
s?hyp(w)?Smul(p,r)
ord(s) (10)
6 Multisets are similar to sets, but allow for repeated members.
642
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 6
Most similar words for Texas and December according to Lin (1998).
Texas Florida 0.249, Arizona 0.236, California 0.231, Georgia 0.221, Kansas 0.217,
Minnesota 0.214, Missouri 0.214, Michigan 0.213, Colorado 0.208, North
Carolina 0.207, Oklahoma 0.207, Arkansas 0.205, Alabama 0.205, Nebraska
0.201, Tennessee 0.197, New Jersey 0.194, Illinois 0.189, Virginia 0.188,
Kentucky 0.188, Wisconsin 0.188, Massachusetts 0.184, New York 0.183
December June 0.341, October 0.340, November 0.333, April 0.330, February 0.329,
September 0.328, July 0.323, January 0.322, August 0.317, may 0.305, March
0.250, Spring 0.147, first quarter 0.135, mid-December 0.131, month 0.130,
second quarter 0.129, mid-November 0.128, fall 0.125, summer 0.125,
mid-October 0.121, autumn 0.121, year 0.121, third quarter 0.119
In case of ties, the role coming first in alphabetical order would be returned. Note that,
similar to the Resnik model (cf. Section 2.1), this model implicitly deals with the word
ambiguity problem.
As with any other approximation to measure specificity of concepts, the use of
depth has some issues, as some deeply rooted stray synsets would take priority. For
instance, Table 4 shows that synset n#01967203 for human being is the deepest synset. In
practice, when we search the synsets of a target word in the SPwn models following Eq.
(10), the most specific synsets (specially stray synsets) are not found, and synsets higher
in the hierarchy are used.
3.3 Distributional SP Models
All our distributional SP models are based on Equation (7). We have used several vari-
ants for sim(w0, w), as presented subsequently, but in all cases, we used the frequency
freq(p, r, w) as the weight in the equation. Given the availability of public resources for
distributional similarity, rather than implementing sim(w0, w) afresh we used (1) the pre-
compiled similarity measures by Lin (1998),7 and (2) the software for semantic spaces
by Pado? and Lapata (2007).
In the first case, Lin computed the similarity numbers for an extensive vocabulary
based on his own similarity formula (cf. Equation (6) in Figure 1) run over a large
parsed corpus comprising journalism texts from different sources: WSJ (24 million
words), San Jose Mercury (21 million words) and AP Newswire (19 million words).
The resource includes, for each word in the vocabulary, its most similar words with
the similarity weight. In order to get the similarity for two words, we can check the
entry in the thesaurus for either word. We will refer to this similarity measure as
simpreLin. Table 6 shows the most similar words for Texas and December according to this
resource.
For the second case, we applied the software to the British National Corpus to
extract co-occurrences, using the optimal parameters as described in Pado? and Lapata
(2007, page 179): word-based space, medium context, log-likelihood association, and
7 http://www.cs.ualberta.ca/?lindek/downloads.htm.
643
Computational Linguistics Volume 39, Number 3
Table 7
Summary of distributional similarity measures used in this work.
Similarity measure Source
simcos cosine BNC
simJac Jaccard BNC
simLin Lin BNC
simpreLin Lin Pre-computed
simpreLin?cos cosine (2nd order) Pre-computed
simpreLin?Jac Jaccard (2nd order) Pre-computed
2,000 basis elements. We tested Jaccard, cosine, and Lin?s measure for similarity, yielding
simJac, simcos, and simLin, respectively.
In addition to measuring the similarity of two words directly, that is, using the co-
occurrence vectors of each word as in Section 2, we also tried a variant which we will
call second-order similarity. In this case each word is represented by a vector which
contains all similar words with weights, where those weights come from first order
similarity. That is, in order to obtain the second-order vector for word w, we need to
compute its first order similarity with all other words in the vocabulary. The second-
order similarity of two words is then computed according to those vectors. For this, we
just need to change the definition of T and T in the similarity formulas in Figure 1: Now
T(w) would return the list of words which are taken to be similar to w, and T(w) would
return the same list but as a vector with weights.
This approximation is computationally expensive, as we need to compute the
square matrix of similarities for all word pairs in the vocabulary, which is highly time-
consuming. Fortunately, the pre-computed similarity scores of Lin (1998) (which use
simLin) are readily available, and thus the second-order similarity vectors can be easily
computed. We used Jaccard and cosine to compute the similarity of the vectors, and we
will refer to these similarity measures as simpreLin?Jac and sim
pre
Lin?cos hereinafter. Due to the
computational complexity, we did not compute second order similarity for the semantic
space software of Pado? and Lapata (2007).
Table 7 summarizes all similarity measures used in this study, and the corpus or
pre-computed similarity list used to build them.
3.4 Selectional Preferences for Prepositions
All the previously described models have been typically applied to verb-role selectional
preferences for NP arguments. Applying them to general semantic role labeling may
not be straightforward, however, and may require some extensions and adaptations.
For instance, not all argument candidates are noun phrases. Common arguments with
other syntactic types include prepositional, adjectival, adverbial, and verb phrases. Any
candidate argument without a nominal head cannot be directly treated by the models
described so far.
644
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 8
Example of prep-role lexical models for the preposition from, listed in alphabetical order.
Prep-role Number of Unique Examples
heads heads
from-Arg0 32 30 Abramson agency association barrier cut ...
from-Arg1 173 118 accident ad agency appraisal arbitrage ...
from-Arg2 708 457 academy account acquisition activity ad ...
from-Arg3 396 165 activity advertising agenda airport ...
from-Arg4 5 5 europe Golenbock system Vizcaya west
from-AM-ADV 19 17 action air air conception datum everyone ...
from-AM-CAU 5 4 air air design experience exposure
from-AM-DIR 79 71 agency alberta amendment america arson ...
from-AM-LOC 20 17 agency area asia body bureau orlando ...
from-AM-MNR 29 28 agency Carey company earnings floor ...
from-AM-TMP 33 21 april august beginning bell day dec. half ...
A particularly interesting case is that of prepositional phrases.8 Prepositions define
relations between the preposition attachment point and the preposition complement.
Prepositions are ambiguous with respect to these relations, which allows us to talk
about preposition senses. The Preposition Project (Litkowski and Hargraves 2005, 2006)
is an effort that produced a detailed sense inventory for English prepositions, which
was later used in a preposition sense disambiguation task at SemEval-2007 (Litkowski
and Hargraves 2007). Sense labels are defined as semantic relations, similar to those of
semantic role labels. In a more recent work, Srikumar and Roth (2011) presented a joint
model for extended semantic role labeling in which they show that determining the
sense of the preposition is mutually related to the task of labeling the argument role of
the prepositional phrase. Following the previous work, we also think that prepositions
define implicit selectional preferences, and thus decided to explore the use of preposi-
tional preferences with the aim of improving the selection of the appropriate semantic
roles. Addressing other arguments with non-nominal heads has been intentionally left
for further work.
The most straightforward way of including prepositional information in SP models
would be to add the preposition as an extra parameter of the SP. Initial experiments
revealed sparseness problems with collecting the ?verb, preposition, NP-head, role?
4-tuples from the training set. A simpler approach consists of completely disregarding
the verb information while collecting the prepositional preferences. That is, the selec-
tional preference for a preposition p and role r is defined as the union of all nouns w
found as heads of noun phrases embedded in prepositional phrases headed by p and
labeled with semantic role r. Then, one can apply any of the variants described in the
previous sections to calculate SP(p, r, w). Table 8 shows a sample of the lexical model for
the preposition from, organized according to the roles it plays.
These simple prep-role preferences largely avoided the sparseness problem while
still being able to capture relevant information to distinguish the appropriate roles in
many PP arguments. In particular, they proved to be relevant to distinguish between
adjuncts of the type ?[in New York]Location? vs. ?[in Winter]Temporal.? Nonetheless, we
8 Prepositional phrase is the second most frequent type of syntactic constituent for semantic arguments
(13%), after noun phrases (45%).
645
Computational Linguistics Volume 39, Number 3
are aware that not taking into account verb information also introduces some lim-
itations. In particular, the simplification could damage the performance on PP core
arguments, which are verb-dependent.9 For instance, our prepositional preferences
would not be able to suggest appropriate roles for the following two PP arguments:
?increase [ from seven cents a share]Arg3? and ?receive [ from the funds]Arg2,? because
the two head nouns (cents and funds) are semantically very similar. Assigning the
correct roles in these cases clearly depends on the information carried by the verbs.
Arg3 is the starting point for the predicate increase, whereas Arg2 refers to the source for
receive.
Our perspective on making this simple definition of prep-role SPs was practical and
just a starting point to play with the argument preferences introduced by prepositions.
A more complex model, distinguishing between prepositional phrases in adjunct and
core argument positions, should be able to model the linguistics better yet aleviate the
sparseness problem, and would hopefully produce better results.
The combination scheme for applying verb-role and prep-role is also very simple.
Depending on the syntactic type of the argument we apply one or the other model, both
in learning and testing:
 When the argument is a noun phrase, we use verb-role selectional
preferences.
 When the argument is a prepositional phrase, we use prep-role
selectional preferences.
We thus use a straightforward method to combine both kinds of SPs. More complex
possibilities like doing mixtures of both SPs are left for future work.
3.5 Role Classification with SP Models
Selectional preference models can be directly used to perform role classification. Given
a target predicate p and noun phrase candidate argument with head w, we simply select
the role r of the predicate which best fits the head according to the SP model. This
selection rule is formalized as:
ROLE(p, w) = arg max
r?Roles(p)
SP(p, r, w) (11)
with Roles(p) being the set of all roles applicable to the predicate p, and SP(p, r, w)
the goodness of fit of the selectional preference model for the head w, which can be
instantiated with all the variants mentioned in the previous subsections, including
the lexical model (Equation (8)) WordNet-based SP models (Equations (3) and (10)),
and distributional SP models (Equation (7)), using different similarity models as in
Table 7. Ties were broken returning the role coming first according to alphabetical
order. Note that in the case of SPwn (Equation 10) we need to use arg min rather than
arg max.
9 The percentage of prepositional phrases in core argument position is 48%, slightly lower than in adjunct
position (52%).
646
Zapirain et al Selectional Preferences for Semantic Role Classification
Note that if the candidate argument is a prepositional phrase with preposition p?
and embedded NP head word w, the classification rule uses the prep-role SP model,
that is:
ROLE(p, p?, w) = arg max
r?Roles(p? )
SP(p?, r, w)
4. Experiments with Selectional Preferences in Isolation
In this section we evaluate the ability of selectional preference models to discriminate
among different roles. For that, SP models will be used in isolation, according to the clas-
sification rule in Equation (11), to predict role labels for a set of (predicate, argument-head)
pairs. That is, we are interested in the discriminative power of the semantic information
carried by the SPs, factoring out any other feature commonly used by the state-of-the-
art SRL systems. The data sets used and the experimental results are presented in the
following.
4.1 Data Sets
The data used in this work are the benchmark corpus provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez 2005). The data set, of over 1 million tokens,
comprises PropBank Sections 02?21 for training, and Sections 24 and 23 for develop-
ment and testing, respectively. The Selectional Preferences implemented in this study
are not able to deal with non-nominal argument heads, such us those of NEG, DIS,
MOD (i.e., SPs never predict NEG, DIS, or MOD roles); but, in order to replicate the
same evaluation conditions of typical PropBank-based SRL experiments all arguments
are evaluated. That is, our SP models don?t return any prediction for those, and the
evaluation penalizes them accordingly.
The predicate?role?head triples (p, r, w) for generalizing the selectional preferences
are extracted from the arguments of the training set, yielding 71,240 triples, from which
5,587 different predicate-role selectional preferences (p, r) are derived by instantiating
the different models in Section 3. Tables 9 and 10 show additional statistics about some
of the most (and least) frequent verbs and prepositions in these tuples.
The test set contains 4,134 pairs (covering 505 different predicates) to be classified
into the appropriate role label. In order to study the behavior on out-of-domain data,
we also tested on the PropBanked part of the Brown corpus (Marcus et al 1994). This
corpus contains 2,932 (p, w) pairs covering 491 different predicates.
4.2 Results
The performance of each selectional preference model is evaluated by calculating
the customary precision (P), recall (R), and F1 measures.10 For all experiments re-
ported in this paper, we checked for statistical significance using bootstrap resampling
(100 samples) coupled with one-tailed paired t-test (Noreen 1989). We consider a result
significantly better than another if it passes this test at the 99% confidence interval.
10 P = Correct/Predicted ? 100, R = Correct/Gold ? 100, where Correct is the number of correct predictions,
Predicted is the number of predictions, and Gold is the total number of gold annotations.
F1 = 2PR/(P + R) is the harmonic mean of P and R.
647
Computational Linguistics Volume 39, Number 3
Table 9
Statistics of the three most and least frequent verbs in the training set. Role frame lists the types
of arguments seen in training for each verb; Heads indicates the total number of arguments for
the verb; Heads per role shows the average number of head words for each role; and Unique
heads per role lists the average number of unique head words for each verb?s role.
Verb Role frame Heads Heads Unique heads
per role per role
say Arg0,Arg1,Arg3,AM-ADV, AM-LOC, 7,488 1,069 371
AM-MNR, AM-TMP, AM-LOC,AM-MNR
have Arg0,Arg1,AM-ADV,AM-LOC 3,487 498 189
AM-MNR,AM-NEG,AM-TMP
make Arg0,Arg1,Arg2,AM-ADV 2,207 315 143
AM-LOC,AM-MNR,AM-TMP
... ... ... ... ...
accrete Arg1 1 1 1
accede Arg0 1 1 1
absolve Arg0 1 1 1
Table 10
Statistics of the three most and least frequent prepositions in the training set. Role frame lists
the types of arguments seen in training for each preposition; Heads indicates the total number
of arguments for the preposition; Heads per role shows the average number of head words for
each role; and Unique heads per role lists the average number of unique head words for each
preposition?s role.
Preposition Role frame Heads Heads Unique heads
per role per role
in Arg0,Arg1,Arg2,Arg3,Arg4,Arg5 6,859 403 81
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-NEG,
AM-PNC,AM-PRD,AM-TMP
to Arg0,Arg1,Arg2,Arg3,Arg4, 3,495 233 94
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-PNC,
AM-PRD,AM-TMP
for Arg0,Arg1,Arg2,Arg3,Arg4, 2,935 225 74
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-LOC,AM-MNR,AM-PNC,AM-TMP
... ... ... ... ...
beside Arg2, AM-LOC 2 1 1
atop Arg2, AM-DIR 2 1 1
aboard AM-LOC 1 1 1
Tables 11 and 12 list the results of the various selectional preference models in
isolation. Table 11 shows the results for verb-role SPs, and Table 12 lists the results
for the combination of verb-role and preposition-role SPs as described in Section 3.4.11
It is worth noting that the results of Tables 11 and 12 are calculated over exactly the
11 Note that the results reported here are not identical to those we reported in Zapirain, Agirre, and
Ma`rquez (2009). The differences are two-fold: (a) in our previous experiments we discarded roles such
as MOD, DIS, and NEG, whereas here we evaluate on all roles, and (b) our previous work used only the
subset of the data that could be mapped to VerbNet (around 50%), whereas here we inspect all tuples.
648
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 11
Results for verb-role SPs in the development partition of WSJ, the test partition of WSJ, and the
Brown corpus. For each experiment, we show precision (P), recall (R), and F1. Values in boldface
font are the highest in the corresponding column. F1 values marked with ? are significantly
lower than the highest F1 score in the same column.
Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 73.94 21.81 33.69? 70.75 26.66 39.43? 59.39 05.51 10.08?
SPRes 43.65 35.70 39.28? 45.07 37.11 40.71? 36.34 27.58 31.33?
SPwn 53.09 43.35 47.73? 55.44 45.58 50.03? 41.76 31.58 35.96?
SPsimLin 53.88 44.35 48.65? 52.27 45.13 48.66? 48.30 32.08 38.56?
SPsimJac 48.40 45.53 46.92? 48.85 46.38 47.58? 42.10 34.34 37.82?
SPsimcos 52.37 49.26 50.77? 53.13 50.44 51.75? 43.24 35.27 38.85?
SPsimpreLin
60.29 59.54 59.91 59.93 59.38 59.65 50.79 48.39 49.56
SPsimpreLin?Jac
60.56 56.97 58.71 61.76 58.63 60.16 51.97 42.39 46.69?
SPsimpreLin?cos
60.22 56.64 58.37 61.12 58.12 59.63 51.92 42.35 46.65?
Table 12
Results for combined verb-role and prep-role SPs in the development partition of WSJ, the test
partition of WSJ, and the Brown corpus. For each experiment, we show precision (P), recall (R),
and F1. Values in boldface font are the highest in the corresponding column. F1 values marked
with ? are significantly lower from the highest F1 score in the same column.
Preposition-role and Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 82.05 39.17 53.02? 82.98 43.77 57.31? 68.47 13.60 22.69?
SPRes 63.72 53.09 57.93? 63.47 53.24 57.91? 55.12 44.15 49.03?
SPwn 71.72 59.68 65.15? 65.70 63.88 64.78? 60.08 48.10 53.43?
SPsimLin 63.84 54.58 58.85? 63.75 56.40 59.85? 54.27 39.96 46.04?
SPsimJac 61.75 61.13 61.44? 61.83 61.40 61.61? 55.42 53.45 54.42?
SPsimcos 64.81 64.17 64.49? 64.67 64.22 64.44? 56.56 54.54 55.53?
SPsimpreLin
67.78 67.10 67.44? 68.34 67.87 68.10? 58.43 56.35 57.37?
SPsimpreLin?Jac
69.90 69.20 69.55 70.82 70.33 70.57 62.37 60.15 61.24
SPsimpreLin?cos
69.47 68.78 69.12 70.28 69.80 70.04 62.36 60.14 61.23
same example set. PP arguments are treated by the verb-role SPs by just ignoring the
preposition and considering the head noun of the NP immediately embedded in the PP.
It is worth mentioning that none of the SP models is able to predict the role when
facing a head word missing from the model. This is especially noticeable in the lexical
model, which can only return predictions for words seen in the training data and is
649
Computational Linguistics Volume 39, Number 3
penalized in recall. WordNet based models, which have a lower word coverage com-
pared to distributional similarity?based models, are also penalized in recall.
In both tables, the lexical row corresponds to the baseline lexical match method.
The following rows correspond to the WordNet-based selectional preference models.
The distributional models follow, including the results obtained by the three similarity
formulas on the co-occurrences extracted from the BNC (simJac, simcos simLin), and the
results obtained when using Lin?s pre-computed similarities directly (simpreLin) and as a
second-order vector (simpreLin?Jac and sim
pre
Lin?cos).
First and foremost, this experiment proves that splitting SPs into verb- and
preposition-role SPs yields better results. The comparison of Tables 11 and 12 shows
that the improvements are seen for both precision and recall, but especially remarkable
for recall. The overall F1 improvement is of up to 10 points. Unless stated otherwise, the
rest of the analysis will focus on Table 12.
As expected, the lexical baseline attains a very high precision in all data sets, which
underscores the importance of the lexical head word features in argument classification.
Its recall is quite low, however, especially in Brown, confirming and extending Pradhan,
Ward, and Martin (2008), who also report a similar performance drop for argument
classification on out-of-domain data. All our selectional preference models improve
over the lexical matching baseline in recall, with up to 24 absolute percentage points
in the WSJ test data set and 47 absolute percentage points in the Brown corpus. This
comes at the cost of reduced precision, but the overall F-score shows that all selectional
preference models are well above the baseline, with up to 13 absolute percentage
points on the WSJ data sets and 39 absolute percentage points on the Brown data set.
The results, thus, show that selectional preferences are indeed alleviating the lexical
sparseness problem.12
As an example, consider the following head words of potential arguments of the
verb wear found in the test set: doctor, men, tie, shoe. None of these nouns occurred as
heads of arguments of wear in the training data, and thus the lexical feature would
be unable to predict any role for them. Using selectional preferences, we successfully
assigned the A0 role to doctor and men, and the A1 role to tie and shoe.
Regarding the selectional preference variants, WordNet-based and first-order distri-
butional similarity models attain similar levels of precision, but the former have lower
recall and F1. The performance loss on recall can be explained by the limited lexical
coverage of WordNet when compared with automatically generated thesauri. Examples
of words missing in WordNet include abbreviations (e.g., Inc., Corp.) and brand names
(e.g., Texaco, Sony).
The comparison of the WordNet-based models indicates that our proposal for a
lighter method of WordNet-based selectional preference was successful, as our simpler
variant performs better than Resnik?s method. In manual analysis, we realized that
Resnik?s model tends to always predict the most frequent roles whereas our model
covers a wider role selection. Resnik?s tendency to overgeneralize makes more frequent
roles cover all the vocabulary, and the weighting system penalizes roles with fewer
occurrences.
12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SP
models on the subset of cases covered by both the lexical and the SP models. In this situation, if we aimed
at constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style of
Chambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one of
the SP models if not. As presented in Section 5, however, our main goal is to integrate these SP models
in a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment.
650
Zapirain et al Selectional Preferences for Semantic Role Classification
The results for distributional models indicate that the SPs using Lin?s ready-made
thesaurus (simpreLin) outperforms Pado? and Lapata?s distributional similarity model (Pado?
and Lapata 2007) calculated over the BNC (simLin) in both Tables 11 and 12. This might
be due to the larger size of the corpus used by Lin, but also by the fact that Lin used a
newspaper corpus, compared with the balanced BNC corpus. Further work would be
needed to be more conclusive, and, if successful, could improve further the results of
some SP models.
Among the three similarity metrics using Pado? and Lapata?s software, the cosine
seems to perform consistently better. Regarding the comparison between first-order and
second-order using pre-computed similarity models, the results indicate that second-
order is best when using both the verb-role and prep-role models (cf. Table 12), although
the results for verb-roles are mixed (cf. Table 11). Jaccard seems to provide slightly better
results than cosine for second-order vectors.
In summary, the use of separate verb-role and prep-role models produces the best
results, and second-order similarity is highly competitive. As far as we know, this is
the first time that prep-role models and second-order models are applied to selectional
preference modeling.
5. Semantic Role Classification Experiments
In this section we advance the use of SP in SRL one step further and show that selec-
tional preferences are able to effectively improve performance of a state-of-the-art SRL
system. More concretely, we integrate the information of selectional preference models
in a SRL system and show significant improvements in role classification, especially
when applied to out-of-domain corpora.13
We will use some of the selectional preference models presented in the previous
section. We will focus on the combination of verb-role and prep-role models. Regarding
the similarity models, we will choose the best two performing models from each of
the three families that we tried, namely, the two WordNet models, the two best models
based on the BNC corpus (simJac,simcos), and the two best models based on Lin?s precom-
puted similarity metrics (sim2Jac,sim
2
cos). We left the exploration of other combinations for
future work.
5.1 Integrating Selectional Preferences in Role Classification
For these experiments, we modified the SwiRL SRL system, a state-of-the-art semantic
role labeling system (Surdeanu et al 2007). SwiRL ranked second among the systems
that did not implement model combination at the CoNLL-2005 shared task and fifth
overall (Carreras and Ma`rquez 2005). Because the focus of this section is on role classi-
fication, we modified the SRC component of SwiRL to use gold argument boundaries,
that is, we assume that semantic role identification works perfectly. Nevertheless, for a
realistic evaluation, all the features in the role classification model are generated using
actual syntactic trees generated by the Charniak parser (Charniak 2000).
The key idea behind our approach is model combination: We generate a battery of
base models using all resources available and we combine their outputs using multi-
ple strategies. Our pool of base models contains 13 different models: The first is the
13 The data sets used for the experiments reported in this section are exactly the ones described in
Section 4.1.
651
Computational Linguistics Volume 39, Number 3
unmodified SwiRL SRC, the next six are the selected SP models from the previous
section, and the last six are variants of SwiRL SRC. In each variant, the feature set of
the unmodified SwiRL SRC model is extended with a single feature that models the
choice of a given SP, for example, SRC+SPres contains an extra feature that indicates the
choice of Resnik?s SP model.14
We combine the outputs of these base models using two different strategies: (a)
majority voting, which selects the label predicted by most models, and (b) meta-
classification, which uses a supervised model to learn the strengths of each base model.
For the meta-classification model, we opted for a binary classification approach: First,
for each constituent we generate n data points, one for each distinct role label proposed
by the pool of base models; then we use a binary meta-classifier to label each candidate
role as either correct or incorrect. We trained the meta-classifier on the usual PropBank
training partition, using 10-fold cross-validation to generate outputs for the base
models that require the same training material. At prediction time, for each candidate
constituent we selected the role label that was classified as correct with the highest
confidence.
The binary meta-classifier uses the following set of features:
 Labels proposed by the base models, for example, the feature SRC+SPres=Arg0
indicates that the SRC+SPres base model proposed the Arg0 label. We add
13 such features, one for each base model. Intuitively, this feature allows
the meta-classifier to learn the strengths of each base model with respect
to role labels: SRC+SPres should be trusted for the Arg0 role, and so on.
 Boolean value indicating agreement with the majority vote, for example, the
feature Majority=true indicates that the majority of the base models
proposed the same label as the one currently considered by the
meta-classifier.
 Number of base models that proposed this data point?s label. To reduce sparsity,
for each number of base models, N, we generate N distinct features
indicating that the number of base models that proposed this label is
larger than k, where k ? [0, N). For example, if two base models proposed
the label under consideration, we generate the following two features:
BaseModelNumber>0 and BaseModelNumber>1. This feature provides finer
control over the number of votes received by a label than the majority
voter, for example, the meta-classifier can learn to trust a label if more
than two base models proposed it, even if the majority vote disagrees.
 List of actual base models that proposed this data point?s label. We store a
distinct feature for each base model that proposed the current label, and
also a concatenation of all these base model names. The latter feature is
designed to allow the meta-classifier to learn preferences for certain
combinations of base models. For example, if two base models, SPres and
SPwn, proposed the label under consideration, we generate three features:
Base=SPres, Base=SPwn, and Base=SPres+SPwn.
14 Adding more than one SP output as a feature in SwiRL?s SRC model did not improve performance in
development over the single-SP SRC model. Our conjecture is that the large number of features in SRC
has the potential to drown the SP-based features. This may be accentuated when there are more SP-based
features because their signal is divided among them due to their overlap. We have also tried to add the
input features of the SP models directly to the SRC model but this also proved to be unsuccessful during
development.
652
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 13
Results for the combination approaches. Accuracy shows the overall results. Core and Adj
contain F1 results restricted to the core numbered roles and adjuncts, respectively. SRC is
SwiRL?s standalone SRC model; +SPx stands for the SRC model extended with a feature given by
the corresponding SP model. Values in boldface font are the highest in the corresponding
column. Accuracy values marked with ? are significantly lower than the highest accuracy score
in the same column.
WSJ test Brown test
Acc. Core F1 Adj. F1 Acc. Core F1 Adj. F1
SRC 90.83? 93.25 81.31 79.52 84.42 57.76
+SPRes 90.76? 93.17 81.08 79.86? 84.52 59.24
+SPwn 90.56? 92.88 81.11 79.73? 84.26 59.69
+SPsimJac 90.86? 93.37 80.30 79.83? 84.43 59.54
+SPsimcos 90.87? 93.33 80.92 80.50? 85.14 60.16
+SPsimpreLin?Jac
90.95? 93.03 82.75 80.75? 85.62 59.63
+SPsimpreLin?cos
91.23? 93.78 80.56 80.48? 84.95 61.01
Meta-classifier 92.43 94.62 84.00 81.94 86.25 63.36
Voting 92.36 94.57 83.68 82.15 86.37 63.78
5.2 Results for Semantic Role Classification
Table 13 compares the performance of both combination approaches against the stand-
alone SRC model. In the table, the SRC+SP? models stand for SRC classifiers enhanced
with one feature from the corresponding SP. The meta-classifier shown in the table com-
bines the output of all the 13 base models introduced previously. We implemented the
meta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomial
kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the results
of the voting strategy, over the same set of base models.
In the columns we show overall classification accuracy and F1 results for both core
arguments (Core) and adjunct arguments (Adj.). Note that for the overall SRC scores, we
report classification accuracy, defined as ratio of correct predictions over total number
of arguments to be classified. The reason for this is that the models in this section always
return a label for all arguments to be classified, and thus accuracy, precision, recall, and
F1 are all equal.
Table 13 indicates that four out of the six SRC+SP? models perform better than the
standalone SRC model in domain (WSJ), and all of them outperform SRC out of domain
(Brown). The improvements are small, however, and, generally, not statistically signifi-
cant. On the other hand, the meta-classifier outperforms the original SRC model both
in domain (17.4% relative error reduction; 1.60 points of accuracy improvement) and
out of domain (13.4% relative error reduction; 2.42 points of accuracy improvement),
and the differences are statistically significant. This experiment proves our claim that
SPs can be successfully used to improve semantic role classification. It also underscores
the fact that combining SRC and SPs is not trivial, however. Our hypothesis is that this
15 http://svmlight.joachims.org.
16 We have also trained the meta-classifier with other learning algorithms (e.g., logistic regression with
L2 regularization) and we obtained similar but slightly lower results.
653
Computational Linguistics Volume 39, Number 3
is caused by the large performance disparity (20 F1 points in domain and 18 out of
domain) between the original SRC model and the standalone SP methods.
Interestingly, the meta-classifier performs only marginally better than the voting ap-
proach in domain and slightly worse out of domain. We believe that this is another effect
of the above observation: Given the weaker SP-based features, the meta-classifier does
not learn much beyond a majority vote, which is exactly what the simpler, unsuper-
vised voting method models. Nevertheless, regardless of the combination method, this
experiment emphasizes that infusing SP information in the SRC task is beneficial.
Table 13 also shows that our approach yields consistent improvements for both
core and adjunct arguments. Out of domain, we see a bigger accuracy improvement
for adjunct arguments (6.02 absolute points) vs. core arguments (1.83 points, for the
voting model). This is to be expected, as most core arguments fall under the Arg0 and
Arg1 classes, which can typically be disambiguated based on syntactic information (i.e.,
subject vs. object). On the other hand, there are no syntactic hints for adjunct arguments,
so the system learns to rely more on SP information in this case.
Regarding the performance of individual combinations of SRC and SP methods
(e.g., SRC+SPRes), the differences among SP models in Table 13 are much smaller
than in Table 12. SPsimpreLin?cos and SPsim
pre
Lin?Jac
yield the best results in both cases, and
distributional methods are slightly stronger than WordNet-based methods. SPRes and
SPwn perform similarly when combined, with a small lead for Resnik?s method. The
smaller differences and changes in the rank among SP methods are due to the complex
interactions when combining SP models with the SRC system.
Table 14
Precision (P), recall (R), and F1 results per argument type for the standalone SRC model and
the meta-classifier, in the two test data sets (WSJ and Brown). Due to space limitations, the
AM- prefix has been dropped from the labels of all adjuncts. When classifying all arguments
(last row), the F1 score is an accuracy score because in this scenario P = R = F1. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SRC Meta-classifier SRC Meta-classifier
P R F1 P R F1 P R F1 P R F1
Arg0 93.6 96.7 95.1 95.1 97.4 96.2 87.6 89.3 88.4 89.4 91.0 90.2
Arg1 93.3 94.5 93.9 94.2 95.7 95.0 84.3 90.6 87.3 86.2 91.9 89.0
Arg2 86.0 82.6 84.3 87.8 87.4 87.6 52.7 56.8 54.7 55.9 59.9 57.8
Arg3 77.6 63.4 69.8 82.4 68.3 74.7 36.4 19.0 25.0 45.8 26.2 33.3
Arg4 86.8 78.6 82.5 89.5 81.0 85.0 59.4 34.5 43.7 67.9 34.5 45.8
Core 92.9 93.6 93.3 94.2 95.1 94.6 82.6 86.3 84.4 84.6 87.9 86.3
ADV 58.5 51.4 54.7 64.4 52.3 57.7 45.1 24.3 31.6 51.9 25.7 34.4
CAU 61.1 71.0 65.7 80.0 77.4 78.7 64.7 45.8 53.7 84.6 45.8 59.5
DIR 46.2 25.0 32.4 68.8 45.8 55.0 64.7 45.8 53.7 73.9 44.5 55.6
DIS 84.3 82.7 83.5 95.6 82.7 88.7 52.6 27.0 35.7 54.5 32.4 40.7
EXT 50.0 12.5 20.0 50.0 12.5 20.0 0.0 0.0 0.0 0.0 0.0 0.0
LOC 85.2 80.9 83.0 85.0 84.7 84.8 67.8 61.2 64.3 68.3 68.7 68.5
MNR 55.8 54.1 55.0 68.9 61.7 65.1 47.4 38.9 42.7 59.2 49.3 53.8
PNC 51.9 37.8 43.8 62.5 40.5 49.2 51.7 39.5 44.8 53.3 42.1 47.1
TMP 93.6 95.9 94.7 92.8 95.9 94.4 79.0 78.1 78.5 84.1 83.2 83.7
Adj 83.1 79.6 81.3 86.2 81.9 84.0 64.9 52.1 57.8 69.8 58.0 63.4
All ? ? 90.8? ? ? 92.4 ? ? 79.5? ? ? 81.9
654
Zapirain et al Selectional Preferences for Semantic Role Classification
Lastly, Table 14 shows a breakdown of the results by argument type for the orig-
inal SRC model and the meta-classifier (results are also presented over all numbered
arguments, Core, adjuncts, and Adj). This comparison emphasizes the previous obser-
vation that SPs are more useful for arguments that are independent of syntax than for
arguments that are usually tied to certain syntactic constructs (i.e., Arg0 and Arg1). For
example, in domain the meta-classifier improves Arg0 classification with 1.1 F1 points,
but it boosts the classification performance for causative arguments (AM-CAU) with 13
absolute points. A similar behavior is observed out of domain. For example, whereas
Arg0 classification is improved with 1.7 points, the classification of manner arguments
(AM-MNR) is improved by 11 points. All in all, with two exceptions, selectional prefer-
ences improve classification accuracy for all argument types, both in and out of domain.
The previous experiments showed that a meta-classifier (and a voting approach)
over a battery of base models improves over the performance of each individual clas-
sifier. Given that half of our base models are all relatively minor changes of the same
original classifier (SwiRL), however, it would be desirable to ensure that the overall
performance gain of the meta-classification system is due to the infusion of semantic
information that is missing in the baseline SRC, and not to a regularization effect coming
from the ensemble of classifiers. The qualitative analysis presented in Section 6 will
reinforce this hypothesis.
5.3 Results for End-to-End Semantic Role Labeling
Lastly, we investigate the contribution of SPs in an end-to-end SRL system. As discussed
before, our approach focuses on argument classification, a subtask of complete SRL,
because this component suffers in the presence of lexical data sparseness (Pradhan,
Ward, and Martin 2008). To understand the impact of SPs on the complete SRL task we
compared two SwiRL models: one that uses the original classification model (the SRC
line in Table 13) and another that uses our meta-classifier model (the Meta-classifier
line in Table 13). To implement this experiment we had to modify the publicly down-
loadable SwiRL model, which performs identification and classification jointly, using a
single multi-class model. We changed this framework to a pipeline model, which first
performs argument identification (i.e., is this constituent an argument or not?), followed
by argument classification (i.e., knowing that this constituent is an argument, what is
its label?).17 We used the same set of features as the original SwiRL system and the
original model to identify argument boundaries. This pipeline model allowed us to
easily plug in different classification models, which offers a simple platform to evaluate
the contribution of SPs in an end-to-end SRL system.
Table 15 compares the original SwiRL pipeline (SwiRL in the table) with the pipeline
model where the classification component was replaced with the meta-classifier previ-
ously introduced (SwiRL w/ meta). The latter model backs off to the original classifi-
cation model for candidates that are not covered by our current selectional preferences
(i.e., are not noun phrases or prepositional phrases containing a noun phrase as the
second child). We report results for the test partitions of WSJ and Brown in the same
table. Note that these results are not directly comparable with the results in Tables 13
and 14, because in those initial experiments we used gold argument boundaries whereas
17 This pipeline model performs slightly worse than the original SwiRL on the WSJ data and slightly better
on Brown.
655
Computational Linguistics Volume 39, Number 3
Table 15
Precision (P), recall (R), and F1 results per argument for the end-to-end semantic role labeling
task. We compared two models: the original SwiRL model and the one where the classification
component was replaced with the meta-classifier introduced at the beginning of the section. We
used the official CoNLL-2005 shared-task scorer to produce these results. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SwiRL SwiRL w/ meta SwiRL SwiRL w/ meta
P R F1 P R F1 P R F1 P R F1
Arg0 87.0 81.6 84.2 87.8 81.9 84.8 86.6 81.3 83.9 87.3 81.7 84.4
Arg1 79.1 71.8 75.3 79.4 72.1 75.6 70.2 64.6 67.3 71.1 65.2 68.0
Arg2 70.0 56.6 62.6 69.2 58.3 63.3 41.8 42.7 42.2 42.3 44.6 43.4
Arg3 72.4 43.9 54.7 72.6 44.5 55.2 36.4 12.9 19.0 34.6 14.5 20.5
Arg4 73.3 61.8 67.0 73.8 60.8 66.7 48.8 25.6 33.6 44.4 25.6 32.5
ADV 59.4 50.6 54.6 59.5 50.0 54.4 49.0 38.2 42.9 49.9 38.5 43.5
CAU 61.5 43.8 51.2 66.0 45.2 53.7 58.7 35.5 44.3 59.1 34.2 43.3
DIR 44.7 20.0 27.6 50.0 22.6 30.9 59.0 27.2 37.2 61.3 25.9 36.5
DIS 76.1 63.8 69.4 77.0 63.8 69.7 58.8 41.0 48.3 59.7 41.3 48.9
EXT 72.7 50.0 59.3 72.7 50.0 59.3 20.0 8.1 11.5 21.4 8.1 11.8
LOC 64.7 52.9 58.2 64.8 55.4 59.7 48.3 37.7 42.3 46.8 40.5 43.5
MNR 59.1 52.0 55.3 61.4 51.7 56.2 53.8 47.3 50.3 55.9 48.3 51.8
PNC 47.1 34.8 40.0 46.4 33.9 39.2 51.8 26.4 35.0 52.4 26.7 35.1
TMP 78.7 71.4 74.9 78.4 71.5 73.8 59.7 60.6 60.2 61.0 61.2 61.1
All 79.7 70.9 75.0? 80.0 71.3 75.4 71.8 64.2 67.8? 72.4 64.6 68.4
Table 15 shows results for an end-to-end model, which includes predicted argument
boundaries.
Table 15 shows that the use of selectional preferences improves overall results when
using predicted argument boundaries as well. Selectional preferences improve F1 scores
for four out of five core arguments in both WSJ and Brown, for six out of nine modifier
arguments in WSJ, and for seven out of nine modifier arguments in Brown. Notably, the
SPs improve results for the most common argument types (Arg0 and Arg1). All in all,
SPs yield a 0.4 F1 point improvement in WSJ and 0.6 F1 point improvement in Brown.
These improvements are small but they are statistically significant. We consider these re-
sults encouraging, especially considering that only a small percentage of arguments are
actually inspected by selectional preferences. This analysis is summarized in Table 16,
which lists how many argument candidates are inspected by the system in its different
stages. The table indicates that the vast majority of argument candidates are filtered
out by the argument identification component, which does not use SPs. Because of this,
even though approximately 50% of the role classification decisions can be reinforced
with SPs, only 4.5% and 3.6% of the total number of argument candidates in WSJ and
Brown, respectively, are actually inspected by the classification model that uses SPs.
6. Analysis and Discussion
We conducted a complementary manual analysis to further verify the usefulness of the
semantic information provided by the selectional preferences. We manually inspected
100 randomly selected classification cases, 50 examples in which the meta-classifier is
656
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 16
Counts for argument candidates for the two test partitions on the end-to-end semantic role
labeling task. The Predicted non-arguments line indicates how many candidate arguments are
classified as non-arguments by the argument identification classifier. The Incompatible with SPs
line indicates how many candidates were classified as arguments but cannot be modeled by our
current SPs (i.e., they are not noun phrases or prepositional phrases containing a noun phrase as
the second child). Lastly, the Compatible with SPs line lists how many candidates were both
classified as likely arguments and can be modeled by the SPs.
WSJ test Brown test
Predicted non-arguments 158,310 184,958
Incompatible with SPs 5,739 11,167
Compatible with SPs 7,691 7,867
Total 171,740 203,992
correct and the baseline SRC (SwiRL) is wrong, and 50 where the meta-classifier chooses
the incorrect classifier and the SRC is right. Interestingly, we observed that the majority
of cases have a clear linguistic interpretation, shedding light on the reasons why the
meta-classifier using SP information manages to correct some erroneous predictions of
the original SRC model, but also on the limitations of selectional preferences.
Regarding the success of the meta-classifier, the studied cases generally correspond
to low frequency verb?argument head pairs, in which the baseline SRC might have
had problems with generalization. In 29 of the cases (?58%), the syntactic information
is not enough to disambiguate the proper role, tends to indicate a wrong role label,
or it confuses the SRC because it contains errors. Most of the semantically based SP
predictions are correct, however, so the meta-classifier does select the correct role label.
In another 15 cases (?30%) the source of the baseline SRC error is not clear, but still,
several SP models suggest the correct role, giving the opportunity to the meta-classifier
to make the right choice. Finally, in the remaining six cases (?12%) a ?chance effect? is
observed: The failure of the baseline SRC model does not have a clear interpretation and,
moreover, most SP predictions are actually wrong. In these situations, several labels are
predicted with the same confidence, and the meta-classifier selects the correct one by
chance.
Figure 2 shows four real examples in which we see the importance of the infor-
mation provided by the selectional preferences. In example (a), the verb flash never
occurs in training with the argument head word news. The syntactic structure alone
strongly suggests Arg0, because the argument is an NP just to the left of a verb in active
form. This is probably why the baseline SRC incorrectly predicts Arg0. Some semantic
information is needed to know that the word news is not the agent of the predicate
(Arg0), but rather the theme (thing shining, Arg1). Selectional preferences make this
work perfectly, because all variants predict the correct label by signaling that news is
much more compatible with flash in Arg1 position rather than Arg0.
In example (b), the predicate promise expects a person as Arg1 (person promised to,
Recipient) and an action as Arg2 (promised action, Theme). Moreover, the presence of
Arg2 is obligatory. The syntactic structure is correct but does not provide the semantic
(Arg1 should be a person) or structural information (the assignment of Arg1 would have
required an additional Arg2) needed to select the appropriate role. SwiRL does not have
it either, and it assigns the incorrect Arg1 label. Most SP models correctly predict that
investigation is more similar to the heads of Arg2 arguments of promise than to the
heads of Arg1 arguments, however.
657
Computational Linguistics Volume 39, Number 3
(a) Several traders could be seen shaking their heads when (([the news]Arg0?Arg1)NP
( flashed)VP)S .
(b) Italian President Francesco Cossiga (promised ([a quick investigation into
whether Olivetti broke Cocom rules]Arg1?Arg2)NP)VP.
(c) Annual payments (will more than double ([from (a year ago)NP]TMP?Arg3)PP to
about $240 million ? ? ? )VP ? ? ?
(d) Procter & Gamble Co. plans to (begin ((testing (next month)NP)VP)S ([a superco.
detergent that ? ? ? washload]Arg0?Arg1)NP)VP .
Figure 2
Examples of incorrect SwiRL role assignments fixed by the meta-classifier. In each sentence, the
verb is emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the incorrect label x
assigned by the baseline SwiRL model is corrected into role label y by the combined system.
Finally, examples also contain simplified syntactic annotations from the test set predicted
syntactic layer, which are used for the discussion in the text.
In example (c) we see the application of prep-role selectional preferences. In that
sentence, the baseline SRC is likely confused by the content word feature of the PP
?from a year ago? (Surdeanu et al 2003). In PropBank, ?year? is a strong indicator
of a temporal adjunct (AM-TMP). The predicate double, however, describes the Arg3
argument as ?starting point? of the action and it is usually introduced by the preposition
from. This is very common also for other motion verbs (go, rise, etc.), resulting in the
from-Arg3 selectional preference containing a number of heads of temporal expressions,
in particular many more instances of the word year than the from-AM-TMP selectional
preference. As a consequence, the majority of SP models predict the correct Arg3 label.
Finally, example (d) highlights that selectional preferences increase robustness in
front of parsing errors. In this example, the NP ?a superco. detergent? is incorrectly
attached to ?begin? instead of the predicate testing by the syntactic parser. This produces
many incorrect features derived from syntax (syntactic frame, path, etc.) that may con-
fuse the baseline SRC model, which ends up producing an incorrect Arg0 assignment.
Most of the SP models, however, predict that detergent is not a plausible Agent for test
(?examiner?), but instead it fits best with the Arg1 position (?examined?).
Nevertheless, selectional preferences have a significant limitation: They do not
model syntactic structures, which often give strong hints for classification. In fact, the
vast majority of the situations where the meta-classifier performs worse than the origi-
nal SRC model are cases that are syntax-driven, hence situations that are incompletely
addressed by the current SP models. Even though the SRC and the SRC+SP models
have features that model syntax, they can be overwhelmed by the SP features and
standalone models, which leads to incorrect meta-classification results. Figure 3 shows a
few representative examples in this category. In the first example in the figure, the meta-
classifier changes the correctly assigned label Arg2 to Arg1, because most SP models
favor the Arg1 label for the argument ?test.? In the PropBank training corpus, however,
the argument following the verb fail is labeled Arg2 in 79% of the cases. Because the
SP models do not take into account syntax or positional information, this syntactic
preference is lost. Similarly, SPs do not model the fact that the verb buy is seldom
preceded by an Arg1 argument, or the argument immediately following the verb precede
tends to be Arg1, hence the incorrect classifications in Figure 3 (b) and (c). All these
658
Zapirain et al Selectional Preferences for Semantic Role Classification
(a) Some ?circuit breakers? installed after the October 1987 crash (failed ([their first
test ]Arg2?Arg1)NP)VP...
(b) Many fund managers argue that now?s ([the time]TMP?Arg1)NP (to buy)VP)S .
(c) Telephone volume was up sharply, but it was still at just half the level of the
weekend (preceding ([Black Monday ]Arg1?TMP)NP)VP .
Figure 3
Examples of incorrect assignments by the meta-classifier. In each sentence, the verb is
emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the correct
x label assigned by the baseline model is wrongly converted into y by the meta-classifier.
As in Figure 2, examples also contain simplified syntactic annotations taken from the test
set predicted syntactic layer.
examples are strong motivation for SP models that model both lexical and syntactic
preferences. We will address such models in future work.
7. Conclusions
Current systems usually perform SRL in two pipelined steps: argument identification
and argument classification. Whereas identification is mostly syntactic, classification
requires semantic knowledge to be taken into account. In this article we have shown
that the lexical heads seen in training data are too sparse to assign the correct role,
and that selectional preferences are able to generalize those lexical heads. In fact, we
show for the first time that the combination of the predictions of several selectional
preference models with a state-of-the-art SRC system yields significant improvements in
both in-domain and out-of-domain test sets. These improvements to role classification
translate into small but statistically significant improvements in an end-to-end semantic
role labeling system. We find these results encouraging considering that in the complete
semantic role labeling task only a small percentage of argument candidates are affected
by our modified role classification model. The experiments were carried out over the
well-known CoNLL-2005 data set, based on PropBank.
We applied several selectional preference models, based on WordNet and distribu-
tional similarity. Our experiments show that all models outperform the pure lexical
matching approach, with distributional methods performing better that WordNet-based
methods, and second-order similarity models being the best. In addition to the tradi-
tional selectional preferences for verbs, we introduce the use of selectional preferences
for prepositions, which are applied to classifying prepositional phrases. The combi-
nation of both types of selectional preferences improves over the use of selectional
preferences for verbs alone.
The analysis performed over the cases where the base SRC system and the com-
bined system differed showed that the selectional preferences are specially helpful when
syntactic information is either incorrect or insufficient to disambiguate the correct role.
The analysis also highlighted that the limitations of selectional preferences for modeling
syntactic structures introduce some errors in the combined model. Those errors could
be addressed if the SP models included some syntactic information.
Our research leaves the door open for tighter integration of semantic and syntactic
information for Semantic Role Labeling. We introduced selectional preferences in the
SRC system as simple features, but models which extend syntactic structures with
659
Computational Linguistics Volume 39, Number 3
selectional preferences (or vice versa) could overcome some of the errors that our system
introduced. Extending the use of selectional preferences to other syntactic types beyond
noun phrases and prepositional phrases would be also of interest. In addition, the
method for combining selectional preferences for verbs and prepositions was naive,
and we expect that a joint model of verb and preposition preferences for prepositional
phrases would improve results further. Finally, individual selectional preference meth-
ods could be improved and newer methods incorporated, which could further improve
the results.
Acknowledgments
The authors would like to thank the three
anonymous reviewers for their detailed
and insightful comments on the submitted
version of this manuscript, which helped
us to improve it significantly in this revision.
This work was partially funded by
the Spanish Ministry of Science and
Innovation through the projects OpenMT-2
(TIN2009-14675-C03) and KNOW2
(TIN2009-14715-C04-04). It also received
financial support from the Seventh
Framework Programme of the EU
(FP7/2007- 2013) under grant agreements
247762 (FAUST) and 247914 (MOLTO).
Mihai Surdeanu was supported by the Air
Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion
or recommendations expressed in this
material are those of the authors and do
not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
References
Agirre, Eneko, Timothy Baldwin, and
David Martinez. 2008. Improving
parsing and PP attachment performance
with sense information. In Proceedings
of ACL-08: HLT, pages 317?325,
Columbus, OH.
Agirre, Eneko, Kepa Bengoetxea, Koldo
Gojenola, and Joakim Nivre. 2011.
Improving dependency parsing with
semantic classes. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 699?703,
Portland, OR.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the 2001
Workshop on Computational Natural
Language Learning (CoNLL-2001),
pages 1?8, Toulouse.
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings of the
16th Conference on Computational
Linguistics - Volume 1, COLING ?96,
pages 16?22, Stroudsburg, PA.
Baroni, Marco and Alessandro Lenci.
2010. Distributional memory: A general
framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59?68,
Honolulu, HI.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1,364?1,371,
Las Palmas de Gran Canaria.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 10th
Conference of the European Chapter of the
Association of Computational Linguistics
(EACL-2003), pages 27?34, Budapest.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004
Shared Task: Semantic Role Labeling.
In Proceedings of the Eighth Conference
on Computational Natural Language
Learning (CoNLL-2004), pages 89?97,
Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling.
In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Chakraborti, Sutanu, Nirmalie Wiratunga,
Robert Lothian, and Stuart Watt. 2007.
Acquiring word similarities with higher
order association mining. In Proceedings
of the 7th International Conference on
Case-Based Reasoning: Case-Based Reasoning
Research and Development, ICCBR ?07,
pages 61?76, Berlin.
660
Zapirain et al Selectional Preferences for Semantic Role Classification
Chambers, Nathanael and Daniel Jurafsky.
2010. Improving the use of pseudo-words
for evaluating selectional preferences. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 445?453, Uppsala, Sweden.
Charniak, E. 2000. A maximum-entropy
inspired parser. In Proceedings of the 1st
Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-2000), pages 132?139,
Seattle, WA.
Clark, Stephen and Stephen Weir. 2002.
Class-based probability estimation
using a semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Edmonds, Philip. 1997. Choosing the word
most typical in context using a lexical
co-occurrence network. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and Eighth
Conference of the European Chapter of the
Association for Computational Linguistics,
ACL ?98, pages 507?509, Stroudsburg, PA.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics
(ACL-2007), pages 216?223, Prague.
Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.
2010. A flexible, corpus-driven model of
regular and inverse selectional preferences.
Computational Linguistics, 36(4):723?763.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. FrameNet and representing
the link between semantic and syntactic
relations. In Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grefenstette, Gregory. 1992. Sextant:
Exploring unexplored contexts for
semantic extraction from syntactic
analysis. In ACL?92, pages 324?326,
Newark, DE.
Higashinaka, Ryuichiro and Hideki Isozaki.
2008. Corpus-based question answering
for why-questions. In Proceedings of the
Third International Joint Conference on
Natural Language Processing (IJCNLP),
pages 418?425, Hyderabad.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-1990), pages 268?275, Pittsburgh, PA.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings
of ACL-08: HLT, pages 595?603,
Columbus, OH.
Lee, Lillian. 1999. Measures of distributional
similarity. In 37th Annual Meeting of the
Association for Computational Linguistics,
pages 25?32, College Park, MD.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL-1998), pages 768?774,
Montreal.
Litkowski, K. C. and O. Hargraves. 2005. The
preposition project. In Proceedings of the
ACL-SIGSEM Workshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistic Formalisms and
Applications, pages 171?179, Colchester.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Litkowski, Ken and Orin Hargraves.
2006. Coverage and inheritance in the
preposition project. In Prepositions ?06:
Proceedings of the Third ACL-SIGSEM
Workshop on Prepositions, pages 37?44,
Trento.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In Proceedings of the Workshop on
Human Language Technology (HLT-94),
pages 114?119, Plainsboro, NJ.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: An introduction to
the special issue. Computational Linguistics,
34(2):145?159.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguisties, 29:639?654.
Melli, Gabor, Yang Wang, Yudong Liu,
Mehdi M. Kashani, Zhongmin Shi,
661
Computational Linguistics Volume 39, Number 3
Baohua Gu, Anoop Sarkar, and Fred
Popowich. 2005. Description of SQUASH,
the SFU question answering summary
handler for the DUC-2005 summarization
task. In Proceedings of Document
Understanding Workshop, HLT/EMNLP
Annual Meeting, Vancouver.
Moschitti, Alessandro, Silvia Quarteroni,
Roberto Basili, and Suresh Manandhar.
2007. Exploiting syntactic and shallow
semantic kernels for question/answer
classification. In Proceedings of the
45th Conference of the Association for
Computational Linguistics (ACL),
pages 776?783, Prague.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva.
Noreen, E. W. 1989. Computer-Intensive
Methods for Testing Hypotheses:
An Introduction, Wiley.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 435?444, Uppsala.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pado?, Sebastian, Ulrike Pado?, and Katrin Erk.
2007. Flexible, corpus-based modelling
of human plausibility judgements. In
Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL-2007),
pages 400?409, Prague.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The proposition bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences.
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 564?571, Rochester, NY.
Pantel, Patrick and Dekang Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In Proceedings of the 38th
Annual Conference of the Association of
Computational Linguistics (ACL-2000),
pages 101?108, Hong Kong.
Pradhan, S., W. Ward, and J. H. Martin. 2008.
Towards robust semantic role labeling.
Computational Linguistics, 34(2):289?310.
Rada, R., H. Mili, E. Bicknell, and M. Blettner.
1989. Development and application of a
metric on semantic nets. IEEE Transactions
on Systems, Man, and Cybernetics,
19(1):17?30.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning (CoNLL-2009),
pages 147?155, Boulder, CO.
Resnik, Philip. 1993a. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Resnik, Philip. 1993b. Semantic classes and
syntactic ambiguity. In Proceedings of the
Workshop on Human Language Technology,
pages 278?283, Morristown, NJ.
Ritter, Alan, Mausam, and Oren Etzioni.
2010. A latent Dirichlet alocation method
for selectional preferences. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 424?434,
Uppsala.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Srikumar, V. and D. Roth. 2011. A joint
model for extended semantic role labeling.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 129?139, Edinburgh.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using predicate-
argument structures for information
extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL-2003), pages 8?15,
Sapporo.
Surdeanu, Mihai, Massimiliano Ciaramita,
and Hugo Zaragoza. 2011. Learning to
rank answers to non-factoid questions
from Web collections. Computational
Linguistics, 37(2):351?383.
Surdeanu, Mihai, Llu??s Ma`rquez, Xavier
Carreras, and Pere R. Comas. 2007.
Combination strategies for semantic role
labeling. Journal of Artificial Intelligence
Research (JAIR), 29:105?151.
Sussna, Michael. 1993. Word sense
disambiguation for free-text indexing
using a massive semantic network. In
Proceedings of the Second International
662
Zapirain et al Selectional Preferences for Semantic Role Classification
Conference on Information and Knowledge
Management, CIKM ?93, pages 67?74,
New York, NY.
Wilks, Yorick. 1975. Preference semantics.
In E. L. Kaenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, MA, pages 329?348.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th International Joint
Conference on Natural Language Processing
(ACL-IJCNLP-2009), pages 73?76, Suntec.
Zapirain, Ben?at, Eneko Agirre, Llu??s
Ma`rquez, and Mihai Surdeanu. 2010.
Improving semantic role classification
with selectional preferences. In Proceedings
of the 11th Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL HLT
2010), pages 373?376, Los Angeles, CA.
663

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373?376,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Semantic Role Classification with Selectional Preferences
Ben?at Zapirain, Eneko Agirre
IXA NLP Group
Basque Country Univ.
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical Univ. of Catalonia
lluism@lsi.upc.edu
Mihai Surdeanu
Stanford NLP Group
Stanford Univ.
mihais@stanford.edu
Abstract
This work incorporates Selectional Prefer-
ences (SP) into a Semantic Role (SR) Clas-
sification system. We learn separate selec-
tional preferences for noun phrases and prepo-
sitional phrases and we integrate them in a
state-of-the-art SR classification system both
in the form of features and individual class
predictors. We show that the inclusion of the
refined SPs yields statistically significant im-
provements on both in domain and out of do-
main data (14.07% and 11.67% error reduc-
tion, respectively). The key factor for success
is the combination of several SP methods with
the original classification model using meta-
classification.
1 Introduction
Semantic Role Labeling (SRL) is the process of
extracting simple event structures, i.e., ?who? did
?what? to ?whom?, ?when? and ?where?. Current
systems usually perform SRL in two pipelined steps:
argument identification and argument classification.
While identification is mostly syntactic, classifica-
tion requires semantic knowledge to be taken into
account. Semantic information is usually captured
through lexicalized features on the predicate and the
head?word of the argument to be classified. Since
lexical features tend to be sparse, SRL systems are
prone to overfit the training data and generalize
poorly to new corpora.
Indeed, the SRL evaluation exercises at CoNLL-
2004 and 2005 (Carreras and Ma`rquez, 2005) ob-
served that all systems showed a significant perfor-
mance degradation (?10 F1 points) when applied to
test data from a different genre of that of the training
set. Pradhan et al (2008) showed that this perfor-
mance degradation is essentially caused by the argu-
ment classification subtask, and suggested the lexi-
cal data sparseness as one of the main reasons. The
same authors studied the contribution of the different
feature types in SRL and concluded that the lexical
features were the most salient features in argument
classification (Pradhan et al, 2007).
In recent work, we showed (Zapirain et al, 2009)
how automatically generated selectional preferences
(SP) for verbs were able to perform better than pure
lexical features in a role classification experiment,
disconnected from a full-fledged SRL system. SPs
introduce semantic generalizations on the type of ar-
guments preferred by the predicates and, thus, they
are expected to improve results on infrequent and
unknown words. The positive effect was especially
relevant for out-of-domain data. In this paper we ad-
vance (Zapirain et al, 2009) in two directions:
(1) We learn separate SPs for prepositions and verbs,
showing improvement over using SPs for verbs
alone.
(2) We integrate the information of several SP mod-
els in a state-of-the-art SRL system (SwiRL1) and
show significant improvements in SR classifica-
tion. The key for the improvement lies in a meta-
classifier, trained to select among the predictions
provided by several role classification models.
2 SPs for SR Classification
SPs have been widely believed to be an impor-
tant knowledge source when parsing and perform-
ing SRL, especially role classification. Still, present
parsers and SRL systems use just lexical features,
which can be seen as the most simple form of SP,
1http://www.surdeanu.name/mihai/swirl/
373
where the headword needs to be seen in the training
data, and otherwise the SP is not satisfied. Gildea
and Jurafsky (2002) showed barely significant im-
provements in semantic role classification of NPs
for FrameNet roles using distributional clusters. In
(Erk, 2007) a number of SP models are tested in
a pseudo-task related to SRL. More recently, we
showed (Zapirain et al, 2009) that several methods
to automatically generate SPs generalize well and
outperform lexical match in a large dataset for se-
mantic role classification, but the impact on a full
system was not explored.
In this work we apply a subset of the SP meth-
ods proposed in (Zapirain et al, 2009). These meth-
ods can be split in two main families, depending on
the resource used to compute similarity: WordNet-
based methods and distributional methods. Both
families define a similarity score between a word
(the headword of the argument to be classified) and a
set of words (the headwords of arguments of a given
role).
WordNet-based similarity: One of the models
that we used is based on Resnik?s similarity mea-
sure (1993), referring to it as res. The other model is
an in-house method (Zapirain et al, 2009), referred
as wn, which only takes into account the depth of
the most common ancestor, and returns SPs that are
as specific as possible.
Distributional similarity: Following (Zapirain et
al., 2009) we considered both first order and second
order similarity. In first order similarity, the simi-
larity of two words was computed using the cosine
(or Jaccard measure) of the co-occurrence vectors of
the two words. Co-occurrence vectors where con-
structed using freely available software (Pado? and
Lapata, 2007) run over the British National Corpus.
We used the optimal parameters (Pado? and Lapata,
2007, p. 179). We will refer to these similarities as
simcos and simJac, respectively. In contrast, sec-
ond order similarity uses vectors of similar words,
i.e., the similarity of two words was computed us-
ing the cosine (or Jaccard measure) between the
thesaurus entries of those words in Lin?s thesaurus
(Lin, 1998). We refer to these as sim2cos and sim
2
Jac.
Given a target sentence with a verb and its argu-
ments, the task of SR classification is to assign the
correct role to each of the arguments. When using
SPs alone, we only use the headwords of the ar-
guments, and each argument is classified indepen-
dently of the rest. For each headword, we select the
role (r) of the verb (c) which fits best the head word
(w), where the goodness of fit (SPsim(v, r, w)) is
modeled using one of the similarity models above,
between the headword w and the headwords seen in
training data for role r of verb v. This selection rule
is formalized as follows:
Rsim(v, w) = arg max
r?Roles(v)
SPsim(v, r, w) (1)
In our previous work (Zapirain et al, 2009), we
modelled SPs for pairs of predicates (verbs) and ar-
guments, independently of the fact that the argu-
ment is a core argument (typically a noun) or an
adjunct argument (typically a prepositional phrase).
In contrast, (Litkowski and Hargraves, 2005) show
that prepositions have SPs of their own, especially
when functioning as adjuncts. We therefore decided
to split SPs according to whether the potential argu-
ment is a Prepositional Phrase (PP) or a Noun Phrase
(NP). For NPs, which tend to be core arguments2,
we use the SPs of the verb (as formalized above).
For PPs, which have an even distribution between
core and adjunct arguments, we use the SPs of the
prepositions alone, ignoring the verbs. Implementa-
tion wise, this means that in Eq. (1), we change v
for p, where p is the preposition heading the PP.
3 Experiments with SPs in isolation
In this section we evaluate the use of SPs for classi-
fication in isolation, i.e., we use formula 1, and no
other information. In addition we contrast the use
of both verb-role and preposition-role SPs, as com-
pared to the use of verb-role SPs alone.
The dataset used in these experiments (and in Sec-
tion 4) is the same as provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez, 2005).
This dataset comprises several sections of the Prop-
Bank corpus (news from the WSJ) as well as an ex-
tract of the Brown Corpus. Sections 02-21 are used
for generating the SPs and training, Section 00 for
development, and Section 23 for testing, as custom-
ary. The Brown Corpus is used for out-of-domain
testing, but due to the limited size of the provided
section, we extended it with instances from Sem-
Link3. Since the focus of this work is on argument
2In our training data, NPs are adjuncts only 5% of the times
3http://verbs.colorado.edu/semlink/
374
Verb-Role SPs Preposition-Role and Verb-Role SPs
WSJ-test Brown WSJ-test Brown
prec. rec. F1 prec. rec. F1 prec. rec. F1 prec. rec. F1
lexical 70.75 26.66 39.43 59.39 05.51 10.08 82.98 43.77 57.31 68.47 13.60 22.69
SPres 45.07 37.11 40.71 36.34 27.58 31.33 63.47 53.24 57.91 55.12 44.15 49.03
SPwn 55.44 45.58 50.03 41.76 31.58 35.96 65.70 63.88 64.78 60.08 48.10 53.43
SPsimJac 48.85 46.38 47.58 42.10 34.34 37.82 61.83 61.40 61.61 55.42 53.45 54.42
SPsimcos 53.13 50.44 51.75 43.24 35.27 38.85 64.67 64.22 64.44 56.56 54.54 55.53
SPsim2
Jac
61.76 58.63 60.16 51.97 42.39 46.69 70.82 70.33 70.57 62.37 60.15 61.24
SPsim2cos 61.12 58.12 59.63 51.92 42.35 46.65 70.28 69.80 70.04 62.36 60.14 61.23
Table 1: Results for SPs in isolation, left for verb SPs, and right both preposition and verb SPs.
Labels proposed by the base models
Number of base models that proposed this datum?s label
List of actual base models that proposed this datum?s label
Table 2: Features of the binary meta-classifier.
classification, we use the gold PropBank data to
identify argument boundaries. Considering that SPs
can handle only nominal arguments, in these exper-
iments we used only arguments mapped to NPs and
PPs containing a nominal head. From the training
sections, we extracted over 140K such arguments for
the supervised generation of SPs. The development
and test sections contain over 5K and 8K examples,
respectively, and the portion of the Brown Corpus
comprises an amount of 8.1K examples.
Table 1 lists the results of the different SPs in iso-
lation. The results reported in the left part of Table
1 are comparable to those we reported in (Zapirain
et al, 2009). The differences are due to the fact that
we do not discard roles like MOD, DIS, NEG and
that our previous work used only the subset of the
data that could be mapped to VerbNet (around 50%).
All in all, the table shows that splitting SPs into verb
and preposition SPs yields better results, both in pre-
cision and recall, improving F1 up to 10 points in
some cases.
4 Integrating SPs in a SRL system
For these experiments we modified SwiRL (Sur-
deanu et al, 2007): (a) we matched the gold bound-
aries against syntactic constituents predicted inter-
nally using the Charniak parser (Charniak, 2000);
and (b) we classified these constituents with their
semantic role using a modified version of SwiRL?s
feature set.
We explored two different strategies for integrat-
ing SPs in SwiRL. The first, obvious method is to
extend SwiRL?s feature set with features that model
the preferences of the SPs, i.e., for each SP model
SPi we add a feature whose value is Ri. The second
method combines SwiRL?s classification model and
our SP models using meta-classification. We opted
for a binary classification approach: first, for each
constituent we generate n datums, one for each dis-
tinct role label proposed by the pool of base models;
then we use a binary meta-classifier to label each
candidate role as correct or incorrect. Table 2 lists
the features of the meta-classifier. We trained the
meta-classifier on the usual PropBank training par-
tition, using cross-validation to generate outputs for
the base models that require the same training ma-
terial. At prediction time, for each candidate con-
stituent we selected the role label that was classified
as correct with the highest confidence.
Table 3 compares the performance of both
combination approaches against the standalone
SwiRL classifier. We show results for both core
arguments (Core), adjunct arguments (Arg) and
all arguments combined (All). In the table, the
SwiRL+SP? models stand for SwiRL classifiers
enhanced with one feature from the correspond-
ing SP. Adding more than one SP-based feature to
SwiRL did not improve results. Our conjecture
is that the SwiRL classifier enhanced with SP-
based features does not learn relevant weights for
these features because their signal is ?drowned? by
SwiRL?s large initial feature set and the correlation
between the different SPs. This observation moti-
vated the development of the meta-classifier. The
meta-classifier shown in the table combines the out-
put of the SwiRL+SP? models with the predictions
of SP models used in isolation. We implemented
the meta-classifier using Support Vector Machines
(SVM)4 with a quadratic polynomial kernel, and
4http://svmlight.joachims.org
375
WSJ-test Brown
Core Adj All Core Adj All
SwiRL 93.25 81.31 90.83 84.42 57.76 79.52
+SPRes 93.17 81.08 90.76 84.52 59.24 79.86
+SPwn 92.88 81.11 90.56 84.26 59.69 79.73
+SPsimJac 93.37 80.30 90.86 84.43 59.54 79.83
+SPsimcos 93.33 80.92 90.87 85.14 60.16 80.50
+SPsim2
Jac
93.03 82.75 90.95 85.62 59.63 80.75
+SPsim2cos 93.78 80.56 91.23 84.95 61.01 80.48
Meta 94.37 83.40 92.12 86.20 63.40 81.91
Table 3: Classification accuracy for the combination ap-
proaches. +SPx stands for SwiRL plus each SP model.
C = 0.01 (tuned in development).
Table 3 indicates that four out of the six
SwiRL+SP? models perform better than SwiRL in
domain (WSJ-test), and all of them outperform
SwiRL out of domain (Brown). However, the im-
provements are small and, generally, not statistically
significant. On the other hand, the meta-classifier
outperforms SwiRL both in domain (14.07% error
reduction) and out of domain (11.67% error reduc-
tion), and the differences are statistically signifi-
cant (measured using two-tailed paired t-test at 99%
confidence interval on 100 samples generated us-
ing bootstrap resampling). We also implemented
two unsupervised voting baselines, one unweighted
(each base model has the same weight) and one
weighted (each base model is weighted by its accu-
racy in development). However, none of these base-
lines outperformed the standalone SwiRL classifier.
This is further proof that, for SR classification, meta-
classification is crucial because it can learn the dis-
tinct specializations of the various base models.
Finally, Table 3 shows that our approach yields
consistent improvements for both core and adjunct
arguments. Out of domain, we see a bigger accuracy
improvement for adjunct arguments (5.64 absolute
points) vs. core arguments (1.78 points). This is
to be expected, as most core arguments fall under
the Arg0 and Arg1 classes, which can typically be
disambiguated based on syntactic information, i.e.,
subject vs. object. On the other hand, there are no
syntactic hints for adjunct arguments, so the system
learns to rely more on SP information in this case.
5 Conclusions
This paper is the first work to show that SPs improve
a state-of-the-art SR classification system. Sev-
eral decisions were crucial for success: (a) we de-
ployed separate SP models for verbs and preposi-
tions, which in conjunction outperform SP models
for verbs alone; (b) we incorporated SPs into SR
classification using a meta-classification approach
that combines eight base models, developed from
variants of a state-of-the-art SRL system and the
above SP models. We show that the resulting system
outperforms the original SR classification system for
arguments mapped to nominal or prepositional con-
stituents. The improvements are statistically sig-
nificant both on in-domain and out-of-domain data
sets.
Acknowledgments
This work was partially supported by projects KNOW-
2 (TIN2009-14715-C04-01 / 04), KYOTO (ICT-2007-
211423) and OpenMT-2 (TIN2009-14675C03)
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proc. of CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proc. of ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING-ACL.
K. Litkowski and O. Hargraves. 2005. The preposi-
tion project. In Proceedings of the Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistic Formalisms and Applica-
tions.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2).
S. Pradhan, W. Ward, and J. Martin. 2007. Towards ro-
bust semantic role labeling. In Proc. of NAACL-HLT.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards ro-
bust semantic role labeling. Computational Linguis-
tics, 34(2).
P. Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proc. of HLT.
M. Surdeanu, L. Ma`rquez, X. Carreras, and P.R. Comas.
2007. Combination strategies for semantic role label-
ing. Journal of Artificial Intelligence Research, 29.
B. Zapirain, E. Agirre, and L. Ma`rquez. 2009. General-
izing over lexical features: Selectional preferences for
semantic role classification. In Proc. of ACL-IJCNLP.
376
