Medical WordNet: 
A New Methodology for the Construction and Validation of Information 
Resources for Consumer Health 
Barry SMITH 
Department of Philosophy 
University at Buffalo 
Buffalo, NY 14260, USA 
and 
Institute for Formal Ontology  
and Medical Information Science 
Saarland University 
D-66041 Saarbr?cken, Germany 
phismith@buffalo.edu 
Christiane FELLBAUM 
Department of Psychology 
Princeton University 
Princeton, NJ 08544, USA 
and  
Berlin-Brandenburg Academy of Sciences 
Berlin, Germany 
fellbaum@princeton.edu 
 
Abstract 
A consumer health information system must 
be able to comprehend both expert and non-
expert medical vocabulary and to map 
between the two. We describe an ongoing 
project to create a new lexical database called 
Medical WordNet (MWN), consisting of 
medically relevant terms used by and intel-
ligible to non-expert subjects and supple-
mented by a corpus of natural-language 
sentences that is designed to provide 
medically validated contexts for MWN terms. 
The corpus derives primarily from online 
health information sources targeted to 
consumers, and involves two sub-corpora, 
called Medical FactNet (MFN) and Medical 
BeliefNet (MBN), respectively. The former 
consists of statements accredited as true on 
the basis of a rigorous process of validation, 
the latter of statements which non-experts 
believe to be true. We summarize the MWN / 
MFN / MBN project, and describe some of its 
applications. 
1 From WordNet to Medical WordNet 
WordNet is the principal lexical database used 
in natural language processing (NLP) research 
and applications. (Miller, 1995), (Fellbaum, ed., 
1998) While WordNet?s current version (2.0) has 
broad medical coverage, it manifests a number of 
defects, which reflect both the lack of domain 
expertise on the part of the responsible 
lexicographers, and also the fact that WordNet 
was not built for domain-specific applications. 
The research community has long been aware of 
these defects (Magnini and Strapparava, 2001), 
(Bodenreider and Burgun, 2002), (Burgun and 
Bodenreider, 2001), (Bodenreider, et al, 2003). 
Our response is to create Medical WordNet 
(MWN), a free-standing lexical database designed 
specifically for the needs of natural-language 
processing in the medical domain, with the goal of 
removing the ?noise? which is associated with the 
application of WordNet and similar resources to 
this specialized domain. 
MWN?s initial focus is on English single-word 
expressions as used and understood by non-
experts. We systematically review WordNet?s 
existing medical coverage by assembling a vali-
dated corpus of sentences involving specific 
medically relevant vocabulary. Input to our 
validation process includes the definitions of 
medical terms already existing in WordNet, and 
also sentences generated via the semantic rela-
tions linking such terms in WordNet. In addition, 
input includes sentences derived from online 
medical information services targeted to 
consumers.  
Our methodology is designed (1) to document 
natural language sentential contexts for each 
relevant word sense in such a way that the 
expressed information can be (2) validated by 
medical experts and (3) accessed automatically by 
NLP applications such as information retrieval, 
machine translation, question-answer systems, 
and text summarization. 
A major stumbling block for existing NLP 
applications is automatic sense disambiguation. 
An automatic system can detect with high 
reliability that a given occurrence of a word like 
feel or dead is a verb or adjective. But it cannot 
easily determine which of a variety of alternative 
meanings such polysemous words have in any 
given context.  
WordNet?s architecture, designed for repre-
senting and distinguishing word senses, has made 
an important contribution towards a solution of 
the automatic word sense disambiguation prob-
lem. Our corpus of English language sentences 
relating to medical phenomena is designed to 
build upon this contribution. The corpus is 
restricted to grammatically complete, syntactically 
simple sentences in natural language which have 
been rated as understandable by non-expert 
human subjects in controlled questionnaire-based 
experiments. It is restricted in addition to sen-
tences which are self-contained in the sense that 
they make no reference to any prior context and 
do not contain any proper names, or anaphoric 
elements (like it or he or then) that need to be in-
terpreted with respect to other sentences or some 
surrounding discourse or context. This corpus is 
designed to be used initially for purposes of 
quality assurance of MWN and also to support the 
population of MWN by yielding new families of 
words and word senses for inclusion. As will 
become clear, however, our use of human 
validators will allow us to extend the usefulness 
of the corpus in a variety of ways. Thus we can 
use it to build new sorts of applications for 
information retrieval in the domain of consumer 
health. But it also allows new avenues of research 
in linguistics and psychology, for example in 
allowing us to explore individual and group 
differences in medical knowledge and vocabulary, 
and in understanding non-expert medical 
reasoning and decision-making.  
2 Medical FactNet and Medical BeliefNet 
To this end, however, we need to exploit our 
validation data to create two sentential 
subcorpora, called Medical FactNet (MFN) and 
Medical BeliefNet (MBN), respectively. 
MFN consists of those sentences in the corpus 
which receive high marks for correctness on 
being assessed by medical experts. MFN is thus 
designed to constitute a representative fraction of 
the true beliefs about medical phenomena which 
are intelligible to non-expert English-speakers. 
MBN consists of those sentences in the corpus 
which receive high marks for assent. MBN is thus 
designed to constitute a representative fraction of 
the beliefs about medical phenomena (both true 
and false) distributed through the population of 
English speakers. 
The validation process that is involved in the 
construction of MFN is used to detect errors in the 
existing WordNet, and also to ensure that the 
coverage of the natural language medical lexicon 
in MWN is of a scientific level sufficient to allow 
MWN technology to work in tandem with 
terminology and ontology systems designed for 
use by experts. 
Both MFN and MBN inherit from MWN the 
formal architecture of the Princeton WordNet. 
(Fellbaum, ed., 1998) However, we enhance this 
architecture to maximize its usefulness in medical 
information retrieval. 
Compiling MFN and MBN in tandem allows 
systematic assessment of the disparity between 
lay beliefs and vocabulary as concerns medical 
phenomena and the corresponding expert medical 
knowledge. The ultimate goal of our work on 
MFN is to document the entirety of the medical 
knowledge that can be understood by average 
adult consumers of healthcare services in the 
United States today. If the methodology for the 
creation and validation of the corpus here 
described proves successful, then we believe that 
the preconditions for the realization of this much 
larger goal will have been established. Responses 
from NLP researchers and from online informa-
tion providers to our initial work on MFN/MBN 
convinces us that this realization would have 
considerable significance for the management and 
retrieval of consumer health information in the 
future. 
3  Background and Motivation 
Recent studies of the use of computer-based 
tools for consumer health information retrieval 
point to a mismatch between existing tools and 
the non-expert language used by most consumers 
? the language used not only by patients but also 
by family members, advisors, administrators, 
lawyers, and so forth, and to some degree also by 
nurses and physicians. (Slaughter, 2002), (C. A. 
Smith, et al, 2002), (Tse, 2003), (Tse and 
Soergel, 2003), (McCray and Tse, 2003), (Zeng, 
et al, in press) 
Where the usage of medical terms by 
professionals is at least in principle subject to 
control by standardization efforts, the highly 
contextually dependent usage of medical terms on 
the part of lay persons is much more difficult to 
capture in applications ? and this in spite of the 
fact that it is in many ways simpler than expert 
usage. The taxonomies reflecting popular 
lexicalizations in all domains are indeed much 
less elaborate at both the upper and lower levels 
than in the corresponding technical lexica. (Medin 
and Atran, eds., 1999) Thus there are no popular 
terms linking infectious disease and mumps, so 
that in the popular medical taxonomy of diseases 
the former immediately subsumes the latter. The 
popular medical vocabulary naturally covers only 
a small segment of the encyclopedic vocabulary 
of medical professionalsm, and it lexicalizes 
mainly at the level of taxonomic orders. Popular 
medical terms (flu) are often fuzzier than technical 
medical terms. Many popular terms also cover a 
larger range of referent types than do technical 
terms; others may cover only part of the extension 
of their technical counterparts. We hypothesize, 
however, that with few exceptions the focal 
meanings (Berlin and Kay, 1969) of expert and 
non-expert terms will be identical. Constructing 
MFN and MBN allows us to test this and related 
hypotheses in a systematic way.  
4 Mismatches in Doctor-Patient 
Communication 
The skills of a physician in general practice 
comprise the ability to acquire relevant and 
reliable information through communication with 
patients through the use of non-expert language 
and to convey diagnostic and therapeutic 
information in ways tailored to the individual 
patient.  
Since the physician, too, is a member of the 
wider community of non-experts, and continues to 
use the non-expert language for everyday 
purposes, one might assume that there are no 
difficulties in principle keeping him from being 
able to formulate medical knowledge in a 
vocabulary that the patient can understand. As 
(Slaughter, 2002) and (C. A. Smith, et al, 2002) 
have shown, however, there are limits to this 
competence. The former examines dialogue 
between physicians and patients in the form of 
question-answer pairs, focusing especially on the 
relations documented in the UMLS Semantic 
Network. Only some 30% of the relations used by 
professionals in their answers directly match the 
relations used by consumers in formulating their 
questions. An example of one such question-
answer pair is taken from (Slaughter, p. 224): 
Question Text: My seven-year-old son developed a 
rash today that I believe to be chickenpox. My concern 
is that a friend of mine had her 10-day-old baby at my 
home last evening before we were aware of the illness. 
My son had no contact with the infant, as he was in bed 
during the visit, but I have read that chickenpox is 
contagious up to two days prior to the actual rash. Is 
there cause for concern at this point?  
Answer Text: (a) Chickenpox is the common name 
for varicella infection. [...] (b) You are correct in that a 
person with chickenpox can be contagious for 48 hours 
before the first vesicle is seen. [...] (c) The fact that 
your son did not come in close contact with the infant 
means he most likely did not transmit the virus. (d) Of 
concern, though, is the fact that newborns are at higher 
risk of complications of varicella, including pneumo-
nia. [...] (e) There is a very effective means to prevent 
infection after exposure. A form of antibody to 
varicella called varicella-zoster immune globulin 
(VZIG) can be given up to 48 hours after exposure and 
still prevent disease.  
Such examples illustrate also that there are 
lexically rooted mismatches in communication 
(which may in part reflect legal and ethical 
considerations) between experts and non-experts. 
Professionals often do not re-use the concepts and 
relations made explicit in the questions put to 
them by consumers. In our example, the 
questioner requests a yes/no-judgment on the 
possibility of contagion in a 10-day-old baby. In 
fact, however, only section (c) of the answer 
responds to this question, and this in a way which 
involves multiple departures from the type of non-
expert language which the questioner can be 
presumed to understand. Rather, physicians 
expand the range of concepts and relations 
addressed (for example through discussion of 
issues of prevention, etc.).  
In all cases, the information source, whether it 
be a primary care physician or an online informa-
tion system, must respond primarily with generic 
information (i.e. with information that is inde-
pendent of case or context), and this is so even 
where requests relate to specific and episodic 
phenomena (occurrences of pain, fever, reactions 
to drugs, etc.). (Patel, et al, 2002) In our example, 
all sections except for (c) are of this generic kind. 
They contain answers in the form of context-
independent statements about causality, about 
types of persons or diseases, about typical or 
possible courses of a disease. MFN is accordingly 
designed to map the generic medical information 
which non-experts are able to understand.  
5 Non-Expert Language in Online 
Communication  
Understanding patients requires both explicit 
medical knowledge and tacit linguistic 
competence dispersed across large numbers of 
more or less isolated practitioners. This is not a 
problem so long as this knowledge is to be applied 
locally, in face-to-face communication with 
patients. However, as a result of recent develop-
ments in technology, including telemedicine and 
internet-based medical query systems, we now 
face a situation where such dispersed, practical 
(human) knowledge does not suffice.  
(Ely, et al, 2000) and (Jacquemart and 
Zweigenbaum, 2003) have shown that clinical 
questions are expressed in a small number of 
different syntactic-semantic patterns (about 60 
patterns account for 90% of the questions). Such 
yes/no questions are of the forms: Do hair dyes 
cause cancer?, Can I use aspirin to treat a 
hangover? With the right sort of information 
resource, questions such as these can easily be 
transformed automatically into statements 
providing correct answers: Hair dyes can cause 
bladder cancer, Aspirin doesn?t help in case of a 
hangover , and these answers can be linked 
further to relevant and authoritative sources. 
 MEDLINEplus is described in its online 
documentation as a source of medical information 
for both experts and non-experts ?that is 
authoritative and up to date.? Enquirers can use 
MEDLINEplus like a dictionary, choosing health 
topics by keywords. Alternatively, they can use 
the system?s search feature to gain access to a 
database of relevant online documents selected for 
reliability and accessibility on the basis of pre-
established criteria. 
Table 1 shows the problems that can arise when 
a system fails to take account of the special 
features of the knowledge and vocabulary of 
typical non-expert users. Here success in finding 
the needed information depends too narrowly on 
the precise formulation of the query text. Thus 
tremble and trembling call forth different 
responses (one lists caffeine, the other phobias), 
even though the terms in question differ only in a 
morphological affix that does not involve a 
meaning distinction. Such problems are 
characteristic of information services of this kind. 
Experienced internet users are of course familiar 
with the limitations of search engines, and so they 
are able to manipulate their query texts in order to 
get more and better results. Even experienced 
users, however, will not be able to overcome the 
arbitrary sensitivities of an information system, 
and the latter cannot have the goal of bringing 
non-experts? ways of using language into line 
with that of the system.  
6 Corpus- and Fact-Based Approaches to 
Information Retrieval 
(Patel, et al, 2002) make clear that if a medical 
information system is to mediate between experts 
and non-experts, then it must rest on an 
understanding of both expert and non-expert 
medical vocabulary. But terms, or word forms, are 
not always associated with word meanings in a 
clear-cut and unambiguous fashion; and the 
problem of polysemy is compounded when 
different speaker populations are involved. A 
lexical database must represent all and only the 
meanings of each given term in such a way that 
these meanings can be clearly discriminated and 
mapped onto word occurrences in natural text and 
speech. Achieving these ends is one of the hardest 
challenges facing both theoretical and applied 
linguistic science today. It is generally agreed that 
the meanings of highly polysemous terms cannot 
be discriminated without consideration of their 
contexts (e.g., Pustejovsky, 1995). People manage 
polysemy without apparent difficulties; but 
modeling human speakers? capacity for lexical 
disambiguation in automatic language processing 
tasks is hard. The idea underlying the present 
proposal draws on currently emerging NLP 
methodologies that harness the ability of powerful 
and fast computers to store and manipulate both 
lexical databases and large collections of text col-
lections or corpora. The strategy is to train auto-
matic systems on large numbers of semantically 
annotated sentences that are naturally used and 
understood by human beings, and to exploit 
standard pattern-recognition and statistical tech-
niques for purposes of disambiguation. Words and 
the representation of their senses, stored in lexical 
databases, can be linked for this purpose to 
specific occurrences in corpora.  
7 Related Work 
Currently, several resources are being built in 
the spirit of this methodology. Examples are 
FrameNet (Baker, et al, 1998), (Baker, et al, 
2003) and Penn Proposition Bank (Kingsbury and 
Palmer, 2002), both of which focus on word usage 
in general, rather than on domain-specific 
contexts. In contrast to our own project, neither of 
the mentioned resources attempts to build a 
corpus in a systematic way that is designed to 
ensure adequate coverage of some given domain. 
Furthermore, neither project is concerned with the 
questions of factuality or validation of statements. 
 Another project with goals similar to those of 
MFN is the CYC (short for enCYClopedia) 
knowledge base, a collection of hundreds of 
thousands of statements, mostly about the external 
world, such as: The earth is round, Albany is the 
capital of New York. (Lenat, 1995), (Guha, et al, 
1990) These statements, which were entered over 
many years by CYC employees, are parcelled out 
into separate micro-theories devoted to different 
domains. (On CYCs medical coverage see 
(Bodenreider and Burgun, in press).)  
 Query text MEDLINEplus? response (with links to 
documents sorted by the following keywords)
tremor Tremor, Multiple Sclerosis, Parkinson?s 
Disease, Degenerative Nerve Diseases, 
Movement Disorders  
intentional 
tremor 
Tremor, Multiple Sclerosis, Parkinson?s 
Disease, Spinal Muscular Atrophy, 
Degenerative Nerve Diseases 
tremble Anxiety, Parkinson?s Disease, Panic 
Disorder, Caffeine, Tremor 
trembling Anxiety, Parkinson?s Disease, Panic 
Disorder, Phobias, Tremor 
Table 1: Online-Inquiry to MEDLINEplus? 
(http://www.nlm.nih.gov/medlineplus) 
Our work differs in a number of ways from 
CYC: (i) we focus on one single (albeit very 
large) domain; (ii) CYC does not store English 
sentences but rather ? in keeping with its goal of 
being language-unspecific ? statements couched 
in the symbolism of a modified first-order logic; 
(iii) CYC incorporates folk beliefs and expert 
knowledge indiscriminately, and its separate 
micro-theories are not designed to be consistent 
either with each other or with the body of 
established science; (iv) only a reduced part of 
CYC is publicly available. 
8 WordNet 
WordNet 2.0 is a large electronic lexical data-
base of English that has found wide acceptance in 
areas as diverse as artificial intelligence, natural 
language processing, and psychology. (Agirre and 
Martinez, 2000), (Al-Halimi and Kazman, 1998), 
(Artale, et al, 1997), (Basili et al, 1997), 
(Berwick, et al, 1990), (Burg and van de Riet 
1998), (Cucchiarelli and Velardi 1997), (Fellbaum 
1990), (Gonzalo, et al, 1998), (Harabagiu and 
Moldovan, 1996) Its coverage, which is 
comparable to that of a collegiate dictionary, 
extends over some 130,000 word forms. The most 
common application is in information technology, 
where it is used for information retrieval, 
document classification, question-answer systems, 
language generation, and machine translation. 
WordNet was originally conceived as a full-scale 
model of human semantic organization, and its 
design was guided by early experiments in 
artificial intelligence. (Collins and Quillian, 1969) 
WordNet was quickly embraced by the NLP 
community, a development that has guided its 
subsequent growth and design, and WordNet is 
now widely recognized as the lexical database of 
choice for NLP. The appeal of WordNet?s design 
is reflected in the fact that wordnets have been, 
and continue to be, built in dozens of languages. 
Wordnets supporting many European and non-
European languages are already available. All are 
linked to the original English WordNet, which 
thereby functions as an interlingual index. In 
consequence, all wordnets can be mapped to one 
another. This means that our work on Medical 
WordNet will ultimately be translatable into 
dozens of languages with very little additional 
effort.  
8.1 Architecture of WordNet 
The building blocks of WordNet are synonym 
sets (?synsets?), which are unordered sets of 
distinct word forms and which correspond closely 
to what, in medical terminology research, are 
called ?concepts.? Membership in a synset 
requires that the word forms express the same 
concept and are in this sense ?cognitively 
synonymous? (Cruse, 1986). More formally, 
synset members must be interchangeable in some 
sentential contexts without altering the truth-value 
of the sentences involved. WordNet?s architecture 
is thus grounded in the notion of truth-preserving 
interchangeability of word forms in sentential 
contexts, although research has not thus far 
focused on this feature. Constructing Medical 
FactNet alows us to rectify this gap by making 
explicit the contexts in which word forms are used 
in an environment that allows the systematic 
testing of the effects of word form substitution. 
Examples of synsets are {car, automobile} or 
{shut, close}. WordNet 2.0 contains some 
115,000 synsets, with many word forms 
belonging to a plurality of synsets. 
WordNet is a net in virtue of the fact that the 
synsets are linked to one another via a small 
number of binary relations that differ for each of 
the four syntactic categories covered by WordNet: 
nouns, adjectives, verbs, adverbs. Noun synsets 
are interlinked by means of the subtype (or is-a) 
relation, as exemplified by the pair poodle-dog, 
and by means of the part-of relation, as 
exemplified by the pair tire-car. Verb synsets are 
connected by a variety of lexical entailment rela-
tions that express manner elaborations, temporal 
relations, and causation (walk-limp, forget-know, 
show-see). (Fellbaum, 2002), (Fellbaum, 2003) 
Thus if X limps, then X also walks, but not vice 
versa. The links among the synsets structure the 
noun and verb lexica into hierarchies, with noun 
hierarchies being considerably deeper than those 
for verbs. 
WordNet?s appeal for NLP applications stems 
from the fact that its synset architecture can be 
exploited in building NLP applications that target 
the problem of automatic word sense dis-
ambiguation. Although most word forms in 
English are monosemous (clinician, epidemic), 
the most frequently occurring words are highly 
polysemous (host, dress, arm). The ambiguity of a 
polysemous word in a context can be resolved by 
distinguishing the multiple senses in terms of their 
links to other words within the WordNet net. For 
example, the noun club can be disambiguated by 
an automatic system that considers the 
superordinates of the different synsets in which 
this word form occurs: association, playing card, 
and stick.  
The information contained in WordNet is of 
two sorts: lexical (i.e. verbal) knowledge, stored 
in WordNet?s synset architecture, and 
encyclopedic (i.e. factual) knowledge, found in 
the definitions (or ?glosses?) associated with each 
concept. These definitions can be problematic, 
however, as they were generated by lexico-
graphers who were not specialists in the domains 
to which the words in the synsets belong. Often, 
the definitions were modelled on those found in 
existing dictionaries, but in these cases, too, 
problems have arisen above all in the form of a 
mismatch between definitions representing 
technical (specialist) knowledge and definitions 
reflecting non-expert usage. To resolve this 
problem each synset in MWN is augmented with 
two glosses. One is formulated for the layman, the 
other in expert language.  
A further problem turns on the fact that the 
sentences included in WordNet 2.0 as illustrations 
of the use of synonyms in sentential contexts do 
not always reflect correct or characteristic usages 
of the words in the synset. Constructing MFN 
addresses this problem in a systematic way. 
9 Uses of WordNet in Medical Informatics 
(Xiao and R?sner, 2003) shows how WordNet 
can be used as a tool for simplifying information 
extraction from MEDLINE. Parsing tools are used 
to extract verbs from the corpus of MEDLINE 
abstracts, and it is then shown that very many 
(both low- and high-frequency) verbs are grouped 
together into WordNet synsets in such that, within 
this specific discourse domain, there is only one 
conceptual relation linking all the verbs in each of 
the relevant synsets. In this way it is possible to 
simplify the process of information abstraction by 
reducing the number of relations that need to be 
taken into account in the analysis of texts. 
(Buitelaar and Sacaleanu, 2001) describe work 
showing how, using the German version of 
WordNet, one can use statistical analysis to 
support automatic selection of the most likely 
synset associated with given nouns appearing in 
medical corpora. 
WordNet?s design allows users with specific 
technical applications to augment the database, 
primarily by adding new terms as leaves to the 
existing branches of its subsumption and part-
whole hierarchies. Such enriched wordnets retain 
all of the original information, and the added 
words are semantically specified in terms of 
WordNet?s relations. (Turcato, et al, 2001) and 
(Buitelaar and Sacaleanu, 2002) describe an 
attempt to extend the German wordnet with 
synsets pertaining to the medical domain using 
automatic methods, in particular the detection of 
semantic similarity from co-occurrence patterns in 
a domain-specific corpus. The results, while good, 
are hampered by problems of lexical polysemy 
and by the characteristically German tendency for 
compound formation, which leads to potentially 
open-ended lexicon growth, and thus poses posing 
great problems for automatic word sense 
recognition and discrimination. One clear 
conclusion from this study is that fully automated 
lexical acquisition provides inadequate results, 
and that much of the work must be performed 
manually. Our proposal reflects this conclusion.  
(Bodenreider and Burgun, 2002) and (Burgun 
and Bodenreider, 2001) characterize the 
definitions of anatomical concepts in WordNet 
and in various portions of the UMLS Meta-
thesaurus. They found that anatomical definitions 
are characteristically of the form: superordinate + 
distinguishing feature (the latter expressed 
through some form of adjectival modification or 
relative clause, etc.). This way of defining words 
is in fact the canonical one (for nouns, and, to 
some degree, for verbs as well) and lexico-
graphers follow it as much as possible when 
writing definitions. MWN will observe this 
standard consistently in its augmentation and 
standardization of WordNet?s definitions, drawing 
on the results of the studies of best practice in the 
formulation of definitions in biomedical termi-
nologies and ontologies in (Smith and Rosse, 
2004), (Bodenreider, et al, 2004) and (Smith, et 
al., 2004).  
10 The Medical Coverage of WordNet 2.0 
For the verb feel, WordNet 2.0 distinguishes in 
all 13 separate meanings, of which at least the 
following have an obvious medical significance, 
and are handled by WordNet in rough accordance 
with their usage in medical contexts:  
3. sense ?  (perceive by a physical sensation, e.g., 
coming from the skin or muscles: He felt the wind; 
She felt an object brushing her arm; He felt his 
flesh crawl; She felt the heat when she got out of 
the car; He feels pain when he puts pressure on his 
knee.) 
4. feel ? (seem with respect to a given sensation 
given: My cold is gone ? I feel fine today; She felt 
tired after the long hike) 
10. palpate, feel ? (examine (a body part) by 
palpation: The nurse palpated the patient?s 
stomach; The runner felt her pulse) 
For the adjective dead, WordNet 2.0 distin-
guishes 21 meanings, with only two approxi-
mating to meanings of this term as used in 
medical contexts:  
1. dead (vs. alive) ? (no longer having or seeming 
to have or expecting to have life: The nerve is dead; 
A dead pallor) 
9. dead, deadened ? (devoid of physical sensation; 
numb: his gums were dead from the Novocain) 
Not only does WordNet fail to distinguish those 
medically relevant meaning distinctions illustrated 
by phrases such as dead tissue, dead organ, dead 
matter, dead cell, dead body, etc., but its 
definition of the primary medically relevant sense 
of dead (as: ?no longer having or seeming to have 
or expecting to have life?) runs together three 
separate notions which it is medically important to 
keep separate. 
WordNet recently added domain labels to many 
synsets. One such label is medicine; others are 
surgery and drug. However, it was left undecided 
on what criteria terms should be selected as 
domain labels and what the relations among the 
relevant domains should be (arguably, surgery 
and drug should be included in the wider domain 
of medicine). In addition, labels were not 
systematically assigned to WordNet terms. 
Currently, when asked to output terms associated 
with medicine, the browser returns some 504 
nouns, verbs, and adjectives (both single words 
and phrases), representing some 270 different 
senses. On the other hand, many cognate senses 
with clear medical uses are currently not labeled 
in this way. Table 2 provides examples, with the 
medicine label picked out in bold: 
autopsy 
#1 
{autopsy, necropsy, postmortem, 
PM, postmortem examination ? 
(an examination and dissection of 
a dead body to determine cause of 
death or the changes produced by 
disease)} 
fester 
#1 
{fester, maturate, suppurate ? 
(ripen and generate pus; her 
wounds are festering)} 
festering 
#1 
{festering, suppuration, 
maturation ? ((medicine) the 
formation of morbific matter in an 
abscess or a vesicle and the 
discharge of pus)} 
festering 
#2 
{pus, purulence, suppuration, 
ichor, sanies, festering ? (a fluid 
product of inflammation)} 
infection 
#1 
{(the pathological state resulting 
from the invasion of the body by 
pathogenic microorganisms)} 
infection 
#3 
{((medicine) the invasion of the 
body by pathogenic 
microorganisms and their 
multiplication which can lead to 
tissue damage and disease)} 
infection 
#4 
{infection, contagion, 
transmission ? (an incident in 
which an infectious disease is 
transmitted)} 
maturation 
#2 
{growth, growing, maturation, 
development, ontogeny, 
ontogenesis ? ((biology) the 
process of an individual organism 
growing organically; a purely 
biological unfolding of events 
involved in an organism changing 
gradually from a simple to a more 
complex level; he proposed an 
indicator of osseous development 
in children)} 
maturation 
#3 
{festering, suppuration, 
maturation ? ((medicine) the 
formation of morbific matter in an 
abscess or a vesicle and the 
discharge of pus)} 
zymosis 
#2 
{((medicine) the development 
and spread of an infectious 
disease (especially one caused by 
a fungus))} 
Table 2. Examples of Medically Relevant Entries 
in WordNet 2.0 
Table 2 also illustrates the degree to which 
WordNet currently includes obsolete medical 
terms (ichor, morbific, unction) and also terms 
drawn seemingly indiscriminately from both tech-
nical medical vocabularies and from natural 
language. Some synsets contain only folk or only 
technical terms, some contain a mixture of both. 
Definitions are largely taken over from medical 
dictionaries prepared for experts. 
To provide a preliminary estimate of the extent 
of WordNet?s somewhat arbitrary medical 
coverage we derived a test lexicon of 2838 single-
word medical terms from an existing digitalized 
lexical resource for medical language processing 
(LinKBase of the Belgian NLP company L&C), 
which was constructed independently of WordNet 
by medical professionals. The method used was to 
transform LinKBase into an alphabetically 
ordered term list and to eliminate automatically all 
acronyms, all multi-word terms, all proprietary 
terms, all terms containing numbers, and all terms 
longer than 10 letters. Remaining technical terms 
were then removed manually. Of the residual 
2838 terms, only 11 were not present in any form 
in WordNet 2.0, though considerably more were 
not treated adequately in regard to their 
specifically medical usages. Almost all missing 
terms were compounds such as bedwetting, 
breastfed, coldsore.  
WordNet 2.0 has inadequate treatment of the 
systematic polysemy of nouns like dizziness and 
itching. These, like many other nouns, are both 
sensations and symptoms. The symptom role is 
also not encoded for many other nouns, including 
redness, retching, swelling, and so forth. WordNet 
states: a tumor is a mass of tissue and a tumor is 
abnormal, but not: some tumors are malignant. 
WordNet?s treatment of is-a, part-of and other 
relations, too, is marked by inadequacies in the 
medical domain. Thus WordNet currently con-
tains a verb entailment relation exemplified by the 
pair snore-sleep defined as: ?if someone snores, 
then he necessarily also sleeps.? In medicine, 
however, it is recognized that it is quite possible 
to snore while awake, since snoring is there 
defined as the respiratory induced vibration of 
glottal tissues and this is associated not only (and 
most usually) with sleep but also with relaxation 
or obesity.  
Our methodology for constructing MFN 
involves the validation by experts of all relations 
between WordNet?s medically relevant synsets. It 
provides us with a systematic means to detect 
such errors. Constructing MBN gives us in 
addition the resources to do justice to the reason 
why such cases were included in WordNet in the 
first place: People can only snore when they are 
asleep and similar sentences belong precisely to 
the folk beliefs about medicine which MBN 
documents ?  not, however, to MFN. More 
generally, constructing MBN in tandem with 
MFN allows us to highlight those cases where 
non-experts and experts use the same term in 
different ways. 
Another family of terms currently poorly 
treated in WordNet are those manifesting 
polysemy along the medical/non-medical axis. 
For example, the medical senses of recession or 
rigors are not recorded in WordNet 2.0. A lexical 
database for purposes of automatic sense 
disambiguation must clearly differentiate all such 
senses. (Computerized medical information 
systems do not offer the possibility of follow-up 
in the sort of cases of misunderstanding which we 
have in communication between laypersons and 
medical practitioners.) Thus while MWN will 
contain only word forms that are used by non-
experts (and thus part of natural rather than 
technical language), it must for practical reasons 
record word senses that are peculiar to the 
technical vocabulary. 
11 Method for Translating Online Content 
into Basic Sentences  
We carried out experiments designed to test a 
variety of methodologies for deriving terms and 
sentences for our corpus, including elicitation 
experiments with expert and non-expert human 
subjects, and data-mining from online bulletin 
boards. We established that the most promising 
sources for both term- and sentence-generation 
are certain online information sources targeted 
specifically to non-specialist users. 
In one experiment the basic sentences meeting 
our MFN/MBN criteria were derived by 
researchers in medical informatics from factsheets 
on Airborne allergens in NIAID?s Health 
Information Publications and on Hay fever and 
perennial allergic rhinitis in the UK NetDoctor?s 
Diseases Encyclopedia.  
There is no good way to 
tell the difference 
between allergy 
symptoms of runny 
nose, coughing, and 
sneezing and cold 
symptoms. Allergy 
symptoms, however, 
may last longer than 
cold symptoms.  
from NIAID HealthInfo 
(information also 
included in 
MEDLINEplus) 
1. Allergies have symptoms. 
2. Colds have symptoms. 
3. A runny nose is a symptom 
of an allergy. 
4. Coughing is a symptom of 
an allergy. 
5. Sneezing is a symptom of 
an allergy. 
6. Cold symptoms are similar 
to allergy symptoms. 
7. A cold is not an allergy. 
8. Allergy symptoms may 
last longer than cold 
symptoms. 
What is hay fever? Hay 
fever, otherwise known 
as seasonal allergic 
rhinitis, is an allergic 
reaction to airborne 
substances such as 
pollen that get into the 
upper respiratory 
passages ? the nose, 
sinus, throat ? and also 
the eyes. 
from NetDoctor (UK) 
1. Hay fever is an allergy. 
2. Hay fever is an allergic 
reaction. 
3. Hay fever is a type of 
allergy. 
4. Hay fever is a type of 
allergic reaction. 
5. Hay fever is a reaction to 
pollen. 
6. Hay favor is a reaction to 
airborne substances. 
7. In hay fever airborne 
substances get into the nose. 
8. In hay fever airborne 
substances get into the throat.
9. In hay fever airborne 
substances get into the eyes. 
Table 3: Sample sentences derived from online 
medical information sources 
 
The initial documents were divided into 
paragraph-length sections, and raters were 
instructed to associate with each section complete 
self-contained sentences expressing the generic 
medical knowledge it contains. Sentences were to 
be formed using simple syntax and as far as 
possible drawing on terms used in the original 
sources. Processors were instructed to eliminate 
sentences containing anaphora, indexical expres-
sions, formulations of instructions, warnings and 
the like, and to replace them where possible by 
complete statements constructed via simple 
syntactic modifications. Subjects were instructed 
to include only such terms and information which 
they themselves judged would be intelligible to 
non-experts.  
1644 sentences were produced, representing 
some 20 person hours of effort; examples are 
presented in Table 3. 500 of these sentences were 
subjected to a preliminary evaluation, each 
sentence being presented to pairs of beginning 
medical students for independent evaluation. 58% 
of the sentences were rated by both members of 
each pair with a score of 5. However, a closer 
analysis of the results revealed that the weighted 
kappa measure for inter-rater agreement was too 
low for these results to be statistically significant. 
Further testing of this methodology will thus call 
for larger sample sizes and for the use of raters 
with specific expertise in relation to the 
phenomena described. 
12 Sources and Selection  
The primary sources for terms in MWN and for 
sentences in our test corpus are the relevant 
general lexical information contained in WordNet, 
supplemented by medical dictionaries and large 
medical terminology and ontology systems such 
as MeSH and LinKBase, and by internet resources 
such as MEDLINEPlus and NetDoctor focusing 
especially on coverage of common diseases. We 
shall maintain an internet portal through which 
links to sources used and the results of our term- 
and sentence-extraction will be made available 
online as raw data for use by other researchers. 
In this initial phase of our project we are 
interested primarily in self-contained generic 
(case- and context-independent) statements with a 
relatively simple syntax. To derive such sentences 
we use two methods: 
Method 1 derives sentences from a lexical 
database such as WordNet. We treat the database 
as a set of links between terms of the form tLu 
(where L ranges over ?is-a?, ?part-of?, and other 
relations) and t, u range over terms which occur in 
the medical sublexicon. Some members of the 
resulting class of tLu formulas can be transformed 
automatically into English sentences with a 
minimal amount of post-processing. For example 
each ?t is-a u? formula can be transformed into 
sentences of the forms ?a t is a u? and ?a t is a type 
of u? (with corrections for articles and plurals, as 
in: A cut is a type of wound; An abrasion is a 
wound; Patients are people). Others must be 
subject to manual extraction, which can be carried 
out by native English-speakers (linguists or others 
trained in manipulation of lexical databases) with 
no special medical expertise. Each extracted 
sentence is given a precise identifying number and 
associated with metadata identifying its source. 
Method 2 derives single sentences from on-line 
consumer health information sources along the 
lines described in section 11 above. Here each 
sentence in the source documentation is given a 
precise identifying number, indicating source 
document, position in this document, and section 
from which sentences have been inferred. 
Extracted sentences, too, are given precise 
identifying numbers and are associated with 
metadata documenting section and document of 
origin, date of processing, and also individual 
responsible for extraction. 
13 Human Subject Validations 
The output sentences from the above will serve, 
together with a random infusion of non-medical, 
folk-medical-but-false and medical-but-technical 
sentences, as inputs to validations carried out by 
human subjects. These will be of three primary 
types, referred to in what follows as U, B and C, 
for understanding, belief, and correctness, 
respectively. All sentences will pass through the 
U filter, in which laypersons will be recruited to 
rate sentences for understandability. Those 
sentences which survive will pass on to B and C. 
In B laypersons will rate sentences for degree of 
belief, in C medically trained participants 
(?experts?) will rate sentences for correctness. 
Statements receiving a rating of 4 or higher (out 
of a range from 1 to 5) from each of two raters in 
B will be stored as components of Medical 
BeliefNet; statements receiving a similarly high 
rating from each of two raters in C will be stored 
as components of Medical FactNet. The ratings 
for all sentences, both those which do and those 
which do not pass through the MBN/MFN filters, 
will be stored for further analysis. 
14 Future Work 
We envisage the MBN/MFN methodology 
being used in the fields of medical education and 
medical literacy to evaluate the reliability of the 
medical knowledge of different non-expert 
communities. On the basis of metadata pertaining 
to the sources of entries in MBN it will be 
possible to keep track of specific kinds of false 
beliefs as originating in specific populations of 
informants. This may prove a valuable source of 
information in targeting particular groups for 
specific types of remedial medical education.  
 Furthermore, the extended MBN will provide 
opportunities for a new type of research in the 
field of consumer health. Specifically, we 
envisage experiments that investigate how the 
domain of medical phenomena is conceptualized 
by non-expert human subjects. Cognitive 
psychologists and anthropologists such as Rosch 
and others (Rosch, 1975), (Rosch, 1973), (Rosch, 
1978) have postulated a level of lexical 
specification that they call ?basic level.? Basic 
level words correspond to basic kinds in the 
ontology of language-using subjects. Such words 
exist in all semantic domains, but they have been 
studied predominantly among words denoting 
natural kinds, such as animals, vegetables, fruit, 
and colors. (Medin and Atran, 1999), (Berlin and 
Kay, 1969) For example, tomato is often cited as 
an example of a basic level word, whereas 
vegetable is a superordinate, and cherry tomato is 
a subordinate. Basic level words have many 
striking properties: they are universally 
lexicalized, characterized by high frequency of 
occurrence, and they are learned first by children. 
The concepts they denote have properties that 
differ maximally from each other (e.g., a tomato is 
very different from a cabbage or a bean), but the 
difference between a basic level word and a 
subordinate (such as between a tomato and a 
cherry tomato) is less pronounced. The basic level 
lexicon in the medical domain has thus far not 
been explored, but such research promises 
important theoretical benefits. MBN might be 
used to determine the basic level in the domain 
under investigation by examining the difference in 
the frequency of occurrence of synonyms: highly 
frequent terms are good candidates for basic level 
words. We can then use the results of this work to 
provide a specification of the non-expert ontology 
of the medical domain and begin to explore 
differences between it and the ontologies 
underlying expert medical terminologies.  
 Note that, in all of the above, MFN and MBN 
have characteristically played different roles. 
Thus where MFN has been associated with 
constructing practical tools designed to assist 
users in coming to believe what is true, MBN has 
been associated with research, for example 
regarding what people believe about medical 
phenomena. 
15 Towards a Comprehensive Documentation 
of Consumer Health Knowledge 
We estimate that the two documents referred to 
in Table 3 above represent together some 0.5 % of 
the information available on these two sites that is 
relevant to the purposes of constructing a com-
prehensive survey of consumer health knowledge. 
This suggests that a future comprehensive version 
of MFN might contain some 320,000 sentences. 
The prospect of constructing a sentence-based 
information resource of this size would until very 
recently have rightly been considered 
overwhelming. The success of WordNet gives us 
confidence that this problem, too, can be solved.  
16 Acknowledgements 
Work on this paper was supported by the 
Wolfgang Paul Program of the Alexander von 
Humboldt Foundation. Smith?s work was further 
supported by the European Union Network of 
Excellence on Semantic Datamining, and by the 
project ?Forms of Life?, sponsored by the 
Volkswagen Foundation. Our thanks go in 
addition to Werner Ceusters, Christopher Chute, 
James Cimino, Jean-Pierre Koenig, David Mark, 
Daniel Osherson, and Martin Trautwein. 
References  
Agirre, E., Martinez, D. Exploring automatic 
word sense disambiguation with decision lists 
and the Web. Proceedings of the Semantic 
Annotation and Intelligent Annotation 
Workshop, organized by COLING, Luxem-
bourg, 2000. 
Al-Halimi, R., Kazman, R. Temporal indexing 
through lexical chaining. Fellbaum C. (ed.), 
WordNet: An Electronic Lexical Database, MIT 
Press, Cambridge, Maryland, May 1998. 
Artale, A., Magnini, B., Strapparava C. WordNet 
for Italian and its use for lexical discrimination. 
Proceedings of the 5th Congresso dell? 
Associazione Italiana per l?Intelligenza Artifi-
ciale, Rome, September 1997; 16-19. 
Baker, C. F., Fillmore, C. J., Cronin, B. The 
structure of the framenet database. International 
Journal of Lexicography, 2003; 16.3: 281-296. 
Baker, C. F., Fillmore, C. J., Lowe, J. B. The 
Berkeley FrameNet project. Proceedings of the 
COLING-ACL, Montreal, Canada, 1998. 
Basili, R., DellaRocca, M., Pazienza, M. T. 
Contextual word sense tuning and 
disambiguation. Applied Artificial Intelligence 
1997; 11 (3): 235-262. 
Berlin, B., Kay, P. Basic color terms. 
Berkeley/Los Angeles: University of California 
Press, 1969. 
Berwick, R., Fellbaum, C., Gross, D., Miller, G. 
WordNet: A lexical database organized on 
psycholinguistic principles. In Zernik U. (ed.), 
Using On-line Resources to Build a Lexicon. 
Erlbaum, Hillsdale, NJ, Erlbaum, 1990; Chapter 
9: 211-231. 
Bodenreider, O., Burgun, A., Mitchell, J. A. 
Evaluation of WordNet as a source of lay 
knowledge for molecular biology and genetic 
diseases: a feasibility study. Studies in Health 
Technology and Informatics 2003; 95: 379-384.  
Bodenreider, O., Burgun, A. Characterizing the 
definitions of anatomical concepts in WordNet 
and specialized sources. Proceedings of the 
First Global WordNet Conference, January 
2002; 223-230. 
Bodenreider, O., Burgun, A. Ontologies in the 
biomedical domain. Part II: examples. Journal 
of the American Medical Informatics 
Association (in press). 
Bodenreider, O., Smith B., Kumar A., Burgun A. 
Investigating Subsumption in DL-based 
terminologies: A case study in Snomed-CT. In: 
R. Cornet and S. Schulz (eds.), Proceedings of 
KR-MED 2004: Knowledge Representation in 
Medicine. (Lecture Notes in Bioinformatics 
2994), Springer, Berlin: 2004. 
Buitelaar, P., Sacaleanu, B. Ranking and selecting 
synsets by domain relevance. In: Proceedings of 
WordNet and Other Lexical Resources: 
Applications, Extensions and Customizations, 
NAACL 2001 Workshop, Carnegie Mellon 
University, Pittsburgh, 3-4 June 2001.  
Buitelaar, P., Sacaleanu, B. Extending synsets 
with medical terms. In: Proceedings of the First 
International WordNet Conference, Mysore, 
India, January 21-25, 2002. 
Burg, J. F. M., van de Riet, R. P. COLOR-X: 
Using knowledge from WordNet for conceptual 
modeling. Fellbaum, C. (ed.), WordNet: An 
Electronic Lexical Database, MIT Press, 
Cambridge, Maryland, May 1998. 
Burgun, A., Bodenreider, O. Comparing terms, 
concepts and semantic classes in WordNet and 
the Unified Medical Language System. Pro-
ceedings of the NAACL Workshop on WordNet 
and Other Lexical Resources, Pittsburgh, 2001; 
77-82. 
Collins, A. M., Quillian, M. R. Retrieval time 
from semantic memory, Journal of Verbal 
Learning and Verbal Behavior 1969; 8: 240-
248. 
Cruse, D. A. Lexical semantics. Cambridge 
University Press, Cambridge, UK, 1986. 
Cucchiarelli, A., Velardi, P. Automatic selection 
of class labels from a thesaurus for an effective 
semantic tagging of corpora. Proceedings of the 
5th Conference on Applied Natural Language 
Processing, Washington, 1997; 380-387. 
Ely, J. W., Osheroff, J. A., Gorman, PN., Ebell, 
M. H., Chambliss, M. L., Pifer, E. A., Stavri, P. 
Z. A taxonomy of generic clinical questions: 
classification study. British Medical Journal, 
2000; 321: 429-432. 
Fellbaum, C. (ed.). WordNet: an electronic lexical 
database. MIT Press, Cambridge, MA, 1998. 
Fellbaum, C. Distinguishing verb types in a lexi-
cal ontology. Proceedings of the Second Inter-
national Workshop on Generative Approaches 
to the Lexicon. ISSCO, Geneva, 2003. 
Fellbaum, C. English verbs as a semantic net. 
International Journal of Lexicography 1990; 3 
(4): 278-301. 
Fellbaum, C. On the semantics of troponymy. In: 
The Semantics of Relationships: An Inter-
disciplinary Perspective. R. Green, C. A. Bean, 
S. H. Myaeng (eds.), Dordrecht, Kluwer, 2002; 
23-34. 
Gonzalo, J., Verdejo, F., Chugur, I., Cigarran, J. 
Indexing with WordNet synsets can improve 
text retrieval. Proceedings of the COLING/ACL 
Workshop on Usage of WordNet in Natural 
Language Processing Systems, Montreal, 1998. 
Guha, R., Lenat, D., Pittman, K., Pratt, D., 
Shepherd, M. Cyc: A midterm report. 
Communications of the ACP 1990; 33 (8). 
Harabagiu, S. M., Moldovan, DI. A marker 
propagation text understanding and inference 
system. J. H. Stewman (ed.), Proceedings of the 
9th Florida Artificial Intelligence Research 
Symposium, Key West, 1996; 55-59. 
Jacquemart, P., Zweigenbaum, P. Towards a 
medical question-answering system: a 
feasibility study. In: P. Le Beux and R. Baud 
(eds.), Proceedings of Medical Informatics 
Europe, IOS Press, Amsterdam, 2003; 463-468. 
Kingsbury, P., Palmer, M. From TreeBank to 
PropBank. Proceedings of 3rd International 
Conference on Language Resources and 
Evaluation (LREC-2002), Las Palmas, Spain, 
2002. 
Lenat, D. Cyc: a large-scale investment in 
knowledge infrastructure. Communications of 
the ACM 1995; 38 (11). 
Magnini, B., Strapparava, C. Using WordNet to 
improve user modelling in a web document 
recommender system. Proceedings of the 
NAACL 2001 Workshop on WordNet and Other 
Lexical Resources, Pittsburgh, June 2001. 
McCray, A. T., Tse, T. Understanding search 
failures in consumer health information sys-
tems. Proceedings of the American Medical 
Informatics Symposium 2003: 430-4. 
Medin, D. L., Atran, S. (eds.) Folkbiology. 
Cambridge, MA: MIT Press, 1999. 
Miller, G. A. WordNet: a lexical database for 
English. Comm ACM 38, 11, November 1995; 
39-41. 
Patel, V. L., Arocha, J. F., Kushniruk, A. Patients? 
and physicians? understanding of health and 
biomedical concepts: relationship to the design 
of EMR systems. Journal of Biomedical 
Informatics, 2002; 35(1): 8-16. 
Pustejovsky, J. The generative lexicon. MIT 
Press, Cambridge, 1995. 
Rosch, E. Cognitive representations of semantic 
categories. Journal of Experimental Psychology, 
General 1975; 104: 192-253. 
Rosch, E. On the internal structure of perceptual 
and semantic categories. Cognitive Develop-
ment and the Acquisition of Language, T. E. 
Moore (ed.), Academic Press, New York, 1973. 
Rosch, E. Principles of categorization. In: 
Cognition and Categorization. E. Rosch and B. 
B. Lloyd (eds.), Erlbaum, Hillsdale, NJ, 1978. 
Slaughter, L. Semantic relationships in health 
consumer questions and physicians? answers: a 
basis for representing medical knowledge and 
for concept exploration interfaces. Doctoral 
dissertation, University of Maryland at College 
Park, 2002. 
Smith, B., K?hler, J., Kumar, A. On the 
application of formal principles to life science 
data: a case study in the Gene Ontology. 
Proceedings of DILS 2004 (Data Integration in 
the Life Sciences), (Lecture Notes in Computer 
Science), Berlin, Springer, 2004, in press. 
Smith B., Rosse C. The role of foundational 
relations in the alignment of biomedical 
ontologies. Proceedings of Medinfo, San 
Francisco, 7-11 September, 2004. 
Smith, C. A., Stavri, P. Z., Chapman, W. W. In 
their own words? A terminological analysis of 
e-mail to a cancer information service. 
Proceedings of AMIA Symp. 2002;: 697-701. 
Tse, A., Soergel, D. Procedures for mapping 
vocabularies from non-professional discourse: a 
case study: in ?consumer medical vocabulary?. 
Proceedings of the Annual Meeting of the 
American Society for Information, 2003. 
Tse, A. Y. Identifying and characterizing a 
consumer medical vocabulary. Doctoral 
dissertation, College of Information Studies, 
University of Maryland, College Park, 
Maryland, March 2003. 
Turcato, D., Fass, D., Tisher, G., Popowich, F. 
Fully automatic bilingual lexical acquisition 
from EuroWordNet. Proceedings of NAACL 
2001 Workshop on WordNet and Other Lexical 
Resources, Pittsburgh, June 2001. 
Xiao, C., R?sner, D. Finding high-frequent 
synonyms of domain-specific verbs in the 
English sub-language of MEDLINE abstracts 
using WordNet. Proc 2nd Global WordNet Conf 
(GWC 2004), Brno, Czech Republic, December 
2003; 242-247. 
Zeng, Q., Kogan, S., Ash, N., Greenes, R. A., 
Boxwala, A. A. Characteristics of consumer 
terminology for health information retrieval: A 
formal study of use of a health information 
service. Methods of Information in Medicine, in 
press.
 
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 54?59,
Prague, June 2007. c?2007 Association for Computational Linguistics
On the Role of Lexical and World Knowledge in RTE3 
Peter Clark1, William R. Murray1, John Thompson1, Phil Harrison1,  
Jerry Hobbs2, Christiane Fellbaum3  
1Boeing Phantom Works, The Boeing Company, Seattle, WA 98124 
2USC/ISI, 4676 Admiralty Way, Marina del Rey, CA 90292 
3Princeton University, NJ 08544 
{peter.e.clark,william.r.murray,john.a.thompson,philip.harrison}@boeing.com, 
hobbs@isi.edu, fellbaum@clarity.princeton.edu 
 
 
Abstract 
To score well in RTE3, and even more so 
to create good justifications for entailments, 
substantial lexical and world knowledge is 
needed. With this in mind, we present an 
analysis of a sample of the RTE3 positive 
entailment pairs, to identify where and 
what kinds of world knowledge are needed 
to fully identify and justify the entailment, 
and discuss several existing resources and 
their capacity for supplying that knowledge. 
We also briefly sketch the path we are fol-
lowing to build an RTE system (Our im-
plementation is very preliminary, scoring 
50.9% at the time of RTE). The contribu-
tion of this paper is thus a framework for 
discussing the knowledge requirements 
posed by RTE and some exploration of 
how these requirements can be met. 
1 Introduction 
The Pascal RTE site defines entailment between 
two texts T and H as holding "if, typically, a hu-
man reading T would infer that H is most likely 
true" assuming "common human understanding of 
language as well as common background knowl-
edge." While a few RTE3 entailments can be rec-
ognized using simple syntactic matching, the ma-
jority rely on significant amounts of this "common 
human understanding" of lexical and world knowl-
edge. Our goal in this paper is to analyze what that 
knowledge is, create a preliminary framework for 
it, and explore a few available sources for it. In the 
short term, such knowledge can be (and has been) 
used to drive semantic matching of the T and H 
dependency/parse trees and their semantic repre-
sentations, as many prior RTE systems perform, 
e.g., (Hickl et al, 2006). In the long term, com-
puters should be able to perform deep language 
understanding to build a computational model of 
the scenario being described in T, to reason about 
the entailment, answer further questions, and create 
meaningful justifications. With this longer term 
goal in mind, it is useful to explore the types of 
knowledge required. It also gives a snapshot of the 
kinds of challenges that RTE3 poses. 
 
The scope of this paper is to examine the underly-
ing lexical/world knowledge requirements of RTE, 
rather than the more syntactic/grammatical issues 
of parsing, coreference resolution, named entity 
recognition, punctuation, coordination, typographi-
cal errors, etc. Although there is a somewhat blurry 
line between the two, this separation is useful for 
bounding the analysis. It should be noted that the 
more syntactic issues are themselves vast in RTE, 
but here we will not delve into them. Instead, we 
will perform a thought experiment in which they 
have been handled correctly. 
2 Analysis 
Based on an analysis of 100 (25%) of the positive 
entailments in the RTE3 test set, we have divided 
the knowledge requirements into several rough 
categories, which we now present. We then sum-
marize the frequency with which examples in this 
sample fell into these categories. The examples 
below are fragments of the original test questions, 
abbreviated and occasionally simplified. 
2.1 Syntactic Matching 
In a few cases, entailment can be identified by syn-
tactic matching of T and H, for example: 
54
489.T "The Gurkhas come from Nepal and??  
489.H "The Gurkhas come from Nepal." 
Other examples include 299, 489, and 456. In 
some cases, the syntactic matching can be very 
complex, e.g., examples 152, 724. 
2.2 Synonyms 
Synonymy is often needed to recognize entailment, 
648.T "?go through ? licencing procedures..." 
648.H "?go through the licencing processes." 
Other examples include 286 ("dismiss"/"throw 
out"), 37 ("begin?/"start"), 236 ("wildfire"/"bush 
fire"), and, arguably, 462 ("revenue"/"proceeds"). 
2.3 Generalizations (Hypernyms) 
Similarly, subsumption (generalization) relation-
ships between word senses need to be recognized 
(whether or not a fixed set of senses are used), eg. 
148.T "Beverly served...at WEDCOR" 
148.H "Beverly worked for WEDCOR." 
Others include 178 ("succumbed" as a kind of 
"killed"), and 453 ("take over" as a kind of "buy"). 
2.4 Noun Redundancy 
Sometimes a noun in a compound can be dropped: 
607.T "single-run production process..." 
607.H "Single-run production..." 
Other examples include 269 ("increasing preva-
lence of" ? "increasing"), 604 ("mini-mill proc-
ess" ? "mini-mill"), and (at the phrase level) 668 
("all segments of the public" ? "the public"). 
2.5 Noun-Verb Relations  
Often derivationally related nouns and verbs occur 
in the pairs. To identify and justify the entailment, 
the relationship and its nature is needed, as in: 
 
480 "Marquez is a winner..." ?"Marquez won..." 
 
Other examples include 286 ("pirated", "piracy"), 
and 75 ("invent", "invention"). In some cases, the 
deverbal noun denotes the verb's event, in other 
cases it denotes one of the verb?s arguments (e.g., 
"winner" as the subject/agent of a "win" event).  
2.6 Compound Nouns 
Some examples require inferring the semantic rela-
tion between nouns in a compound, e.g., 
168 "Sirius CEO Karmazin" ? "Karmazin is an 
executive of Sirius" 
583 "physicist Hawking" ? "Hawking is a physi-
cist" 
In some cases this is straightforward, others require 
more detailed knowledge of the entities involved. 
2.7 Definitions 
Although there is somewhat of a fuzzy boundary 
between word and world knowledge, we draw this 
distinction here. Some examples of RTE pairs 
which require knowing word meanings are: 
667 "? found guilty..." ? "?convicted..." 
328 "sufferers of coeliac disease..." ? "coeliacs..." 
 
The second example is particularly interesting as 
many readers (and computers) will not have en-
countered the word "coeliacs" before, yet a person 
can reasonably infer its meaning on the fly from 
context and morphology - something challenging 
for a machine to do. Definitions of compound 
nouns are also sometimes needed, e.g., ?family 
planning? (612) and ?cough syrup? (80).  
2.8 World Knowledge: General 
A large number of RTE pairs require non-
definitional knowledge about the way the world 
(usually) is, e.g.,: 
273 "bears kill people"  ? "bears attack people" 
 
People recognize this entailment as they know 
(have heard about) how people might be killed by 
a bear, and hence can justify why the entailment is 
valid. (They know that the first step in the bear 
killing a person is for the bear to attack that person.) 
Some other examples are: 
499 "shot dead" ? "murder" 
705 "under a contract with" ? "cooperates with" 
721 "worked on the law" ? "discussed the law" 
731 "cut tracts of forest" ? "cut trees in the forest" 
732 "establishing tree farms"  
? "trees were planted" 
639 "X's plans for reorganizing"  
? "X intends to reorganize" 
328 "the diets must be followed by <person>"  
? "the diets are for <person>" 
722 X and Y vote for Z ? X and Y agree to Z. 
 
All these cases appeal to a person's world knowl-
edge concerning the situation being described. 
55
2.9 World Knowledge: Core Theories 
In addition to this more specific knowledge of the 
world, some RTE examples appeal to more general, 
fundamental knowledge (e.g., space, time, plans, 
goals). For example 
 
6.T "Yunupingu is one of the clan of..." 
6.H "Yunupingu is a member of..." 
 
appeals to a basic rule of set inclusion, and 10 (a 
negative entailment: "unsuccessfully sought elec-
tion" ? not elected) appeals to core notions of 
goals and achievement. Several examples appeal to 
core spatial knowledge, e.g.: 
 
491.T "...come from the high mountains of Nepal." 
491.H "...come from Nepal." 
 
178.T "...3 people in Saskatchewan succumbed to 
the storm." 
178.H "...a storm in Saskatchewan." 
 
491 appeals to regional inclusion (if X location Y, 
and Y is in Z, then X location Z), and 178 appeals 
to colocation (if X is at Y, and X physically inter-
acts with Z, then Z is at Y). Other spatial examples 
include 236 ("around Sydney" ? "near Sydney"), 
and 129 ("invasion of" ? "arrived in"). 
2.10 World Knowledge: Frames and Scripts 
Although loosely delineated, another category of 
world knowledge concerns stereotypical places, 
situations and the events which occur in them, with 
various representational schemes proposed in the 
AI literature, e.g., Frames (Minsky 1985), Scripts 
(Schank 1983). Some RTE examples require rec-
ognizing the implicit scenario ("frame", "script", 
etc.) which T describes to confirm the new facts or 
relationships introduced in H are valid. A first ex-
ample is: 
 
358.T "Kiesbauer was target of a letter bomb..."  
358.H "A letter bomb was sent to Kiesbauer." 
 
A person recognizes H as entailed by T because 
he/she knows the "script" for letter bombing, 
which includes sending the bomb in the mail. Thus 
a person could also recognize alternative verbs in 
358.H as valid (e.g., "mailed", "delivered") or in-
valid (e.g., "thrown at", "dropped on"), even 
though these verbs are all strongly associated with 
words in T. For a computer to fully explain the 
entailment, it would need similar knowledge.  
 
As a second example, consider: 
 
538.T "...the O. J. Simpson murder trial..." 
538.H "O. J. Simpson was accused of murder." 
 
Again, this requires knowing about trials: That 
they involve charges, a defendant, an accusation, 
etc., in order to validate H as an entailed expansion 
of T. In this example, there is also a second twist to 
it as the noun phrase in 538.T equally supports the 
hypothesis "O. J. Simpson was murdered." (e.g., 
consider replacing "O. J." with "Nicole"). It is only 
a reference elsewhere in the T sentence to "Simp-
son's attorneys" that suggests Simpson is still alive, 
and hence couldn't have been the victim, and hence 
must be the accused, that clarifies 538.H as being 
correct, a highly complex chain of reasoning.  
 
As a third example, consider: 
736.T "In a security fraud case, Milken was sen-
tenced to 10 years in prison." 
736.H "Milken was imprisoned for security fraud." 
 
This example is particularly interesting, as one 
needs to recognize security fraud as Milken's crime, 
a connection which not stated in T. A human 
reader will recognize the notion of sentencing, and 
thus expect to see a convict and his/her crime, and 
hence can align these expectations with T, validat-
ing H. Thus again, deep knowledge of sentencing 
is needed to understand and justify the entailment. 
 
Some other examples requiring world knowledge 
to validate their expansions, include 623 ("narcot-
ics-sniffing dogs" ? "dogs are used to sniff out 
narcotics"), and 11 ("the Nintendo release of the 
game" ? "the game is produced by Nintendo"). 
2.11 Implicative Verbs 
Some RTE3 examples contain complement-taking 
verbs that make an implication (either positive or 
negative) about the complement. For example: 
 
668 "A survey shows that X..." ? "X..." 
657 "...X was seen..." ? "...X..." 
725 ?...decided to X..." ? "...X..." 
716 "...have been unable to X..." ? "...do not X" 
56
 Table 1: Frequency of different entailment 
phenomena from a sample of 100 RTE3 pairs. 
In the first 3 the implication is positive, but in the 
last the implication is negative. (Nairn et al 2006) 
provide a detailed analysis of this type of behavior. 
In fact, this notion of implicature (one part of a 
sentence making an implication about another part) 
extends beyond single verbs, and there are some 
more complex examples in RTE3, e.g.: 
 
453 "...won the battle to X..." ? "...X..." 
 
784.T "X  reassures Russia it has nothing to fear..." 
784.H "Russia fears..." 
 
In this last example the implication behavior is 
quite complex: (loosely) If X reassures Y of Z, 
then Y is concerned about not-Z. 
2.12 Metonymy/Transfer 
In some cases, language allows us to replace a 
word (sense) with a closely related word (sense):  
708.T "Revenue from stores funded..." 
708.H "stores fund..." 
 
Rules for metonymic transfer, e.g., (Fass 2000), 
can be used to define which transfers are allowed. 
Another example is 723 ??pursue its drive to-
wards X? ? ??pursue X?. 
2.13 Idioms/Protocol/Slang 
Finally, some RTE pairs rely on understanding 
idioms, slang, or special protocols used in lan-
guage, for example: 
 
12 "Drew served as Justice. Kennon returned to 
claim Drew's seat"  ? "Kennon served as Justice." 
486 "name, 1890-1970" ? "name died in 1970" 
408 "takes the title of" ? "is" 
688 "art finds its way back" ? "art gets returned" 
 
The phrases in these examples all have special 
meaning which cannot be easily derived composi-
tionally from their words, and thus require special 
handling within an entailment system. 
2.14 Frequency Statistics 
Table 1 shows the number of positive entailments 
in our sample of 100 that fell into the different 
categories (some fell into several). While there is a 
certain subjectivity in the boundaries of the catego-
ries, the most significant observation is that very 
few entailments depend purely on syntactic ma-
nipulation and simple lexical knowledge (syno-
nyms, hypernyms), and that the vast majority of 
entailments require significant world knowledge, 
highlighting one of the biggest challenges of RTE. 
In addition, the category of general world knowl-
edge -- small, non-definitional facts about the way 
the world (usually) is -- is the largest, suggesting 
that harvesting and using this kind of knowledge 
should continue to be a priority for improving per-
formance on RTE-style tasks. 
3 Sources of World Knowledge 
While there are many potential sources of the 
knowledge that we have identified, we describe 
three in a bit more detail and how they relate to the 
earlier analysis. 
3.1 WordNet 
WordNet (Fellbaum, 1998) is one of the most ex-
tensively used resources in RTE already and in 
computational linguistics in general. Despite some 
well-known problems, it provides broad coverage 
of several key relationships between word senses 
(and words), in particular for synonyms, hy-
pernyms (generalizations), meronyms (parts), and 
semantically (?morphosemantically?) related 
forms. From the preceding analysis, WordNet does 
contain the synonyms {"procedure","process"}, 
{"dismiss","throw out"}, {"begin","start"}, but 
does not contain the compound "wild fire" and 
(strictly correctly) does not contain {"reve-
nue","proceeds"} as synonyms. In addition, the 
57
three hypernyms mentioned in the earlier analysis 
are in WordNet. WordNet alo links (via the ?mor-
phosemantic? link) the 3 noun-verb pairs men-
tioned earlier (win/winner, pirated/piracy, in-
vent/invention) ? however it does not currently 
distinguish the nature of that link (e.g., agent, re-
sult, event). WordNet is currently being expanded 
to include this information, as part of the 
AQUAINT program.  
 
Two independently developed versions of the 
WordNet glosses expressed in logic are also avail-
able: Extended WordNet (Moldovan and Rus, 
2001) and a version about to be released with 
WordNet3.0 (again developed under AQUAINT). 
These in principle can help with definitional 
knowledge. From our earlier analysis, it turns out 
"convicted" is conveniently defined in WordNet as 
"pronounced or proved guilty" potentially bridging 
the gap for pair 667, although there are problems 
with the logical interpretation of this particular 
gloss in both resources mentioned. WordNet does 
have "coeliac", but not in the sense of a person 
with coeliac disease1. 
 
In addition, several existing ?core theories? (e.g., 
TimeML, IKRIS) that can supply some of the fun-
damental knowledge mentioned earlier (e.g., space, 
time, goals) are being connected to WordNet under 
the AQUAINT program. 
3.2 The DIRT Paraphrase Database 
Paraphrases have been used successfully by several 
RTE systems (e.g., Hickl et al, 2005). With re-
spect to our earlier analysis, we examined Lin and 
Pantel's (2001) paraphrase database built with their 
system DIRT, containing 12 million entries. While 
there is of course noise in the database, it contains 
a remarkable amount of sensible world knowledge 
as well as syntactic rewrites, albeit encoded as 
shallow rules lacking word senses. 
 
Looking at the examples from our earlier analysis 
of general world knowledge, we find that three are 
supported by paraphrase rules in the database: 
273: X kills Y ? X attacks Y 
499: X shoots Y ? X murders Y 
                                                 
1  This seems to be an accidental gap; WordNet contains 
many interlinked disease-patient noun pairs, incl. "dia-
betes-diabetic," "epilepsy-eplileptic," etc. 
721: X works on Y ? X discusses Y 
 
And one that could be is not, namely: 
705: X is under a contract with Y ? X coop-
erates with Y (not in the database) 
 
Other examples are outside the scope of DIRT's 
approach (i.e., ?X pattern1 Y? ? ?X pattern2 Y?), 
but nonetheless the coverage is encouraging. 
3.3 FrameNet 
In our earlier analysis, we identified knowledge 
about stereotypical situations and their events as 
important for RTE. FrameNet (Baker et al 1998) 
attempts to encode this knowledge. FrameNet was 
used with some success in RTE2 by Burchardt and 
Frank (2005). FrameNet's basic unit - a Frame - is 
a script-like conceptual schema that refers to a 
situation, object, or event along with its partici-
pants (Frame Elements), identified independent of 
their syntactic configuration. 
 
We earlier discussed how 538.T "...the O. J. Simp-
son murder trial..." might entail 538.H "O. J. Simp-
son was accused of murder." This case applies to 
FrameNet?s Trial frame, which includes the Frame 
Elements Defendant and Charges, with Charges 
being defined as "The legal label for the crime that 
the Defendant is accused of.", thus stating the rela-
tionship between the defendant and the charges, 
unstated in T but made explicit in H, validating the 
entailment. However,  the lexical units instantiat-
ing the Frame Elements are not yet disambiguated 
against a lexical database, limiting full semantic 
understanding. Moreover, FrameNet's  world 
knowledge is stated informally in English descrip-
tions, though it may be possible to convert these to 
a more machine-processable form either manually 
or automatically. 
3.4 Other Resources 
We have drawn attention to these three resources 
as they provide some answers to the requirements 
identified earlier. Several other publicly available 
resources could also be of use, including VerbNet 
(Univ Colorado at Boulder), the Component Li-
brary (Univ Texas at Austin), OpenCyc (Cycorp), 
SUMO, Stanford's additions to WordNet, and the 
Tuple Database (Boeing, following Schubert's 
2002 approach), to name but a few. 
58
4 Sketch of our RTE System 
Although not the primary purpose of this paper, we 
briefly sketch the path we are following to build an 
RTE system able to exploit the above resources. 
Our implementation is very preliminary, scoring 
50.9% at the time of RTE and 52.6% now (55.0% 
on the 525 examples it is able to analyze, guessing 
"no entailment" for the remainder). Nevertheless, 
the following shows the direction we are moving in 
 
Like several other groups, our basic approach is to 
generate logic for the T and H sentences, and then 
explore the application of inference rules to elabo-
rate T, or transform H, until H matches T. Parsing 
is done using a broad coverage chart parser. Sub-
sequently, an abstracted form of the parse tree is 
converted into a logical form, for example: 
 299.H "Tropical storms cause severe damage." 
subject(_Cause1, _Storm1) 
sobject(_Cause1, _Damage1) 
mod(_Storm1, _Tropical1) 
mod(_Damage1, _Severe1) 
input-word(_Storm1, "storm", noun) 
[same for other words] 
 
Entailment is determined if every clause in the se-
mantic representation of H semantically matches 
(subsumes) some clause in T. Two variables in a 
clause match if their input words are the same, or 
some WordNet sense of one is the same as or a 
hypernym of the other. In addition, the system 
searches for DIRT paraphrase rules that can trans-
form the sentences into a form which can then 
match. The explicit use of WordNet and DIRT re-
sults in comprehensible, machine-generated justifi-
cations when entailments are found, , e.g.,: 
 
T: "The Salvation Army operates the shelter under 
a contract with the county." 
H: "The Salvation Army collaborates with the 
county." 
Yes! Justification: I have general knowledge that: 
  IF X works with Y THEN X collaborates with Y 
Here: X = the Salvation Army, Y = the county 
Thus, here: 
        I can see from T: the Salvation Army works 
with the county (because "operate" and "work" 
mean roughly the same thing) 
Thus it follows that:  
  The Salvation Army collaborates with the county.  
We are continuing to develop our system and ex-
pand the number of knowledge sources it uses.  
5 Summary 
To recognize and justify textual entailments, and 
ultimately understand language, considerable lexi-
cal and world knowledge is needed. We have pre-
sented an analysis of some of the knowledge re-
quirements of RTE3, and commented on some 
available sources of that knowledge. The analysis 
serves to highlight the depth and variety of knowl-
edge demanded by RTE3, and contributes a rough 
framework for organizing these requirements. Ul-
timately, to fully understand language, extensive 
knowledge of the world (either manually or auto-
matically acquired) is needed. From this analysis, 
RTE3 is clearly providing a powerful driving force 
for research in this direction. 
Acknowledgements 
This work was performed under the DTO 
AQUAINT program, contract N61339-06-C-0160.  
References 
Collin Baker, Charles Fillmore, and John Lowe. 1998. 
"The Berkeley FrameNet Project". Proc 36th ACL 
Aljoscha Burchardt and Anette Frank. 2005. Approach-
ing Textual Entailment with LFG and FrameNet 
Frames. in 2nd PASCAL RTE Workshop, pp 92-97. 
Dan Fass. 1991. "Met*: A Method for Discriminating 
Metonymy and Metaphor by Computer". In Compu-
tational Linguistics 17 (1), pp 49-90. 
Christiane Fellbaum. 1998. ?WordNet: An Electronic 
Lexical Database.? The MIT Press. 
Andrew Hickl, Jeremy Bensley, John Williams, Kirk 
Roberts, Bryan Rink, and Ying Shi. ?Recognizing 
Textual Entailment with LCC?s Groundhog System?, 
in Proc 2nd PASCAL RTE Workshop, pp 80-85. 
Dekang Lin and Patrick Pantel. 2001. "Discovery of 
Inference Rules for Question Answering". Natural 
Language Engineering 7 (4) pp 343-360.  
Marvin Minsky. 1985. "A Framework for Representing 
Knowledge". In Readings in Knowledge Repn. 
Dan Moldovan and Vasile Rus, 2001. Explaining An-
swers with Extended WordNet, ACL 2001.  
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen. 
2006. Computing Relative Polarity for Textual Infer-
ence. In the Proceedings of ICoS-5. 
Len Schubert. 2002. "Can we Derive General World 
Knowledge from Texts?", Proc. HLT'02, pp84-87. 
59
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 93?98,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semeval 2007 Task 18: Arabic Semantic Labeling
Mona Diab
Columbia University
mdiab@cs.columbia.edu
Christiane Fellbaum
Princeton University
fellbaum@clarity.princeton.edu
Musa Alkhalifa
University of Barcelona
musa@thera-clic.com
Aous Mansouri
University of Colorado, Boulder
aous.mansouri@colorado.edu
Sabri Elkateb
University of Manchester
Sabri.Elkateb@manchester.ac.uk
Martha Palmer
University of Colorado, Boulder
martha.palmer@colorado.edu
Abstract
In this paper, we present the details of the
Arabic Semantic Labeling task. We describe
some of the features of Arabic that are rele-
vant for the task. The task comprises two
subtasks: Arabic word sense disambiguation
and Arabic semantic role labeling. The task
focuses on modern standard Arabic.
1 Introduction
Recent years have witnessed a surge in available re-
sources for the Arabic language.1 The computa-
tional linguistics community is just about starting
to exploit these resources toward several interesting
scientific and engineering goals. The Arabic lan-
guage is interesting from a computational linguistic
perspective. It is significantly different from English
hence creating a challenge for existing technology to
be easily portable to Arabic. The Arabic language is
inherently complex due to its rich morphology and
relative free word order. Moreover, with the exis-
tence of several interesting varieties, the spoken ver-
naculars, we are witnessing the emergence of written
dialectal Arabic everyday on the web, however there
are no set standards for these varieties.
We have seen many successful strides towards
functional systems for Arabic enabling technolo-
gies, but we are yet to read about large Arabic NLP
applications such as Machine Translation and Infor-
mation Extraction that are on par with performance
on the English language. The problem is not the ex-
istence of data, but rather the existence of data an-
notated with the relevant level of information that
1Author 1 is supported by DARPA contract Contract No.
HR0011-06-C-0023. Authors 2, 3 and 4 are supported by the
US Central Intelligence Service.
is useful for NLP. This task attempts a step towards
the goal of creating resources that could be useful
for such applications.
In this task, we presented practitioners in the field
with challenge of labeling Arabic text with seman-
tic labels. The labels constitute two levels of gran-
ularity: sense labels and semantic role labels. We
specifically chose data that overlapped such that we
would have the same data annotated for different
types of semantics, lexical and structural. The over-
all task of Arabic Semantic Labeling was subdivided
into 4 sub-tasks: Arabic word sense disambiguation
(AWSD), English to Arabic WSD task (EAWSD),
argument detection within the context of semantic
role labeling, and argument semantic role classifica-
tion.
Such a set of tasks would not have been feasible
without the existence of several crucial resources:
the Arabic Treebank (ATB) (Maamouri et al,
2004), the Arabic WordNet (AWN) (Elkateb et
al., 2006), and the Pilot Arabic Propbank
(APB).2
This paper is laid out as follows: Section 2 will
describe some facts about the Arabic language; Sec-
tion 3 will present the overall description of the
tasks; Section 4 describes the word sense disam-
biguation task; Section 5 describes the semantic role
labeling task.
2 The Arabic Language
In the context of our tasks, we only deal with MSA.3
Arabic is a Semitic language. It is known for its
templatic morphology where words are made up of
2Funded by DARPA subcontract to BBN Inc. to University
of Colorado, LDC-UPenn and Columbia University.
3In this paper we use MSA and Arabic interchangeably.
93
roots and affixes. Clitics agglutinate to words. For
instance, the surface word  	
  wbHsnAthm4
?and by their virtues[fem.]?, can be split into the con-
junction w ?and?, preposition b ?by?, the stem HsnAt
?virtues [fem.]?, and possessive pronoun hm ?their?.
Arabic is different from English from both the mor-
phological and syntactic perspectives which make it
a challenging language to the existing NLP technol-
ogy that is too tailored to the English language.
From the morphological standpoint, Arabic ex-
hibits rich morphology. Similar to English, Ara-
bic verbs are marked explicitly for tense, voice and
person, however in addition, Arabic marks verbs
with mood (subjunctive, indicative and jussive) in-
formation. For nominals (nouns, adjectives, proper
names), Arabic marks case (accusative, genitive and
nominative), number, gender and definiteness fea-
tures. Depending on the genre of the text at hand,
not all of those features are explicitly marked on nat-
urally occurring text.
Arabic writing is known for being underspecified
for short vowels. Some of the case, mood and voice
features are marked only using short vowels. Hence,
if the genre of the text were religious such as the
Quran or the Bible, or pedagogical such as children?s
books in Arabic, it would be fully specified for all
the short vowels to enhance readability and disam-
biguation.
From the syntactic standpoint, Arabic, different
from English, is considered a pro-drop language,
where the subject of a verb may be implicitly en-
coded in the verb morphology. Hence, we observe
sentences such as       Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 123?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre
IXA NLP group
UBC
Donostia, Basque Country
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
oier.lopezdelacalle@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Andrea Marchetti
IIT
CNR
Pisa, Italy
andrea.marchetti@iit.cnr.it
Antonio Toral
ILC
CNR
Pisa, Italy
antonio.toral@ilc.cnr.it
Piek Vossen
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambiguation
systems present new challenges. The diffi-
culties found by supervised systems to adapt
might change the way we assess the strengths
and weaknesses of supervised and knowledge-
based WSD systems. Unfortunately, all ex-
isting evaluation datasets for specific domains
are lexical-sample corpora. With this paper
we want to motivate the creation of an all-
words test dataset for WSD on the environ-
ment domain in several languages, and present
the overall design of this SemEval task.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in the last Senseval and Semeval competitions (Kil-
garriff, 2001; Mihalcea et al, 2004; Pradhan et al,
2007). Specific domains pose fresh challenges to
WSD systems: the context in which the senses occur
might change, distributions and predominant senses
vary, some words tend to occur in fewer senses in
specific domains, and new senses and terms might
be involved. Both supervised and knowledge-based
systems are affected by these issues: while the first
suffer from different context and sense priors, the
later suffer from lack of coverage of domain-related
words and information.
Domain adaptation of supervised techniques is a
hot issue in Natural Language Processing, includ-
ing Word Sense Disambiguation. Supervised Word
Sense Disambiguation systems trained on general
corpora are known to perform worse when applied
to specific domains (Escudero et al, 2000; Mart??nez
and Agirre, 2000), and domain adaptation tech-
niques have been proposed as a solution to this prob-
lem with mixed results.
Current research on applying WSD to specific do-
mains has been evaluated on three available lexical-
sample datasets (Ng and Lee, 1996; Weeber et al,
2001; Koeling et al, 2005). This kind of dataset
contains hand-labeled examples for a handful of se-
lected target words. As the systems are evaluated on
a few words, the actual performance of the systems
over complete texts can not be measured. Differ-
ences in behavior of WSD systems when applied to
lexical-sample and all-words datasets have been ob-
served on previous Senseval and Semeval competi-
tions (Kilgarriff, 2001; Mihalcea et al, 2004; Prad-
han et al, 2007): supervised systems attain results
on the high 80?s and beat the most frequent base-
line by a large margin for lexical-sample datasets,
but results on the all-words datasets were much more
modest, on the low 70?s, and a few points above the
most frequent baseline.
Thus, the behaviour of WSD systems on domain-
specific texts is largely unknown. While some words
could be supposed to behave in similar ways, and
thus be amenable to be properly treated by a generic
123
WSD algorithm, other words have senses closely
linked to the domain, and might be disambiguated
using purpose-built domain adaptation strategies (cf.
Section 4). While it seems that domain-specific
WSD might be a tougher problem than generic
WSD, it might well be that domain-related words
are easier to disambiguate.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain, that of
environment-related texts. The paper is structured
as follows. The next section presents current lexi-
cal sample datasets for domain-specific WSD. Sec-
tion 3 presents some possible settings for domain
adaptation. Section 4 reviews the state-of-the art in
domain-specific WSD. Section 5 presents the design
of our task, and finally, Section 6 draws some con-
clusions.
2 Specific domain datasets available
We will briefly present the three existing datasets
for domain-related studies in WSD, which are all
lexical-sample.
The most commonly used dataset is the Defense
Science Organization (DSO) corpus (Ng and Lee,
1996), which comprises sentences from two differ-
ent corpora. The first is the Wall Street Journal
(WSJ), which belongs to the financial domain, and
the second is the Brown Corpus (BC) which is a bal-
anced corpora of English usage. 191 polysemous
words (nouns and verbs) of high frequency in WSJ
and BC were selected and a total of 192,800 occur-
rences of these words were tagged with WordNet 1.5
senses, more than 1,000 instances per word in aver-
age. The examples from BC comprise 78,080 oc-
currences of word senses, and examples from WSJ
consist on 114,794 occurrences. In domain adapta-
tion experiments, the Brown Corpus examples play
the role of general corpora, and the examples from
the WSJ play the role of domain-specific examples.
Koeling et al (2005) present a corpus were the
examples are drawn from the balanced BNC cor-
pus (Leech, 1992) and the SPORTS and FINANCES
sections of the newswire Reuters corpus (Rose et al,
2002), comprising around 300 examples (roughly
100 from each of those corpora) for each of the 41
nouns. The nouns were selected because they were
salient in either the SPORTS or FINANCES domains,
or because they had senses linked to those domains.
The occurrences were hand-tagged with the senses
from WordNet version 1.7.1 (Fellbaum, 1998). In
domain adaptation experiments the BNC examples
play the role of general corpora, and the FINANCES
and SPORTS examples the role of two specific do-
main corpora.
Finally, a dataset for biomedicine was developed
by Weeber et al (2001), and has been used as
a benchmark by many independent groups. The
UMLS Metathesaurus was used to provide a set of
possible meanings for terms in biomedical text. 50
ambiguous terms which occur frequently in MED-
LINE were chosen for inclusion in the test set. 100
instances of each term were selected from citations
added to the MEDLINE database in 1998 and man-
ually disambiguated by 11 annotators. Twelve terms
were flagged as ?problematic? due to substantial dis-
agreement between the annotators. In addition to the
meanings defined in UMLS, annotators had the op-
tion of assigning a special tag (?none?) when none
of the UMLS meanings seemed appropriate.
Although these three corpora are useful for WSD
research, it is difficult to infer which would be the
performance of a WSD system on full texts. The
corpus of Koeling et al, for instance, only includes
words which where salient for the target domains,
but the behavior of WSD systems on other words
cannot be explored. We would also like to note that
while the biomedicine corpus tackles scholarly text
of a very specific domain, the WSJ part of the DSO
includes texts from a financially oriented newspaper,
but also includes news of general interest which have
no strict relation to the finance domain.
3 Possible settings for domain adaptation
When performing supervised WSD on specific do-
mains the first setting is to train on a general domain
data set and to test on the specific domain (source
setting). If performance would be optimal, this
would be the ideal solution, as it would show that a
generic WSD system is robust enough to tackle texts
from new domains, and domain adaptation would
not be necessary.
The second setting (target setting) would be to
train the WSD systems only using examples from
124
the target domain. If this would be the optimal set-
ting, it would show that there is no cost-effective
method for domain adaptation. WSD systems would
need fresh examples every time they were deployed
in new domains, and examples from general do-
mains could be discarded.
In the third setting, the WSD system is trained
with examples coming from both the general domain
and the specific domain. Good results in this setting
would show that supervised domain adaptation is
working, and that generic WSD systems can be sup-
plemented with hand-tagged examples from the tar-
get domain.
There is an additional setting, where a generic
WSD system is supplemented with untagged exam-
ples from the domain. Good results in this setting
would show that semi-supervised domain adapta-
tion works, and that generic WSD systems can be
supplemented with untagged examples from the tar-
get domain in order to improve their results.
Most of current all-words generic supervised
WSD systems take SemCor (Miller et al, 1993) as
their source corpus, i.e. they are trained on SemCor
examples and then applied to new examples. Sem-
Cor is the largest publicly available annotated cor-
pus. It?s mainly a subset of the Brown Corpus, plus
the novel The Red Badge of Courage. The Brown
corpus is balanced, yet not from the general domain,
as it comprises 500 documents drawn from differ-
ent domains, each approximately 2000 words long.
Although the Brown corpus is balanced, SemCor is
not, as the documents were not chosen at random.
4 State-of-the-art in WSD for specific
domains
Initial work on domain adaptation for WSD sys-
tems showed that WSD systems were not able to
obtain better results on the source or adaptation set-
tings compared to the target settings (Escudero et
al., 2000), showing that a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would not be useful when moved to new do-
mains.
Escudero et al (2000) tested the supervised adap-
tation scenario on the DSO corpus, which had exam-
ples from the Brown Corpus and Wall Street Journal
corpus. They found that the source corpus did not
help when tagging the target corpus, showing that
tagged corpora from each domain would suffice, and
concluding that hand tagging a large general corpus
would not guarantee robust broad-coverage WSD.
Agirre and Mart??nez (2000) used the same DSO cor-
pus and showed that training on the subset of the
source corpus that is topically related to the target
corpus does allow for domain adaptation, obtaining
better results than training on the target data alone.
In (Agirre and Lopez de Lacalle, 2008), the au-
thors also show that state-of-the-art WSD systems
are not able to adapt to the domains in the context
of the Koeling et al (2005) dataset. While WSD
systems trained on the target domain obtained 85.1
and 87.0 of precision on the sports and finances do-
mains, respectively, the same systems trained on the
BNC corpus (considered as a general domain cor-
pus) obtained 53.9 and 62.9 of precision on sports
and finances, respectively. Training on both source
and target was inferior that using the target examples
alone.
Supervised adaptation
Supervised adaptation for other NLP tasks has been
widely reported. For instance, (Daume? III, 2007)
shows that a simple feature augmentation method
for SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously explored
more sophisticated methods (Daume? III and Marcu,
2006; Chelba and Acero, 2004). In contrast, (Agirre
and Lopez de Lacalle, 2009) reimplemented this
method and showed that the improvement on WSD
in the (Koeling et al, 2005) data was marginal.
Better results have been obtained using purpose-
built adaptation methods. Chan and Ng (2007) per-
formed supervised domain adaptation on a manu-
ally selected subset of 21 nouns from the DSO cor-
pus. They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding just
30% of the target data to the source examples the
same precision as the full combination of target and
source data could be achieved. They also showed
that using the source corpus significantly improved
results when only 10%-30% of the target corpus
was used for training. In followup work (Zhong et
125
Projections for 2100 suggest that temperature in Europe will have risen by between 2 to 6.3 C above 1990
levels. The sea level is projected to rise, and a greater frequency and intensity of extreme weather events are
expected. Even if emissions of greenhouse gases stop today, these changes would continue for many decades
and in the case of sea level for centuries. This is due to the historical build up of the gases in the atmosphere
and time lags in the response of climatic and oceanic systems to changes in the atmospheric concentration
of the gases.
Figure 1: Sample text from the environment domain.
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation ex-
periment. They significantly reduced the effort of
hand-tagging, but only obtained positive domain-
adaptation results for smaller fractions of the target
corpus.
In (Agirre and Lopez de Lacalle, 2009) the au-
thors report successful adaptation on the (Koeling
et al, 2005) dataset on supervised setting. Their
method is based on the use of unlabeled data, re-
ducing the feature space with SVD, and combina-
tion of features using an ensemble of kernel meth-
ods. They report 22% error reduction when using
both source and target data compared to a classifier
trained on target the target data alone, even when the
full dataset is used.
Semi-supervised adaptation
There are less works on semi-supervised domain
adaptation in NLP tasks, and fewer in WSD task.
Blitzer et al (2006) used Structural Correspondence
Learning and unlabeled data to adapt a Part-of-
Speech tagger. They carefully select so-called pivot
features to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source and
target domains. Agirre and Lopez de Lacalle (2008)
show that methods based on SVD with unlabeled
data and combination of distinct feature spaces pro-
duce positive semi-supervised domain adaptation re-
sults for WSD.
Unsupervised adaptation
In this context, we take unsupervised to mean
Knowledge-Based methods which do not require
hand-tagged corpora. The predominant sense acqui-
sition method was succesfully applied to specific do-
mains in (Koeling et al, 2005). The methos has two
steps: In the first, a corpus of untagged text from the
target domain is used to construct a thesaurus of sim-
ilar words. In the second, each target word is disam-
biguated using pairwise WordNet-based similarity
measures, taking as pairs the target word and each of
the most related words according to the thesaurus up
to a certain threshold. This method aims to obtain,
for each target word, the sense which is the most
predominant for the target corpus. When a general
corpus is used, the most predominant sense in gen-
eral is obtained, and when a domain-specific corpus
is used, the most predominant sense for that corpus
is obtained (Koeling et al, 2005). The main motiva-
tion of the authors is that the most frequent sense is a
very powerful baseline, but it is one which requires
hand-tagging text, while their method yields simi-
lar information automatically. The results show that
they are able to obtain good results. In related work,
(Agirre et al, 2009) report improved results using
the same strategy but applying a graph-based WSD
method, and highlight the domain-adaptation poten-
tial of unsupervised knowledge-based WSD systems
compared to supervised WSD.
5 Design of the WSD-domain task
This task was designed in the context of Ky-
oto (Piek Vossen and VanGent, 2008)1, an Asian-
European project that develops a community plat-
form for modeling knowledge and finding facts
across languages and cultures. The platform op-
erates as a Wiki system with an ontological sup-
port that social communities can use to agree on the
meaning of terms in specific domains of their inter-
est. Kyoto will focus on the environmental domain
because it poses interesting challenges for informa-
tion sharing, but the techniques and platforms will
be independent of the application domain. Kyoto
1http://www.kyoto-project.eu/
126
will make use of semantic technologies based on
ontologies and WSD in order to extract and repre-
sent relevant information for the domain, and is thus
interested on measuring the performance of WSD
techniques on this domain.
The WSD-domain task will comprise comparable
all-words test corpora on the environment domain.
Texts from the European Center for Nature Con-
servation2 and Worldwide Wildlife Forum3 will be
used in order to build domain specific test corpora.
We will select documents that are written for a gen-
eral but interested public and that involve specific
terms from the domain. The document content will
be comparable across languages. Figure 1 shows an
example in English related to global warming.
The data will be available in a number of lan-
guages: English, Dutch, Italian and Chinese. The
sense inventories will be based on wordnets of the
respective languages, which will be updated to in-
clude new vocabulary and senses. The test data will
comprise three documents of around 2000 words
each for each language. The annotation procedure
will involve double-blind annotation plus adjudica-
tion, and inter-tagger agreement data will be pro-
vided. The formats and scoring software will fol-
low those of Senseval-34 and SemEval-20075 En-
glish all-words tasks.
There will not be training data available, but par-
ticipants are free to use existing hand-tagged cor-
pora and lexical resources (e.g. SemCor and pre-
vious Senseval and SemEval data). We plan to make
available a corpus of documents from the same do-
main as the selected documents, as well as wordnets
updated to include the terms and senses in the se-
lected documents.
6 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. Unfor-
tunately, all existing evaluation datasets for specific
2http://www.ecnc.org
3http://www.wwf.org
4http://www.senseval.org/senseval3
5http://nlp.cs.swarthmore.edu/semeval/
domains are lexical-sample corpora. With this paper
we have motivated the creation of an all-words test
dataset for WSD on the environment domain in sev-
eral languages, and presented the overall design of
this SemEval task.
Further details can be obtained from the Semeval-
20106 website, our task website7, and in our distri-
bution list8
7 Acknowledgments
The organization of the task is partially funded
by the European Commission (KYOTO FP7 ICT-
2007-211423) and the Spanish Research Depart-
ment (KNOW TIN2006-15049-C03-01).
References
Eneko Agirre and Oier Lopez de Lacalle. 2008. On ro-
bustness and domain adaptation using SVD for word
sense disambiguation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 17?24, Manchester, UK, August.
Coling 2008 Organizing Committee.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Super-
vised domain adaptation for wsd. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-09).
E. Agirre, O. Lopez de Lacalle, and A. Soroa. 2009.
Knowledge-based WSD and specific domains: Per-
forming over supervised WSD. In Proceedings of IJ-
CAI, Pasadena, USA.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
49?56, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Barcelona, Spain.
6http://semeval2.fbk.eu/
7http://xmlgroup.iit.cnr.it/SemEval2010/
8http://groups.google.com/groups/wsd-domain
127
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Gerard Escudero, Lluiz Ma?rquez, and German Rigau.
2000. An Empirical Study of the Domain Dependence
of Supervised Word Sense Disambiguation Systems.
Proceedings of the joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
A. Kilgarriff. 2001. English Lexical Sample Task De-
scription. In Proceedings of the Second International
Workshop on evaluating Word Sense Disambiguation
Systems, Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense
acquisition. In Proceedings of the Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing.
HLT/EMNLP, pages 419?426, Ann Arbor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
David Mart??nez and Eneko Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. Conference
on Empirical Method in Natural Language.
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004.
The Senseval-3 English lexical sample task. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Human Language Technology Workshop. Dis-
tributed as Human Language Technology by San Ma-
teo, CA: Morgan Kaufmann Publishers., pages 303?
308, Princeton, NJ.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computationla Linguistics (ACL), pages 40?47.
Nicoletta Calzolari Christiane Fellbaum Shu-kai Hsieh
Chu-Ren Huang Hitoshi Isahara Kyoko Kanzaki An-
drea Marchetti Monica Monachini Federico Neri
Remo Raffaelli German Rigau Maurizio Tescon
Piek Vossen, Eneko Agirre and Joop VanGent. 2008.
Kyoto: a system for mining, structuring and distribut-
ing knowledge across languages and cultures. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of
the Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 87?92, Prague, Czech
Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volumen 1: from Yes-
terday?s News to Tomorrow?s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-2002),
pages 827?832, Las Palmas, Canary Islands.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In Proceedings of the
AMAI Symposium, pages 746?750, Washington, DC.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An em-
pirical study. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1002?1010, Honolulu, Hawaii, October.
Association for Computational Linguistics.
128
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 125?129,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
WordNet and FrameNet as Complementary Resources for Annotation
Collin F. Baker
International Computer Science Institute
1947 Center St., Berkeley, California 94704
collinb@icsi.berkeley.edu
Christiane Fellbaum
Princeton University
Princeton, NJ 08540-5233
fellbaum@princeton.edu
Abstract
WordNet and FrameNet are widely used lexi-
cal resources, but they are very different from
each other and are often used in completely
different ways in NLP. In a case study in which
a short passage is annotated in both frame-
works, we show how the synsets and defini-
tions of WordNet and the syntagmatic infor-
mation from FrameNet can complement each
other, forming a more complete representa-
tion of the lexical semantic of a text than ei-
ther could alone. Close comparisons between
them also suggest ways in which they can be
brought into alignment.
1 Background and motivation
FrameNet and WordNet are two lexical databases that
are widely used for NLP, often in conjunction. Because
of their complementary designs they are obvious candi-
dates for alignment, and an exploratory research project
within the larger context of the semantic annotation of
the the American national Corpus is currently under-
way. We give specific illustrative examples of annota-
tions against both resources, highlighting their different
contributions towards a rich semantic analysis.
WordNet (WN):1 (Fellbaum, 1998), is a large elec-
tronic lexical database of English. Originally con-
ceived as a full-scale model of human semantic orga-
nization, it was quickly embraced by the Natural Lan-
guage Processing (NLP) community, a development
that guided its subsequent growth and design. Word-
Net has become the lexical database of choice for NLP
and has been incorporated into other language tools,
including VerbNet (Kipper et al, 2000) and OntoNotes
(Hovy et al, 2006). Numerous on-line dictionaries, in-
cluding Google?s ?define? function, rely significantly
on WordNet.
WordNet?s coverage is sometimes criticized as be-
ing too fine-grained for automatic processing, though
its inventory is not larger than that of a standard col-
legiate dictionary. But the present limitation of auto-
matic WSD cannot be entirely blamed on existing sys-
tems; for example, Fellbaum and Grabowski (1997)
1http://wordnet.princeton.edu
have shown that humans, too, have difficulties identi-
fying context-appropriate dictionary senses. One an-
swer is clearly that meanings do not exist outside con-
texts. Furthermore, although WN does contain ?sen-
tence frames? such as ?Somebody ?-s something?
for a transitive verb with a human agent, it provides
little syntagmatic information, except for what can
be gleaned from the example sentences. WordNet?s
great strength is its extensive coverage, with more than
117,000 synonym sets (synsets), each with a definition
and relations to other synsets covering almost all the
general vocabulary of English.
FrameNet (FN):2 (Fontenelle, 2003) is a lexical
resource organized not around words per se, but se-
mantic frames (Fillmore, 1976): characterizations of
events, relations, and states which are the conceptual
basis for understanding groups of word senses, called
lexical units (LUs). Frames are distinguished by the
set of roles involved, known as frame elements (FEs).
Much of the information in the FrameNet lexicon is
derived by annotating corpus sentences; for each LU,
groups of sentences are extracted from a corpus, sen-
tences which collectively exemplify all of the lexico-
graphically relevant syntactic patterns in which the LU
occurs. A few examples of each pattern are annotated;
annotators not only mark the target word which evokes
the frame in the mind of the hearer, but also mark
those phrases which are syntactically related to the tar-
get word and express its frame elements. FrameNet is
much smaller than WordNet, covering roughly 11,000
LUs, but contains very rich syntagmatic information
about the combinatorial possibilities of each LU.
Given these two lexical resources with different
strengths, it seems clear that combining WN and FN
annotation will produce a more complete semantic rep-
resentation of the meaning of a text than either could
alone. What follows is intended as an example of how
they can usefully be combined.
2 Case Study: Aegean History
The text chosen for this study is a paragraph from the
American National Corpus3 (Ide et al, 2002), from the
Berlitz travel guide to Greece, discussing the history of
2http://framenet.icsi.berkeley.edu
3http://www.americannationalcorpus.org
125
Greece, specifically the Aegean islands after the fall of
Byzantium to the Crusaders. Although brief, its three
sentences provide ample material to demonstrate some
of the subtlety of both WN and FN annotation:
(1) While Byzantine land was being divided, there
was no one in control of the seas, so pirates raided
towns on many of the islands. (2) To counter this, the
populations moved from their homes on the coast and
built settlements inland, out of sight of the raiding par-
ties. (3) This created a pattern seen today throughout
the Aegean of a small port (skala) which serves an in-
land settlement or chora, making it easier to protect the
island from attack.
Below, we present three tables containing the anno-
tation of both the WordNet synsets for each open class
(content) word in the text4 and the FrameNet frames
and the fillers of the frame elements in each sentence.
We also provide brief notes on some interesting fea-
tures of the semantics of each sentence.
2.1 Discussion of Sentence 1, shown in Table 1 on
page 4 :
(2) Information about what the land was separated into
is not given in the sentence nor clear from the context,
so the PARTS FE has been annotated as ?indefinite null
instantiated? (INI). Clearly this is an intentional action,
but because the verb is passive, the agent can be (and
is) omitted, so the AGENT FE is marked as ?construc-
tionally null instantiated? (CNI).5
(4) In addition to FEs and their phrase types and
grammatical functions, FrameNet annotates a limited
set of syntactic facts: here, in is annotated as at ?sup-
port preposition?, allowing control to function as an ad-
jectival, and was as a copula, allowing no one to fill the
External syntactic position of in control.
(5) Since FN is based on semantic frames, annota-
tion of nouns is largely limited to those which express
events (e.g. destruction), relations (brother), or states
(height). For the most part, nouns denoting artifacts
and natural kinds evoke relatively uninteresting frames,
and hence relatively few of them have been included
in FN. However, there are three such instances in this
sentence, seas, islands (9), and towns (12); In all three
cases, the frame-evoking noun also denotes the filler of
the FE LOCALE.
(6) At the top level of organization, so evokes
the Causation frame. Actually, it is misleading to
simply annotate control of the seas in the frames
Be in control and Natural features; here, we regard
seas as metonymic for ?ship traffic on the seas?, but
neither the FN annotation nor the WN definition indi-
cates this.
(7) The noun pirates evokes the very rich frame of
4Note that for reasons of space, many WN examples have
been omitted.
5In fact, the previous sentence describes the sack of Con-
stantinople by the Crusaders, so they can be inferred to be the
dividers of the lands, as well.
Piracy, and also denotes the filler of the FE PERPE-
TRATOR, but that is the only FE filled in in that frame.
Instead, pirates actually fills the ASSAILANT FE of the
Attack frame, (8); the main idea is about the raids, not
the piratical acts on the seas that the same people have a
habit of committing. Note that the WN definition takes
the view that raiding coastal towns is a typical part of
piracy.
(10) Political locales roughly corresponds to
?Geopolitical entity? in named entity recognition.
Despite the relatively fine level of detail of the anno-
tations, there are still many important semantic features
of the sentence not represented in FrameNet or Word-
Net. For example, there is no treatment of negation cum
quantification, no representation of the fact that there
was no one in control should mean that Be in control
is not happening.
2.2 Discussion of Sentence 2, shown in Table 2 on
page 5:
The two highest level predicates in this sentence are
moved (2) and built (6), in the frames Motion and
Building respectively; since they are conjoined, the
phrase to counter this fills the FE PURPOSE in both
frames. 6 In (2) the GOAL FE of the Motion is marked
as definite null instantiation (DNI), because, although
it is not expressed in the VP headed by moved, it is
recoverable from context (i.e. the second VP).
(4) Note that FN puts this sense of home in the Build-
ings frame7, but WN has a less specific definition. (6)
Coast is a Relational natural feature because it is de-
fined in relation to another natural feature; a coast
has to be the coast of some land mass, although here
the land mass is DNI. (9) Inland both evokes a Loca-
tive relation and denotes the GROUND FE. (10) FN and
WN agree on a sense of sight denoting the range of vi-
sion. (11) WN?s example sentence for raid is precisely
about pirates.
2.3 Discussion of Sentence 3 shown in Table 3 on
page 5:
(2) The concept of ?pattern? is very slippery?the ar-
rangement of port and inland settlement is both spa-
tial and temporal in terms of building practices over
centuries. (3) This sense of see can refer to the area
in which something is seen, the time, or the condi-
tions under which it can be seen; these are subsumed
by the FE STATE. (4) Today expresses a Tempo-
ral collocation and denotes the LANDMARK. (Repe-
titions of the words settlement and island have been
omitted.) The interrelation among (7), (10), (11) and
(12) is rather complex: the arrangement in which the
port serves the settlement has the making easier as a
result. The arrangement is also the CAUSE FE of mak-
ing. Easier in the Difficulty frame requires an EX-
6This is a peripheral FE, common to all frames which
inherit from the Intentionally act frame.
7Not to be confused with the Building frame, in (7).
126
PERIENCER FE which is not specified here (thus INI)
and an ACTIVITY FE, to protect. The FE PROTEC-
TION (which can be a person, a thing, or an activity) is
marked CNI, because it is the external argument of the
infinitive.
3 Towards an alignment of WordNet and
FrameNet
We hope these examples have shown that finding re-
lated WN and FN senses can contribute to text under-
standing. Fellbaum and Baker (2008) discuss the re-
spective strengths and weaknesses of WN and FN as
well as their complementary advantages that could be
fruitfully exploited aligning the two resources. Work
of this type is actually underway; researchers are semi-
automatically annotating selected lemmas in the Amer-
ican National Corpus with both FN frames and WN
senses. The lemmas are chosen so as to reflect the part
of speech distribution in text and to represent a spec-
trum of frequency and polysemy. A preliminary group
of instances are manually tagged by trained annotators,
and then the teams working on WN and FN annota-
tion discuss and resolve discrepancies among the tag-
gers before the remaining tokens are annotated.
Three cases sum up the annotation and alignment
process:
(1) In the very unlikely case that a synset and a frame
contain exactly the same set of lexemes, their corre-
spondence is simply recorded.
(2) In the more common case in which all the words
in a synset are a subset of those in the frame, or all the
words in a frame are a subset of those in the synset, this
fact is also recorded.
(3) In case two synsets are subsets of the LUs of one
frame, we will record this and note that it as a possible
candidate for collapsing the synsets, respectively.
FN and WN are two comprehensive but comple-
mentary lexical resources. Both WN?s paradigmatic
and FN?s syntagmatic approach to lexical semantics are
needed for a rich representation of word meaning in
context. We have demonstrated how text can be an-
notated against both resources to provide the founda-
tion for deep language understanding and, as an im-
portant by-product, help to align the word senses of
these widely-used resources. Of course, these ex-
amples were manually annotated, but automatic sys-
tems for word-sense disambiguation (largely based on
WordNet) and FrameNet role labeling (Johansson and
Nugues, 2007; Coppola et al, 2008) are improving
rapidly. The project just described is intended to pro-
vide more gold-standard annotation (both WN and FN)
to help train automatic systems for both WN and FN
annotation, which are clearly related tasks e.g. (Prad-
han et al, 2007; Erk, 2005).
Acknowledgment
We gratefully acknowledge support from the National
Science Foundation (#IIS-0705199) for the work re-
ported here.
References
Bonaventura Coppola, Alessandro Moschitti, Sara
Tonelli, and Giuseppe Riccardi. 2008. Automatic
framenet-based annotation of conversational speech.
In Proceedings of IEEE-SLT 2008, pages 73?76,
Goa, India, December.
Katrin Erk. 2005. Frame assignment as word sense
disambiguation. In Proceedings of IWCS 6, Tilburg.
Christiane Fellbaum and Collin F. Baker. 2008. Can
WordNet and FrameNet be made ?interoperable??
In Jonathan Webster, Nancy Ide, and Alex Chengyu
Fang, editors, Proceedings of The First International
Conference on Global Interoperability for Language
Resources, pages 67?74, Hong Kong. City Univer-
sity.
Christiane Fellbaum and J. Grabowski. 1997. Anal-
ysis of a hand-tagging task. In Proceedings of
the ACL/Siglex workshop. Association for Compu-
tational Linguistics.
Christane Fellbaum, editor. 1998. WordNet. An
electronic lexical database. MIT Press, Cam-
bridge/Mass.
Charles J. Fillmore. 1976. Frame semantics and the
nature of language. Annals of the New York Academy
of Sciences, 280:20?32.
Thierry Fontenelle, editor. 2003. International Jour-
nal of Lexicography?Special Issue on FrameNet,
volume 16. Oxford University Press.
Eduard H. Hovy, Mitch Marcus, Martha Palmer,
Sameer Pradhan, Lance Ramshaw, and Ralph-
Weischedel. 2006. OntoNotes: The 90% solution.
In Proceedings of HLT-NAACL 2006, New York.
Nancy Ide, Randi Reppen, and Keith Suderman. 2002.
The American National Corpus: More than the
web can provide. In Proceedings of the Third
Language Resources and Evaluation Conference
(LREC), pages 839?44, Las Palmas, Canary Islands,
Spain.
Richard Johansson and Pierre Nugues. 2007. LTH:
Semantic structure extraction using nonprojective
dependency trees. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 227?230, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
Seventeenth National Conference on Artificial Intel-
ligence, Austin, TX. AAAI-2000.
127
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87?92,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
1. Frame: Political locales: [CONTAINER POSSESSOR
Byzantine] [LOCALE LAND]
WN: (adj) Byzantine (of or relating to or characteristic
of the Byzantine Empire or the ancient city of Byzan-
tium) (n) domain, demesne, land (territory over which
rule or control is exercised) ?his domain extended into
Europe?; ?he made it the law of the land?
2. Frame: Separating: [WHOLE Byzantine land] was
being DIVIDED [AGENT CNI] [PARTS INI]
WN: (v) divide, split, split up, separate, dissever, carve
up (separate into parts or portions) ?divide the cake into
three equal parts?; ?The British carved up the Ottoman
Empire after World War I?)
3. Frame: Existence: [TIME While Byzantine land was
being divided], THERE WAS [ENTITY no one in con-
trol of the seas]
4. Frame: Be in control: there [was COPULA]
[CONTROLLING ENTITY no one] [in SUPPORT] CON-
TROL [DEPENDENT ENTITY of the seas]
WN: (n) control (power to direct or determine) ?under
control?)
5. Frame: Natural features: [LOCALE SEAS]
WN: (n) sea (a division of an ocean or a large body of
salt water partially enclosed by land)
6. Frame: Causation:
[CAUSE While Byzantine land was being divided, there
was no one in control of the seas], SO [EFFECT pirates
raided towns on many of the islands]
7. Frame: Piracy: [PERPETRATOR PIRATES]
WN: (n) pirate, buccaneer, sea robber, sea rover (some-
one who robs at sea or plunders the land from the sea
without having a commission from any sovereign na-
tion)
8. Frame: Attack: [ASSAILANT pirates] RAIDED
[VICTIM towns on many of the islands]
WN: (v) foray into, raid (enter someone else?s territory
and take spoils) ?The pirates raided the coastal villages
regularly?)
9. Frame: Political locales: [LOCALE TOWNS]
[RELATIVE LOCATION on many of the islands].
WN: (n) town (an urban area with a fixed boundary that
is smaller than a city)
10. Frame: Locative relation: [FIGURE towns] ON
[GROUND many of the islands]
11. Frame: Quantity: [QUANTITY MANY]
[INDIVIDUALS of the islands]
12. Frame: Natural features: [LOCALE ISLANDS]
WN: (n) island (a land mass (smaller than a continent)
that is surrounded by water)
Table 1: FN/WN Annotation of sentence 1
128
1. Frame: Thwarting: To COUNTER [ACTION this],
[PREVENTING CAUSE the populations moved . . . raiding
parties]
WN:(v) anticipate, foresee, forestall, counter (act in
advance of; deal with ahead of time)
2. Frame: Aggregate: [AGGREGATE POPULATIONS]
WN: (n) population (the people who inhabit a terri-
tory or state) ?the population seemed to be well fed and
clothed?
3. Frame: Motion:
[PURPOSE To counter this], [THEME the populations]
MOVED [SOURCE from their homes on the coast]
[GOAL DNI]
WN: (v) move (change residence, affiliation, or place
of employment)
4. Frame: Buildings: [BUILDING HOMES] [PLACE on
the coast]
WN: (n) home, place (where you live at a particular
time) ?deliver the package to my home?
5. Frame: Locative relation: [FIGURE their homes] ON
[GROUND the coast]
6. Frame: Relational natural features:
[FOCAL FEATURE COAST] [RELATIVE LOCATION
DNI]
WN: (n) seashore, coast, seacoast, sea-coast (the shore
of a sea or ocean)
7. Frame: Building:
[PURPOSE To counter this], [AGENT the populations]
. . . BUILT [CREATED ENTITY settlements] [PLACE in-
land], [PLACE out of sight of the raiding parties].
WN: (v) construct, build, make (make by combining
materials and parts)
8. Frame: Locale by use: [LOCALE SETTLE-
MENTS]
WN: (n) village, small town, settlement (a community
of people smaller than a town)
9. Frame: Locative relation: built [FIGURE settle-
ments] [GROUND INLAND]
WN: (adv) inland (towards or into the interior of a re-
gion) ?the town is five miles inland?
10. Frame: Range: . . . out of [DISTANCE SIGHT]
[PARTICIPANT of the raiding parties]
WN: (n) sight, ken (the range of vision) ?out of sight of
land?
11. Frame: Attack: RAIDING [ASSAILANT parties]
WN: (v) foray into, raid (enter someone else?s territory
and take spoils) ?The pirates raided the coastal villages
regularly?
12. Frame: Aggregate: [AGGREGATEPROPERTY raid-
ing] [AGGREGATE PARTIES]
WN: (n) party, company (a band of people associated
temporarily in some activity) ?they organized a party to
search for food?
Table 2: FN/WN Annotation of sentence 2
1. Frame: Creating:
[CAUSE This] CREATED [CREATED ENTITY a pattern
seen today . . . from attack].
WN: (v) create (bring into existence) ?He created a new
movement in painting?
2. Frame: Pattern: PATTERN [DESCRIPTOR seen to-
day throughout the Aegean] [ENTITIES of a small port
(skala) which serves an inland settlement or chora]
WN: (n) practice, pattern (a customary way of opera-
tion or behavior) ?they changed their dietary pattern?
3. Frame: Perception experience: [PHENOMENON a
pattern] SEEN [TIME today] [STATE throughout the
Aegean] [PHENOMENON of a small port . . . from attack].
[PERCEIVER PASSIVE CNI]
WN: (v) witness, find, see (perceive or be contempora-
neous with) ?You?ll see a lot of cheating in this school?
4. Frame: Temporal collocation: [TRAJECTOR EVENT
a pattern seen] [LANDMARK EVENT TODAY]
[TRAJECTOR EVENT throughout the Aegean. . . attack]
WN: (n) today (the present time or age) ?the world
of today? (n) Aegean, Aegean Sea (an arm of the
Mediterranean between Greece and Turkey. . . )
5. Frame: Dimension: [DIMENSION SMALL] [OBJECT
port]
WN: (adj) small, little (limited or below average in
number or quantity or magnitude or extent)
6. Frame: Locale by use: [DESCRIPTOR small]
[LOCALE PORT]
WN: (n) port (a place (seaport or airport) where people
and merchandise can enter or leave a country)
7. Frame: Assistance: [HELPER a small port (skala)]
[HELPER which] SERVES [BENEFITED PARTY an in-
land settlement or chora], [RESULT making it easier to
protect the island from attack]
WN: (v) service, serve (be used by; as of a utility) ?The
sewage plant served the neighboring communities?
8. Frame: Locative relation: [GROUND INLAND]
[FIGURE settlement]
10. Frame: causation: [CAUSE a small port (skala)
which serves an inland settlement or chora], MAK-
ING it [EFFECT easier to protect the island from attack.]
[AFFECTED DNI]
WN: chora: not in WordNet (v) make, get (give certain
properties to something) ?This invention will make you
a millionaire?
11. Frame: Difficulty: EASIER [ACTIVITY to protect
the island from attack]. [EXPERIENCER INI]
WN: (adj) easy (posing no difficulty; requiring little ef-
fort) ?an easy job?; ?an easy victory?
12. Frame: Protecting: [PROTECTION CNI] PROTECT
[ASSET the island] [DANGER from attack]
WN: (v) protect (shield from danger, injury, destruc-
tion, or damage) ?Weatherbeater protects your roof
from the rain?
14. Frame: Attack: from ATTACK. [ASSAILANT DNI]
WN: (n) attack, onslaught, onset, onrush ((military) an
offensive against an enemy (using weapons)) ?the at-
tack began at dawn?
Table 3: FN/WN Annotation of sentence 3
129
Project Note 
A Corpus-based Lexical resource of German Idioms 
G. Neumann, C. Fellbaum, A. Geyken, A. Herold, C. 
H?mmer, F. K?rner, U. Kramer, K. Krell, A. Sokirko, D. 
Stantcheva, E. Stathi 
 
Project: ?Collocations in the German Language?, 
Berlin-Brandenburg Academy of Sciences and Humanities 
J?gerstra?e 22/23 
Germany, 10117 Berlin 
gneumann@bbaw.de 
 
 
 
Abstract 
In this paper, we present the design of a 
lexical resource focusing on German verb 
phrase idioms. The entry for a given idiom 
combines appropriate corpus examples with 
rich linguistic and lexicographic annotations, 
giving the user access to linguistic information 
coupled with the appropriate attested data and 
laying the groundwork for empirical, 
reproducible research. 
1 Introduction 
The project "Collocations in the German 
Language", located at the Berlin-Brandenburg 
Academy of Sciences, studies the properties of 
verb phrase idioms such as jmdm. einen B?ren 
aufbinden (lit. ?tie a bear onto sb.'s back,? i.e. ?tell 
a big lie?) and einen Bock schiessen (lit. ?shoot a 
buck,? i.e. ?commit a grave error?). 
Idioms are sometimes referred to informally as 
"long words" and are treated as fixed strings with 
no internal structure. While some idioms (like the 
much-discussed English example kick the bucket) 
have semantically opaque components and do not 
undergo syntactic or lexical alternations, others 
(like bury the hatchet) behave more like freely 
composed VP. 
Nunberg, Sag, and Wasow (1994) and 
Dobrovolskij (1999) are among the linguists who 
have observed a correlation between an idiom's 
fixedness and its semantic opacity: The more fixed 
an idiom, the more semantically opaque it can be. 
But this correlation is not always straightforward, 
and to understand the way speakers represent 
idioms in their internal grammars we must 
investigate the full range of behavior. For example, 
the German equivalent of kick the bucket is lit. 
?bite into the grass? (ins Gras beissen). While Gras 
(just like ?bucket?) does not seem to be mappable 
onto the concept, it can be modified with an 
adjective, as our corpus example ?bit the Texan 
grass? shows. In fact, the adjective must be 
interpreted as having scope over the entire VP, not 
just the NP, as the phrase refers to someone dying 
in Texas. 
Lexical substitution and variability of the 
idiom?s components in general can show that an 
element is assigned a particular interpretation, as in 
the (diachronically differentiated) variants sich auf 
die Str?mpfe machen and sich auf die Socken 
machen (lit. ?make oneself on the stockings/socks?, 
i.e. ?to get going or get moving?). Here, the 
substitution of a near-synonym indicates that 
speakers assign some meaning to the noun, most 
likely the footwear that is associated with travel on 
foot. But speakers make substitutions without 
necessarily assigning a meaning to the constituents 
of the idiom. 
Current lexicographic and linguistic treatment of 
VP idioms does not attempt to reflect the full range 
of the idioms' properties or classify them 
accordingly. Our goal is to give a data-oriented, 
comprehensive linguistic account of a broad 
spectrum of German idioms. 
The empirical basis for our work is the corpus of 
the Digitales W?rterbuch der deutschen Sprache 
(DWDS), a corpus of almost 1 billion words from 
texts drawn from a variety of genres and covering 
the entire 20th century. 
We are creating a resource that combines 
features of a dictionary, a grammar of idioms, and 
a corpus. Central to our methodology are two main 
components: an electronic knowledge base created 
via structured annotations, and a corresponding 
sub-corpus of examples (example corpora) for each 
entry. 
 2 The Example Corpora 
Our lexicon combines subtle linguistic 
annotations with specific corpus data. 
For each target idiom, a sub-corpus, containing 
appropriate examples drawn from the 1-billion-
word-corpus, is created. Following the 
identification of a target idiom, corpus queries are 
written to generate the candidate set of relevant 
corpus data, taking advantage of a search engine 
developed in-house. The queries include specific 
lexical and morphological elements as well as 
Boolean operators. The resultant candidate 
example corpus, in XML/TEI format, is manually 
inspected for false positives, i.e. strings which are 
not instances of the idiom but rather accidental 
products of the query. 
The number of true positives gives an idea of the 
frequency of idioms in the language ? information 
that was not easy to gather until now. For example, 
the idiom jmdn. zur Minna machen (lit. ?make sb. a 
Minna,?, i.e. ?to publicly reprimand sb.?) is attested 
only 41 times, while ein Auge auf etw. haben (lit. 
?to keep an eye on sth.?) yields 560 examples. 
To ensure reproducibility of result sets, 
examples are marked with tags identifying their 
corpus query and the particular corpus version. 
Thus we have the possibility of comparison 
between the manually inspected example corpora 
and the dynamically increasing DWDS-Corpus. 
3 The Annotation Template 
We created a template for linguistic and 
lexicographic annotation, a kind of digital 
questionnaire. It serves as both input and output 
interface and links the annotated example corpora 
with the idiom knowledge base. The data entry 
interface supports a structured entry created by the 
linguists/lexicographers, who record various 
properties of the idioms as evidenced by the 
examples. 
The template design reflects the information 
which is considered relevant in the discussion of 
idioms from a lexicographic and linguistic point of 
view, focusing on the interplay between normal 
usage and flexibility. The template consists of the 
following major parts: 
The first part contains information which is 
typically found in dictionaries. This includes the 
citation form of the idiom, together with a 
definition. A difference from standard dictionaries 
is the link to the data. We record the first and last 
occurrence in the corpus and in this way we 
indicate the time span of corpus examples of the 
idiom during the 20th century. The citation form is 
derived on the basis of the corpus evidence 
according to statistical significance (cf. example 
below). The typical usages lead to the formulation 
of the citation form, which is linked to some of 
them. In addition, there is information on alternate 
forms (e.g. different aspectual varieties of an 
idiom, transitive vs. intransitive uses, etc.) with a 
reference to the corresponding entry. 
The second part is the syntactic structure of the 
idiom, in particular the structure of the VP 
including the subject. We show dependencies 
between constituents on three levels (two phrase 
levels and a terminal level), using the category 
variables of a German tag set1. At the terminal 
level we fill in the lexical material which 
corresponds to the citation form. Finally, we note 
the status of each component in the idiom 
according to degree of fixedness (external 
argument, core component, obligatory, optional, 
etc.). 
This ?tree? structure automatically creates a table 
which records the morphosyntactic properties of 
idiom components. Since idioms are considered as 
typically restricted according to morphosyntactic 
properties (e.g., noun complements may occur only 
in the singular or only in the plural), we explore to 
which extent this is supported by the data. 
On the basis of these morphosyntactic 
regularities, we derive the citation form. This is 
considered to be the normal (or typical) form, 
according to frequency of usage. We are also 
interested in non-typical or ?deviant? usages. The 
term should be understood not in the sense of 
?abnormal? or ?ungrammatical?, but as infrequent 
or not statistically significant (sometimes 
idiosyncratic, sometimes more widespread), yet 
important for insight into the linguistic properties 
of an idiom. 
Idioms are also subject to restrictions concerning 
the passivization of the nominal components, their 
pronominalization, relativization, and other 
syntactic processes sometimes called 
?transformations?. If we take the example 
mentioned earlier, one would not expect to 
encounter *das Gras, in das er gebissen hat 
(relativization), *das Gras wurde gebissen 
(passivization), etc. In reality, these 
transformations can be observed in most cases. We 
record them in a separate table and link every 
transformation type to the corresponding 
occurrences in the example corpora. 
The template also offers the possibility to 
include semantic as well as historical information, 
if diachronic changes or noteworthy developments 
can be observed in the corpus data. 
By way of illustration, let us consider an 
example, focusing attention on variability. In 
                                                     
1 STTS (Stuttgart-T?bingen Tagset) 
 Modern German the noun Bockshorn2 is 
considered to be a phraseologically bound word, 
i.e. it does not occur in isolation, but only in a 
fixed expression3. Idioms with phraseologically 
bound words are said to display the highest degree 
of fixedness and absence of flexibility. We want to 
test this hypothesis against corpus data. 
The phraseological dictionary ?Duden (Vol. 11)? 
gives s.v. Bockshorn the citation form jmd. l?sst 
sich nicht ins Bockshorn jagen, which is roughly 
equivalent to English ?refuse to allow oneself to be 
intimidated?. The ?W?rterbuch der deutschen 
Gegenwartssprache? (WDG) records also the 
transitive alternation jmdn. ins Bockshorn jagen, 
which might be translated as ?to put the wind up 
sb.? 
The following table shows the results which 
were given by three different queries to the corpus 
for this idiom: 
 
Query Number of hits 
in the corpus 
Bockshorn 285 
*horn && jagen && 
!@Bockshorn 
27 
((Bockshorn* && !@Bockshorn) 
|| *bockshorn) 
35 
Total 347 
Table 1: Queries and hits for Bockshorn in the 
DWDS-Corpus 
 
As the table shows, the queries leave open the 
possibility of finding variation at every slot: The 
position of the verb is left unspecified in the first 
query. In the second query we try to find instances 
where the first component of the noun (Bock-) 
might be substituted by another noun4, while the 
third query should deliver instances of compounds 
with Bockshorn (cf. the form Bockshornklee in 
Table 2 below). 
                                                     
2 The etymology of the word is obscure. R?hrich 
(2001) offers nine etymologies for the idiom according 
to different possible sources of the noun. He reaches no 
conclusion. Synchronically, the word is identical to the 
name of a low-growing plant (fenugreek) with tough 
wiry stems, also called Bockshornklee. But it can also 
literally denote the horn of a goat (Bock ?male goat? and 
Horn ?horn?). 
3 Moon (1998) terms these ?cranberry? collocations. 
Actually, this particular word (Bockshorn) occurs 
outside a fixed expression, but it occurs only in highly 
technical contexts (cf. footnote 5 below). 
4 The opposite case, i.e. looking for cases with the 
component stable (-horn), gave no results. 
Linguistic analysis of the data led to the 
formulation of two alternate citation forms and 
consequently two entries for the idiom: 
1) jmd. l?sst sich nicht ins Bockshorn jagen 
(192 hits) 
2) jmd. jagt jmdn. ins Bockshorn (44 hits).5 
The first is a form with the so-called ?lassen-
passive?. It is much more frequent than the 
transitive form; actually it is the typical form 
associated with Bockshorn. The results show a 
remarkable uniformity of the idiom components, as 
the following table shows: 
 
Total  192
Bockshorn 182
Boxhorn 8 
Bockshornklee 1 
Noun 
Hasenhorn 1 
ins 191Preposition
in6 1 
jagen 191Infinitive 
jache7 1 
Verb lassen 192
Overt negation (different 
possibilities) 
141
Irrealis 15 
Negation 
Affirmation 36 
Table 2: Distribution of components for the ?lassen-
passive? alternant 
 
The forms which are mentioned first in Table 2 
(bold face) are seen as the ?normal? components in 
the sense of statistical significance, the other forms 
are considered as ?deviations? from the norm. 
These include the substitution of components. Cf. 
the noun: The form Boxhorn is phonologically 
identically to Bockshorn; perhaps its usage 
represents the need to motivate the original noun.8 
The substitution of Bockshornklee can be 
explained with reference to the context: This noun 
appears earlier in the text and triggers the use of 
the idiom, a not unusual phenomenon in journalese 
texts. Finally, Hasenhorn shows the typical pattern 
of noun substitution in idioms in German, i.e. the 
substitution of one part of a compound noun 
typically by a synonym or a word belonging to the 
same lexical field (Hase- ?rabbit? for Bock- 
?buck?). Again, this substitution is triggered by the 
topic of the text. Note that this substitution occurs 
                                                     
5 111 hits were non-idiomatic uses of the noun, 
especially in popular medicine and compounds. 
6 Typing error 
7 Dialectal form for jagen. 
8 An alternative interpretation relates the form 
Boxhorn to 15th century words relating to God (cf. 
R?hrich 2001). 
 even though the speaker cannot assign a meaning 
to the lexeme Bockshorn (cf. Introduction). 
Now consider the figures for the transitive 
alternant jmd. jagt jmdn. ins Bockshorn: 
 
Total  44
Bockshorn 41
Boxhorn 1 
Bockhorn 1 
Noun 
Gruselhorn 1 
ins 43Preposition 
in?s 1 
jagen 42
einjagen 1 
Verb 
f?hren 1 
Table 3: Distribution of components for the transitive 
alternant 
 
The citation form given is derived on the basis of 
these quantities, which distinguish normal usage 
from ?deviant? usages. Beside the form Boxhorn 
(cf. above) Bockhorn occurs once.9 Gruselhorn 
(Grusel- ?scary?) is a nonce creation for the 
expression of emphasis since it combines the 
meaning of the idiom and the meaning of ?to scare 
sb.? 
As for the verb, it is substituted twice. The verb 
f?hren ?to lead? fulfils the same semantic function 
as jagen in this idiom, i.e. the expression of 
causativity. The difference is that f?hren is 
unmarked: In German it is often used as a function 
verb to express causativity (to cause a change of 
state for sb.). The substitution of einjagen on the 
other hand may be due to its occurrence in the 
near-synonymous expression (jmdm.) 
Angst/Schrecken einjagen ?to scare sb.? 
To sum up, if variability correlates with 
flexibility (or fixedness) of an idiom, then these 
figures lead to the conclusion that idioms with 
phraseologically bound forms are typically fixed. 
But contrary to claims in the literature that there is 
no variability at all, corpus evidence shows that 
there is some patterned variability. The interesting 
question is whether it is also predictable. 
Finally, it should be noted that the postulation of 
two entries (the lassen-Passive and the transitive/ 
causative) are necessary for two reasons. Firstly, 
the two entries differ in syntactic structure 
(argument structure, negation, etc.). This 
difference automatically forces us to establish 
separate entries, because the whole design of the 
template is based on syntactic structure. This 
means that the subsequent sections (morpho-
                                                     
9 It could be an error or just omission of the 
connecting -s-. 
syntactic properties and variations or deviations 
from the citation form) are created automatically 
on the basis of the syntactic structure of the idiom. 
Of course, they have to be filled in manually, but 
their design is uniform for all idioms which 
ensures comparability. 
Secondly, the lassen-Passive (plus negation and 
reflexivity) as a typical instance of this idiom is not 
a regular (or necessary) correlate of the transitive 
form, as a pure passive form would be. Moreover, 
the two entries differ pragmatically. 
4  The Knowledge Base 
The information recorded in the template is 
stored in a MySQL database, which constitutes the 
actual knowledge base for German idioms. This 
knowledge base can be queried in various different 
ways. In particular, it is possible to query for 
syntactical structures of idioms and for all kinds of 
variation from the citation form found in the 
example corpus. 
We developed a label language that permits a 
precise, automatic assignment of examples to most 
of the properties recorded in the template. Thus the 
lexicographers? decisions preserve a high degree of 
transparency. 
5 Use of Resource 
The contents of the knowledge base will be 
made available via the Internet. The template, in its 
function as a user interface, is browser-based and 
directly accesses the database. 
Users can search for specific idioms (or 
substrings thereof), examine their linguistic 
properties, and find the appropriate corpus 
examples. 
This automatic linking of examples to the 
corpora makes the analysis transparent in a way 
that is not possible in conventional dictionaries. 
6  Conclusion 
In sum, our resource combines properties of a 
dictionary and a grammar with a corpus. A learner 
or general user can look up a specific idiom and 
study its properties. A linguist can search for 
specific linguistic structures (passives, clefts, etc.) 
and find those idioms that show the features in 
question. Other users will no doubt find other 
modes of application. 
7 Acknowledgements 
This work is supported by the Alexander von 
Humboldt Foundation?s Zukunftsinvestitions-
programm through the Wolfgang Paul-Preis. 
 
 5
References  
Dobrovol'skij, Dmitrij. 1999. ?Haben 
transformationelle Defekte der Idiomstruktur 
semantische Ursachen?? In: Fernandez-Bravo, 
Nicole, Irmtraud Behr und Claire Rozier (eds.) 
1999: Phraseme und typisierte Rede 
(=Eurogermanistik 14). T?bingen: Stauffenburg, 
25-37. 
Dobrovol'skij, Dmitrij. 2002. ?Zum syntaktischen 
Verhalten deutscher Idiome (am Beispiel der 
Passivtransformation)? In: Wiesinger, Peter (ed.) 
2002: Akten des X. Internationalen 
Germanistenkongresses Wien 2000. "Zeitenwende 
- Die Germanistik auf dem Weg vom 20. ins 21. 
Jahrhundert". Jahrbuch f?r Internationale 
Germanistik. Reihe A, Kongressberichte Bd. 54, 
Band 2, Entwicklungstendenzen der deutschen 
Gegenwartssprache; Lexikologie und 
Lexikographie. Bern et al: Peter Lang, 379-384. 
Duden, Redewendungen: W?rterbuch der 
deutschen Idiomatik. 2002. Hrsg. von der 
Dudenredaktion. [Red. Bearb.: Brigitte Alsleben; 
Werner Scholze-Stubenrecht]. - 2., neu bearb. und 
aktualisierte Aufl.. Mannheim et al: Dudenverlag. 
Fellbaum, Christiane, Undine Kramer und Diana 
Stantcheva. 2004. ?Eins, einen, eine und etwas in 
deutschen VP-Idiomen? In: Steyer, Kathrin (ed.) 
2004: Wortverbindungen - mehr oder weniger fest. 
Jahrbuch des Instituts f?r Deutsche Sprache 2003. 
Berlin, New York: Walter de Gruyter, 167-193. 
Fleischer, Wolfgang. 1997. Phraseologie der 
deutschen Gegenwartssprache. 2., durchgesehene 
und erg?nzte Auflage. T?bingen: Max Niemeyer. 
Moon, Rosamund. 1998. ?Frequencies and Forms 
of Phrasal Lexemes in English?. In: Cowie, A. P. 
(ed.) 1998. Phraseology: Theory, Analysis, 
Applications. Oxford: Oxford University Press, 79-
100. 
Nunberg, Geoffrey; Ivan A. Sag und Thomas 
Wasow. 1994. ?Idioms? Language 70/3, 491-538. 
R?hrich, Lutz. 2001. Das gro?e Lexikon der 
sprichw?rtlichen Redensarten [Elektronische 
Ressource]. Darmstadt: Wissenschaftliche 
Buchgesellschaft. 
W?rterbuch der deutschen Gegenwartssprache. 
1967-1977. Akademie der Wissenschaften der 
DDR, Zentralinstitut f?r Sprachwissenschaft. Hrsg. 
von Ruth Klappenbach und Wolfgang Steinitz. 
Berlin: Akademie-Verlag. 
Relevant web pages: 
http://www.dwds.de 
http://www.bbaw.de/forschung/kollokationen/ 
Obituary
George A. Miller
Christiane Fellbaum
Princeton University
George Armitage Miller died on July 22, 2012, at the age of ninety-two. He led a rich
life full of accomplishments in the three areas of activity that he had chosen as a young
man: psychology, writing, and golf.
Miller was not only a witness but a key player in the major paradigm shift of the
20th century that came to be known as the cognitive revolution. Incredible as it may
seem today, his teachers at Harvard followed the behaviorist dogma and recognized
neither the autonomy nor the significance of the human mind. It took two courageous
young scientists?George Miller and Jerome Bruner?to assert, each in their own do-
main of investigation, that the mind is a worthwhile subject of study. They started out
by teaching a course boldly entitled ?Cognition? and eventually established the Center
for Cognitive Studies, ultimately making behaviorism obsolete.
Miller drew an analogy between the human mind and a computer, noting that
both store and process huge amounts of information. At the same time, human short-
term memory is limited, as his most celebrated paper on the ?Magical Number Seven?
demonstrates (Miller 1956/1994). Miller showed that chunking information into mean-
ingful units helps recall, though the number of units that can be memorized seems to
hover around seven. For example, U.S. telephone numbers are broken down into three
groups of three, three, and four digits (area code, local exchange, and individual num-
ber). Chunk parsers (Abney 1991) build on the idea that sentence processing proceeds
in phrases, reflected in prosodic patterns.
Among our cognitive faculties, it was language in particular that fascinated George,
a gifted writer. One attraction was that linguistic behavior could be observed, tested,
and evaluated quantitatively with the experimental paradigms available to psycholin-
guists at a time when brain imaging techniques had not yet been developed. The rules
of language, with their recursive aspects, could be seen as a kind of program. Although
he collaborated with Noam Chomsky on the formal aspects of language, Miller in later
life harbored a suspicion of highly abstract theories of syntax. His interest lay primarily
in the lexicon, not only because of his authorial love of words, but also because of its
size, open-endedness, and dynamic aspects. Moreover, the growth of children?s lexicons
offered a window into their cognitive development.
Miller is probably best known to readers of Computational Linguistics for his creation
of the large lexical database WordNet (Miller 1995). WordNet?s use as a resource for
natural language processing was in fact unintended, and its rapid adoption by the NLP
community came as a surprise. George was interested in human semantic organization
and wanted to test the then-fashionable concept of semantic networks, which allowed
for plausible and elegant models of semantic representation and seemed supported
by experiments testing lexical access and retrieval (Collins and Quillian 1969). Miller
wondered whether a semantic network could in fact be built for the bulk of the English
lexicon. In the mid 1980s, he recruited a group of colleagues, students, and his wife
Kitty and, without much further instruction, asked them to cluster nouns, verbs, and
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
adjectives into ?synsets? that could be interrelated with a handful of semantic relations.
Relying on conventional lexical resources and intuition, the WordNet team created tens
of thousands of entries manually, a fact that provokes head-shaking among the new
generation of WordNet builders, who proceed fully or semi-automatically. Each senior
member was assigned a different part of speech; Kitty was the ?Adjective Lady,? and
George took charge of the noun lexicon. Both would come to the Cognitive Science
Laboratory every day and patiently perform their lexicographic labor. Once or twice a
week, George would leave in the afternoon for the golf course. Well into this eighties, he
participated in tournaments and, more often than not, his team was among the winners.
A government sponsor?s requirement that the database be publicly released was
duly followed, but the response from the budding NLP community was entirely
unanticipated by the WordNet team, which was unaware of the challenge of word
sense disambiguation. WordNet turned out to be a tool that promised help with the
vexing task of word sense discrimination, and its graph structure became the basis
for a number of algorithms that measured semantic similarity among words in terms
of their distance in the network. Being unique in coverage and design, WordNet not
only survived but continued to grow, though its claims to modeling human seman-
tic memory were largely abandoned. George was proud to say that WordNet de-
fined a new kind of electronic lexicography, and WordNet became a common noun.
WordNets have been built for dozens of genetically and typologically unrelated
languages (http://www.globalwordnet.org).
While working on WordNet, George became interested in children?s literacy. He
believed that children did not learn words and their meanings from dictionaries, as they
were instructed to do in school. To test this hypothesis, he asked children to look up
unfamiliar words in a dictionary and write sentences using the new words. As George
had guessed, the results were both appalling and amusing. For example, when asked
to write a sentence with the word meticulous, children would produce sentences like she
was meticulous about falling off the cliff after having seen a dictionary entry that defined
this adjective as careful, scrupulous, fastidious.
Moreover, George guessed that children enjoy reading but, when encountering
an unfamiliar word, are generally disinclined to put their books down and consult a
dictionary. His idea was to present new words in their contexts, based on evidence that
context-based learning was both natural and efficient (Miller and Gildea 1987). He and
his team began to manually annotate a digitized book with entries from WordNet; an
interface would allow the children to read the book on the screen, click on unfamiliar
words and be presented with the context-appropriate WordNet sense. It should be
remembered that George?s idea of reading books on a screen was long before the
invention of e-readers!
The text-to-WordNet link gave birth to the idea of the semantic concordance. A large
team of Princeton students manually annotated nouns, verbs, and adjectives from texts
in the Brown Corpus against the corresponding WordNet senses. George thought that
this would be a straightforward task: Just as lexicographers create dictionary entries
based on tokens in a text, their entries should be mappable back to words in texts in a
one-to-one fashion. We learned that dictionaries with enumerative, discrete word senses
are in fact not a particularly good way of modeling speakers? lexicons. Today, research
into semantic annotation and measurements of inter-annotator agreement is a lively
area of investigation.
Although he had to transfer to emeritus status at the then-mandatory retirement
age, George did not give up teaching. He organized an informal course on the lexicon
and for each of twelve weekly meetings prepared beautifully written lectures that
2
Fellbaum Obituary
the participants would discuss and critique. The lectures would become The Science
of Words, a prize-winning book on the lexicon that was translated into numerous lan-
guages. In this and several other books (including Language and Communication [1963]
and Spontaneous Apprentices [1977]), George?s lively, lucid prose made a scientific sub-
ject accessible to the general public and conveyed his fascination with language and
cognition. His landmark book Language and Perception (1976), co-authored with Philip
Johnson-Laird, remains a classic among psycholinguists to this day. Co-authoring a
paper with George invariably involved a final editing step on his part, which he referred
to as ?Millerizing.?
George collected many prizes, medals, and honorary doctorates. He was modest
about it and only very occasionally did a new frame appear on his office walls alongside
graduation and wedding pictures of former students and the fake Renoir that he had
kindly bought from a street vendor.
References
Abney, Steven. 1991. Parsing by chunks. In
Principle-Based Parsing. Kluwer Academic
Publishers, pages 257?278.
Collins, Allan M. and M. Ross Quillian. 1969.
Retrieval time from semantic memory.
Journal of Verbal Learning and Verbal
Behavior, 8(2):240?247.
Miller, George A. 1956/1994. The magical
number seven, plus or minus two: Some
limits on our capacity for processing
information. Psychological Review (special
centennial issue), 101:343?352.
Miller, George A. 1963. Language and
Communication. McGraw Hill, New York.
Miller, George A. 1977. Spontaneous
Apprentices. Seabury Press, New York.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the
ACM, 38(11):39?41.
Miller, George A. and Patricia M. Gildea.
1987. How children learn words. Scientific
American, 257(3):94?99.
Miller, George A. and Philip Johnson-Laird.
1976. Language and Perception. Belknap
Press, Cambridge, MA.
3

Proceedings of the ACL 2010 Conference Short Papers, pages 68?73,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Manually Annotated Sub-Corpus:
A Community Resource For and By the People
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Collin Baker
International Computer Science Institute
Berkeley, California USA
collinb@icsi.berkeley.edu
Christiane Fellbaum
Princeton University
Princeton, New Jersey USA
fellbaum@princeton.edu
Rebecca Passonneau
Columbia University
New York, New York USA
becky@cs.columbia.edu
Abstract
The Manually Annotated Sub-Corpus
(MASC) project provides data and annota-
tions to serve as the base for a community-
wide annotation effort of a subset of the
American National Corpus. The MASC
infrastructure enables the incorporation of
contributed annotations into a single, us-
able format that can then be analyzed as
it is or ported to any of a variety of other
formats. MASC includes data from a
much wider variety of genres than exist-
ing multiply-annotated corpora of English,
and the project is committed to a fully
open model of distribution, without re-
striction, for all data and annotations pro-
duced or contributed. As such, MASC
is the first large-scale, open, community-
based effort to create much needed lan-
guage resources for NLP. This paper de-
scribes the MASC project, its corpus and
annotations, and serves as a call for con-
tributions of data and annotations from the
language processing community.
1 Introduction
The need for corpora annotated for multiple phe-
nomena across a variety of linguistic layers is
keenly recognized in the computational linguistics
community. Several multiply-annotated corpora
exist, especially for Western European languages
and for spoken data, but, interestingly, broad-
based English language corpora with robust anno-
tation for diverse linguistic phenomena are rela-
tively rare. The most widely-used corpus of En-
glish, the British National Corpus, contains only
part-of-speech annotation; and although it con-
tains a wider range of annotation types, the fif-
teen million word Open American National Cor-
pus annotations are largely unvalidated. The most
well-known multiply-annotated and validated cor-
pus of English is the one million word Wall Street
Journal corpus known as the Penn Treebank (Mar-
cus et al, 1993), which over the years has been
fully or partially annotated for several phenomena
over and above the original part-of-speech tagging
and phrase structure annotation. The usability of
these annotations is limited, however, by the fact
that many of them were produced by independent
projects using their own tools and formats, mak-
ing it difficult to combine them in order to study
their inter-relations. More recently, the OntoNotes
project (Pradhan et al, 2007) released a one mil-
lion word English corpus of newswire, broadcast
news, and broadcast conversation that is annotated
for Penn Treebank syntax, PropBank predicate ar-
gument structures, coreference, and named enti-
ties. OntoNotes comes closest to providing a cor-
pus with multiple layers of annotation that can be
analyzed as a unit via its representation of the an-
notations in a ?normal form?. However, like the
Wall Street Journal corpus, OntoNotes is limited
in the range of genres it includes. It is also limited
to only those annotations that may be produced by
members of the OntoNotes project. In addition,
use of the data and annotations with software other
than the OntoNotes database API is not necessar-
ily straightforward.
The sparseness of reliable multiply-annotated
corpora can be attributed to several factors. The
greatest obstacle is the high cost of manual pro-
duction and validation of linguistic annotations.
Furthermore, the production and annotation of
corpora, even when they involve significant scien-
tific research, often do not, per se, lead to publish-
able research results. It is therefore understand-
68
able that many research groups are unwilling to
get involved in such a massive undertaking for rel-
atively little reward.
The Manually Annotated Sub-Corpus
(MASC) (Ide et al, 2008) project has been
established to address many of these obstacles
to the creation of large-scale, robust, multiply-
annotated corpora. The project is providing
appropriate data and annotations to serve as the
base for a community-wide annotation effort,
together with an infrastructure that enables the
representation of internally-produced and con-
tributed annotations in a single, usable format
that can then be analyzed as it is or ported to any
of a variety of other formats, thus enabling its
immediate use with many common annotation
platforms as well as off-the-shelf concordance
and analysis software. The MASC project?s aim is
to offset some of the high costs of producing high
quality linguistic annotations via a distribution of
effort, and to solve some of the usability problems
for annotations produced at different sites by
harmonizing their representation formats.
The MASC project provides a resource that is
significantly different from OntoNotes and simi-
lar corpora. It provides data from a much wider
variety of genres than existing multiply-annotated
corpora of English, and all of the data in the cor-
pus are drawn from current American English so
as to be most useful for NLP applications. Per-
haps most importantly, the MASC project is com-
mitted to a fully open model of distribution, with-
out restriction, for all data and annotations. It is
also committed to incorporating diverse annota-
tions contributed by the community, regardless of
format, into the corpus. As such, MASC is the
first large-scale, open, community-based effort to
create a much-needed language resource for NLP.
This paper describes the MASC project, its corpus
and annotations, and serves as a call for contribu-
tions of data and annotations from the language
processing community.
2 MASC: The Corpus
MASC is a balanced subset of 500K words of
written texts and transcribed speech drawn pri-
marily from the Open American National Corpus
(OANC)1. The OANC is a 15 million word (and
growing) corpus of American English produced
since 1990, all of which is in the public domain
1http://www.anc.org
Genre No. texts Total words
Email 2 468
Essay 4 17516
Fiction 4 20413
Gov?t documents 1 6064
Journal 10 25635
Letters 31 10518
Newspaper/newswire 41 17951
Non-fiction 4 17118
Spoken 11 25783
Debate transcript 2 32325
Court transcript 1 20817
Technical 3 15417
Travel guides 4 12463
Total 118 222488
Table 1: MASC Composition (first 220K)
or otherwise free of usage and redistribution re-
strictions.
Where licensing permits, data for inclusion in
MASC is drawn from sources that have already
been heavily annotated by others. So far, the
first 80K increment of MASC data includes a
40K subset consisting of OANC data that has
been previously annotated for PropBank predi-
cate argument structures, Pittsburgh Opinion an-
notation (opinions, evaluations, sentiments, etc.),
TimeML time and events2, and several other lin-
guistic phenomena. It also includes a handful of
small texts from the so-called Language Under-
standing (LU) Corpus3 that has been annotated by
multiple groups for a wide variety of phenomena,
including events and committed belief. All of the
first 80K increment is annotated for Penn Tree-
bank syntax. The second 120K increment includes
5.5K words of Wall Street Journal texts that have
been annotated by several projects, including Penn
Treebank, PropBank, Penn Discourse Treebank,
TimeML, and the Pittsburgh Opinion project. The
composition of the 220K portion of the corpus an-
notated so far is shown in Table 1. The remain-
ing 280K of the corpus fills out the genres that are
under-represented in the first portion and includes
a few additional genres such as blogs and tweets.
3 MASC Annotations
Annotations for a variety of linguistic phenomena,
either manually produced or corrected from output
of automatic annotation systems, are being added
2The TimeML annotations of the data are not yet com-
pleted.
3MASC contains about 2K words of the 10K LU corpus,
eliminating non-English and translated LU texts as well as
texts that are not free of usage and redistribution restrictions.
69
Annotation type Method No. texts No. words
Token Validated 118 222472
Sentence Validated 118 222472
POS/lemma Validated 118 222472
Noun chunks Validated 118 222472
Verb chunks Validated 118 222472
Named entities Validated 118 222472
FrameNet frames Manual 21 17829
HSPG Validated 40* 30106
Discourse Manual 40* 30106
Penn Treebank Validated 97 87383
PropBank Validated 92 50165
Opinion Manual 97 47583
TimeBank Validated 34 5434
Committed belief Manual 13 4614
Event Manual 13 4614
Coreference Manual 2 1877
Table 2: Current MASC Annotations (* projected)
to MASC data in increments of roughly 100K
words. To date, validated or manually produced
annotations for 222K words have been made avail-
able.
The MASC project is itself producing annota-
tions for portions of the corpus forWordNet senses
and FrameNet frames and frame elements. To de-
rive maximal benefit from the semantic informa-
tion provided by these resources, the entire cor-
pus is also annotated and manually validated for
shallow parses (noun and verb chunks) and named
entities (person, location, organization, date and
time). Several additional types of annotation have
either been contracted by the MASC project or
contributed from other sources. The 220K words
ofMASC I and II include seventeen different types
of linguistic annotation4, shown in Table 2.
All MASC annotations, whether contributed or
produced in-house, are transduced to the Graph
Annotation Framework (GrAF) (Ide and Suder-
man, 2007) defined by ISO TC37 SC4?s Linguistic
Annotation Framework (LAF) (Ide and Romary,
2004). GrAF is an XML serialization of the LAF
abstract model of annotations, which consists of
a directed graph decorated with feature structures
providing the annotation content. GrAF?s primary
role is to serve as a ?pivot? format for transducing
among annotations represented in different for-
mats. However, because the underlying data struc-
ture is a graph, the GrAF representation itself can
serve as the basis for analysis via application of
4This includes WordNet sense annotations, which are not
listed in Table 2 because they are not applied to full texts; see
Section 3.1 for a description of the WordNet sense annota-
tions in MASC.
graph-analytic algorithms such as common sub-
tree detection.
The layering of annotations over MASC texts
dictates the use of a stand-off annotation repre-
sentation format, in which each annotation is con-
tained in a separate document linked to the pri-
mary data. Each text in the corpus is provided in
UTF-8 character encoding in a separate file, which
includes no annotation or markup of any kind.
Each file is associated with a set of GrAF standoff
files, one for each annotation type, containing the
annotations for that text. In addition to the anno-
tation types listed in Table 2, a document contain-
ing annotation for logical structure (titles, head-
ings, sections, etc. down to the level of paragraph)
is included. Each text is also associated with
(1) a header document that provides appropriate
metadata together with machine-processable in-
formation about associated annotations and inter-
relations among the annotation layers; and (2) a
segmentation of the primary data into minimal re-
gions, which enables the definition of different to-
kenizations over the text. Contributed annotations
are also included in their original format, where
available.
3.1 WordNet Sense Annotations
A focus of the MASC project is to provide corpus
evidence to support an effort to harmonize sense
distinctions in WordNet and FrameNet (Baker and
Fellbaum, 2009), (Fellbaum and Baker, to appear).
The WordNet and FrameNet teams have selected
for this purpose 100 common polysemous words
whose senses they will study in detail, and the
MASC team is annotating occurrences of these
words in the MASC. As a first step, fifty oc-
currences of each word are annotated using the
WordNet 3.0 inventory and analyzed for prob-
lems in sense assignment, after which the Word-
Net team may make modifications to the inven-
tory if needed. The revised inventory (which will
be released as part of WordNet 3.1) is then used to
annotate 1000 occurrences. Because of its small
size, MASC typically contains less than 1000 oc-
currences of a given word; the remaining occur-
rences are therefore drawn from the 15 million
words of the OANC. Furthermore, the FrameNet
team is also annotating one hundred of the 1000
sentences for each word with FrameNet frames
and frame elements, providing direct comparisons
of WordNet and FrameNet sense assignments in
70
attested sentences.5
For convenience, the annotated sentences are
provided as a stand-alone corpus, with the Word-
Net and FrameNet annotations represented in
standoff files. Each sentence in this corpus is
linked to its occurrence in the original text, so that
the context and other annotations associated with
the sentence may be retrieved.
3.2 Validation
Automatically-produced annotations for sentence,
token, part of speech, shallow parses (noun and
verb chunks), and named entities (person, lo-
cation, organization, date and time) are hand-
validated by a team of students. Each annotation
set is first corrected by one student, after which it
is checked (and corrected where necessary) by a
second student, and finally checked by both auto-
matic extraction of the annotated data and a third
pass over the annotations by a graduate student
or senior researcher. We have performed inter-
annotator agreement studies for shallow parses in
order to establish the number of passes required to
achieve near-100% accuracy.
Annotations produced by other projects and
the FrameNet and Penn Treebank annotations
produced specifically for MASC are semi-
automatically and/or manually produced by those
projects and subjected to their internal quality con-
trols. No additional validation is performed by the
ANC project.
The WordNet sense annotations are being used
as a base for an extensive inter-annotator agree-
ment study, which is described in detail in (Pas-
sonneau et al, 2009), (Passonneau et al, 2010).
All inter-annotator agreement data and statistics
are published along with the sense tags. The re-
lease also includes documentation on the words
annotated in each round, the sense labels for each
word, the sentences for each word, and the anno-
tator or annotators for each sense assignment to
each word in context. For the multiply annotated
data in rounds 2-4, we include raw tables for each
word in the form expected by Ron Artstein?s cal-
culate alpha.pl perl script6, so that the agreement
numbers can be regenerated.
5Note that several MASC texts have been fully annotated
for FrameNet frames and frame elements, in addition to the
WordNet-tagged sentences.
6http://ron.artstein.org/resources/calculate-alpha.perl
4 MASC Availability and Distribution
Like the OANC, MASC is distributed without
license or other restrictions from the American
National Corpus website7. It is also available
from the Linguistic Data Consortium (LDC)8 for
a nominal processing fee.
In addition to enabling download of the entire
MASC, we provide a web application that allows
users to select some or all parts of the corpus and
choose among the available annotations via a web
interface (Ide et al, 2010). Once generated, the
corpus and annotation bundle is made available to
the user for download. Thus, the MASC user need
never deal directly with or see the underlying rep-
resentation of the stand-off annotations, but gains
all the advantages that representation offers. The
following output formats are currently available:
1. in-line XML (XCES9), suitable for use with
the BNCs XAIRA search and access inter-
face and other XML-aware software;
2. token / part of speech, a common input for-
mat for general-purpose concordance soft-
ware such as MonoConc10, as well as the
Natural Language Toolkit (NLTK) (Bird et
al., 2009);
3. CONLL IOB format, used in the Confer-
ence on Natural Language Learning shared
tasks.11
5 Tools
The ANC project provides an API for GrAF an-
notations that can be used to access and manip-
ulate GrAF annotations directly from Java pro-
grams and render GrAF annotations in a format
suitable for input to the open source GraphViz12
graph visualization application.13 Beyond this, the
ANC project does not provide specific tools for
use of the corpus, but rather provides the data in
formats suitable for use with a variety of available
applications, as described in section 4, together
with means to import GrAF annotations into ma-
jor annotation software platforms. In particular,
the ANC project provides plugins for the General
7http://www.anc.org
8http://www.ldc.upenn.edu
9XML Corpus Encoding Standard, http://www.xces.org
10http://www.athel.com/mono.html
11http://ifarm.nl/signll/conll
12http://www.graphviz.org/
13http://www.anc.org/graf-api
71
Architecture for Text Engineering (GATE) (Cun-
ningham et al, 2002) to input and/or output an-
notations in GrAF format; a ?CAS Consumer?
to enable using GrAF annotations in the Un-
structured Information Management Architecture
(UIMA) (Ferrucci and Lally, 2004); and a corpus
reader for importing MASC data and annotations
into NLTK14.
Because the GrAF format is isomorphic to in-
put to many graph-analytic tools, existing graph-
analytic software can also be exploited to search
and manipulate MASC annotations. Trivial merg-
ing of GrAF-based annotations involves simply
combining the graphs for each annotation, after
which graph minimization algorithms15 can be ap-
plied to collapse nodes with edges to common
subgraphs to identify commonly annotated com-
ponents. Graph-traversal and graph-coloring al-
gorithms can also be applied in order to iden-
tify and generate statistics that could reveal in-
teractions among linguistic phenomena that may
have previously been difficult to observe. Other
graph-analytic algorithms ? including common
sub-graph analysis, shortest paths, minimum span-
ning trees, connectedness, identification of artic-
ulation vertices, topological sort, graph partition-
ing, etc. ? may also prove to be useful for mining
information from a graph of annotations at multi-
ple linguistic levels.
6 Community Contributions
The ANC project solicits contributions of anno-
tations of any kind, applied to any part or all of
the MASC data. Annotations may be contributed
in any format, either inline or standoff. All con-
tributed annotations are ported to GrAF standoff
format so that they may be used with other MASC
annotations and rendered in the various formats
the ANC tools generate. To accomplish this, the
ANC project has developed a suite of internal tools
and methods for automatically transducing other
annotation formats to GrAF and for rapid adapta-
tion of previously unseen formats.
Contributions may be emailed to
anc@cs.vassar.edu or uploaded via the
ANC website16. The validity of annotations
and supplemental documentation (if appropriate)
are the responsibility of the contributor. MASC
14Available in September, 2010.
15Efficient algorithms for graph merging exist; see,
e.g., (Habib et al, 2000).
16http://www.anc.org/contributions.html
users may contribute evaluations and error reports
for the various annotations on the ANC/MASC
wiki17.
Contributions of unvalidated annotations for
MASC and OANC data are also welcomed and are
distributed separately. Contributions of unencum-
bered texts in any genre, including stories, papers,
student essays, poetry, blogs, and email, are also
solicited via the ANC web site and the ANC Face-
Book page18, and may be uploaded at the contri-
bution page cited above.
7 Conclusion
MASC is already the most richly annotated corpus
of English available for widespread use. Because
the MASC is an open resource that the commu-
nity can continually enhance with additional an-
notations and modifications, the project serves as a
model for community-wide resource development
in the future. Past experience with corpora such
as the Wall Street Journal shows that the commu-
nity is eager to annotate available language data,
and we anticipate even greater interest in MASC,
which includes language data covering a range of
genres that no existing resource provides. There-
fore, we expect that as MASC evolves, more and
more annotations will be contributed, thus creat-
ing a massive, inter-linked linguistic infrastructure
for the study and processing of current American
English in its many genres and varieties. In addi-
tion, by virtue of its WordNet and FrameNet anno-
tations, MASC will be linked to parallel WordNets
and FrameNets in languages other than English,
thus creating a global resource for multi-lingual
technologies, including machine translation.
Acknowledgments
The MASC project is supported by National
Science Foundation grant CRI-0708952. The
WordNet-FrameNet algnment work is supported
by NSF grant IIS 0705155.
References
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as complementary resources for
annotation. In Proceedings of the Third Linguistic
17http://www.anc.org/masc-wiki
18http://www.facebook.com/pages/American-National-
Corpus/42474226671
72
Annotation Workshop, pages 125?129, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, 1st edition.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: A
framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of ACL?02.
Christiane Fellbaum and Collin Baker. to appear.
Aligning verbs in WordNet and FrameNet. Linguis-
tics.
David Ferrucci and Adam Lally. 2004. UIMA: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10(3-4):327?348.
Michel Habib, Christophe Paul, and Laurent Viennot.
2000. Partition refinement techniques: an interest-
ing algorithmic tool kit. International Journal of
Foundations of Computer Science, 175.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural Language Engineering, 10(3-4):211?225.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-
based format for linguistic annotations. In Proceed-
ings of the Linguistic Annotation Workshop, pages
1?8, Prague, Czech Republic, June. Association for
Computational Linguistics.
Nancy Ide, Collin Baker, Christiane Fellbaum, Charles
Fillmore, and Rebecca Passonneau. 2008. MASC:
The Manually Annotated Sub-Corpus of American
English. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
Nancy Ide, Keith Suderman, and Brian Simms. 2010.
ANC2Go: A web application for customized cor-
pus creation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Valletta, Malta, May. European Lan-
guage Resources Association.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and
Nancy Ide. 2009. Making sense of word sense
variation. In SEW ?09: Proceedings of the Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 2?9, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Rebecca Passonneau, Ansaf Salleb-Aouissi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense an-
notation of polysemous words by multiple annota-
tors. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified relational
semantic representation. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 517?526, Washington, DC, USA.
IEEE Computer Society.
73
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75?80,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Shu-Kai Hsieh
Department of English
National Taiwan Normal University
Taipei, Taiwan
shukai@ntnu.edu.tw
Maurizio Tesconi
IIT
CNR
Pisa, Italy
maurizio.tesconi@iit.cnr.it
Monica Monachini
ILC
CNR
Pisa, Italy
monica.monachini@ilc.cnr.it
Piek Vossen, Roxanne Segers
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl,roxane.segers@gmail.com
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambigua-
tion systems present new challenges. The
difficulties found by supervised systems to
adapt might change the way we assess the
strengths and weaknesses of supervised
and knowledge-based WSD systems. Un-
fortunately, all existing evaluation datasets
for specific domains are lexical-sample
corpora. This task presented all-words
datasets on the environment domain for
WSD in four languages (Chinese, Dutch,
English, Italian). 11 teams participated,
with supervised and knowledge-based sys-
tems, mainly in the English dataset. The
results show that in all languages the par-
ticipants where able to beat the most fre-
quent sense heuristic as estimated from
general corpora. The most successful ap-
proaches used some sort of supervision in
the form of hand-tagged examples from
the domain.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in previous Senseval and SemEval competitions
(Kilgarriff, 2001; Mihalcea et al, 2004; Snyder
and Palmer, 2004; Pradhan et al, 2007). Spe-
cific domains pose fresh challenges to WSD sys-
tems: the context in which the senses occur might
change, different domains involve different sense
distributions and predominant senses, some words
tend to occur in fewer senses in specific domains,
the context of the senses might change, and new
senses and terms might be involved. Both super-
vised and knowledge-based systems are affected
by these issues: while the first suffer from differ-
ent context and sense priors, the later suffer from
lack of coverage of domain-related words and in-
formation.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain. All
datasets and related information are publicly avail-
able from the task websites
1
.
This task was designed in the context of Ky-
oto (Vossen et al, 2008)
2
, an Asian-European
project that develops a community platform for
modeling knowledge and finding facts across lan-
guages and cultures. The platform operates as a
Wiki system with an ontological support that so-
cial communities can use to agree on the mean-
ing of terms in specific domains of their interest.
Kyoto focuses on the environmental domain be-
cause it poses interesting challenges for informa-
tion sharing, but the techniques and platforms are
1
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
2
http://www.kyoto-project.eu/
75
independent of the application domain.
The paper is structured as follows. We first
present the preparation of the data. Section 3 re-
views participant systems and Section 4 the re-
sults. Finally, Section 5 presents the conclusions.
2 Data preparation
The data made available to the participants in-
cluded the test set proper, and background texts.
Participants had one week to work on the test set,
but the background texts where provided months
earlier.
2.1 Test datasets
The WSD-domain comprises comparable all-
words test corpora on the environment domain.
Three texts were compiled for each language by
the European Center for Nature Conservation
3
and
Worldwide Wildlife Forum
4
. They are documents
written for a general but interested public and in-
volve specific terms from the domain. The docu-
ment content is comparable across languages. Ta-
ble 1 shows the numbers for the datasets.
Although the original plan was to annotate mul-
tiword terms, and domain terminology, due to time
constraints we focused on single-word nouns and
verbs. The test set clearly marked which were
the words to be annotated. In the case of Dutch,
we also marked components of single-word com-
pounds. The format of the test set followed that of
previous all-word exercises, which we extended to
accommodate Dutch compounds. For further de-
tails check the datasets in the task website.
The sense inventory was based on publicly
available wordnets of the respective languages
(see task website for details). The annotation pro-
cedure involved double-blind annotation by ex-
perts plus adjudication, which allowed us to also
provide Inter Annotator Agreement (IAA) figures
for the dataset. The procedure was carried out us-
ing KAFnotator tool (Tesconi et al, 2010). Due
to limitations in resources and time, the English
dataset was annotated by a single expert annota-
tor. For the rest of languages, the agreement was
very good, as reported in Table 1.
Table 1 includes the results of the random base-
line, as an indication of the polysemy in each
dataset. Average polysemy is highest for English,
and lowest for Dutch.
3
http://www.ecnc.org
4
http://www.wwf.org
Total Noun Verb IAA Random
Chinese 3989 754 450 0.96 0.321
Dutch 8157 997 635 0.90 0.328
English 5342 1032 366 n/a 0.232
Italian 8560 1340 513 0.72 0.294
Table 1: Dataset numbers, including number of
tokens, nouns and verbs to be tagged, Inter-
Annotator Agreement (IAA) and precision of ran-
dom baseline.
Documents Words
Chinese 58 455359
Dutch 98 21089
English 113 2737202
Italian 27 240158
Table 2: Size of the background data.
2.2 Background data
In addition to the test datasets proper, we also pro-
vided additional documents on related subjects,
kindly provided by ECNC and WWF. Table 2
shows the number of documents and words made
available for each language. The full list with the
urls of the documents are available from the task
website, together with the background documents.
3 Participants
Eleven participants submitted more than thirty
runs (cf. Table 3). The authors classified their runs
into supervised (S in the tables, three runs), weakly
supervised (WS, four runs), unsupervised (no runs)
and knowledge-based (KB, the rest of runs)
5
. Only
one group used hand-tagged data from the domain,
which they produced on their own. We will briefly
review each of the participant groups, ordered fol-
lowing the rank obtained for English. They all par-
ticipated on the English task, with one exception
as noted below, so we report their rank in the En-
glish task. Please refer to their respective paper in
these proceedings for more details.
CFILT: They participated with a domain-
specific knowledge-based method based on Hop-
field networks (Khapra et al, 2010). They first
identify domain-dependant words using the back-
ground texts, use a graph based on hyponyms in
WordNet, and a breadth-first search to select the
most representative synsets within domain. In ad-
dition they added manually disambiguated around
one hundred examples from the domain as seeds.
5
Note that boundaries are slippery. We show the classifi-
cations as reported by the authors.
76
English
Rank Participant System ID Type P R R nouns R verbs
1 Anup Kulkarni CFILT-2 WS 0.570 0.555 ?0.024 0.594 ?0.028 0.445 ?0.047
2 Anup Kulkarni CFILT-1 WS 0.554 0.540 ?0.021 0.580 ?0.025 0.426 ?0.043
3 Siva Reddy IIITH1-d.l.ppr.05 WS 0.534 0.528 ?0.027 0.553 ?0.023 0.456 ?0.041
4 Abhilash Inumella IIITH2-d.r.l.ppr.05 WS 0.522 0.516 ?0.023 0.529 ?0.027 0.478 ?0.041
5 Ruben Izquierdo BLC20SemcorBackground S 0.513 0.513 ?0.022 0.534 ?0.026 0.454 ?0.044
- - Most Frequent Sense - 0.505 0.505 ?0.023 0.519 ?0.026 0.464 ?0.043
6 Ruben Izquierdo BLC20Semcor S 0.505 0.505 ?0.025 0.527 ?0.031 0.443 ?0.045
7 Anup Kulkarni CFILT-3 KB 0.512 0.495 ?0.023 0.516 ?0.027 0.434 ?0.048
8 Andrew Tran Treematch KB 0.506 0.493 ?0.021 0.516 ?0.028 0.426 ?0.046
9 Andrew Tran Treematch-2 KB 0.504 0.491 ?0.021 0.515 ?0.030 0.425 ?0.044
10 Aitor Soroa kyoto-2 KB 0.481 0.481 ?0.022 0.487 ?0.025 0.462 ?0.039
11 Andrew Tran Treematch-3 KB 0.492 0.479 ?0.022 0.494 ?0.028 0.434 ?0.039
12 Radu Ion RACAI-MFS KB 0.461 0.460 ?0.022 0.458 ?0.025 0.464 ?0.046
13 Hansen A. Schwartz UCF-WS KB 0.447 0.441 ?0.022 0.440 ?0.025 0.445 ?0.043
14 Yuhang Guo HIT-CIR-DMFS-1.ans KB 0.436 0.435 ?0.023 0.428 ?0.027 0.454 ?0.043
15 Hansen A. Schwartz UCF-WS-domain KB 0.440 0.434 ?0.024 0.434 ?0.029 0.434 ?0.044
16 Abhilash Inumella IIITH2-d.r.l.baseline.05 KB 0.496 0.433 ?0.024 0.452 ?0.023 0.390 ?0.044
17 Siva Reddy IIITH1-d.l.baseline.05 KB 0.498 0.432 ?0.021 0.463 ?0.026 0.344 ?0.038
18 Radu Ion RACAI-2MFS KB 0.433 0.431 ?0.022 0.434 ?0.027 0.399 ?0.049
19 Siva Reddy IIITH1-d.l.ppv.05 KB 0.426 0.425 ?0.026 0.434 ?0.028 0.399 ?0.043
20 Abhilash Inumella IIITH2-d.r.l.ppv.05 KB 0.424 0.422 ?0.023 0.456 ?0.025 0.325 ?0.044
21 Hansen A. Schwartz UCF-WS-domain.noPropers KB 0.437 0.392 ?0.025 0.377 ?0.025 0.434 ?0.043
22 Aitor Soroa kyoto-1 KB 0.384 0.384 ?0.022 0.382 ?0.024 0.391 ?0.047
23 Ruben Izquierdo BLC20Background S 0.380 0.380 ?0.022 0.385 ?0.026 0.366 ?0.037
24 Davide Buscaldi NLEL-WSD-PDB WS 0.381 0.356 ?0.022 0.357 ?0.027 0.352 ?0.049
25 Radu Ion RACAI-Lexical-Chains KB 0.351 0.350 ?0.015 0.344 ?0.017 0.368 ?0.030
26 Davide Buscaldi NLEL-WSD WS 0.370 0.345 ?0.022 0.352 ?0.027 0.328 ?0.037
27 Yoan Gutierrez Relevant Semantic Trees KB 0.328 0.322 ?0.022 0.335 ?0.026 0.284 ?0.044
28 Yoan Gutierrez Relevant Semantic Trees-2 KB 0.321 0.315 ?0.022 0.327 ?0.024 0.281 ?0.040
29 Yoan Gutierrez Relevant Cliques KB 0.312 0.303 ?0.021 0.304 ?0.024 0.301 ?0.041
- - Random baseline - 0.232 0.232 0.253 0.172
Chinese
Rank Participant System ID Type P R R nouns R verbs
- - Most Frequent Sense - 0.562 0.562 ?0.026 0.589 ?0.027 0.518 ?0.039
1 Meng-Hsien Shih HR KB 0.559 0.559 ?0.024 0.615 ?0.026 0.464 ?0.039
2 Meng-Hsien Shih GHR KB 0.517 0.517 ?0.024 0.533 ?0.035 0.491 ?0.038
- - Random baseline - 0.321 0.321 0.326 0.312
4 Aitor Soroa kyoto-3 KB 0.322 0.296 ?0.022 0.257 ?0.027 0.360 ?0.038
3 Aitor Soroa kyoto-2 KB 0.342 0.285 ?0.021 0.251 ?0.026 0.342 ?0.040
5 Aitor Soroa kyoto-1 KB 0.310 0.258 ?0.023 0.256 ?0.029 0.261 ?0.031
Dutch
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.526 0.526 ?0.022 0.575 ?0.029 0.450 ?0.034
2 Aitor Soroa kyoto-2 KB 0.519 0.519 ?0.022 0.561 ?0.027 0.454 ?0.034
- - Most Frequent Sense - 0.480 0.480 ?0.022 0.600 ?0.027 0.291 ?0.025
3 Aitor Soroa kyoto-1 KB 0.465 0.465 ?0.021 0.505 ?0.026 0.403 ?0.033
- - Random baseline - 0.328 0.328 0.350 0.293
Italian
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.529 0.529 ?0.021 0.530 ?0.024 0.528 ?0.038
2 Aitor Soroa kyoto-2 KB 0.521 0.521 ?0.018 0.522 ?0.023 0.519 ?0.035
3 Aitor Soroa kyoto-1 KB 0.496 0.496 ?0.019 0.507 ?0.020 0.468 ?0.037
- - Most Frequent Sense - 0.462 0.462 ?0.020 0.472 ?0.024 0.437 ?0.035
- - Random baseline - 0.294 0.294 0.308 0.257
Table 3: Overall results for the domain WSD datasets, ordered by recall.
This is the only group using hand-tagged data
from the target domain. Their best run ranked 1st.
IIITTH: They presented a personalized PageR-
ank algorithm over a graph constructed from
WordNet similar to (Agirre and Soroa, 2009),
with two variants. In the first (IIITH1), the vertices
of the graph are initialized following the rank-
ing scores obtained from predominant senses as in
(McCarthy et al, 2007). In the second (IIITH2),
the graph is initialized with keyness values as in
77
0.3 0.35 0.4 0.45 0.5 0.55
Rel. Cliques
Rel. Sem. Trees-2
Rel. Sem. Trees
NLEL-WSD
RACAI-Lexical-Chains
NLEL-WSD-PDB
BLC20BG
Kyoto-1
UCF-WS-domain.noPropers
IIITH2-d.r.l.ppv.05
IIITH1-d.l.ppv.05
RACAI-2MFS-BOW
IIITH1-d.l.baseline.05
IIITH2-d.r.l.baseline.05
UCF-WS-domain
HIT-CIR-DMFS
UCF-WS
RACAI-MFS
Treematch-3
Kyoto-2
Treematch-2
Treematch
CFILT-3
BLC20SC
BLC20SCBG
IIITH2-d.l.ppr.05
IIITH1-d.l.ppr.05
CFILT-1
CFILT-2
MFS
Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond
to one system (denoted in axis Y) according each recall and confidence interval (axis X ). Systems are
ordered depending on their rank.
(Rayson and Garside, 2000). Some of the runs
use sense statistics from SemCor, and have been
classified as weakly supervised. They submitted a
total of six runs, with the best run ranking 3rd.
BLC20(SC/BG/SCBG): This system is super-
vised. A Support Vector Machine was trained us-
ing the usual set of features extracted from con-
text and the most frequent class of the target word.
Semantic class-based classifiers were built from
SemCor (Izquierdo et al, 2009), where the classes
were automatically obtained exploiting the struc-
tural properties of WordNet. Their best run ranked
5th.
Treematch: This system uses a knowledge-
based disambiguation method that requires a dic-
tionary and untagged text as input. A previously
developed system (Chen et al, 2009) was adapted
to handle domain specific WSD. They built a
domain-specific corpus using words mined from
relevant web sites (e.g. WWF and ECNC) as
seeds. Once parsed the corpus, the used the de-
pendency knowledge to build a nodeset that was
used for WSD. The background documents pro-
vided by the organizers were only used to test how
exhaustive the initial seeds were. Their best run
ranked 8th.
Kyoto: This system participated in all four
languages, with a free reimplementation of
the domain-specific knowledge-based method for
WSD presented in (Agirre et al, 2009). It
uses a module to construct a distributional the-
saurus, which was run on the background text, and
a disambiguation module based on Personalized
PageRank over wordnet graphs. Different Word-
Net were used as the LKB depending on the lan-
guage. Their best run ranked 10th. Note that this
team includes some of the organizers of the task.
A strict separation was kept, in order to keep the
test dataset hidden from the actual developers of
the system.
RACAI: This participant submitted three differ-
ent knowledge-based systems. In the first, they use
the mapping to domains of WordNet (version 2.0)
in order to constraint the domains of the content
words of the test text. In the second, they choose
among senses using lexical chains (Ion and Ste-
fanescu, 2009). The third system combines the
previous two. Their best system ranked 12th.
HIT-CIR: They presented a knowledge-based
system which estimates predominant sense from
raw test. The predominant senses were calculated
with the frequency information in the provided
background text, and automatically constructed
78
thesauri from bilingual parallel corpora. The sys-
tem ranked 14.
UCFWS: This knowledge-based WSD system
was based on an algorithm originally described in
(Schwartz and Gomez, 2008), in which selectors
are acquired from the Web via searching with lo-
cal context of a given word. The sense is cho-
sen based on the similarity or relatedness between
the senses of the target word and various types
of selectors. In some runs they include predom-
inant senses(McCarthy et al, 2007). The best run
ranked 13th.
NLEL-WSD(-PDB): The system used for the
participation is based on an ensemble of different
methods using fuzzy-Borda voting. A similar sys-
tem was proposed in SemEval-2007 task-7 (Bus-
caldi and Rosso, 2007). In this case, the com-
ponent method used where the following ones:
1) Most Frequent Sense from SemCor; 2) Con-
ceptual Density ; 3) Supervised Domain Relative
Entropy classifier based on WordNet Domains;
4) Supervised Bayesian classifier based on Word-
Net Domains probabilities; and 5) Unsupervised
Knownet-20 classifiers. The best run ranked 24th.
UMCC-DLSI (Relevant): The team submitted
three different runs using a knowledge-based sys-
tem. The first two runs use domain vectors and
the third is based on cliques, which measure how
much a concept is correlated to the sentence by
obtaining Relevant Semantic Trees. Their best run
ranked 27th.
(G)HR: They presented a Knowledge-based
WSD system, which make use of two heuristic
rules (Li et al, 1995). The system enriched the
Chinese WordNet by adding semantic relations for
English domain specific words (e.g. ecology, en-
vironment). When in-domain senses are not avail-
able, the system relies on the first sense in the Chi-
nese WordNet. In addition, they also use sense
definitions. They only participated in the Chinese
task, with their best system ranking 1st.
4 Results
The evaluation has been carried out using the stan-
dard Senseval/SemEval scorer scorer2 as in-
cluded in the trial dataset, which computes preci-
sion and recall. Table 3 shows the results in each
dataset. Note that the main evaluation measure is
recall (R). In addition we also report precision (P)
and the recall for nouns and verbs. Recall mea-
sures are accompanied by a 95% confidence in-
terval calculated using bootstrap resampling pro-
cedure (Noreen, 1989). The difference between
two systems is deemed to be statistically signifi-
cant if there is no overlap between the confidence
intervals. We show graphically the results in Fig-
ure 1. For instance, the differences between the
highest scoring system and the following four sys-
tems are not statistically significant. Note that this
method of estimating statistical significance might
be more strict than other pairwise methods.
We also include the results of two baselines.
The random baseline was calculated analytically.
The first sense baseline for each language was
taken from each wordnet. The first sense baseline
in English and Chinese corresponds to the most
frequent sense, as estimated from out-of-domain
corpora. In Dutch and Italian, it followed the in-
tuitions of the lexicographer. Note that we don?t
have the most frequent sense baseline from the do-
main texts, which would surely show higher re-
sults (Koeling et al, 2005).
5 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. With
this paper we have motivated the creation of an
all-words test dataset for WSD on the environ-
ment domain in several languages, and presented
the overall design of this SemEval task.
One of the goals of the exercise was to show
that WSD systems could make use of unannotated
background corpora to adapt to the domain and
improve their results. Although it?s early to reach
hard conclusions, the results show that in each of
the datasets, knowledge-based systems are able to
improve their results using background text, and
in two datasets the adaptation of knowledge-based
systems leads to results over the MFS baseline.
The evidence of domain adaptation of supervised
systems is weaker, as only one team tried, and the
differences with respect to MFS are very small.
The best results for English are obtained by a sys-
tem that combines a knowledge-based system with
some targeted hand-tagging. Regarding the tech-
niques used, graph-based methods over WordNet
and distributional thesaurus acquisition methods
have been used by several teams.
79
All datasets and related information are publicly
available from the task websites
6
.
Acknowledgments
We thank the collaboration of Lawrence Jones-Walters, Amor
Torre-Marin (ECNC) and Karin de Boom (WWF), com-
piling the test and background documents. This work
task is partially funded by the European Commission (KY-
OTO ICT-2007-211423), the Spanish Research Department
(KNOW-2 TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings of the
12th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL09), pages 33?
41. Association for Computational Linguistics.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
Davide Buscaldi and Paolo Rosso. 2007. Upv-wsd : Com-
bining different wsd methods by means of fuzzy borda
voting. In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007), pages
434?437.
P. Chen, W. Ding, and D. Brown. 2009. A fully unsupervised
word sense disambiguation method and its evaluation on
coarse-grained all-words task. In Proceeding of the North
American Chapter of the Association for Computational
Linguistics (NAACL09).
Radu Ion and Dan Stefanescu. 2009. Unsupervised word
sense disambiguation with lexical chains and graph-based
context formalization. In Proceedings of the 4th Language
and Technology Conference: Human Language Technolo-
gies as a Challenge for Computer Science and Linguistics,
pages 190?194.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense dis-
ambiguation. In EACL ?09: Proceedings of the 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 389?397, Morristown,
NJ, USA. Association for Computational Linguistics.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense dis-
ambiguation combining corpus based and wordnet based
parameters. In Proceedings of the 5th International Con-
ference on Global Wordnet (GWC2010).
A. Kilgarriff. 2001. English Lexical Sample Task Descrip-
tion. In Proceedings of the Second International Work-
shop on evaluating Word Sense Disambiguation Systems,
Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
6
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
Natural Language Processing. HLT/EMNLP, pages 419?
426, Ann Arbor, Michigan.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995. A
wordnet-based algorithm for word sense disambiguation.
In Proceedings of The 14th International Joint Conference
on Artificial Intelligence (IJCAI95).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2007. Unsupervised acquisition of predominant
word senses. Computational Linguistics, 33(4).
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Eric W. Noreen. 1989. Computer-Intensive Methods for Test-
ing Hypotheses. John Wiley & Sons.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague, Czech Republic.
Paul Rayson and Roger Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the workshop
on Comparing corpora, pages 1?6.
Hansen A. Schwartz and Fernando Gomez. 2008. Acquir-
ing knowledge from the web to be used as selectors for
noun sense disambiguation. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CONLL08).
B. Snyder and M. Palmer. 2004. The English all-words task.
In Proceedings of the 3rd ACL workshop on the Evalua-
tion of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
M. Tesconi, F. Ronzano, S. Minutoli, C. Aliprandi, and
A. Marchetti. 2010. Kafnotator: a multilingual seman-
tic text annotation tool. In In Proceedings of the Second
International Conference on Global Interoperability for
Language Resources.
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Christiane
Fellbaum, Shu kai Hsieh, Chu-Ren Huang, Hitoshi Isa-
hara, Kyoko Kanzaki, Andrea Marchetti, Monica Mona-
chini, Federico Neri, Remo Raffaelli, German Rigau,
Maurizio Tescon, and Joop VanGent. 2008. Kyoto: a
system for mining, structuring and distributing knowl-
edge across languages and cultures. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
80
Augmenting WordNet for Deep
Understanding of Text
Peter Clark1
Christiane Fellbaum2
Jerry R. Hobbs3
Phil Harrison1
William R. Murray1
John Thompson1
1The Boeing Company (USA)
2Princeton University (USA)
3University of Southern California (USA)
email: peter.e.clark@boeing.com
Abstract
One of the big challenges in understanding text, i.e., constructing an over-
all coherent representation of the text, is that much information needed
in that representation is unstated (implicit). Thus, in order to ?fill in
the gaps? and create an overall representation, language processing sys-
tems need a large amount of world knowledge, and creating those knowl-
edge resources remains a fundamental challenge. In our current work,
we are seeking to augment WordNet as a knowledge resource for lan-
guage understanding in several ways: adding in formal versions of its
word sense definitions (glosses); classifying the morphosemantic links
between nouns and verbs; encoding a small number of ?core theories?
about WordNet?s most commonly used terms; and adding in simple rep-
resentations of scripts. Although this is still work in progress, we describe
our experiences so far with what we hope will be a significantly improved
resource for the deep understanding of language.
45
46 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
1 Introduction
Much information that text is intended to convey is not explicitly stated. Rather, the
reader constructs a mental model of the scene described by the text, including many
?obvious? features that were not explicitly mentioned. By one estimate, the ratio of
explicit to implicit facts is 1:8 (Graesser, 1981), making the task of understanding
text, i.e., constructing a coherent representation of the scene that the author intended
to convey, very difficult, even given the generally reasonable quality of syntactic in-
terpretation that today?s systems produce. For example, given the sentence:
A soldier was killed in a gun battle.
a reader will infer that (probably):
The soldier was shot; The soldier died; There was a fight; etc.
even though none of these facts are explicitly stated. A person is able to draw these
plausible conclusions because of the large amounts of world knowledge he/she has,
and his/her ability to use them to construct an overall mental model of the scene being
described.
A key requirement for this task is access to a large body of world knowledge. How-
ever, machines are currently poorly equipped in this regard. Although a few knowl-
edge encoding projects are underway, e.g., Cyc (Lenat and Guha, 1989), developing
such resources continues to be a major challenge, and any contribution to this task
has significant potential benefit. WordNet (Miller, 1995; Fellbaum, 1998) presents an
unique avenue for making inroads into this problem: It already has broad coverage,
multiple lexicosemantic connections, and significant knowledge encoded (albeit infor-
mally) in its glosses. It can thus be viewed as on the way to becoming an extensively
leveragable, ?lightweight? knowledge base for reasoning. In fact, WordNet aleady
plays a central role in many question-answering systems e.g., 21 of the 26 teams in
the recent PASCAL RTE3 challenge used WordNet (Giampiccolo et al, 2007), and
most other large-scale resources already include mappings to it and thus can leverage
it easily. In our work we are developing several augmentations to WordNet to improve
its utility further, and we report here on our experiences to date.
Althoughwe are performing experimentswith recognizing textual entailment (RTE)
(determining whether a hypothesis sentence H follows from some text T), it is impor-
tant to note that RTE is not our end-goal. Many existing RTE systems, e.g., (Adams
et al, 2007; Chambers et al, 2007) largely work by statistically scoring the match be-
tween T and H, but this to an extent sidesteps ?deep? language understanding, namely
building a coherent, internal representation of the overall scenario the input text was
intended to convey. RTE is one way of measuring success in this endeavor, but it is
also possible to do moderately well in RTE without the system even attempting to
?understand? the scenario the text is describing. It is yet to be seen whether very high
performance in RTE can be obtained without some kind of deep language understand-
ing of the entire scene that a text conveys.
We are testing our work with BLUE, Boeing?s Language Understanding Engine,
which we first describe. We then present the WordNet augmentations that we are de-
veloping, and our experience with these as well as with the DIRT paraphrase database.
Augmenting WordNet for Deep Understanding of Text 47
The contribution of this paper is some preliminary insight into avenues and challenges
for creating and leveraging more world knowledge, in the context of WordNet, for
deeper language understanding.
2 Text Interpretation and Subsumption
2.1 Text Interpretation
For text interpretation we are using BLUE, Boeing?s Language Understanding Engine
(Clark and Harrison, 2008), comprising a parser, logical form (LF) generator, and fi-
nal logic generator. Parsing is performed using SAPIR, a mature, bottom-up, broad
coverage chart parser (Harrison and Maxwell, 1986). The parser?s cost function is
biased by a database of manually and corpus-derived ?tuples? (good parse fragments),
as well as hand-coded preference rules. During parsing, the system also generates
a logical form (LF), a semi-formal structure between a parse and full logic, loosely
based on Schubert and Hwang (1993). The LF is a simplified and normalized tree
structure with logic-type elements, generated by rules parallel to the grammar rules,
that contains variables for noun phrases and additional expressions for other sentence
constituents. Some disambiguation decisions are performed at this stage (e.g., struc-
tural, part of speech), while others are deferred (e.g., word senses, semantic roles),
and there is no explicit quantifier scoping. A simple example of an LF is shown below
(items starting with underscores _?A?I? denote variables):
;;; LF for "A soldier was killed in a gun battle."
(DECL ((VAR _X1 "a" "soldier")
(VAR _X2 "a" "battle" (NN "gun" "battle")))
(S (PAST) NIL "kill" _X1 (PP "in" _X2)))
The LF is then used to generate ground logical assertions of the form r(x,y), con-
taining Skolem instances, by applying a set of syntactic rewrite rules recursively to it.
Verbs are reified as individuals, Davidsonian-style. An example of the output is:
;;; logic for "A soldier was killed in a gun battle."
object(kill01,soldier01)
in(kill01,battle01)
modifier(battle01,gun01)
plus predicates associating each Skolem with its corresponding input word. At this
stage of processing, the predicates are syntactic relations (subject(x,y), object(x,y),
modifier(x,y), and all the prepositions, e.g., in(x,y)). Definite coreference is computed
by a special module which uses the (logic for the) referring noun phrase as a query
on the database of assertions. Another module performs special structural transforma-
tions, e.g., when a noun or verb should map to a predicate rather than an individual.
Two additional modules perform (currently naive) word sense disambiguation (WSD)
and semantic role labelling (SRL), described further in Clark and Harrison (2008).
However, for our RTE experiments we have found it more effective to leave senses
and roles underspecified, effectively considering all valid senses and roles (for the
given lexical features) during reasoning until instantiated by the rules that apply.
48 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
2.2 Subsumption
A basic operation for reasoning is determining if one set of clauses subsumes (is more
general than, is thus implied by) another, e.g., (the logic for) ?A person likes a person?
subsumes ?A man loves a woman?. This basic operation is used both to determine if
an axiom applies, and in RTE to determine if a text H subsumes (is implied by) a text
T or its axiom-expanded elaboration. A set S1 of clauses subsumes another S2 if each
clause in S1 subsumes some (different) member of S2. A clause C1 subsumes another
C2 if both (for binary predicates) of C1?s arguments subsume the corresponding argu-
ments in C2, and C1 and C2?s predicates ?match?. An argument A1 subsumes another
A2 if some word sense for A1?s associated word is equal or more general (a hypernym
of) some word sense of A2?s associated word (thus effectively considering all possible
word senses for A1 and A2)1. We also consider adjectives related by WordNet?s ?sim-
ilar? link, e.g., ?clean? and ?pristine?, to be equal. Two syntactic predicates ?match?
(i.e., are considered to denote the same semantic relation) according to the following
rules:
1. both are the same;
2. either is the predicate ?of? or ?modifier?;
3. the predicates ?subject? and ?by? match (for passives);
4. the two predicates are in a small list of special cases that should match e.g., ?on?
and ?onto?.
These rules for matching syntactic roles are clearly an approximation to match-
ing semantic roles, but have performed better in our experiments than attempting to
explicitly assign (with error) semantic roles early on and then matching on those.
In addition, in language, ideas can be expressed using different parts of speech
(POS) for the same basic notion, e.g., verb or noun as in ?The bomb destroyed the
shrine? or ?The destruction of the shrine by the bomb? (Gurevich et al, 2006). To
handle these cross-POS variants, when finding the word senses of a word (above) our
system considers all POS, independent of its POS in the original text. Combined with
the above predicate-matching rules, this is a simple and powerful way of aligning
expressions using different POSs, e.g.:
? ?The bomb destroyed the shrine? and ?The destruction of the shrine by the
bomb? (but not ?The destruction of the bomb by the shrine?) are recognized as
equivalent.
? ?A person attacks with a bomb? and ?There is a bomb attack by a person? are
recognized as equivalent.
? ?There is a wrecked car?, ?The car was wrecked?, and ?The car is a wreck?
(adjective, verb, and noun forms) are recognized as equivalent.
Although clearly these heuristics can go wrong, they provide a basic mechanism
for assessing simple equivalence and subsumption between texts.
1Clearly this can go wrong, e.g., if the contexts of T and H are different so repeated/matching words
have incompatible intended senses, although such discontinuities are unusual in natural text.
Augmenting WordNet for Deep Understanding of Text 49
2.3 Experimental Test Bed
As an experimental test bed we have developed a publically available RTE-style test
suite2 of 250 pairs (125 entailed, 125 not entailed). As our goal is deeper semantic
processing, the texts are syntactically simpler than the PASCAL RTE sets (at www.
pascal-network.org) but semantically challenging to process. We use examples
from this test suite (and others) in this paper.
3 Exploiting Lexical & World Knowledge
3.1 Use of WordNet?s Glosses
Translation to Logic WordNet?s word sense definitions (glosses) appear to contain
substantial amounts of world knowledge that could help with semantic interpretation
of text, and we have been exploring leveraging these by translating them into first-
order logic. We have also experimented with Extended WordNet (XWN), a similar
database constructed several years ago by Moldovan and Rus (2001).
To do the translation, a different language interpreter, developed by ISI, was used
(for historic reasons? BLUE was not available at the time the translations were done,
and has not been exercised or extended for definition processing). ISI?s system works
as follows: First each gloss is converted into a sentence of the form ?word is gloss?
and parsed using the Charniak parser. Then the parse tree is then converted into a log-
ical syntax by a system called LFToolkit, developed by Nishit Rathod. In LFToolkit,
lexical items are translated into logical fragments involving variables. Finally, as syn-
tactic relations are recognized, variables in the constituents are identified as equal. For
example, ?John works? is translated into John(x1) & work(e,x2)& present(e), where e
is a working event, and then a rule which recognizes ?John? as the subject of ?works?
sets x1 and x2 equal to each other. Rules of this sort were developed for a large
majority of English syntactic constructions. ISI?s system was then used to translate
the modified WordNet glosses into axioms. For example (rewritten from the original
eventuality notation):
;;; "ambition#n2: A strong drive for success"
ambition(x1) -> a(x1) & strong(x1) & drive(x1) & for(x1,x6) & success(x6)
Predicates are assigned word senses using the new-ly released WordNet sense-
tagged gloss corpus3. This process was applied to all ? 110,000 glosses, but with
particular focus on glosses for the 5,000 ?core? (most frequently used) synsets. It
resulted in good translations for 59.4% of the 5,000 core glosses, with lower quality
for the entire gloss corpus. Where there was a failure, it was generally the result of a
bad parse, with constructions for which no LFToolkit rules had been written. In these
cases, the constituents are translated into logic, so that no information is lost; what
is lost is the equalities between variables that provides the connections between the
constituents. For instance, in the ?John works? example, we would know that there
was someone named John and that somebody works, but we would not know that they
were the same person. Altogether 98.1% of the 5,000 core glosses were translated
into correct axioms (59.4%) or axioms that had all the propositional content but were
2http://www.cs.utexas.edu/~pclark/bpi-test-suite/
3http://wordnet.princeton.edu/glosstag
50 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
disconnected in this way (38.7%). The remaining 1.9% of these glosses had bizarrely
wrong parses due to noun-adjective ambiguities or to complex conjunction ambigui-
ties.
Using the Glosses We have used a combination of these logicalized glosses and those
from XWN to infer implicit information from text. Although the quality of the logic
is generally poor (for a variety of reasons, in particular that the glosses were never
intended for machine processing in the first place), our software was able to infer
conclusions that help answer a few entailment problems, for example:
T: Britain puts curbs on immigrant labor from Bulgaria and Romania.
H: Britain restricted workers from Bulgaria.
using the logic for the definition:
restrict#v1: "restrict", "restrain": place limits on.
plus WordNet?s knowledge that: ?put? and ?place? are synonyms; ?curb? and ?limit?
are synonyms; and a laborer is a worker. In our experiments, the glosses were used to
answer 5 of the 250 entailment questions (4 correctly). More commonly, the glosses
came ?tantalizingly close? to providing the needed knowledge. For example, for:
T: A Union Pacific freight train hit five people.
H: A locomotive was pulling the train.
it seems that the definition:
train#n1: "train", "railroad train": public transport provided
by a line of railway cars coupled together and drawn by a locomotive.
is very close to providing the needed knowledge. However, unfortunately it defines
a train as ?public transport provided by cars pulled by a locomotive? rather than just
?cars pulled by a locomotive? (the locomotive pulls the cars, not the train/public-
transport), hence the hypothesis H is not concluded. Similarly:
T: The Philharmonic orchestra draws large crowds.
H: Large crowds were drawn to listen to the orchestra.
essentially requires knowledge that crowds (typically) listen to orchestras. WordNet?s
glosses come very close to providing this, with knowledge that:
orchestra = collection of musicians
musician = someone who plays musical instrument
music = sound produced by musical instruments
listen = hear = perceive sound
However, the connection that the playing results in sound production is missing,
and hence again H cannot be inferred. These experiences with the WordNet glosses
were very common. In summary, our experience is the WordNet glosses provided
some value, being used 5 times (4 correctly) on the 250 examples in our test suite,
Augmenting WordNet for Deep Understanding of Text 51
with the short, simple definitions (e.g., bleed = lose blood) being the most reliable.
The low quality of the logic was a problem (definitional text is notoriously difficult to
interpret automatically (Ide and Veronis, 1993)), although often the knowledge came
close. Finally, 110,000 rules (approx. one per gloss) is actually quite a small number;
typically only 10?s of rules fired per sentence, rarely containing the implications we
were looking for.
3.2 Typed Morphosemantic Links
WordNet contains approximately 21,000 links connecting derivationally related verb
and noun senses, e.g., employ#v2-employee#n1; employ#v2-employment#n3. These
links turn out to be essential for mapping between verbal and nominalized expressions
(e.g. using ?destroy?-?destruction?, as mentioned earlier). However, the current links
do not state the semantic type of the relation (e.g., that employee#n1 is the UNDER-
GOER of an employ#v2 event; employment#n3 is the employ#v2 EVENT itself),
which limits WordNet?s ability to help perform semantic role labeling. In addition,
not being able to distinguish the semantics of the relationships can cause errors in
reasoning, for example distinguishing between H1 and H2 in:
T: Detroit produces fast cars.
H1: Detroit?s product is fast.
H2?: Detroit?s production is fast. [NOT entailed]
T: The Zoopraxiscope was invented by Mulbridge.
H1: Mulbridge was the inventor of the Zoopraxiscope.
H2?: Mulbridge was the invention of the Zoopraxiscope. [NOT entailed]
To type these links, we have used a semi-automatic process: First, the computer
makes a ?guess? at the appropriate semantic relation based on the morphological re-
lationship between the noun and the verb (e.g., ?A?IJ-er?A?I? nouns usually refer to the
agent), and the location of the two synsets in WordNet?s taxonomy. Second, a human
validates and corrects these, a considerably faster progress than entering them from
scratch. 9 primary semantic relations (as well as 5 rarer ones) were used, namely:
agent (e.g., employ#v2-employer#n1)
undergoer/patient (e.g., employ#v2-employee#n1)
instrument (e.g., shred#v1-shredder#n1)
recipient (e.g., grant#v2-grantee#n1)
result (e.g., produce#v2-product#n2)
body-part (e.g., adduct#v1-adductor#n1)
vehicle (e.g., cruise#v4-cruiser#n3)
location (e.g., bank#v3-bank#n4)
identity/equality (eg employ#v2-employment#n1)
The resulting database of 21,000 typed links was recently completed, constituting
a major new addition to WordNet in support of deep language processing. One of the
surprising side results of this effort was discovering how often the normal morpholog-
ical defaults (e.g., ?-er? nouns refer to agents) are violated, described in more detail in
Fellbaum et al (2007). We are now in the process of incorporating the database into
our software.
52 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
3.3 Core Theories
While WordNet?s glosses and links contain world knowledge about specific entities
and relations, there is also more fundamental knowledge about language and the world
? e.g., about space, time, and causality ? which is essential for understanding many
types of text, yet is unlikely to be expressed in dictionary definitions or automatically
learnable. To address this need, we are also encoding by hand a number of theories to
support deeper reasoning (in the style of lexical decomposition). We have axiomatized
a number of abstract core theories that underlie the way we talk about events and event
structure (Hobbs, 2008). Among these are theories of composite entities (things made
of other things), scalar notions (of which space, time, and number are specializations),
change of state, and causality. For example, in the theory of change of state, the
predication change(e1,e2) says there is a change of state from state e1 to state e2. The
predication changeFrom(e1) says there is a change out of state e1. The predication
changeTo(e2) says there is a change into state e2. An inference from changeFrom(e1)
is that e1 no longer holds. An inference from changeTo(e2) is that e2 now does hold.
In the theory of causality (Hobbs, 2005), the predication cause(e1,e2), for e1 causes
e2, is explicated. One associated inference is that if the causing happens, then the
effect e2 happens. A defeasible inference is that not-cause-not often is the same as
cause:
not(cause(x,not(e)))? cause(x,e)
In the rightward direction this is of course sometimes wrong, but if we go to the
trouble of saying that the negation of something was not caused, then very often it is
a legitimate conclusion that the causing did happen.
We are connecting these theories with WordNet by mapping the core (5,000 most
common) WordNet synsets to the theory predicates. For example, the core part of
WordNet contains 450 word senses having to do with events and event structure, and
we are in the process of encoding their meanings in terms of core theory predicates.
For example, if x lets e happen (WordNet sense let#v1), then x does not cause e not to
happen:
let#v1(x,e)? not(cause(x,not(e)))
One sense of ?go? is ?changeTo?, as in ?I go crazy?
go#v4(x,e)? changeTo(e)
(The entity x is the subject of the eventuality e.) If x frees y (the verb sense of ?free?),
then x causes a change to y being free (in the adjective sense of ?free?):
free#v1(x,y)? cause(x,changeTo(free#a1(y)))
Given these mappings and the core theories themselves, this is enough to answer
the entailment pair:
T: The captors freed the hostage.
H: The captors let the hostage go free.
Augmenting WordNet for Deep Understanding of Text 53
via successive application of the above axioms:
(part of) H interpretation? let(x,go(y,free(y)))
? not(cause(x,not(changeTo(free#a1(y)))))
? cause(x,changeTo(free#a1(y)))
? free#v1(x,y)
We are still in the early stages of developing this resource and have not yet evaluated
it, but we have already seen a number of examples of its potential utility in the text
inference problem such as above.
3.4 Scripts
Simple inference rules, such as in the above resources, provide a direct means of
drawing conclusions from a few words in the input text. However, they are largely
context-independent, i.e., not sensitive to the bigger picture which the surrounding
text provides. Consider the following example:
T: A dawn bomb attack devastated a major shrine.
H: The bomb exploded.
In this case, it is hard to express the required knowledge (to conclude H follows
from T) as simple rules (e.g., the rules ?bomb ?E?? bomb explode? or ?bomb attack
? bomb explode? are not adequate, as we do not want H to follow from ?The police
destroyed the bomb.? or ?The bomb attack was thwarted?). Rather, when a person
reads T, he/she recognizes a complete scenario from multiple bits of evidence (possi-
bly in multiple sentences), and integrates what is read with that scenario. This kind of
top-down, expectation-driven process seems essential for creating an overall, coherent
representation of text.
Although scripts are an old idea (e.g., (Schank and Abelson, 1977)) there are rea-
sons their use may be more feasible today. First, rapid advances in paraphrasing sug-
gests that the matching problem ? deciding if some text is expressing part of of a
script ? may be substantially eased. (Script work in the ?70s required stories to
be worded in exactly the right way to fire a script). Second, two new approches for
amassing knowledge are available today that were not available previously, namely au-
tomated learning from corpora, and use of Web volunteers (e.g., (Chklovski, 2005)),
and may be applicable to script acquisition (Script work in the ?70s typically worked
with tiny databases of scripts). Finally, techniques for language processing have sub-
stantially improved, making core tasks (e.g., parsing) less problematic, and opening
the possibility to easy authoring of scripts in English, followed by machine interpre-
tation. FrameNet (Baker et al, 1998) already provides a few small scripts, but does
not currently encode the complex scenarios that we would like; a vastly expanded
resource would be highly useful.
We are in the early stages of exploring this avenue, encoding scripts as a list of
simple English sentences, which are then automatically translated to WordNet-sense
tagged logic using our software. For example, a ?bombing? script looks:
A building is bombed by an attacker.
The attacker plants the bomb in the building.
54 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
The bomb explodes.
The explosion damages or destroys the building.
The explosion injures or kills people in the building.
In addition, some of these sentences are flagged as ?salient?. If any salient sentence
matches (subsumes) part of the text, then the script is triggered. When triggered,
a standard graph-matching algorithm searches for the maximal overlap between the
clauses in the (interpreted) script and the clauses in the text, and then the script is
unified with the text according to that maximal overlap, thus asserting the additional
facts contained in the script to the text under consideration. In the example earlier,
the script is triggered by, and matched with, the text, thus aligning ?building? with
?shrine?, and asserting additional facts including (the logic representation of) ?The
bomb explodes? and ?The bomb was planted in the building.?.
3.5 Using DIRT Paraphrases
Like others, we have also explored the use of the DIRT paraphrase database for rea-
soning, and we report our experiences here for comparison. The database contains
12 million rules, discovered automatically from text, of form (X relation1 Y) ? (X
relation2 Y), where relation is a path in the dependency tree/parse between constitu-
tents X and Y. Although they are noisy (informally, about 50% seem reliable), they
provided some leverage for us also, for example correctly answering:
T: William Doyle works for an auction house in Manhattan.
H?: William Doyle never goes to Manhattan. [NOT entailed]
using the DIRT rule ?IF Y works in X THEN Y goes to X? combined with negation,
and
T: The president visited Iraq in September.
H: The president traveled to Iraq.
using the (slightly strange but plausible) DIRT rule ?IF Y is visited by X THEN X
flocks to Y? and that ?A?IJflock?A?I? is a type (hyponym) of ?A?IJtravel?A?I?. In our
experiments, DIRT rules were used 47 times (27 correctly) on our 250 example test
suite. The main cause of incorrect answers was questionable/incorrect rules in the
database, e.g.:
T: The US troops stayed in Iraq.
H?: The US troops left Iraq. [NOT entailed]
was found to be entailed using the DIRT rule ?IF Y stays in X THEN Y leaves X?.
In addition, DIRT does not distinguish word senses (e.g., according to DIRT, shooting
a person/basket implies killing the person/basket and scoring a person/basket), also
contributing errors.
Despite this, the DIRT rules were useful because they go beyond just the defini-
tional knowledge in WordNet. For example, according to DIRT ?X marries Y? im-
plies, among other things: Y marries X; X lives with Y; X kisses Y; X has a child with
Y; X loves Y ? all examples of plausible world knowledge. The main limitations we
Augmenting WordNet for Deep Understanding of Text 55
found were they were noisy, did not account for word senses, and only cover one rule
pattern (X r1 Y? X r2 Y). So, for example, a rule like ?X buys Y? X pays Money?
is outside the expressive scope of DIRT.
4 Preliminary Evaluation
Although this is work in progress, we have evaluated some of these augmentations
using our test suite. As our ultimate goal is deeper understanding of text, we have de-
liberately eschewed using statistical similarity measures between T and H, and instead
used abductive reasoning to create an axiom-elaborated representation of T, and then
seen if it is subsumed by H. Although not using statistical similarity clearly hurts our
score, in particular assuming ?no entailment? when the elaborated representation of T
is not subsumed by H, we believe this keeps us appropriately focused on our longer-
term goal of deeper understanding of text. The results on our 250 pairs currently are:
H or ?H predicted by: Corrrect Incorrect
Simple syntax manipulation 11 3
WordNet taxonomy + morphosemantics 14 1
WordNet logicalized glosses 4 1
DIRT paraphrase rules 27 20
H or ?H not predicted: Corrrect Incorrect
(assumed not entailed) 97 72
Thus our overall score on this test suite is 61.2%. We have also run our software on
the PASCALRTE3 dataset (Giampiccolo et al, 2007), scoring 55.7% (excluding cases
where no initial logical representation could be constructed due to parse/LF generation
failures). In some cases, other known limitations of WordNet (eg. hypernym errors,
fine-grained senses) also caused errors in our tests (outside the scope of this paper).
However, the most significant problem, at least for these tests, was lack of world
knowledge.
5 Conclusion
A big challenge for deep understanding of text ? constructing a coherent represen-
tation of the scene it is intended to convey ? is the need for large amounts of world
knowledge. We have described our work-in-progress to augment WordNet in vari-
ous ways so it can better provide some of this knowledge, and described some initial
experiences with those augmentations, as well as with the DIRT database. Existing
WordNet aleady provides extensive leverage for language processing, as evidenced
by the large number of groups using it. The contribution of this paper is some pre-
liminary insight into avenues and challenges for further developing this resource. Al-
though somewhat anecdotal at this stage, our experience suggests the augmentations
have promise for further improving deep language processing, and we hope will result
in a significantly improved resource.
Acknowledgements This work was performed under the DTO AQUAINT program,
contract N61339-06-C-0160.
56 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
References
Adams, R., G. Nicolae, C. Nicolae, and S. Harabagiu (2007). Textual entailment
through extended lexical overlap and lexico-semantic matching. In Proc. ACL-
PASCAL Workshop on Textual and Entailment and Paraphrasing, pp. 119?124.
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998). The Berkeley FrameNet project.
In C. Boitet and P. Whitelock (Eds.), Proc 36th ACL Conf., CA, pp. 86?90. Kauf-
mann.
Chambers, N., D. Cer, T. Grenager, D. Hall, C. K. MacCartney, M.-C. de Marneffe,
D. R. Yeh, and C. D. Manning (2007). Learning alignments and leveraging natural
logic. In Proc. ACL-PASCAL Workshop on Textual and Entailment and Paraphras-
ing, pp. 165?170.
Chklovski, T. (2005). Collecting paraphrase corpora from volunteer contributors. In
Proc 3rd Int Conf on Knowledge Capture (KCap?05), NY, pp. 115?120. ACM.
Clark, P. and P. Harrison (2008, September). Boeing?s NLP System and the Chal-
lenges of Semantic Representation. In J. Bos and R. Delmonte (Eds.), Semantics
in Text Processing. STEP 2008 Conference Proceedings, Venice, Italy.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Cambridge, MA:
MIT Press.
Fellbaum, C., A. Osherson, and P. Clark (2007). Putting semantics into wordnet?s
morphosemantic links. In Proc. 3rd Language and Technology Conference, Poz-
nan, Poland.
Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan (2007). Textual entailment
through extended lexical overlap and lexico-semantic matching. In Proc. ACL-
PASCAL Workshop on Textual and Entailment and Paraphrasing, pp. 1?9.
Graesser, A. C. (1981). Prose Comprehension Beyond the Word. NY: Springer.
Gurevich, O., R. Crouch, T. King, and V. de Paiva (2006). Deverbal nouns in knowl-
edge representation. In Proc. FLAIRS?06.
Harrison, P. and M. Maxwell (1986). A new implementation of GPSG. In Proc. 6th
Canadian Conf on AI (CSCSI-86), pp. 78?83.
Hobbs, J. (2005). Toward a useful notion of causality for lexical semantics. Journal
of Semantics 22, 181?209.
Hobbs, J. (2008). Encoding commonsense knowledge. Technical report, USC/ISI.
http://www.isi.edu/?hobbs/csk.html.
Ide, N. and J. Veronis (1993). Extracting knowledge-bases from machine-readable
dictionaries: Have we wasted our time? In Proc KB&KB?93 Workshop, pp. 257?
266.
Augmenting WordNet for Deep Understanding of Text 57
Lenat, D. and R. Guha (1989). Building Large Knowledge-Based Systems. MA:
Addison-Wesley.
Miller, G. (1995). WordNet: a lexical database for english. Comm. of the
ACM 38(11), 39?41.
Moldovan, D. and V. Rus (2001). Explaining answers with extended wordnet. In
Proc. ACL?01.
Schank, R. and R. Abelson (1977). Scripts, Plans, Goals and Understanding. Hills-
dale, NJ: Erlbaum.
Schubert, L. and C. Hwang (1993). Episodic logic: A situational logic for NLP. In
Situation Theory and Its Applications, pp. 303?337.
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 62?70,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Multimodal Vocabulary for Augmentative and Alternative Communi-
cation from Sound/Image Label Datasets 
 
Xiaojuan Ma Christiane Fellbaum Perry R. Cook 
 Princeton University  
 
 
35 Olden St. Princeton, NJ 08544, USA 
{xm,fellbaum,prc}@princeton.edu 
 
 
 
Abstract 
Existing Augmentative and Alternative Com-
munication vocabularies assign multimodal 
stimuli to words with multiple meanings. The 
ambiguity hampers the vocabulary effective-
ness when used by people with language dis-
abilities. For example, the noun ?a missing 
letter? may refer to a character or a written 
message, and each corresponds to a different 
picture. A vocabulary with images and sounds 
unambiguously linked to words can better 
eliminate misunderstanding and assist com-
munication for people with language disorders. 
We explore a new approach of creating such a 
vocabulary via automatically assigning se-
mantically unambiguous groups of synonyms 
to sound and image labels. We propose an un-
supervised word sense disambiguation (WSD) 
voting algorithm, which combines different 
semantic relatedness measures. Our voting al-
gorithm achieved over 80% accuracy with a 
sound label dataset, which significantly out-
performs WSD with individual measures. We 
also explore the use of human judgments of 
evocation between members of concept pairs, 
in the label disambiguation task. Results show 
that evocation achieves similar performance to 
most of the existing relatedness measures.  
1 Introduction 
In natural languages, a word form may refer to dif-
ferent meanings. For instance, the word ?fly? 
means ?travel through the air? in context like ?fly 
to New York,? while it refers to an insect in the 
phrase ?a fly on the trashcan.? Speakers determine 
the appropriate sense of a polysemous word based 
on the context. However, people with language 
disorders and access/retrieval problems, may have 
great difficulty in understanding words individual-
ly or in a context. To overcome such language bar-
riers, visual and auditory representations are intro-
duced to help illustrate concepts (Ma et al, 
2009a)(Ma et al, 2010). For example, a person 
with a language disability can tell the word ?fly? 
refers to ?travel through the air? when he sees a 
plane in the image (rather than an insect); likewise 
he can distinguish the meaning of ?fly? given the 
plane engine sound vs. the insect buzzing sound.  
This approach has been employed in Augmentative 
and Alternative Communication (AAC), in the 
form of multimodal vocabularies in assistive de-
vices (Steele et al 1989)(Lingraphica, 2010). 
However, current AAC vocabularies assign vis-
ual stimuli to words instead of specific meanings, 
and thus bring in ambiguity when a user with lan-
guage disability tries to comprehend and commu-
nicate a concept. For example, for the word ?fly,? 
Lingraphica only has an icon showing a plane and 
a flock of birds flying. Confusion arises when a 
sentence like ?I want to kill the fly (the insect)? is 
explained using the airplane/bird icon. Similarly, it 
will lead to miscommunication if the sound of keys 
jingling is used to express ?a key is missing? when 
the person intends to refer to a key on the keyboard. 
People with language impairment are relying on 
the AAC vocabularies for language access, and any 
ambiguity may result in communication failure.  
To address this problem, we propose building a 
semantic multimodal AAC vocabulary with visual 
and auditory representations expressing concepts 
rather than words (Figure 1), as the backbone of 
the language assistant system for people with 
aphasia (Ma et al 2009b). Our work is exploratory 
with the following innovations: 1) we target the 
insufficiency of current assistive vocabularies by 
resolving ambiguity; 2) we enrich concept invento-
ry and connect concepts through language, envi-
ronmental sounds, and images (little research has 
looked into conveying concepts through natural 
nonspeech sounds); and 3) our vocabulary has a 
dynamic scalable semantic network structure rather 
62
than simply grouping words into categories as 
conventional assistive devices do.  
One intuitive way to build a disambiguated mul-
timodal vocabulary is to manually assign meanings 
to each word in the existing vocabulary. However, 
the task is time consuming with poor scalability ? 
no new multimedia representations are generated 
for concepts that are missing in the vocabulary. 
ImageNet (Jia et al, 2009) was constructed by 
people verifying the assignment of web images to 
given synonym sets (synsets). ImageNet has over 
nine million images linked to about 15 thousands 
noun synsets in WordNet (Fellbaum, 1998). De-
spite the huge human effort, ImageNet, with the 
goal of creating a computer vision database, does 
not yet include all the most commonly used words 
across different parts of speech. It is not yet suita-
ble for a language support application. 
We explore a new approach for generating a vo-
cabulary with concept to sound/image associations, 
that is, conducting word sense disambiguation 
(WSD) techniques used in Natural Language 
Processing on sound/image label datasets. For ex-
ample, the labels ?car, drive, fast? for the sound 
?car ? passing.wav? are assigned to synsets ?car: a 
motor vehicle,? ?drive: operate or control a ve-
hicle,? and ?fast: quickly or rapidly? via WSD. It 
means the sound ?car ? passing.wav? can be used 
to depict those concepts. This approach is viable 
because the words in the sound/image labels were 
shown to evoke one another based on the audito-
ry/visual content, and their meanings can be identi-
fied by considering all the tags generated for a 
given sound or image as a context.  With the avail-
ability of large sound/image label datasets, the vo-
cabulary created from WSD can be easily 
expanded. 
A variety of WSD methods (e.g. knowledge-
based methods (Lesk, 1986), unsupervised me-
thods (Lin, 1997), semi-supervised methods 
(Hearst, 1991) (Yarowsky, 1995), and supervised 
methods (Novischi et al, 2007)) were developed 
and evaluated with corpus data and other text doc-
uments like webpages. Compared to the text data 
that WSD methods work with, labels for sounds 
and images have unique characteristics. The labels 
are a bag of words related to the visual/auditory 
content; there is no syntactic or part of speech in-
formation, nor are the words necessarily contextual 
neighbors. For example, contexts suggest land-
scape senses for the word pair ?bank? and ?water?, 
whereas in an image, a person may drink water 
inside a bank building. Furthermore, few annotated 
image or sound label datasets are available, making 
it hard to apply supervised or semi-supervised 
WSD methods.  
To efficiently and effectively create a disambi-
guated multimodal vocabulary, we need to achieve 
two goals. First, optimize the accuracy of the WSD 
algorithm to minimize the work required for ma-
nual checking and correction afterwards. Second, 
construct a semantic network across different parts 
of speech, and thus explore linking semantic rela-
tedness measures that can capture aspects different 
from existing ones. In this paper, we target the first 
goal by proposing an unsupervised sense disam-
 
Figure 1. Disambiguated AAC multimedia vocabulary; dash arrows are semantic relations between concepts. 
63
biguation algorithm combining a variety of seman-
tic relatedness measures. We chose an unsuper-
vised method because of the lack of a large 
manually annotated gold standard. The measure-
combined voting algorithm presented here draws 
advantages from different semantic relatedness 
measures and has them vote for the best-fitting 
sense to assign to a label. Evaluation shows that 
the voting algorithm significantly exceeds WSD 
with each individual measure. 
To approach the second goal, we proposed and 
tested a semantic relatedness measure called evo-
cation (Boyd-Graber et al, 2006) in disambigua-
tion of sound/image labels. Evocation measures 
human judgements of relatedness between a di-
rected concepts pair. It provides cross parts of 
speech evocativeness information which supple-
ments most of the knowledge-based semantic rela-
tedness measures. Evaluation results showed that 
the performance of WSD with evocation is no 
worse than most of the relatedness measures that 
we applied, despite the relatively small size of the 
current evocation dataset. 
2 Dataset: Semantic Labels for Environ-
mental Sounds and Images 
Our ultimate goal is to create an AAC vocabulary 
of associations between environmental sounds and 
images and groups of synonymous words that are 
relevant to the content. We are working with two 
datasets of human labels for multimedia data, 
SoundNet and the Peekaboom dataset. 
2.1 SoundNet Sound Label Dataset 
The SoundNet Dataset (Ma, Fellbaum, and Cook, 
2009) consists of 327 environmental ?soundnails? 
(5-second audio clips) each with semantic labels 
collected from participants via a large scale Ama-
zon Mechanical Turk (AMT) study. The sound-
nails cover a wide range of auditory scenes, from 
vehicle (e.g. car starting), mechanical tools (e.g. 
handsaw) and electrical devices (e.g. TV), to natu-
ral phenomena (e.g. rain), animals (e.g. a dog bark-
ing), and human sounds (e.g. a baby crying). In the 
AMT study, participants were asked to generate 
tags for each soundnail labeling its source, possible 
location, and actions involved in making the sound.  
Each soundnail was labeled by over 100 people. 
The tags were clustered into meaning units that 
SoundNet refers to as ?sense sets.? A sense set in-
cludes a set of words with similar meanings. For 
instance, for the soundnail pre-labeled ?bag, zipO-
pen? which is the sound of opening the zipper of a 
bag, the following sense sets were generated:  
(a) ?zipper? {zipper, zip up, zip, unzip};  
(b) ?bag? {bag, duffle bag, nylon bag, suitcase, 
luggage, backpack, purse, pack, briefcase};  
(c) ?house? {house, home, building}, and 
(d) ?clothes? {clothes, jacket, coat, pants, jeans, 
dress, garment}.  
 The word in bold is was judged by SoundNet to 
be the best representative of the sense set, and oth-
er words, possibly belonging to different parts of 
speech are included in the curly brackets enclosing 
the sense sets. SoundNet uses sense sets rather than 
single words because 1) people may use different 
words to describe the same underlying concept, 
(e.g. ?baby? and ?infant;? ?rain? as a noun and as a 
verb); 2) people cannot draw fine distinctions be-
tween objects and events that generate similar 
sounds, and thus may come up with different but 
related categories (e.g. ?plate,? ?cup,? and ?bowl? 
for the dish clinking sound); and 3) people may 
perceive objects and events that are not explicitly 
presented in the sound very differently (e.g. ?bag? 
vs. ?clothes? for the sound made by a zipper). In 
this experiment, only sense sets (labels) that were 
generated by at least 25% of the labelers were 
used.  
In our disambiguation experiment, two kinds of 
contexts were explored. In the Context 1 scheme, 
each label is treated separately: all its members 
plus the representatives of the other sense sets are 
considered. Take the soundnail ?bag, zipOpen? as 
an example. The context for disambiguating label 
(a) ?zipper? {zipper, zip up, zip, unzip} is: 
zipper, zip up, zip, unzip, bag, house, clothes. 
The context for label (d) ?clothes? {clothes, jacket, 
coat, pants, jeans, dress, garment} is:  
clothes, jacket, coat, pants, jeans, dress, garment, 
zipper, bag, house.   
In the Context 1 scheme, all representative 
words will be disambiguated multiple times. The 
final result will be the synset that gets the most 
votes. In the Context 2 scheme, as for the image 
dataset described below, all members from each 
sense set are put together to create the context, and 
each word is disambiguated only once. 
2.2 Peekaboom Image Label Dataset 
64
The ESP Game Dataset (Von Ahn and Dabbish, 
2004) contains a large number of web images and 
human labels produced via an online game. For 
example, an image of a glass of hard liquor is la-
beled ?full, shot, alcohol, clear, drink, glass, beve-
rage.? The Peekaboom Game (Von Ahn et al, 
2006) is the successor of the ESP Game. In our 
experiment, part of the Peekaboom Dataset (3,086 
images) was used. For each image, all the labels 
together form the context for sense disambigua-
tion.  
The Peekaboom labels are noisier than the 
SoundNet labels for several reasons. First, random 
objects may appear in a picture and thus be in-
cluded in the labels. For example, an image is la-
beled ?computer, shark? because there is a shark 
picture on the computer screen. Second, texts in 
the images are often included in the labels. For 
example, the word ?green? is one of the labels for 
an image with a street sign ?Green St.? Third, the 
Peekaboom labels are not stemmed, which adds 
another layer of ambiguity. For example, the labels 
?bridge, building? could refer to a building event 
or to a built entity. In the experiment, all labels for 
an image are used in their unstemmed form to con-
struct the context for WSD.  
3 Evocation and Other Semantic Related-
ness Measures 
A set of measures were selected to assess the rela-
tedness between possible senses of words in the 
sound/image labels. Apart from existing methods, 
an additional measure, evocation, is introduced. 
3.1 Evocation 
Evocation (Boyd-Graber et al, 2006) measures 
concept similarity based on human judgment. It is 
a directed measure, with evocation(synset A, syn-
set B) defined as how much synset A brings to 
mind synset B. The evocation dataset has been ex-
tended to scores for 100,000 directed synset pairs 
(Nikolova et al, 2009).  
The evocation data were collected independently 
of WordNet or corpus data. We propose the use of 
evocation in WSD for image and sound labels for 
the following reasons. First, the sound and image 
labels are generated based on human perception of 
the content and common knowledge. In SoundNet 
in particular, many of the evoked labels reflected 
the most obvious objects or events in a sound 
scene. For example, ?bag? and ?coat? were evoked 
from the zipper soundnail. In this case, the evoca-
tion score may be a good evaluation of the related-
ness between the labels. Second, evocation 
assesses relatedness of concepts across different 
parts of speech, which is suitable for identifying 
image and sound labels containing nouns, verbs, 
adjectives, adverbs, etc. 
This paper is a first attempt to compare the ef-
fectiveness of the use of evocation measure in 
sense disambiguation to the conventional, relative-
ly better tested similarity measures, in the context 
of assigning synsets to sound/image labels. Consi-
dering that the evocation dataset is small in size 
and susceptible to noise given the method by 
which it was collected, we have not yet incorpo-
rated evocation into the measure-combined voting 
algorithm described in the Section 4. 
3.2 Semantic Relatedness Measures 
Nine measures of semantic relatedness1  between 
synsets are used in the experiment, both as contri-
butors to the voting algorithm and as baselines for 
comparison, including: 
1) WordNet path based measures. 
? ?path? ? shortest path length between syn-
sets,  inversely proportional to the number 
of nodes on the path. 
? ?wup? (Wu and Palmer, 1994) ? ratio of the 
depth of the Least Common Subsumer 
(LCS) to the depths of two synsets in the 
Wordnet taxonomy. 
? ?lch? (Leacock and Chodorow, 1998) ? 
considering the length of the shortest path 
between two synsets to the depth of the 
WordNet taxonomy. 
2) Information and content based measures. 
? ?res? (Resnik, 1995) ? the informational 
content (IC) of a given corpus of the LCS 
between two synsets. 
? ?lin? (Lin, 1997) ? the ratio of the IC of the 
LCS to the IC of the two synsets. 
? ?jcn? (Jiang and Conrath, 1997) ? inversely 
proportional to the difference between the 
IC of the two synsets and the IC of the LCS. 
                                                           
1 ?hso? (Hirst and St-Onge, 1998) extensively slows down the 
WSD process with over five context words, and thus, is not 
included in the experiment. 
65
3) WordNet definition based measures. 
? ?lesk? (Banerjee and Pedersen, 2002) ? 
overlaps in the definitions of two synsets.  
? ?vector? (Patwardhan and Pedersen, 2006) 
? cosine of the angle between the co-
occurrence vector computed from the defi-
nitions around the two synsets. 
? ?vector_pairs? ? co-occurrence vectors are 
computed from definition pairs separately. 
The computation of the relatedness scores using 
measures listed above were carried out by codes 
from the WordNet::Similarity (Pedersen et al, 
2004) and WordNet::SenseRelate projects (Peder-
sen and Kolhatkar, 2009). In contrast to Word-
Net::SenseRelated, which employs only one 
similarity measure in the WSD process, this paper 
proposes a strategy of having several semantic re-
latedness measures vote for the best synset for each 
word. The voting algorithm intends to improve 
WSD performance by combining conclusions from 
various measures to eliminate a false result. Since 
there is no syntax among the words generated for a 
sound/image, they should all be considered for 
WSD. Thus, the width of the context window is the 
total number of words in the context. 
4 Label Sense Disambiguation Algorithm 
 
Figure 2 shows the overall process of the measure-
combined voting algorithm for disambiguating 
sound/image labels. After the context for WSD is 
generated, the process is divided into two steps. In 
Step I, the relatedness scores of each sense of a 
word based on the context is computed by each 
measure separately. Step II combines results from 
all measures and generates the disambiguated syn-
sets for all words in the sound/image labels. Evo-
cation did not participate in Step II. 
4.1 Step I: Generate Candidate Synsets Based 
on Individual Measures 
Given the context of M words (w1, ?, wM), and K 
relatedness measures (k = 1, ?, K), the task is to 
assign each word wj (j = 1, ?, M) to the synset 
sx,wj that is the most appropriate within the context. 
Here, the word wj has Nj synsets, denoted as sn,wj (n 
= 1, ?, Nj). Step I is to calculate the relatedness 
score for each synset of each word in the context. 
, , ,
1,...,
1,...,
( ) max ( ( , ))
j j m
m
m j
k i w k i w n w
n N
m M
score s measure s s
?
==
= ?  
The evocation score between two sysnets sa, sb is 
the maximum of the directed evocation ratings.  
( , ) max( ( , ), ( , ))a b a b b a
evocation
score s s evocation s s evocation s s=  
, , ,
1,...,
1,...,
( ) max ( ( , ))
j j m
m
m j
i w i w n w
evocation n N evocation
m M
score s score s s
?
==
= ?  
The synset that evocation assigns to word j is the 
one with the highest score. 
, ,j jw x ws s if=
=
  
, ,
1,...,
( ) max ( ( ))
j j
j
x w i w
evocation i N evocation
score s score s
=
=  
4.2 Step II: Vote for the Best Candidate 
Three voting schemes were tested, including un-
weighted simple votes, weighted votes among top 
candidates, and weighted votes among all synsets. 
1) Unweighted Simple Votes 
Synset sn,wj of word wj gets a vote from related-
ness measure k if its scorek is the maximum among 
all the synsets for wj, and it becomes the candidate 
synset for wj elected by measure k (Ck,wj): 
, ,
1,...,
,
1, ( ) max ( ( ))
( )
0,
j j
j
j
k x w k i w
i N
k x w
if score s score s
vote s
else
=
=??= ?
??
 , ,( ) , ( ) 1j j jk w x w k x wcandidate s s if vote s= =  
The candidate list for word wj (candidates(Swj)) 
is the union of all candidate synsets elected by in-
dividual relatedness measures. 
1,...,
( ) ( ( ))
j jw k wk K
candidates s union candidate s
=
=  
For each candidate in the list, the votes from all 
measures are calculated. The one receiving the 
most votes becomes the proposed synset for wj. 
, ,
1
( ) ( )
j j
K
i w k i w
k
voteCount s vote s
=
=?  
Figure 2. Measure-Combined Voting Algorithm. 
66
,,
, ,
( )
,
( ) max ( ( ))
j j
j j
i w wj j
w x w
x w i w
s candidates s
s s if
voteCount s voteCount s
?
=
=  
2) Weighted Votes among Top Candidates 
The weighted voting scheme avoids a situation 
where the false results win by a very small margin. 
The weight under relatedness measure k for si,wj is 
calculated as the relative score to the maximum 
scorek among all synsets for word wj. It suggests 
how big of a difference in relatedness score of any 
given synset is to the highest score among all the 
possible synsets for the target word. 
, , ,
1,...,
( ) ( ) / max ( ( ))
j j j
j
k x w k x w k i w
i N
weight s score s score s
=
=  
The weighted votes synset si,wj receives over all 
measures is the sum of its weight under individual 
measure. In voting scheme 2, the synset from the 
candidate list which gets the highest weighted 
votes becomes the winner. 
, ,
1
( ) ( )
j j
K
i w k i w
k
weightedVote s weight s
=
=?  
,
,
, ,
( )
,
( ) max ( ( ))
j j
j j
i w wj j
w x w
x w i w
s candidates s
s s if
weightedVote s weightedVote s
?
=
=
 
3) Weighted Votes among All Synsets 
Voting scheme 3 differs from 2 in that the synset 
from all synsets for word wj which gets the highest 
weighted votes is the proposed synset for wj. 
,
, ,
1,...,
,
( ) max ( ( ))
j j
j j
j
w x w
x w i w
i N
s s if
weightedVote s weightedVote s
=
=
=  
5 Evaluation 
The evaluation of WSD with evocation and the 
measure-combined voting algorithm was carried 
out primarily on the SoundNet label dataset be-
cause of the availability of ground truth data. 
SoundNet provides manual annotation for 1,553 
different words for 327 soundnails (e.g. the word 
?road? appears in 41 sounds). 
The accuracy rate (precision) was computed for 
each WSD method. The sound level accuracy of a 
WSDk is the average percentage of correct sense 
assignments over the 327 sounds. The word level 
accuracy is the mean over 1553 distinctive words. 
Accuracy rates of different measures at both level 
accepted the null hypothesis in homogeneity test. 
327
1
1553
1
( ) ( (% ) ) / 327
( ) ( (% ) ) /1553
k i
sound level i
k w
word level w
accuracy WSD correctness
accuracy WSD correctness
? =
? =
=
=
?
?
 
Due to the lack of ground truth in the Peekaboom 
dataset, we only computed the overlap between the 
WSD result of 3,086 images from the voting algo-
rithm, evocation and each relatedness measures. 
5.1 Overall Comparison across WSD me-
thods with Various Relatedness Measures 
Figures 3 show the overall comparison among dif-
ferent methods at both sound level and word level. 
It suggests that the performance of the evocation 
measure in sense disambiguation is as good as the 
path-based and context-based measures. The defi-
nition-based measures (?lesk? and ?vector?) are 
significantly better than other measures if used in-
dividually (similar to (Patwardhan et al2003)). 
 
Figure 3. Accuracy rate at word and sound level in comparison among evocation, voting, and nine individual 
sense similarity measures. 
67
However, the voting algorithms proposed in this 
work significantly outperformed each individual 
measure based on ANOVA results. At sound level, 
Context 1: (F(12, 20176) = 102.92, p < 0.001); 
Context 2: (F(12, 4238) = 89.42, p < 0.001). At 
word level, Context 1: (F(12, 20176) = 68.78, p < 
0.001); Context 2: (F(12, 4238) = 60.72, p < 0.001). 
The scheme of composing context (Section 2.1) 
has significant impact on the accuracy, with Con-
text 1 (taking all members in the related sense set 
and representatives from the others) outperforming 
Context 2 (taking all words in all sense sets) at the 
word level (F(1, 40352) = 20.19, p < 0.001). The 
influence of context scheme is not significant at the 
sound level (F(1, 8476) = 0.35, p = 0.5546). The 
interaction between measures and context schemes 
is not significant, indicating that accuracy differ-
ences are similar regardless of context construction. 
5.2 Performance of the Voting Algorithm  
Figure 4 shows the histogram (distribution) for the 
accuracy rate at sound and word levels. We see 
that for the voting algorithm, the accuracy rates are 
greater than 0.7 for most of the sounds, and greater 
than 0.9 for majority of the words to disambiguate. 
Figure 5 show the percentage of sense disam-
biguation results overlapping between voting algo-
rithm and individual relatedness measures. Note 
that any two methods may come up with different 
correct results (e.g. ?lesk? assigned ?chirp? as ?a 
sharp sound? while the voting algorithm assigned 
?chirp? as ?making a sharp sound?). This indicates 
the change of the contribution of each relatedness 
measures in different voting schemes. In the simple 
voting scheme, more disambiguation results came 
from the ?path,? ?wup,? and ?lch? (the WordNet 
path based measures), while the weighted voting 
 
Figure 4. Histogram of accuracy rate at sound (327, left) and word level (1553, right) among different measures, 
contexts, and voting schemes. EVC1 = Evocation (Context 1); SR11 = Voting (Context 1, voting scheme 1). 
 
Figure 5. Percentage of sense disambiguation results overlap between voting algorithm, evocation, and individ-
ual sense relatedness measures at image (3,086 images) and sound (327 sounds) level. 
68
scheme took more of the recommendations from 
?lesk,? ?lin,? and ?jcn? (context and definition 
based measures) into consideration. At the sound 
level, there is no significant accuracy difference 
among the three voting schemes, and the influence 
of the context composition is similar. However, at 
the word level (Figure 3), the weighted voting 
schemes significantly outperformed the simple vot-
ing scheme (F(2, 9312) = 5.20, p = 0.0055), and all 
of them have significantly better accuracy when 
the context contains mainly members from the 
same sense set (F(1, 9312) = 4.79, p = 0.0287). 
5.3 Performance of WSD with Evocation 
As shown in Figures 3, the performance of the 
evocation measure is not significantly different 
from path-based and some context-based measures 
at sound level, including ?path,? ?wup,? ?lch,? 
?res,? ?lin,? and ?jcn? (for Context 1, F(6, 2282) = 
2.0582, p = 0.0551; for Context 2, F(6, 2282) = 
1.6679, p = 0.1249); and is significantly better than 
the vector_pairs measure (for Context 1, F(1, 652) 
= 61.37, p < 0.001; for Context 2, F(1, 652) = 
36.47, p < 0.001). At the word level, the perfor-
mance of the evocation measure is not significantly 
different from that of measures including ?path,? 
?wup,? ?lch,? ?res? (F(4, 7760) = 0.39, p = 0.8135), 
and ?lin,?  ?jcn,? and ?vector_pairs? (F(3, 6208) = 
1.52, p = 0.2077). Figure 8 (SoundNet) and Figure 
9 (Peekaboom) show the percentage of synset as-
signment overlap between evocation and the other 
nine relatedness measures. The overlap with ?lesk? 
and ?vector? are significantly higher than that with 
the other measures (F(8, 5877) = 34.67, p < 0.001). 
It suggests that evocation as a semantic relatedness 
measure may be closer to the definition-based 
measures than path and content based measures. 
For the SoundNet dataset, 34% to 44% of evoca-
tion WSD results overlap with that of other meas-
ures; for the Peekaboom dataset, the overlap is 
25% to 35% (Figure 6). Given that evocation per-
formed similarly in accuracy to most of other 
measures with relatively low overlap in WSD re-
sults, evocation may capture different aspects of 
semantic relatedness from existing measures. 
6 Conclusion and Future Work 
We explored the construction of a sense disambi-
guated semantic AAC multimodal vocabulary from 
sound/image label datasets. Two WSD approaches 
are introduced to assign specific meanings to envi-
ronmental sound and image labels, and further 
create concept-sound/image associations. The 
measure-combined voting algorithm targets the 
accuracy of WSD and achieves significantly better 
performance than each relatedness measure indivi-
dually. Our second approach applies a new rela-
tedness measure, evocation. Evocation achieves 
similar performance to most of the existing rela-
tedness measures with sound labels. Results sug-
gest that evocation provides different semantic 
information from current measures. 
Future work includes: 1) expanding the evoca-
tion dataset and investigating the potential im-
provement in its WSD accuracy; 2) incorporating 
the extended evocation dataset into the voting al-
gorithm; 3) exploring additional information such 
as image and sound similarity to help with WSD. 
Acknowledgments  
 
Figure 6. Percentage of WSD results overlap between evocation and various relatedness measures. 
69
We thank the Kimberley and Frank H. Moss ?71 
Princeton SEAS Research Fund for supporting our 
project. 
References  
Satanjeev Banerjee and Ted Pedersen. 2002. An 
Adapted Lesk Algorithm for Word Sense Disambig-
uation Using WordNet. Proceedings of the 3rd Inter-
national Conference on Intelligent Text Processing 
and Computational Linguistics. 
Jordan Boyd-Graber, Christaine Fellbaum, Daniel 
Osherson, and Robert Schapire. 2006. Adding Dense, 
Weighted Connections to WordNet. Proceedings of 
the Thirds International WordNet Conference. 
Jia Deng, Wei Dong, Richard Socher, Li -J. Li, Kai Li 
and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hie-
rarchical Image Database. Proceedings of the IEEE 
Computer Vision and Pattern Recognition (CVPR). 
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press. 
Marti Hearst. 1991. Noun Homograph Disambiguation 
Using Local Context in Large Text Corpora. Proc. of 
the 7th Annual Conference of the University of Water-
loo Center for the New OED and Text Research. 
Graeme Hirst and David St. Onge. 1998. Lexical Chains 
as Representations of Context for the Detection and 
Correction of Malapropisms. In Christiane Fellbaum, 
editor, WordNet: An Electronic Lexical Database. 
Jay Jiang and David Conrath. 1997. Semantic Similarity 
Based on Corpus Statistics and Lexical Taxonomy. 
Proceedings on International Conference on Re-
search in Computational Linguistics.  
Claudia Leacock and Martin Chodorow. 1998. Combin-
ing Local Context and WordNet Similarity for Word 
Sense Identification. In Christiane Fellbaum, editor, 
WordNet: An Electronic Lexical Database. 
Michael Lesk. 1986. Automatic Sense Disambiguation 
Using Machine Readable Dictionaries: How to Tell a 
Pine Cone from an Ice Cream Cone. Proceedings of 
SIGDOC?86. 
Dekang Lin. 1997. Using Syntactic Dependency as a 
Local Context to Resolve Word Sense Ambiguity. 
Proceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 64-71. 
Lingraphica. http://www.aphasia.com/. 2010. 
Xiaojuan Ma, Christiane Fellbaum. and Perry Cook. 
2010. SoundNet: Investigating a Language Com-
posed of Environmental Sounds. In Proc. CHI 2010. 
Xiaojuan Ma, Jordan Boy-Graber, Sonya Nikolova, and 
Perry Cook. 2009a. Speaking Through Pictures: Im-
ages vs. Icons. Proceedings of ASSETS09. 
Xiaojuan Ma, Sonya Nikolova and Perry Cook. 2009b. 
W2ANE: When Words Are Not Enough - Online 
Multimedia Language Assistant for People with 
Aphasia. Proceedings of ACM Multimedia 2009.  
Sonya Nikolova, Jordan Boyd-Graber, and Christiane 
Fellbaum. 2009. Collecting Semantic Similarity Rat-
ings to Connect Concepts in Assistive Communica-
tion Tools (in press). Modelling, Learning and 
Processing of Text-Technological Data Structures, 
Springer Studies in Computational Intelligence. 
Adrian Novischi, Muirathnam Srikanth, and Andrew 
Bennett. 2007. Lcc-wsd: System Description for 
English Coarse Grained All Words Task at SemEval 
2007. Proceedings of the 4th International Workshop 
on Semantic Evaluations(SemEval-2007), pp 223-226. 
Siddharth Patwardhan, Satanjeev Benerjee and Ted Pe-
dersen. Using Measures of Semantic Relatedness for 
Word Sense Disambiguation. 2003. Proceeding of 
CICLing2003, pp. 241-257. 
Siddharth Patwardhan and Ted Pedersen Using Word-
Net Based Context Vectors to Estimate the Semantic 
Relatedness of Concepts. 2006. Proceedings of the 
EACL 2006 Workshop Making Sense of Sense - 
Bringing Computational Linguistics and Psycholin-
guistics Together, pp. 1-8 
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WorNet::Similarity ? Measuring the Re-
latedness of Concepts.  Proceedings of Human 
Language Technology Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics Demonstrations, pp. 38-41. 
Ted Pedersen and Varada Kolhatkar. 2009. Word-
Net::SenseRelate::AllWords - A Broad Coverage 
Word Sense Tagger that Maximimizes Semantic Re-
latedness. Proceedings of Human Language Tech-
nology Conference of the North American Chapter of 
the Association for Computational Linguistics Dem-
onstrations, pp. 17-20. 
Philip Resnik. 1995. Using Information Content to Eva-
luate Semantic Similarity in a Taxonomy. Proceed-
ings of the 14th International Joint Conference on 
Artificial Intelligence. 
Richard Steele, Michael Weinrich, Robert Wertz, Gloria 
Carlson, and Maria Kleczewska. Computer-based 
visual communication in aphasia. Neuropsychologia. 
27(4): pp 409-26. 1989. 
Luis von Ahn, Laura Dabbish. 2004. Labeling images 
with a computer game. Proceedings of the SIGCHI 
conference on Human factors in computing systems, 
p.319-326. 
Luis von Ahn, Ruoran Liu, Manuel Blum. 2006 Peeka-
boom: a game for locating objects in images. Pro-
ceedings of the SIGCHI conference on Human 
Factors in computing systems. 
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics 
and Lexical Selection. Proc. of ACL, pp 133-138. 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Proceed-
ings of the 33rd Annual Meeting on Association For 
Computational Linguistics. 
70
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 38?41,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
The Role of Adverbs in Sentiment Analysis
Eduard C. Dragut
Computer and Information Sciences Dept.
Temple University
edragut@temple.edu
Christiane Fellbaum
Department of Computer Science
Princeton University
fellbaum@princeton.edu
Abstract
Sentiment Analysis, an important area of
Natural Language Understanding, often
relies on the assumption that lexemes carry
inherent sentiment values, as reflected in
specialized resources. We examine and
measure the contribution that eight intensi-
fying adverbs make to the sentiment value
of sentences, as judged by human anno-
tators. Our results show, first, that the
intensifying adverbs are not themselves
sentiment-laden but strengthen the senti-
ment conveyed by words in their contexts
to different degrees. We consider the con-
sequences for appropriate modifications of
the representation of the adverbs in senti-
ment lexicons.
1 Introduction
It was probably Chuck who coined the term ?arm-
chair linguist? (Svartvik, 1991). Chuck Fillmore?s
deep commitment to the study of language ? in
particular lexical semantics ? on the basis of cor-
pus data served as a model that kept many of us
honest in our investigation of language. Today,
we are lucky to be able to work from our office
chairs while collecting data from a broad speaker
group by means of crowdsourcing. And Chuck?s
FrameNet taught us the importance of consider-
ing word meanings in their contexts. Our paper
presents work that tries to take this legacy to heart.
2 Sentiment Analysis
Broadly speaking, sentiment analysis (SA) at-
tempts to automatically derive a writer?s ?senti-
ment? about the topic of a text. ?Sentiment? is
usually categorized into ?positive,? ?neutral? and
?negative,? where positive corresponds to satisfac-
tion or happiness and ?negative? to dissatisfaction
or unhappiness. Some work in SA further dis-
tinguishes degrees of positive and negative senti-
ment. SA often refers to lexical resources where
words are annotated with a sentiment value. Sen-
tiWordNet (SWN) (Esuli and Sebastiani, 2006) as-
signs one of three sentiment values to each synset
in WordNet (Fellbaum, 1998). Opinion Finder
(OF) (Wilson et al., 2005) identifies the sentiment
of the writer. Other resources include Appraisal
Lexicon (AL) (Taboada and Grieve, 2004) and
Micro-WNOp (Cerini et al., 2007).
Much of this work relies on the assumption that
specific lexemes (unique mappings of word forms
and word meanings) carry an inherent sentiment
value. This seems intuitively correct for words like
enjoy (positive), pencil (neutral) and pain (nega-
tive).
Other words may not carry inherent sentiment
value yet, in context, contribute to that of the
words they co-occur with or modify. One such
class of words comprises what we call polarity
intensifiers. In this preliminary study, we ana-
lyze the contribution of adverbial intensifiers to
the sentiment value of the sentences in which they
occur.
Consider the adverb absolutely in two sam-
ple sentences from movie reviews:
S1 He and Leonora have absolutely no chemistry
on screen whatsoever.
S2 I was absolutely delighted by the simple story
and amazing animation.
The goal of this preliminary experimental study
is to seek answers to the following questions
38
Adverbs OF AL SWN
absolutely Neu. ? Neu.
awfully Neg. Neg. Neu.
enormously Neg. ? Neu.
extremely Neg. ? Pos.
horribly Neg. Neg. Neu.
incredibly Pos. Pos. Neu.
pretty Pos. Pos. Neu.
seriously Neg. ? Neu.
Table 1: Eight intensifying adverbs and their po-
larities in sentiment lexicons.
1. Do the adverbs we investigate carry inherent
sentiment values, as postulated by some sen-
timent lexicons?
2. Which adverbs have the strongest sentiment
intensifying effect?
3. Do some adverbs have a stronger effect on
sentences with a negative polarity or on sen-
tences with a positive polarity?
4. Does the presence or absence of each adverb
affect the direction of the polarity of the sen-
tence?
3 The Experiment
We analyze whether human judgments show an ef-
fect on the sentiment ratings of sentences in the
presence or absence of selected adverbs, and how
strong the effect of each adverb is.
Let S1? be the sentence S1 from which an ad-
verb like absolutely is removed. S2? is de-
fined similarly. Three main observations can be
made: (1) the adverb appears in both positive and
negative sentiment-bearing sentences (S1 is nega-
tive and S2 is positive); (2) its removal from either
S1 or S2 does not change the overall polarity of the
sentence; (3) intuitively, S1 has a stronger negative
polarity value than S1? and S2 has a stronger posi-
tive polarity value than S2?. We conduct a prelim-
inary study of polarity intensifier words and show
that they all have characteristics (1) - (3). We ex-
amine data with eight different adverbs (Table 1).
3.1 Data
We extracted sentences containing the target ad-
verbs from a corpus of 50,000 movie reviews
(Maas et al., 2011). Each sentence is extracted
from a review that is labeled either ?positive? or
?negative? and correlated with a star rating. We
manually inspected the sentences and discarded
those where the target adverb was used in a modal
sense, as in Seriously, there was not one re-
spectable character in the entire script while re-
taining sentences like There is no doubt that Al-
fred Hitchcock was a seriously talented director.
For each adverb, we retained ten sentences from
positive and negative reviews each, for a total 160
sentences. We copied the original sentences, re-
moved the adverbs without making additional al-
terations. Our final dataset consisted of a total
of 320 sentences with 160 sentence pairs whose
members were identical except for the presence or
absence of the target adverbs. Below is an exam-
ple of a sentence pair, where the original sentence
with the adverbs was pre-classified by (Pang and
Lee, 2004) as carrying positive sentiment.
1. I was absolutely delighted by the simple story
and amazing animation.
2. I was delighted by the simple story and amaz-
ing animation.
3.2 Collecting Judgments via Crowdsourcing
We submitted single sentences (not pairs) to be
annotated with sentiment scores for crowdsourc-
ing, using Amazon Mechanical Turk (AMT). To
avoid any bias we shuffled the sentences and dis-
played them individually. We asked the Turkers
to select, for each sentence, one of five sentiment
scores: strong positive (2), positive (1), neutral (0),
negative (-1), strong negative (-2). Each sentence
was rated by five annotators. Altogether, twenty
annotators completed the task within eight hours.
Since the annotators did not all judge the same set
of sentences, we computed the agreement between
annotators as follows. For each annotator, his/her
agreement with the others is given be the follow-
ing formula:
1
|S(i)|
?
j?S(i)
ps
ji
,
where S(i) is the set of sentences annotated by
the i
th
Turker and ps
ji
is the percentage of Turkers
who have the same annotation with the i
th
Turker
for sentence j. |S(i)| is the cardinality of set S(i).
The agreement ranges from 0.52 to 0.8. Although
the annotation of some Turkers is close to that of
flipping a coin, all judgments were retained and
included in the results reported here.
3.3 Results
We report the main results. The polarity rating of
a sentence j is the (un-weighted) average rating
39
Adverbs Avg. Pol. Change Pol. Reversal
absolutely 0.2 0/20
awfully 0.6 2/20
enormously 0.2 1/20
extremely 0.2 2/20
horribly 0.2 0/20
incredibly 0.2 4/20
pretty 0.2 1/20
seriously 0.4 3/20
Table 2: Effects of adverbs on sentiment ratings.
of the five annotators for the sentence, denoted ?
j
and ?
j
=
?
i
ps
ji
. We use uniform weighting. A
sentence j is classified into one of the five polarity
categories according to the following criteria:
strong positve if ?
j
? [1.5, 2]
positive if ?
j
? (1.5, 0.5]
neutral if ?
j
? (0.5,?0.5)
negative if ?
j
? [?0.5,?1.5)
strong negative if ?
j
? [?0.5,?2]
3.3.1 Do Adverbs Change Sentiment Rating?
We first examine the polarity intensifying effects
of the eight adverbs and determine their relative
intensifying effects. For each adverb we compute
the average polarity rating change between the
members of the 20 sentence pairs with and with-
out the target adverb. The second column of Table
2 shows the average polarity rating change for the
adverbs. All adverbs have polarity intensifying ef-
fect, which ranges from 0.2 to 0.6. Awfully and
seriously have the strongest effect.
3.3.2 Change of Sentiment Rating in Positive
vs. Negative Contexts
Next we ask whether the adverbs have a stronger
polarity intensifying effect on sentences with a
negative, positive or neutral ratings. We partition
the 20 sentences with/without each adverb into
the three polarity categories according to their av-
erage polarity ratings. A sentence j is negative
(positive) if ?
j
? ?0.5 (?
j
? 0.5). Figure 1
shows the results. For six out of the eight adverbs,
the graph follows a V-shaped pattern, indicating
that the adverbs have stronger polarity influence
on sentences conveying opinionated, but not neu-
tral, statements. Pretty shows the weakest ef-
fect across, which makes intuitive sense, as this
Figure 1: The polarity intensifying effects of ad-
verbs over the sentiment categories.
adverb seems to have a ?softening/weakening? ef-
fect: consider ?pretty good,? which one could
judge to be slightly less good than ?good.? For ex-
ample, the sentence
He has a pretty strident rant about how impor-
tant it is.
received an average rating score of 0 with the
adverb present and -0.2 without it. The results
for awfully and extremely are surprising. A
closer look at the annotations revealed some pos-
sible unreliable ratings. For example, the sentence
The part of the movie set in Vietnam was
extremely inaccurate.
has average polarity score of 0 (i.e., neutral)
with the adverb and -0.8 without. Intuitively,
it seems that the first sentence conveys a strong
negative sentiment. Such data indicate the need
for further study. A more complex scheme for
computing the average polarity scores, such as
weighted by inter-annotator agreement, might pro-
duce better results.
3.3.3 Can Adverbs Reverse Sentiment
Orientation?
We ask whether their presence can have the effect
of reversing the polarity of a sentence. We again
consider three sentiment categories: positive, neg-
ative and neutral. The third column in Table 2
shows for each adverb, how many sentences out
of the total of 20 were judged to have a reversed
polarity when the adverb was removed. Overall,
the polarities of only 13 out of 160 sentences (i.e.,
about 8%) change.
3.3.4 Do Adverbs Have an Inherent
Sentiment Value?
Our target adverbs have inherent polarity as
claimed in some sentiment lexicons (see Table 1).
40
If the polarity of a sentence does not change when
the adverbs is present or absent, we conclude that
the adverb has no inherent polarity but may merely
affect the intensity of the constituents that it mod-
ifies. These results, as displayed in Figure 1 indi-
cate that our target adverbs do not carry inherent
polarity. Instead, they modify the intensity of the
sentiment connoted by the context.
4 Discussion
We examined the effect of eight intensifying ad-
verbs on the sentiment ratings of the sentences in
which they occur. Our study showed that, contrary
to their representation in some widely used senti-
ment lexicons, these adverbs do not carry an inher-
ent sentiment polarity, but merely alter the degree
of the polarity of the constituents they modify; cor-
rections of the corresponding entries in the senti-
ment resources seem warranted. Our results show
further that all adverbs strengthen the polarity of
the context to different degrees. If confirmed on a
larger data set, this indicates that the intensifying
force of different adverbs should be reflected in
lexical resources, perhaps along an ordered scale.
5 Related Work
Two recent surveys give a detailed account of the
SL acquisition techniques (Feldman, 2013; Liu,
2012). We give only an overview of the related
work here. SLs are acquired by one of three meth-
ods. Manual tagging is performed by human an-
notators: e.g., OF, and AL. Dictionary-based ac-
quisition relies on a set of seed words that is ex-
panded by using external resources, such as Word-
Net: e.g., (Dragut et al., 2010; Hassan and Radev,
2010; Mohammad et al., 2009; Dragut et al., 2012;
Takamura et al., 2005). In corpus-based acquisi-
tion a set of seed words is expanded by using a
large corpus of documents (Feng et al., 2013; Lu
et al., 2011; Yu et al., 2013; Wu and Wen, 2010).
To our knowledge, none of these works include
the polarity intensifiers that we introduce in this
paper.
References
S. Cerini, V. Compagnoni, A. Demontis, M. For-
mentelli, and G. Gandini, 2007. Language resources
and linguistic theory: Typology, second language
acquisition, English linguistics.
Eduard C. Dragut, Clement T. Yu, A. Prasad Sistla, and
Weiyi Meng. 2010. Construction of a sentimental
word dictionary. In CIKM, pages 1761?1764.
Eduard C. Dragut, Hong Wang, Clement Yu, Prasad
Sistla, and Weiyi Meng. 2012. Polarity consistency
checking for sentiment dictionaries. In ACL.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. In LREC.
Ronen Feldman. 2013. Techniques and applications
for sentiment analysis. CACM, 56(4):82?89, April.
C. Fellbaum. 1998. WordNet: An On-Line Lexical
Database and Some of its Applications. MIT Press.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In ACL.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In ACL.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Yue Lu, Malu Castellanos, Umeshwar Dayal, and
ChengXiang Zhai. 2011. Automatic construction of
a context-aware sentiment lexicon: an optimization
approach. In WWW, pages 347?356. ACM.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In ACL, pages 142?150.
Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009. Generating high-coverage semantic orienta-
tion lexicons from overtly marked words and a the-
saurus. In EMNLP.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL.
Jan Svartvik, editor. 1991. Directions in Corpus Lin-
guistics. Nobel Symposium 82, Mouton de Gruyter.
M. Taboada and J. Grieve. 2004. Analyzing appraisal
automatically. In AAAI Spring Symposium.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL, pages 133?140.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT/EMNLP.
Yunfang Wu and Miaomiao Wen. 2010. Disambiguat-
ing dynamic sentiment ambiguous adjectives. In
COLING, pages 1191?1199.
Hongliang Yu, Zhi-Hong Deng, and Shiyingxue Li.
2013. Identifying sentiment words using an
optimization-based model without seedwords. In
ACL.
41
