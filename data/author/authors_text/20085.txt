First Joint Conference on Lexical and Computational Semantics (*SEM), pages 643?647,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DERI&UPM: Pushing Corpus Based Relatedness to Similarity: Shared
Task System Description
Nitish Aggarwal? Kartik Asooja? Paul Buitelaar?
?Unit for Natural Language Processing
Digital Enterprise Research Institute
National University of Ireland, Galway, Ireland
firstname.lastname@deri.org
?Ontology Engineering Group
Universidad Politecnica de Madrid
Madrid, Spain
asooja@gmail.com
Abstract
In this paper, we describe our system submit-
ted for the semantic textual similarity (STS)
task at SemEval 2012. We implemented two
approaches to calculate the degree of simi-
larity between two sentences. First approach
combines corpus-based semantic relatedness
measure over the whole sentence with the
knowledge-based semantic similarity scores
obtained for the words falling under the same
syntactic roles in both the sentences. We fed
all these scores as features to machine learn-
ing models to obtain a single score giving the
degree of similarity of the sentences. Lin-
ear Regression and Bagging models were used
for this purpose. We used Explicit Semantic
Analysis (ESA) as the corpus-based seman-
tic relatedness measure. For the knowledge-
based semantic similarity between words, a
modified WordNet based Lin measure was
used. Second approach uses a bipartite based
method over the WordNet based Lin measure,
without any modification. This paper shows
a significant improvement in calculating the
semantic similarity between sentences by the
fusion of the knowledge-based similarity mea-
sure and the corpus-based relatedness measure
against corpus based measure taken alone.
1 Introduction
Similarity between sentences is a central concept
of text analysis, however previous studies about
semantic similarities have mainly focused either
on single word similarity or complete document
similarity. Sentence similarity can be defined by the
degree of semantic equivalence of two given sen-
tences, where sentences are typically 10-20 words
long. The role of sentence semantic similarity mea-
sures in text-related research is increasing due to
potential number of applications such as document
summarization, question answering, information
extraction & retrieval and machine translation.
One plausible limitation of existing methods for
sentence similarity is their adaptation from long text
(e.g. documents) similarity methods, where word
co-occurrence plays a significant role. However,
sentences are too short, thats why taking syntac-
tic role of each word with its narrow semantic
meaning into account, can be highly relevant to
reflect the semantic equivalence of two sentences.
These narrow semantics can be reflected from any
existing large lexicons [(Wu and Palmer, 1994)
and (Lin, 1998)]; nevertheless, these lexicons can
not provide the semantics of words which are out
of lexicon (e.g. guy) or multiword expressions.
These semantics can be represented by a large
distributed semantic space such as Wikipedia and
similarity can be reflected by relatedness of these
extracted semantics. However, relatedness covers
broader space than similarity, which forced us to
tune the Wikipedia based relatedness with lexical
structure (e.g. WordNet) based similarities driven
by linguistic syntactic structure, in reflecting more
sophisticated similarity of two given sentences.
In this work, we present a sentence similarity using
ESA and syntactic similarities. The rest of this
paper is organized as follows. Section 2 explores the
related work. Section 3 describes our approaches
643
in detail. Section 4 explains our three different
submitted runs for STS task. Section 5 shows the
results and finally we conclude in section 6.
2 Related Work
In recent years, there have been a variety of efforts
in improving semantic similarity measures, however
most of these approaches address this problem from
the viewpoint of large document similarity based on
word co-occurrence using string pattern or corpus
statistics. Corpus based approaches such as Latent
Semantic Analysis (LSA) [(Landauer et. al, 1998)
and (Foltz et. al, 1998)] and ESA (Gabrilovich and
Markovitch, 2007) use corpus statistics information
about all words and reflect their semantics in
distributional high semantic space. However, these
approaches perform quite well for long texts as they
use word co-occurrence and relying on the principle
that words which are used in the same contexts
tend to have related meanings. In case of short text
similarities, syntactic role of each word with its
meaning plays an important role.
There are several linguistic measures [( Achananu-
parp et. al, 2008) and (Islam and Inkpen, 2008)],
which can account for pseudo-syntactic information
by analyzing their word order using n-gram. To do
this, Islam and Inkpen defined a syntactic measure,
which considers the word order between two
strings by computing the maximal ordered word
overlapping. (Oliva et. al, 2011) present a similarity
measure for sentences and short text that takes
syntactic information, such as morphology and
parsing tree, into account and calculate similarities
between words with same syntactic role, by using
WordNet.
Our work takes inspiration from existing approaches
that exploit a combination of Wikipedia based re-
latedness with lexical structure based similarities
driven by linguistic syntactic structure.
3 Methodology
We implemented two approaches for the STS
task [(Agirre et. al, 2012)]. First approach is a
fusion of corpus-based semantic relatedness and
knowledge-based semantic similarity measures.
The core of this combination is the corpus-based
measure because the combination includes the
corpus-based semantic relatedness score over the
whole sentences and the knowledge-based semantic
similarity scores for the words falling under the
same syntactic roles in both the sentences. Machine
learning models are trained by taking all these
scores as different features. For the submission,
we used Linear regression and Bagging models.
Also, the equation obtained after training the linear
regression model shows more weightage to the score
obtained by the corpus-based relatedness measure
as this is the only score (feature), which reflects the
semantic relatedness/similarity score over the full
sentences, out of all the considered features for the
model. We used ESA as the corpus based semantic
relatedness measure and modified WordNet-based
Lin measure as the knowledge-based similarity.
The WordNet-based Lin relatedness measure was
modified to reflect better the similarity between
the words. For the knowledge-based similarity,
currently we considered only the words lying in the
three major syntactic role categories i.e. subjects,
actions and the objects. We see the first approach
as the corpus-based measure ESA tuned with the
knowledge-based measure. Thus, it is referred as
TunedESA later in the paper.
Our second approach is based on the bipartite
method over the WordNet based semantic relat-
edness measures. WordNet-based Lin measure
(without any modification) was used for calcu-
lating the relatedness scores for all the possible
corresponding pair of words appearing in both the
sentences. Then, the similarity/relatedness score
for the sentences is calculated by perceiving the
problem as the computation of a maximum total
matching weight of a bipartite graph having the
words as nodes and the relatedness scores as the
weight of the edges between the nodes. To solve
this, we used Hungarian method. Later, we refer
this method as WordNet-Bipartite.
3.1 TunedESA
In this approach, the ESA based relatedness score
for the full sentences is combined with the modified
WordNet-based Lin similarity scores calculated for
the words falling under the corresponding syntactic
role category in both the sentences.
644
ALL Rank-ALL ALLnrm RankNrm Mean RankMean
Baseline 0.3110 87 0.6732 85 0.4356 70
Run1 0.5777 52 0.8158 20 0.5466 52
Run2 0.5833 51 0.8183 17 0.5683 42
Run3 0.4911 67 0.7696 57 0.5377 53
Table 1: Overall Rank and Pearson Correlation of all runs
MSRpar MSRvid SMTeuro OnWN SMTnews
Baseline 0.4334 0.2996 0.4542 0.5864 0.3908
ESA? 0.2778 0.8178 0.3914 0.6541 0.4366
Run1 0.3675 0.8427 0.3534 0.6030 0.4430
Run2 0.3720 0.8330 0.4238 0.6513 0.4489
Run3 0.5320 0.6874 0.4514 0.5827 0.2818
Table 2: Pearson Correlation of all runs with all five STS test datasets
TunedESA could be summarized as these four
basic steps:
? Calculate the ESA relatedness score between
the sentences.
? Find the words corresponding to the linguistic
syntactical categories like subject, action and
object of both the sentences.
? Calculate the semantic similarity between the
words falling in the corresponding subjects, ac-
tions and objects in both the sentences using
modified WordNet-based measure Lin.
? Combine these four scores for ESA, Subject,
Action and Object to get the final similarity
score on the basis of an already learned ma-
chine learning model with the training data.
ESA is a promising technique to find the relatedness
between documents. The texts which need to be
compared are represented as high dimensional vec-
tors containing the TF-IDF weight between the term
and the Wikipedia article. The semantic relatedness
measure is calculated by taking the cosine measure
between these vectors. In this implementation of
ESA 1, the score was calculated by considering the
1ESA? considering full sentence at a time to make the vector
i.e. different from standard ESA
full sentence at a time for making the Wikipedia
article vector while in the standard ESA, vectors
are made for each word of the text followed by the
addition of all these vectors to represent the final
vector for the text/sentence. It was done just to
reduce the time complexity.
To calculate the lexical similarity between the
words, we implemented WordNet-based semantic
relatedness measure Lin. This score was modified to
reflect a better similarity between the words. In the
current system, basic linguistic syntactic categories
i.e. subjects, actions and objects were used. For
instance, below is a sentences pair from the training
MSRvid dataset with the gold standard score and
the syntactic roles.
Sentence 1: A man is playing a guitar.
Subject: Man, Action: play, Object: guitar
Sentence 2: A man is playing a flute.
Subject: Man, Action: play, Object: flute
Gold Standard Score (0-5): 2.2
As the modification, the scores given by Lin
measure were used only for the cases where sub-
sumption relation or hypernymy/hyponymy exists
645
between the words. This modification was done
only for the words falling under the category of
subjects and objects.
3.2 WordNet Bipartite
WordNet-based semantic relatedness measure was
used for the second approach.
Following steps are performed :
? Each sentence is tokenized to obtain the words.
? Semantic relatedness between every possible
pair of words in both the sentences is calculated
using WordNet-based measure e.g. Lin.
? Using the scores obtained in the second step,
the semantic similarity/relatedness between the
sentences is calculated by transforming the
problem as that of computing the maximum to-
tal matching weight of a bipartite graph, which
can be done by using Hungarian method.
4 System Description
We submitted three runs in the semantic textual
similarity task. The first two runs are based on the
first approach i.e. TunedESA and they differ only in
the machine learning algorithm used for obtaining
the final similarity score based on all the considered
scores/features.
ESA was implemented on the current Wikipedia
dump. WordNet based relatedness measure Lin
was modified to give a better semantic similarity
degree. Stanford Core-NLP library was used for
obtaining the words with their syntactic roles.
All the required scores/feature i.e. ESA based
relatedness for the complete sentences and mod-
ified WordNet-based Lin similarity scores were
calculated for the corresponding words lying in
the same syntactic categories. Bagging and Linear
Regression models were built using the training data
for the first and second runs respectively. Based on
the category of the test dataset, model was trained
on the corresponding training dataset.
For the surprise test datasets, we trained our
model with the training dataset of the MSRvid data
based on the fact that we obtained good results with
this category. Then the built models were used for
calculating the similarity scores for the test data.
For the third run, WordNet Bipartite method
was used to calculate the similarity scores. It didn?t
require any training.
5 Results and Discussion
All above described runs are evaluated on STS
test dataset. Table 1 shows the overall results2 of
our three runs against the baseline system which
follows the bag of words approach. Table 2 shows
the Pearson correlation on different test datasets for
all the three runs. It provides a comparison between
corpus based relatedness measure ESA and our
system TunedESA (Run 1 & Run 2).
The results show significant improvement against
ESA. Although, it can be seen that the baseline
results are even better than of the ESA in the cases
of MSRpar and SMTeuro. It may be because this
implementation of ESA is not the standard one.
6 Conclusion
We presented a method to calculate the degree of
sentence similarity based on tuning the corpus based
relatedness measure with the knowledge-based sim-
ilarity measure over the syntactic roles. The results
show a definite improvement by the fusion. As
future work, we plan to improve the syntactic role
handling and considering more syntactical cate-
gories. Also, experimentation3 with standard ESA
and other semantic similarity/relatedness measures
needs to be performed.
Acknowledgments
This work is supported in part by the European
Union under Grant No. 248458 for the Monnet
project as well as by the Science Foundation Ireland
under Grant No. SFI/08/CE/I1380 (Lion-2).
2results can also be found at http://www.cs.york.
ac.uk/semeval-2012/task6/index.php?id=
results-update with the name nitish aggarwal
3We plan to provide the further results and information at
http://www.itssimilar.com/
646
References
Achananuparp Palakorn and Xiaohua Hu and Xiajiong
Shen 2008 The Evaluation of Sentence Similarity
Measures, In: DaWaK. pp. 305-316
Agirre Eneko , Cer Daniel, Diab Mona and Gonzalez-
Agirre Aitor 2012 SemEval-2012 Task 6: A Pilot on
Semantic Textual Similarity. In: Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Foltz P. W., Kintsch W. and Landauer T. K. 1998. In:
journal of the Discourse Processes. pp. 285-307, The
measurement of textual Coherence with Latent Se-
mantic Analysis,
Gabrilovich Evgeniy and Markovitch Shaul 2007 Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis, In: Proceedings of The
Twentieth International Joint Conference for Artificial
Intelligence. pp. 1606?1611,
Islam, Aminul and Inkpen, Diana 2008 Semantic
text similarity using corpus-based word similarity and
string similarity, In: journal of ACM Trans. Knowl.
Discov. Data. pp. 10:1?10:25
Landauer Thomas K. ,Foltz Peter W. and Laham Darrell
1998. An Introduction to Latent Semantic Analysis,
In: Journal of the Discourse Processes. pp. 259-284,
Lin Dekang 1998 Proceeding of the 15th International
Conference on Machine Learning. pp. 296?304 An
information-theoretic definition of similarity
Oliva, Jesu?s and Serrano, Jose? Ignacio and del Castillo,
Mar??a Dolores and Iglesias, A?ngel April, 2011
SyMSS: A syntax-based measure for short-text seman-
tic similarity In: journal of Data Knowledge Engineer-
ing. pp. 390?405
Wu, Zhibiao and Palmer, Martha 1994 Verbs seman-
tics and lexical selection, In: Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics,
647
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 51?56,
Dublin, Ireland, August 23-24 2014.
Exploring ESA to Improve Word Relatedness
Nitish Aggarwal Kartik Asooja Paul Buitelaar
Insight Centre for Data Analytics
National University of Ireland
Galway, Ireland
firstname.lastname@deri.org
Abstract
Explicit Semantic Analysis (ESA) is an ap-
proach to calculate the semantic relatedness
between two words or natural language texts
with the help of concepts grounded in human
cognition. ESA usage has received much at-
tention in the field of natural language pro-
cessing, information retrieval and text analy-
sis, however, performance of the approach de-
pends on several parameters that are included
in the model, and also on the text data type
used for evaluation. In this paper, we investi-
gate the behavior of using different number of
Wikipedia articles in building ESA model, for
calculating the semantic relatedness for differ-
ent types of text pairs: word-word, phrase-
phrase and document-document. With our
findings, we further propose an approach to
improve the ESA semantic relatedness scores
for words by enriching the words with their
explicit context such as synonyms, glosses and
Wikipedia definitions.
1 Introduction
Explicit Semantic Analysis (ESA) is a distributional
semantic model (Harris, 1954) that computes the
relatedness scores between natural language texts
by using high dimensional vectors. ESA builds
the high dimensional vectors by using the explicit
concepts defined in human cognition. Gabrilovich
and Markovitch (2007) introduced the ESA model
in which Wikipedia and Open Directory Project
1
was used to obtain the explicit concepts. ESA con-
siders every Wikipedia article as a unique explicit
1
http://www.dmoz.org
topic. It also assumes that the articles are topically
orthogonal. However, recent work (Gottron et
al., 2011) has shown that by using the documents
from Reuters corpus instead of Wikipedia articles
can also achieve comparable results. ESA model
includes various parameters (Sorg and Cimiano,
2010) that play important roles on its performance.
Therefore, the model requires further investigation
in order to better tune the parameters.
ESA model has been adapted very quickly in
different fields related to text analysis, due to the
simplicity of its implementation and the availability
of Wikipedia corpus. Gabrilovich and Markovitch
(2007) evaluated the ESA against word relatedness
dataset WN353 (Finkelstein et al., 2001) and doc-
ument relatedness dataset Lee50 (Lee et al., 2005)
by using all the articles from Wikipedia snapshot of
11 Nov, 2005. However, the results reported using
different implementations (Polajnar et al., 2013)
(Hassan and Mihalcea, 2011) of ESA on same
datasets (WN353 and Lee50) vary a lot, due the
specificity of ESA implementation. For instance,
Hassan and Mihalcea (2011) found a significant
difference between the scores obtained from their
own implementation and the scores reported in the
original article (Gabrilovich and Markovitch, 2007).
In this paper, first, we investigate the behavior
of ESA model in calculating the semantic related-
ness for different types of text pairs: word-word,
phrase-phrase and document-document by using
different number of Wikipedia articles for building
the model. Second, we propose an approach
51
for context enrichment of words to improve the
performance of ESA on word relatedness task.
2 Background
The ESA model can be described as a method of
obtaining the relatedness score between two texts by
quantifying the distance between two high dimen-
sional vectors. Every explicit concept represents a
dimension of the ESA vector, and the associativity
weight of a given word with the explicit concept
can be taken as magnitude of the corresponding
dimension. For instance, there is a word t, ESA
builds a vector v, where v =
?
N
i=0
a
i
? c
i
and c
i
is
i
th
concept from the explicit concept space, and a
i
is the associativity weight of word t with the concept
c
i
. Here, N represents the total number of concepts.
In our implementation, we build ESA model by
using Wikipedia articles as explicit concepts, and
take the TFIDF weights as associativity strength.
Similarly, ESA builds the vector for natural lan-
guage text by considering it as a bag of words. Let
T = {t
1
, t
2
, t
3
...t
n
}, where T is a natural language
text that has n words. ESA generates the vector
V, where V =
?
t
k
T
v
k
and v =
?
N
i=0
a
i
? c
i
. v
k
represents the ESA vector of a individual words as
explained above. The relatedness score between two
natural language texts is calculated by computing
cosine product of their corresponding ESA vectors.
In recent years, some extensions (Polajnar et
al., 2013) (Hassan and Mihalcea, 2011) (Scholl et
al., 2010) have been proposed to improve the ESA
performance, however, they have not discussed the
consistency in the performance of ESA. Polajnar
et al. (2013) used only 10,000 Wikipedia articles
as the concept space, and got significantly different
results on the previously evaluated datasets. Hassan
and Mihalcea (2011) have not discussed the ESA
implementation in detail but obtained significantly
different scores. Although, these proposed exten-
sions got different baseline ESA scores but they
improve the relatedness scores with their additions.
Polajnar et al. (2013) used the concept-concept
correlation to improve the ESA model. Hassan and
Mihalcea (2011) proposed a model similar to ESA,
which builds the high dimensional vector of salient
concepts rather than explicit concepts. Gortton et
al. (2011) investigated the ESA performance for
document relatedness and showed that ESA scores
are not tightly dependent on the explicit concept
spaces.
Minimum unique Total number of
words (K) articles (N)
100 438379
300 110900
500 46035
700 23608
900 13718
1100 8322
1300 5241
1500 3329
1700 2126
1900 1368
Table 1: The total number of retrieved articles for differ-
ent values of K
3 Investigation of ESA model
Although Gortton et al. (2011) has shown that ESA
performance on document pairs does not get af-
fected by using different number of Wikipedia ar-
ticles, we further examine it for word-word and
phrase-phrase pairs. We use three different datasets
WN353, SemEvalOnWN (Agirre et al., 2012) and
Lee50. WN353 contains 353 word pairs, SemEval-
OnWN consists of 750 short phrase/sentence pairs,
and Lee50 is a collection of 50 document pairs.
All these datasets contain relatedness scores given
by human annotators. We evaluate ESA model
on these three datasets against different number of
Wikipedia articles. In order to select different num-
ber of Wikipedia articles, we sort them according to
the total number of unique words appearing in each
article. We select N articles, where N is total num-
ber of articles which have at least K unique words.
Table 1 shows the total number of retrieved articles
for different values of K. We build 20 different ESA
models with the different values of N retrieved by
varying K from 100 to 2000 with an interval of 100.
Figure 1 illustrates Spearman?s rank correlation of
all the three types of text pairs on Y-axis while X-
axis shows the different values of N which are taken
to build the model. It shows that ESA model gener-
ates very consistent results for phrase pairs similar
to the one reported in (Aggarwal et al., 2012), how-
52
Figure 1: ESA performance on different types of text
pairs by varying the total number of articles
ever, the correlation scores decreases monotonously
in the case of word pairs as the number of articles
goes down. In the case of document pairs, ESA pro-
duces similar results until the value of N is chosen
according to K = 1000, but after that, it decreases
quickly because the number of articles becomes too
low for making a good enough ESA model. All this
indicates that word-word relatedness scores have a
strong impact of changing the N in comparison of
document-document or phrase-phrase text pairs. An
explanation to this is that the size of the ESA vec-
tor for a word solely depends upon the popularity
of the given word, however, in the case of text, the
vector size depends on the popularity summation of
all the words appearing in the given text. It suggests
that the word relatedness problem can be reduced to
short text relatedness by adding some related con-
text with the given word. Therefore, to improve
the ESA performance for word relatedness, we pro-
pose an approach for context enrichment of words.
We perform context enrichment by concatenating re-
lated context with the given word and use this con-
text to build the ESA vector, which transforms the
word relatedness problem to phrase relatedness.
4 Context Enrichment
Context enrichment is performed by concatenating
the context defining text to the given word before
building the ESA vector. Therefore, instead of build-
ing the ESA vector of a word, the vector is built for
the short text that is obtained after concatenating the
related context. This is similar to classical query ex-
pansion task (Aggarwal and Buitelaar, 2012; Pan-
tel and Fuxman, 2011), where related concepts are
concatenated with a query to improve the informa-
tion retrieval performance. We propose three differ-
ent methods to obtain related context: 1) WordNet-
based Context Enrichment 2) Wikipedia-based Con-
text Enrichment, and 3) WikiDefinition-based Con-
text Enrichment.
4.1 WordNet-based Context Enrichment
WordNet-based context enrichment uses the Word-
Net synonyms to obtain the context, and concate-
nates them into the given word to build the ESA vec-
tor. However, WordNet may contain more than one
synset for a word, where each synset represents a
different semantic sense. Therefore, we obtain more
than one contexts for a given word, by concatenat-
ing the different synsets. Further, we calculate ESA
score of every context of a given word against all the
contexts of the other word which is being compared,
and consider the highest score as the final related-
ness score. For instance, there is a given word pair
?train and car?, car has 8 different synsets that build
8 different contexts, and train has 6 different synsets
that build 6 different contexts. We calculate the ESA
score of these 8 contexts of car to the 6 contexts of
train, and finally select the highest obtained score
from all of the 24 calculated scores.
4.2 Wikipedia-based Context Enrichment
In this method, the context is defined by the word
usage in Wikipedia articles. We retrieve top 5
Wikipedia articles by querying the articles? content,
and concatenate the short abstracts of the retrieved
articles to the given word to build the ESA vector.
Short abstract is the first two sentences of Wikipedia
article and has a maximum limit of 500 characters.
In order to retrieve the top 5 articles from Wikipedia
for a given word, we build an index of all Wikipedia
articles and use TF-IDF scores. We further explain
53
our implementation in Section 5.1.
4.3 WikiDefinition-based Context Enrichment
This method uses the definition of a given word from
Wikipedia. To obtain a definition from Wikipedia,
we first try to find a Wikipedia article on the given
word by matching the Wikipedia title. As definition,
we take the short abstract of the Wikipedia article.
For instance, for a given word ?train?, we take the
Wikipedia article with the title ?Train?
2
. If there is
no such Wikipedia article, then we use the previous
method ?Wikipedia-based Context Enrichment? to
get the context defining text for the given word. In
contrary to the previous method for defining context,
here we first try to get a more precise context as it
comes from the Wikipedia article on that word only.
After obtaining the definition, we concatenate it to
the given word to build the ESA vector. At the time
of experimentation, we were able to find 339 words
appearing as Wikipedia articles out of 437 unique
words in the WN353 dataset.
Figure 2: Effect of different types of context enrichments
on WN353 gold standard
2
http://en.wikipedia.org/wiki/Train
5 Experiment
5.1 ESA implementation
In this section, we describe the implementation of
ESA and the parameters used to build the model.
We build an index over all Wikipedia articles from
the pre-processed Wikipedia dump from November
11, 2005 (Gabrilovich, 2006). We use Lucene
3
to
build the index and retrieve the articles using TF-
IDF scores. As described in section 3, we build 20
different indices with different values of total num-
ber of articles (N).
5.2 Results and Discussion
To evaluate the effect of the aforementioned
approaches for context enrichment, we compare
the results obtained by them against the results
generated by ESA model as a baseline. We cal-
culated the scores on WN353 word pairs dataset
by using ESA, WordNet-based Context Enrich-
ment (ESA CEWN), Wikipedia-based Context
Enrichment (ESA CEWiki) and WikiDefition-based
Context Enrichment (ESA CEWikiDef). Further,
we examine the performance of context enrichment
approaches by reducing the total number of articles
taken to build the model. Figure 2 shows that the
proposed methods of context enrichment signifi-
cantly improve over the ESA scores for different
values of N.
Table 2 reports the results obtained by using
different context enrichments and ESA model.
It shows Spearman?s rank correlation on four
different values of N. All the proposed con-
text enrichment methods improve over the ESA
baseline scores. Context enrichments based on
Wikipedia outperforms the other methods, and
ESA CEWikiDef significantly improves over the
ESA baseline. Moreover, given a very less number
of Wikipedia articles used for building the model,
ESA CEWikiDef obtains a correlation score which
is considerably higher than the one obtained by
ESA baseline. ESA CEWN and ESA CEWiki can
include some unrelated context as they do not care
about the semantic sense of the given word, for
instance, for a given word ?car?, ESA CEWiki
3
https://lucene.apache.org/
54
K Total articles (N) ESA ESA CEWN ESA CEWiki ESA CEWikiDef
100 438,379 0.711 0.692 0.724 0.741
200 221,572 0.721 0.707 0.726 0.743
500 46,035 0.673 0.670 0.679 0.698
1000 10,647 0.563 0.593 0.598 0.614
Table 2: Spearman rank correlation scores on WN353 gold standard
includes the context about the word ?car? at surface
level rather than at the semantic level. However,
ESA CEWikiDef only includes the definition if it
does not refer to more than one semantic sense,
therefore, ESA CEWikiDef outperforms all other
types of context enrichment.
We achieved best results in all the cases by tak-
ing all the articles which has a minimum of 200
unique words (K=200). This indicates that further
increasing the value of K considerably decreases
the value of N, consequently, it harms the overall
distributional knowledge of the language, which is
the core of ESA model. However, decreasing the
value of K introduces very small Wikipedia articles
or stubs, which do not provide enough content on a
subject.
6 Conclusion
In this paper, we investigated the ESA performance
for three different types of text pairs: word-word,
phrase-phrase and document-document. We showed
that ESA scores varies significantly for word re-
latedness measure with the change in the number
(N) and length (?K which is the number of unique
words) of the Wikipedia articles used for building
the model. Further, we proposed context enrichment
approaches for improving word relatedness compu-
tation by ESA. To this end, we presented three dif-
ferent approaches: 1) WordNet-based, 2) Wikipedia-
based, and 3) WikiDefinition-based, and we real-
ized that concatenating the context defining text im-
proves the ESA performance for word relatedness
task.
Acknowledgments
This work has been funded in part by a research
grant from Science Foundation Ireland (SFI) under
Grant Number SFI/12/RC/2289 (INSIGHT) and by
the EU FP7 program in the context of the project
LIDER (610782).
References
Nitish Aggarwal and Paul Buitelaar. 2012. Query expan-
sion using wikipedia and dbpedia. In CLEF.
Nitish Aggarwal, Kartik Asooja, and Paul Buitelaar.
2012. DERI&UPM: Pushing corpus based relatedness
to similarity: Shared task system description. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceedings of
the Main Conference and the Shared Task, and Volume
2: Proceedings of the Sixth International Workshop on
Semantic Evaluation, SemEval ?12, pages 643?647,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385?393. Association for Computational
Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of the 10th international
conference on World Wide Web, pages 406?414. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich. 2006. Feature generation for
textual information retrieval using world knowledge.
Ph.D. thesis, Technion - Israel Institute of Technology,
Haifa, Israel, December.
Thomas Gottron, Maik Anderka, and Benno Stein. 2011.
Insights into explicit semantic analysis. In Proceed-
ings of the 20th ACM international conference on In-
formation and knowledge management, pages 1961?
1964. ACM.
Zellig Harris. 1954. Distributional structure. In Word 10
(23), pages 146?162.
55
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Michael David Lee, BM Pincombe, and Matthew Brian
Welsh. 2005. An empirical evaluation of models of
text document similarity. Cognitive Science.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures:
Associating web queries with structured entities. In
ACL, pages 83?92.
Tamara Polajnar, Nitish Aggarwal, Kartik Asooja, and
Paul Buitelaar. 2013. Improving esa with docu-
ment similarity. In Advances in Information Retrieval,
pages 582?593. Springer.
Philipp Scholl, Doreen B?ohnstedt, Renato Dom??nguez
Garc??a, Christoph Rensing, and Ralf Steinmetz. 2010.
Extended explicit semantic analysis for calculating
semantic relatedness of web resources. In Sustain-
ing TEL: From Innovation to Learning and Practice,
pages 324?339. Springer.
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natural
Language Processing and Information Systems, pages
36?48. Springer.
56
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 43?47,
Dublin, Ireland, August 23 2014.
Using Distributional Semantics to Trace Influence and Imitation in
Romantic Orientalist Poetry
Nitish Aggarwal
Insight Centre for Data Analytics
National University of Ireland
Galway, Ireland
nitish.aggarwal@deri.org
Justin Tonra
School of Humanities
National University of Ireland
Galway, Ireland
justin.tonra@nuigalway.ie
Paul Buitelaar
Insight Centre for Data Analytics
National University of Ireland
Galway, Ireland
paul.buitelaar@deri.org
Abstract
In this paper, we investigate whether textual analysis can yield evidence of shared vocabulary or
formal textual characteristics in the works of 19th century poets Lord Byron and Thomas Moore
in the genre of Romantic Orientalism. In particular, we identify and trace Byron?s influence on
Moore?s writings to query whether Moore imitated Byron, as many reviewers of the time sug-
gested. We use a Distributional Semantic Model (DSM) to analyze if there is a shared vocabulary
of Romantic Orientalism, or if it is possible to characterize a literary genre in terms of vocabu-
lary, rather than in terms of the particular plots, characters and themes. We discuss the results
that DSM models are able to provide for an abstract overview of the influence of Lord Byron?s
work on Thomas Moore.
1 Introduction
Literary criticism has often marshalled the serendipitous discovery in the service of constructing an ar-
gument or a critical judgment. Such serendipity can take material or cognitive form, and provide the
raw materials for analysis and conjecture. In literary criticism, arguments are often based upon evidence
gleaned from close reading of a text in support of a hypothesis, but quantitative methods have shown how
literary texts can yield evidence that is not immediately discernible to the human eye for a similar inter-
pretive purposes. In literary studies, computers have assisted in the collection of such data with varying
degrees of complexity and sophistication for about half a century. How can we use the information from
such computing processes for creating new knowledge, or, in literary-critical terms, for articulating the
meaning in a text? To what degree can literary criticism and computing enrich one another? Is algorith-
mic criticism derived from algorithmic manipulation of text (Ramsay, 2011) possible?
Inspired by general questions such as these, this paper discusses a particular project that uses Distribu-
tional Semantics to trace influence and imitation between two particular poets writing in the genre of
Romantic Orientalism. Our intuition is that if text analysis can yield evidence of shared vocabulary to
trace influence between poets, we can build a network of different authors with their degree of influences.
This can help a reader in finding a similar literature and in discovering implicit information.
In the period from 1813 to 1817, friends and fellow-poets Lord Byron and Thomas Moore wrote a series
of long poems which are now seen as representative of Romantic Orientalism (a subset of Romantic
literature recognisable by its Oriental and Middle-Eastern themes and settings). Throughout this period,
an unusual pattern of coincidence is evident in the writings of the two poets, with correspondence be-
tween the poets describing similar plots, settings, and characters names in their respective works. The
publication of Byron?s quartet of Oriental tales in 1813 and 1814 (The Giaour, The Bride of Abydos, The
Corsair, Lara) anticipated much of the substance of Moore?s work, and delayed the publication of his
own suite of four Oriental poems, Lalla Rookh, until 1817. On the publication of the latter, many re-
viewers accused Moore of imitating Byron?s work, correctly fulfilling Moore?s own prediction of 1813,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
43
that he would be seen as ?an humble follower?a Byronian? (Moore, 1964). Subsequent critics have
generally acknowledged Byron as a direct influence on Moore, but the basis of these acknowledgements
is usually subjective critical interpretation of plot, character, and poetic form in the published texts (see,
in particular, (Vail, 2001), (Sultana, 1989), (Gregory, 2008)). More general accounts of Byron?s and
Moore?s literary association can be found in (Hamilton, 1948), (Jordan, 1948), (Tessier, 2014).
The purpose of this project is to investigate further the possible causes for the unusual pattern of co-
incidence in the writings of these two poets during this time. A Computational Linguistics approach to
Byron?s and Moore?s Orientalism was identified as a potentially productive way of studying coincidence,
influence, and imitation between their writings and how they related to the genre of Romantic Oriental-
ism. Fresh empirical insight into this topic is desirable because of the difficulty of thinking about and
articulating these issues in a way that is not speculative or nebulous. Such methodologies have not been
applied to these texts, and offer the possibility of yielding fresh perspectives on questions about the texts,
and the genre of Romantic Orientalism: is it defined by a limited vocabulary which inevitably leads to
similarities and coincidences between its practitioners? Does writing within a specific genre impose top-
ical or semantic constraint upon the author?
The motivations for the project emerge from a conviction that Computational Linguistics techniques may
reveal evidence of shared vocabulary or formal textual characteristics in the works of Byron and Moore
during the period 1813-17. The questions that the project seeks to answer include: can we identify and
trace Byron?s influence on Moore?s writings? Did Moore imitate Byron, as many reviewers of the time
suggested? Is there a shared vocabulary of Romantic Orientalism? Is it possible to characterise a literary
genre in these terms, rather than in terms of plot, character, theme, etc.? The basis for such enquiries
must go beyond a subjective comparison of the poems: the method that has characterised literary-critical
approaches to these texts to date.
2 Distributional semantics
How related are love and emotion? Reasoning about semantic relatedness of natural language text is
not a very difficult task for a human because of sufficient background knowledge and other related in-
formation to understand the semantics of natural language text. However, for computers, it is still an
open issue to provide significant background knowledge to understand the complex structure of natural
language. One plausible way to provide such background knowledge is taking the usage of given text in
large contextual space into account.
Semantic relatedness of two given terms (text fragments, phrases or words) can be obtained by calculat-
ing the correlation between two high dimensional vectors of a Distributional Semantic Model (DSM),
which is based on the assumption that semantic meaning of a text can be inferred from its usage in context
(Harris, 1954), i.e. its distribution in text. DSM builds this semantic representation through a statistical
analysis over the large contextual information in which a term occurs (see for details (Landauer, 1998),
(Blei, 2003)). One recent popular model to calculate this semantic relatedness by using the distributional
semantics is Explicit Semantic Analysis (ESA) proposed by (Gabrilovich and Markovitch, 2007), which
attempts to represent the semantics of the given term by a high dimensional vector in explicit concept
space such as Wikipedia concepts. Every explicit concept represents a dimension of the ESA vector, and
associativity weight of a given term with the explicit concept reflects the vector dimension weight. For
instance, for a given term t, ESA builds a vector v, where v =
?
N
i=0
a
i
? c
i
and c
i
is i
th
concept from the
explicit concept space, and a
i
is the associativity weight of term t with the concept c
i
. Here, N represents
the total number of concepts. The semantic relatedness score is calculated by taking cosine between the
corresponding high dimensional vectors.
3 Approach
Section 1 described our aim to investigate whether textual analysis techniques can yield evidence about
Byron?s influence on Moore?s writings by analyzing the four long poems (published in 1813-14) by
Lord Byron and a collection of four long poems (published in 1817) by Thomas Moore. To analyze this
influence, we calculate semantic relatedness scores between Byron?s poems and Moore?s poems. We split
44
these poems in line-groups
1
and obtain 227 line-groups from Byron?s poems and 246 line-groups from
Moore?s poems. We calculate ESA scores of every line-group of Byron?s poems with every line-group
of Moore?s poems. All the line-group pairs can be sorted according to their relatedness scores, which
can provide highly related line-group pairs. After getting these highly related pairs, we can manually
analyze them, and if manual analysis confirms the high relatedness of the pairs provided by ESA, then it
may indicate some degree of influence or imitation between the poets. Also, these results will conclude
that text analysis techniques can reduce the human effort in analyzing the influence between work by
different authors.
4 Evaluation
4.1 Experiment
We built two ESA models; one by using Wikipedia and the other by using a corpus of poetry primarily
from the eighteenth and nineteenth centuries
2
. In the first model, we take every Wikipedia article as a
dimension of the ESA vector, and TF-IDF weight of a given text with article content is considered as the
associativity strength with the corresponding dimension. We use modified ESA (Aggarwal, 2012) which
builds the ESA vector by taking all words of a given text together rather than taking them individually.
Wikipedia may not have a good coverage of the vocabulary of poems in Romantic Orientalism, which
led us to try another ESA model that utilizes a Poetic Corpus. This corpus consists of 892 long poems
and some of the poems contain more than 7K lines. Therefore, we split each poem with their line-groups
and obtain 22K different line-groups. Similar to Wikipedia-based ESA, we take every line-group as a
dimension of the ESA vector, and TF-IDF weight of the given text with line-group is considered as as-
sociativity strength with the corresponding dimension.
We use both ESA models: Wikipedia-based ESA and Poetic Corpus-based ESA to calculate the semantic
relatedness scores of every line-group of Byron?s poems with every line-group of Moore?s poems. Both
the results obtained by these two models are analyzed manually to check if Poetic Corpus-based ESA
outperforms Wikipedia-based ESA as Poetic Corpus has better vocabulary coverage for Romantic Ori-
entalism poems. We described in section 3 that we obtain 227 line-groups from Byron?s poems and 246
line-groups from Moore?s poems that means 56K line-group pairs. Manual analysis of 56K line-group
pairs will take a very long time, therefore, we analyze only a small subset of the 56K pairs. To select the
sample, we categorize the line-group pairs in three different categories: Highly-Related, May-be-Related
and Not-Related. In the ranked list of line-group pairs, the top 1K are considered Highly-Related, the
pairs ranked between 25K to 26K are considered May-be-Related, and the bottom 1K are taken as Not-
Related. We randomly selected 5 line-group pairs from each category and manually analyzed the results
obtained from ESA. Hence, we analyzed 15 pairs obtained according to Wikipedia-based ESA and 15
pairs according to PoeticCorpus-based ESA.
4.2 Results and Discussion
Manual (close-reading) analysis of 15 line-group pairs from the Wikipedia-based ESA took place first.
At first glance, the pairs identified as Highly-Related were indeed quite closely related, particularly in
terms of their narrative content. While some individual line-groups appeared in more than one pair-
ing identified by the model, the pair exhibited a frequently occurring narrative scenario where a female
character addressed her male lover before the departure or death of one of the parties. The model also
succeeded in identifying this scenario in poems by both Byron and Moore. The scenes are unsurprisingly
united by the presence of strong emotional language and imagery on the theme of love. However, the
recognition of a leavetaking (whether in death or departure) in the scenes is also noteworthy, as is the fact
that the identified line-groups are comprised of direct quotations from characters (as opposed to poetic
narrative).
1
Line-groups in poetry are similar to paragraphs in prose. On the printed page, a line of white space separates one line group
from the next. Like paragraphs, they vary in length, and are often semantically, syntactically, or thematically self-contained.
2
The poems in this corpus come from Women Writers Project (1560-1845), Eighteenth-Century Collections Online (1701-
1800), and poetic corpora shared by Ted Underwood (1701-1899)
45
Subjected to manual analysis, pairs of line-groups in the May-be-Related and Not-Related categories
exhibit varying degrees of relatedness. Most are lacking the immediate recognition of narrative similar-
ity evident in the Highly-Related pairs, with some pairs containing vastly different narrative scenarios.
Many of the consistencies from the Highly-Related category are also absent: some pairs vary greatly in
length, and some contain a mix of narrative and quotation. One example from the manually-analysed
examples proved to be a potential anomaly: a pair determined by the model to be Not-Related (i.e. in the
bottom 1K of pairs in terms of relatedness) might easily be considered related in that both line-groups
are florid poetic descriptions of a pastoral landscape.
The results of the Poetic Corpus-based ESA model were similar, if a little more refined. Interestingly,
the line-group pairs in the Highly-Related category were largely similar to those resulting from the
Wikipedia-based ESA. They were comprised of direct quotation (rather than narrative), and featured
a character speaking to their lover in strong emotional language. In some cases (though not consistently)
greater linguistic similarities between the pairings were more evident than in the results of the Wikipedia-
based ESA. This was an anticipated consequence of using the Poetic Corpus-based ESA, where the model
would be more likely to recognise the more unconventional features of nineteenth-century poetic diction
than the Wikipedia-based ESA.
From a literary-critical perspective, however, identification of the Highly-Related pairs by a computer is
no great advance on the capabilities of human scholarship. A traditional scholar can just as easily recog-
nise the similarities in the scenes identified by both the Wikipedia- and Poetic Corpus-based ESAs in the
course of reading the eight poems by Byron and Moore. Their narrative similarity is the most prominent
characteristic that contributes to their relatedness. This identification can be made by the lone scholar
because the dataset is relatively small in this project, and the time needed to read and analyse it is not
prohibitive. The potential value of this kind of automated semantic-relatedness identification is increased
when it is applied in a more exploratory fashion to larger datasets, and to poetic corpora whose scales
are beyond the reasonable comprehension of the individual scholar. In this scenario, a potential appli-
cation of the process would involve identifying and mapping the patterns and networks of relatedness in
large-scale poetic corpora. For the present purposes of this project?studying imitation and influence in
the texts of Byron and Moore?semantic relatedness measurements have been of limited value on their
own, but have offered promise in other areas. The first aspect of their success has been in identifying
sentiment analysis as a potential next step in drilling down into the texts to further reveal the essence of
their similarity. The second is revealing a wider application of semantic relatedness in examining broader
patterns of similarity within the history of poetry.
5 Conclusions and Future Work
We developed a method to identify influence and imitation in Romantic Orientalism poetry. We built two
Explicit Semantic Analysis (ESA) models by using Wikipedia and a Poetic Corpus. The results from
the analysis conducted with the Poetic Corpus-based ESA were a slight improvement on those resulting
from the Wikipedia-based ESA. This was as anticipated, and results might be improved even further with
a refined Poetic Corpus comprised of works from a more concentrated time period, which are more likely
to share linguistic similarities with the Byron and Moore poems.
The performance of ESA model depends on several parameters (Aggarwal, 2014) that are included
in the model, therefore, future work will include an investigation of ESA model in literature research.
Also, we are planning to use an improved version of the ESA (Polajnar, 2013) model which reduce
the orthogonality problem in the model. The value of ESA to the particular task of tracing imitation
and influence in the Romantic Orientalist poetry of Byron and Moore has been limited thus far, but it
has provided evidence of linguistic similarities in the expression of emotion. The next step for further
investigation of imitation and influence between the two poets will involve the use of sentiment analysis.
ESA was successful in identifying line groups that were closely related in terms of their narrative content
and in their use of similarly emotional language. For such a small dataset, this does not represent a
significant improvement on close reading, as similar results could have been obtained in this manner
quite quickly. But the automated identification of semantic relatedness demonstrated in this project has
46
potentially valuable applications for exploring broader literary corpora. For instance, a semantic mapping
of transnational and transhistorical poetic relatedness is a possible future venue for our research.
Acknowledgments
This work has been funded in part by a research grant from Science Foundation Ireland (SFI) under Grant
Number SFI/12/RC/2289 (INSIGHT) and by the EU FP7 program in the context of the project LIDER
(610782).
References
Harris, Zellig, Distributional structure, 1954. Word 10 (23): 146-162.
Gabrilovich, Evgeniy and Markovitch, Shaul, Computing semantic relatedness using Wikipedia-based explicit
semantic analysis, 2007. Proceedings of the 20th international joint conference on Artifical intelligence Hy-
derabad, India 1606?1611
Landauer, Thomas K and Foltz, Peter W and Laham, Darrell, An introduction to latent semantic analysis, Dis-
course processes, 25, 2-3, 259?284, 1998, Taylor & Francis
Blei, David M and Ng, Andrew Y and Jordan, Michael I Latent dirichlet allocation, the Journal of machine
Learning research, 3, 993?1022, 2003, JMLR. org
Aggarwal, Nitish and Asooja, Kartik and Buitelaar, Paul DERI&UPM: Pushing corpus based relatedness to simi-
larity: Shared task system description., Proceedings of the First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics, 2012.
Ramsay, Stephen Reading Machines: Toward an Algorithmic Criticism, Urbana: University of Illinois Press,
2011. Print.
Moore, Thomas The Letters of Thomas Moore, Ed. Wilfred S. Dowden. 2 vols. Oxford: Clarendon Press, 1964.
Print.
Gregory, Allan, ?Thomas Moore?s Orientalism.? Byron and Orientalism, Ed. Peter Cochran. Newcastle upon
Tyne: Cambridge Scholars, 2008. 173-82. Print.
Hamilton, Ian, ?Byron and the Best of Friends.? Keepers of the Flame Literary Estates and the Rise of Biography.
London: Hutchinson, 1992. Print.
Jordan, Hoover H., ?Byron and Moore.? Modern Language Quarterly, 9.4 (1948): 429-39. Print.
Sultana, Fehmida, ?Romantic Orientalism and Islam: Southey, Shelley, Moore, and Byron.?, Unpublished disser-
tation. Tufts University, 1989. Print.
Polajnar, Tamara and Aggarwal, Nitish and Asooja, Kartik and Buitelaar, Paul, Improving ESA with document
similarity, Advances in Information Retrieval, 582?593, 2013, Springer
Aggarwal, Nitish and Asooja, Kartik and Buitelaar, Paul Exploring ESA to Improve Word Relatedness, Third Joint
Conference on Lexical and Computational Semantics (*SEM), 2014
Tessier, Therese, ?Byron and Thomas Moore: A Great Literary Friendship.?, The Byron Journal 20 (1992): 4658.
MetaPress. Web. 22 Jan. 2014.
Vail, Jeffery W., The Literary Relationship of Lord Byron & Thomas Moore, Baltimore: Johns Hopkins University
Press, 2001. Print.
47
