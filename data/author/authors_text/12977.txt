Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145?154,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generic binarization for parsing and translation
Matthias Bu?chse
Technische Universita?t Dresden
matthias.buechse@tu-dresden.de
Alexander Koller
University of Potsdam
koller@ling.uni-potsdam.de
Heiko Vogler
Technische Universita?t Dresden
heiko.vogler@tu-dresden.de
Abstract
Binarization of grammars is crucial for im-
proving the complexity and performance
of parsing and translation. We present a
versatile binarization algorithm that can
be tailored to a number of grammar for-
malisms by simply varying a formal pa-
rameter. We apply our algorithm to bi-
narizing tree-to-string transducers used in
syntax-based machine translation.
1 Introduction
Binarization amounts to transforming a given
grammar into an equivalent grammar of rank 2,
i.e., with at most two nonterminals on any right-
hand side. The ability to binarize grammars is
crucial for efficient parsing, because for many
grammar formalisms the parsing complexity de-
pends exponentially on the rank of the gram-
mar. It is also critically important for tractable
statistical machine translation (SMT). Syntax-
based SMT systems (Chiang, 2007; Graehl et
al., 2008) typically use some type of synchronous
grammar describing a binary translation rela-
tion between strings and/or trees, such as syn-
chronous context-free grammars (SCFGs) (Lewis
and Stearns, 1966; Chiang, 2007), synchronous
tree-substitution grammars (Eisner, 2003), syn-
chronous tree-adjoining grammars (Nesson et al,
2006; DeNeefe and Knight, 2009), and tree-to-
string transducers (Yamada and Knight, 2001;
Graehl et al, 2008). These grammars typically
have a large number of rules, many of which have
rank greater than two.
The classical approach to binarization, as
known from the Chomsky normal form transfor-
mation for context-free grammars (CFGs), pro-
ceeds rule by rule. It replaces each rule of rank
greater than 2 by an equivalent collection of rules
of rank 2. All CFGs can be binarized in this
way, which is why their recognition problem is
cubic. In the case of linear context-free rewriting
systems (LCFRSs, (Weir, 1988)) the rule-by-rule
technique also applies to every grammar, as long
as an increased fanout it permitted (Rambow and
Satta, 1999).
There are also grammar formalisms for which
the rule-by-rule technique is not complete. In the
case of SCFGs, not every grammar has an equiva-
lent representation of rank 2 in the first place (Aho
and Ullman, 1969). Even when such a represen-
tation exists, it is not always possible to compute
it rule by rule. Nevertheless, the rule-by-rule bi-
narization algorithm of Huang et al (2009) is very
useful in practice.
In this paper, we offer a generic approach
for transferring the rule-by-rule binarization tech-
nique to new grammar formalisms. At the core of
our approach is a binarization algorithm that can
be adapted to a new formalism by changing a pa-
rameter at runtime. Thus it only needs to be im-
plemented once, and can then be reused for a va-
riety of formalisms. More specifically, our algo-
rithm requires the user to (i) encode the grammar
formalism as a subclass of interpreted regular tree
grammars (IRTGs, (Koller and Kuhlmann, 2011))
and (ii) supply a collection of b-rules, which rep-
resent equivalence of grammars syntactically. Our
algorithm then replaces, in a given grammar, each
rule of rank greater than 2 by an equivalent collec-
tion of rules of rank 2, if such a collection is li-
censed by the b-rules. We define completeness of
b-rules in a way that ensures that if any equivalent
collection of rules of rank 2 exists, the algorithm
finds one. As a consequence, the algorithm bina-
rizes every grammar that can be binarized rule by
rule. Step (i) is possible for all the grammar for-
malisms mentioned above. We show Step (ii) for
SCFGs and tree-to-string transducers.
We will use SCFGs as our running example
throughout the paper. We will also apply the algo-
145
rithm to tree-to-string transducers (Graehl et al,
2008; Galley et al, 2004), which describe rela-
tions between strings in one language and parse
trees of another, which means that existing meth-
ods for binarizing SCFGs and LCFRSs cannot be
directly applied to these systems. To our knowl-
edge, our binarization algorithm is the first to bi-
narize such transducers. We illustrate the effec-
tiveness of our system by binarizing a large tree-
to-string transducer for English-German SMT.
Plan of the paper. We start by defining IRTGs
in Section 2. In Section 3, we define the gen-
eral outline of our approach to rule-by-rule bina-
rization for IRTGs, and then extend this to an ef-
ficient binarization algorithm based on b-rules in
Section 4. In Section 5 we show how to use the
algorithm to perform rule-by-rule binarization of
SCFGs and tree-to-string transducers, and relate
the results to existing work.
2 Interpreted regular tree grammars
Grammar formalisms employed in parsing and
SMT, such as those mentioned in the introduc-
tion, differ in the the derived objects?e.g., strings,
trees, and graphs?and the operations involved in
the derivation?e.g., concatenation, substitution,
and adjoining. Interpreted regular tree grammars
(IRTGs) permit a uniform treatment of many of
these formalisms. To this end, IRTGs combine
two ideas, which we explain here.
Algebras IRTGs represent the objects and op-
erations symbolically using terms; the object in
question is obtained by interpreting each symbol
in the term as a function. As an example, Table 1
shows terms for a string and a tree, together with
the denoted object. In the string case, we describe
complex strings as concatenation (con2) of ele-
mentary symbols (e.g., a, b); in the tree case, we
alternate the construction of a sequence of trees
(con2) with the construction of a single tree by
placing a symbol (e.g., ?, ?, ?) on top of a (pos-
sibly empty) sequence of trees. Whenever a term
contains variables, it does not denote an object,
but rather a function. In the parlance of universal-
algebra theory, we are employing initial-algebra
semantics (Goguen et al, 1977).
An alphabet is a nonempty finite set. Through-
out this paper, let X = {x1, x2, . . . } be a set,
whose elements we call variables. We let Xk de-
note the set {x1, . . . , xk} for every k ? 0. Let ?
be an alphabet and V ? X . We write T?(V ) for
the set of all terms over ? with variables V , i.e.,
the smallest set T such that (i) V ? T and (ii) for
every ? ? ?, k ? 0, and t1, . . . , tk ? T , we
have ?(t1, . . . , tk) ? T . Alternatively, we view
T?(V ) as the set of all (rooted, labeled, ordered,
unranked) trees over ? and V , and draw them
as usual. By T? we abbreviate T?(?). The set
C?(V ) of contexts over ? and V is the set of all
trees over ? and V in which each variable in V
occurs exactly once.
A signature is an alphabet ? where each symbol
is equipped with an arity. We write ?|k for the
subset of all k-ary symbols of ?, and ?|k to denote
? ? ?|k. We denote the signature by ? as well.
A signature is binary if the arities do not exceed 2.
Whenever we use T?(V ) with a signature ?, we
assume that the trees are ranked, i.e., each node
labeled by ? ? ?|k has exactly k children.
Let ? be a signature. A ?-algebra A consists
of a nonempty set A called the domain and, for
each symbol f ? ? with rank k, a total function
fA : Ak ? A, the operation associated with f .
We can evaluate any term t in T?(Xk) in A, to
obtain a k-ary operation tA over the domain. In
particular, terms in T? evaluate to elements of A.
For instance, in the string algebra shown in Ta-
ble 1, the term con2(a, b) evaluates to ab, and the
term con2(con2(x2, a), x1) evaluates to a binary
operation f such that, e.g., f(b, c) = cab.
Bimorphisms IRTGs separate the finite control
(state behavior) of a derivation from its derived
object (in its term representation; generational be-
havior); the former is captured by a regular tree
language, while the latter is obtained by applying
a tree homomorphism. This idea goes back to the
tree bimorphisms of Arnold and Dauchet (1976).
Let ? be a signature. A regular tree grammar
(RTG) G over ? is a triple (Q, q0, R) where Q
is a finite set (of states), q0 ? Q, and R is a fi-
nite set of rules of the form q ? ?(q1, . . . , qk),
where q ? Q, ? ? ?|k and q, q1, . . . , qk ? Q.
We call ? the terminal symbol and k the rank
of the rule. Rules of rank greater than two are
called suprabinary. For every q ? Q we de-
fine the language Lq(G) derived from q as the set
{?(t1, . . . , tk) | q ? ?(q1, . . . , qk) ? R, tj ?
Lqj (G)}. If q = q0, we drop the superscript and
write L(G) for the tree language of G. In the lit-
erature, there is a definition of RTG which also
permits more than one terminal symbol per rule,
146
strings over ? trees over ?
example term
and denoted object
con2
a b
7? ab
?
con2
?
con0
?
con0
7?
?
? ?
domain ?? T ?? (set of sequences of trees)
signature ? {a|0 | a ? ?} ? {?|1 | ? ? ?} ?
{conk|k | 0 ? k ? K, k 6= 1} {conk|k | 0 ? k ? K, k 6= 1}
operations a : () 7? a ? : x1 7? ?(x1)
conk : (x1, . . . , xk) 7? x1 ? ? ?xk conk : (x1, . . . , xk) 7? x1 ? ? ?xk
Table 1: Algebras for strings and trees, given an alphabet ? and a maximum arity K ? N.
or none. This does not increase the generative ca-
pacity (Brainerd, 1969).
A (linear, nondeleting) tree homomorphism is a
mapping h : T?(X) ? T?(X) that satisfies the
following condition: there is a mapping g : ? ?
T?(X) such that (i) g(?) ? C?(Xk) for every
? ? ?|k, (ii) h(?(t1, . . . , tk)) is the tree obtained
from g(?) by replacing the occurrence of xj by
h(tj), and (iii) h(xj) = xj . This extends the
usual definition of linear and nondeleting homo-
morphisms (Ge?cseg and Steinby, 1997) to trees
with variables. We abuse notation and write h(?)
for g(?) for every ? ? ?.
Let n ? 1 and ?1, . . . ,?n be signatures. A
(generalized) bimorphism over (?1, . . . ,?n) is a
tuple B = (G, h1, . . . , hn) where G is an RTG
over some signature ? and hi is a tree homo-
morphism from T?(X) into T?i(X). The lan-
guage L(B) induced by B is the tree relation
{(h1(t), . . . , hn(t)) | t ? L(G)}.
An IRTG is a bimorphism whose derived trees
are viewed as terms over algebras; see Fig. 1.
Formally, an IRTG G over (?1, . . . ,?n) is a
tuple (B,A1, . . . ,An) such that B is a bimor-
phism over (?1, . . . ,?n) and Ai is a ?i-algebra.
The language L(G) induced by G is the relation
{(tA11 , . . . , tAnn ) | (t1, . . . , tn) ? L(B)}. We call
the trees in L(G) derivation trees and the terms
in L(B) semantic terms. We say that two IRTGs
G and G? are equivalent if L(G) = L(G?). IRTGs
were first defined in (Koller and Kuhlmann, 2011).
For example, Fig. 2 is an IRTG that encodes
a synchronous context-free grammar (SCFG). It
contains a bimorphism B = (G, h1, h2) consist-
ing of an RTG G with four rules and homomor-
L(G)
T?1 ? ? ? T?n
A1 ? ? ? An
h1 hn
(.)A1 (.)An
? T?
bimorphism B = (G, h1, h2)
IRTG G = (B,A1,A2)
derivation
trees
semantic
terms
derived
objects
Figure 1: IRTG, bimorphism overview.
A? ?(B,C,D)
B ? ?1, C ? ?2, D ? ?3
con3
x1 x2 x3
h1?? [ ? h27?? con
4
x3 a x1 x2
b h1?? [ ?1 h27?? b
c h1?? [ ?2 h27?? c
d h1?? [ ?3 h27?? d
Figure 2: An IRTG encoding an SCFG.
phisms h1 and h2 which map derivation trees to
trees over the signature of the string algebra in Ta-
ble 1. By evaluating these trees in the algebra,
the symbols con3 and con4 are interpreted as con-
catenation, and we see that the first rule encodes
the SCFG rule A ? ?BCD,DaBC?. Figure 3
shows a derivation tree with its two homomorphic
images, which evaluate to the strings bcd and dabc.
IRTGs can be tailored to the expressive capacity
of specific grammar formalisms by selecting suit-
able algebras. The string algebra in Table 1 yields
context-free languages, more complex string al-
147
con3
b c d
h1?? [
?
?1 ?2 ?3
h27??
con4
d a b c
Figure 3: Derivation tree and semantic terms.
A? ??(A?, D)
A? ? ???(B,C)
con2
x1 x2
h?1?? [ ?? h
?
27??
con2
con2
x2 a
x1
con2
x1 x2
h?1?? [ ??? h
?
27??
con2
x1 x2
Figure 4: Binary rules corresponding to the ?-rule
in Fig. 2.
gebras yield tree-adjoining languages (Koller and
Kuhlmann, 2012), and algebras over other do-
mains can yield languages of trees, graphs, or
other objects. Furthermore, IRTGs with n = 1 de-
scribe languages that are subsets of the algebra?s
domain, n = 2 yields synchronous languages or
tree transductions, and so on.
3 IRTG binarization
We will now show how to apply the rule-by-rule
binarization technique to IRTGs. We start in this
section by defining the binarization of a rule in an
IRTG, and characterizing it in terms of binariza-
tion terms and variable trees. We derive the actual
binarization algorithm from this in Section 4.
For the remainder of this paper, let G =
(B,A1, . . . ,An) be an IRTG over (?1, . . . ,?n)
with B = (G, h1, . . . , hn).
3.1 An introductory example
We start with an example to give an intuition of
our approach. Consider the first rule in Fig. 2,
which has rank three. This rule derives (in one
step) the fragment ?(x1, x2, x3) of the derivation
tree in Fig. 3, which is mapped to the semantic
terms h1(?) and h2(?) shown in Fig. 2. Now con-
sider the rules in Fig. 4. These rules can be used to
derive (in two steps) the derivation tree fragment ?
in Fig. 5e. Note that the terms h?1(?) and h1(?)
are equivalent in that they denote the same func-
tion over the string algebra, and so are the terms
h?2(?) and h2(?). Thus, replacing the ?-rule by
the rules in Fig. 4 does not change the language of
the IRTG. However, since the new rules are binary,
(a) con3x1 x2 x3
con4
x3 a x1 x2
(b)
con2
x1 con2
x2 x3
con2
con2
x1 x2
x3
t1 : con2
con2
x3 a
con2
x1 x2
t2 : con
2
con2
x3 con2
a x1
x2
(c)
(d)
con2
x1 x2
x1 con
2
x1 x2
x1 x2
con2
con2
x2 a
x1
x1 con
2
x1 x2
x1 x2
(e)
h1?? [ ? h27??
{x1, x2, x3}
{x1} {x2, x3}
{x2} {x3}
{x1, x2, x3}
{x1, x2}
{x1} {x2}
{x3}
? : {x1, x2, x3}
{x1, x3}
{x1} {x3}
{x2}
con2
con2
x1 x2
x3
t1 :
h?1?? [
??
???
x1 x2
x3
? :
h?27??
con2
con2
x3 a
con2
x1 x2
t2 :
Figure 5: Outline of the binarization algorithm.
parsing and translation will be cheaper.
Now we want to construct the binary rules sys-
tematically. In the example, we proceed as fol-
lows (cf. Fig. 5). For each of the terms h1(?) and
h2(?) (Fig. 5a), we consider all terms that satisfy
two properties (Fig. 5b): (i) they are equivalent
to h1(?) and h2(?), respectively, and (ii) at each
node at most two subtrees contain variables. As
Fig. 5 suggests, there may be many different terms
of this kind. For each of these terms, we ana-
lyze the bracketing of variables, obtaining what we
call a variable tree (Fig. 5c). Now we pick terms
t1 and t2 corresponding to h1(?) and h2(?), re-
spectively, such that (iii) they have the same vari-
able tree, say ? . We construct a tree ? from ? by a
simple relabeling, and we read off the tree homo-
morphisms h?1 and h?2 from a decomposition we
perform on t1 and t2, respectively; see Fig. 5, dot-
ted arrows, and compare the boxes in Fig. 5d with
the homomorphisms in Fig. 4. Now the rules in
Fig. 4 are easily extracted from ?.
These rules are equivalent to r because of (i);
they are binary because ? is binary, which in turn
holds because of (ii); finally, the decompositions
of t1 and t2 are compatible with ? because of (iii).
We call terms t1 and t2 binarization terms if they
satisfy (i)?(iii). We will see below that we can con-
148
struct binary rules equivalent to r from any given
sequence of binarization terms t1, t2, and that bi-
narization terms exist whenever equivalent binary
rules exist. The majority of this paper revolves
around the question of finding binarization terms.
Rule-by-rule binarization of IRTGs follows the
intuition laid out in this example closely: it means
processing each suprabinary rule, attempting to
replace it with an equivalent collection of binary
rules.
3.2 Binarization terms
We will now make this intuition precise. To this
end, we assume that r = q ? ?(q1, . . . , qk) is a
suprabinary rule of G. As we have seen, binariz-
ing r boils down to constructing:
? a tree ? over some binary signature ?? and
? tree homomorphisms h?1, . . . , h?n of type
h?i : T??(X)? T?i(X),
such that h?i(?) and hi(?) are equivalent, i.e., they
denote the same function over Ai. We call such a
tuple (?, h?1, . . . , h?n) a binarization of the rule r.
Note that a binarization of r need not exist. The
problem of rule-by-rule binarization consists in
computing a binarization of each suprabinary rule
of a grammar. If such a binarization does not exist,
the problem does not have a solution.
In order to define variable trees, we assume a
mapping seq that maps each finite set U of pair-
wise disjoint variable sets to a sequence over U
which contains each element exactly once. Let
t ? C?(Xk). The variable set of t is the set of
all variables that occur in t. The set S(t) of sub-
tree variables of t consists of the nonempty vari-
able sets of all subtrees of t. We represent S(t)
as a tree v(t), which we call variable tree as fol-
lows. Any two elements of S(t) are either compa-
rable (with respect to the subset relation) or dis-
joint. We extend this ordering to a tree struc-
ture by ordering disjoint elements via seq. We let
v(L) = {v(t) | t ? L} for every L ? C?(Xk).
In the example of Fig. 5, t1 and t2 have the same
set of subtree variables; it is {{x1}, {x2}, {x3},
{x1, x2}, {x1, x2, x3}}. If we assume that seq or-
ders sets of variables according to the least vari-
able index, we arrive at the variable tree in the cen-
ter of Fig. 5.
Now let t1 ? T?1(Xk), . . . , tn ? T?n(Xk).
We call the tuple t1, . . . , tn binarization terms of
r if the following properties hold: (i) hi(?) and ti
are equivalent; (ii) at each node the tree ti contains
at most two subtrees with variables; and (iii) the
terms t1, . . . , tn have the same variable tree.
Assume for now that we have found binariza-
tion terms t1, . . . , tn. We show how to construct a
binarization (?, h?1, . . . , h?n) of r with ti = h?i(?).
First, we construct ?. Since t1, . . . , tn are bi-
narization terms, they have the same variable tree,
say, ? . We obtain ? from ? by replacing every la-
bel of the form {xj} with xj , and every other label
with a fresh symbol. Because of condition (ii) in
in the definition of binarization terms, ? is binary.
In order to construct h?i(?) for each symbol ?
in ?, we transform ti into a tree t?i with labels from
C?i(X) and the same structure as ?. Then we read
off h?i(?) from the node of t?i that corresponds to
the ?-labeled node of ?. The transformation pro-
ceeds as illustrated in Fig. 6: first, we apply the
maximal decomposition operation d; it replaces
every label f ? ?i|k by the tree f(x1, . . . , xk),
represented as a box. After that, we keep applying
the merge operation  m as often as possible; it
merges two boxes that are in a parent-child rela-
tion, given that one of them has at most one child.
Thus the number of variables in any box can only
decrease. Finally, the reorder operation o orders
the children of each box according to the seq of
their variable sets. These operations do not change
the variable tree; one can use this to show that t?i
has the same structure as ?.
Thus, if we can find binarization terms, we
can construct a binarization of r. Conversely, for
any given binarization (?, h?1, . . . , h?n) the seman-
tic terms h?1(?), . . . , h?n(?) are binarization terms.
This proves the following lemma.
Lemma 1 There is a binarization of r if and only
if there are binarization terms of r.
3.3 Finding binarization terms
It remains to show how we can find binarization
terms of r, if there are any.
Let bi : T?i(Xk) ? P(T?i(Xk)) the mapping
with bi(t) = {t? ? T?i(Xk) | t and t? are equiv-
alent, and at each node t? has at most two chil-
dren with variables}. Figure 5b shows some ele-
ments of b1(h1(?)) and b2(h2(?)) for our exam-
ple. Terms t1, . . . , tn are binarization terms pre-
cisely when ti ? bi(hi(?)) and t1, . . . , tn have the
same variable tree. Thus we can characterize bi-
narization terms as follows.
Lemma 2 There are binarization terms if and
only if?i v(bi(hi(?))) 6= ?.
149
con2
con2
x3 a
con2
x1 x2
 d
con2
x1 x2
con2
x1 x2
x3 a
con2
x1 x2
x1 x2
 m
con2
x1 x2
con2
x1 a
x3
con2
x1 x2
x1 x2
 m
con2
con2
x1 a
x2
x3 con
2
x1 x2
x1 x2
 o
con2
con2
x2 a
x1
con2
x1 x2
x1 x2
x3
Figure 6: Transforming t2 into t?2.
This result suggests the following procedure
for obtaining binarization terms. First, determine
whether the intersection in Lemma 2 is empty. If
it is, then there is no binarization of r. Otherwise,
select a variable tree ? from this set. We know that
there are trees t1, . . . , tn such that ti ? bi(hi(?))
and v(ti) = ? . We can therefore select arbitrary
concrete trees ti ? bi(hi(?))? v?1(?). The terms
t1, . . . , tn are then binarization terms.
4 Effective IRTG binarization
In this section we develop our binarization algo-
rithm. Its key task is finding binarization terms
t1, . . . , tn. This task involves deciding term equiv-
alence, as ti must be equivalent to hi(?). In gen-
eral, equivalence is undecidable, so the task can-
not be solved. We avoid deciding equivalence by
requiring the user to specify an explicit approxi-
mation of bi, which we call a b-rule. This param-
eter gives rise to a restricted version of the rule-
by-rule binarization problem, which is efficiently
computable while remaining practically relevant.
Let ? be a signature. A binarization rule (b-
rule) over ? is a mapping b : ? ? P(T?(X))
where for every f ? ?|k we have that b(f) ?
C?(Xk), at each node of a tree in b(f) only two
children contain variables, and b(f) is a regular
tree language. We extend b to T?(X) by setting
b(xj) = {xj} and b(f(t1, . . . , tk)) = {t[xj/t?j |
1 ? j ? k] | t ? b(f), t?j ? b(tj)}, where [xj/t?j ]
denotes substitution of xj by t?j . Given an alge-
bra A over ?, a b-rule b over ? is called a b-rule
over A if, for every t ? T?(Xk) and t? ? b(t),
t? and t are equivalent inA. Such a b-rule encodes
equivalence in A, and it does so in an explicit and
compact way: because b(f) is a regular tree lan-
guage, a b-rule can be specified by a finite collec-
tion of RTGs, one for each symbol f ? ?. We will
look at examples (for the string and tree algebras
shown earlier) in Section 5.
From now on, we assume that b1, . . . , bn are
b-rules over A1, . . . ,An, respectively. A bina-
rization (?, h?1, . . . , h?n) of r is a binarization of r
with respect to b1, . . . , bn if h?i(?) ? bi(hi(?)).
Likewise, binarization terms t1, . . . , tn are bi-
narization terms with respect to b1, . . . , bn if
ti ? bi(hi(?)). Lemmas 1 and 2 carry over to
the restricted notions. The problem of rule-by-
rule binarization with respect to b1, . . . , bn con-
sists in computing a binarization with respect to
b1, . . . , bn for each suprabinary rule.
By definition, every solution to this restricted
problem is also a solution to the general prob-
lem. The converse need not be true. However,
we can guarantee that the restricted problem has
at least one solution whenever the general problem
has one, by requiring v(bi(hi(?)) = v(b(hi(?)).
Then the intersection in Lemma 2 is empty in the
restricted case if and only if it is empty in the gen-
eral case. We call the b-rules b1, . . . , b1 complete
on G if the equation holds for every ? ? ?.
Now we show how to effectively compute bina-
rization terms with respect to b1, . . . , bn, along the
lines of Section 3.3. More specifically, we con-
struct an RTG for each of the sets (i) bi(hi(?)),
(ii) b?i = v(bi(hi(?))), (iii)
?
i b?i, and (iv) b??i =
bi(hi(?))?v?1(?) (given ? ). Then we can select ?
from (iii) and ti from (iv) using a standard algo-
rithm, such as the Viterbi algorithm or Knuth?s
algorithm (Knuth, 1977; Nederhof, 2003; Huang
and Chiang, 2005). The effectiveness of our pro-
cedure stems from the fact that we only manipulate
RTGs and never enumerate languages.
The construction for (i) is recursive, following
the definition of bi. The base case is a language
{xj}, for which the RTG is easy. For the recursive
case, we use the fact that regular tree languages
are closed under substitution (Ge?cseg and Steinby,
1997, Prop. 7.3). Thus we obtain an RTG Gi with
L(Gi) = bi(hi(?)).
For (ii) and (iv), we need the following auxiliary
150
construction. Let Gi = (P, p0, R). We define the
mapping vari : P ? P(Xk) such that for every
p ? P , every t ? Lp(Gi) contains exactly the vari-
ables in vari(p). We construct it as follows. We
initialize vari(p) to ?unknown? for every p. For
every rule p ? xj , we set vari(p) = {xj}. For
every rule p? ?(p1, . . . , pk) such that vari(pj) is
known, we set vari(p) = ?j vari(pj). This is iter-
ated; it can be shown that vari(p) is never assigned
two different values for the same p. Finally, we set
all remaining unknown entries to ?.
For (ii), we construct an RTG G?i with L(G?i) =
b?i as follows. We let G?i = ({?vari(p)? | p ?
P}, vari(p0), R?) where R? consists of the rules
?{xj}? ? {xj} if p? xi ? R ,
?vari(p)? ? vari(p)(?U1?, . . . , ?Ul??)
if p? ?(p1, . . . , pk) ? R,
V = {vari(pj) | 1 ? j ? k} \ {?},
|V | ? 2, seq(V ) = (U1, . . . , Ul) .
For (iii), we use the standard product construc-
tion (Ge?cseg and Steinby, 1997, Prop. 7.1).
For (iv), we construct an RTG G??i such that
L(G??i ) = b??i as follows. We let G??i = (P, p0, R??),
where R?? consists of the rules
p? ?(p1, . . . , pk)
if p? ?(p1, . . . , pk) ? R,
V = {vari(pj) | 1 ? j ? k} \ {?},
if |V | ? 2, then
(vari(p), seq(V )) is a fork in ? .
By a fork (u, u1 ? ? ?uk) in ? , we mean that there
is a node labeled u with k children labeled u1 up
to uk.
At this point we have all the ingredients for our
binarization algorithm, shown in Algorithm 1. It
operates directly on a bimorphism, because all the
relevant information about the algebras is captured
by the b-rules. The following theorem documents
the behavior of the algorithm. In short, it solves
the problem of rule-by-rule binarization with re-
spect to b-rules b1, . . . , bn.
Theorem 3 Let G = (B,A1, . . . ,An) be
an IRTG, and let b1, . . . , bn be b-rules over
A1, . . . ,An, respectively.
Algorithm 1 terminates. Let B? be the
bimorphism computed by Algorithm 1 on B
and b1, . . . , bn. Then G? = (B?,A1, . . . ,An) is
equivalent to G, and G? is of rank 2 if and only
Input: bimorphism B = (G, h1, . . . , hn),
b-rules b1, . . . , bn over ?1, . . . ,?n
Output: bimorphism B?
1: B? ? (G|?2, h1, . . . , hn)
2: for rule r : q ? ?(q1, . . . , qk) of G|>2 do
3: for i = 1, . . . , n do
4: compute RTG Gi for bi(hi(?))
5: compute RTG G?i for v(bi(hi(?)))
6: compute RTG Gv for ?i L(G?i)
7: if L(Gv) = ? then
8: add r to B?
9: else
10: select t? ? L(Gv)
11: for i = 1, . . . , n do
12: compute RTG G??i for
13: b??i = bi(hi(?)) ? v?1(t?)
14: select ti ? L(G??i )
15: construct binarization for t1, . . . , tn
16: add appropriate rules to B?
Algorithm 1: Complete binarization algorithm,
whereG|?2 andG|>2 isG restricted to binary and
suprabinary rules, respectively.
if every suprabinary rule of G has a binarization
with respect to b1, . . . , bn.
The runtime of Algorithm 1 is dominated by the
intersection construction in line 6, which isO(m1 ?
. . . ?mn) per rule, where mi is the size of G?i. The
quantity mi is linear in the size of the terms on the
right-hand side of hi, and in the number of rules in
the b-rule bi.
5 Applications
Algorithm 1 implements rule-by-rule binarization
with respect to given b-rules. If a rule of the given
IRTG does not have a binarization with respect to
these b-rules, it is simply carried over to the new
grammar, which then has a rank higher than 2. The
number of remaining suprabinary rules depends
on the b-rules (except for rules that have no bi-
narization at all). The user can thus engineer the
b-rules according to their current needs, trading off
completeness, runtime, and engineering effort.
By contrast, earlier binarization algorithms for
formalisms such as SCFG and LCFRS simply at-
tempt to find an equivalent grammar of rank 2;
there is no analogue of our b-rules. The problem
these algorithms solve corresponds to the general
rule-by-rule binarization problem from Section 3.
151
NP
NP
DT
the
x1:NNP POS
?s
x2:JJ x3:NN ?? das x2 x3 der x1
Figure 7: A rule of a tree-to-string transducer.
We show that under certain conditions, our algo-
rithm can be used to solve this problem as well.
In the following two subsections, we illustrate this
for SCFGs and tree-to-string transducers, respec-
tively. In the final subsection, we discuss how to
extend this approach to other grammar formalisms
as well.
5.1 Synchronous context-free grammars
We have used SCFGs as the running example in
this paper. SCFGs are IRTGs with two interpre-
tations into the string algebra of Table 1, as illus-
trated by the example in Fig. 2. In order to make
our algorithm ready to use, it remains to specify a
b-rule for the string algeba.
We use the following b-rule for both b1 and b2.
Each symbol a ? ?i|0 is mapped to the language
{a}. Each symbol conk, k ? 2, is mapped to
the language induced by the following RTG with
states of the form [j, j?] (where 0 ? j < j? ? k)
and final state [0, k]:
[j ? 1, j]? xj (1 ? j ? k)
[j, j?]? con2([j, j??], [j??, j?])
(0 ? j < j?? < j? ? k)
This language expresses all possible ways in
which conk can be written in terms of con2.
Our definition of rule-by-rule binarization with
respect to b1 and b2 coincides with that of Huang
et al (2009): any rule can be binarized by
both algorithms or neither. For instance, for the
SCFG rule A ? ?BCDE,CEBD?, the sets
v(b1(h1(?))) and v(b2(h2(?))) are disjoint, thus
no binarization exists. Two strings of length N
can be parsed with a binary IRTG that represents
an SCFG in time O(N6).
5.2 Tree-to-string transducers
Some approaches to SMT go beyond string-to-
string translation models such as SCFG by exploit-
ing known syntactic structures in the source or tar-
get language. This perspective on translation nat-
urally leads to the use of tree-to-string transducers
NP? ?(NNP, JJ,NN)
NP
con3
NP
con3
DT
the
con0
x1 POS
?s
con0
x2 x3
h1?? [ ? h27?? con
5
das x2 x3 der x1
Figure 8: An IRTG rule encoding the rule in Fig. 7.
(Yamada and Knight, 2001; Galley et al, 2004;
Huang et al, 2006; Graehl et al, 2008). Figure 7
shows an example of a tree-to-string rule. It might
be used to translate ?the Commission?s strategic
plan? into ?das langfristige Programm der Kom-
mission?.
Our algorithm can binarize tree-to-string trans-
ducers; to our knowledge, it is the first algorithm
to do so. We model the tree-to-string transducer
as an IRTG G = ((G, h1, h2),A1,A2), where
A2 is the string algebra, but this time A1 is the
tree algebra shown in Table 1. This algebra has
operations conk to concatenate sequences of trees
and unary ? that maps any sequence (t1, . . . , tl) of
trees to the tree ?(t1, . . . , tl), viewed as a sequence
of length 1. Note that we exclude the operation
con1 because it is the identity and thus unneces-
sary. Thus the rule in Fig. 7 translates to the IRTG
rule shown in Fig. 8.
For the string algebra, we reuse the b-rule from
Section 5.1; we call it b2 here. For the tree algebra,
we use the following b-rule b1. It maps con0 to
{con0} and each unary symbol ? to {?(x1)}. Each
symbol conk, k ? 2, is treated as in the string
case. Using these b-rules, we can binarize the rule
in Fig. 8 and obtain the rules in Fig. 9. Parsing
of a binary IRTG that represents a tree-to-string
transducer is O(N3 ?M) for a string of length N
and a tree with M nodes.
We have implemented our binarization algo-
rithm and the b-rules for the string and the tree
algebra. In order to test our implementation, we
extracted a tree-to-string transducer from about a
million parallel sentences of English-German Eu-
roparl data, using the GHKM rule extractor (Gal-
ley, 2010). Then we binarized the transducer. The
results are shown in Fig. 10. Of the 2.15 million
rules in the extracted transducer, 460,000 were
suprabinary, and 67 % of these could be binarized.
Binarization took 4.4 minutes on a single core of
an Intel Core i5 2520M processor.
152
NP? ??(NNP, A?)
A? ? ???(JJ,NN)
NP
con2
NP
con2
DT
the
con0
con2
x1 POS
?s
con0
x2
h?1?? [ ?? h
?
27??
con2
con2
das x2
con2
der x1
con2
x1 x2
h?1?? [ ??? h
?
27??
con2
x1 x2
Figure 9: Binarization of the rule in Fig. 8.
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
ext bin
# 
ru
le
s 
(m
ill
io
ns
) rank
0
1
2
3
4
5
6-7
8-10
Figure 10: Rules of a transducer extracted from
Europarl (ext) vs. its binarization (bin).
5.3 General approach
Our binarization algorithm can be used to solve
the general rule-by-rule binarization problem for
a specific grammar formalism, provided that one
can find appropriate b-rules. More precisely,
we need to devise a class C of IRTGs over the
same sequence A1, . . . ,An of algebras that en-
codes the grammar formalism, together with b-
rules b1, . . . , bn over A1, . . . ,An that are com-
plete on every grammar in C, as defined in Sec-
tion 4.
We have already seen the b-rules for SCFGs and
tree-to-string transducers in the preceding subsec-
tions; now we have a closer look at the class C
for SCFGs. We used the class of all IRTGs with
two string algebras and in which hi(?) contains
at most one occurrence of a symbol conk for ev-
ery ? ? ?. On such a grammar the b-rules are
complete. Note that this would not be the case
if we allowed several occurrences of conk, as in
con2(con2(x1, x2), x3). This term is equivalent
to itself and to con2(x1, con2(x2, x3)), but the b-
rules only cover the former. Thus they miss one
variable tree. For the term con3(x1, x2, x3), how-
ever, the b-rules cover both variable trees.
Generally speaking, given C and b-rules
b1, . . . , bn that are complete on every IRTG in C,
Algorithm 1 solves the general rule-by-rule bina-
rization problem on C. We can adapt Theorem 3 by
requiring that G must be in C, and replacing each
of the two occurrences of ?binarization with re-
spect to b1, . . . , bn? by simply ?binarization?. If C
is such that every grammar from a given grammar
formalism can be encoded as an IRTG in C, this
solves the general rule-by-rule binarization prob-
lem of that grammar formalism.
6 Conclusion
We have presented an algorithm for binarizing
IRTGs rule by rule, with respect to b-rules that
the user specifies for each algebra. This improves
the complexity of parsing and translation with any
monolingual or synchronous grammar that can be
represented as an IRTG. A novel algorithm for
binarizing tree-to-string transducers falls out as a
special case.
In this paper, we have taken the perspective that
the binarized IRTG uses the same algebras as the
original IRTG. Our algorithm extends to gram-
mars of arbitrary fanout (such as synchronous
tree-adjoining grammar (Koller and Kuhlmann,
2012)), but unlike LCFRS-based approaches to bi-
narization, it will not increase the fanout to en-
sure binarizability. In the future, we will ex-
plore IRTG binarization with fanout increase. This
could be done by binarizing into an IRTG with
a more complicated algebra (e.g., of string tu-
ples). We might compute binarizations that are
optimal with respect to some measure (e.g., fanout
(Gomez-Rodriguez et al, 2009) or parsing com-
plexity (Gildea, 2010)) by keeping track of this
measure in the b-rule and taking intersections of
weighted tree automata.
Acknowledgments
We thank the anonymous referees for their insight-
ful remarks, and Sarah Hemmen for implementing
an early version of the algorithm. Matthias Bu?chse
was financially supported by DFG VO 1011/6-1.
153
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler.
Journal of Computer and System Sciences, 3:37?56.
Andre? Arnold and Max Dauchet. 1976. Bi-
transduction de fore?ts. In Proc. 3rd Int. Coll. Au-
tomata, Languages and Programming, pages 74?86.
Edinburgh University Press.
Walter S. Brainerd. 1969. Tree generating regular sys-
tems. Information and Control, 14(2):217?231.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree-adjoining machine translation. In Proceedings
of EMNLP, pages 727?736.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of the 41st ACL, pages 205?208.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL, pages 273?280.
Michael Galley. 2010. GHKM rule extractor. http:
//www-nlp.stanford.edu/?mgalley/
software/stanford-ghkm-latest.tar.
gz, retrieved on March 28, 2012.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, chap-
ter 1, pages 1?68. Springer-Verlag.
Daniel Gildea. 2010. Optimal parsing strategies for
linear context-free rewriting systems. In Proceed-
ings of NAACL HLT.
Joseph A. Goguen, Jim W. Thatcher, Eric G. Wagner,
and Jesse B. Wright. 1977. Initial algebra seman-
tics and continuous algebras. Journal of the ACM,
24:68?95.
Carlos Gomez-Rodriguez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proceedings of NAACL HLT.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th IWPT, pages 53?
64.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th AMTA,
pages 66?73.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Donald E. Knuth. 1977. A generalization of Dijkstra?s
algorithm. Information Processing Letters, 6(1):1?
5.
Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th IWPT, pages 2?13.
Alexander Koller and Marco Kuhlmann. 2012. De-
composing TAG algorithms using simple alge-
braizations. In Proceedings of the 11th TAG+ Work-
shop, pages 135?143.
Philip M. Lewis and Richard E. Stearns. 1966. Syn-
tax directed transduction. Foundations of Computer
Science, IEEE Annual Symposium on, 0:21?35.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computational Linguis-
tics, 29(1):135?143.
Rebecca Nesson, Stuart M. Shieber, and Alexander
Rush. 2006. Induction of probabilistic synchronous
tree-insertion grammars for machine translation. In
Proceedings of the 7th AMTA.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223(1?2):87?
120.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th ACL, pages 523?530.
154
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 46?54,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
n-Best Parsing Revisited?
Matthias Bu?chse and Daniel Geisler and Torsten Stu?ber and Heiko Vogler
Faculty of Computer Science
Technische Universita?t Dresden
01062 Dresden
{buechse,geisler,stueber,vogler}@tcs.inf.tu-dresden.de
Abstract
We derive and implement an algorithm
similar to (Huang and Chiang, 2005) for
finding the n best derivations in a weighted
hypergraph. We prove the correctness and
termination of the algorithm and we show
experimental results concerning its run-
time. Our work is different from the afore-
mentioned one in the following respects:
we consider labeled hypergraphs, allowing
for tree-based language models (Maletti
and Satta, 2009); we specifically handle
the case of cyclic hypergraphs; we admit
structured weight domains, allowing for
multiple features to be processed; we use
the paradigm of functional programming
together with lazy evaluation, achieving
concise algorithmic descriptions.
1 Introduction
In statistical natural language processing, proba-
bilistic models play an important role which can
be used to assign to some input sentence a set of
analyses, each carrying a probability. For instance,
an analysis can be a parse tree or a possible trans-
lation. Due to the ambiguity of natural language,
the number of analyses for one input sentence can
be very large. Some models even assign an infinite
number of analyses to an input sentence.
In many cases however, the set of analyses can
in fact be represented in a finite and compact way.
While such a representation is space-efficient, it
may be incompatible with subsequent operations.
In these cases a finite subset is used as an approx-
imation, consisting of n best analyses, i. e. n anal-
yses with highest probability. For example, this
approach has the following two applications.
(1) Reranking: when log-linear models (Och
and Ney, 2002) are employed, some features may
? This research was financially supported by DFG VO
1101/5-1.
not permit an efficient evaluation during the com-
putation of the analyses. These features are com-
puted using individual analyses from said approx-
imation, leading to a reranking amongst them.
(2) Spurious ambiguity: many models produce
analyses which may be too fine-grained for further
processing (Li et al, 2009). As an example, con-
sider context-free grammars, where several left-
most derivations may exist for the same terminal
string. The weight of the terminal string is ob-
tained by summing over these derivations. The
n best leftmost derivations may be used to approx-
imate this sum.
In this paper, we consider the case where the
finite, compact representation has the form of a
weighted hypergraph (with labeled hyperedges)
and the analyses are derivations of the hypergraph.
This covers many parsing applications (Klein and
Manning, 2001), including weighted deductive
systems (Goodman, 1999; Nederhof, 2003), and
also applications in machine translation (May and
Knight, 2006).
In the nomenclature of (Huang and Chiang,
2005), which we adopt here, a derivation of a hy-
pergraph is a tree which is obtained in the follow-
ing way. Starting from some node, an ingoing hy-
peredge is picked and recorded as the label of the
root of the tree. Then, for the subtrees, one con-
tinues with the source nodes of said hyperedge in
the same way. In other words, a derivation can be
understood as an unfolding of the hypergraph.
The n-best-derivations problem then amounts
to finding n derivations which are best with re-
spect to the weights induced by the weighted hy-
pergraph.1 Among others, weighted hypergraphs
with labeled hyperedges subsume the following
two concepts.
(I) probabilistic context-free grammars (pcfgs).
1Note that this problem is different from the n-best-
hyperpaths problem described by Nielsen et al (2005), as
already argued in (Huang and Chiang, 2005, Section 2).
46
In this case, nodes correspond to nonterminals,
hyperedges are labeled with productions, and the
derivations are exactly the abstract syntax trees
(ASTs) of the grammar (which are closely related
the parse trees). Note that, unless the pcfg is un-
ambiguous, a given word may have several cor-
responding ASTs, and its weight is obtained by
summing over the weights of the ASTs. Hence,
the n best derivations need not coincide with the
n best words (cf. application (2) above).
(II) weighted tree automata (wta) (Alexandrakis
and Bozapalidis, 1987; Berstel and Reutenauer,
1982; ?Esik and Kuich, 2003; Fu?lo?p and Vogler,
2009). These automata serve both as a tree-based
language model and as a data structure for the
parse forests obtained from that language model
by applying the Bar-Hillel construction (Maletti
and Satta, 2009). It is well known that context-free
grammars and tree automata are weakly equiv-
alent (Thatcher, 1967; ?Esik and Kuich, 2003).
However, unlike the former formalism, the latter
one has the ability to model non-local dependen-
cies in parse trees.
In the case of wta, nodes correspond to states,
hyperedges are labeled with input symbols, and
the derivations are exactly the runs of the automa-
ton. Since, due to ambiguity, a given tree may
have several accepting runs, the n best derivations
need not coincide with the n best trees. As for
the pcfgs, this is an example of spurious ambigu-
ity, which can be tackled as indicated by appli-
cation (2) above. Alternatively, one can attempt
to find an equivalent deterministic wta (May and
Knight, 2006; Bu?chse et al, 2009).
Next, we briefly discuss four known algorithms
which solve the n-best-derivations problem or
subproblems thereof.
? The Viterbi algorithm solves the 1-best-
derivation problem for acyclic hypergraphs. It is
based on a topological sort of the hypergraph.
? Knuth (1977) generalizes Dijkstra?s algorithm
(for finding the single-source shortest paths in a
graph) to hypergraphs, thus solving the case n = 1
even if the hypergraph contains cycles. Knuth as-
sumes the weights to be real numbers, and he re-
quires weight functions to be monotone and supe-
rior in order to guarantee that a best derivation ex-
ists. (The superiority property corresponds to Di-
jkstra?s requirement that edge weights?or, more
generally, cycle weights?are nonnegative.)
? Huang and Chiang (2005) show that the n-
best-derivations problem can be solved efficiently
by first solving the 1-best-derivation problem and
then extending that solution in a lazy manner.
Huang and Chiang assume weighted unlabeled hy-
pergraphs with weights computed in the reals, and
they require the weight functions to be monotone.
Moreover they assume that the 1-best-
derivation problem be solved using the Viterbi
algorithm, which implies that the hypergraph must
be acyclic. However they conjecture that their
second phase also works for cyclic hypergraphs.
? Pauls and Klein (2009) propose a variation
of the algorithm of Huang and Chiang (2005) in
which the 1-best-derivation problem is computed
via an A?-based exploration of the 1-best charts.
In this paper, we also present an algorithm
for solving the n-best-derivations problem. Ulti-
mately it uses the same algorithmic ideas as the
one of Huang and Chiang (2005); however, it is
different in the following sense:
1. we consider labeled hypergraphs, allowing
for wta to be used in parsing;
2. we specifically handle the case of cyclic
hypergraphs, thus supporting the conjecture of
Huang and Chiang; for this we impose on the
weight functions the same requirements as Knuth
and use his algorithm;
3. by using the concept of linear pre-orders (and
not only linear orders on the set of reals) our ap-
proach can handle structured weights such as vec-
tors over frequencies, probabilities, and reals;
4. we present our algorithm in the framework
of functional programming (and not in that of im-
perative programming); this framework allows to
decribe algorithms in a more abstract and concise,
yet natural way;
5. due to the lazy evaluation paradigm often
found in functional programming, we obtain the
laziness on which the algorithm of Huang and Chi-
ang (2005) is based for free;
6. exploiting the abstract level of description
(see point 4) we are able to prove the correctness
and termination of our algorithm.
At the end of this paper, we will discuss experi-
ments which have been performed with an imple-
mentation of our algorithm in the functional pro-
gramming language HASKELL.
2 The n-best-derivations problem
In this section, we state the n-best-derivations
problem formally, and we give a comprehensive
47
example. First, we introduce some basic notions.
Trees and hypergraphs The definition of
ranked trees commonly used in formal tree lan-
guage theory will serve us as the basis for defining
derivations.
A ranked alphabet is a finite set ? (of symbols)
where every symbol carries a rank (a nonnegative
integer). By ?(k) we denote the set of those sym-
bols having rank k. The set of trees over ?, de-
noted by T? , is the smallest set T such that for
every k ? N, ? ? ?(k), and ?1, . . . , ?k ? T ,
also ?(?1, . . . , ?k) ? T ;2 for ? ? ?(0) we ab-
breviate ?() by ?. For every k ? N, ? ?
?(k) and subsets T1, . . . , Tk ? T? we define
the top-concatenation (with ?) ?(T1, . . . , Tk) =
{?(?1, . . . , ?k) | ?1 ? T1, . . . , ?k ? Tk}.
A ?-hypergraph is a pair H = (V,E) where
V is a finite set (of vertices or nodes) and E ?
V ????V is a finite set (of hyperedges) such that
for every (v1 . . . vk, ?, v) ? E we have that ? ?
?(k).3 We interpret E as a ranked alphabet where
the rank of each edge is carried over from its label
in ?. The family (Hv | v ? V ) of derivations of H
is the smallest family (Pv | v ? V ) of subsets
of TE such that e(Pv1 , . . . , Pvk) ? Pv for every
e = (v1 . . . vk, ?, v) ? E.
A ?-hypergraph (V,E) is cyclic if there
are hyperedges (v11 . . . v1k1 , ?1, v
1), . . . ,
(vl1 . . . vlkl , ?l, v
l) ? E such that vj?1 occurs
in vj1 . . . v
j
kj for every j ? {2, . . . , l} and v
l occurs
in v11 . . . v1k1 . It is called acyclic if it is not cyclic.
Example 1 Consider the ranked alphabet ? =
?(0)??(1)??(2) with ?(0) = {?, ?}, ?(1) = {?},
and ?(2) = {?}, and the ?-hypergraph H =
(V,E) where
? V = {0, 1} and
? E = {(?, ?, 1), (?, ?, 1), (1, ?, 1), (11, ?, 0),
(1, ?, 0)}.
A graphical representation of this hypergraph is
shown in Fig. 1. Note that this hypergraph is cyclic
because of the edge (1, ?, 1).
We indicate the derivations of H , assuming that
e1, . . . , e5 are the edges in E in the order given
above:
2The term ?(?1, . . . , ?k) is usually understood as a string
composed of the symbol ?, an opening parenthesis, the
string ?1, a comma, and so on.
3The hypergraphs defined here are essentially nondeter-
ministic tree automata, where V is the set of states and E is
the set of transitions.
? ?
0 1 ?
? ?
e5
e3
e4
e1
e2
Figure 1: Hypergraph of Example 1.
? H1 = {e1, e2, e3(e1), e3(e2), e3(e3(e1)), . . . }
and
? H0 = e4(H1,H1) ? e5(H1) where, e. g.,
e4(H1,H1) is the top-concatenation of H1,
H1 with e4, and thus
e4(H1,H1) = {e4(e1, e1), e4(e1, e2),
e4(e1, e3(e1)), e4(e3(e1), e1), . . . } .
Next we give an example of ambiguity in hyper-
graphs with labeled hyperedges. Suppose that E
contains an additional hyperedge e6 = (0, ?, 0).
Then H0 would contain the derivations e6(e5(e1))
and e5(e3(e1)), which describe the same ?-tree,
viz. ?(?(?)) (obtained by the node-wise projec-
tion to the second component). 
In the sequel, let H = (V,E) be a ?-hypergraph.
Ordering Usually an ordering is induced on the
set of derivations by means of probabilities or,
more generally, weights. In the following, we will
abstract from the weights by using a binary rela-
tion - directly on derivations, where we will in-
terpret the fact ?1 - ?2 as ??1 is better than or
equal to ?2?.
Example 2 (Ex. 1 contd.) First we show how an
ordering is induced on derivations by means of
weights. To this end, we associate an operation
over the set R of reals with every hyperedge (re-
specting its arity) by means of a mapping ?:
?(e1)() = 4 ?(e2)() = 3
?(e3)(x1) = x1 + 1 ?(e4)(x1, x2) = x1 + x2
?(e5)(x1) = x1 + 0.5
The weight h(?) of a tree ? ? TE is obtained by
interpreting the symbols at each node using ?, e. g.
h(e3(e2)) = ?(e3)(?(e2)()) = ?(e2)() + 1 = 4.
Then the natural order ? on R induces the bi-
nary relation - over TE as follows: for every
?1, ?2 ? TE we let ?1 - ?2 iff h(?1) ? h(?2),
meaning that trees with smaller weights are con-
sidered better. (This is, e. g., the case when calcu-
lating probabilites in the image of ? log x.) Note
48
that we could just as well have defined - with the
inverted order.
Since addition is commutative, we obtain
for every ?1, ?2 ? TE that h(e4(?1, ?2)) =
h(e4(?2, ?1)) and thus e4(?1, ?2) - e4(?2, ?1) and
vice versa. Thus, for two different trees (e4(?1, ?2)
and e4(?2, ?1)) having the same weight, - should
not prefer any of them. That is, - need not be
antisymmetric.
As another example, the mapping ? could as-
sign to each symbol an operation over real-valued
vectors, where each component represents one
feature of a log-linear model such as frequencies,
probabilities, reals, etc. Then the ordering could
be defined by means of a linear combination of the
feature weights. 
We use the concept of a linear pre-order to cap-
ture the orderings which are obtained this way.
Let S be a set. A pre-order (on S) is a binary
relation - ? S ? S such that (i) s - s for ev-
ery s ? S (reflexivity) and (ii) s1 - s2 and s2 - s3
implies s1 - s3 for every s1, s2, s3 ? S (transi-
tivity). A pre-order - is called linear if s1 - s2
or s2 - s1 for every s1, s2 ? S. For instance, the
binary relation - on TE as defined in Ex. 2 is a
linear pre-order.
We will restrict our considerations to a class
of linear pre-orders which admit efficient algo-
rithms. For this, we will always assume a lin-
ear pre-order - with the following two properties
(cf. Knuth (1977)).4
SP (subtree property) For every e(?1, . . . , ?k) ?
TE and i ? {1, . . . , k} we have ?i -
e(?1, . . . , ?k).5
CP (compatibility) For every pair e(?1, . . . , ?k),
e(??1, . . . , ??k) ? TE with ?1 - ??1, . . . ,
?k - ??k we have that e(?1, . . . , ?k) -
e(??1, . . . , ??k).
It is easy to verify that the linear pre-order - of
Ex. 2 has the aforementioned properties.
In the sequel, let- be a linear pre-order
on TE fulfilling SP and CP.
4Originally, these properties were called ?superiority? and
?monotonicity? because they were viewed as properties of
the weight functions. We use the terms ?subtree property?
and ?compatibility? respectively, because we view them as
properties of the linear pre-order.
5This strong property is used here for simplicity. It suf-
fices to require that for every v ? V and pair ?, ?? ? Hv we
have ? - ?? if ? is a subtree of ??.
Before we state the n-best-derivations problem
formally, we define the operation minn, which
maps every subset T of TE to the set of all se-
quences of n best elements of T . To this end, let
T ? TE and n ? |T |. We define minn(T ) to be
the set of all sequences (?1, . . . , ?n) ? T n of pair-
wise distinct elements such that ?1 - . . . - ?n and
for every ? ? T \ {?1, . . . , ?k} we have ?n - ?.
For every n > |T | we set minn(T ) = min|T |(T ).
In addition, we set min?n(T ) =
?n
i=0 mini(T ).
n-best-derivations problem The n-best-
derivations problem amounts to the following.
Given a ?-hypergraph H = (V,E), a vertex v ?
V , and a linear pre-order - on TE fulfilling
SP and CP,
compute an element of minn(Hv).
3 Functional Programming
We will describe our main algorithm as a func-
tional program. In essence, such a program is a
system of (recursive) equations that defines sev-
eral functions (as shown in Fig. 2). As a conse-
quence the main computational paradigm for eval-
uating the application (f a) of a function f to an
argument a is to choose an appropriate defining
equation f x = r and then evaluate (f a) to r?
which is obtained from r by substituting every oc-
currence of x by a.
We assume a lazy (and in particular, call-by-
need) evaluation strategy, as in the functional pro-
gramming language HASKELL. Roughly speak-
ing, this amounts to evaluating the arguments of
a function only as needed to evaluate the its body
(i. e. for branching). If an argument occurs multi-
ple times in the body, it is evaluated only once.
We use HASKELL notation and functions for
dealing with lists, i. e. we denote the empty list by
[] and list construction by x:xs (where an ele-
ment x is prepended to a list xs), and we use the
functions head (line 01), tail (line 02), and take
(lines 03 and 04), which return the first element in
a list, a list without its first element, and a prefix
of a list, respectively.
In fact, the functions shown in Fig. 2 will be
used in our main algorithm (cf. Fig. 4). Thus,
we explain the functions merge (lines 05?07) and
e(l1, . . . ,lk) (lines 08?10) a bit more in detail.
The merge function takes a set L of pairwise
disjoint lists of derivations, each one in ascend-
ing order with respect to -, and merges them into
49
-- standard Haskell functions: list deconstructors, take operation
01 head (x:xs) = x
02 tail (x:xs) = xs
03 take n xs = [] if n == 0 or xs == []
04 take n xs = (head xs):take (n-1) (tail xs)
-- merge operation (lists in L should be disjoint)
05 merge L = [] if L \ {[]} = ?
06 merge L = m:merge ({tail l | l ? L, l != [], head l == m} ?
{l | l ? L, l != [], head l != m})
07 where m = min{head l | l ? L, l != []}
-- top concatenation
08 e(l1, . . . ,lk) = [] if li == [] for some i ? {1, . . . , k}
09 e(l1, . . . ,lk) = e(head l1, . . . , head lk):merge {e(li1, . . . ,lik) | i ? {1, . . . , k}}
10 where lij =
?
?
?
?
?
lj if j < i
tail lj if j = i
[head lj] if j > i
Figure 2: Some useful functions specified in a functional programming style.
one list with the same property (as known from the
merge sort algorithm).
Note that the minimum used in line 07 is based
on the linear pre-order -. For this reason, it
need not be uniquely determined. However, in an
implementation this function is deterministic, de-
pending on the the data structures.
The function e(l1, . . . ,lk) implements the top-
concatenation with e on lists of derivations. It is
defined for every e = (v1 . . . vk, ?, v) ? E and
takes lists l1, . . . , lk of derivations, each in as-
cending order as for merge. The resulting list is
also in ascending order.
4 Algorithm
In this section, we develop our algorithm for solv-
ing the n-best-derivations problem. We begin by
motivating our general approach, which amounts
to solving the 1-best-derivation problem first and
then extending that solution to a solution of the n-
best-derivations problem.
It can be shown that for every m ? n, the
set minn(Hv) is equal to the set of all prefixes of
length n of elements of minm(Hv). According
to this observation, we will develop a function p
mapping every v ? V to a (possibly infinite) list
such that the prefix of length n is in minn(Hv)
for every n. Then, by virtue of lazy evaluation, a
solution to the n-best-derivations problem can be
obtained by evaluating the term
take n (p v)
where take is specified in lines 03?04 of Fig. 2.
Thus, what is left to be done is to specify p appro-
priately.
4.1 A provisional specification of p
Consider the following provisional specification
of p:
p v = merge {e(p v1, . . . ,p vk) |
e = (v1 . . . vk, ?, v) ? E} (?)
where the functions merge and e(l1, . . . ,lk) are
specified in lines 05?07 and lines 08?10 of Fig. 2,
respectively. This specification models exactly the
trivial equation
Hv =
?
e=(v1...vk ,?,v)?E
e(Hv1 , . . . ,Hvk)
for every v ? V , where the union and the top-
concatenation have been implemented for lists via
the functions merge and e(l1, . . . ,lk).
This specification is adequate if H is acyclic.
For cyclic hypergraphs however, it can not even
solve the 1-best-derivation problem. To illustrate
this, we consider the hypergraph of Ex. 2 and cal-
50
culate6
take 1 (p 1)
= (head (p 1)):take 0 (tail (p 1)) (04)
= head (p 1) (03)
= head (merge {e1(), e2(), e3(p 1)}) (?)
= min{head e1(), head e2(), head e3(p 1)}
(01, 06, 07)
= min{head e1(), head e2(), e3(head (p 1))}.
(09)
Note that the infinite regress occurs because the
computation of the head element head (p 1) de-
pends on itself. This leads us to the idea of
?pulling? this head element (which is the solu-
tion to the 1-best-derivation problem) ?out? of the
merge in (?). Applying this idea to our particular
example, we reach the following equation for p 1:
p 1 = e2: merge {e1(), e3(p 1)}
because e2 is the best derivation in H1. Then, in
order to evaluate merge we have to compute
min{head e1(), head e3(p 1)}
= min{e1, e3(head (p 1))}
= min{e1, e3(e2)}.
Since h(e1) = h(e3(e2)) = 4, we can choose any
of them, say e1, and continue:
e2: merge {e1(), e3(p 1)}
= e2: e1: merge {tail e1(), e3(p 1)}
= e2: e1: e3(e2): merge {tail e3(p 1)}
= ...
Generalizing this example, the function p could
be specified as follows:
p 1 = (b 1) : merge {exp} (??)
where b 1 evaluates the 1-best derivation in H1
and exp ?somehow? calculates the next best
derivations. In the following subsection, we elabo-
rate this approach. First, we develop an algorithm
for solving the 1-best-derivation problem.
4.2 Solving the 1-best-derivation problem
Using SP and CP, it can be shown that for ev-
ery v ? V such that Hv 6= ? there is a mini-
mal derivation in Hv which does not contain any
subderivation in Hv (apart from itself). In other
words, it is not necessary to consider cycles when
solving the 1-best-derivation problem.
6Please note that e1() is an application of the function in
lines 08?10 of Fig. 2 while e1 is a derivation.
We can exploit this knowledge in a program by
keeping a set U of visited nodes, taking care not to
consider edges which lead us back to those nodes.
Consider the following function:
b v U = min{e(b v1 U?, . . . , b vk U?) |
e = (v1 . . . vk, ?, v) ? E,
{v1, . . . , vk} ? U? = ?}
where U? = U ? {v}
The argument U is the set of visited nodes. The
term b v ? evaluates to a minimal element of Hv,
or to min ? if Hv = ?. The problem of this divide-
and-conquer (or top-down) approach is that man-
aging a separate set U for every recursive call in-
curs a big overhead in the computation.
This overhead can be avoided by using a
dynamic programming (or bottom-up) approach
where each node is visited only once, and nodes
are visited in the order of their respective best
derivations.
To be more precise, we maintain a family (Pv |
v ? V ) of already found best derivations (where
Pv ? min?1(Hv) and initially empty) and a set C
of candidate derivations, where candidates for all
vertices are considered at the same time. In each
iteration, a minimal candidate with respect to - is
selected. This candidate is then declared the best
derivation of its respective node.
The following lemma shows that the bottom-up
approach is sound.
Lemma 3 Let (Pv | v ? V ) be a family such that
Pv ? min?1(Hv). We define
C = ?e=(v1...vk ,?,v)?E,
Pv=?
e(Pv1 , . . . , Pvk) .
Then (i) for every ? ? ?v?V,Pv=? Hv there is a
?? ? C such that ??  ?, and (ii) for every v ? V
and ? ? C ? Hv the following implication holds:
if ? ? ?? for every ?? ? C , then ? ? min1(Hv).
An algorithm based on this lemma is shown in
Fig. 3. Its key function iter uses the notion of ac-
cumulating parameters. The parameter q is a map-
ping corresponding to the family (Pv | v ? V ) of
the lemma, i. e., q v = Pv; the parameter c is a
set corresponding to C . We begin in line 01 with
the function q0 mapping every vertex to the empty
list. According to the lemma, the candidates then
consist of the nullary edges.
As long as there are candidates left (line 04),
in a recursive call of iter the parameter q is up-
dated with the newly found pair (v, [?]) of ver-
tex v and (list of) best derivation ? (expressed by
51
Require ?-hypergraph H = (V,E), linear pre-
order - fulfilling SP and CP.
Ensure b v ? min1(Hv) for every v ? V
such that if b v == [e(?1, . . . , ?k)] for some
e = (v1 . . . vk, ?, v) ? E, then b vi == [?i] for
every i ? {1, . . . , k}.
01 b = iter q0 {(?, ?, v) ? E | ? ? ?(0)}
02 q0 v = []
03 iter q ? = q
04 iter q c = iter (q//(v,[?])) c?
05 where
06 ? = min c and ? ? Hv
07 c? =
?
e=(v1...vk,?,v)?E
q v == []
e(q v1, . . . ,q vk)
Figure 3: Algorithm solving the 1-best-derivation
problem.
q//(v,[?])) and the candidate set is recomputed
accordingly. When the candidate set is exhausted
(line 03), then q is returned.
Correctness and completeness of the algorithm
follow from Statements (ii) and (i) of Lemma 3,
respectively. Now we show termination. In every
iteration a new next best derivation is determined
and the candidate set is recomputed. This set only
contains candidates for vertices v ? V such that
q v == []. Hence, after at most |V | iterations
the candidates must be depleted, and the algorithm
terminates.
We note that the algorithm is very similar to that
of Knuth (1977). However, in contrast to the latter,
(i) it admits Hv = ? for some v ? V and (ii) it
computes some minimal derivation instead of the
weight of some minimal derivation.
Runtime According to the literature, the run-
time of Knuth?s algorithm is in O(|E| ? log|V |)
(Knuth, 1977). This statement relies on a number
of optimizations which are beyond our scope. We
just sketch two optimizations: (i) the candidate set
can be implemented in a way which admits ob-
taining its minimum in O(log|C|), and (ii) for the
computation of candidates, each edge needs to be
considered only once during the whole run of the
algorithm.
4.3 Solving the n-best-derivations problem
Being able to solve the 1-best-derivation problem,
we can now refine our specification of p. The re-
fined algorithm is given in Fig. 4; for the func-
tions not given there, please refer to Fig. 3 (func-
tion b) and to Fig. 2 (functions merge, tail, and
the top-concatenation). In particular, line 02 of
Fig. 4 shows the general way of ?pulling out? the
head element as it was indicated in Section 4.1 via
an example. We also remark that the definition of
the top-concatenation (lines 08?09 of Fig. 2) cor-
responds to the way in which multk was sped up
in Fig. 4 of (Huang and Chiang, 2005).
Theorem 4 The algorithm in Fig. 4 is correct with
respect to its require/ensure specification and it
terminates for every input.
PROOF (SKETCH). We indicate how induction on n
can be used for the proof. If n = 0, then the statement
is trivially true. Let n > 0. If b v == [], then the
statement is trivially true as well. Now we consider the
converse case. To this end, we use the following three
auxiliary statements.
(1) take n (merge {l1, . . . ,lk}) =
take n (merge {take n l1, . . . ,take n lk}),
(2) take n e(l1, . . . ,lk) =
take n e(take n l1, . . . ,take n lk),
(3) take n (tail l) = tail (take (n+1) l).
Using these statements, line 04 of Fig. 2, and line 02
of Fig. 4, we are able to ?pull? the take of take n (p
v) ?into? the right-hand side of p v, ultimately yield-
ing terms of the form take n (p vj) in the first line
of the merge application and take (n-1) (p v?j) in
the second one.
Then we can show the following statement by induc-
tion on m (note that the n is still fixed from the outer
induction): for every m ? N we have that if the tree
in b v has at most height m, then take n (p v) ?
minn(Hv). To this end, we use the following two aux-
iliary statements.
(4) For every sequence of pairwise disjoint sub-
sets P1, . . . , Pk ?
?
v?V Hv, sequence of nat-
ural numbers n1, . . . , nk ? N, and lists l1 ?
minn1(P1), . . . , lk ? minnk(Pk) such that
nj ? n for every j ? {1, . . . , k} we have that
take n (merge {l1, . . . , lk}) ? minn(P1?. . .?Pk).
(5) For every edge e = (v1 . . . vk, ?, v) ? E, subsets
P1, . . . , Pk ?
?
v?V Hv , and lists l1 ? minn(P1), . . . ,
lk ? minn(Pk) we have that take n e(l1, . . . , lk) ?
minn(e(P1, . . . , Pk)).
Using these statements, it remains to show that
{e(?1, . . . , ?k)} ? minn?1
(
(e(Hv1 , . . . , Hvk) \
{e(?1, . . . , ?k)}) ?
?
e? 6=e e?(Hv?1 , . . . , Hv?k)
)
?
minn(Hv) where b v = [e(?1, . . . , ?k)] and ?
denotes language concatenation. This can be shown by
using the definition of minn.
Termination of the algorithm now follows from the
fact that every finite prefix of p v is well defined. 
52
Require ?-hypergraph H = (V,E), linear pre-order - fulfilling SP and CP.
Ensure
(
take n (p v)
)
? minn(Hv) for every v ? V and n ? N.
01 p v = [] if b v == []
02 p v = e(?1, . . . , ?k):merge ({tail e(p v1, . . . , p vk) | e = (v1 . . . vk, ?, v) ? E} ?
{e?(p v?1, . . . , p v?k) | e? = (v?1 . . . v?k, ??, v) ? E, e? 6= e})
if b v == [e(?1, . . . , ?k)]
Figure 4: Algorithm solving the n-best-derivations problem.
4.4 Implementation, Complexity, and
Experiments
We have implemented the algorithm (consisting
of Figs. 3 and 4 and the auxiliary functions of
Fig. 2) in HASKELL. The implementation is
rather straightforward except for the following
three points.
(1) Weights: we assume that - is defined by
means of weights (cf. Ex. 2), and that comparing
these weights is in O(1) (which often holds be-
cause of limited precision). Hence, we store with
each derivation its weight so that comparison ac-
cording to - is in O(1) as well.
(2) Memoization: we use a memoization tech-
nique to ensure that no derivation occurring in p v
is computed twice.
(3) Merge: the merge operation deserves some
consideration because it is used in a nested fash-
ion, yielding trees of merge applications. This
leads to an undesirable runtime complexity be-
cause these trees need not be balanced. Thus, in-
stead of actually computing the merge in p and in
the top-concatenation, we just return a data struc-
ture describing what should be merged. That data
structure consists of a best element and a list of
lists of derivations to be merged (cf. lines 06 and
09 in Fig. 2). We use a higher-order function to
manage these data structures on a heap, perform-
ing the merge in a nonnested way.
Runtime Here we consider the n-best part of the
algorithm, i. e. we assume the computation of the
mapping b to take constant time. Note however
that due to memoization, b is only computed once.
Then the runtime complexity of our implementa-
tion is in O
(
|E|+ |V | ?n ? log(|E|+n)
)
. This can
be seen as follows.
By line 02 in Fig. 4, the initial heaps in the
higher-order merge described under (3) have a to-
tal of |E| elements. Building these heaps is thus
in O(|E|). By line 09 in Fig. 2, each newly found
derivation spawns at most as many new candidates
n total time [s] time for n-best part [s]
1 8.713 ?
25 000 10.832 2.119
50 000 12.815 4.102
100 000 16.542 7.739
200 000 24.216 15.503
Table 1: Experimental results
on the heap as the maximum rank in ?. We assume
this to be constant. Moreover, at most n deriva-
tions are computed for each node, that is, at most
|V |?n in total. Hence, the size of the heap of a node
is in O(|E|+n). For each derivation we compute,
we have to pop the minimal element off the heap
(cf. line 07 in Fig. 2), which is in O(log(|E|+n)),
and we have to compute the union of the remaining
heap with the newly spawned candidates, which
has the same complexity.
We give another estimate for the total number
of derivations computed by the algorithm, which
is based on the following observation. When pop-
ping a new derivation ? off the heap, new next best
candidates are computed. This involves comput-
ing at most as many new derivations as the number
of nodes of ?, because for each hyperedge occur-
ring in ? we have to consider the next best alter-
native. Since we pop off at most n elements from
the heap belonging to the target node, we arrive at
the estimate d ?n, where d is the size of the biggest
derivation of said node.
A slight improvement of the runtime complex-
ity can be obtained by restricting the heap size to
n best elements, as argued by Huang and Chiang
(2005). This way, they are able to obtain the com-
plexity O(|E| + d ? n ? log n).
We have conducted experiments on an Intel
Core Duo 1200 MHz with 2 GB of RAM using
a cyclic hypergraph containing 671 vertices and
12136 edges. The results are shown in Table 1.
This table indicates that the runtime of the n-best
part is roughly linear in n.
53
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inform. Process. Lett., 24(1):1?4.
Jean Berstel and Christophe Reutenauer. 1982. Recog-
nizable formal power series on trees. Theoret. Com-
put. Sci., 18(2):115?148.
Matthias Bu?chse, Jonathan May, and Heiko Vogler.
2009. Determinization of weighted tree automata
using factorizations. Talk presented at FSMNLP 09
in Pretoria, South Africa.
Zolta?n ?Esik and Werner Kuich. 2003. Formal tree se-
ries. J. Autom. Lang. Comb., 8(2):219?285.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9. Springer.
Joshua Goodman. 1999. Semiring parsing. Comp.
Ling., 25(4):573?605.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Parsing ?05: Proceedings of the
Ninth International Workshop on Parsing Technol-
ogy, pages 53?64. ACL.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT, pages
123?134.
Donald E. Knuth. 1977. A Generalization of Dijkstra?s
Algorithm. Inform. Process. Lett., 6(1):1?5, Febru-
ary.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL-IJCNLP ?09, pages 593?601.
ACL.
Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Conf. Parsing Technologies, pages 1?12. ACL.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In Proc. HLT, pages 351?358. ACL.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Comp. Ling., 29(1):135?
143.
Lars Relund Nielsen, Kim Allan Andersen, and
Daniele Pretolani. 2005. Finding the k shortest hy-
perpaths. Comput. Oper. Res., 32(6):1477?1497.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302.
Adam Pauls and Dan Klein. 2009. k-best a* parsing.
In Proc. ACL-IJCNLP ?09, pages 958?966, Morris-
town, NJ, USA. ACL.
J. W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization
of finite automata theory. J. Comput. Syst. Sci.,
1(4):317?322.
54
Proc. EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 11?20,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Deciding the Twins Property for
Weighted Tree Automata over Extremal Semifields
Matthias B?chse and Anja Fischer
Department of Computer Science
Technische Universit?t Dresden
01062 Dresden, Germany
Matthias.Buechse@tu-dresden.de
Abstract
It has remained an open question whether
the twins property for weighted tree au-
tomata is decidable. This property is crucial
for determinizing such an automaton, and
it has been argued that determinization im-
proves the output of parsers and translation
systems. We show that the twins property
for weighted tree automata over extremal
semifields is decidable.
1 Introduction
In natural-language processing (NLP), language
and translation are often modeled using some
kind of grammar, automaton or transducer, such
as a probabilistic context-free grammar, a syn-
chronous context-free grammar, a weighted tree
automaton, or a tree transducer, among others
(May and Knight, 2006; Petrov et al, 2006; Chi-
ang, 2007; Graehl, Knight and May, 2008; Zhang
et al, 2008; Pauls and Klein, 2009). In statisti-
cal NLP, the structure of the grammar is extracted
heuristically from a large corpus of example sen-
tences or sentence pairs, and the rule weights are
estimated using methods from statistics or ma-
chine learning.
In general, a grammar such as those named
above will be ambiguous, i.e., offering several
ways of deriving the same object (sentence or sen-
tence pair). While the derivation of an object is
crucial to the intrinsics of a system, it is neither
relevant to the user nor observed in the corpus.
Hence, we speak of spurious ambiguity (Li, Eis-
ner and Khudanpur, 2009).
As a consequence, the true importance of an
object can only be assessed by aggregating all
its derivations. Unfortunately, this proves com-
putationally intractable in almost all cases: for
instance, finding the best string of a probabilis-
tic regular grammar is NP hard (Sima?an, 1996;
Casacuberta and de la Higuera, 2000). Finding
the best derivation, on the other hand, is possible
in polynomial time (Eppstein, 1998; Huang and
Chiang, 2005), and thus, most NLP systems ap-
proximate the importance of an object by its best
derivation (Li, Eisner and Khudanpur, 2009).
There is, however, a line of research that deals
with the costly aggregating approach, and it is
closely related to determinization techniques from
automata theory.
For instance, May and Knight (2006) argue
that the output of a parser or syntax-based trans-
lation system can be represented by a weighted
tree automaton (wta), which assigns a weight to
each parse tree. Under some circumstances, the
wta can be determinized, yielding an equivalent,
but unambiguous wta, which offers at most one
derivation for each object. Then the weight of an
object is equal to the weight of its derivation, and
the aforementioned polynomial-time algorithms
deliver exact results.
The caveat of the determinization approach is
that deterministic weighted automata are strictly
less powerful than their general counterparts,
i.e., not every automaton can be determinized.
B?chse, May and Vogler (2010) give a review of
known sufficient conditions under which deter-
minization is possible. One of these conditions
requires that (i) the weights are calculated in an
extremal semiring, (ii) there is a maximal factor-
ization, and (iii) the wta has the twins property.1
1Items (i) and (iii) guarantee that the wta only computes
weight vectors that are scalar multiples of a finite number
11
Regarding (i), we note that in an extremal semi-
ring the weight of a parse tree is equal to the
weight of its best derivation. It follows that, while
the determinized wta will have at most one deriva-
tion per parse tree, its weight will be the weight of
the best derivation of the original wta. The benefit
of determinization reduces to removing superflu-
ous derivations from the list of best derivations.
Regarding (ii), the factorization is used in
the determinization construction to distribute the
weight computation in the determinized automa-
ton between its transition weights and its state be-
havior. A maximal factorization exists for every
zero-sum free semifield.
Regarding (iii), the question whether the twins
property is decidable has remained open for a long
time, until Kirsten (2012)2 gave an affirmative an-
swer for a particular case: weighted string au-
tomata over the tropical semiring. He also showed
that the decision problem is PSPACE-complete.
In this paper, we close one remaining gap by
adapting and generalizing Kirsten?s proof: we
show that the twins property is decidable for wta
over extremal semifields (Theorem 3.1). We pro-
ceed by recalling the concepts related to deter-
minizing wta, such as ranked trees, semirings,
factorizations, wta themselves, and the twins
property (Sec. 2). Then we show our main the-
orem, including two decision algorithms (Sec. 3).
Finally, we conclude the paper with a discussion
and some open questions (Sec. 4).
2 Preliminaries
2.1 Ranked Trees
A ranked alphabet is a tuple (?, rk) where ? is an
alphabet, i.e., a finite set, and rk : ?? N assigns
an arity to every symbol ? ? ?. Throughout this
paper we will identify (?, rk) with ?. For every
k ? N the set ?(k) = {? ? ? | rk(?) = k}
contains all symbols of arity k.
Let H be a set and ? a ranked alphabet. The
set T?(H) of trees over ? indexed by H is de-
fined inductively as the smallest set T such that:
(i) H ? T and (ii) ?(?1, . . . , ?k) ? T for every
of vectors corresponding to a set of height-bounded trees,
while Item (ii) ensures that the latter vectors suffice as the
states of the constructed deterministic wta; cf. (B?chse, May
and Vogler, 2010, Lm. 5.9 and Lm. 5.8, respectively).
2A manuscript with the same content has been available
on Daniel Kirsten?s website for a year from Sept. 2010 on.
k ? N, ? ? ?(k), and ?1, . . . , ?k ? T . We write
T? instead of T?(?).
For every ? ? T?(H), we define the set
pos(?) ? N? of positions of ? by
(i) if ? ? H , then pos(?) = {?};
(ii) if ? = ?(?1, . . . , ?k), then pos(?) = {?} ?
{i ? w | i ? {1, . . . , k}, w ? pos(?i)}.
The mapping ht: T?(H) ? N maps each tree ?
to its height, i.e., the length of a longest position
of ?. We denote the label of ? at position w by
?(w), the subtree of ? rooted at w by ?|w, and the
tree obtained by replacing the subtree of ? rooted
at position w with ??, ?? ? T?(H), by ?[??]w.
A ?-context is a tree in T?({z}) that contains
exactly one occurrence of the special symbol z.
The set of all ?-contexts is denoted by C?. Let
? ? T??C? and ? ? C?. Then the concatenation
of ? and ?, denoted by ? ? ?, is obtained from ? by
replacing the leaf z by ?. If ? ? T?, then so is
? ? ?, and likewise for ? ? C?.
2.2 Semirings
A semiring (Hebisch and Weinert, 1998; Golan,
1999) is a quintuple S = (S,+, ?, 0, 1) where S
is a set, + and ? are binary, associative operations
on S, called addition and multiplication, respec-
tively, + is commutative, ? distributes over + from
both sides, 0 and 1 are elements of S, 0 is neu-
tral with respect to +, 1 is neutral with respect
to ?, and 0 is absorbing with respect to ? (i.e.,
s ? 0 = 0 = 0 ? s).
Let S = (S,+, ?, 0, 1) be a semiring. In nota-
tion, we will identify S with S. We call S commu-
tative if the multiplication is commutative; a semi-
field if it is commutative and for every a ? S\{0}
there is an a?1 ? S such that a ? a?1 = 1; zero-
sum free if a + b = 0 implies a = b = 0; zero-
divisor free if a ? b = 0 implies a = 0 or b = 0;
and extremal (Mahr, 1984) if a + b ? {a, b}. We
note that every extremal semiring is also zero-sum
free and every semifield is zero-divisor free.
Example 2.1 We present four examples
of semirings. The Boolean semiring
B = ({0, 1},?,?, 0, 1), with disjunction
and conjunction, is an extremal semifield. The
formal-language semiring (P(??),?, ?, ?, {?})
over an alphabet ?, with union and language
concatenation, is neither commutative nor ex-
tremal, but zero-divisor free and zero-sum free.
The tropical semiring (R ? {?},min,+,?, 0),
with minimum and conventional addition, is
12
an extremal semifield. The Viterbi semiring
([0, 1],max, ?, 0, 1) is a commutative, extremal,
zero-divisor-free semiring, but not a semifield. 2
Let Q be a set. The set SQ contains all map-
pings u : Q ? S, or, equivalently, all Q-vectors
over S. Instead of u(q) we also write uq to de-
note the q-component of a vector u ? SQ. The
Q-vector mapping every q to 0 is denoted by 0?.
For every q ? Q we define eq ? SQ such that
(eq)q = 1, and (eq)p = 0 for every p 6= q.
2.3 Factorizations
We use the notion of a factorization as defined in
(Kirsten and M?urer, 2005).
Let Q be a nonempty finite set. A pair (f, g) is
called a factorization of dimension Q if f : SQ \
{0?} ? SQ, g : SQ\{0?} ? S, and u = g(u)?f(u)
for every u ? SQ \ {0?}. A factorization (f, g) is
called maximal if for every u ? SQ and a ? S,
we have that a ? u 6= 0? implies f(a ? u) = f(u).
Example 2.2 Let Q be a nonempty finite set. We
show three factorizations of dimension Q.
If S is an arbitrary semiring, g(u) = 1 and
f(u) = u constitute the trivial factorization. It
is not maximal in general.
If S is a zero-sum free semifield, such as the
tropical semiring or the semifield of non-negative
reals, then g(u) =
?
q?Q uq and f(u) =
1
g(u) ?
u constitute a factorization (B?chse, May and
Vogler, 2010, Lemma 4.2). It is maximal: f(a ?
u) = 1g(a?u) ? (a ? u) =
1
a?g(u) ? a ? u = f(u).
As shown in (B?chse, May and Vogler, 2010,
Lemma 4.4) a maximal factorization only exists
if S is zero-divisor free or |Q| = 1.
2.4 Weighted Tree Automata
A weighted tree automaton (?sik and Kuich,
2003) is a finite-state machine that represents a
weighted tree language, i.e., a mapping ? : T? ?
S. It assigns a weight to every tree based on
weighted transitions.
Formally, a weighted tree automaton (wta) is
a tuple A = (Q,?, S, ?, ?) such that Q is a
nonempty finite set (of states), ? is a ranked al-
phabet, S is a semiring, ? is the transition map-
ping, mapping transitions (q1 ? ? ? qk, ?, q) into S
where q1, . . . , qk, q ? Q and ? ? ?(k), and
? ? SQ maps every state to its root weight.
A wta A is bottom-up deterministic if for ev-
ery (q1 ? ? ? qk, ?), there is at most one q such that
?(q1 ? ? ? qk, ?, q) 6= 0.
Example 2.3 Let A = (Q,?, S, ?, ?) be the wta
where ? = {?(0), ?(1), ?(2)}, S is the arctic semi-
ring (N ? {??},max,+,??, 0), ? is given by
the directed functional hypergraph in Fig. 1, and
? = (0,??). Each node in the hypergraph
(drawn as circle) corresponds to a state, and each
hyperedge (drawn as box with arbitrarily many in-
going arcs and exactly one outgoing arc) repre-
sents a weighted transition. Ingoing arcs of a hy-
peredge are meant to be read counter-clockwise,
starting from the outgoing arc. The final weight 0
of q1 is indicated by an additional arc. Transitions
not shown have the weight ??. 2
q1 q2?/0
0
?/0
?/1
?/1
?/0
?/0?/1
Figure 1: Hypergraph representation of wta A.
Typically, wta are given initial-algebra seman-
tics (Goguen et al, 1977). In this paper, we use
the equivalent run semantics (F?l?p and Vogler,
2009, Sec. 3.2) as it constitutes the basis for our
proofs. In this setting, every node of a given tree is
decorated with a state; this decoration is called a
run. The label of a node, its state, and the states of
its successors comprise a transition. The weight
of a run is given by the product of the weights
of all these transitions (under ?), calculated in the
semiring S. Roughly speaking, the weight of a
tree is then the sum of the weights of all runs on
that tree, again calculated in S.
Now we formalize the notions of a run and
its weight. For our proofs, we will need runs
and their weights to be as easily composable and
decomposable as trees and contexts. Therefore,
we will consider trees indexed by semiring ele-
ments and even Q-vectors over S. Let H be a set,
? ? T?(H), and q ? Q. The setR
q
A(?) of all runs
on ? that end in state q at the root of ? is
RqA(?) = {(?, ?) | ? : pos(?)? Q, ?(?) = q} .
13
?q1
?q2 ?q1
?q1 ?q2
?q1
?q1
Figure 2: A tree together with a run.
We will denote the pair (?, ?) just by ? and indi-
cate ? by stating ? ? RqA(?). We will also omit
the subscript A. We set R(?) =
?
q?QR
q(?).
Let w ? pos(?) and ? ? Rq(?). The fol-
lowing notions are defined in the obvious way:
(i) ?|w ? R?(w)(?|w), (ii) ?[??]w ? Rq(?[??]w)
for every ?? ? T?(H) and ?? ? R?(w)(??), and
(iii) ? ? ?? ? Rq
?
(? ? ?) for every q? ? Q, ? ? C?,
and ?? ? Rq
?
(?) that maps the z-labelled posi-
tion to q. We will abuse the above notation in
two ways: (i) we write ?[z]w to denote ?[??]w
where ?? is the only element of R?(w)(z), and
(ii) for every s ? S, we write s ? ? to denote the
run on s ? ? which coincides with ?.
Let ? ? T?(S ? SQ) and ? ? R(?). We define
the weight ???A ? S of ? as follows (omitting the
subscript A): if ? ? S, then ??? = ?; if ? ? SQ,
then ??? = ??(?); if ? = ?(?1, . . . , ?k), then ??? =
??|1? ? . . . ? ??|k? ? ?(?(1) ? ? ??(k), ?, ?(?)).
We define the mapping J.KA : T?(SQ) ? SQ
such that J?KA(q) =
?
??Rq(?)???. Again, we
will often omit the subscript A. If we have a fac-
torization (f, g), we will shorten f(J?K) to fJ?K.
We will often use relationships such as ?? ? ??? =
???? ? ??? and J? ? ?K = JJ?K ? ?K.
The weighted tree language run-recognized
by A is the mapping ?A : T? ? S such that for
every ? ? T? we have ?A(?) =
?
q?QJ?Kq ? ?q.
Example 2.4 (Ex. 2.3 contd.) Figure 2 shows a
tree together with a run ?. We compute ??? (recall
that we use the arctic semiring):
??? = ??|1?+ ??|2?+ ?(q2q1, ?, q1)
= ?(?, ?, q2) + ?(?, ?, q1) + ?(q1, ?, q1)
+ ?(q1, ?, q1) + ?(?, ?, q2)
+ ?(q1q2, ?, q1) + ?(q2q1, ?, q1)
= 0 + 0 + 1 + 1 + 0 + 1 + 1 = 4 .
It can be shown that J?Kq1 = ht(?) and J?Kq2 =
0, and thus, that ?A = ht. 2
For every ? ? T?(SQ) and ? ? R(?) we call
? victorious (on ?) if ??? = J?K?(?). The follow-
ing observations are based on (B?chse, May and
Vogler, 2010, Obs. 5.11 and 5.12).
Observation 2.5 Let S be an extremal semiring.
For every ? ? T?(SQ) and q ? Q there is a ? ?
Rq(?) such that ? is victorious.
Observation 2.6 Let ? ? T?(SQ), w ? pos(?),
and ? ? R(?) victorious. Then we obtain ??? =
J(??|w? ? e?(w)) ? ?[z]wK?(?).
PROOF.
J(??|w? ? e?(w)) ? ?[z]wK?(?)
=
?
???R?(?)
(
(??|w??e?(w))??[z]w
)????
=
?
???R?(?)(?[z]w),??(w)=?(w)???|w? ? ?
??
=
?
???R?(?)(?[z]w),??(w)=?(w)??|w ? ?
??
= ??? .
For the last equation, we note that the summands
on the left-hand side form a subset of {??? | ? ?
R?(?)(?)}, which contains ???. Since S is ex-
tremal and ??? = J?K?(?), the equation holds. 
2.5 Twins Property
We define two binary relations SIBLINGS(A) and
TWINS(A) overQ as follows. Let p, q ? Q. Then
? (p, q) ? SIBLINGS(A) iff there is a tree ? ?
T? such that J?Kp 6= 0 and J?Kq 6= 0.
? (p, q) ? TWINS(A) iff for every context ? ?
C? we have that Jep ??Kp 6= 0 and Jeq ??Kq 6=
0 implies Jep ? ?Kp = Jeq ? ?Kq.
The wta A is said to have the twins property if
SIBLINGS(A) ? TWINS(A).
Example 2.7 We cover two examples.
First, consider the wta from Ex. 2.3. Its two
states are siblings as witnessed by the tree ? = ?.
However, they are not twins, as witnessed by the
context ? = ?(z): Jeq1 ? ?(z)Kq1 = 1, whereas
Jeq2 ? ?(z)Kq2 = 0.
Second, consider the wta over the Viterbi semi-
ring shown Fig. 3. Its two states are siblings as
witnessed by the tree ? = ?. Furthermore, they
are twins because their transitions are symmetric.
Hence, this wta has the twins property. 2
The following observation shows that we can
enumerate SIBLINGS(A) in finite time.
14
q1 q2
?/0.5 ?/0.5
?/0.5
?/0.5
?/0.5
?/0.5
Figure 3: Siblings and twins.
Observation 2.8 If S is zero-sum free, we have
SIBLINGS(A) = SIB(A) where SIB(A) is de-
fined like SIBLINGS(A), with the additional con-
dition that ht(?) < |Q|2.
PROOF. The direction ? is trivial. We show ?
by contradiction. Let p, q ? Q and ? ? T? such
that (i) J?Kp 6= 0 and J?Kq 6= 0, and (ii) (p, q) 6?
SIB(A). We assume that ? is smallest, and we
show that we find a smaller counterexample.
By (ii), we have (iii) ht(?) ? |Q|2. By (i),
there are ?p ? Rp(?) and ?q ? Rq(?) such that
(iv) ??p? 6= 0 and ??q? 6= 0.
By (iii), there are positions w1, w2 such that w1
is above w2, ?p(w1) = ?p(w2), and ?q(w1) =
?q(w2). Cutting out the slice between w1 and w2,
we construct the tree ?? = ?[?|w2 ]w1 . Moreover,
we construct the runs ??p and ?
?
q accordingly, i.e.,
??x = ?x[?x|w2 ]w1 .
We have that ???p? 6= 0, ??
?
q? 6= 0, because oth-
erwise (iv) would be violated. Since S is zero-
sum free, we obtain J??Kp 6= 0, J??Kq 6= 0. 
3 Decidability of the Twins Property
This section contains our main theorem:
Theorem 3.1 The twins property of wta over ex-
tremal semifields is decidable.
The following subsections provide the infra-
structure and lemmata needed for the proof of the
theorem. Henceforth, we assume that S is an ex-
tremal semifield. As noted in Ex. 2.2, there is a
maximal factorization (f, g).
3.1 Rephrasing the Twins Relation
In the definition of TWINS(A), we deal with two
vectors Jep ? ?K and Jeq ? ?K for each ? ? C?. In
the following we concatenate these vectors into
one, which enables us to use a factorization. To
this end, we construct a wta A ? A? that runs two
instances of A in parallel, as shown in Fig. 4.
Let A = (Q,?, S, ?, ?) a wta and A? =
(Q?,?, S, ??, ??) be the wta obtained from A by re-
naming states via q 7? q?. We construct the wta
A? A? = (Q ? Q?,?, S, ??, ? ?) where ?? coincides
with ? and ?? on the transitions of A and A?, re-
spectively; it maps all other transitions to 0; and ? ?
coincides with ? and ?? on Q and Q?, respectively.
For every p, q ? Q we define the set Tp,q ?
SQ?Q? by Tp,q = {J(ep + eq?) ? ?KA?A? | ? ? C?};
note that ep, eq? ? SQ?Q?. With this definition, we
observe the following trivial equivalence.
Observation 3.2 Let p, q ? Q. Then (p, q) ?
TWINS(A) iff for every u ? Tp,q we have that
up 6= 0 and uq? 6= 0 implies up = uq?.
For every pair (p, q) ? SIBLINGS(A), a vector
u ? SQ?Q? is called a critical vector (for (p, q))
if it does not fulfill the centered implication of
Obs. 3.2. Any critical vector in Tp,q thereby wit-
nesses (p, q) 6? TWINS(A). Consequently, A has
the twins property iff Tp,q contains no critical vec-
tor for any (p, q) ? SIBLINGS(A). Deciding the
twins property thus amounts to searching for a
critical vector.
3.2 Compressing the Search Space
In this subsection we approach the decidability
of the twins property by compressing the search
space for critical vectors. First we show that the
vectors in Tp,q are scalar multiples of a finite num-
ber of vectors.
Lemma 3.3 Let S be a commutative, extremal
semiring. Assume that A has the twins property.
Then there is a finite set S? ? SQ?Q? such that for
every (p, q) ? SIBLINGS(A) we have
Tp,q ? S ? S
?.
PROOF. We construct sets S?, S?? ? SQ?Q? and
show the following inclusions:
Tp,q ? S ? S
?? ? S ? S?. (?)
15
? :
(ep + eq?)
z
p?
Jep??KA
? ?? ?
?
?
?
?
...
...
?
?
?
?
p?
?
?
?
?
...
1
...
?
?
?
?
? ?? ?
ep
A
q ?
Jeq??KA
? ?? ?
?
?
?
?
...
...
?
?
?
?
q ?
?
?
?
?
...
1
...
?
?
?
?
? ?? ?
eq
A
J(ep+eq?)??KA?A??Tp,q
? ?? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
...
...
...
...
?
?
?
?
?
?
?
?
?
?
?
?
?
p?
q? ?
?
?
?
?
?
Q
?
?
?
?
?
Q?
?
?
?
?
?
?
?
?
?
?
?
?
?
...
1
...
...
1
...
?
?
?
?
?
?
?
?
?
?
?
?
?
? ?? ?
(ep+eq?)
p?
q? ?
A ? A?
Figure 4: Moving from parallel execution of A (left-hand side) to the union wta A ? A? (right-hand side).
To this end, we consider each entry in each vector
to be induced by an according (victorious) run.
In this spirit we define for every p, q ? Q and
? ? C? the set Cp,q(?) ? R((ep + eq?) ? ?)Q?Q? of
vectors of runs of A ? A? as follows: ? ? Cp,q(?)
iff (i) ?r ? Rr((ep + eq?) ? ?) for every r ? Q? Q?
and (ii) for every pair w1, w2 ? pos(?) with w1
above w2 and ?r(w1) = ?r(w2) we have that
?r|w1 is victorious on ((ep + eq?) ? ?)|w1 . We map
each vector of runs to the corresponding weight
vector as follows. For every Q? ? Q ? Q? let
?Q? : R((ep + eq?) ? ?)Q?Q? ? SQ?Q? be the map-
ping such that for every ? and q? ? Q ? Q?:
?Q?(?)q? =
{
??q?? if q? ? Q?
0 otherwise.
We set S?? = {?Q?(?) | (p, q) ? SIBLINGS(A),
? ? C?, ? ? Cp,q(?), Q? ? Q? Q?}. The set S? is
defined in the same way, with the additional con-
dition that ht(?) < 2|Q|2|Q|.
The first inclusion of (?) can be proved in the
same way as (B?chse, May and Vogler, 2010,
Lemma 5.14). Here we show the second inclu-
sion by contradiction. To this end, let s ? S,
(p, q) ? SIBLINGS(A), ? ? C?, ? ? Cp,q(?),
and Q? ? Q ? Q? such that s ? ?Q?(?) 6? S ? S?,
and thus ht(?) ? 2|Q|2|Q|. We can assume that
??r? 6= 0 for every r ? Q? because otherwise we
could adjustQ? without harm. Finally, we assume
that ? is smallest.
We will construct a new context ? ? and a cor-
responding vector ?? ? Cp,q(? ?) such that ? ? is
smaller than ? and s ? ?Q?(?) = s ? s? ? ?Q?(??)
for some s? ? S. Then, if the right-hand side is in
S ? S?, so is the left-hand side. By contraposition,
this shows that ? was not a smallest counterexam-
ple, yielding the contradiction.
First, let w be the position in ? labelled z. We
show that we are able to find a pair (w1, w2) of
positions such that w1 is above w2, ?r(w1) =
?r(w2) for every r, and either both or none of w1
and w2 are above w. To this end, we distinguish
two cases (cf. Fig. 5).
(a) If |w| ? |Q|2|Q|, then the length of the com-
mon prefix of w and any path of length at least
2|Q|2|Q| can be at most |Q|2|Q|. Hence, on such a
path remain at least |Q|2|Q| + 1 positions that are
not above w. By the pidgeonhole principle, we
find said pair (w1, w2).
(b) If |w| > |Q|2|Q|, then we find the pair im-
mediately on the path to the position labelled z.
Second, we pick a pair (w1, w2) such that the
position w1 has minimal length. Cutting out the
slice between the positions w1 and w2 yields the
smaller context ? ? = ?[?|w2 ]w1 . We construct ?
?
accordingly, i.e., ??r = ?r[?r|w2 ]w1 for every r ?
Q ? Q?. We have that ?? ? Cp,q(? ?); for this we
16
(a)
? :
w1
w2? ??
z
(ep + eq?)
=?
? ? :
w1
z
(ep + eq?)
(b)
? :
w1
w2? ??
z
(ep + eq?)
=?
? ? :
w1
z
(ep + eq?)
Figure 5: Two cases for the construction of ? ? = ?[?|w2 ]w1 .
need that we chose w1 with minimal length.
Third, we use the twins property to show that
there is an s? ? S such that s ? ?Q?(?) = s ? s? ?
?Q?(??). If Q? = ?, we set s? = 0, and the proof is
done. Otherwise we choose some r? ? Q? and set
s? = Je?r? (w2)??
??K?r? (w1) where ?
?? = ?[z]w2 |w1 is
the slice we have cut out. We prove that ?Q?(?) =
s???Q?(??). To this end, let r ? Q?, p? = ?r(w1) =
?r(w2), and q? = ?r?(w1) = ?r?(w2). Then
?Q?(?)r = ??r? = ???r|w1? ? ?r[z]w1?
= ?J(??r|w2? ? ep?) ? ?
??Kp? ? ?r[z]w1? (Obs. 2.6)
= ??r|w2? ? Jep? ? ?
??Kp? ? ?1 ? ?r[z]w1?
(commutativity)
= ??r|w2? ? Jeq? ? ?
??Kq? ? ?1 ? ?r[z]w1? (?)
= s? ? ???r|w2? ? ?r[z]w1? (commutativity)
= s? ? ???r? = s
? ? ?Q?(?
?)r .
At (?) we have used the twins property. We show
that this is justified. First, we show that (p?, q?) ?
SIBLINGS(A ? A?). To this end, we distinguish
two cases.
If z occurs in ?|w2 : by (p, q) ? SIBLINGS(A)
we obtain a tree ? such that J?Kp 6= 0 and J?Kq 6=
0. By our assumption we have ??r? 6= 0, ??r?? 6=
0, and thus, ??r|w2? 6= 0, ??r? |w2? 6= 0. Since S
is extremal, and thus, zero-sum free, we obtain
J? ? ?|w2Kp? 6= 0, J? ? ?|w2Kq? 6= 0.
If z does not occur in ?|w2 : we derive in a sim-
ilar fashion that ??r|w2? 6= 0, ??r? |w2? 6= 0, and
thus, J?|w2Kp? 6= 0, J?|w2Kq? 6= 0.
Second, by the twins property, we have that
(p?, q?) ? TWINS(A ? A?). Using again that
??r? 6= 0, ??r?? 6= 0, we derive ??r[z]w2 |w1? 6= 0,
??r? [z]w2 |w1? 6= 0. Hence, Jep? ? ?
??Kp? 6= 0,
Jeq? ? ? ??Kq? 6= 0. Consequently, we have (?). 
We note that u ? SQ?Q?, u 6= 0?, is a critical
vector iff f(u) is a critical vector. Hence, ap-
plying the factorization to Tp,q for every (p, q) ?
SIBLINGS(A) results in a compressed search
space for critical vectors. It follows from the pre-
ceding lemma that the resulting search space is
finite.
Lemma 3.4 Let (f, g) be a maximal factorization
of dimension Q? Q?. Assume thatA has the twins
property. For every (p, q) ? SIBLINGS(A) the set
f(Tp,q \ {0?}) is finite.
PROOF. By Lemma 3.3 there is a finite set S? with
f(Tp,q \ {0?}) ? f(S ? S
?) ? f(S?) ,
where we used that (f, g) is maximal. Since S? is
finite, so is f(Tp,q \ {0?}). 
17
Algorithm 1 Decision algorithm
Require: A = (Q,?, S, ?, ?) a wta, S commu-
tative, extremal, (f, g) maximal factorization
Ensure: print ?yes? iff A has the twins property
1: compute SIBLINGS(A)
2: for (p, q) ? SIBLINGS(A) in parallel do
3: for u ? f(Tp,q \ {0?}) do
4: if u is a critical vector then
5: print ?no? and terminate
6: print ?yes?
3.3 Two Decision Algorithms
In this section we consider two decision algo-
rithms. The first one is part of the following proof.
PROOF (OF THM. 3.1). Algorithm 1 proceeds as
follows. First, it enumerates SIBLINGS(A). This
is possible as shown by Obs. 2.8. Second, for each
(p, q) ? SIBLINGS(A) in parallel, it enumerates
f(Tp,q \ {0?}), checking for critical vectors. For
this step, we distinguish two cases.
Either A has the twins property. Then, by
Lemma 3.4, f(Tp,q \ {0?}) is finite, and the algo-
rithm will terminate without finding any critical
vector, in which case it outputs ?yes?.
OrA does not have the twins property, but then,
by Obs. 3.2, the algorithm is guaranteed to find
a critical vector at some point, in which case it
outputs ?no?. Note that the parallel processing
(line 2) is critical in this case because there may
be (p, q) ? SIBLINGS(A) such that f(Tp,q \ {0?})
is infinite, but does not contain a critical vector. 
Note that Algorithm 1 basically enumerates the
set
?
(p,q)?SIBLINGS(A) f(Tp,q \ {0?}). In principle,
this can be done by enumerating C? and comput-
ing fJ(ep + eq?) ? ?K for each ? ? C?. However,
the computation of weights already done for sub-
contexts of ? is not reused in this approach.
In the following we show an alternative proce-
dure (Algorithm 2) that does not enumerate C?
explicitly but works on weight vectors instead,
thereby avoiding redundant calculation. This pro-
cedure maintains a pair of subsets of SQ?Q?. It
begins with (?, ?) and keeps adding vectors by
applying a monotone operation F until either the
second component contains a critical vector or no
new vectors are added.
To this end, we define the unary operation F
over pairs of subsets of SQ?Q? by (T,C) 7?
Algorithm 2 Improved decision algorithm
Require: A = (Q,?, S, ?, ?) a wta, S commu-
tative, extremal, (f, g) maximal factorization
Ensure: print ?yes? iff A has the twins property
1: compute SIBLINGS(A)
2: (T,C)? (?, ?)
3: repeat
4: (T ?, C ?)? (T,C)
5: (T,C)? F (T ?, C ?) . uses SIBLINGS(A)
6: until C contains critical vector or C = C ?
7: if critical vector has been found then
8: print ?no?
9: else
10: print ?yes?
(T ?, C ?) where T ? and C ? contain exactly the fol-
lowing elements:
(F1) for every k ? 0, ? ? ?(k), and
u1, . . . , uk ? T , if J?(u1, . . . , uk)K 6= 0?,
then fJ?(u1, . . . , uk)K ? T ?,
(F2) for every (p, q) ? SIBLINGS(A), we have
f(ep + eq?) ? C ?,
(F3) for every k ? 1, ? ? ?(k), i ? {1, . . . , k},
ui ? C, and u1, . . . , ui?1, ui+1, . . . , uk ?
T , if J?(u1, . . . , uk)K 6= 0?, then
fJ?(u1, . . . , uk)K ? C ?.
Kleene?s fixpoint theorem (Wechler, 1992,
Sec. 1.5.2, Theorem 7) yields that F has a least
fixpoint (where we use the pointwise subset or-
der), and that it can be calculated by the satura-
tion procedure outlined above. In the forthcoming
Lemma 3.6, we show that said fixpoint contains
the desired set
?
(p,q)?SIBLINGS(A) f(Tp,q \ {0?}).
This implies both the correctness of our procedure
and its termination, by the same line of reasoning
as for Algorithm 1. As a preparation we recall two
auxiliary statements.
Observation 3.5 Let S be commutative and
(f, g) maximal. Then for every k ? 0, ? ? ?(k),
and ?1, . . . , ?k ? T?(SQ), we have that
J?(?1, . . . , ?k)K = J?(J?1K, . . . , J?kK)K and
fJ?(J?1K, . . . , J?kK)K = fJ?(fJ?1K, . . . , fJ?kK)K.
PROOF. By (F?l?p and Vogler, 2009, Sec 3.2)
and (B?chse, May and Vogler, 2010, Lemma 5.5),
respectively. 
Lemma 3.6 Let (T f , Cf ) be the least fixpoint
of F . Then (i) T f = f(JT?K \ {0?}) and (ii) Cf =?
(p,q)?SIBLINGS(A) f(Tp,q \ {0?}).
18
PROOF. In this proof we will often use Obs. 3.5.
For ??? of Statement (i), we refer to (B?chse,
May and Vogler, 2010, Lemma 5.8).
We prove ??? of Statement (i) by contradic-
tion. To this end, let ? ? T? a smallest tree such
that J?K 6= 0? and fJ?K 6? T f . By definition of T?,
there are k ? 0, ? ? ?(k), and ?1, . . . , ?k ? T?
such that ? = ?(?1, . . . , ?k). We derive
fJ?(?1, . . . , ?k)K = fJ?(J?1K, . . . , J?kK)K
= fJ?(fJ?1K, . . . , fJ?kK)K .
Now either fJ?iK ? T f for every i ? {1, . . . , k},
but then so is fJ?K, or ? was not the smallest coun-
terexample.
For ??? of Statement (ii), we show that
(
T f ,
?
(p,q)?SIBLINGS(A) f(Tp,q \ {0?})
)
is a pre-
fixpoint of F . It is easy to see that (F1) and (F2)
hold. Now let k, ?, i, u1, . . . , uk as in (F3) such
that J?(u1, . . . , uk)K 6= 0?. Hence, u1, . . . , uk 6=
0?. By (i) there are ?1, . . . , ?i?1, ?i+1, . . . , ?k such
that uj = fJ?jK for j 6= i. Moreover there are
(p, q) ? SIBLINGS(A) and ?i ? C? such that
ui = fJ(ep + eq?) ? ?iK. We derive
fJ?(u1, . . . , uk)K
= fJ?(fJ?1K, . . . , fJ(ep + eq?) ? ?iK, . . . , fJ?kK)K
= fJ?(J?1K, . . . , J(ep + eq?) ? ?iK, . . . , J?kK)K
= fJ?(?1, . . . , (ep + eq?) ? ?i, . . . , ?k)K
= fJ(ep + eq?) ? ?(?1, . . . , ?i, . . . , ?k)K ,
which, by definition, is in f(Tp,q \ {0?}).
We prove ??? of (ii) by contradiction. To
this end, let (p, q) ? SIBLINGS(A) and ? ?
C? a smallest context such that fJ(ep + eq?) ?
?K ? f(Tp,q \ {0?}) \ Cf . Hence, J(ep + eq?) ?
?K 6= 0?. By (F2), we obtain ? 6= z. Hence,
there are k ? 1, ? ? ?(k), i ? {1, . . . , k},
?1, . . . , ?i?1, ?i+1, . . . , ?k ? T?, and ?i ? C?
such that ? = ?(?1, . . . , ?i?1, ?i, ?i+1, . . . , ?k).
We have that J?jK 6= 0? for j 6= i, and J(ep +
eq?) ? ?iK 6= 0?. We derive
fJ(ep + eq?) ? ?K
= fJ(ep + eq?) ? ?(?1, . . . , ?i, . . . , ?k)K
= fJ?(?1, . . . , (ep + eq?) ? ?i, . . . , ?k)K
= fJ?(J?1K, . . . , J(ep + eq?) ? ?iK, . . . , J?kK)K
= fJ?(fJ?1K, . . . , fJ(ep + eq?) ? ?iK, . . . , fJ?kK)K
By (i), we have that fJ?jK ? T f . Now ei-
ther fJ(ep + eq?) ? ?iK ? Cf , but then so is
fJ(ep + eq?) ? ?K, or ? was not the smallest coun-
terexample. 
4 Discussion and Further Research
The notion that the twins property can be decided
by searching for critical vectors in a compressed
search space is due to Kirsten (2012). We have
generalized his work in two ways: (i) We allow
arbitrary extremal semifields instead of the trop-
ical semiring. To this end, we use the notion of
a maximal factorization, which is implicit in his
work. (ii) We consider weighted tree automata
instead of weighted string automata. This makes
the proof more complex, as we have to distinguish
between contexts and trees.
Kirsten?s result that deciding the twins property
is PSPACE-hard directly transfers to our setting,
giving a lower bound on the complexity of our al-
gorithms. In addition, he shows that the problem
is PSPACE-complete by giving an algorithm that
is in PSPACE. We did not investigate whether this
result can be transferred to our setting as well.
To check for critical vectors, Algorithm 1 does
not need all components from the vectors in Tp,q
but only the p- and q?-components; thus in the
proof of Lemma 3.3 the height restriction ht(?) ?
2|Q|2|Q| for S? can ultimately be lowered to
ht(?) ? 2|Q|2. It is an open question which of
the two algorithms performs better in practice.
For further research, it would be interesting
to investigate sufficient properties for determiniz-
ability that do not require the semifield to be ex-
tremal. Then the determinized wta could truly ag-
gregate the weights of the original runs.
Acknowledgments
The authors wish to thank Heiko Vogler for stim-
ulating remarks on a draft, as well as the anony-
mous referees for pointing out mistakes. The
first author was financially supported by DFG VO
1011/6-1.
References
B?chse, Matthias and May, Jonathan and Vogler,
Heiko. 2010. Determinization of Weighted Tree
Automata using Factorizations. J. Autom. Lang.
Comb., 15(3/4).
Casacuberta, Francisco and de la Higuera, Colin.
2000. Computational complexity of problems on
19
probabilistic grammars and transducers. In Proc.
ICGI, LNCS. Springer.
Chiang, David. 2007. Hierarchical phrase-based
translation. Comp. Ling., 33(2):201?228.
Droste, Manfred and Kuich, Werner and Vogler,
Heiko, editors. 2009. Handbook of Weighted Au-
tomata. EATCS Monographs in Theoretical Com-
puter Science. Springer.
Eppstein, David. 1998. Finding the k shortest paths.
SIAM Journal on Computing, 28(2):652?673.
?sik, Zolt?n and Kuich, Werner. 2003. Formal tree
series. J. Autom. Lang. Comb., 8(2):219?285.
F?l?p, Zolt?n and Vogler, Heiko. 2009. Weighted tree
automata and tree transducers. In (Droste, Kuich
and Vogler, 2009), Chapter 9.
Goguen, Joseph. A. and Thatcher, James W. and Wag-
ner, Eric G. and Wright, Jesse B. 1977. Initial Al-
gebra Semantics and Continuous Algebras. J. ACM,
24(1):68?95.
Golan, Jonathan Samuel. 1999. Semirings and their
Applications. Kluwer Academic Publishers.
Graehl, Jonathan and Knight, Kevin and May,
Jonathan. 2008. Training tree transducers. Comp.
Ling., 34(3):391?427.
Hebisch, Udo and Weinert, Hanns Joachim. 1998.
Semirings: Algebraic Theory and Applications in
Computer Science, Series in Algebra, volume 5.
World Scientific.
Huang, Liang and Chiang, David. 2005. Better k-best
parsing. In Proc. IWPT, pp. 53?64. ACL.
Kirsten, Daniel. 2012. Decidability, Undecidability,
and PSPACE-Completeness of the Twins Property
in the Tropical Semiring. Theoretical Computer
Science, 420:56?63.
Kirsten, Daniel and M?urer, Ina. 2005. On the deter-
minization of weighted automata. J. Autom. Lang.
Comb., 10:287?312.
Li, Zhifei and Eisner, Jason and Khudanpur, Sanjeev.
2009. Variational decoding for statistical machine
translation. In Proc. ACL-IJCNLP, pp. 593?601.
ACL.
Mahr, Bernd. 1984. Iteration and summability
in semirings. Annals of Discrete Mathematics,
19:229?256.
May, Jonathan and Knight, Kevin. 2006. A better N-
best list: practical determinization of weighted fi-
nite tree automata. In Proc. NAACL-HLT, pp. 351?
358. ACL.
Pauls, Adam and Klein, Dan. 2009. k-best A* pars-
ing. In Proc. ACL-IJCNLP, pp. 958?966. ACL.
Petrov, Slav and Barrett, Leon and Thibaux, Romain
and Klein, Dan. 2006. Learning accurate, com-
pact, and interpretable tree annotation. In Proc.
COLING-ACL, pp. 433-440.
Sima?an, Khalil. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proc. COLING, pp. 1175?1180.
ACL.
Wechler, Wolfgang. 1992. Universal Algebra for
Computer Scientists, volume 25 of Monogr. Theo-
ret. Comput. Sci. EATCS Ser. Springer.
Zhang, Min and Jiang, Hongfei and Aw, Aiti and Li,
Haizhou and Tanm, Chew Lim and Li, Sheng 2008.
A tree sequence alignment-based tree-to-tree trans-
lation model. In Proc. ACL, pp. 559?567. ACL.
20
