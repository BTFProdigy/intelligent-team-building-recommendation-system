XiSTS ? XML in Speech Technology Systems 
 
Michael Walsh Stephen Wilson Julie Carson-Berndsen 
Department of Computer Science 
University College Dublin 
Ireland 
{michael.j.walsh, stephen.m.wilson, julie.berndsen}@ucd.ie 
 
Abstract: This paper describes the use of XML in three generic interacting speech technology 
systems.  The first, a phonological syllable recognition system, generates feature-based finite-state 
automaton representations of phonotactic constraints in XML. It employs axioms of event logic to 
interpret multilinear representations of speech utterances and outputs candidate syllables to the second 
system, an XML syllable lexicon. This system enables users to generate their own lexicons and its default 
lexicon is used to accept or reject the candidate syllables output by the speech recognition system. 
Furthermore its XML representation facilitates its use by the third system which generates additional 
lexicons, based on different feature sets, by means of a transduction process. The applicability of these 
alternative feature sets in the generation of synthetic speech can then be tested using these new lexicons. 
 
 
1. Introduction 
 
The flexibility and portability provided by 
XML, and its related technologies, result in 
them being well suited to the development 
of robust, generic, Natural Language 
Processing applications. In this paper we 
describe the use of XML within the context 
of speech technology software, with a 
particular focus on speech recognition. We 
present a framework, based on the model of 
Time Map Phonology (Carson-Berndsen, 
1998), for the development  and testing of 
phonological well- formedness constraints 
for generic speech technology applications. 
Furthermore, we illustrate how the use of a 
syllable lexicon, specified in terms of 
phonological features, and marked-up in 
XML, contributes to both speech recognition 
and synthesis. In the following sections 
three inter-connected systems are discussed. 
The first, the Language Independent 
Phonotactic System, LIPS, a syllable 
recognition application based on Time Map 
Phonology and a significant departure from 
current ASR technology, is described. The 
second system, Realising Enforced Feature-
based Lexical Entries in XML, REFLEX, is 
outlined and finally, the third system, 
Transducing Recognised Entities via XML, 
T-REX, is discussed. All three systems build 
on earlier work on generic speech tools 
(Carson-Berndsen, 1999; Carson-Berndsen 
& Walsh, 2000a). 
 
2. The Time Map Model  
 
This paper focuses on representing speech 
utterances in terms of non-segmental 
phonology, such as autosegmental 
phonology (Goldsmith, 1990), where 
utterances are represented in terms of tiers 
of autonomous features (autosegments) 
which can spread across a number of 
sounds. The advantage of this approach is 
that coarticulation can be modelled by 
allowing features to overlap. The Time Map 
model (Carson-Berndsen, 1998, 2000) 
builds on this autosegmental approach by 
allowing multilinear representations of 
autonomous features to be interpreted by an 
event-based computational linguistic model 
  
of phonology. The Time Map model 
employs a phonotactic automaton (finite-
state representation of the permissible 
combinations of sounds in a language), and 
axioms of event logic, to interpret 
multilinear feature representations. Indeed, 
much recent research (e.g. Ali et al, 1999; 
Chang, Greenberg & Wester, 2001) has 
focused on extracting  similar features to 
those used in our model. Figure 1 below, 
illustrates a mulitlinear feature-based 
representation of the syllable [So:n] 1. 
 
Figure 1. Multilinear representation of [So:n] 
 
Two temporal domains are distinguished by 
the Time Map model. The first, absolute 
(signal) time, considers features as events 
with temporal endpoints. The second, 
relative time, considers only the temporal 
relations of overlap and precedence as 
salient. Input to the model is in absolute 
time. Parsing, however, is performed in the 
relative time domain using only the overlap 
and precedence relations, and is guided by 
the phonotactic automaton which imposes 
top-down constraints on the relations that 
can occur in a particular language. The 
construction of the phonotactic automaton 
and the actual parsing process is carried out 
by LIPS. 
 
3. LIPS 
 
LIPS is the generic framework for the Time 
Map model. It incorporates finite-state 
                                                 
1All phonemes are specified in  SAMPA notation. 
methodology which enables users to 
construct their own phonotactic automata for 
any language by means of a graphical user 
interface. Furthermore, LIPS employs an 
event logic, enabling it to map from absolute 
time to relative time, and in a novel 
approach to ASR, carry out parsing on the 
phonological feature level. The system is 
comprised of two principal components, the 
network generator and the parser, outlined in 
the following subsections. 
 
3.1. The Network Generator 
 
The network generator interface allows 
users to build their own phonotactic 
automata. Users input node values and select 
from a list of feature overlap relations those 
that a given arc is to represent. These 
relations can be selected from a default list 
of IPA-like features or the user can specify 
their own set. In this way LIPS is feature-set 
independent. The network generator 
constructs feature-based networks and 
parsing takes place at the feature level. Once 
the user has completed the network 
specification, the system generates an XML 
representation of the phonotactic automaton. 
An automaton representing a small 
subsection of the phonotactics of English is 
illustrated in Figure 2. It is clear from this 
automaton that English permits an [S] 
followed by a [r] in syllable-initial position, 
but not the other way around. 
  
 
Figure 2. Phonotactic automaton  
  
 
 
Figure 3. XML representation of subsection of phonotactic automaton for English. 
 
Figure 3 illustrates a subsection of the XML 
representation of the English phonotactics 
output by the network generator. A single 
arc with a single phoneme, [S], and its 
overlap constraints, is shown.  
The motivation for generating an XML 
representation for our phonotactic  automata 
is that XML enables us to specify a well-
defined, easy to interpret, portable template, 
without compromising the generic nature of 
the network generator. That is to say the 
user can still specify a phonotactic 
automaton independent of any language or 
feature-set. The generated phonotactic 
automaton is then used to guide the second 
principal component of the system, the 
parser. 
3.2 The Parser 
 
LIPS employs a top-down and breadth-first 
parsing strategy and is best explained 
through exemplification. 
 
Purely for the purposes of describing how 
the parsing procedure takes place, we return 
to the phonotactic automaton of Figure 2, 
which of course represents only a very small 
subsection of English. This automaton will 
recognise such syllables as shum, shim, 
shem, shown, shrun, shran etc., some being 
actual lexicalised syllables of English and 
others being phonotactically well- formed, 
potential, syllables of English. For our 
example we take the multilinear 
  
representation of the utterance [So:n] as 
depicted in Figure 4 as our input to the 
parser. 
 
 
 
Figure 4. Interaction between the input and the 
automaton. 
 
At the beginning of the parsing process the 
phonotactic automaton is anticipating a [S] 
sound, that is it requires three temporal 
overlap constraints to be satisfied, the 
feature voiceless must overlap the feature 
fricative,  the feature palato  must overlap 
the feature voiceless, and the feature 
fricative must overlap the feature palato. A 
variable window is applied over the input 
utterance and the features within the window 
are examined to see if they satisfy the 
overlap constraints. As can be seen from 
Figure 4 the three features are indeed 
present and all overlap in time. Thus the [S] 
is recognised and the two arcs bearing the 
[S] symbol are traversed and the window 
moves on. At this point then the automaton 
is anticipating either an [r] or a vowel sound. 
In a similar fashion the contents of the new 
window are examined and in the case of our 
example the vowel [o:] is recognised (the [r]  
is rejected). The vowel transition is 
traversed, the window moves on, and the 
automaton is expecting an [n] or an [m]. For 
full details of the parsing process see 
Carson-Berndsen & Walsh (2000b). Output 
from LIPS is then fed through the REFLEX 
system to determine if actual or potential 
syllables have been found. 
 
4. REFLEX 
 
REFLEX is a generic, language independent 
application, which allows for the rapid 
design and construction of syllable lexicons, 
for any language. One of the main focuses 
of other research working on broadening the 
scope of the lexicon across languages, has  
been in the development of multilingual 
lexicons. One such project, PolyLex (Cahill 
& Gazdar, 1999), captures commonalities 
across related languages using hierarchical 
inheritance mechanisms. One of the main 
concerns of the work presented here 
however, is to provide generic, reusable, 
tools which facilitate the development and 
testing of phonological systems, rather than 
the creation of such multilingual lexicons. 
 
Work on phonological features and lexical 
description has either been within this 
multilingual context (Tiberius & Evans, 
2000) or has concentrated on using a 
feature-based lexicon for comparison with 
features extracted from a sound signal 
(Reetz, 2000). By removing reference to 
specific languages and concentrating on 
providing mechanisms for lexical 
generation, REFLEX can generate a syllable 
lexicon for any language that can be 
adequately represented in a phonetic 
notation. 
 
Furthermore, the decision to use XML to 
represent the output data means that it is 
readily available for use and manipulation 
by other outside systems with minimal 
effort. All background processing is 
completely hidden; one deals only with the 
marked-up output, from which idiosyncratic 
user-required structures can be rapidly 
generated.  
  
The REFLEX system outputs a feature-
based syllable lexicon. This lexicon is a 
valid XML document, meaning that it 
conforms to the given REFLEX Document 
Type Definition (DTD). The DTD stipulates 
the structure, order and number of XML 
element tags and attributes, modelling all 
potential syllable structures (e.g. V, CV, 
CVC etc). 
 
An example of a typical lexical entry, in this 
case corresponding to the multilinear 
representation specified in Figure 5, [So:n] 
is given below. 
 
 
Figure 5. Typical lexical entry in XML 
 
The syllable element shown has four 
children, described as follows: 
 
1) A text child, in this case So:n, the 
SAMPA representation of the entire 
syllable. 2) An <onset> element whose 
attribute list denotes its position within the 
syllable, i.e.<onset type=?first?>, <onset 
type=?second?> etc. 3) Nucleus and 4) 
coda elements are similarly defined. 
 
 Each of the syllable?s elements, <onset>, 
<nucleus> and <coda>, may have only one 
child element, <segment>, which tags the 
given phoneme. Its attribute list describes 
the phonemes specification in terms of 
phonological features.  It also has a duration 
attribute, which is derived from corpus 
analysis. 
 
<segment phonation=?voiced? 
   manner=?nasal? place=?apical?  
  duration=?null?>n</segment> 
 
REFLEX provides two methods by which  
syllables can be added to the lexicon. The 
first, requires users to specify an input file of 
monosyllables represented in a phonetic 
notation, in this case SAMPA.  The second, 
enables the user to specify syllables, in 
terms of phonemes, position, and if desired, 
a typical duration, by means of a GUI 
illustrated below in Figure 6. 
  
 
 
 
Figure 6. REFLEX lexicographer interface 
 
Regardless of the input option chosen, new 
entries are added to the lexicon via a 
background process.  REFLEX makes use of 
DATR, a non-monotonic inheritance based 
lexical representation language (Evans & 
Gazdar, 1996) to carry out this process. 
DATR is used to quickly and 
comprehensively define the phonological 
feature descriptions for a given language. 
For a greater understanding of how this can 
be achieved see Cahill, Carson-Berndsen & 
Gazdar (2000). Using DATR?s inference 
mechanisms, REFLEX manipulates the 
output into a valid XML document, creating 
a sophisticated phonological feature-based 
lexicon, shown in Figure 5. 
  
All syllable elements are enclosed within the 
root <lexicon> tag, whose sole attribute 
specifies the lexicon?s language. 
 
        <lexicon language=?English?> 
                  <syllable>?</syllable> 
                        :  
                   <syllable>?</syllable> 
        </lexicon>                    
 
The REFLEX lexicon is a versatile tool that 
has a number of potential applications 
within the domain of speech technology 
systems. The following sub-sections 
illustrate how this syllable lexicon, by virtue 
of its being marked up in XML, can 
contribute to both speech recognition and 
synthesis. 
 
4.1 LIPS and REFLEX 
 
By allowing feature overlap constraints to be 
relaxed in the case of underspecified input, 
LIPS can produce a number of candidate 
syllables.  In Figure 4 above, at the final 
transition, the automaton is expecting either 
an [m] or an [n]. The input, however, is  
underspecified, no feature distinguishing 
between [m] or [n], or indeed any voiced 
nasal, is present. By allowing the overlap 
constraints for the [m]  and the [n] to be 
relaxed, LIPS can consider both [So:n] and 
[So:m] to be candidate syllables for the 
utterance. Both candidate syllables are well-
formed, adhering to the phonotactics of 
English, however only one, [So:n], is an 
actual syllable of English. Thus at this point 
a lexicon providing good coverage of the 
language should reject [So:m] and accept 
[So:n]. In order to achieve this, REFLEX 
makes use of the XPath specification (a 
means for locating nodes in an XML 
document) and formulates a query before 
applying it to the syllable lexicon. 2 In the 
                                                 
2 The full W3C XPath specification can be found at 
http://www.w3c.org/TR/xpath 
example given, REFLEX searches the 
document, checking the value of the text 
child of each syllable element, against each 
candidate syllable output by LIPS. Any 
successful matches returned are therefore 
not only well- formed, but are deemed to be 
actual syllables. Thus at this point, the 
lexicon is searched and the syllable [So:n] is 
recognised. The granularity of the REFLEX 
search capability is such, that it can be 
extended to the feature level. Users can 
search the lexicon for syllables that contain 
a number of specific features in certain 
positions, e.g. search for syllables that 
contain a voiced, labial, plosive in the first 
onset. Again, REFLEX forms an XPath 
expression and queries the lexicon, returning 
all matches.  REFLEX also functions as a 
knowledge source for the T-REX system. 
This system is responsible for mapping 
output from the lexicon into syllable 
representations using different feature sets, 
e.g. features from other phonologies, and is 
discussed below in the context of speech 
synthesis.  
 
5. T-REX 
 
The role of this module is to enable 
lexicographers and speech scientists etc. to 
generate, via a transduction process,  
syllable lexicons based on different  
phonological feature sets. The default 
feature set employed by REFLEX is based 
on IPA-like features. However, T-REX 
provides a GUI that permits lexicographers 
to define phoneme to feature attribute 
mappings. Given this functionality T-REX 
operates as a testbed for investigating the 
merits of different feature sets in the context 
of speech synthesis. Different lexicons are 
generated by associating new feature sets 
with the same phonetic alphabet (SAMPA) 
via a GUI. The new lexicon is then 
transduced by T-REX which maps all 
syllable entries from the default lexicon 
(with IPA-like features) to the new lexicon,  
  
applying the features input by the user, to 
their associated phonemes. In order to 
exemplify this we return to our sample 
syllable, [So:n]. Figure 2 above shows the 
lexical representation, using IPA-like 
features, for [So:n]. Figure 7 below shows 
new features being associated with the 
phoneme [S].  
 
 
 
Figure 7. GUI for T-REX 
 
Similarly, new features are associated with 
the remaining phonemes, [o:]  and [n], and 
indeed the rest of the SAMPA alphabet. On 
completion the user initiates the transduction 
process and a new lexicon is produced. The 
XML representation of the phoneme [S], in 
the new lexicon, is depicted in Figure 8. 
Note how the feature attributes differ from 
those in the default lexicon.  
 
 
 
Figure 8. Phoneme with transduced features 
 
The advantages of this transduction 
capability are that numerous lexicons can be 
rapidly developed and used to investigate 
the appropriateness of specific formal 
models of phonological representation for 
the purposes of speech synthesis. 
Furthermore, the same computational 
phonological model, i.e. the Time Map 
model, can be employed. Bohan et al(2001) 
describe how the phonotactic automaton is 
used to generate a multilinear event 
representation of overlap and precedence 
constraints for an utterance, which is then  
mapped to control parameters of the HLsyn 
(Sensimetrics Corporation) synthesis engine. 
Different feature sets can be evaluated by 
assessing how they influence the various 
control parameters of the HLsyn engine and 
the quality of the synthesised speech. 
 
6. Conclusion 
 
This paper has described how the use of 
XML together with a computational 
phonological model can contribute 
significantly to the tasks of speech 
recognition, speech synthesis and lexicon 
development. Phonotactic automata and 
multilinear representations were introduced 
and the interpretation of these 
representations was discussed. Three robust, 
well-defined systems, LIPS, REFLEX, and 
T-REX, were outlined. These systems offer 
generic structures coupled with the 
portability of XML. In doing so, they enable 
users to recognise speech, synthesise speech, 
and develop lexicons for different languages 
using different feature sets while 
maintaining a common interface.  The 
generic and portable nature of these systems 
means that languages with significantly 
different phonologies are supported. In 
addition, languages which, to date, have 
received little attention with respect to 
speech technology are equally provided for.  
 
Ongoing projects include work on Irish, 
which has a notably different phonology 
from English and on developing phonotactic 
automata and phonological lexicons for 
other languages. Furthermore, the models 
are being extended to include phoneme-
  
grapheme mappings based on the contexts 
defined by the phonotactic automata.  
 
7. Bibliography 
 
Ali, A.M..A.; J. Van der Spiegel; P. Mueller; G. 
Haentjaens & J. Berman (1999): An 
Acoustic-Phonetic Feature-Based System for 
Automatic Phoneme Recognition in 
Continuous Speech. In: IEEE International 
Symposium on Circuits and Systems (ISCAS-
99), III-118-III-121, 1999. 
Bohan, A.; E. Creedon, , J. Carson-Berndsen & 
F. Cummins (2001): Application of a 
Computational Model of Phonology to 
Speech Synthesis. In: Proceedings of 
AICS2001, Maynooth, September 2001. 
Cahill, L. & G. Gazdar (1999). The PolyLex 
architecture: multilingual lexicons for 
related languages. Traitement Automatique 
des Langues, 40(2), 5-23. 
Cahill, L.; J. Carson-Berndsen & G. Gazdar 
(2000), Phonology-based Lexical 
Knowledge Representation. In: F. van Eynde 
& D. Gibbon (eds.) Lexicon Development 
for Speech and Language Processing, 
Kluwer Academic Publishers, Dordrecht. 
Carson-Berndsen, J. (1998): Time Map 
Phonology: Finite State Models and Event 
Logics in Speech Recognition. Kluwer 
Academic Publishers, Dordrecht. 
Carson-Berndsen, J. (1999): A Generic Lexicon 
Tool for Word Model Definition in 
Multimodal Applications. Proceedings of 
EUROSPEECH 99, 6th European 
Conference on Speech Communication and 
Technology, Budapest, September 1999. 
 
 
 
Carson-Berndsen, J. (2000): Finite State Models, 
Event Logics and Statistics in Speech 
Recognition, In: Gazdar, G.; K. Sparck 
Jones & R. Needham (eds.): Computers, 
Language and Speech: Integrating formal 
theories and statistical data. Philosophical 
Transactions of the Royal Society, Series A, 
358(1770), 1255-1266. 
Carson-Berndsen, J. & M. Walsh (2000a): 
Generic techniques for multilingual speech 
technology applications, Proceedings of the 
7th Conference on Automatic Natural 
Language Processing, Lausanne, 
Switzerland, 61-70. 
Carson-Berndsen, J. & M. Walsh (2000b): 
Interpreting Multilinear Representations in 
Speech. In: Proceedings of the Eight 
International Conference on Speech Science 
and Technology, Canberra, December 2000. 
Chang, S.; S. Greenberg & M. Wester (2001): 
An Elitist Approach to Articulatory-
Acoustic Feature Classification. In: 
Proceedings of Eurospeech 2001, Aalborg. 
Evans, R & G. Gazdar (1996), DATR: A 
language for lexical knowledge 
representation. In: Computational 
Linguistics 22, 2, pp. 167-216. 
Goldsmith, J. (1990): Autosegmental and 
Metrical Phonology. Basil Blackwell, 
Cambridge, MA. 
Reetz, H. (2000) Underspecified 
Phonological Features for Lexical 
Access. In: PHONUS 5, pp. 161-173. 
Saarbr?cken: Institute of Phonetics, 
University of the Saarland. 
Tiberius, C. & R. Evans, 2000 
"Phonological feature based Multilingual 
Lexical Description," Proceedings of 
TALN 2000, Geneva, Switzerland. 
 
 
Automatic Acquisition of Feature-Based Phonotactic Resources
Julie Carson-Berndsen & Robert Kelly & Moritz Neugebauer
Department of Computer Science
University College Dublin
Dublin 4, Ireland
{julie.berndsen,robert.kelly,moritz.neugebauer}@ucd.ie
Abstract
Automata induction and typed feature theory are de-
scribed in a unified framework for the automatic
acquisition of feature-based phonotactic resources.
The viability of this data-driven procedure is il-
lustrated with examples taken from a corpus of
syllable-labelled data.
1 Introduction
This paper combines two hitherto distinct areas of
research, namely automata induction and typed fea-
ture theory, for the purposes of acquiring phonotac-
tic resources for use in speech technology. In order
to illustrate the methodology a small annotated data
set for Italian has been chosen1; however, given an-
notated data, the techniques can be applied to any
language thus supporting language documentation
at the phonotactic level and eventually building up
a catalogue of reusable multilingual phonotactic re-
sources.
There are numerous ways in which phonotactic
information has been represented for use in speech
technology applications ranging from phrase struc-
ture rules to n-grams. In this paper, the feature-
based phonotactic automaton of the Time Map
model (Carson-Berndsen, 1998) is used as the rep-
resentational device. A phonotactic automaton de-
scribes all permissible sound combinations of a lan-
guage within the domain of a syllable in terms of
a finite state automaton, describing not only ac-
tual lexicalised syllables but also idiosyncratic gaps
which would be considered well-formed by a na-
tive speaker of a language. The advantage of this
representation of phonotactic constraints in the con-
text of speech recognition is that it allows out-
of-vocabulary items (new words) to be classified
as well-formed if they adhere to the constraints.
Furthermore, since the phonotactic automaton con-
strains with respect to the syllable domain, it pro-
vides a more flexible and linguistically motivated
1We use phonemically annotated data from the EUROM1
Multilingual European Speech Database.
context than n-grams which restrict their context to
a domain of fixed length (the n-1 preceding units).
A phonotactic automaton describes language-
specific constraints. Therefore, in order to develop
multilingual phonotactic resources, phonotactic au-
tomata for different languages must be produced.
Phonotactic automata for German and English have
already been constructed for the Time Map model
using manual techniques (Carson-Berndsen, 1998;
Carson-Berndsen and Walsh, 2000). Since manual
construction of phonotactic automata is time con-
suming and laborious, more recently focus has been
placed on combining manual and automatic tech-
niques in order to reduce the level of required hu-
man linguistic expertise. This will become more im-
portant when lesser-studied languages are addressed
when an expert may not always be available. The
techniques presented here are regarded as support
tools for language documentation which allow in-
ferences to be made based on generalisations found
in an annotated data set. The linguist is free to ac-
cept or reject the suggestions made by the system.
In what follows, a technique is described in
which phonotactic automata are acquired automat-
ically given annotated data for a language. While
this technique describes all forms found in the data,
acquired automata cannot be considered complete
since the data is likely to be sparse (in this paper
we illustrate this using a small data sample). How-
ever, by combining phonotactic automata with a
typed feature classification of sounds encountered
in the data, it is possible to highlight not only dis-
tributional similarities, but also phonetic similarities
which can be used to predict gaps in the represen-
tation. These can be presented to a user (at least a
native speaker of a language) who can accept or re-
ject these. Accepted forms are then integrated into
the phonotactic automaton.
2 Automatic Acquisition of Phonotactic
Automata
The approach described in this section is one alter-
native to a fully manual construction of phonotac-
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
tic automata whereby they are rapidly acquired au-
tomatically and at a low cost. Given a corpus of
well-formed syllables for the language in question,
it is assumed here that the phonotactics for the lan-
guage is implicit in the syllables themselves and can
be automatically extracted by examining each syl-
lable structure in turn. An extracted phonotactics
is assumed to describe at least the syllables in the
corpus and is also assumed to be an approximation
of the complete phonotactics for the language from
which the data was drawn. Since the phonotactics in
question are finite-state structures and the data avail-
able for acquiring phonotactics is a corpus of posi-
tive examples of well-formed syllable structures, the
approach adopted here is to apply a regular gram-
matical inference procedure which can learn from
positive data alone. The field of grammatical infer-
ence has yielded many important learnability results
for different language classes. A full discussion of
these results is beyond the scope of this paper how-
ever see Belz (2000, Chapter 3) for a concise sum-
mary and discussion and Angluin and Smith (1983)
for a survey style introduction to the field. Suffice
to say that since the formal language of well-formed
syllables in a given natural language is finite, it is
possible to learn the structure of a regular grammar
i.e. the required phonotactic automaton from posi-
tive data alone i.e. the corpus of well-formed sylla-
bles.
The choice of regular inference algorithm is in
fact arbitrary. Many algorithms have been devel-
oped which can perform this learning task. For the
purposes of this paper however the ALERGIA (Car-
rasco and Oncina, 1999) regular inference algorithm
is used. This algorithm as applied to the problem of
inferring phonotactic automata is described in detail
elsewhere (Kelly, 2004b) . Here, the workings of
the algorithm are described by example. Note that
ALERGIA in fact treats any positive data sample
as having been generated by a stochastic process.
Thus, learned automata are in fact stochastic au-
tomata i.e. automata in which both states and tran-
sitions have associated probabilities, however tradi-
tional automata can be obtained by simply ignoring
these probabilities. Table 1 shows a small subset
of the Italian data set consisting of 14 well-formed
Italian syllables each consisting of 3 segments and
transcribed using the SAMPA phonetic alphabet 2.
The ALERGIA inference algorithm takes as in-
put a sample set of positive strings S (represent-
ing well-formed syllables in this case) together with
a confidence value ? and outputs a determinis-
tic stochastic automaton A which is minimal for
2http://www.phon.ucl.ac.uk/home/sampa/
/v e n/ /r a n/ /b e n/ /m e n/ /t w a/
/k a n/ /n o n/ /t o n/ /f j o/ /r a n/
/d j o/ /s t o/ /s t e/ /t s a/ /p l o/
Table 1: Training set of Italian syllables.
the language it describes. ALERGIA proceeds by
building a Prefix Tree Automaton (PTA) from the
strings in S. The PTA is a deterministic automa-
ton with a single path of state-transitions from its
unique start state for each unique prefix which oc-
curs in S. Also, the PTA has a single acceptance
path, i.e. path of state-transitions from the start
state to some final state, for each unique string in
S where an initial subset of the transitions in ac-
ceptance paths for strings are shared if those strings
have a common prefix. Thus, common prefixes are
essentially merged giving the PTA its tree shape.
The PTA for S accepts exactly the strings in S and
each state of the PTA is associated with a unique
prefix of S. ALERGIA also assigns each transi-
tion in the PTA a frequency count dependent on the
number of prefixes in S which share that transition.
Similarly, each state has an assigned frequency de-
pendent on the number of strings from S which are
accepted (or equivalently, generated) at that state.
The PTA for the Italian syllables of table 1 is shown
in figure 1. Note that final states are denoted by
double circles and the single start state is state 0. In
figure 1 final state 26 has a frequency of 2 since the
two occurrences of the syllable /r a n/ terminate at
this state. All other final states have a frequency of
1 since exactly one syllable terminates at each final
state. All other states have a frequency of 0. Simi-
larly, the transition from state 0 to state 13 has a fre-
quency of 3 since three of the syllables in the train-
ing set begin with /t/and the transitions from state 0
to 17 and from state 17 to 18 have a frequency of
2 since two syllables begin with the segment com-
bination /s t/. The frquencies associated with the
states and transitions of the PTA can be used to as-
sociate a stochstic language with each state. The
set of acceptance paths from a given state determine
the set of strings in its associated language and the
probability for a given string in the language is eas-
ily derived from the frequencies of the states and
transitions on the acceptance path for that string.
ALERGIA uses the PTA as the starting point for
constructing a canonical, i.e. minimal deterministic,
automaton for S. The canonical automaton is iden-
tified by performing an ordered search of the au-
tomata derivable from the PTA by partitioning and
merging subsets of states of the PTA. Using the stan-
Figure 1: Prefix Tree Automaton for the syllables in
table 1.
dard order on the prefixes associated with the states
of the PTA, pairs of states are subsequently exam-
ined to determine if they generate a similar stochas-
tic language within a statistical significance bound
dependent on the supplied confidence value ?. If
a pair of states are deemed to statistically generate
the same language then they are merged into a sin-
gle state and the state-transitions of the automaton
are altered to reflect this merge. The canonical au-
tomaton is identified when no more state merges are
possible. Figure 2 shows the canonical automaton
derived from the PTA in figure 1.
Since automata are derived from training sets of
syllables through the use of a language indepen-
dent regular inference algorithm, the procedure de-
scribed above is generic and language independent.
However, the procedure is of course dependent on
the existence of a corpus of training syllables for
the language in question and since it is entirely data
Figure 2: Canonical Automaton for the PTA in fig-
ure 1.
driven, the quality of the resulting phonotactics will
be dependent on the quality and completeness of the
syllable corpus. Thus, firstly the corpus must have
high quality annotations. Fortunately, the need for
high quality annotations in corpora is now recog-
nised and has become an essential part of speech
technology research and we assume here that high
quality annotations are available. Secondly, if valid
sound combinations are not detailed in the train-
ing corpus then they may never be represented in
the learned phonotactics. In order to be complete
the learned automaton must model all valid sound
combinations, however. In this case, generalisation
techniques must be applied in conjunction with the
inference algorithm in order to identify and rectify
gaps in the training corpus. This ensures that the
acquired phonotactics describes as close an approx-
imation as possible to the complete phonotactics for
the language. One such approach to generalisation
which operates independently of the chosen regular
inference algorithm is described in Kelly (2004a).
An alternative technique is discussed in section 3.
Finally, note that learned automata represent the
first stage in the development of multilingual phono-
logical resources called Multilingual Time Maps
(MTMs) (Carson-Berndsen, 2002). An MTM ex-
tends the single tape model of a phonotactic au-
tomaton to a multitape transducer whereby the dif-
ferent transition tapes detail linguistic information
of varying levels of granularity and related to the
original segment label. An MTM might have in-
dividual tapes detailing the segment, the phonolog-
ical features associated with that segment, the av-
erage duration of the segment in a particular syl-
labic position etc. In particular, the segment tape of
the learned phonotactic automata can be augmented
with additional tapes detailing feature type labels
associated with the segments. These additional type
label tapes are discussed in more detail in the fol-
lowing section.
3 Phonotactic Automata and Typed
Feature Structures
Lexical knowledge representation in computational
phonology has already made extensive use of in-
heritance hierarchies to model lexical generalisa-
tions ranging from higher level prosodic categories
to the phonological segment. In contrast to the ap-
proach presented in this section, the work described
in (Cahill et al, 2000) is set in an untyped fea-
ture system using DATR to define inheritance net-
works with path-value equations (Evans and Gaz-
dar, 1996). The merits of applying a type discipline
even to untyped feature structures is considered in
Wintner and Sarkar (2002) from a general perspec-
tive and in Neugebauer (2003b) with special refer-
ence to phonological lexica.
Previous proposals to cast phonological structure
in a typed feature system can be found in Bird
and Klein (1994) and Walther (1999). However,
there are two major differences with regard to our
work. First, while types may denote sets of seg-
ments, we go beyond the idea of sets as arc labels in
finite-state automata (Bird and Ellison, 1994; Eis-
ner, 1997; van Noord and Gerdemann, 2001) which
says that boolean combinations of finitely-valued
features can be stored as a set on just one arc, rather
than being multiplied out as a disjunctive collection
of arcs. This choice has no theoretical consequences
but is merely a convenience for grammar develop-
ment (Bird and Ellison, 1994). The difference in
our approach consists in the hierarchical ordering
of types (or sets) which relates each arc label to
any other type in a given phonological typed feature
system; such type-augmented automata have been
formally defined in Neugebauer (2003c). Second,
inheritance of type constraints is assumed to gov-
ern all subsegmental feature information (Neuge-
bauer, 2003b). Since here the crucial inheritance
relationships are induced automatically, we elabo-
rate on work by Walther (1999) where a complex
hand-crafted type hierarchy for internal segmental
structure is mentioned instead of simple appropri-
ateness declarations (Bird and Klein, 1994).
The interaction of finite-state automata and typed
feature structures is depicted in figure 3. Transitions
are exhaustively defined over a set of type labels
which are characterised by a unique position in the
underlying type hierarchy. This hierarchy is key to
the compilation of well-formed segment definitions
which are achieved by unification of partial feature
structures. In the simplest case, only atomic types
appear on the arcs which means that types corre-
spond to singleton sets. This can be achieved for
a phonemically annotated corpus (just like the cor-
pus in figure 1) by replacing all occurrences of a
phoneme with its appropriate atomic type label.
Figure 3: Example of the type-augmented automa-
ton for [traf].
The semantics of the type system assumed here
are extremely simple: the denotation of a parent
type in the directed acyclic graph that constitutes a
type hierarchy is defined as the union of the deno-
tation of its children, whereas a type node without
children denotes a unique singleton set (A??t-Kaci et
al., 1989). Complex type formulae ? as constructed
by logical AND ? are implicitly supported for the
case of intersections since the greatest lower bound
condition (Carpenter, 1992) is assumed: its the for-
mal definition (also known as meet) states that in
a bounded complete partial order that is an inheri-
tance hierarchy, two types are either incompatible or
compatible. While in the first case, type constraints
are shared, in the latter case we require them to have
a unique highest common descendant. As suggested
in the LKB system (Copestake, 2002), these types
will in our approach be generated automatically if a
type hierarchy does not conform to this condition.
These types ? such as glbtype2 in figure 3 ? do not
have their own local constraint description and thus
do not rely on purely linguistic motivation.
A useful application of the greatest lower bound
condition seems to be the possibility that we can re-
fer to a set of compatible types simply by reference
to their common descendant. As indicated by the
hierarchical structure which is built over type09 in
figure 3, atomic types encode maximal information
whereas non-atomic types characteristically contain
only partial information. Thus, by defining transi-
tions over types such as type34 we might elegantly
capture phonotactic generalisations over a subset of
fricative sounds. This naturally raises the question
as to how the hierarchies are actually determined;
a suitable algorithm is described below, a detailed
specification is provided in Neugebauer (2003a).
Given a set of phonological feature bundles, an
inheritance hierarchy may be generated in the fol-
lowing way. For each feature which is defined for a
linguistic object (here: a phoneme) we compute the
corresponding extent or set description. The algo-
rithm then inserts these set descriptions into a lattice
and looks them up at the same time: it asks for the
smallest description that is greater than a singleton
set o with respect to the total order ? used inside
the tree. Every fully specified feature structure for
a given phoneme will deliver such a singleton set,
given that no two segments have been defined using
the identical feature structure.
The algorithm can be employed to recursively
compute all set descriptions of a feature system by
starting from the smallest set description of the lat-
tice. We need the lattice structure to encode the
inheritance relationships between sets; in the con-
text of lattice computation we will refer to these sets
in terms of nodes. Every set description o has two
lists associated with it: the list o? of its upper nodes
and the list o? of its lower nodes. One node may
be shared by two different set descriptions as their
upper node. While the algorithm processes each of
those two set descriptions, their shared upper node
must be detected in order to configure the relation-
ships correctly. To this end, all set descriptions are
stored in a search tree T . Every time the algo-
rithm finds a node it searches for it in the tree T to
find previously inserted instances of that set descrip-
tion. If the description is found, the existing lists
of nodes are updated; otherwise the previously un-
known set description is entered into the tree. Figure
4 demonstrates this procedure for the feature bundle
{fricative,labiodental,voiceless}. Once the smallest
set description [labiodental] is not able to include
one of segments which are successively added to it
a new upper node is created.
To make sure that all set descriptions that are in-
serted into the tree are also considered for their up-
Figure 4: Induction of subsumption hierarchies.
per nodes, the total tree order ? must relate to the
partial lattice order ? in the following way: o1 < o2
implies o1 ? o2. This is how recently inserted
nodes are greater than the actual set description with
respect to ? and will be considered later.3
Once we have computed all set descriptions, we
finally assign types and type constraints to all nodes
in the hierarchy. Therefore, the set of feature struc-
tures which constituted the starting point of our al-
gorithm has now been computed into a data struc-
ture which supersedes the previous level of infor-
mation in terms of a type inheritance network. The
last step consists of the insertion of greatest lower
bounds thus generating a well-formed lattice. Fig-
ure 5 visualises the final type inheritance hierarchy
for an Italian corpus containing 22 phonemes, each
corresponding to a unique atomic type (type01, . . . ,
type22). While the types numbered 23 to 39 are
generated by our set-theoretic algorithm, the great-
est lower bounds (the glb-types) are required by the
formal characteristics of our type system.
Any of the non-atomic types may be used to ex-
press generalisations over sets of phonological seg-
ments since each partial feature structure subsumes
all compatible fully specified segment entries. Ad-
ditionally, non-atomic nodes may be associated with
constraints which define appropriateness declara-
tions for linguistic signs of a particular type. For
example, all segments are at least characterised with
respect to four attributes (phonation, manner, place
and phonetic symbol). The next section sketches
an application of typed feature structures addressing
3Just computing the (ordered) set descriptions turns out to
be more effective since no hierarchy has to be computed. Com-
puting set descriptions as well as their hierarchical structure
takes twice as long for the same input when the algorithm is
used. This is also due to memory usage: while the computation
of all set descriptions is only based on a single predecessor, the
integration of a lattice algorithm stores all set descriptions in a
persistent search tree.
Figure 5: Complete generated type hierarchy.
data sparseness in automatically learned phonotac-
tic automata.
4 Examples
The integrated approach utilising both automata in-
duction and typed feature theory as presented in
the previous sections requires a phonemically an-
notated corpus. Each phoneme is then mapped to a
canonical feature bundle which is based on the pho-
netic characteristics specified in the International
Phonetic Alphabet (IPA); the features used in Fig-
ure 3 serve as an example. Our set-theoretic algo-
rithm operates on these feature structures thus de-
riving a type inheritance network for the corpus in
question. Note that phonemes and features are not
corpus-specific but rather a subset of a language-
independent set of linguistic descriptions that is the
IPA. As a result, we obtain a representation of our
annotation alphabet (phoneme and feature labels)
which exclusively refers to (sets of) linguistic ob-
jects via their corresponding types. This is exempli-
fied in figure 3 for an individual sound and in figure
5 for the full corpus.
Once the complete type hierarchy has been gen-
erated the inheritance relationships described can be
used to construct more compact finite-state struc-
tures than the automata learned over the original
data set. In addition, the linguistic generalisations
described by the hierarchy can be used to address
data sparseness in the training corpus. To illustrate
this, the automata are learned over type labels rather
than segments. Since all transitions of the learned
automata will now be labelled with types the infor-
mation in the feature hierachy can be used to ex-
press generalisations. To ensure that automata are
learned over types the segments in the training data
must be replaced with type labels that correspond to
singleton sets containing only the original segment.
For example, the syllable /r a n/ in table 1 would be
replaced by /type03 type01 type10/. Note that in this
case the transitions of the learned automaton will be
labelled with type labels rather than segments.
In the first case, the type hierarchy allows more
compact automata to be constructed by examining
the set of transitions emanating from each state of
the learned automaton. If each of the transitions em-
anating from a given state s1 have the same destina-
tion state s2 then the type labels on each transition
are examined to determine if they have a common
ancestor node in the hierarchy. If a common an-
cestor exists and if no other type label is the child
of that parent other than those appearing on the set
of transitions then they can be replaced by a sin-
gle transition from s1 to s2 labelled with the parent
type. The topmost diagram of figure 6 illustrates a
small section of the learned automaton for the full
Italian data set. In this case there are two transi-
tions from state 22 to state 35, one labelled with
type15 and the other labelled with type06. Refer-
ring to the hierarchy in figure 5, a common parent of
type15 and type06 is type30 and the only children of
type30 are type15 and type06. Therefore these two
transitions can be replaced by the single transition
labelled with type30.
Note that replacements of the kind described
above serve only to produce more compact au-
tomata and do not extend the coverage of the au-
tomaton. However, it is possible to use the type
hierarchy to achieve a more complete phonotactics.
The middle diagram of figure 6 shows another small
section of the learned automaton for the full Italian
data set. Referring again to the hierarchy in figure 5,
it can be seen that type29 is a parent of type14 which
labels the transition from state 0 to state 7 and is also
a parent of type18 which labels the transition from
state 0 to state 5. Similarly, type25 is a common par-
ent of type06 and type01 which label the transitions
from state 7 to state 17 and state 5 to state 25 respec-
tively. Finally, type35 is a common parent of type10
and type14 which label the transitions from state 17
to state 35 and state 25 to state 35. If each type label
is replaced by its common parent then both transi-
tions from state 0 are labelled with type29. Also,
the paths emanating from the destination states of
these transitions (states 5 and 7) are both labelled
with type25. In this case state 5 and state 7 can be
merged into a single state. A similar state merging
can be performed for states 17 and 35 resulting in a
new automaton as shown in figure 6. This process
yields a more general phonotactics since type29 ac-
tually denotes the segment set {p,m, b} and type25
denotes the set {a, i, e, E}. Thus, the segment p has
been effectively introduced as a new onset conso-
nant cluster that can precede any vowel in the set
denoted by type25. Also, as a result of introducing
type25 the additional vowels i and E have been in-
troduced as new vowel clusters. Note however that
type35 denotes exactly the set {m,n} and so no new
coda clusters are introduced.
Figure 6: Finite-state diagrams.
5 Conclusion
An important pre-requisite for the development of
robust multilingual speech technology applications
is the availability of language resources at vary-
ing levels of granularity. This paper has presented
generic techniques for acquisition of language-
specific phonotactic resources. The techniques were
exemplified using a small data set for Italian,4 but
scale to larger data sets and can be applied to any
language. Although the induction techniques as de-
scribed here assume that data is annotated at the syl-
lable level, only very few corpora are actually an-
notated at this level; a more usual annotation is at
the phonemic level. As a result, a cyclical learn-
ing procedure has been developed which learns as
syllable annotation is being performed and uses the
phonotactic automaton developed thus far to predict
syllable boundaries for annotation support (Kelly,
2004b). The work presented in this paper repre-
sents one specific step towards the provision of fine-
grained representations for speech recognition and
4Due to space constraints this paper only in-
cludes selected examples of the acquired resources.
Additional information is publicly available at
http://muster.ucd.ie/sigphon/. This includes
the complete annotation alphabet (phoneme and feature set),
the typed feature system and complete state diagrams for all
phonotactic automata.
synthesis based on a combination of data-driven and
user-driven techniques.
Acknowledgements
This material is based upon works supported by
the Science Foundation Ireland under Grant No.
02/IN1/ I100. The opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of Science Foundation Ireland.
References
A??t-Kaci, Hassan, R. Boyer, P. Lincoln, and R. Nasr.
1989. Efficient Implementation of Lattice Oper-
ations. ACM Transactions on Programming Lan-
guages and Systems, 11(1):115?146.
Dana Angluin and Carl H. Smith. 1983. Inductive
inference: Theory and methods. ACM Comput-
ing Surveys, 15(3):237?269.
Anja Belz. 2000. Computational Learning of
Finite-State Models for Natural Language Pro-
cessing. Ph.D. thesis, University of Sussex.
Steven Bird and T. Mark Ellison. 1994. One?
Level Phonology. Computational Linguistics,
20(1):55?90.
Steven Bird and Ewan Klein. 1994. Phonological
Analysis in Typed Feature Systems. Computa-
tional Linguistics, 20:455?491.
Lynne Cahill, Julie Carson-Berndsen, and Gerald
Gazdar. 2000. Phonology?based Lexical Knowl-
edge Representation. In Lexicon Development
for Speech and Language Processing, pages 77?
114. Kluwer Academic Publishers, Dordrecht.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures, volume 32 of Cambridge Tracts in
Theoretical Computer Science. Cambridge Uni-
versity Press, Cambridge.
Rafael C. Carrasco and Jose Oncina. 1999. Learn-
ing deterministic regular grammars from stochas-
tic samples in polynomial time. ITA, 33(1):1?19.
Julie Carson-Berndsen and Michael Walsh. 2000.
Interpreting multilinear representations in
speech. In Proceedings of the 8th Australian
Conference on Speech Science and Technology,
pages 472?477, Canberra, December.
Julie Carson-Berndsen. 1998. Time Map Phonol-
ogy: Finite State Models and Event Logics in
Speech Recognition. Kluwer Academic Publish-
ers, Dordrecht, Holland.
Julie Carson-Berndsen. 2002. Multilingual time
maps: Portable phonotactic models for speech
technology applications. In Proceedings of the
LREC 2002 Workshop on Portability Issues in
Human Language Technology.
Ann Copestake. 2002. Implementing Typed Fea-
ture Structure Grammars, volume 110 of CSLI
Lecture Notes. CSLI Publications, Center for the
Study of Language and Information.
Jason Eisner. 1997. Efficient generation in primi-
tive optimality theory. In Proceedings of the 35th
Annual Meeting of the Association for Compu-
tational Linguistics and the 8th Conference of
the European Association for Computational Lin-
guistics, Madrid.
Roger Evans and Gerald Gazdar. 1996. DATR: A
language for Lexical Knowledge Representation.
Computational Linguistics, 22(2):176?216.
Robert Kelly. 2004a. Generalisation in the auto-
matic acquisition of phonotactic resources. To
Appear in Proceedings of The University of Cam-
bridge Second Postgraduate Conference in Lan-
guage Research.
Robert Kelly. 2004b. A language independent ap-
proach to acquiring phonotactic resources for
speech recognition. In Proceedings of the 7th
Annual Colloquium for the UK Special Interest
Group for Computational Linguistics, pages 126?
133. CLUK04.
Moritz Neugebauer. 2003a. Automatic Generation
of Constraint Hierarchies. Poster presented at the
14th Meeting of Computational Linguistics in the
Netherlands, University of Antwerp.
Moritz Neugebauer. 2003b. Computational
Phonology and Typed Feature Structures. In
Proceedings of the First CamLing Postgraduate
Conference on Language Research. Cambridge.
University of Cambridge.
Moritz Neugebauer. 2003c. Subsumption in
Speech Recognition and Feature Theory. In Pro-
ceedings of the Twenty-ninth Annual Meeting of
the Berkeley Linguistics Society, University of
California at Berkeley. Berkeley Linguistics So-
ciety.
Gertjan van Noord and Dale Gerdemann. 2001. Fi-
nite State Transducers with Predicates and Iden-
tities. Grammars, 4(3):263?286.
Markus Walther. 1999. One?Level Prosodic Mor-
phology. In Marburger Arbeiten zur Linguistik,
volume 1, Philipps?Universita?t Marburg.
Shuly Wintner and Anoop Sarkar. 2002. A note
on typing feature structures. Computational Lin-
guistics, 28(3):389?397.
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 5?8,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
WinkTalk: a demonstration of a multimodal speech synthesis platform
linking facial expressions to expressive synthetic voices
E?va Sze?kely, Zeeshan Ahmed, Joa?o P. Cabral, Julie Carson-Berndsen
CNGL, School of Computer Science and Informatics, University College Dublin
Belfield, D4, Dublin, Ireland
{eva.szekely|zeeshan.ahmed}@ucdconnect.ie,{joao.cabral|julie.berndsen}@ucd.ie
Abstract
This paper describes a demonstration of the
WinkTalk system, which is a speech synthe-
sis platform using expressive synthetic voices.
With the help of a webcamera and facial ex-
pression analysis, the system allows the user
to control the expressive features of the syn-
thetic speech for a particular utterance with
their facial expressions. Based on a person-
alised mapping between three expressive syn-
thetic voices and the users facial expressions,
the system selects a voice that matches their
face at the moment of sending a message.
The WinkTalk system is an early research pro-
totype that aims to demonstrate that facial
expressions can be used as a more intuitive
control over expressive speech synthesis than
manual selection of voice types, thereby con-
tributing to an improved communication expe-
rience for users of speech generating devices.
1 Introduction
During a human verbal communication process, ex-
pressive features of face and speech are congru-
ent, operating in a synchronised manner (Campbell,
2008), (Graf et al, 2002). Facial expressions and
expressive speech styles often help to convey the
emotional intent of the speaker that is only partially
contained in the words. The application described
in this paper aims to make use of this synchrony
and applies facial expressions as a real time voli-
tional control over the expressive features of syn-
thetic utterance productions of augmented speakers.
The WinkTalk system is currently a research proto-
type in progress, operating on a personal computer
equipped with a webcamera. The goal of the system
is to respond to the need of integrated multimodality
in speech generating devices of users of augmenta-
tive and alternative communication1 (AAC) applica-
tions (Higginbotham, 2010). Being able to correctly
link facial expression to synthetic speech output is
a step forward to a more intuitive way of control-
ling the expressiveness of synthetic speech. The ap-
proach can be considered novel, as the authors are
not aware of another system using facial expressions
to control expressive TTS.
2 WinkTalk system architecture
The WinkTalk system is a web based application de-
veloped using AJAX and PHP technologies. The
web application provides a flexible interface and al-
lows for easy integration of new components such
as synthetic voices or gesture recognisers running
on a web server. The internal architecture of the
system is shown in figure 1. The system operates
based on a configurable workflow defining the three
modes of the system: a personalisation mode, an au-
tomatic voice selection mode based on facial expres-
sion, which is the core functionality of the system,
and a control mode of manual voice selection, that
was included for evaluation purposes. In the man-
ual voice selection application the user is presented
with the three options and selects the voice style that
1Augmentative and alternative communication (AAC) refers
to an area of research, clinical, and educational practice. AAC
involves attempts to study and when necessary compensate for
temporary or permanent impairments, activity limitations, and
participation restrictions of individuals with severe disorders of
speech-language production and/or comprehension, including
spoken and written modes of communication.(ASHA, 2005)
5
Figure 1: Architecture and working modes of the Wink-
Talk system
matches the emotional or expressive intent of the
message. It has previously been shown that after a
short familiarisation with the voices, it is possible
for the user to make a fairly good prediction of how
a particular utterance will sound when synthesised
with one of the voices (Sze?kely et al, 2012). This
makes it possible to use the system in a conversation
situation, in which the user does not have the oppor-
tunity to listen to the three possible speech samples
but needs to make a choice ahead of the time of the
synthesis. The automatic voice choice mode and the
personalisation mode will be described in sections 3
and 4, respectively.
3 Facial expression based voice selection
3.1 Expressive synthetic voices
The synthesiser component of the application uses
three expressive HMM-based synthetic voices of
a middle aged American male. The voices have
been built using the HTS speech engine 2.1., from
an audiobook corpus made available for Blizzard
Challenge 2012 by Toshiba Research Europe Ltd,
Cambridge Research Laboratory. Each synthetic
voice was trained from different subcorpora of the
audiobook obtained using an unsupervised cluster-
ing technique based on glottal source parameters
(Sze?kely et al, 2011). Perceptual experiments have
shown (Hennig et al, 2012) that the three voices can
be characterised on an expressiveness gradient: from
calm (A voice), through intense (B voice) to very in-
tense (C voice). This expressiveness gradient can
be described with characteristics such as with rising
pitch, greater prosodic variation, increased power
and voice quality changing from lax to tense.
3.2 Facial expression analysis
For facial expression recognition, the system uses
the Sophisticated Highspeed Object Recognition
Engine (SHORE) library by Fraunhofer. To detect
faces and expressions, SHORE analyses local struc-
ture features in images and outputs scores for four
distinct facial expressions: happy, sad, angry and
surprised, with an indication of the intensity of the
expression (Kueblbeck and Ernst, 2006). The inten-
sity ranges from 0-100, a higher value meaning a
more intense expression in that category.
3.3 Mapping between facial expressions and
voices
The system uses the facial expression categories and
intensity scores outputted by SHORE to select from
the three synthetic voices. The initial mapping be-
tween facial expression categories and ranges of in-
tensity values and voices are shown in Table 1. For
example, an image analysed as containing the fa-
cial expression suprised with an intensity of 25, the
system will synthesise the corresponding utterance
with the C voice. The system always uses the fa-
cial expression category with the highest value for
a particular image. These initial values have been
Figure 2: Initial thresholds for mapping different inten-
sity values of the facial expressions to the synthetic voices
6
Figure 3: Interface of the dialogue simulation with Wink-
Talk.
chosen based on considerations about arousal lev-
els of the underlying basic emotion of the facial ex-
pression categories, for example with surprise be-
ing a high arousal emotion, the intensity scores of
it result sooner in a higher intensity voice choice.
The values have also been supported by the results
a perceptual test carried out by 25 participants on a
dataset that was balanced to contain equal amount
of stimuli from all facial expression categories. Par-
ticipants were asked to select from three synthesised
utterances the one best matching the facial expres-
sion of a person on a picture. The perceptual test
has shown that 90% of all majority votings (above
66% agreement among participants) fell within the
initial threshold values. When a message is being
sent to the synthesiser, the system makes a snapshot
of the user?s face. Based on the image scores and
threshold table, the system decides which voice best
suits the current facial expression and returns the re-
sults accordingly. The system also provides an op-
tion to take streaming video input from the camera
rather than a single image, and calculate the feature
values over an interval of the video around the time
of sending a message. To take into consideration the
cases where individual preferences of voice choice
differ greatly, as well as to account for individual
differences in facial characteristics, a personalisa-
tion component has been integrated in the system,
which will be introduced in section 4.
4 Personalisation component
In order to optimise the performance of the Wink-
Talk system, a personalisation session needs to be
completed by each user. The objective of the per-
sonalisation is to adjust the voice selection thresh-
old according to users? facial characteristics and in-
dividual preferences. In the personalisation phase,
Figure 4: Interface of the personalisation component of
WinkTalk.
the user is presented with a sentence and makes an
appropriate facial expression to accompany the ut-
terance. The facial expression is captured and anal-
ysed by the system and the user is presented with
the three options of synthetic speech samples, along
with an indication of which sample the system chose
to match their facial expression. If the user does
not agree with the selection provided by the sys-
tem, a preference can be indicated by choosing from
the other two options. The system then adjusts the
threshold by moving it by a standard factor towards
the outlying training example. The new threshold is
applied in next trial. The thresholds for each facial
expression-voice pair are normalised so that there is
no overlap between the different voices for the same
feature.
5 Conclusions and future work
The WinkTalk system has been evaluated within an
interactive evaluation session involving 10 subjects,
each of them acting out pre-scripted dialogues with
a conversation partner. The evaluation has shown
that while there is a general preference to manual se-
lection of expressive voices, 90% of the participants
described facial expression control as a valuable ad-
dition to the simulated augmented communication
process. A strong learning effect in the ease of using
the system has also been observed. Future work is
planned to research further input strategies of ges-
tures as well as to integrate a female expressive syn-
thetic voice. An essential next step is to extend the
7
personalisation component to include the possibility
of fully personalised training of the facial expression
analysis to fit individual needs of users who are re-
stricted with respect to their gestural expressiveness.
6 Demonstration
6.1 Overview
The demonstration will give participants an oppor-
tunity to use the WinkTalk system by conducting
the personalisation phase and using the system with
pre-scripted dialogues. It is intended for those in-
terested in using multimodal tools and expressive
speech to improve the communication experience
of individuals with complex communication needs.
The demonstration will give participants a chance
to experience the facial expression control over the
voice choice of the system as well as get an im-
pression of how the range of expressive voices can
be used in an acted dialogue situation. A 3 minute
video of the system in use will also be available for
viewing.
6.2 Familiarisation/Personalisation phase
First, a short introduction will be given to the system
and its aims, then the participants will be introduced
to the synthetic voices by listening to a few sam-
ples receiving a brief description of their character-
istics. Subsequently, the participants will be asked to
conduct a personalisation session including 20 iter-
ations, that will help optimise the system to adapt to
the participants? preferences, as described in section
4. It will also familiarise the users with the char-
acteristics of the voices and the mapping of facial
expressions and voices.
6.3 Dialogue simulation with synthetic voices
After the users are familiarised with the system, they
can choose from a set of 8 dialogues representing a
range of social interactions and emotional sentiment
and intensity. Participants will act out some of the
dialogues with a conversation partner, using facial
expressions to control the selection of the synthetic
voices instead of speaking with their own voice.
They will also have the option to compare the facial
expression control of the WinkTalk system with a
simple manual selection of synthetic voices for each
utterance. At the end of the dialogue session there
will be a chance to fill out a feedback form to help
the further development of the system.
Acknowledgments
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Genera-
tion Localisation (www.cngl.ie) at University College Dublin
(UCD). The opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the authors and do
not necessarily reflect the views of Science Foundation Ireland.
The authors would also like to thank Shannon Hennig (IIT),
Nick Campbell and the Speech Communication Lab (TCD), for
their invaluable help with the interactive evaluation.
References
American Speech-Language-Hearing Association. 2005
Roles and Responsibilities of Speech-Language
Pathologists With Respect to Augmentative and Alter-
native Communication: Position Statement. Available
from www.asha.org/policy.
Campbell, N. 2008. Multimodal processing of discourse
information; the effect of synchrony Proc. of Interna-
tional Symposium on Universal Communication, Os-
aka.
Graf, H.P., Cosatto, E., Strom, V., and Huang, F.J.
2002. Visual prosody: Facial movements accompany-
ing speech. Proc. of the 5th International Conference
on Automatic Face and Gesture Recognition.
Hennig, S., Sze?kely, E?., Carson-Berndsen, J. and Chel-
lali, R. 2012. Listener evaluation of an expressiveness
scale in speech synthesis for conversational phrases:
implications for AAC. to appear in: Proc. of ISAAC,
Pittsburgh.
Higginbotham, D. J. 2010. Humanizing Vox Artificialis:
The Role of Speech Synthesis in Augmentative and
Alternative Communication Computer Synthesized
Speech Technologies: Tools for Aiding Impairment,
J. Mullennix and S. Stern, Eds. IGI Global, pp. 50-70.
HTS-2.1 toolkit, HMM-based speech synthesis system
version 2.1. http://hts.sp.nitech.ac.jp.
Kueblbeck., C. and Ernst, A. 2006. Face detection and
tracking in video sequences using the modified census
transformation. Journal on Image and Vision Comput-
ing, vol. 24, issue 6, pp. 564-572.
SHORE face detection engine, Fraunhofer Institute
http://www.iis.fraunhofer.de/en/bf/bsy/fue/isyst
Sze?kely, E?., Cabral, J., Abou-Zleikha, M., Cahill, P. and
Carson-Berndsen, J. 2012. Evaluating expressive
speech synthesis from audiobook corpora for conver-
sational phrases. Proc. of LREC, Istanbul.
Sze?kely, E?., Cabral, J. P., Cahill, P. and Carson-Berndsen,
J. 2011. Clustering expressive speech styles in audio-
books using glottal source parameters. Proc. of Inter-
speech, Florence.
8
