Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88?97,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Recession Segmentation: Simpler Online Word Segmentation Using
Limited Resources?
Constantine Lignos, Charles Yang
Dept. of Computer and Information Science, Dept. of Linguistics
University of Pennsylvania
lignos@cis.upenn.edu, charles.yang@ling.upenn.edu
Abstract
In this paper we present a cognitively plau-
sible approach to word segmentation that
segments in an online fashion using only
local information and a lexicon of pre-
viously segmented words. Unlike popu-
lar statistical optimization techniques, the
learner uses structural information of the
input syllables rather than distributional
cues to segment words. We develop a
memory model for the learner that like a
child learner does not recall previously hy-
pothesized words perfectly. The learner at-
tains an F-score of 86.69% in ideal condi-
tions and 85.05% when word recall is un-
reliable and stress in the input is reduced.
These results demonstrate the power that a
simple learner can have when paired with
appropriate structural constraints on its hy-
potheses.
1 Introduction
The problem of word segmentation presents an
important challenge in language acquisition. The
child learner must segment a continuous stream of
sounds into words without knowing what the in-
dividual words are until the stream has been seg-
mented. Computational models present an op-
portunity to test the potentially innate constraints,
structures, and algorithms that a child may be us-
ing to guide her acquisition. In this work we de-
velop a segmentation model from the constraints
suggested by Yang (2004) and evaluate it in ideal-
ized conditions and conditions that better approx-
imate the environment of a child learner. We seek
to determine how these limitations in the learner?s
input and memory affect the learner?s performance
and to demonstrate that the presented learner is ro-
bust even under non-ideal conditions.
?Portions of this work were adapted from an earlier
manuscript, Word Segmentation: Quick But Not Dirty.
2 Related Work
Most recent work in word segmentation of child-
directed speech has operated within statistical op-
timization frameworks, particularly Bayesian ap-
proaches (Goldwater et al, 2009; Johnson and
Goldwater, 2009). These models have established
the state-of-the-art for the task of selecting appro-
priate word boundaries from a stream of unstruc-
tured phonemes. But while these models deliver
excellent performance, it is not clear how they in-
form the process of acquisition.
Trying to find cognitive insight from these types
of models is difficult because of the inherent mis-
match in the quality and types of hypotheses they
maintain during learning. Children are incremen-
tal learners (Brown, 1973), and learners relying
on statistical optimization are generally not. A
child?s competence grows gradually as she hears
and produces more and more utterances, going
through predictable changes to her working gram-
mar (Marcus et al, 1992) that statistical optimiza-
tion techniques typically do not go through and do
not intend to replicate.
Statistical models provide excellent information
about the features, distributional cues, and priors
that can be used in learning, but provide little in-
formation about how a child learner can use this
information and how her knowledge of language
develops as the learning process evolves. Previ-
ous simulations in word segmentation using the
same type of distributional information as many
statistical optimization-based learners but without
an optimization model suggest that statistics alone
are not sufficient for learning to succeed in a com-
putationally efficient online manner; further con-
straints on the search space are needed (Yang,
2004).
Previous computational models have demanded
tremendous memory and computational capacity
from human learners. For example, the algorithm
88
of Brent & Cartwright (1996) produces a set of
possible lexicons that describe the learning cor-
pus, each of which is evaluated as the learner it-
erates until no further improvement is possible. It
is unlikely that an algorithm of this type is some-
thing a human learner is capable of using given the
requirement to remember at the very least a long
history of recent utterances encountered and con-
stantly reanalyze them to find a optimal segmenta-
tion. Work in this tradition makes no claims, how-
ever, that these methods are actually the ones used
by human learners.
On the other hand, previous computational
models often underestimate the human learner?s
knowledge of linguistic representations. Most of
these models are ?synthetic? in the sense of Brent
(1999): the raw material for segmentation is a
stream of segments, which are then successively
grouped into larger units and eventually, conjec-
tured words. This assumption may make the child
learner?s job unnecessarily hard; since syllables
are hierarchical structures consisting of segments,
treating the linguistic data as unstructured segment
sequences makes the problem harder than it actu-
ally is. For a given utterance, there are fewer sylla-
bles than segments, and hence fewer segmentation
possibilities.
Modeling the corpus using hierarchical gram-
mars that can model the input at varying levels
(word collocations, words, syllables, onsets, etc.)
provide the learner the most flexibility, allowing
the learner to build structure from the individual
phonemes and apply distributions at each level of
abstraction (Johnson and Goldwater, 2009). While
this results in state-of-the-art performance for seg-
mentation performed at the phoneme level, this
approach requires significant computational re-
sources as each additional level of representation
increases the complexity of learning. In addition,
it is not clear that some of the intermediate levels
in such an approach, such as word level colloca-
tions which are not syntactic constituents, would
have any linguistic or psychological reality to a
human learner.
A number of psychologically-motivated mod-
els of word segmentation rely on the use of syl-
labic transitional probabilities (TPs), basing the
use of TPs on experimental work in artificial lan-
guage learning (Saffran et al, 1996a; Saffran et
al., 1996b) and in corpus studies (Swingley, 2005).
The identification of the syllable as the basic unit
of segmentation is supported research in experi-
mental psychology using infants as young as 4-
days-old (Bijeljac-Babic et al, 1993), but when
syllable transitional probabilities are evaluated in
online learning procedures that only use local in-
formation (Yang, 2004), the results are surpris-
ingly poor, even under the assumption that the
learner has already syllabified the input perfectly.
Precision is 41.6%, and recall is 23.3%, which
we will show is worse than a simple baseline of
assuming every syllable is a word. The below-
baseline performance is unsurprising given that in
order for this type of model to posit a word bound-
ary, a transitional probability between syllables
must be lower than its neighbors. This condition
cannot be met if the input is a sequence of mono-
syllabic words for which a boundary must be pos-
tulated for every syllable; it is impossible to treat
every boundary as a local minimum.
While the pseudo-words used in infant stud-
ies measuring the ability to use transitional prob-
ability information are uniformly three-syllables
long, much of child-directed English consists of
sequences of monosyllabic words. Corpus statis-
tics reveal that on average a monosyllabic word
is followed by another monosyllabic word 85%
of time (Yang, 2004), and thus learners that use
only local transitional probabilities without any
global optimization are unlikely to succeed. This
problem does not affect online approaches that
use global information, such as computing the
maximum likelihood of the corpus incrementally
(Venkataraman, 2001). Since these approaches do
not require each boundary be a local minimum,
they are able to correctly handle a sequence of
monosyllable words.
We believe that the computational modeling of
psychological processes, with special attention to
concrete mechanisms and quantitative evaluations,
can play an important role in identifying the con-
straints and structures relevant to children?s acqui-
sition of language. Rather than using a prior which
guides the learner to a desired distribution, we ex-
amine learning with respect to a model in which
the hypothesis space is constrained by structural
requirements.
In this paper we take a different approach than
statistical optimization approaches by exploring
how well a learner can perform while processing
a corpus in an online fashion with only local in-
formation and a lexicon of previously segmented
89
words. We present a simple, efficient approach
to word segmentation that uses structural informa-
tion rather than distributional cues in the input to
segment words. We seek to demonstrate that even
in the face of impoverished input and limited re-
sources, a simple learner can succeed when it op-
erates with the appropriate constraints.
3 Constraining the Learning Space
Modern machine learning research (Gold, 1967;
Valiant, 1984; Vapnik, 2000) suggests that con-
straints on the learning space and the learning
algorithm are essential for realistically efficient
learning. If a domain-neutral learning model fails
on a specific task where children succeed, it is
likely that children are equipped with knowledge
and constraints specific to the task at hand. It
is important to identify such constraints to see to
what extent they complement, or even replace, do-
main neutral learning mechanisms.
A particularly useful constraint for word seg-
mentation, introduced to the problem of word
segmentation by Yang (2004) but previously dis-
cussed by Halle and Vergnaud (1987), is as fol-
lows:
Unique Stress Constraint (USC): A word can
bear at most one primary stress.
A simple example of how adult learners might
use the USC is upon hearing novel names or
words. Taking Star Wars characters as an exam-
ple, it is clear that chewbacca is one word but
darthvader cannot be as the latter bears two pri-
mary stresses.
The USC could give the learner many isolated
words for free. If the learner hears an utterance
that contains exactly one primary stress, it is likely
it is a single word. Moreover, the segmenta-
tion for a multiple word utterance can be equally
straightforward under USC. Consider a sequence
W1S1S2S3W2, where W stands for a weak sylla-
ble and S stands for a strong syllable. A learner
equipped with USC will immediately know that
the sequence consists of three words: specifically,
W1S1, S2, and S2W2.
The USC can also constrain the use of other
learning techniques. For example, the syllable
consequence S1W1W2W3S2 cannot be segmented
by USC alone, but it may still provide cues that
facilitate the application of other segmentation
strategies. For instance, the learner knows that the
sequence consists of at least two words, as indi-
cated by two strong syllables. Moreover, it also
knows that in the window between S1 and S2 there
must be one or more word boundaries.
Yang (2004) evaluates the effectiveness of the
USC in conjunction with a simple approach to us-
ing transitional probabilities. The performance of
the approach presented there improves dramati-
cally if the learner is equipped with the assump-
tion that each word can have only one primary
stress. If the learner knows this, then it may
limit the search for local minima to only the win-
dow between two syllables that both bear primary
stress, e.g., between the two a?s in the sequence
languageacquisition. This assumption is plau-
sible given that 7.5-month-old infants are sensi-
tive to strong/weak prosodic distinctions (Jusczyk,
1999). Yang?s stress-delimited algorithm achieves
the precision of 73.5% and recall of 71.2%, a sig-
nificant improvement over using TPs alone, but
still below the baseline presented in our results.
The improvement of the transitional
probability-based approach when provided
with a simple linguistic constraint suggests
that structural constraints can be powerful in
narrowing the hypothesis space so that even
sparse, local information can prove useful and
simple segmentation strategies can become more
effective.
It should be noted that the classification of every
syllable as ?weak? or ?strong? is a significant sim-
plification. Stress is better organized into hierar-
chical patterns constructed on top of syllables that
vary in relative prominence based on the domain
of each level of the hierarchy, and generally lan-
guages avoid adjacent strong syllables (Liberman
and Prince, 1977). We later discuss a manipula-
tion of the corpus used by Yang (2004) to address
this concern.
Additionally, there are significant challenges
in reconstructing stress from an acoustic signal
(Van Kuijk and Boves, 1999). For a child learner
to use the algorithm presented here, she would
need to have mechanisms for detecting stress in
the speech signal and categorizing the gradient
stress in utterances into a discrete level for each
syllable. These mechanisms are not addressed in
this work; our focus is on an algorithm that can
succeed given discrete stress information for each
syllable. Given the evidence that infants can dis-
tinguish weak and strong syllables and use that in-
90
formation to detect word boundaries (Jusczyk et
al., 1999), we believe that it is reasonable to as-
sume that identifying syllabic stress is a task an
infant learner can perform at the developmental
stage of word segmentation.
4 A Simple Algorithm for Word
Segmentation
We now present a simple algebraic approach to
word segmentation based on the constraints sug-
gested by Yang (2004). The learner we present is
algebraic in that it has a lexicon which stores pre-
viously segmented words and identifies the input
as a combination of words already in the lexicon
and novel words. No transitional probabilities or
any distributional data are calculated from the in-
put. The learner operates in an online fashion, seg-
menting each utterance in a primarily left-to-right
fashion and updating its lexicon as it segments.
The USC is used in two ways by the learner.
First, if the current syllable has primary stress and
the next syllable also has primary stress, a word
boundary is placed between the current and next
syllable. Second, whenever the algorithm is faced
with the choice of accepting a novel word into the
lexicon and outputting it as a word, the learner
?abstains? from doing so if the word violates USC,
that is if it contains more than one primary stress.
Since not all words are stressed, if a word contains
no primary stresses it is considered an acceptable
word; only a word with more that one primary
stress is prohibited. If a sequence of syllables has
more than one primary stress and cannot be seg-
mented further, the learner does not include that
sequence in its segmentation of the utterance and
does not add it to the lexicon as it cannot be a valid
word.
The algorithm is as follows, with each step ex-
plained in further detail in the following para-
graphs.
For each utterance in the corpus, do the following:
1. As each syllable is encountered, use Initial
Subtraction and USC Segmentation to seg-
ment words from the beginning of the utter-
ance if possible.
2. If unsegmented syllables still remain, apply
Final Subtraction, segmenting words itera-
tively from the end of the utterance if pos-
sible.
3. If unsegmented syllables still remain, if those
syllables constitute a valid word under the
USC, segment them as a single word and add
them to the lexicon. Otherwise, abstain, and
do not include these syllables in the segmen-
tation of the sentence and do not add them to
the lexicon.
Initial Subtraction. If the syllables of the utter-
ance from the last segmentation (or the start of the
utterance) up to this point matches a word in the
lexicon but adding one more syllable would result
in it not being a known word, segment off the rec-
ognized word and increase its frequency. This iter-
atively segments the longest prefix word from the
utterance.
USC Segmentation. If the current and next syl-
lables have primary stress, place a word bound-
ary after the current syllable, treating all syllables
from the last segmentation point up to and and in-
cluding the current syllable as a potential word. If
these syllables form a valid word under the USC,
segment them as a word and add them to the lex-
icon. Otherwise, abstain, not including these syl-
lables in the segmentation of the sentence and not
adding them to the lexicon.
Final Subtraction. After initial subtraction and
USC Segmentation have been maximally applied
to the utterance, the learner is often left with a
sequence of syllables that is not prefixed by any
known word and does not have any adjacent pri-
mary stresses. In this situation the learner works
from right to left on the remaining utterance, iter-
atively removing words from the end of the utter-
ance if possible. Similar to the approach used in
Initial Subtraction, the longest word that is a suf-
fix word of the remaining syllables is segmented
off, and this is repeated until the entire utterance is
segmented or syllables remain that are not suffixed
by any known word.
The ability to abstain is a significant difference
between this learner and most recent work on this
task. Because the learner has a structural descrip-
tion for a word, the USC, it is able to reject any
hypothesized words that do not meet the descrip-
tion. This improves the learner?s precision and
recall because it reduces the number of incorrect
predictions the learner makes. The USC also al-
lows the learner keep impossible words out of its
lexicon.
91
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
1.0
Count
Prob
abili
ty of
 Rec
all
Figure 1: The selected probabilistic memory func-
tion for ? = 0.05. The dashed line at 0.05 rep-
resents the threshold above which a word is more
likely than not to be recalled, occurring at a count
of approximately 14.
5 A Probabilistic Lexicon
To simulate the imperfect memory of a child
learner, we use a simple exponential function to
generate the probability with which a word is re-
trieved from the lexicon:
pr(word) = 1.0? e
??c(word)
pr(word) is the probability of a word being re-
trieved, ? is a constant, and c(word) is the number
of times the word has been identified in segmen-
tations thus far. This type of memory function is
a simplified representation of models of humans?
memory recall capabilities (Anderson et al, 1998;
Gillund and Shiffrin, 1984). This memory func-
tion for the value of ? = 0.05, the value used in
our experiments, is given in Figure 1. We later
show that the choice of ? has little impact on the
learner?s segmentation performance, and thus the
more or less arbitrary selection of a value for ? is
of little consequence.
When the algorithm attempts to subtract words
from the beginning or end of an utterance, it may
miss words in the lexicon due to this probabilis-
tic retrieval. The learner only has one opportu-
nity to recall a word in a given utterance. For ex-
ample, in the utterance P.EH1.N S.AH0.L (pencil),
if the learner has P.EH1.N and P.EH1.N S.AH0.L
in its lexicon but P.EH1.N is more frequent, it
may fail to recall P.EH1.N S.AH0.L when exam-
ining the second syllable but succeed in recog-
nizing P.EH1.N in the first. Thus it will break
off P.EH1.N instead of P.EH1.N S.AH0.L. This
means the learner may fail to reliably break off
the longest words, instead breaking off the longest
word that is successfully recalled.
While probabilistic memory means that the
learner will fail to recognize words it has seen be-
fore, potentially decreasing recall, it also provides
the learner the benefit of probabilistically failing
to repeat previous mistakes if they occur rarely.
Probabilistic word recall results in a ?rich
get richer? phenomenon as the learner segments;
words that are used more often in segmentations
are more likely to be reused in later segmentations.
While recent work from Bayesian approaches has
used a Dirichlet Process to generate these distri-
butions (Goldwater et al, 2006), in this learner the
reuse of frequent items in learning is a result of
the memory model rather than an explicit process
of reusing old outcomes or generating new ones.
This growth is an inherent property of the cogni-
tive model of memory used here rather than an ex-
ternally imposed computational technique.
6 Evaluation
Our computational model is designed to process
child-directed speech. The corpus we use to eval-
uate it is the same corpus used by Yang (2004).
Adult utterances were extracted from the Brown
(1973) data in the CHILDES corpus (MacWhin-
ney, 2000), consisting of three children?s data:
Adam, Eve, and Sarah. We obtained the pho-
netic transcriptions of words from the Carnegie
Mellon Pronouncing Dictionary (CMUdict) Ver-
sion 0.6 (Weide, 1998), using the first pronunci-
ation of each word. In CMUdict, lexical stress
information is preserved by numbers: 0 for un-
stressed, 1 for primary stress, 2 for secondary
stress. For instance, cat is represented as K.AE1.T,
catalog is K.AE1.T.AH0.L.AO0.G, and catapult is
K.AE1.T.AH0.P.AH2.L.T. We treat primary stress
as ?strong? and secondary or unstressed syllables
as ?weak.?
For each word, the phonetic segments were
grouped into syllables. This process is straightfor-
ward by the use of the principle ?Maximize On-
set,? which maximizes the length of the onset as
long as it is valid consonant cluster of English, i.e.,
92
it conforms to the phonotactic constraints of En-
glish. For example, Einstein is AY1.N.S.T.AY0.N
as segments and parsed into AY1.N S.T.AY0.N as
syllables: this is because /st/ is the longest valid
onset for the second syllable containing AY0 while
/nst/ is longer but violates English phonotac-
tics. While we performed syllabification as a pre-
processing step outside of learning, a child learner
would presumably learn the required phonotac-
tics as a part of learning to segment words. 9-
month old infants are believed to have learned
some phonotactic constraints of their native lan-
guage (Mattys and Jusczyk, 2001), and learning
these constraints can be done with only minimal
exposure (Onishi et al, 2002).
Finally, spaces and punctuation between
words were removed, but the boundaries be-
tween utterances?as indicated by line breaks in
CHILDES?are retained. Altogether, there are
226,178 words, consisting of 263,660 syllables.
The learning material is a list of unsegmented
syllable sequences grouped into utterances, and
the learner?s task is to find word boundaries that
group substrings of syllables together, building a
lexicon of words as it segments.
We evaluated the learner?s performance to ad-
dress these questions:
? How does probabilistic memory affect
learner performance?
? How much does degrading stress information
relied on by USC segmentation reduce per-
formance?
? What is the interaction between the proba-
bilistic lexicon and non-idealized stress infor-
mation?
To evaluate the learner, we tested configurations
that used a probabilistic lexicon and ones with per-
fect memory in two scenarios: Dictionary Stress,
and Reduced Stress. We create the Reduced Stress
condition in order to simulate that stress is of-
ten reduced in casual speech, and that language-
specific stress rules may cause reductions or shifts
in stress that prevent two strong syllables from oc-
curring in sequence. The difference between the
scenarios is defined as follows:
Dictionary Stress. The stress information is
given to the learner as it was looked up in CMU-
dict. For example, the first utterance from the
Adam corpus would be B.IH1.G D.R.AH1.M (big
drum), an utterance with two stressed monosyl-
lables (SS). In most languages, however, condi-
tions where two stressed syllables are in sequence
are handled by reducing the stress of one syllable.
This is simulated in the reduced stress condition.
Reduced Stress. The stress information ob-
tained from CMUdict is post-processed in the con-
text of each utterance. For any two adjacent pri-
mary stressed syllables, the first syllable is re-
duced from a strong syllable to a weak one. This is
applied iteratively from left to right, so for any se-
quence of n adjacent primary-stress syllables, only
the nth syllable retains primary stress; all others
are reduced. This removes the most valuable clue
as to where utterances can be segmented, as USC
segmentation no longer applies. This simulates the
stress retraction effect found in real speech, which
tries to avoid adjacent primary stresses.
Learners that use probabilistic memory were al-
lowed to iterate over the input two times with ac-
cess to the lexicon developed over previous iter-
ations but no access to previous segmentations.
This simulates a child hearing many of the same
words and utterances again, and reduces the effect
of the small corpus size used on the learning pro-
cess. Because the probabilistic memory reduces
the algorithm?s ability to build a lexicon, perfor-
mance in a single iteration is lower than perfect
memory conditions. In all other conditions, the
learner is allowed only a single pass over the cor-
pus.
The precision and recall metrics are calculated
for the segmentation that the learner outputs and
the lexicon itself. For an utterance, each word
in the learner?s segmentation that also appears in
the gold standard segmentation is counted as cor-
rect, and each word in the learner?s segmentation
not present in the gold standard segmentation is
a false alarm. F-score is computed using equally
balanced precision and recall (F0). The correct
words, false words, and number of words in the
gold standard are summed over the output in each
iteration to produce performance measures for that
iteration.
Precision, recall, and F-score are similarly com-
puted for the lexicon; every word in the learner?s
lexicon present in the gold standard is counted as
correct, and every word in the learner?s lexicon not
present in the gold standard is a false alarm. These
computations are performed over word types in
the lexicon, thus all words in the lexicon are of
93
equal weight in computing performance regard-
less of their frequency. In the probabilistic mem-
ory conditions, however, the memory function de-
fines the probability of each word being recalled
(and thus being considered a part of the lexicon)
at evaluation time.
In addition to evaluating the learner, we also im-
plemented three baseline approaches to compare
the learner against. The Utterance baseline seg-
menter assumes every utterance is a single word.
The Monosyllabic baseline segmenter assumes ev-
ery syllable is a single word. The USC segmenter
inserts word boundaries between all adjacent syl-
lables with primary stress in the corpus.
6.1 Results
The performance of the learner and baseline seg-
menters is given in Table 1. While the Utterance
segmenter provides expectedly poor performance,
the Monosyllabic segmenter sets a relatively high
baseline for the task. Because of the impoverished
morphology of English and the short words that
tend to be used in child-directed speech, assuming
each syllable is a word proves to be an excellent
heuristic. It is unlikely that this heuristic will per-
form as well in other languages. Because the USC
segmenter only creates segmentation points where
there are words of adjacent primary stress, it is
prone to attaching unstressed monosyllabic func-
tion words to content words, causing very low lex-
icon precision (13.56%).
With both perfect memory and dictionary stress
information, the learner attains an F-score of
86.69%, with precision (83.78%) lower than re-
call (89.81%). First, we consider the effects of
probabilistic memory on the learner. In the Dictio-
nary Stress condition, using probabilistic memory
decreases Fo by 1.15%, a relatively small impact
given that with the setting of ? = 0.05 the learner
must use a word approximately 14 times before it
can retrieve it with 50% reliability and 45 times
before it can retrieve it with 90% reliability. In the
first iteration over the data set, 17.87% of lexicon
lookups for words that have been hypothesized be-
fore fail. The impact on F0 is caused by a drop in
recall, as would be expected for a such a memory
model.
To examine the effect of the ? parameter for
probabilistic memory on learner performance, we
plot the utterance and lexicon F0 after the learner
iterates over the corpus once in the Probabilistic
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Memory Parameter
F?S
core
l l l l l l l l l l l
l UtteranceLexicon
Figure 2: Learner utterance and lexicon F-scores
after two iterations when ? is varied in the Proba-
bilistic Memory, Dictionary Stress condition
Perfect
Memory,
Dictionary
Stress
Perfect
Memory,
Reduced
Stress
USC Seg. 114,333 0
Initial Sub. 65,800 164,989
Final Sub. 5,690 14,813
Total 185,823 179,802
Table 2: Number of segmentations performed by
each operation: USC Segmentation, Initial Sub-
traction, and Final Subtraction.
Memory, Dictionary Stress condition. As Figure 2
shows, the choice of ? has little effect on the ut-
terance F0 through most of a broad range from
0.01 to 0.9. Because the setting of ? determines
the number of times a word must be hypothesized
before it can reliably be recalled, it expectedly
has a significant effect on lexicon F0. The selec-
tion of ? = 0.05 for our experiments is thus un-
likely to have had any significant bearing on the
utterance segmentation performance, although for
lower values of ? precision is favored while for
larger values recall is favored. Larger values of
? imply the learner is able to recall items after
fewer exposures. While a larger value of ? would
have yielded higher performance in lexicon per-
formance, it also assumes much more about the
learner?s memory capabilities.
The Reduced Stress condition also has only a
94
Utterances Lexicon
Segmenter Precision Recall F0 Precision Recall F0
Utterance 18.61% 4.67% 7.47% 3.57% 30.35% 6.39%
Monosyllabic 73.29% 85.44% 78.90% 55.41% 43.88% 48.97%
USC 81.06% 61.52% 69.95% 13.56% 66.97% 22.55%
Perfect Memory, Dictionary Stress 83.78% 89.81% 86.69% 67.72% 58.60% 62.83%
Perfect Memory, Reduced Stress 82.32% 85.81% 84.03% 39.18% 50.08% 43.97%
Prob. Memory, Dictionary Stress 84.05% 87.07% 85.54% 72.34% 30.01% 42.42%
Prob. Memory, Reduced Stress 84.85% 85.24% 85.05% 41.13% 22.91% 29.43%
Table 1: Baseline and Learner Performance. Performance is reported after two iterations over the corpus
for probabilistic memory learners and after a single iteration for all other learners.
small impact on utterance segmentation perfor-
mance. This suggests that the USC?s primary
value to the learner is in constraining the contents
of the lexicon and identifying words in isolation as
good candidates for the lexicon. In the Reduced
Stress condition where the USC is not directly re-
sponsible for any segmentations as there are no
adjacent primary-stressed syllables, the learner re-
lies much more heavily on subtractive techniques.
Table 2 gives the number of segmentations per-
formed using each segmentation operation. The
total number of segmentations is very similar be-
tween the Dictionary and Reduced Stress condi-
tions, but because USC Segmentation is not effec-
tive on Reduced Stress input, Initial and Final Sub-
traction are used much more heavily.
7 Discussion
The design of the segmenter presented here sug-
gests that both the quality of memory and the
structural purity of the input would be critical fac-
tors in the learner?s success. Our results suggest,
however, that using probabilistic memory and a
less idealized version of stress in natural language
have little impact on the performance of the pre-
sented learner. They do cause the learner to learn
much more slowly, causing the learner to need to
be presented with more material and resulting in
worse performance in the lexicon evaluation. But
this slower learning is unlikely to be a concern for
a child learner who would be exposed to much
larger amounts of data than the corpora here pro-
vide.
Cognitive literature suggests that limited mem-
ory during learning may be essential to a learner in
its early stages (Elman, 1993). But we do not see
any notable improvement as a result of the prob-
abilistic memory model used in our experiments,
although the learner does do better in the Reduced
Stress condition with Probabilistic Memory than
Perfect Memory. This should not be interpreted
as a negative result as we only analyze a single
learner and memory model. Adding decay to the
model such that among words of equal frequency
those that have not been used in segmentation re-
cently are less likely to be remembered may be
sufficient to create the desired effect.
The success of this learner suggests that the
type of ?bootstrapping? approaches can succeed
in word segmentation. The learner presented uses
USC to identify utterances that are likely to be
lone words, seeding the lexicon with initial infor-
mation. Even if these first items in the lexicon are
of relatively low purity, often combining function
words and content words into one, the learner is
able to expand its lexicon by using these hypothe-
sized words to segment new input. As the learner
segments more, these hypotheses become more re-
liable, allowing the learner to build a lexicon of
good quality.
The subtraction approaches presented in this
work provide a basic algorithm for to handling
segmentation of incoming data in an online fash-
ion. The subtractive heuristics used here are of
course not guaranteed to result in a perfect seg-
mentation even with a perfect lexicon; they are
presented to show how a simple model of pro-
cessing incoming data can be paired with struc-
tural constraints on the hypothesis space to learn
word segmentation in a computationally efficient
and cognitively plausible online fashion.
8 Conclusions
The learner?s strong performance using minimal
computational resources and unreliable memory
suggest that simple learners can succeed in un-
95
supervised tasks as long as they take advantage
of domain-specific knowledge to constrain the hy-
pothesis space. Our results show that, even in ad-
versarial conditions, structural constraints remain
powerful tools for simple learning algorithms in
difficult tasks.
Future work in this area should focus on learn-
ers that can take advantage of the benefits of a
probabilistic lexicon and memory models suited
to them. Also, a more complex model of the type
of stress variation present in natural speech would
help better determine a learner that uses USC?s
ability to handle realistic variation in the input.
Our model of stress reduction is a worst-case sce-
nario for USC segmentation but is unlikely to be
an accurate model of real speech. Future work
should adopt a more naturalistic model to deter-
mine whether the robustness found in our results
holds true in more realistic stress permutations.
Acknowledgements
We thank Kyle Gorman, Josef Fruehwald, and
Dan Swingley for their helpful discussions regard-
ing this work. We are grateful to and Mitch Mar-
cus and Jana Beck for their feedback on earlier
versions of this paper.
References
J.R. Anderson, D. Bothell, C. Lebiere, and M. Matessa.
1998. An integrated theory of list memory. Journal
of Memory and Language, 38(4):341?380.
R. Bijeljac-Babic, J. Bertoncini, and J. Mehler. 1993.
How do 4-day-old infants categorize multisyllabic
utterances? Developmental Psychology, 29:711?
711.
M.R. Brent and T.A. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful for
segmentation. Cognition, 61(1-2):93?125.
M.R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1):71?105.
R. Brown. 1973. A First Language: The Early
Stages. Harvard Univ. Press, Cambridge, Mas-
sachusetts 02138.
J.L. Elman. 1993. Learning and development in neural
networks: The importance of starting small. Cogni-
tion, 48(1):71?99.
G. Gillund and R.M. Shiffrin. 1984. A retrieval model
for both recognition and recall. Psychological Re-
view, 91(1):1?67.
E.M. Gold. 1967. Language identification in the limit.
Information and control, 10(5):447?474.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. In-
terpolating between types and tokens by estimating
power-law generators. Advances in Neural Informa-
tion Processing Systems, 18:459.
S. Goldwater, T.L. Griffiths, and M. Johnson. 2009.
A Bayesian framework for word segmentation: Ex-
ploring the effects of context. Cognition.
M. Halle and J.R. Vergnaud. 1987. An essay on stress.
MIT Press.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on un-
supervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 317?325. Association for
Computational Linguistics.
P.W. Jusczyk, D.M. Houston, and M. Newsome. 1999.
The Beginnings of Word Segmentation in English-
Learning Infants. Cognitive Psychology, 39(3-
4):159?207.
P.W. Jusczyk. 1999. How infants begin to extract
words from speech. Trends in Cognitive Sciences,
3(9):323?328.
M. Liberman and A. Prince. 1977. On stress and lin-
guistic rhythm. Linguistic Inquiry, 8(2):249?336.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates.
G.F. Marcus, S. Pinker, M. Ullman, M. Hollander, T.J.
Rosen, F. Xu, and H. Clahsen. 1992. Overregular-
ization in language acquisition. Monographs of the
Society for Research in Child Development, 57(4).
S.L. Mattys and P.W. Jusczyk. 2001. Phonotactic cues
for segmentation of fluent speech by infants. Cogni-
tion, 78(2):91?121.
K.H. Onishi, K.E. Chambers, and C. Fisher. 2002.
Learning phonotactic constraints from brief auditory
experience. Cognition, 83(1):B13?B23.
J.R. Saffran, R.N. Aslin, and E.L. Newport. 1996a.
Statistical Learning by 8-month-old Infants. Sci-
ence, 274(5294):1926.
J.R. Saffran, E.L. Newport, and R.N. Aslin. 1996b.
Word Segmentation: The Role of Distributional
Cues. Journal of Memory and Language,
35(4):606?621.
D. Swingley. 2005. Statistical clustering and the con-
tents of the infant vocabulary. Cognitive Psychol-
ogy, 50(1):86?132.
LG Valiant. 1984. A theory of the learnable. Commu-
nications of the ACM, 27(11):1142.
96
D. Van Kuijk and L. Boves. 1999. Acoustic char-
acteristics of lexical stress in continuous telephone
speech. Speech Communication, 27(2):95?111.
V.N. Vapnik. 2000. The nature of statistical learning
theory. Springer.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational
Linguistics, 27(3):351?372.
R.L. Weide. 1998. The Carnegie Mellon Pronouncing
Dictionary [cmudict. 0.6].
C.D. Yang. 2004. Universal Grammar, statistics or
both? Trends in Cognitive Sciences, 8(10):451?456.
97
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 29?38,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Modeling Infant Word Segmentation
Constantine Lignos
Department of Computer and Information Science
University of Pennsylvania
lignos@cis.upenn.edu
Abstract
While many computational models have been
created to explore how children might learn
to segment words, the focus has largely been
on achieving higher levels of performance and
exploring cues suggested by artificial learning
experiments. We propose a broader focus that
includes designing models that display prop-
erties of infants? performance as they begin
to segment words. We develop an efficient
bootstrapping online learner with this focus in
mind, and evaluate it on child-directed speech.
In addition to attaining a high level of perfor-
mance, this model predicts the error patterns
seen in infants learning to segment words.
1 Introduction
The last fifteen years have seen an increased inter-
est in the problem of how infants learn to segment
a continuous stream of speech into words. Much of
this work has been inspired by experiments with in-
fants focusing on what capabilities infants have and
which cues they attend to. While experimental work
provides insight into the types of cues infants may be
using, computational modeling of the task provides
a unique opportunity to test proposed cues on rep-
resentative data and validate potential approaches to
using them.
While there are many potential approaches to the
problem, a desirable solution to the problem should
demonstrate acceptable performance in a simula-
tion of the task, rely on cues in the input that an
infant learner is able to detect at the relevant age,
and exhibit learning patterns similar to those of in-
fant learners. Most work in computational model-
ing of language acquisition has primarily focused
on achieving acceptable performance using a sin-
gle cue, transitional probabilities, but little effort has
been made in that work to try to connect these learn-
ing solutions to the actual learning patterns observed
in children outside of performance on short artificial
language learning experiments.
In this work we present a simple, easily extended
algorithm for unsupervised word segmentation that,
in addition to achieving a high level of performance
in the task, correlates with the developmental pat-
terns observed in infants. We discuss the connec-
tions between the design and behavior of our algo-
rithm and the cognitive capabilities of infants at the
age at which they appear to begin segmenting words.
We also discuss how our technique can easily be ex-
tended to accept additional cues to word segmenta-
tion beyond those implemented in our learner.
2 Related Work
As this paper examines the intersection of infants?
capabilities and computational modeling, we discuss
work in both domains, beginning with experimental
approaches to understanding how infants may per-
form the task of word segmentation.
2.1 Infant Word Segmentation
A potential account of how infants learn to iden-
tify words in fluent speech is that they learn words
in isolation and then use those words to segment
longer utterances (Peters, 1983; Pinker et al, 1984).
It is not clear, however, that infant-directed speech
provides enough detectable words in isolation for
29
such a strategy (Aslin et al, 1996). Whatever iso-
lated words children do hear, they appear to attend to
them; whether a word is heard in isolation is a better
predictor of whether a child has learned a word than
the word?s frequency (Brent and Siskind, 2001).
A more plausible alternative account to assume
children attend to patterns in the input, using them to
identify likely word units. Much experimental work
has followed from the finding that in artificial learn-
ing tasks, infants and adults appear to prefer word-
like units that match statistical patterns in the input
(Saffran et al, 1996b; Saffran et al, 1996a). Saffran
et al and the authors of following studies (Aslin et
al., 1998; Saffran, 2001, among many others) sug-
gest that participants used transitional probabilities
to succeed in these experiments, but the actual strat-
egy used is unclear and may even be an artifact of
the perceptual system (Perruchet and Vinter, 1998;
Hewlett and Cohen, 2009).
More recent work using real language data has
not shown transitional probabilities to be as useful
a cue as originally suggested. Lew-Williams et al
(2011) found that 9-month-old English-learning in-
fants were not able to learn high-transitional prob-
ability words in fluent Italian speech unless those
words were also presented in isolation. Given this
finding and the extensive exisiting modeling work
focusing on the used of transitional probabilities, we
believe it is crucial to additionally explore segmen-
tation strategies that rely on other cues in the input.
2.2 Modeling Word Segmentation
While experimental work has posited simple algo-
rithms that infants might use to accomplish the task
of word segmentation, when applied to real language
data these techniques have yielded very poor results
(Yang, 2004). This problem has created a chal-
lenge for researchers modeling language acquisition
to suggest more sophisticated strategies that infants
might use. These approaches have fallen into two
primary categories: optimization-based and boot-
strapping algorithm strategies.
Optimization-based strategies have focused on
techniques that a learner might use to arrive at an
optimal segmentation, either through a dynamic pro-
gramming approach (Brent, 1999), online learning
(Venkataraman, 2001), or nonparametric Bayesian
inference (Goldwater et al, 2009; Johnson and
Goldwater, 2009). These approaches fit within stan-
dard statistical approaches to natural language pro-
cessing, defining statistical objectives and inference
strategies, with the learners trying to optimize some
combination of the quality of its lexicon and repre-
sentations of the corpus.
In contrast, bootstrapping approaches (Gambell
and Yang, 2004; Lignos and Yang, 2010) to word
segmentation have focused on simple heuristics for
populating a lexicon and strategies for using the con-
tents of the lexicon to segment utterances. These ap-
proaches have focused on a procedure for segmen-
tation rather than defining an optimal segmentation
explicitly, and do not define a formal objective that
is to be optimized.
While bootstrapping approaches have generally
made stronger attempts to align with infants abili-
ties to process the speech signal (Gambell and Yang,
2004) than other approaches, little effort has been
made to connect the details of an implemented seg-
mentation strategy with children?s learning patterns
since the earliest computational models of the task
(Olivier, 1968). It is important to draw a con-
trast here between attempts to match patterns of hu-
man development with regard to word segmentation
with attempts to model performance in artificial lan-
guage learning experiments whose goal is to probe
word segmentation abilities in humans (Frank et al,
2010). In this paper we are focused on matching the
progression of development and performance in nat-
uralistic experiments to characteristics of a segmen-
tation strategy, an approach similar to that employed
in English past tense learning (Rumelhart and Mc-
Clelland, 1986; Pinker, 2000; Yang, 2002).
We will now discuss the patterns of development
for children learning to segment English words,
which form the motivation for the design of our seg-
menter.
3 Infant Performance in Word
Segmentation
While the developmental patterns of English-
learning infants have been broadly studied, it has
been difficult to identify errors that must be caused
by failures to correctly segment words and not other
cognitive limitations, issues of morphological pro-
ductivity, or syntactic competency issues.
30
Brown (1973) offers one of the most compre-
hensive examinations of the types of errors that
young infants make regarding word segmentation.
He notes that Adam?s common errors included treat-
ing it?s-a, that-a, get-a, put-a, want-to, and at-that
as single words, as judged by various misproduc-
tions that involved these items. A possible analysis
of these errors is that in addition to the high level of
frequency with which those syllables co-occur, ele-
ments such as a and to do not carry any identifiable
amount of stress in natural speech.
In addition to the undersegmentations that Brown
identifies, Peters (1983) identifies the pattern of
oversegmenting function words begin other words,
including this famous dialog between a parent and
child, where in the child?s response have is pro-
nounced in the same way as the second syllable of
behave: Parent: Behave! Child: I am have!
The response by the child indicates that they have
analyzed behave as be have. There are two major
factors that could contribute to such an analysis: the
high frequency of be leading to it being treated as a
separate word (Saffran et al, 1996b), and the lack of
stress on be but stress on have which forms a word
contrary to the dominant pattern of stress in English
(Cutler and Butterfield, 1992).
Infants appear to use the ends of utterances to aid
segmentation, and as early at 7.5 months old they
are able to recognize novel words in fluent speech if
the novel words are presented at the ends of an utter-
ance and not utterance medially (Seidl and Johnson,
2006). Thus the reliable boundaries presented by the
edge of an utterance should be treated as informative
for a learner.
Most crucially, the syllable seems to be the unit
children use to form words. Experiments that have
been performed to gauge adult and infant compe-
tency in word segmentation have been designed with
the assumption that the only possible segmentation
points are at syllable boundaries. That infants should
be able to operate on syllables is unsurprising; in-
fants as young as 4-days-old are able to discrimi-
nate words based on syllable length (Bijeljac-Babic
et al, 1993) and phonotactic cues to syllable bound-
aries seem to be rapidly acquired by infants (On-
ishi et al, 2002). The use of the syllable in exper-
imental work on word segmentation stands in con-
trast to many computational models that have oper-
ated at the phoneme level (Brent, 1999; Venkatara-
man, 2001; Goldwater et al, 2009). An exception
to the focus on phoneme-based segmentation is the
joint learning model proposed by Johnson (2008)
that learns syllabification and other levels of repre-
sentation jointly with word segmentation, but that
model poses problems as a developmentally relevant
approach in that it predicts unattested joint syllabifi-
cation/segmentation errors by infants and problems
as a linguistically relevant approach due to its non-
phonotactic approach to learning syllabification.
From this survey, we see some relevant phenom-
ena that a good model of infant word segmentation
should replicate. (1) The learner should operate on
syllables. (2) At some stage of learning, underseg-
mentation function word collocations (e.g., that-a
should occur. (3) At some stage of learning, over-
segmentation of function words that may begin other
words (e.g., be-have) should occur. (4) The learner
should attend to the ends of utterances as use them
to help identify novel words.
4 An Algorithm for Segmentation
The algorithm we propose is similar in style to previ-
ous online bootstrapping segmenters (Gambell and
Yang, 2004; Lignos and Yang, 2010) but varies in a
few crucial aspects. First, it inserts word boundaries
in a left-to-right fashion as it processes each utter-
ance (i.e., in temporal order), unlike previous mod-
els which have worked from the outside in. Second,
it can handle cases where the segmentation is am-
biguous given the current lexicon and score multiple
possible segmentations. Finally, the use of word-
level stress information is an optional part of the
model, and not an essential part of the segmenta-
tion process. This allows us to examine the addi-
tional power that stress provides on top of a sub-
tractive segmentation system and allows the model
to generalize to languages where word-level stress
is not present in the same fashion as English (e.g.,
French). We first discuss the individual operations
the algorithm uses to segment an utterance, and then
discuss how they are combined in the segmenter.
4.1 The Lexicon
The learner we propose will primarily use items in
its lexicon to help identify new possible words. The
31
structure of the lexicon is as follows:
Lexicon. The lexicon contains the phonological ma-
terial of each word that the learner has previously
hypothesized. The lexicon stores a score along with
each word, which the segmenter may increment or
decrement.
The score assigned to each entry in the lexicon
represents the relative confidence that it is a true
word of the language. Each increment simply adds
to the score of an individual word and each decre-
ment subtracts from it.
4.2 Subtractive Segmentation
Subtractive segmentation is the process of using
known words to segment the speech signal, which
infants appear to be able to do as young as at six
months of age (Bortfeld et al, 2005).
Subtractive Segmentation. When possible, remove
a known word in the lexicon from the front of the
utterance being segmented.
One way to apply subtractive segmentation is a
greedy score-based heuristic for subtractive segmen-
tation (Lignos and Yang, 2010), such that whenever
multiple words in the lexicon could be subtracted
from an utterance, the entry with the highest score
will deterministically be used. This greedy approach
results in a ?rich get richer? effect of the sort seen in
Dirichlet processes (Goldwater et al, 2009). We will
first discuss this approach and then later extend this
greedy search to a beam search.
Figure 1 gives the implementation of subtractive
segmentation in our algorithm. This algorithm re-
sults in the following properties:
Initially, utterances are treated as words in isola-
tion. When the lexicon is empty, no word bound-
aries will be inserted and the full contents of each
utterance will be added to the lexicon as a word.
High-frequency words are preferred. When pre-
sented with a choice of multiple words to subtract,
the highest scored word will be subtracted, which
will prefer higher frequency words over lower fre-
quency words in segmentation.
Syllables between words are not necessarily con-
sidered words. Syllables that occur between sub-
tractions are not added as words in the lexicon. For
example, if play and please are in the lexicon but
checkers is not, the utterance play checkers please
will be correctly segmented, but checkers will not
be added to the lexicon. Much like infants appear
to do, the learner does not place as much weight on
less reliable boundaries hypothesized in the middle
of an utterance (Seidl and Johnson, 2006).
4.3 Incorporating Stress Information
A particularly useful constraint for defining a word,
introduced to the problem of word segmentation by
Yang (2004) but previously discussed by Halle and
Vergnaud (1987), is as follows:
Unique Stress Constraint (USC): A word can bear
at most one primary stress.
Yang (2004) evaluated the effectiveness of the
USC in conjunction with a simple approach to us-
ing transitional probabilities, showing significant
performance improvements. The availability of
such stress cues is not, however, an uncontroversial
assumption; there are no language-universal cues
to stress and even within a single language auto-
matic detection of word-level stress is still unreli-
able (Van Kuijk and Boves, 1999), making auto-
matic capture of such data for simulation purposes
difficult.
Before taking advantage of word-level stress in-
formation, the infant learner would need to iden-
tify the acoustic correlates to word-level stress in her
language, and we will not address the specific mech-
anisms that an infant learner may use to accomplish
the task of identifying word-level stress in this paper.
Based on strong experimental evidence that infants
discriminate between weakly and strongly stressed
syllables and use it to group syllables into word-like
units (Jusczyk et al, 1999), we assume that an infant
may attend to this cue and we evaluate our model
with and without it.
We adopt the USC for segmentation in the follow-
ing fashion:
Unique Stress Segmentation (USS). Insert word
boundaries such that no word contains two strong
stresses. Do so in a lazy fashion, inserting bound-
aries as a last resort just before adding another syl-
lable to the current would cause it to contain two
strong stresses.
32
u? the syllables of the utterance, initially with no word boundaries
i? 0
while i < len(u) do
if u starts with one or more words in the lexicon then
Choose the highest scoring word w and remove it from the front of u by inserting a word boundary before and after it.
Increment the score of w
Advance i to the last word boundary inserted
else
Advance i by one syllable
end if
end while
Add the syllables between the last boundary inserted (or the beginning of the utterance if no boundaries were inserted) and the
end of the utterance as a word in the lexicon with a score of 1
Figure 1: Subtractive segmentation procedure
u? the syllables of the utterance, initally with no word boundaries
i? 0
seenStress? False
while i < len(u)? 1 do
if u[i] is stressed then
seenStress? True
end if
if seenStress and u[i+ 1] is stressed then
Insert a word boundary between u[i] and u[i+ 1]
w ? the syllables between the previous boundary inserted (or the beginning of the utterance if no boundaries were inserted)
and the boundary just inserted
Increment w?s score in the lexicon, adding it to the lexicon if needed
seenStress? False
end if
i? i+ 1
end while
w ? the syllables between the last boundary inserted (or the beginning of the utterance if no boundaries were inserted) and the
end of the utterance
Increment w?s score in the lexicon, adding it to the lexicon if needed
Figure 2: A Unique Stress Segmentation Algorithm
This strategy is expressed in an algorithmic form
in Figure 2. The learner uses USS as a last resort
to prevent creating a segmentation with an impossi-
ble amount of stress in a single word. For example
consider an unsegmented English utterance with the
stressed syllables underlined: Givemetheball. Ap-
plying USS would create the following segmenta-
tion: Givemethe ball.
A USS-based algorithm would note the stress on
the first syllable, then keep scanning until another
stress is located on the fourth syllable, inserting a
break between the two. Givemethe and ball would
be added to the lexicon. While this is not a per-
fect segmentation, it can be used to aid subtractive
segmentation by seeding the lexicon, even if not all
entries added to the lexicon are not correct.
4.4 Combining Subtraction and Stress
Information
Given our bootstrapping methodology, it is highly
desirable to be able to integrate USS along with sub-
tractive segmentation. An algorithm that combines
both is shown in Figure 3.
4.5 Extending to Beam Search
The greedy segmentation proposed is limited in its
ability to find a good segmentation by its reliance on
local decisions. A frequent undersegmentation error
of the greedy segmenter is of this type: partof an
apple. Because partof has a higher score than part
at the point in learning where this utterance is en-
countered, the greedy segmenter will always choose
partof.
An alternative approach is to let the segmenter
33
u? the syllables of the utterance, initally with no word boundaries
i? 0
while i < len(u) do
if USS requires a word boundary then
Insert a word boundary and advance i, updating the lexicon as needed
else if Subtractive Segmentation can be performed then
Subtract the highest scoring word and advance i, updating the lexicon as needed
else
Advance i by one syllable
end if
end while
w ? the syllables between the last boundary inserted (or the beginning of the utterance if no boundaries were inserted) and the
end of the utterance
Increment w?s score in the lexicon, adding it to the lexicon if needed
Figure 3: An algorithm combining USS and Subtractive Segmentation
explore multiple hypotheses at once, using a sim-
ple beam search. New hypotheses are added to
support multiple possible subtractive segmentations.
For example, using the utterance above, at the be-
ginning of segmentation either part or partof could
be subtracted from the utterance, and both possi-
ble segmentations can be evaluated. The learner
scores these hypotheses in a fashion similar to the
greedy segmentation, but using a function based on
the score of all words used in the utterance. The
geometric mean has been used in compound split-
ting (Koehn and Knight, 2003), a task in many ways
similar to word segmentation, so we adopt it as the
criterion for selecting the best hypothesis. For a
hypothesized segmentation H comprised of words
wi . . . wn, a hypothesis is chosen as follows:
argmax
H
(
?
wi?H
score(wi))
1
n
For any w not found in the lexicon we must assign
a score; we assign it a score of one as that would
be its value assuming it had just been added to the
lexicon, an approach similar to Laplace smoothing.
Returning to the previous example, while the
score of partof is greater than that of part, the score
of of is much higher than either, so if both partof
an apple and part of an apple are considered, the
high score of of causes the latter to be chosen.
When beam search is employed, only words used in
the winning hypothesis are rewarded, similar to the
greedy case where there are no other hypotheses.
In addition to preferring segmentations that use
words of higher score, it is useful to reduce the
Algorithm Word Boundaries
Precision Recall F-Score
No Stress Information
Syllable Baseline 81.68 100.0 89.91
Subtractive Seg. 91.66 89.13 90.37
Subtractive Seg. + Beam 2 92.74 88.69 90.67
Word-level Stress
USS Only 91.53 18.82 31.21
USS + Subtractive Seg. 93.76 92.02 92.88
USS + Subtractive Seg. +
Beam 2
94.20 91.87 93.02
Table 1: Learner and baseline performance
score of words that led to the consideration of a los-
ing hypothesis. In the previous example we may
want to penalize partof so that we are less likely to
choose a future segmentation that includes it. Set-
ting the beam size to be two, forcing each hypothesis
to develop greedily after an ambiguous subtraction
causes two hypotheses to form, we are guaranteed
a unique word to penalize. In the previous example
partof causes the split between the two hypotheses
in the beam, and thus the learner penalizes it to dis-
courage using it in the future.
5 Results
5.1 Evaluation
To evaluate the performance of our model, we mea-
sured performance on child-directed speech, using
the same corpus used in a number of previous stud-
ies that used syllabified input (Yang, 2004; Gambell
and Yang, 2004; Lignos and Yang, 2010). The eval-
34
uation set was comprised of adult utterances from
the Brown (1973) data of the CHILDES database
(MacWhinney, 2000).1 Phonemic transcriptions of
words from the Carnegie Mellon Pronouncing Dic-
tionary (CMUdict) Version 0.7 (Weide, 1998), us-
ing the first pronunciation for each word and mark-
ing syllables with level 1 stress as strong syllables.
The corpus was syllabified using onset maximiza-
tion. Any utterance in which a word could not be
transcribed using CMUDICT was excluded, leaving
55,840 utterances. We applied a probabilistic re-
call function to the lexicon to simulate the fact that
a child learner will not perfectly recall all hypothe-
sized words either due to memory limitations, vari-
ability in the input, or any other possible source of
failure. We used the same function and constant as
used by Lignos and Yang (2010).
To adjust the word-level stress information to bet-
ter reflect natural speech, the stress information ob-
tained from CMUdict was post-processed in the con-
text of each utterance using the technique of Lig-
nos and Yang (2010). For any n adjacent primary-
stress syllables, only the nth syllable retains primary
stress; all others are made into weak syllables. This
reflects the fact that stress clash is avoided in English
and that infants may not reliably detect acoustic cor-
relates of stress in the input.
In addition to variations of our algorithm, we eval-
uated a baseline segmenter which marks every syl-
lable boundary as a word boundary, treating each
syllable as a word. We tested five variants of our
algorithm, adding combinations of USS, subtractive
segmentation, and adding beam search with a beam
size of two2 to subtractive segmentation.
Precision and recall metrics were calculated over
all word boundaries over all utterances in the cor-
pus. The segmenter?s task is effectively to classify
each syllable boundary as a word boundary or not.
As single-syllable utterances are unambiguously a
single word with no possible boundaries, they are
1A separate set of previous studies have used a corpus se-
lected by Brent (1999) for evaluation. Due to length limitations
and the fact that the results presented here cannot be meaning-
fully compared to those studies, we only present results on the
Brown (1973) data here.
2As larger beam sizes did not lead to any benefits, partly
because they do not straightforwardly allow for penalization,
we do not report results for larger beam sizes.
excluded from evaluation but still given as input.
Evaluation was performed by giving each algo-
rithm a single pass over the data set, with the perfor-
mance on every utterance included in the total score.
This is the most challenging metric for an online
segmenter, as early mistakes made when the learner
has been exposed to no data still count against it.
5.2 Performance
The performance of several variations of our algo-
rithm is given in Table 1. The most surprising re-
sult is the high performance provided by the sylla-
ble baseline. This good performance is both an arti-
fact of English and the metrics used to evaluate the
segmenters. In English, there are larger number of
monosyllabic words than in other languages, result-
ing in high precision in addition to the guaranteed
100% recall because it predicts every possible word
boundary. The standard metric of evaluating pre-
cision and recall over word boundaries rather than
words identified in each utterance also contributes
to this performance; when this baseline is evaluated
with a word-level precision and recall it does not
perform as well (Lignos and Yang, 2010).
Subtractive Segmentation provides an improve-
ment in utterance evaluation over the Syllable Base-
line, and adding beam search to it slightly improves
F-score, sacrificing precision for recall. This is to be
expected from the penalization step in beam search;
as the penalization penalizes some good words in ad-
dition to undesirable ones, the purification of the ut-
terance segmentation and the lexicon comes at the
cost of recall from over-penalization.
While USS alone is clearly not a sufficiently rich
segmentation technique, it is important to note that
it is a high precision indicator of word boundaries,
suggesting that stress information can be useful to
the learner even when used in this simple way. More
importantly, USS contributes unique information to
subtractive segmentation, as the utterance F-score
of subtractive segmentation improves from 90.37 to
92.88.
While the performance numbers show that the
segmenter performs competently at the task, the
more significant question at hand is whether the er-
rors committed by the learner match developmental
patterns of infants. As the design of the segmenter
predicts, the main error types of the Subtractive Seg-
35
mentation + USS algorithm fall into two classes:
Function word collocations. For example, the
third highest-scored non-word in the lexicon is
that?sa, congruent with observations of function
word collocations seen in children (Brown, 1973).
Oversegmentation of function words. The
greedy approach used for segmenting the words
of highest score results in function words being
aggressively segmented off the front of words, for
example a nother. The highest scored non-word in
the lexicon is nother as a result.
Adding beam search reduces the number of func-
tion word collocations in the segmenter?s output; the
learner?s most commonly penalized lexicon entry is
isthat. However, beam search also penalizes a lot of
words, such as another. Thus the strategy used in
beam search predicts an early use of function word
collocations, followed by later oversegmentation.
6 Discussion
In the discussion of related work, we identified two
major paradigms in modeling word segmentation:
optimization and bootstrapping approaches. The al-
gorithm presented here combines elements of both.
Its behavior over time and across utterances is that of
a bootstrapping learner, but when processing each
utterance it selects a segmentation based on a sim-
ple, cognitively plausible beam search.
By using a beam search of the kind suggested, it
is easy to see how a variety of other cues could be
integrated into the learning process. We have given a
simple function for selecting the best hypothesis that
only relies on lexicon scores, but more sophisticated
functions could take multiple cues into account. For
example it has been observed that 7-month-olds at-
tend more to distributional cues while 9-month-olds
attend more to stress cues (Thiessen and Saffran,
2003). A learner in which the weight placed on
stress cues increases as the learner receives more
data would match this pattern. Other research has
suggested a more complex hierarchy of cues (Mat-
tys et al, 2005), but how the weighting of the vari-
ous cues can be adjusted with more input remains an
open question.
A crucial frontier in word segmentation is the ex-
pansion of evaluation to include other languages. As
with many other tasks, creating solutions that per-
form well in a broad variety of languages is im-
portant but has not yet been pursued. Future work
should attempt to match developmental patterns in
other languages, which will require adding morpho-
logical complexity to the system; the techniques
developed for English are unlikely to succeed un-
changed in other languages.
Comparing with other algorithms? published re-
sults is difficult because of varying choices of data
sets and metrics. For example, other syllable-based
algorithms have evaluated their performance using
word-level, as opposed to boundary-level, precision
and recall (Gambell and Yang, 2004; Lignos and
Yang, 2010). We have adopted the more popular
boundary-based metric here, but there is no way to
directly compare with work that does not use syllab-
ified input. The variety of possible evaluation met-
rics obviates the need for a longer-form exploration
of how existing approaches perform when evaluated
against varying metrics. Additionally, a more stan-
dard set of evaluation data in many languages would
greatly improve the ability to compare different ap-
proaches to this task.
7 Conclusion
The work presented here represents a step toward
bringing together developmental knowledge regard-
ing word segmentation and computational model-
ing. Rather than focusing on cues in artificial learn-
ing experiments which may or may not generalize
to the natural development of word segmentation in
children, we have shown how a simple algorithm
for segmentation mimics many of the patterns seen
in infants? developing competence. We believe this
work opens the door to a promising line of research
that will make a stronger effort to see simulations
of language acquisition as not just an unsupervised
learning task but rather a modeling task that must
take into account a broad variety of phenomena.
8 Acknowledgments
I would like to thank Charles Yang and Mitch Mar-
cus for many enlightening discussions regarding this
work. The author was supported by an NSF IGERT
grant to the University of Pennsylvania Institute for
Research in Cognitive Science.
36
References
R.N. Aslin, J.Z. Woodward, N.P. LaMendola, and T.G.
Bever. 1996. Models of word segmentation in fluent
maternal speech to infants. Signal to syntax: Boot-
strapping from speech to grammar in early acquisi-
tion, pages 117?134.
R.N. Aslin, J.R. Saffran, and E.L. Newport. 1998.
Computation of conditional probability statistics by 8-
month-old infants. Psychological Science, 9(4):321.
R. Bijeljac-Babic, J. Bertoncini, and J. Mehler. 1993.
How do 4-day-old infants categorize multisyllabic ut-
terances? Developmental Psychology, 29:711?711.
H. Bortfeld, J.L. Morgan, R.M. Golinkoff, and K. Rath-
bun. 2005. Mommy and me. Psychological Science,
16(4):298.
M.R. Brent and J.M. Siskind. 2001. The role of ex-
posure to isolated words in early vocabulary develop-
ment. Cognition, 81(2):B33?B44.
M.R. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34(1):71?105.
R. Brown. 1973. A First Language: The Early
Stages. Harvard Univ. Press, Cambridge, Mas-
sachusetts 02138.
A. Cutler and S. Butterfield. 1992. Rhythmic cues
to speech segmentation: Evidence from juncture
misperception. Journal of Memory and Language,
31(2):218?236.
M.C. Frank, S. Goldwater, T.L. Griffiths, and J.B. Tenen-
baum. 2010. Modeling human performance in statis-
tical word segmentation. Cognition.
T. Gambell and C. Yang. 2004. Statistics learning and
universal grammar: Modeling word segmentation. In
First Workshop on Psycho-computational Models of
Human Language Acquisition, page 49.
S. Goldwater, T.L. Griffiths, and M. Johnson. 2009. A
Bayesian framework for word segmentation: Explor-
ing the effects of context. Cognition.
M. Halle and J.R. Vergnaud. 1987. An essay on stress.
MIT Press.
D. Hewlett and P. Cohen. 2009. Word segmentation
as general chunking. In Psychocomputational Models
of Language Acquisition Workshop (PsychoCompLA),
July 29, 2009.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on unsu-
pervised word segmentation with adaptor grammars.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 317?325. Association for Computational
Linguistics.
M. Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In 46th Annual Meeting of the ACL, pages
398?406. Citeseer.
P.W. Jusczyk, D.M. Houston, and M. Newsome. 1999.
The Beginnings of Word Segmentation in English-
Learning Infants. Cognitive Psychology, 39(3-4):159?
207.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of the tenth con-
ference on European chapter of the Association for
Computational Linguistics-Volume 1, pages 187?193.
Association for Computational Linguistics.
C. Lew-Williams, B. Pelucchi, and J. Saffran. 2011. Iso-
lated words enhance statistical learning by 9-month-
old infants. In Budapest CEU Conference on Cogni-
tive Development 2011.
C. Lignos and C. Yang. 2010. Recession Segmenta-
tion: Simpler Online Word Segmentation Using Lim-
ited Resources. In Proceedings of CoNLL-2010, pages
88?97.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates.
S.L. Mattys, L. White, and J.F. Melhorn. 2005. In-
tegration of multiple speech segmentation cues: A
hierarchical framework. Journal of Experimental
Psychology-General, 134(4):477?500.
D.C. Olivier. 1968. Stochastic grammars and language
acquisition mechanisms: a thesis. Ph.D. thesis, Har-
vard University.
K.H. Onishi, K.E. Chambers, and C. Fisher. 2002.
Learning phonotactic constraints from brief auditory
experience. Cognition, 83(1):B13?B23.
P. Perruchet and A. Vinter. 1998. PARSER: A model
for word segmentation. Journal of Memory and Lan-
guage, 39:246?263.
A.M. Peters. 1983. The units of language acquisition.
CUP Archive.
S. Pinker, Harvard University. The President, and Fellows
of Harvard College. 1984. Language learnability
and language development. Harvard University Press
Cambridge, MA.
S. Pinker. 2000. Words and rules: The ingredients of
language. Harper Perennial.
D.E. Rumelhart and J.L. McClelland. 1986. Parallel dis-
tributed processing: Explorations in the microstruc-
ture of cognition. MIT Press, Cambridge, MA.
J.R. Saffran, R.N. Aslin, and E.L. Newport. 1996a.
Statistical Learning by 8-month-old Infants. Science,
274(5294):1926.
J.R. Saffran, E.L. Newport, and R.N. Aslin. 1996b. Word
Segmentation: The Role of Distributional Cues. Jour-
nal of Memory and Language, 35(4):606?621.
37
J.R. Saffran. 2001. Words in a sea of sounds: The output
of infant statistical learning. Cognition, 81(2):149?
169.
A. Seidl and E.K. Johnson. 2006. Infant word segmenta-
tion revisited: edge alignment facilitates target extrac-
tion. Developmental Science, 9(6):565?573.
E.D. Thiessen and J.R. Saffran. 2003. When cues col-
lide: Use of stress and statistical cues to word bound-
aries by 7-to 9-month-old infants. Developmental Psy-
chology, 39(4):706?716.
D. Van Kuijk and L. Boves. 1999. Acoustic character-
istics of lexical stress in continuous telephone speech.
Speech Communication, 27(2):95?111.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
R.L. Weide. 1998. The Carnegie Mellon Pronouncing
Dictionary [cmudict. 0.6].
C.D. Yang. 2002. Knowledge and learning in natural
language. Oxford University Press, USA.
C.D. Yang. 2004. Universal Grammar, statistics or both?
Trends in Cognitive Sciences, 8(10):451?456.
38
